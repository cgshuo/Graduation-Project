 Huayan Wang huayanw@cs.stanford.edu Daphne Koller koller@cs.stanford.edu We address the problem of energy minimization (MAP inference) in cycle MRFs. Specifically, consider a MRF over X = { X 1: N } with connectivity X 1  X  X 2  X  X  X  X   X  X N  X  X 1 . It is parametrized by N pairwise potentials: where  X  i has the scope  X  i ( X i , X i +1 ) ( X N +1 is X Our goal is to find assignments to X 1: N that minimize the energy function (1).
 Cycle MRFs could be useful as a modeling tool in its own right. For example, a cycle over landmarks could be used for top-down image segmentation (Heitz et al., 2009). More importantly, cycles have been recognized as a crucial component in tackling general MRFs (Werner, 2008; Sontag &amp; Jaakkola, 2008; Komodakis &amp; Paragios, 2008). In particular, a popular framework for solving the (NP-hard) MAP inference problem for general MRFs is to solve a LP relaxation of it by dual decomposition (decomposing it into a number of  X  X asier X  subproblems) (Komodakis et al., 2011; Sontag et al., 2011). The tightness of the underlying LP relaxation is determined by the choice of the subproblems. If all subproblems are tree-structured, the resulted LP relaxation is usually not tight for rich-structured MRFs. Researchers have been focusing on how to selectively add cycle subproblems in order to tighten the relaxation (Sontag et al., 2008; 2012; Batra et al., 2011). Solving a cycle MRF (usually a triplet cluster) is a crucial subroutine in both (1) evaluating the criterion for adding cycles/clusters and (2) computing subgradients/messages for optimizing the dual objective. However, it is usually the bottleneck procedure especially when the variable state spaces are large.
 One way of solving the cycle MRF is by loopy max-product BP, which has shown to be exact (Weiss, 2000) for MRFs with a single cycle. Although each message passing step only takes O ( K 2 ) time ( K is variable cardinality), the number of iterations needed for convergence is usually large, especially for small and  X  X rustrated X  cycles encountered in tightening dual decomposition. Another way of solving the cycle MRF (which we build on) is to pass messages in the clique tree (triangulated cycle), which takes O ( K 3 ) time if the messages are computed in the naive way. Faster algorithms for computing the messages have been proposed recently (Felzenszwalb &amp; McAuley, 2011; McAuley &amp; Caetano, 2011); they still sequentially compute all full messages. However, we observe that only a tiny portion of the message entries need to be computed in order to find the MAP assignment X  others can be pruned by conditions derived from the global structure of the cycle.
 The pruning strategy in general has been used by existing work including search-based branch-and-bound methods (Marinescu &amp; Dechter, 2006; 2007; Flerova et al., 2011; Sun et al., 2012), column generation (Belanger et al., 2012), and fast min-sum (Felzenszwalb &amp; McAuley, 2011). We will elaborate on the relation between our algorithm and existing methods in Section 5. As our algorithm is specifically tailored to cycle MRFs, it is usually more than an order of magnitude faster than these general-purpose methods. To solve the cycle MRF we triangulate it in a special way (as in Fig. 1) such that X 1 is in the scope of all clique tree nodes. If we perform clique tree message passing from left to right, we would compute clique tree messages:  X  i ( X 1 , X i ) = min X for i = 3 : N . (Note that  X   X  2 =  X  1 in computing the first message.) They are shown by wide red arrows in Fig. 1.
 Instead of computing the full messages  X   X  i , we compute incomplete versions of them, denoted as  X  i . To explain how these incomplete messages are computed we perceive each message  X  i as a set of all its entries:  X  i = {  X  i ( x 1 , x i ) | x 1  X  V al ( X 1 ) , x i  X  V al ( X where V al ( X ) is the domain of X . We use lowercase x to denote specific assignments to variable X . Our algorithm will split the set into an active subset and an inactive one:  X  i =  X  + i  X   X   X  i . Similarly we also split the original energy terms  X  i into active and inactive subsets. Only the active parts participate in the partial min-sum operation (explained later). The notations are summarized in Table 1.
  X  i original energy terms (factors)  X  i active entries of  X  i  X  i inactive entries of  X  i  X   X  i clique tree messages, from (2)  X  i  X  X ncomplete X  clique tree messages, from (4)  X  + i active entries of  X  i  X   X  i inactive entries of  X  i  X  i messages on cycle edges To define partial min-sum, we abbreviate (2) as  X  i = minsum(  X  sum operation. Note that it can be performed (on two sets of entries) as follows: 3.  X   X  i ( x 1 , x i )  X  min(  X   X  i ( x 1 , x i ) , a + b ) Therefore it is straightforward to define partial min-sum: by restricting the loops in the above algorithm to the active subsets.
 At the end of the clique chain we compute Note that  X  N and  X  N have the same scope ( X N , X 1 ). Therefore we compute the sum and the min directly (without restricting to active subsets), and trace back to recover the corresponding assignment.
 The general idea of our method is to repeatedly update all  X  i by gradually adding entries to each  X  + i and each  X  i . This process stops when we are guaranteed (by the conditions given below) that we have obtained the MAP solution of the problem.
 We will first describe the stopping conditions and then show the overall algorithm. To derive the stopping conditions we need to use another set of messages  X  2: N : where  X  N +1 = 0 for i = N . These messages are passed along the cycle edges (shown by the narrow blue arrows in Fig. 1). Note that unlike in loopy BP,  X  2: N not form a loop, so they are computed once and for all. Computing them only has the time complexity of O ( K 2 ).
 Suppose that we are in the process of iteratively updating the incomplete messages. Given current  X  3: N and the active/inactive split of  X  3: N  X  1 and  X  1: N  X  1 , we assume that each incomplete message has been computed according to (4), and we have the current best solution with energy b  X  computed by (5). The optimality of b  X  can be established by the following conditions.
 Proposition 1. b  X  is the global minimum of the cycle MRF energy if all entries  X  i ( x i , x i +1 )  X   X   X  i , for i = 1 : N  X  1 , satisfy:  X  i ( x i , x i +1 ) &gt; b  X   X  min and all entries  X  i ( x 1 , x i )  X   X   X  i , for i = 3 : N  X  1 , satisfy: Proof is given in supplement material. Here we give some intuitive interpretations.
 Given an entry in  X  or  X  we want to judge whether or not it could possibly lead to a lower energy than b  X . If not, we can safely leave it in the inactive set. Ideally, we would like a tight bound, i.e. all entries except for the ones corresponding to the MAP assignment satisfy (7) and (8). Now the question is, how far are we from that ideal bound? It turns out that we are only off by the uncertainty on one variable X 1 . Specifically, for choice in X 1 , and the term  X  i +1 ( x i +1 ) also (implicitly) involves a choices in X 1 , because it has minimized over X 1 in the beginning the  X  messages. If we somehow know that these two terms agree on their choices of X 1 , it can be shown that we have the ideal bound, i.e. all entries except for the ones corresponding to the MAP assignment satisfy (7). We will make this precise by a corollary in supplement material.
 An analogous argument can be made for the two terms  X  i ( x 1 , x i ) and  X  i ( x i ) in (8).
 Indeed, the connectivity on one variable is how the cycle differs from a chain. In this regard, the conditions (7) and (8) fully exploit the structure of a cycle.
 Now we come to the overall structure of the algorithm. The general idea is to gradually put small batches of Algorithm 1 Algorithm sketch of our fast cycle solver entries into the active sets (using gradually increasing thresholds), and performing the partial min-sums iteratively. Each pass through the clique chain will update b  X  and therefore the RHS of the conditions (7) and (8) will become smaller. Once these decreasing bounds meet the increasing thresholds, we are guaranteed that the current solution (with energy b  X ) is optimal. Our algorithm is sketched in Alg. 1. The strategy of using an increasing threshold to update messages has also been used in (Felzenszwalb &amp; McAuley, 2011). In practice we offset all potentials to be non-negative, so we can start with a small positive number as threshold and increase it by multiplying with some constant. Consider energy minimization for general MRFs using dual decomposition. For brevity we only give a minimum introduction to the background, the general framework and implications have been thoroughly discussed in (Komodakis et al., 2011; Sontag et al., 2011).
 The general idea is to decompose the MRF into subproblems {  X  c } (e.g., trees, cycles), where c indexes subproblems. Summing over {  X  c } gives a reparametrization of the original MRF energy. Presumably the subproblems are easy to solve, and each subproblem chooses its own assignment to the variables. We aim at maximizing the dual objective: which lower bounds the minimum energy of the original MRF. Optimizing the dual objective can be viewed as exchanging messages for mutual agreements among subproblems (Wang &amp; Koller, 2013). Maximizing (9) turns out to be the dual of some underlying LP relaxation of the original energy minimization problem, whose tightness depends on the choice of subproblems in the decomposition.
 For rich structured models we usually have to add cycle subproblems to the decomposition to tighten the underlying LP relaxation. In such a scenario we need a cycle solver in three situations: 1. To evaluate the criterion in selecting cycles to 2. To compute subgradients for optimizing the dual, 3. To compute messages for optimizing the dual In the first two cases our algorithm can be directly plugged in. The third case has more subtlety, which we will focus on in this section.
 To compute the messages in dual algorithms listed in the third case above, we usually need min-marginals of the cycle subproblems. The min-marginal over one variable X i is defined as: And we can define M c ( X However, our algorithm does not compute the min-marginals. In the following we show a heuristic method to address this issue, which works well in practice as we demonstrate in experiments. However, it remains an interesting open problem to design more principled and efficient dual methods building on the fast cycle solver. 3.1. Sparse updates in dual methods We consider the dual method of (Kolmogorov, 2006) 1 , which iterates over nodes/edges of the MRF and, for each node/edge, average the min-marginals of all subproblems sharing that node/edge. Specifically, if X is selected as the node to be updated, the dual updates are: for  X  c  X  J ( X ) ,  X  x  X  V al ( X ), where J ( X ) denotes all subproblems sharing X . The updates are applied to  X  c X , the potential of subproblem c on X . And J ( X ) set of subproblems containing variable X
 X  c energy function of subproblem c  X  X potential of subproblem c on variable X M c X min-marginal of subproblem c on variable X
M X average of M c X over c  X  X  ( X ) c
M X average of  X  X ormalized X  M c X over c  X  X  ( X ) f
M c X upper bound approximation of M c X x c minimizer of M L subset of V al ( X ) with high priority (13) M updates monotonically increase the dual objective (9). To compute the dual updates we need min-marginals from each subproblem. Table 2 summarizes notations used in this section.
 In our cycle solver we have computed  X  N +  X  N in (5), which would give us the min-marginal M ( X if we were using the complete message  X   X  N . With the incomplete message we get an element-wise upper bound of the min-marginal, denoted by: And we can easily compute f M X N and f M X 1 from it. As the cycle is symmetric, this can be computed for any edge and node.
 Directly using these upper-bound approximations in dual methods does not work well, because the resulted dual updates (messages) tend to overshoot, causing large oscillation in the dual objective. However, these approximate min-marginals bear important information that we can exploit.
 Specifically, let us denote x  X  = arg min f M X ( x ). For any other assignment x , the difference f M X ( x )  X  f M
X ( x  X  ) is an upper bound approximation of the actual difference M X ( x )  X  M X ( x  X  ) (because we know that f M X ( x  X  ) = M X ( x  X  ) = b  X  from Proposition 1). This difference is an indication of the  X  X elevance X  of label x . The smaller the difference, the more likely that x will become optimal (for that cycle subproblem) and therefore affect the dual objective in subsequent dual updates.
 To make use of this information, given X as the next node to be updated, we select an active label set L  X  V al ( X ) as the assignments with highest priority defined as: where x  X  c is the optimal label for subproblem c . Given the selected label set L , we perform the dual updates (11) only for x  X  L . So we only need min-marginals M c X ( x ) for a sparse subset of labels. Each M X ( x ) can be computed exactly by conditioning on X = x and performing inference on the resulted chain MRF. In practice we choose |L| to be a small number to balance the effectiveness of dual updates and the extra cost of computing M c X ( x ).
 When the dual updates (11) are applied to a subset L instead of V al ( X ), monotonicity in the dual is no longer guaranteed. This can be fixed by  X  X ormalizing X  the dual updates as follows: Proposition 2. Given X , if we perform sparse dual updates for all c  X  J ( X ) and x  X  L  X  V al ( X ) , the dual objective (9) increases monotonically under normalized updates (14), but not under (11).  X 
X ( x )  X   X  Proof is given in supplement material. Intuitively we just replaced each min-marginal with its  X  X ormalized X  version (subtracted the minimum energy of that subproblem). This maintains the monotonicity of the dual updates when performed on a subset of labels. It worth noting that monotonicity is not sufficient for converging to some desired dual state (such as weak tree agreement (Kolmogorov, 2006)). Specifically, we observe that if we only include currently optimal labels in L , the algorithm would easily get stuck. However, our selection criterion (13) also includes many non-optimal labels into L . In our experiments the sparse update (14) always converges to dual optimal whenever the full TRW-S update (11) is able to converge.
 All the above discussion was for updating node X . It can be easily generalized to updating edges as well, in which case L would be a subset of the joint assignments V al ( X i )  X  V al ( X j ). All experiments are conducted using a single thread on a 3.3GHz CPU and 16 Gigabytes of memory. 4.1. Synthetic cycle MRFs First we evaluate our cycle solver on synthetic cycle MRFs to compare it with various fast inference algorithms. In Fig. 2, we show how running time scales with the state space size K and number of variables N , respectively. For each setting, running time is averaged over 20 problem instances generated by sampling all edge potentials from N (0 , 1). All methods are plotted with error bars indicating standard deviation across problem instances.
 We can see that adapting to the problem structure improves performance. AOBB (Marinescu &amp; Dechter, 2006) is a completely general-purpose algorithm 2 , thus it is not surprising that it does not beat specialized methods. (Indeed, it is in some sense unfair to include it in the comparison.) Naive and fast min-sums (Felzenszwalb &amp; McAuley, 2011) are tailored for tree-width-two graphs 3 . Branch-and-Bound (Sun et al., 2012) is designed for large state spaces. Our method fully exploits the structure of a cycle. It outperforms all other methods by more than an order of magnitude. (Note that the time axis is in log scale.) We also observe (from Fig. 2 right) that the advantage of our cycle solver is especially prominent in solving triangles ( N = 3). Note that in dual decomposition, we could triangulate any  X  X ntight X  cycle and only handle triangle subproblems. The trade off between having fewer subproblems (large cycles) and having smaller subproblems (triangles) deserves further investigation. 4.2. Cycle subproblems in dual decomposition To evaluate our cycle solver in dual decomposition, we used four MAP inference tasks as described below. Synthetic Grid . We generated a five-by-five grid MRF (four-neighbors connectivity) with 25 variables and 1,000 states for each variables. All potentials are drawn from N (0 , 1).
 Pose Estimation . The task is to estimate the image positions of six upper-body parts of a human depicted in a single image. Each part is a variable, whose state space consists of discrete image positions pre-selected by some detector. The MRF is fully connected (with 15 edges), and the edge potentials capture spatial consistency of the parts. We used the problem instances provided in the software package of (Sun et al., 2012). The variable cardinality varies from 269 to 750.
 SIFT Matching . We formulate the problem of matching two sets of image feature as MAP inference. Each feature point in the source image is a variable, its state space is all feature points in the target image 4 . The problem instance we constructed has 68 variables, each has 1,021 states. The unary potentials measure similarity between the SIFT feature descriptors (Lowe, 2004). The MRF is constructed by connecting each source image feature point to its 3-nearest-neighbors (according to Euclidean distance, in source image). The resulted graph has many cycles. Pairwise potentials measure spatial consistency of two assignments 5 .
 Protein Design . The protein design benchmark (Yanover et al., 2006) features very densely connected MAP-MRF instances. Each MRF typically has hundreds of variables, thousands of edges, tens of thousands of triplet cycles. The cardinality of variables range from 2 to 180.
 Experiments on these MRF inference problems focus on evaluating our method in two aspects: 1. Does the sparse dual update strategy (Section 3.1) 2. How does our cycle solver compare to other meth-We address each of them below.
 Comparing Dual Objectives . Among all cycle solvers we compared in Section 4.1, Fast-MinSum (Felzenszwalb &amp; McAuley, 2011) is the fastest one that provides exact min-marginals, for which there is no need to use sparse dual updates as in Section 3.1). Therefore we focus on comparing Fast-MinSum and our cycle solver.
 In Fig. 3, we show primal-dual plots for each of the four tasks. The three methods been compared are: 1. Tree Decomposition : Decompose the MRF 2. Cycle Decomposition Fast-MinSum : Use 3. Cycle Decomposition Ours : Same as the For the tasks with multiple problem instances (Protein Design and Pose Estimation), we only show the primal-dual plot of one instance. Different problem instances are qualitatively very similar.
 Comparing Cycle Solver Time . We compare our cycle solver with three methods (that appeared more competitive in Section 4.1) on cycle subproblems encountered in dual decomposition. For each of the four tasks we sample about a hundred cycle instances, which span all stages of optimizing the dual. In Table 3 we show the average running time for solving a cycle, as well as its ratio with our cycle solver. We observe that Loopy-BP performs very badly in these usually  X  X rustrated X  cycles encountered in dual decomposition. Fast-MinSum and Branch-and-Bound may work well in one case but not so good in another. This could be because they are sensitive to different properties of the potentials in different problems. Our cycle solver works consistently well in all cases. 4.3. Effect of cycle reparametrization In this part we further analyze the behavior of our cycle solver. Notably it does not have a preferable worst-case guarantee (just like most branch-and-bound or pruning based methods). So one might ask: in what situations does our method work unsatisfactorily, and how to avoid those situations? We do not have a conclusive theoretical characteriza-tion at this point. But we will show some empirical results to provide some insights on this issue. Intuitively, our algorithm works better if the stopping criteria in Proposition 1 are tighter, which requires different parts of the cycle having more  X  X utual agreement X . So we consider a local reparametrization to increase/decrease mutual agreement among edge potentials. Specifically, consider the two edge potentials  X  1 ( X 1 , X 2 ) and  X  N ( X N , X 1 ). Let  X  min X 2  X  1 and  X  N = min X N  X  N . Note that both  X  are unary tables of X 1 . Consider the following updates (denoted as  X   X  ):  X  Note that  X + will increase mutual agreement, whereas  X   X  will decrease mutual agreement (between  X  1 and  X  To see the effect of these reparametrizations, we repeatedly apply  X + (or  X   X  ) to randomly selected nodes in the cycle, and measure the running time of different cycle solvers. Fig. 4 shows the running time (and its standard deviations) of three methods under repeated applications of  X + (or  X   X  ) to the cycle (  X  20 means applying  X   X  to randomly selected nodes 20 times). The problem instances are randomly generated (as in Section 4.1) with N = 10, K = 500. We observe that the negative reparametrizations have no influence on Naive-MinSum, small influence on Fast-MinSum, and very large influence on our method. This experiment characterizes the situation that our method needs to avoid, and also suggests a simple heuristic to get out of it: using  X +.
 In dual decomposition, when solving cycle subprob-lems under repeated dual updates. The updates tend to  X  X ull X  different edges of the cycle away from each other (a similar effect as  X   X  ). To tackle this, we apply  X + once for each node of the cycle before running our cycle solver. This is very effective as shown in Fig. 5. Note that the extra cost of applying  X + has been included. This heuristic was used in all experiments of Section 4.2. Our method is closely related to the fast min-sum (Felzenszwalb &amp; McAuley, 2011). The key difference is that, fast min-sum computes each min-sum message in isolation, and prunes out entries that are not necessary in computing that (full) message ; our method updates all min-sum messages iteratively, and prunes out entries that are not necessary in computing the MAP assignment . Our strategy exploits the global structure of the cycle and prunes out much more entries. Comparing to traditional branch-and-bound methods (e.g., (Sun et al., 2012)), our algorithm is different in two aspects: (1) We do not explicitly maintain branches. Indeed, the message entries implicitly define a large number of overlapping  X  X ranches X . (2) The  X  X ound X  in our method (Proposition 1) fully exploits the structure of a cycle, thus is much tighter than that derived from general principles.
 Pruning in message passing has also been implemented using column generation (Belanger et al., 2012), which is very different from our approach. First of all, (Belanger et al., 2012) handles chains instead of cycles. Even if we generalize it by applying the same principles to the  X  X lique chain X  (triangulated cycle), there are still two key differences. (1) Since column generation is equivalent to cutting plane in the dual, the messages (dual variables) upon convergence need to be dual feasible. However, when our stopping criteria are met, the (incomplete) messages are generally not dual feasible, i.e. our stopping criterion is tighter than that derived from the general principles of column generation. (2) Moreover, the efficiency of column generation oracle relies on data-independent terms (precomputed bounds on the transitions), whereas our approach does not have such constraints.
 The standard LP relaxation can also be tightened without explicitly solving cycle MRFs. For example (Komodakis &amp; Paragios, 2008) proposed to repair cy-cles , which resembles subgradient updates (Komodakis et al., 2011) in dual decomposition X  X hey both focus on updating dual variables associated with currently optimal labels. Note that it does not circumvent the intrinsic cubic complexity of cycle MRFs, because each cycle may need to be repaired with respect to all labels (anchor nodes) for one variable, and each repairing operation is quadratic in the state space size. Sontag et al. (2009) proposed to partition the state spaces and only enforce consistency at the coarse level. Yarkony et al. (2011) proposed to use binary cycle subproblems constructed by partitioning the state spaces. These binary cycles enforce fewer constraints, so we may need a large number of them for one cycle in the graph.
 Acknowledgement This work has been supported by the Max Planck Center for Visual Computing and Communication. Proof of Proposition 1 Proposition 1. b  X  is the global minimum of the cycle MRF energy if all entries  X  i ( x i , x i +1 )  X   X   X  i , for i = 1 : N  X  1 , satisfy (7): and all entries  X  i ( x 1 , x i )  X   X   X  i , for i = 3 : N  X  1 , satisfy (8): (7), any assignment with X i = x i and X i +1 = x i +1 must have higher energy than b  X ; if  X  i ( x 1 , x i ) satisfies (8), any assignment with X 1 = x 1 and X i = x i must have higher energy than b  X .
 Note that we have i = 2 : N  X  1 for both (7) and (8). (Remember that  X  2 is just  X  1 .) We use induction on i simultaneously for (7) and (8), i.e. the i case for (7) and (8) needs to be derived from the 2 : i  X  1 cases for both of them.
 For i = 2, the energy with assignment ( x 2 , x 3 ) is  X  ( X ) =  X  1 ( X 1 , x 2 ) +  X  2 ( x 2 , x 3 ) +  X  3 ( x where the first inequality follows from the definition of the cycle messages  X  and the second inequality is just (7). Similarly, the energy with assignment ( x 1 , x 2 ) is where the second inequality is just (8).
 For i &gt; 2, the energy with assignment ( x i , x i +1 ) is  X  ( X )  X  ) However, because  X  i has been computed from partial min-sums, the first two terms are not lower bounded by min X 1  X  i ( X 1 , x i ). (If they were, we would not need induction in the proof.) We divide the space X =  X  i V al ( X i ) of all assignments into two halves. Let X 0 be all such X that the assignment to X 1: i  X  1 corresponds to at least one entry in  X  or  X  left out in partial min-sums. By induction we know that  X  ( X ) | X  X  X  0 &gt; b  X .
 For any X /  X  X  0 , by definition all the entries in  X  3: i  X  1 and  X  1: i  X  1 have participated in the partial min-sums. Therefore we have Substituting it back to (18) and using (7) we have  X  ( X ) &gt; b  X . The induction step for (8) can be proved analogously.
 The following corollary attempts to make the following statement (from Section 2) precise.  X  X f we somehow know that these two terms (last two terms in (7)) agree on their choices of X 1 , it can be shown that we have the ideal bound, i.e. all entries except for the ones corresponding to the MAP assignment satisfy (7). X  Corollary 1. If b  X  is the global minimum of the cycle MRF energy, for any entry  X  i ( x i , x i +1 ) , i = 1 : N  X  1 , satisfying  X  if the last two terms agree on the assignment of X 1 , then there exists a minimum energy assignment (with energy b  X  ) that has X i = x i and X i +1 = x i +1 . Proof. Note that (20) can be written as:  X  Consider tracing back through the messages  X  i ( X 1 , x i ), and minimize over X 1 , we would get assignment to X 1: i , with X i = x i . Tracing back through the messages  X  i +1 ( x i +1 ) we would get that X N +1 is X 1 . However, their assignments were obtained independently in the above process. By our assumption they should agree. So we have an assignment to X 1: N by putting together the two parts from above. Its energy is the LHS of (21). Therefore we have found a minimum energy assignment with X Proof of Proposition 2 Proposition 2. Given X , if we perform sparse dual updates for all c  X  J ( X ) and x  X  L  X  V al ( X ) , the dual objective (9) increases monotonically under normalized updates (14), but not under (11).  X 
X ( x )  X   X  Proof. In performing dual updates, all potentials of subproblem c are fixed except for  X  c X . If we perform clique tree min-sum message passing for subproblem c with X in the root node. The incoming message of the root node will not change with updates to  X 
X . Therefore the updates can be perceived as been applied to the min-marginals directly. From (14) we have: for normalized updates, where  X  M c X is the new min-marginal. The new minimum energy for subproblem c is min x  X  M c X ( x ).
 By definition we know that c M X ( x )  X  0 for all x . been included in updates. If M c X ( x ) is not updated Combining these we have: This says that the new minimum energy for subproblem c is no less than its old minimum energy. Since this holds for all subproblems, the dual objective increases monotonically. However we do not have such guarantees for unnormalized moves. To complete the proof we need a counter example.
 Consider a case with two subproblems c 0 and c 1 . Let M
X be (0 . 2 , 0 . 3 , 0 . 1) and M old dual objective is 0.9. If we perform unnormalized dual updates to the first and third label, the min-
 Huayan Wang huayanw@cs.stanford.edu Daphne Koller koller@cs.stanford.edu We address the problem of energy minimization (MAP inference) in cycle MRFs. Specifically, consider a MRF over X = { X 1: N } with connectivity X 1  X  X 2  X  X  X  X   X  X N  X  X 1 . It is parametrized by N pairwise potentials: where  X  i has the scope  X  i ( X i , X i +1 ) ( X N +1 is X Our goal is to find assignments to X 1: N that minimize the energy function (1).
 Cycle MRFs could be useful as a modeling tool in its own right. For example, a cycle over landmarks could be used for top-down image segmentation (Heitz et al., 2009). More importantly, cycles have been recognized as a crucial component in tackling general MRFs (Werner, 2008; Sontag &amp; Jaakkola, 2008; Komodakis &amp; Paragios, 2008). In particular, a popular framework for solving the (NP-hard) MAP inference problem for general MRFs is to solve a LP relaxation of it by dual decomposition (decomposing it into a number of  X  X asier X  subproblems) (Komodakis et al., 2011; Sontag et al., 2011). The tightness of the underlying LP relaxation is determined by the choice of the subproblems. If all subproblems are tree-structured, the resulted LP relaxation is usually not tight for rich-structured MRFs. Researchers have been focusing on how to selectively add cycle subproblems in order to tighten the relaxation (Sontag et al., 2008; 2012; Batra et al., 2011). Solving a cycle MRF (usually a triplet cluster) is a crucial subroutine in both (1) evaluating the criterion for adding cycles/clusters and (2) computing subgradients/messages for optimizing the dual objective. However, it is usually the bottleneck procedure especially when the variable state spaces are large.
 One way of solving the cycle MRF is by loopy max-product BP, which has shown to be exact (Weiss, 2000) for MRFs with a single cycle. Although each message passing step only takes O ( K 2 ) time ( K is variable cardinality), the number of iterations needed for convergence is usually large, especially for small and  X  X rustrated X  cycles encountered in tightening dual decomposition. Another way of solving the cycle MRF (which we build on) is to pass messages in the clique tree (triangulated cycle), which takes O ( K 3 ) time if the messages are computed in the naive way. Faster algorithms for computing the messages have been proposed recently (Felzenszwalb &amp; McAuley, 2011; McAuley &amp; Caetano, 2011); they still sequentially compute all full messages. However, we observe that only a tiny portion of the message entries need to be computed in order to find the MAP assignment X  others can be pruned by conditions derived from the global structure of the cycle.
 The pruning strategy in general has been used by existing work including search-based branch-and-bound methods (Marinescu &amp; Dechter, 2006; 2007; Flerova et al., 2011; Sun et al., 2012), column generation (Belanger et al., 2012), and fast min-sum (Felzenszwalb &amp; McAuley, 2011). We will elaborate on the relation between our algorithm and existing methods in Section 5. As our algorithm is specifically tailored to cycle MRFs, it is usually more than an order of magnitude faster than these general-purpose methods. To solve the cycle MRF we triangulate it in a special way (as in Fig. 1) such that X 1 is in the scope of all clique tree nodes. If we perform clique tree message passing from left to right, we would compute clique tree messages:  X  i ( X 1 , X i ) = min X for i = 3 : N . (Note that  X   X  2 =  X  1 in computing the first message.) They are shown by wide red arrows in Fig. 1.
 Instead of computing the full messages  X   X  i , we compute incomplete versions of them, denoted as  X  i . To explain how these incomplete messages are computed we perceive each message  X  i as a set of all its entries:  X  i = {  X  i ( x 1 , x i ) | x 1  X  V al ( X 1 ) , x i  X  V al ( X where V al ( X ) is the domain of X . We use lowercase x to denote specific assignments to variable X . Our algorithm will split the set into an active subset and an inactive one:  X  i =  X  + i  X   X   X  i . Similarly we also split the original energy terms  X  i into active and inactive subsets. Only the active parts participate in the partial min-sum operation (explained later). The notations are summarized in Table 1.
  X  i original energy terms (factors)  X  i active entries of  X  i  X  i inactive entries of  X  i  X   X  i clique tree messages, from (2)  X  i  X  X ncomplete X  clique tree messages, from (4)  X  + i active entries of  X  i  X   X  i inactive entries of  X  i  X  i messages on cycle edges To define partial min-sum, we abbreviate (2) as  X  i = minsum(  X  sum operation. Note that it can be performed (on two sets of entries) as follows: 3.  X   X  i ( x 1 , x i )  X  min(  X   X  i ( x 1 , x i ) , a + b ) Therefore it is straightforward to define partial min-sum: by restricting the loops in the above algorithm to the active subsets.
 At the end of the clique chain we compute Note that  X  N and  X  N have the same scope ( X N , X 1 ). Therefore we compute the sum and the min directly (without restricting to active subsets), and trace back to recover the corresponding assignment.
 The general idea of our method is to repeatedly update all  X  i by gradually adding entries to each  X  + i and each  X  i . This process stops when we are guaranteed (by the conditions given below) that we have obtained the MAP solution of the problem.
 We will first describe the stopping conditions and then show the overall algorithm. To derive the stopping conditions we need to use another set of messages  X  2: N : where  X  N +1 = 0 for i = N . These messages are passed along the cycle edges (shown by the narrow blue arrows in Fig. 1). Note that unlike in loopy BP,  X  2: N not form a loop, so they are computed once and for all. Computing them only has the time complexity of O ( K 2 ).
 Suppose that we are in the process of iteratively updating the incomplete messages. Given current  X  3: N and the active/inactive split of  X  3: N  X  1 and  X  1: N  X  1 , we assume that each incomplete message has been computed according to (4), and we have the current best solution with energy b  X  computed by (5). The optimality of b  X  can be established by the following conditions.
 Proposition 1. b  X  is the global minimum of the cycle MRF energy if all entries  X  i ( x i , x i +1 )  X   X   X  i , for i = 1 : N  X  1 , satisfy:  X  i ( x i , x i +1 ) &gt; b  X   X  min and all entries  X  i ( x 1 , x i )  X   X   X  i , for i = 3 : N  X  1 , satisfy: Proof is given in supplement material. Here we give some intuitive interpretations.
 Given an entry in  X  or  X  we want to judge whether or not it could possibly lead to a lower energy than b  X . If not, we can safely leave it in the inactive set. Ideally, we would like a tight bound, i.e. all entries except for the ones corresponding to the MAP assignment satisfy (7) and (8). Now the question is, how far are we from that ideal bound? It turns out that we are only off by the uncertainty on one variable X 1 . Specifically, for choice in X 1 , and the term  X  i +1 ( x i +1 ) also (implicitly) involves a choices in X 1 , because it has minimized over X 1 in the beginning the  X  messages. If we somehow know that these two terms agree on their choices of X 1 , it can be shown that we have the ideal bound, i.e. all entries except for the ones corresponding to the MAP assignment satisfy (7). We will make this precise by a corollary in supplement material.
 An analogous argument can be made for the two terms  X  i ( x 1 , x i ) and  X  i ( x i ) in (8).
 Indeed, the connectivity on one variable is how the cycle differs from a chain. In this regard, the conditions (7) and (8) fully exploit the structure of a cycle.
 Now we come to the overall structure of the algorithm. The general idea is to gradually put small batches of Algorithm 1 Algorithm sketch of our fast cycle solver entries into the active sets (using gradually increasing thresholds), and performing the partial min-sums iteratively. Each pass through the clique chain will update b  X  and therefore the RHS of the conditions (7) and (8) will become smaller. Once these decreasing bounds meet the increasing thresholds, we are guaranteed that the current solution (with energy b  X ) is optimal. Our algorithm is sketched in Alg. 1. The strategy of using an increasing threshold to update messages has also been used in (Felzenszwalb &amp; McAuley, 2011). In practice we offset all potentials to be non-negative, so we can start with a small positive number as threshold and increase it by multiplying with some constant. Consider energy minimization for general MRFs using dual decomposition. For brevity we only give a minimum introduction to the background, the general framework and implications have been thoroughly discussed in (Komodakis et al., 2011; Sontag et al., 2011).
 The general idea is to decompose the MRF into subproblems {  X  c } (e.g., trees, cycles), where c indexes subproblems. Summing over {  X  c } gives a reparametrization of the original MRF energy. Presumably the subproblems are easy to solve, and each subproblem chooses its own assignment to the variables. We aim at maximizing the dual objective: which lower bounds the minimum energy of the original MRF. Optimizing the dual objective can be viewed as exchanging messages for mutual agreements among subproblems (Wang &amp; Koller, 2013). Maximizing (9) turns out to be the dual of some underlying LP relaxation of the original energy minimization problem, whose tightness depends on the choice of subproblems in the decomposition.
 For rich structured models we usually have to add cycle subproblems to the decomposition to tighten the underlying LP relaxation. In such a scenario we need a cycle solver in three situations: 1. To evaluate the criterion in selecting cycles to 2. To compute subgradients for optimizing the dual, 3. To compute messages for optimizing the dual In the first two cases our algorithm can be directly plugged in. The third case has more subtlety, which we will focus on in this section.
 To compute the messages in dual algorithms listed in the third case above, we usually need min-marginals of the cycle subproblems. The min-marginal over one variable X i is defined as: And we can define M c ( X However, our algorithm does not compute the min-marginals. In the following we show a heuristic method to address this issue, which works well in practice as we demonstrate in experiments. However, it remains an interesting open problem to design more principled and efficient dual methods building on the fast cycle solver. 3.1. Sparse updates in dual methods We consider the dual method of (Kolmogorov, 2006) 1 , which iterates over nodes/edges of the MRF and, for each node/edge, average the min-marginals of all subproblems sharing that node/edge. Specifically, if X is selected as the node to be updated, the dual updates are: for  X  c  X  J ( X ) ,  X  x  X  V al ( X ), where J ( X ) denotes all subproblems sharing X . The updates are applied to  X  c X , the potential of subproblem c on X . And J ( X ) set of subproblems containing variable X
 X  c energy function of subproblem c  X  X potential of subproblem c on variable X M c X min-marginal of subproblem c on variable X
M X average of M c X over c  X  X  ( X ) c
M X average of  X  X ormalized X  M c X over c  X  X  ( X ) f
M c X upper bound approximation of M c X x c minimizer of M L subset of V al ( X ) with high priority (13) M updates monotonically increase the dual objective (9). To compute the dual updates we need min-marginals from each subproblem. Table 2 summarizes notations used in this section.
 In our cycle solver we have computed  X  N +  X  N in (5), which would give us the min-marginal M ( X if we were using the complete message  X   X  N . With the incomplete message we get an element-wise upper bound of the min-marginal, denoted by: And we can easily compute f M X N and f M X 1 from it. As the cycle is symmetric, this can be computed for any edge and node.
 Directly using these upper-bound approximations in dual methods does not work well, because the resulted dual updates (messages) tend to overshoot, causing large oscillation in the dual objective. However, these approximate min-marginals bear important information that we can exploit.
 Specifically, let us denote x  X  = arg min f M X ( x ). For any other assignment x , the difference f M X ( x )  X  f M
X ( x  X  ) is an upper bound approximation of the actual difference M X ( x )  X  M X ( x  X  ) (because we know that f M X ( x  X  ) = M X ( x  X  ) = b  X  from Proposition 1). This difference is an indication of the  X  X elevance X  of label x . The smaller the difference, the more likely that x will become optimal (for that cycle subproblem) and therefore affect the dual objective in subsequent dual updates.
 To make use of this information, given X as the next node to be updated, we select an active label set L  X  V al ( X ) as the assignments with highest priority defined as: where x  X  c is the optimal label for subproblem c . Given the selected label set L , we perform the dual updates (11) only for x  X  L . So we only need min-marginals M c X ( x ) for a sparse subset of labels. Each M X ( x ) can be computed exactly by conditioning on X = x and performing inference on the resulted chain MRF. In practice we choose |L| to be a small number to balance the effectiveness of dual updates and the extra cost of computing M c X ( x ).
 When the dual updates (11) are applied to a subset L instead of V al ( X ), monotonicity in the dual is no longer guaranteed. This can be fixed by  X  X ormalizing X  the dual updates as follows: Proposition 2. Given X , if we perform sparse dual updates for all c  X  J ( X ) and x  X  L  X  V al ( X ) , the dual objective (9) increases monotonically under normalized updates (14), but not under (11).  X 
X ( x )  X   X  Proof is given in supplement material. Intuitively we just replaced each min-marginal with its  X  X ormalized X  version (subtracted the minimum energy of that subproblem). This maintains the monotonicity of the dual updates when performed on a subset of labels. It worth noting that monotonicity is not sufficient for converging to some desired dual state (such as weak tree agreement (Kolmogorov, 2006)). Specifically, we observe that if we only include currently optimal labels in L , the algorithm would easily get stuck. However, our selection criterion (13) also includes many non-optimal labels into L . In our experiments the sparse update (14) always converges to dual optimal whenever the full TRW-S update (11) is able to converge.
 All the above discussion was for updating node X . It can be easily generalized to updating edges as well, in which case L would be a subset of the joint assignments V al ( X i )  X  V al ( X j ). All experiments are conducted using a single thread on a 3.3GHz CPU and 16 Gigabytes of memory. 4.1. Synthetic cycle MRFs First we evaluate our cycle solver on synthetic cycle MRFs to compare it with various fast inference algorithms. In Fig. 2, we show how running time scales with the state space size K and number of variables N , respectively. For each setting, running time is averaged over 20 problem instances generated by sampling all edge potentials from N (0 , 1). All methods are plotted with error bars indicating standard deviation across problem instances.
 We can see that adapting to the problem structure improves performance. AOBB (Marinescu &amp; Dechter, 2006) is a completely general-purpose algorithm 2 , thus it is not surprising that it does not beat specialized methods. (Indeed, it is in some sense unfair to include it in the comparison.) Naive and fast min-sums (Felzenszwalb &amp; McAuley, 2011) are tailored for tree-width-two graphs 3 . Branch-and-Bound (Sun et al., 2012) is designed for large state spaces. Our method fully exploits the structure of a cycle. It outperforms all other methods by more than an order of magnitude. (Note that the time axis is in log scale.) We also observe (from Fig. 2 right) that the advantage of our cycle solver is especially prominent in solving triangles ( N = 3). Note that in dual decomposition, we could triangulate any  X  X ntight X  cycle and only handle triangle subproblems. The trade off between having fewer subproblems (large cycles) and having smaller subproblems (triangles) deserves further investigation. 4.2. Cycle subproblems in dual decomposition To evaluate our cycle solver in dual decomposition, we used four MAP inference tasks as described below. Synthetic Grid . We generated a five-by-five grid MRF (four-neighbors connectivity) with 25 variables and 1,000 states for each variables. All potentials are drawn from N (0 , 1).
 Pose Estimation . The task is to estimate the image positions of six upper-body parts of a human depicted in a single image. Each part is a variable, whose state space consists of discrete image positions pre-selected by some detector. The MRF is fully connected (with 15 edges), and the edge potentials capture spatial consistency of the parts. We used the problem instances provided in the software package of (Sun et al., 2012). The variable cardinality varies from 269 to 750.
 SIFT Matching . We formulate the problem of matching two sets of image feature as MAP inference. Each feature point in the source image is a variable, its state space is all feature points in the target image 4 . The problem instance we constructed has 68 variables, each has 1,021 states. The unary potentials measure similarity between the SIFT feature descriptors (Lowe, 2004). The MRF is constructed by connecting each source image feature point to its 3-nearest-neighbors (according to Euclidean distance, in source image). The resulted graph has many cycles. Pairwise potentials measure spatial consistency of two assignments 5 .
 Protein Design . The protein design benchmark (Yanover et al., 2006) features very densely connected MAP-MRF instances. Each MRF typically has hundreds of variables, thousands of edges, tens of thousands of triplet cycles. The cardinality of variables range from 2 to 180.
 Experiments on these MRF inference problems focus on evaluating our method in two aspects: 1. Does the sparse dual update strategy (Section 3.1) 2. How does our cycle solver compare to other meth-We address each of them below.
 Comparing Dual Objectives . Among all cycle solvers we compared in Section 4.1, Fast-MinSum (Felzenszwalb &amp; McAuley, 2011) is the fastest one that provides exact min-marginals, for which there is no need to use sparse dual updates as in Section 3.1). Therefore we focus on comparing Fast-MinSum and our cycle solver.
 In Fig. 3, we show primal-dual plots for each of the four tasks. The three methods been compared are: 1. Tree Decomposition : Decompose the MRF 2. Cycle Decomposition Fast-MinSum : Use 3. Cycle Decomposition Ours : Same as the For the tasks with multiple problem instances (Protein Design and Pose Estimation), we only show the primal-dual plot of one instance. Different problem instances are qualitatively very similar.
 Comparing Cycle Solver Time . We compare our cycle solver with three methods (that appeared more competitive in Section 4.1) on cycle subproblems encountered in dual decomposition. For each of the four tasks we sample about a hundred cycle instances, which span all stages of optimizing the dual. In Table 3 we show the average running time for solving a cycle, as well as its ratio with our cycle solver. We observe that Loopy-BP performs very badly in these usually  X  X rustrated X  cycles encountered in dual decomposition. Fast-MinSum and Branch-and-Bound may work well in one case but not so good in another. This could be because they are sensitive to different properties of the potentials in different problems. Our cycle solver works consistently well in all cases. 4.3. Effect of cycle reparametrization In this part we further analyze the behavior of our cycle solver. Notably it does not have a preferable worst-case guarantee (just like most branch-and-bound or pruning based methods). So one might ask: in what situations does our method work unsatisfactorily, and how to avoid those situations? We do not have a conclusive theoretical characteriza-tion at this point. But we will show some empirical results to provide some insights on this issue. Intuitively, our algorithm works better if the stopping criteria in Proposition 1 are tighter, which requires different parts of the cycle having more  X  X utual agreement X . So we consider a local reparametrization to increase/decrease mutual agreement among edge potentials. Specifically, consider the two edge potentials  X  1 ( X 1 , X 2 ) and  X  N ( X N , X 1 ). Let  X  min X 2  X  1 and  X  N = min X N  X  N . Note that both  X  are unary tables of X 1 . Consider the following updates (denoted as  X   X  ):  X  Note that  X + will increase mutual agreement, whereas  X   X  will decrease mutual agreement (between  X  1 and  X  To see the effect of these reparametrizations, we repeatedly apply  X + (or  X   X  ) to randomly selected nodes in the cycle, and measure the running time of different cycle solvers. Fig. 4 shows the running time (and its standard deviations) of three methods under repeated applications of  X + (or  X   X  ) to the cycle (  X  20 means applying  X   X  to randomly selected nodes 20 times). The problem instances are randomly generated (as in Section 4.1) with N = 10, K = 500. We observe that the negative reparametrizations have no influence on Naive-MinSum, small influence on Fast-MinSum, and very large influence on our method. This experiment characterizes the situation that our method needs to avoid, and also suggests a simple heuristic to get out of it: using  X +.
 In dual decomposition, when solving cycle subprob-lems under repeated dual updates. The updates tend to  X  X ull X  different edges of the cycle away from each other (a similar effect as  X   X  ). To tackle this, we apply  X + once for each node of the cycle before running our cycle solver. This is very effective as shown in Fig. 5. Note that the extra cost of applying  X + has been included. This heuristic was used in all experiments of Section 4.2. Our method is closely related to the fast min-sum (Felzenszwalb &amp; McAuley, 2011). The key difference is that, fast min-sum computes each min-sum message in isolation, and prunes out entries that are not necessary in computing that (full) message ; our method updates all min-sum messages iteratively, and prunes out entries that are not necessary in computing the MAP assignment . Our strategy exploits the global structure of the cycle and prunes out much more entries. Comparing to traditional branch-and-bound methods (e.g., (Sun et al., 2012)), our algorithm is different in two aspects: (1) We do not explicitly maintain branches. Indeed, the message entries implicitly define a large number of overlapping  X  X ranches X . (2) The  X  X ound X  in our method (Proposition 1) fully exploits the structure of a cycle, thus is much tighter than that derived from general principles.
 Pruning in message passing has also been implemented using column generation (Belanger et al., 2012), which is very different from our approach. First of all, (Belanger et al., 2012) handles chains instead of cycles. Even if we generalize it by applying the same principles to the  X  X lique chain X  (triangulated cycle), there are still two key differences. (1) Since column generation is equivalent to cutting plane in the dual, the messages (dual variables) upon convergence need to be dual feasible. However, when our stopping criteria are met, the (incomplete) messages are generally not dual feasible, i.e. our stopping criterion is tighter than that derived from the general principles of column generation. (2) Moreover, the efficiency of column generation oracle relies on data-independent terms (precomputed bounds on the transitions), whereas our approach does not have such constraints.
 The standard LP relaxation can also be tightened without explicitly solving cycle MRFs. For example (Komodakis &amp; Paragios, 2008) proposed to repair cy-cles , which resembles subgradient updates (Komodakis et al., 2011) in dual decomposition X  X hey both focus on updating dual variables associated with currently optimal labels. Note that it does not circumvent the intrinsic cubic complexity of cycle MRFs, because each cycle may need to be repaired with respect to all labels (anchor nodes) for one variable, and each repairing operation is quadratic in the state space size. Sontag et al. (2009) proposed to partition the state spaces and only enforce consistency at the coarse level. Yarkony et al. (2011) proposed to use binary cycle subproblems constructed by partitioning the state spaces. These binary cycles enforce fewer constraints, so we may need a large number of them for one cycle in the graph.
 Acknowledgement This work has been supported by the Max Planck Center for Visual Computing and Communication.
