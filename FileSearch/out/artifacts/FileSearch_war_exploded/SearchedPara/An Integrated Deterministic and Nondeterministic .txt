 The sequential chunk labeling aims at finding non-recursive chunk fragments in a sentence. Arbitrary phrase chunking and named entity recognition are the well-known instances. Over the past few years, the structural learning methods, like conditional random fields (CRFs) [8] and maximum-margin Markov models (M 3 N) [16] showed great accuracy in many natural language learning tasks. Structured learners also have the advantage of taking the entire structur e into consideration instead of a limited history. 
On the contrary, the goal of local-classifier-based approaches (e.g. maximum en-tropy models) is to learn to predict labels with fixed context window features. Those methods need to encode the history to inform the learners explicitly. Although high time of local-classifier-based methods is very efficient since the training instances can be treated independently. 
Support vector machines (SVMs) which is one of the state-of-the-art supervised learning algorithms have been widely employ ed as local classifiers to many sequential labeling tasks [7, 5, 19]. In particular, the learning time of linear kernel SVM can now be trained in linear time [4]. Even though local classifier-based approaches have the drawbacks of label-bias problems [8], training linear kernel SVM is not difficult to the case of large-scale and large-category data. By means of so-called one-versus-all or one-versus-one multiclass SVM training, the learning process could be decom-posed into a set of independent tasks. 
In this paper, we present a hybrid deterministic and nondeterministic inference al-gorithm based on conditional Markov model (CMMs) framework. The algorithm makes use of the  X  X o-far X  local optimal incoming information and traverses all possi-ble incoming arcs to predict current label. Then a modified Viterbi search method characteristics of sequential chunk labeling tasks, we propose two heuristics to en-hance the efficiency and performance. One is to automatically construct the connec-tions between chunk tags, while the other is designed to centralize the computation efforts. To demonstrate our method, we conduct the experiments with six famous chunking tasks. We also compare our method with different inference strategies. The goal of conditional Markov models (CMMs) is to assign the tag sequence with maximum conditional probability given the observation sequence, decomposes the probabilistic function as: Based on the above setting, one can employ a local classifier to predict Viterbi search. 
Fig. 1 illustrates the graph of employing variant order of the CMMs (0, 1 st , 2 nd , and the proposed 2 nd order CMMs). The chain probability decompositions of the four CMMs types in Fig. 1 can be shown as follows: Equation (2), (3), and (4) are merely standard zero, first and second order decomposi-tions, while equation (5) is the proposed hybrid second order CMMs decomposition which will be discussed in next section. 
The above decompositions merge the transition and emission probability with sin-gle function. McCallum et al. [9] further combined the locally trained maximum en-tropy with the inferred transition score. However, our conditional support vector Markov models make different chain probability. We replace the original transition probability with transition validity score, i.e. 
The transition validity score is merely a Boolean flag which indicates the relation-ships between two neighbor labels. Equation (6) and (7) are zero-order and our sec-ond order chain probabilities. We will introduce the proposed inference algorithm and how to obtain the transition validity score automatically without concerning the change of chunk representation. 2.1 Hybrid Deterministic and Nondeterministic Inference Algorithm In general, incorporating high order information (rich history) could improve the accuracy. However, it is often the case that the inference time of high order CMMs exponentially scales with the number of history, i.e., K . For example, to compute best incoming arc for s i in the second order CMM, it needs to enumerate all combinations 
To reduce the curse of high order information, we present an approximate infer-ence algorithm that has the same computational time complexity as first-order Markov models. This method considers all combinations between previous nodes and current node while the history is kept as greedy. More specific, for each previous viewed as a variant type of traditional Viterbi algorithm. Fig. 2 illustrates the pro-posed inference algorithm. 
The same as most local classifier-based approaches, in training phase, the history information is known in advanced. In testing, it determines the optimal label sequence by three factors: the accumulated probability, transition validity score, and the output of local classifier. The local classifier (S VMs) should generate different decisions when the given history changes. The algorithm iteratively keeps and stores the opti-mal incoming arcs in order to trace the local best history. By following this line, the local classifier can concentrate on compar ing all possible label pairs (previous and current nodes). Finally, the algorithm traces back the path to determine the output label sequence. 3.1 Automatic Chunk Relation Construction In this paper, we argue that the transition probability is not useful to our CMMs framework. Nevertheless, one important property to sequential chunk labeling is that there is only one phrase type in a chunk. For example, if the previous word is tagged as begin of noun phrase (B-NP), the current word must not be end of the other phrase (E-VP, E-PP, etc). Therefore, we only model relationships between chunk tags to generate valid phrase structure. In other words, we eliminate the influence of tag transition probability. Hence the local classifiers play the critical roles in the Markov process. 
Wu et al. [19] presented an automatic ch unk pair relation construction algorithm which can handle so-called IOB1/IOB2/IOE1/IOE2 [7 17] chunk representation struc-is that it only works well on IOB or IOE tags rather than more complex phrase struc-tures, for example SBIE. As reported by [20], the use of rich chunk representation such as second begin and third begin of chunk yielded better accuracy in most Chi-nese word segmentation tasks. It is very useful to represent the Chinese proper name which usually contains more than 5 characters. 
To remedy this, we extend the observations from [19] and propose a more general way to automatically build the valid chunk tag relations. The same as previous litera-main concept is to find the valid relationships between leading tags (for example B/I/E/S) and project them to all chunk tags. The summary of this method is listed below. invalid/ 1: valid). That is, ) | ( ~ 1  X  i i s s P is derived via the above approach. 3.2 Speed-Up Local Classifiers As described in Section 2, it is necessary to take all the previous states into account. Directly retrieving the prediction scores from the classifiers might be slow especially when the feature set is large. According to observations, most feature weights can be accessed only once before considering next positions of word. We can use the follow-ing techniques to speed up computational efforts in accessing feature weights. 
For second order CMMs, the feature set co uld be decomposed into three parts: word and the second words before. The deci sion function of conventional linear ker-nel support vector machines has the following form to the input x : decomposing the features as above, we can re-write the decision function as: For each category, the first two terms in Eq. (8) is independent to previous predicted states, while the remaining terms are the sum of weights of the first-order and second-adding the remaining terms during inference. In this paper, we simply adopt the pre-dicted states of previous 1 and 2 words as features and hence | f p 1 |=| f p 2 |=1. 
This technique is also applicable to maximum entropy models where the category generalized to different K -order CMMs. Assume the testing time complexity of a full 2-order CMMs is ( C 2 *| F |) where C is the number of category. By contrast, the testing complexity of our hybrid 2-order CMMs is (| f c |+| f p 2 |+ C *(| f p 1 |). Six well-known sequential labeling tasks are used to evaluate our method, namely, CoNLL-2000 syntactic chunking, base-chunking, CoNLL-2003 English NER, Bio-Text NER, Chinese POS tagging, and Chinese word segmentation. Table 1 shows the statistics of the six datasets. 
CoNLL-2000 chunking task is a well-known and widely evaluated in previous work [13 14 7 19 3]. The training data was derived from Treebank WSJ sec. 15-18 while section 20 was used for testing. The goal is to find the non-recursive phrase structures in a sentence, such as noun phrase (NP), verb phrase (VP), etc. There are 11 phrase types in this dataset. We follow the previous best settings for SVMs [7 19]. The IOE2 is used to represent the phrase st ructure and tagged the data with backward direction. 
Second, the base chunking dataset is the extension of CoNLL-2000 phrase recogni-tion in which the goal is to find the base phrase of the full parse tree. The WSJ sec. 2-tagger with the same training set to label POS tags for both training and testing data. 
The training and testing data of the Chinese POS tagging is mainly derived from the Penn Chinese Treebank 5.0. Ninety percent out of the data is used for training while the remaining 10% is used for testing. However, the task of the Chinese POS boundary information in Chinese text. To achieve this, [10] presented a transforma-tion method to encode each Chinese character with IOB-like tags. For example, the tag B-ADJ means the first character of a Chinese word which POS tag is ADJ (adjec-tive). In this task, we simply use the IOB2 to represent the chunk structure. As a re-sult, there are 64 chunk tags. 
Similar to the Chinese POS tagging task, the target of Chinese word segmentation (WS) is even simpler. The sequential tagger learns to determine whether the Chinese chunk representation bring better segmentation accuracy in most Chinese word seg-and S to represent the Chinese word. BI and IE are the interior after begin and inte-rior before end of a chunk. B/I/E/S tags indicate the begin/interior/end/single of a chunk. 
We employed the official provided training/testing data from CoNLL-2003 to run the experiments on the NER task. The BioText NER is a small-scale biomedical named entity recognition task [15]. We randomly split 75% of the data for training while the remaining 25% was used for testing. 
For the above six tasks, we do not include any external resources such as gazetteers for NER. This make us clear to see the impact of each search algorithm under full supervised learning frameworks. 4.1 Settings Since a couple of the tasks are similar, for example the CoNLL-2000 and base chunk-ing, we adopt three feature types for the six datasets. We replicated coordinate sub-gradient descent optimization approaches [4] with L 2 -SVM as learners. In basic, the SVM was designed for binary classification problems. To port to multiclass problems, we adopted the well-known one-versus-all (OVA) method. One good property of OVA is that parameter estimation process can be trained ber of features and categories. To obtain the probability output from SVM, we employ the sigmoid function with fixed parameter A =-2 and B =0 as noted by [11]. On the other hand, the maximum entropy model used in this paper was derived from [6]. 
As shown in Table1, the parameters of SVM and ME were the same for the six tasks. However, the ME is not scalable to the large-scale and large category tasks, such as base chunking and Chinese WS where more than 80M feature weights needs to be estimated per category. Hence, we incrementally increase the feature threshold until the feature size can be handled in our environment. Though one can increase the hardware performance to cover the shortage, this is not the way to solve problems. 4.2 Overall Results The first experiment is used to evaluate the benefit of incorporating higher order information and the proposed inference al gorithm. We also compared with greedy search and beam search with variant beam width ( B =10,50,250) for all tasks. The overall experimental results are summarized in Table 2. Each row indicates the corresponding inference algorithm. The beam search was applied with different width of beams. Term  X 2 -order X  is the proposed hybrid deterministic and nondeterministic inference algorithm. Without consideri ng learners, the inference algorithm of MXPOST is beam search-based with combinin g previous predicted states as features. 
Clearly, 2 -order inference method seems to be very suitable for SVM. It achieves the best accuracy in four datasets. On the contrary, beam search with ME yields very competitive result as optimal algorithm. In most case, SVM with beam search yields close (but not equal) performance as well as 2 -order inference. ME with beam search shows even more close accuracy in most cases. In CoNLL-2003, ME with no history (i.e. 0-order) achieves better performance than the first order and full second order CMMs. 
To see the significant difference among these search algorithms, we perform s-test [18] and McNemar-test to examine the statistical test. In most cases, it shows statistical significant difference between our 2 -order inference method and Beam search. In BioText and Chinese POS tagging tasks, our method and beam search has the significant difference (p&lt;0.01) under 99% confidence. 
The proposed 2 -order decoding algorithm shows better results than full second order decoders. The main reason is that the history feature is not stable to local classifiers. In consideration, while in the 2 -order CMMs, only the first order information is needed. In some cases, enumerating all previous states may misguide the decisions to the local predicted decisions, while the 2 -order CMMs could reduce the ease of such case. 
In addition, we continue the observations and run a trial experiments to CoNLL-2000 with famous structural learning method, namely CRF. Settings of CRF is the same as our ME and we use the same feature set with excepted for history features to train CRF. The overall training and time took 5.62 hours and 4.64 sec. and achieved about 44 sec and 2.82 sec, respectively. Our method achieves better accuracy and runtime efficiency. In base chunking and Chinese word segmentation and POS tagging tasks, CRF showed very limited training scalability. For example, the number of features in Chinese POS tagging task is more 20M per category. By contrast, our 2 -order CMMs takes less than 10 minutes to train with the same feature set. In this paper, we present a new search algorithm based on conditional Markov mod-els for sequential labeling tasks. We compare our method to Greedy, Viterbi, and Beam search algorithms. The experiments were conducted on six well-known sequen-tial labeling benchmarks. The experimental results showed that our method scales very well while achieving satisfactory performance in accuracy. In the CoNLL-2000 chunking task, our method outperforms previous best supervised systems. In terms of search algorithms, our method yields more superior accuracy than beam search and runtime efficiency (except for greedy search). The full online demonstration of the proposed conditional support vector Markov models can be found at the web site 1 . 
