 show that this sample complexity is tight.
 a prediction  X   X  , which satisfies the following Lemma:  X   X  with To use this Lemma, first define P (inv( X   X  ) &gt; t 1 )  X   X  = 1 E (inv( X   X  )) as follows. Since inv( X   X  )  X  0, All that remains is to express  X  in terms of t . Rewriting, we find that Returning to our original expectation, complexity in Proposition 3.1 is effectively tight.
  X   X  with an expected risk of must on expectation use at least  X ( n ) comparisons in the worst case. Giesen et al. (2009) give the following theorem (we restate slightly): (i.e. D ( X   X  )  X  2inv( X   X  )) we have that if then 2.1. Proof of Theorem 4.1 ability m ( n ) /n . Define the scores Predict  X   X  by the ordering  X   X  of the estimated scores, breaking ties randomly. c ( p, X  ) /n and n &gt; n 0 , the Balanced Rank Estimation Algorithm satisfies score of object j is denoted by Balanced Rank Estimation Algorithm produces an unbiased estimate of the scores interest lies in computing P estimated scores and the target scores: Note that given  X   X  , the variables in the sum are independent. Some algebra reveals: expected Kendall tau distance of Eq. (24), we may bound For this bound to be no larger than  X  2 so that a suitable m ( n ) exists which satisfies m ( n )  X   X (1 / ((2 p  X  1) 2  X  2 )). 2.2. Proof of Theorem 4.2 ability m ( n ) /n . Define the scores Predict  X   X  by the ordering  X   X  of the estimated scores, breaking ties randomly. where a n is a sequence with a n  X  1 .
 score of object j is denoted by Balanced Rank Estimation Algorithm produces an unbiased estimate of the scores this difference apart in  X   X  .
 b between the estimated scores and the rescaled and translated target scores: in Theorem 4.2, can be shown Since there are n items to be sorted, we apply a union bound we have the following Lemma.
 assume that the sorting algorithm breaks ties in the least favorable way. Note that Hence, |  X   X  ( j )  X   X   X  ( j ) | X  2 tn/a .
 |  X   X  ( j )  X   X   X  ( j ) | X   X n .
 c ( p, X  ). Specifically, where a n = n/ ( n + c ( p, X  ) log( n ))  X  1.
 2.3. Proof of Theorem 4.5 probability m ( n ) /n . Define the scores Predict  X   X  by the ordering  X   X  of the estimated scores, breaking ties randomly. satisfies of object j is denoted by Unbalanced Rank Estimation Algorithm produces an unbiased estimate of the scores expected Kendall tau distance achieves the target. Our interest lies in upper bounding Given  X   X  , the random variables inside the sum are independent and one can show that Conditioned on  X   X  , Then plugging this in the expected Kendall tau distance, For this bound to be no larger than  X  2 so that a suitable m ( n ) exists which satisfies m ( n )  X   X  1 / ((2 p  X  1) 2  X  2 ) . 2.4. Proof of Theorem 4.6 probability m ( n ) /n . Define the scores Predict  X   X  by the ordering  X   X  of the estimated scores, breaking ties randomly. Rank Estimation produces with probability at least a permutation  X   X  with denoted by Unbalanced Rank Estimation Algorithm produces an unbiased estimate of the scores previously used in Theorem 4.5 And if j  X   X n , by rescaling t , Define the events Applying a union bound, we find To use the bound in Eq. (100), we first prove the following Lemma. Lemma 4.7. For some a &gt; 0 , 0 &lt;  X  &lt; a 2 and arbitrary b  X  R , if then  X  differ from  X   X   X  ( j ) /a =  X   X  ( j ) + b/a if j  X   X n/a 2 . Suppose that j  X   X n/a 2 . The largest element  X  k that can overlap with j satisfies Since With this, we have t 2 t k n = 2  X n/a ties in the least favorable way, we have  X  j  X   X n/a 2 j =  X n/a 2 . Thus, in Lemma 4.7. If we set t = a remaining elements with accuracy |  X   X  ( j )  X   X   X  ( j ) | X  4 c ( p, X  ). Specifically, Learning , pp. 208 X 240. Springer, 2004.
 3176 of Lecture Notes in Computer Science , pp. 169 X 207. Springer, 2003. Society. Series B (Methodological) , 39(2):262 X 268, 1977.
 2009.

