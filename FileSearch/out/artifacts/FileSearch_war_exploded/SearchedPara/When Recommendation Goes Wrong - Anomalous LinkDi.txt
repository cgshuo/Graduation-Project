 We present a secondary ranking system to find and remove erroneous suggestions from a geospatial recommendation sys-tem. We discover such anomalous links by  X  X ouble check-ing X  the recommendation system X  X  output to ensure that it is both structurally cohesive, and semantically consistent. Our approach is designed for the Google Related Places Graph, a geographic recommendation system which provides results for hundreds of millions of queries a day. We model the quality of a recommendation between two geographic en-tities as a function of their structure in the Related Places Graph, and their semantic relationship in the Google Knowl-edge Graph.

To evaluate our approach, we perform a large scale human evaluation of such an anomalous link detection system. For the long tail of unpopular entities, our models can predict the recommendations users will consider poor with up to 42% higher mean precision (29 raw points) than the live system.

Results from our study reveal that structural and seman-tic features capture different facets of relatedness to human judges. We characterize our performance with a qualitative analysis detailing the categories of real-world anomalies our system is able to detect, and provide a discussion of addi-tional applications of our method.
 D.2.8 [ Database Management ]: Database applications X  Data mining anomaly detection; knowledge graph; link prediction; rec-ommendation systems  X  Work performed while at Google, Inc.
 Figure 1: The People Also Search For feature, showing five good recommendations from the Related Places Graph for the Empire State Building . Prominent use of recommen-dations raises the risk of bad suggestions negatively affecting a user X  X  product experience.
Recommendation systems have become an integral part of modern information retrieval systems. They are used to suggest almost anything to users, including places, products, publications, and on social networks -even friends.
Despite their prevalence, recommendation systems are still capable of making recommendations that users might find irrelevant or unhelpful. These bad recommendations can occur from noise in the real world processes that generate the data that they are trained on, or can be the result of a subtle dependency that the recommendation system doesn X  X  properly model. As the size of a dataset grows, so does its long tail of less popular items, which worsens both problems. Not only do spurious correlations occur more often, but the effects of improperly modeled dependencies become more apparent. These sources of error directly affects the utility of these recommendation systems for information retrieval and content recommendation tasks.

In the literature this problem is usually addressed by chang-ing the original model to include additional features and dependencies. Unfortunately, the cost of properly engineer-ing and validating such an enhanced model for web-scale lines), but more subtle anomalies (red) may remain. recommendation is frequently not justified by speculative performance gains. 1 However, simply ignoring the problem is often equally undesirable as incorrect recommendations have been shown to lower a user X  X  opinion of the system [6].
In this work, we present a secondary ranking system for the detection of such erroneous recommendations in the Google Related Places Graph, a large geographic recommendation system which provides recommendations for hundreds of millions of queries a day. Our approach detects anoma-lous entity recommendations by fusing semantic information from the Google Knowledge Graph with network features from the Related Places Graph. While we focus on a spe-cific problem instance, the approach we present is general, and can be used with any similarly constructed recommen-dation network and knowledge graph (e.g. Freebase [5]).
To evaluate our approach, we perform what is (to our knowledge) the first comprehensive human evaluation of such an anomalous link detection system, and show that we are able to achieve relative increases of 9% to 42% in mean pre-cision (7 to 29 raw points, respectfully) in an anomaly detec-tion task against a very challenging baseline -the live sys-tem itself. We also perform a qualitative evaluation which illustrates the categories of anomalies our system is able to detect.

Specifically, our contributions are the following:
For example, the winning algorithm for the Netflix Chal-lenge was never launched into production [3]. Our dataset for this paper is the Google Related Places Graph, a very large geographic recommendation system with hundreds of millions of entities and tens of billions of similar-ity relations. It is used to provide pairwise geographic entity similarity results for hundreds of millions of search queries a day. In this section, we briefly describe its construction and challenges associated with web-scale geographic recommen-dation.
The Google Related Places Graph G = ( V,E ) is a simi-larity network composed of V entities which are geolocated businesses or organizations, and E edges which encode a similarity score between them (i.e. edge E ij = s ( i,j ), the similarity between entities v i , and v j ). It is an instance of an item-based collaborative filtering system [24], which intuitively captures how related two places are for the pur-poses of web search. Exact details of the similarity function are proprietary, but a number of memory and model based approaches to similar problems have been discussed in the literature [28]. The top k highest weighted outgoing edges of an entity can be used to return ranked lists of similar places for geographic recommendation. A typical result of such use is shown in Figure 1.

An outline of the Related Places Graph is shown in Figure 2. The process starts by collecting entities associated with user search sessions (2a). This is used to populate a Sessions-Entity matrix M from the set of sessions S and establish-ments V (2b). Finally a similarity function s : V  X  V  X  R is used to construct the recommendation network (2c). Unfortunately, the relations captured by the Related Places Graph are not perfect. In particular, there are a number of adverse effects which make it difficult to correctly model es-tablishment similarity, including:
Dealing with such challenges is a non-trivial task, and ge-ographic recommendation systems are is the subject of ac-tive research [15, 33]. The most straightforward strategy is to discard all relationships with low similarity ( E or high distance (dist( i,j ) &gt; D ). While this thresholding strategy can mitigate some forms of errors (e.g. from fre-quency imbalance), it adversely effects recall (especially for establishments with low volume). Additionally, it does noth-ing to address other, more semantic, sources of error. These remaining errors can have an extremely negative influence on how users perceive some queries, typically when multiple semantic meanings or polysemous queries link two seemingly dissimilar places. Examples of anomalous relationships de-tected by our system are shown in Section 7.2.
We note that there are many possible definitions of what might be considered a  X  X elated place X . For the purposes of this work, we consider a place to be  X  X elated X  when it is:
These attributes are subjective, and can vary with the type of entity. For example, whether a business is  X  X seful X  will vary based on location (2 blocks may be too far in NYC, while ten miles may be reasonable in rural Kentucky), or its type (two amusement parks may be far away and still be related). The concept of  X  X elevance X  is also deeply integrated with the nature of the search task which a user is performing at the time of query. For example, when searching for a hotel to stay in, a user might wish to see alternative hotels in the area. However, a user who has already chosen a hotel (and may be already staying there) might prefer to see nearby restaurants or tourist attractions.

In order to capture this relation in all of its nuance, the evaluation of our system utilizes human raters. More details are discussed in Section 5.
Given a graph G = ( V,E ), the Anomalous Link Discovery (ALD) problem is to find a set of anomalous edges E a  X  E which represent a deviation from the dominant patterns in G [21]. ALD is related to the link prediction problem [13], which models the evolution of the network by predicting unseen edges.
Link prediction can be viewed as classifying the set of possible edges of G to those that existent and should non-existent. Many real world graphs are sparse, and so the ex-istent edges (positive label) are a small fraction ( m  X  O ( n )) out of all possible edges ( O ( n 2 )) This asymptotically skewed class distribution is a core challenge of link prediction, in-creasing the variance of link models, and making complete model evaluation computationally expensive.

Unlike link prediction, ALD focuses on the m edges which actually exist -a distinction which becomes increasingly im-portant in sparse real world graphs. We note that this does not eliminate class skew entirely, as most graphs of interest will have more good edges than noisy ones. As such, ALD models must also account for variance arising from both the structure and higher-order semantic properties of a network.
Just as with link prediction, early approaches to ALD consisted of using individual connectedness metrics to pro-vide a ranking of edges by their anomaly value (i.e. edges with score( i,j ) &lt; are returned as the anomalies). Modern link prediction uses supervised learning [14], which provides much more flexibility for link modeling. We apply a similar supervised approach in our ALD model, extracting multiple topological properties as features, and seamlessly combining them with additional semantic information.

Specifically, we seek to model P ( E ij ), the probability of an edge found between v i and v j in G . We assume that edges are a function of topological properties from G , and the intrinsic semantic attributes of the nodes in our network as represented in the Google Knowledge Graph (denoted X KG ), and therefore concern ourselves with P ( E ij | G,X We assume that the input to our process is a similarity graph where edges are weighted with E ij = s ( i,j ) and that G has already been appropriately thresholded to only contain high similarity edges (i.e. E ij &gt; ).
In this section we discuss the features used for link mod-eling in our anomalous link detection system for recom-mendation networks. Our choice of features is based on the assumption that recommendation networks should con-tain strong homophily -that is they should exhibit both structural transitivity, and semantic consistency. We model structural transitivity through features derived from the net-work itself and model semantic consistency through features captured by the Google Knowledge Graph. For each feature we briefly discuss its motivation and outline the its construc-tion function f ( v i ,v j ) 7 X  R .
Our first assumption is that a recommendation network should exhibit homophily through transitivity. That is, if entity A is related to entity B and C, then items B and C should have a much higher chance of being related. We quantify this relationship by introducing topological features from the recommendation network representing the struc-tural connectedness of two entities. These network features are very valuable, and can implicitly capture information which has not been explicitly annotated.
The simplest measures of structural transitivity can be derived from the neighborhoods of two vertices. Features of this variety have been widely used in link prediction[13]. We denote the neighbors of a node v by N ( v ), (i.e. N ( v ) = { i ; ( i,v )  X  E  X  ( v,i )  X  E } ).
More advanced measures of structural equivalence con-sider information beyond the node neighborhoods.
We note that many additional features have been pro-posed for link modeling, but limit our discussion to the tech-niques listed here.
Our second assumption is that good recommendations are those which are semantically consistent with the entity of in-terest. This semantic consistency varies with both the type of entity being considered and the location of the entity. For example, consider two medical practitioners: one who spe-cializes in cardiology, and the other in pediatrics. Although they are both doctors, they might be bad recommendations for one another in an area with many doctors. However, they might well be reasonable recommendations if they were the only two doctors in town. Fine grained semantic consistency can be quite nuanced in other domains as well, such as food service. To capture these relationships, we annotated enti-ties in our recommendation network with information from the Google Knowledge Graph.
The Google Knowledge Graph is a comprehensive knowl-edge store that contains semantic information about entities in the real world [26]. Storing information on people, places, and things, it aids in query disambiguation and summariza-tion.

In this work, we are concerned with the subset of the knowledge graph which refers to places. For our purposes, a place is a location entity which people may wish to visit such as a business, monument, or school. Each place in the Figure 3: An idealized hierarchy of entity categories in the Knowledge Graph (a), showing how even a seemingly straightforward concept like pizza restaurant can be encoded in a number of different ways. In (b), an example of 3 hy-pothetical businesses which all offer pizza, but due to noise in the labeling process, have different sets of semantic cate-gories. knowledge graph has an associated set of semantic categories associated with it. These categories are related to each other through a hierarchy ranging from the specific (e.g. pizza-delivery, French restaurant) to general (store). Figure 3 il-lustrates an example of an idealized hierarchy (3a) and the semantic labels for several similar hypothetical businesses (3b). Notice how related entities do not necessarily share the same semantic categories.

The size and scope of the Knowledge Graph afford a va-riety of ways to create features for classification tasks. We briefly discuss some of them below:
We note that the inclusion of categorical features from the Knowledge Graph has the potential to greatly increase a model X  X  complexity. Our approach relies on using a small number of training examples, and so we have discussed ag-gregating categorical information. However, as available training data grows, model complexity can be easily in-creased by treating each particular category (or pairwise combination of categories) as separate features.
Our approach to Anomalous Link Discovery uses user in-put to train discriminative link models. This user input is collected during the course of routine product quality eval-uations. The anomalies detected by these models are then evaluated by human raters to determine model performance. In this section, we briefly discuss the process used to con-struct datasets and link models, the design of the experi-ments, and our human evaluations.
We use human evaluations of the similarity between two place entities as our training data. This data was collected in the course of routine product quality evaluations.
For each pair of entities, at least three different human raters were asked to judge the recommendation (on a scale of { Not , Somewhat , Very } ) based on its relevance (their in-ternal view of how analogous the two places were), and its usefulness (how helpful it would be to a user searching for a the original place). The distinction between relevance and usefulness is elaborated on in Section 2.3.

To convert their ratings to binary labels, a value of { 1,2,3 } was assigned to each judgment of { Not , Somewhat , Very } . An edge was labeled as relevant (or useful) when its respec-tive average score for that category was above 2. As we assume that a good geographic recommendation is both rel-evant and useful, we labeled an edge as related only when judged as having both properties.

Using this process, we distilled over 28,800 human ratings into 9,600 labeled examples pairs for training. Of these 7,637 pairs were labeled as related and 1,963 labeled as not-related (a positive class label imbalance of 5:1).
Having discussed our method for generating training data, we turn to our choice of classifier for link modeling.
We compared the performance of Logistic Regression (reg-ularized with the L 2 loss), and Random Forest Classifiers to model the probability of a recommendation link being re-lated. We used stratified 5-fold cross validation for model selection, where Random Forests significantly out performed Logistic Regression (shown in Table 1). For the remainder of our paper, we present results using models trained with Random Forests, which we abbreviate RFC.

The baseline models we consider in decreasing order of difficulty: Table 1: Average model performance with different feature sets in stratified 5-fold cross validation.
The method under consideration are:
The decision as to whether two geographic entities are re-lated is nuanced, varying with their location, the entities, and ultimately the individual. In order to satisfactorily cap-ture this relation, we evaluate our model X  X  performance using human evaluation.

The human raters we use in our experiments are paid evaluators used for product quality assessment. These raters have been professionally trained using a rigorous process and guidelines. They all reside in the region where our study takes place (United States), and have a full proficiency in English. These vetted raters allow us to avoid some of the quality issues which may be present in other large scale hu-man evaluations (e.g. Mechanical Turk [12]).

We generate test datasets for our evaluation by sampling entities from a subgraph of the Related Places Graph con-taining only entities located in the United States. This sub-graph is large enough to be interesting (millions of nodes and billions of edges), but it eliminates some cultural vari-ance which might complicate evaluation. Our test datasets are generated in one of two ways:
To evaluate the performance of a model, we score the en-tire test datasets, and send each of the 200 most anoma-lous edges (highest P ( E ij = 0 | G,X KG )) out to three human raters. An individual rater was allowed to answer a maxi-mum of 20 out of the 600 judgments per model, ensuring that at least 30 unique raters contributed to each assess-ment. The raters were instructed to perform research (e.g. visit an entity homepage, read reviews, etc.) about the two entities and then to deliver a judgment. The raters results are converted to labels (as discussed in Section 5.1), which we use to calculate the area under the precision curve. 2
After evaluating our models through cross validation, we conducted two large scale human evaluations of feature qual-ity from links present in the Google Related Places Graph. As described in Section 5.2, one evaluation consists of loca-tion entities which were sampled uniformly at random and the other of entities sampled in proportion to their web traf-fic volume.
The results of our uniform sampling evaluation are pre-sented in Figure 2. Here, we briefly discuss some observa-tions in terms of relevance and usefulness .

Of all the features considered, we find that the those from the Knowledge Graph perform best for detection of links which are not relevant, initially outperforming the strong RFC Related baseline by 14 precision points ( k =25), and nar-rowing to 5 at k =200. Structural features were much less competitive on this task, failing to outperform the baseline at all. A final observation is that the distance model is not able to determine relevance (as judged by humans) signifi-cantly better than random.

In contrast, we find that structural features are much better predictors of recommendations which are not useful. We attribute the strong performance of structural features for k&lt; =50 to those features (e.g. a low number of com-mon neighbors) which can provide strong evidence for non-relation. As k grows larger, the network structure X  X  signal is less valuable, and performance degrades.

Both models perform well on detecting recommendations which are not related, with RFC Network initially beating the baseline by 42% at k =25, but again degrading as more re-sults are returned. The joint model (RFC All ) performs gen-erally well on this task, leveraging the strengths of both the structural and semantic features to beat the baseline by 37% at k =25, to 9% at k =200. However, we note that in some cases the model performs worse than its constituent parts. This occurs when the structural features provide poor dis-criminating power, and is a result of the highly heteroge-neous phenomenon we are modeling. The performance of the joint model will improve as our system collects more human judgments.

We remark that the use of uniform sampling emphasizes entities which lie in the long tail of popularity. These rela-tively unpopularity entities have similarity estimates based on weaker statistical evidence, and are more likely to have
As our task is anomaly detection, we consider the positive class label to be 0 when calculating the mean precision. data quality issues. Lower popularity then, impacts both structural and semantic feature quality. Our results show that even under such constraints, it is indeed possible to identify anomalous recommendations with a high degree of precision.
In order to better understand the user impact of our sys-tem, we designed an evaluation where the entities were sam-pled in proportion to their percentage of total search volume. This experiment reflects the real world situation in which our product is used. Instead of dealing with unpopular entities, this experiment allows us to understand how our approach works when the entities have both good similarity estimates and detailed Knowledge Graph entries. This allow the study of nuanced anomalies (and not those that are simply a re-sult of noise). The results of this evaluation are presented in Table 3.

With respect to detecting relations categorized as not rel-evant, we find that models using features from the Knowl-edge Graph (RFC Knowledge , RFC All ) again perform much better than RFC Network . We also see that the relative per-formance of the baseline is much stronger on this task, and that only the combination of topological and semantic fea-tures (RFC All ) is able to exceed it (by up to 12% at k =200)
For not useful recommendations, we again see that net-work features are very strong indicators of whether users consider a recommendation to be useful. When entities have good similarity estimates, the network features much more closely model user behavior. Conversely, the rela-tively poor performance of the Knowledge Graph features on this task can be explained by the more robust semantic similarities captured between entities. For example, when given an airport, we may recommend nearby hotels. Such a recommendation is quite useful, but may be flagged as an anomaly because it is between two very distinct semantic categories (and there was a lack of training data indicating that these types of entities are appropriate to recommend for each other). We discuss this further in Section 7.
Finally we see that the performance of structural features again carries over to detecting entities which are not related, beating the baseline by 19% at k = 25 and still by 5% at k = 200. We see that RFC All initially suffers from its re-liance on semantic features, but recovers and is superior to both methods by k =200. The worst performing model is RFC Knowledge , as it can not take advantage of the enhanced similarity estimates available for popular entities.
Here we discuss the conclusions we have derived from our experiments, provide a qualitative analysis of the types of anomalies we detect, and discuss applications of our work.
As seen in Section 6, our experiments show that our pro-posed approach is able to detect erroneous recommendations on a very large recommendation system with much higher precision than any of the baselines we consider. In addi-tion to the raw performance benefits, we have draw several higher-level conclusions from our human evaluation, which we highlight here: Structural and semantic features are not equal. Our results on the long tail of entities (Uniform Random Sampling) show for entity pairs which are not useful. that semantic features from the Knowledge Graph are good at identifying recommendations which are judged as not rel-evant, while structural features from the recommendation network are good at determining relationships which judged to be are not useful.
 Network features help, regardless of location popularity. As locations receive more web traffic, we are able to build a recommendation network that more accurately models the underlying relationships. This decreases the noise of features directly derived from the network, allowing them to perform well even as the number of anomalies decreases (as in our traffic weighted experiment).
 Distance alone is not enough. Many geographic recommen-dation systems simply include distance-based constraints or regularization to model geospatial dependencies. Our results show that these models can be improved by using structural features of the network, and semantic features of locations.
We have performed a qualitative analysis of the types of anomalies our approach is able to capture, which we briefly discuss here. Table 4 shows a summary of representative examples of actual anomalies we have detected using our approach. Specifically, the variety of anomalies which we were able to discover included those due to: Unmodeled Phenomena : Unmodeled interactions between entities can lead to very interesting anomalies, such as the recommendation between IKEA and hotels. We discuss how this class of anomalies can be used for targeted ontology im-provements in Section 7.3.2.
 Ambiguous Phrases : We are able to detect anomalies created both polysemous words and phrases (e.g. Mom X  X  Bar / Bar Association ).
 Conglomerate entities Multi-sense entities can result in surprising recommendations. For example, normally one would not expect recommendations between a government entity and firearm clubs. However, as the Florida Depart-ment of Agriculture also issues gun permits, several such recommendation links appear. We note that such recom-mendations for conglomerate entities may in fact be useful to users. In such cases, (as with unmodeled phenomena) the anomalies we find can be used to improve the quality of data stored in the Knowledge Graph itself.
 Data-sparsity : In some rural locations, there is not enough data for confident estimates of similarity (e.g. Tony X  X  Small Engine Services / Academy Animal Hospital ). When this happens, features from the Knowledge Graph allow detec-tion of entities which are not related.
 Mobile users &amp; Search Completion : Finally, two sources of anomalies seemed tied to mobile users and search com-pletion technologies. First are auto-completion errors . This category of errors seemed due to substring similarity be-tween entity names. Examples include people X  X  names to locations (e.g. Tuscano / Tuscany ), and between very un-related entity types (e.g. Boy X  X  and Girls Club / Girls Club ). Second, are nearby entities . This category of anoma-lies contained dissimilar businesses entities which are very close to each other. We suspect this behavior is due to in-dividuals searching for information about that location (e.g. in order to plan a shopping trip).

Our qualitative analysis shows that our approach is able to address the challenges we outlined earlier in Section 2.2, and that we are able to discover anomalous links across a variety of categories, causes, and geographic locations.
Although our work has thus far focused on a specific prob-lem instance, we believe that the approach we present is gen-eral, and has applications to any recommendation network which has structured information available. In this section we highlight additional applications of our method which go beyond the simple removal of low-quality recommendations.
In our qualitative analysis, we have seen that anomalous links (specifically due to unmodeled phenomena, multi-sense entities, and data sparsity) tend to come in  X  X lumps X  for an entity. A natural extension of our approach, then, is to consider the detection of anomalous entities in addition to anomalous links. We note that an outlier score for an entity may be constructed as a function of it X  X  link probabilities, for example:
Maintaining an accurate web-scale knowledge base is a very challenging endeavor, and this score could be useful for highlighting entities which have incorrect information about them (perhaps from recent changes). Entities flagged as anomalous by this measure could then be prioritized to be investigated by the relevant data quality team.
When an anomalous entity is not the result of incorrect information, it can indicate that our underlying semantic model is not expressive enough. These data-driven improve-ments can result in groups of entities acquiring new semantic categories, or even changes to the hierarchical relationships between categories.

As a case study, we return to the furniture store IKEA . Ta-ble 5 illustrates some highlights from our investigation into the IKEA store anomaly found during our qualitative analy-sis. Interestingly, we see two things. First, in areas of high density (such as Brooklyn, NY), users focus on transporta-tion methods to/from the store. This is understandable, as many residents of New York City do not have cars capable of transporting large furniture. Second, in areas adjacent to large rural regions, IKEA is treated as a travel destination. Top recommendations include Hotels and tourist attractions like Amusement Parks or Aquariums . These anomalies sug-gest that IKEA should be modeled differently than a tra-ditional furniture store, perhaps with additional semantic senses (such as a Tourist Attraction or Travel Destination ).
We believe that the investigative analysis of anomalies ex-posed by our approach will allow us to not only improve the quality of our recommendation system, but also extend the expressiveness of the Knowledge Graph itself. Such improve-ments can allow a better understanding of user intent and improve user experience across a variety of products.
The problem of detecting anomalous links touches on the domains of Link Prediction, and Recommender Systems, which we briefly discuss here.

Link Prediction models the strength of relations, usu-ally in order to find high probability links which are miss-ing from a network. Early work on link prediction was un-supervised, using topological features [13]. More recently, the problem has been addressed as in a supervised fash-ion [2, 14]. Supervision allows the blending of topological features with semantic ones, and a number of methods do-ing such have been proposed. Semantic features used in the literature include textual features (e.g. paper titles or keywords[2], sometimes with TF-IDF weighting [30]), loca-tion features, [7, 18], or social interactions [32]. Other recent work examines using community information [27] or trans-ferability of models across networks [29]. More information is available from several surveys [9, 16, 22].

Although there has been considerable work on link pre-diction, the vast majority of the literature deals with the discovery of non-existent links and not the detection and removal of anomalous ones. The anomalous link discov-ery problem was introduced by Rattigan and Jensen [21], who noted that topological measures of similarity performed well for anomalous co-authorship detection. Relatively little work followed. In [11] Huang and Zeng examined anomalous links in an email network, and Ding et al. [8] examined clus-tering and betweenness centrality for detecting links that represented network intrusion. Our work helps address this gap in the literature by analyzing the performance of an anomalous link discovery system in a very large industrial setting. Furthermore, our comprehensive human evaluation is the first such human evaluation (to our knowledge) of any anomalous link detection approach.

More recent work in graph anomaly detection has focused on discovering anomalous communities [19, 20]. A compre-hensive survey is available from Akoglu et al. [1].
Recommender Systems model user/item interactions to suggest additional content to users. Similar to link pre-diction, the focus is on finding highly confident recommen-dations (i.e. those which future users will have the highest likelihood of interacting with). Item based filtering was in-troduced in [24]. Since then, entity recommendation has been proposed for use in search in a variety of different set-tings [4, 34, 35]. Removal of anomalous recommendations is typically performed implicitly during model improvement, usually through the inclusion of additional features to the model , and several recent works target geographic recom-mendation [15, 33].

We view our work as complementary to that of the exist-ing recommender system development process for two rea-sons. First, by explicitly modeling the detection of anoma-lous links, our approach provides an insightful view of what is being effectively captured by a recommendation system. This analysis is quite useful for understanding the limits of an existing system, and where likely areas of the best im-provements are (e.g. Qualitative Analysis in Section 7.2). Second, it is sometimes impossible to replace an entire rec-ommender system used in production as substantial model changes require validation (which can be both expensive and time consuming). Our lightweight modeling of anoma-lous link discovery approach allows for high precision focused changes changes to existing models, which can allow for eas-ier validation.

Finally, we note that the disambiguation of user intent from keyword search is a mainstay of research in information retrieval [17, 23, 25, 31]. We believe that our work helps further the discussion on this important problem.
In this work, we have proposed and evaluated a unified approach for anomalous link discovery in recommendation systems. Given a recommendation system, we treat its out-put as a network and extract graph features which quantify the structural transitivity present in the recommendations. We fuse this information with features constructed from an appropriate knowledge graph, which capture the semantic consistency of the entities.

Experiments on the Google Related Places Graph using human raters show the effectiveness of our approach on a very large geographic recommendation system. Interest-ingly, our experiments also show that structural features from the recommendation network capture a sense of a rec-ommendation X  X  usefulness to users, while semantic features better capture a sense of the relevance of a recommendation.
In addition to strong quantitative results, our qualitative analysis illustrates how the anomalies exposed by our system provide a valuable lens to study a recommendation system X  X  behavior. Investigating such anomalies can enhance under-standing of user intent and has the potential to improve user experience across all information retrieval systems leverag-ing the same knowledge graph.
 We thank the anonymous reviewers for their comments.
