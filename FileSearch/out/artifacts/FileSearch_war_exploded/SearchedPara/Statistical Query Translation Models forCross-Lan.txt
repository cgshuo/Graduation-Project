 JIANFENG GAO Microsoft Research JIAN-YUN NIE
University of Montreal and MING ZHOU Microsoft Research Asia 1. INTRODUCTION
With the huge expansion of documents in many languages on the Web and the desire of non-native speakers of a language to be able to retrieve them, cross-language information retrieval (CLIR) systems have become increasingly important in recent years.

The goal of CLIR is to resolve the language mismatch between documents and queries. This can be achieved by translating either documents or queries. Since translating queries is more efficient and easier, research in the area of
CLIR has focused mainly on methods for query translation. The goal of query translation is to determine the best translation words and weights for a given query. A common approach to query translation is based on the dictionary. This approach is popular because of its simplicity and the increasing availability of machine readable bilingual dictionaries. However, besides the problem of the incompleteness of the dictionary, we are also faced with the problem of ambiguity in translation, that is, multiple translations are stored in a dictionary for the same word. This article focuses on this problem: We try to select the best set of translation words by solving ambiguities according to the coherence of translation words and syntactic information.

In general, information retrieval (IR) can be viewed as a reasoning process that tries to determine if there is a relationship between a document and a query and how strong the relationship is [van Rijsbergen 1986]. This process may involve any type of reasoning, such as reasoning based on synonymy rela-tions, which is often used in a query expansion process. Query translation can also be viewed as a reasoning process that tries to determine a related query, although in a different language. The relationship between query translation and inference in IR has been clearly shown in Nie [2003]. It turns out that CLIR is a particular case of more general inferential IR that exploits relationships be-tween terms. In query translation, the term relationships used are translation relationships. In this article, we will concentrate on several concrete approaches to implement such a process to determine (or infer) the best query translation according to both the translation relationships and the other translation words.
Our approaches are defined within a statistical framework. Statistical reason-ing is popular in IR due to the fact that most available knowledge in IR is of a statistical nature. Query translation in CLIR is not an exception. Much of the translation and linguistic knowledge available is statistical: a word can be translated to another to a certain degree, and different words in a language are coherent (i.e., tend to co-occur) to some degree. The statistical framework allows us to integrate such different types of knowledge.
 In this article, we present three statistical models for query translation for
English-Chinese CLIR. These models differ in the information used to deter-mine the translations. The first model is the decaying co-occurrence model .It assumes that the selection of the translation of a query term depends on both the translation probability and the translations of other terms within a query.
The best set of translation terms contains those that are good translations of the original terms and form a coherent set together. Following the previous studies [e.g., Adrian 2000; Ballesteros and Croft 1998; Gao et al. 2000; Gao et al. 2001b; Gao et al. 2002b], a mutual information value between translation terms is estimated according to their co-occurrence within a predefined win-dow. The translation term that has the highest mutual information score with other translation terms is considered to be the most coherent and is selected.
In addition, we also take into account the distance between terms, assuming that closer terms have stronger relationships [Gao et al. 2002b].

While in the co-occurrence model, a query is simply viewed as a sequence of words without any linguistic structure, the other two models take advantage of the syntactic structure among terms. In the noun phrase (NP) translation model , we first identify NPs in a query, and then translate them as units by assuming that the selection of a translation only depends on the other transla-tions within the same NP [Gao et al. 2001b].

The third model is the dependency translation model . A dependency, repre-sented as a triple, is a pair of words that have a syntactic dependency relation, such as verb-object. It is our observation that there is a strong correspondence in dependency relations in the translation between English and Chinese despite the great differences between the two languages. In the dependency translation model, we first detect dependency triples in a query using a parser, and then translate them as units. Similar to the NP translation, we assume that the se-lection of a translation only depends on the translation of the other word in the same dependency triple [Gao et al. 2002b]. While NPs only capture dependence of adjacent terms in a query, dependency triples can captures syntactic de-pendences between nonadjacent terms. Therefore, the dependency translation model can be viewed as a generalization of the NP translation model.
We evaluate the three models using TREC Chinese collections. Our results show that each of the methods achieves significant improvement over the simple dictionary-based approaches. We demonstrate that linguistic structures such as phrases and dependency triples are beneficial to query translation if they can be detected and used properly.

The remainder of this article is organized as follows. Section 2 formulates the query translation problem and discusses major research tasks. Sections 3 to 5 describe in detail each of the three query translation models. Evaluations are presented where appropriate. Section 6 presents experimental results of CLIR on TREC Chinese collections. The discussion and related work are presented in Section 7. Finally, the article is concluded in Section 8. 2. QUERY TRANSLATION
We refer to the language of queries as the source language (i.e., English in this article), and the language of documents as the target language (i.e., Chinese in this article). In the rest of this article, we use the following notation.  X  X et an English query be denoted by e ={ e 1 , e 2 , ... , e ber of distinct terms in e . We also assume some way of detecting linguistic structures s (e.g., phrases or dependency triples) of e .  X  X e assume there is an English-Chinese bilingual dictionary D , which defines for each English query term e i , a set of m distinct Chinese translations: D ( e ={ c  X  X e assume a one-to-one word translation between source and target languages 1 . Therefore, the translated query is represented by c ... , c the dictionary, that is, c j  X  D ( e i ). In particular, either e word as suggested by Brown et al. [1993].  X  X e assume some way of generating a set of candidate query translations given e denoted by GEN ( e ). In our experiments, GEN ( e ) is represented as a lattice where each node is a Chinese word c i , j , that is, the j -th translation of the i -th query term in e .

The task of a query translation model is to assign a score for each of the translation candidates in GEN ( e ) and select the one with the highest score:
Notice that Equation (1) indicates that the translation model works as a ranking function. Therefore, the score can be either the conditional probability P ( c s ) or any order-preserving transformation of the probability. The probability is typically broken down into its component probabilities as where c i denotes the set of translated query terms excluding c tion that maps ( e , s , c i ) into equivalence classes according to the independence assumptions used by the translation model.

Sections 3 to 5 will describe the three query translation models in turn. For each of them, we focus the discussion on the following three basic research tasks:  X  Modeling: defining and detecting the linguistic structures s , and defining the mapping function , that is, the independence assumptions (e.g., the Markov assumption) that are used to break down the probabilities in Equation (2),  X  Training: learning the free parameters of the statistical model using bilingual or monolingual training data, and  X  Decoding: performing the argmax operation of Equation (1) in an efficient way. 3. CO-OCCURRENCE MODEL 3.1 Basic Principle
A correct translation is the one that best fits the context of the whole sentence (or query). The sentence in source language reflects the context of the sentence, but it would be difficult to directly compare a translation with the source sen-tence unless there is a well-defined similarity measurement between words across languages. An alternative approach is to assume that all the transla-tions selected for the other words of the source sentence form a specification of the context. Then a good translation is the one that has a high cohesion with the other translations. The advantage of the alternative approach is that there is no need to measure cross-language word similarities. Only relationships be-tween words of the same language are used. They can be obtained through co-occurrence statistics in a monolingual text corpus. This is the principle of the co-occurrence approach to translation selection which will be described in this section. It can also be expressed as follows: correct translations of query terms tend to co-occur in the target language and incorrect translations do not.
In what follows, we describe (1) how to measure the term similarity via a so-called decaying co-occurrence model, and (2) how to select an optimal set of query term translations. 3.2 Decaying Co-Occurrence Model
The definition of similarity between two terms, w i and w forms of co-occurrence statistics. Mutual information is among the most com-monly used [van Rijsbergen 1979]. It is defined as follows where
Here C ( w i , w j ) is the frequency of co-occurrences of terms w a predefined window (e.g., a sentence) in the collection of the target language, C ( w ) is the number of occurrences of term w in the collection.

We observe that in Equation (3) any co-occurrence within the windows is treated in the same way, no matter how far they are from each other. In reality, we find that closer words usually have stronger relationships, and thus should be more similar. Therefore, we add a distance factor D ( w information calculation. This factor decreases exponentially when the distance between two terms w i and w j , increases, that is, where  X  is the decaying rate, which is determined empirically, and Dis ( w is the average distance between w i and w j in the corpus.

Term similarity in the extended co-occurrence model consists of two compo-nents (1) the mutual information MI ( w i , w j ) as defined in Equation (3), and (2) the decaying factor D ( w i , w j ): 3.3 Training
The decaying co-occurrence model parameters (i.e., MI (.) and D (.) in Equa-tion (5)) are estimated on a Chinese newspaper corpus consisting of approxi-mately 80 million characters. The text corpus was first word-segmented using a Chinese word segmentation system MSRSeg [Gao et al. 2005a]. Then all stop words were removed. We set the window size as a sentence when estimating
MI (.) and D (.). To deal with the sparse data problem, we used good-turning smoothing when estimating P ( w i , w 2 ) and P ( w ) in Equation (3). That is, we as-sume that the number of unseen events (i.e., term w and term-pair ( w our case) is the same as the number of the events that occur once. As a result, the final estimate for P ( w ), for example, is P ( w ) = r  X  /
Here r is the number of occurrences of w in the training corpus, N is the total number of word occurrences, and n r is the number of words which occur r times in the training data.

The decaying rate  X  in Equation (4) was optimized using tests with query expansion in Chinese monolingual IR. That is, for each term, we expand it by an additional synonym term that has the highest cohesion value with the other words of the original query. This expansion task is very similar to the trans-lation selection in CLIR. Therefore, it gives a good indication of the possible impact on query translation.
 A Chinese synonym dictionary is generated for query expansion from the
LDC bilingual dictionaries 2 as follows: for each Chinese term c and each of its English translation e , we consider all the Chinese translations c of e as synonyms of c .
 Our tests were carried out on several TREC Chinese collections. For example,
Figure 1 shows the TREC-9 retrieval results with query expansion, while the decaying rate varies. It can be seen that the decaying co-occurrence model per-forms generally better (when  X &lt; 2) than the basic co-occurrence model (when  X  = 0). With a decaying rate of 0.8, we obtain the best performance of the average precision 23.3%, which is 5% better than the basic model. These experiments show that the decaying factor allows us to better distinguish between strong and weak term relationships. As the problem of translation selection in CLIR is similar to this expansion task, we can expect a similar effect with the decaying factor. Although in our CLIR experiments we found that the optimal value of the decay rate varies slightly from collection to collection, we will only report
CLIR results in Section 6 by setting the decaying rate to 0.8. 3.4 Approximate Translation Selection Algorithm
Given the measurement of term similarity, ideally, we should select for each query term the translation that co-occurs the most often with (or the most similar to) the selected translations of other terms in the same query. However, finding such an optimal translations is computationally very expensive as will be described. Therefore, we use an approximate greedy algorithm as follows [e.g., Adriani 2000; Gao et al. 2001b]:
Apparently, this algorithm is suboptimal. As pointed out in Liu et al. [2005], the cohesion score for a translation as in Equation (7) is computed with regard to all possible translations of other query terms. It does not differentiate correct translations from incorrect ones. As a result, the translation of differ-ent query terms is determined independently. In spite of the deficiency, the greedy search algorithm has been widely used since an exact algorithm is prohibitively expensive. In the next section, we will formulate the translation selection problem under the framework of a graphic model (GM) [e.g., Jordan et al. 1999] and discuss the underlying assumptions of the greedy algorithm. 3.5 A GM View A query translation model can be viewed as an undirected GM. For example,
Figure 2 shows a query translation model of a 5-term query. Each node repre-sents a distribution of a translation set of a query term. The edges of the graph represent a set of independency assumptions among query term translations.
The task of query translation is to find a set of translations that maximize the joint probability P ( w 1 , w 2 , w 3 , w 4 , w 5 ).

The GM view imposes three research tasks on query translation. The first is how to generate translation candidates for each term and how to model the distribution of the candidates. Traditionally, a bilingual dictionary is used and all translations of a query term are assumed to be uniformly distributed. We may also induce a distribution using a statistical translation model learned from parallel bilingual corpora.

The second task is how to determine the graph topology, that is, what in-dependence assumptions we may use. The third is how to compute the joint probability. These two problems are closely related. The efficiency of the joint probability computing largely depends on the graph topology.

In the co-occurrence model as described previously, we assume that the se-lection of each translation is consistent with the selected translations for other query terms. Therefore, we assume that the five nodes form a clique as shown in
Figure 2(a). Suppose that we wish to compute the marginal probability P ( w We obtain this marginal probability by summing over the other variables as: where h (.) is a feature function, and Z is a normalization factor.
We see that the computational complexity of P ( w 1 ) scales as d that each query term has d possible translations). This is prohibitively expen-sive even for a very short query. We therefore resort to an approximated word selection algorithm as described in Section 3.4 by introducing a translation in-dependence assumption. The corresponding GM is shown in Figure 2(b). P ( w can then be factored as:
Note that no more than two variables appear together in any summand, and thus the computational complexity is reduced to d 2 . As discussed, the reduction of complexity comes at the sacrifice of accuracy.

In general, the computation complexity depends on the largest size of the clique in the graph. The NP and dependency translation models described in
Sections 4 and 5 are used to implement the idea that the linguistic structure of a sentence can be utilized to identify cliques. Linguistic units, such as NPs or dependency triples, can be translated as units, and the translation can be done accurately using only the internal information of the unit. As a consequence, the graph is divided into a few smaller subgraphs. The probability of each subgraph can be inferred independently with an optimal order that leads to a lower computation complexity.

Using the three translation models that we propose in this article, our query translation process can be cast in a sequential manner as follows.  X  Identify NPs and dependency triples of a query.  X  Translate words in NPs using the NP translation model described in
Section 4.  X  Translate words in dependencies using the dependency translation model described in Section 5.  X  Translate remaining words using the co-occurrence model.

We call this approach to combining the three translation models the sequen-tial combining approach . Using such an approach, we only keep one translation for each query term. We can also combine these translation models using a par-allel combining approach where, for a given query, we first obtain three sets of translation terms obtained using the three translation models, and then com-bine the three sets. As a result, there are multiple translations for each query term. In our experiments, we only use the sequential combining approach when we directly evaluate the translation accuracy, and we use the parallel com-bining approach when we evaluate the CLIR performance. This is due to the fact that multiple translations often lead to better CLIR performance [Xu and
Weischedel 2000; Gao et al. 2000]. Readers can refer to Section 6 for a detailed description and analysis. 4. NP TRANSLATION MODEL
Although the translation of multiword phrases is usually more precise than a word-by-word translation, many significant NPs are not stored in the dictionary.
For instance, in TREC-9 queries, more than 50% of noun phrases, which can be detected by our method described in this section, are not in our dictionary.
In previous IR research, NPs have been identified using a set of syntactic patterns (e.g., Ballestero and Croft [1997]; Fagan [1988]). Sequences of nouns and adjective-noun pairs were taken as phrases. However, this simple method has not produced consistent improvement. Fagan [1988] reported a decrease in performance, while Ballestero and Croft [1997] did not obtain a significant improvement over single words. One of the problems is that this simple ap-proach often over-generates NPs: non-NPs may be identified as NPs. This may negatively affect the monolingual IR performance because of a deformed dis-tribution of occurrences of these items. In addition, the identified phrases are still translated word-by-word in Ballestero and Croft [1997].

In our approach, we use a more sophisticated NP identification process. It is carried out in a bottom-up manner: we first identify base NPs, and then complex NPs. The reason to separate the process into two steps lies in the fact that base NPs can be identified with high accuracy, while the complex NPs cannot be. Therefore, we only use a small set of syntactic patterns in the second step in order to select sufficiently reliable complex NPs that may significantly affect the performance of retrieval. Though we focus on NPs in this section, the methods can be extended to other consecutive phrases such as the chunks defined in Tjong and Buchholz [2000]. 4.1 Base NP Identification 4.1.1 Principle. A base NP is a simple noun phrase that does not contain other noun phrases recursively. For example, the elements within [...] in the example shown in Figure 3 are base NPs. The part-of-speech (POS) tags NNS (plural noun), IN (preposition), and VBG (verb-ing) etc. are those defined in Marcus et al. [1993].

The identification of base NPs usually involves two steps (1) POS tagging, and (2) base NP chunking.

In classical statistical approaches [Church 1988; Ramshaw and Marcus 1998], these two steps have been separated. POS tagging often serves as a pre-cursor, and NP chunking uses POS patterns (e.g., DT-JJ-NNS) that are learned from a tagged corpus.

By separating the two steps, the solution of the first step is used in the second step as if it is certain. The uncertainty involved in the first step is no longer taken into account in the second step. In fact, the correct solution of the first step may be ranked second, third, etc. This is particularly the case when the probabilities of these solutions are close to that of the first solution. Therefore, a too early selection in the first step may be an important source of error.
In our approach, we try to integrate the two steps and their uncertainties together and use a unified statistical model to choose the globally optimal so-lution [Xun et al. 2000]. We keep the n -best ( n &gt; 1) ranked POS assignments in the first step. Then, in the second step, we determine the best base NPs by considering both the probability of POS tagging and that of base NP pattern.
The value of n is chosen empirically to obtain the optimum balance between efficiency and accuracy. 4.1.2 Base NP Tagging Model. This section formulates the above two steps in mathematical terms. A more detailed description can be found in Xun et al. [2000]. The task of base NP tagging can be stated as follows. Given an English sentence e ={ e 1 , ... , e n } , we search the most probable base NP sequence b { b , ... , b m } ( m  X  n ) that maximizes the conditional probability P ( b POS tag sequence t ={ t 1 , ... , t n } is introduced as a hidden variable:
The base NP tagging is performed using the so called maximum approximation: In this formula, P ( t | e ) aims to determine the best POS tags for a sentence e , and
P ( b | t , e ) aims to determine the best base NP tag sequences from them. Hence the search space consists of the set of all possible POS tag sequences and all possible base NP sequences. In practice, in order to reduce the search space, only n -best POS tagging of e are retained in the first step. In our implementation, we used the A  X  algorithm to search for the n -best t according to the probability P ( t | e ). According to Bayes X  rule, we have We also assume independence among the relationships between tags and English words; and we use a tag trigram model to approximate P ( t ). In the second step, we determine the best base NP sequence, given the n -best
POS sequences. A similar approach to the first step is used. According to Bayes X  rule, we have For a given e and its t , the denominator is a constant and can be dropped. Let b i , j denote a base NP that spans from t i to t j .Wehave Therefore, we get Equation (11) Here, the two terms on the right-hand side can be decomposed as follows where b k is the base NP that contains the tag t i . 4.1.3 Training. The tagging models are trained on Penn Treebank [Marcus et al. 1993]. First of all, all possible base NP patterns (i.e., POS tag sequences) are extracted from the annotated corpus. There are more than 6,000 patterns in the Penn Treebank. After being filtered by a set of linguistic rules, 1,169 pat-terns are kept. Then, all parameters in our statistical model are estimated on training data using maximum-likelihood estimation (MLE) with a particular smoothing method, called Modified Absolute Discounting, a variant of the mod-ified Kneser-Ney smoothing method [Chen and Goodman 1998] as described in
Gao et al. [2001]. These parameters, as shown in Equations (10) and (11), are (1) P ( t i | t i  X  2 t i  X  1 ), (2) P ( e i | t i ), (3) P ( b
It is worth noting that we used a specific method of estimating P( e e is unseen in training data referred to as unknown words u afterwards. We rewrite the probability as where P ( u ) is a constant for all unknown words, and P ( t ) can be estimated from training data. We now describe the way P ( t | u ) is estimated. We split the training data into two folds. We construct a lexicon using the first fold and treat all words that occur in the second fold, but are not stored in the lexicon, as unknown words. A set of probabilities P ( t | u ), for each POS tag, is estimate on the second fold. We then used the second fold to construct the lexicon and estimate P ( t | u ) on the first fold. The final probability is the average of the two estimates. 4.1.4 Decoding. We take two steps to identify the base NP sequence of a given English sentence e . In the first step, an A  X  search algorithm is applied for POS tagging. The top n t are retained, and each is assigned a POS tagging probability P t according to Equation (10). In the second step, for each t , a Viterbi algorithm is applied to search for the best base-NP sequence. Every resulting b is assigned a base NP tagging probability P b according to Equation (11). The final score of a base NP sequence is computed as P  X  t P b model weight (  X  = 2 . 4 in our experiments which is tuned on Penn Treebank, see the next section). 4.1.5 Evaluations. To test the performance of our approach, we used sec-tion 20 of Penn Treebank as test data, sections 1 X 19 as training data to estimate the four model parameters in Equations (10) and (11), and sections 21 X 24 as held-out data to estimate unknown word probabilities in Equation (12) and tune other parameters such as the POS tagging model weight t retained in the first step.

We retained 10-best POS tag sequences for each sentence in the first step because it achieves the best trade-off between efficiency and accuracy in our experiments. We achieve 92.5% in precision and 93.8% in recall. The results [2000]. 4.2 Complex NP Identification
Unlike base NP, there is not a widely accepted definition of complex NP. It is even more debatable in Chinese. In addition, a lot of complex NPs in English cannot be translated into Chinese as a unit. Therefore, with the help of a linguist, we selected 14 frequently-used English NP patterns which can be translated into Chinese as a unit. We define a NP pattern as a sequence of word classes. Each word class can be a terminal label (or word) such as in , of , and and , or a nonterminal label such as base NP tag or POS tags defined in Marcus et al. [1993]. The top three most frequently used NP patterns are shown in
Figure 4. Any sequence of words or base NPs corresponding to one of the pat-terns is identified as a complex NP. 4.3 NP Translation
We notice that many NPs that we identified are not stored in the bilingual dictionary. This section describes how we translate the NPs better than a word-by-word method.

Our NP translation model is motivated by two observations. First of all, we observe that most English NPs are translated into Chinese as NPs. For exam-ple, on a 60K sentence-pair word-aligned English-Chinese bilingual corpus, we found that more than 80% of English NPs can be aligned to their translated Chi-nese NPs. Secondly, as pointed out in Koehn [2003], word selection can almost always be resolved depending solely upon the internal context of the NP. 4.3.1 Principle. This section describes the concept of the translation tem-plate which is fundamental to the NP translation model.

We observed that there are some translation templates between English NP patterns and Chinese NP patterns. For example, a [NN-1 NN-2] English phrase is usually translated into a [NN-1 NN-2] sequence in Chinese, and a [NN-1 of NN-2] phrase is usually translated into a [NN-2 NN-1] sequence in Chinese.
So for an English NP corresponding to such a pattern, even if its translation is not stored in the dictionary, we can still generate its possible translation according to the corresponding translation template. For instance, we can derive the translation of the multiword phrase drug sale as (drug)/ (sale), and the translation of security committee of UN as (UN)/ (security committee).

The concept of translation templates is very similar to alignment templates described in Och and Ney [2004]. Formally, a NP translation template, denoted by z , is a triple ( E , C , A ) which describes the alignment A between an English
NP pattern E and a Chinese NP pattern C . The alignment A is represented as to the j -th Chinese word class in C . Either i or j can be empty, denoted by indicating that an English (or Chinese) word class is connected to no Chinese (or English) word class.

In our experiments, translation templates are extracted from a word-aligned bilingual corpus. We first used the NP identification method described previously to tag POS, base NP, and complex NP for English sentences. Then, for each English NP pattern E , we extracted its translated Chinese NP pat-terns C and the alignment A . An example is shown in Figure 5, where (a) is an English sentence with each word marked by its POS tag, and position and elements within [ ... ] are base NPs or complex NPs; (b) is the aligned Chinese sentence that has been segmented into a sequence of words; (c) shows the word alignment between the English and Chinese sentences; and (d) shows three translation templates extracted, respectively, for two base NPs and the whole phrase. Notice that the word positions in the alignments shown in (d) are those in E and C of each z . Also notice that translation templates can be recursively defined.

As mentioned earlier, the NP translations obtained do not always correspond to document indexes. If they do not, the segmentation process will break them down into several words. Even in this case, we can still benefit from the word selection in the process that partially solves the translation ambiguity problem. 4.3.2 NP Translation Model. We first describe the translation model within the framework of source-channel models, and then generalize it within the framework of linear models for parameter estimation.

Given an English NP e ={ e 1 , ... , e I } , we search among all possible transla-tions for the most probable Chinese translation c  X  ={ c 1
Here, P ( c ) is the Chinese language model probability estimated via a trigram model as
P ( e | c ) is the translation probability. Formally, the NP translation template z is introduced as a hidden variable as
Hence, there are two probabilities to be estimated. The probability P ( z apply a translation template, and the probability P ( e | template, for word selection.

First, we describe the way P ( z | c ) is estimated. Recall that z call z applicable to c if c matches the NP pattern C . Let C ( c , z ) be the number of occurrences of c to which z is applicable and C ( c ) be the number of occurrences of c in the training data. P ( z | c ) is estimated as Second, we describe the way P ( e | z , c ) is estimated. We assume that the
English words are translated independently. We then decompose the probability as Here, P ( e | c ) is a translation probability estimated by relative frequencies: is the frequency of word c in the training data.

Notice that the model of Equation (17) is a deficient model since the con-
However, it is not necessary to normalize it since we use the model as a feature function in translation as will be described later. We also notice that it is possi-ble to define an alignment in A at the level of base NP such as z
As shown in Figure 5(d), we assume that all alignments in A are pairs of word positions. Therefore, when we apply A in NP translation, we recursively map each alignment pair of base NP position to a set of pairs of word positions. For example, the pair (1, 2) in z 3 in Figure 5(d), which is an alignment between the positions of two base NPs, can be mapped into a set of word position pairs using the alignment of z 2 .

Substituting Equation (15) for Equation (13), we have
We see that the NP translation model consists of three component models: (1) the Chinese language model P ( c ) of Equation (14), (2) the translation template selection model P ( z | c ) of Equation (16), and (3) the word selection model P ( e c ) of Equation (17). It should be noted that different component models are trained on different corpora. The dynamic value ranges of different component model probabilities can be so different (e.g., P ( e | z , c ) of Equation (17) is not a probability but a score) that it is inappropriate to combine all these models through simple multiplication as in Equations (13) and (15). One way to balance these score quantities is to introduce for each component model a model weight  X  to adjust the model score P ( . )to P ( . )  X  .In our experiments, these weights are optimized so as to minimize the NP translation errors on training data under the framework of linear models 3 .

It is worth noting that the source-channel models are the rationale frame-work behind the NP translation model. Linear models are just another rep-resentation, based on which we describe the optimization algorithm of model weights.

Now, let us reformulate the NP translation model within the framework of the linear model [Duda et al. 2001]. It includes (1) a set of D feature functions that map the given English NP, the Chinese NP, and the translation templates into a real value, that is, f d ( e , c , z ), for d = 1 ...
 one for each feature,  X  i for i = 1 ... D . Then the decision rule of Equation (19) can be rewritten as
We can see that using the linear models, NP translation is viewed as a reranking problem. In our experiments, we used three feature functions. They are derived from the previous three component models, respectively.  X  Chinese language model feature. It is defined as the logarithm of the trigram model of Equation (14), that is, h LM ( c )  X  Translation template selection model feature. It is defined as the logarithm of P ( z | c ), that is, h TS ( z , c ) = log P ( z | c ).  X  Word selection model feature. It is defined as the logarithm of P (e
Equation (17), that is, h WS ( e , z , c ) = log P ( e | ( E , C , A ), c ) 4.3.3 Training. For the three different component models, we used differ-ent training approaches. The Chinese language model probabilities are trained on a word-segmented Chinese text corpus consisting of approximately 1.6 bil-lion Chinese characters. It contains documents of different domain, style, and time [Gao et al. 2002a]. The trigram probabilities are computed using MLE with Modified Absolute Discounting smoothing described in Gao et al. [2001a].
The translation template selection model is trained on a word-aligned bilin-gual corpus containing approximately 60K English-Chinese sentence pairs.
Translation templates are first extracted automatically from the corpus, and then filtered by a linguist. The probability P ( z | c ) is then estimated accord-ing to Equation (11). For each Chinese NP pattern, there are 4.21 translation templates on average.

The word selection model probabilities are computed according to Equa-tion (18) using the same word-aligned bilingual corpus containing 60K
English-Chinese sentence pairs. To deal with the data sparseness problem, the probabilities P E ( e | c ) estimated via Equation (18) are linearly interpolated with the probabilities P D ( e | c ) derived from a bilingual dictionary by assuming a uniform distribution. That is, if c has n translations in the dictionary, each of them is assigned the same probability P D ( e | c ) = 1/ n .

The model weights  X  , as shown in Equation (20), are estimated using an iterative procedure that is used for multidimensional function optimization [Press et al. 1992]. Assume that we can minimize NP translation errors with respect to one parameter  X  using line search . The procedure works as follows: first direction so that the number of NP translation errors on training data is minimized; then move from there along the second direction to the minimal error rate, and so on. Continue cycling through the whole set of directions as many times as necessary until the error number stops decreasing. In our experiments, we found that the procedure can converge on different minima given different starting points. We thus perform the procedure multiple times, each from a different, random starting point and pick the parameter setting that achieves the minimal errors. Note that this optimization approach is limited to a very small number of model parameters. Efficient algorithms for tuning a larger number of model parameters can be found in Och [2003] and Gao et al. [2005b]. 4.3.4 Decoding. Given an English NP e , we take the following steps to search for the best Chinese translation.
Notice that Equation (20) does not take into account the sum on z in Equa-tion (19) because considering the sum in decoding directly is computationally expensive. We therefore approximate the sum during decoding in two steps.
First, for each z , we find the best translation, as shown in Steps 2 and 3 in the algorithm. Second, we select the translation among all retained best transla-tions according to Equation (20) as described in Step 4 of the algorithm. 5. DEPENDENCY TRANSLATION MODEL
The NP translation model is limited to NPs contiguous in both source and tar-get languages. The dependency translation model which we will describe aims at eliminating the limitation to some degree by taking into account syntactic dependencies in translation selection. We can, therefore, translate more pre-cisely discontinuous phrases including dependency triples, such as verb-object and adjective-noun, regardless of the number of intervening words.
Similar to that of the NP translation model, the dependency translation model is also built on the basis of two hypotheses. First, dependencies have the best cohesion properties across languages [Fox 2002], that is, dependency representation usually remains in the translations, and an ideal query trans-lation should contain the same syntactic dependences as in the original query.
Second, word selection can mostly be resolved via the internal context of the de-pendency. Thus, syntactic dependencies also provide an additional criterion to the earlier cohesion measure used in the co-occurrence model described in Sec-tion 3.4; a good translation word should not only co-occur with other translation words, but also have the required syntactic dependency relations with them. 5.1 Principle resents a syntactic dependency relation r between two words w as verb-object and subject-verb. Figure 6 shows an English sentence and the dependency triples that are extracted from it. For example, dog is the subject of the verb barked. It is our observation that there is a strong correspondence in dependency relations in the translation between English and Chinese despite the great differences between the two languages. For example, a subject-verb re-lation in English, for example, dog, subject-verb, barking), is usually translated into the same subject-verb relation in Chinese, for instance, , subject-verb, . This suggests that similar to NP translation, a translation template exists between English dependency triples and Chinese ones.
 Unlike NP translation templates, there is only one translation template: an
Chinese dependency triple c t = ( c 1 , r c , c 2 ), where c translations of the English terms e 1 and e 2 , respectively, and r counterpart of r e .

Among all the dependency relations, we only consider the following four types that can be precisely detected using our parser and cannot be handled by the NP translation model 4 : (1) subject-verb, (2) verb-object, (3) adjective-noun, and (4) adverb-verb. That is, r  X  X  subject-verb, verb-object, adjective-noun, adverb-verb } .

To prove the validity of the previous translation template, we perform the following test. We used the aforementioned word-aligned bilingual corpus con-taining 60K English-Chinese sentence pairs. The corpus was first parsed using an English and Chinese parser NLPWIN, a broad-coverage rule-based parser developed at Microsoft Research, which is able to produce syntactic analysis at varying levels of depth [Heidorn 2000]. For the purposes of our experiments, we used a dependency tree output with unstemmed surface words. The four types of dependency triples were then extracted from the trees. We analyzed the correspondence on dependency relations between Chinese and English. The results are shown in Table I. As we can see, more than 80% of dependency re-lations of subject-verb, adjective-noun, and adverb-verb have one-to-one map-pings between English and Chinese, while the mapping rate of verb-object is approximately 65%.

Further analyses showed that the mapping errors of verb-object occur in the following situations: (1) a single English verb maps a Chinese dependency triple (e.g., read  X  [read] verb | [book] object) or (2) an English verb-prep-object sequence maps a Chinese verb-object sequence (e.g., change-to-currency  X  [use] verb | [currency] object). As the first case is not a dependency translation and will not affect the dependency model, it is ignored. The second problem is quite common. In fact, the combination of verb-preposition in English is very often translated into a single verb in Chinese. If we consider such a combination in English as verb, the mapping rate of verb-object is increased to more than 80% (see Table I, (*) case). This is the method we will use to map verb-object dependencies between Chinese and English. 5.2 Dependency Translation Model
Given an English dependency triple e t = ( e 1 , r e , e 2 of Chinese dependency triple translation, the best Chinese dependency triple c = ( c
Here, P ( c t ) is the a priori probability of words of the translated Chinese depen-dency triple. It can be estimated using MLE as where C ( c t ) is the number of occurrences of c t in the collection, and N is the number of all dependency triples.
 be translated with each other only if they have the same type of dependency independently. We therefore decompose the probability P ( e where  X  ( r e , r c ) = 1if r e = r c and 0 otherwise.
 aligned bilingual corpus using Equation (18). However, we observe that, within a dependency triple ( w 1 , r , w 2 ), the translation selection of a word (e.g., w largely depends on the other word w 2 and the relation r . For example, the word bear in a dependency triple (bear, verb-object, child) is translated to , while it is most likely to be translated to as an individual word (if the translation probability is trained directly on a word-aligned corpus or the translation is obtained via dictionary look-up). This suggests that translation probabilities in Equation (23) are better trained on a set of aligned bilingual dependency triple pairs. Unfortunately, it is difficult to obtain such a corpus in large quan-tity. Therefore, in our model, instead of using a translation probability, we as-sume that the likelihood that c will be translated to e can be measured by their semantic similarity, denoted by sim ( e , c ) (see section 5.2.1 for its calculation).
Notice that e and c are not necessarily to be a translation pair stored in a dictionary but just a pair of cross-lingual synonyms derived via their semantic similarity, for instance, is not a translation of bear defined in a dictionary, but a synonym. Since our goal is to obtain good IR results, such cross-lingual syn-onyms may solve the term mismatch problem and boost the CLIR performance, playing a similar role as synonym-based query expansion in monolingual IR as described in Section 5.5.

Now, we see from Equations (21) and (23) that the likelihood that e lated to c t , assuming that r e = r c , can be scored via two factors: (1) P ( c
Equation (22) and (2) sim ( e , c ). Similar to the NP translation model described in Section 4.3, we define a feature function for each type of factors and combine them under the framework of linear models as shown in Equation (20). In the dependency translation model, we used two type of features.  X  Chinese language model feature. It is defined as the logarithm of the model of Equation (21), that is, h LM ( c t ) = log P ( c t ).  X  Cross-lingual word similarity feature. It is defined as the similarity between relations each with 2 words, there are in total 8 types of word pair. We define 8 feature functions, each for one type of word pair, such as the similarity between a verb pair in a verb-object dependency.

Similar to the NP translation model, the linear model parameters are used to combine the previous two features, are estimated using the iterative procedure for multidimensional function optimization (Section 4.3.3). 5.2.1 Cross-lingual Word Similarity. This section describes the way sim ( e , c ) is computed. Lin [1997, 1998] presents a method of computing the similarity between two words in the same language on the basis of dependency context .
Zhou et al. [2001] extended the word similarity measurement of Lin X  X  to the cross-lingual case. The description that follows is adapted from Lin [1998] and Zhou et al. [2001].

Let us first discuss the way to compute sim ( w 1 , w 2 ) for w same language. The basic idea is to consider all dependencies including a word w as its dependency context, denoted by T ( w ). For example, in Figure 5, the dependency context of the word dog consists of three dependency triples, that is,
T(dog) ={ (*, subject-verb, barked), (the, det-noun, *), (brown, adjective-noun, *) } , where the wildcard symbol * denotes any word. It is assumed that two words are likely to have similar semantic meanings if their dependency contexts are identical. For simplicity, in what follows, we use ( r , w ) to denote either ( or ( w , r ,  X  ).

Using an information-theoretic definition, sim ( w 1 , w 2 ratio between the amount of information needed to describe the commonality of w 1 and w 2 , denoted by I ( common ( w 1 , w 2 )), and the information needed to fully describe what w 1 and w 2 are, denoted by I ( describe ( w
We assume that a dependency triple ( w 1 , r , w 2 ) is generated via three steps:  X  A : a randomly selected word is w 1 ;  X  B : a randomly selected dependency type is r ;  X  C : a randomly selected word is w 2 .

We assume that A and C are conditionally independent given B . Let I ( w w ) be the amount of information needed to describe a dependency triple ( w r , w 2 ). Its value can be computed as
All the probabilities in the right-hand side of Equation (25) can be computed using MLE as shown in Equations (26) to (29), where C ( x ) is the occurrence of x in training data, and the wildcard symbol  X  denotes any word or dependency type.
 Then, substituting Equations (26) to (29) for (25), we have
Next, we assume that information can be additive, then I ( common ( w is calculated as the sum of the information contained in common dependency triples belonging to both dependency context sets T ( w 1
Similarly, I ( describe ( w 1 , w 2 )) is calculated as the sum of the information con-tained in dependency triples belonging to either dependency context sets T ( w or T ( w 2 ):
Now, let us discuss how the similarity measurement of Equation (24) is ex-tended to measure the cross-lingual word similarity sim ( e , c ). For an English dependency context ( r e , e ), a Chinese dependency context ( r r = r case, we say that ( r c , c ) is a possible translation of ( r
The probability of such a translation may be estimated as P ( c where  X  ( r c , r e ) = 1if r c = r e ; and 0 otherwise. P(c which can be estimated from a bilingual corpus as Equation (18).

So, the cross-lingual commonality I ( common ( e , c )) is adapted from Equation (31) as Similarly, the descriptions of e and c are adapted from Equation (32) as 5.3 Training
There are two types of parameters in the dependency translation model, sim ( e , c ) and P ( c t ). One advantage of our model is that sim ( e , c ) can be trained on unrelated English and Chinese corpora. In our experiments, we used an En-glish text containing articles published in the Wall Street Journal from 1987 to 1992, which amount to 750MB, and a Chinese text corpus containing articles published in the People X  X  Daily from 1980 to 1998, which amount to 1200MB.
We estimated sim ( e , c ) in the following steps. First, NLPWIN is used to ex-tract triples in both corpora. Table II shows the number of different types of dependency triples extracted. Then, for each e or c , we construct its dependency context. Finally, at runtime, we computed sim ( e , c ) using the method described in Section 5.2.1. Notice that there is a risk that unrelated dependency triples in
Chinese and English can be connected since e and c might not be a translation pair but synonyms. However, as we will show in Section 5.4, the gain outweighs the loss significantly in our translation evaluation experiments.

P ( c t ) is trained via MLE (as shown in Equation (18)) on the Chinese de-pendency triple corpus extracted from the Chinese text corpus by NLPWIN as described. In addition, we used Good-Turning smoothing as described in Section 3.3 to deal with the sparse data problem. 5.4 Decoding
Given an English sentence, we translate all its dependency triples to Chinese using the dependency translation model in the following.
  X  Dependency triple detection. We use NLPWIN to detect all dependency triples of the four types in a given English sentence.  X  Candidate generation. For each of the dependency triple e generate all Chinese translation candidates c t = ( c 1 , r sim ( e 1 , c 1 ) &gt; X  1 , and sim ( e 2 , c 2 ) &gt; X  2 determined empirically.  X  Candidate ranking. All translation candidates are ranked using the linear models of Equation (20), where two types of feature functions are used. As described in Section 5.2, they are (1) h LM ( c t ), and (2) a set of h 5.5 Evaluating Dependency Translation
This section discusses evaluation results of dependency translation. We first present verb-object dependency triple translation results. Then, we show that our method of computing sim ( e , c ), instead of word-to-word translation proba-bility, takes into account semantic similarities between words and can trans-late dependency triples which contain words whose translations can be neither found in a dictionary nor learned on word-aligned bilingual corpora. 5.5.1 Verb-Object Dependency Triple Translation Results. Verb-object de-pendency triples represent the most serious translation ambiguities among the four dependency types. Therefore, we focus on them in our translation evalu-ations. We performed the evaluation on the Chinese and English verb-object dependency triple sets shown in Table II. We used 80% of the data as training data. For the remaining 20% of dependency triples, we constructed the follow-generated manually.  X  T 1. The set contains 1,000 dependency triples with high frequent verbs.  X  T 2. The set contains 275 dependency triples with low frequent verbs.  X  T 3. The set contains top 1,000 high-frequency dependency triples.  X  T 4. The set contains 700 low-frequency dependency triples.

The translation performance is measured for accuracy, defined as
The following three translation methods are compared.  X  Method A . Each English word in a dependency triple is translated to the most frequent translation word among those stored in the dictionary.  X  Method B . For each English word in a dependency triple, translation selection is achieved by the decaying co-occurrence model described in Section 3.  X  Method C . The dependency translation model is used.

The results are shown in Table III. We can see that Method C, which uses the dependency translation model, achieves the most precise translations in all four test sets. The results also confirm that the risk of relating unrelated triples by using nonparallel corpora for training is small. We also notice that the improvements achieved by the dependency translation model accuracy are consistent among different test sets, demonstrating that the model is robust against the frequency of dependency triples and verbs. 5.5.2 Impact of Using Cross-Lingual Semantic Similarity. This section discusses the impact of introducing the cross-lingual semantic word similar-ity, that is, sim ( e , c ) described in Section 5.2.1 on dependency translation.
We can view the dependency translation model as a method of expanding the candidate translation set of an English word e by adding all semantically similar Chinese words according to sim ( e , c ) which are not stored in a bilin-gual dictionary. In comparison, we also used an English synonym dictionary, which is constructed from the LDC bilingual dictionaries as follows. For each
English term e and each of its Chinese translation c , we consider all the English translations e of c as synonyms of e .

We compared three methods of translating an English dependency triple e . For all three methods, we used the translation procedure as described in
Section 5.4. The differences among the three methods are (1) the way the trans-lation candidates are generated or expanded and (2) how sim ( e , c ) is computed.  X  Method A . For each English word e in a dependency triple, we consider all its possible translations stored in the LDC bilingual dictionary. The resulting candidate set is denoted by TD ( e ). We assume that every c same value, that is, sim ( e , c ) = 1/ | TD ( e ) | for all c  X  Method B . For each English word e in a dependency triple, we consider all its possible translations in TD ( e ) and the candidate set TS ( e ) that contains all possible translations of all the synonyms of e according to the synonym dictionary mentioned previously. For every c that only belongs to TD ( e ), we have sim ( e , c ) =  X  / | TD ( e ) |+  X  / | TS ( e ) | .  X  Method C . We use the dependency translation model to generate and rank translation candidates as described in Section 5.4.
 We performed the comparison experiments on T1. The results are shown in
Table IV. We see that the use of a synonym dictionary does not bring any ben-efit because it introduces a lot of noise along with some correct expansion of the set of translation candidates. The dependency translation model achieves the best results. The use of cross-lingual semantically-similar words in depen-dency translation eliminates to some degree the limited coverage of bilingual dictionaries. As an example, for the word bear in a dependency triple (bear, verb-object, child), the correct translation is not stored in the dictionary. The use of the dependency translation model (i.e., Method C) leads to a correct trans-lation. We found that the dependency triple ( , verb-object, ) and (bear, verb-object, child) are very frequent in the corpora, so both I ( , verb-object, between and bear is high, and as a result, sim ( | bear) is also very high.
Unfortunately, there are also some negative examples in using the depen-dency translation model as shown in Figure 7. We found that the wrong transla-tions usually occur when the combinations of incorrectly expanded translation candidates are frequent ones in the corpus, thus they cannot be filtered out using the translation-ranking model. The Chinese dependency triples in the two negative examples of Figure 7 are very frequent ones. In these cases, even if the component words separately are strongly related to the original English words, their combination corresponds to a meaning different from the original one. Our model is unable to deal with this problem, and leave it to future work. 6. CLIR EXPERIMENTS 6.1 Settings
We evaluate the three proposed query translation models on CLIR experiments on TREC Chinese collections. The TREC-9 collection contains articles published in the Hong Kong Commercial Daily, Hong Kong Daily News, and Takungpao.
They amount to 260MB. A set of 25 English queries (with translated Chinese queries) has been set up and evaluated by staff members at NIST (National
Institute of Standards and Technology). The TREC-5 and 6 corpus contains articles published in the People X  X  Daily from 1991 to 1993 and a part of the news released by the Xinhua News Agency in 1994 and 1995. A set of 54 English queries (with translated Chinese queries) has been set up and evaluated by the staff at NIST.

All Chinese texts, articles, and translated queries are word-segmented using the Chinese word segmentation system MSRSeg [Gao et al. 2005a]. The system also identifies named entities of various types. Then, stop words are removed.
Each of the TREC queries has three fields, namely, title, description, and nar-ratives. In our experiments, we used two versions of queries, short queries that contain titles only and long queries that contain all three fields.
The bilingual dictionary we used is a combination of three human-compiled bilingual lexicons, including the LDC English-Chinese dictionary and a bilin-gual lexicon generated from a parallel bilingual corpus automatically. The com-bined dictionary contains 401,477 English entries, including 109,841 words, and 291,636 phrases. The use of the combined dictionary is motivated by previous studies (e.g., Gao et al. [2000] and Xu and Weieschedel [2000]), which showed that larger lexicon resource improves CLIR performance significantly.
The Okapi system with BM2500 weighting [Robertson and Walker 2000] is used as the basic retrieval system. The main evaluation metric is interpolated 11-point average precision. Statistical t-test and query-by-query analysis are also employed. To decide whether the improvement by method X over method
Y is significant, the t-test calculates a p -value based on the performance data of X and Y . The smaller the p-value, the more significant the improvement. In our experiments reported in the following, we conclude that the improvement is statistically significant if p -value is less than 0.05. 6.2 Main Results The main results are shown in Tables V to VII (i.e., average precisions) and
Figure 8 (i.e., precision-recall curves). To investigate the effectiveness of our models for query translation, the following three baseline methods are compared.
 ML (Monolingual) . We retrieve documents using the manually-translated
Chinese queries provided in the TREC collections. Its performance has been considered as an upper-bound of CLIR because the translation process always introduces translation errors. However, recent studies show that CLIR results can be better than monolingual retrieval results which is also observed in our experiments.

ST ( Simple Translation ). We retrieve documents using query translation ob-tained from the bilingual dictionary. Phrase entries in the dictionary are first used for phrase matching and translation, and then the remaining words are translated by their translations stored in the dictionary. For each phrase/word with multiple translations stored in the dictionary, we only take the first trans-lation which is supposed to be the most frequently used translation. We could take more translations for each phrase/words, but our pilot experiments show that it hurts the performance in most cases.

BST ( Best-Sense Translation ). We retrieve documents using translation words selected manually from the dictionary, one translation per-word, by a native Chinese speaker. If none of the translations stored in the dictionary is correct, the first one is chosen. This method reflects the upper bound perfor-mance using the dictionary.
 COTM (co-occurrence translation model), NPTM (NP translation model) and
DPTM (dependency translation model) are the three query translation models described in Sections 3 to 5, respectively. Notice that since NLPWIN, which is used to detect dependency triples, is a rule-based parser and performs well only when the input word sequence is a grammatical sentence, we only tested
DPTM on long queries. The NP detector as described in Sections 4.1 and 4.2 is statistical in nature, and can handle arbitrary word sequences, so we tested
NPTM using both long and short queries. 6.3 Discussion
The experimental results shown in Tables V to VII and Figure 8 give rise to the following observations. 6.3.1 Impact of COTM. We see that COTM brings statistically significant improvements over ST for long queries as shown in Table VI (Row 4) and
Table VII (Row 4). However, its improvement over ST for short queries is marginal. This is expected because COTM resolves translation ambiguities with resort to context terms. Long queries contain much richer contextual in-formation than short queries. 6.3.2 Impact of NPTM. We observe that unlike COTM, NPTM achieves substantial improvements over ST for both long and short queries as shown in
Tables V to VII. We notice that NPTM even outperforms BST for short queries as shown in Rows 3 and 5 in Table V. It is thus interesting to compare the phrase translation results using NPTM with that using dictionary look-up (Rows 2 and 3 in Table V). A further analysis shows that by using NP identification and translation, we obtained better translations. For example, in TREC-9 short query retrieval, only 11 multiword phrases out of 25 queries are stored in the dictionary and translated as a phrase, while using NPTM, 26
NPs are identified and translated. It thus leads to a significant improvement over BST. However, NPTM, described in Section 4, also has some limitations as indicated in the following. We divide NP into two types, compositional NP and noncompositional NP.

A compositional NP refers to a phrase whose translation can be assembled by translations of words within the phrase, such as computer hacker ( ), public key ( ), and environmental protection laws ( ). Gen-erally speaking, NPTM is good for compositional NP translation. However, it failed to correctly translate some domain-specific NPs. For example, stealth technology ( ) and stealth countermeasure ( ) in #59, and syn-thetic aperture radar ( ) in #66 correspond to special terminology in Chinese, and they are not translated correctly by NPTM.

A noncompositional NP is a phrase whose translation cannot be assembled by translations of its component words. Our method NPTM is unable to deal with the translation of noncompositional NPs. Examples include three-links ( ) in #65, vehicle fatalities ( ) in #68, most-favored nation ( ), and World Conference on Women ( ). A large portion of noncompositional NPs in queries are political abbreviations. If these NPs are not stored in the dictio-nary, they are most likely to be translated incorrectly. This indicates that the coverage of the dictionary is still an important problem to be solved to improve the performance of CLIR. 6.3.3 Impact of DPTM. We find that the use of DPTM leads to an effec-tiveness well below that with the other two models, COTM and NPTM. For example, as shown in Table VI (Rows 2 and 6), the improvement of DPTM over ST is not statistically significant. This is, however, to be expected because dependency triples have a much lower coverage than the other models. Con-sider TREC-9 long-query retrieval, only a few triples from 11 queries out of 25 have been translated by DPTM, while all the other words are translated by the first translation word in the dictionary. So this counter performance is not surprising. Figure 9 shows a closer view on the 11 queries. From the 11 queries, NLPWIN extracted 52 dependency triples which appear at least 5 times in the corpus. The minimal occurrences of 5 is set due to the fact that many low-frequency dependency triples are, in fact, noise. The 52 triples in-clude 12 verb-object dependency triples, 8 sub-verb triples, 32 adjective-noun triples, and no adv-verb triple. As shown in Figure 9, for these queries, the dependency triple translation has positive impact on the methods of ST and
COTM for almost all the 11 queries except for #61. In the case of query #61, only one translation word differs: (coal) consumption is translated to by
COTM, and to by DPTM. Both translations are correct, but the first trans-lation word is often used for industrial consumption (which is the case for this query), whereas the second is often used for consumption of particular con-sumers. For all the 11 queries globally, the triple method makes a statistically significant improvement of 56% over ST, and 10% over COTM.

A further analysis shows that DPTM is able to assemble translation words correctly in a dependency triple because the triples can capture the syntactic dependency between not only sequential words (i.e., phrases) but also nonse-quential words in a sentence as the (computer-virus, subject-verb, originate) triple in query #64 shown in Figure 10. All the translations in Figure 10 are correct.
 We can compare some of examples of Figure 10 with those translated by
COTM. For Queries #63 and #64, COTM gives some different but correct trans-lations for the following words: develop is translated to , and originate to (v.s. to and by DPTM). However, the word hacker in Query #65 is incorrectly translated to (joker), and charge to (protect) by COTM. For these cases, DPTM generates the correct translations. These examples show that the translations with dependency triples can successfully correct some of the incorrect selections of translation words. 6.3.4 Impact of Combining Translation Models. Previous work (e.g., Xu and Weischedel [2000]) showed that, if multiple translations of a query term were accepted in query translation, it is possible to obtain better performance of cross-language retrieval because of the query expansion effect. Therefore, we combine models using the parallel combining approach as described in Section 3.5, that is, whenever we combine these query translation models, we combine their translation results directly. That is, we combine linearly the three (or two) sets of translation queries obtained by the three (or any two of the three) translation models.

As expected, the combined models always perform better than each compo-nent model to be combined. Interestingly, for some queries, their CLIR results are even better than their monolingual retrieval results. Some examples we observed in TREC-9 long-query experiments are public key in query #68 is translated to as well as , Olympics in query #71 to (Olympic) and (Olympic games), and panda bear in query #76 to and . All these terms are commonly used translations. Thus, as a result, by combining all three query translation models, we get the best CLIR results which are very close to, or even better than, the monolingual retrieval results as shown in Tables V to VII.
 7. RELATED WORK AND DISCUSSION
Bilingual dictionaries have been used in several CLIR experiments. However, previous work showed that English-Chinese CLIR using simple dictionary translation yields a performance lower than 60% of the monolingual perfor-mance as described in Kowk [2000]. The main problems observed are that (1) the dictionary may have a poor coverage, and (2) it is difficult to select the cor-rect translation of a word from all the translations provided by the dictionary.
For the first problem, much effort has been spent on collecting larger lex-ical resources either manually or automatically (e.g., Kowk [2000]; Nie et al. [1999]; Xu and Weischedel [2000]; Zhang and Vines [2004]). The coverage of the dictionary can be increased to some extent.

The second problem is also called the translation selection or disambiguation problem. This is the focus of this article. For each of the three query transla-tion models described, there has been a large amount of related work. In what follows, we present a brief review. The study reported in this article is an exten-sion of our previous research reported in Gao et al. [2000, 2001b, 2002b] which will not be reviewed in this section. 7.1 Remarks on the Co-Occurrence Model Co-occurrence information has been utilized by several recent studies (e.g.,
Ballesteros and Croft [1998]; Bian and Chen [1998]; Fung et al. [1999]; Jang et al. [1999]; Peters and Picchi [1996]; Mandala et al. [1999]; Liu et al. [2005]) to deal with the selection of the correct translation terms from a bilingual dictio-nary. A term similarity is determined by the mutual information (or its variants) between terms. Then the most similar translation term among those in the dic-tionary is selected. Our co-occurrence model is an extension of the previous methods in that we incorporate a decaying factor that decreases the mutual information when the distance between the terms increases. A similar idea has been applied successfully to statistical language modeling [Clarkson and Robinson 1997], showing improved performance of the cache language model. Our experiments show similar improvements on CLIR.

One potential problem of most proposed co-occurrence models is the use of the approximate word selection algorithm. As described in Section 3.4, each query term translation is actually determined independently. To remedy the problem, Liu et al. [2005] presented a so-called maximum coherent model that is able to estimate translations of multiple query terms simultaneously. In this article, we remedy the problem simply by combining it with two other trans-lation models. The basic idea is that the translations of a set of query terms need to be joined only when they are tightly correlated such as query terms within a NP or a dependency. In this sense, our query translation methods are both stochastically and linguistically motivated, stochastically because we use statistics from corpus, linguistically because the structures (NPs and depen-dencies) we defined are informed by syntactic analysis. 7.2 Remarks on the NP Translation Model
A technique often used to deal with the problem of translation ambiguity is to identify phrases in the query and translate them as a whole using a phrase dictionary. It has been shown that this technique can improve IR performance.
Hull and Grefenstette [1996] showed that the performance achieved by manu-ally translating phrases in queries is significantly better than that of a word-by-word translation using a dictionary. Davis and Ogden [1997] showed that by using a phrase dictionary extracted from parallel sentences in French and
English, the performance of CLIR is improved. Ballesteros and Croft [1998] performed phrase translation using information on phrase and word usage contained in the Collins machine-readable dictionary. They demonstrated that translations of multiword concepts as phrases are more precise. However, a critical problem remains: if a phrase is not stored in a lexicon, how can one identify it in a query and translate it correctly? It is unrealistic to expect a com-plete phrase dictionary. New phrases are constantly created. Therefore, we will always face the problem of identification and translation of unknown phrases no matter how complete a phrase dictionary may be. This problem is one of the foci of this article as described in Sections 4.1 and 4.2.

Now, let us compare in more detail our NP translation model to the clas-sic methods proposed in Ballesteros and Croft [1997] and Brown et al. [1993].
Ballesteros and Croft used a word-by-word strategy for phrase translation. It is based on two assumptions that are suboptimal. First, they assume that there is a one-one mapping between words in English NP and words in Chinese NP.
However, in our experiments, we found that only 56% NP translation patterns have such one-one mappings. Second, they assume that the translation words in a phrase will remain in the same order as in the source language phrase.
In our experiments, we found that 35% of translation patterns change word order. On the other hand, the IBM statistical models incorporate very little linguistic knowledge [Brown et al. 1993]. It is hard to capture non-local depen-dencies of the language with local models such as n-gram models. So even if the translation model generates the correct set of words, the language model will not assemble them correctly. In our method, we incorporate the language model with translation patterns. While the language model captures the local dependency, the translation patterns provide information on global dependency within a phrase. Although the method is not powerful enough for sentence-level translation, it performs well for NP translation.

Recently, phrase-based statistical machine translation [Och and Ney 2004; Chiang 2005] advanced the state-of-the-art. As mentioned earlier, our
NP translation model is very similar to the template-based translation model described in Och and Ney [2004]. The use of a hierarchical structure in our
NP translation templates (e.g., z 3 in Figure 5) can be viewed as a special case of the hierarchical phrase-based model in Chiang [2005]. There are, however, two major differences between our work and that of Och and Ney [2004] and
Chiang [2005]. First, the NPs that we deal with are syntactically well-defined constitutes. Och and Ney [2004] and Chiang [2005] extract phrases from bilin-gual corpus. These phrases are just a sequence of consecutive words and could be completely meaningless syntactically. Second, our translation templates use
POS tag as the word class, while in Och and Ney [2004], the templates use word classes that are automatically learned from a bilingual corpus. In a word, our model is more syntactically-motivated, and it is potentially more accurate and efficient. Moreover, in our study, we view NP translation as a subtask of machine translation. We believe that focusing on a narrower problem allows more dedicated modeling. Koehn [2003] presents a fairly comprehensive piece of work along this line. The rich feature set used for NP translation presented in Koehn [2003] might also improve the accuracy of our method. 7.3 Remarks on the Dependency Translation Model
The dependency translation model aims at incorporating syntax information to resolve translation ambiguities. The same goal has also motivated the research of syntax-based MT which is closely related to our work.

Similar to our method, Ding and Palmer [2005] also use parsers to identify linguistic structures of both Chinese and English languages. Then, they identify those substructures from both languages that can be mapped. The identified mappings form the so-called transduction grammar .

Due to the structural difference between source and target language, some people use a parser in one language and map the extracted linguistic structure to the other language [Yamada and Knight 2001; Quirk et al. 2005], assuming that there exist a large set of word-aligned bilingual sentence pairs.
There are also some methods that can learn a transduction grammar with-out parsing monolingual sentences. For example, Wu [1997] views translation as a process of bilingual parsing via a synchronized grammar. Each rule of the grammar is a transduction that generates two output strings simultaneously, one for each language. However, for efficiency, the rules have to be unrealisti-cally simple such as using a single nonterminal symbol in a rule. This limitation is resolved in Chiang [2005] where the grammar is learned on a word-aligned bilingual corpus using a more sophisticated probabilistic model.

All the work mentioned requires a large amount of word-aligned bilingual corpus which is not always available. Our model can be learned from unrelated bilingual corpus. This benefit results from the fact that we define dependency translation as a subtask of MT, like the case of the NP translation model. We also argue that, while most existing methods rely on constituency analysis, we believe that dependency analysis brings semantically-related words together and is more effective for resolving translation ambiguities as we showed in
Section 5.5. 8. CONCLUSION
This article presents three statistical query translation models for dealing with the problem of query translation ambiguity. The models differ in their use of linguistic information. The co-occurrence model does not take into account any linguistic structure explicitly and simply views a query as a bag of words. The other two models, the NP translation model and the dependency translation model, exploit linguistic dependency constraints between terms in NPs or in higher-level dependencies. Our evaluations of translation accuracy show that linguistic structures always lead to more precise translations. Our experiments of CLIR on TREC Chinese collections show that all three models have a posi-tive impact on query translation and lead to significant improvements of CLIR performance over the simple dictionary-based translation method. The best re-sults are obtained by combining the three models. This is consistent with the observations on general reasoning, that is, when more information is available and is used in reasoning, we usually obtain better results. The integration of different types of knowledge in query translation is the most apparent in the second and third models where different information is combined as feature functions. This combination method is very effective, and it is also a flexible one for integrating more types of information or knowledge when it is available.
There are many areas for future research. One area is to improve the robust-ness of the parsers that we used to detect phrases and dependency triples. A large portion of translation errors in our experiments are due to the incorrect detection of those linguistic structures. We notice that we only need to extract some specific grammatical relations of a query for translation, and it is un-necessary to get the full analysis result such as the full parse tree of a given sentence. Therefore, a partial parser or a grammatical relation detector may be more suitable. This alternative will be investigated in the future. Since all the three models are statistical in nature, the lack of a large amount of proper training data would be a disaster. For example, the lack of large enough aligned bilingual corpus would prevent us from extracting more reliable translation templates. The lack of domain-specific datasets leads to the failure of trans-lating domain-specific terms. Recently, researchers have tried to automatically collect bilingual corpora from the Web [Nie et al. 1999; Resnik and Smith 2003;
Zhang et al. 2005]. Since the Web provides a potentially unlimited data source, it turns out to be a very promising research area.

Thanks to Hongzhao He, Weijun Chen, Jian Zhang and Yi Su for their help with our experiments; Endong Xun and Cong Li for the implementation of the NP identification system, and to Chang-Ning Huang for useful discussions.
