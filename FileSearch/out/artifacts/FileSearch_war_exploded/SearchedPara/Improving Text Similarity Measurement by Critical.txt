 retrieval [2,3,4,5], text classification [6,7], document summarization [8], question and answering [9,10,11], etc. The predominating method for this purpose is Vector Space Model (VSM) [1], which converts a text document into a weighted vector and deter-mines the similarity between two documents by the distances of the vectors. Accord-could achieve reasonable accuracy. However, VSM or keywords alone cannot capture the semantic information in the text leading to many redundant results [3]. embedded in selected sentences. In this model, critical sentence vectors are extracted complete event description and the semantic associated information. Furthermore, it is ment. Experiments show that CSVM outperforms VSM in calculation of text similar-ity. of CSVM. Experiments and result analysis are presented in Sect. 3. We conclude the paper in the last section. 2.1 Overview Basically, a document consists of many sentences, in which only a few are important In CSVM, similarity between two documents is calculated by summing all similarities between each pair of critical sentences within each document, shown as formula (1). tence within document Y . ) ( uring the significance of the critical sentences based similarity calculation: critical sentence vector extraction and their weights cal-culation; similarity calculation between two critical sentences. 2.2 Critical Sentence Vector Extraction We use certain classes of content words to calculate sentence weight. Classes of con-tent words being used include title words, proper nouns (e.g. person name and place name), words with high occurrence frequency, and normal words. Sentence weight ( W ) can be calculated as formula (2). where cance of the position of the sentence to the sentence weight, ranging between 0 and 1. quadric function to determine the value of ) ( i f which is shown in formula (3). 2.3 Critical Sentence Similarity Calculation 2.3.1 Method 1: Longest Common Word String The longest common word string is extracted from two comparing sentences. Features of the common string, such as length, can be used to calculate the similarity between the two sentences. If word order is not considered, a common word string then degen-524 W. Li et al. which c denotes a word, and n and m are the numbers of words of the two sentences, respectively. The following formula takes the length of a common word array as the similarity measure. Note that we also assign different weights ) ( u different word classes. 2.3.2 Method 2: Using Structural Information Basically, a normal sentence comprises of a subject (S), a main predicate (V) and an object (O). We propose a fine-grained similarity measurement by calculating the SVO similarities between the two sentences, see formula (5): [13,14] is required. In our implementation, we simplify the process of partial parsing with a manual rule set. 2.3.3 Method 3: Combined Method Longest common word string and structural information of SVO are typical sentence features. We can generalize formula (5) to cope with other features such as number of nouns within sentence. These features can be combined to obtain the overall sentence similarity. The generalized formula is shown in formula (6). of the features. In our experiments, we employ five sentence features, namely, SVO, the number of common nouns, verbs, adjectives and proper names. 3.1 Setup We use accuracy and recall as the evaluation criteria, shown as formula (7). where Nr denotes the number of similar news retrieved, Nrc denotes the number of news in the collection. ticles. Different methods were deployed to retrieve news similar to the ten documents similarity calculation methods in CSVM and selected the best one for the comparison between the VSM-based and CSVM-based approaches. In our experiments, for sim-plicity, two articles are considered similar if their similarity is higher than 5%. 3.2 Significance of S.V.O. We performed experiments with different weights assigned to SVO. Fig. 1 shows that recall while the weight vector of (1/2, 0, 1/2) yields the worst result. Meanwhile, we find that the accuracy decreases as the weight of predicate increases from 1/5 to 1/2. This indicates that predicate should be less significant than the other two. We perform an experiment in which the predicate is completely ignored, i.e. with weight vector of with weight vector of (2/5, 1/5, 2/5), we choose to use (2/5, 1/5, 2/5) to perform the remaining experiments. 3.3 Different Combined Methods features or increasing their weights have more impact on accuracy than recall. Weight assignment (0.6, 0.1, 0.1, 0.1, 0.1) achieves the best accuracy, which will be used for later experiments. signment was (0.8, 0.1, 0.1). 526 W. Li et al. 3.4 Different Methods for Sentence Similarity Calculation uses five features: SVO, number of common nouns, verbs, adjectives and proper accuracy but also the worst recall. This is not surprising for high accuracy often mis-takenly neglects correct results leading to decreased recall. And this is also the reason why VSM-based method could achieve higher recall than CSVM at some points. Our CSVM-based sentence similarity. under CSVM framework. Methods 1, 2 and 3 used the longest common word string, SVO and combined method, respectively. 3.5 VSM Versus CSVM-Based Method method, we used SVO to determine the similarity between sentences. It shows that the CSVM-based method achieves higher accuracy than its VSM-based counterpart. The average accuracy of CSVM-based method is 90%. There is a significant improvement over VSM whose average accuracy is less than 80%. CSVM-based method also achieves reasonable recall. However, both methods produce low accuracy at some points, e.g. the fifth point. This is caused by high retrieval redundancy. At this point, caused significant loss in accuracy. This problem also renders a low recall, showed at point 4 for VSM-based method in the recall graph. Combining with NLP techniques, the proposed CSVM provides a general framework method outperforms the traditional VSM-based method. Science Foundation of China (NSFC No. 69975008), 863 project of China (No. 2001AA114210) and Hong Kong RGC CERG (Grant No. PolyU5181/03E). 2. V.V. Raghavan and S.K.M. Wong: A Critical An alysis of Vector Space Model for Infor-4. N. Maria and M. J. Silva: Theme-based Retrieval of Web News. Proceedings of the Third 8. H. Jing. Sentence Reduction for Automatic Text Summarization. In Proc. of the 6th Con-9. K.C. Litkowski: Question-answering Using Semantic Relation Triples. TREC8 (1999) 10. S. Abney, M. Collins, and A. Singhal: Answer Extraction. In Proc. of the 6th ANLP Con-12. N. Friburger, D. Maurel: Textual Similarity Based on Proper Names, MF/IR (2002) 14. Steven Abney: Partial Parsing. Tutorial at ANLP (1994) 16. Strehl, J. Ghosh and R. Mooney: Impact of Similarity Measures on Web Page Clustering, 
