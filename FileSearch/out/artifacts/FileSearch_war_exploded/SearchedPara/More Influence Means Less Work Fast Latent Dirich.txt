 There have recently been considerable advances in fast in-ference for (online) latent Dirichlet allocation (LDA). While it is widely recognized that the scheduling of documents in stochastic optimization and in turn in LDA may have sig-nificant consequences, this issue remains largely unexplored. Instead, practitioners schedule documents essentially uni-formly at random, due perhaps to ease of implementation, and to the lack of clear guidelines on scheduling the docu-ments. In this work, we address this issue and propose to schedule documents for an update that exert a dispropor-tionately large influence on the topics of the corpus before less influential ones. More precisely, we justify to sample documents randomly biased towards those ones with higher norms to form mini-batches. On several real-world datasets, including 3M articles from Wikipedia and 8M from PubMed, we demonstrate that the resulting influence scheduled LDA can handily analyze massive document collections and find topic models as good or better than those found with online LDA, often at a fraction of time.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval; I.2.7 [ Artificial Intelligence ]: Natural Language Processing Algorithms Latent Dirichlet Allocation, Stochastic Gradient
LDA has recently become popular due to its effectiveness at extracting low-dimensional representations from sparse high-dimensional data, with numerous applications in areas such as text analysis and computer vision [1]. Unfortunately, fitting a LDA topic model given a set of training documents requires approximate inference techniques that are computa-tionally expensive. This makes it challenging to apply LDA to large-scale document collections that nowadays become increasingly common.

A promising approach to scaling LDA to large datasets are online variants, see e.g. [5] and references in there, that incrementally build topic models when a new document (or a set of documents) appears. Specifically, Hoffman et al. [5] presented an online variational Bayes (VB) algorithm for LDA based on online stochastic optimization with a natu-ral gradient step that can easily analyze massive document collections. Here, we explore another avenue opened up by online LDA (oLDA) algorithms, namely, to view LDA as a search process and to ask the question whether we can im-prove it by scheduling documents respectively mini-batches for processing. Somewhat surprisingly, there has been virtu-ally no attempt to study the question of determining a good order for documents to be processed. While it is widely recognized that the scheduling of documents in stochastic optimization of LDA topic models may have significant con-sequences, this issue remains largely unexplored. Instead, practitioners schedule documents essentially uniformly at random, due perhaps to ease of implementation, and to the lack of clear guidelines on scheduling the documents. In this work, we address the question of how to schedule documents and show that convergence can be reached faster.
LDA is a Bayesian probabilistic model of collections of text documents [1]. It assumes a fixed number of K under-lying topics in a document collection. Topics are assumed to be drawn from a Dirichlet distribution,  X  k  X  Dir (  X  ), which is a convenient conjugate to the multinomial distri-bution of words appearing in documents. According to LDA, documents are generated by first drawing topic proportions according to  X  d  X  Dir (  X  ), where  X  is the parameter of the Dirichlet prior on the per-document topic distributions. Then for each word i a topic is chosen according to z di  X  Mult (  X  d ) and the observed word w di is drawn from the se-lected topic, w di  X  Mult (  X  z di ).

In this paper, we focus on variational Bayesian (VB) in-ference. Here, the true posterior is approximated using a simpler, fully factorized distribution q . Following [1, 5], we choose q ( z, X , X  ) of the form q ( z di = k ) =  X  dw di k Dir (  X  d , X  d ), and q (  X  k ) = Dir (  X  k , X  k ). The variational pa-rameters  X  ,  X  , and  X  are optimized to maximize the Evidence Lower BOund (ELBO) log p ( w |  X , X  )  X  L ( w, X , X , X  ) , [log p ( w,z, X , X  |  X , X  )]  X  E q [log q ( z, X , X  )], which is equiva-lent to minimizing the Kullback -Leibler divergence between q ( z, X , X  ) and the true posterior p ( z, X , X  | w, X , X  ).
Based on VB, Hoffman et al. [5] have introduced an on-line variant that we here present for the batch case run-ning over mini-batches (chunks of multiple observations). That is, we assume that the corpus of documents has been sorted according to some schedule, i.e. permutation  X  and chunked into l mini-batches B 1 ,B 2 ,...,B l of size S . That is, the ELBO L is set to maximize L ( w, X , X , X  ) , P B P d  X  B i ` ( n d , X  d ( n d , X  ) , X  d ( n d , X  ) , X  ), where n contribution of document d to the ELBO. As Hoffman et al. [5] have shown this mini-batch VB-LDA corresponds to a stochastic natural gradient algorithm on the variational objective L . Using mini-batches reduces the noise in the stochastic gradient estimation as we consider multiple obser-vations per update:  X   X  kw =  X  + D/S P s  X  B n sw is the s -th document in the i -th mini-batch and D de-note the number of documents.
It is known that LDA and its precursor probabilistic la-tent semantic analysis (pLSA) are closely related. In partic-ular, one can show that pLSA is tantamount to LDA with a uniform prior [4]. Moreover, it is well known that pLSA is deeply connected to instances of the problem of non-negative matrix factorization [2]. Putting both results together, there is a relation between LDA and certain settings of low-rank matrix factorization. Thus, it is natural to ask:  X  Can we improve LDA by adapting techniques developed for matrix factorization?  X  Here, we show that this is actually the case. Specifically, we utilize randomized matrix factorization ap-proaches, see e.g. [3, 6]. That is, we approximate a given matrix A by S rescaled rows/columns sampled from A . To do so, we compute an  X  X mportance score X  for each row, and sample rows using that score as an importance sampling probability distribution.

A common score is p ( i ) = P j n 2 ij / P i,j n 2 ij , and the rescal-ing factor is 1 / p p ( i )  X  S . This importance score depends on the whole corpus and intuitively captures the  X  X nfluence X  of a given document on the LDA topic model. By preferen-tially choosing documents that exert a disproportionately large influence on the topic model, we expect to capture the important part of a given corpus at hand.

Our key idea now is to apply the importance sampling procedure to LDA. However, whereas the randomized ma-trix factorization approaches sample a subset of documents with replacement, we want to keep all documents exactly once. Consequently, we schedule documents by sampling all documents with replacement biased towards those ones with higher norms. In other words, the documents with higher entry values will have higher chance to be processed earlier.
We compute a schedule, i.e., a permutation  X  of the doc-ument collection { d 1 ,d 2 ,...,d n } by sweeping through the list of documents in order of increasing indices until all doc-uments have been selected. To decide whether the current document d i should be placed at position  X  ( idx ) of the sched-ule  X  , we draw a random number in [0 , 1] and check whether it is larger than the influence score p ( i ) of d i . If that is not the case, we place d i at position  X  ( idx ) of the schedule  X  . Alternatively, we just sort the documents according to their norm, from large to small documents. Setting  X  to a random permutation would recover oLDA.

Putting everything together results in influenced sched-uled LDA (isLDA): Compute p ( i ), rescale and shuffle the documents according to  X  , and build mini-batches as de-scribed earlier  X  and then run online variational Bayes in-ference using these mini-batches. Because we only change the schedule in which documents are considered, we do not change the expected number of times a document is seen. In turn, the analysis of [5] carries over to isLDA: isLDA con-verges to a stationary point of the objective L ( w, X , X , X  ).
As we will show now, influence scheduling can be seen as a stochastic search problem [7]. More formally, assume that we have a distribution p ( z |  X  ) over search directions, i.e., gradients parameterized by  X  . In each iteration, we generate m samples of search directions z 1 ,..., z m and use some fit-ness function to evaluate them  X  such as the contribution ` of document d to the ELBO  X  to evaluate and in turn to adjust the parameters  X  of the search distribution.
Let J (  X  ) be the expected fitness under search distribu-to adjust  X  such that the expected fitness J (  X  +  X  X  ) is in-creased. The most straightforward way to do this is to set Carlo, we can approximate the last term as  X   X  s J (  X  ) = seen any document, it is natural to assume the fitness f ( z is identical for all search directions, for the ease of simpli-fication say f ( z i ) = 1 for i = 1 ,...,m . The last equa-tion simplifies to  X   X  s J (  X  ) = (1 /m ) P m i =1  X   X  ln p ( z what is  X   X  ln p ( z i |  X  )? With a rough approximation, one can assume all the search directions are independent and iden-tically distributed. The central limit theorem then gives z  X  N ( x , ( C/m )) where x represents the mean and C is the covariance matrix of the search directions. Again, if we have not seen any document it is sensible to assume  X  C is the identity matrix. Now, it is easy to show that  X  x ln p ( z |  X  ) = C  X  1 ( z  X  x ) = z  X  x .

What do we gain by this? It allows us to see isLDA as an offline, greedy search learning approach and in turn that it has close links to well-known active learning approaches. Specifically, we have a set of i.i.d. documents simultaneously available. Then, isLDA queries documents so as to attempt to improve the ELBO as much as possible without comput-ing the ELBO. Actually, isLDA takes a myopic approach that greedily chooses the next query based on this criterion. To see this, assume that the expected variational parame-ters are uniform for documents not seen, i.e.,  X  dwk = K  X  1 Now, the expected influence of an unseen document d to  X  the number of words in the document. This tells us that the document with the largest norm will have the greatest expected impact on the search distribution. The sampling implements a simple exploration and exploitation strategy. Because the norm of a document is not changing, we can stick to the corresponding schedule after having seen each document once and continue running batch LDA following the computed schedule. This essentially proves:
Theorem 3.1. isLDA is a myopic stochastic search pol-icy assuming all documents are i.i.d. and uncorrelated. Moreover, we can generalise isLDA to the online case. Intu-Algorithm 1 Influence Scheduled Online LDA (isoLDA) itively, we draw a batch of documents at random, apply one iteration of isLDA on this batch, and iterate. That is, we draw again randomly a batch, apply one iteration of isLDA, and so on. This influence scheduled oLDA (isoLDA) is sum-marized in Alg. 1.

The benefits of isoLDA are manifold. In contrast to isLDA, it does not require a full pass through the entire corpus in order to schedule documents for an update. Instead, it real-izes a dynamic scheduling by scheduling each random batch again and again. In turn, it can be quite fast and flexible when applying to massive datasets. Finally, it has a con-stant memory consumption and naturally applies to growing datasets. In a nutshell, it combines the benefits of isLDA and oLDA: faster convergence and better quality. And, since we sample batches uniformly at random, the analysis of [5] again carries over:
Theorem 3.2. isoLDA converges to a stationary point of the objective L ( w, X , X , X  ) .
Our intention here is to investigate the following ques-tions: Q1 Can isLDA be faster than batch LDA and oLDA on small and medium scale datasets? Q2 If so, does isLDA find solutions that are as good as the ones of batch LDA and oLDA for small and medium scale datasets? Q3 Does isLDA scale also well to large datasets? Q4 If not, does isoLDA scale better than isLDA and oLDA?
To this aim, we implemented batch LDA and all isLDA variants in Python based on Hoffman et al.  X  X  [5] Python code 1 for oLDA and evaluated their performances on several datasets. Specifically, we considered the following datasets where D denotes the number of documents, W the number of unique words, and N the total number of words.

The WebKB 2 dataset consists of webpages of various uni-versities with four different categories (student, course, fac-ulty, project) with D = 3,869 and N = 217,671. We chose a vocabulary of W =3,000 unique words consisting of the terms with the highest TFIDF. Finally, we also used the 20-newsgroups 3 dataset ( 20N ) with D = 18,576, N =1,847,456, and W = 10,000. From all corpora, we removed documents with less than six words. For the scaling experiment ( Q3 , Figure 1: (Left and middle col.) Perplexity vs. CPU time (sec.; K = 100 ; in log scale) resp. the number of topics. (Right col.) Influence scheduling yields competitive classification performance. Here, is = isLDA, iss = isLDA (sorted), and o = oLDA. A  X  (  X  ) denotes that the first (second) algorithm achieved a higher mean. A  X  denotes significant differences (paired t-test, p = 0 . 05 ). (Best viewed in color) Q4 ), we crawled our own Wiki pedia (english) corpus of D = 3,295,656 documents with N  X  294,000,000 and a fixed vo-cabulary of W =7,686 words. We preprocessed the crawled Wikipedia articles as done in [5], using the provided positive-list vocabulary to remove terms not appearing in our arti-cles. Additionally we used PubMed ( PM ) abstracts from the UCI repository with D = 8,200,000 documents with N  X  737,000,000 and a vocabulary of W =141,043 words. And finally, to compare the influence of different parameter set-tings, we use the NY Times ( NYT ) news articles from UCI with D = 300,000 documents with N  X  100,000,000 and a vocabulary of W = 102,660 words. For both (after stopp-word removal), the vocabulary was reduced by keeping just words which appeared more than ten times, and additionally for NYT we excluded all multi token phrases.

For WebKB and 20N , we held out 500 randomly se-lected documents for evaluation purposes; 1000 documents for the Wiki , PM and NYT datasets. On the test sets, we computed perplexity (the lower, the better) to measure the model X  X  ability to generalise to unseen data [1]. Addition-ally, we evaluated all approaches in a classification setting for WebKB and 20N . Specifically, we used the 7 first-level classes for 20N and all classes for WebKB . Then, we used a multi-class linear support vector machine 4 to predict the class labels merely using the topic distributions of the docu-ments as estimated by the learned LDA models. We report on the average accuracy achieved in a 5-fold cross-validation.
For all experiments, we set  X  close to 0 . 5 as suggested in [5] and  X  0 = 4 for the small and medium corpora (deter-mined by cross-validation on the training set of 20N but used for all experiments). To set the mini-batch size for isLDA on the small datasets we used the following heuristic: norm of document i resp. the whole corpus. Intuitively, it measures how many documents are required to capture Figure 2: Perplexity vs. CPU time on Wiki sub-sets (min.; K = 100 ; upper two rows) resp. for (d) Wiki and (e) PM (h.; K = 100 ). (f ) isoLDA is more stable for different parameter settings than oLDA: Perplexity on NYT after 5 hours. The numbers be-hind (is)(o)LDA indicate the batch sizes used. (Best viewed in color) the important part of a corpus. We always stopped when each document was seen 25 times. We used fixed symmetric hyperparameters  X  = 0 . 01 and  X  = 0 . 01.

Q1, Q2: Small and Medium Corpora The perplexity results are summarized in Fig. 1. As one can see, isLDA ap-proaches find solutions in the range of the batch algorithm X  X  solution with much less computation. For small corpora, the higher bias of isLDA (sorted) results in significantly better performance compared to isLDA and oLDA. With increas-ing sizes of the corpora, however, isLDA catches up. Taking all experiments together, either influence scheduling outper-forms oLDA on all document selections. We note, however, that batch LDA X  X  perplexity can drop below the ones of the mini-batch approaches. The accuracies of the classification experiments as summarized in Fig. 1 clearly show that influ-ence scheduling yield competitive predictive accuracies. In 8 cases, isLDA produced significantly higher classification accuracies than oLDA. Only in one case oLDA produced a significantly higher classification accuracy.

Q3, Q4: Scaling Experiments The results so far sug-gest that isLDA finds solutions in the range of (o)LDA but much faster. To investigate this further for large datasets, we ran experiments on several subsets of the Wikipedia cor-pus ranging from 100K to 500K documents using the optimal parameters as determined by Hoffman et al. [5]. For isoLDA, we used a mini-batch size of 1024. The results are summa-rized in Fig. 2. As one can see, isLDA does not scale well. It actually slows down for larger document collections. In con-trast, isoLDA scales well. To further investigate if isoLDA scales better than oLDA we conducted experiments on two massive datasets with approx. 3.3M ( Wiki ) respectively 8.2M ( PM ) documents. Here, we used | B | / 64 to set the mini-batch size for isoLDA given that oLDA used a batch size | B | . For Wiki we set  X  0 = 1 based on the proposed set-tings in [5] for large batch sizes. For PM we set  X  0 = 1024 to account for its large vocabulary size. We considered large batch sizes of | B | = 65536 and | B | = 262144 as we can ex-pect to find models of higher quality for them. The results are summarized in Fig. 2. On Wiki , isoLDA converged af-ter about only 4 hours to a model with a lower perplexity than the model oLDA found after 24 hours. On PM , this difference in performances is even more pronounced. More-over, the learning curves are much more stable compared to oLDA. That isoLDA is more stable w.r.t. to different pa-rameter settings is also confirmed by our final experiment on the NYT corpus. Fig. 2 shows learning curves for several different parameter settings. Clearly, the variance is much lower for isoLDA than for oLDA.

Putting all experimental results together, we can clearly answer questions Q1 , Q2 and Q4 affirmatively.
 Triggered by the recent success stories of oLDA approach for the problem of inferring topics in growing document col-lections, we revisited batch LDA. We turned batch LDA into a quasi-online LDA approach that forms mini-batches of highly influential documents first and processes them before less influential ones, called isLDA. Then, we turned isLDA into a novel, easy-to-implement oLDA approach, called isoLDA, that scales well to massive and growing datasets by applying influence scheduling to randomly formed batches. Based on the results of the present paper, [8] have recently developed the first active LDA.
 Acknowledgements: MW and KK were supported by the Fraunhofer ATTRACT fellowship STREAM. AP was funded by the German Federal Ministry of Economy and Technology (BMWi) under the THESEUS project.
 [1] D.M. Blei, A. Ng, and M. Jordan. Latent dirichlet allo-[2] C. Ding, T. Li, and W. Peng. NMF and PLSI: Equiva-[3] A. Frieze, R. Kannan, and S. Vempala. Fast monte-carlo [4] M. Girolami and A. Kaban. On an Equivalence between [5] M. Hoffman, D.M. Blei, and F. Bach. Online learning [6] M.W. Mahoney and P. Drineas. Cur matrix decompo-[7] Y. Sun, D. Wierstra, T. Schaul, and J. Schmidhuber. [8] M. Wahabzada and K. Kersting. Larger residuals, less
