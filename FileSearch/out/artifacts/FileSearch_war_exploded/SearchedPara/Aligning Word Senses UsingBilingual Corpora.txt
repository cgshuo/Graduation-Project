 MARINE CARPUAT and PASCALE FUNG
Hong Kong University of Science and Technology and GRACE NGAI Hong Kong Polytechnic University 1. INTRODUCTION 1.1 Motivation
Natural Language Processing (NLP) aims at understanding human language automatically. Understanding language means, among other things, knowing what words mean and how they relate to each other.

Modeling this knowledge even in a simplistic way is extremely useful to achieve automatic understanding. WordNet [Miller 1990], HowNet [Dong 1988], CYC [Lenat 1995], and FrameNet [Baker et al. 1998] are examples of such knowledge repositories, which are usually called ontologies or semantic networks . They represent knowledge by organizing words into concepts and defining relationships between them.

The growing importance of multilingual information retrieval and machine translation has made multilingual ontologies an extremely valuable resource.
Multilingual ontologies arrange words in language-specific semantic network and link these networks together.

The construction of an ontology from scratch is a very expensive and time-consuming undertaking. The cost for building the monolingual CYC ontology has been estimated to a person-century to produce 100,000 terms [Rigau i
Claramunt and Agirre 2002]. The EuroWordNet project [Vossen 1998], which aimed to build a multilingual ontology for 8 European languages, involved 11 academic and commercial institutions and took 3 years to complete. It is, there-fore, attractive to consider ways of automatically aligning monolingual ontolo-gies, which already exist for many of the world X  X  major languages.
In this work, we propose to align ontologies at the node level: given a node in one ontology in a given language, our goal is to find the node which describes the same concept in a second language ontology. We define this task as word sense alignment or node alignment . 1.2 Challenge 1.2.1 Differences in Ontology Design. The difficulty with linking ontologies is that even in the same language, ontologies can be built with different design philosophies and present very different structures, making a surface structural alignment impossible.

Instead, we propose a bottom-up approach by aligning ontologies at the node level. It can be assumed that, regardless of the structure adopted, a node always contains a set of words sharing the same meaning. 1.2.2 Translation Ambiguity. The first and easiest way to map words from one language to another is to translate them using a bilingual dictionary. How-ever, each word has multiple translations, and each of these translations has multiple senses.

In order to compare nodes in different ontologies in a different language, it is necessary to quantify the similarity between nodes. One of the most effective ways of comparing words is by comparing the contexts in which they occur [Lee 1999; Lin 1998]. As ontologies themselves do not usually provide much contextual information, bilingual corpora are needed to capture the patterns of usage of these words. 1.2.3 Bilingual Corpora Comparability. Parallel corpora  X  X exts that are translation of each other X  X re commonly used in machine translation research.
However, the amount of easily available text with close manual translation is limited in size and scope. Moreover, such parallel texts are not easy to come by for all language pairs. In contrast, nonparallel corpora can easily be compiled from texts independently written in each language. Using nonparallel corpora is, therefore, attractive, but requires having to deal with an additional amount of noise. 1.3 Outline This paper addresses the problem of word sense alignment across languages.
Given a concept represented by a node in one ontology, our task is to find the best corresponding node or word sense in the second language ontology. We propose a bilingual corpus-based approach to evaluate the similarity between the nodes and investigate how the degree of comparability of the corpora used influences the result.
 Two different ontologies in very different languages, the American English WordNet and the Mandarin Chinese HowNet, are used as sense inventories.
The rest of this paper is laid out as follows. An overview of related work on automatic ontology learning is given in section 2. Section 3 introduces the structure of the ontologies used and describes the alignment methodology pro-posed. Section 4 deals with issues related to bilingual corpora utilization. The application to the alignment of WordNet and HowNet is presented and evalu-ated in Sections 5 and 6. Finally, a discussion and future work will be presented in Section 7. 2. RELATED WORK ON AUTOMATIC ONTOLOGY LEARNING
Considering the cost of manually building such resources, methods have been developed to enrich ontologies by merging the lexical knowledge located in pre-existing resources. Two types of resources can be used: unstructured resources (raw text, corpora) structured resources, that already encode some kind of semantic knowledge (e.g., thesauri, lexicons, wordnets) 2.1 Enriching Existing Ontologies
The first type of approach uses the knowledge encoded in another resource X  another ontology or dictionary X  X o enrich a given ontology. Knight and Luk [1994] merged word senses from WordNet, with definitions from an English dictionary. Their method, called the definition match algorithm , is based on the idea that two word senses should be matched if their two definitions share words. Another possibility is to use the structure of the ontologies instead of the words. The hierarchy match algorithm uses the various sense hierarchies instead of definitions that do not always exist. Based on a set of unambigu-ous words that can be directly matched, the algorithm extends the mapping by moving upward in the hierarchies starting from the matched senses and matching the senses of words that appear in both hierarchies. Resnik [1995] also proposed measures of semantic similarity based on the distance of concepts in a semantic network.

The second type of approach exploits the distributional patterns of words in a corpus to expand ontologies with new words or to add topical information
Lin [1998] chose dependency relations in a parsed corpus as the word features contained by the dependency relations that the two words have in common divided by the amount of information in all the dependency relations for each
WordNet than Roget X  X  thesaurus is extracted. Agirre et al. [2000] proposes to exploit the large amount of text available on the Internet to enrich concepts in existing ontologies with topical information. A search engine is used to link each concept in WordNet to relevant documents from the Web. A boolean query is built using all the words from the given synset, adding hypernyms, hyponyms, and also words from sibling synsets. The retrieved documents are then used to define a topic signature for each concept, i.e., a list of words closely related to the concepts.

These methods have been efficiently used for enriching monolingual ontolo-gies. However, they are not directly adaptable to our cross-lingual problem.
Using a bilingual dictionary to match translations, instead of the words them-selves, leads to the difficult word sense disambiguation problem and there are no cross-lingual search engines available to search the web in a language other than the one of the query. 2.2 Aligning Ontologies Across Languages
Some other approaches have been developed in order to tackle the cross-lingual alignment task. Most of these methods exploit structural information in order to solve the ambiguity added by translation.
 Dorr et al. [2000] and Palmer and Wu [1995] focused on verbs from the
Chinese ontology HowNet and used thematic-role information, which denotes the contexts in which a particular verb may occur. The HowNet thematic-role specifications are mapped to word classes in the existing EVCA classification of English verbs [Levin 1993]. The structure of the EVCA classification is very similar to that of the verb classes in HowNet. These mappings are then used to align English EVCA verbs to Chinese HowNet verbs.

In Japanese, Asanoma [2001] used structural link information to align nouns from WordNet to a preexisting Japanese ontology called Goi-Taikei. The
Japanese WordNet, which was constructed by manually translating a subset of WordNet nouns, was used as a bridge between the two languages. Sekine et al. [1999] mapped EDR, a Japanese X  X nglish dictionary organized in a is-a taxonomy, to WordNet. They use a method that combines local structural in-formation and a definition match algorithm between the nodes in the English EDR and WordNet.

All of these methods exploit some similarity in the structure of the ontologies to align and/or exploit manually created bilingual resources to avoid the transla-tion disambiguation problem. Therefore, they require expensive preexisting re-sources and cannot be used to align ontologies with vastly different structures. 2.3 Word Alignment Using Bilingual Corpora
Word alignment aims at finding words that are translations of each other, usu-ally in sentence-aligned parallel corpora. It can be a step in training a machine translation system or a method to build bilingual resources.

Previous work include the IBM Candide project [Brown et al. 1990], which in an unsupervised fashion through the EM algorithm; Church [1993] used character frequencies to align words in a parallel corpus; Wu and Xia [1995] automatically extract an English X  X hinese translation lexicon, by statistical analysis of a large parallel corpus, using limited amounts of linguistic knowl-edge; Smadja et al. [1996] used cooccurrence functions to extract phrasal collocations for translation, and Melamed [1997] identified noncompositional compounds (NCCs) by comparing the objective functions of a translation model with and without NCCs.

All of the above methods require clean parallel corpora, which are not al-ways easily available for all language pairs. To address this problem, some re-searchers have proposed methods that exploit the large amount of text available online, to create large, but noisy, nonparallel corpora. In this case, positional information, such as occurrence of word pairs in matching sentences, cannot be used. These methods are, therefore, based on the assumption that words that are translations of each other should tend to occur in similar contexts across languages.

Dagan and Itai [1994] compared cooccurrence ratios of words in corpora in different languages to pick a translation among multiple candidates. Rapp [1995] proposed a matrix permutation method matching cooccurrence patterns in German and English nonparallel texts. In spite of the computational lim-itations, he showed that words that cooccur in a text are likely to cooccur in another text as well. Instead of finding the best translation in a set of can-didates, Fung and Lo [1998] directly extracted the best translation candidate from nonparallel, yet comparable, bilingual corpora. Their method, which is in-spired by techniques used in information retrieval, captures the cooccurrence of the words in the corpus with a list of seed words, for which the translation in both languages is defined.

In this paper, we are interested in mapping together nodes that describe similar concepts, in different languages. This problems differs from the trans-lation induction problem, because we assume that the translations at the word level are available in a bilingual dictionary. Moreover, instead of aligning single words or phrases, we want to map groups of words or phrases that are synonyms in one language to a group of words in the other languages that are used in the same sense. 3. METHODOLOGY
In this section, we propose a methodology to align word senses, given two on-tologies: the American English WordNet and the Mandarin Chinese HowNet.
We will first give a description of these two resources, show how nodes can be mapped, and use a semantic similarity measure to pick the most appropriate node among several candidates. 3.1 Ontologies
This section describes the philosophy and basic structure behind the two on-tologies that are used as sense inventories. 3.1.1 WordNet. The first ontology selected for alignment is the Ameri-can English WordNet [Miller 1990], version 1.7. Wordnet is based on synsets that are constructed as to allow a user to easily distinguish between differ-ent senses of a word and as to represent one underlying concept. For example, the synsets { crooked, corrupt } and { hunched, round-backed, round-shouldered, stooped, stooping, crooked } make a clear distinction between two senses of the adjective crooked .

WordNet 1.7 currently contains more than 144,600 words and 109,000 synsets. In addition to synonymy, semantic relations such as antonymy, hy-ponymy (is-a relationship) and meronymy (part-of relationship) are defined.
The hypernymy relation is used to organize nouns, verbs, adjectives, and ad-verbs in a separate hierchical structures.

The WordNet entry for the noun teacher is given here to illustrate this hierarchy.

Sense 1 teacher, instructor =&gt; educator, pedagogue =&gt; professional, professional person
Sense 2 teacher
In this example, instructor is a synonym of the sense 1 of teacher . It helps identifying this sense as a person who teaches and to distinguish it from sense 2, which corresponds to an abstraction (as in  X  X ooks were his teachers. X ). The sign = &gt; denotes a hyponymy relation: { teacher, instructor { educator, pedagogue } . This example shows that it is possible for the senses of a word to occupy very different positions in the Wordnet structure. 3.2 HowNet
In contrast to WordNet, HowNet was built using a constructive approach : basic units of meaning, called sememes , are used to build up concept definitions [Dong 1988]. This approach is especially adapted to the Chinese language: as Chinese characters are monosyllabic and convey meaning, they can be used to define the concepts represented by Chinese words, which are mostly composed of several characters. In this work, we use the first version of HowNet (2000), where 1503 sememes are defined and used to build a total of almost 17,000 definitions.
The philosophy behind HowNet is based on the idea that all things are in constant motion and are ever changing in a given time and space. Attributes can be used to record the changes as things evolve from one state to another. A concept can, therefore, be defined by setting the values of different attributes for each thing. Given a set of sememes representing things, attributes, and their values, HowNet gives a definition for any word and any of their particular senses.

For example, whereas in WordNet the different senses of the adjective crooked were distinguished by adding the word to different synonym sets, in
HowNet, the word is given a different definition for each of the senses it can take:  X  crooked 1:  X  X Value | , form | , curved |  X  X  X rooked is an adjective, it de-scribes a form that is curved.  X  crooked 2:  X  X Value | , behavior | , sly | , undesired | jective, it describes a behavior that is sly and undesired.

Another example further illustrates how sememes are used. The Chi-nese word , ( teacher ) is given the following definition:  X  X uman education | . The first sememe,  X  X uman |  X , is the categorical attribute, which corresponds to an hypernym of  X   X  X  teacher (a teacher is a human).  X  X each and  X  X ducation | describe the specific characteristics of teacher . The second sememe  X *teach | is used with the pointer  X  , which denotes an event X  X ole re-lation. Here, teacher is an agent of teach . The definition can therefore be read  X  X  teacher is a human, his role is to teach, in the domain of education. X  Table I shows several other pointers that describe more detailed relation between se-memes; examples for each of these relations are given in Table II.
All the sememes are organized within a hierarchical structure. Sememes representing content words are organized into the following main categories: Entity Event Attribute and Quantity Attribute Value and Quantity Value
Most nominal concepts, such as  X  X eacher, X  belong to the Entity category. Verbal concepts always belong to the Event category, whereas adjectives are Attribute
Values. The sememes in each category are organized hierarchically in an ontol-ogy tree. 3.3 Comparison
The structural differences between WordNet and HowNet make the alignment task more difficult, but aligning the nodes will eventually allow to enrich both ontologies by inferring semantic relations that were encoded in one and not in the other.

Table II gives a summary of the semantic relation coded in each semantic network for nouns. For example, whereas WordNet organizes nouns and verbs in two independent hierarchical structures, HowNet provides a link between the nouns and verbs classifications with the Event X  X ole relation.

A detailed analysis and comparison of the semantic relations between nouns encoded in WordNet and HowNet can be found in Wong and Fung [2002]. 3.4 Mapping Ontology Nodes
We propose a method that allows the mapping of two ontologies at the node level and which only requires two additional resources: a bilingual dictionary and a bilingual corpus, not necessarily parallel. Figures 1 and 2 give an overview of the method. Figure 1 shows how a list of seed X  X ord pairs are selected; Figure 2 shows how, given a node in one ontology (HowNet, in our case), a node is selected for alignment in the other ontology (WordNet). The node alignment method is described in detail in this section, while the seed X  X ord selection will be dis-cussed in section 4. 3.4.1 Representing the Nodes. The nodes of the two ontologies, Word-
Net and HowNet, are constructed and organized very differently. In order to compare them, a common representation has to be defined. In this section, we propose a very simple representation, which can be easily adapted to any ontology, regardless of the way they are constructed.

In a given ontology, we define a node as a set of words that share a com-mon definition. A node N can, therefore, be represented by two sets of words:
N = ( S , D ). S represents synonyms for the word sense N , while D is a set of definition words for N .

The S and D components allow to accomodate differences in ontology de-sign. While WordNet is built around the notion of synonym sets or synsets, in
HowNet, the definition is built first, and nodes that share the same definition are defined as synonyms afterward. In order to accurately understand the con-cept represented by a node, both the words and the definitions are, therefore, needed.

For the HowNet nodes, N consists of: the set S of Chinese words that share the definition corresponding to the particular node the set D of Chinese sememes that form the definition itself.

For example, the definition BecomeMore | , scope = value | is shared by the following Chinese words  X   X  and  X  appreciation . X  In our alignment method, the corresponding node is defined as a pair of word sets: ( | , } , { } ).

In WordNet, the synonym set is built first and then a gloss is added to describe the concept represented by the synset. Therefore, for the WordNet nodes, N similarly consists of: 1. the set S of the synset words. If it is a single word synset, its hypernyms and hyponyms are added, in order to distinguish that node from other nodes that have the same synonym set. 2. the set D of words formed by all the open-class words in the gloss given with each WordNet synset.

Sense 5 of the word appreciation in WordNet is the following: The corresponding node is, therefore, represented by:
S ={ appreciation, increase, step-up } and D ={ increase, price, value, appreci-ation, real estate } ).
 3.4.2 Defining an Initial Node Mapping by Translation. If the two ontolo-gies were in the same language, the more words the nodes share, the more similar they should be. For bilingual alignment, nodes are similar if they share translations.

However, because of multiple translations and polysemy, nodes that repre-sent different concepts may also share translations.

Another problem derives from the differences in the way definitions are nite set of sememes, whereas WordNet glosses give both a definition and ex-amples of use in natural language. It is, therefore, possible that the English gloss and the Chinese Definition do not overlap even if they describe a sim-ilar concept. For example, the HowNet definition { GetMarried is the equivalent to the WordNet single word synset translations.

Translation pairs extracted from a bilingual dictionary thus do not provide enough information to perform the node mapping. They are only used to ob-tain an initial noisy mapping: a set of WordNet nodes, candidates to the final alignment, are mapped to each HowNet node.

The set of alignment candidates is C ={ N WordNet ,1 , ... 3.4.3 Evaluating Semantic Similarity between Nodes 3 . 4 . 3 . 1 Building Context Vectors. The initial mapping uniquely relies on a list of translations to define relationships between words. In order to character-ize each node more accurately and to have more information to discriminate it against other nodes, it is necessary to define features that capture contextual information.

To this end, we propose a corpus-based method, which is inspired by the Con-vec algorithm [Fung and Lo 1998]. Convec was originally designed for trans-lating new words from nonparallel, comparable texts. The algorithm defines a similarity metric between words in two different languages, based on the underlying assumption that there is a correlation between word cooccurrence patterns that persists across languages, and the similarity between word cooc-currence patterns is indicative of the semantic similarity.

The task considered in this paper differs from dictionary induction. Instead of finding a translation for a given word in one language among all words in the vocabulary of a second language, we want to map a HowNet node (or Chinese sense) to a WordNet node (or English sense), choosing among a given set of alignment candidates. While the units being mapped are different, the Convec definition of semantic similarity is still very useful to aligning WordNet and HowNet nodes.

We use Convec instead of other similarity metrics, because it has been specif-ically defined for exploiting nonparallel corpora and there is already evidence lated task of translation dictionary induction. Experiments demonstrate that associations between a word and its context words, as modeled in Convec, are well preserved in nonparallel, comparable texts of different languages [Fung and Lo 1998].

Convec X  X  underlying assumption is that there is a correlation between word cooccurrence patterns that persists across languages and the similarity be-tween word cooccurrence patterns is indicative of the semantic similarity. To construct a representation of the cooccurrence patterns, a list of seed words is compiled. There is a seed-word list for each of the two languages considered.
Each word in the seed-word list in the first language is a translation of a word in the seed-word list in the other language. Given a bilingual corpus, a context vector can then be constructed for each of the words of interest, where each element in the vector is a weight corresponding to a function of the signifi-cance of a particular seed word and its cooccurrence frequency with the word of interest. This method, which has been applied to the problem of automatic dictionary induction, has the advantage of being able to use bilingual corpora, translations are supposed to be known X  X ut the methodology proposed to repre-sent and compare contextual information accross corpora can be adapted to our purpose.
 Given the language pair of interest, the first step consists in defining a set SW of seed-word pairs:
SW ={ ( s 10 , s 20 ), ( s 11 , s 21 ), ... ( s 1 t , s 2 t
SW is defined once and for all for a given language pair. It might be adapted to the particular bilingual corpus used, as described in Section 4.4.
In each language l  X  X  1, 2 } , a context vector v for each word w in the sets S and D representing a node, is then built as follows:
Create a vector v ={ v 0 , v 1 , ... , v t } where v j = ( log ( TF ( w , s lj )) + 1)  X  IDF ( s lj )
Note that the set of seed-word pairs SW is independent from the context seed-word pairs.
 3 . 4 . 3 . 2 Comparing Nodes. Given a node in one ontology, our approach con-siders the average similarity between its words and words from all potential alignment candidates.

The contribution of the definition words in D and the synonyms in S are taken into account independently. If all words were compared together, since function words or common words used in D are likely to occur more frequently, than the synonyms S in the corpus, the information given by the words in S would be lost in the final average similarity score.
 HowNet, N HowNet = ( S HowNet , D HowNet ), their similarity is defined as follows:
For a set of Chinese words A and a set of English words B , with w v  X  B
For a pair of words ( w , v )  X  A , w and v their respective context vectors, the similarity is defined as: sim ( w , v ) = cos( w , v ) = w  X  v | w || v | 3 . 4 . 3 . 3 Ranking the Alignment Candidates. Eventually, the candidate nodes from C are ranked according to their similarity with the node N and the synset with the largest similarity is considered to be the alignment  X  X inner. X  4. BILINGUAL CORPORA COMPARABILITY 4.1 Motivation and Background
Our method assumes that a bilingual corpus and a set of seed-words are avail-able. Even if we do not require the use of a clean parallel bilingual corpus, the selection of the corpus and the seed words are crucial. If the two sides of the bilingual corpus are completely unrelated, the comparison of cooccurrence patterns in each of them will not be meaningful. However, the notion of com-parability between corpora in different languages is difficult to evaluate. Intu-itively, a corpus from the Chinese newspaper People X  X  Daily is closer to articles from the Wall Street Journal than to the Complete Works of Shakespeare. If this distance could be quantified, it could be used to choose appropriate corpora and seed words, as well as to analyze the alignment results obtained with a particular corpus.

Devising such a metric, however, is a complex question. The reason is because this distance depends on several dimensions, such as: the type of the texts (e.g., news articles, technical manuals, newsgroup mes-sages or novel) the topic of the texts (e.g., history, sport, local or international news) the period when the texts were written.

Kilgarriff [2001] conducted one of the first systematic attempts at quanti-fying similarities and differences between corpora and proposed a particularly interesting method to compare measures of corpora similarity. The Known-
Similarity Corpora (KSC) method consists in creating a set of corpora of dif-ferent degrees of similarity by merging two corpora of distinct types, A and B. For instance, if Corpus 1 is taken at 100% from A, Corpus 2 contains 80% from
A and 20% from B, and Corpus 3 contains 30% from A and 70% from B, then we know that Corpus 1 is more similar to Corpus 2 than to Corpus 3. A good similarity metric should reflect this ranking. Unfortunately this method can-not be used when comparing corpora in different languages. In the monolingual case, the KSC method was used to compare a wide range of existing measures.
The best one was the measure proposed in Kilgarriff and Rose [1998], which is inspired by the  X  2 test for statistical independence: for each of the 500 most common words in corpora 1 and 2, they calculated the expected number of oc-currences in each corpus, as if both corpora were random samples taken from the same population. The  X  2 metric basically measures the difference between these expected frequencies and the actual frequencies observed in the corpora.
When texts in different languages are compared, the problem is more compli-cated. The main difficulty stems from the combination of multiple translations and polysemy: words usually have more than one translation in another lan-guage, their translations can be polysemous. For closely related languages such as English and French, polysemy is sometimes shared. For example, the French with great interest , and financial interest . However, this is not the case for most of the words, nor for other language pairs, such as English and Chinese, that developed more independently. When comparing word frequencies in corpora of the same language, polysemy, which leads to differences in word usages, is a serious problem. For parallel corpora, this problem is alleviated, to a certain extent, by the fact that since the texts are translations by definition, we know used with the same sense. For parallel corpora, the differences in word usage therefore essentially come only from multiple translations. When using word frequencies to compare nonparallel bilingual corpora, however, both problems are combined.

Apart from the general dimensions of type, topic, and period, a measure of corpora similarity should also reflect more subtle word usage differences. It is not possible to define an absolute corpus similarity. The similarity has to be eval-uated using different criteria depending on the task for which the corpora are needed. In this work, we are interested in comparing patterns of cooccurrences of words in two different languages. For this comparison to be significant, we need to make sure that the two sides of the corpus are comparable with respect to the set of seed words used. 4.2 Dotplots
Dotplots have been shown to be an appropriate tool to compare word frequencies in two corpora [Church 1993]. This method was applied to language difference measurement by Fung and Lo [Fung and Lo 1999], whose goal was to evaluate the cost of adapting a language model for continuous speech recognition from Mandarin to Cantonese. Even though Mandarin and Cantonese are distinct
Chinese languages, the Chinese characters used in writing are the same, which simplifies the translation problem.

Here, we show how dotplots illustrate bilingual corpora comparability for several corpora and two language pairs. Our method was originally developed with French and English corpora, and we apply it to Chinese and English corpora since they are the languages of WordNet and HowNet. The corpora used in this section are described in Table III.
 For the French X  X nglish dotplots, a set of translation pairs is extracted from a English X  X rench dictionary, freely available from the Internet Dictonary Project.
When an English word is given several French translations, a pair is created for each of them. Translations that are not single words but multiple word descrip-tions are filtered out. For the Chinese X  X nglish dotplots, we extract translation pairs from the Chinese-English dictionary, described in Section 5.1.
In Figures 3, 4, and 5, the relative frequency of the seed words in French cor-pora are plotted against the relative frequency of their counterpart in English corpora. If the French and English corpora were word for word translations of each other, the seed-word frequencies would be the same in both languages, and the points would be aligned close to the diagonal line y = gual case, if we compare a corpus against itself, we would get a perfect diagonal.
However, in the bilingual case, a perfect diagonal can never be achieved because of ambiguity in the translation of seed words. However, the more the data points diverge from the diagonal, the more difference there is in the usage of the seed words in the two corpora.

Using texts from the same period of time and comparable origin generates points closer to the diagonal. For example, in Figure 5, the majority of the points are clearly above the diagonal, whereas they are better distributed on both sides for the two other dotplots 3 and 4. Similarly, comparing Figures 6 and 7 show that the data points are more scattered in the comparable corpus dotplot than in the parallel corpus dotplot. 4.3 Linear Regression Analysis
The dotplots give a good visual indication of the comparability of two corpora, but it does not translate into a quantifiable metric. To quantify the distance, we propose using standard linear regression and goodness of fit statistics.
Given a corpus in a language l 1 and a corpus in a second language l 1. Define a set S of seed words:
S ={ ( s 10 , s 20 ), ( s 11 , s 21 ), ... ( s 1 t , s 2 t 2. Assume the relation between the logarithm of the word frequencies f and f 2 in l 2 can be estimated by where 3. Evaluate the goodness-of-fit of the model with the R 2 of f 2 i with the linear regression equation
The R 2 coefficient can be interpreted as the proportion of the data that is explained by the linear equation, while the slope  X  diagonal the actual data lies.

The linear regression scores for our French X  X nglish and Chinese X  X nglish corpora are given in Table IV. Combined with the dotplots, they give a good mea-sure of the degree of comparability of these corpora. For the Chinese X  X nglish than for the Chinese X  X nglish parallel corpus, and the R that the linear regression approximation explains a larger proportion of the data for the parallel corpus than for the comparable corpora. Similarly, for the French X  X nglish corpora, the linear regression approximation fits the parallel
Canadian Hansards better than the unrelated corpus consisting of Le Monde and Shakespeare texts. Note that the slope obtained for the French X  X nglish corpora is much higher than with the Chinese X  X nglish corpora. This can be explained by the fact that French is much closer to English than Chinese and might also indicate a difference in the quality of the bilingual dictionary used for each language pair, as will be discussed in Section 7.1. 4.4 Seed-Words Filtering
The dotplots can also be used to select appropriate seed words for a given bilin-gual corpus. In the method proposed for ontology nodes and node similarity computation, the seed words form the basis of the feature space. If the word pairs used as seed words are used very differently in the two sides of the corpus, patterns of cooccurrence with these seeds will be very different for any words.
In order to select seed words that are likely to be good features, we filter out all the pairs that are outliers in the dotplot of the bilingual corpus used. The resulting dotplot for the L.A.Times/Le Monde corpus is shown in Figure 8.
Filtering out the outliers is an easy systematic method to select reliable translations by: eliminating erroneous translations and misspellings. For example, contestant is translated as combattant (combatant) in the dictionary. selecting the most appropriate translation for a word when several are given.
For example, the French word racisme is given two translations: racism and racialism . Since the second translation is not used frequently in the
L.A.Times, the corresponding point is an outlier in the dotplot 4 and it is filtered out. filtering out translations that are accurate in a particular context, but not in the domain represented by the corpus. For example, the pair acad  X  emique/ academic is a good translation, but only when the adjective is used in the art domain. In the most frequent usage, such as academic year , a better translation would be universitaire .

Tables V and VI give some more examples of word pairs that are kept as seed words or filtered out.

In order to show the influence of seed-word selection, we compute co-sine similarity scores obtained for several French/English word pairs in the
L.A.Times/LeMonde corpus, using the formulae in Section 3.4.3. Two sets of seed words X  X efore and after selection X  X re used. Some of the results are given in Table VII. The similarity scores obtained with the filtered seed words set give stronger evidence when words are accurate translations of each other.
However, there is a trade-off between clean features and data scarcity: the higher the number of seed words, the more words will cooccur with them. 5. EXPERIMENTS To get a first estimation of the performance of our method, a test set of 300 HowNet definitions were randomly chosen to be aligned to WordNet synsets.
The alignment is performed with three different methods: (1) similarity-based alignment with a parallel bilingual corpus, (2) similarity-based alignment with a nonparallel bilingual corpus, and (3) a simple mapping based on translations is used as a baseline. 5.1 Bilingual Dictionary A bilexicon is needed for two purposes: (1) create the initial mapping between HowNet and WordNet nodes, and (2) build the seed-word list.

Our bilexicon is built from HowNet, augmented with the Chinese X  X nglish dictionary from the Linguistic Data Consortium. It is possible to learn a bilexi-con from a parallel corpus, using, for example, the IBM alignment model, how-ever, this results in very noisy alignments, so we decide to use existing dictio-nary resources which are more reliable, but still need to be processed in order to get a clean and reliable bilexicon.

For each HowNet entry, we extract the Chinese word, its part-of-speech, and the English translation. However some of these translations are problematic: some are misspelled or incorrect, some are lenghty X  elaborate translations that tend not to occur in corpora. For example: is translated as  X  X all through and stand exposed. X  Including all the words from the phrase, would likely result in the introduction of a lot of undesirable noise. In order to compensate for these problems, a greedy maximum forward match algorithm for Chinese word segmentation was used with the WordNet lexicon to identify useful phrases in the translations. Any translation that contained more than one such phrase, or one word, was discarded. These translations were further augmented using the LDC bilexicon.

These preprocessing steps result in a highly precise, clean set of translations, built from the HowNet and WordNet vocabulary. They are, therefore, appropri-ate for creating the initial mapping between HowNet and WordNet nodes. The final seed-word list contains 10453 seed-word pairs (9637 nouns and 816 verbs), and every Chinese entry has 2.5 English translations, on average.

Note that we only use the surface form of the words found in the dictio-nary. Although suboptimal, this should be sufficient for Chinese and English, since they have relatively little morphological variations compared to other languages. 5.2 Bilingual Corpora
Although the method does not require using parallel texts, one was used for the first step of the experiments. The Hong Kong News Corpus is a Chinese X 
English parallel corpus, which comprises almost 18,500 aligned article pairs from news documents released between 1997 and 2000. On the English side, it contains over 6 million words.

In addition to the parallel corpus, the experiments were also repeated on a nonparallel corpus. The amount of nonparallel text easily available is much larger, and not restricted to texts from Hong Kong. Since we want to align the
American English WordNet and the Mandarin Chinese HowNet, we can se-lect corpora from the relevant origin: newspapers articles from 1987 to 1992 ensure we were working on well-formed, sentences that were shorter than 10 Chinese words and 15 English words were filtered out. A set of sentences were then randomly picked in the remaining corpus. The final corpus totals 15 million English words (540k sentences) and 11.6 million Chinese words (390k sentences).

The final corpus obtained is not parallel and not even comparable. Dotplots and linear regression results showed in Section 4 confirm that the Wall Street Journal and People X  X  Daily corpora are less comparable than the two sides of the
Hong Kong news corpus. This departs from the experiments reported by Fung and Lo [1998], who used English and Chinese Hong Kong newspaper articles from the same time period to build a nonparallel, yet comparable, corpus. 5.3 Preprocessing the Corpus
Before the corpora can be used for our experiments, a number of preprocessing steps had to be done. First, the text was sentence-delimited on both sides. The
Chinese corpora were word segmented using a greedy maximum forward match algorithm with the HowNet vocabulary as a lexicon. On the English side, the same algorithm was used to group together individual words forming phrases listed in WordNet.

Texts were also part-of-speech tagged using fnTBL [Ngai and Florian 2001], the fast adaptation of Brill X  X  transformation-based learning tagger [Brill 1995], and, also, in order to avoid additional noise caused by homonyms with a differ-ent type (e.g., break as a noun and as a verb). 5.4 Evaluation
Some methods have been proposed to evaluate automatically created monolin-gual thesauri: Lin [1998] measures and compares the distance between a new thesaurus and two existing resources, WordNet and Roget X  X  Thesaurus, to get a sense of the correctness of the thesaurus obtained. However, the distance mea-sure proposed requires matching words in the same language and cannot be used to evaluate our bilingual ontology. For bilingual ontology creation, most of the previous work was evaluated by manually checking the correctness of the alignment.

Because there is no annotated data available for the evaluation, two judges who speak the languages, were asked to hand-evaluate the resulting align-ments. The convention was to decide if the top-ranked synset candidate was an acceptable alignment for the HowNet definition given the set of sememes, rather than using the set of words that are contained in the definition. This was done in order to avoid sense ambiguity problems with the Chinese words.
To get a sense of the complexity of the problem, it is necessary to construct a reasonable baseline system from which to compare against. For a baseline, all of the synsets that directly correspond to the English translations were extracted and enumerated. Ties were broken randomly and the synset with the highest number of corresponding translations was selected as the alignment candidate. 6. RESULTS
Table VIII gives an overview of the alignment results obtained on the test set with the three different methods.

The low score (below 50%) obtained by the baseline shows the difficulty of the task. The parallel corpus-based node alignment is only slightly better than the baseline, but using nonparallel corpus clearly outperforms the two other languages of the ontologies are more useful than parallel texts, when parallel corpora of the right domain are not available. This issue will be discussed in Section 7.2.

The top rankings obtained with the Hong Kong News parallel corpus are given in Table IX. The method succeeds at finding correct alignment candidates for some of the HowNet definitions. However, the overall performance is not significantly better than that of the baseline. Table X shows the top 20 highest-scoring alignment mappings for the nonparallel corpus.

In addition to the overall results, it is also interesting to examine the rank-ings of the alignment candidates for some particular HowNet definitions. 6.1 A Troupe or a Carnival?
Table XI shows an example HowNet definition and a subset of its align-ments. The definition { community | , *perform | , entertainment shared by 9 Chinese words: , , , , , , , , and pany, theatrical troupe, acrobatic troupe, art troupe, circus, and ringside . The noun phrases do not belong to the WordNet collocation list and they are filtered out. The remaining translations are troupe, circus, and ringside , which give eight WordNet synsets candidate for alignment.

The method correctly identifies the most accurate synset equivalent. More-over, the similarity scores yield a consistent ranking: the second best synset describes a concept close to the definition, but less general than the top one, all the unrelated synsets are at the end of the list.
 6.2 Insurance Coverage or Dictionary Coverage?
Table XII shows the example of definition 13817 quantity #cover | , &amp;space X  where there is a single entry in HowNet, with a single English translation: coverage . The alignment task consists in this case of a Word Sense Disambiguation problem between the three WordNet senses of coverage . Since there is a single word in the HowNet node, most of the disam-biguation information comes from the similarity between the Chinese definition and the English glosses.
 6.3 Spend Money or Spend Time?
Table XIII gives another example. The HowNet entries are translated as ex-penditure, expend, consumption, consume, decrement, and spend . The sense distinction between the corresponding WordNet synsets are not as clear as they were for the two previous examples, making the alignment task more difficult.
This is, therefore, a good illustration of the strength and power of the cross-lingual word similarity calculation, as the system correctly ranks the synsets describing the idea of paying money to the top. 7. ANALYSIS
As can be seen, the algorithm is often successful at finding an appropriate synset alignment candidate for most of the selected HowNet definitions. The previous section presented some of the results; this section will focus on analyzing the strengths and weaknesses of the method. 7.1 Problems Encountered One of the biggest problems encountered was that there are quite a number of
HowNet definitions that do not have a WordNet synset equivalent. There are two main reasons for this: 1. Given any arbitrary pair of languages, there will exist some words in one 2. The inherent difference in sense granularity and structure between any
Another problem encountered is in the translations given by the bilingual dictionary.

First, some alignments are poor because the translation of the HowNet words does not give access to the most appropriate WordNet synsets. For example, a better equivalent for the HowNet definition spend | , commercial be the WordNet synset pay (give money, usually in exchange for goods or ser-vices) . However, none of the Chinese words in the HowNet node translate as pay and the definition is aligned to { spend, expend, drop
Chinese X  X nglish corpora comparison, in section 4, show that the scores ob-tained with the first corpora set are much higher than those obtained with the second. Apart from the inherent language difference, this might indicate that the quality of the Chinese X  X nglish bilingual dictionary is not good enough. The other problem caused by the dictionary is in the selection of seed words.
The set of seed words defines the feature set that characterizes each word and group of words, and allows to distinguish them from the others. In the absence of any closely parallel texts, the seed words are the only bridge used to map the two sides of a bilingual corpus.

This creates problems when comparing scores across definitions. While the similarity scores usually give consistent rankings for the WordNet synsets mapped to a particular HowNet definition, they cannot be used as an indicator of the confidence in the result for different definitions. The alignment accuracy is not higher when the top synset gets a higher similarity score. Good rank-ings are actually obtained even if the similarity scores are low. For instance,
HowNet definition AmountTo | , scope = weight | is correctly aligned to the synset weigh ( have a certain weight ) with a similarity score of 0.043 only. 7.2 Robustness of the Method
Despite all these problems, the method is successful at achieving a partial align-ment between two very different ontologies in two very different languages. The advantage of our method is that it is completely unsupervised and, therefore, cheap on resource requirement. It does not use any annotated data, and the only resource that it requires X  X eyond the ontologies that are to be aligned X  X s a bilingual machine-readable dictionary, which can usually be obtained for free or at very low cost.

The comparison of the results obtained for nonparallel and parallel corpora show that the method proposed is robust enough to perform efficiently, even when the texts in the two languages are unrelated.
 The English and Chinese halves of the nonparallel corpus are very different.
Even though both halves were extracted from news articles, the content and style is different for each newspaper: the People X  X  Daily corpus is a government publication, written in a very terse and brief style, and does not concern itself much with nongovernment affairs. The Wall Street Journal, on the other hand, sources. Figure 7 shows the resulting dotplot for the whole lexicon vocabulary.
In spite of the distance between the two sides of the corpus, the nonparallel corpus gives results that are 28% more than the Hong Kong News parallel cor-pus. Among the 144 definitions that were correctly aligned with the Hong Kong news corpus, all but 5 are also correctly aligned with the nonparallel corpus.
The similarity scores for the top alignment synsets are higher, on average, with the nonparallel corpus. This is simply because of the fact that there is more overlap between the English and Chinese context vectors in the nonparallel corpus. For example, for the words and bed , there is no overlap at all between the seed words that get nonzero frequencies on the English and on the Chinese side. In the nonparallel corpus, the words both occur with the seed-word pair and sleep.

In the parallel corpus, both sides should be more similar by definition. How-ever, the word entries in HowNet were extracted from Mandarin Chinese cor-pora, which differs enough from the style of Mandarin Chinese used in Hong
Kong that many words from HowNet do not exist in the Hong Kong News cor-pus. This effect of this difference can be seen in the dotplot for the Hong Kong
News Corpus (Figure 6), where there is no clear diagonal. This dotplot seems closer to the comparable corpora dotplot (Figure 7) than to the parallel corpora dotplot obtained for French X  X nglish. Linear regression analysis nevertheless shows a clearer difference between the Hong Kong News parallel corpus and the comparable corpus.

In order to confirm that the difference in word usage between the Hong Kong corpus and the mainland Chinese HowNet is a key factor, we use our corpora comparison method to compare (1) the English side of the Hong Kong news corpus with the Wall Street Journak corpus (Figure 9), and (2) the Chinese side of the Hong Kong news corpus with the People X  X  Daily (Figure 10.) The linear regression analysis results show that the English side of the Hong Kong news corpus is much closer to the Wall Street Journal than the Chinese side of the Hong Kong corpus to the People X  X  Daily.

This, added to the relatively smaller size of the corpus, creates a serious sparse-data problem. The context vectors are very sparse and result in small or null similarity scores, even for words that are translations of each other. Apart for some common words in the glosses, the frequency of cooccurrence of the words with the seed words results tend to be small. As a result, the similarity scores are very close for all the word pairs: the average similarity for all the match method, such as the one used in the baseline.

The choice of the corpora is, therefore, crucial. Unrelated texts that use vo-cabulary close to the ontology are more useful than roughly parallel texts. Given that there is no better parallel corpus currently available, the nonparallel cor-pus allows successful alignment of the WordNet and HowNet nodes without any annotated data. 8. CONCLUSION
We proposed a bilingual corpus-based method to address the problem of word sense alignment across languages. We demonstrate its effectiveness by perform-ing a partial mapping of HowNet and WordNet, two very different ontologies from very different languages. Moreover, we propose a methodology to measure the distance between corpora in two different languages and choose appropri-ate seed words. The results show that nonparallel texts that are well adapted to the language of the ontologies are more useful than parallel texts, when large and clean parallel corpora of the right domain are not available.

This represents a first step toward a corpus-based approach for cross-lingual identification of word concepts and alignment of ontologies. The method bor-rows from techniques used in machine translation and information retrieval, and does not make any assumptions about the structure of the ontology, or use any but the most basic structural information. Therefore, it is capable of per-forming alignments across ontologies of vastly different structure. In addition, our method does not require the use of parallel or even comparable corpora, making the task of data acquisition far easier. 9. FUTURE WORK 9.1 Refining the Alignment The first direction for future work is to refine the alignment obtained. A better
HowNet to WordNet mapping can be obtained if we do not impose the con-straint of a one-to-one mapping. Thresholding can be used in conjunction with local structural information (surrounding hypernyms and hyponyms) in order to align a definition to a set of synsets or to group several synsets into a single hypernym. 9.2 Large-Scale Evaluation on a Translation Task
After a better mapping is obtained, the next step is to evaluate the quality of the bilingual resource created on a translation task. Comparing the results obtained with the WordNet-HowNet resource and with a bilingual dictionary for a Cross-Lingual Information Retrieval or machine translation task would be a better evaluation of the quality of the mapping obtained. 9.3 Ontology Expansion
Eventually, both HowNet and WordNet can be expanded: new words can be added to increase their coverage relation between nodes in one language can be used to infer relation in the other language, thus enriching the structure of each ontology.

