 The location pro les of social media users are valuable for various applications, such as marketing and real-world anal-ysis. As most users do not disclose their home locations, the problem of inferring home locations has been well stud-ied in recent years. In fact, most existing methods perform batch inference using static (i.e., pre-stored) social media contents. However, social media contents are generated and delivered in real-time as social streams . In this situation, it is important to continuously update current inference results based on the newly arriving contents to improve the results over time. Moreover, it is effective for location inference to use the spatiotemporal correlation between contents and lo-cations. The main idea of this paper is that we can infer the locations of users who simultaneously post about a local event (e.g., earthquakes) . Hence, in this paper, we propose an online location inference method over social streams that exploits the spatiotemporal correlation, achieving 1) contin-uous updates with low computational and storage costs, and 2) better inference accuracy than that of existing methods. The experimental results using a Twitter dataset show that our method reduces the inference error to less than 68% of existing methods. The results also show that the proposed method can update inference results in constant time re-gardless of the amount of accumulated contents.
 H.2.8 [ Database Management ]: Database Applications| Data Mining ; H.3.5 [ Information Storage and Retrieval ]: On-line Information Services| Web-based services online location inference; spatiotemporal; social streams Figure 1: Geographical distributions of tweets con-taining the word \earthquake" (a) in ordinary times, and (b) after the earthquake at Hiroshima prefec-ture. Each circle represents the ratio of the number of tweets posted there. The distribution of the word occurrence changes over time.
The location pro les of social media users have become an important information source for analyzing social me-dia. For example, real-world event detection [24], disaster-analysis [26], and modeling outbreaks [19] require home lo-cations of individual users to identify the locations of these phenomena being analyzed. However, location pro les are not available for many users; it has been reported that many users do not disclose their home locations for various reasons (e.g., privacy) [9]. Hence, it is quite useful for many appli-cations if we can infer home locations of social media users.
For the last few years, the home location inference prob-lem has been well studied. Most existing methods can be categorized as either a graph-based or a content-based ap-proach. The graph-based approach [2, 11, 22] is based on the structure of social graphs where friends are connected. In contrast, the content-based approach [6, 8, 9] leverages user-generated contents in the form of texts. Social media contents are generated in real-time, which is called social streams . Such dynamism is one of the remarkable features of social media. However, we would say that most of the existing content-based methods do not fully utilize such dy-namic characteristics, because they typically assume that cont ents are stored in advance and do not change. Hence, we focus on the dynamic characteristics of contents, which poses the following two major research issues.
 Continuous updates. Since social media contents are con-tinuously generated and delivered with high rate, it is de-sirable if we can keep updating the inference results. For example, suppose that a user has generated some contents about Massachusetts and the location inference result of this user is Massachusetts. If this user begins to post about the Red Sox, then the inference result can be updated to Boston. However, updating inference results based on social streams has two main challenges: computational and storage costs. Prior studies propose batch inference methods, which re-quire the entire inference process to be repeated using all the old content as well as the newly arriving content when updating the results. Obviously, these methods suffer from unacceptable computational costs to deal with frequently ar-riving social media contents. In addition, storage costs also become large because existing methods need to store all the old contents for future updates.
 Spatiotemporal correlation. It is reported that social media users tend to simultaneously post when they expe-rience a local event in the real world [24], which leads to the spatiotemporal correlation between contents and loca-tions. Figure 1 shows the geographical distribution of occur-rences of the word "earthquake"in Twitter (see Section 5 for dataset details). We can see that (a) the distribution does not correlate to any locations at ordinary times (i.e., the dis-tribution is similar to the population distribution of Japan), while (b) the distribution appears to have a strong correla-tion to Hiroshima prefecture after an earthquake happened there, which indicates that the word \earthquake" is spa-tiotemporally correlated with Hiroshima prefecture. This correlation tells us that a user who uses the word \earth-quake" right after the earthquake at Hiroshima is likely to live there. In fact, existing methods use only static spa-tial correlation between contents and locations (e.g., place-names and cuisines); in other words, existing methods do not exploit spatiotemporal correlation.

In this paper we devise an online location inference method (OLIM), which can update inference results using only newly arriving contents without using previous contents. Con-cretely, our method is based on Bayesian inference over Dirichlet-multinomial (Polya) distribution [18], which is widely adopted distribution to model maltivariate discrete random variables from the Bayesian point of view. Consequently, we can perform online updates in constant time regardless of the amount of accumulated contents, and the amount of storage size stays constant. Moreover, we extend the de nition of local words [6], which are the words that have a static corre-lation with a speci c location. Based on our new de nition of local words, we can exploit both traditional static correla-tion and spatiotemporal correlation being introduced, which results in higher accuracy than that of existing methods. The following summarizes the contributions in this paper:
More concretely, the experimental results show that OLIM achieves a 28km median error distance, which means that the error distance is reduced to less than 68% of existing methods. Our method is also theoretically and experimen-tally shown to be more efficient in computational time than existing content-based methods. Additionally, the results demonstrate that OLIM can identify and exploit novel temporally-local words that have spatiotemporal correlations to speci c locations, in addition to traditional static local words.
The rest of this paper is organized as follows: Section 2 overviews studies on user location inference. Section 3 de nes the terminology and states the problem being ad-dressed in this paper. The proposed method is described in Section 4, and is experimentally evaluated in Section 5. Finally, Section 6 concludes the paper.
A lot of research has investigated the problem of user lo-cation inference in social media. This section brie y sur-veys the three types of existing location inference methods, namely, content-based, graph-based, and hybrid approach. Content-based approaches. The content-based approach leverages user generated contents. The idea of this approach is that users are likely to post contents about their residen-tial areas more frequently than other areas. For example, it is natural to assume that a user who lives in Boston is more likely to post about the Red Sox.

The content-based approach has its roots in Cheng et al. [6]. They introduced a concept of local words and used them to infer home locations of Twitter users. Local words are those that have strong correlation with a speci c loca-tion. For example, it is reported that the word \ rockets " is local because it is frequently posted by users living in Hous-ton [6]. With this local word, Cheng et al. inferred that users who tend to use it live in Houston.

There have appeared various types of content-based meth-ods other than the one by Cheng et al. Eisenshtein et al. [8] and Hong et al. [10] proposed inference methods based on topic models. Yuan et al. [28] also proposed a topic model to infer W 4 (Who, Where, When, and What) aspects of social media users. Kinsella et al. [12], Chandra et al. [3], and Chang et al. [4] extended Cheng et al.'s method [6] in several aspects. The inference method by Hecht et al. [9] adopted Naive Bayes Classi er [16]. Schulz et al. [25] re-ported that various indicators such as home pages of users and time zones are effective to infer home locations. Pontes et al. [20] leveraged the check-in history, GPS tags, and other attributes to infer home locations.

This paper differs from these existing content-based meth-ods in that 1) existing methods do not have the scheme of online updating, and 2) existing methods do not consider the spatiotemporal correlation.
 Graph-based approaches. Graph-based approaches an-alyze social graphs that home locations of connected users Figure 2: Sliding time window. The window slides toward the next word occurrence. in social graphs are likely to be close to each other. The graph-based approach has its roots in Backstrom et al. [2]. Their method maximizes the likelihood of generating the Facebook social graph assuming that short-distance edges have a larger likelihood than long-distance ones.
After Backstrom et al., varieties of graph-based methods have been proposed. The method by Clodoveu et al. [7] is based on the majority vote of friends' home locations. Abrol et al. [1] proposed TweetHood, which recursively in-fers friends' home locations. Jurgens [11] also developed a recursive inference method based on the label propagation method [5]. Rout et al. [22] takes into account the popula-tions of locations. McGee et al. [17] classi ed informative friends that live close to the target user and others. Yam-aguchi et al. [27] utilized users who attract a lot of local at-tentions for location inference. Sadilek et al. [23] addressed the integrated problem of inferring Twitter users' trajecto-ries and predicting links in the Twitter social graph. Our method differs from these studies in that it does not use social graphs.
 Hybrid approaches. Hybrid approaches, which use both social media contents and social graphs, are proposed by Li et al. [15, 14]. They rst proposed the uni ed discriminative in uence model ( UDI ) [15], which models user-relationships and venue names extracted from contents as a heterogeneous graph assuming that each node (i.e., user or venue) has its own in uence scope. Their method considers that nodes with a large in uence scope (e.g., Lady Gaga) do not pro-vide good clues to infer location because numerous users in diverse locations follow them.

Li et al. proposed another model [14], which assigns mul-tiple locations for a single user (e.g., home, work, and former home locations). These two hybrid approaches use only to-ponyms extracted by Gazetteer, meaning that local words other than toponyms are not considered. User accounts and locations. Each user account u 2 U has its own home location l u 2 L . The set of users is composed of two types of components U = U L [ U N where U
L is a set of labeled users whose home locations are known, while U N is a set of unlabeled users whose home locations are unavailable.
 Social stream and time window. Each post p is de-ned via three elements: timestamp p s , text p t , and user p , and is denoted as p = ( p s ; p t ; p u ). We assume that posts continuously arrive from social stream SS in chronological order. For each word w , we de ne the time window T ( N ) Time window T ( N ) w is the sliding window with the length of N that is a parameter specifying the number of occurrences of word w from labeled users as shown in Figure 2. In this gure, N is set to 5, and it slides toward the next word occurrence. Hereafter, for the simplicity of description, we omit ( N ) from T ( N ) w if there is no ambiguity. The de nition of window length by the number of words N has several ad-vantages for location inference using social streams as we discuss in the later sections.

Using the notations de ned so far, our problem can be stated as follows:
Problem 1 (Online Location Inference). Each time post p is observed from user u , infer the home location ^ of u based on p and the previous inference result ^ l u ( k 1) that the distance between ^ l u ( k ) and the true home location l is reduced compared to the previous result.
 Note that since an unlabeled user's true home location l u unavailable, it is not guaranteed that the distance between the inferred home location and the true home location will decrease in the next inference. Population distribution. We de ne the population distri-bution R where each probability value R ( l ) at location l denotes the ratio of labeled users whose home locations are l . For example, in the U.S., the population distribution has high values at metropolises like New York and Chicago. Word distribution. Each word w is assigned the word distribution Q w where each probability value Q w ( l ) denotes the ratio of the number of occurrences of w at l in the current time window T w . Precisely, the occurrence of word w at l means that a user who lives in l posts p that contains word w . For example, the word distribution of \RedSox" has a high value at Boston because the word is likely to be posted at Boston frequently. Note that the word distribution Q w changes over time as the corresponding time window T w slides.
 Local word. Intuitively, the word distributions of regular words tend to be similar to the population distribution be-cause all users are equally likely to post the words. On the other hand, local words have a strong correlation to a speci c location, so their word distributions are expected to differ from the population distribution. Furthermore, according to Figure 1, word distributions change over time, meaning that the set of local words varies at different times. Based on this observation, we de ne the local word as follows:
Definition 1 (Local word). A local word w is a word that satis es the following equation.
 wher e d min is a prede ned parameter.
 Note that we determine whether or not word w is local when-ever w is observed. We adopt KL divergence because it is a stan dard measure to calculate the difference between two probability distributions. This de nition of local words is a natural extension of the traditional static local word. If we use unlimited length of time windows (i.e., N ! 1 ), the above de nition is equal to the traditional one that does not consider temporal features.

Furthermore, by adopting KL divergence as the measure of locality, our algorithm can identify local words with multi-modal word distribution (i.e., the distribution with multiple peaks). Clearly, for example, the word \rain" has multi-modal distribution even if it is local. The de nition of local words of [6] that uses the focus of the word distribution can-not extract this kind of local words.

According to the above de nition of local words, we obtain the sequence of KL divergence values for each word. Namely, the value of KL divergence is calculated each time the word occurs. Figure 3 shows an example of the sequence of KL divergence values of the word\earthquake,"where the x-axis denotes the ow of time from left to right, and the y-axis de-notes the value of KL divrergence at the corresponding time point. We used a Twitter dataset described in Section 5 and the updating scheme described in Section 4.1. We observe that the word \earthquake" sometimes shows high values, and these bursts indeed correspond to the real earthquakes in Japan, which indicates a lot of users close to the earth-quake generate contents about it when it happens. Further discussions are described in Section 5.3.
This section describes our proposed method, called OLIM (Online Location Inference Method). OLIM can perform online inference with lower computational and storage costs than those of existing methods under the situation where posts continuously arrive from the social stream. Online inference maintains user distributions for all unlabeled users by continuously inferring and updating them based solely on the newly arriving contents. User u 's home location can be inferred based on the user distribution of u by choosing the location with the largest probability value.

Our algorithm treats the geographical space as a discrete set of locations L rather than numerical coordinates of lat-itude and longitude. In other words, each unlabeled user will be assigned the most likely location label l 2 L . To con-struct L , we partition the geographical space by quad-tree partitioning as used in [13] in the preprocessing step. Quad-tree partitioning is suitable for location inference because populated areas, which requires ne grained inference, are divided into small cells (i.e., location), whereas depopulated areas are divided into large cells. Note that the number of locations affects the inference accuracy, which is experimen-tally examined in Section 5.

Algorithm 1 shows the algorithm of OLIM, which receives social stream SS , a set of users U = U L \ U N , the length of time window N , the threshold value d min , and the set of locations L . OLIM st constructs population distribution based on home locations of labeled users in line 1. Con-cretely, probability mass of population distribution at loca-tion l is calculated as R ( l ) = jf u 2 U L j l u = l gj
Every time post p arrives from social stream SS , OLIM performs the steps from line 3 to 8. If user u of p is a labeled user, OLIM updates values of KL divergence for each word in the text of p using u 's home location l u (Section 4.1). The maintained values of KL divergence are used to determine Algori thm 1 OLIM Input: SS , U , N , d min , L 1 : R calcP opulation ( U L ; L ) 2: for post p from SS do 3: u user ( p ) 4: if u 2 U L then 5: updateKL( u; p; N; R ) 6: else if u 2 U N then 7: updateUD( u; p; d min ; L ) 8: end if 9: end for Figure 3: Sequence of KL divergence of \earth-quake." This word sometimes shows high values, which indicates that earthquakes happen at corre-sponding time points. whether or not the corresponding word is local. On the other hand, if user u is an unlabeled user, OLIM updates the user distribution of u based on the local words in the text of p (Section 4.2). In the following subsections, we describe the details of each step.
By the de nition of local words (Eqn. (1)), determining whether or not word w is local requires the KL divergence between the word distribution of w and the population dis-tribution. Since the time complexity of calculating KL di-vergence is O ( j L j ), recalculating it whenever word w is ob-served is computationally expensive. We therefore update KL divergence by the following scheme, which requires O (1).
Suppose that word w i is observed at location l k , and the oldest observation of w i in the previous time window was at l where n ik is th e number of observations of w i at l k in the previous time window. Proof is omited as the above updat-ing equation can be derived by simple transformation. Note that the word distribution Q w is calculated using n ik value as Q w i ( l k ) = n ik =N .

Here we discuss why we de ne the window length by the number of word occurrence N . The rst reason is that the Algo rithm 2 updateKL() Input: u , p , N , R 1: for word w i in p do 2: l k = getUserLocation( u ) 3: window[ w i ].enqueue( l k ) 4: if window[ w i ].length == N then 6: d i KL ( Q w i jj R ) 7: else if window[ w i ].length &gt; N then 8: l k  X  = window[ w i ].dequeue() 9: d i d i +  X ( i; k; k  X  ) 11: end if 12: end for Algo rithm 3 updateUD() Input: u , p , d min , L 1: for word w i in p do 2: if d i &gt; d min then 3: for location l k in L do 5: end for 6: end if 7: end for abo ve updating scheme holds only if the window length is xed to N . If the window length N varies over time, we have to recalculate the value of KL divergence by de nition, lead-ing to the time complexity of O ( j L j ). The second reason is that the xed length of window guarantees that a sufficient number of word occurrences are in the window, which results in reliable values of KL divergence. Although one may think that too long window length in terms of time duration for rare words can degrade the inference accuracy, it is not the case. Suppose that word w is rare but local (e.g., less-known location names), our algorithm can reliably extract such a local word with a sufficient number of word occurrences. In contrast, if the window length is xed by time duration (e.g., 6 hours), non-local words with insufficient occurrences may happen to be extracted as local by unreliable KL divergence value. By our preliminary study, we observe that the thresh-old to avoid too long window length (i.e., discard words with insufficient occurrences) results in the low accuracy.
Algorithm 2 shows the procedure of updating KL diver-gence. Lines 2 through 12 update the KL divergence for each word contained in the text of observed post p . window is the dictionary of queues where each queue window [ w ] stores locations where w is observed, chronologically. Note that queues are initialized as empty.
 When the length of the queue window [ w i ] rst reaches N , OLIM calculates the KL divergence based on the def-inition, which is stored as d i (lines 4 through 6). On the other hand, for the word whose KL divergence is already calculated, OLIM updates the value by Eqn. (2).
Every time post p is observed from an unlabeled user u , OLIM updates the user distribution of u based on local words contained in the text of p . OLIM infers home loca-tions by Bayesian inference based on Dirichlet-multinomial (Polya) distribution [18] because of the following reasons. First, it is natural to model user distributions as the multino-mial distribution over the set of locations L , where the prob-ability at location l means the probability that the user's home location is l . Since there may not be sufficient clues to infer user home locations (especially in the initial inference stage), it is also natural to adopt Bayesian inference with Dirichlet distribution as prior, which results in Dirichlet-multinomial. Second, an online updating scheme of prob-ability distribution, which is our goal, can be achieved by Bayesian inference.

Let z ij be the categorical random variable with j L j cat-egories that denotes the location of the j th occurrence of local word w i in the current time window. For i.i.d categor-ical variables Z i = ( z i 1 ; z i 2 ; ; z iN ) corresponding to the current time window of local word w i , the marginal joint distribution is obtained by the Dirichlet-multinomial (Polya) distribution as: where is the gamma function, n ik is the number of z ij 's with the value k 1 , and A = bution P ( Z i j ) is the categorical multinomial distribution with parameter , and P ( j ) is Dirichlet distribution with parameter .

Eqn. (3) denotes the probability of generating local word w i at a speci c time point from the geographical point of view. By integrating out the parameter , we consider all the possible value of the parameter to calculate the prob-ability of generate w i , instead of simply using MLE or MAP estimation of the multinomial distribution.

Here, let z u be also the categorical random variable with j
L j categories that denotes the home location of user u . Sup-pose that unlabeled user u posts p that contains a local word w , we can write the posterior predictive distribution of u 's home location as:
P ( z u = k  X  j Z i ; ) = P ( z u = k which means that u 's home location can be simply inferred by counting the number of z ij 's with the value k and adding it to k for all k . This predictive distribution is based on the assumption that a user is likely to live close to the local words the user used.

After the observation of w i , suppose that u further posts p containing another local word w i  X  , we can rewrite the distribution as: dently.
It is the same value of n ik in the previous section.
From the above discussion, we can infer the home location of user u after u posts local words w 1 ; ; w m as: which indicates that all we have to do to infer ^ l u is main-taining the value of
Algorithm 3 shows the procedure of updating user distri-butions. B uk maintains the sum of n ik for each u in Eqn. (4). For each word w i in the text of post p , this algorithm determines whether w i is a local word or not by comparing the value of KL divergence d i and the threshold d min . If w is a local word, the algorithm adds the value of n ik to the maintained variable B uk for all k . Before OLIM starts run-ning, B uk is initialized as 0 for all u and k . We can obtain the current inference result whenever we want by Eqn. (4).
Table 1 shows time complexities of our method and ma-jor existing methods. The rst three methods including our method are content-based methods, the next three meth-ods are graph-based methods, and the last one is the hybrid method that uses both posts and the social graph. P is the set of accumulated posts, k is the number of neighbors on the social graph, and E is set of edges on Li et al.'s heterogeneous graph [15]. For content-based methods in-cluding our method, time complexities shown in the table are required for each update. We can see that the complex-ity of our method depends only on the number of locations j
L j , indicating that it is in constant time regardless of the amount of accumulated posts and the number of users. On the other hand, existing content-based methods must repeat the whole inference process by using all the old contents, which requires O ( j U jj P jj L j ). These points are experimen-tally examined in Section 5.

The complexities of graph-based and hybrid approaches depend on the size of the graph. The graph-based methods all depend on the square of the number of neighbors, which results in huge computational cost if the graph has hubs (i.e., nodes with a large number of neighbors). Indeed, social graphs have hubs because the degree distributions of those graphs follow the power law. The hybrid approach does not require high computational cost if the graph is sparse.
We would note that our method proposed in this paper is purely based on the word occurrence in user-generated contents. This simple scheme, as we will see later, achieves signi cant improvement over existing inference methods in-cluding graph-based and content-based ones. However, one may think that taking other information (e.g., check-in his-tory or graph structure) into account or incorporating some NLP techniques can improve the inference accuracy. Here we discuss how to incorporate such information or NLP tech-niques into our method, which can be taken into account naturally. Evaluating the possible extensions discussed be-low is our future work.
 Other information. OLIM can be divided into two parts: identifying local words and exploiting them for user location inference. The rst part is necessary because most of words Metho d Time comp lexity posts graph OLIM (Pro posed) O ( j L j )  X  Cheng [6] O ( j U jj P jj L j )  X  Hech t [9] O ( j U jj P jj L j )  X  Bac kstrom [2] O ( j U j k 2 )  X  Jurgen s [11] O ( j U j k 2 )  X  LMM [27 ] O ( j U j k 2 )  X 
UDI [15 ] O ( j E j )  X   X  are no t local and cannot be used for user location inference. On the other hand, check-in history, which clearly shows the locations of users, can be directly utilized for user location inference. Concretely, when we observe user u checks-in the venue close to the location l k , we can update u 's location distribution by incrementing the value B uk (see Algorithm 3). However, we notice that user check-ins do not always in-dicate the home location , meaning that the check-in history may degrade the accuracy of home location inference.
Social graphs can also be incorporated naturally into our method. The simplest way to take it into account is to bring together posts of a user and those of its neighbors, which is based on the assumption that user u 's neighbors (i.e., followers or followees) are also likely to post about u 's home location. This is potentially effective against the data sparsity problem, in other words, even if a user does not post about his/her location, we can still infer the location based on the posts of its neighbors.
 NLP techniques. One may think that extracting loca-tion names or landmark names by named entity recognition (NER) [21] techniques improves our method. However, it is not the case, because, as discussed later in Section 5.3, there exist location names that are not local. For exam-ple, in Japan, a lot of users mention the popular city name Tokyo , which makes that word non-local (see Figure 5).
We should also note that synonyms and homonyms of-ten degrade the text-based methods. In this sense, word sense disambiguation techniques or topic models may im-prove the inference accuracy. Those techniques and our in-ference method are orthogonal because the results of those techniques can be utilized for our method without any mod-i cation. Moreover, according to the experimental results, OLIM achieves high inference accuracy without these tech-niques, which indicates that our method can handle syn-onyms and homonyms by ltering them out if these words are mentioned from distant locations. We conducted the experiments listed below: Datasets and evaluation metrics used in the experiments are detailed in Section 5.1. Data. We randomly collected Twitter users who satisfy (1) the location pro le text, which describes the home location, can be geocoded into coordinates of latitude and longitude by using Yahoo! Geocoder 2 , (2) the geocoded coordinate is in Japan, and (3) with at least 200 tweets.
 As a result, we obtained 201,570 labeled Twitter users in Japan. In addition, we also collected the latest 200 tweets of each collected user (40,314,000 tweets in total), and 33,569,924 follow edges among these users by using Twitter API. Nouns were extracted from the text of each tweet by using MeCab which is a widely used Japanese morphological analyzer. Geocoded coordinates are used as true home locations l u . The number of distinct locations in the dataset is 11,142, which is reduced to j L j by quad-tree partitioning. We di-vided users in the dataset into the training set (90%), the validation set (5%), and the test set (5%). Users in the val-idation set and the test set, which are treated as unlabeled users, are used for the parameter validation (Section 5.6) and accuracy evaluation, respectively. Although misreports of location pro les may degrade the location inference, Ju-rgens [11] has experimentally demonstrated that users tend to report accurate information. Thus, we assumed that the users' location pro les show their true home locations. Evaluation metrics. The results were evaluated based on the error distance , which is the distance between the cen-ter point of inferred location cell ^ l u and the true location l . Concretely, we used Median Error Distance and Pre-cision (the ratio of users whose error distance is less than 100 miles (about 160km)), which is widely used [6, 15]. Details of the proposed method. Parameters of OLIM are set to N = 400, d min = 1 : 0, and j L j = 211, which are experimentally determined using the validation set in Section 5.6. Note that j L j is obtained by quad-tree parti-tioning where cells with more than 3,000 data points (i.e., labeled users) are partitioned. Hyperparameter of Dirichlet distribution is set to k = R ( l k ), which enables reasonable inference for users with no observation based on the prior. Our code and data are available at https://github.com/ yamaguchiyuto/olim .
In this experiment, we compared our proposed method with the six existing methods listed in Table 1 in terms of inference accuracy. None of the existing methods consider temporal features.

Figure 4(a) shows the accumulative precision at various error distances, where the x-axis denotes the error distance and the y-axis denotes the accumulative precision (i.e., ratio http ://developer.yahoo.co.jp/webapi/map/ openlocalplatform/v1/geocoder.html http://code.google.com/p/mecab/ Metho d Media n ED (m) Precisio n Covera ge OLIM 28,507 0.663 1.000 Cheng [6] 200, 851 0.47 8 0.96 0 Hech t [9] 212, 333 0.46 1 1.000 Bac kstrom [2] 41,9 46 0.57 1 0.98 1 Jurgen s [11] 91,3 63 0.56 0 0.98 4 LMM [27 ] 89,8 81 0.61 1 0.92 9
UDI [15 ] 138, 653 0.55 6 0.99 9 of test us ers whose error distances are lower than x ). Ac-cording to this gure, our proposed method outperformed all compared existing methods at any error distance, mean-ing that OLIM correctly placed more users with small error distances than any of the other methods. Table 2 summa-rizes the results. We can see that OLIM achieves the best results for all metrics. Coverage is the ratio of inferred test users regardless of the inference result. OLIM shows 1.0 cov-erage because it can infer home locations of users using the prior distribution even if they have no observations. Note that in the case of not using prior prediction, OLIM also shows a high coverage (about 0.98). OLIM successfully re-duced the median error distance of the second best method of Backstrom to about 68%.

UDI, which is the state-of-the-art method, shows low ac-curacy although it achieves good results in the original pa-per [15]. The reason of this is thought to be the highly biased population distribution of Japan, where majority of labeled users live in Tokyo, the capital of Japan. UDI adopts the concept of maximizing the likelihood of the whole graph , which prefers most unlabeled users to live around Tokyo al-though some of them live far from Tokyo. On the other hand, OLIM and Backstrom consider the population distri-bution, leading to avoiding such an inference.
In this experiment, we investigated sequences of KL di-vergence values of various kinds of words over time. Figure 5 shows the results of some representative words. The x-axis indicates the elapsed time, and the y-axis indicates the value of KL divergence at the corresponding time point.
We found that words can be categorized as local (a and b), non-local (c and d), and temporally-local words (e to h) by their different types of KL sequences. Examples of local words include Tsukuba and San-nomiya, which are city names in Japan. This type of words shows high values of KL divergence at any time point. These words are also used by existing content-based methods for location inference. \Tokyo" and \Friends" are examples of non-local words. \Friends" is a obvious non-local word, while \Tokyo" is also a non-local although it is a city name. Popular city names are frequently mentioned by many users from various loca-tions [15], which results in low values of KL divergence. Examples of temporally-local words include Thunder, Rain, Fireworks, and Hokkaido. We can see that the rst three words show high values of KL divergence occasionally, while the last word shows low values occasionally. Such bursty be-haviors are considered to be triggered by real-world events the words describe. For example, when it thunders, many people around it tend to post about it, which results in high values. In contrast, interestingly, although the city name location inference by existing content-based methods. \Hokkaido"is generally a local word, its KL divergence drops when the popular news about the city is mentioned from all over Japan. This type of behaviors cannot be identi ed by existing methods that do not use spatiotemporal correla-tions. For that reason, our method outperformed existing content-based methods in terms of accuracy.

Although our focus in this paper is the location inference, extracted local words themselves are interesting. We are also interested in the local words in other cultures or countries not limited to Japan. If a real-world event happens and peo-ple around the event post about it, temporally-local words are also expected to be extracted in other countries. Further experiments using other datasets are our major future work.
This experiment examined the ability of OLIM to reduce the inference error over time. All tweets in the dataset were considered as a pseudo social stream, and OLIM continu-ously receives tweets from it chronologically. Each time 1% of tweets (about 400,000 tweets) were processed, OLIM pre-dicted the home locations of all test users by Eqn. (4).
We compared the full version of OLIM and a different ver-sion of OLIM, called OLIM-P. OLIM-P rst extracts static local words (at least with N occurrences) in the prepro-cessing step by using all posts in the dataset. OLIM-P then runs Algorithm 1 without performing updateKL be-cause values of KL divergence are already calculated for all words. This method cannot leverage temporally-local words because temporal features of words cannot be ex-ploited without taking time windows into account.
Figure 4(b) shows the results, where the x-axis denotes the percentage of tweets processed, and the y-axis shows the values of median error distance. Error bars denote the 95% con dence intervals. The results show that both variants of OLIM reduce the error distance as new tweets arrive. However, the full version of OLIM, which considers time windows, shows a greater improvement in reducing the error distance, suggesting that leveraging temporally-local words is effective for location inference.
We compared computational costs of content-based meth-ods including our method. Graph-based methods are not compared in this experiment because they cannot update the inference results, and because their time complexities do not depend on the amount of tweets being used. In this ex-periment, each method predicted home locations every time 1% of tweets were processed similar to the previous section. We measured the computational time required for each up-date. Note that existing content-based methods must repeat the whole inference process using all the contents to up-date the results every time 1% of tweets arrive. Figure 4(c) shows the results, where the x-axis denotes the percentage of tweets processed and the y-axis shows the computational time. OLIM-R is our method but recalculates KL divergence for each word every time the word occurs (i.e., does not use updating scheme of Eqn. (2)).

The results show that computational time of three vari-ants of our methods do not increase except for the initial part, meaning that our method can update the results in constant time regardless of accumulated posts. In the ini-tial part, many words reach N occurrence for the rst time, which requires the initial calculation of KL divergence, lead-ing to the initial increase of computational time. On the other hand, computational time of Cheng and Hecht linearly increases as the amount of accumulated tweets increases. These results con rm the discussion of time complexities of the content-based methods in Section 4.3. Furthermore, in addition to the efficiency of OLIM in updating the results, it can also efficiently update the model (i.e., set of local words) by considering the spatiotemporal correlations. In contrast, Cheng and Hecht do not update the models because they do not consider the dynamic feature of social streams. OLIM can successfully reduce the computational time of OLIM-R, which means that the updating scheme of KL divergence is efficient. Note that the inference results of these two variants are identical. Moreover, OLIM-P shows smaller computational time than the other two variants be-cause OLIM-P calculates KL divergence values of all words in preprocessing and therefore does not perform updateKL . Regarding the storage cost, in contrast to Cheng and Hecht, our method can discard old contents because it does not need them to update the results. Therefore, the storage cost of our method does not increase over time.
This section describes the parameter validation of window length N , threshold value d min , and the number of locations j
L j partitioned by the quad-tree. In this experiments, we used the validation user set. Figure 6 shows the results. The following paragraphs discuss the effect of each parameter.
Figure 6(a) shows the result of validating d min , where the x-axis indicates the value of d min , and the y-axis indicates the Median Error Distance and the Precision. Error bars denote the 95% con dence intervals. We can see that too small d min degrades the accuracy because words with small KL divergence values are used as local words. On the other hand, too large d min also shows poor results because the number of extracted local words becomes too small, leading to the number of users with no observation increased.
Figure 6(b) shows the result of validating N value. With a small N , non-local words may accidentally be used as lo-cal words, leading to the lower accuracy. A too large N also results in lower accuracy because OLIM is no longer able to identify spatiotemporal correlation. For example, the word \Thunder" cannot be identi ed as local with too large N be-cause the word shows high KL divergence quite temporally.
As mentioned in Section 4.1, we also tried to assign the threshold of window length in terms of time duration (e.g., 6 hours) because too long time window may have the bad effect on inference. However, contrary to our prediction, assigning the window length threshold resulted in poor ac-curacy. This is because of the existence of rare but local words with long window length (e.g., less-popular location names), which would be discarded if we assign the window length threshold.

On the other hand, we would note that there exists the burst-and-lean effect for our de nition of window length. For example, suppose that a tornado hits a place and many users around it start to post about it, which makes the word \tor-nado" local in the current window, and after that, no one posts about it. The word \tornado" will stay local for a long time because the time window of it will not slide without further word occurrences. In this case, if someone far from it happen to post the word, say, one week after, this \local" word causes the wrong inference. Although we observed very few cases, it may result in the more accurate inference if we can suppress this effect.

Figures 6(c) and 6(d) show the accuracy and the cost for different number of partitions by quad-tree. We obtain each value of j L j by quad-tree where cells with more than 500, 1,000, 1,500, 2,000, 3,000, 4,000, 5,000, 10,000 data points are partitioned. We can see that too small j L j results in low accuracy, which means that too coarse partition cannot accomplish good inference. On the other hand, too large j also shows low accuracy, suggesting that overly ne-grained partition goes into over tting without sufficient number of observations of local words. Figure 6(d) indicates that the computational time is almost linearly proportional to j L which con rms the time complexity of OLIM.
This paper tackles the problem of online location infer-ence in social media where contents are generated and de-livered in real-time. Our proposed method, named OLIM, can perform online inference based solely on the newly ar-riving content, which achieves constant computational and storage costs regardless of accumulated contents. In addi-tion , OLIM exploits the spatiotemporal correlation between contents and locations thereby achieving more accurate in-ference than that of existing methods. Experimental results demonstrate that the proposed method outperforms existing methods in terms of accuracy and cost.

Our future work includes the followings. First, exploit-ing both contents and social graphs for online inference is interesting. Propagating observations of local words in so-cial graphs is thought to be effective against the data spar-sity problem. Second, we plan to conduct experiments on the other datasets to further study temporally-local words in the other cultures or countries. Third, our method can potentially predict users' current location with a few modi-cations. We will explore the forgetting factor that assigns less weight to old observations.
 Acknowledgements : This research was partly supported by the program Research and Development on Real World Big Data Integration and Analysis of the Ministry of Ed-ucation, Culture, Sports, Science and Technology, Japan. This work was also supported in part by JSPS KAKENHI, Grant-in-Aid for JSPS Fellows #242322. [1] S. Abrol and L. Khan. Tweethood: Agglomerative [2] L. Backstrom, E. Sun, and C. Marlow. Find me if you [3] S. Chandra, L. Khan, and F. B. Muhaya. Estimating [4] H.-W. Chang, D. Lee, M. Eltaher, and J. Lee. [5] O. Chapelle, B. Sch  X  olkopf, A. Zien, et al. [6] Z. Cheng, J. Caverlee, and K. Lee. You are where you [7] C. A. Davis Jr, G. L. Pappa, D. R. R. de Oliveira, and [8] J. Eisenstein, B. O'Connor, N. A. Smith, and E. P. [9] B. Hecht, L. Hong, B. Suh, and E. H. Chi. Tweets [10] L. Hong, A. Ahmed, S. Gurumurthy, A. J. Smola, and [11] D. Jurgens. That's what friends are for: Inferring [12] S. Kinsella, V. Murdock, and N. O'Hare. "i'm eating a [13] R. Lee and K. Sumiya. Measuring geographical [14] R. Li, S. Wang, and K. C.-C. Chang. Multiple location [15] R. Li, S. Wang, H. Deng, R. Wang, and K. C.-C. [16] A. McCallum, K. Nigam, et al. A comparison of event [17] J. McGee, J. Caverlee, and Z. Cheng. Location [18] T. Minka. Estimating a dirichlet distribution, 2000. [19] M. J. Paul and M. Dredze. You are what you tweet: [20] T. Pontes, G. Magno, M. A. Vasconcelos, A. Gupta, [21] A. Ritter, S. Clark, O. Etzioni, et al. Named entity [22] D. P. Rout, K. Bontcheva, D. Preotiuc-Pietro, and [23] A. Sadilek, H. A. Kautz, and J. P. Bigham. Finding [24] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake [25] A. Schulz, A. Hadjakos, H. Paulheim, J. Nachtwey, [26] S. Vieweg, A. L. Hughes, K. Starbird, and L. Palen. [27] Y. Yamaguchi, T. Amagasa, and H. Kitagawa.
 [28] Q. Yuan, G. Cong, Z. Ma, A. Sun, and
