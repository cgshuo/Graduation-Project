 A crucial problem in Chinese Information Retrieval (IR) is to determine the appropri-ate elements to serve as index. Two general families of approaches have been pro-posed in the literature: using characters (mainly character unigrams and bigrams) and using words. It has been found in several studies that it is beneficial to combine dif-ferent types of index [5, 6, 11]. Indeed, while a word can represent precisely a mean-ing, the meaning can also be expressed by other words and characters. In Chinese, related words often share some characters. Therefore, character unigrams and bigrams can be used as a means to perform partial matching between them. 
However, the previous approaches usually assumed that different types of index are independent. Typically, each type of index is considered as forming a distinct representa-tion space from other types. The typical approach to Chinese IR determines a matching these scores. It is obvious that the assumption of independence between different indexes does not hold in reality. For example, the word or bigram  X   X  (tree planting, forestation) tried to deal with the relationships between different indexes. For example, Shi et al. [13] considers that longer and shorter words, as well as the constituent characters, are strongly strength of relationship. In general IR, several models have also been proposed to capture dependencies between terms [4, 8, 9]. Usually, a dependence model is defined in addition to the traditional bag-of-words or word unigram model. Each of these models is assigned have equal importance in IR. Some dependencies (or word groups) are mandatory to consider because the constituent words without their dependency could mean different separately can represent the same meaning equally well and they can retrieve a similar set of documents. 
The necessity to consider dependencies in Chinese IR is even higher. Indeed, a Chi-words. Characters can be strongly dependent. The dependencies between characters can be more or less useful for IR, depending on whether the meaning can be expressed case for  X  X  X  (house). Indeed, the characters  X  and  X  alone can also express the same meaning. The dependency between them does not provide much additional in-formation to IR. On the other hand, in an expression such as  X  X   X  X  X  (Beijing-Kowloon Railroad), it is important to capture the dependency between  X  (Beijing, capital) and  X  (nine) because these characters are very ambiguous and they would not mean Beijing-Kowloon when considered separately. The dependency between them in this expression helps determine a specific meaning and retrieve documents on a specif-ic topic. Therefore, this dependency is important to capture. 
The above examples illustrate the variab le necessity to take into account the dependency between a pair of Chinese characters according to how strong the de-dependency between  X  and  X  , the dependency should play a limited role. On the other hand, the dependency between  X  and  X  in the second example should be assigned a much higher importance. This aspect has not been considered in the previ-ous IR models. In this paper, we will define a model capable of coping with this prob-lem. We will consider characters as forming the basic index for Chinese IR. Then dependencies between pairs of characters ar e incorporated with variable weights depending on the usefulness of considering the dependency. Several types of depen-dency will be considered: dependency between adjacent characters and dependency between co-occurring characters at different distances. For example, the characters in the word  X  X  X  (railway) form a dependency between adjacent characters. Co-this helps account for the many abbreviations often used in Chinese, which are usu-ally formed by co-occurring, non-adjacent, characters. 
In this paper, we use SVM to determine the appropriate weight of a pair of charac-ters according to a set of features. The experiments on several TREC and NTCIR collections show that our model can significantly outperform the previous indepen-dent models and dependency models using fixed weights for types of index. 
The remaining of the paper is organized as follows. In the next section, we review some related studies on Chinese IR and term dependency models. Then, we will de-scribe our character-based dependency model and our parameter learning method. In Section 4, we present the experiments on TREC and NTCIR collections, and finally conclude our work in Section 5. A number of studies have investigated the effectiveness of Chinese IR using either or both characters and words. It is found that approaches using either characters (bi-grams) or words can lead to comparable re trieval effectiveness [6, 11, 12]. In [14], it is further found that the retrieval effectiveness using a character-based language mod-el is highly competitive to, and on several collections, is even higher than that using words and bigrams. This result motivates our use of characters as our basic index units in this study. 
Within the language modeling framework, given a vocabulary V , the score of a document D to a query Q can be determined according to the following equation [2]: ment D . The vocabulary  X  in the model can be unigrams (U), bigrams (B), words determine a score according to each type of index and then to combine the scores as follows: where  X  denote a type of index, which could be U, B and W. 
We notice that in such a combination, no relationship between different types of argued earlier, this is counterintuitive: some dependencies should be attributed a unambiguously a meaning and to retrieve the required documents, in comparison to characters. 
Term dependency is a general phenomenon present in all the languages. Several at-tempts have been made to define IR models capable of capturing term dependencies. Gao et al. [4] proposed a dependency model in which term dependency is captured by a biterm model. The final model combines the unigram model and the biterm model. Metzler and Croft [8] proposed Markov Random Field (MRF) models for IR, in which considered. In the full dependence model (MRF-FD), all the terms in a sentence (query) are assumed to be dependent. This will lead quickly to the problem of complexity when dependence model (MRF-SD) is used, in which only dependencies between adjacent terms are considered. In addition to unigrams, two types of dependence are considered: (  X   X  , X   X  , X   X  ) in the final score function. 
As we mentioned earlier, dependencies between non adjacent characters are also matching process varies. To deal with the last problem, Bendersky et al. [1] extended recently the MRF-SD model to a weighted MRF-SD model (which we denote by WSD), in which the weight of a term and a pair of terms becomes dependent on the individual term and pair of terms. The scoring function is as follows: feature defined over unigrams or bigrams, and  X   X  is its weight, a free parameter to be estimated. This goes in the same direction as our model, i.e. to assign variable weights to unigrams and pairs of terms. However, the relationship between non-adjacent query terms is still ignored in [1] and the ordered and un-ordered pairs of terms are treated in the same way. Our model will go a step further: we will consider dependencies will also separate ordered and unordered pairs of characters. 3.1 Variable Dependency Model cover dependencies between distant characters. On the other hand, discriminative models have the advantage that one can selectively use a subset of useful dependen-cies as features rather than all the dependencies [10]. Discriminative models have following equation: The model we propose is a discriminative model. We limit the dependencies to pair-wise dependencies, which often correspond to the strongest dependencies that we want to capture. The flexibility of discriminative models allows us to consider depen-dencies between more distant characters, without having to increase the complexity of the model to account for more complex and less useful dependencies. 
The previous experiments show that the LM based on character unigrams works well consider the following types of dependencies: (1) bigram, (2) unordered co-occurring use 3 window sizes: 2, 4, and 8) when we construct document models. The idea of using The ranking function is extended from Equation (1) to the following one: any pair of (not necessarily adjacent) characters in a query. Each feature is associated tion allows us to take into account the vari able dependencies between bigrams and co-occurring characters according to their strength and utility. The discriminative feature functions we use are simply the language model features defined as follows: a window of size w . The features are defined in this way in order to make it easier to compare our model with other approaches using language modeling. However, tone can well define other features. 
For the ranking purpose, we will simply fix  X   X   X  X   X  |  X   X  at the constant 1, and try to vary the other  X  functions for bigrams and co-occurring terms. Putting all together, we have the following final model: This model will be called Variable Dependency Model (VDM). The fundamental difference of our model with most of previous models is that the  X  functions are now dependent on the specific pair of characters. For example, for  X  X  X  X   X  (world trade), consider it in IR. The weight will be learnt using SVM (see Section 3.2). 
For the query models in Equation (2), we will simply use Maximum Likelihood es-and  X   X   X   X   X ;  X  its count in the query: For the document model, Dirichlet smoothing is used:  X  the corresponding type of model; and |  X  |  X  is the document length in the expression of R , i.e. the total number of unigrams, bigrams or co-occurring terms within the corres-ponding window size. 3.2 Parameter Estimation The  X  functions are determined according to the strength and utility of bigrams or co-Vector Machine Regression (  X  -SVR) [15] method to train  X   X   X  X  X  X  . The toolkit LIBSVM 1 is used for this purpose. 
The training examples are obtained as follows. For each training query, we first different windows (  X   X   X  . We use a coordinate-level ascent search algorithm [7] to find a learning example  X  X   X   X ,  X  ). 
In our experiment, we use 10-fold cross-validation method, i.e., 1/10 of the data  X  X  ters in  X  ): the bigram: We have not defined a large set of features because our primary goal of this study is to show the importance to incorporate depe ndencies between characters at variable degrees. The set of features could be easily extended in the future. 4.1 Test Collections We use the collections from TREC and NTCIR. The characteristics of the collections are summarized in Table 1. 
In our experiments, we use topic titles as our queries (see Table 2). This choice is made to better correspond to real queries on search engines. 4.2 Pre-processing and Indexing into GB codes. To compare to the word-based method, we use a word segmentation tool ICTCLAS 2 to segment Chinese texts to words, and use another segmentation program from LDC 3 to further segment long words into short words. For example, the long word  X  X  X   X  X  X  X  X  X  (World Trade Organization) will be further segmented in the second step into its constituent short words:  X  X  X  (World),  X  X  X  (Trade),  X  X  X  (Organization). The previous experiments showed that short words perform better than long words [5]. 
We use Indri 4 to build the index for our model. The basic index units are Chinese characters (which we denote by U). To compare to the baseline models, we also build the indexes for other index units: words (W), bigrams (B), words and bigrams com-bined with unigrams (WU, W+U, BU and B+U). In the combinations WU and BU, all types of index are indexed together, while in W+U and B+U, they are indexed sepa-rately and the scores from each index are combined linearly. 4.3 Experimental Results We first provide the retrieval results of the baseline methods in Table 3. The combina-tion parameters in W+U and B+U are tuned to their best. For a Chinese query  X   X   X   X ...  X  , we assume the word segmentation result to be  X   X   X ,  X   X ...,  X  . The baseline models are listed below: To see the importance of different type of index, we plot the results of the methods B+U and W+U on Trec6 and Ntcir6 collections in Figure 2. We can see that a reason-index (the two extremities of the curves). This shows that different types of index are complementary and it is useful to combine them. However, the best weight for each type of index depends on the collection and on the types of index combined. Indeed, the usefulness of different words and bigrams varies largely. The weight we assign to a type of index corresponds to a compromise among all the words and bigrams. As we weight to a word or a bigram depending on its usefulness. In Table 4, we show the effectiveness with other baselines -MRF-SD and Weighted MRF-SD (WSD). For MRF-SD, we use a grid search to find the best parameters  X   X  , X   X  , X   X  so as to maximize MAP for each collection. Therefore, the effectiveness of this model is tuned to its best. The results with MRF-SD are slightly better than B+U. Indeed, if we remove the unordered part, the MRF-SD becomes identical to B+U. The differen ce between MRF-SD and B+U corresponds to the contribution of unordered unigram pairs. The WSD model is slight better than MRF-SD except on Trec6. However, the differences between the two models are not statistically significant. 
The results with our variable dependency model (VDM) are shown in Table 5. The results show that the VDM model with fixed weights is slightly better than MRF-SD. This is due to the fact that we added non-adjacent co-occurring characters. When we vary the weights of the bigram and the pair of co-occurring characters, the result becomes much better. In general, our model outperforms all the baseline methods except in two cases. Many of the improvements are statistically significant. In comparison to B+U, W+U, MRF-SD and VDM with fixed weights, this result clearly validates the general approach we used in our model. 
Notice that in the above comparison, we ga ve considerable advantage to the base-model. In order to have an idea of the potential of our model, we also show (in the last column) the effectiveness of our model using the best parameters (best weights for each bigram and pair of characters). We can see that our model with the ideal parame-ters can largely outperform the existing models. This shows that the assignment of Chinese IR, which our model captures. The large difference between the ideal VDM and VDM using parameters set by cross-validation also shows that the parameter learning process can be much improved. This is part of our future work. 4.4 Analysis and Discussion model can outperform the other models. We have observed two categories of cases in which our model can increase the retrieval effectiveness. and dependency model, and avoid the disadvantages of them. 2. Our model can capture the dependencies between non-adjacent characters. matching process according to how useful they are. This is the very goal of our model. Chinese is a language basically constructed from characters. Even though words can flexibility in Chinese to express the meaning using different combinations of charac-ters makes it challenging to match a query against documents using similar but differ-ent words. This characteristic of the language is the very reason why a combination of words with characters in Chinese IR has been successful. However, words and cha-racters are not independent. The previous approaches that assume their independence model to take into account the relationships between different types of index. Pairs of according to its strength and usefulness for IR. The assignment of variable weights to pairs of characters has not been investigated in previous studies. It turns out that this is highly beneficial in our experiments. The model we propose in this paper points to an interesting direction for future research  X  the integration of dependencies according to their usefulness in IR. Several aspects could be further improved. For example, we have considered depen-dencies only between pairs of characters. It would be possible to extend them to more dencies between characters. This set could be extended in the future. 
