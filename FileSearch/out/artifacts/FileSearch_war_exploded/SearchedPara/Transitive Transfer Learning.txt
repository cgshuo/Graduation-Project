 Transfer learning, which leverages knowledge from source domains to enhance learning ability in a target domain, has been proven effective in various applications. A major limitation of transfer learning is that the source and target domains should be directly re-lated. If there is little overlap between the two domains, performing knowledge transfer between these domains will not be effective. In-spired by human transitive inference and learning ability, whereby two seemingly unrelated concepts can be connected by series of in-termediate bridges using auxiliary concepts, in this paper we study a novel learning problem: Transitive Transfer Learning (abbrevi-ated to TTL). TTL is aimed at breaking the large domain distances and transferring knowledge even when the source and target do-mains share few factors directly. For example, when the source and target domains are text and images respectively, TTL can use some annotated images as the intermediate domain to bridge them. To solve the TTL problem, we propose a framework wherein we first select one or more domains to act as a bridge between the source and target domains to enable transfer learning, and then perform the transferring of knowledge via this bridge. Extensive empirical evidence shows that the framework yields state-of-the-art classifi-cation accuracies on several classification data sets.
 H.2.8 [ Database Management ]: Database Applications -Data Min-ing Machine Learning Figure 1: An pictorial illustration of the transitive transfer learning problem. In TTL, the source and target domains have few common factors, but they can be connected by intermediate domains through some underlying factors.
 Transfer Learning, Transitive Transfer Learning, Nonnegative Ma-trix Tri-factorizations
Transfer learning, which aims to borrow knowledge from source domains to help the learning in a target domain, has been estab-lished as one of the most important machine learning paradigms [18]. Various algorithms have been widely used and proven effective in many applications, for example, classification [1, 6], reinforcement learning [24] and recommendation systems [19], and so on. A crit-ical requirement for successful transfer learning is that the source and target domains should be related. This relation can be in the form of related instances, features or models. If no direct relation can be found, forcibly transferring knowledge will not work. In the worst case, it could lead to having no improvement, or even worse performance, in the target domain [21]. This is one of the major limitations of traditional transfer learning. However, as human be-ings, we naturally have the ability to carry out inference and learn-ing via transitivity [7]. This ability helps humans connect many concepts and transfer knowledge between two seemingly unrelated concepts by introducing a few intermediate concepts as a bridge. For example, after taking a class in elementary computer science, we may find it easier to transfer the knowledge to theoretical com-puter science if we have taken an applied algorithm design course in between, since the algorithm course may involve both concepts in programming and theory. Likewise, having learned some basic math, we may find it impossible to directly take a course in con-vex optimization. However, this becomes feasible when we take an intermediate course in linear algebra and probability. The linear algebra and probability course serves as the intermediate domain for knowledge transfer.

Human ability to conduct transitive inference and learning in-spires us to study a novel learning problem known as Transitive Transfer Learning (TTL). As illustrated in Figure 1, in TTL, the source and target domains have few common factors, but they can be connected by intermediate domains through some underlining factors. We expect TTL to have wide practical applications. For example, when the source domain is composed of text documents and the target domain contains image data, they share no overlap feature spaces, knowledge learned in text documents can hardly be transferred to images. However, TTL can introduce some anno-tated images to learn a feature mapping between these two different feature spaces and have a smooth knowledge transfer. In other ap-plications, such as text sentiment classification, all the data have the same feature space, but two group of data may have large dis-tribution gap, TTL can introduce some auxiliary intermediate data to form a transitive knowledge transfer structure with which we can obtain a more versatile sentiment classification system.
In this paper, we propose a learning framework for the TTL prob-lem. The framework is composed of two steps. The first step is to find an appropriate domain to bridge the given source and target do-mains. The second step is to do effective knowledge transfer among all domains. In the first step, we propose a probability model to se-lect appropriate domains that is able to draw the source and target domains closer, based on domain characteristics such as domain difficulty and pairwise closeness. As data from different domains are collected from different data sources, each pair of domains may have distribution shift. In the second step, considering both of the domain relationship and distribution shift, we propose a transfer learning algorithm that allows to learn overlap features among do-mains and propagate label information through them. A high-level description of the TTL framework is summarized in Table 1. We give a formal definition of the TTL problem in Section 2, and de-scribe the technical details of these two steps in Sections 3 and 4 respectively.
In the problem, we have labeled source domain data S = { ( x y ) } n s i =1 , unlabeled target domain data T = { x t beled intermediate domains D j = { x d j i } n j i =1 , j = 1 ,...,k , x R m  X  is a m  X  dimensional feature vector. The data from different domains could have different dimensions. S and T have a large distribution gap, thus directly transferring knowledge between them may cause a substantial performance loss in the target domain. The TTL framework is aimed at finding intermediate domain(s) to bridge S and T , and minimizing the performance loss in T .
Formally, given a domain distribution gap measure g (  X  ,  X  ) , the first step is to find an intermediate domain that satisfies g ( S , T | D &lt; g ( S , T ) . The second step performs transfer learning from the source domain S to target domain T via intermediate domain D this is implemented via learning two feature clustering functions p sd ( S , D i ) and p dt ( D i , T ) , such that the distribution gap of data on common feature clusters selected by p sd ( S , D i ) and p are further reduced. The label information in the source domain is Input : The source,target and candidate intermediate domains Step 1: Intermediate domain selection (see Section 3) Step 2: Transitive knowledge transfer (see Section 4)
Output : Prediction results in the target domain propagated to the intermediate and target data on the selected com-mon feature clusters.
Intermediate domain selection is problem specific, different prob-lems may have different strategies. For example, when the source domain is composed of text data and the target domain is image data, one can crawl some annotated images from Flickr as the in-termediate domain data [22]. In other problems when there are multiple candidate intermediate domains, one should propose some selection algorithms according to domain properties. In this pa-per, we propose an algorithm for text sentiment classification prob-lem as an example. As studied by previous research, domain diffi-culty [20] and domain distance [3] are two major factors that affect the transfer learning performance between two domains. On one hand, intuitively, if the source domain is less difficult than the in-termediate and target domains, the model learned from the source data is highly predictive and is very likely to achieve high perfor-mance on the intermediate and target domains as well. On the other hand, if the intermediate domain is able to draw closer the source and target domains than their original distance, then the knowledge transfer process between the source and target domains will have less information loss. Hence, in this paper, we introduce domain complexity [20] and A -distance [3] to estimate domain difficulty and pairwise domain distance respectively. We summarize these measures as follows: cplx_src ( c 1 ) source domain complexity cplx_inter ( c 2 ) intermediate domain complexity cplx_tar ( c 3 ) target domain complexity Given a triple tr = { S , D , T } , we can extract six features as de-scribed in Table 2. The first three features summarize individual in-domain characteristics, the last three features capture the pair-wise cross domain distances. These features together affect the success probability of a transfer learning algorithm. However, it is impossible to design a universal domain selection criteria, as dif-ferent problems may have different preferences (weights) on these features. To model the success probability of the introduced inter-mediate domain, we propose the following logistic function: where  X  ( x ) = 1 1+ e  X  x . We estimate the parameters  X  = {  X   X  } to maximize the log likelihood defined as:
L (  X  ) = l ( i ) is a binary label, indicating whether the intermediate domain We get the label by the following strategy. We perform a semi-supervised label propagation algorithm with input S and T , and obtain a prediction accuracy acc st on the target domain. We also perform the same algorithm with input { S , D , T } , and obtain an-other accuracy acc sit on the target domain. If acc sit &gt; acc set l ( i ) = 1 , otherwise, l ( i ) = 0 . The label is determined by both the domain characteristics and the propagation model. A sophisti-cated model may accept more intermediate domains than a simple model. In this paper, we prefer to use a simple model such as KNN that are able to provide us strictly fitted candidates.

We transform the intermediate domain selection problem to a probability estimation problem. A candidate intermediate domain with high f ( tr ) is more likely to be selected.
In the first step, an intermediate domain that can bridge the source and target domains has been selected, however, there is still dis-tribution shift among these domains. Thus, in the second step of the TTL framework, we propose a novel transfer learning algo-rithm that considers both of the transitive relationship and distri-bution shift among all the domains. The algorithm is based on non-negative matrix tri-factorization that can perform feature clustering and label propagation simultaneously, so we first give some back-ground knowledge.
Non-negative Matrix Tri-factorization (NMTF) is a popular and effective technique for data clustering and classification [10]. In NMTF, the feature-instance matrix is decomposed into three sub-matrices. In general, given a feature-instance matrix X  X  R m is the number of dimensions, n is the number of instances. One can obtain the factorized sub-matrices by solving the optimization problem as follows: where || X || denotes the Frobenius norm of matrix.

The matrix F  X  R m  X  p indicates the information of feature clus-ters and p is the number of hidden feature clusters. The element F i,j indicates the probability that the i th feature belongs to the j th feature cluster.

The matrix G  X  R c  X  n is the instance cluster assignment matrix and c is the number of instance clusters. If the largest element of the i th row is located in the j th column, it means that the i th instance belongs to the j th instance cluster. In the classification problem, each instance cluster can be regarded as a label class.

A  X  R p  X  c is the association matrix. c is the number of instance clusters or label classes, for the binary classification problem c = 2 . The element A i,j is the probability that the i th feature cluster is associated with the j th instance cluster.
NMTF is also used as a basic technique for transfer learning al-gorithms. Given the source and target domains S and T , X X t are their feature-instance matrices respectively, one can decom-pose these two matrices simultaneously, and allow the decomposed matrices share some cross-domain information (sub-matrices). For-mally, given two related domains S and T , their feature-instance matrices can be decomposed simultaneously as follows: L
ST = || X s  X  F s A s G s || + || X t  X  F t A t G t || where F 1  X  R m  X  p 1 + and A 1  X  R p 1  X  c + contain the common factors shared by the source and target domains. F 2 s ,F 2 t  X  R A ,A 2 t  X  R p 2  X  n + contain domain-specific information. They are not shared by domains. p 1 , p 2 are two parameters that indicate the number of hidden feature clusters. G s  X  R n  X  c is the label class matrix and generated from the instance labels { y i | i = 1 ,  X  X  X  ,n } of the source domain S . If the i th instance belongs to j th class, then the ( i,j ) element in G s equals to 1, otherwise, it equals to 0. G s is a constant matrix and keeps unchanged during the factoriza-tion process. G t is the label class matrix of the target domain. Its elements are variables that we want to learn by the matrix decom-position.

From Eq. (6), we can notice that the label information of the source domain is propagated to the target domain through the shared common factors F 1 and A 1 .
As shown in Figure 1, the source, intermediate and target do-mains have a transitive relationship. In other words, the interme-diate domain bridges the source and target domains, but has differ-ent common factors to them respectively. Hence, to capture these properties, we propose a coupled NMTF algorithm. The proposed Figure 2: An illustration of the proposed transfer learning al-gorithm in the TTL framework. The algorithm learns two coupled feature representations by feature clustering, and then propagates the label information from the source to the target domain through the intermediate domain on the coupled fea-ture representation. transfer learning algorithm is illustrated in Figure 2, and written in Eq. (7)
L = || X s  X  F s A s G T s || + || X I  X  F I A I G T I || + From the above equation, we can see that the first two terms ( clustering and label propagation between the source and interme-diate domains in Figure 2, the last two terms refer to the second feature clustering and label propagation between the intermediate and target domains. In Eq. (7), it is worth noting that we decompose X
I twice with different decomposition matrices, since X I shares different knowledge with X s and X t respectively. At the same time, we couple these two decomposition processes together by the label matrix G I . It is reasonable that the instances in the intermedi-ate domain should have the same labels in different decomposition processes. Moreover, if we solve the matrix decomposition by it-erative algorithms, in every iteration, each decomposition process is able to consider the feedbacks from the other decomposition. If these two processes are separately solved, the first decomposi-tion process will not consider the results from the second one, and may suffer from the bias problem. In the experiment, we find that the coupled strategy achieves better performance than separated de-composition.

Overall, the proposed learning algorithm fits the transitive rela-tionship among domains. The label information in the source do-main is transferred through  X  F 1 and  X  A 1 to the intermediate domain, and affects the learning results of G I . The knowledge on class la-bels incorporated with G I from the intermediate domain is further transferred to the target domain through  X  F 1 and  X  A 1
As we discussed in Section 4.1, the decomposed matrix F con-tains the information on hidden feature clusters, indicating the dis-tribution of features on each hidden cluster. Therefore, the summa-tion of each column of F has to be equal to one. The label matrix G indicates the label distribution of each instance. Thus, the sum-mation of each row of G has to be equal to one. Considering these matrix constrains, we obtain the final optimization objective func-tion for the proposed learning algorithm:
Since the objective function in Eq. (8) is non-convex, it is in-tractable to obtain the global optimal solution. Therefore, we de-velop an alternating optimization algorithm to achieve the local op-timal solution. We first show the updating rules of matrices  X  F , and G t . We summarize the notations of matrix multiplications in Table 3, and show the updating rules as follows: From Eq. (8), after the matrices are updated, the constrained matri-ces have to be normalized as:
The updating rules and normalization methods for other sub-matrices are similar and are shown in the Appendix. We need not update G s , which contains the ground-truth label information. We give the procedure of the proposed learning algorithm in Algo-rithm 1. As shown in Eq. (7) and the Appendix section, the updat-ing rule for G I is constrained by F I , F 0 I , A I and A the sub-matrices  X  F 1 ,  X  A 1 and,  X  F 1 ,  X  A 1 are constrained by X and X t , G t respectively. Therefore, the updating rule of G sitively constrained by X s , G s and, the discriminative information in the source domain is transitively transferred to the target domain. The updating processes of F s , F I , F 0 I and F t refer to the feature clusterings in Figure 2. The updating processes of G I and G to the label propagations in Figure 2.

We analyze the convergence property of Eq. (9) with normaliza-tion rules in Eq. (10). We first analyze the convergence of the rest of the parameters fixed. By using the properties of trace operation and frobenius norm || X || 2 = tr ( X T X ) = tr ( XX we re-formulate the objective function Eq. (8) as a Lagrangian func-tion and keep the terms related to  X  F 1 :  X 
M  X 
T t = [  X  A 1  X  A 2 t ] Algorithm 1 The TTL Transfer Learning Algorithm 1: Input : Source, target, intermediate domains S , T and D , the 2: Initialize the matrices F s , A s , F I , A I , G I , F 3: while iter &lt; Iter max do 4: Update the sub-matrices of F s , A s , F I , A I , F t 5: Normalize the sub-matrices of F s , F I , F t , and label ma-6: end while 7: Output : the predicted results of G t .
 where  X   X  R p  X  p is a diagonal matrix. 1 m and 1 p are all-ones vectors with dimension m and p respectively.

L EMMA 1. Using the update rule in Eq. (9) and normalization rules in Eq. (10), the loss function in Eq. (11) will monotonously decrease.

The proof of Lemma 1 is shown in the Appendix. The conver-gence of other terms can be proven in the same way. According to the convergence analysis on the update rules and the multiplicative update rules [13], each update step in Algorithm 1 will not increase Eq. (8). The objective has a lower bounded by zero. The conver-gence of the proposed transfer learning algorithm is proven.
In this section, we perform three tests. The first test is designed to analyze how the intermediate domain and model parameters af-fect the performance of the TTL framework, and to evaluate the convergence rate empirically. This is done by conducting experi-ments on six synthetic text classification tasks generated from the 20Newsgroups data set 1 .

The second test is designed to evaluate the TTL framework when the source and target domain data have completely different struc-tures. The experiments are conducted on the text-to-image data set. The intermediate domains for all tasks in the data set are crawled from Flicker. http://qwone.com/~jason/20Newsgroups/
Finally, the third test is designed to test the efficiency of the inter-mediate domain selection algorithm and the transfer learning algo-rithm in the framework. The experiments are conducted on some text sentiment classification tasks 2 . The data from different do-mains have the same feature space but different distribution. More-over, there are many candidate intermediate domains for each pair of source and target domains.
In the synthetic text classification and sentiment classification tasks, all the data have the same feature space. We compare the proposed framework with three baseline methods to verify the ef-fectiveness.

The first baseline is SVM, which is a classical supervised learn-ing algorithm. We use the linear kernel of SVM with the implemen-tation in LibLinear 3 . The second one is the triplex transfer learning (TriplexTL) algorithm, which is a state-of-the-art transfer learning method implemented with NMTF [32]. The other transfer learn-ing algorithm is LatentMap [25], which is also a state-of-the-art transfer learning algorithm. It draws the joint distribution of two domains closer by mapping the data to a low dimensional latent space. The three baseline methods are tested under two different settings. The first one is direct-transfer. We train the learners based on the labeled data in the source domain and test them directly on the data in the target domain. We use subscript ST to indicate the methods under this setting in the following experiments, for ex-ample, TriplexTL ST and LM ST . The second setting is a 2-stage transfer learning process. We first apply TriplexTL/LM between the source and the intermediate domain to predict the intermediate domain labels, and then again apply TriplexTL/LM between the intermediate domain and the target domain. The major difference between this naive transitive transfer learning strategy and the pro-posed transfer learning algorithm is that no iterative feature clus-tering and label propagation is performed. We use subscript to represent methods under this setting, for instance, TriplexTL In the text-to-image data set, the data have different feature spaces. The above mentioned baselines cannot handle these data. Hence, we compare TTL with two heterogeneous transfer learning (HTL) algorithms.

The first baseline is co-transfer [17]. It models the problem as a coupled Markov chain with restart. The transition probabilities of the Markov chain is construdture by using the intra-relationship based on affinity metric among data in the source and target do-mains, and the inter-relationship between the source and target do-mains based on co-occurrence information of the intermediate do-http://www.cs.jhu.edu/~mdredze/datasets/ sentiment/ http://www.csie.ntu.edu.tw/~cjlin/ liblinear/
Figure 3: The problem setting of the 20Newsgroup data set. main. The second one is HTLIC [31] 4 . It learns a new target feature representation by using data from the source, intermediate and tar-get domain data via the collective matrix factorization technique. A SVM classifier is then learned on the new target feature repre-sentation.

All methods in the experiments are performed ten times, and we report their average performances and variances.
The 20Newsgroups is a hierarchical text collection, containing some top categories like  X  X omp X ,  X  X ci X ,  X  X ec X  and  X  X alk X . Each cat-egory has some sub-categories, such as  X  X ci.crypt X  and  X  X ci.med X . We use four main categories to generate six tasks, in each of which two top categories are chosen for generating binary categorization. With a hierarchical structure, for each category, all of the subcat-egories are then organized into three parts, where each part has different subcategories and is of a different distribution. Therefore, they can be treated as the source, intermediate and target domains, respectively. To generate the transitive transfer learning setting, we divide the vocabularies into two separated subsets Set A and Set B. Then, we set the term frequencies of words in Set A of the source domain to zero. Similarly, we set the term frequencies of words in Set B of the target domain to zero. Therefore, the source and target domains have no overlapping words. The problem setting on this data set is illustrated in Figure 3, where the blocks with texture indicate that the features have values. We can see that the source and target domains have no shared features, but they have shared features with the intermediate domain, respectively. Apparently, the intermediate domains here can bridge the generated source and target domains. We give a detailed description of the six tasks in Table 4. The feature dimensions in these tasks range from 2405 to 5984 . The number of instances in these tasks are around 7000.
In experiments, we compare the proposed framework with the baseline methods on six text classification tasks.

The text classification tasks are very challenging. The source and target domains have no overlapping features. The SVM clas-sifiers trained with labeled source data have almost no discrimi-native ability on the target data. From the results in Table 5, we can see that the SVM ST classifiers obtain a very bad performance. Likewise, the source classifiers can barely be adapted for the tar-get domain data. Hence, TriplexTL ST and LM ST obtain bad per-formance also, but better than SVM ST . The naive transfer learn-http://www.cse.ust.hk/~yinz/htl4ic.zip
Figure 4: Performance with different intermediate domains. ing algorithms, TriplexTL SIT and LM SIT , achieve relative good performance, because they use the intermediate domain data as a bridge to perform a 2-stage knowledge transfer. The proposed TTL framework achieves the best performance. This can be ascribed to the reason that TTL not only bridges the source and target domains by using the intermediate domain data, but also has iterative fea-ture clustering and label propagation loops where the knowledge provided by the source domain can be deeply reshaped and reorga-nized to be exploited for the target domain.
The intermediate domain plays an important role in bridging the source and target domains. Hence, we also conduct some experi-ments on the  X  X omp-vs-talk X  task to test the proposed TTL frame-work when 1) the amount of labeled intermediate data increases; 2) the connection between the source/target and the intermediate domains becomes weaker.

In the first setting, we compare TTL with TriplexTL IT that trans-fers knowledge from labeled intermediate domain data to the target data. We vary the amount of labeled intermediate data from 50 to 400. We randomly sample the labeled intermediate domain data ten times, and show the average performance and variance in Fig-ure 4(a). From the results, we can see that the performance of TTL is better than TriplexTL IT when the amount of labeled intermedi-ate domain data is small. However, when there is a large amount of labeled intermediate data, the performance of TriplexTL IT ter. The results are reasonable, because when we have large amount of data that are near and adaptable to the target data, we need not seek help from domains that are far away.

In the second setting, some overlap features in the intermedi-ate domain are removed. We compare the TTL framework with TriplexTL SIT . In each comparison experiment, we randomly re-move d features ten times, and show the average performance and its variance in Figure 4(b). From the results we can see that the performance decreases as features are removed. The reason is that the connection between the intermediate and source/target domain becomes weaker when more features are removed.
In the Appendix, we have theoretically proven the convergence of the transfer learning algorithm in the TTL framework. Here we test the convergence rate. We conduct an experiment on  X  X omp-vs-talk X  task, and set the number of iterations to 100. We show the objective value of Eq. (8) as the dashed line in Figure 5(a), and see that after around five to ten iterations, the objective value ex-periences almost no change. Similarly, we show the classification accuracy of the target domain of each iteration as the solid line in Figure 5: Convergence analysis and model parameter analysis. Figure 5(a). The results show that there is no change in the per-formance after 60-80 iterations. The convergence trends on other tasks are similar.

We also analyze the model parameter p . We vary p from 5 to 100 to test how it affects the classification performance. The ex-periments are also conducted on  X  X omp-vs-talk X  task. The results are shown in Figure 5(b), from which we can see that the algorithm achieves better performance when p is between 20 and 40. For different tasks, we can use ten-fold cross validation to choose the value. In this paper, we simply set p to be 30 in the experiments.
The NUS-WISE data set for heterogeneous transfer learning prob-lem is generated by [17]. It contains 45 text-to-image tasks. Each task is composed of 1200 text documents, 600 images, and 1600 co-occurred text-image pairs. The data in each task are about two different categories, such as  X  X oat X  and  X  X lower X . Therefore, we can do binary classification for each task. There are 10 categories in the data set, including  X  X ird X ,  X  X oat X ,  X  X lower X ,  X  X ood X ,  X  X ock X ,  X  X un X ,  X  X ower X ,  X  X oy X ,  X  X ree X  and  X  X ar X . The text vocabulary size is 500. Each text data is represented by a 500 dimensional bag-of-word vector. For image data, we extract SIFT features [16] and represent each image in a 512 dimensional feature vector. In this Figure 6: The classification accuracy on the tasks of the text-to-image data set. data set, our task is to transfer knowledge from source text docu-ments to images through co-occurred text-image pairs. As HTLIC needs some labeled target domain data to train the SVM classifier, in the text-to-image tasks, we assume all the source domain data and a few target domain data are labeled. We vary the amount of labeled data in the target domain from 5 to 25, and show the average classification accuracies of all the tasks in Fig. 6(a), from which we can see that the performance of each algorithm in-creases when more labeled target data are used. We can also find that SVM achieves the worst performance, since it considers no auxiliary information. HTLIC and co-transfer achieve better per-formance than SVM, since they successfully leverage some knowl-edge from the source domain by using the intermediate domain data. The proposed TTL framework obtains the best performance. The reason is that TTL takes the distribution shift between three domains into account and explicitly exploits the transitively shared knowledge for label propagation from the source to the target do-main.

We also report the detailed results on each individual task with 25 labeled target domain data. The classification accuracies and variances on each task are shown in Fig. 6(b). The x -axis indicates the task and the y -axis represents the classification accuracy. We sort the tasks by the performance of the proposed TTL framework in ascend order. From the results, we can find that TTL is supe-rior to other algorithms on most tasks and is always at the top. In addition, TTL is more stable than other algorithms.
The sentiment classification data set used in our experiment con-sist of Amazon product reviews on 12 different categories, includ-ing  X  X pparel X ,  X  X ooks X ,  X  X amera_&amp;_photo X ,  X  X VD X ,  X  X lectron-ics X ,  X  X ealth_&amp;_personal_care X ,  X  X itchen_&amp;_housewares X ,  X  X u-sic X ,  X  X ports_&amp;_outdoors X ,  X  X oys_&amp;_games X  and  X  X ideo X . Each product review consists of review text and a sentiment label. The data from different domains have different distributions. For ex-ample, reviews in  X  X itchen_&amp;_housewares X  may have adjectives such as  X  X alfunctioning X ,  X  X eliable X  and  X  X turdy X . However, re-views in the  X  X VD X  domain may have  X  X hrilling X ,  X  X orrific X  and  X  X ilarious X . In this data set, the data within each domain are bal-anced. One half of the data are positive reviews and the other half are negative. The data size in each domain ranges from 2,000 to 20,000. The vocabulary size for each domain is around 20,000. We randomly sample around 2,000 instances for each domain. From the 12 domains, we can generate P 3 12 =1,320 triples, such as &lt; X  X p-parel X ,  X  X ooks X ,  X  X amera_&amp;_photo X &gt; where  X  X pparel X ,  X  X ooks X  and  X  X amera_&amp;_photo X  are the source, intermediate and target do-mains respectively. We conduct experiments on all the 1320 triple to evaluate the performance of the proposed intermediate domain selection algorithm. We also conduct experiments on triples that are selected by the intermediate domain selection algorithm to test the proposed transfer learning algorithm in the TTL framework.
In order to evaluate the proposed intermediate domain selection algorithm, we propagate labels from the labeled source domain data to the unlabeled target domain data, and evaluate the prediction ac-curacy acc st on the target domain data. We also propagate labels from the labeled source domain data to the unlabeled intermedi-ate and target domain data by the same algorithm, and evaluate the prediction accuracy acc sit on the target domain data. In the exper-iment, we use semi-supervised learning with RBF kernel [30] to do the intermediate domain data are able to bridge the source and tar-get domain, and we assign a positive label to the triple. Otherwise, we assign a negative label. In the experiment, we set t = 1 . 03 , and get 102 positive labels among 1,320 triples.

We then randomly split all the triples into two parts, each part contains the same number of positive and negative triples. The first part is used to train the intermediate domain selection algorithm, the second part is for testing. Since the data are unbalanced, we randomly sampled some negative triples to form a balanced data set. We do the random sampling ten times. Each time, we use 10-fold cross validation to assess the performance of the intermediate domain selection algorithm on the first part. The average accuracy is 0 . 845  X  0 . 034 .
We also test the proposed transfer learning algorithm in the TTL framework on some triples selected by the intermediate domain se-lection algorithm with high confidence from the second part. We learn the selection model on the training triples and select 10 triples with highest confidence from the testing triple set. The selected triples are listed in Table 6. Some results are interesting and ex-plainable. For example,  X  X ideo X  domain is able to bridge the  X  X u-sic X  and  X  X pparel X  domains. Intuitively, most music review words are about sound such as rhythm and melody. Most apparel reviews may talk about the appearance like the color. The video reviews contain both the vocal and visual aspects, and are able to draw the music and apparel domains close.

From the results in Table 6, we can see that Triplex ST has almost the same results as SVM ST . The direct transfer learning algorithm here achieves no performance improvement. This is because the source and target domains have large distribution gap. TTL and Triplex SIT are better than Triplex ST . We can also see that TTL always achieves the best performance.
We discuss two categories of research related to transitive trans-fer learning: transfer learning and multi-task learning.
Transfer Learning solves the lack of class label problem in the target domain by  X  X orrowing X  supervised knowledge from related source domains [18]. There are mainly two typical types of al-gorithms. The first one is instance based knowledge transfer [9, 23], which selects or adapts the weights of the relevant data from source domains for the target domain. The second one is feature based knowledge transfer [29], that transforms both source and tar-get data into a common feature space where data follow similar dis-tributions. More recently, multi-source transfer learning performs transfer learning with multiple source domains. For instance, the work in [26] extends TrAdaboost [9] by adding a wrapper boosting framework on weighting each source domain. Different from pre-vious transfer learning, transitive transfer learning does not assume that the source domain and the target domain should be related. That means, transitive learning can be more general and more use-ful when the existing labeled and related source domains are not adequate enough to improve the target domain.

Multi-task Learning algorithms simultaneously learn several tasks together and mutually enhance the classification results of each task [2, 4, 5]. It assumes that different tasks share some natu-ral  X  X ompact X  representations, such as the information reflected by shared data clusters or subspaces. In practice, for example, classi-fiers for different tasks can be designed to share some global param-eters [11] or even a global classifier [8]. More recently, approaches that learn the relationships between pairwise tasks are also being developed [12, 27, 28]. However, these methods require reasonably large amounts of labeled data for each task to learn the relationship. In contrast, transitive transfer learning works even when both inter-mediate and target domains are unlabeled. It only assumes that the source domain should have sufficient labeling information to trans-fer. The intermediate domain serves as a bridge between source and target domains. Even if the intermediate domain is not labeled, the classification information passed from the source domain still contributes to the final classification task through the latent factors learnt in the learning process.
In this paper, we study a new problem, transitive transfer learn-ing (TTL), which transfers knowledge from a source domain to an indirectly related target domain with the help of some intermedi-ate domains. We propose a TTL framework to solve the prob-lem. The framework first selects one or more intermediate domains to bridge the given source and target domains, and then performs knowledge transfer along this bridge by capturing overlap hidden features among them. The experiments are conducted on three data sets, showing that the proposed framework achieves state-of-the-art performance. The convergence of the proposed TTL framework has also been theoretically and experimentally proven.

Future Work As a new learning problem, it raises several issues for further exploration in the future. For example, when the source and target need a string of domains to build a connection, how to find the string of intermediate domains to enable max transfer is a valuable research problem. In addition, extending the algorithm to multiple source domains may be an interesting way to generalize transitive transfer learning to be more powerful.
 We thank the support of China National 973 project 2014CB340304 and Hong Kong RGC Projects 621013, 620 812, and 621211. We also thank Yin Zhu, Lili Zhao, Zhongqi Lu, Kaixiang Mo and Ying Wei for discussion. [1] R. K. Ando and T. Zhang. A framework for learning [2] J. Baxter. A bayesian/information theoretic model of [3] S. Ben-David, J. Blitzer, K. Crammer, F. Pereira, et al. [4] S. Ben-David, J. Gehrke, and R. Schuller. A theoretical [5] S. Ben-David and R. Schuller. Exploiting task relatedness for [6] J. Blitzer, M. Dredze, and F. Pereira. Biographies, [7] P. E. Bryant and T. Trabasso. Transitive inferences and [8] O. Chapelle, P. Shivaswamy, S. Vadrevu, K. Weinberger, [9] W. Dai, Q. Yang, G.-R. Xue, and Y. Yu. Boosting for transfer [10] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal [11] T. Evgeniou and M. Pontil. Regularized multi X  X ask learning. [12] Z. Kang, K. Grauman, and F. Sha. Learning with whom to [13] D. D. Lee and H. S. Seung. Algorithms for non-negative [14] C.-K. Lin, Y.-Y. Lee, C.-H. Yu, and H.-H. Chen. Exploring [15] M. Long, J. Wang, G. Ding, W. Cheng, X. Zhang, and [16] D. G. Lowe. Distinctive image features from scale-invariant [17] M. Ng, Q. Wu, and Y. Ye. Co-transfer learning using coupled [18] S. J. Pan and Q. Yang. A survey on transfer learning. TKDE , [19] W. Pan, N. N. Liu, E. W. Xiang, and Q. Yang. Transfer [20] N. Ponomareva and M. Thelwall. Biographies or blenders: [21] M. T. Rosenstein, Z. Marx, L. P. Kaelbling, and T. G. [22] B. Tan, E. Zhong, M. Ng, and Q. Yang. Mixed-transfer: [23] B. Tan, E. Zhong, W. Xiang, and Q. Yang. Multi-transfer: [24] M. E. Taylor and P. Stone. Transfer learning for [25] S. Xie, W. Fan, J. Peng, O. Verscheure, and J. Ren. Latent [26] Y. Yao and G. Doretto. Boosting for transfer learning with [27] Y. Zhang and D.-Y. Yeung. A convex formulation for [28] Y. Zhang and D.-Y. Yeung. Multi-task boosting by exploiting [29] E. Zhong, W. Fan, Q. Yang, O. Verscheure, and J. Ren. Cross [30] X. Zhu. Semi-supervised learning literature survey. 2005. [31] Y. Zhu, Y. Chen, Z. Lu, S. J. Pan, G.-R. Xue, Y. Yu, and [32] F. Zhuang, P. Luo, C. Du, Q. He, and Z. Shi. Triplex transfer  X   X   X   X  F I = [  X  F 1  X  F 2 I ] A I = [  X  A 1  X  A 2 I ]
We summarize some other matrix multiplication notations in Ta-ble 7, and give the update rules for  X  F s ,  X  A s ,  X  F I  X 
F 1 ( i,j ) =  X  F 1 ( i,j )  X   X 
F 2 s ( i,j ) =  X  F 2 s ( i,j )  X   X 
F 2 I ( i,j ) =  X  F 2 I ( i,j )  X   X 
A 1 ( i,j ) =  X  A 1 ( i,j )  X   X 
A 2 s ( i,j ) =  X  A 2 s ( i,j )  X   X 
A 2 I ( i,j ) =  X  A 2 I ( i,j )  X 
G I ( i,j ) = G I ( i,j )  X 
The normalization methods for  X  F s and  X  F I are: We first analyze the convergence of  X  F 1 with the rest parameters are fixed. By using the properties of trace operation and frobenius norm || X || 2 = tr ( X T X ) = tr ( XX T ) , we re-formulate the ob-jective function Eq. (8) as a Lagrangian function and keep the terms related to  X  F 1 : where  X   X  R p  X  p is a diagonal matrix. 1 m and 1 p are all-ones vectors with dimension m s and p respectively. The differential of Eq. (14) is:
Then, we obtain the temporary updating rule:  X 
F
As proved in [15], the temporary update rule in Eq. (16) is able to monotonously decrease the Eq. (14). Therefore, there is still one variable  X  that needs further calculation. Considering the con-strains in Eq. (8), we find that  X  is used to satisfy the conditions that the summation of each column of  X  F 1 has to be equal to one. We use the the normalization method in Eq. (13) to normalize method satisfies the condition regardless of  X  . After that, 1 is equal to 1 m 1 T m  X  F 1  X  . By getting rid of the terms that contain  X  , we get the final update rule in Eq. (12) that is approximately equal to Eq. (16) in terms of convergence, since both 1 m 1 m 1 T m  X  F 1  X  are constants. Using update rule in Eq. (12) will also monotonously decrease the value of Eq. (14).

We can use similar methodology to analyze the convergence of the update rules and normalization methods for other terms in Eq. (8).
According to the Multiplicative Update Rules in [13], using the update rules in Eq. (9) and Eq. (12) and using the normalization methods in Eq. (10) and Eq. (13), the value of the objective function in Eq. (8) will not increase. The objective function has a zero lower bound. The convergence of Algorithm 1 is guaranteed.
