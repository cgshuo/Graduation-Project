 Query recommendation predominantly relies on search logs to use existing queries for recommendation, typically cal-culating query similarity metrics or transition probabilities from the log. While effective, such recommendations are limited to the queries, words, and phrases in the log. They hence do not recommend potentially useful, entirely novel queries. Recent query recommendation methods have pro-posed generating queries on a topical or thematic level, though current approaches are limited to generating single words. We propose a hybrid method for constructing multi-word queries in this generative sense. It uses Latent Dirichlet Al-location to generate a topic for exploration and skip-gram modeling to generate queries from the topic. According to additional evaluation metrics we present, our model im-proves diversity and has some room for improving relevance, yet offers an interesting avenue for query recommendation.  X  Information systems  X  Query suggestion; Personal-ization; Document topic models; Retrieval effectiveness; Search session analysis; Diversity; User simulations; Ex-ploratory search; Query recommendation; Latent Dirichlet Allocation Query recommendation dates back to least the early 2000s. Most approaches harness search logs -a set of users X  search sessions -to make personalized recommendations to users. Each session contains information about a user X  X  sequence of queries, clicks on search engine result pages (SERPs), and possibly time spent on results. Even without explicit relevance feedback on the logs from annotators, this data provides a rich source of implicit feedback -such as click-through behavior and dwell time -that can be used to quan-tify query satisfaction, page usefulness and pairwise query similarity scores. While query logs are indispensable for per-sonalization, many algorithms either recommend queries di-rectly from the logs or reformulated versions of prior queries. While effective on a large query log, this may work less well for smaller data. For recommending rarely-discovered, topic-relevant information or making session-based recom-mendations, long-tail queries become less likely to produce satisfactory results, necessitating novel query generation. This contrasts with reformulation approaches that suggest textually similar queries, recommending queries purely on topical relatedness to previous queries in a session.
In this paper, we discuss an approach to recommending novel queries that combines a user X  X  session and the the-matic structure of web pages to create new queries. We create recommendations using Latent Dirichlet Allocation and skip-gram modeling and apply new evaluation metrics to this problem setting. In Section 2, we discuss the neces-sary background information to understanding this work. In Section 3, we explain our algorithm. We outline our data set in Section 4. We give our performance metrics, results, and discussion in Section 5, and we conclude in Section 6. While there is room for improvement, our model improves diversity of actual performance and offers interesting possibilities.
Some past and current query recommendation methods recommend queries directly from a query log. [1], for in-stance, clusters queries by similarity, offering similar sub-stitutes for failing queries, and [4] offers suggestions by cal-culating explicit transition probabilities between queries in a log. Contrastingly, [11] uses query distance to recom-mend orthogonal queries -dissimilar queries that are simi-lar enough to be topically relevant. Some methods generate new queries incrementally. [5], for instance, uses WordNet to replace words with their synonyms. While our approach recommends orthogonal queries like [11], unlike [11] and [5] it generates queries without incrementing from a base string. Hence, our approach risks retrieving less relevant results. Non-zero transition probabilities in a query log give implicit relevance feedback. Our approach discards this feature for relevance in our algorithm (assuming no multitasking).
We attempt to address this loss of relevance feedback and capture diversity with topic models, modeling the topics a user has explored and recommending terms from unexplored topics. For at least the past decade, topic modeling has gained recent interest among researchers as an alternative approach to modeling documents. A commonly cited topic model is Latent Dirichlet Allocation (LDA) [2], which can infer the topic distribution of a document and bag-of-words distribution of a topic. [6, 8] have previously applied LDA to recommending queries. [6] only mapped existing queries to a thematic representation and recommend those queries. [8] only recommends single-word queries and generates virtual training  X  X ocuments X  by combining query sessions with an identical ending. This cannot be done with smaller data and may be insufficient for exploratory search tasks (see Task and Dataset). Our approach extends these efforts.
Typically, results of query recommendation are evaluated in a URL-based fashion. For instance, discounted cumu-lative gain sums up the relevance scores of each document in a list (e.g. query results) and tempers them according to their position in the list. Recent efforts like [9] evaluate whole session metrics; in particular, [9] defined URL-based measures like the coverage, unique relevant coverage, and likelihood of discovery of URLs for individual and collabo-rative tasks. Since our approach recommends new queries, it is likely that on smaller data, most URLs in a SERP have never been seen in our sessions. To our knowledge, most of the literature performs purely URL-based evaluation, so we define different effectiveness measures in our results.
We first train our LDA topic model T = (  X , X  1: K ) over D , a set of web documents.  X  is a Dirichlet prior, and  X  k  X   X  a  X  X opic X , or a distribution over the vocabulary. For this paper, we trained on all of the pages linked through the top 10 URLs of SERPs in our log. All text -SERP snip-pets and web page text -is preprocessed through stopword removal and removing words with less than 4 characters. We also remove punctuation, numbers, and words with non-alphanumeric letters (e.g., hashtags).

Assume a user u in our query log has issued queries Q u = { q 1 ,q 2 ,...,q h } and has visited pages P u in their entire ses-sion. We create simulations starting at each step 3 &lt; t &lt; h ; the completion of a simulation results in several injections { q we concatenate the current SERP text (i.e. result snippets) of { q 1 ,q 2 ,...,q t } and infer a topical distribution using T -call this distribution  X  S, 1: S,K . This tells us the proportions in which each topic has currently been explored by the user. We then select a topic for query generation and then gener-ate the query (details discussed in Sections 3.1-3.2).
We issue the generated query q 0 t +1 using a commercial search API and inject it into the user X  X  current set of queries, and injection loop until a timer in our algorithm reaches | Q u | + | P u | , the total number of pages visited and queries issued by user u in our actual log data. This stopping crite-rion simplifies and accommodates for individual user factors, such as page viewing time and user frustration. Each simu-lated query counts as a timestep, as do the first | S | results, where | S | is the size of a single SERP. See Algorithm 1 for a summarization of the algorithm.
After calculating  X  S, 1: S,K , we iterate through topics in as-cending probability, to search for underrepresented yet rele-vant topics. We calculate topic k  X  X  relevance with a weighted divergence score against the SERPs X  topic distribution:
Where JSD is the Jensen-Shannon Divergence score [7], a symmetric score based on KL-divergence. The first topic to score below 0.15 (chosen through trial and error) is chosen for query generation. We incrementally increase the thresh-old each time no satisfactory topics are found.
After topic selection, we extract the topic X  X  top 15 words and their topic-specific probabilities, as given by LDA. We generate 3-word queries randomly, sampling the first word from the topic probabilities. The remaining 2 words are then sampled from the skip-gram probabilities, conditioned on the first word. Skip-grams generalize n-grams; words are allowed to occur within a window of k words in order to be counted in the model. For smaller data, skip-grams can cope with data sparsity that can affect n-gram models, and they can be an effective smoothing method for language model estimation [10]. We used a 4-skip-3-gram model, i.e., we calculated the probabilities of sequences of 3 words (the most common query length in our data) that were no more than 4 words apart from each other. We trained our model ( SG ) on the titles of the web pages that were used for LDA, rather than the whole content; many non-stop words are irrelevant while titles tend to be related to the query. Data:
T = (  X  ,  X  1: K ), SG, Q u , P u , | S | -See Section 3 for explanations ts -current timestamp
Q curr -queries currently issued by a user index best = Null; while ts &lt; | P u | + | Q u | do end We applied our approach to exploratory search task data. Exploratory search tasks have been estimated to comprise 10% of search sessions and 25% of overall queries [3]. In these tasks, information needs are ill-structured and cannot Topic # users &gt; 3 SERPs | Q uni | Avg. queries | D | TEC 13 9 99 12.63 513 HEA 16 15 148 10.13 915 ENT 11 11 159 14.72 581 ART 5 5 91 16 740 ENV 2 2 28 14 115 Table 1: Users, queries, and documents per topic be satisfied in a single query, and users engage in learning during the task [13]. Our queries could hit poorly explored areas in such lengthy tasks, increasing diversity while main-taining relevance. In our data, 29 users completed 47 ex-ploratory search sessions on 5 topics:  X  X echnology X  (TEC),  X  X ealth and Wellness X  (HEA),  X  X ntertainment X  (ENT),  X  X rt and History X  (ART), and  X  X nvironment and Energy X  (ENV). Users were undergraduate participants recruited through open calls in e-mail lists at a local university, from majors such as Computer Science, Information Technology, Informatics, Nursing and Biological studies. Users completed a prompt within a 2-hour time limit and were provided monetary com-pensation. They were allowed to quit after a minimum of 1 hour but were incentivized with an additional reward for top performance. Users downloaded a Firefox plugin and conducted the task live through their web browsers, i.e., at any location at their convenience. Despite lack of lab super-vision, users can conduct more natural searching behavior. Users selected search topics from a brief list ( X  X echnology X ,  X  X ealth and Wellnes X , etc.). After selection, they saw a full task description and could not retract their choices. They had a 5-minute warm up task before choosing the main task.
Our plugin recorded basic browsing behavior, like chang-ing tabs and issuing queries. It had buttons for snipping page text that users highlight, reviewing the task descrip-tion, and composing a final report on an online text editor. The plugin collected the content of users X  SERP and web page visits live, by issuing curl and search API calls from our server, approximating results users saw at visitation time. We created topics we thought were unfamiliar enough to be unaffected by personalization from users X  browsers.
Here is the task description (paraphrased) for TEC:  X  X ith the recent focus on security breaches and consumer data breaches, vulnerability in data and software has become an important topic of interest. You have been asked to write an article about data and software vulnerabilities; it should be around 1000 words. It should appeal to as many people as possible, including an unfamiliar, lay audience and people in affected businesses. It should cover different aspects of software vulnerabilities that are relevant to their daily lives. It should also focus on the measures taken to minimize these risks at industry, governments and consumer levels. You can use snippets to collect information that you deem useful for writing the report and copy them into your article. X 
Table 1 shows user and query counts for each topic. We trained a single LDA model once over the entire corpus of Table 4: Percent of off-topic words and ODP topics covered by LDA-SG. (Results for LDA-SG only.) documents -the documents linked through users X  first pages in their SERPs. We did this to simulate a real web envi-ronment absent of supervised topic labels on pages may not be available. We set the number of topics K = 35. We only used sessions with over 3 queries; others do not provide sufficient search results for our algorithm. We then filtered topics with less than 3 users as well. Many users issued over 3 queries, providing motivation for this algorithm.
We compared our algorithm (LDA-SG) against the real user sessions ( X  X ession X  in the tables) and Vahabi et al. [11] (ORTH). ORTH is strictly recommends queries from the logs to diversify results. ORTH recommends queries that overlap with the most recent query by (0.0,0.06]. Due to sparse logs we incremented the ceiling of 0.06, eventually choosing a random candidate if no suitable ones could be found.
In light of Section 2.3, we chose lexical and ontological evaluation metrics instead of URL-based ones. Suppose we let Words ( Queries ) be the set words in a sequence of queries (including SERP snippets), E Words ( T i ) be the set of topic-exclusive words for topic T i , and W = E Words ( T i S j 6 = i E Words ( T j ) our score Cov(Queries,i) is | Words ( Queries )  X  W | divided by | W | . This measures the topic word coverage, or the percentage of words that a set of SERPs covers in topic i . If we substitute W = S j 6 = i Words T j in Equation 2, we get our second measure: the percentage of off-topic words that are covered. The former measures diversity. The latter measures relevance as a penalty score, since the word  X  X ia-betes X , for instance, belongs exclusively to the  X  X ealth and Wellness X  topic and none of the others. We omit off-topic scores for actual user sessions; by definition, these results are all 0%. We similarly omit ORTH results for because all off-topic queries have 0.0 overlap due to a small query log.
Our ontological approach uses the Open Directory Project (ODP) 1 to convert URLs to topics. ODP is a web ontol-ogy that categorizes websites into hand-tailored topics, such as  X  X omputers/Internet/Resources/Research X  and has been used to categorize URLs for evaluation [14]. We convert all URLs in our simulations to their ODP categories (truncat-ing URLs incrementally until a matching category can be found). Replacing words in the above equations with ODP categories gives us our ODP-based measures.

Our results are given in Tables 2-6. Our approach in-creases the percentage of topic-specific words and ODP cat-egories (with a couple exceptions) over the baseline, suggest-http://www.dmoz.org Table 5: Query sequences returned by our algo-rithm. Boldfaced queries are generated queries. The first sequence shows a shift from the original topic. diet for type 2 diabetics, type 2 diabetes, risks of diabetic diet, exercise plans for type 2 diabetes, diet healthy-eating clinic , information control solutions , an-imation degree campus-based , health guidelines micronutrients , diabetes stick-to-it plan diets for diabetics type 2, type 2 diabetes, type 2 diabetes causes, diet claims clinic , health aetna featuring , diabetes weight loss , people diabetic cookbook Table 6: Top 10 words of returned LDA topics. Starred words belong to a sub-corpus (e.g. health) less dominant in the list (e.g. technology)
Topic 1:  X  X nimated X ,  X  X nimation X ,  X  X vatar X ,  X  X haracters X ,  X  X omputer X ,  X  X ata X *,  X  X iabetes X *,  X  X isney X ,  X  X ilm X ,  X  X nforma-tion X *
Topic 8:  X  X lood X ,  X  X ody X ,  X  X ata X *,  X  X iabetes X ,  X  X iet X ,  X  X x-ercise X ,  X  X ilm X *,  X  X ood X ,  X  X lucose X ,  X  X ealth X  ing an increase in diversity. However, the our approach is not as diverse ORTH and is penalized on off-topic measures.
Interestingly, the penalty for off-topic words is small while the penalty for off-topic ODP categories is large. Off-topic words are those that exist exclusively in other sub-corpora, and likewise for off-topic ODP categories. Our scores suggest that many of the queries (and hence web pages) were from other topics, even though few of the words were exclusively from other topics. Assuming that topics do not mix in a single set of SERP results, this suggests that there is a large percentage of overlapping words between topics. This would suggest a shift between topics in our list of suggested queries, which is consistent with our data. See Tables 5 and 6. This suggests the need for perhaps a stronger method of topic segmentation that discounts for overlapping words.
In future work, we would need to address arbitrary pa-rameter values chosen for proof of concept. Though obvious extrinsic evaluation metrics for cross-validation, like URL-based nDCG, are eliminated. Retrievability scores could fit but require large search logs (or simulations) for score esti-mates. We would have inferred the optimal number of topics with the online Hierarchical Dirichlet Process [12], but to our knowledge, implementations take the number of topics as a  X  X runcation level X  parameter. Also, the number of generated query terms could be probabilistically modeled from the log.
We could also improve our skip-gram method to generate queries more closely resembling human ones. We neglect the linguistic and information roles of stop words in real hu-man queries (e.g.  X  X ainbow X  vs.  X  X ver the rainbow X ), which would require further analysis. We could then add human judgments for evaluation, such as the comprehensibility of queries and whether they seemed to be human-generated.
We extended topic modeling-based methods of novel query generation. Our multi-word approach uses web documents as a lexical resource and attempts to maintain relevance that can be lost with topically-based generative approaches. We achieved higher diversity at the cost of relevance, and one desideratum of recommendation research is to close this gap while maximizing both. We believe this approach offers an interesting direction for query recommendation. If more widespread, topical recommendations could not only make recommendations at a topical level but could also increase the pool of unique queries in a log if users were to take the recommendations, improving non-generative methods.
A portion of this work was supported by the US Institute of Museum and Library Services (IMLS) Career Develop-ment grant # RE-04-12-0105-12.
