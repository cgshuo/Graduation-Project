 Hidden Markov Models (HMMs) are today employed in a variety of applications, ranging from sp eech recognition to bioinformat-ics. In this paper, we present the List Viterbi training algorithm, a version of the Expectation-Maximization (EM) algorithm based on the List Viterbi algorithm instead of the commonly used forward-backward algorithm. We developed the batch and online versions of the algorithm, and we also describe an interesting application in the context of keyword search over databases, where we exploit a HMM for matching keywords into database terms. In our exper-iments we tested the online version of the training algorithm in a semi-supervised setting that allo ws us to take into account the feed-backs provided by the users.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Search process; Retrieval models Algorithms, Experimentation, Performance Hidden Markov Model, List Viterbi algorithm, Keyword Search over Databases, Relational Databases, Metadata
HiddenMarkovModels(HMMs)havebeenlargelyusedinthe last thirty years for modeling stochastic processes, with several ap-plications that range from the field of speech recognition to com-putational biology. The most common training approaches for HMMs, e.g., Bayesian, maximum likelihood, are batch algorithms that require multiple passes through the dataset to converge. These algorithms perform the training offline using a training dataset pre-viously collected that needs to match as close as possible the run-time conditions. When such a training dataset is not available in advance and/or the run-time conditions vary over time, we need to resort to an online training algorithm. In online approaches the model parameters are continuously updated every time a new piece of data is collected without the need to store previously seen infor-mation. Therefore, the HMM performances improve over time and adapt to changing conditions [7].
 Training algorithms for HMM are typically based on Expectation-Maximization (EM) procedures, where the model pa-rameters are iteratively estimated following a two-step process [8]. In this paper, we propose a version of the EM training algorithm, the List Viterbi training algorithm, that bases the expectation step on the List Viterbi algorithm. List Viterbi [12] produces a ranked list of the K sequences of states that have the highest probability of having generated a given sequence of observations. Consequently, the estimation process is based on the top-K maximum likelihood state sequences and a state optimized joint likelihood function is employed during the maximization step. We developed both the batch and online versions of the training algorithm.

For evaluating the approach, we propose the application of the al-gorithm in the context of keyword search over databases. Keyword queries have become popular since they allow non-expert users to easily formulate queries even without any knowledge about how the data is structured in the source. Nevertheless, existing approaches rely on a-priori instance-analysis that scans the whole database and constructs some indexing structure which is later used during run time to identify the parts of the database in which each keyword appears. Consequently, these approaches cannot easily be applied in web environments where a complete access to the database in-stance is not always possible, and the data are subject to frequent update.

In this field, we developed KEYRY (From KEYwords to queRY) [4], a system exploiting a HMM for mapping user keywords into database terms (names of tables and attributes, domains of at-tributes). The KEYRY parameters are  X  X tatic X , i.e. they are de-fined in the bootstrap phase explo iting some database metadata. In this paper, we extend KEYRY by coupling it with the online ver-sion of the List Viterbi training algorithm, thus making the system able to learn by the use, exploiting the users X  feedbacks for improv-ing its performance. The approach has been largely evaluated in a semi-supervised setting that allows us to take into account different levels of feedbacks provided by the users. The experimental results show that the algorithm improves its ranking performance during time and requires a small amount of data for training.

The remainder of the paper is as follows. Section 2 intro-duces our approach that has been applied in Section 3 for solving keyword-based queries over relational databases. Section 4 is about related work and Section 5 proposes the results or our experiments. Finally, in Section 6 we sketched out some conclusion and future work.
We introduce a version of the EM algorithm, called List Viterbi training algorithm, which can be seen as a  X  X  winners take all X  algorithm since only the top-K state sequences are considered in the objective function. Following the formalism adopted in [8, 7], we propose the batch and online versions of the algorithm.
Let O = { O 1 ,O 2 ,...O L } be a dataset of L mutually inde-pendent observations. A single observation O l =( o 1 ,o 2 is composed of a sequence of observed symbols and has an arbi-trary length T . A correspondent sequence of states of the same length is denoted as Q l =( q 1 ,q 2 ,...,q T ) . We assume to use a HMM  X  =( A, B,  X ) composed of a finite number N of states S = { s 1 ,s 2 ,...,s N } and having the parameters defined as fol-lows:
A = { a ij = P ( q t +1 = s j | q t = s i ) } , 1  X  i  X  N, 1 B = { b i ( v m )= P ( o t = v m | q t = s i ) } , 1  X  i  X 
 X = {  X  i = P ( q 1 = s i ) } , 1  X  i  X  N where M represents the number of observable symbols. A more detailed technical background on HMMs can be found in the tuto-rial [11].

The List Viterbi algorithm is a dynamic programming algorithm that was initially used for the decoding of convolutional codes [13]. The algorithm has also been applied, using a different formulation, to HMMs in order to solve the following problem:
Given a HMM  X  and an observation sequence O l = ( o 1 ,o 2 ,...,o T ) find the ordered list of the K state sequences  X  Q l =( q k 1 ,q k 2 ,...,q k T ) , 1  X  k  X  K which have the highest prob-ability of generating O l
In other words, the algorithm generalizes the well-known Viterbi algorithm finding the top-K maximum likelihood state sequences (MLSSs) instead of the single MLSS found by the original Viterbi algorithm [11].

We assume to use a training dataset O composed of L mutually independent records. Without l oosing generality, we consider each record composed of a sequence of observations and we present the unsupervised versions of the algorithm (batch and online). The su-pervised versions of the algorithm can be straightforwardly derived from the unsupervised: in this case the E-step relies on the cor-rect state sequences that are given in the training dataset, instead of estimating the top-K MLSSs. Given the assumption that the ob-servations are mutually independent, the distribution over the state sequence outputs used in the E-step  X  P ( z ) ( Q ) can be decomposed as the product of the distributions over the single records:
The batch version of the algorithm performs each EM iteration using the whole training dataset O . During the E-step, the top-K MLSSs  X  Q k ( z ) l are computed for each record l using the List Viterbi algorithm. The distribution  X  P is constrained to assume value zero to all state sequences but the top-K MLSSs. For the top-K sequences,  X  signs the probability calculated by the List Viterbi algorithm nor-malized by the sum of the top-K MLSS probabilities.  X 
In the maximization step the model parameters are chosen to maximize a state-optimized log likelihood function that takes into account only the top-K MLSSs. The number of iterations required for convergence depends on the specific dataset used for the training and can be calculated using cross validation.
In the online version of the algorithm, each iteration is performed over a single record, or a small groups of records, at a time. Thus, the algorithm requires only a single pass through the dataset and there is no need to store the past records. Each iteration consists of a partial update of the distribution  X  P and the parameters  X  .The information provided by each record O l is exploited immediately instead of waiting to process the other records in the dataset, as in the batch version. This speeds up the convergence of the algo-rithm, which is a crucial aspect in online applications. We present here the formalization of the algorithm for the unsupervised sce-nario, where only a single record is utilized in each iteration. The formalization of the case where m records are processed at each step is straightforward.

Given some guess for the initial distributions  X  P (0) l rameters  X  (0) , the algorithm proceeds as follows. Assuming that a single record n is selected from the training dataset at iteration z , in the E-step the top-K MLSSs are only computed for the n -th record, while the MLSSs of the other records are kept equal to the ones calculated in the previous iteration.  X   X  The distribution  X  P is calculated as in the batch version (1), but since the top-K MLSSs are partially computed,  X  P is only partially updated.

The maximization step is performed using the distribution par-tially updated during the E-step (2). We will show in our experi-ment that this version of the algorithm converge quikly, even only using fractions of the whole training dataset.
In the case where the observations assume discrete values, i.e., there exists a finite number M of observable symbols V = { v 1 ,v 2 ,...,v M } as in our applicative scenario, the emission prob-ability distribution at each state s i can be represented using a multi-nomial distribution. Let us introduce the following variables: where, given an observation dataset O and a HMM  X  ,  X  t ( i, j ) rep-resents the probability of entering state s i at time t and state s time step later. Summing over j , we obtain  X  t ( i ) , i.e. the probabil-ity of entering state s i at time t . Using these variables, the E-step of the batch algorithm can be stated as estimating, i.e. counting, the following quantities: in the top-K MLSSs  X  Q k ( z ) l , 1  X  l  X  L , found by the List Viterbi algorithm.
 in the top-K MLSSs  X  Q k ( z ) l , 1  X  l  X  L . During the M-step, the parameters of the HMM are re-estimated using the quantities pre-viously calculated: In the online version of the algorithm only the n -th observation O n is processed at each iteration. Thus, the sufficient statistics are accumulated during time: Note that the HMM parameters can be initialized at z =0 ran-domly or using some initial guess on the distribution. In our ap-plicative scenario we use heuristic rules to initialize them, as ex-plained in Section 3.2.
Keyword search over databases is a hot challenging topic that has recently received a considerable attention. In this paper, we model the keyword search process with a HMM and we apply the List Viterbi algorithm for enabling keyword-based searches over rela-tional databases in cases where the access to the instances is pre-cluded. We conceive the searching process as a two-step approach where firstly the keyword query is analyzed for discovering its in-tended meaning, and then a ranked set of SQL queries, expressing the discovered meaning according to the database structure, is for-mulated. Our approach relies on the knowledge about the database schema for providing an initial HMM parameter estimate and for formulating correct SQL queries. We employ the online version of the List Viterbi training algorithm to improve the HMM perfor-mances taking into account both the users X  feedbacks and a query log.
Each keyword represents some piece of information that has been modeled in the database as data or metadata. We call con-figuration the description of what each keyword models in a spe-cific data source and to which metadata/data may be associated to. In general, there are many possible configurations for a keyword query: the same keyword may be retrieved as domain value of sev-eral attributes, or may be at the same time part of the domain of an attribute and name of some schema element. As usual in these ap-proaches, a score is computed for each configuration, representing its likelihood of describing the user intended meaning.

We defi ne as database vocabulary V D the set of all the tables and all the attributes and their respective domains in a database D . We distinguish two subsets of the database vocabulary: the schema vocabulary V SC , i.e. the list of the database tables and attributes, and the domain vocabulary V DO , i.e., the attribute domains. We call as database term a generic element of V D .

We assume that: (i) each keyword cannot have more than one meaning in the same configuration, i.e., it is mapped into only one database term; and (ii) every keyword is relevant to the database content, i.e., keywords always have a correspondent database term. Furthermore, while modeling the keyword-to-database term map-pings, we also assume that every keyword denotes an element of interest to the user, i.e., there are no stop-words or unjustified key-words in a query.

Answering a keyword query over a database D means finding the SQL queries that describe its possible semantics in terms of the database vocabulary. Each such SQL query is referred to as an interpretation of the keyword query in database terms. An inter-pretation is based on a configuration and includes in its clauses all the database terms that are par t of the configuration. A more de-tailed elaboration on the issues related to the query formulation is out of the scope on this paper. In the following, we will study the application of HMMs to the computation of configurations. We model the process for computing configurations using a HMM, where the keywords inserted by the user are the observable part of the process, while the correspondent database terms are the unknown variables that have to be inferred. An initial HMM pa-rameter estimate is defined by means of some heuristic rules that are based on the database schema [4].

By means of these parameters, the HMM is able to work without any training set, and is used to compute the top-K configurations by applying the List Viterbi algorithm. List Viterbi produces a ranked list of the K sequences of states that have the highest probability of having generated a given sequence of observations. This is equiva-lent of finding, given a keyword query KQ =( k 1 ,k 2 ,...,k top-K sequences of database terms SDT =( sdt 1 ,sdt 2 ,...,sdt that best represent the intended meaning of the query, i.e, the top-K configurations.

Is important to note that keyword queries are composed of free text while the observations in a HMM are constrained to assume a fixed number M of possible symbols. For this reason, the majority of the applications in the literature [11] assumes a closed vocab-ulary of possible symbols that is known in advance. However, in our applicative scenario, this assumption is not realistic since it is not possible to predict all the possible keywords that the users will request to the system. Moreover, since we assume that the database instance is not available, we cannot use it to extract a closed key-word vocabulary. We adopt, instead, an open world assumption where the observation symbols are not known a-priori and their number M can vary during time. Therefore, the multinomial dis-tributions used to represent the emissions take into account the new keywords that the users type during time. This implies that, when a new symbol is inserted, the size of the vocabulary M is increased and the previously calculated probabilities are normalized. Our ex-periments empirically show that, even after removing the closed world assumption, the algorithm converges fast and achieves high performances.
A theoretical framework that permits the study of variations of the EM algorithm, including incremental versions, has been pro-posed in [8]. In this work, the EM algorithm is viewed as maximiz-ing a joint function of the parameters and of the distribution over the hidden variables that justify the partial E-step employed in the online versions. Following this work, in [7] an online version of the segmental K-means algorithm has been derived and implemented in a speech recognizer. Our approach extends these works in order to take into account for training the top-k MLSSs calculated by the List Viterbi algorithm.

There has been already a number of different systems that con-sider keyword searching over structured or semi-structured data. Well-known systems include BANKS, DISCOVER, DBXplorer, QUICK, SQAK, PerK, DivQ and many others presented in various surveys [5, 16]. Their typical approach is to perform an off-line pre-processing step that scans the whole data instance and constructs an index, a symbol table or some structure of that kind which is later used at run-time to identify the parts of the database in which each keyword appears. After that, they perform a path discovery algorithm to find the different ways in which these tables are con-nected. In contrast to all these approaches, our system is able to achieve similar results without the need of accessing and scanning the dataset instance.

A related field is keyword disambiguation where several ap-proaches have been developed. For example, in [1] an incremental technique based on WordNet and Description Logics is proposed, in [14] context and ontologies are exploited for removing ambigu-ity, and in [9] attribute disambiguation enables faceted searches.
To the best of our knowledge, HMMs have been applied in field related to the keyword search over databases only in [10], where a HMM has been exploited for performing keyword query cleaning. Nevertheless, the approaches differ in several aspects: in particular, [10] requires to access the database instance for setting the HMM parameters and its training is based on a gradient based optimiza-tion algorithm. Moreover we propose an extensive evaluation of our approach in different domains. Finally, Keymantic [2, 3] has goals similar to ours but it follows a fundamentally different ap-proach. It handles the keyword search as a bipartite graph assign-ment problem and finds the solutions using an extended version of the Hungarian algorithm.
The evaluation of keyword-based search engines is a difficult task, mainly due to the lack of standard evaluation criteria and benchmarks allowing the comparison with other approaches [15]. Only recently, [6] proposed a benchmark composed of 3 datasets and 50 queries for each one of them and evaluated some of the most popular searching systems against it according to four met-rics (the number of top-1 relevant results, the reciprocal rank, the mean average precision and the 11-point interpolated average pre-cision). Nevertheless, a direct comparison of our approach against this benchmark is substantially unfair, since we are interested in finding configurations, i.e. associations of keywords to database terms. Moreover, the benchmark, and the existing searching tech-niques, assume the availability of the data instance, thus making the comparison based on unequal footing.

In the following, our approach is evaluated according to three perspectives: a) the convergence speed, i.e. how many queries are required for training the systems; b) the effectiveness of the ap-proach, i.e. how much the system outputs are relevant to the user X  X  information need; c) the time performances, i.e. the time required by the algorithm for generating the configurations.
 Datasets. For the evaluation of our approach we used three databases: IMDB 1 , Mondial 2 and UNIV . IMDB and Mondial have been frequently used for the experiments of keyword search sys-tems [6]. The database UNIV is a fragment of an internal database used in our department for managing research and teaching activi-ties. The databases are respectively composed of 197, 303 and 131 terms.

The construction of a query training dataset is a critical task since synthetic keyword queries do not resemble the actual distribution of information needs, and, on the other hand, self-authored queries have a strong potential for bias. Moreover, query logs cannot be used for evaluation, since they are missing the intended meaning of the users X  queries required for measuring the algorithm perfor-mances. Finally, it is difficult to have a sufficient number of queries formulated by independent third parties with a description of their intended meaning. For this reason, we adopted a mixed approach, that exploited a small dataset of queries provided by independent users for creating a large dataset. In particular, we asked a small set of real users (29) to provide keyword queries for these databases alongside a natural language explanation of what they were look-ing for. We obtained a total of 44, 76 and 99 queries for the IMDB, Mondial and UNIV database respectively. A database expert trans-lated each natural language explanation of what the user was look-ing into a configuration, obtaining a reference to evaluate the results returned by our system. These configurations have been exploited by a software application as a template for the generation of new keyword queries, having the same structure, but different values that are extracted from the database. For the evaluation we adopted a 10-fold cross validation approach, where each fold is composed of 10000 keyword queries. In each fold, 9000 queries are used as training dataset, as described in the experiments in Section 5.1, and 1000 as test dataset, as described in the experiments in Section 5.2. We performed the experiments for each fold, and we computed the result as the average of the results obtained in each fold. Setup. The experiments in this section were performed on a Win-dows machine with a 2.27GHz Centrino Duo vPro 32bit and 3GB of RAM. The system is implemented in C++.
We expect that our application will be used for querying un-known databases whe re little or no informa tion about the queries previously submitted to the system is available for the training. In such case, the implicit feedback provided by the user selecting an answer among the top-K returned by the system can be used for the training as supervised data. In this specific applicative scenario, there is only one correct answer provided by the user. Neverthe-less, the algorithm can be used also when the feedback is a set of answers. In other cases, a small set of queries along with their configurations could be available and used for a supervised initial training phase. In absence of such an information, the top-K results are used as unsupervised data for the training. http://www.imdb.com/ http://www.dbis.informatik.uni-goettingen.de/Mondial/ (a) Learning curve without any query super-vised
We conducted the training with an initial phase of supervised data and after that a semi-supervised training is adopted to simulate the users X  behavior (we assume that the users X  feedbacks are not always available). We also tested the supervised algorithm, which is a reference point for the performances.

In Figure 1 we experimented the approach against the UNIV database, taking into account the accuracy as percentage of cor-rect answers with rank 1 and different amounts of supervised ini-tial training queries, ranging from 0 (see Figure 1(a)) to 400 queries (see Figure 1(c)). In the semi-supe rvised phase, different levels of user feedback have been consider ed. In particular, we considered scenarios where the user provides a feedback for 1 out of 2, out of 3, out of 4, and out of 5 results respectively. Note that, in Figure 1, a label of a series with the format ###S_1/X means that there is a supervised initial training set of ### queries and after there is a user feedback for 1 out of X queries (i.e., 1 out X queries in the dataset is supervised); ###S_U means that after ### queries super-vised the approach is unsupervised; S means that the approach is completely supervised. We used small blocks of 5 records for each iteration of the online algorithm, thus generating 1800 iterations of the algorithm, as shown in figure.

The results show that a small initial set of supervised queries is useful for achieving a good training. Figure 1(a) for example shows that with an initial set of 100 supervised queries and a user feedback about 1 out of 2 results, the system converges after 500 it-erations on values with an accuracy greater than 85%. With a larger set of initial supervised queries, the algorithm converges faster and the effects of the user feedbacks are less marked (see Figure 1(c)). However, a small amount of supervised training data is necessary because an a-priori semantics to be learned by the system is as-sociated to each HMM state, i.e., it represents a specific database term.
 Obviously, the learning curves are different in different domains. In Figure 2(a), a comparison between the UNIV ,the Mondial ,and the IMDB database is shown. As expected, the larger number of terms in the IMDB and Mondial databases requires a larger dataset for converging. Nevertheless, the algorithm converges fast.
In order to evaluate the effectiveness of the approach, we tested the algorithm with different training settings and we measured how many times the configuration expected by the user was part of the first, second, ..., tenth results computed by the algorithm. Fig-ure 3 shows 3 the results of this experiment considering the UNIV database after the training represented in Figure 1. The worst re-sults are obtained in absence of a supervised initial training set as shown in Figure 3(a). In this test condition, the differences in the amount of user feedbacks during the training vary the accuracy on the first answer from 48% (users have provided a feedback on 1 out of 5 queries) to 75% (the feedback was about 1 out of 2 queries).
The importance of the user feedbacks decreases with the num-ber of supervised initial queries. There is no substantial difference among the training conditions if we consider the cases where the user expected results are in the top-10 results provided by the algo-rithm as shown in Figure 3.

The Mean Reciprocal Rank (MRR) provides a concise indica-tor about the effectiveness of the approach. The comparison of the MRR values computed against the three databases in Table 1 shows that increasing the database dimension, the number of records in the training dataset needs to increase for obtaining the same per-formances. This is motivated by the direct relationships between the number of database terms and number of parameters in the HMM. Nevertheless, in all the settings we obtained good results,
This figure has the same legenda of Figure 1. See Section 5.1 for further details. even compared with the ones obtained by other approaches in sim-ilar domains [6].
We studied the time performance in answering keyword queries of the system with respect to three different parameters: the num-ber of keywords in a query, the number of configurations to com-pute and the number of the database terms. The results, depicted in figure 4, show performances consistent with the computational complexity of the List Viterbi algorithm O ( lKd 2 ) ,where l is the number of keywords, K the number of results and d the number of database terms. In the experiments above, we did not report the time needed to actually execute the queries on the database, but only for computing the configurations in order to avoid having the query engine performance blurring the results. In this paper, we proposed the batch and the online version of List Viterbi training , a version of the EM algorithm that, for each query in the dataset, considers for training only the top-K MLSSs generated by List Viterbi. List Viterbi and List Viterbi training have been applied to a hot challenging scenario such as keyword-based searching system over relational databases. The experimental re-sults showed that List Viterbi training fast converges to high accu-racy levels after a small number of iterations and that List Viterbi provides an effective and the efficient solution to the keyword-based search problem.

Future work will be devoted to the application of the approach to multiple distributed data sources, where solving a keyword query implies to find the data sources relevant for the queries (since they provide a result to all or part of the query), defining a query plan and fusing the obtained result in a unique answer.
 This work was partially supported by the project  X  X earching for a needle in mountains of data X  (http://www.dbgroup.unimo .it/keymantic/). [1] S. Bergamaschi, P. Bouquet, D. Giacomuzzi, F. Guerra, [2] S. Bergamaschi, E. Domnori, F. Guerra, R. T. Lado, and [3] S. Bergamaschi, E. Domnori, F. Guerra, M. Orsini, R. T. [4] S. Bergamaschi, F. Guerra, S. Rota, and Y. Velegrakis. A [5] S. Chakrabarti, S. Sarawagi, and S. Sudarshan. Enhancing [6] J. Coffman and A. C. Weaver. A framework for evaluating [7] V. Digalakis. Online adaptation of hidden markov models [8] R. Neal and G. E. Hinton. A view of the em algorithm that [9] J. Pound, S. Paparizos, and P. Tsaparas. Facet discovery for [10] K. Q. Pu. Keyword query cleaning using hidden markov [11] L. R. Rabiner. A tutorial on Hidden Markov Models and [12] N. Seshadri and C.-E. Sundberg. List Viterbi decoding [13] N. Seshadri and C.-W. Sundberg. Generalized viterbi [14] R. Trillo, J. Gracia, M. Espinoza, and E. Mena. Discovering [15] W. Webber. Evaluating the effectiveness of keyword search. [16] J. X. Yu, L. Qin, and L. Chang. Keyword Search in
