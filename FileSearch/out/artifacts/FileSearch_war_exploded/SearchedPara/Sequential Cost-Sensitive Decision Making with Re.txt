
Recently, there has been increasing interest in the issues of cost-seusitive learning and decision making in a variety of applications of data mining. A number of approaches have been developed that are effective at optimizing cost-sensitive decisions when each decision is considered in isolation. How-ever, the issue of sequential decision making, with the goal of maximizing total benefits accrued over a period of time instead of immediate benefits, has rarely been addressed. 
In the present paper, we propose a novel approach to se-quential decision making based on the reinforcement learn-ing framework. Our approach attempts to learn decision rules that optimize a sequence of cost-sensitive decisions so as to maximize the total benefits accrued over time. We use the domain of targeted' marketing as a testbed for em-pirical evaluation of the proposed method. We conducted experiments using approximately two years of monthly pro-motion data derived from the well-known KDD Cup 1998 donation data set. The experimental results show that the proposed method for optimizing total accrued benefits out-performs the usual targeted-marketing methodology of op-timizing each promotion in isolation. We also analyze the behavior of the targeting rules that were obtained and dis-cuss their appropriateness to the application domain. 1.2.6 [Artificial Intelligence]: Learning 
Algorithms *Additional authors: Haixun Wang, Wei Fan, and Chi-danand Apte ~The work presented herein was performed while this author was visiting IBM T.J.Watson Research Center. permission and/or a fee. SIGKDD '02 Edmonton, Alberta, Canada 
Copyright 2002 ACM 1-58113-567-X/02/0007 ... $5.00. In the last several years, there has been an increasing in-terest in the machine learning community on the issue of cost-sensitive learning and decision making. Various au-thors have noted the limitations of classic supervised learn-ing methods when the acquired rules are used for cost-sensi-tive decision making (e.g., [13, 4, 5, 17, 8]). A number of cost-sensitive learning methods have been developed [4, 17, 6] that have been shown to be superior to traditional classification-based methods. However, these cost-sensitive methods only try to maxi-mize the benefit (equivalently, minimize the cost) of a single decision, whereas in many applications sequences of deci-sions need to be made over time. In this more general set-ting, one must take into account not only the costs and benefits associated with each decision, but also the interac-tions among decision outcomes when sequences of decisions are made over time. For example, in targeted marketing, customers are often selected for promotional mailings based on the profits or rev-enues they are expected to generate on each mailing when viewed in isolation. Profits or revenues are estimated us-ing predictive models that are constructed based on histor-ical customer-response data. To maximize expected prof-its for a given promotion, only those customers should be mailed whose predicted expected profit is nonzero when tak-ing mailing costs into consideration [17]. 
However, the above decision policy of selecting customers to maximize expected profits on each promotion in isola-tion is not guaranteed to maximize total profits generated over time. It may be, for example, that the expected profit obtained by mailing the current promotion to a certain cus-tomer might exceed the current cost of mailing; however, it might also increase the profits generated by that customer in future mailings. More generally, marketing actions that are desirable from the perspective of maximizing customer loyalty over time may sacrifice immediate rewards in the anticipation of larger future revenues. 
The opposite can also be true. Saturating profitable cus-tomers with frequent promotional mail might decrease the profitability of those customers, either because of the am noyance factor or becau~ of the simple fact that everyone has budgetary limits on the amounts they are willing or are able to spend per unit time. The latter implies that a point of diminishing returns will necessarily be reached for each customer as the frequency of mail they receive increases. 
In the present paper, we propose to apply the framework of reinforcement learning to address the issue of sequential decision making when interactions can occur among deci-sion outcomes. Reinforcement learning refers to a class of problems and associated techniques in which the learner is to learn how to make sequential decisions based on delayed reinforcement so as to maximize cumulative rewards. More specifically, we adopt the popular Markov Decision Process model with function approximation. In a Markov Decision Process (MDP), the environment is assumed to be in some state at any given point in time. In the case of tar-geted marketing, such states would be represented as feature vectors comprising categorical and numerical data fields that characterize what is known about each customer at the time a decision is made. 
When the learner takes an action, it receives a finite re-ward and the environment makes a probabilistic transition to another state. The goal of a learner is to learn to act so as to maximize the cumulative reward it receives (usually with future rewards discounted) as the learner takes actions and traverses through the state space. 
In the example of targeted marketing, a customer, with all her past history of purchases and promotions, is in a certain state at any given point in time. When a retailer takes an action, the customer then makes a probabilistic transition to another state, possibly generating a reward. This process continues throughout the life of the customer's relationship with the retailer. The reward at each state transition is the net profit to the retailer. It takes into account both the purchases by the customer in response to the retailer's action and the cost of that action. The reward can thus be negative if the customer makes no purchases, which represents a net loss. Application of reinforcement learning to this problem amounts to maximizing the net present value of profits and losses over the life cycle of a customer. 
As a proof of concept, we test our approach on the well-known donation data set from the KDD Cup 1998 compe-tition. This data set contains approximately two years of direct-mail promotional history in each donor's data record. We transformed this data set and applied a reinforcement learning approach to acquire sequential targeting rules. The results of our experiments show that, in terms of the cumu-lative profits that are obtained, our approach outperforms straightforward (repeated) applications of single-event tar-geting rules. We also observe that the sequential target-ing rules acquired by our proposed methods are often more cost-containment oriented in nature as compared to the cor-responding single-event targeting rules. 
The rest of the paper is organized as follows. In Section 2, we describe our proposed approach, including the reinforce-ment learning methods and the base learning algorithm we employ for function approximation. In Section 3, we explain our experimental setup, including the features employed and how the training data are generated from the original KDD Cup data set. We also describe the simulation model we use to evaluate the performance of our approach. In Section 4, we present the experimental results. Section 5 concludes with a discussion of results and future research issues. 
For illustrative purposes, we will make use of the domain of targeted marketing throughout this paper. However, it should be noted that the approach set forth in the present paper is applicable to a wide variety of applications. We will use the term single-event targeted-marketing approach to mean one in which customers are selected for promotions based on maximizing the benefits obtained from each pro-motion when each is considered in isolation. A sequential targeted-marketing approach, by contrast, is one in which a series of promotional actions are to be taken over time, and promotions are selected for each customer based on max-imizing the cumulative benefits that can be derived from that customer. In an ideal sequential targeted-marketing approach, each decision would be made with the goal of maximizing the net present value of all profits and losses ex-pected now and in the future. The challenge in implement-ing a sequential targeted-marketing approach lies in the fact that information about the future is available only in a de-layed fashion. We appeal to the apparatus of reinforcement learning to resolve this difficulty. 
As briefly explained in the introduction, we adopt the popular Markov Decision Process (MDP) model in reinforce-ment learning with function approximation. For an intro-duction to reinforcement learning see, for example, [11, 7]. The following is a brief description of an MDP. 
At any point in time, the environment is assumed to he in one of a set of possible states. At each time tick (we assume a discrete time clock), the environment is in some state s, the learner takes one of several possible actions a, receives a finite reward (i.e., a profit or loss) r, and the environment makes a transition to another state s'. Here, the reward r and the transition state s' are both obtained with probability distributions that depend on the state s and action a. 
The environment starts in some initial state so and the learner repeatedly takes actions indefinitely. This process results in a sequence of actions {at}t=o, rewards {rt}~=o, and transition states {st}~=l. The goal of the learner is to maximize the total rewards accrued over time, usually with future rewards discounted. That is, the goal is to maximize the cumulative reward R, where rt is the reward obtained at the t'th time step and "7 is some positive constant less than 1. In financial terms, '7 is a discount factor for calculating the net present value of future rewards based on a given interest rate. 
Generally speaking, a learner follows a certain policy to make decisions about its actions. This policy can be repre-sented as a function 7r mapping states to actions such that 7r(s) is the action the learner would take in state s. A theo-rem of Markov Decision Processes is that an optimum pol-icy It* exists that maximizes the cumulative reward given by Equation 1 for every initial state so. 
In order to construct an optimum policy 7r  X  , a useful quan-tity to define is what is known as the value function Q'~ of a policy. A value function maps a state s and an action a to the expected value of the cumulative reward that would be obtained if the environment started in state s, and the learner performed action a and then followed policy r for-
A remarkable property of Markov Decision Processes is 
The Bellman equation can be solved via fixed-point iter-
Qk+l(S,~) = R(s,~) 
The use of Equation 4, however, requires knowledge of 
Several approaches are known in the literature. One popu-
The policy that is followed during Q-learning must there-
One drawback of Q-learning is that it has a tendency to 
Another popular learning method, known as sarsa [10], is 
When the policy is not updated but is held fixed, it can 
In the foregoing description of reinforcement learning, two each customer. To ensure a fair comparison, ProbE's multi-variate linear-regression tree method was used to construct models for both targeting strategies. A single-event tar-geting strategy was constructed by applying the procedure shown in Figure 1 with final set to a value of zero. Doing so causes the reinforcement learning loop at line 4 to be omit-ted, thereby producing a policy that maximizes immediate reward. Because the same base learning algorithm is used for constructing both single-event and sequential marketing strategies, any differences in performance that are observed should reflect inherent differences in the strategies. 
An important issue in any data mining application is that of scalability. This is especially critical in applying our ap-proach to domains like targeted marketing. Not only can the volume of the business transaction data be huge (well over millions of records), but the iterative nature of rein-forcement learning requires generating a sequence of models from such data. 
In an effort to lighten the load of data size, we consider a series of sampling methods that are specifically designed for hatch reinforcement learning. One obvious approach is ran-dom sampling. However, more efficient sampling methods can be obtained by taking into account the episodic nature of the data and the objectives of the learning strategy. 
Recall that in batch reinforcement learning, training is performed on data that have already been collected, pre-sumably using some sampling or control policy. This is to be contrasted with the online learning setting, in which the learner has control over the sampling policy. However, in domains that involve a potentially huge amount of data, it is possible to simulate online reinforcement learning with a particular policy by electing to use just those data that conform to the policy. 
Based on this latter idea, we propose a sampling method we call Q-sampling in which only those states are selected that conform to the condition that the action taken in the next state is the best action with respect to the current es-timate of the Q-value function. The value update is akin to Equation 7 used in sarsa-learning, but the effect of the learn-ing that occurs corresponds to Equation 5 used in Q-learning because the sampling strategy ensures that Q(st+l, at+l) = 
Taking this line of reasoning a step further, it is also pos-sible to look ahead an arbitrary number of states and select only those states in which optimal actions are taken in all of those subsequent states, In this case, it makes sense to take advantage of the lookahead that is being done for up-dating the Q-vaiue. There is a well-known method of value update with lookahead known in the literature as TD(A). This method updates the value function estimate using a weighted average of the Q-value estimate from the last state and the discounted partial sums of rewards obtained over the next several states. More precisely, TD(A) uses the fol-lowing update rule for estimating Q-vaiues: where Procedure Bat choRL (Random-sampling) Premise: A base learning module, Base, for regression is given. 
Input data: D = {ei[i = 1, ..., N} where 1. For all e, in randomly selected subset R0 of D 2. Do = U,ieR Do,i 3. Q0 = Base(D0). 4. For k = 1 to final 5. Output the final model, Qf~,~,t. Procedure Bateh-RL(Q-sampling) Identical to Batch-RL(Random-sampling) except that 
Block 4.1 is replaced by: and where R~ ") is the so-cailed n-step return defined as fol-lows. We employ this update rule in our sampling method based on multistep lookahead, and thus name it TD(A)-sampling. 
Pseudo-code for the above three sampling methods is pre-sented in Figures 3, 4, and 5. The Q-sampling and TD(A)-sampling strategies are presented as variants of the basic random sampling method shown in Figure 3. Note that dif-ferent random samples are selected at each iteration at line 4.1 of these algorithms. This is done to reduce overfitting effects as updated Q-value functions are learned at line 4.3. 
As mentioned in the introduction, we performed prelim-inary evaluation experiments using an existing benchmark data set in the general domain of targeted marketing, and using simulation. We use the well-known donation data set the state features and the action taken, and the other for the amount of donation given that there is a response, as a function of the state features and the action. The model was constructed using ProbE's naiveBayes tree algo-rithrn, while A(s, a) was constructed using ProbE's linear-regression tree algorithm. 
Given models for P(s, a) and A(s,a), it is possible to con-struct an MDP in the following way. First, the immediate reward r(s, a), for a given state, action pair can be speci-fied using the two models as follows: Flip a coin with bias P(s, a) to determine if there is a response. If there is no response, then the amount of donation is zero. If there is a response, then determine the amount of donation as The reward obtained is the amount of donation minus the mailing cost, if any. Next, the state transition function can be obtained by calculating the transition of each feature us-ing the two models. For example, ngiftall (number of gifts to date) is incremented by one if the above coin with bias P(s, a) came up heads; otherwise, it remains unchanged. Similarly, numprom (number of promotions to date) is in-cremented if the action taken was 1, and remains constant otherwise. Using the above two features, frequency (i.e., ngiftall / numprom) can be computed. Updates for other features are computed similarly. 
Given the above functional definition of an MDP, we con-ducted our evaluation experiment as follows. Initially, we selected a large enough subset (5,000) of the individuals, and set their initial states to correspond to their states prior to a fixed campaign number (in experiments reported here, campaign number 7 was used). We then throw all these in-dividuals to the MDP and use the valuefunction output of our batch reinforcement learning procedure to make deci-sions about what actions to take for each individual. Utiliz-ing the response probability model and the expected amount model, we compute the resulting rewards and next states. 
We record the rewards thus obtained, and then go on to the next campaign. We repeat this procedure 20 times, simu-lating a sequence of 20 virtual campaigns. tion concerning our premise that online interaction with an 
MDP is infeasible. A natural inclination may be to use the above MDP as a model of the environment, and use an on-line learning method (such as online versions of sarsa and 
Q-learning) to estimate the value function from interactions with it. Our view is that the human behavior in application domains such as targeted marketing is too complicated to be well captured by such a simplified model of MDP. We are using the simulation model to evaluate the policy obtained by our method, only as a preliminary experiment prior to a real-world evaluation experiment. using a simulation model. We evaluate our proposed ap-proach with respect to a number of performance measures, including the total lifetime profits obtained and the quali-tative behaviors of the acquired targeting rules. important, measure of total cumulative benefits (lifetime profits) obtained by the competing methods. In particular, we compare the lifetime profits obtained by two variants of reinforcement learning to that obtained by the single-event targeting method. Here, the single-event method is obtained by using the base regression module to learn a model of the expected immediate rewards (profits) as a function of state features and the action, and then mailing to an individual just in case the expected immediate reward for mailing ex-ceeds that for not mailing, at each campaign. Notice that, since the state features contain temporal information, such as recency, frequency, and the number of recent promotions, the targeting decisions obtained this way are sensitive to the past history and, hence, to the campaign number. 
Figure 6 shows the total life-time profits obtained by the sarsa-learning version of batch reinforcement learning, plot-ted as a function of the number of value iterations per-formed. The plots were obtained by averaging over 5 runs, each run with episode data size 10,000, which translates to training data size of 160,000 for reinforcement learning (i.e., 10,000 episodes times 16 campaigns). The total profits are obtained using the simulation model as described in the pre vious section, and totaled over 20 campaigns. The error bars shown in the graph are the standard errors calculated from the total profits obtained in the five independent runs, namely where P~ is the total profit obtained in the i-th run, P is the average total profit, and n is the number of runs (5 in this case). Note that the iteration number 0 corresponds to the singleevent targeting method. Thus, the total lifetime profits obtained in later iterations represent statistically sig-nificant improvements over the singleevent approach. versions of batch reinforcement learning methods, sarsa and 
Q-learning. Figure 7 shows the total profits obtained by these two versions, again using 10,000 episode data and aver-aged over five runs. These results show that, in this particu-lar case, Q-learning resulted in a more profitable policy than sarsa-learning, although the statistical significance of the dif-ference was unconvincing with our limited experimentation. 
This is indeed not surprising considering that Q-learning at-tempts to obtain the optimal policy, whereas sarsa-learning is trying to perform a local improvement based on the cur-rent policy. In the context of batch reinforcement learning, lip 9-
T-
In addition to analyzing the profits that were attained, 
We also examined the policy obtained by the Q-learning 
The profitability of a policy is obviously an important 
How is it possible that the cost-containment oriented poli-
Note also that profits are obtained during campaign 2 
We also conducted experiments to examine the effect of 
