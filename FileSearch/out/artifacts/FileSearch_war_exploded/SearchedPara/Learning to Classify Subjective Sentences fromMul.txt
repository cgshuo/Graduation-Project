 Sentence subjectivity is very crucial to Opinion Mining and Sentiment Classi-fication. Therefore, it is very importan t to differentiate between subjective and objective sentences such that subjectiv e sentences get higher weights compared to objective sentences. The process of identifying such subjective sentences is termed subjectivity classification . Subjective sentences contain certain level of subjectivity (e.g. presence of modifiers (a djectives), intensifiers and diminishers) or express a private state such as described in [1]. Given the many ways sub-jectivity can be expressed, it is often difficult to efficiently identify subjective sentences from opinionated documents. The problem has indeed been the focus of the research community over the past decade. Subjectivity may not only be de-termined by modifiers and intensifiers. Study has also shown nouns (e.g. garbage and prank) and verbs (e.g. admire, prefer and hate) to be very good indicators of subjectivity in sentences [1,2]. Interest ingly, objective sentences (factual sen-tences) may also express subjectivity. For example, the sentence  X  X he phone worked for an hour and no more X  expresses certain level of  X  X ndesirable X  fact about the entity  X  X hone X . Such subjectivity is often difficult to identify in sen-tences. Idiomatic and Sarcastic expressi ons may also be subjective. For example,  X  X ord is a giant in the auto industry X  and  X  X  cubicle is just a padded cell without adoor X  are two idiomatic and sarcastic sente nces, respectively. While the for-mer expresses appraisal on the Ford brand of automobile (very commonly used in news), the latter expresses discomfort on an office cubicle. In our experience, such sentences are very challengin g to classify as subjective or not.
In this study, we investigate a semi-supervised approach that uses a novel and extended set of predictive features to classify sentences from different do-mains. The idea is to understand how best we might capture subjective sentences across different domains, and with good precision. First we developed a pattern-based algorithm that extracts subjective predicates from a large collection of blog documents. The subjective predicates are then combined with an extended subjectivity lexicon that contains subject ive clues and subjective annotations de-rived from the MPQA corpus [1]. The resulting lexicon including the subjective predicates is then used to identify subj ective sentences for training a subjec-tive classifier. On different datasets, we show that our approach outperform a state-of-the-art High-precision subject ivity classifier in terms of precision, recall and F-measure, respectively. Finally, we p erform cross-domain subjective classi-fication, training with a balanced mixture of subjective sentences from different domains (e.g. blogs, news, and movie re views) and objective sentences from Wikipedia. The classifier achieved a better cross-domain accuracy of 84.6%.
The rest of this paper is organized as follows. We discuss related research work in Section 2. Section 3 describes the construction of the extended lexicon with multiple subjectivity clues. In Sectio n 4, we extracted subjective predicates from a large collection of subjective sen tences and add such predicates to our extended lexicon. Experiments and resul ts are presented in Section 5. Finally, we give conclusions on our study in Section 6. In the literature, subjectivity classificat ion is often treated as supervised learn-ing problem. Early works such as [3,4] use machine learning techniques (e.g. Na  X   X ve Bayes(NB) and Support Vector Machines (SVM)) with different types of features. In [3], the presence of pronouns ; adjectives; cardinals; modals (except will ); and verbs (except not ) were used as features to classify subjective sen-tences. In [4], a subjectivity detector was used to detect subjective sentences based on minimum cuts in sentence graph. The technique first built sentence graph using local labelling consistences that produce association score between two sentences. Sentences with similar asso ciation scores are more likely to belong to the same subjective or objective classe s. More recently, [5] used nouns abstrac-tion for in-domain and cross-domain subjectivity classification. While the above approaches have performed moderately, however, it is still difficult to determine effective features for subjectivity classification [1,2,6].

Unsupervised and semi-supervised techniques have also been applied to sub-jectivity classification. [7] used an unsupervised technique to detect the presence of subjective expressions using opinio n seed words and similar words based on distributional similarity. [8,9,10] used the number of subjective adjectives that occur within a sentence window to retrieve subjective documents. Those tech-niques recorded certain level of success a nd show that subjective adjectives can indeed be beneficial to subjectivity classification. [11] proposed a semi-supervised technique that uses a bootstrapping process to learn extraction pattern for sub-jective sentences. The technique first use d a High-precision subjective classifier to label subjective sentences using set of subjective clues. Linguistic patterns are then extracted from the subjective sente nces using a learning algorithm similar to AutoSlog-TS [12]. Additional subjectiv e sentences are further identified with the linguistic patterns for the purpose of training a subjective classifier.
Our technique differs from existing works by using a large number of subjec-tive predicates (which are independent of any domain) combined with various subjectivity indicators. This allows the classifier to be used for both in-domain and cross-domain subjectivity classification with better performance. In this section, we describe the extended s ubjectivity lexicon creation technique. The technique is further improved in Section 4 using the subjective predicates approach. Our goal is to automatically identify likely subjective sentences with high precision using a set of 8221 subjective clues [13], and the subjective anno-tation spans which are manually annotated in the MPQA corpus [1].

The 8221 subjective clues presented in [13] was aggregated from different sources including the subjective adjectiv es presented in [8] and the subjectivity clues presented in [14]. The subjective annotations in MPQA corpus was achieved by using the annotation technique described in [1]. 535 news articles from 187 different sources were manually annotated using the GATE 1 implementation. Using the MPQA corpus, we extract subj ective expressions which satisfy the following MPQA subjectivity annotation pattern: INSIDE(text: X  X lice love Chocolate X ; source:writer/Ryan); DIRECT-SUBJECTIVE(text: X  X ove X ;
Using the subjectivity annotation pattern , a sentence is considered subjective if the output contains a  X  X irect-subjective X  annotation component. In addition, the  X  X ntensity X  attribute must be  X  X igh X ; the  X  X xpression-intensity X  attribute must be  X  X igh X ; and the  X  X nsubstantial X  attribute must be  X  X alse X . A sentence is also considered subjective if and only if the annotation output contains an  X  X xpressive-subjectivity X  annotation co mponent with all  X  X ntensity X  attributes that must not be  X  X ow X  [1].

It could be straightforward to directly train a subjective classifier using the subjective sentences extracted by the pa tterns above. However, the application of such classifier would give less performance on other domains without adequate domain adaptation technique [15]. It is important to state that the MPQA corpus consists of news articles. Thus, extracting only the necessary and independent subjective features from the subjective sen tences could increase the performance of a subjective classifier. An evaluation results to that effect is presented in our experiment Section 5. Since our goal is to construct a high precision domain-independent subjective classifier, we did not use the entire MPQA subjective sentences for training. Instead, we extracted only the annotated spans from the subjective sentences in the corpus. We not iced that each annotated span consists of a  X  X ord X  or a  X  X hrase X  or rarely, a clause, in the annotated discourse. In MPQA corpus, an annotated span is the start and end byte of an annotation [1]. For example, an annotation could span from byte 718 to byte 748 in a paragraph of a news article as shown in Figure 1. In this case, the subjective annotation is  X  X eeling of uncertainty X  .

Thus, we combined the extracted subjective annotations with the 8221 subjec-tive clues to form an extended subjectivity lexicon. Using an empirical threshold  X  , the lexicon is used to automatically id entify subjective sentences from each of our datasets. Where  X  is the number of clues and/or annotations that must be present in a subjective sentence.

In order to select an optimal threshold, we conducted a pilot study on a la-belled subjective and objective datasets by setting  X  to 1, 2, 3 and 5, respectively. The subjective dataset 2 was introduced in [4]. It contains 5000 subjective sen-tences or snippets from Rotten Tomatoes 3 . However, we did not use its set of objective sentences which are from IMDb plot summaries . Our pilot study re-veals that the plot summaries contains considerable number of subjective words. Rather, we used 5000 Wikipedia sentences that were extracted across the eight Wikipedia categories. We assume Wikipedia could be a good source of objec-tivity. Thus, using the subjectivity lex icon, we extracted subjective sentences from the Rotten Tomatoes dataset by setting  X  one after the other. This process was also repeated on the Wikipedia sentences. The idea was to identify what  X  retrieves a larger percentage of the labelled subjective sentences and at the same time minimises the number of objective (Wikipedia) sentences retrieved. We set  X   X  3 to retrieve subjective sentences across all our multiple datasets. Given a sentence, the subjectivity is determined as follows: where S sbjv is the subjectivity determinant and C s is the number of subjectivity clues in a given sentence S . A subjective sentence is identified when C s is more than or equal to  X  , otherwise the sentence is objective.
 Table 1 shows the result of a comparative pilot study on different thresholds. Interestingly, we found  X   X  3 to retrieve a larger per centage of subjective sen-tences (98.3%) and when the same thresho ld is used on the Wikipedia sentences, it found 44% subjective sentences leaving 56% as true objective sentences.  X   X  3 also retrieved a combination of short , medium and long sentences compared to starting with a higher threshold such as 5. Note that higher thresholds may likely reduce the number of subjective sentences . Nevertheless, our result is compar-atively better than using the IMDb plot summaries from which we retrieved 97.4% subjective sentences on an assumed  X  X bjective X  dataset 4 . It also confirms our assumption about Wikipedia as a likely source of objectivity.

In order to verify the quality of the extr acted subjective sentences, we trained a subjective classifier and evaluated its performance on a different test set con-taining 1000 manually identified subjective and objective sentences from blogs. With 5-fold cross-validation using NB [16], we achieved 89% precision and 71.5% recall, respectively (see Table 4). This r esult makes sense given the difficulty of the task on unseen sentences [2]. To im prove on this performance, we there-fore propose to extract subjective predicates from a large collection of subjective sentences. The subjective predicates will be added to the extended lexicon for improved accuracy. We define  X  X ubjective predicates X  as predicates that exist within an already identified subjective sentence. A predi cate contains a verb o r verb phrase with optional compliments. Predicate  X  X erbs X  are transitive verbs because they re-quire one or two compliments unlike  X  X ntransitive verbs X . Below, we show an example sentence with a predicate:  X  X he president fascinates me about the economy . X  In the example sentence, fascinates me is a predicate with a direct object  X  X e X  and indirect object  X  X conomy X . Thus we co uld see that the verb  X  X ascinates X  has two compliments (the direct and indirect objects) which makes it a transitive verb. The sample sentence also illustrates a subjective predicate - X  X ascinates me X . The idea of using subjective predicates to detect subjective sentences is that some sentences may not necessarily contain explicit subjective words such as adjectives (e.g. love, hate, good), ye t they may express certain level of sub-jectivity or private state. The example sentence above meets this criteria as it expresses a  X  X avourable X  subjectivity about the subject (entity)  X  X resident X . Further examples of subjective predicates are shown below: The car drives slowly .
 The product works for me and my family.
 Therefore, extracted subjective predicates can be combined with the existing  X  X ubjective clues X  and  X  X ubjective annotations X  to form a more extended sub-jectivity lexicon . The lexicon can then be used to identify further subjective sentences which are used to improve the accuracy of subjective classifiers.
The proposed technique is similar to the bootstrapping technique proposed in [11]. However, rather than learning case f rame-based extraction patterns, we di-rectly extract subjective predicates to automatically detect additional or omitted subjective sentences. Adding the subjective predicates to the extended lexicon retrieves further subjective sentences from the classified objective sentences (sen-tences that do not contain subjective c lues or subjective annotations by using the previous lexicon) and improves the p recision and recall of the subjective classifier substantially. In total, we extracted 10618 subjective predicates from 20000 subjective sentences from blogs. Table 2 shows the composition of the extended subjectivity lexicon.

We identify subjective predicates from subjective sentences using the tem-plate patterns below: 1. &lt; transitive-verb &gt;&lt; Opt: ANY &gt;&lt; punctuation &gt; 2. &lt; transitive-verb &gt;&lt; Opt: ANY &gt;&lt; Opt: ADJECTIVE &gt;&lt; NOUN &gt; We determine Part-of-Speech using [18]. Below, we show different subjective predicates extracted from some e xample subjective sentences.
 Sentence I: The technology amazes me .
 Thetemplatepattern1extractsthesubj ective predicate  X  X mazes me X  as follows: = X . X .
 Sentence II: The government killed many citizens during the civil war. The template pattern 2 extracts the subjective predicate  X  X illed many citizens X  as follows: &lt; transitive-verb &gt; =  X  X illed X , &lt; Opt: ANY &gt; =  X  X any X  (optional, any word after the transitive verb), &lt; Opt: ADJECTIVE &gt; =  X   X  (optional), &lt; NOUN &gt; = X  X iti-zens X .
 Sentence III: He deceived many American taxpayers inthenameofpeacein the middle-east.
 The template pattern 2 extracts the su bjective predicate  X  X eceived many Amer-ican taxpayers X  as follows: JECTIVE &gt; =  X  X merican X  (optional), &lt; NOUN &gt; = X  X axpayers X .

Table 3 shows additional examples of subjective predicates and their equiva-lent probability of occurrence in a different sample data other than the data from which the subjective predicates were extracted. The probability was computed as the ratio of each predicate to the total number of sentences in the sample data. The idea is to show that the subj ective predicates are likely to appear at least once in any randomly selected sample data. This shows the benefit of subjective predicates in training data. 5.1 Datasets Blogs: Using the extended subjectivity lex icon and the threshold discoursed earlier, we randomly select sample 5000 subjective sentences and 5000 objective sentences from the TREC Blog08 dataset which was crawled over different time frames[17]. The sentences are not speci fic to a particular topic or entity but a sample data that represents how senten ces are generally wr itten in blog docu-ments. Also, the sample data contains different types of blog domains such as music, video, and technology, thus making the sample data quite representative of blogs written from different perspectives.
 Rotten Tomatoes/IMDb: This subjectivity dataset was used in [4]. It con-tains 5000 subjective sentences or snippets which are extracted from Rotten Tomatoes movie reviews. The dataset also contains 5000 objective sentences from IMDb plot summaries following the assumption made in [4], that the IMDb plot summaries are mostly objective sentences.
 MPQA/News: The MPQA opinion corpus contains 535 news articles from 187 different sources. The corpus was manually annotated for different kinds of subjectivity. The annotation scheme used for identifying subjective sentences in MPQA has been discussed earlier in this paper. Using such mechanism, we iden-tified 5000 subjective sentences and 5000 objective sentences for our experiment. Mixed-Subjective: Again, using the extended subjectivity lexicon and appro-priate threshold, we selected a combination of 5000 subjective sentences from the TREC blog dataset, rotten tomatoes movie reviews, and the MPQA news cor-pus. Note that this set of sentences are c ompletely different from the sentences used in the datasets described earlier. The idea is that, as long as the extended lexicon with the subjective predicates is used, it does not matter where one se-lects subjective sentences for training s ubjective classifiers, although, we suggest it should be a combination of more than one domain. We will use this dataset for the cross-domain subjective training set.
 Wikipedia: We mentioned earlier that Wikipedia is a likely source of objectivity following the outcome of our pilot study (see Table 1). Thus, we selected 5000 random sentences from Wikipedia articles using the Wikipedia corpus described in [18]. Again, the dataset is a combination of sentences from the eight top Wikipedia categories 5 ( i.e. Arts, Biography, Geography, History, Mathematics, Science, Society, and Technology). We believe that these categories could be sufficient to capture objectivity from different domains/datasets. We will use the Wikipedia sentences for the cross-domain objective training set. 5.2 Classification We used two baselines in our experiments. One is our implementation of the High-Precision subjective classifier pro posed in [11]. Again, the technique used case frame-based extraction patterns to i dentify subjective sentences. The other baseline is a self baseline using the performance of the in-domain/domain-specific classification on each of the three datasets (Blogs, Rotten Tomatoes, and MPQA). We used the first baseline to validate the significance of the ex-tended lexicon with the subjective predicates over the baseline technique on the same data. The second baseline is used to further measure the performance of the extended lexicon with the subjective predicates in cross-domain classification.
We performed three different classification experiments to validate the pro-posed technique. First, using the different subjectivity criteria for building the extended lexicon, we experimented with a Na  X   X ve Bayes classifier and show the significance of the proposed extended lexicon with the subjective predicates on an out-of-sample blog data (see Table 4). Second, using 5-fold cross validation, we trained another NB classifier and a Language Model (LM) classifier[19], with 60% of each dataset and and tested with the remaining 40%. As per [4], we used unigram features for the NB classifier, denoting the independent count of each word learned from the training set. We used n -gram features for the LM classifier by setting n -gram=6. The study in [20] suggested n -gram &gt; 5 leads to improved classification for LM. Finally, we evaluated the performance of our technique us-ing cross-domain classification. Specifically, we trained the two classifiers on the Mixed-Subjective dataset and tested with the testing set of each domain-specific dataset (i.e. 40%-Blogs, 40%-Rotten Tomatoes, and 40%-MPQA). Both NB and LM classifiers were implemented using the LingPipe 6 NLP APIs.

We refer to Table 4 for the first experiment. We could see that the subjective classifier that combines subjective clues and subjective annotations to extract training sentences has a precision of 89% on the subjective test set (i.e. 89% of the subjective sentences are subjective) and a recall of 70% (i.e. 70% of the entire test set were found to be subjective). The F-measure 78.3% is also encour-aging, while  X  is the relative weight of precision . The interesting thing is that both precision and recall are comparatively high. The performance is also better than the baseline technique. However, our aim is to further increase the perfor-mance of the subjectivity classifier while maintaining the same order of precision and recall. Which is why we proposed the subjective predicates as described in Section 4. We see that the precision of th e subjective classifier improved upon introducing the subjective predicates. The precision is increased by 8.7% and the recall increased by 3.4%. This shows tha t subjective predicates is likely to im-prove subjectivity classification. Overall, the subjective classifier performs better as both precision and recall values are high.

In Table 5, we could see that the LM classifier outperforms the NB classifier on all datasets. We believe this confirms the benefit of using higher order n  X  gram as suggested in [20]. Also, a better improvement is noticed on MPQA news corpus. We suggest this could be as a result of news articles often containing a lot of sarcastic and/or idiomatic expressions[2]. Thus, we think the n -gram based LM classifier was able to capture such expressions as a sequence of words rather than using the NB X  X  independent assumption which avoids dependencies between words.

Table 6 shows the effect of the extended lexicon with subjective predicates on cross-domain classification. The result is rather encouraging given the nature of the cross-domain classification task[15]. Furthermore, the results are compar-atively better than those reported in [5]. They reported 73.9% using Support Vector Machines (SVM) on the Chesley 7 dataset and 74.5% using Linear Dis-criminant Analysis (LDA) on blogs/wiki dataset. We could also see that the difference between the performance of the cross-domain and the in-domain clas-sifications is less than 10% for both NB and LM. We suggest that this is an expected trade-off given the difficulty of the cross-domain classification prob-lem. On the other hand, we improved the accuracy of the NB cross-domain classifier on the MPQA corpus, while the LM cross-domain classifier performed comparably with the in-domain LM classifier. Thus, we believe that the pro-posed extended subjectivity lexicon including the subjective predicates could be helpful to high precision subjectivity classification. In this study, we have used an extended subjectivity lexicon including subjec-tive predicates to improve in-domain and cross-domain subjectivity classifica-tions. The proposed subjectivity classification technique is comparatively better than a in-domain baseline technique and has improved accuracy on cross-domain subjectivity classification. The significance of the technique is that it identifies subjective sentences with high precision regardless of the domain. This suggests the improved accuracy on the cross-domain subjectivity classification. In future, we will investigate additional linguistic features that could improve the accuracy of both in-domain and cross-domain classifiers.

