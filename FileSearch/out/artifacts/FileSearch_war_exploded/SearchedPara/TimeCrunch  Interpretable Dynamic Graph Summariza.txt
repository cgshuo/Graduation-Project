 How can we describe a large, dynamic graph over time? Is it ran-dom? If not, what are the most apparent deviations from random-ness  X  a dense block of actors that persists over time, or perhaps a star with many satellite nodes that appears with some fixed period-icity? In practice, these deviations indicate patterns  X  for example, botnet attackers forming a bipartite core with their victims over the duration of an attack, family members bonding in a clique-like fashion over a difficult period of time, or research collaborations forming and fading away over the years. Which patterns exist in real-world dynamic graphs, and how can we find and rank them in terms of importance? These are exactly the problems we focus on in this work. Our main contributions are (a) formulation : we show how to formalize this problem as minimizing the encoding cost in a data compression paradigm, (b) algorithm : we propose T
IME C RUNCH , an effective, scalable and parameter-free method for finding coherent, temporal patterns in dynamic graphs and (c) practicality : we apply our method to several large, diverse real-world datasets with up to 36 million edges and 6.3 million nodes. We show that T IME C RUNCH is able to compress these graphs by summarizing important temporal structures and finds patterns that agree with intuition.
 dynamic graph; network; clustering; summarization; compression
Given a large phonecall network over time, how can we describe it to a practitioner with just a few phrases? Other than the tradi-tional assumptions about real-world graphs involving degree skew-ness, what can we say about the connectivity? For example, is the dynamic graph characterized by many large cliques which appear at fixed intervals of time, or perhaps by several large stars with dominant hubs that persist throughout? Our work aims to answer these questions, and specifically, we focus on constructing concise summaries of large, real-world dynamic graphs in order to better understand their underlying behavior.
 c  X 
This problem has numerous practical applications. Dynamic graphs are ubiquitously used to model the relationships between various entities over time , which is a valuable feature in almost all appli-cations in which nodes represent users or people. Examples in-clude online social networks, phone-call networks, collaboration and coauthorship networks and other interaction networks.
Though numerous graph algorithms suitable for static contexts such as modularity-based community detection, spectral cluster-ing, and cut-based partitioning exist, they do not offer direct dy-namic counterparts. Furthermore, the traditional goals of clustering and community detection tasks are not quite aligned with the en-deavor we propose. These algorithms typically produce groupings of nodes which satisfy or approximate some optimization function. However, they do not offer characterization of the outputs  X  are the detected groupings stars or chains, or perhaps dense blocks? Furthermore, the lack of explicit ordering in the groupings leaves a practitioner with limited time and no insights on where to begin understanding his data.

In this work, we propose T IME C RUNCH , an effective approach to concisely summarizing large, dynamic graphs which extend be-yond traditional dense and isolated  X  X avemen X  communities. Our method works by leveraging MDL (Minimum Description Length) in order to identify and appropriately describe graphs over time using a lexicon of temporal phrases which describe temporal con-nectivity behavior. Figure 1 shows several interesting results found from applying T IME C RUNCH to real-world dynamic graphs.
In this work, we seek to answer the following informally posed problem:
P ROBLEM 1 (I NFORMAL ). Given a dynamic graph, find a set of possibly overlapping temporal subgraphs to concisely de-scribe the given dynamic graph in a scalable fashion. C RUNCH with alternative approaches.

Our main contributions are as follows: 1. Problem Formulation: We show how to define the problem 2. Effective and Scalable Algorithm: We develop T IME C RUNCH 3. Practical Discoveries: We evaluate T IME C RUNCH on mul-Reproducibility : Our code for T IME C RUNCH is open-sourced at www.cs.cmu.edu/~neilshah/code/timecrunch.tar .
The related work falls into three main categories: static graph mining, temporal graph mining, and graph compression and sum-marization. Table 1 gives a visual comparison of T IME C with existing methods.
 Static Graph Mining . Most works find specific, tightly-knit struc-tures, such as (near-) cliques and bipartite cores: eigendecompo-sition [23], cross-associations [6], modularity-based optimization methods [19, 5]. Dhillon et al. [9] propose information theoretic co-clustering based on mutual information optimization. However, these approaches have limited vocabularies and are unable to find other types of interesting structures such as stars or chains. [13, 17] propose cut-based partitioning, whereas [3] suggests spectral partitioning using multiple eigenvectors  X  these schemes seek hard clustering of all nodes as opposed to identifying communities, and are not usually parameter-free. Subdue [7] and other fast frequent-subgraph mining algorithms [11] operate on labeled graphs. Our work involves unlabeled graphs and lossless compression. Temporal Graph Mining . [2] aims at change detection in stream-ing graphs using projected clustering. This approach focuses on anomaly detection rather than finding recurrent temporal patterns. GraphScope [25] uses graph search for hard-partitioning of tem-poral graphs to find dense temporal cliques and bipartite cores. Com2 [4] uses CP/PARAFAC tensor decomposition with MDL for the same. [10] uses incremental cross-association for change detec-tion in dense blocks over time, whereas [21] proposes an algorithm for mining cross-graph quasi-cliques (though not in a temporal con-text). These approaches have limited vocabularies and do not of-fer temporal interpretability. Dynamic clustering [27] aims to find stable clusters over time by penalizing deviations from incremen-tal static clustering. Our work focuses on interpretable structures, which may not appear at every timestep.
 Graph Compression and Summarization . SlashBurn [12] is a re-cursive node-reordering approach to leverage run-length encoding for graph compression. [26] uses structural equivalence to collapse nodes/edges to simplify graph representation. These approaches do not compress the graph for pattern discovery, nor do they operate on dynamic graphs. VoG [15] uses MDL to label subgraphs in terms of a vocabulary on static graphs, consisting of stars, (near) cliques, (near) bipartite cores and chains. This approach only ap-plies to static graphs and does not offer a clear extension to dynamic graphs. Our work proposes a suitable lexicon for dynamic graphs, uses MDL to label temporally coherent subgraphs and proposes an effective and scalable algorithm for finding them.
In this section, we give the first main contribution of our work: formulation of dynamic graph summarization as a compression prob-lem, using MDL. For clarity, see Table 2 for a reference of the re-current symbols used in this section.

The Minimum Description Length (MDL) principle aims to be a practical version of Kolmogorov Complexity [18], often associated with the motto Induction by Compression . MDL states that given a model family M , the best model M  X  M for some observed data D is that which minimizes L ( M ) + L ( D| M ) , where L ( M ) is the length in bits used to describe M and L ( D| M ) is the length in bits used to describe D encoded using M . MDL enforces lossless compression for fairness in the model selection process.
We focus on analysis of undirected dynamic graphs using fixed-length, discretized time intervals. However, our notation will re-flect the treatment of the problem as one with a series of individ-ual snapshots of graphs, rather than a tensor, for readability pur-poses. We consider a dynamic graph G ( V , E ) with n = |V| nodes, m = |E| edges and t timesteps, without self-loops. Here, G =  X 
G x ( V , E x ) , where G x and E x correspond to the graph and edge-set for the x th timestep. The ideas proposed in this work, however, can easily be generalized to other types of dynamic graphs.
For our summary, we consider the set of temporal phrases  X  =  X   X   X  , where  X  corresponds to the set of temporal signatures,  X  corresponds to the set of static structure identifiers and  X  de-notes Cartesian set product. Though we can include arbitrary tem-poral signatures and static structure identifiers into these sets de-pending on the types of temporal subgraphs we expect to find in a given dynamic graph, we choose 5 temporal signatures which we anticipate to find in real-world dynamic graphs [4] : oneshot ( o ), ranged ( r ), periodic ( p ), flickering ( f ) and constant ( c ), and 6 very common structures found in real-world static graphs [14, 23]  X  stars ( st ), full and near cliques ( fc , nc ), full and near bipartite cores ( bc , nb ) and chains ( ch ) . Summarily, we have the signatures  X  = { o,r,p,f,c } , static identifiers  X  = { st,fc,nc,bc,nb,ch } and temporal phrases  X  =  X   X   X  . We will further describe these signatures, identifiers and phrases after formalizing our objective.
In order to use MDL for dynamic graph summarization using these temporal phrases, we next define the model family M , the means by which a model M  X  M describes our dynamic graph and how to quantify the cost of encoding in terms of bits.
We consider models M  X  X  to be composed of ordered lists of temporal graph structures with node, but not edge overlaps. Each s  X  M describes a certain region of the adjacency tensor A in terms of the interconnectivity of its nodes. We will use area ( s,M, A ) to describe the edges ( i,j,x )  X  A which s induces, writing only area ( s ) when context for M and A is clear.

Our model family M consists of all possible permutations of subsets of C , where C =  X  v C v and C v denotes the set of all possible temporal structures of phrase v  X   X  over all possible combinations of timesteps. That is, M consists of all possible models M , which are ordered lists of temporal phrases v  X   X  such as flickering stars ( fst ), periodic full cliques ( pfc ), etc. over all possible subsets of V and G 1  X  X  X  G t . Through MDL, we seek the model M  X  X  which best mediates between the encoding length of the model M and the adjacency tensor A given M .
 Our fundamental approach for transmitting the adjacency tensor A via the model M is described next. First, we transmit M . Next, given M , we induce the approximation of the adjacency tensor M as described by each temporal structure s  X  M  X  for each structure s , we induce the edges in area ( s ) in M accordingly. Given that M is a summary approximation to A , M 6 = A most likely. Since MDL requires lossless encoding, we must also transmit the error E = M  X  A , obtained by taking the exclusive OR between M and A . Given M and E , a recipient can construct the full adjacency tensor A in a lossless fashion.
 Thus, we formalize the problem we tackle as follows: Given a dynamic graph G with adjacency tensor A and temporal phrase lexicon  X  , find the smallest model M which minimizes the total encoding length where E is the error matrix computed by E = M  X  A and M is the approximation of A induced by M .

In the following subsections, we further formalize the task of encoding the model M and the error matrix E . To fully describe a model M  X  X  , we have the following:
L ( M ) = L N ( | M | + 1) + log 2 We begin by transmitting the total number of temporal structures in M using L N , Rissanen X  X  optimal encoding for integers greater than or equal to 1 [22]. Next, we optimally encode the number of temporal structures for each phrase v  X   X  in M . Then, for each structure s , we encode the type v ( s ) for each structure s  X  M using optimal prefix codes [8], the connectivity c ( s ) and the temporal presence of the s , consisting of the ordered list of timesteps u ( s ) in which s appears.

In order to have a coherent model encoding scheme, we next define the encoding for each phrase v  X   X  such that we can com-pute L ( c ( s )) and L ( u ( s )) for all structures in M . The connectivity c ( s ) corresponds to the edges in area ( s ) which are induced by s , whereas the temporal presence u ( s ) corresponds to the timesteps in which s is present. We consider the connectivity and tempo-ral presence separately, as the encoding for a temporal structure s described by a phrase v is the sum of encoding costs for the con-nectivity of the corresponding static structure identifier in  X  and its temporal presence as indicated by a temporal signature in  X  . In this section, we describe how to compute the encoding cost L ( c ( s )) for the connectivity for each type of static structure identi-fier in our identifier set  X  . Stars: A star is characteristic of a single  X  X ub X  node connected to a set of 2 or more  X  X poke X  nodes. We compute L ( st ) of a star st as follows: First, we identify the number of spokes of the star. Next, we iden-tify the hub out of n nodes using an index over the combinatorial number system. Lastly, we identify the spokes from the remainder. Cliques: Cliques are comprised of densely connected sets of nodes. For a full clique fc , in which all nodes are directly connected to all other nodes in the clique, we give the cost L ( fc ) as follows: In this case, we encode the number of nodes in the clique followed by their ids. Note that as M is an approximation of G , fc need not actually be a full clique in G . If only a few edges of the full clique are not present in G , it may be worthwhile from a compression standpoint to describe it as such. In this case, each falsely repre-sented edge will add to the error cost E . Errors in connectivity encoding will be elaborated on in Sec. 3.3.1.

Less dense near-cliques are still interesting from a graph under-standing perspective, provided they stand out from the background. For a near clique nc , we give L ( nc ) as follows: Here, we encode the number of nodes and their ids as in the full clique case. However, we additionally encode the edges in the near clique by encoding the number of total edges in area ( nc ) by opti-mal prefix codes. We use || nc || and || nc || 0 to denote the counts for existing and non-existing edges in area ( nc ) . Then,  X   X  log ( || nc || / ( || nc || + || nc || 0 )) and  X  0 =  X  log ( || nc || || nc || 0 )) represent the length of the optimal prefix codes for the ex-isting and non-existing edges respectively. Intuitively, the more sparse or dense the near clique is, the cheaper its encoding be-comes. As the encoding in this case is exact, we do not add any edges to E .
 Bipartite Cores: Bipartite cores consist of non-empty, non-inter-secting node-sets L and R for which there only exist edges from L and R , but not within L or R . Note that stars can be construed as a fixed case of bipartite cores in which | L | = 1 . The encoding cost L ( bc ) for a full bipartite core bc is as follows:
L ( fb ) = L N ( | L | ) + L N ( | R | ) + log 2 n | L | In this case, we encode the number of nodes in L and R followed by the node ids in each set.

As with near cliques, near bipartite cores are also interesting if they stand out from the background. In this case, encoding is given analogously as follows:
L ( nb ) = L N ( | L | ) + L N ( | R | ) + log 2 n | L | Furthermore, as with near-cliques, encoding in this case is exact so we do not add any edges to E .
 Chains: A chain is characterized by series of nodes in which each node has an edge connecting it to the next node  X  for example, con-sider the node-set { 1 , 2 , 3 , 4 } in which 1 is connected to 2, 2 is connected to 3, and 3 is connected to 4. Given the right permuta-tion, a perfect chain in an undirected graph will have edges only along two diagonals of the adjacency matrix. For a chain ch , we have the encoding cost L ( ch ) as follows: We first encode the number of nodes in the chain, followed by their node ids in order of connection.
For a given phrase v  X   X  , it is not sufficient to only encode the connectivity of the underlying static structure. We must also encode the temporal presence u ( s ) , consisting of a set of ordered timesteps in which s appears, for each structure. In this section, we describe how to compute the encoding cost L ( u ( s )) for each of the temporal signatures in the signature set  X  .

We note that describing a set of timesteps u ( s ) in terms of tem-poral signatures in  X  is yet another model selection problem for which we can leverage MDL. As with connectivity encoding, la-beling u ( s ) with a given temporal signature may not be precisely accurate  X  however, any mistakes will add to the cost of transmit-ting the error. Errors in temporal presence encoding will be further detailed in Sec. 3.3.2.
 Oneshot: Oneshot structures appear at only one timestep in G  X  that is, | u ( s ) | = 1 . These structures represent graph anomalies, in the sense that they are non-recurrent interactions which are only observed once. The encoding cost L ( o ) for the temporal presence of a oneshot structure o can be written as: As the structure occurs only once, we only have to identify the timestep of occurrence from the t observed timesteps.
 Ranged: Ranged structures are characterized by a short-lived exis-tence. These structures appear for several timesteps in a row before disappearing again  X  they are defined by a single burst of activity. The encoding cost L ( r ) for a ranged structure r is given by: We first encode the number of timesteps in which the structure oc-curs, followed by the timestep ids of both the start and end timestep marking the span of activity.
 Periodic: Periodic structures are an extension of ranged structures in that they appear at fixed intervals. However, these intervals are spaced greater than one timestep apart. As such, the same encoding cost function we use for ranged structures suffices here. That is, L ( p ) for a periodic structure p is given by L ( p ) = L ( r ) .
For both ranged and periodic structures, periodicity can be in-ferred from the start and end markers along with the number of timesteps | u ( s ) | , allowing reconstruction of the original u ( s ) . Flickering: A structure is flickering if it appears only in some of the G 1  X  X  X  G t timesteps, and does so without any discernible ranged/periodic pattern. The encoding cost L ( f ) for a flickering structure f is as follows: We encode the number of timesteps in which the structure occurs in addition to the ids for the timesteps of occurrence.
 Constant: Constant structures persist throughout all timesteps. That is, they occur at each timestep G 1  X  X  X  G t without exception. In this case, our encoding cost L ( c ) for a constant structure c is defined as L ( c ) = 0 . Intuitively, information regarding the timesteps in which the structure appears is  X  X ree, X  as it is already given by encoding the phrase descriptor v ( s ) .
Given that M is a summary and the M induced by M is only an approximation of A , it is necessary to encode errors made by M . In particular, there are two types of errors we must consider. The first is error in connectivity  X  that is, if area ( s ) induced by structure s is not exactly the same as the associated patch in A , we encode the relevant mistakes. The second is the error induced by encoding the set of timesteps u ( s ) with a fixed temporal signature, given that u ( s ) may not precisely follow the temporal pattern used to encode it.
We encode the error tensor E = M  X  A as two different pieces  X  specifically, we encode E + and E  X  where the former refers to the area of A which M models and M includes extraneous edges not present in the original graph, and the latter consists of the area of A which M does not model and therefore does not describe. Our reasoning for encoding these two separately is that they likely have different error distributions. Given that near cliques and near bipartite cores are encoded exactly per our model, we ignore the associated areas when encoding E + . The encoding for E E  X  , denoted as L ( E + ) and L ( E  X  ) respectively is as follows: In both cases, we encode the number of 1s in E + (or E  X  ), followed by the actual 1s and 0s using optimal prefix codes.
For encoding errors induced by identifying u ( s ) as one of the temporal signatures, we turn to optimal prefix codes applied over the error distribution for each structure s . Given the information encoded for each signature type in  X  , we can reconstruct an ap-|  X  u ( s ) | . Using this approximation, the encoding cost L ( e the error e u ( s ) = u ( s )  X   X  u ( s ) is defined as: where h ( e u ( s )) denotes the set of elements with unique magni-tude in e u ( s ) , c ( k ) denotes the count of element k in e  X  k denotes the length of the optimal prefix code for k . For each magnitude error, we encode the magnitude of the error, the num-ber of times it occurs and the actual errors using optimal prefix codes. Using the model in conjunction with temporal presence and connectivity errors, a recipient can first recover the u ( s ) for each s  X  M , approximate A with M induced by M , produce E from E + and E  X  , and finally recover A losslessly through A = M  X  E . Remark: For a dynamic graph G of n nodes, the search space M for the best model M  X  M is intractable, as it consists of all per-mutations of all possible temporal structures over the lexicon  X  , Algorithm 1 T IME C RUNCH 1: 2: 3: 4: over all possible subsets over the node-set V and over all possi-ble graph timesteps G 1  X  X  X  G t . Furthermore, M is not easily ex-ploitable for efficient search. As a result, we propose several prac-tical approaches for the purpose of finding good and interpretable temporal models/summaries for G .
Thus far, we have described our strategy of formulating dynamic graph summarization as a problem in a compression context for which we can leverage MDL. Specifically, we have detailed how to encode a model and the associated error which can be used to loss-lessly reconstruct the original dynamic graph G . Our models are characterized by ordered lists of temporal structures which are fur-ther classified as phrases from the lexicon  X   X  that is, each s  X  M is identified by a phrase p  X   X   X  over the node connectivity c ( s ) (an induced set of edges depending on the static structure identifier st , fc , etc.) and the associated temporal presence u ( s ) (ordered list of timesteps captured by a temporal signature o , r , etc. and de-viations) in which the temporal structure is active, while the error consists of those edges which are not covered by M , or the approx-imation of A induced by M .

Next, we discuss how we find good candidate temporal struc-tures to populate the candidate set C , as well as how we find the best model M with which to summarize our dynamic graph. The pseudocode for our algorithm is given in Alg. 1 and the next sub-sections detail each step of our approach.
T IME C RUNCH takes an incremental approach to dynamic graph summarization. Our approach begins by considering potentially useful subgraphs over static graphs G 1  X  X  X  G t . Sec. 2 mentions several such algorithms for community detection and clustering in-cluding EigenSpokes, METIS, SlashBurn, etc. Summarily, for each G 1  X  X  X  G t , a set of subgraphs F is produced.
Once we have the set of static subgraphs from G 1 we next seek to label each subgraph in F according to the static structure identifiers in  X  that best fit the connectivity for the given subgraph. That is, for each subgraph construed as a set of nodes L  X  V for a fixed timestep, does the adjacency matrix of L best resemble a star, near or full clique, near or full bipartite core or a chain? To answer this question, we leverage the encoding scheme discussed in Sec. 3.2.1: we try encoding the subgraph L using each of the static identifiers in  X  and label it with the identifier x  X   X  which minimizes the encoding cost.

Consider the model  X  which consists of only the subgraph L and a yet to be determined static identifier. In practice, instead of computing the global encoding cost L ( G, X  ) when encoding L as each static identifier in  X  to find the best fit, we compute the local encoding cost defined as L (  X  ) + L ( E +  X  ) + L ( E L ( E +  X  ) and L ( E  X   X  ) indicate the encoding costs for the extraneous and unmodeled edges for the subgraph L respectively. This is done for purpose of efficiency  X  intuitively, however, the static identifier that best describes L is independent of the edges outside of L .
The challenge in this labeling step is that before we can encode L as any type of identifier, we must identify a suitable permutation of nodes in the subgraph so that our model encodes the correct edges. For example, if L is a star, which is the hub? Or if L is a bipartite core, how can we distinguish the parts?
For stars, we identify the highest-degree node as the hub and all other nodes as spokes. For near and full bipartite cores, finding the right permutation can be reduced to finding the maximum bipartite subgraph, which is equivalent to finding the maximum cut and is NP-hard. As a result, we use a heuristic approach which formu-lates the problem as a two-class classification task. To this end, we initialize L to contain the highest-degree node in L , and R to con-tain its neighbors. We then use Fast Belief Propagation [16] with heterophily (assuming connected nodes belong to different classes) to propagate the class labels and determine L and R . For near and full cliques, any permutation is equally good. Lastly, for chains, finding the right permutation is equivalent to finding the longest path, which is NP-hard. As a result, we again employ a heuris-tic approach in which we select a node in L at random, use BFS to find the furthest node away, and repeat with the resulting node while extending the chain through local search iteratively. For both near cliques and bipartite cores, we do not encode E + nc L ( nc ) and L ( nb ) encode the relevant edges exactly.
Thus far, we have a set of static subgraphs F over G labeled with the associated static identifiers which best represent subgraph connectivity (from now on, we refer to F as a set of static structures instead of subgraphs as they have been labeled with identifiers). From this set, our goal is to find meaningful tem-poral structures  X  namely, we seek to find static subgraphs which have the same patterns of connectivity over one or more timesteps and stitch them together. Thus, we formulate the problem of find-ing coherent temporal structures in G as a clustering problem over F . Though there are several criteria we could use for clustering static structures together, we employ the following based on their intuitive meaning: two structures in the same cluster should have (a) substantial overlap in the node-sets composing their respective subgraphs, and (b) exactly the same, or similar (full and near clique, or full and near bipartite core) static structure identifiers. These cri-teria, if satisfied, allow us to find groups of nodes that share inter-esting connectivity patterns over time.

Conducting the clustering by naively comparing each static struc-ture in F to the others will produce the desired result, but is quadratic on the number of static structures and is thus undesirable from a scalability point of view. Instead, we propose an incremental ap-proach using repeated rank-1 Singular Value Decomposition (SVD) for clustering the static structures, which offers linear time com-plexity on the number of edges m in G .

We begin by defining B as the structure-node membership ma-trix (SNMM) of G . B is defined to be of dimensions |F| X |V| , where B i,j indicates whether the i th row (structure) in F ( B ) con-tains node j in its node-set. Thus, B is a matrix indicating the membership of nodes in V to each of the static structures in F . We note that any two equivalent rows in B are characterized by structures that share the same node-set (but possibly different static identifiers). As our clustering criteria mandate that we cluster only structures with the same or similar static identifiers, in our algo-rithm, we construct 4 SNMMs  X  B st , B cl , B bc and B ch sponding to the associated matrices for stars, near and full cliques, near and full bipartite cores and chains respectively. Now, any two equivalent rows in B cl are characterized by structures that share the same-node set and the same, or similar static identifiers, and analogue for the other matrices. Next, we utilize SVD to cluster the rows in each SNMM, effectively clustering the structures in F .
Recall that the rank-k SVD of an m  X  n matrix A factorizes A into 3 matrices  X  the m  X  k matrix of left-singular vectors U , the k  X  k diagonal matrix of singular values  X  and the n  X  k matrix of right-singular vectors V , such that A = U X V T . A rank-k SVD effectively reduces the input data into the best k -dimensional rep-resentation, each of which can be mined separately for clustering and community detection purposes. However, one major issue with using SVD in this fashion is that identifying the desired number of clusters k upfront is a non-trivial task. To this end, [20] evidences that in cases where the input matrix is sparse, repeatedly clustering using k rank-1 decompositions and adjusting the input matrix ac-cordingly approximates the batch rank-k decomposition. This is a valuable result in our case  X  as we do not initially know the num-ber of clusters needed to group the structures in F , we eliminate the need to define k altogether by repeatedly applying rank-1 SVD using power iteration and removing the discovered clusters from each SNMM until all clusters have been found (when all SNMMs are fully sparse and thus deflated ). However, in practice, full defla-tion is unneeded for summarization purposes, as most  X  X mportant X  clusters are found in early iterations due to the nature of SVD. For each of the SNMMs, the matrix B used in the ( i + 1) th iteration of this iterative process is computed as where G i denotes the set of row ids corresponding to the structures which were clustered together in iteration i , I G i denotes the in-dicator matrix with 1 s in rows specified by G i and  X  denotes the Hadamard matrix product. This update to B is needed between iterations, as without subtracting out the previously-found cluster, repeated rank-1 decompositions would find the same cluster ad in-finitum and the algorithm would not converge.

Although this algorithm works assuming we can remove a clus-ter in each iteration, the question of how we find this cluster given a singular vector has yet to be answered. First, we sort the singular vector, permuting the rows by magnitude of projection. The intu-ition is that the structure (rows) which projects most strongly to that cluster is the best representation of the cluster, and is considered a base structure which we attempt to find matches for. Starting from the base structure, we iterate down the sorted list and compute the Jaccard similarity, defined as J ( L 1 , L 2 ) = |L 1  X  X  2 node-sets L 1 and L 2 , between each structure and the base. Other structures which are composed of the same, or similar node-sets will also project strongly to the cluster, and be stitched to the base. Once we encounter a series of structures which fail to match by a predefined similarity criterion, we adjust the SNMM and continue with the next iteration.

Having stitched together the relevant static structures, we label each temporal structure using the temporal signature in  X  and re-sulting phrase in  X  which minimizes its encoding cost using the temporal encoding framework derived in Sec. 3.2.2. We use these temporal structures to populate the candidate set C for our model.
Given the candidate set of temporal structures C , we next seek to find the model M which best summarizes G . However, actually finding the best model is combinatorial, as it involves consider-ing all possible permutations of subsets of C and choosing the one which gives the smallest encoding cost. As a result, we propose several heuristics that give fast and approximate solutions without entertaining the entire search space. To reduce the search space, we associate with each temporal structure a metric by which we measure quality, called the local encoding benefit . The local encod-ing benefit is defined as the ratio between the cost of encoding the given temporal structure as error and the cost of encoding it using the best phrase (local encoding cost). Large local encoding benefits indicate high compressibility, and thus meaningful structure in the underlying data. Our proposed heuristics are as follows: V
ANILLA : This is the baseline approach, in which our summary contains all the structures from the candidate set, or M = C . T
OP -K : In this approach, M consists of the top k structures of C , sorted by local encoding benefit.
 S TEPWISE : This approach involves considering each structure of C , sorted by local encoding benefit, and adding it to M if the global encoding cost decreases. If adding the structure to M increases the global encoding cost, the structure is discarded as redundant or not worthwhile for summarization purposes.

In practice, T IME C RUNCH uses each of the heuristics and identi-fies the best summary for G as the one that produces the minimum encoding cost.
In this section, we evaluate T IME C RUNCH and seek to answer the following questions: Are real-world dynamic graphs well-struc-tured, or noisy and indescribable? If they are structured, how so  X  what temporal structures do we see in these graphs and what do they mean? Lastly, is T IME C RUNCH scalable?
For our experiments, we use 5 real dynamic graph datasets  X  they are summarized in Table 3 and described below.
 Enron : The Enron e-mail dataset is publicly available. It con-tains 20 thousand unique links between 151 users based on e-mail correspondence, over 163 weeks (May 1999 -June 2002).
 Yahoo! IM : The Yahoo-IM dataset is publicly available. It con-tains 2.1 million sender-receiver pairs between 100 thousand users over 5709 zip-codes selected from the Yahoo! messenger network over 4 weeks starting from April 1st, 2008.
 Honeynet : The Honeynet dataset is not publicly available. It contains information about network attacks on honeypots (i.e., com-puters which are left intentionally vulnerable to attackers) It con-tains source IP, destination IP and attack timestamps of 372 thou-sand (attacker and honeypot) machines with 7.1 million unique daily attacks over a span of 32 days starting from December 31st, 2013.
 DBLP : The DBLP computer science bibliography is publicly avail-able, and contains yearly co-authorship information, indicating joint publication. We used a subset of DBLP spanning 25 years, from 1990 to 2014, with 1.3 million authors and 15 million unique author-author collaborations over the years.
 Phonecall : The Phonecall dataset is not publicly available. It describes the who-calls-whom activity of 6.3 million individuals from a large, anonymous Asian city and contains a total of 36.3 million unique daily phonecalls. It spans 31 days, starting from December 1st, 2007.

In our experiments, we use  X  X lashBurn X  for generating candidate static structures, as it is scalable and designed to extract structure from real-world, non- X  X avemen X  graphs. We note that including other graph decomposition methods can only improve results given MDL. Furthermore, when clustering each sorted singular vector during the stitching process, we move on with the next iteration of matrix deflation after 10 failed matches with a Jaccard similar-ity threshold of 0.5  X  we choose 0.5 based on experimental results which show that it gives the best encoding cost and balances be-tween excessively terse and overlong (error-prone) models. Lastly, we run T IME C RUNCH for a total of 5000 iterations for all graphs (each iteration uniformly selects one SNMMs to mine, resulting in 5000 total temporal structures), except for the Enron graph which is fully deflated after 563 iterations and the Phonecall graph which we limit to 1000 iterations for efficiency.
In this section, we use T IME C RUNCH to summarize each of the real-world dynamic graphs from Table 3 and report the resulting encoding costs. Specifically, evaluation is done by comparing the compression ratio between encoding costs of the resulting models to the null encoding (O RIGINAL ) cost, which is obtained by encod-ing the graph using an empty model.

We note that although we provide results in a compression con-text, compression is not our main goal for T IME C RUNCH , but rather the means to our end for identifying suitable structures with which to summarize dynamic graphs and route the attention of practition-ers. For this reason, we do not evaluate against other, compression-oriented methods which prioritize leveraging any correlation within the data to reduce cost and save bits. Other temporal clustering and community detection approaches which focus only on extracting dense blocks are also not compared to for similar reasons. In our evaluation, we consider (a) O RIGINAL and (b) T C RUNCH summarization using the proposed heuristics. In the O INAL approach, the entire adjacency tensor is encoded using the empty model M =  X  . As the empty model does not describe any part of the graph, all the edges are encoded using L ( E We use this as a baseline to evaluate the savings attainable using T
IME C RUNCH . For summarization using T IME C RUNCH , we ap-ply the V ANILLA , T OP -10, T OP -100 and S TEPWISE model selec-tion heuristics. We note that we ignore small structures of &lt;5 nodes for Enron and &lt;8 nodes for the other, larger datasets.
Table 4 shows the results of our experiments in terms of encod-ing costs of various summarization techniques as compared to the O
RIGINAL approach. Smaller compression ratios indicate better summaries, with more structure explained by the respective mod-els. For example, S TEPWISE was able to encode the Enron dataset using just 78% of the bits compared to 89% using V ANILLA experiments, we find that the S TEPWISE heuristic produces mod-els with considerably fewer structures than V ANILLA , while giving even more concise graph summaries (Fig. 2). This is because it is highly effective in pruning redundant, overlapping or error-prone structures from the candidate set C , by evaluating new structures in the context of previously seen ones. of model is parenthesized). The lowest description cost is bolded. Figure 2: T IME C RUNCH -S TEPWISE summarizes Enron using just 78% of O RIGINAL  X  X  bits and 130 structures compared to 89% and 563 structures of T IME C RUNCH -V ANILLA by prun-ing unhelpful structures from the candidate set.

O BSERVATION 1. Real-world dynamic graphs are not unstruc-tured. T IME C RUNCH gives better encoding cost than O RIGINAL , indicating the presence of temporal graph structure. In this section, we discuss qualitative results from applying T C RUNCH to the graphs mentioned in Table 3.
 Enron : The Enron graph is characteristic of many periodic, ranged and oneshot stars and several periodic and flickering cliques. Peri-odicity is reflective of office e-mail communications (e.g. meetings, reminders). Figure 3a shows an excerpt from one flickering clique which corresponds to the several members of Enron X  X  legal team, including Tana Jones, Susan Bailey, Marie Heard and Carol Clair  X  all lawyers at Enron. Figure 3b shows an excerpt from a flickering star, corresponding to many of the same members as the flickering clique  X  the center of this star was identified as the boss, Tana Jones (Enron X  X  Senior Legal Specialist). Note that the satellites of the star oscillate over time. Interestingly, the flickering star and clique extend over most of the observed duration. Furthermore, several of the oneshot stars corresponds to company-wide emails sent out by key players John Lavorato (Enron America CEO), Sally Beck (COO) and Kenneth Lay (CEO/Chairman).
 Yahoo! IM : The Yahoo-IM graph is composed of many tem-poral stars and cliques of all types, and several smaller bipartite cores with just a few members on one side (indicative of friends who share mostly similar friend-groups but are themselves uncon-nected). We observe several interesting patterns in this data  X  Fig. 3d corresponds to a constant star with a hub that communicates with 70 users consistently over 4 weeks. We suspect that these users are part of a small office network, where the boss uses group messag-ing to notify employees of important updates or events  X  we notice that very few edges of the star are missing each week and the aver-age degree of the satellites is roughly 4, corresponding to possible communication between employees. Figure 3c depicts a constant clique between 40 users, with an average density over 55%  X  we suspect that these may be spam-bots messaging each other in an effort to appear normal.
 Honeynet : Honeynet is a bipartite graph between attacker and honeypot (victim) machines. As such, it is characterized by tempo-ral stars and bipartite cores. Many of the attacks only span a single day, as indicated by the presence of 3512 oneshot stars, and no at-tacks span the entire 32 day duration. Interestingly, 2502 of these oneshot star attacks (71%) occur on the first and second observed days (Dec. 31 and Jan. 1st) indicating intentional  X  X ew-year X  at-tacks. Figure 3e shows a ranged star, lasting 15 consecutive days and targeting 589 machines for the entire duration of the attack. DBLP : Agreeing with intuition, DBLP consists of a large num-ber of oneshot temporal structures corresponding to many single instances of joint publication. However, we also find numerous ranged/periodic stars and cliques which indicate coauthors pub-lishing in consecutive years or intermittently. Figure 3f shows a ranged clique spanning from 2007-2012 between 43 coauthors who jointly published each year. The authors are mostly members of the NIH NCBI (National Institute of Health National Center for Biotechnology Information) and have published their work in vari-ous biotechnology journals such as Nature , Nucleic Acids Research and Genome Research . Figure 3g shows another ranged clique from 2005 to 2011, consisting of 83 coauthors who jointly pub-lish each year, with an especially collaborative 3 years (timesteps 18-20) corresponding to 2007-2009 before returning to status quo. Phonecall : The Phonecall dataset is largely comprised of tem-poral stars and few dense clique and bipartite structures. Again, we have a large proportion of oneshot stars which occur only at sin-gle timesteps. Further analyzing these results, we find that 111 of the 187 oneshot stars (59%) are found on Dec. 24, 25 and 31st, corresponding to Christmas Eve/Day and New Year X  X  Eve holi-day greetings. Furthermore, we find many periodic and flickering stars typically consisting of 50-150 nodes, which may be associ-ated with businesses regularly contacting their clientele, or public phones which are used consistently by the same individuals. Fig-ure 3h shows one such periodic star of 111 users over the last week of December, with particularly clear star structure on Dec. 25th and 31st and other odd-numbered days, accompanied by substantially weaker star structure on the even-numbered days. Figure 3i shows an oddly well-separated oneshot near-bipartite core which appears on Dec. 31st, consisting of two roughly equal-sized parts of 402 and 390 callers. Though we do not have ground truth to interpret these structures, we note that a practitioner with the appropriate information could better interpret their meaning.
All components of T IME C RUNCH are carefully designed to be linear or near-linear on the number of nonzero edges. Figure 4 shows the near-linear runtime of T IME C RUNCH on several induced temporal subgraphs (up to 14M edges) taken from the DBLP dataset at varying time-intervals. Our experiments were conducted on a machine with 80 Intel Xeon(R) 4850 2GHz cores and 256GB RAM. We use MATLAB for candidate subgraph generation and temporal stitching and Python for model selection heuristics.

Furthermore, much of the T IME C RUNCH pipeline (per-timestep summarization) is embarassingly parallelizable and can be easily split over nodes. Distributed eigensolver implementations also ex-ist in practice for the stitching component.
In this work, we tackle the problem of identifying significant and structurally interpretable temporal patterns in large, dynamic graphs. Specifically, we formalize the problem of finding impor-tant and coherent temporal structures in a graph as minimizing the encoding cost of the graph from a compression standpoint. To this end, we propose T IME C RUNCH , a fast and effective, incre-mental technique for building interpretable summaries for dynamic graphs which involves generating candidate subgraphs from each static graph, labeling them using static identifiers, stitching them over multiple timesteps and composing a model using practical ap-Figure 4: T IME C RUNCH scales near-linearly on the number of edges in the graph. Here, we use several induced temporal sub-graphs from DBLP , up to 14M edges in size. proaches. Finally, we apply T IME C RUNCH on several large, dy-namic graphs and find numerous patterns and anomalies which in-dicate that real-world graphs do in fact exhibit temporal structure.
This material is based upon work supported by the National Sci-ence Foundation under Grant Nos. IIS-1217559, CNS-1314632 and DGE-1252522. Prepared by LLNL under Contract DE-AC52-07NA27344. Any opinions, findings, and conclusions or recom-mendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foun-dation, DARPA, or other funding parties. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on. [1] DBLP network dataset. konect.uni-koblenz.de/ [2] C. C. Aggarwal and S. Y. Philip. Online analysis of [3] C. J. Alpert, A. B. Kahng, and S.-Z. Yao. Spectral [4] M. Araujo, S. Papadimitriou, S. G X nnemann, C. Faloutsos, [5] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and [6] D. Chakrabarti, S. Papadimitriou, D. S. Modha, and [7] D. J. Cook and L. B. Holder. Substructure discovery using [8] T. M. Cover and J. A. Thomas. Elements of information [9] I. S. Dhillon, S. Mallela, and D. S. Modha.
 [10] J. Ferlez, C. Faloutsos, J. Leskovec, D. Mladenic, and [11] R. Jin, C. Wang, D. Polshakov, S. Parthasarathy, and [12] U. Kang and C. Faloutsos. Beyond X  X aveman communities X : [13] G. Karypis and V. Kumar. Multilevel k-way hypergraph [14] J. M. Kleinberg, R. Kumar, P. Raghavan, S. Rajagopalan, and [15] D. Koutra, U. Kang, J. Vreeken, and C. Faloutsos. Vog: [16] D. Koutra, T.-Y. Ke, U. Kang, D. H. P. Chau, H.-K. K. Pao, [17] B. Kulis and Y. Guan. Graclus -efficient graph clustering [18] M. Li and P. M. Vit X nyi. An introduction to Kolmogorov [19] M. E. Newman and M. Girvan. Finding and evaluating [20] E. E. Papalexakis, N. D. Sidiropoulos, and R. Bro. From [21] J. Pei, D. Jiang, and A. Zhang. On mining cross-graph [22] J. Rissanen. Modeling by shortest data description. [23] N. Shah, A. Beutel, B. Gallagher, and C. Faloutsos. Spotting [24] J. Shetty and J. Adibi. The enron email dataset database [25] J. Sun, C. Faloutsos, S. Papadimitriou, and P. S. Yu. [26] H. Toivonen, F. Zhou, A. Hartikainen, and A. Hinkka. [27] K. S. Xu, M. Kliger, and A. O. Hero III. Tracking [28] Yahoo! Webscope. webscope.sandbox.yahoo.com .
