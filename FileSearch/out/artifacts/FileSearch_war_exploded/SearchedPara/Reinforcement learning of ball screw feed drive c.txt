 1. Introduction
A ball screw feed drive is a high precision mechanisms converting rotational into linear motion used for positioning in machining systems. They provide high performance in machine tools with a minimum rolling friction. A long threaded shaft with a helical track for balls is rotated to move the worktable of a CNC machine. The nut containing the balls moves along the shaft with an endless recirculation system. The ball screw is powered by a servomotor which can ef fi ciently apply or withstand high thrust loads. It achieves high mechanical ef fi ciency, converting around 90% of the applied force into movement, which is equivalent to one third of what is required with the Acme thread lead screw. It provides accurate tool positioning even in the presence of cutting the development of feedback control of the ball screw feed drive. It can be achieved by Proportional Integrative Derivative (PID) controllers, as it has been reported for other kinds of positioning systems ( Koren and Lo, 1992; Srinivasan and Tsao, 1997 ). The simplest form of PID can be easily designed, and their properties have been extensively studied in the fi eld of control theory.
However, tuning PID parameters is not trivial and must be done empirically using heuristic methods, such as the Ziegler  X  method ( Ziegler and Nichols, 1993 ). Arti fi cial Intelligence methods are becoming accepted for the development of feedback control problems following a diversity of approaches, such as neural network-based algorithms ( Douligeris and Singh, 1999; Madar ( Du et al., 2003; Kundu and Kawata, 1996; Moore et al., 2001 ), and hybrid approaches ( Alam and Tokhi, 2008; Hui et al., 2012; Senthilkumar and Bharadwaj, 2009 ). Lately, some researchers have also explored the use of Reinforcement Learning (RL) methods to develop feedback control methods ( Hafner and Riedmiller, 2011; Neumann, 2005; Lewis et al., 2012; Lewis and Liu, 2013 ).
RL trained agents learn by iterative interaction with the con-trolled environment. After each interaction the agent receives a reward assessing how desirable the last action was. The learning process aims to fi nd a decision policy optimizing the received reward signal. RL methods require very little intervention by the system designer. Learning feedback controllers by RL is a promis-ing but not yet mature research fi eld, and the literature lacks relevant empirical comparisons. In this paper, we have applied current state of the art RL methods to learn the control of a ball screw feed drive. Namely, we compare the performance of two value-iteration methods (Q-Learning and SARSA), three different policy-iteration methods (a TD  X   X   X  critic with three different actors), and a double-loop PID controller which is considered a baseline performance reference. To our knowledge, this is the work dealing with the ball screw feed drive control in the literature so far, including the application of the PID.
This article is structured as follows. Section 2 introduces the ball screw system and its simulation model. Section 3 reviews the basics of feedback control and Section 4 gives some necessary background on approximated RL methods. A detailed description of the computational experiments is given in Section 5 , and Section 6 presents the results. Finally, we offer our conclusions and point out some interesting lines of future work in Section 7 .
 2. Ball screw feed system
Computational experiments are carried out simulating the ball screw feed drive using an inertial model of a commercial version, the Ideko manufactured by Fagor. It consists of the following components: a Fagor Fkm 42.30.A motor, a Rotex Gs 24/28 coupling, a NSK-Rhp Bsb 025 062 screw-nut, a Kondia D32x10 nut, and Ina Kuse 45 850 guideways. A picture of the actual system is shown in Fig. 1 .

In the RL experiments we have used a rather simple inertial model. Although more accurate mechatronic models exist in the literature ( Chen et al., 2004 ), our model requires much less time to provide the simulated behavior than them, which is of high importance applying RL methods that require many repetitions.
It has been validated on error measurements taken on the actual mechanical device. The components of the speci fi c ball-screw feed drive we are dealing with are depicted in Fig. 2 . The corresponding inertial model is expressed as  X  J
M  X  J C  X  J S  X  where  X  represents the torque of the motor, J C , J S and J coupling, ball-screw and motor inertia, respectively.  X  represents the motor's angular position and p the screw pitch. M is the joint mass of both the positioning table and the nut, and x is the lineal position of the supporting table. The rotational movement of the shaft is transformed to lineal movement of the table by the nut.
This relation is given by  X   X  2  X  p
After substituting Eq. (2 ) into Eq. (1 ), we obtain the acceleration of the table as a function of the servomotor's torque  X  :  X  x  X 
M p 2  X   X  X  J c  X  J s  X  J m  X  2
Table 1 gives the model parameter settings. Besides, the maximum torque of the motor was set to  X  max  X  37 N m. The length of the guideways was 1 m, and the initial position of the table was x  X  0  X  X  0 : 5m.

The goal of the plant is to enable the controller to drive the position of the table x to a possibly dynamically changing desired position w  X  t  X  or setpoint. This setpoint is usually given by the mechanism operator or some control subsystem. The setpoint signal w  X  t  X  used in our experiments represented in Fig. 3 ,was generated as follows: The setpoint was changed to a random value in range  X  0 : 1 ; 0 : 9 every T seconds, where T was also randomly chosen from a range of  X  0 : 75 ; 1 : 5 s. 3. Feedback control
The input of a feedback controller consists of a time dependent output is the control signal u  X  t  X  . Assuming noisy measurements, a random variable representing the additive noise. The goal of the controller is to minimize the measurement error e  X  t  X  X j
Usually, some tolerance range  X  is de fi ned, such that the goal of the controller is to keep the error within the tolerance range  X  e  X  t  X  o  X   X  . In a typical feed drive set-up, the system receives as input the position of the table ^ x  X  t  X  and its speed ^ different means. The output of the controller will be assumed to be the motor's torque  X   X  t  X  . The desired properties of a feedback controller in a production environment are
Accuracy: the off-set error due to the controller should be minimal.

Time optimality: controllers must be able to drive the con-trolled variable to the setpoint within a tolerance range as fast as possible.

Stability: after some time, the controller should be able to keep the controlled variable within the tolerance range inde fi
Easy tuning: some controllers require a degree of expertise from the designer and large amounts of time, which can result in higher costs. In the ideal case, controllers should be able to auto-tune their parameters, without requiring any input from the designers.

Noise robustness: controllers should be able to perform accep-tably even in the presence of noisy measurements. 3.1. Proportional integrative derivative controller The most popular type of feedback controller is the Proportional
Integrative Derivative (PID) controller, which is de fi ned by the following equation: u  X  t  X  X  K p e  X  t  X  X  K i derivative gain. The output signal is calculated adding three terms: the Proportional term, which is proportional to the current measured error, the Integral term, which is proportional to the accumulated errors, and the Derivative term, which is proportional to the error derivative.

Controller designers usually use double-loop PID control ( Hui et al., 2012; Suppachai et al., 2009 ) for second-order systems such as the ball screw feed drive model of the system we are concerned within this paper. The control scheme used in our experiments is represented in Fig. 4 . The outer controller (labeled PID-1 ) takes the position error e x  X  t  X  as input and outputs a goal setpoint for the speed w v  X  t  X  . The position error is the difference between the setpoint and the measured position: e x  X  t  X  X  ^ x  X  t  X  w ence between the goal speed w v  X  t  X  and the measured speed becomes the input of PID-2 , which outputs the motor torque control signal  X   X  t  X  . 4. Reinforcement learning 4.1. Markov decision processes RL methods ( Sutton and Barto, 1998 ) model environments as a
Markov Decision Process (MDP), which is de fi ned by a tuple  X 
S ;
A ; P ; R  X  , where S is the state space, A is the action space, P is the transition function P : S A S - X  0 ; 1 , and R is the reward function R : S A S -R . The state space S can be either a state set, or a possibly multi-dimensional Euclidean space S where n is the dimensionality of the state space spanned by state action set or a possibly multi-dimensional Euclidean space A with m dimensions ( Hasselt, 2012 ). For the scope of this work, we have only considered continuous state spaces, but both discrete and continuous uni-dimensional action spaces.
The goal of the learning agent is to learn a policy  X   X  s maximizes the expected accumulated rewards R  X  s  X  X  E  X   X  1 where s t and r t represent the state and reward observed in time-step t , and  X  is the discount parameter. When the action space is fi nite, a stochastic policy is de fi ned as the probability of selecting each action in each observable state  X  : S A - X  0 ; 1 . When the action space is continuous, the policy is de fi ned as the correspond-ing probability density function. We will distinguish the determi-nistic continuous policy learned by the actor in Actor-critic algorithms using a different notation:  X  a : S -R m . For clarity, we will assume a discrete set of states and actions in the remainder of this section.

The state value function of a given state s is given by the following Bellman equation : V  X  s  X  X   X  accumulated rewards obtained after executing action a in state s and following policy  X  thereafter: Q  X  s ; a  X  X   X  The optimal action-state value function Q n  X  s ; a  X  is Q  X  s ; a  X  X   X  provided that the optimal state value function: V  X  s  X  X  max 4.2. Approximate reinforcement learning
The size of the state-action space j S A j grows exponentially with the size of the action and state spaces, therefore tabular representations of the action-state and state value functions are unfeasible in real-life applications. In order to improve scalability, Value Function Approximators (VFA), which can be linear (e.g. tile-coding) or non-linear (neural networks), are proposed ( Busoniu et al., 2010 ). In this work, we restrict ourselves to consider linear VFAs.

Linear function approximators map the state space S using mappings  X  1 ;  X  2 ; ... ;  X  n pings. The whole approximation can be expressed as a vector called features . Often, a different feature vector of size n for each dimension of the state space i , such that n f  X  associated mapping functions  X  i ; 1 ; ... ;  X  i ; n function V  X  s  X  can be approximated by ^ V t  X  s  X  X   X  T t is the parameter vector being learned. Usually, only N { n are non-zero for each state in S , and, for practical reasons, approximated as a set of m state value functions if the action set is fi an additional feature vector would be added.
 The most common linear approximators are Tile-coding and
Radial Basis Functions (RBF). RBF are better suited for continuous state spaces because the fuzzy activation functions they use provide a smooth approximation of the real value function. We have used Gaussian networks, with activation functions  X  x where c i ; j is the center and s 2 the shape parameter. We will use different notation for these feature functions, depending on what they actually approximate:  X  Q for Q ,  X  V for V , and  X   X  are three main categories of RL approaches ( Busoniu et al., 2010 ): value iteration, policy iteration and policy search. The latter belongs somehow to a different research area, and will therefore not be discussed here. Neither will be model-based or off-line algorithms, because our aim is to use on-line learning algorithms with as little intervention from the designer as possible. 4.2.1. Value iteration algorithms
Value iteration algorithms approximate V n or Q n by iteratively updating its estimates. Q -iteration algorithms estimate Q to approximate the optimal policy that selects at each time-step the action with the highest Q -value. A policy cannot be directly inferred from V n , and they are mainly used in model-based planning problems and actor-critic architectures. The most popu-lar value iteration algorithms are Q-Learning ( Watkins and Dayan, 1992 ), SARSA and Time-Difference  X  0  X  X  TD  X  0  X  X  ( Sutton and Barto, 1998 ). Q-Learning and SARSA learn the optimal state-action function Q n , while TD  X  0  X  learns the optimal state value function
V . These algorithms are described as follows:
The approximated version of Q-Learning (Q-L) updates its state-action estimates using the following update-rule:  X  Q where  X  is the learning gain.

SARSA estimates the value of the reached state using the policy being followed instead of the maximum state-action value:  X  Q
TD  X  0  X  updates its estimates using a similar update-rule:  X  V 4.2.2. Policy iteration algorithms
The most popular policy iteration methods are Actor-critic architectures. These methods use separate memory structures to represent the actor (the policy) and a critic, learning V iteration method, such as TD  X  0  X  . After the actor selects and executes an action following its own policy, it receives a scalar value from the critic assessing how desirable the last action was.
The actor can then update its policy according to the critic. Usually, the critic response is the Time-Difference error  X  t n ^
V  X  s t  X  ^ V  X  s t 1  X  . In this paper, we have used three different actors along with a TD critic:
Q-AC: the actor derives a policy from a Q -function ( Sutton and Barto, 1998 ). Using a discrete action space, the actor executes an action a in state s , receives the TD error from the critic, and updates the ^ Q  X  s ; a  X  estimation using the following update-rule:  X  Q where min is a constant ensuring that actions with high probabilities will also be updated ( Neumann, 2005 ).

Policy gradient actor-critic (PG-AC): The actor explicitly repre-sents a continuous policy  X  a  X  s  X  and updates it:  X  a  X  s  X   X   X  a t  X  s  X  X   X  t  X  t  X  a t  X  a  X  s  X  X  where a t is the action executed in time-step t .

Continuous action-critic learning automaton (CACLA) ( Hasselt, 2012 ). The actor only updates its policy if the critic is positive, that is, if the policy has been improved: if
The main difference with respect to PG-AC is that the policy update is done in action space, proportional to the difference between the executed action and the policy's output, instead of proportional to the critic  X  t . This algorithm is considered as the state-of-the-art. 4.2.3. Exploration versus exploitation
Agents must make a compromise between exploitation (i.e. greedily selecting the action with highest Q -value) and exploration (i.e. selecting some not yet selected state-action). Without suf cient exploration of the action-state space, agents may not be able to fi nd an optimal policy, and pure exploration can be expected to perform very poorly. Policies based on Q -values such as Q-Learn-ing, SARSA or Q-AC often use Soft-Max action selection  X  s ; a  X  X  exp where  X  is a scalar temperature parameter. A null value of all actions the same probability of being selected, and as its value increases, the selection of actions with higher state-action values become more probable. As  X  grows towards in fi nity, the policy becomes a greedy selection of the maximum state-action value.
Continuous Actor-Critic methods such as PG-AC or CACLA, use a disturbance added to the output of the policy, so that the actor can explore yet unknown policies. In this paper, we have used a
Gaussian distributed random variable N  X  0 ; s 2  X  , added to the algo-rithm's output  X  t  X  s  X  .
 4.2.4. Eligibility traces
RL algorithms usually require great amount of interaction with the environment in order to learn an acceptable policy, because the value of each state-action pair is only updated taking into account the value of the states observed immediately after. Algorithms such as TD  X  previously taken actions. The eligibility traces approach associates an whenever s  X  s t and a  X  a t and decays each time-step according to parameter  X  . In our experiments, all algorithms have been used incorporating with eligibility traces. CACLA has also been applied are only reported without eligibility traces, because they were con-sistently better than with them. 5. Experimental design
State variables : For the control of ball screw drives, RL con-trollers perceive the system state as a set of three variables: x  X  t  X  A  X  0 : 0 ; 1 : 0 m, v  X  t  X  A  X  2 : 0 ; 2 : 0 m = domain of each state variable was approximated with 50 Gaussian
RBF, which were set so that only 3 features per variable were active each time. When required, actions were discretized as 50 different discrete actions in range  X   X  max ;  X  max .

Reward signal : The reward signal used was de fi ned as r  X  t  X  X  C 1 1 tanh 2 j e  X  t  X j  X  c  X  t  X  ;  X  17  X  where  X   X  0.005 is the size of the tolerance interval around w  X  t  X  ,
C  X  1000, and the second term c  X  t  X  represents a cost function that penalizes collisions with the end of the guideway, which is proportional to the force exerted on the limiting table ( F c  X  t  X  X  C 2 j F c j ;  X  18  X  where C 2  X  1 ; 000. The fi rst term of the reward function is very similar to the one proposed in Hafner and Riedmiller (2011) . Fig. 5 plots this fi rst term of the reward function.

Conventional approach : As a reference benchmark, we used a double-loop PID with the best parameter con fi guration found after performing random search ( Bergstra and Bengio, 2012 ) consisting of 50,000 parameter combinations that were randomly generated and evaluated. The con fi guration with the smallest accumulated error on the test setpoint signal of Fig. 3 was selected for comparison with the results of RL.

Methodology : The performance was measured using a training / evaluation methodology ( Vamplew et al., 2011 ): during the train-ing process, the policies learnt were evaluated every 20 episodes using directly the input of the controller. We will refer to these as evaluation episodes . The learning process consisted of n episodes of T  X  20 s.

The exploration parameter was linearly decreased each epi-sode. When the action set was fi nite, we used a Soft-Max action selection policy with initial value  X   X  0.0, decreasing it linearly every episode by  X   X   X  20 : 0 = n e . PG-AC and CACLA used a Sigmoid policy's output. Initially s 2 0  X  0 : 2, decreasing it linearly every episode by  X  s 2  X  s 2 0 = n e . The learning gains were constant during the training episodes, and null during evaluations. Q-Learn-ing, SARSA and Q-AC used  X   X  0.05 to learn the Q and V value functions, and continuous-action actors used  X   X  0.001 to update the policy. All these values were determined empirically after running preliminary tests. Because continuous-action actors use a Sigmoid policy that outputs values in range  X  0 : 76 ; 0 : action sets were normalized in this range too.

Performance measures : The performance measures are the average rewards per episode, which is one of the typical performance measures in RL, and two additional measures pro-posed in Hafner and Riedmiller (2011) :
N  X  then N  X   X  0 ; w  X  X  T . We averaged the stabilization times for all the setpoint changes during an episode. e error of the controlled variable x  X  t  X  from the setpoint w  X  t  X  .
We recall now those properties enumerated in Section 3 . Calculat-ing the average rewards per episode is directly related with time optimality, whereas the stabilization time directly measures sta-bility, and the average absolute offset is meant to measure the accuracy. In the presence of noise, all three performance measures relate to the noise robustness. Determining the usability of a controller (dif fi culty of fi nding an acceptable set of parameters) is out of the scope of this paper. 6. Experimental results
We have conducted two different sets of tests. First, we compare the performance of the learned controllers with noise-free measurements of the process variables ( n  X  t  X  X  0  X  . Second, we introduce a Gaussian noise signal following a probability distribu-ables. This noise variable was added to the actual simulated process variables, setting s 2  X  0 : 001. 6.1. Experiment A: noise-free observation experiments
Fig. 6 shows the average rewards obtained in the evaluation episodes for all fi ve RL algorithms. As a reference, we have also plotted the average rewards that would be obtained by the benchmark PID control if it was the decision policy during an RL episode (constant black line). All RL approaches, except from
Q-Learning, converge to decision policies with better average rewards than the benchmark PID controller. CACLA consistently outperforms all the other controllers. Also, it is quite noticeable that Actor-critic algorithms obtain better results than value itera-tion methods, most likely due to the separate representation and learning of the critic and the actor. Although we could have expected PG-AC to perform better than Q-AC because the action space needs not be discretized, it is the other way round: Q-AC improves over PG-AC. The average rewards obtained by CACLA were 16.01% higher than those obtained by the benchmark double-loop PID. With the exception of Q-Learning, all RL methods obtained performance gains between 4.04% and 16.01% relative to the double-loop PID.

In Table 2 we show the convergence time and the average absolute off-set error for each controller. In general, the two performance measures seem to be aligned with the RL accumu-lated discounted reward performance measure. Nevertheless, PG-AC gets worse score values than SARSA and Q-AC. The unexpected poor behavior of PG-AC and Q-Learning is due to the fact that they were not able to learn how to drive the table within the tolerance region for one speci fi c value of the setpoint. Fig. 7 shows the response achieved by the Q-Learning policy, which was not able to cope with the magnitude of the change of the setpoint. More exploration time would alleviate this problem, but determining the amount of exploration required by each RL method requires a convergence analysis which is beyond the scope of the paper. In relative terms, CACLA's convergence time was 42.77% lower than the double-loop PID's, and between 5.43% and 15.12% lower than the rest of RL methods. It is also worth mentioning that the worst behaving RL method, Q-Learning, had a 32.58% lower convergence time than the double-loop PID. On the other hand, the offset-error using SARSA and CACLA as the controller was 16.02% lower than when the double-loop PID was used. Even the worst behav-ing Q-Learning algorithm was able to obtain an improvement of 2.5% in the error rate.

For visual assessment, we plot the position of the ball screw the plots, we separate the results obtained with discrete action
RL algorithms in Fig. 8 from the results obtained with continuous action RL algorithms in Fig. 9 . The benchmark PID controller saturates its output rather quickly, with short transitions ( Fig. 8 c), producing a behavior comparable to that of a bang-bang controller. Among all the tested controllers, the one pro-duced by CACLA is the only one able to provide a rather smooth output until the tolerance region is reached after some oscillations around it. Figs. 8c and 9c show that the rest present bigger output oscillations. Overall, it is clear that CACLA is the best option. 6.2. Experiment B: robustness against noisy measurements
Fig. 10 shows the average rewards obtained by the controllers during the evaluation episodes. The two continuous action Actor-critic algorithms clearly outperform the rest. As expected, most of the controllers performed worse than in the previous experiment because of the noise introduced in the measurements. The average rewards dropped 11.14% for SARSA, 31.89% for Q-AC, and 7.29% for
CACLA. Nevertheless, Q-Learning obtained a 22.68% performance gain, and PG-AC 5.82%. In this experiment both Q-Learning and
PG-AC were able to learn how to cope with the largest variations of the setpoint, driving the table within the tolerance range for all the setpoint values. Once again, the amount of exploration of the state-action space and the initialization and exploration of the memory structures are critical aspects of the RL approach to the control of feed drives.

In Table 3 we can see that the convergence time and average absolute off-set error are in general worse than in the previous experiment, as expected. The offset error is smaller for CACLA, because the controller does not behave as aggressively around the tolerance region as the others ( Figs. 11c and 12c ). RL methods obtain convergence times between 6.54% and 40.92% lower than with the benchmark double-loop PID. Furthermore, it is paradox-ical that both scores are considerably better for CACLA, though it was overcome by PG-AC considering the average rewards. This suggests that there could be a misalignment between the reward signal and the other two performance criteria in the presence of noise.

The variables of the system are plotted in Fig. 11 for discrete action algorithms, and Fig. 12 for continuous action algorithms. 7. Discussion and future work In this paper, we report the performance of fi ve approximated
Reinforcement Learning methods learning the feedback controller of a ball screw feed drive. The benchmark controller was a double loop PID whose parameters are tuned by random search, selecting the best of a large number of trials. Among the RL methods, we have tested the two most popular value-iteration methods (Q-Learning and SARSA), and three policy-iteration methods. Overall,
RL policy-iteration methods improve over value-iteration meth-ods. Also, RL algorithms with continuous action spaces outperform those with discrete action sets. Our tests show that RL can be an alternative to typical feedback control approaches such as PID controllers: (a) they require less iterations than the conventional random search and (b) they require less feedback and tuning from the designer.

On the one hand, our tests have been conducted with a rather long and randomly generated but unique setpoint signal. Including the command signal as part of the state of the MDP increases considerably the complexity of the problem, and how to train a RL algorithm so that the response is optimal whatever the setpoint signal is remains an open question. Our future work will consider different training techniques, such as learning from an existing controller or general Transfer Learning ( Taylor and Stone, 2009;
Fern X ndez-Gauna et al., 2013 ). On the other hand, more research effort should be focused on de fi ning a reward signal that com-pletely aligns with the desired properties a controller should have.
Furthermore, a multi-objective RL ( Handa, 2009; Vamplew et al., 2008 ) approach with several reward signals could be taken, so that the system learns not only to keep the controlled variable within the tolerance range, but also considers other control criteria, such as energy consumption.
 Acknowledgments We acknowledge support from MICINN through project
TIN2011-23823, and EU SandS project under grant agreement 317947. GIC participates at UIF 11/07 of UPV/EHU.
 References
