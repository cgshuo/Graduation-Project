 data), unannotated data is abundant and can be collected at a lmost no cost. However, supervised machine learning techniques require large quantities of da ta be manually labeled so that automatic learning algorithms can build sophisticated models. Unfor tunately, manual annotation of a large quantity of data is both expensive and time-consuming. The c hallenge is to find ways to exploit the pervised machine learning algorithms. Meeting this challe nge requires research at the cutting edge of automatic learning techniques, useful in many fields such as language and speech technology, im-age processing and computer vision, robot control and bioin formatics. A surge of semi-supervised training schemes. Most of these semi-supervised learning a lgorithms are applicable only to multi-els suitable for structured prediction [2, 9, 16, 20, 21, 22] .
 In this paper, we propose an information theoretic approach for semi-supervised learning of condi-tional random fields (CRFs) [19], where we use the mutual info rmation between the empirical distri-bution of unlabeled data and the discriminative model as a da ta-dependent regularized prior. Grand-used the conditional entropy of their discriminative model s on unlabeled data as a data-dependent regularization term to obtain very encouraging results. Mi nimum entropy approach can be explained from data-smoothness assumption and is motivated from semi -supervised classification, using unla-beled data to enhance classification; however, its degenera cy is even more problematic and arguable by noting minimum entropy 0 can be achieved by putting all mas s on one label and zeros for the minimum conditional entropy approach. Instead, our approa ch can be naturally cast into the rate distortion theory framework which is well-known in informa tion theory [14]. The closest work to ours is the one by Corduneanu et al. [11, 12, 13, 28]. Both work s are discriminative models and do indeed use mutual information concepts. There are two maj or distinctions between our work of view and formulated as a communication game, while our app roach is based on a completely formulated as a data compression scheme, thus leads to a form ulation distinctive from Corduneanu minimization algorithm, whereas our model is more complex a nd the objective function becomes non-convex. In particular, training a simple chain structu red CRF model [19] in our framework turns out to be intractable even if using Blahut-Arimoto X  X  type of alternating minimization algorithm. We develop a convergent variational approach to approximatel y solve this problem. Another relevant work is the information bottleneck (IB) method introduced b y Tishby et al [30]. IB method is an information-theoretic framework for extracting relevant components of an input random variable X , with respect to an output random variable Y . Instead of directly compressing X to its repre-sentation Y subject to an expected distortion through a parametric prob abilistic mapping like our proposed approach, IB method is performed by finding a third, compressed, non-parametric and model-independent representation T of X that is most informative about Y . Formally speaking, the notion of compression is quantified by the mutual informatio n between T and X while the informa-tiveness is quantified by the mutual information between T and Y . The solutions are characterized by the bottleneck equations and can be found by a convergent r e-estimation method that general-izes the Blahut-Arimoto algorithm. Finally in contrast to o ur approach which minimizes both the negative conditional likelihood on labeled data and the mut ual information between the hidden vari-ables and the observations on unlabeled data for a discriminative model, Oliver and Garg [24] have proposed maximum mutual information hidden Markov models ( MMIHMM) of semi-supervised data and the mutual information between the hidden variable s and the observations on unlabeled data for a generative model. It is equivalent to minimizing conditional entropy o f a generative HMM for the part of unlabeled data. The maximum mutual information o f a generative HMM was originally ent from Oliver and Garg X  X  approach in that an individual HMM is learned for each possible class (e.g., one HMM for each word string), and the point-wise mutu al information between the choice of HMM and the observation sequence is maximized. It is equiv alent to maximizing the conditional likelihood of a word string given observation sequence to im prove the discrimination across differ-generative HMMs of training utterances in speech recogniti on.
 In the following, we first motivate our rate distortion appro ach for semi-supervised CRFs as a data compression scheme and formulate the semi-supervised lear ning paradigm as a classic rate distortion problem. We then analyze the tractability of the framework f or structured prediction and present a convergent variational learning algorithm to defy the comb inatorial explosion of terms in the sum over label configurations. Finally we demonstrate encourag ing results with two real-world problems problem and hand-written character recognition as a sequen ce labeling problem. Similar ideas have been successfully applied to semi-supervised boosting [31 ]. over corresponding label sequences. All components, Y p ( y | x ) = 1 f ( such a model from the combined set of labeled and unlabeled ex amples, D l  X  D u . For notational convenience, we assume that there are no identical examples in D l and D u . The standard supervised training procedure for CRFs is base d on minimizing the negative log con-ditional likelihood of the labeled examples in D l degeneracy in the case of correlated features.
 data, Grandvalet and Bengio [15] and Jiao et al. [16] propose d a semi-supervised learning algo-rithm that exploits a form of minimum conditional entropy regularization on the unlabeled data. Specifically, they proposed to minimize the following objec tive is the conditional entropy of the CRF model on the unlabeled d ata. The tradeoff parameters  X  and  X  control the influences of U (  X  ) and the unlabeled data, respectively.
 This is equivalent to minimizing the following objective (w ith different values of  X  and  X  ) where D  X  p P Y on labeled data D l ,  X  p to denote the empirical distribution of X on unlabeled data D u .
 In this paper, we propose an alternative approach for semi-s upervised CRFs. Rather than using minimum conditional entropy as a regularization term on unl abeled data, we use minimum mutual information on unlabeled data. This approach has a nice and strong inform ation theoretic interpre-tation by rate distortion theory.
 We define the marginal distribution p p ( y ) = P empirical distribution  X  p ( x ) and the discriminative model is where H p the label Y on unlabeled data. Thus in rate distortion terminology, the empirical distribution of unlabeled data  X  p abilistic mapping from X to Y , and p strained optimization problem, The rationale for this formulation can be seen from an inform ation-theoretic perspective using the pressed representation Y through a probabilistic mapping p for its minimum description. What determines the quality of t he compression is the information without confusion. According to the standard asymptotic ar guments [14], this quantity is bounded below by the mutual information I p ( x ) , p ing of X is given by the ratio of the volume of X to the average volume of the elements of X that are mapped to the same representation Y through p mutual information is the minimum information rate and is us ed as a good metric for clustering [26, 27]. True distribution of X should be used to compute the mutual information. Since it is unknown, we use its empirical distribution on unlabeled dat a set D u and the mutual information representation since the rate can always be reduced by throw ing away many features in the prob-abilistic mapping. This makes the mapping likely too simple and leads to distortion. Therefore we need an additional constraint provided through a distort ion function which is presumed to be and maximum distortion. Since joint distribution gives the distribution for the pair of X and its representation Y , we choose the log likelihood ratio, log p ( x , y ) but they are unknown. In semi-supervised setting, we have la beled data available which provides to help the clustering. There is a monotonic tradeoff betwee n the rate of the compression and the measure between X and Y on the labeled data set D l , what is the minimum rate description re-by solving (4).
 Following standard procedure, we convert the constrained o ptimization problem (4) into an uncon-strained optimization problem which minimizes the followi ng objective: where  X  &gt; 0 , which again is equivalent to minimizing the following obje ctive (with  X  = 1 particular value of d , there is some corresponding value of  X  in the optimization problem (6) that will give the same  X  . Thus, these are two equivalent re-parameterizations of th e same problem. The equivalence between the two problems can be verified using co nvex analysis [8] by noting that the a constant that does not depend on  X  ), where  X  is the Lagrange multiplier. Thus, (4) can be solved problem, because its objective I  X  p argument as in the minimum conditional entropy regularizat ion case [15, 16]. There may be some minima of (4) that do not minimize (5) or (6) whatever the valu e of  X  or  X  may be. This is however not essential to motivate the optimization criterion. More over there are generally local minima in (5) or (6) due to the non-convexity of its mutual information regularization term.
 Another training method for semi-supervised CRFs is the maximum entropy approach, maximizing constraint on labeled data D l , again following standard procedure, we convert the constra ined optimization problem (7) into an unconstrained optimization problem which minimizes the fo llowing objective: Again minimizing (8) is not exactly equivalent to (7); howev er, it is not essential to motivate the optimization criterion. When comparing maximum entropy app roach with minimum conditional entropy approach, there is only a sign change on conditional entropy term.
 For non-parametric models, using the analysis developed in [5, 6, 7, 25], it can be shown that maxi-mum conditional entropy approach is equivalent to rate dist ortion approach when we compress code vectors in a mass constrained scheme [25]. But for parametri c models such as CRFs, these three approaches are completely distinct.
 The difference between our rate distortion approach for sem i-supervised CRFs (6) and the minimum conditional entropy regularized semi-supervised CRFs (2) is not only on the different sign of condi-tional entropy on unlabeled data but also the additional ter m  X  entropy of p approach for semi-supervised CRFs intractable. To see why, we take derivative of this term with respect to  X  , we have: In the case of structured prediction, the number of sums over Y is exponential, and there is a sum inside the log . These make the computation of the derivative intractable e ven for a simple chain structured CRF.
 tion function established by Blahut [6] and Arimoto [3]. Cor duneanu and Jaakkola [12, 13] proposed a distributed propagation algorithm, a variant of Blahut-A rimoto algorithm, to solve their problem. our case.
 By extending a lemma for computing rate distortion in [14] to parametric models, we can rewrite the minimization problem (5) of mutual information regular ized semi-supervised CRFs as a double minimization, We can use an alternating minimization algorithm to find a loc al minimum of RL assign the initial CRF model to be the optimal solution of the supervised CRF on labeled data and denote it as p In order to define p g for a given r ( y ) . The gradient of g (  X , r ( y )) with respect to  X  is Even though the first term in Eq. (10) and (12) can be efficientl y computed via recursive formu-las [16], we run into the same intractable problem to compute the second term Eq. (10) and Eq. 11) since the number of sums over Y is exponential and implicitly there is a sum inside the log due to r ( y ) . This makes the computation of the derivative in the alterna ting minimization algorithm intractable. supervised CRFs for sequence labeling. The basic idea of con vexity-based variational inference is to make use of Jensen X  X  inequality to obtain an adjustable up per bound on the objective function possible upper bound.
 H ( p  X  ( y )) using Jensen X  X  inequality as the following, Thus the desideratum of finding a tight upper bound of RL the following alternative optimization problem: Minimizing U with respect to q has a closed form solution, It can be shown that where q mization algorithm monotonically decreases U and converges.
 following, we show how to compute the derivative of the last t erm in Eq.(13) using an idea similar Sec. 4.
 If we define A ( y , x ( j ) , x ( l ) ) = P late the derivative of A ( y , x ( j ) , x ( l ) ) with respect to the k -th parameter  X  can be computed through standard dynamic programming techn iques in CRFs except one term P to the subsequence constrained entropy defined in [21]) as: then the term P dence property of linear-chain CRF, we have the following: Given H  X  The base case for the dynamic programming is H  X  p be similarly computed using dynamic programming. semi-supervised learning algorithms, minimum conditiona l entropy approach and maximum condi-tional entropy approach on two real-world problems: text ca tegorization and hand-written character over minimum and maximum conditional entropy approaches wh en no approximation is needed in training. In the second task, a variational method has to be u sed to train semi-supervised chain structured CRFs. We demonstrate the effectiveness of the ra te distortion approach over minimum and maximum conditional entropy approaches even when an app roximation is used during training. 4.1 Text categorization We use Porter stemmer to reduce the morphological word forms . For each label, we rank words the top 100 words as our features. For each problem, we select 15% of the training data, almost 150 instances, as the labeled training data and select the unlab eled data from the remaining data. The contains about 700 instances. We vary the ratio between the a mount of unlabeled and labeled data, repeat the experiments ten times with different randomly se lected labeled and unlabeled training model parameter for mutual information (MI) regularizatio n and maximum/minimum conditional entropy (CE) regularization using the parameter learned fr om a l the ratio between the amount of unlabeled and labeled data on different classification problems. We can see that mutual information regularization outperform s the other three regularization schemes. In most cases, maximum CE regularization outperforms minim um CE regularization and the base-line (logistic regression with l nificant difference in the performance of the learned models based on the baseline; since for each the reason for the performance differences of the baselines models in Figure 1 is due to our feature selection phase. 4.2 Hand-written character recognition Our dataset for hand-written character recognition contai ns  X  6000 handwritten words with average binary image. We choose  X  600 words as labeled data,  X  600 words as validation data,  X  2000 words labeled data, and report the mean and standard deviation of c lassification accuracies over several trials.
 We use a chain structured graph to model hand-written charac ter recognition as a sequence labeling problem, similar to [29]. Since the unlabeled data may have d ifferent lengths, we modify the mu-tual information as I = P data with length  X  . We compare our approach (MI) with other regularizations (m aximum/minimum conditional entropy, l considering the correlation between adjacent characters i n a word. The results are shown in Fig. 2 (right). We can see that MI regularization outperforms maxC E, minCE and l compared with the standard multi-class classification sett ing. We have presented a new semi-supervised discriminative lea rning algorithm to train CRFs. The proposed approach is motivated by the rate distortion frame work in information theory and utilizes the mutual information on the unlabeled data as a regulariza tion term, to be more precise a data dependent prior. Even though a variational approximation h as to be used during training process for approach outperforms supervised CRFs with l minimum conditional entropy approach as well as semi-super vised maximum conditional entropy approach in both multi-class classification and sequence la beling problems. As future work, we would like to apply this approach to other graph structures, develop more efficient learning algo-rithms and illuminate how reducing the information rate hel ps generalization.
