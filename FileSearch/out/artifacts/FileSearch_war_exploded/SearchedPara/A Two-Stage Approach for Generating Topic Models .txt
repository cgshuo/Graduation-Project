 The statistical topic modeling technique has attracted big attention due to its more robust and interpretable topic representations and wide applications in the fields of information retrieval, text mining, text classification, scientific publication topic anal-ysis and prediction[1-4] etc. It starts from Latent Semantic Analysis (LSA) [5] that can capture most significant feature of collection based on semantic structure of rele-vant documents. Probabilistic LSA (pLSA) [6] and Latent Dirichlet Allocation (LDA) [7] are variations to improve the interpretation of results from statistical view of LSA. These techniques are more effective on document modeling and topic extraction, which are represented by topic-document and word-topic distribution, respectively. Many topic models not only automatically extract topics from text, but also detect the evolution of topics over time [8], discover the relationship among the topics [9], supervise the topics [10] with other information (authorship, citations, et al.) for extensional applications, such as recommendation [11] and so on. 
Basically, the existing statistical topic modeling approaches generate multinomial distributions over words to represent topics in a given text collection. The word distributions are derived based on word frequency in the collection. Therefore, popu-lar words are very often chosen to represent topics. For instance, Table 1 shows an example of multinomial word distributions used to represent four topics of a scientific publication collection. It can be seen from Table 1 that word  X  X ethod X  dominantly occurs across all four topics with high probability. It is obvious that  X  X ethod X  is a general word and very popularly used in describing research works in almost all different areas. It actually will not contribute much to uniquely represent distinctive features of any research area or topic. These kind of popular words bring a lot of confusion to the topic representation other than distinctively representing the topics. 
Except for the ambiguity problem produced by popular words, another fundamen-tal problem is that topics are represented by multinomial distribution of isolated words which lack semantic and interpretable meaning. Although topic models can supply much information and annotate documents with the discovered topics and also supply word distribution for each topic, users still have difficulties to interpret the semantic meanings of the topics only based on the distribution of words, especially for those who are not very familiar with the related area. Mei et al. [12] and Lau et al. [13] developed automatic labeling methods for interpreting the semantics of topics by phrases. But, they heavily depend on candidate resources for labeling topics. If the topics themselves are diverse or novel to the candidate dataset, the systems will mislabel the topics. Although Lau et al. [14] labeled a topic by selecting a single term from the known distribution of words rather than candidate resources, the selected word can hardly represent the whole topic well. 
In order to solve the problems of word ambiguity and semantic coherence that exist in almost all topic models, we need new model to update the topic representations. The new method should extract more distinctive representations and discover the hidden associations under multinomial words distributions. In text mining, many methods have been developed to generate text representation for a collection of documents. Most text mining methods are keyword-based approaches which use single words to represent documents. Based on the hypothesis that phrases may carry more semantic meaning than keywords, approaches to use phrases instead of keywords have also been proposed. However, investigations have found that phrase-based methods were not always supe-rior to keyword based methods [15-17]. Recently, data mining based methods have been proposed to generate patterns to represent documents which have achieved promising results [18]. Topic modeling has the advantage of classification from large collections, while text mining is good at extracting interesting features to represent collections. So, it leads us to improve the accuracy and coherence of topic representations by utilizing text mining techniques, especially term weighting and pattern mining methods. 
In this paper, a two-stage approach is proposed to combine the statistical topic modeling technique with the classical data mining techniques with the hope to improve the accuracy of topic modeling in large document collections. In stage 1, the most rec-ognized topic modeling method Latent Dirichlet Allocation (LDA) is used to generate initial topic models. In stage 2, the most popular used term weighting method tf-idf and the frequent pattern mining method are used to derive more discriminative terms and patterns to represent topics of the collections. Moreover, the frequent patterns reveal structural information about the associations between terms that make topics more understandable, semantically relevant and cover broaden meanings. most common topic modeling tool currently in use. It can discover the hidden topics in collections of documents with the appearing words. Let  X  X  X  X  X   X   X , X ,  X   X  be a collec-tion of documents, called documents database. The total number of documents in corpus is M . The idea behind LDA is that every document is considered involving multiple topics and each topic can be defined as a distribution over fixed vocabulary of terms that appear in documents. Specifically, LDA models a document as a probabilistic mixture of topics and treats each topic as a probability distribution over words. For the i th word in document d , denoted as  X   X , X  , the probability of  X   X , X  ,  X  X  X   X , X   X  is defined as: topics. Let  X   X  be the multinomial distribution over words for  X   X  ,  X   X   X  X  X  X   X , X  ,  X  ic indicates the proportion of topic j in document d . LDA is generative model that only which optimize the topics and documents distributions. 
Among many available algorithms for estimating hidden variables, the Gibbs sampling method is a very effective strategy for parameter estimation [19, 20]. The results of LDA are at two levels, corpus level and document level. At corpus level, D is represented by a set of topics each of which is represented by a probability distribu-tion over word,  X   X  for topic j . Overall, we have  X  X   X   X   X  , X   X   X , X ,  X   X  for all topics. For illustrating the results derived by LDA, let X  X  look at a simple example depicted in there are 12 words appearing in the documents. Assuming the documents in D involve 3 topics, Z 1 , Z 2 , and Z 3 . Table 2 illustrates the word distribution for each of the topics. the simple example mentioned above, the document representation is illustrated in Table 3. Apart from these two level outcomes, LDA also generates word  X  topic assignment, that is, the word occurrence is considered related to the topics by LDA. Table 4 illustrates an example of the word-topic assignments. 
The topic representation using word dist ribution and the document representation using topic distribution are the most important contributions provided by LDA. The topic representation indicates which words are important to which topic and the doc-ument representation indicates which topics are important for a particular document. These representations have been widely used in various application domains such as information retrieval, document classification, text mining etc. On the other hand, the word-topic assignments also indicate which words are important to which topics, which is similar to the topic representation. However, the topic representation is at corpus level, while the word-topic assignments are at document level, which implicate more detailed or more specific association between topics and words. In this paper, we propose to mine word-topic assignments generated by LDA for more accurate or more discriminative topic representations for a given collection of documents. For most LDA based applications, the words with high probabilities in topics X  word distributions are usually chosen to represent topics. For example, the top 4 words for words w 1 and w 7 have relatively high probabilities for all the three topics. That means, they most likely represent general concepts or common concepts of the three topics and cannot distinctively represent the three topics. Moreover, the words in topic representations generated by LDA are individual single words. These single words provide too limited information about the relationships between the words and too limited semantic meaning to make the topics understandable. In this section, we pro-pose two methods based on text mining and pattern mining techniques, which are detailed in the following sub sections, aiming at alleviating the mentioned problems. 3.1 Tf-idf Weighting Based Topic Modeling The first method is based on the well-known term weighting method tf-idf (term fre-quency  X  inverse document frequency). The distinct feature of the tf-idf method is that it chooses discriminative terms to represent a document or a topic rather than popular terms. As we illustrated in the above example, there exist general or common terms in the topics X  word distributions generated by LDA. We propose to utilize the tf-idf technique to process the topics X  word distributions in order to generate more discriminative words to represent topics. As illustrated in Table 4, LDA generates word-topic assignments for each document, which reveal word importance to topics for that document. The basic idea of the proposed tf-idf based method is to find the discriminative words from the words which are assigned to a topic by LDA to construct a collection called topical document collection , denoted as D topic . Each doc-ument in the collection consists of all the word-topic assignments to a topic in the original document collection D . The second step is to generate a set of words for representing each document in D topic by applying the tf-idf method to the collection. (1) Construct Collection D topic sequence of words assigned to topic Z j in document d i . For the example illustrated in = w 1 w 2 w 3 w 2 w 1 . Each document  X   X   X  in D topic is defined as  X  consists of the word-topic assignments  X   X   X  , X   X  to topic Z j , each word-topic assign-document since it consists of the words for a particular topic. Assuming that the original document collection D has V number of topics, the collection D topic is defined collection can be constructed as showed in Fig.1. (2) Generate Document Representation for Collection D topic generated based on their tf-idf scores, which are calculated by equation (3).  X  X  X  X   X , X   X  is  X  number of topical documents and  X  X  X  X   X , X   X  is the document frequency. Thus, high tf-idf term weighting indicates high term frequency but low overall collection frequency. Table 5 provides an example of the results which shows that, the tf-idf method wea-kens the effect of the common words w 1 and w 7 , in the meanwhile, increases the weights for the distinctive words in each topic. 3.2 Pattern-Based Topic Modeling A pattern is usually defined as a set of related terms or words. As discussed in Section 1, patterns carry more semantic meaning and are more understandable than isolated words. The idea of the pattern based representations starts from the knowledge of frequent patterns mining. It plays an essential role in many data mining tasks that try to find interesting patterns from datasets. We believe that pattern based representa-tions can be more meaningful and more accurate to represent topics. Moreover, pattern based representations contain structural information which can reveal the association between the terms. (1) Construct Transactional Dataset The purpose of the proposed pattern based method is to discover associated words (i.e., patterns) from the words assigned by LDA to topics. With this purpose in mind, we construct a set of words from each word-topic assignment  X   X   X  , X   X  instead of using the sequence of words in  X   X   X  , X   X  , because for pattern mining, the frequency of a word =  X  X   X | X  X   X  struct a transactional dataset  X   X  . Let  X  X  X  X  X   X   X , X ,  X   X  be the original document col-For the topics in D , we can construct V transactional datasets. An example of the transactional datasets is illustrated in Fig.2, which is generated from the example in Table 4. (2) Generate Pattern-based Representation Frequent itemsets are the most widely used patterns generated from transactional data-based method is to use the frequent patterns generated from each transactional dataset  X  generated from  X   X  are given in Table 6. {  X   X  } and {  X   X  ,  X   X  } are the dominant patterns for topic 2. Comparing with the term based topic representation, patterns represent the associated words that carry more concrete and identifiable meaning. For instance,  X  X ata mining X  is more concrete than just one word  X  X ining X  or  X  X ata X . We have conducted experiments to evaluate the performance of the proposed two topic modeling methods. In this section, we present the results of the evaluation. 4.1 Datasets Four datasets are used in the experiments, which contain the abstracts of the papers published in the proceedings of KDD, SIGIR, CIKM and HT from 2002 to 2011. The four datasets contain 1227, 1722, 2048 and 483 abstracts, respectively. The abstracts are crawled from the ACM digital library 1 , and stemmed by using Porter X  X  stemmer package 2 in the Apache X  X  Lucene Java. 4.2 Experiment Procedure dataset preparation to construct the datasets described in Section 4.1. Then in the step of topic generation, we utilize the sampling-based LDA tool provided in MALLET 3 to generate LDA topic models. The number of topics V = 20, the number of iterations of Gibbs sampling is 1000, the hyperparameters of LDA  X  = 50/ V =2.5,  X  = 0.01 in this experiment [20]. Step 3 is to construct the topical document datasets and the transactional datasets for optimizing topic representations, and the final step is to generate the discriminative terms based and the frequent pattern based topic represen-tations using the pro-posed methods introduced in Section 3. We divide each dataset into training set and testing set, 90% of the documents in each dataset are used as the training set for generating topic models, while the other 10% of the documents in each dataset are left for evaluation. 4.3 Experiment Result Analysis LDA is chosen as the baseline model to compare with the two proposed methods in the experiments. Table 7 demonstrates some examples of the topic representations generated by using the three models, i.e., the LDA model, the tf-idf based model, and the pattern based model. The top 12 words or patterns in each of the topic represen-tations generated by the three models are displayed in Table 7 for two topics, topic 4 and topic 0, of dataset KDD. 
From the results we can see that the top 12 words or patterns have a large overlap between each pair of the three methods, which could indicate that all the three me-thods can derive similar representations. But, when taking a close look, we can find that the results generated by the pattern based method provide much more concrete and specific meaning. For example, for topic 4, all the three methods rank  X  X arge X  as the top 1 word which is a general term. However, the pattern based method generates more specific patterns  X  X arge algorithm X ,  X  X arge scale X , and  X  X arge compute X  which make the topic representation much easier to understand, while the other two methods cannot. Similar evidence can be seen for topic 0 as well. We have showed an example in Table 1 that the word  X  X ethod X  was chosen by LDA for representing three topics including topic 0. In Table 7, the topic representations for topic 0 generated by the three methods are listed, from which we can see that, the ranking of the word  X  X e-thod X  was decreased by the tf-idf based method. This indicates that the word  X  X ethod X  is not a discriminative word for uniquely representing topic 0. Moreover, the pattern based representations enrich the content of the topic representations generated by existing models such as LDA by discovering hidden associations among words, which makes the topics more detailed and comprehensive. Just for illustrating the usefulness of the pattern based method, we display in Table 8 some other patterns contained in the topic representations for dataset KDD. From the results we can see that patterns supply meaningful and semantic topic representations. 4.4 Evaluation The ultimate goal of the proposed methods as well as other existing topic modeling methods is to represent the topics of a given collection of documents as accurately as possible. For the existing topic modeling methods and the proposed methods, the topic representations are word or pattern distributions with probabilities. The more certain the chosen words or patterns are in the topic representations, the more accurate the topic representations become. By taking this view, in this paper, we use informa-tion entropy , a well known certainty measurement developed in information theory, as the merit to evaluate the generalization performance of the proposed methods. Using the documents in the testing set, we compute the entropy of the topic models generat-ed from the training set to evaluate the performance of the proposed models. The lower the entropy, the more certain the topic models to represent the topics and there-fore the more predictable the documents X  topics are. Formally, for a testing set  X   X  X  X  X  , the entropy of the topic models is defined as: based, and the pattern based methods.  X   X   X   X  is the document representation  X   X  gen-erated from LDA. For the evaluation, both the tf-idf weighting and patterns supports have been normalized into probabilities. The evaluation result is presented in Table 9. 
The evaluation clearly indicates that the tf-idf based model fairly achieved lower entropy values than the baseline model, meaning that, it has better performance when interpreting the meaning of the topics. Furthermore, the pattern based method achieved even much lower entropy values than any of the other two. Based on the results, we can conclude that the pattern based method apparently can generate more certain and more accurate representations for the topics of a document collection. Topic models have been extended to capture more interesting properties [7-10,19-20], but most of them represent topics by multinomial word distributions. Topic labe-ling [12-14] is a prevalent method to express semantic meaning of topics as mentioned in Introduction. For another example, Magatti et al. [21] present a method to calculate the similarities between given topics and known hierarchies, then choose the most agreed labels to represent the topics. However, the drawback of the existing methods of topic labeling is that they are he avily restricted to candidate resources and limited on semantic coverage. Topical n -gram (TNG) [22] model discovers topically-relevant phrases by Markov dependencies in word sequences based on the structure of LDA, which is relevant to our work. Except for the method of generating topic phras-es, Zhao et al. [23] proposed a principled probabilistic phrase ranking algorithm for extracting top keyphrases as topic representations from the candidate phrases. The results provided in [22] and [23] show that the topics represented by the phrases are more interpretable than that of its LDA counterpart. But comparing with the pattern based representations proposed in this paper, the phrases may share low occurrences in documents, which can X  X  achieve effective retrieval performance. This paper proposed a two stage model to generate more discriminative and semantic rich representations for modeling the topics in a given collection of documents. The main contribution of this paper is the novel approach of combining data mining tech-niques and statistical topic modeling techniques to generate pattern based representa-tions and discriminative term based representations for modeling topics. In the first stage of the proposed approach, any topic modeling method, as long as it can generate words distributions over topics, can be used to generate the initial topic representa-tions for documents in the collection. In the second stage, we proposed to mine the initial topic representations generated in the first stage for more accurate topic repre-sentations by using the term weighting method tf-idf and the pattern mining method. Our experiment results show that the pattern based representations and the discrimina-tive term based representations generated in the second stage are more accurate and more certain than the representations generated by the typical statistical topic model-ing method LDA. Another strength provided by the pattern based representations is study the structure of the patterns and discover the relationship between words which will represent the topics at a more detailed level. 
