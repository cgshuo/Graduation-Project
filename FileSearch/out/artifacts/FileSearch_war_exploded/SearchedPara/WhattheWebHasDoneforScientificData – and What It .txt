 Try to imagine the unthinkable: you have lost your internet connection. So you go to the reference shelves of your local library for some information relevant to your work. Perhaps you are interested in demography and want the GDP and population of some country. The chances are that you will find a rather sorry and little-used collection of reference book s, most of them relics of the time before the web  X  only a few years ago  X  when libraries were the main vehicle for the dissemination of scientific and scholarly information. The reference books have been replaced by databases to which, if they are not open-access, the library has subscribed on your behalf. You would, of course, be much better off using the web. In fact, for scientific data the web has had huge benefits  X  it has provided access to much large r and richer data collections;  X  the information is much more timely  X  we do not have to wait for a new  X  access to the information is much faster and simpler;  X  the information is better presented; and  X  as a result, new information sources ha ve been created which both classify enormous. Michael Lesk [Les] has argued that it has actually changed the scien-tific method from  X  X ypothesize, design and run experiment, analyze results X  to  X  X ypothesize, look up answer in data base X . Almost all of modern science is now dependent on databases. Biology has led the way in the use of organised data collections to disseminate knowledge (I shall refer to these as databases) but nearly all branches of scientific research are now dependent on web-accessible data resources. Databases are vehicles for publishing data (in fact the databases themselves can be considered publications), and it is often a condition of scien-tific funding that an appropriate database be generated and made accessible to the scientific community.
 library is no longer the primary vehicle for the dissemination of scholarship. But is it possible that in the rush to place our data  X  X n the web X  we are losing some important functions that libraries  X  whether or not by design  X  traditionally provided? Consider again your journey to the library.
 chances are that some other library has a copy of it. By having copies of the same work distributed among many libraries, there is some guarantee that the the information is being preserved. Cop ying has always been the best guarantee of preservation. Now that your data is kept at some centralised database, it is not at all clear that it is in a form appropriate for long-term presentation. Also, the responsibility for keeping the information is now assumed by the organisation that maintains the database rather than being distributed among a number of libraries.
 updated from time to time. If the library decided not to buy the new edition, at least you could revert to an old edition. Now, if the library drops its subscription to the on-line data, what do you have? This underlines the fact that the economic and intellectual property issues with databases on the web are very different from those that apply to traditional paper-based dissemination of knowledge. However the law that applies to digital data collections is effectively based on the traditional copyright laws.
 a serviceable method of citing it accord ing to one of a few accepted methods. You could, if needed, localise the information by giving a page number, and the citation could be checked by other peo ple who had access to the cited document. Now it is not at all clear how you cite something you find in a database; and you have no guarantee that it can be checked. Maybe the web site has disappeared, or maybe the database has been updated.
 old information  X  perhaps the GDP from some past year. The old publications in the library may have this information, but the database does not. dissemination of scientific data on the web. Having fast access to up-to-date research material may come at the price of data quality. Arguably, the web is losing the scientific record as fast as it is creating it; and users of web data have little faith in it until they can verify its provenance and authorship. drawbacks of web data, we are led to some new and challenging problems that involve databases, the web and other areas of computer science.
 1.1 Scientific Data The use of database technology  X  in a broad sense of the term  X  to support sci-entific data is increasing rapidly. However, scientific data puts different demands on database technology. For example, transaction rates are typically lower in the maintenance of most scientific databases. Scale [HT] is arguably important, and complexity is surely important. Not only is  X  X chema X  complexity high, but the kinds of interactions between query languages and scientific programming require relatively complex algorithms and place new demands on the already complex area of query optimisation [Gra04]. The latter paper deals well with some of the issues of scale. In this note I want to deal with a largely orthogo-nal set of issues that have arisen in discussion with scientists who are dealing with databases in which the primary issues, at least for the time being, do not concern scale, but involve the manipulation, transfer, publishing, and long-term sustainability of data. Biological data has been the prime mover in some of these issues, but other sciences are catching up. Data integration, of course, a relatively old topic in database research, which is crucial to curated databases. While low-level tools such as query languages that can talk to a variety of data formats and databases are mow well-developed; declarative techniques for integration and transformation based on schema anno-tation and manipulation have been slow to come to market; and where progress has been made it is with relatively simple data models[HHY + 01,PB03].Asur-vey of the status quo is well beyond the scope of this paper, but it is worth remarking that that while the emergence of XML as universal data exchange format may help in the low-level aspects of data integration through the use of common tools such as XQuery [XQu], it is not at all clear whether XML, has helped in the higher-level, schema based approach to data integration. The com-plexities of constraint systems such as XML Schema [XML] appear to defy any attempt at schema-based integration. Moreover, it is not clear what serialisation formats  X  upon which XML Schema is based  X  have to do with the data models in which people naturally describe data.
 made, is that of data publishing . There is again a growing literature on this topic. The idea is that individual organisations will maintain their data using a variety of models and systems but will agree to common formats, probably XML, for the exchange of data. The problem now is to export data efficiently in XML and, possibly, to transform and import the XML into other databases. This not only requires efficient and robust tools fo r describing and effecting the transfor-mation [FTS00, BCF + 02] but also tools for efficien tly recomputing differences when the source database is modified  X  a new form of the view maintenance problem. A common complaint from people who annotate databases based on what is in the printed literature is that citations are not specific enough. For example, in the process of verifying an entry in some biological database, one needs to check that a given article mentions a specific chemical compound. Even if one is given a page number, it can be quite time consu ming to pinpoint the reference. The point here is that the more we can localise citations the better. Now consider the issues involved with citing something in a database. There are two important issues.  X  How does one cite the database itself and localise the information within the  X  How does one cope with the fact that the database itself changes? Does a identifiers, such as identifiers of a database or web site. This has led people in-terested in long-term preservation to c onsider a variety of techniques for main-taining digital object identifiers that persist over an extended period. But even when the domain of citation is in our control, for example we want to spec-ify localised citations within a websit e or database, how do we specify these citations, and what whose responsibility is it to deal with changing data? For relational databases, a solution to the localisation problem is to use keys or sys-tem tuple identifiers. For hierarchical data such as XML, a solution is suggested in [BDF + 02]. However these are partial solutions to the localisation problem. Standards for data citation need to be developed, and dealing with change is a major problem. This is a growing area of activity in scientific databases. Some biological data-bases describe themselves as  X  X nnotation databases X , and there are some systems [SED] which are designed to display an overlay of annotations on existing data-bases. Database management systems typically do not provide  X  X oom X  for ad hoc or unanticipated annotation, and on ly recently has any attempt been made to understand what is required of database systems to provide this functional-ity [CTG04]. Also known as  X  X ineage X  and  X  X edigree X  X C W01, BKT00], this topic has a variety of meanings. Scientific programming often involves complex chains or workflows of computations. If only because one does not want to repeat some expensive analysis, it is important to keep a complete record of all the parameters of a workflow and of its execution. This is sometimes known as  X  X orkflow X  or  X  X oarse-grained X  provenance [SM03].
 copied from one database to the next. As part of data quality know where a data element has come from, which databases it has passed through, and what actions or agents created and modified it. Even formulating a model in which it is possible to give precise definitions to these is non-trivial. Moreover, since much copying is done by programs external to the databases or by user actions, it is a major challenge to create a system in which provenance is properly recorded. It involves much more than database technology. For example, data is often copied by simple  X  X opy-and-paste X  operations from one database to another. To provide proper mechanisms for tracking this data movement will involve not only re-engineering the underlying database systems but also to the operating systems and interactive environments in which the changes are made. Keeping the past states of a database is an important part of the scientific record. Most of us have been  X  X urned X  by a reference to a web page or on-line publication that has disappeared or has been changed without any acknowledg-ment of the previous version. This is one area in which we have made some progress [BKTT04]. A system has been im plemented which records every ver-sion of a database in a simple XML file. For a number of scientific databases on which this has been tested, the size of an archive file containing a year X  X  worth of changes exceeds the size of one ver sion of the databas e by about 15%. Yet any past version of the database may be retrieved from the archive by a simple linear scan of the archive.
 versions, one records one database with the changes to each component or object recorded at a fine-grained level. This relies on each component of the database having a canonical location in the database, which is described by a simple path from the root of the database. It is common for scientific data to exhibit such an organisation, and this organisation may be of use in other aspects of curated data such as annotation, where some notion of  X  X o-ordinate X  or  X  X ey X  is needed for the attachment of external annotation. In fact, it relies crucially on a system for fine-grain citation such as that advocated in section 3.
 relational database, it dumps the database into XML making it independent of a specific database management system and intelligible to someone who un-derstands the structure of the data. The subject of digital preservation is more than preserving bits. It is about preserving the interpretation of a digital re-source. For example, you have a document in the internal format of an early word processor. Should you be concerned about preserving the text, the format-ted text, or the  X  X ook and feel X  of the document as it was to the users of that word processor [CLB01, LMM01, OAI]. Databases may be in a better position because there is arguably only one level of interpretation  X  the SQL interface for relational databases, or the syntactic representation of the data in XML. But this does not mean that there is nothing to worry about. How do you preserve the schema, and are there o ther issues of context tha t need to be maintained for the long-term understanding of the data? Integration, annotation, provenance, citation, and archiving are just a few of the new topics that have emerged from the increasing use of curated databases. Some progress can be made by augmenting existing database technology. But fully to deal with provenance and integration, we need a closer integration of databases with programming languages and operating systems. These require better solutions to the impedance mismatch problem. Some progress was made with object-oriented databases, and mo re recently in progr amming languages [Com] and web programming [Wad] which understand typed data and in which file systems are replaced by database systems as a fundamental approach to solving the impedance mismatch problem. It is not clear whether the natural inertia in software development will ev er allow such a radical change to take place. In addition, even if all the technica l problems are solved, the social, legal and economic problems with web data are enormous.

