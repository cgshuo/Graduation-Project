 One of the overarching tasks of document analysis is to find what topics people talk about. One of the main techniques for this purpose is topic modeling. So far many models have been proposed. However, the existing models typically per-form full analysis on the whole data to find all topics. This is certainly useful, but in practice we found that the user almost always also wants to perform more detailed analyses on some specific aspects, which we refer to as targets (or targeted aspects ). Current full-analysis models are not suit-able for such analyses as their generated topics are often too coarse and may not even be on target. For example, given a set of tweets about e-cigarette, one may want to find out what topics under discussion are specifically related to chil-dren . Likewise, given a collection of online reviews about a camera, a consumer or camera manufacturer may be inter-ested in finding out all topics about the camera X  X  screen , the targeted aspect. As we will see in our experiments, current full topic models are ineffective for such targeted analyses. This paper studies this problem and proposes a novel tar-geted topic model (TTM) to enable focused analyses on any specific aspect of interest. Our experimental results demon-strate the effectiveness of the TTM.
 Targeted Topic Modeling; Targeted Aspect; Targeted Anal-ysis; Focused Analysis; Targeted Modeling; Topic Modeling
One of the important text mining tasks is to discover the topics discussed in a collection of text documents (or a cor-pus). Topic modeling is one of the main techniques used for this purpose. So far numerous topic models have been proposed in the literature, which may mine topics only or jointly mine topics and other types of useful information, for example, sentiment information [17, 5, 32].
 However, existing models typically perform full analysis on the entire corpus to discover all topics. This is certainly useful, but it is inevitably coarse. In practice we found that the user almost always also wants to perform deeper and more focused analysis on some specific aspects of the data, which we refer to as targets , or targeted aspects in this paper. For example, given a set of tweets about e-cigarette , the user (or researcher) wants to gain insight into topics that have been discussed about children . Here children is the targeted aspect. If a topic model can find topics such as regulations and fears about children smoking e-cigarette that are specif-ically related to this target, it will be very useful. Formally, the proposed targeted analysis problem is defined as follows (note that we use targeted analysis and focused analysis in this paper interchangeably).

Problem Definition : Given a corpus C of documents of a broad area/domain, discover related topics T of a user-interested aspect (called targeted aspect) represented with a set of keywords S provided by the user.

To solve this problem, a natural approach to start with is to use a regular full-analysis topic model such as LDA [4]. Following the previous example, we can see that e-cigarette is a broad area (with corpus C provided) and children is the targeted aspect. Note that the target children is represented by some keywords S (e.g., S = {  X  X hildren, X   X  X ids X  } ). After applying LDA on the corpus C to produce a set of topics Q , we find those topics in Q that contain some keywords from S in their top ranked words and study them in order to find the target-related topics T ( T  X  Q ). However, this approach is often unsatisfactory due to a few reasons (or issues ). 1. The user does not know all the keywords that can represent a targeted aspect. In the above example, if the user specifies  X  X hildren X  as the only keyword and miss out other related keywords such as  X  X oung X  and  X  X inors X , he may lose some important topics. 2. It may not find any topics for the user-interested as-pect. Because there may be many other more prevalent or dominating topics in the data, the model may not find the related topics of some less frequent aspects. For example, when our targeted aspect is weight represented by the key-word  X  X eight X  in camera reviews, the word  X  X eight X  has a relatively low occurrence frequency as people often mention this concept implicitly in the sentences like  X  X he lens is so heavy X  or  X  X ts battery is light X . A full-analysis topic model such as LDA may not find any topic about weight at all. 3. Even if the keywords are frequent, one still may not find good coherent topics due to two reasons: (1) Topic suppression : Since the targeted aspect is only one of many aspects discussed in a broad area and a full-analysis topic model generates all topics for all aspects, the related top-ics of the targeted aspect may be suppressed. Many gen-eral words may be ranked at the top. For example, a topic about children (the targeted aspect) with topical words like  X  X hildren X ,  X  X ids X ,  X  X oung X  is not informative. (2) Word in-trusion : Words from other non-targeted aspects X  topics may be intruder words appearing in the related topics of the tar-geted aspect, which makes the detection and understanding of the target-related topics difficult.

The cause of the above problems is related to some prop-erties of topic modeling. First, useful information may not be easily detected under the condition of data sparsity or small data size. That is because classic topic models are unsupervised and governed by the phenomena called higher order co-occurrence [13]. As a result, some informative but infrequent words may be ranked low or even can not be cor-rectly grouped. Second, the existing models are not targeted towards any user interest. As identified in [6], the objective functions of topic models may not correlate well with human judgments and needs. For example, a given broad corpus often cover a large number of topics, the topics for the user-interested aspect (i.e., target) may be mixed up with other non-target related topics and become incoherent. Although increasing the number of topics may help, the problem re-mains and other problems like information fragmentation may show up (the problems will be further discussed in Sec-tion 4.4). One may use knowledge-based models for targeted modeling [23, 32], but these models only try to put words related to the user specified keywords in the same topic. They do not distill the topics related to target aspect rep-resented by keywords as we do. They still suffer from the aforementioned issues.

Another intuitive approach to solving the proposed prob-lem is to select a subset of documents from the corpus C that contains at least one keyword s  X  S (denoted by C 0 ) and apply LDA to the resulting documents in C 0 . Clearly, this approach has the problem as illustrated in Issue 1. It also has Issue 2 but manifested differently. For example, when the keyword set S is {  X  X eight X  } the number of docu-ment C 0 might be so small that a topic model is unable to produce many good topics. Moreover, since it discards many potentially relevant documents, it diminishes the quality of topics and also loses many potentially related topics. Recently, topic modeling with sparsity has been proposed. Sparse topic models such as those in [8, 33, 18] can (a) iden-tify focused topics of a document, or (b) extract focused words of a topic. However, they are still full-analysis mod-els and not for targeted modeling, and thus still suffer from the aforementioned issues. More discussions will be given in Section 2. Inspired by them, we employ the sparsity idea in designing our new model for targeted/focused analysis.
To address the proposed problems, we designed a new model called the targeted topic model (TTM). It is used for focused analysis as it can directly generate related topics of a given targeted aspect. The novelty of TTM is that it models using the entire corpus C while targeted at the user-specified aspect. This enables TTM to discover more related topics and also improve the topic quality because it can better exploit the information from other relevant documents in C that do not contain the given keywords in S .

In summary, this paper makes the following contributions: 1. It proposes the new problem of targeted topic modeling to discover only topics that are related to a user-specified aspect. To the best of our knowledge, no existing topic model can perform this task. Such targeted/focused analysis is important because not everyone is interested in everything in a corpus. When one is interested in a particular aspect, he/she often wants to perform deeper and focused analysis. 2. It proposes a new probabilistic topic model called Tar-geted Topic Model (TTM) that is able to perform the pro-posed focused analysis, which is also the first such model. 3. Our experimental results using five real-life datasets and a set of aspects show the effectiveness of the proposed model. It outperforms state-of-art baseline models markedly.
To our knowledge, there is no existing topic model that is able to perform the proposed targeted analysis as we do. Our work is, however, clearly related to the classic topic models such as PLSA [14] and LDA [4] and their variants. These models have been used to discover hidden thematic structures in a collection of documents or corpus. There are numerous existing models (e.g., [25, 24, 34, 23, 8, 11, 27]). They either identify topics only or jointly identify both top-ics and other types of information. For example, while both LDA and PLSA only identify topics, [30, 22] jointly model both topics and ratings in reviews. [24, 10] model labeled data with the class information. [19, 15] conduct time-series analysis of topics. However, as we indicated in the intro-duction section, all these models and their variants are full analysis models. They aim to find all topics in the corpus, and none of them is able to perform targeted analysis based isting research also proposed several knowledge-based topic models, which can incorporate prior domain knowledge in topic modeling [1, 23, 9, 32] to generate better results. But they are also full-analysis models, and do not help discover related topics of the user interested aspect.

Our work is also related to sparse topic models. The mod-eling of sparsity can represent skewed distributions, e.g., a topic usually focuses on a narrow range of words instead of a wide range of them in the vocabulary [18]. This type of models is inspired by the influential power of the asymmetric Dirichlet prior [28]. Existing sparse topic models can be cat-egorized into two types: (1) those that discover the salient (focused) topics of a specific document [8, 33], and (2) those that discover representative (focused) terms of a particular topic [29]. Researchers also tried to achieve both at the same time in a hybrid manner [18, 2]. However, it is important to note that focused topics or terms in sparse models are entirely different from our targeted analysis because they aim to achieve very high probabilities for a small number of topics in a document or for a small set of words/terms in a topic. They are still full-analysis models and cannot focus their modeling only on a user-specified aspect.

Although not directly related, there are other works of non-probabilistic models with sparse coding [35, 20, 7, 31]. However, they function quite differently in the goal or the methodology compared to probabilistic generative models. For example, they do not need to tackle the knotty issue of decoupling sparsity and smoothing [29] for sparsity realiza-tion in probabilistic models. Furthermore, they have been shown with poorer performances as the number of topics in-creases and are inferior compared to the probabilistic sparse topic models reported in [18]. Most importantly, like other models, they do not do targeted analysis like we do.
As discussed in the introduction section, our problem state-ment is that given a corpus C of a broad area, the proposed model can generate topics of the targeted aspect specified by the user using a set of keywords S .

Since the proposed model needs to discover fine-grained topics (also called topics of the targeted aspect), we treat each sentence as a document in topic modeling like several other fine-grained models [5, 26]. Following previous work in [26, 5, 34, 23, 17], we also assume that each sentence focuses on only one aspect. Since we regard each sentence as a document in modeling, each document focuses on only one aspect. Although it might not always be correct, it holds up well in practice [17, 34] and generates good results (shown in section 4). The graphical model of the proposed TTM is given in Figure 1. Since a document talks about one aspect, when the targeted aspect is specified, there can be two possible statuses for a document, that is, relevant or irrelevant to the targeted aspect. The status variable is denoted by r .
Here we present and illustrate the generative process: 1. Draw  X  ir  X  Dirichlet (  X  ir ) as a word distribution of a irrelevant topic to the targeted aspect; 2. For each target-relevant topic t  X  X  1 , 2 ,...,T } : 3. For each document m  X  X  1 , 2 ,...,M } :
For better understanding, we use three sample documents from the e-cigarette ( e-cig for short) domain for illustration: As we assumed above, a document talks about one as-pect. Then when the targeted aspect is given by a user, a document can be identified as relevant or irrelevant to the targeted aspect. r represents this relevance. r  X  { 0 , 1 } , where r =1 means the document is relevant to the target and r =0 is irrelevant. Another related variable is x , which represents whether a document contains at least one key-word s  X  S . x  X  { 0 , 1 } , where x =1 indicates the document contains the keyword(s) and x =0 indicates it does not. For example, when S is {  X  X hildren X  } and a document m says  X  X -cigarette is a gateway to smoking for children  X  (i.e., example d 1), the keyword indicator x m =1 because the document m contains the keyword  X  X hildren X . In this case ( x m is regarded as relevant ( r m =1) because it is unlikely that a short sentence contains the word  X  X hildren X  and is not talk-ing about children . However, this is a soft constraint that can be relaxed by adjusting a control factor  X  (presented in Equation 1) and 0 6  X  6 1, i.e.,  X  controls how much we believe a document contains a keyword is actually relevant. When it comes to the opposite situation that there is no key-word found in a documents m (i.e., x m =0), it is a different case because the document can be either relevant or irrele-vant. For instance, the above example d 2 is clearly relevant to target children while example d 3 is not and they both do not contain the keyword  X  X hildren X . We will discuss how to handle this case ( x m =0) in the following sub-sections.
After the relevance r of a document is drawn, we see how a word w i is generated. As discussed above, there are two types of relevance status for each document. When r =1, a topic t is chosen from  X  r . The total number of topics is | T | and these topics are the topics related to the target. After that a word is emitted from the selected topic by  X  r t . When r =0 the generative process is similar but TTM does not further generate multiple topics of the non-targeted aspects because they are not related to the user-interested aspect. Thus a word w i is emitted directly from  X  ir . In other words, the words in the irrelevant documents are drawn from only one (irrelevant) topic.
This sub-section presents how TTM achieves the targeted modeling by exploiting the idea of sparsity. Traditionally, sparsity in topic modeling indicates that a topic usually fo-cuses on a narrow range of words rather than a wide range of them in the vocabulary, or a document focuses on a very small number of topics [29, 18]. Similarly, in TTM we use a similar concept called aspect sparsity , which consists of two parts: document-aspect sparsity and targeted-aspect sparsity , which are detailed below.

Since a sentence is regarded as a document, TTM already assumes that each document focuses on only one aspect. The document-aspect sparsity is then naturally achieved. Additionally, the idea of targeted modeling with sparsity is based on two importance observations.

First, the targeted aspect may only be a small part or a minority among all aspects in a given corpus. For exam-ple, Children is only one aspect in the e-cigarette domain, which has a large number of other aspects such as Elderly , Vaporizer and Health . This observation gives raise to the targeted-aspect sparsity.

Second, since each document is coupled to one aspect (us-ing document-aspect sparsity), if we can better represent the targeted aspect, we should be able to better extract its rele-vant documents. If we can extract more relevant documents for the targeted aspect, we are likely to discover other more important words to better represent the target. As a re-sult, better and more topics are more likely to be generated because of better representative words and more relevant documents about the target are identified. Thus, we be-lieve jointly modeling of relevance status r and the targeted aspect can benefit the topic discovery.

As introduced in section 2, sparsity has been used to rep-resent the skewed data distribution in topic modeling. It provides a possible way to represent the property of minor-ity (the first observation above) and to model the targeted aspect with document relevance in a unified framework (the second observation above). We therefore follow this direc-tion and propose the idea of biased sparsity to help achieve the targeted modeling and the aspect sparsity.
Note that a target (aspect) is essentially represented by words. So the problem is how to automatically discover rep-resentative words of the targeted aspect in a joint modeling manner. As discussed above, a targeted aspect is sparse among all aspects. From it we can further posit that (in most cases) a targeted aspect is also sparse compared to the combination of all non-targeted aspects. When combin-ing this statement with the scenario that a targeted aspect is represented by words , we can conclude that the number of important words for distinguishing the targeted aspect from non-targeted aspects is in a narrow range. That is, the representative words for the targeted aspect are sparse. These words are denoted by V r =1 . However, the represen-tative words from the combination of all other non-targeted aspects are probably not sparse because that combination needs to contain almost all possible words for describing all other aspects. These words are denoted by V r =0 . Now we introduce the proposed biased sparsity , which is used for re-alizing the above idea.

The proposed biased sparsity approach is to make the word sparsity biased much more towards the targeted as-pect (i.e., | V r =1 | | V | ) and keep the non-targeted aspects almost non-sparse (i.e., | V r =0 |  X  | V | ). In other words, the word distribution of the targeted aspect (demoted by  X  only focuses on a small number of representative words. But the word distribution of non-targeted aspects (denoted by  X  ir , where ir means a unified irrelevant topic) contains al-most all possible words. With this setting, only those words that simultaneously satisfy both the following two conditions can be selected as representative words for the targeted as-pect: (a) they are semantically correlated with the known words of the target (achieved by the power of topic model-ing) and (b) they can distinguish the targeted aspect from non-targeted aspects (constrained by biased sparsity ). This sub-section demonstrates the targeted modeling in TTM, with the incorporation of biased sparsity. Here we follow the previous examples in section 3.1 to illustrate. Re-call that S = {  X  X hildren X  } and the document d 1 containing the keyword ( x d 1 =1) is known to be relevant ( r d 1 =1). Although the word  X  X hildren X  is not in document d 2, the word  X  X ate-way X  (also in d 1) serves as a bridge to connect the words in d 2 via topic modeling. If the  X  X ateway X  has been identi-fied as a discriminative word for target children , it makes d 2 more probable to be relevant ( r d 2 =1) as d 2 also contains the word  X  X ateway X . In this case, even though d 2 has no key-word  X  X hildren X  it still has a high probability to be relevant. In contrast, although  X  X moking X  appears in both documents d 1 and d 3, if  X  X moking X  is not identified as a discrimina-tive word for aspect children , d 3 will be mostly treated as irrelevant ( r d 3 =0).

Biased sparsity is the key to identify discriminative words for the targeted aspect, which is done by imposing the sparse constraint ( | V r =1 || V | and | V r =0 | X  X  V | ) discussed above. Specifically, when the target is children ,  X  X ateway X  will be a word included in  X  r and  X  X moking X  will not be. An expla-nation is that  X  X moking X  is also widely used by other non-targeted aspects like Elderly and Health , while  X  X ateway X  is used in Children more often. Thus,  X  X ateway X  is more discriminative for children but  X  X moking X  is too genreal.
Another crucial factor is the relevance status r . When more discriminative words (like  X  X ateway X ) are included in  X  , they in return increase the overall probability of the doc-uments (like d 2) that contain those words (like  X  X ateway X ) to be relevant ( r =1). When those documents are identified as relevant, new representative words (like  X  X ids X  in d 2) for the target aspect are also found and added to  X  r . Note that the growing of words in  X  r will not be an endless process be-cause of the sparsity constraint. After that these new words (like  X  X ids X ) in  X  r will help detect other relevant documents. This process shows the strength of joint modeling of the tar-get aspect (using biased sparsity) and the relevance status r in TTM. It also explains why TTM can finally shape bet-ter topics. Because it is able to exploit the information in other relevant documents in C even without knowing the ad-ditional keywords such as  X  X ids X ,  X  X oung X  and  X  X inors X , as they are automatically found and involved in the modeling process.

In a nutshell, the relevance variable r and the biased-sparsity related variable  X  r and  X  ir function together to ensure the property of aspect sparsity. Specifically, the en-coding of  X  r and  X  ir to achieve biased sparsity is based on the implementation of spike-and-slab prior.
The spike-and-slab prior is incorporated in the probabilis-tic topic model to realize a switcher-like  X  X n X  and  X  X ff X  selec-tor. This prior is first introduced in [21] and is recently reported as an effective way to reduce the model uncer-tainty [16]. It can also be used to decouple the sparsity and smoothing problem in Dirichlet distribution [3]. It has been used in topic modeling for word sparsity [29, 18]. In brief,  X  X pike X  controls the selection of a word, while  X  X lab X  smooths the word selected by  X  X pike X .

The spike-and-slab prior and other related components are together formulated as random variables in our model for the implementation of the biased sparsity. The related variables are  X  , p , q ,  X  r ,  X  , and  X  r . For every word v  X  V ,  X  { 0 , 1 } is the specific word selector. When  X  r v =1 a word is selected and  X  r v =0 a word is not selected. Note that in TTM, for a keyword s  X  S the  X  r s is set to 1. It is intuitive as those keywords are given by the user for specifying the targeted symmetric for all other words except the known keywords s  X  S , because the known keywords given by the user is very unlikely to be irrelevant to the target.

Unlike non-probabilistic models, one knotty issue for prob-abilistic models is the decoupling of sparsity and smooth-ing [29]. Motivated by [18], we incorporate a weak smooth-ing prior in addition to the regular smoothing prior  X  .
Double Sparsity : In addition to the target sparsity , the words for a specific topic related to the targeted aspect is also sparse. That is reasonable because TTM aim at discov-ering more fine-grained topics and the words for describing such a topic are naturally sparse. Likewise, the  X  r t,v t  X  T and v  X  V is also encoded with a slpike-and-slab prior in TTM. Together with the target sparsity, the sparsity of topical words enables the model to generate more coherent topics of the targeted aspect.
We use Gibbs Sampling [12] for model inference. The con-ditional distributions are shown in Equations 1, 2, 3, 4 and 5 (see Table 1 for the meanings of notations). The * symbol means the summation of all instances of the corresponding under relevance status r and topic t except the word in po-sition i .

First, we sample the relevance status r for every document m , where r  X  R and m  X  M .

P ( r m = c | x m = d,  X  ) ,  X  = { r  X  i m , w ,  X  ,  X  r ,  X  g ( c,  X  ) =  X   X   X   X   X   X   X   X   X   X   X   X   X   X   X 
Second, we sample the term selector  X  r v , where v  X  V . |  X   X  | is the sum of the values of all such term selectors.
P (  X  r v = b |  X  r (  X  v ) , r , w , X ,,p,q )  X   X   X   X   X   X   X   X   X   X 
Third, we sample a topic of the word in position i . We do it for all words in the corpus. |  X  r t,  X  | is the sum of the values of all such term selectors in topic t .

P ( z i = t | z  X  i , r , w , X ,  X  r ,  X  ir , X , )  X   X   X   X   X   X   X   X 
Last, we sample the term selector  X  t,v , where t  X  T and v  X  V .

P (  X  r t,v = s |  X  r (  X  v ) t , z , r , w , X ,,p,q )  X   X   X   X   X   X   X   X   X   X 
Data and targeted aspects : Five real-world data sets in different domains are used in our experiments, namely, E-Cigarette, Cigar, Camera, Cell-Phone and Computer. The first two data sets are tweets collected from Twitter in Octo-ber 2014. Specifically, E-Cigarette and Cigar are two types of tobacco-related products, which are the research areas of the last author from health science. The last three datasets are product reviews of three popular electronic products. We crawled the reviews from Amazon.com. More detailed information about the data sets is presented in Table 2.
Three targeted aspects are picked from each domain for targeted analysis. The aspects cover a wide range of diverse areas: Some of them are typical or frequent aspects in its domain like Children in E-Cigarette and Lens in Camera. Some are small or infrequent aspects like Weight , Warranty and Horse 1 . Note that one infrequent topic is specially cho-sen for each domain, listed as the last one in Table 2. Table 2: Five datasets, targeted aspects, and initial documents (tweets or review sentences)
Parameter Setting: For the hyper-parameter setting, we place: q = p = 1 for a uniform Beta;  X  = 1,  X  = 1,  X  ir =  X  = 0.001 and =1.0  X  10  X  7 . The models for comparison are also with the same setting.  X  in Equation 1 is set to 1 for TTM.
To evaluate our proposed targeted topic model TTM, we compare it with the following baseline models:
LDA : LDA is a well-known topic model [4]. It finds all topics in a corpus. To identify topics that are relevant to the targeted aspect, we manually inspect all resulting top-ics from LDA and find the subset of relevant topics. Note that the targeted aspect may be split into multiple topics by LDA. This is a labor-intensive and tedious process if there is a large number of topics from LDA.

LDA* : We still use LDA for topic generation, but instead of manual inspection to find related topics, we use keywords in S to search for relevant topics. We refer to this as the search strategy , which eliminates the tedious manual process. In this approach the number of topics T can be set large because only the retrieved topics will be analyzed. We search only the top 20 words of each topic.

DS-LDA : This is a state-of-art probabilistic sparse topic model [18] that models both the sparsity of topic mixtures (mining salient topics of a document) and topical words (mining representative words of a topic). We follow the im-plementation in [18] and refer to this as dual-sparse topic model, DS-LDA, in this paper. Like that for LDA, the rel-evant topics for DS-LDA are found via manual inspection.
DS-LDA* : Like LDA*, we use keywords S and adopt the search strategy to find possible relevant topics from all topics generated by DS-LDA.
 SS-LDA : We also use a single-sparse topic model named SS-LDA in this paper for comparison, to see whether the di-rect injection of the word sparsity can help our task, because it enables the topical word to be more focused. The realiza-tion is similar to DS-LDA but it only addresses the sparsity of topical words. Likewise, it requires manual inspection.
SS-LDA* : Like DS-LDA*, a search strategy is adopted to find possible relevant topics from all topics.

LDA-PD : This model runs LDA only on the documents in each dataset that contains one or more keywords from S . For instance, to find topics about targeted aspect Chil-dren , we use the keyword  X  X hildren X  to search for tweets in the E-Cigarette tweet corpus. After that we run LDA on the resulting tweets to find topics. We name this approach LDA-PD (where PD means Partial Data) for short.
A special topic: there was an well-known race horse called  X  X igar X  who died in October 2014. It is covered in the Cigar dataset as people talked about it in social media using its name. It is an evidently infrequent aspect in the Cigar data.
Since our goal is to discover topics for a given targeted aspect but the correct number of topics and the number of terms/words under each topic are unknown, a natural evaluation metric is to give the precision results at different rank position n , called Precision @ n (or P @ n for short).
For the evaluation of the first two datasets, two experts from our health science collaborator X  X  team who are special-ized in the tobacco-related products and social media were invited to judge the results. Two human labelers who are familiar with Amazon product reviews labeled the results from the other three datasets. Cohen X  X  Kappa scores of the two genres are 0.792 and 0.822 respectively.

Evaluation Measure : We use a normalized form of pre-cision (defined in Equation 6) that can evaluate both the cor-rectness of the topical words and the number of detected top-ics in a unified manner as both are important. Specifically, after the human judgment of all models results, the number of Maximum Unique Topics (MUT) can be obtained. For ex-ample, when the target is X , Model-1 finds one unique topic A (which is correct), Model-2 finds two unique topics A, B and Model-3 finds three unique topics A, B, C then MUT is 3. In another case, when Model-1 finds two unique topics A, B, Model-2 finds two unique topics B, C and Model-3 finds topics B, C then the MUT is also 3.

In Equation 6, P ( i ) @ n indicates the precision @ n for model number of correct words found in the topic st , given that there are ST topics found by model i . # C mt ( words @ n ) is the maximum number of correct words from all models.
This evaluation measure is fair and reasonable because a model may only find one correct topic with high topical word precision but miss some correct topics. Note that if there are more than one identical topics generated by a model, their average score is used. If there are multiple topics mixed in a single topic generated by a model, we use the best topic based on the number of relevant words in the top 20 words.
Two Comparison Settings : Two different experiment settings are used for comparison due to different properties of the candidate models. The reasons and differences will be elaborated in the following sub-sections.
Here we compare LDA, DS-LDA, SS-LDA and LDA-PD with TTM. In this setting the search strategy is not used. Instead, the annotators were asked to go through all top (20) words in the generated topics. For LDA-PD, we use the tar-get keyword itself (e.g.,  X  X hildren X ) to extract documents. For TTM, we also use the target keyword for targeted mod-eling. The topic number T in LDA-PD and TTM is set to 5 or 10 because we have no prior information about the num-ber of target-related topics but we know that it intuitively depends on the prevalence of the targeted aspect (e.g., in the Camera domain the targeted aspect Screen is likely to be frequent and have more topics than the infrequent aspect Weight ). In general, T is set to 5 for infrequent targets and to 10 for other more frequent targets. Although T could be 5 or 10 according to the targeted aspect, the same value of T is used for the comparison of results for both LDA-PD and TTM. Likewise, for LDA, DS-LDA and SS-LDA, T is set to 15 or 30 (and choose the one produces higher preci-sion results). The numbers are larger because they do not directly generate topics for the targeted aspect like LDA-PD and TTM. They also produce topics for other non-targeted aspects.

The precision results at the rank position of 5, 10 and 20 are reported in Table 3 and we observe the following: 1. TTM significantly outperforms other models. The av-2. LDA-PD is the second best because it rules out the ir-3. LDA is neither as good as TTM nor LDA-PD. But it
One might argue that we can keep increasing T to a larger number to make further improvement. That is possible, but it becomes rather messy and impractical (labor intensive and time consuming) for the user to manually inspect all gener-ated topics. An alternative is to apply the search strategy with increased T , which leads to our following evaluation of setting two.
This subsection compares LDA*, DS-LDA* and SS-LDA* against TTM. Different from setting one, here the annota-tors utilize the search strategy to identify relevant topics from all models except TTM. That is, the targeted aspect keyword is used to search in the top 20 topical words in each topic to find possibly relevant topics to the target. Only those resulting topics are evaluated. Since search is used, the number of topics T can be large. For LDA* we set T to 15, 30 and 50. For DS-LDA* and SS-LDA* we follow the same setting but their results are not good as LDA*. Due to the limited space we show the T =50 for DS-LDA* and SS-LDA* only.

The precision results at the rank position of 5, 10 and 20 are reported in Table 4 and we observe the following: 1. TTM again outperforms the other models by a large 2. The increase of T for LDA* helps improve the perfor-3. Neither SS-LDA nor SS-LDA* can produce better re-
This section presents the qualitative evaluation. We show several resulting topics in Tables 5 and 6 to give a flavor of each system. The domain and the targeted aspect are shown at top of the tables above the model names and topic names (given by us). Incorrect words of a topic are italicized and marked in red. Notice that the targeted aspect keyword itself is also shown but it is excluded from the computation of precision in the previous section because it is already known.
E-Cigarette (e-cig) is a key area studied by our collabo-rators from the health science and Children is one aspect that they are highly interested in and thus want to know its topics discussed on Twitter. Table 5 shows the topical words discovered by different models. The models are at-tached with the setting in the previous section so the topic number T is not explicitly given in the table. We explain the results below. (1) Topic fears : It means the fears or concerns about children using e-cig. By comparison, we see that TTM gen-erates a clearer and more reasonable topic. While the other methods generate incoherent topics with many wrong words, the words in TTM are more informative and interpretable, because the words like  X  X icotine X ,  X  X ateway X , and  X  X ale X  can better indicate the reasons of the fears as well as what people are actually concerned about for the children. That is, peo-ple worry about the  X  X icotine X  in e-cig that is bad for kids; they are afraid that e-cig becomes a  X  X ateway X  for their kids to smoking, and the increasing  X  X ale X  to the young. (2) Topic regulation : It is about the regulation of e-cig for children, mostly about the policy of purchase. Simi-lar to topic fears , TTM finds more informative words like  X  X aws X  and  X  X afe X . Particularly, the word  X  X oung X  is also included in the topic regulation of TTM, which is mean-ingful. It can infer that other documents that contain the unknown/unprovided keyword  X  X oung X  are probably iden-tified by TTM as relevant to the targeted aspect children (i.e., r =1) and thus involved in the topic generation of the target. Because one short sentence is unlikely to mention the keyword  X  X hildren X  and unknown keyword  X  X oung X  at the same time (they are semantically similar). The docu-ments containing the word  X  X oung X  is unlikely to also contain  X  X hildren X  and vice versa. This is an advantage that LDA-PD cannot achieve. Those documents (containing  X  X oung X ) identified by TTM as relevant help generate a more coherent topic for the targeted aspect.

Here we also analyze the topics formed by LDA and LDA* (T-50) to demonstrate some aforementioned problems. (a) Although the topic regulation in LDA looks good with many good words grouped, the topic is actually not very informa-tive as the words  X  X hildren, kids, minors X  all ranked high but they do not tell anything about a topic related to e-cig and children. (b) Since these words are grouped together, it may lead to missing of some good topics of the targeted aspect. (c) When comes to LDA*, although T is set to 50 this method actually finds only 2 topics that contain  X  X hil-dren X  by using the search strategy . (d) Also in the regulation topics, one may notice that the word  X  X inors X  which is in LDA (where its topic number is 15 or 30) is not included in LDA*(T-50) any more. However, since we did not previ-ously know all those keywords (e.g., X  X inors X ,  X  X oung X ), the topic is not found with T = 50, i.e., a related topic is unfor-tunately lost.
The previous sub-section presents the example from tweets, which mainly reveals the trend of a discussed topic in Twit-ter. Now let us take a look at online reviews in the Camera domain. The data is from Amazon.com and the analysis is also related to opinion mining and sentiment analysis.
Since a full/comprehensive comparison has been done in the E-Cig domain, this subsection shows two different as-pects (a popular aspect and an infrequent aspect) instead of repeating the full comparison with all models. Thus we only select some good models for comparison. Since LDA-PD achieves the second best score most of the time, it is included in our comparison. In addition, we pick one addi-tional model that can also find the same topic. We analyze the two aspects screen and weight in the camera domain. (1) Aspect Screen : When the target is screen, we first pick up the topic of picture for analysis. It discusses the features of the picture displayed on the Screen . From Ta-ble 6 we observe that TTM produces a good result. LDA also has a topic about picture but it has an issue (though it might look good at the first glance). The problem is that it groups the words  X  X icture X ,  X  X hoto X ,  X  X ic X  together as they are synonyms but the topic becomes vague because these words may belong to different fine-grained topics. In addi-tion, while LDA finds some more general features like  X  X on-derful X ,  X  X eautiful X  and  X  X un X  (these words can be regarded as general opinion words) because they actually can mod-ify many different aspects) but TTM finds more specific (or coherent) features like  X  X harp X  and  X  X lear X . This is an impor-tant property in opinion mining because people always want to know the specific reasons for opinions. Further illustra-tion about this property in opinion mining can be found in our previous work in [32].

The quality of commonly found topics like menu is also improved. Additionally, it is worth noting that TTM iden-tifies a new unique topic called imaging , which is not found by any other model, but is in fact related to aspect screen . (2) Infrequent Aspect Weight : Only LDA-PD and TTM find meaningful related topics for the aspect Weight. Here lens and battery are two detected topics for demon-stration. Because people often complain the heavy weight of these two components. However, they usually mention it in an implicit manner like  X  X he lens is so heavy X  or  X  X he heavy weight battery is annoying X . On the contrary, the topic gen-erated by DS-LDA*(T-50) is not so clear. Moreover, TTM solely detects other interesting topics like  X  X arrying X , which is also closely related to the aspect Weight.
In this paper, we studied the novel problem of targeted modeling. Instead of finding all topics from a corpus like existing models based on full modeling, the proposed model focuses on finding topics of a targeted aspect to help the user perform deeper or finer-grained analysis. This is moti-vated by real-life applications that researchers are often not interested in everything in a corpus but only some aspects of it in order to answer their research questions. Existing full models are not the most effective methods for such fo-cused analysis because their results are often too coarse and they may not find topics that the user is really interested in and/or miss many details. Experimental results showed that this is indeed the case and the proposed new model outperforms the state-of-the-art existing models markedly.
Shuai Wang and Sherry Emery X  X  research was supported by the National Science Foundation (NSF) under award num-ber NSF1524750 and the National Cancer Institute of the National Institutes of Health (NIH) and FDA Center for To-bacco Products (CTP) under Award Number P50CA179546. Bing Liu X  X  research was supported in part by a grant from National Science Foundation (NSF) under grant no. IIS-1407927, a NCI grant under grant no. R01CA192240, and a gift from Bosch. The content is solely the responsibility of the authors and does not necessarily represent the official views of the NSF, NIH, FDA, or Bosch. [1] D. Andrzejewski, X. Zhu, and M. Craven.
 [2] C. Archambeau, B. Lakshminarayanan, and [3] Y. Bengio, A. C. Courville, and J. S. Bergstra. [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [5] S. Brody and N. Elhadad. An unsupervised [6] J. Chang, S. Gerrish, C. Wang, J. L. Boyd-Graber, [7] X. Chen, Y. Qi, B. Bai, Q. Lin, and J. G. Carbonell. [8] X. Chen, M. Zhou, and L. Carin. The contextual [9] Z. Chen and B. Liu. Mining topics in documents: [10] J. Eisenstein, A. Ahmed, and E. P. Xing. Sparse [11] D. Griffiths and M. Tenenbaum. Hierarchical topic [12] T. L. Griffiths and M. Steyvers. Finding scientific [13] G. Heinrich. A generic approach to topic models. In [14] T. Hofmann. Probabilistic latent semantic indexing. In [15] L. Hong, D. Yin, J. Guo, and B. D. Davison. Tracking [16] H. Ishwaran and J. S. Rao. Spike and slab variable [17] Y. Jo and A. H. Oh. Aspect and sentiment unification [18] T. Lin, W. Tian, Q. Mei, and H. Cheng. The [19] Q. Mei and C. Zhai. Discovering evolutionary theme [20] K. Min, Z. Zhang, J. Wright, and Y. Ma.
 [21] T. J. Mitchell and J. J. Beauchamp. Bayesian variable [22] S. Moghaddam and M. Ester. The flda model for [23] A. Mukherjee and B. Liu. Aspect extraction through [24] D. Ramage, D. Hall, R. Nallapati, and C. D. Manning. [25] M. Rosen-Zvi, T. Griffiths, M. Steyvers, and [26] I. Titov and R. McDonald. Modeling online reviews [27] H. M. Wallach. Topic modeling: beyond bag-of-words. [28] H. M. Wallach, D. M. Mimno, and A. McCallum.
 [29] C. Wang and D. M. Blei. Decoupling sparsity and [30] H. Wang, Y. Lu, and C. Zhai. Latent aspect rating [31] Q. Wang, J. Xu, H. Li, and N. Craswell. Regularized [32] S. Wang, Z. Chen, and B. Liu. Mining aspect-specific [33] S. Williamson, C. Wang, K. A. Heller, and D. M. Blei. [34] W. X. Zhao, J. Jiang, H. Yan, and X. Li. Jointly [35] J. Zhu and E. P. Xing. Sparse topical coding. arXiv
