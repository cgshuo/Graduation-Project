 School of Library and Information Science The goal of this paper is to use innovative text and graph mining algorithms along with full-text citation analysis and topic modeling to enhance classical bibliometric analysis and publication ranking. By utilizing citation contexts extracted from a large number of full-text publications, each citation or publication is represented by a probability distribution over a set of predefined topics, where each topic is labeled by an author contributed keyword. We then used publication/citation topic distribution to generate a citation graph with vertex prior and edge transitioning probability distributions. The publication importance score for each given topic is ca lculated by PageRank with edge and vertex prior distributions. Ba sed on 104 topics (labeled with keywords) and their review papers , the cited publications of each review paper are assumed as  X  X m portant publicati ons X  for ranking evaluation. The result shows that full text citation and publication content prior topic distribution along with the PageRank algorithm can significantly enhance bibliometric analysis and scientific publication ranking performance for academic IR system. H.3.3 [[Information Storage and Re trieval]]: Information Search and Retrieval Algorithms, Experimentation, Measurement Bibliometrics, Publication Ranking, Citation Analysis, Topic Modeling, PageRank, Prior Knowledge Bibliometrics is a set of methods to quantitatively analyze the relatedness of scientific publications [3], i.e. scholarly networks, publication or venue importance, and co-authorship analysis. Citation analysis along with gr aph mining is a commonly used bibliometric method. In most previous works, while various methods were used to characterize the citation network, the basic assumption was easy and straightforward: either Publication1 cites Publication2 , or Author1 cites Author2 , regardless of sentiment, reason, topic, or motivation. More recent studies have shown, however, that this assumption is oversimplified. The reason or motivation for a citation matters. Taking the SDR model [9] as an example, it proposed the Structural Descriptive Referential model to capture the structural knowledge of citation, i.e., res earch question or methodology. However, most of these studies st ay on the conceptual level, for two reasons. First, most researchers are only able and willing to provide simple reference metadata due to the time and skill required to create more sophisti cated metadata. Creating refined referential metadata would be beyond most authors X  capacity. Second, fully automatic citation reasoning or classification requires a large amount of training data, which is unavailable for most research disciplines. The combination of citation bibliometrics and text mining provides a synergy unavailable with each approach taken independently [7]. In our research, instead of classifying citations, we used a supervised topic modeling algorithm, Labeled LDA (LLDA), to infer the publication and citation topic distribution, where each topic is a probability distribution of words and the label of the topic is an author contributed publi cation keyword. The publication and citation topic probability distributions, then, can be converted to the vertex (publication) prior and edge (citation) transitioning probability distributions to enhance citation network PageRank (with prior distributions) for publication ranking. More specifically, we assume that words surrounding a target citation can provide semantic evidence to infer the topical reason or motivation for the target citation, and that a citation network with prior (topic) k nowledge can enha nce classical paper contributes to the core topic(s) of the citing paper, this cited paper should get more credit from the citing paper (higher transitioning probability). Because each vertex or edge on the citation network is associated with a topic probability distribution, the enhanced PageRank can generate an authority vector, and each score in the vector tells the publication X  X  topical importance. In the remainder of this paper, we: 1) introduce our novel methods literature and methodology for bibliometric analysis, topic modeling, and network mining, 3) describe the experiment setting and evaluation results, and 4) disc uss the findings and limitations of the study and identify subsequent research steps. Most previous bibliometrics st udies share a common assumption: if  X  X  X  X  X   X  cites  X  X  X  X  X   X  , then  X  X  X  X  X   X  and  X  X  X  X  X  Most of the time, the reasons or motivations for this putative connection are ignored. Here, we ch aracterize citation relations in terms of two kinds of prior knowle dge: publication (citing or cited paper) topic probability distribution, and citation topic probability distribution. Within this framework, each publication makes different degrees of contribution fo r different scientific topics, and each citation is characterized by a topic probability distribution inferred by the citation X  X  su rrounding (context) words. There are three major contributions of this research. First, even with the same citation network topology, different publications can make different contributions to different scientific topics. In addition, topic authorities can be non-uniformly distributed to other cited publications in terms of the citation topic distributions X  inferred transitioning probabilitie s. Second, unlike classical, unsupervised topic modeling algor ithms, the topics in this research are associated with scientific keywords (supervised learning), which can help to interpret and evaluate the results. Last but not least, because we utilize full text citation analysis, one paper can have more than one cita tion edge with the other paper. For instance, if  X  X  X  X  X   X  cites  X  X  X  X  X   X  three times, there will be three distinct edges on the citation network between these two papers. Hence, the accumula ted transitioni ng probabilities between  X  X  X  X  X   X  and  X  X  X  X  X   X  can be higher than others, resulting in more accurate PageRank random walk modeling. Blei et al., [1] propos ed Latent Dirichlet Allocation (LDA) as a promising unsupervised topic m odeling algorithm. LDA employs a generative probabilistic model in the hierarchical Bayesian framework. As a conjugate prior for the multinomial topic distribution, the Dirichlet di stribution assumption has some advantages, which can simplify the problem. The probability density of a T-dimensional Di richlet distribution over the multinomial distribution  X  = (  X   X  ,  X   X  ...,  X   X   X  ,  X   X  ...,  X   X  are parameters of the Dirichlet distribution. These parameters can be simplified to a single value  X  which is dependent on the number of topics. However, one limitation of LDA is the challenge of interpreting and evaluating the statistical topics. For example, it is difficult to assign a label to each statistical topic automatically. In addition, arbitrary numbers of topic may not be appropriate fo r bibliometric studies because, while some topics may be very sparse, others may only focus on quite detailed knowledge of the same scientific topic. These limitations motivated us to utilize a supervised or semi-supervised topic modeling algorithm, one stemming from LDA, which employs existing topics from scientific metadata. Here, we assume that each (autho r-assigned) scientific keyword is a topic label and that each scientif ic publication is a mixture of its author-assigned topics (keywords). As a result, both topic labels and topic numbers (the total numbe r of keywords in the metadata repository) are given. The labeled LDA (LLDA) algorithm [11] was used in training the labele d topic model. Unlike the LDA method, LLDA is a supervised topic modeling algorithm that assumes the availability of topic labels (keywords) and the characterization of each topic by a multinomial distribution  X  over all vocabulary words. During the Bayesian generative topic modeling process, each word w in a publication is chosen from a (keywords). The word is picked in proportion to the publication X  X  preference for the associated label  X   X  X  X  X  X  X  X , X  X  X  X  preference for the word  X   X  X  X  X  generative process. For each topic (keyword)  X  X  X   X  multinomial distribution  X   X  X  X  X  the deterministic prior  X  . Finally, one selects a multinomial distribution  X   X  X  X  X  X  X  X  over the labels  X  paper from Dirichlet prior  X  . Paper (author provided) keywords can provide high quality topic labels for each scientific publication, however, this is not an ideal solution in that a large number of publications in the metadata repository have very few keywords, and often not enough to cover examining 200,000 publications from the ACM digital library, we found that 41.49% had no keywor d information (either keyword metadata was missing or authors didn X  X  provide any), and 6.13% had only 1 or 2 keywords, which is probably not enough to cover the whole paper. To cope with this problem, we used greedy match, where we assumed that author-assigned keywords were not enough to cover the semantics of the paper, to ex pand the paper topi c space. First, we loaded all possible keyword (t opic label) strings into memory, and we then searched each keyword from the paper title and abstract by using greedy matc hing. For example, if  X  X usic information retrieval X  existed in the title, we didn X  X  use the keyword  X  X nformation retrieval X . Ma tched keywords were used as  X  X seudo-keyword X  metadata for th e target publication. For the { X  X uthor keywords X  +  X  X seudo-ke ywords X  X  collection we used LLDA inference to assume topic pr obability scores. All topics not in this collection were ignored. For this approach, a subset of keywords (topics) from the training LLDA model was used to infer the paper topic distribution. The topic scores for  X  X  X   X  , i.e.,  X  X  X   X  X  X  X  future experiments, where  X   X  X  X   X  X  X  X  Each citation context in the citing paper is located for this research. One reference could be cited more than once in a paper, and the citation distributions coul d be different. The text window infer the citation topic distribution via LLDA. Intuitively, n should be a small number, as ne arby words should provide more accurate citation information. However, n should not be too small to minimize randomness. In this experiment, we used an arbitrary parameter setting, where n = 150. However, the ideal parameter setting should be further trained. That is a task for future work. We assumed that citations may not relate to all topics in the LLDA model. Instead, citations may only relate to topics provided by citing or cited topics. For any topic,  X   X  X  X  X  this research, as we didn X  X  want to totally remove these citations in the graph or make the citation transitioning probability = 0 in the citation network. As with publication topic inference, citation distributions for this method were normalized. Classical citation networks tend to ignore citation and publication content. In this study, we created a large citation directed network,  X  X , X  X   X   X  , with two kinds of prior knowledge: publication topic prior and citation topic transitioning probability distribution. Each vertex,  X  X  X  , on the citation graph represents a publication,  X  X  probability of vertex  X  for topic  X   X  X  X  X  Each edge,  X  X  X  , on the graph represents a citation connecting  X  and  X   X  (  X   X  cites  X   X  ). The topic transitioning vector for each edge is  X   X   X   X  for topic  X   X  X  X  X  For a given publication  X  , we used  X   X  X   X  X  X  X  and  X  represent a set of incoming and outgoing edges (citations) to node  X   X  X  X   X   X   X   X | X   X  X  X   X   X   X  | . Thus,  X   X   X   X  X  X  X   X   X  X   X  example, if a publication cites onl y 3 papers and, for a specific topic, the transitioning probabilities to these 3 papers are 0.1, 0.1, and 0.8, then most of the paper X  X  credit on this topic (topic authority) goes to the third paper. Based on these definitions, we can calculate each vertex X  X  (i.e., each publication X  X ) prior probability: and each edge X  X  (i.e., each cita tion X  X ) transitioning probability: where  X  X  X   X  X  X  X  from section 2.2, and  X  X  X   X  X  X  X  inference score from section 2.3. Unlike classical PageRank, a citati on graph with vertex and edge priors permits non-uniformly-distr ibuted random jumps. Based on section 2.1, topic distributions for each publication could be sparse for the greedy match assumption, and for a given topic,  X  could be zero. Thus, for each topic, the updated PageRank respect to a set of  X  X oot vertices X   X  X  X  , where for each  X  X  X  ,  X  important publications given a to pic (prior knowledge). A special considered, and root vertices  X  X  X  . We used the PageRank with prior algorithm [13] to calculate each vertex X  X  (topic relative) importance,  X   X  X  X  X   X  This equation represents a Markov chain for a random surfer who transitions  X  X ack X  to the root vertexes R with probability  X  each time-step. For each incoming link (citation) from  X  the PageRank score is updated with respect to edge (citation) transitioning probability  X   X  The output, for each vertex (publication),  X  , is an authority vector  X  X  vector indicates the publication topic importa nce with respect to both paper topic and full text citation priors. And we can get n ranking lists as a result. Unlike unsupervised topic modeling approaches, in this study we projected full-text scientific publications and citations onto labeled topic spaces, where each topic X  X  label is a scientific keyword. As a result, we are able to assess and interpret the topic publication authority vector and topic publication ranking by using keyword information. However, as this research focuses on the method of calculating publi cation topic importance, we can hardly compare the authority vector with other classical bibliometric indicators, such as h-index or impact factor, which are topic independent. For evaluation, we tried to find the  X  X round truth X  of the most important publications for a specific scientific keyword. In order their cited papers was collected. Collected review papers were screened so that they only focu sed on one topic (keyword). We assume that if a publication is cited by a review paper, and if this review paper concentrates on keyword  X  X  X   X  , then this publication may be different, we used the number of citations (by a review paper) to characterize the importa nce. Thus, if a review paper for keyword  X  X  X   X  cited  X  X  X  X  X   X  twice and  X  X  X  X  X   X  once, then,  X  X  X  X  X  X  X  X  X  X   X  X  X  X  1 . We also assume that if a paper is not cited by the target review We also assume that if a paper is cited 4 or more times by the review paper, then its importance is equal to 4. The goal of this evaluation is to compare the performance of our approach against the baseline algorithms: Citation PageRank : For this baseline we built the citation network without publication an d citation prior knowledge. We then calculated the PageRank authority score for each publication without considering keyword or topic information or citation context. TFIDF/BM25/Language Model : For these methods we used the keyword (topic label) as the query, e.g.,  X  X ultimedia information retrieval X , to search all the paper content (abstract and full text). Ranking lists based on TFIDF + vector space model, for a list of keywords, were used as the baseline. For a specific keyword, if a publication recei ved a high ranking score, this publication was assumed to be important for the target topic. Language Model + PageRank : For this baseline we combined the language model with PageRank (without topic priors), using random walk probability as the model prior. We used two indicators to measure ranking algorithm performance: Mean Average Precision (MAP), and normalized Discounted Cumulative Gain (nDC G) [6]. nDCG estimates the relevance gain a user receives by examining retrieval results up to score, 0 -4, as the relevance la bel to calculate nDCG scores. In this section, we survey exis ting studies focusing on two fields: PageRank analysis for citation network and bibliometrics for scientific publications. Drawing on classic bibliometric s papers, many scholars have focused their research on citati on frequency and citation impact and applied it in different domai ns. Harhoff, Narin, Scherer and Vopel [4] judged the value of patented inventions by citation frequency and concluded,  X  X he higher an invention X  X  economic value estimate was, the more the patent was subsequently cited X  (p. 511). Other authors have studied the association between the citation frequency of articles and various characteristics of journals, articles, and authors [8] and concluded that annual citation rates of ecological papers are affected by many factors, including the hypothesis tested, article length, and authors X  information. Traditionally, citation analysis treats all citations equally. However, in reality, not all ci tations are equal. Some scholars consider location to be a factor affecting the relative importance of a citation. Herlach [5] found that a publication cited in the introduction or literature review section and mentioned again in the methodology or discussion sections is likely to make a greater overall contribution to the citing pub lication than others that have been mentioned only once. The stylistic aspects of a citation also matter. Bonzi [2] distinguished between a number of broad categories of citations, i.e., those not specific ally mentioned in the text and those barely mentioned direct quotation. PageRank [10] has become a sign ificant method for evaluating the most important nodes in complex gr aphs analysis. From the point of citation analysis in bibiometri cs, PageRank is also an efficient way to evaluate a paper X  X  ranking score in a specific domain. White and Smyth [13] first proposed the priors idea in their formalization of a relative-rank extension to both PageRank and HITS. They experimentally evalua ted different properties of some algorithms on toy graphs and demonstrated how the approach could be used to study relative importance in real-world networks. Rodriguez and Bollen [12] describe d implementation of a particle-swarm that can simulate the performance of the popular PageRank algorithm in both its global-rank a nd relative-rank incarnations. We used 41,370 publications from 111 journals and 1,442 conference proceedings or workshops on computer science for the experiment (mainly from the ACM di gital library), where full text and citations were extracted from the PDF files. The selected papers were published between 1951 and 2011. From these we extracted 28,013 publications X  text (accounting for 67.7% of all the sampled publications), including titles, abstracts, and full text. For the other publications, we used the title and abstract from a metadata repository to represent the content of the paper. possible citations from paper X  X  full text. For instance, the rules could extract  X ... [number]... X  and  X ... [number, number..., number]...  X  as citations from the conten t of publication. In a total of 223,810 references, we successfully identified 94,051 references, which accounted fo r 42.0% of all references. Then, we sampled 10,000 publicati ons (with full text) to train the LLDA topic model. Author-provided keywords were used as topic labels. For instance, this paper has 6 author provided keywords. Thus, our LLDA training would have assumed that this paper is a multinomial distribution over these 6 topics. If a keyword appeared less than 10 times in the selected publications, we removed it from the training topic space. For publication content we first used tokenization to extract words length of the word &lt; 3, this word was removed. Snowball stemming was then employed to extract the root of the target word. We also removed the mo st frequent 100 stemmed words and words appearing less than 3 times in the training collection. Finally, we trained a LLDA model with 3,911 topics (keywords). These topics were used to infer the publication and citation topic distribution. By using the method proposed in section 2, we constructed a directed citation graph with each vertex as a publication, with its associated publication topic distribution, and each edge as a citation, with its citation topic distribution. For each topic we then calculated each publication X  X  root prior probabilities and each citation X  X  transitioning probabilities. We then used our approach to compare with other baseline methods, including PageRank, TF IDF, BM25, language model, and PageRank + Language Model. Th e results are presented in the following tables. The best performi ng algorithm is highlighted for each row, and these results are visualized in Figures 2 and 3. Clearly, for baseline ranking methods, PageRank + language model achieved the best performan ce, and PageRank alone (topic independent) performed the worst. For MAP@n, PageRank + language model was better than our method (PageRank with prior, highlighted) when n  X  50. But for n  X  100, PageRank with priors was better than all other methods. We also used significant testing to compare PageRank with prior and PageRank + language model (*t &lt; 0.01, **t &lt; 0.005, ***t &lt; 0.001). After n  X  1000, PageRank with priors is significantly better than all other baseline methods. nDCG@n is a more important indicato r in this research, for it tells the degree of (publication topic) importance. If nDCG score is large, the target algorithm can prioritize the most important on the ranking list. In Table 2 and Figure 3, it X  X  clear that PageRank with priors is always better than PageRank + language model and all other baseline methods. After n  X  10, the results are significant. Based on the MAP@n and nDCG@n evaluation, we find that PageRank with publication priors and citation transitioning probability distributions extracted from full-text data can produce reliable, high quality topic ranki ng results, which significantly outperform a list of baseline algor ithms. Meanwhile, as the topics extracted from publications are labeled with author provided interpretable, which is importan t for bibliometrics analysis. Another interesting finding of this research is that considering full text publication and citation transitioning probabilities for bibliometric analysis can help us to find the most significant publications for each topic. This new method may favor publications that make significant contributions but which have not yet received many citations. The limitations of this work are tw ofold. With respect to data, our test corpus came mostly from the ACM digital library, from which we cannot access full text data for all papers. In our experiment we only extracted 67.7 % of the papers X  full text, and most of those papers were published after 1995 (because old paper PDF files are scanned, we cannot extract text directly from them). As mentioned in section 4, when full text was unavailable, we used the title and abstract as a co mpromise, but this can be biased. This problem could be fixed by using image based text identified 42.0% of the references in the paper text. The main reason is, again, lack of full text data. But we also faced additional challenges having to do with di fferent citation styles, formatting errors, and encoding problems. These problems need to be addressed in future work. With respect to evaluation, beca use we proposed a topic based ranking method, some existing well-established bibliometric algorithms, like h-index and impact factor, cannot be used directly as the baseline. In future work we will tailor our method to facilitate comparison with other bibliometric methods. In addition, we will plug our method into other bibliometric methods for better scientific publication, author, and venue characterization, e.g., by introducing topical h-index or topical impact factors. [1] Blei, D.M., Ng, A.Y. and Jordan , M.I. 2003. Latent dirichlet [2] Bonzi, S. 1982. Characteristics of a literature as predictors of [3] De Bellis, N. 2009. Bibliometrics and citation analysis: from [4] Harhoff, D., Narin, F., Scherer, F.M. and Vopel, K. 1999. [5] Herlach, G. 1978. Can retrieval of information from citation [6] J X rvelin, K. and Kek X l X inen, J. 2002. Cumulated gain-based [7] Kostoff, R.N., del Rio, J.A., Hu menik, J.A., Garcia, E.O. and [8] Leimu, R. and Koricheva, J. 2005. What determines the [9] Liu, X., Qin, J. and Chen, M. , 2011. ScholarWiki system for [10] Page, L., Brin, S., Motwani, R. and Winograd, T. 1999. The [11] Ramage, D., Hall, D., Nallapat i, R. and Manning, C.D., [12] Rodriguez, M.A. and Bollen, J. 2006. Simulating network [13] White, S. and Smyth, P., 2003. Algorithms for estimating 
