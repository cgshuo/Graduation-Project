 In order to derive high quality information from text, the field of text mining has advanced swiftly from simple document cluster-ing to co-clustering documents and words. However, document co-clustering without any prior knowledge or background informa-tion is a challenging problem. In this paper, we propose a Semi-Supervised Non-negative Matrix Factorization (SS-NMF) based framework for document co-clustering. Our method computes a new word-document matrix by incorporating user provided con-straints through distance metric learning. Using an iterative algo-rithm, we perform tri-factorization of the new matrix to infer the document and word clusters. Through extensive experiments con-ducted on publicly available data sets, we demonstrate the superior performance of SS-NMF for document co-clustering.
 I.5.3 [ Pattern Recognition ]: Clustering-algorithms. Algorithms, Performance, Experimentation.
 Semi-supervised co-clustering, Non-negative matrix factorization
Document clustering is the task of automatically organizing text documents into meaningful clusters (groups) such that documents in the same cluster are similar, and are dissimilar from the ones in other clusters. It is one of the most important tasks in text mining and has received extensive attention in the data mining community.
With the rapid development of the Internet and computational technologies in the past decade, the field of text mining has ad-vanced swiftly from simple document clustering to more demand-ing tasks such as the production of granular taxonomies, sentiment analysis, and document summarization, in the hope of deriving higher quality information from text. These new applications in text mining typically involve interrelated types of objects (e.g., doc-uments and words). Consequently, co-clustering [1] was proposed in the literature.

However, current co-clustering methods are mostly developed based on the graph model, and thus inapplicable to large text data sets. Moreover, they are completely unsupervised. Accurately co-clustering documents without domain dependent background infor-mation is still a challenging task. In this paper, we propose a Semi-Supervised NMF (SS-NMF) based framework to incorporate prior knowledge into document co-clustering. Under the proposed SS-NMF co-clustering methodology, user is able to provide constraints on a few documents specifying whether they  X  X ust X  ( must-link )or  X  X annot X  ( cannot-link ) be clustered together. Our goal is to im-prove the quality of document co-clustering by learning a distance metric based on these constraints. Using an iterative algorithm, we perform tri-factorizations of the new word-document matrix, ob-tained with the learned distance metric, to infer the document clus-ters while deriving the text representation (word clusters).
We propose a semi-supervised NMF (SS-NMF) model for docu-ment co-clustering. NMF is initia lly proposed for  X  X arts-of-whole" decomposition [2], and later extended to a general framework for data clustering [3]. It can model widely varying data distributions and do both hard and soft clustering.
 Given a Heterogenous Relational Data (HRD) set, X 1 ,..., and X l , each representing one data type, our goal is to simultane-ously cluster X 1 into k 1 disjoint clusters, ..., and X l joint clusters. In semi-supervised document co-clustering, super-vision is typically provided as two sets of pairwise constraints de-rived from given labels on the documents: must-link constraints M = { ( x i , x j ) } and cannot-link constraints C = { ( x ( x , x j )  X  M implies that x i and x j are labeled as belonging to the same cluster, while ( x i , x j )  X  C implies that x i labeled as belonging to different clusters. Figure 1 shows the pair-wise data (e.g., documents and words), which is a basic element of the general HRD. The relation between words and documents is de-noted by a word-document matrix R (12) . The green edges indicate the must-link constraints M , while the red edges denote cannot-link constraints C . The dotted line shows the optimal co-clustering re-sult. Note that if we can successfully co-cluster such data, the cor-responding technique can be easily extended to structures involving more data types.

The objective of semi-supervised document co-clustering is to cluster the n 2 documents along with the n 1 words while constraint violations are kept to a minimum. In order to accomplish this goal, it is necessary to discover a new distance metric over the words from the user provided constraints on the documents. That is, we Figure 1: Word-document co-clustering with must-link and cannot-link constraints. solve the problem as, where . is the Frobenius matrix norm. This maximization prob-lem is equivalent to the generalized Semi-Supervised Linear Dis-criminate Analysis (SS-LDA) problem, can be solved accordingly [4].

Through learning, the new distance metric L (12) implicitly em-beds the must-link and cannot-link constraints. Thus, we can project the original data R (12) into a new space R perform non-negative tri-factorization of the new matrix R where G (1) and G (2) cluster indicator matrices, and S (12) cluster association matrix which gives the relation among the clus-ters of different data types. The minimization of Equation (2) can be done by updating one factor while fixing others [5]. In this section, we empirically demonstrate the performance of SS-NMF in co-clustering documents and words by comparing it with well-established co-clustering algorithms. All algorithms were implemented using MATLAB 7.0.

We primarily utilized the different text data used in the Univer-sity of Minnesota 1 and the Newsgroup data which contains about 2000 articles from 20 newsgroup 2 . In our experiments, we mixed up some of the data sets mentioned above. Table 1 gives the details of the data sets we used for word-document co-clustering.
We use term frequency to build word-document matrix and eval-uate the clustering results using the accuracy rate AC [5].
We conduct co-clustering experiments on the word-document ma-trix, and compare the performance of SS-NMF with BSGP [1] and unsupervised NMF (i.e., SS-NMF with 0% constraints).

In Figure 2, we plot the AC value against increasing percentage of pairwise constraints for SS-NMF. It is clear to see that BSGP and unsupervised NMF are outperformed by SS-NMF on all the data sets. Another important observation is that the accuracy of SS-NMF consistently increases with the gradual increase of the pairwise constraints (from 0 . 5% to 10% ). Moreover, in certain cases, SS-NMF is able to generate significantly better results by quickly learning from a few constraints ( 0 . 5% ), as demonstrated in the data set CT2, CT4, CT5 and CT7. So, the document clustering http://www.cs.umn.edu/  X  han/data/tmdata.tar.gz http://www.cs.uiuc.edu/homes/dengcai2/Data/TextData.html performance can be greatly improved even with very limited prior knowledge. It is also worth pointing out that the AC value of SS-NMFisashighas 99% on the data sets CT2, CT5 and CT7 with 10% constraints. In other words, SS-NMF provides near perfect clustering results on these data sets.
 Figure 2: Comparison of document clustering accuracy be-tween BSGP, unsupervised NMF, and SS-NMF with different amounts of constraints in word-document co-clustering. This research was partially funded by the 21st Century Jobs Fund Award, State of Michigan, under grant: 06-1-P1-0193, and by U.S. National Science Foundation, under grants: IIS-0713315 and CNS-0751045.
