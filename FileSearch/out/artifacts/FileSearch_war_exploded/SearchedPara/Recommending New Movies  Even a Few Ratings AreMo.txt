 The Netflix Prize (NP) competition gave much attention to collaborative filtering (CF) approaches. Matrix factor-ization (MF) based CF approaches assign low dimensional feature vectors to users and items. We link CF and content-based filtering (CBF) by finding a linear transformation that transforms user or item descriptions so that they are as close as possible to the feature vectors generated by MF for CF.
We propose methods for explicit feedback that are able to handle 140 000 features when feature vectors are very sparse. With movie metadata collected for the NP movies we show that the prediction performance of the methods is compara-ble to that of CF, and can be used to predict user preferences on new movies.

We also investigate the value of movie metadata compared to movie ratings in regards of predictive power. We compare our solely CBF approach with a simple baseline rating-based predictor. We show that even 10 ratings of a new movie are more valuable than its metadata for predicting user ratings. I.2.6 [ Artificial Intelligence ]: Learning X  parameter learn-ing Algorithms, Experimentation content-based filtering, collaborative filtering, matrix factor-ization, NSVD1, Netflix Prize, RMSE Both authors are also affiliated with Gravity Research &amp; Development Ltd., H-1092 Budapest, Kinizsi u. 11., Hun-gary, info@gravitrd.com D. Tikk was supported by the Alexander-von-Humboldt Foundation.

The goal of recommender systems is to give personalized recommendation on items to users. Typically the recom-mendation is based on the former and current activity of the users, and metadata about users and items, if available.
There are two basic strategies that can be applied when generating recommendations. Collaborative filtering (CF) methods are based only on the activity of users, while content-based filtering (CBF) methods use only metadata. In this paper we propose hybrid methods, which try to benefit from both information sources.

The two most important families of CF methods are ma-trix factorization (MF) and neighbor-based approaches. Usu-ally, the goal of MF is to find a low dimensional represen-tation for both users and movies, i.e. each user and movie is associated with a feature vector. Movie metadata (which are mostly textual) can also be represented as a vector, using the usual vector-space model of text mining.

Our approach to connect CF and CBF methods works via the analog representation form. We aim to find a linear transformation that transforms the vectorial representation of movie metadata into the feature vector of the movie. This transformation can serve as a bridge between CF and CBF. We also propose generalizations to this basic method.
Content-based methods are essential when there is no available rating information on items, which is the case when new items are added into product lists. We investigate on a movie recommendation example how efficient predictors are metadata-based methods compared to rating-based ones. We point out that even if very few ratings are available, simple rating-based predictors outperform purely metadata-based ones.

We performed our experiments on the Netflix Prize (NP) dataset. This is currently the largest available CF dataset provided for the scientific community by Netflix. The NP dataset consists of 100 480 507 ratings of 480 189 users on 17 770 movies. Ratings are integer numbers on a 1-to-5 scale. In addition to that, the date of the ratings and the title and release year of the movies are also provided. We collected movie metadata from the internet for content-based filtering.
We also report on the time efficiency of our algorithms and show that the running time of the proposed methods ranges between a few minutes and an hour.

This paper is organized as follows. Section 2 introduces notations. Section 3 describes in detail some methods that form the basis of our proposed methods. Section 4 intro-duces our basic approach for CBF, which is then generalized in Section 5. Section 6 surveys related works on content-based filtering, and finally Section 7 evaluates the proposed methods on the NP-dataset, and this also includes the com-parison of CBF and CF approaches. Throughout this paper we use the following notations: N : number of users
M : number of movies or items. u,v  X  X  1 ,...,N } : indices for users. i,j  X  X  1 ,...,M } : indices for movies or items. r ui : the rating of user u on item i .  X  r ui : the prediction of r ui . In general, superscript  X  X at X  R : the matrix of r ui values.

R : the set of ( u,i ) indices of R where ratings are provided; n u : number of ratings of user u , i.e. n u = |{ i : ( u,i )  X  X }| .
I : denotes the identity matrix of the appropriate size.  X  : regularization parameter.  X  : learning rate for gradient methods.

C : dimension of the vector space of the vectorial represen-
C 0 , C 00 : like C , but for users and movies (items), respec-l : index for the metadata features ( l  X  { 1 ,...,C } or l  X 
In this section we survey some factorization methods pro-posed originally for the Netflix Prize problem, which we will use in our further investigations. We assume that methods strive to minimize the prediction error in terms of RMSE on a validation set V , which is computed as:
Matrix factorization approaches have been applied suc-cessfully for both rating-based and implicit feedback-based CF problems [3, 4, 5, 6, 8, 9, 11]. The goal of MF methods is to approximate the matrix R  X  R N  X  M as a product of two lower rank matrices: R  X  PQ T , where P  X  R N  X  K is the user feature matrix, Q  X  R M  X  K is the item (or movie) feature matrix, K is the number of features that is a prede-fined constant, and the approximation is only performed at ( u,i )  X  X  positions. The r ui element of R is approximated by Here p u  X  R K  X  1 is the user feature vector, the u -th row of P , and q i  X  R K  X  1 is the movie feature vector, the i -th row of Q . The approximation aims to minimize the error of prediction, e ui = r ui  X   X  r ui while keeping the Euclidean norm of the user and movie feature vectors small: ( P  X  , Q  X  ) = arg min The predefined regularization parameter  X  trades off be-tween small training error and small model weights.
In the NP competition two different algorithms were found to be very effective to approximately solve the above opti-mization problem: one based on Alternating Least Squares (ALS), proposed by team BellKor [3] and the other based on incremental gradient descent (also know as stochastic gra-dient descent), namely the Biased Regularized Incremental Simultaneous MF (BRISMF) proposed by team Gravity [9].
BellKor X  X  alternating least squares approach alternates be-tween two steps: step 1 fixes P and recomputes Q , step 2 fixes Q and recomputes P . The recomputation of P is per-formed by solving a separate least squares problem for each user: for the u -th user it takes the feature vector ( q movies rated by the user as input variables, and the value of the ratings ( r ui ) as output variables, and finds the optimal p u by ridge regression.
 Borrowing the notations from [3], let the matrix Q [ u ]  X  R u  X  K denote the restriction of Q to the movies rated by user u , the vector r u  X  R n u  X  1 denote the ratings given by the u -th user to the corresponding movies, and let Then the ridge regression recomputes p u as
According to [3, 5], the number of recomputations needed ranges between 10 and a  X  X ew tens X .

Gravity X  X  BRISMF approach works as follows [9]: the dataset is first ordered by user id, and then by rating date. The update of the model is performed per-rating, not per-user or per-movie. Suppose that we are at the ( u,i )-th rating of R . Then compute e ui = r ui  X   X  r ui , the error of the predic-tion. The update formulae for p u and q i are the followings: One iteration over the database requires O ( |R| X  K ) steps, and the required number of iterations ranges between 1 and 14 [11]. It has been pointed out that larger K yields more accurate predictions [5, 8, 11].
 Paterek introduced another factorization approach, called NSVD1 [6], where a prediction is made by the following rule: Where: Here the user feature vector p u is a function of other vari-ables, as opposed to MF, where it is  X  X rbitrary X . The vari-ables b u and c i are the user and movie biases. The w j tors are the secondary movie feature vectors, i.e. NSVD1 has two sets of movie feature vectors. Paterek did not provide a training algorithm for NSVD1, we will apply the NSVD1 training algorithm of [9], which we will describe in the next section. In this section we propose a pure movie-metadata-based CBF framework.

Paterek X  X  NSVD1 algorithm can be interpreted in the fol-lowing way: first, users are represented by M dimensional binary vectors. Such a vector reflects which movies are rated by a user and which not, and is used to infer p u by a linear transformation, denoted by the matrix W . As opposed to a learning algorithm for MF that finds P and Q , a learning algorithm for NSVD1 finds Q and W . Users can also be represented by vectors derived not only from their ratings but also from their metadata (demographic data). From the viewpoint of a learning algorithm, there is no difference between a vector describing user metadata and a vector con-taining 1 at the movies rated by the user. In other words, we can consider the list of movie IDs rated by the user as a textual description about the user.

One can run the NSVD1 algorithm interchanging the role of users and movies. Then movies are represented by vectors indicating which users rated them. For clarity, we refer to the original version as user-NSVD1, and to its dual as movie-NSVD1.

Algorithm 1 is the training algorithm for movie-NSVD1, based on [9], using the following notations: With this notation: where  X  R is the approximation (prediction) of R . Line 5 of Algorithm 1 computes the movie feature vector. Lines 8 X 14 updates the movie feature vector (together with the user feature vector and the two biases), in the same way as in BRISMF. Line 16 is new in this algorithm compared to the NSVD1 algorithm in [9], it allows x il to be arbitrary; in the original version x il is set to 1 / 0 otherwise, thus, for any i , P l x 2 il = 1.

Lines 17 X 19 backpropagate the change in q i to W . After this step, q i is equal to W T x i , i.e. as defined in line 5. To prove it, we rewrite that 3 lines in the following form: and check what line 5 yields with the new W : Since a is W T x i by definition, they are equal.
 Algorithm 1 : Training algorithm for movie-NSVD1
Let us analyze the computational complexity of Algo-rithm 1: Let X = { ( i,l ) : x il 6 = 0 } denote the set of indices of non-zero elements of matrix X . With this notation, the time requirement of one epoch is: The optimal number of epochs depends on the experiment.
Note that the time complexity is very favorable: if |X| is proportional to the size of the movie descriptions measured in bytes (this holds usually when most of the information is textual, and we apply the vector-space model to represent texts as vectors), then this complexity is equal to reading the movie descriptions and the ratings K -times.

We also propose a different training algorithm for NSVD1, which is described in Algorithm 2. It contains two important modifications compared to Algorithm 1: Algorithm 2 does not aim to perfectly appoximate Q , un-less  X  2 = 1 and  X  2 = 0. To compensate this, the update process is repeated for n 2 times, to have a more accurate approximation of Q .

The main advantage of this method will be clear in the next section: One epoch of Algorithm 1 has time complexity O ( K  X |X| + K  X |R| ). Algorithm 2 updates the whole W for each i , unless  X  2 = 0, in which case the sparsity of x i can be exploited by updating only those rows of W , which corresponds to x il 6 = 0. However, we can apply a trick to speed up the process for the  X  2 6 = 0 case as well: we can decompose W into W = v  X  V . In the beginning, let V = W , and v = 1. We can replace line 15 of Algorithm 2 by the following two equivalent lines: The first line does the regularization, the second decreases the training error. With the decomposition of W into v  X  V , we can rewrite these lines as: With this trick, the sparsity of x i can be exploited in the update of V , thus the time complexity of Algorithm 2 with this modification is: Algorithm 2 : Batch training algorithm for movie-NSVD1
Algorithm 1 requires the training examples to be pre-ordered by i , otherwise lines 17 X 19 would have to be exe-cuted more than once per movie. Algorithm 2 can process examples in arbitrary order, as it applies batch-backpropa-gation to update W .
Recall that a prediction for the ( u,i )-th rating of factori-zation-based models is the following: The idea is to generalize Algorithm 2 in two ways:
In [9] a hybrid MF-NSVD1 approach was proposed, which contained a similar idea: part of the user features were arbi-trary (the MF-part), and the rest of the user features were computed by a linear transformation from a binary vector indicating which movies were rated by the user (the NSVD1-part).

Let the user features with indices k  X  { K 1 ,...,K 2 computed by linear transformation from metadata. Here K 1 and K 2 are the first and last indices of such features. Simi-larly, let the item features with indices k  X  X  K 3 ,...,K dependenent on metadata, where K 3 and K 4 are the first and last indices of such features. Let P [1 ..N ; K note the submatrix of P , referring to rows 1  X  u  X  N and columns K 1  X  k  X  K 2 . Similarly, let Q [1 ..M ; K 3 ..K note the submatrix of Q , referring to rows 1  X  i  X  M and columns K 3  X  k  X  K 4 .

The learning algorithm for our general variant is described the matrix of user and movie metadata, resp. The matrices W to recompute the submatrices of P and Q from metadata.
Algorithm 3 : Batch training algorithm for a gener-alized NSVD1
Although Algorithm 3 updates W 0 and W 00 at the end of the epochs (batch updating), there are also another possi-bilities, for example, we can update W 0 at the end of the epochs (as in Algorithm 2), and update W 00 after process-ing all ratings of an item (as in Algorithm 1) and before turning to the next item. We propose to use the following three learning methods for modifying b u , c i , p u and q decrease e ui : To update W 0 or W 00 , we can apply incremental or batch method, independently on user side and on movie side, how-ever, there is one exception: incremental updates on both side, with IGD, since user-incremental updating requires user-ordering of the ratings. Moreover, in case of ALS the incremental updates makes no sense, since the user or movie features are completely recomputed, not updated.

Advantages of AGD: if we have both user and movie meta-data, we can apply incremental backpropagation for both X 00 and X 0 , using user-ordered data in even epochs, movie-ordered data in odd epochs. Note that AGD allows paral-lelization (like ALS), since the order how we process users (or movies) does not change the results.

With ALS, we can get NSVD1-like training for implicit feedback datasets (currently only ALS has an efficient method to cope with the fully filled matrix, see [5]).

Using batch training and only movie metadata, one can consider the ALS approach in the following way: using R = P ( XW ) T , in even epochs, P is estimated, in odd epochs, first Q = XW is estimated, and then using Q = XW , only W is estimated.
There are many works on CBF and hybrid CF-CBF sys-tems. Our methods are built on new methods that emerged in the Netflix Prize competition, namely on MF with ALS or incremental gradient descent, and NSVD1. We give a briefly survey on some previous articles: Basilico et al. sug-gested the following formula for prediction (rewritten for better readability, [1]): where sim (1) uv and sim (2) ij are the predefined user and movie similarity measures that tell how similar are the metadata of two users or items; here  X  vj is to be optimized. One prediction requires O ( |R| ) time, which makes this approach unpractical.

Zhang et al. proposed an interesting approach for CBF [12] which is somewhat similar to ours: in their approach, Q is filled with movie metadata (i.e. Q = X 00 ), and using the R = PQ T formula, P is estimated by least squares solver, however, the covariance matrix is modified with a matrix  X  . Their method alternates between recomputing P and  X  . Both their method and ours aim to intensify (or suppress) in a collaborative way those patterns in the metadata that can be useful (or useless). For example: the term  X  X eason X  can have large impact on  X  r ui by either giving it high weight in the movie or in the user feature vectors. If it is a good feature (many users rely on that feature), it should get high weight on the movie side (allowing lower norm of p u in general). On the other hand, if it is a bad feature (few users rely on that feature), it should be given a low weight, allowing more important movie features to have higher weight. They concluded that the method was slow, thus they proposed a diagonal  X  that could handle the NP dataset in 4 hours. They reported results only on a small subset of the NP dataset, which makes that incomparable to ours.

Singh et al. proposed a general approach to solve multi-ple matrix factorization tasks simultaneously [7], called Col-lective Matrix Factorization (CMF). We briefly summarize their idea: suppose that we have two matrices (there may be missing values), one for the user-movie relation, describing how users rate movies ( R ), the other for the movie-metadata relation ( X 00 ). We can factorize both matrices separately, and get movie feature vectors from factorizing R , and get different movie feature vectors from factorizing X 00 idea is to share the movie feature vectors (no difference), and formulate a new optimization problem by linearly combin-ing the target function of the two factorizations (cf. eq. (1)), with coefficients  X  1 and  X  2 . The idea can be easily general-ized to factorize any number of matrices simultaneously.
For content-based filtering, we are interested only in the movie-metadata and user-metadata relations. One can think at Singh X  X  optimization problem in the following, more intu-itive way: we concatenate R  X  R N  X  M and X 00 T  X  R C 00 and have a new R 0  X  R ( N + C 00 )  X  M matrix of ratings and  X  X seudo-ratings X . This matrix is then to be factorized. Sup-pose that one column of X 00 is 1 iff the term  X  X eason X  occurs in the movie title, and 0 otherwise. In R 0 , we have a corre-sponding pseudo-user, who rated every movie having  X  X ea-son X  in the title 1, and all other movies 0. When a new movie gets into the recommender system, no real ratings are avail-able, but we have pseudo-ratings. As more ratings become available, the less the impact of pseudo-ratings is. Two is-sues have to be solved: handling  X  1 and  X  2 , and the effective factorization of R 0 . Since R 0 can contain many ratings (e.g. when C 00 = 60000 and M = 17770, it is 10 times larger than the Netflix Prize dataset), incremental gradient descent may be slow. Hu et al. proposed an efficient weighted ALS algo-rithm to handle implicit feedback datasets [5], i.e. when the matrix is fully filled (mostly with zeros) and an importance weight is assigned to each element. To factorize R that method, we set the weight of ( u,i ) /  X  X  examples to 0, the weight of ( u,i )  X  R to  X  1 , and the rest of the weights (which corresponds to the metadata part) to  X  2 .

Both user and movie metadata may be incorporated, by extending R 0 with C 0 new columns, corresponding to user metadata. The lower right C 00  X  C 0 block of the new R 00 set to zero.
We evaluated our algorithm against the Netflix Prize da-taset. Netflix provides a train-test split of the ratings. The test set contains 1 408 395 ratings and is called Probe. Note-worthy that Probe contains newer ratings for each user than the Train. In our experiments we kept only a fixed 1 / 10 sub-set of the Probe ratings (140 840 ratings), and put the rest of Probe into Train. This new test set is termed as  X  X robe10 X . Thus, we train using 100 480 507  X  140 840 ratings, and eval-uate on 140 840 ratings.

Netflix has also released another evaluation set, which is termed as Quiz. It contains cca. 2 817 131 / 2 ratings, but r -s are withheld. The goal of the competition is to predict those r ui values. We observed that evaluating on Probe10 yields similar results in terms of RMSE as evaluating on Quiz [9]. The difference is almost always less than 0 . 0002. Unless we explicitly mention, from now on the RMSE values refer to the Probe10 RMSE.

We collected movie metadata from www.netflix.com for all but 106 movies out of the 17 770. We considered all metadata as text, and used the vector space model com-mon in text-mining to represent texts as vectors. We dis-tinguished between word occurrences in different zones by zone-prefixing. That is the word  X  X omedy X  occurring in the genre and in the title zones are represented in two different dimensions  X  X enre:comedy X  and  X  X itle:comedy X  of the meta-data vector space. We used the following zones: title, syn-opsis, actor, director, release year, genre. We did not split the name of the actors and directors into first and last name.
Altogether, movies are described with C 00 = 146 810 di-mensional vectors. These vectors have 81 nonzero values on average. Movie vectors were normalized.

Before experimentation, we subtracted the global mean, 3 . 6043, from all ratings. The prediction of the algorithms were linearly transformed to have the same mean and stan-dard deviation as the Probe10 set.
 All experiments were performed on an Intel Core2 Quad Q9300 cpu on 3.3GHz, using only 1 core.
First, we experimented with some MF methods, using no metadata: The main contribution is that AGD01 converges, although its performance is not as good as of IGD01. Interestingly, it requires cca. twice as many epochs as IGD01, that is, almost the same number of user and movie feature updates.
A Perl script is available at our homepage, gravityrd.com , which selects the Probe10 from the original Netflix Probe set to ensure repeatability.
 We now evaluate the movie-NSVD1 proposed in Section 4. We experimented with various subsets of movie metadata, which are denoted by: We used the same learning parameters as in IGD01 or AGD01, but now all movie features are computed by a linear transformation from their descriptions. We experimented with 4 different variants: For B variants, we experimented with n 2 = 1 and n 2 = 10.
Table 1 summarizes the results of experiments. The op-timal number of epochs varied between 24 X 31 for AGD-I and AGD-B, and 11 X 15 for IGD-I and IGD-B. One epoch required 40 seconds for IGD-I and AGD-I on average. Batch variants gave better Probe10 RMSE, when n 2 = 10. When we set n 2 to 1, Probe10 RMSE got worse. IGD-B can be better than IGD-I due to another reason: it allows to use the (user,date)-ordered training set (no speed prob-lems), which was found to be beneficial [10] for incremental gradient descent. IGD-I  X  0.9143 0.9155 0.9161 0.9200 0.9285 IGD-B 1 0.9121 0.9130 0.9147 0.9219 0.9370 IGD-B 10 0.9089 0.9093 0.9101 0.9157 0.9322 AGD-I  X  0.9165 0.9175 0.9186 0.9224 0.9306 AGD-B 1 0.9148 0.9163 0.9184 0.9265 0.9401 AGD-B 10 0.9112 0.9117 0.9120 0.9170 0.9312 ALS-B 1 0.9429 0.9461 0.9509 0.9616 0.9706 ALS-B 10 0.9327 0.9350 0.9389 0.9608 0.9716 Table 1: Probe10 RMSE of movie-NSVD1 with dif-ferent metadata and learning methods
Clearly, more metadata imply better prediction perfor-mance. However, this is obvious: consider a simple exam-ple, where each movie has only one unique metadata, that is, C 00 = M , and X 00 is the identity matrix. This case falls back to the matrix factorization, since the movie features have no inter-dependency, As we add infrequent metadata (i.e. words that occur only in one or few movie descriptions), the movie features have more freedom (less inter-dependency between movies), as movie descriptions tend to be dissimilar, thus the results should get closer to the results of pure ma-trix factorization, i.e. to the RMSE of the IGD01, AGD01 and ALS01 methods (0 . 9079, 0 . 9096 and 0 . 9305).
CBF has a great advantage over CF: the ability to handle new movies. When a new movie is added to a recommen-dation system, it has no ratings, but metadata are usually available. In this case, CBF algorithms may help. We cre-ated the following setup to measure the capability of some CBF algorithms at handling new movies: first, we created 10 partitions for movies, and then applied 10-fold cross-validation: we trained a model using only the appropriate 9/10 of the original training data (i.e. all ratings except Probe10), and then evaluated the model on the appropriate 1/10 of the original test data (i.e. Probe10). Thus, one eval-uation means training on  X  0 . 9  X  (100 480 507  X  140 840) rat-ings, and evaluating on  X  0 . 1  X  140 840 ratings. Final Probe10 RMSE was calculated by adding the sum of squared errors of each of the 10 evaluations. We refer to this final Probe10 RMSE as X10 RMSE .

If the descriptions of movies are as valuable as their rat-ings, then X10 RMSE should be equal to the Probe10 RMSE of the previous experiments where movies has many ratings. On the other hand, if it is unhelpful in predicting user pref-erences, then X10 RMSE should be not much better than that of a user-bias-based method (where the prediction is based on the average rating of the active user).

First, we define some basic predictors: We did many experiments to optimize  X  1 ,  X  2 and  X  3 . For user-average predictor, X10 RMSE = 1 . 0616 (  X  1 = 2).
Interestingly, for the bias predictor,  X  1 = 2 and  X  5  X  10 5 yielded the best results: X10 RMSE = 1 . 0615. Larger values (e.g.  X  2 = 2  X  10 8 ) increased RMSE only by 1/3 of 0.0001, while smaller values (e.g.  X  2 = 100) increased it by 0 . 0038, even smaller  X  2 values were even worse. We interpret this as modeling movie biases does not decrease the X10 RMSE.

For the user-bias predictor with incremental gradient method,  X  = 0 . 05 and  X  = 0 . 005 gave X10 RMSE = 1 . 0504.
For the input-bias predictor, we tried all the T1, . . . , T20 datasets. T1 performed the best at  X  1 = 5 and  X  3 = 5000 and yielded X10 RMSE = 1 . 0305. Changing the weight-ing scheme of the metadata vector (e.g. to tf-idf) did not improve the results.

Next, we examined the prediction performance of the pro-posed movie-NSVD1 for CBF without movie biases using T4. We fine-tuned the learning parameters for NSVD1 with K = 30 using the parameter optimizer of [10], optimizing only on the first tenth of the 10 folds. We stopped the pa-rameter tuning process after 127 runs. The model with the best X10 RMSE (1 . 0080) was built in 10 epochs, requiring 366s in total; the 10-fold cross validation thus required about 1 hour. We submitted this predictor to Netflix in order to test the possible overlearning of the parameter optimization. As Quiz RMSE was 1 . 0078, we could exclude this side-effect.
Then we tried the T1, . . . , T20 datasets. T2 performed best, using tf-idf term weighting. Increasing K yielded bet-ter results, with K = 500, we obtained X10 RMSE = 0.9990. The running time was 2570 seconds.

Clearly, NSVD1 has better RMSE than the other meth-ods. However, the Probe10 RMSE (not the cross-validated) usually between 0 . 9700 and 0 . 9900. This means, that when we know the movie average, c i , it is a much more valuable piece of information than any metadata (the title, genre, etc. of movies). Next we investigate how many ratings a movie should have to obtain a reliable enough estimate of c i .
We compared the following two methods: NSVD1 with the cross-validation procedure mentioned above (1/10 of the movies are skipped), and the bias predictor (using both user-bias and movie-bias) with incremental gradient method, where the first N ratings of the skipped movies were included in the training set. We varied N , and checked when the RMSE of such a simple predictor will pass over the RMSE of the movie-NSVD1. Results are summarized in Table 2. When all ratings are used, Probe10 RMSE is 0.9721.

One can observe in Table 2 that even N = 10 ratings are more valuable than the metadata. Recall that this compar-ison was performed between the very simple bias predictor evaluated on not-so-new movies and the optimized movie-NSVD1 evaluated on new movies. Most probably, having a more sophisticated rating-based method would yield an even larger gap between the results of the two approaches. N 0 1 2 5 10 20 RMSE 1.0564 1.0447 1.0325 1.0131 0.9978 0.9880 N 30 40 50 100 200 500 RMSE 0.9841 0.9819 0.9802 0.9772 0.9754 0.9740 Table 2: X10 RMSE of bias predictors, when N rat-ings from each skipped movie are added to the train-ing set
We implemented the CMF method of [7]. We were un-able to get X10 RMSE results below 1 . 0800, however the algorithm was fast (400 seconds for training with K = 10).
Then we focused on the models. The factorization of the matrix of pseudo-ratings gives interesting results, com-pared to the W 00 of movie-NSVD1. Each metadata ele-ment is associated with a K -dimensional feature vector in both approaches ( w l in the movie-NSVD1 approach). Let W 000  X  R C 00  X  K denote the feature vectors of CMF for movie metadata.

In order to compare the two approaches, we investigated how effective are the models in finding similar actors. Given an actor, we can specify a ranked list of similar actors via the feature vectors of movie metadata using an appropriate similarity measure (we used S2 of [10]). When querying for Bruce Willis we obtained as top 3 similar actors: The top 10 list of NSVD1 contains also one director and two synopsis words, while CMF returns only actors. For Jennifer Lopez we obtained Although the results are similar, NSVD1 returned one male actor beside the two females. We examined a couple of other actors, and found that CMF performed consistently better, i.e., it yielded only actors for each queried actor and did not mix male and female actors.

We also examined the movie release year similarity. Each release year was represented with a different binary feature. Both methods tended to give another release years as most similar metadata, however, CMF did slightly better. We observed similar results for CMF, when W 000 was computed after a regular MF-run.

To sum up, the K -dimensional feature vectors of CMF yields a more intuitive similarity than that of the movie-NSVD1. We explain this remarkable difference as follows. Assuming that only X 00 or W 000 are to be optimized, only the following equations should be taken into consideration: When we substitute one equation into the other, we get W 000 T and W 00 are a kind of pseudoinverses of each other. Another interpretation: W 00 is optimized to get movie fea-ture vectors from movie descriptions, while W 000 T is opti-mized to get movie descriptions from movie feature vectors, i.e. the direction of the mapping is reverse.
We presented a simple approach for CBF, and particu-larly for CBF with movie-metadata. The method is a simple modification of Paterek X  X  NSVD1 [6], termed user-NSVD1, where users are represented with sparse binary vectors in-dicating which movies are rated by the user or not. In our proposed movie-NSVD1, the role of users and movies are in-terchanged, and movies are represented with the sparse vec-tor representation of their metadata. We showed how the presented approach can be extended to handle both movie and user metadata, or implicit feedback datasets.

We investigated how effective CBF methods are in pre-dicting ratings on new movies. We showed that even 10 rat-ings of a new movie are more valuable than the best solely metadata-based representation. We think that this is due to the large gap between the movie descriptions and the movies themselves: people rate movies, not their descriptions. We reckon that CBF methods can be applied more successfully to news recommendation, where the texts themselves are to be recommended and the amount of new items is higher.
We also investigated the capability of the algorithm to find similar movie actors. We concluded, that in this concern Collective Matrix Factorizations, proposed by Singh et al. [7] gives better results than movie-NSVD1.

In future work, we intend to investigate when ratings over-weight the rating + movie metadata combination in predic-tion. In addition to that we also intend to explore how user metadata can contribute to the prediction. We conjecture that a mere few ratings of a user is more valuable than the gender, age, zip-code, etc. attributes.

Note that the proposed approach for movie-CBF decom-poses R into R = PQ T = P ( W 00 T X 00 T ) = ( PW 00 T ) X We would like to investigate PW 00 T , since this matrix con-tains a C 00 -dimensional feature vector for each user, which can describe users preferences on actors, genres, etc. [1] J. Basilico and T. Hofmann. Unifying collaborative [2] R. M. Bell and Y. Koren. Improved [3] R. M. Bell and Y. Koren. Scalable collaborative [4] R. M. Bell, Y. Koren, and C. Volinsky. The BellKor [5] Y. Hu, Y. Koren, and C. Volinsky. Collaborative [6] A. Paterek. Improving regularized singular value [7] A. P. Singh and G. J. Gordon. Relational learning via [8] G. Tak  X acs, I. Pil  X aszy, B. N  X emeth, and D. Tikk. On the [9] G. Tak  X acs, I. Pil  X aszy, B. N  X emeth, and D. Tikk. A [10] G. Tak  X acs, I. Pil  X aszy, B. N  X emeth, and D. Tikk. [11] G. Tak  X acs, I. Pil  X aszy, B. N  X emeth, and D. Tikk. [12] Y. Zhang and J. Koren. Efficient Bayesian hierarchical
