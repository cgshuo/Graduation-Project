 We investigate using topic prediction data, as a summary of document content, to compute measures of search result quality. Unlike existing quality measures such as query clar-ity that require the entire content of the top-ranked results, class-based statistics can be computed efficiently online, be-cause class information is compact enough to precompute and store in the index. In an empirical study we compare the performance of class-based statistics to their language-model counterparts for predicting two measures: query dif-ficulty and expansion risk. Our findings suggest that using class predictions can offer comparable performance to full language models while reducing computation overhead. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval General Terms: Algorithms, Performance Keywords: Query difficulty, text classification
When the performance of an IR system on a query can be accurately predicted, an informed decision can be made as to whether the query should be expanded, reformulated, biased toward a particular intent or altered in some other way. Increasing evidence points to the fact that valuable clues to a query X  X  ambiguity and quality of corresponding results can be gleaned from query pre-retrieval features, and post-retrieval properties of the query X  X  result set [4]. For example, the query clarity score [3] measures the divergence of a language model over the top-ranked pages from the generic language model of the collection.

However, a significant drawback of methods that analyze the result set is that they must perform an initial retrieval. Additionally, most proposed methods must also process the full text of each document, which adds extra processing time to each query. The search engine must fetch the full text of each top-ranked document, and then perform additional computation that is proportional to the size of the docu-ments. Since performance prediction is only one part of the entire retrieval process, adding computational load at inter-mediary steps is undesirable, especially in applications like Web search where speed is critical. Thus, a compelling re-search question is what benefits of result-set analysis like query clarity can be retained with less computational cost.
We thus investigate the benefits of pre-computing a low-dimensional summary of a document, such as a vector of topic class predictions for a standard topic hierarchy, e.g. ODP [6]. We focus on two query performance measures: query difficulty , which measures retrieval risk in terms of the average precision (AP) of the top-ranked results; and expansion risk , which measures the absolute value of gain or loss in AP from using query expansion. Predicting the latter directly is an interesting problem since whether or not to do expansion may be the end goal. Furthermore, query difficulty and expansion risk are distinct problems and have been shown [1] to be only weakly correlated. Our analy-sis below suggests that query performance prediction using class-based analogues may offer results comparable to tradi-tional measures that use full document content.
Existing work on query performance prediction, summa-rized in Fig. 1, can be seen as calculating various distances between a global background model of the collection,  X  G , a query model using pre-retrieval features,  X  Q , a language model based on the results of the original query,  X  R 1 , and a language model based on the results of the expanded query,  X 
R 2 . We focus on the change in pre-and post-retrieval mod-els relative to the global background model, since this is where the majority of effects are observed. This comprises the arcs  X  QG , a type of query specificity;  X  QR 1 , a mea-sure of query drift; and  X  R 1 G , the specificity of results, the analogue of traditional clarity. A variety of other work has examined query classification and use of class labels. Re-cently [7] quantified query ambiguity using ODP metadata for individual query terms. Class entropy in result sets has been used to identify ambiguous queries [8] and for adver-tising decisions [2]. In contrast, our focus is on developing class-based analogues for query performance prediction.
We use the following two representations to model the lan-guage of the query and top-ranked documents.
 Unigram language models. This is a V -dimensional vec-tor representing the parameters of a multinomial distribu-tion over the K words in the vocabulary. Typically, no stop-ping or stemming of words is performed. Model similarity is computed using KL-divergence with Dirichlet smoothing. Topic prediction vector. A logistic regression classifier trained over a crawl from ODP is used to label every doc-ument with 1 to 3 classes depending on whether they sur-passed a threshold (optimized for F1 over validation data). Figure 1: Graphical depiction of model divergences. Table 1: Kendall- X  correlation of different model similarities with query difficulty (top) and expan-sion risk (bottom). Superscripts  X  and + denote significance of p &lt; 0 . 01 and p &lt; 0 . 10 respectively. For simplicity, we flattened the top two-levels of the hier-archy and only predict for the T = 219 classes that were most frequent in our training set. Model similarity be-tween representations u and v is computed using the metric  X ( u, v ) = 1 / 2  X  P T i =1 | u i  X  v i | . We computed  X  by aggregating the topic distribution for all documents in the collection and result set respectively. We computed  X  in two steps. First, we pre-computed topic distributions for each word in the corpus by aggregating the predicted classes of the documents in which the word occurs. Then, for a given query we combined the topic representations for its individual terms using a fuzzy-AND operator.
We used title queries over two TREC Web datasets: wt10g (1.7m pages, topics 451 X 550) and gov2 (25m pages, topics 701-850). Indexing and retrieval were performed using the sion baseline we used the default Relevance model expansion method in Indri 2.2 2 , with interpolation parameter  X  = 0 . 5, feedback with top 50 documents and top 20 expansion terms. We compared how the LM and topic (TP) representations affected query difficulty and expansion risk prediction, as measured using Kendall X  X  tau correlation with average pre-cision and the absolute magnitude of expansion gain or loss respectively. Results are summarized in Table 1. The LM Figure 2: Example showing how expansion-neutral ( &lt; 15% AP gain/loss) wt10g queries (dark squares) typically have high topic specificity (TP:  X  QG ) and low post-retrieval topic drift (TP:  X  QR 1 ). representation was slightly more effective at predicting query difficulty for both collections, while the TP representation was more effective at predicting expansion risk, especially when topic specificity of a query  X  QG was combined with topic query drift  X  QR 1 . Fig. 2 shows how these features isolate expansion-neutral queries for wt10g.
We explored the use of new sources of evidence in estimat-ing two important measures of query performance, query difficulty and expansion risk, by comparing two document representations  X  a low-dimensional pre-computed topic rep-resentation and a much larger unigram language model  X  over two standard Web collections. We found that while the LM representation can sometimes give slightly better performance for query difficulty and expansion risk, using pre-computed topic predictions is not far behind. In partic-ular, the topic-based representation is especially effective for pre-retrieval prediction (query classification). This suggests that topic information may often serve as an acceptable, and much more efficient, proxy for predicting query properties and analyzing search results.
