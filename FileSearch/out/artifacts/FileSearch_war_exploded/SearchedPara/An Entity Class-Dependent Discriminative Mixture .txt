 This paper studies Cumulative Citation Recommendation (CCR) for Knowledge Base Acceleration (KBA). The CCR task aims to detect potential citations of a set of target en-tities with priorities from a volume of temporally-ordered stream corpus. Previous approaches for CCR that build an individual relevance model for each entity fail to handle un-seen entities without annotation. A baseline solution is to build a global entity-unspecific model for all entities regard-less of the relationship information among entities, which cannot guarantee to achieve satisfactory result for each en-tity. In this paper, we propose a novel entity class-dependent discriminative mixture model by introducing a latent entity class layer to model the correlations between entities and latent entity classes. The model can better adjust to differ-ent types of entities and achieve better performance when dealing with a broad range of entities. An extensive set of ex-periments has been conducted on TREC-KBA-2013 dataset, and the experimental results demonstrate that the proposed model can achieve the state-of-the-art performance.
 H.3.3 [ Information Search and Retrieval ]: Retrieval Models
This work was partially done when the first author was visiting Purdue University and Microsoft Research Asia. Corresponding Author c  X  Cumulative Citation Recommendation; Knowledge Base Ac-celeration; Mixture Model; Information Filtering
In recent years, we have witnessed a proliferation of open domain Knowledge Bases (KBs) such as Freebase 1 and Yago 2 They have been used in many applications such as query answering, entity search and entity linking and have shown great promises. These KBs are usually organized around entities such as persons, organizations, locations, and so on. Currently, the maintenance of a KB mainly relies on human editors. However, with the explosion of information, large-scale KBs are hard to be kept up-to-date solely by human editors. Taking English Wikipedia for example, there are approximately 4.7 million entities but merely 132,938 active editors 3 . The less popular entities cannot be updated in time because they are not spotlighted. As reported in [14], the median time delay between a cited document X  X  publish-ing and its citation in Wikipedia is almost one year. An outdated KB severely limits the effectiveness of applications depending on it. This gap could be bridged if relevant doc-uments of KB entries can be automatically detected as soon as they emerge online and then be recommended to the ed-itors with various levels of relevance. This is called the Cu-mulative Citation Recommendation (CCR). Formally, given a KB entity, CCR is a task to filter highly relevant docu-ments from a chronological stream corpus and evaluate their citation-worthiness to the target entity.

Most previous approaches (e.g., [2, 25 ]) for CCR are highly supervised and require sufficient training data to build an in-dividual relevance model for each entity. These approaches are infeasible when dealing with a large-scale KB, since the https://www.freebase.com/ http://www.mpi-inf.mpg.de/departments/ databases-and-information-systems/research/ yago-naga/yago/ http://meta.wikimedia.org/wiki/List_of_ Wikipedias#1_000_000.2B_articles labeling work is labor intensive. One solution is to build a global entity-unspecific discriminative model and optimize it to achieve an overall optimal performance for all enti-ties [29 , 36]. However, these models ignore the distinctions between different entities and learn a set of fixed model pa-rameters for all entities, which leads unsatisfactory perfor-mance when dealing with a diverse entity set. For instance, it is not intuitve to apply the same discriminative model for Geoffery Hinton and Appleton Museum of Art . The for-mer entity is a computer scientist, while the latter one is a museum. Nevertheless, the global model treats them equally without considering the prior entity class knowledge. We as-sume that entities from a same class have similar tastes and preferences when citing relevant documents, which means they have similar combination weights in the discriminative model. Therefore, for an entity with little training data, the training data of its similar entities from the same class can be utilized to learn the combination weights. In comparison to the global model, more accurate combination weights are learned for each entity by this manner.

Based on this observation, we build an adaptive discrim-inative model for different types of entities by utilizing the underlying entity class information, i.e. entity class depen-dent discriminative mixture model. We introduce an in-termediate latent entity class layer and define a joint dis-tribution over the entity-document pairs and latent classes conditioned on the observations. The aim is to achieve rel-evance estimation through learning a mixture model which is expected to outperform the global model, while maintain-ing the capability to reveal the hidden correlations between entities and entity classes. The model can be viewed as a hierarchical combination of a discriminative component and a mixing component, so two types of features are required: entity-document features for the discriminative component and entity-class features for the mixing component.

For the discriminative component, we develop a set of bursty features as temporal features in addition to semantic features. The bursty features are detected from two indepen-dent data sources: the stream corpus (internal) and certain third-party data (external) like Google Trends.

For the mixing component, we explore two types of entity-class features to model the correlations between entities and hidden classes, including profile-based features and category-based features. Profile-based features are constructed from the entity X  X  profile in KB, while category-based features rely on the existing category labels for the entity in KB.
To the best of our knowledge, this is the first research work that focuses on modeling correlations between enti-ties and hidden entity classes in discriminative model for CCR. Our model is capable of tackling less popular enti-ties with little training data and unseen entities that do not exist in the training set, which is indispensable in a prac-tical CCR system. Empirical studies have been conducted on TREC-KBA-2013 dataset to show the effectiveness and robustness of the proposed mixture model. Experimental results demonstrate that our model achieves the state-of-the-art performance on TREC-KBA-2013 dataset.

The rest of this paper is organized as follows. Section 2 summarizes related works. Section 3 introduces an entity class-dependent discriminative mixture model for CCR. Sec-tion 4 describes features required in our model, especially the temporally bursty features and their detection methods. Section 5 presents the detailed experimental results and pro-vides some discussion. Section 6 concludes this paper and points out possible future work.
Although CCR was first proposed in TREC-KBA tracks, the similar research problem has been studied in several top-ics of information retrieval.
 Topic Detection and Tracking (TDT) is a track hosted by TREC from 1997 to 2004 [1]. A similar research topic in recent years is event detection. Both TDT and event de-tection are concerned with the development of techniques for finding and following events in broadcast news or social media. The techniques adopted for TDT and event detec-tion can be broadly classified into two categories: (1) clus-tering documents based on the semantic distance between them [ 34], or (2) grouping the frequent words together to represent events [ 22 ]. In [22 ], a finite automaton model is proposed to detect events in stream by modeling events as state transitions. This method has been validated widely by lots of other studies [ 18 , 17 , 35 ]. We also adopt this model to detect KB entities X  bursts in the stream corpus and then ex-tract bursty features for them. Different from above works, we model entities X  occurrences to capture bursty activities instead of words X  occurrences. Another difference between CCR and TDT is that CCR needs to make fine-grained citation-worthiness distinctions between relevant documents further.

TREC has launched the KBA-CCR track since 2012. Par-ticipants treat CCR as either a ranking problem [3, 2, 4] or a classification problem [3, 5, 29 ]. Classification and Learn-ing to Rank methods have been compared and evaluated [2, 15 ], and both of them can achieve the state-of-art per-formance with a powerful feature set. Several supervised learning techniques, such as SVM [21], language models [ 26 , 10 ], Markov Random Fields [7], and Random Forests [4 , 5, 29 ] are utilized. Meanwhile, a variety of relevance scoring methods have been tried, including standard Lucene scor-ing [ 6], and custom ranking functions based on entity co-occurrences [25 ]. A time-aware evaluation paradigm is de-veloped to study time-dependent characteristics of CCR [9].
However, some highly supervised methods require training instances for each entity to build a relevance model, limit-ing their scalabilities. Entity-unspecific methods, regardless of entity distinctions, are employed to address this problem [29, 28 ]. Nevertheless, characteristics of different entities are lost in the entity-unspecific methods. Some other re-searchers employ transfer learning techniques to learn across entities by using entity-unspecific meta-features [36 ], or uti-lize a semi-supervised approach to profile an entity by lever-aging its related entities and weighting them with the train-ing data [23 ]. These methods have demonstrated that the correlations between entities are useful for CCR. Neverthe-less, all these methods are empirically designed and the per-formance can be improved further.

What X  X  more, query expansion is often employed because the name of the target entity is too sparse to be a good query. Other name variants and contextual information of terms or related entities from Wikipedia or from the document stream [7, 6] are used to enrich the semantic features of entities. In addition to semantic features, temporal features have been proved especially helpful in CCR [2, 5, 29 ].

Mixture model has been proved effective to address the problem of data insufficiency in several information retrieval tasks, including expert search [11], federated search [19 ], col-laborative filtering [20 ] and image retrieval [30]. By intro-ducing latent layers to learn flexible combination weights for different feature vectors, mixture model can always outper-form simple discriminative models with fixed combination weights. Hence, we propose an entity class-dependent dis-criminative mixture model to deal with the entities with little training data, which will be described in next section.
This section proposes a novel learning framework by mod-eling each entity X  X  distribution across hidden entity classes and combining it with a logistic regression model to form a final discriminative model. First we provide a formal defini-tion of the research problem and model it as a classification task, and then present two discriminative models: a global model and an entity class-dependent mixture model.
We consider CCR as a binary classification problem that treats the relevant entity-document pairs as positive instances and irrelevant ones as negative instances. Many probabilis-tic classification techniques in the literature generally fall into two categories: generative models and discriminative models. Discriminative models have attractive theoretical properties [ 24 ] and generally perform better than their gen-erative counterparts in the field of information retrieval [16, 32 ]. Therefore, we adopt discriminative probabilistic models in this paper.

Given a set of KB entities E = { e u } ( u = 1 ,  X  X  X  ,M ) and a document collection D = { d v } ( v = 1 ,  X  X  X  ,N ), our objective is to estimate the relevance of a document d to a given entity e . In other words, we need estimate the conditional probabil-ity of relevance P ( r | e,d ) with respect to an entity-document pair ( e,d ). Each entity-document pair ( e,d ) is represented as a feature vector f ( e,d ) = ( f 1 ( e,d ) ,  X  X  X  ,f K ( e,d )), where K indicates the number of entity-document features. More-over, to model the hidden entity class information, each entity can be represented as an entity-class feature vector of entity-class features. The entity-document features and entity-class features will be introduced in Section 4 later.
This paper utilizes logistic regression, a traditional dis-criminative model, to estimate the conditional probability P ( r | e,d ), in which r ( r  X  { 1 ,  X  1 } ) is a binary label to indi-cate the relevance of the entity-document pair ( e,d ). The value of r is 1 if the document d is relevant to the entity e , otherwise r =  X  1. Formally, the parametric form of P ( r = 1 | e,d ) can be expressed as follows in terms of logistic functions over a linear combination of features, where  X  ( x ) = 1 / (1 + exp(  X  x )) is the standard logistic func-tion, and  X  i is the combination parameter for the i th entry of the feature vector. For the irrelevant class, we have P ( r =  X  1 | e,d )=1  X  P ( r = 1 | e,d )=  X  (  X  It is worth noting that for different values of r , the only difference in P ( r | e,d ) is the sign within the logistic func-tion. Therefore, we adopt the general representation of The conditional probability of relevance P ( r | e,d ) represents the extent to which the document d is relevant to the entity e . The entity-documents pairs are then classified as positive or negative according to the value of P ( r = 1 | e,q ). Since the learned weights are identical for all entity-document pairs and regardless of specific entities, this model is also denoted as global discriminative model (GDM) in this paper.

Several other approaches for CCR [5, 29 ] can be deemed as global discriminative models adopting different classification functions such as decision trees and Support Vector Machine (SVM).
In the GDM introduced in Subsection 3.2 , a fixed set of combination weights (i.e.,  X  ) are learned to optimize the overall performance for all entities. However, the best com-bination strategy for a given entity is not always the best for others. The entities stored in KBs are extremely diverse, including persons, organizations, locations, events, etc. Dif-ferent entities have personalized criteria to detect relevant documents.

We propose an entity class-dependent discriminative mix-ture model (ECDMM) by introducing an intermediate la-tent class layer to capture the entity class information in the learning framework. A latent variable z is utilized to indicate which entity class the combination weights  X  (  X  z 1 ,  X  X  X  , X  zK ) are drawn from. The choice of z depends on the target entity e in the entity-document pair ( e,d ). The joint probability of relevance r and the latent variable z is represented as where P ( z | e ;  X  ) is the mixing coefficient, representing the probability of choosing hidden entity class z given entity e , and  X  is the corresponding parameter. P ( e,d,z ;  X  ) denotes the mixture component which takes a logistic functions for r = 1 (or r =  X  1).  X  = {  X  zi } is the set of combination pa-rameters where  X  zi is the weight for the ith feature vector entry for the given training instance ( e,d ) under the hid-den class z . By marginalizing out the latent variable z , the corresponding mixture model can be written as where N z is the number of latent entity classes. If P ( z | e ;  X  ) follows the multinomial distribution, the model cannot eas-ily generalize the combination weights to unseen entities beyond the training set since each parameter in multino-mial distribution specifically corresponding to a training en-tity. To address this problem, we adopt a soft-max function the weight parameter associated with the jth entity feature in the latent entity class z and Z e is the normalization factor that scales the exponential function to be a proper proba-bility distribution. In this representation, each entity e is denoted by a bag of entity-class features ( g 1 ( e ) ,  X  X  X  ,g where L z is the number of entity features. By plugging the soft-max function into Eq. 4 , we can get P ( r | e,d ;  X , X  )= 1 Because  X  zj is associated with each entity feature instead of each entity, the above model allows the estimated  X  zj to be applied to less popular entities and even unseen entities.
Suppose entity-document pairs in training set are repre-sented as T = { ( e u ,d v ) } , and R = { r uv } denotes the cor-responding relevance judgment (i.e., +1 or  X  1) of ( e u ,d where u = 1 ,  X  X  X  ,M and v = 1 ,  X  X  X  ,N . Assume training instances in T are independently generated, the conditional likelihood of the training data is written as follows. P ( R|T )=
Y
The parameters (i.e.  X  and  X  ) in Eq. 6 can be estimated by maximizing the following data log-likelihood function, L (  X , X  )= where M is the number of the entities and N is the number of the documents in training set. g j ( e u ) denotes the jth feature for the uth entity and r uv denotes the relevance judgment for the pair ( e u ,d v ). A typical approach to maximize Eq. 7 is to use Expectation-Maximization (EM) algorithm [8].
The E-step can be derived as follows by computing the posterior probability of z given entity e u and document d
By optimize the auxiliary Q function, we can derive the following parameter update rules. arg max  X  arg max The M-step can be optimized by any gradient descent method. To optimize Eq. 9 and Eq. 10 , we employ the minFunc toolkit a collection of Matlab functions for solving optimization problems using Quasi-Newton strategy. When the value of L (  X , X  ) converges to a local optima, the estimated param-eters can be plugged back into the model to compute the probability of relevance for entity-document pairs. Since EM is only guaranteed to converge to local optima given different starting points, we try several starting points and choose the model that leads to the greatest log-likelihood.
The ECDMM can exploit the following two advantages over the GDM: (1) the combination weights are able to change across entities and hence lead to a gain of flexibil-ity. (2) it offers probabilistic semantics for the latent entity classes and thus each entity can be associated with multiple classes.

The number of hidden entity classes can be determined by some model selection criterion. We choose Akaike In-formation Criteria (AIC), which has been shown suitable in determining the number of latent classes in mixture models. As a measure of the goodness of fit of an estimated statistical model, AIC is defined as where m is the total number of parameters in the model. AIC offers a relative estimation of the information loss when a given model is used to represent the process that generates the data. Given a set of models, the preferred model is the one with the minimum AIC value.
In this section, we present the two types of features used in the discriminative models. Entity-document features f ( e,d ) are used in the discriminative components of GDM and ECDMM. In addition, ECDMM requires entity-class fea-tures g ( e ) to learn the mixing coefficients in the mixture component.
Entity-document features (i.e., f ( e,d )) are composed of semantic and temporal features.
We adopt the semantic features listed in Table 1 , which have been proved effective in CCR [28, 29]. Semantic fea-tures can model semantic characteristics of document-entity pairs.
Entities are evolving in the stream corpus as time goes by, yet semantic features are not capable of portraying the http://www.cs.ubc.ca/~schmidtm/Software/minFunc. html dynamic characteristics of entities. So we resort temporal features to make up this deficiency. Previous work [2 , 5, 29 ] considering temporal features can be summarized as a straightforward strategy that counts the daily (or hourly) occurrences of target entities in the stream corpus and cal-culates some statistical indicators as temporal features. To exploit the effectiveness of temporal features, novel bursty features are introduced in this paper. The underlying intu-ition is that the occurrences of entities in the stream do not distribute uniformly. If the amount of documents referring to an entity increases sharply in a short time period when something important is happening around the entity, this time period is detected as one bursty period of this entity. We make an assumption that documents occur in a bursty period of an entity are more likely to be related to it than those not.

The bursty periods of an entity can be detected either from stream corpus or from third-party data sources, de-noted as internal bursty periods and external bursty peri-ods respectively. Due to the heterogeneity of data sources, we use different burst detection methods to identify internal bursty periods and external bursty periods for entities.
Burst detection from a stream of documents have been thoroughly investigated in TDT and event detection [22, 17 , 31 ].

Since our goal is not to develop a new burst detection algo-rithm, we simply adopt Kleinberg X  X  2-state finite automaton model [ 22 ] to identify bursty periods of entities. There are two states q 0 and q 1 in the finite automaton A . For every target entity e , when A in state q 0 , it has low emission rate  X  0 = | R d ( e ) | T , where R d ( e ) is the number of all documents referring to e over the whole time range T . When A in state q , the rate is increased to  X  1 = s  X  | R d ( e ) | T , where  X  cause s is a scaling factor larger than 1 . 0 and s is empirically set as 2 . 0 in our work. The larger the number of documents referring to entity e at time t , the higher the likelihood of e being identified as a bursty entity at t .

After performing the burst detection algorithm, if the au-tomaton of entity e is in the state q 1 during a time period [ t start ,t end ], [ t start ,t end ] is a burtsy period of e with a bursty weight bw ( t start ,t end ) ( e ). The bursty weight is defined as the cost improvement incurred by assigning state q 1 over the bursty period instead of q 0 , and can be found in [22].
External resources, such as daily view statistics of enti-ties X  profile pages, are utilized as temporal features in pre-vious work [2 , 29 ]. Since some KBs do not provide page view statistics for entities as Wikipedia, we also include Google Trends 5 to detect external bursts. Akin to Wikipedia statistics, Google Trends can provide a numeric sequence v = ( v 1 ,  X  X  X  ,v T ) for each entity e , where v i normalized search volume of e in the ith day.

We detect external bursts of entity e from v with a tailored moving average (MA) method [27]. More concretely, for each v in v , 1. Calculate a moving average sequence of length w as 2. Calculate a cutoff c ( i ) based on previous MA sequences 3. Detect bursty day sequence d , where d = { i | MA w ( i )  X  4. Calculate the bursty weight sequence w = ( w 1 ,  X  X  X  ,w 5. Compact each segment of consecutive days in d into a The moving average length can be varied to detect long-term or short-term bursts. We set the moving average length as 7 days (i.e., w = 7). The cutoff value is empirically set as 2 times the standard deviation of the MA (i.e.,  X  = 2).
Given an entity-document pair ( e,d ), we define a bursty value b ( e,d ) to represent the temporal correlation between d and e . Let t be the timestamp of d . If t falls in one of e  X  X  bursty periods, say [ t start ,t end ], then b ( d,e ) is calculated as Eq. 12 shows. If t is not in any bursty period of e , b ( d,e ) is set as 0. the intuition that the documents appear at the beginning of a bursty period are more informative than those appear http://www.google.com/trends/ Figure 1: Two entities without common labeled cat-egories but with shared parent categories. at the end. Please note that b ( d,e ) can be calculated based on external bursts and internal bursts respectively. To avoid using future information during burst detection, we carefully perform burst detection algorithm (either internal or exter-nal) in a daily incremental manner. When dealing with an entity-document pair, the bursty periods are determined by the data before the timestamp of this document.
In ECDMM, besides entity-document features, entity-class features (i.e., g ( e ) in Eq. 5) are required to learn the mixing coefficients. Here we consider two types of prior knowledge to design entity-class features.
Each entity in KBs is uniquely identified by its profile page, which contains the basic information of this entity, such as name, address and experiences. We crawl the pro-file pages of all the entities as a profile collection. After removing stop words, we represent each entity as a feature vector with the bag-of-words model, where term weights are determined by the TF-IDF scheme.
Some KBs like Wikipedia organize entities with hierarchi-cal categories. For example, Geoffrey Hinton in Wikipedia, is labeled with categories such as Canadian computer sci-entists , Artificial intelligence researchers , and Fel-lows of AAAI . Besides these labeled categories, we take the parent categories of the labeled categories into considera-tion to deal with the circumstance in Figure 1 . The two alike entities can not be correlated if we only consider la-beled categories.

Similar to profile-based feature vector, we leverage a  X  X ag-of-categories X  model to represent each entity as a category-based feature vector. Given an entity without category in-formation, we manually assign a meta-category for it ac-cording to its profile. We supplement three meta-categories: person , facility and organization , which can cover all the entities in our dataset. The category-based feature vector of entity e is denoted as g c ( e ) = ( c 1 ( e ) ,  X  X  X  ,c N is the total number of categories. c i ( e ) equals to 1 if e is labeled with category c i , otherwise c i ( e ) is 0.
Therefore, given a target entity set E , we can generate two feature vectors for each e  X  X  : profile-based vector g p ( e ) and category-based vector g c ( e ) respectively.
In this section, we first introduce the dataset for exper-iments. After that, we report an extensive set of experi-mental results of our proposed models and baselines in two scenarios of CCR. At last, analysis and discussion are pre-sented based on the experimental results.
We conduct our experiments on TREC-KBA-2013 dataset 6 , a standard test bed provided by TREC. The data set is com-posed of a target entity set and a document collection called stream corpus.

The target entity set includes 121 Wikipedia entities and 20 Twitter entities, more specifically, 98 people, 19 organi-zations, and 24 facilities from 14 inter-related communities such as small towns like Danville, KY and academic com-munities like Turing award winners .

The temporally-ordered stream corpus, containing approx-imately 1 billion documents crawled from October 2011 to the end of February 2013. Each document is associated with a timestamp indicating its time of crawling. The corpus have been split with documents from October 2011 to February 2012 as training instances and the remainder for evaluation. We adopt the same training/test range setting in our exper-iments.

The relevance of entity-document pairs are labeled follow-ing a four-point scale relevance setting, including vital , use-ful , neural and garbage . The definitions are listed in Table 2 . Table 2: Four-point scale relevance estimation in TREC-KBA-2013.

Neutral informative but not citable, e.g., tertiary
Garbage no information about the target entity could
The details of the annotations for Wikipedia and Twitter entities are demonstrated in Table 3.
According to different granularity settings, we evaluate the proposed models in two classification scenarios respec-tively. http://trec-kba.org/kba-stream-corpus-2013.shtml respectively.

Only vital entity-document pairs are treated as positive instances, and the others are negative instances. This sce-nario is the essential task of CCR.

Both vital and useful entity-document pairs are treated as positive instances, and the others are negative ones.
Experiments in this section investigate the effectiveness of our proposed mixture model and baseline methods in the two scenarios. The following methods are compared:
For reference, we also include three top-ranked approaches in the TREC-KBA-2013 track as baselines.
For all mixture models, the number of hidden classes are determined according to AIC value. The optimal numbers of latent classes of all variants of ECDMM are reported in Ta-ble 4 . The number of optimal classes of category ECDMM is obviously larger than the optimal numbers of the other mixture models, which possibly caused by the hierarchical structures of categories in our category-based feature set. Although the incorporation of parent categories can build the correlation between two similar entities without com-mon labeled categories, it brings some noisy correlations in the meantime. For instance, a politician and a business man both living in Florida share a common parent cate-gory  X  Living people from Ocala, FLorida  X , this correla-tion will mislead the model and come to an non-optimal fit to the data.
 Table 4: Number of hidden classes determined by AIC for each mixture model.

This section presents the overall performance of all ex-perimental methods. We adopt F 1 (harmonic mean be-tween precision and recall), accuracy and AUC (Area Under Curve) [12] as the evaluation measurements. All the mea-surements are computed in an entity-insensitive manner. In other words, the measurements are computed based on the test pool of all entity-document pairs regardless of specific entities. The results are reported in Table 5 .
We notice that combine ECDMM achieves best on all measurements except recall. The official baseline achieve the best recall of all methods, which is not surprising since the official baseline is a manual method to detect as many relevant documents as possible by manually selecting reli-able aliases of an entity in advance.

Compared with GDM regardless of entity class informa-tion, all the mixture models employing entity-class features explicitly (i.e., profile ECDMM, category ECDMM and com-bine ECDMM) achieve better classification performance in both scenarios. Even Na  X   X ve ECDMM which does not em-ploy entity-class features explicitly can outperform GDM and other three baselines. This reveals that the mixture model is an effective strategy to enhance the straightforward discriminative model. Na  X   X ve ECDMM is not robust in two scenarios. Although it outperforms GDM in vital + useful scenario, it cannot beat GDM in vital only scenario. This is possibly caused by its implicitly employment of entity-class features. The entity-document features are noisy, be-cause the document-related counterpart contributes nothing to capture hidden entity classes.

Both profile ECDMM and category ECDMM outperform na  X   X ve ECDMM remarkably, revealing that profile-based fea-tures and category-based features are effective in modeling hidden entity classes. Category-based features are more promising than profile-based features, which is reasonable because the category labels in KBs contain prior human knowledge on entity class and taxonomy information. Even though combine ECDMM combines profile-based features and category-based features in a straightforward manner, it achieves the best performance. In comparison to GDM, combine ECDMM improve F 1 more than 10 percent and AUC approximately 10 percent. We believe that the per-formance can be enhanced further with more comprehensive entity class information and combination strategy. This section compares the methods in a fine-grained level. We need guarantee our mixture models not only achieve remarkable overall performance, but also perform well in entity-level. Hence, we recomputed the measurements in an entity-sensitive manner.

Based on the classification results for each entity e i ( i = 1 ,  X  X  X  ,M ) in the test set, precision and recall of each model are first calculated as P ( e i ) and R ( e i ). Then, we compute the macro-averaged precision and recall over all entities, de-respectively. At last, macro-averaged F 1 is computed ac-cording to macro P and macro R .

The macro-averaged measurements in two scenarios are reported in 2(a) and 2(b) respectively. The three baselines are labeled with red color, and the blue dots represent our proposed methods. The best method is labeled with pen-tagram in both figures. The parallel solid curves are con-tour lines of F 1 value, which means the dots in the same curve achieve same F 1 values. The dots lying in upper right achieve higher F 1 than the lower left ones. Obviously, com-bine ECDMM achieves the best F 1 in both scenarios. We also find our mixture models (i.e., blue dots) achieve higher precision in vital only scenario, demonstrating our models can detect vital documents more accurately than the base-lines.
This section evaluates the generalization ability of our pro-posed models to handle unseen entities in the training set. A robust model is able to handle unseen entities as well as training entities. As listed in Table 6 , there are 10 unseen entities in the TREC-KBA-2013 dataset. We evaluate the performance of our models on the unseen entity set com-posed of these 10 entities. We choose macro-averaged ac-curacy as the evaluation measurement. Due to the sparse positive instances for some unseen entities, it is improper to adopt precision, recall and F 1 for evaluation because they possibly become 0, in which case these measurements cannot reflect the performance suitably. The results are reported in Table 7 .
 Table 7: The averages of accuracies over 10 unseen entities.
 Official Baseline .175 .532 na  X   X ve ECDMM .587 .608 profile ECDMM .623 .647 category ECDMM .565 .431 combine ECDMM .580 .582
In both scenarios, the best classification results are achieved by profile ECDMM, which outperforms category ECDMM and combine ECDMM. A possible explanation for the un-satisfactory performance of category ECDMM is that the category information of unseen entities are not covered well in the training set, especially the Twitter entities. For these entities, there is too little category information to model their hidden classes accurately. dotted as pentagram.

In vital only scenario, all the variants of our mixture model can achieve better classification performance than GDM and the other baselines. The results validate the flexibility of our mixture model as expectation, which is essential for a practical CCR system. Our mixture model is not only good at handling existing entities in the training set, but also capable of dealing with unseen entities.
To further validate the effectiveness of the proposed bursty features, we evaluate them with the help of Information Gain (IG). Table 8 reports the IGs of the proposed features in two scenarios. All the IGs are computed following the method proposed in [ 33 ]. The higher of the IG achieved by a feature, the more powerful role it plays in the classification. The maximum , mean and median IGs of semantic features are also presented for reference. Since the bursty features are only used in the discriminative counterpart of ECDMM, we evaluate them with GDM.

In Table 8 , external bursty features perform best out of all features in both scenarios, conforming that external bursts of an entity are accompanied with occurrences of its relevant documents. However, internal bursts are not so helpful in vital + useful scenario as in vital only scenario. This is possibly caused by the incompleteness of the stream corpus. As we know, the stream corpus are crawled from the web, so it is possibly a biased snapshot of the true web. In addition, we only utilize the occurrences of a target entity itself in the stream corpus to detect its internal bursts currently. We can include more evidences to improve the accuracy of external bursty feature 0.130 0.286 internal burst detection. For instance, contextual related entities can be resorted to enhance the detection accuracy of internal burst.
The objective of Cumulative Citation Recommendation (CCR) is to detect citation-worthy documents for a set of KB entities from a chronological stream corpus. To address the problem of training data insufficiency for less popular entities, we propose an entity class-dependent discrimina-tive mixture model (ECDMM) by introducing a latent en-tity class layer to model the hidden entity class information. The model can be adjusted to different types of entities by learning flexible combination parameters according to un-derlying entity classes. Experimental results demonstrate that ECDMM can improve the performance of CCR. Entity-document features and entity-class features are developed for the discriminative and mixing components of ECDMM respectively. In terms of entity-class features, profile-based and category-based features are validated separately and in a combination strategy. The novel bursty features devel-oped as entity-document features are proved rewarding. Our ECDMM with proposed semantic and temporal features can achieve the state-of-the-art performance on TREC-KBA-2013 dataset.

For future work, we wish to explore more useful entity-class features and apply more proper combination strategies to improve the entity class-dependent mixture model. The authors would like to thank Jing Liu and Ning Zhang for their valuable suggestions and the anonymous review-ers for their helpful comments. This work is funded by the National Program on Key Basic Research Project (973 Program, Grant No. 2013CB329600), National Natural Sci-ence Foundation of China (NSFC, Grant Nos. 61472040 and 60873237), and Beijing Higher Education Young Elite Teacher Project (Grant No. YETP1198).
