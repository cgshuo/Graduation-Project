 One important problem in recent years in the field of information extraction has been how to classify, i.e., recognize, the found entities. A lot of research has been done in this area with very good results in different domains. However, in difficult domains such as biology there is still room for improvement. 
There are three main reasons why entity recognition (ER) is a real problem in biol-dictionary which contains a comprehensive set of biological entities. Second, in biology the same word can mean different things depending upon the context. For example, ferritin can refer to a biological substance and a laboratory test. Third, many biological entities have synonyms (e.g., PTEN and MMAC1 refer to the same gene). [2] 
The methods that tackle these problems are generally based on three different ap-proaches: lexicons, rules and statistics. Lexicon-based methods use large dictionary which contains as comprehensive set of relevant names as possible. Hanisch et al. [3] describe a lexicon-based method which uses a large dictionary of gene and protein names and semantically classified words that tend to appear in context with these names. Rule-based methods usually use part-of-speech (POS) tagger, which is con-sidered as the most basic form of linguistic corpus annotation. The POS information can be used for rule conditions along with morphological clues and indicator words. Tanabe and Wilbur [4] describe their rule-based method called AbGene which is considered as one of the most successful ru le-based system [2]. Chang et al. [1] pre-sent a statistical approach they used when creating the GAPSCORE system. They used syntax, appearance, morphology and context in their method by quantifying them for each gene and non-gene as a numerical vector. They used the vectors to train a classifier which was used to identify new words by scoring them based on the simi-larities to the previously observed training set. Cohen and Hersh have written a survey [2] of current work in biomedical text mining in which they give more extensive overview of these ER methods. 
The method presented in this paper differs considerably from other ER methods as it does not use natural language processing techniques or other commonly used ER techniques. Instead, our method uses both existing knowledge and the context of the entity. 
The method has some characteristics of the existing methods: it reminds of the lexicon-based method described by Hanisch, et al. [3] as we use the knowledge base partly as a lexicon. Also, the use of the context has been studied in ER in some extent. using the dictionary for finding matching names we use also the knowledge how the entities are related to each other in the knowledge base. In this process the context of the entity plays an integral part. 
This approach has benefits compared to other ER solutions. First, it does not use complex natural language processing techniques making it simpler. Second, this same name, our approach can identify the entities more precisely. 
We have done preliminary tests in the domain of type 1 diabetes. As the knowl-early results are encouraging as our method can recognize the entities most of the time if they can be found from the knowledge base. In order to fully understand our approach the Ontology Aided Text mining system OAT is described in next Section. Section 3 gives a full description of our method for entity recognition, which is then evaluated in Section 4. Finally, in Section 5 we give a conclusion. We developed Ontology Aided Text mining system (OAT) to assist biologists to automatically collect knowledge about biological entities relevant to their studies. The goal was to create a knowledge base which describes these biological entities and their relationships to one another. By presenting the relationships between the entities the knowledge base can be used for finding interesting paths from one entity to an-other making the knowledge usable for example for drug discovery. Figure 1 shows an example of the knowledge that can be found from the OAT knowledge base. 
The knowledge is extracted from scientific articles using OAT X  X  information ex-traction module. The module extracts subject X  X redicate X  X bject X  X riplets that hold rele-vant information about the domain. 
After information extraction, the triplets are manually checked and verified as be-ing correct or false. This phase is needed because some of the knowledge IE process produces may be noisy and incorrect. Some of the knowledge could be automatically reasoned to be correct or incorrect by using existing knowledge but it is impossible to do this to all the new entities OAT collects. That is why we need to get input from an expert who in this case is a biologist. cause it affects the later usability of the knowledge base. 
Even though experts can recognize the entities quite well they misclassify them surprisingly often. This problem is present especially with genes that have several names and when a name or a symbol can refer to several genes [3]. In order to make the job easier for biologists we developed a method that narrows down the list of possibilities and suggests the most probable concept from the ontology. This method given in [5]. The main focus of OAT is to extract biological knowledge from different data sources and store them to a knowledge base called OAT knowledge base (OATkb). OATkb consists of instances of concepts that are defined in an ontology called OAT ontology (OATo). OATo consists of four components: biological concepts, taxonomy which organizes the concepts into a parent-child hierarchy, relationships which define how the instances of the concepts can relate to one another, and axioms which are used for reasoning. When new entities are added to the knowledge base they must be identi-fied, i.e., mapped to a concept described in the ontology. 
The method we have developed for identification of entities gives scores to possi-ble classifications (concepts) for the entity. As a result, the method produces a set of concepts described in the ontology, which all have scores to portray the belief that the entity is of that concept. 
The intuition behind our method is that the unclassified entity is likely to be similar deduce the concept of the unclassified ent ity. For example, if we have a triplet TNFAlpha X  X ecrease X  X PARGamma and from the knowledge base we can find that all the neighbors of TNFAlpha with decrease relationship are Protein s, it is highly likely that the unclassified entity PPARGamma is also a Protein . 
The process of entity recognition can be divided into three tasks which all produce results that are used later in the ER process. First step is to check the lexicon for the from previous processes. The query produces a set of concepts that have been previ-problem of homonyms is relevant in biology, the set may hold several different con-cepts. Now, we can calculate the distributio n of the concepts. For example, if the entity e has been assigned to a concept X 65 times, concept Y 20 times and concept Z 15 times, the distribution is ( e , X ) = 0.65, ( e , Y ) = 0.2, ( e , Z ) = 0.15. 
As the type of the relationship and its direction are relevant, we must take them into consideration when calculating the score for possible concepts for unclassified entities. For instance, if we have several facts that state entity A inhibits entities of Protein , and activates entities of Vitamin , and the triplet is A activates B , the score of B for Vitamin should be higher than score for Protein . Also, the direction of the relationship is rele-tion. In other words, the triplets are not symmetric, i.e., it does not apply that if entities have a relationship a  X  b there would automatically be a relationship a  X  b . 
The second step is to check the neighborhood of the context entity. For instance, if we have the example triplet TNFAlpha X  X  ecrease X  X PARGamma and we want to clas-sify PPARGamma, we will check the neighborhood of the context entity TNFAlpha. When checking the neighborhood the method takes into consideration only the enti-produces a set of concepts which is called a neighborhood set . The final step is to combine the results of the lexicon set and the neighborhood set. For this, we use the following equation: given concept, lc is the entity count from the lexicon set for the given concept, and te is the total entity count for all the concepts in both of the sets. The total entity count multiply nc and tnc because we want to give more emphasis to the context. This vari-able can be changed depending on how much emphasis is given. In our tests we are using 2 =  X  . w is used to give more emphases for the cases where the entity is found both from the lexicon and the neighborhood sets. (PPARGamma, Protein ), and from the knowledge base we get nc = 15 (context en-tity X  X  neighborhood has 15 entities of Protein ), lc = 30 (PPARGamma has been classi-fied as Protein 30 times), tnc = 50 (there are 50 entities in the neighborhood set), and tlc = 50 (the entity PPARGamma has been classified 50 times). We get the following result: score ( Protein ) 45 . 0
It should be noted that the equation takes the volume of knowledge into considera-neighborhood set, the score is based mainly on the distribution of concepts in the lexicon. We have done a set of preliminary tests which are for the most part theoretical. Table 1 shows a few test cases which we have used to assess the scores for entity concept pair ( e , C ) in different theoretical situations. nc 20 80 80 0 50 25 75 65 0 lc 80 20 0 80 50 100 0 50 50 Score 0,4267 0,6267 0,53 0,267 0,542 0,542 0,5 0,654 0,167 
In cases 1 and 2 we demonstrate how our method emphasizes the context: in both cases there are the same total amount of entities in the sets but the score differs quite a lot. In case 1, we can see that in most cases the entity has been classified as C (80% of the times) but it has appeared with the context entity in the given context only 20% of previously classified entities located in the context entity X  X  neighborhood. 
The same can be seen from cases 3 and 4. They show how the score changes when context more important factor than the lexicon. However, this approach brings also problems. Some may argue that the case 4 should get higher score than the case 3. In other words, the lexicon should be valued higher than the context since there are not been assigned to that concept most of the time. 
In the cases 5 and 6 there is quite a lot of evidence from both neighborhood and the lexicon which makes the concept a good candidate. However, in the case 7, there is no evidence from the lexicon, i.e., the entity has never been classified to that concept but the entities in the context X  X  neighborhood are mainly of that concept. In this case, introduced the variable w . 
Finally, the cases 8 and 9 show an example situation of two entities sharing the same name. As there are 100 instance of the entity in the lexicon and it has been as-the concept in the neighborhood set which makes that concept much more probable over the concept in case 9. We have described a novel method for entity recognition which utilized existing knowledge about the relationships and the context of the entities. This method does not use complex natural language processing techniques which are difficult to imple-ment and not always that reliable. In contrast, this method is quite simple and easy to implement. Also, the method tackles the problem of complex naming conventions in biology, especially the problem of several entities sharing the same name. As we take the context and the existing knowledge about relationships into consideration, we can classify the entities more precisely In the future, we are planning to introduce improvements to our entity recognition. In the early stages of operational knowledge base there might be situations when the entities cannot be found from the knowledge base and therefore cannot be recognized. But as the knowledge base grows this situation becomes less and less common. Also, as we use the context, it is more likely that at least one of the entities in the triplet can be found from the knowledge base making the identification easier. 
The method itself could also be improved. We could introduce a taxonomy for predi-also. In this case, we could lower the score as the similarity of predicates decreases. But the first thing is to do complete tests to verify our theory in practice. 
