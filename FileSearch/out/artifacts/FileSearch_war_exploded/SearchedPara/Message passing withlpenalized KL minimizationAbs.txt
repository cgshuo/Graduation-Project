 Yuan Qi ALANQI@PURDUE.EDU Yandong Guo GUOY@PURDUE.EDU Bayesian learning provides a principled framework for modeling complex systems and making predictions. A critical component of Bayesian learning is the compu-tation of posterior distributions that represent estima-tion uncertainty. However, the exact computation is often so expensive that it has become a bottleneck for practical applications of Bayesian learning. To reduce the computational cost, we can use message passing methods to efficiently approximate the exact poste-riors. Two exemplary message passing methods are belief propagation (i.e., the sum-product algorithm) (Kschischang et al., 1998; Pearl, 1982) and expectation propagation (Minka, 2001), a generalization of BP. Despite their wide success in various applications, BP and EP may degrade their approximation quality and diverge when the exact target distribution is far from the approximating family used by them X  X or example, when many samples are mislabeled for classification or variables are strongly coupled in a MRF. We can force BP and EP to converge using the CCCP algorithm (Heskes et al., 2005; Yuille, 2002). But not only are the CCCP updates slower than the message passing updates, but also the forced convergence might not be desirable X  X ccording to Minka (2001), EP diverges for a good reason, indicating a poor approximating family (or a poor energy function) used by EP. For the difficult cases, it may be too rigid to use moment matching, a natural consequence of KL minimization in BP and EP (see Section 2).
 To improve both approximation quality and algorith-mic stability of message passing, we propose a new approximate inference method, relaxed expectation propagation (REP). Specifically, we introduce a relax-ation factor in the KL minimization and penalize it by a l 1 penalty (See Section 3). The penalized KL minimization is adaptive in moment matching: the l 1 penalty completely prunes the relaxation factor and gives the same moment matching update as in BP or EP, if the original and approximate distributions are similar; if they differ significantly (i.e., when EP strug-gles), the relaxation factor survives the l 1 penalty and renders the original and projected distributions with different moments. To better understand REP, we also present its primal energy function in Section 3 and its dual energy function in Appendix. The primal energy function of REP has a larger feasible set than that of EP and the Bethe energy of BP, providing a higher chance for finding a better approximation.
 In Section 4, we present REP inference on two im-portant models: Gaussian process (GP) classification models and discrete MRFs. GP classification models are powerful predictive tools and have been trained by EP (Kuss &amp; Rasmussen, 2005); MRFs are ubiq-uitous in scientific and engineering applications and BP is a popular choice for estimating marginal distri-butions in MRFs. For MRF inference, REP reduces to relaxed belief propagation (RBP). Note that we can easily adopt RBP to inference on Bayesian net-works because both MRFs and Bayesian networks can be morphed into factor graph representations (Kschis-chang et al., 1998). In Section 5, we discuss differences between REP, power EP, and damped EP.
 In Section 6, we report experimental results on syn-thetic and UCI benchmark datasets for GP classifi-cation. REP consistently outperforms EP, damped EP, and power EP (Minka, 2004) X  X n terms of algo-rithmic stability, estimation accuracy, and predictive performance. The MRF inference results show greatly improved estimation accuracy of RBP over BP and fractional BP (Wiegerinck &amp; Heskes, 2003) when in-teractions between MRF nodes are strong. Given observations D , the posterior distribution of a probabilistic model with factors { t i ( w ) } i =1 ,...,N where Z is the normalization constant and w i is a subvector of w that is associated with the i-th factor t . Factors t i are linked to the observations D . EP approximates each factor in (1): where q ( w ) and  X  t i ( w i ) approximate p ( w |D ) and t ( w i ), respectively, and have the form of the expo-nential family X  X uch as Gaussian or factorized discrete distributions. The approximation factor  X  t i ( w ) is a message from the i th factor t i to variables w i in a factor graph representation (Kschischang et al., 1998). To obtain q ( w ), EP refines the messages by repeat-ing the following three steps: message deletion, belief projection, and message update on each factor. In the message deletion step, we compute the partial poste-rior q \ i ( w ) by removing a message  X  t i from the approx-imate posterior q old ( w ): q \ i ( w )  X  q old ( w ) /  X  the projection step, we minimize the KL divergence mate posterior q ( w ), such that the information from each factor is incorpo-rated into q ( w ). Finally, the message  X  t i is updated via  X  t ( w i )  X  q ( w ) /q \ i ( w ). On discrete Bayesian networks or Markov random fields (MRFs), we can use a fac-torized approximation q ( w ) = Q j q ( w j ) where j is the node index. Then the EP updates reduce to classical BP or sum-product updates (Minka, 2001).
 Since q ( w ) is in the exponential family, it has the fol-lowing form where  X  ( w ) are the features of the exponential fam-ily. Given this representation, minimizing the KL (3) amounts to the following moment matching constraint between  X  p i ( w ) and q ( w ): For BP, moment matching means q ( w j ) and the marginal of  X  p i ( w ) are matched, q ( w j ) = P w Based on moment matching, EP and BP message pass-ing updates capture critical statistics we care about. However, when the approximating family is far from the true distribution, message passing can be too rigid, causing EP and BP to deteriorate their performance. In this section, we present a new Bregman distance with l 1 penalty, describe the REP algorithm based on this distance, discuss choices of relaxation factors, and provide the energy function of REP. 3.1. A new divergence To relax moment matching between  X  p i ( w ) and q ( w ), we introduce a relaxation factor r i ( w )  X  exp(  X  T i  X  ( w )) into the KL divergence and put l 1 penalty on the pa-rameters of r i . Specifically, we propose the following penalized divergence between  X  p i and q where |  X  i | 1 is the l 1 norm of  X  i , the weight c controls how much the penalty we give to the relaxation, and the KL r divergence is defined for unnormalized distri-butions. It is easy to show, given r i , KL r is a valid Bregman distance between  X  p i and q . Minimizing (5) relaxes moment matching between  X  p i and q . This re-laxation is adaptive : when the approximating family is significantly different from  X  p i , the relaxation factor yields different moments for  X  p i and q ; when  X  p i and q are similar, the l 1 penalty will set  X  i = 0 so that we obtain exact moment matching as in EP or BP. 3.2. Algorithm By iteratively minimizing the penalized divergence (5), we obtain the following REP algorithm: 1. Initialize q ( w ) = 1 and all the messages  X  t i ( w ) = 1. 2. Repeat until all  X  t i ( w ) converge: Pick a factor i . For simplicity, we drop the subscript of w in the i -th factor. Note that when some factors are in the expo-nential family and have the same feature form  X  ( w ) as q ( w ), we can absorb them directly into q ( w ) in ini-tialization without iterative updates. 3.3. Choice of relaxation factors want to parameterize  X  i to make the minimization of (5) easy. Clearly there are many choices available. A convenient one is to set  X  i to be a scaled version of the parameters of the old message  X  t i . It does not cause double-counting of factors, because r i appears in both sides of the penalized divergence (5) and the new pos-terior q does not include r i . With this choice, we can compute (5) analytically, making it easy for joint min-imization over q and r i . 3.4. Energy function To gain further insight into REP, we derive its energy functions. The primal energy function is  X  ( n  X  1) subject to 1  X  Z where R w  X  p i ( w ) d w = 1, R w q ( w ) d w = 1,  X  R w  X  p i ( w ) r i ( w ) d w , and Z q = Based on a KL duality bound, we obtain the dual form of the energy function (See Appendix for details). Set-ting the gradient of the dual function to zero gives the fixed-point updates. If we set the relaxation factor as a scaled version of the old messages as disccused in Section 3.3, we only need to slightly modify (6) (See Appendix for details). The fixed-point updates do not guarantee convergence like the classical EP updates. However, by relaxing the moment matching require-ment between  X  p i ( w ) and q ( w ), the new updates are much more robust than classical EP updates. In our experiments, while EP diverged many times on diffi-cult datasets, the new algorithm did not diverge once. From the energy function perspective, we enlarge the feasible set for the energy function of EP. The min-max cost function (6) reduces to that of EP as a spe-cial case if we set r i ( w ) = 1. As shown by (Heskes et al., 2005), the cost function of EP corresponds to the Bethe energy (Yedidia et al., 2003) that approximates the system entropy with the exact moment matching constraint. With the larger feasible set, we can poten-tially obtain better entropy approximation. In this section, we apply the new algorithm to train binary Gaussian process classification models and to perform inference on discrete Markov random fields. 4.1. REP for Gaussian process classification First, let us denote N independent and identically dis-tributed samples by D = { ( x i ,y i ) } N i =1 , where x d dimensional input and y i is a scalar output. We as-sume there is a latent function f that we are modeling and a noisy realization of f at x i is y i .
 We use a GP prior with zero mean over f . Its pro-jection at the samples { x i } defines a joint Gaussian distribution: p ( f ) = N ( f | 0 ,K ) where K ij = k ( x is the covariance function encoding the prior notation of smoothness. For classification, the data likelihood has the following form where models the labeling error,  X (  X  ) is a step func-tion ( X ( a ) = 1 if a  X  0, and  X ( a ) = 0 otherwise). Given the GP prior and the data likelihood, the pos-terior process of f is Gaussian process and we cannot compute the param-eters in the posterior process analytically. To ob-tain an approximation to p ( f |D ), we approximate each non-Gaussian factor p ( y i | f ( x i )) by a Gaussian factor  X  t ( f i ) = N ( f i | m i ,v i ). Then we obtain a Gaussian pro-cess approximation q ( f ) to p ( f |D ): For REP, we parameterize the relaxation factor r i as so that r i shares the mean as  X  t i and b i is the only free parameter in r i . To simplify the notation in the follow-ing presentation, we define  X  t i,b ( f i )  X N ( f i | m r ( f i )  X  t i ( f i ). Then we have the following REP training algorithm for GP classification. 1. Initialize all m i = 0, v i =  X  , and b i = 0. Also, 2. Until all ( m i ,v i ,b i ) converge: Pick a sample i . 4.2. Relaxed belief propagation (RBP) Just as EP reduces to belief propagation on Bayesian networks or MRFs, REP becomes a relaxed version of belief propagation on these models. In particular, let us consider the joint distribution of N discrete vari-ables x = ( x 1 ,...,x N ) in a MRF: where Z is the normalization constant,  X  i ( x i ) and  X  i,j ( x i ,x j ) are unitary and pairwise potential func-tions, and E represents the set of edges.
 We obtain classical BP updates by adopting a factor-ized EP approximation (Minka, 2001): q ( x ) = Y tion to the factor  X  ij ( x i ,x j ); they are messages from the factor t X  ij to the nodes x i and x j .
 It is well known that if the MRF contains cycles and the variables are strongly coupled, BP can suffer from low approximation quality and divergence. To address this issue, we use the following relaxation factor where b ij  X  [0 , 1]. We present the RBP updates below: 1. Initialize q ( x i ) =  X  i ( x i ) and  X   X  ij ( x i ) = 1. 2. Until all  X   X  ij converge: Pick an edge ( i,j )  X  X  . Various message passing algorithms, such as power EP (Minka, 2004), can be interpreted as iterative mini-mization of a general  X  -divergence with different  X  val-ues (Minka, 2005; Zhu &amp; Rohwer, 1995). On MRFs, power EP reduces to fractional BP (Wiegerinck &amp; Hes-kes, 2003). When  X  = 1, power EP and fractional BP become EP and BP; when  X  6 = 1, minimizing the  X  -divergence does not require EP X  X  moment matching in message passing updates. However, moment match-ing may contribute to great empirical performance of BP and EP, and is desirable for many tasks such as classification X  X here moment matching can help pre-serve the posterior probability in critical regions. Un-like power EP and fractional BP, REP and RBP not only stabilize message updates but also maintain mo-ment matching whenever possible in an adaptive way. We can damp the step size for message updates to help convergence (Minka, 2005). The damping method re-cursively minimizes the KL divergence just as EP with the same same energy function. For cases where EP diverges, the damping method can be very slow with a small step size needed for convergence and provide poor approximations after convergence. By contrast, using the (penalized) divergence different from the KL divergence, REP can improve both algorithmic stabil-ity and approximation accuracy over EP. 6.1. Results on GP classification For GP classification, we compared REP with EP, power EP (PEP), and damped EP (DEP) on approxi-mation quality, convergence speed, and prediction ac-curacy. We chose GP classification as a testbed be-cause EP has been shown to be an excellent choice for training GP classification models (Kuss &amp; Rasmussen, 2005). Since there is no previous reported work that uses PEP for GP training, we derived the updates and presented them in Appendix.
 Toy example. First, we considered linear classifica-tion of five data points shown in Figure 1. The red  X  X  X  and blue  X  X  X  data points belong two classes. The red point on the right is mislabeled. To reflect the true labeling error rate in the data, we set = 0 . 2 in (8). To obtain linear classifiers, we used the linear ker-nel k ( x i , x j ) = x T i x j for the GP training algorithms. After each algorithm converged, we recovered the pos-terior mean and covariance of the linear classifier w in the 2-dimensional input space. To measure the approximation quality, we used im-portance sampling (IS) with 10 8 samples to obtain the exact posterior distribution of the classifier w . We treated the (approximate) posterior means as the esti-mated classifiers and used them to generate their de-cision boundaries (see Figure 1). For PEP, we set the power u to 0.8; for REP, we set c = 20; for DEP, we used both a fixed step-size 0.5 and an adaptive step-size that is based on a local prediction confidence level (based on (12) and (13)). We denote DEP with the adaptive step-size as DEP  X  in Figure 1.
 The EP decision boundary differs from the exact Bayesian decision boundary significantly. DEP and DEP  X  give the same wrong decision boundary as EP. The PEP decision boundary is slightly closer to the exact one. The REP decision boundary perfectly over-laps with the exact one. Note that the relaxation pa-rameter b i was automatically pruned to be zero for all the points except the red point on the right (the corre-sponding b i = 0 . 01), demonstrating REP X  X  adaptive-ness in relaxation.
 We also varied the power for PEP, the step size of DEP, and the penalty weight c in (5) for REP to examine their impact on approximate quality. We measured the mean square distance between the estimated and the exact mean vectors, as well as the mean square distances between the estimated and the exact covari-ance matrices. The results of PEP and REP are sum-marized in Figure 2. We did not show the estimation accuracies of DEP since they are identical to those of EP. Figures 2.(a)-(b) show that the estimated poste-rior means of PEP are always worse than what EP achieves. In contrast, when c is big for REP, the l penalty forces the relaxation factor b i = 0. Accord-ingly, REP reduces to EP and gives the same results. When c is small X  X or a wide range of values X  X EP greatly improves the posterior approximation quality. Finally, for the classification models with various val-ues (e.g., 0.1 and 0.25) in (8), our further experiments showed that REP consistently provided more accurate posterior estimation than EP and PEP.
 Synthetic data. We then compared these algorithms on a nonlinear classification task. We sampled 200 data points for each class: for class 1 the points were sampled from a single Gaussian distribution and, for class 2, the points from a mixture of two Gaussian com-ponents. The data points from these two classes are represented by red circles and blue  X  X  X , respectively in Figure 3. We randomly flipped the labels of some data points to introduce labeling errors and varied the error rates from 10% to 20%. For each case, we let match the error rate. We used a Gaussian kernel for all these training algorithms and applied cross-validation on the training data to tune the kernel width. We set the power u = 0 . 8 for power EP, the step-size 0 . 5 for damped EP, and c = 10 for REP.
 In Figure 3, we visualized the decision boundaries of EP, PEP, DEP, and REP on one of the datasets with 20% labeling errors. Clearly, EP diverged and led to a chaotic decision boundary. PEP and DEP converged in 30 and 45 iterations and their decision boundaries are better than that of EP but still do not match the underlying generative distributions of the data (one Gaussian vs two Gaussians). Using an adaptive step-size, DEP  X  actually diverged all the time. So we did not show its decision boundaries. By contrast, REP converged in only 15 iterations and provides a much more reasonable decision boundary than all the other algorithms.
 To illustrate the convergence of PEP, DEP, and REP, we visualized in Figure 4 the change of the GP message parameter m along iterations: R (iter)  X  k m iter  X  m iter  X  1 k 2 . Clearly, EP is less stable than the other algorithms.
 To conduct a systematic comparison between EP, PEP, DEP, and REP on algorithmic robustness, con-vergence speed and estimation accuracy, we repeated the experiments 10 times; each time we sampled 400 training and 39,600 test points. Figure 5.(a) shows the number of iterations until convergence. To reach the convergence, we required R &lt; 10  X  3 . Clearly, REP converged faster than PEP, DEP, and EP. Figure 5.(b) shows that while EP, DEP, and PEP diverged some-times, REP never did.
 Figure 5.(c) shows that, with 10% labeling errors in the training set, REP gave significantly higher predic-tion accuracies than EP and PEP. The test errors of EP, DEP, and PEP were averaged only over the con-verged cases out of the 10 runs; their average accura-cies would degrade if we include their divergent cases here. Note that we did not introduce labeling errors in the test data and the prediction error rates can be lower than the labeling error rates in the training sets. With 10% labeling errors, the prediction accuracy of REP is slightly higher than that of DEP with no signif-icance, but REP converges three time faster (see Fig-ure 5.(a)). With 20% labeling errors in the training set, with much fewer number of iterations, REP sig-nificantly outperforms all the alternative algorithms in terms of prediction accuracy.
 Real data. Finally we tested these algorithms on five UCI benchmark datasets: Heart, Pima, Diabetes, Haberman, and Spam. For PEP and REP, we tuned the power and penalty weight based on a small valida-tion set. For DEP, we used a step-size 0.5; a smaller step size would make DEP really slow for convergence. For the Heart dataset, the task is to detect heart dis-eases with 13 features per sample. We randomly split the dataset into 81 training and 189 test samples 100 times. For the Pima dataset, we randomly split it into 319 training and 213 test samples, again 100 times. For the Diabetes dataset, medical measurements and personal history are used to predict whether a patient is diabetic. R  X atsch et al. (2001) split the UCI Dia-betes dataset into two groups (468 training and 300 test samples) for 100 times. We used the same parti-tions in our experiments. For the Haberman X  X  survival dataset, the task is to estimate whether a patient sur-vives more than five years (including 5 years) after a surgery for breast cancer. The whole dataset contains information from 306 patient samples and 3 attributes per sample. We randomly split the dataset into 183 training and 123 test samples 100 times. Note that we did not add any labeling errors to these four datasets. Figure 6 summarizes the averaged results. REP out-performs the competing algorithms significantly. For the Spam dataset, the task is to detect spam emails. We partitioned the dataset to have 276 train-ing and 4325 test samples, and flipped the labels of multiple data points randomly from both the train-ing and test sets. The experiment was repeated for 100 times. Figure 7 demonstrated that, with various additional labeling errors, REP achieves significantly higher prediction accuracies than EP, DEP, and PEP. 6.2. Results on MRF inference Now we test RBP, BP, damped BP, and fractional BP for inference on a binary MRF model x i  X  { X  1 , 1 } with weak or strong suppressive interactions: tion condition, we chose edge weights J ij  X  X  ( N (5 , 5) | independently for each edge. We repeated experiments 50 times for each condition. To obtain the exact single-node marginal distributions, p ( x i ), we used the junc-tion tree algorithm. We set the power 0 . 8 for frac-tional BP, the step-size 0.8 for damped BP, and c = 0 . 1 for REP. To measure the estimation accuracy, we cal-culated the averaged absolute difference between the exact and approximate single-node marginal distribu-tions. We did not report the number of divergent cases for each algorithm, but the divergence is reflected in the averaged absolute difference.
 The results are summarized in Figure 9.(a)-(b). On the weak interaction case, BP, damped BP, and RBP all achieved comparable results, while factional BP per-formed worse than all the others. This is not surprising because BP works well on MRFs with weak interac-tions. RBP shrinked its relaxation and almost all the estimated b ij were exactly zeros. Thus RBP behaved like BP. Fractional BP did worse because it did not use the KL minimization to capture important mo-ment statistics. For the strong interaction case, RBP relaxed the moment matching constraint (more b ij are estimated to be nonzero) and achieved significantly higher accuracy than all the other methods. In the paper we have presented the new REP inference method based on the l 1 -penalized divergence. Unlike the  X  -divergence minimization in power EP, the l 1 -penalized divergence minimization adaptively relaxes the moment matching constraint in EP and BP. Ex-perimental results demonstrate that the new inference algorithm avoids divergence and improves approxima-tion quality performance over the previous message passing methods.
 This work was supported by NSF IIS-0916443, NSF ECCS-0941533, NSF CAREER Award IIS-1054903, and the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370.
 Heskes, T., Opper, M., Wiegerinck, W., Winther, O., and Zoeter, O. Approximate inference techniques with expectation constraints. Journal of Statistical Mechanics: Theory and Experiment , 11:11015, 2005. Kschischang, F. R., Frey, B. J., and Loeliger, H.-A. Factor graphs and the sum-product algorithm.
IEEE Transactions on Information Theory , 47(2): 498 X 519, 1998.
 Kuss, M. and Rasmussen, C. E. Assessing approximate inference for binary Gaussian process classification.
Journal of Machine Learning Research , 6(10):1679 X  1704, 2005.
 Minka, T. P. A family of algorithms for approximate
Bayesian inference . PhD thesis, Massachusetts In-stitute of Technology, 2001.
 Minka, T. P. Power EP. Technical Report MSR-TR-2004-149, Microsoft Research, Cambridge, January 2004.
 Minka, T. P. Divergence measures and message pass-ing. Technical Report MSR-TR-2005-173, Microsoft Research, Cambridge, 2005.
 Pearl, J. Reverend Bayes on inference engines: A dis-tributed hierarchical approach. In Proceedings of the
American Association of Artificial Intelligence Na-tional Conference on AI , pp. 133 X 136, Pittsburgh, PA, 1982.
 R  X atsch, G., Onoda, T., and M  X uller, K.-R. Soft mar-gins for adaboost. Mach. Learn. , 42:287 X 320, March 2001.
 Wiegerinck, W. and Heskes, T. Fractional belief prop-agation. Advances in Neural Information Processing Systems , 12:438 X 445, 2003.
 Yedidia, J. S., Freeman, W. T., and Weiss, Y. Under-standing belief propagation and its generalizations. In Lakemeyer, Gerhard and Nebel, Bernhard (eds.),
Exploring artificial intelligence in the new millen-nium , pp. 239 X 269. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2003.
 Yuille, A. L. CCCP algorithms to minimize the Bethe and Kikuchi free energies: Convergent alternatives to belief propagation. Neural Computation , 14:1691  X  1722, 2002.
 Zhu, H. and Rohwer, R. Bayesian invariant mea-surements of generalisation for continuous distribu-tions. Technical Report NCRG/4352, Aston Univer-
