 Medical data is often presented, at least partially in the form of free text (e.g. medical reports attached to patients X  records). Such documents contain important information about patients, disease progression and management, but are difficult to analyse with conventional data mining techniques due to their unstructured or semi-structured na-ture. Medical staff may have a number of interesting questions that can be asked of such data, but they certainly need automatic methods for reading, categorising and analyzing thousands of electronic patients X  reports. 
The Gastroenterology unit of a local hospital had just such a problem as they col-lected electronic reports on thousands of colonoscopy procedures, but could not give answer to simple questions, such as the percentage of successful colonoscopies under-taken. Colonoscopy refers to the passage of the colonoscope from the lowest part (anus and rectum) right around the colon to the caecum. This constitutes a complete examination. The aim of colonoscopy is to check for medical problems such as bleeding, colon cancer, polyps, colitis, etc. [6]. After each colonoscopy procedure, the port is extremely valuable for clinical purposes but difficult to handle due to the lack of structure. The procedure can be classified as successful or unsuccessful depending on what the clinicians claim they have been able to examine and the reasons for any limited examinations. Classifying colonoscopy procedure reports into categories is a text classification task. 
Text classification is defined as the process of assigning pre-defined category la-bels to documents based on what a classifier has learned from training examples [9]. interest ( positive documents) from a set of mixed documents. There can also be multi-class problems in which the classifier has to distinguish documents from each of sev-documents and then use a learning algorithm to produce a classification [15]. This approach, called supervised learning [8], ha s the problem of the considerable effort required to manually label a large number of training examples for every class, par-ticularly for multi-class problems. An alternative approach called partially supervised classification has recently been introduced [1, 2] for binary classification problems, and earlier [7] for multi-class problems. It is based on the use of a large set of unla-beled documents and a small set of labelled documents for every class so as to reduce and unlabeled documents to learn a classification [10], cutting down more on the la-belling effort. This approach is based on a two-step strategy. Step 1 identifies the positive documents from the unlabeled documents, and step 2 builds the final classi-fier. There are a number of algorithms that ar e applicable in step 1 and step 2. Decid-effective application of the technique to real-world data. 
The main purpose of this paper is to perform a practical evaluation of partially su-pervised classification. The methods available in each step of the process will be tested in combination. The combination that produces the best performance accord-ing to some evaluation measures will be recommended. The evaluation will be per-formed through a real-world medical problem: the classification of a set of colono-features used to represent a document. The partially supervised classification approach uses a reduced set of positive docu-ments, P , and a large set of unlabeled documents, U . There is initially no labeling of reliable set of negative documents, RN , from the unlabeled documents. This can be achieved by a number of algorithms; in this paper we used Rocchio (ROC) [11], Na X ve classification algorithm to the newly labeled data. Since some documents are still in the intention of extracting more possible negative data at each iteration and improving the overall performance of the classifier. The procedure will stop when no further negative documents are found in the unlabeled set, U-RN . There are two classifiers used in this step: Expectation-Maximization (EM) [16, 19] and Support Vector Ma-chines (SVM) [13]. The algorithms were select ed based on their availability to the authors. For these experiments we used real world medical documents collected from the Gas-troenterology unit of a local hospital. These documents contain information on colonoscopy procedures includ ing preparation of the bowel, features of the colon identified in examination, abnormalities found during examination with their descrip-tion, patient X  X  reaction to the procedure, etc. The number of documents in this collec-tion is 4,876. 25% of these documents were selected using a 1-in-4 sampling strategy to be used as test documents. The rest (75%) were used to create training sets as fol-lows: 120 documents from the positive class were selected as the positive set. The rest of the documents were used as the unlabeled set. 
The most frequently used method to represent text is bag-of-words representation where all words from the set of documents are taken and no ordering of words or any structure of text is used [4]. Each distinct word corresponds to a feature of the set of documents. Each feature weighted using term frequency-inverse document frequency (tf-idf) [20] which is refined model of term frequency. 
Four different measures were used to evaluate the performance of different classi-fiers: precision , recall , F-measure and accuracy [14]. Not all the words in the documents are importa nt, so they may degrade the classifier X  X  performance. In addition, representing small set of documents that may have hundreds of different words using bag-of words approach will generate a huge feature space and thus will increase the processing time. To solve these problems, approaches to reduce the feature space dimension are needed. We used three approaches: 1. As a result of consulting an expert in the domain field, we removed unhelpful sen-2. We have removed stop words from all data sets using stop-lists; 3. We stemmed the words using Porter X  X  suffix-stripping algorithm [3]. 
The total number of words before applying any of the feature reduction approaches is 319689 word, after applying the three approaches only 154999 words left. That means the total number of words reduced to 48.5%. strategy for partially supervised classification for a real-world application. It will then be possible to test the claim that his method is effective and computationally efficient [2] using a challenging medical problem. The combination of different methods used in step 1 (spy, NB and ROC) and step 2 (SVM and EM) will produce six techniques (classifiers) when we used one method for step1 and one method for step 2. These six classifiers will be investigated and evaluated in our experiments. The results shown in Table 1 illustrate the recall, precision, F-measure and accuracy obtained by different classifiers. 
Figure 1 illustrates graphically the F-measure and accuracy results for the six clas-sifiers. The axes x and y represent the classification techniques and the percentage of the F-measure and the accuracy respectively. The main observation from Table 2 and Figure 1is that the best results are obtained by classifiers using EM in step 2, regard-accuracy results obtained by SVM and EM we find that EM significantly outperforms SVM. We also observe that when NB is used in step 1 to identify the RN set, it pro-duces the worst results in term of accuracy and F-measure. Spy-SVM also under-performs. This may be due to a small positive set, resulting in a small number of spies added to U . This in turn produces a poor RN set. In the case of S-EM the problem is ameliorated since EM used in step 2 will first fill the missing data. According to both the F-measure and accuracy, the highest results are obtained by S-EM, but ROC-EM and NB-EM performed very close to it with less than 0.2% difference. It is worth noting that those classifiers represent the best balance of recall and precision but lower precision than can otherwise be obtained. 
Another set of experiments was conducted to attempt to improve the performance of different classifiers by reducing the numb er of features used. As shown in table 1, the final total number of distinct features in the collection is 2,636. The frequencies of these features vary from the highest frequency 7,111 to the lowest frequency of 1. 1,124 of these features occurred only once. The previous set of experiments was re-peated with a reduced feature set. In each case, only the  X  top features according to their frequency will be selected to build the classifier. The four values of  X  used are 100, 200, 300 and 500. 
Table 2 shows the resulting accuracy (acc.) and F-measure (f-m) values respec-graphically. The x axe in both figures represents the six classification techniques, and y axe in Figure 2 represent the percentage of the accuracy and in Figure 3 represents the F-measure values. 
Using the top 100 features improved the performance of the SVM based methods but significantly degraded the performance of the EM based methods. This may indi-labels of the documents in U-RN when the EM method is used. 
The results obtained using the top 200 features slightly improve the performance of a number of classifiers whilst producing no significant deterioration in others. Larger some cases produced slightly worse results. The main observations from the last set of experiments are: (1) Selecting a reduced set of features to represent the documents can improve the performance of all classifiers based on F-measure and accuracy; (2) A very reduced feature set may affect the performance of certain classifiers such as EM; and (3) Finding a sufficient set of features can improve performance while also increasing efficiency, but it may require some experimentation. The objective of the research to test partially supervised classification on a real world problem. To this effect, a number of experiments were conducted to evaluate the per-formance of different methods within the two-step approach . The approach has the advantage of requiring only a small set of labeled positive documents to operate. Our step yielded the best results, regardless of the method used to identify negative docu-ments in the first step. We also experimentally showed that the careful use of feature selection can improve the performance and should obviously improve efficiency. In our case, selecting the top 200 features to represent the documents yielded satisfactory result for all classifiers. 
Our results are very competitive for this real world problem and could be used to automatically label and classify medical reports. We believe the method is widely applicable to other text classification problems in the medical domain that requires two-class or binary classification. This work was supported by the Engineering and Physical Sciences Research Council (EPSRC) grant number GR/T04298/01. 
