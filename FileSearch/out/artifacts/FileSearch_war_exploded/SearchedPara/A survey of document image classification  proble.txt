 ORIGINAL ARTICLE Nawei Chen  X  Dorothea Blostein Abstract Document image classification is an impor-tant step in Office Automation, Digital Libraries, and other document image analysis applications. There is great diversity in document image classifiers: they differ in the problems they solve, in the use of training data to construct class models, and in the choice of docu-ment features and classification algorithms. We survey this diverse literature using three components: the prob-lem statement, the classifier architecture, and perfor-mance evaluation. This brings to light important issues in designing a document classifier, including the definition of document classes, the choice of document features and feature representation, and the choice of classifica-tion algorithm and learning mechanism. We emphasize techniques that classify single-page typeset document images without using OCR results. Developing a gen-eral, adaptable, high-performance classifier is challeng-ing due to the great variety of documents, the diverse criteria used to define document classes, and the ambi-guity that arises due to ill-defined or fuzzy document classes.
 Keywords Document image classification  X  Document classifiers  X  Document classification  X  Document categorization  X  Document features  X  Feature representations  X  Class models  X  Classification algorithms  X  Learning mechanisms  X  Performance evaluation 1 Introduction Document classification is an important task in docu-ment processing. It is used in the following contexts:  X  Document classification allows the automatic distri- X  Document classificationimproves indexingefficiency  X  Document classification plays an important role in  X  Document classification facilitates higher-level doc-
Document classification can be done with or without use of the text content of the document. We use the following terminology, which is not standardized.
Document classification (Also called document im-age classification or page classification ). Assign a sin-gle-page document image to one of a set of predefined document classes. Classification can be based on vari-ous features, such as image-level features, structural or textual features.
 Text categorization (Also called text classification ). Assign a text document to one of a set of predefined document classes. The text document may be a plain text document (e.g. ASCII) or a tagged text document (e.g. HTML/XML). Classification is based on textual features (such as word frequency or word histogram) or on structural information known from tags.

Sebastiani [49] provides a comprehensive survey of text categorization, which is an active research area in information retrieval. The need for text categorization continues to grow, due to the increased availability of text documents, especially on the Internet. More re-cently, researchers are proposing classification methods that use both textual and structural information [48]. The structural information may be directly available from the tags in a tagged text document. Text catego-rization techniques can be applied as part of document image classification, using OCR results extracted from the document image. However, OCR errors must be considered.

In this survey, a document refers to a single-page typeset document image. The document image may be produced from a scanner, a fax machine or by converting an electronic document into an image format (e.g. TIFF or JPEG). We focus on classification of mostly-text doc-uments, using image-level or structural features, rather than textual features. Mostly-text documents include business letters, forms, newspapers, technical reports, proceedings, and journal papers, etc. These are in contrast to mostly-graphics documents such as engi-neering drawings, diagrams, and sheet music. Among mostly-text documents, we further focus on classifica-tion of documents with significant structure variations within a class, such as business letters, article-pages and newspaper-pages. Forms have rather restricted physical layout. Many papers have been published about form classification (also called form type identification) [10, 23,50,56,59]. We refer to some of this literature, but do not provide an exhaustive survey of form classification. 2 Three components of a document classifier There is great diversity in document classifiers. Classifi-ers solve a variety of document classification problems, differ in how they use training data to construct mod-els of document classes, and differ in their choice of document features and recognition algorithms. We sur-vey this diverse literature using three components: the problem statement, the classifier architecture and per-formance evaluation. These components are illustrated in Fig. 1.

The problem statement for a document classifier de-fines the problem being solved by the classifier. It con-sists of two aspects: the document space and the set of document classes. The document space defines the range of input document samples. The training samples and the test samples are drawn from the document space. The set of document classes defines the possible outputs produced by the classifier and is used to label document samples. Most surveyed classifiers use manually defined document classes, with class definitions based on simi-larity of contents, form, or style. The problem statement is discussed further in Sect. 3.

The classifier architecture includes four aspects: docu-ment features and recognition stages, feature represen-tations, class models and classification algorithms, and learning mechanisms. The classifier architecture is dis-cussed further in Sect. 4 with Table 2 presenting an over-view of the surveyed classifiers along these four aspects.
Performance evaluation is used to gauge the per-formance of a classifier, and to permit performance comparisons between classifiers. The diversity among document classifiers makes performance comparisons difficult. Issues in performance evaluation include the need for standard data sets, standardized performance metrics, and the difficulty of separating classifier perfor-mance from pre-processor performance. Performance evaluation is discussed further in Sect. 5. 3 The problem statement The problem statement for a document classifier has two aspects: the document space and the set of doc-ument classes. The former defines the range of input documents, and the latter defines the output that the classifier can produce. 3.1 The document space The document space is the set of documents that a clas-sifier is expected to handle. The labeled training samples and test samples are all drawn from this document space. The training samples are assumed to be representative of the defined set of classes. The document space may include documents that should be rejected, because they do not lie within any document class. In this case, the training samples might consist of positive samples only, or they might consist of a mixture of positive and neg-ative samples. Document classifiers with reject options are reported in [12,21,23,33,41,55].

For any classifier, the document space is a subset of the entire set of possible documents (which includes all existing documents, as well as documents that are yet to be created). There is no precise definition of document . We use the document taxonomy defined by Nagy [38] as shown in Fig. 2.

Structureddocuments aremostly-text documents that have identifiable layout characteristics. All classifiers we survey use a document space consisting of structured documents. Text categorization methods, dealing with plain text documents, are surveyed by Sebastiani [49].
Nagy X  X  characterization of documents focuses on doc-ument format: mostly-graphics or mostly-text, hand-written or typeset, etc. Another way of characterizing documents is by application domain, such as documents related to income tax or documents from insurance com-panies. Some of the classifiers we survey use document spaces that are restricted to a single application domain. Others use document spaces that span several applica-tion domains. Here is a summary of the document space of selected classifiers characterized by application do-mains.  X  A single domain document space  X  A multiple-domain document space 3.2 The set of document classes The set of document classes defines how the document space is partitioned. The name of a document class is the output produced by the classifier. Several possible partitions of document space are shown in Fig. 3. A set of document classes may uniquely separate the docu-ment space (Fig. 3a), with a single class label assigned to a document. If the document space is larger than the union of the document classes (Fig. 3b), the classifier is expected to reject all documents that do not belong to any document class. Fuzziness may exist in the definition of document classes (Fig. 3c), with multiple class labels assigned to a document.

A document class (also called document type or doc-ument genre) is defined as a set of documents character-ized by similarity of expressions, style, form or contents [3]. This definition states that various criteria can be used for defining document classes. Document classes can be defined based on similarity of contents. For example, consider pages in conference papers, with classes con-sisting of  X  X ages with experimental results X ,  X  X ages with conclusions X ,  X  X ages with description of a method X  [62]. Alternatively, document classes can be defined based on similarity of form and style (also called visual similarity ), such as page layout, use of figures, or choice of fonts [19]. Figure 4 shows an example of document classes defined based on visual similarity. Doermann et al. provide a functional description of a document, which gives insight into defining document classes based on domain-inde-pendent functional structures, such as headers, footers, lists, tables, and graphics [17].

Typically, the set of document classes is not given as an explicit input to a document classifier. Instead, a description of the set of classes is provided implicitly, by the labeled training samples. Of course, labeling the training samples requires a definition of document clas-ses. This might be an informal, implicit definition: the document classes are manually defined, and the training samples are manually labeled. Alternatively, document classes can be defined automatically, by clustering unla-beled document samples. Most of the systems we sur-vey use manual definition of the document classes. An exception is Shin et al., who, in addition to defining clas-ses manually, use a self-organizing map to find clusters in unlabeled input data and assign each input document to one of the clusters [51].

Table 1 summarizes the classification problems solved by selected document classifiers. The great diversity of document classes is clearly illustrated.

The set of document classes that are required depend on the goal of the document classification. Document classification is often followed by further document im-age analysis. The classification allows subsequent pro-cessing to be tuned to the document class.

Bagdanov and Worring characterize document classi-fication at two levels of detail, coarse-grained and fine-grained [3]. A coarse-grained classification is used to classify documents with a distinct difference of features, such as business letters versus technical articles. A fine-grained classification is used to classify documents with similar features, such as business letters from different senders, or journal title pages from various journals.
This completes our discussion of the problem state-ment for a document classifier. Next, we discuss the clas-sifier architecture. 4 The classifier architecture We use the following four aspects to characterize clas-sifier architecture: (1) document features and recogni-tion stage, (2) feature representations, (3) class models and classification algorithms, and (4) learning mecha-nisms. These aspects are interrelated: design decisions made regarding one aspect have influence on design of other aspects. For example, if document features are represented in fixed-length feature vectors, then statisti-cal models and classification algorithms are usually con-sidered. Table 2 provides an overview of the surveyed document classifiers using these four aspects. As seen in Table 2, classification may be performed at different stages of document recognition, with a diverse choice of document features, feature representations, class mod-els and classification algorithms.

We now discuss each of the four aspects in Sects. 4.1 X 4.4. In the process, we refer to various entries in Table 2. 4.1 Document features and recognition stage Choice of document features is an important step in classifier design. Table 2 illustrates the great variety of document features used for document classification. Relevant surveys about document features include the following. Commonly used features in OCR are sur-veyed in [57]. A set of commonly used features for page segmentation and document zone classification are given in [42,58]. Structural features produced in physical and logical layout analysis are surveyed in [22,37,38].
All the features in our surveyed systems are extracted from black and white document images. The gray-scale or color images (e.g. advertisements, magazine articles) are binarized into binary images. Unavoidably, for cer-tain documents, the binarization process removes essen-tial discriminate information. As suggested in the report of the DAS02 working group on document image anal-ysis [52], more research should be devoted to the use of features extracted directly from gray-scale or color images to classify documents.

Before discussing the choice of document features further, we first consider the document recognition stage at which classification is performed. 4.1.1 Document recognition stages Document classification can be performed at various stages of document processing. The choice of document features is constrained by the document recognition stage at which document classification is performed.
Figure 5 shows a typical sequence of document recog-nition for mostly-text document images [21]. Block seg-mentation and classification identify rectangular blocks (or zones) enclosing homogeneous content portions, such as text, table, figure, or half-tone image. Physical layout analysis (also called structural layout analysis or geometric layout analysis ) extracts layout structure: a hierarchical description of the objects in a document image, based on the geometric arrangements in the im-age [54]. For example, WISDOM++ uses six levels of layout hierarchy: basic blocks, lines, sets of lines, frame 1, frame 2, and page [21]. Logical layout analysis (also called logical labeling ) extracts logical structure: a hier-archy of logical objects, based on the human-perceptible meaning of the document contents [54]. For example, the logical structure of a journal page is a hierarchy of log-ical objects, such as title, authors, abstract, and sections [37].

Document classification can be performed at various recognition stages, as shown in Table 2. The choice of recognition stage depends on the goal of document clas-sification and the type of documents. 4.1.2 Choice of document features We characterize document features using three catego-ries adapted from those discussed in [12]: image features, structural features and textual features. Image features are either extracted directly from the image (e.g. the density of black pixels in a region) or extracted from a segmented image (e.g. the number of horizontal lines in a segmented block). Image features extracted at the level of a whole image are called global image features; image features extracted from the regions of an im-age are called local image features. Structural features (e.g. relationships between objects in the page) are ob-tained from physical or logical layout analysis. Textual features (e.g. presence of keywords) may be computed from OCR output or directly from document images. Some classifiers use only image features, only structural features, or only textual features; others use a combina-tion of features from several groups.

The classifiers that use only image features are fast since they can be implemented before document layout analysis. But they may be limited to providing coarse classification, since image features alone do not cap-ture characteristic structural information. More elab-orate methods are needed to verify the classification result.

Structural features arenecessarytoclassifydocuments with structural variations within a class. However, there is a risk to using high-level structural features: these rely on the results produced by physical layout analysis, a complex and error-prone process. Some classifiers ob-tain document layout information from the segmen-tation results produced by commercial OCR systems [3,12,34].

Most of the surveyed systems use a combination of physical layout features and local image features; this provides a good characterization of structured images. The classification is done before logical labeling, allow-ing the classification results to be used to tailor logi-cal labeling. For example, Bagdanov and Worring use physical layout features to classify the document, and then adapt the logical labeling phase to the document class [3].

Document classification using logical structural fea-tures is expensive since it needs a domain-specific log-ical model for each type of document. Early systems use manually-built logical models for each class [33]. The current trend is to learn models automatically from labeled samples [21,34]. However, document labeling is labor intensive, since logical meanings must be assigned to the physical layout objects in each training document.
Classification using textual features is closely related to text categorization inInformationRetrieval [49]. Purely textual measures, such as frequency and weights of key-words or index terms, can be used on their own, or in combination with image features. Textual features may be extracted from OCR results which may be noisy [13, 28]. Alternatively textual features may be extracted di-rectly from document images [51]. Techniques are be-ing developed for classification based on OCR results from low-quality images. These include n -gram-based text categorization to reduce the effect of OCR errors [11] and morphological analysis [30]. The effects of noisy OCR results on classification performance are noticed and considered in the updated OfficeMAID system [15, 24]. 4.1.3 Document features used in selected classifiers We now describe the document features used in selected document classifiers. This elaborates on the summary in Table 2.

Shin et al. [51] measure document image features directly from the unsegmented bitmap image. The doc-ument features include density of content area, statis-tics of features of connected components, column/row gaps and relative point sizes of fonts. These features are measured in four types of windows: cell windows, horizontal strip windows, vertical strip windows and the page window.

Eglin and Bres [19,20] measure spatial positions of segmented blocks, and use the results of functional label-ing. Functional labeling is a special case of logical label-ing, which doesn X  X  require information dependent on document types. Functional labeling uses texture fea-tures of the text blocks, including complexity and visi-bility.
 Spitz and Maghbouleh [53] use Character Shape Codes for content-based document classification. Char-acter Shape Codes rely on the gross shape and loca-tion of character images with respect to their text lines. Alphabetic Character Shape Codes are aggregated into WordShapeTokens. TheWordShapeTokens aretreated like keywords, and the frequency of their occurrences in each document is counted. 4.2 Feature representation Document features extracted from each sample docu-ment in a classifier can be represented in various ways, such as a flat representation (fixed-length vector or string), a structural representation, or a knowledge base. Document features that do not provide structural infor-mation are usually represented in fixed-length feature vectors. Features that provide structural information are represented in various formats as summarized in Table 2. 4.2.1 Recommendations for choosing feature Different classes of documents have different charac-teristics so they require different representation tech-niques. Diligenti et al. [16] discuss the effects of various formats of feature representation. They claim that a flat representation does not carry robust information about the position and the number of basic constituents of the image, whereas a recursive representation preserves relationships among the image constituents.

Watanabe [60] recommends using certain types of fea-ture representations for each of the five categories of structured documents shown in Table 3. Watanabe also gives the following guideline for the selection of a fea-ture representation: The simpler, the better. If the doc-ument can be represented using a list, then use a list because of higher processing efficiency, easier knowl-edge definition and management. Similarly, a tree rep-resentation is better than a graph representation due to its relative simplicity. A rule-based representation is powerful; however, it is complex and the interpretation phase takes longer.
The choice of a feature representation is also con-strained by the kind of class model and classification algorithm that is used. 4.2.2 Feature representations used in selected classifiers An overview of the use of feature representations is given in Table 2. We now describe a few of these repre-sentations in detail.

The XY-tree representation is a well-known approach for describing the physical layout of documents [39]. The root of an XY-tree is associated with the whole docu-ment image. The document is split into regions that are separated by white spaces. Horizontal and vertical cuts are alternately performed. Each tree node is associated with a document region. A modified XY-tree (MXY tree) is used in some classification systems; a region can be subdivided using either white spaces or lines [1,5,16]. Each node of the MXY tree contains a feature vector describing the region associated with the node. A disad-vantage of an XY tree (or MXY tree) representation is that it can be strongly affected by noise and document skew [16].

Graph representations are used in some classification systems. Liang and Doermann represent document lay-out using a fully connected Attributed Relational Graph [34]. Each node corresponds to a segmented block on a page, and it also corresponds to a logical compo-nent. An edge between two nodes represents a spa-tial relation between the two corresponding blocks in the image. The spatial relation is decomposed into rela-tions between vertical and horizontal block edges. The Attributed Relational Graphs in [2,3] are not fully con-nected. They model the relations between neighboring text zones only. Each node corresponds to a text zone in the segmented document image. The presence of an edge between two nodes indicates a Voronoi neighbor relation.

Several authors use fixed-length vectors as a feature representation. Interval encoding encodes region layout information in fixed-length vectors [26]. The block-seg-mented image is partitioned into an m  X  n grid. Each cell in the grid is distinguished as a text bin or a white space bin. Each row is represented as a fixed length vec-tor, recording how far each text bin is from a white space bin. Cesarini et al. [12] encode an MXY tree into a fixed-length vector. The vector represents the occurrences of tree patterns consisting of three tree nodes.
Various feature representations are used in knowl-edge-based systems. For example, layout structures are represented in a first-order language, where attributes (e.g. height and length) are used to describe proper-ties of a single layout component, while relations (e.g. contain, on-top) are used to express interrelationship among layout components [21]. Attributes and relations can be both symbolic and numeric. 4.3 Class models and classification algorithms Class models define the characteristics of the document classes. The class models can take various forms, includ-ing grammars, rules, and decision trees; the class models are trained using features extracted from the training samples. They are either manually built by a person or automatically built using machine learning techniques. Class models and classification algorithms are tightly coupled, so we discuss them together. A class model and classification algorithm must allow for noise or uncer-tainty in the matching process. We begin by reviewing traditional statistical and structural pattern classification techniques that have been applied to document classifi-cation. 4.3.1 Statistical pattern classification techniques There are many traditional statistical pattern classifica-tion techniques, such as Nearest Neighbor, decision tree, and Neural Network [18,29]. These techniques are rel-atively mature and there are libraries and classification toolboxes implementing these techniques. Traditional statistical classifiers represent each document instance with a fixed-length feature vector; this makes it difficult to capture much of the layout structure of document images. Therefore, these techniques are less suitable for fine-grained document classification [3].

Decision trees provide semantically intuitive descrip-tions of how decisions are made, and can have good performance with limited number of training samples [45]. Shin et al. [51] use a decision tree for document classification.

Neural Networks have been successfully used in many pattern recognition applications. A Multi-Layer Percep-tron is a type of Neural Network that has advantages concerning decision speed and generalization capacity [23]. Multi-Layer Perceptrons have been used for docu-ment classification [12,23].

Eglin and Bres [19] use a linear combination classifier for coarse-grained document classification. The linear function is the weighted sum of correlation coefficients between the input image and the reference image for each class.

A Hidden Markov Model (HMM) is a powerful tool for probabilistic sequence modeling [27]. It is viewed as a particular case of Bayesian networks [6]. An HMM is robust, suitable for handling uncertainties and noise in document image processing [32]. Hu et al. [26] use a top-to-bottom sequential HMM to classify documents. The HMM states correspond to the vertical regions of a document, and the observations are the cluster centers of interval encoding. 4.3.2 Structural pattern classification techniques In this section, we discuss traditional structural classi-fication techniques [43], as well as those extending tra-ditional statistical classification techniques to deal with structural feature representations. These techniques have higher computational complexity than statistical pattern recognition techniques. Also, machine learning techniques for creating class models based on struc-tural representations are not yet standard. Many authors provide their own methods for training class models [1,16,33].

Decision trees can be extended to consider tree-based document representations [1,59]. A Document Decision Tree is used to classify documents [1]. The leaves of a Document Decision Tree contain labeled MXY trees, and the internal nodes contain common sub-trees ex-tracted from MXY trees. A Document Decision Tree is built through the application of insertion, descending, and splitting operations. Splitting decisions are based on sub-tree similarity matching. In related earlier work, a Geometric Tree is automatically created to classify busi-ness letters based on physical layout [14].

Baldi et al. [5] use a tree-based K Nearest Neighbor classifier to classify pages, where the distance between pages is computed by means of tree-edit distance. They use an algorithm proposed by Zhang and Shasha to com-pute the tree-edit distance [64].
 Diligenti et al. [16] propose the Hidden Tree Markov Model, an extension to HMM, to classify documents us-ing structural features. A Hidden Tree Markov Model with 11 states is trained for each class. The state transi-tions are restricted to a left-to-right topology. Based on the view that HMM is a special case of Bayesian net-works, the two main algorithms in Hidden Tree Markov Model (inference and parameter estimation) are de-rived from corresponding algorithms for Bayesian net-works.

Graph matching is a common tool in structural pat-tern recognition [9]. General graph matching is NP-hard, but various heuristic graph-matching techniques can be used. Graph matching is used in document classification [3,34]. Bagdanov and Worring [2,3] introduce statistical uncertainty into the graph matching. They use First Or-der Gaussian Graphs to model document classes; these are extensions of First Order Random Graphs proposed by Wong et al. [63]. First Order Gaussian Graphs use continuous Gaussian distributions to model the densi-ties of all random elements in a random graph instead of the discrete densities used by Wong et al. A First Order Gaussian Graph for each class is trained based on hier-archical entropy minimization techniques. Classification is done by computing the probability that an Attributed Relational Graph is an outcome graph of a First Order Gaussian Graph. 4.3.3 Knowledge-based document classification A knowledge-based document classification technique uses a set of rules or a hierarchy of frames encoding expert knowledge on how to classify documents into a given set of classes. This is described as an appealing, natural way to encode document knowledge [3]. The knowledge base can be constructed manually or auto-matically. Manually built knowledge-based systems only perform what they were programmed to do [33,55]. Sig-nificant efforts are required to acquire knowledge from domain experts and to maintain and update the knowl-edge base. Also it is not easy to adapt the system to a different domain [49]. Recently developed knowledge-based systems learn rules automatically from labeled training samples [21,62]. Rule learning is discussed fur-ther in Sect. 4.4. 4.3.4 Template matching Template matching is used to match an input document with one or more prototypes of each class. This tech-nique is most commonly applied in cases where docu-ment images have fixed geometric configurations, such as forms. Matching an input form with each of a few hundred templates is time consuming. Computational cost can be reduced by hierarchical template matching [41,46]. Byun and Lee [10] propose a partial matching method, in which only some areas of the input form are considered. Template matching has also been applied to broad classification tasks, with documents from vari-ous application domains such as business letters, reports, and technical papers [31]. The template for each class is defined by one user-provided input document, and the template does not describe the structure variability with the class. Therefore, the template is only suitable for coarse classification. 4.3.5 Combination of multiple classifiers Multiple classifiers may be combined to improve classifi-cation performance [25]. The OfficeMAID system con-sists of two competing classifiers and a neural net voting mechanism [15,61]. One classifier uses a linear statisti-cal method, based on word and layout information of certain keywords. The other classifier is based on rules, employing linguistic features such as text patterns and morphological information. Experimental results show that the performance of the voting method is higher than that of either of the two single classifiers. H X roux et al. [23] implement three classifiers for form classification: K Nearest Neighbor, Multi-Layer Perceptron and tree matching. K Nearest Neighbor and Multi-Layer Percep-tron use image features as input. The tree matching uses structural features based on physical layout. Possible strategies for combining these classifiers include hier-archical combination, and parallel classifier application followed by voting. 4.3.6 Multi-stage classification A document classifier can perform classification in mul-tiple stages, first classifying documents into a small number of coarse-grained classes, and then refining this classification. Maderlechner et al. [36] implement a two-stage classifier, where the first stage classifies documents as either journal articles or business letters, based on physical layout information. The second stage further classifies business letters into 16 application categories accordingtocontent informationfrom OCR. TheOffice-Maid system also implements a two-stage classification [15]. The first stage identifies business letters from differ-ent senders [14] and the second stage classifies message types [61]. Classification performed in multiple stages requires multiple class models and classification algo-rithms. Most surveyed systems use single-stage classifi-cation.

This concludes our discussion of class models and classification algorithms. 4.4 Learning mechanisms A learning mechanism provides an automated way for a classifier to construct or tune class models, based on observation of training samples. Hand coding of class models is most feasible in applications that use a small number of document classes, with document features that are easily generalized by a system designer. For example, Taylor et al. [55] manually construct a set of rules to identify functional components in a document and learn the frequency of those components from train-ing data. However, manual creation of entire class mod-els is difficult in applications involving a large number of document classes, especially when users are allowed to define document classes. With a learning mechanism, the classifier can adapt to changing conditions, by updating class models or adding new document classes.

The entire class model may be learned, or aspects of a manually defined class model may be tuned dur-ing learning. The last column of Table 2 describes the automated aspects of classifier construction, for each surveyed approach.

Methods for automatically learning traditional sta-tistical models are well developed and there are many software packages available. Shin et al. [50] use OC1, an off-the-shelf decision tree software package to con-struct decision trees automatically. For some statistical models, training samples are used to tune parameters of the model [5]. Neural Network models typically in-volve manual specification of network topology; design samples are used to iteratively update the weights [12]. Hidden Markov Models typically involve manual speci-fication of the structure of the model for each class; the probabilities used in each model are learned [26].
For structural models and knowledge-based models, automatic learning is complex, and learning methods are not standardized. In earlier work, class models for a small set of classes are created manually. However, as shown in Table 2, recently developed classifiers exhibit a trend toward increasing automation in the construc-tion of class models. Esposito et al. [21] use Inductive Logic Programming to induce a set of rules from a set of labeled training samples.

There are challenges in automatically learning mod-els from training samples. To generalize class models well, a sufficient number of well labeled training sam-ples are necessary. Wnek [62] mentions that correct and representative labeled samples are crucial for the qual-ity of learning rules. Providing sufficient training data for learning can be expensive. Baldi et al. [5] propose a method to expand the training set: new labeled samples are created by modifying the given labeled samples to simulate distortions occurring in segmentation. The dis-tortions are modeled with tree grammars. The Winnow algorithm [7,35] can be used on-line to incrementally update class models [40]. The on-line nature of this algo-rithm makes the system more flexible and requires less time in the learning phase.

Class models differ intheamount of retrainingneeded when document classes change. When new document classes are added or existing document classes are changed, a Neural Network must be retrained from scratch, re-estimating all the weights in the network. In contrast, a Hidden Markov Model requires less training when the set of classes changes. It is not necessary to retrain all the Hidden Markov Models since each class has its own class model. Only the models for new or changed classes are trained on the document samples belonging to those classes. This localized retraining is important since many classifiers deal with a relatively large number of classes, and classes normally vary over time [16].

This concludes our discussion of the classifier archi-tecture, with the four aspects: (1) document features and recognition stage, (2) feature representations, (3) class models and classification algorithms, and (4) learning mechanisms. 5 Performance evaluation Performance evaluation is a critically important com-ponent of a document classifier. It involves challenging issues, including difficulties in defining standard data sets and standardized performance metrics, the diffi-culty of comparing multiple document classifiers, and the difficulty of separating classifier performance from pre-processor performance.

Performance evaluation includes the metrics for evaluating a single classifier, and the metrics for compar-ing multiple classifiers. Most of the surveyed classifica-tion systems measure the effectiveness of the classifiers, which is the ability to take the right classification deci-sions. Various performance metrics are used for classi-fication effectiveness evaluation, including accuracy [1, 3], correct rate [34], recognition rate [10], error rate [55], false rate [21], reject rate [12], recall and precision [4,28]. The significance of the reported effective performance is not entirely standard, since some classifiers have reject ability while others do not, and some classifiers output a ranked list of results [1,26,62], while others produce a single result. Standard performance metrics are neces-sary to evaluate performance.

Document classifiers are often difficult to compare because they are solving different classification prob-lems, drawing documents from different input spaces, and using different sets of classes as possible outputs. For example, it is difficult to compare a classifier that deals with fixed-layout documents (forms or table-forms) to onethat classifies documents withvariablelayouts (news-paper or articles). Another complication is that the num-ber of document classes varies widely. The classifiers use as few as 3 classes [21] to as many as 500 classes [62], and various criteria are used to define these classes. Also many researchers collect their own data sets for training and testing their document classifiers. These data sets are of varying size, ranging from a few dozen [10,26,34], or a few hundred [1,4], to thousands of document instances [62]. The sizes of training set and test set affect the classi-fier performance [18]. These factors make it very difficult to compare performance of document classifiers. The authors of WISDOM++ lead in the right direction by making data available on line (http://www.di.uniba.it/  X  malerba/wisdom++/). Nattee and Numao [40] use the data provided by WISDOM++ and add their own data to test their classification system.

To compare the performance of two classifiers, a stan-dard data set providing ground-truth information should be used to train and test the classifiers. The University of Washington document image database (UWI, II, and III) is one source of ground truth data for document image analysis and understanding research [44]. UW data is used for text categorization in [28,53]. Spitz and Maghbouleh [53] conclude that UW data is far from optimal for document classification, since it has a small number of documents from a relatively large number of classes. The set of classes defined for UW data by Spitz and Maghbouleh is one of many possible types of class definition for this data set. Finland X  X  MTDB Oulu Doc-ument Database defines 19 document classes and pro-vides ground truth information for document recogni-tion [47]. The number of documents per class ranges from less than ten up to several hundred. The docu-ments in this database are diverse, and assigned to pre-defined document classes, making this database a useful starting point for research into document classification. For example, the Oulu database is used in [19]. Fur-ther discussion of standard datasets may be found in the reports of the DASO2 working group [52]. They raise an interesting issue concerning the huge collection of doc-uments in on-line Digital Libraries. How can document classification research make use of these documents? And how will document classification contribute to the construction of Digital Libraries?
It is difficult to separate classifier performance from pre-processor performance. The performance of a clas-sifier depends on the quality of document processing performed prior to classification. For example, classi-fication based on layout-analysis results is affected by the quality of the layout analysis, by the number of split and merged blocks. Similarly, OCR errors affect classification based on textual features. In order to com-pare classifier performance, it is important to use stan-dardized document processing prior to the classification step. One method of achieving this is through use of a standard document database that includes not only labeled document images, but also includes sample re-sults from intermediate stages of document recognition. Construction of such databases is a difficult and time-consuming task. 6 Conclusions We summarize the document classification literature along three components: the problem statement, the classifier architecture, and performance evaluation. There are important research opportunities in each of these areas.

The problem statement is characterized in terms of the document space and the set of document classes. We need techniques for more formally specifying docu-ment classification problems. Current practice is to de-fine each class via an informal English description and/or via sample documents. Neither gives a complete or pre-cise definition of a document class. The ill-defined na-ture of the problem statement hampers many aspects of classifier development.

The classifier architecture includes four aspects: doc-ument features and recognition stage, feature repre-sentations, class models and classification algorithms, and learning mechanisms. We need techniques to better understand the effects of these four aspects. In current classifiers, these four aspects are so closely bound to-gether that it is nearly impossible to evaluate any one of these aspects independently of the others. Our ability to make advances in classifier-construction technology de-pends on being able to investigate the effects of changing one of these aspects of classifier architecture.
Advances in performance evaluation techniques for document classifiers are needed. Existing standard doc-ument databases (University of Washington and Oulu) have been used to test document classifiers. There is need for larger standard databases, with many docu-ments for each document class. These databases should includenot onlylabeleddocument images, but alsointer-mediate results from document recognition. This would allow document classifiers to be tested under the same conditions, classifying documents based on the same document-recognition results. Currently, it is difficult to separate classifier performance from the performance of preceding document-recognition steps.
 References
