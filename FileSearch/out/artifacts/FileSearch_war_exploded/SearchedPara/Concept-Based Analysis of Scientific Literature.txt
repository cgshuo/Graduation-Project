 This paper studies the importance of identifying and cate-gorizing scientific concepts as a way to achieve a deeper un-derstanding of the research literature of a scientific commu-nity. To reach this goal, we propose an unsupervised boot-strapping algorithm for identifying and categorizing men-tions of concepts. We then propose a new clustering algo-rithm that uses citations X  context as a way to cluster the ex-tracted mentions into coherent concepts. Our evaluation of the algorithms against gold standards shows significant im-provement over state-of-the-art results. More importantly, we analyze the computational linguistic literature using the proposed algorithms and show four different ways to sum-marize and understand the research community which are difficult to obtain using existing techniques.
 I.2.6 [ Artificial Intelligence ]: Learning Concept recognition; clustering; bootstrapping algorithm
The dramatic growth in scientific communities and the publication trace they generate brought with it the chal-lenge of understanding a scientific community  X  identifying important concepts, trends, key techniques, applications and the relations between them  X  by analyzing the publication trace. Earlier studies of this question made use of bibliomet-rics techniques, mostly considering citation graphs [12] and topic models, forming crude topic clustering based on con-textual cues [1, 16]. However, these methods cannot address some key questions such as  X  X hat methods were developed to solve a particular problem? X ,  X  X ow did these change over the years? X , and  X  X hat applications have matured enough to be used as components of other applications? X 
In this paper, we propose that if we want to achieve a deeper understanding of a scientific community from the paper trace generated by the community, there is a need to better analyze the text itself; there is a need to identify mentions of scientific concepts, categorize them and cluster them into coherent concepts , and study the relations between concepts of various categories. We develop methods to do that, and our evaluation on the ACL text collection reveals interesting observations and insights on the Computational Linguistics scientific community.

The most basic component of our model is an unsuper-vised algorithm that identifies mentions of concepts; in this paper we focus on two categories of concepts: TECHNIQUE s and APPLICATION s. For example, in the sentence  X  X e apply support vector machines on text classification. X  our goal is to identify  X  X upport vector machines X  as a TECHNIQUE and  X  X ext classification X  as an APPLICATION . We define the concept ex-traction problem in a way that is similar to the named entity recognition problem. This instance of the problem was also studied earlier in [5]. Our first contribution is a bootstrap-ping algorithm [20, 3] that makes use of a small number of per-category pre-specified seeds. The algorithm is used to induce a decision list of features for each category, that is used, in turn, to annotate mentions as belonging to the cat-egory and then to extract additional features based on the newly annotated mentions. By iteratively repeating these two steps, we propagate information from a small number of seeds and learn a robust mention identifier. Our results indicate a significant improvement over earlier results in [5].
However, for the purpose of studying a scientific commu-nity, we argue that it is essential to attend to the significant variability in the way authors express a given concept. Our notion of a  X  X oncept X  needs to capture both minor differ-ences in expressing the concept ( X  X VMs X  and  X  X upport vec-tor classifiers X ) and levels of granularity (such as  X  X upport vector machines X  and  X  X arge margin classifiers X ). Existing techniques such as topic models [1] do not support the abil-ity to categorize mentions and, as we show, do not allow one to generate tight enough clusters or provide the level of granularity needed to support a careful analysis of the scien-tific literature. Similarly, naive mention clustering based on lexical similarity also does not support grouping mentions to semantically coherent concepts. Our second contribu-tion is a new clustering algorithm that makes use of citation contexts as a way to group concept mentions to meaningful coherent concepts. Given a citation to paper p , we assume that two mentions m 1 ,m 2 of a given category (e.g., TECH-NIQUE ) appear in the citation context in a way that indicates both m 1 and m 2 are described in p . In this case, we assume a degree of similarity between the two mentions. With this as the basis metric for our clustering algorithm, we show that we can group mentions such as  X  X VM X ,  X  X upport vector machine X , and  X  X aximal margin classifiers X , which clearly represent the same concept, but are expressed using very different surface forms.

We quantify the performance of our techniques relative to gold standards and then move to study their impact on the analysis of the Computational Linguistics literature. We present four different ways to summarize our understanding of the community. First, we consider scientific trends in the community over a period of thirty years. We compare our context-citation driven concept formation method to topic models and lexical clustering and exhibit it X  X  advantage in identifying trends and accurately pinpointing the emergence of techniques such as topic modeling and of applications such as sentiment analysis. We also quantify the advantage of our clustering method by evaluating it as a method for forecast-ing the change in scientific trends. The significance of cat-egorizing concepts to TECHNIQUE and APPLICATION is shown by several studies in which we exhibit the ability to iden-tify what techniques contribute to a given application and how this has changed over time; for example, we accurately identify the emergence of phrase-based methods in machine translation, and that of conditional random fields as a key techniques for tasks such as named entity recognition.
Several components of our approach have been seen in earlier work. Our mention extraction algorithm addresses the need to categorize mentions in an unsupervised way by using a Bootstrapping algorithm, building on successful ear-lier applications of this approach to word sense disambigua-tion [20] and named entity classification [3, 8]. Closer to our application, [19] used bootstrapping to learn general-ized names, such as the names of diseases and infectious agents. They argue that identifying generalized names is much harder than identifying conventional names. Our con-cepts are also generalized names but with greater ambiguity than in the aforementioned case. The work on semantic class induction (e.g., [14, 15, 17]) is also related to ours. [6] proposed a domain-specific semantic class tagger that makes use of a bootstrapping algorithm. In their problem, a given noun phrase can be labeled with different class labels in dif-ferent contexts, in a setting that is similar to our concepts. Their work is done in the domain of veterinary medicine.
We apply our techniques to study the scientific literature of a research community. Topic models [1, 4] is perhaps the most commonly used technique recently. Many variants of topic models have been proposed in recent years, from the basic model to those that deal with phrases and those that take temporal aspects into account [2, 18].

The key advantage of our approach over topic models is that we deal with different types of concepts and argue that this is necessary to the type of refined analysis we per-form. Topic models will not distinguish TECHNIQUE from APPLICATION . Moreover, topic model do not produce tight enough cluster for our application; as we show, topics typi-cally contain several different concepts and it is thus harder to summarize a specific concept by just considering the clus-ters produced using topic models. Citation based methods are also used in this domain: [11] make use of  X  X iting sen-tences X  in scientific papers to analyze research trends and understand the impact of research. However, rather than identifying scientific concepts in the whole collection as we do, their analysis is based on a specific paper. Finally, [5] present work that is used as our starting point in this paper: they extract three types of concepts, focus, technique, and application domain by applying a bootstrapping algorithm, as a way to analyze the dynamics of research communities. In our experiments, we use their results as the baseline for our mention extraction approach. However, they use deeper level of sentence analysis  X  a dependency graph of each sen-tence, while we only use shallow parsing; nevertheless, our mention identification algorithm performs significantly bet-ter. Moreover, they only extract mentions of concepts, and does it only from abstracts, while we use full articles and use our identified mentions to further generate concepts which contribute significantly to the refined analysis we present.
This section presents the algorithmic components of our model, the concept mention extraction and the citation-context based concept clustering.
The proposed bootstrapping algorithm is based on the framework presented in [20, 3]. It learns a decision list for each category y  X  { TECHNIQUE , APPLICATION } . A decision list contains representative features for the corresponding category, and is used to predict the category of mentions in unseen data.

Pre-Processing: The training corpus is passed through a chunker [10]. Let X be all noun phrases (NPs) derived from the chunker. We only consider noun phrases as concept mention candidates.

Features: We extract six types of features for each noun phrase. These features are then matched with features in the decision lists to determine if the noun phrase belongs to any category. Note that several of the features we extract here are common in the bootstrapping algorithm for named entity recognition problem [3]. The following features are used, 1. The words and bigrams of the NP. (e.g., the NP  X  X ootstrap-ping algorithm X  has three features unigram:bootstrapping , unigram:algorithm , and bigram:bootstrapping-algorithm .) 2. The words and bigrams next to the NP. (e.g., in a sen-tence  X  X e use bootstrapping algorithm to extract ...  X , the NP  X  X ootstrapping algorithm X  has features context-uni:l:use , context-uni:r:to , context-bi:l:we-use , and context-bi:r:to-extr-act .) 3. A feature indicates if the spelling of the NP is all capitals. 4. The closest verb before the NP, and it should be after the previous NP.

Initialization: We seed the decision lists using a small set of feature seeds which are extracted from some concept mentions of each category. The complete set of seeds used in our experiments is shown in Table 1. Then we go through all the documents once to annotate NPs. An NP is annotated as category y if it has at least one feature in the decision list of y (i.e. the seed set of category y ). These newly labeled NPs are added into a list L y , which maintains the labeled noun phrases of category y throughout our algorithm.
Learning: The bootstrapping algorithm iterates through the following three steps: 1. Compute feature scores. The score of a feature f in category y is defined by score y ( f ) = | x has f,  X  x  X  L features which are frequently associated with labeled NPs have higher scores than those that are frequent but their corresponding NPs are not labeled. We calculate scores for all the features associated with labeled NPs in this step. Features which contain any stop word are removed. 2. Select new features. The feature scores are used to select (up to) top k features which are not in the current decision list and have scores above a pre-specified threshold, t , for each concept y . That is, to ensure we select the features with high precision predictive quality, we not only just select top k features, but also set a threshold of the score. Moreover, if the number of selected features is less than n , we decrease the threshold by t . This policy ensures it only includes accurate features into the decision lists in the early iterations, and utilizes low-scored features in the later iterations. We find that some features which have low scores are actually useful. Using a small n allows a cautiously selection of features but may increase training time. 3. Annotate noun phrases. We now use the updated decision lists to annotate NPs in the corpus. An NP x which hasn X  X  been annotated in the previous iterations is labeled as category y if the features of x match at least r features in the decision list of category y .
Recognizing different mentions that refer to the same con-cept is essential to support meaningful analysis of a commu-nity. Some well-known clustering algorithms (e.g., k -means) do not serve our needs here. Because we don X  X  know how many clusters will be there; our goal is to group together mentions of the same concept. Pre-specifying the number of clusters may not generate tight enough clusters.

In this section, we introduce a citation-context based clus-tering algorithm, which utilizes both citation context infor-mation and lexical similarity to group the extracted concept mentions to coherent concepts. The following discussion fo-cuses on a given type of concept (e.g. TECHNIQUE ). We first define the lexical similarity between two mentions m 1 and m where l i is the number of words in m i and l i,j is the number of words common to m i and m j . The similarity between two clusters of mentions C 1 and C 2 is where  X  1 , 2 (  X  ) is the number of mentions m  X  C 1 msim ( m,m j ) &gt;  X  , for some m j  X  C 2 . This similarity repre-sents the proportion of mentions in C 1 which have at least one similar mention in C 2 . The clustering algorithm for a given concept is summarized in Algorithm 1. The key step in computing this similarity is that we initialize our clusters to contain mentions that appear in the context of a citation to the same paper. We note that it is possible to consider the context in which a mention occurs, but we have found that considering only citation context is sufficient, and more ro-bust. The intuition is that if multiple mentions are referred to as introduced in paper p , this provides a strong clue of these mention representing the same concept. If cit( m ) is the set of citations that follow the mention of m in an arti-cle, then we initialize m 1 and m 2 to be in the same cluster if cit ( m 1 )  X  cit ( m 2 )  X   X . That is, we only use mentions that are followed up by a citation somewhere in our training data, and use the fact that they are followed by the same Algorithm 1 Concept clustering (CitClus) 1. Group mentions based on the citation context. It generates C  X  { C 1 ,  X  X  X  ,C n } , such that  X  m k ,m l cit ( m k )  X  cit ( m l )  X   X  2. Clean up clusters. For each mention m in C i , remove m from C i if there exists a C j such that | C j | &lt; | C | msim ( m,m k ) &gt;  X  | &lt; | msim ( m,m l ) &gt;  X  | ,  X  m  X  m l  X  C j . 3. Merge the clusters in C to form the final clustering C
C 0  X   X  while C is not empty do end while citation as a way to initialize them into the same cluster. Not only this helps grouping mentions into semantically co-herent concepts, it also cleans up the prediction results of our bootstrapping mention extractor. In our experiments, we set  X  = 1.

The initialization results is a set of clusters, each repre-senting a cited paper with its cited concept mentions. We further clean up the clusters from mentions that are wrongly cited or do not represent important concepts in the cited paper. To do that we remove a mention m from cluster C i if there exists another cluster C j , which has fewer men-tions than C i but contains more similar mentions of m than C i does. This step prevents us from combining different concepts into one cluster in the next step. Finally, we re-cursively merge the similar clusters based on the similarity between two clusters, which is defined in equation (2).
We evaluate the performance of our algorithmic compo-nents in this section.
We evaluate our approach on the ACL Anthology Net-work (AAN) Corpus [13], which contains 18,292 full text articles. Because the text documents are converted from pdf files, full text is usually noisy. For the mention extrac-tion evaluation we train on 11,005 abstracts along with the corresponding titles. For evaluation, we use 474 abstracts and titles which were manually annotated by [5], and mea-sure the precision and recall scores of each concept type. We randomly sample 50 abstracts from the test set as the de-velopment set. This set is used to select the parameters of the proposed mention extractor and determine the number of iterations. The parameters used in our mention extractor We initialize the decision lists using 5 feature seeds for TECH-NIQUE and 9 feature seeds for APPLICATION . For APPLICATION we seed with names of ACL applications, while for TECH-NIQUE we use generic terms such as  X  X lgorithm X  and  X  X odel X  (see Table 1). At test time we remove stop words from the predicted noun phrases to obtain cleaner results.

We compare our proposed approach with the results of [5], which applies a bootstrapping algorithm to a similar prob-lem, but used a dependency graph representation of sen-Table 1: The complete set of seed features which are used in all the experiments for the concept mention extractor. Table 2: Comparing the proposed mention extraction algo-rithm with [5].
 Table 3: A comparison of clustering approaches. Cluster-ing performance is evaluated on manually annotated gold clusters, using variation of information (eq. (3)). tences. That is, instead of using n-gram features as seeds, they use patterns of dependency (sub)graphs as seed pat-terns. In each bootstrapping iteration, top k patterns in each category are included into their decision lists. The com-parison between their method and the proposed approach is shown in Table 2. Note that when comparing the initial de-cision lists, which only use the seeds, the F1 scores of both approaches are similar. However, our seeds have a much higher precision (and lower recall) than the seed patterns used by [5]. The reason is that we use specific names as seeds, while the dependency tree patterns are more general. However, after running the algorithm, our approach clearly outperforms their method in terms of both precision and recall and leads F1 score by 11% in TECHNIQUE and 8% in APPLICATION . Our analysis indicates that this is due to the careful policies of feature selection and the adaptation step that allows us to continue selecting tail features.
To evaluate the quality of concept clustering, we randomly sampled 1000 full text of articles from the period 2000 X 2011 in the AAN corpus and manually clustered the mentions recognized by our concept mention extractor, separately for each concept type. To reduce the number of mentions and get a cleaner list of mentions, we only use the mentions which appear, somewhere in the collection, followed by a citation. We further remove clusters with less than four different mentions to focus on important concepts. It results in 17 clusters for APPLICATION and 19 clusters for TECHNIQUE in that time period.
 We compare our citation-context based Algorithm 1 (Cit-Clus) with a baseline which only utilizes lexical similarity between mentions. The baseline is derived by running Algo-rithm 1 without using the information of citation context. That is, it doesn X  X  make use of the first step of Algorithm 1 and it simply views each mention as a single cluster in the beginning. The mentions with high similarity will be merged in the third step of Algorithm 1. We call this baseline Lex-Clus. In both CitClus and LexClus, we set the threshold hold  X  of similarities (equation (1) and (2)) to be 0.5.
We use a standard method for clustering evaluation to quantify our results. A clustering is compared with the gold clustering by selecting the cluster which covers most of the mentions in each gold cluster, and then computing the varia-tion of information (VI) [7] between the gold clustering and the selected clusters. The variation of information of two clustering C and C 0 is defined as: where H ( C ) is the entropy associated with clustering C and I ( C , C 0 ) is the mutual information between the two cluster-ings. The lower V I ( C , C 0 ) is, the more similar C and C Table 3 compares LexClus and CitClus. We can see that CitClus clearly performs better than LexClus, especially on TECHNIQUE s. Carefully analyzing the emerging clusters re-veals, as expected, that the lexical similarity metric cannot capture semantic similarity; it doesn X  X  recognize the simi-larity between  X  X opic modeling X  and  X  X atent dirichlet allo-cation X ,  X  X uinlan X  X  c4.5 X  and  X  X ecision tree X , and  X  X aximal entropy classifier X  and  X  X ogistic classifier X . In contrast, these pairs of mentions are likely to cite the same paper, thus they will be clustered correctly by a citation-context based clustering algorithm. This also indicates why there was no need to consider other context of mentions; citation context provides a very robust clue.

Two experts in the ACL literature generate our gold stan-dard. The inter-annotator agreement is 0.81 for TECHNIQUE and 0.92 for APPLICATION . Most of inter-annotator disagree-ments are due to concept granularity problems. For exam-ple, should  X  X tructured prediction X  be clustered with SVM, CRF, or perceptron? If we have a single cluster representing structured prediction, we cannot analyze structured predic-tion accurately without knowing that the other concepts are closely related to it. In the second step of Algorithm 1, we attempt to remove the hierarchical concepts from the clus-ters in order to focus on lower level concepts. Further study of this would be interesting.
In this section we use our model to study several metrics that indicate an understanding of the research done in the computational linguistics community.
We use the clustered concepts to analyze trends of some important concepts. In this experiment, we use the model trained on abstracts in the previous experiment to extract concept mentions from 15,717 full text articles from the pe-riod 1990 X 2011. We then apply the two concept clustering algorithms, LexClus and CitClus to the extracted concept mentions. We further compare the trends derived by these two concept clustering algorithms with latent dirichlet allo-cation (LDA) [1] approach. For computing the clusters of LDA we remove all the stop words and numbers and low-ercase and stem every word of all the papers. We then use GibbsLDA++ [9] by setting the number of topics to K = 200,  X  = 50 /K , and  X  = 0 . 1. For a given query con-cept string, we first select the topic which gives the highest probability to this string. Following the method used in [4], we average  X  ij over i in each year, where  X  i is the distribu-tion over topics for document i , and j represents the selected topic. This value is analogous to the fraction of papers which discuss topic j . To compare LDA with the other two con-(a) The trends obtained from CitClus. The y -axis represents the percentage of papers. (b) The trends derived from LexClus which only uses lexical similarity to cluster mentions. (c) The results of LDA. The y -axis is the averaged probability of the selected topic over documents.
 Figure 1: A comparison of trends derived by three clustering approaches. Figure 2: The ratio between number of technique and ap-plication mentions for three concepts.
 Table 4: The predictive quality for four concepts showed in Figure 1. CitClus has lower relative errors in all cases. cept clustering approaches, we first find the concept cluster which contains the largest number of similar mentions of the query string, and call it the query cluster. We define a pa-per to be one that focuses on the query concept if most of the extracted mentions in it are also in the query cluster. Therefore, we can derive the fraction of papers which dis-cuss the query concept for each year. Figure 1 shows the trends of four representative concepts. Note that we do not distinguish the categories ( TECHNIQUE or APPLICATION ) here, since LDA does not label the topics.
 Figure 1a clearly shows that the curves obtained from Cit-Clus nicely capture several important trends. For example, the rise of support vector machine from late 1990s, a huge boost of topic modeling research in recent years, and the re-cent emergence of sentiment analysis. Comparing Figure 1a with Figure 1b, we see that the numbers of papers obtained by LexClus is much smaller than the numbers identified by CitClus. This results in some misleading trends, as shown for support vector machine and topic modeling. In partic-ular, the LexClus graph shows a sharp decrease in support vector machines starting in 2008 and a slight increase in topic modeling, only starting in 2010. Note that LexClus is a general case of not doing concept clustering at all. If we set a very high threshold to the lexical similarity, the result will be the same as counting the number of the query mention instead of counting the number of mentions in the query cluster. In this case, we will only get fewer papers in each year thus the resulting trends will still be misleading.
Although the LDA graph captures the growth of support vector machine and of sentiment analysis, most of the curves in Figure 1c are very different from those in Figure 1a. Since the selected topics may not represent the given concept only, but rather include other concepts as well, we can see, for example that the curve of topic modeling is already very high before this research area emerged. Carefully looking at the clusters we observed that the cluster corresponding to topic modeling also contains  X  X enerative model X , which is one reason for these inaccuracies.
We further quantitatively evaluate LexClus and CitClus by considering the predictive power of the resulting trends. For the four concepts in Figure 1, we ask whether we can predict the number of papers in year i , given the number of papers in the previous three years. We apply linear regres-sion to every three consecutive years and use it to predict the value in the fourth year. We omit from this statistics time periods that precede the development of the concept; for instance, topic modeling and sentiment analysis are almost zero before 2000 and this isn X  X  taken into account. Then the averaged relative errors for each concept is calculated by is the true value obtained from clustering, and j is the year which y i = 0 ,  X  i &lt; j . Note that we do not have a notion of ground truth, so each prediction  X  y i only compares with the true value y i derived by the corresponding clustering. The intuition is that the better the grouping of mentions into coherent concepts is, the more stable the trend graph is, the more accurate the prediction is. Table 4 shows the results. As expected, CitClus has a much lower relative error in most cases, indicating that trends obtained by CitClus are more predictive and stable.
Classifying concepts into two categories is useful in several ways. Because a concept may belong to different categories in different contexts, we can study a given concept by ana-lyzing, for example, the ratio between the number of times it appears as an application and the number of times it ap-pears as a technique. In Figure 2, we show three important concepts studied in the ACL community which exhibit very different behaviors. For the curve of support vector ma-chines, we plot it by the number of applications divided by the number of techniques. This curve stays at the bottom, indicating that support vector machines always serve as a technique in the ACL community. In contrast, since ma-chine translation is always a popular application, the ratio: number of techniques over applications always stays low. Fi-nally, the curve of part-of-speech tagging grows through the years. It indicates the maturity of this concept, as more and more people use part-of-speech tagging as a technique in recent years. However, there are still many papers aim at solving semi-supervised or unsupervised part-of-speech tag-ging problems, it still has a significant presence also as an application.

In addition to studying different categories within a given concept, another natural question is to consider how differ-ent concepts that belong to different categories interact with each other. Here we study the  X  X pply X  relationship between different types of concepts. More specifically, for a given application, we are interested in the techniques which have been applied to it. We first use the same method as in the previous experiments to select the papers that focus on the given application. Then, among all the technique concepts extracted from those papers, we picked the ten most popular concepts and analyzed the change in their usage over time. Figure 3a shows ten techniques used in machine translation and clearly captures the growth of phrase-based and min-imum error rate training in the last five years. In Figure 3b, decision tree and decision lists are shown to be popular for the named entity recognition problem early on, only to disappear after 2007. We can also see the obvious rise of conditional random fields after 2002 and it dominates other techniques after 2007.
This work proposed algorithmic tools for identifying, cat-egorizing and clustering mentions of scientific concepts. We showed that these tools can provide a rather deep under-standing and useful insights into the progress, changes and trends of research in the ACL community. This work opens up a range of questions from algorithmic issues for improv-ing our approaches to better deal with hierarchies, to under-stand concepts that are being used across communities. [1] D. Blei, A. Ng, and M. Jordan. Latent dirichlet [2] D. M. Blei and J. D. Lafferty. Dynamic topic models. [3] M. Collins and Y. Singer. Unsupervised models for [4] T. L. Griffiths and M. Steyvers. Finding scientific [5] S. Gupta and C. D. Manning. Analyzing the dynamics [6] R. Huang and E. Riloff. Inducing domain-specific [7] M. Meil  X a. Comparing clusterings by the variation of [8] C. Niu, W. Li, J. Ding, and R. K. Srihari. A [9] X.-H. Phan and C.-T. Nguyen. Gibbslda++: A [10] V. Punyakanok and D. Roth. The use of classifiers in [11] D. Radev and A. Abu-Jbara. Rediscovering acl [12] D. R. Radev, M. T. Joseph, B. Gibson, and [13] D. R. Radev, P. Muthukrishnan, and V. Qazvinian. [14] E. Riloff and J. Shepherd. A corpus-based approach [15] B. Roark and E. Charniak. Noun-phrase co-occurrence [16] Y. Sim, N. A. Smith, and D. A. Smith. Discovering [17] M. Thelen and E. Riloff. A bootstrapping method for [18] X. Wang, A. McCallum, and X. Wei. Topical n-grams: [19] R. Yangarber, W. Lin, and R. Grishman.
 [20] D. Yarowsky. Unsupervised word sense disambiguation
