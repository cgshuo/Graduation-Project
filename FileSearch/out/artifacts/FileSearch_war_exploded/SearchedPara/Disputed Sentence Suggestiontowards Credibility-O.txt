 Nowadays, the Web is a huge information resource. People can easily and freely obtain information through Web search engines and specific Web sites. However, the problem of Web information credibility is emerging [1,2].

The quality control of Web information is g enerally insufficient due to low publish-ing barriers. As a result, there is a large amount of incorrect and unreliable information on the Web that can have detrimental effects on users. For instance, Sillence et al. re-ported that there are more than 20,000 medical Web sites on the Web, but over half of them have not been checked by medical experts [1]. On the other hand, Nakaura et al. reported that a lot of Web users trust Web information to some extent [2]. In particular, for Web search engines, they also reported that more than 50% of search engine users perceive the search results to be somewhat credible.

Although Web information credibility is b ecoming more critical, conventional Web search engines still focus on searching for onl y popular and relevant information [3,4]. In addition, they provide few clues to judge information credi bility from Web search results. For instance, suppose that a user is searching for Web pages about effective diet . A lot of Web pages describe various diet methods. However, as in the case of the Atkins diet 1 , Web pages containing suspicious information are often listed with high rankings as popular and relevant Web search results, although there exists counter evidence against the information. Conventional Web search results provide only titles, a snippet, and URLs. This makes it difficult for users to check the credibility of such Web pages and obtain credible Web inform ation using the conventional Web search engines. In worst cases, if users are not skeptical about Web pages, they can be mislead by incorrect information without knowing it. Therefore, automatic tools for alerting users and helping them judge Web informatio n credibility are becoming increasingly necessary.

In this paper, to support users X  credibility judgment in the Web search process, we propose a novel type of query suggestion to make users aware of suspicious Web in-formation. When users issue queries to search for Web pages, our proposed system suggests some disputed sentences about the given queries. Fig.1-(a) and (b) show the case where a user issues keyword query effective diet to our Web search engine and to a conventional Web search engine, respectively. For this query, the conventional Web search engine suggested keywords effective diet plans , effective atkins diet , and so on, in Fig.1(b). As this example shows, conv entional query suggestion techniques focus on supporting users in making their intent clearer and making it easy to search for Web pages matching their intent. In addition, suggested queries are often keywords. On the other hand, our implementation suggested that some phrases such as  X  X iet is an effective treatment X  and  X  X he atkins diet is not any more effective than other diet strate-gies X  , were disputed by some Web pages, in Fig.1(a). We think that disputed sentence suggestion has the following advantages over conventional keyword query suggestion techniques for supporting use rs X  credibility judgment in the Web search process:  X  Users can clearly recognize the existence of suspicious statements on the Web even  X  Users can find some queries to collect cl ues for credibility j udgment of suspicious  X  Users can intuitively understand the meaning of disputed statements as opposed to The remainder of the paper is organized as follows. In the next section, we discuss related work. Section 3 describes our methods for collecting disputed sentences from the Web and ranking them. In Sections 4, we show the effectiveness of our system for supporting users X  credib ility judgment in the Web search process, comparing it to some query suggestion techniques. The las t section concludes the paper and outlines our future research directions. 2.1 Query Suggestion There has been a lot of work on query suggestion, including query expansion [5] and query substitution [6,7]. Cui et al. studied a method to expand given queries by con-sidering the gap between document space and query space [5]. Boldi et al. proposed a method to classify users X  query reformulation operations into generalization , special-ization , error correction ,or parallel move towards query substitution [6]. Kotov et al. addressed a framework to automatically generate potentially interesting questions from Web pages and suggest them in the Web search process, so that Web search engines can naturally lead the users to Web search results they want [7]. These approaches are very sophisticated. However, they still focus on only mismatching problems between users X  intent and given queries to improve users X  Web search experience. Even if some queries are suggested, it is still difficult to call users  X  attention to suspicious Web information or to support users in collecting clues for credibility judgm ent from the Web. 2.2 Measuring Web Information Credibility Some researchers have developed methods to measure the credibility of specific Web contents from specific credibility aspects. Especially, research aimed at measuring the credibility of contents on social networks like Yahoo Answers 2 and Twitter 3 is becom-ing more popular. Suryanto et al. focused on the expertise of answerers on Q&amp;A sites to evaluate answers posted on them [8]. Castillo et al. proposed a method to automatically judge the credibility level of news propaga ted through Twitter by analyzing tweets and re-postings about news [9]. To measure t he credibility of Web pages and Web search results from various viewpoints, some researchers have developed prototype systems [10,11]. These systems visualize scores of Web search results on the basis of impor-tant credibility criteria, such as popularity, c ontent typicality, and content freshness. If users want to briefly filter out non-credible Web information, measuring the credibility of Web information can be useful. 2.3 Supporting Users X  Credibility Judgment on the Web Some studies have focused on promoting and helping users judge credibility by them-selves. Pirolli et al. used W IKI D ASHBOARD , the system for visualizing edit histories of Wikipedia articles, to study how the system af fects users X  credibility judgments on those articles [12]. Ennals et al. developed D ISPUTE F INDER , a system that highlights suspi-cious sentences on Web pages users are browsing [13]. D ISPUTE F INDER can highlight only suspicious sentences in its database and it is not robust in a Web search process where different Web search results are shown for different queries. On the other hand, our system is robust. In addition, our system enables users to get an overview of sus-picious statements for given queries. Some researchers have developed methods to find alternative statements for checking the credibility of suspicious statements [14,15]. In this section, we describe our method for searching for disputed sentences about a given query on the Web and suggesting disputed sentences to users. Our system works as follows: 1. Collect disputed sentence candidates about a given query from the Web. 2. Score the disputed sentence candidates by considering the typicality of the disputed 3. Suggest top-k of the disputed sentences to users 3.1 Extracting Disputed Sentences from the Web Detecting sentences that other documents dispute is quite hard because it requires deep natural language processing for large corpora. To bypass this problem, Ennals et al. pro-posed a method to collect disputed sentences from the Web by using linguistic patterns [16]. We apply their method to collect disputed sentences about a given query. Ennals et al. prepared a set of dispute clue patterns that could indicate that a statement S is disputed, such as  X  X he misconception that S  X  and  X  X t is not true that S  X  . Then, as shown in Fig.2, they issued the prepared linguistic patterns as phrase queries into Web search engines. After that, they extracted th e sentences that appeared just behind one of the patterns from obtained Web search results.

While Ennals et al. focus on collecting a large indefinite number of disputed sen-tences in batch processing, our goal is to collect disputed sentences related to a user X  X  query in real time. To achieve this goal, we selected 15 of the 54 effective linguistic patterns that Ennals et al. suggested in their paper. The linguistic patterns are shown in Table 1. We implemented the method to collect disputed sentences by using these selected linguistic patterns. Our method works as follows: 1. Issue a given query and each of the prepare d linguistic patterns into Yahoo Search 2. Split snippets of Web search results into sentences using some delimiters (e.g.  X . X , 3. Extract sentences that appear just behind one of the prepared linguistic patterns 4. If the extracted sentences start with a specific stopword, such as a pronoun, a con-3.2 Ranking Disputed Sentences After collecting disputed sentence candidates from Web search results about a given query, the system ranks the candidates to suggest useful disputed sentences to users.
Here we denote a given query and a disputed sentence by q and d , respectively. We estimate the importance of each collected disputed sentence D = { d 1 ,d 2 ,...,d n } , focusing on the relevance to given query q . Then we order them by using the following equations on the query likelihood model [3]: where M d is the language model of disputed sentence d , tf t,d is the term frequency of term t in disputed sentence d ,and L d is the number of terms in disputed sentence d .  X  is a weighting parameter and 0 &lt; X &lt; 1 . M c is the language model built from the entire disputed sentence collection D .

When using the query likelihood model, the prior probab ility of a document p ( d ) is often treated as uniform across all d . However, we calculate p ( d ) to consider how popular the collected disputed sentences are on the Web.

One way to determine the popularity of disputed sentences on the Web is to count how many times they appear. However, as exemplified by sentences  X  X  mobile phone is bad for your health X  and  X  X obile phones are a health risk X  , the same statements are often expressed in different words. Therefore, it is difficult to measure the popu-larity or typicality of sentences by simply using their frequency. To solve this prob-lem, we use the LexRank algorithm [17]. In this algorithm, a graph is created from text contents, where text contents are nodes and textual similarity between text nodes is the weight of the edge. Then the centrality of the text nodes is calculated by us-ing the graph. The system measures p ( d ) as the typicality of each disputed sentence d  X  D = { d ized as follows: Here, S  X  is the column normalized adjacency matrix of the similarity matrix S ,where each sim( d i ,d j ) denotes the cosine similarity between disputed sentences d i and d j . T is the typicality score vector, where each t ( d ) denotes the typicality of disputed sentence d on the Web. We approximate p ( d ) by t ( d ) .

In practice, p ( d | q ) of disputed sentence d  X  D is calculated as follows: 1. Collect disputed sentences D = { d 1 ,d 2 ,...,d n } from the Web (as explained in 3. Create feature vectors of the disputed sentences by ignoring stopwords. To weight 4. Calculate typicality t ( d ) as p ( d ) of all disputed sentence d  X  D using 3. The system 5. Calculate p ( d | q ) of all d  X  D usingtheresultsofstep2and4oneq.(1). We conducted an experiment to examine how effective our method is for enhancing users X  credibility judgment in the Web search process. In this experiment, we used the following three viewpoints to evaluate our disputed sentence suggestion and compared our method with two baseline methods:  X  Relevance: How relevant are suggested disputed sentences to a given query?  X  Alertingness: How critical will users become about Web search results for a given  X  Usefulness: How useful are suggested disputed sentences in searching for addi-We compared our method with two baseline methods. One is a conventional keyword query suggestion method, which suggests related queries based on users X  query logs (called CKS ). To imitate the conventional keyword query suggestion, we issued each of the prepared queries into Yahoo Web Search 5 and used its suggested keywords. As for ranking order of suggested queries, we simply used the order in which Yahoo Web Search suggested the queries. The other bas eline method is the typicality-based disputed sentence suggestion method, which focuses on only p ( d ) in the process we explained in Section 3 (called TDS ). To execute disputed sentence suggestion, we set our method (called Ours )and TDS to collect 40 Web search results for each of the given queries. In addition, we set  X  =0 . 5 on eq. (1) for Ours . 4.1 Metrics for Evaluation To evalate the effectiveness of Ours , TDS ,and CKS from the viewpoints of relevance, usefulness, and alertingness, we checked how many relevant/alerting/useful suggestions appeared in top-k rankings. To evaluate them, we used the following equations: Here Q means a query set and S q ( k ) means top-k suggested keywords/sentences that each algorithm outputs for the input query q . | S | indicates the size of the set S . Relevant ( S ) , Alerting ( S ) ,and Useful ( S ) respectively mean the suggested key-words/sentences in S judged as relevant, alerting, and useful by least two human eval-uators. 4.2 Participants and Materials We recruited three human evaluators for this experiment. They all had experience in using Web search engines. For this experiment, we manually prepared 20 queries from five categories. The queries are shown in Table 2. 4.3 Procedure In this experiment, we input each of the que ries in Table 2 to the three methods ( Ours , TDS ,and CKS ) and showed the suggested keywords/sentences to the evaluators. Then we asked them to evaluate the suggested keywords/sentences in terms of relevance, alertingness, and usefulness. Before the evaluators evaluated suggested keywords/ sentences for each query, we showed them a brief description like: In each evaluation task for CKS , we showed the evaluators raw keyword queries sug-gested by Yahoo. In each evaluation task for Ours and TDS , we added the expression  X  X ome people doubt: X  to each of disputed sentences and showed the combined sen-tences to the evaluators (e.g., if the system found the disputed sentence  X  X obile phone has a health risk X  for the query  X  X obile phone health X  , the system suggested  X  X ome people doubt:x mobile phone has a health risk X  to the evaluators).

To evaluate the relevance, alertingness, an d usefulness of each of the suggested key-words/sentences, we asked the evaluators to answer the following three questions with YES or No:  X  Relevance: Are suggested keywords/sentences related to a given query?  X  Alertingness: Will you become more critical of Web search results about a given  X  Usefulness: Are suggested keywords/sentences useful in searching for additional 4.4 Results Fig.3 shows the averages of relevance scores, usefulness scores, and alertingness scores for each method on a query set, using equations r @ k , a @ k ,and u @ k . According to Fig.3(a), in terms of relevance, CKS performed much better than Ours and TDS .How-ever, Ours was not so bad at suggesting relevant sentences ( r @1=0 . 80 , r @3=0 . 70 , and r @5 = 0 . 69 ). CKS is based on query logs a lot of users issued in actual Web search sessions. Therefore, CKS can precisely suggest relevant keywords for given queries, unlike Ours and TDS , which analyze documents to suggest disputed sentences. The relevance performance of Ours was higher than TDS on r @1 , r @3 ,and r @5 .Espe-cially, according to r @1 , Ours was higher by 33% than TDS . We think that query score.

In terms of alertingness, Fig.3(b) shows that with Ours and TDS , at least 50% of suggested sentences in the top 5 rankings calle d the evaluators X  attention to credibility judgment. The performance of Ours was higher than that of TDS on a @1 , a @3 ,and a @5 . On the other hand, as the result for CKS indicates, the evaluators did not become more careful of their credibility judgment a t all, even when looking at the keywords suggested ( a @1 =0.05, a @3 =0.07, a @5 =0.09). This result is natural because the objec-tive of conventional keyword suggestion is different from that of our disputed sentence suggestion, which aims at alerting users to suspicious information.

Fig.3(c) shows that the two methods that suggest disputed sentences performed better than CKS . This indicates that the disputed sentence suggestion can be more useful in searching for additional clues for cre dibility judgment on W eb pages about given queries than conventional keyword suggestion.

Before this experiment, we expected that disputed sentence suggestion would be more useful in searching for clues for cr edibility judgment. Howe ver, neither of the two disputed sentence suggestion methods showed show usefulness scores as high as we expected. In addition, u @1 and u @3 of Ours were less than those of TDS (scores of the two methods were the same on u @5 ). We interviewed the evaluators after the experiment. Below is a typical evaluator comment: This indicates that disputed sentence suggestion was not necessarily useless, although Fig.3(c) indicate that it was not so useful in searching for additional credibility evidence from the Web. We think that if systems can suggest disputed sentences relevant to given queries, the disputed sentences themselves are likely to be useful clues for credibility judgment on Web pages for given queries. The experimental results shows that Ours suggested more relevant disputed sentences than TDS .

Finally, these experimental results indicate that, by suggesting disputed sentences with reasonable relevance to given queries, our proposed method can make users more critical in the Web search process than the baseline methods can. 4.5 Case Study on Disputed Sentence Suggestion Table 3 and 4 show some examples of good and bad disputed sentences the system suggested for given queries using our proposed method. We manually checked the sug-gested disputed sentences the evaluators judged as bad and categorized them into four classes: non-relevant , ambiguous , difficult to understand ,and grammatical error .
As evidenced by sentences like  X  X otatoes are fattening X  for query potato poison , the system sometimes suggested non-relevant disputed sentences to given queries. We think this is because even if extracted disputed sentences are not very relevant to given queries, our system gives them high scores if they are typical on the Web. We have to think about a way to balance query likelihood scores and typicality scores of disputed sentences on eq. (1).

Even when the system maintained the relevance of disputed sentence suggestion, ambiguous disputed sentences were sometimes suggested, such as  X  X lobal warming was the cause X  for query global warming . In our method, the system simply extracts disputed sentences that appear behind linguistic patterns like  X  X t is not true that X  and  X  X o proof that X  . Therefore, the system often suggests that ambiguous and difficult-to-understand disputed sentences without context be extracted from Web search results.
The evaluators misunderstood that the system suggested some meaningless disputed sentences for given queries, although the disputed sentences were actually relevant to the queries and alerting. One possible reason is that the evaluators could not understand the meaning of some terms in the disputed sentences because they were not familiar with the sentences. For instance, the system suggested disputed sentence  X  X gb was not the inventor X  for query telephone inventor . If users do not know that term agb means Alexander Graham Bell , they may think the disputed sentence is useless and then will not become more critical of Web pages for the given query. It is important to consider the comprehensibility of sentences when suggesting them to users.

The system also sometimes suggested disputed sentences that were grammatically difficult to read like  X  X omeone doubts: sh ipping plastics for recycling X  for query plastic recycling . Our system splits sentences with punctuation marks and extracts disputed sentences by simply focusing on declarative content clauses ( that clauses ). Therefore, the system often fails to extract grammati cally complete sentences. We need deeper natural language processing to handle this problem. In this paper, we addressed a new type of query suggestion to call users X  attention to credibility judgment of Web search results for given queries in the Web search pro-cess. Given a query, such as mobile phone health , our system searches for statements some Web pages dispute about the query and suggests ones like  X  X obile phone are bad for your health X  to users. While conventional keyword query suggestion focuses on making it easy for users to search for Web pages that match their search intent, our disputed sentence suggestion focuses on making them aware of the existence of suspi-cious information or to get clues for a cre dibility judgment on Web pages about their input query. Even if users are not concerne d with Web information credibility and do not know about the existence of suspicious Web information, they can notice suspicious information and become more critical in searching for credible Web pages.

The experiment results show our method suggested effective disputed sentences to help users more critically search for credible Web pages, although the method had lower performance in terms of relevancy than conventional keyword suggestion. To improve our disputed sentence suggestion, we first have to improve the relevance of suggested disputed sentences by optimizing the balance between the query likelihood scores and typicality scores of them. In addition, we n eed to think about a way to suggest compre-hensible disputed sentences because some times users cannot understand the meaning of suggested disputed sentences even if they are relevant and typical to given queries. An-other important issue is how to support users X  search for credible Web pages efficiently using suggested disputed sentences.

To safely and efficiently obtain information from the vast Web, search systems focus-ing on credibility will become more impor tant in the future . We believe our proposed system can contribute to credibility-oriented Web search.
 Acknowledgment. The author wishes to thank Xiaojing Zhang, Yosuke Ishihara, and Daisuke Kitayama for their valuable comments. This work was supported by Grants-in-Aid for Scientific Research (No. 18049041) from MEXT of Japan, a Kyoto Uni-versity GCOE Program entitled  X  X nformatics Education and Research for Knowledge-Circulating Society X .

