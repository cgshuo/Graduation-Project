 Most previous Web-page summarization metho ds treat a Web page as plain text. However, suc h metho ds fail to un-cover the full kno wledge asso ciated with a Web page needed in building a high-qualit y summary , because man y of these metho ds do not consider the hidden relationships in the Web. Unco vering the hidden kno wledge is imp ortan t in building good Web-page summarizers. In this pap er, we extract the extra kno wledge from the clic kthrough data of a Web searc h engine to impro ve Web-page summarization. We rst analyze the feasibilit y in utilizing the clic kthrough data to enhance Web-page summarization and then prop ose two adapted summarization metho ds that tak e adv antage of the relationships disco vered from the clic kthrough data. For those pages that are not covered by the clic kthrough data, we design a thematic lexicon approac h to generate im-plicit kno wledge for them. Our metho ds are evaluated on a dataset consisting of man ually annotated pages as well as a large dataset that is crawled from the Op en Directory Pro ject website. The exp erimen tal results indicate that sig-ni can t impro vemen ts can be achiev ed through our prop osed summarizer as compared to the summarizers that do not use the clic kthrough data.
 H.4 [ Information Systems Applications ]: Miscellaneous; I.5.4 [ Pattern Recognition ]: Applications| Text process-ing This work was conducted when the rst author was visiting Microsoft Researc h Asia, Beijing, China.
 Algorithms, Exp erimen tation, Veri cation Generic Web-page Summarization, Clic kthrough Data, La-ten t Seman tic Analysis, Thematic Lexicon
With the rapid gro wth of the WWW, there is an increas-ing need to succinctly summarize Web pages for Web users. For example, Web portals suc h as Yaho o! and LookSmart organize their Web pages into a hierarc hical structure and pro vide a short summary for eac h page. These summaries are useful in satisfying the Web users' information needs by means of fast and accurate bro wsing and searc h. For a searc h engine suc h as Google, after a query is issued, a list of URL's is returned, eac h accompanied with a snip-pet that gives a brief summary of the target page's con ten t. Web-page summarization is also useful when users access Internet via small-displa y devices suc h as the personal dig-ital assistan ts (PD A's), since it is hard to t the original con ten t of a Web page into a small screen [3]. However, it is quite exp ensiv e to generate the summaries man ually in large scale. Therefore, it is a critical issue how to automatically generate high qualit y Web-page summaries from Web pages.
Web-page summaries can be abstracts or extracts. An ex-tract summary consists of sen tences extracted from a Web page while an abstract summary may con tain words and phrases whic h do not exist in the original documen t [15]. A Web-page summary can also be either generic or query dep enden t. A query-dep enden t summary presen ts the infor-mation that is most relev ant to the initial searc h query , while a generic summary gives an overall sense of the documen t's con ten t. A generic summary should meet two conditions: main tain a wide coverage of the page's topics and keep low redundancy at the same time [6]. In this pap er, we focus on extract-based generic Web-page summarization. Although much work has been done on this kind of summarization, the result is often not satisfying [7]. One reason is that the textual information of a Web page may be scarce, and in some cases, the con ten t con tained in a Web page is very di-verse in topics. For the traditional summarization metho ds whic h focus on local con ten ts of a documen t, it is dicult to capture the true meaning of a Web page. What's more, Web pages usually con tain much noise whic h is hard to be remo ved simply by statistical metho ds. Additional kno wl-edge will be very helpful in distinguishing the real con ten t of a Web page from noise.

The objectiv e of this researc h is to study how to use extra kno wledge of the clic kthrough data to impro ve Web-page summarization. The clic kthrough data con tain man y users' kno wledge on Web pages' con ten ts. Typically , a user's query words issued before a target page is selected often re ect the true meaning of the target Web-page con ten t. There-fore, it would be helpful if the kno wledge con tained in the clic kthrough data can be unco vered to complemen t the Web page con ten ts. What's more, the clic kthrough data are col-lected from users all over the world. For a Web page with multiple topics, Web users may submit di eren t queries to nd the topic speci c to their resp ectiv e information needs. Thus the query-w ord set of a page may cover the multiple topics of the target Web page. If we can use this query-w ord set for our summaries, it would be easier for us to pro duce a generic summary to meet the needs of general Web users, in case the summary is biased towards a speci c user.
In this pap er, we prop ose a novel approac h to utilize the clic kthrough data for Web-page summarization. To be sure, this is a challenging task. First, Web pages may have no asso ciated query words since they are not visited by any Web users through a searc h engine. This is esp ecially true for nearly emerging pages, where no clic kthrough data can be used when they are summarized. Second, the clic kthrough data are often very noisy . Web users may clic k on pages that are not relev ant with their interests and the clic k will be recorded by a searc h engine.

Both problems above could be solv ed by our prop osed thematic lexicon. By using the annotated hierarc hical tax-onom y of Web pages suc h as the one pro vided by ODP web-site (http://dmoz.org/), we can build a thematic lexicon. For eac h category , the lexicon con tains terms submitted by all Web users to bro wse the pages of this category , as well as the weigh ts of these terms. In conjunction with the prop osed summarization metho ds, the thematic lexicon can be used to complemen t the scarcit y problem of Web-page con ten t, even when no clic kthrough data are collected asso ciated with these pages. In addition, our metho d can help lter out noise con tained in query words for any Web page through the use of statistics over all Web pages of a given category . The category-sp eci c thematic lexicon pro vides term distri-bution at a category level and re ects users' kno wledge on term usage when they locate pages of this category . Some noisy terms whic h may be relativ ely frequen t in one page's query words will be given a low weigh t by our approac h. The result is a more robust summarization system.
In our new algorithm, we adapt two text-summarization metho ds to summarize Web pages. The rst approac h is based on signi can t-w ord selection adapted from Luhn's me-tho d [13]. The second metho d is based on Laten t Seman tic Analysis (LSA) [7]. The exp erimen tal results sho w that both approac hes achiev e impro vemen ts compared with the pure-text-based summarization.

The rest of the pap er is organized as follo ws. In Section 2, we presen t the related works on Web-page summarization and clic kthrough data analysis. Our prop osed summariza-tion algorithms are discussed in Section 3. In Section 4, the exp erimen tal results are given as well as some discussions. Finally , we conclude our work in Section 5.
Web-page summarization techniques have been widely ap-plied in man y applications [3, 10, 16]. In order to generate high-qualit y summaries, sev eral researc hers utilized extra in-formation in Web-page summarization. A few works utilize the con text information constructed by hyperlinks among Web pages [1, 5]. With the InCommonSense system, Amita y and Paris rst extracted the text segmen ts con taining a link to a Web page. They then chose the most accurate sen tence from the text segmen ts as the snipp et of the target page [1]. Since the con text information may be related to the target Web page but con tains no clues for summarization, Delort et al. prop osed two enhanced Web-page summarization meth-ods using hyperlinks [5]. In their work, the authors studied the characteristics of con text information of a Web page as well as the relations between the con text information and the target Web page con ten t. Our work di ers from these previous ones mainly in the approac hes to unco ver and uti-lize the extra kno wledge. The previous metho ds obtain their kno wledge from hyperlinks directly whic h may be sparse and noisy; in con trast, we get the additional kno wledge from the searc h-engines clic kthrough data whic h are ltered through our noise-remo val algorithms. The searc h queries pro vide a more accurate seman tic lab eling for the subsequen tly clic ked Web pages, and are found to perform better for Web-page summarization. In addition, in the previous works, Web-page summaries are extracted from the con text text seg-men ts, while we extract sen tences from a Web page, with help of the clic kthrough data.

Muc h researc h has also been done on using clic kthrough data analysis to impro ve the performance of searc h algo-rithms, Web-page classi cation metho ds or metadata ex-traction techniques. Sun et al. prop osed a Cub eSVD ap-proac h to utilizing the clic kthrough data for personalized Web searc h [17]. Liu et al. prop osed a technique for cat-egorizing Web-query terms from the clic kthrough logs into pre-de ned sub ject taxonom y based on their popular searc h interests [12]. To the best of our kno wledge, few rep orts in the literature have focused on using the clic kthrough data for Web-page summarization. There are sev eral works on auto-matic metadata extraction whic h are related with our work. Hulth et al. prop osed to extract keyw ords using domain kno wledge [9]. In [8], classi cation based metho ds were pro-posed in order to extract metadata inheren t in Web pages and to build relationships among the metadata and Web page categories. In [4], the authors construct taxonom y of queries by retrieving Web pages from searc h engine to help represen t query terms. In this pap er, we also construct a thematic hierarc hy of query terms. However, we use the clic kthrough data instead of Web pages. In addition, the researc h works aforemen tioned did not concern how to use the extracted kno wledge data for Web-page summarization, whic h is the focus of this pap er.
In this section, we rst give sev eral observ ations on clic k-through data, whic h validate our assumption that the clic k-through data is bene cial for Web-page summarization. Then we describ e two prop osed summarization metho ds whic h can leverage the kno wledge data extracted from the clic kthrough data. Finally , we prop ose an approac h to building a the-matic lexicon as a kno wledge source for summarizing Web pages whic h are not covered by the clic kthrough data.
Consider a typical searc h scenario: a user ( u ) submits a query ( q ) to searc h engine, the searc h engine returns a rank ed list of Web pages. Then the user clic ks on the pages ( p ) of interest. After a perio d, the serv er side will accum u-late a collection of clic kthrough data, whic h can be repre-sen ted by a set of triples &lt; u; q; p &gt; . Thus the clic kthrough data records how Web users nd information through queries. From a statistical point of view, the query word set corre-sponding with a Web page con tains human's kno wledge on how the pages are related with their issued queries. Some users even re ne their queries in order to nd the desired information. Therefore, the collection of queries is supp osed to well re ect the topics of the target Web page.
We rst conduct an exp erimen t to investigate whether the query words are related to the topics of the Web page. For exp erimen tal purp ose, we have crawled a set of Web pages from ODP directory . One mon th's clic kthrough data collected by MSN searc h engine is also available for our ex-perimen ts. In the clic kthrough data, among the 260,763 web pages accessed by users during this mon th, 109,694 of them con tain \KEYW ORD" metadata, whic h is an imp or-tan t indicator of the con ten t of Web pages. According to the statistics, 45.5% of the keyw ords occur in the query words and 13.1% of query words app ear as keyw ords. This result supp orts our hypothesis that query words are indicativ e of Web pages' con ten ts, whic h motiv ates us to leverage the clic kthrough data to impro ve Web-page summarization.
In order to give more evidence that clic kthrough data are helpful in summarizing Web pages, we conducted a second exp erimen t. We collected a small dataset consisting of 90 pages whic h are covered by the clic kthrough data. For evalu-ation purp oses, we ask ed three human evaluators to conduct a man ual summarization task on these Web pages, without kno wing the queries. Eac h evaluator was ask ed to extract the sen tences whic h are imp ortan t for a Web page. There is no limit on the num ber of sen tences extracted. According to the statistics, about 58% of the sen tences in the original Web page con tain query words and eac h sen tence con tains 1.48 query words on average. However, for man ually created summaries, the percen tage of sen tences con taining queries becomes 71.3% and the average query word length in eac h sen tence becomes 2.0. The exp erimen t indicates that the human evaluators tend to extract the sen tences with query words as Web-page summaries. Therefore, it is natural for us to pay close atten tion to the query words when summa-rizing the Web pages automatically .

From the above empirical study , we conclude that the clic kthrough data are useful in impro ving the qualit y of Web-page summarization.
Supp ose that we have a set of query terms for eac h page now, we prop ose two adapted summarization metho ds to leverage the clic kthrough data.
Our rst summarization metho d is adapted from Luhn's algorithm, a classical algorithm designed for text summa-rization [13]. In Luhn's metho d, eac h sen tence is assigned a signi cance factor and the sen tences with high signi cance factors are selected to form the summary . In order to com-pute the signi cance factor of eac h sen tence, a set of sig-ni can t words are constructed rst. In Luhn's algorithm, signi can t words are selected according to word frequency in a documen t. That is, those words with frequency between high-frequency cuto and low-frequency cuto are selected as signi can t words. Then the signi cance factor of a sen-tence can be computed as follo ws: (1) Set a limit L for the distance at whic h any two signi can t words could be consid-ered as being signi can tly related. (2) Find out a portion in the sen tence that is brac keted by signi can t words not more than L non-signi can t words apart. (3) Coun t the num ber of signi can t words con tained in the portion and divide the square of this num ber by the total num ber of words within the portion. The result is the signi cance factor of eac h sen tence.

In order to customize this pro cedure to leverage query terms for Web-page summarization, we mo dify the signi -can t word selection metho d. The basic idea is to use both the local con ten ts of a Web page and query terms collected from the clic kthrough data to decide whether a word is sig-ni can t or not. Eac h candidate word is assigned with a signi cance factor w i given in Equation 1. In Equation 1, tf p i and tf q i denote frequencies of the i-th term in the local text con ten t of a Web page and in the query set resp ectiv ely. The signi cance factor w i is mea-sured in a weigh ted com bination: is a trade-o parameter when com bining the two signi cance measuremen ts. After the signi cance factors for all words are calculated, we rank them and select the top N % as signi can t words. Then we emplo y Luhn's algorithm to compute the signi cance factor of eac h sen tence.
An LSA based text summarization metho d was prop osed in [7]. Supp ose that there are m distinct terms in a n docu-men t collection. The corpus can be represen ted by a term-documen t matrix X 2 R m n , whose comp onen t x ij is the weigh t of term t i in documen t d j . The Singular Value De-comp osition (SVD) of X is given by: In Equation 2, U and V are the matrices of the left and righ t singular vectors. is the diagonal matrix of singular values. LSA appro ximates X with a rank-k matrix: by setting the smallest r k singular values to zero ( r is rank of X ). That is, the documen ts are represen ted in the k dimensional space spanned by column vectors of U k [2].
The adv antage of LSA deriv es from its abilit y to cap-ture the laten t relations between terms. In the dimension-reduced space, eac h singular vector corresp onds to a laten t concept, with the corresp onding singular value measuring the imp ortance of the concept. In [7], Gong et al. prop osed an extraction based summarization algorithm. Firstly , a term-sen tence matrix is constructed from the original text documen t. Next, LSA analysis is conducted on the ma-trix. In the singular vector space, the i -th sen tence is repre-sen ted by the column vector ' i = [ v i 1 ; v i 2 ; ; v ir Eac h elemen t in ' i measures the imp ortance factor of this sen tence on the corresp onding laten t concept. In the last step, a documen t summary is pro duced incremen tally . For the most imp ortan t concept, the sen tence having the largest imp ortance factor is selected into the summary . Then, the second sen tence is selected for the next most imp ortan t con-cept. This pro cedure rep eated until a prede ned num ber of sen tences are selected.
 Our LSA-based summarization metho d is a varian t of Gong's metho d. We utilize the query-w ord kno wledge by changing the term-sen tence matrix: if a term occurs as query words, its weigh t is increased according to its frequency in query word collection. In this approac h, we exp ect to ex-tract sen tences whose topics are related to the ones re ected by query words. As discussed in [2, 7], LSA is capable of capturing the laten t asso ciations among terms. If a word com bination pattern is salien t and recurring frequen tly in a documen t, this pattern will be captured and represen ted by one of the singular vectors. If we increase the weigh ts of query terms, these terms, as well as others whic h frequen tly co-o ccur, will mak e more con tributions to singular vector form ulation. Thus in the sen tence extraction step, the can-didate sen tences whic h are seman tically related with query terms will be selected rstly .

Since the term-sen tence matrix determines the SVD re-sult, its represen tation may in uence the summarization re-sults. The term frequency vector of eac h sen tence can be weigh ted by di eren t weigh ting (global weigh ting and local weigh ting) and normalization metho ds. These schemes are studied in [7]. According to their exp erimen ts, the global weigh ting and normalization schemes lower the summariza-tion performance, while the local weigh ting schemes pro duce similar results. Thus, in this pap er, a term frequency (TF) approac h without weigh ting or normalization is used to rep-resen t the sen tences in Web pages. Terms in a sen tence are augmen ted by query terms as follo ws: In this equation, is a parameter used to tune the weigh ts of query terms. Here, tf p i is the frequency of term i in a sen tence, while tf q i denotes term frequency in query set.
There are sev eral adv antages for both the above mo di ed summarization algorithms. First, the extra kno wledge of query terms is utilized to help select signi can t words and to mo dify the page represen tation. Because of the sub jectiv e characteristic of word usage, some words may have a rela-tively low term frequency in the Web page, even though they are topic-related. The con tributions of these words are con-strained because both signi can t-w ord metho d and LSA are term-frequency based metho ds. However, these words may be used as query words by Web-searc h users as they are re-lated with the topics of the Web page. Second, our approac h can, to some exten t, handle the noises of query words, be-cause the con tribution of eac h query term is prop ortional to its frequency in the query term collection. Thus a query term with very low frequency mak es few con tributions to sig-ni cance factor calculation and laten t concept formation re-spectiv ely. Finally , for Luhn's metho d, the frequency-cuto metho d may lead to a lot of signi can t words for long pages. This problem is avoided in our ASW approac h by keeping the num ber of signi can t words to be N % times the num ber of distinct terms occurring in the Web page. Therefore, both the prop osed approac hes are supp osed to pro duce better summarization results by leveraging the clic kthrough data.
According to statistics on the one mon th's clic kthrough data, only 23.1% out of the crawled ODP pages (in English) was bro wsed and asso ciated with query words (the detailed num bers are given in Section 4.1). Thus for pages whic h are not bro wsed by Web users, neither summarization metho d prop osed in Section 3.2 can be directly applied on them. In this section, we build a hierarc hical lexicon using the clic k-through data and apply it to help summarize those pages.
Since all ODP Web pages have been man ually organized into a hierarc hical taxonom y, we com bine this large kno wl-edge source and the clic kthrough data to build a thematic lexicon. For eac h category of the taxonom y, the lexicon con-tains all query terms that users have submitted to bro wse Web pages of this category , as well as weigh ts of these terms, where the latter measures the likeliho od that Web users will use this term to locate pages of this category . In this pap er, we use T S ( c ) to represen t a set of terms asso ciated with category c , as well as their corresp onding weigh ts. Thus the thematic lexicon is a set of T S , whic h corresp ond with cat-egories in ODP and are organized using the ODP category structure. The lexicon is built as follo ws: rst, T S corre-sponding to eac h category is set empt y. Next, for eac h page covered by the clic kthrough data, its query words are added into T S of categories whic h this page belongs to as well as all its paren t categories. When a query word is added into T S , its frequency is added to its original weigh t in T S . If a page belongs to more than one category , its query terms will be added into all T S asso ciated with all its categories. At last, term weigh t in eac h T S is multiplied by its Inverse Category Frequency (ICF). The ICF value of a term is the recipro cal of its frequency occurring in di eren t categories of the hierarc hical taxonom y.

After the hierarc hical lexicon is built, we can use it to summarize Web pages that are not covered by the clic k-through data. For eac h Web page to be summarized, we rst look up the lexicon for T S according to the page's cat-egory . Then the summarization metho ds prop osed in Sec-tion 3.2 are used. Weigh ts of the terms in T S can be used to select signi can t words or to update the term-sen tence matrix. If a page to be summarized has multiple categories, the corresp onding T S are merged together and weigh ts are averaged. When a T S does not have sucien t terms, T S corresp onding with its paren t category is used.
The hierarc hical lexicon-based Web-page-summarization metho d has at least two adv antages. First, the category-speci c T S pro vides a distribution of topic terms in this category , whic h re ects Web users' kno wledge on term usage when they locate information of this category . Second, some noisy terms whic h may be relativ ely frequen t in one page's query words will be given a low weigh t through the use of statistics over all Web pages of this category . In this section, we will investigate whether the adapted Web-page summarization metho ds are sup erior to the ones without using clic kthrough data. We introduce the exp er-imen t data set, the evaluation metrics and the exp erimen t results.
The clic kthrough data were collected from the MSN searc h engine. This data set con tains about 44.7 million records of 29 days from Dec 6 of 2003 to Jan 3 of 2004. As we collected the clic kthrough data, a set of Web pages of the ODP direc-tory are crawled. Among the 3,074,678 Web pages crawled, we remo ved those whic h belong to \W orld" and \Regional" categories, as man y of them are not in English. At last we got 1,125,207 Web pages, 260,763 of whic h are clic ked by Web users using 1,586,472 di eren t queries.

Tw o di eren t data sets were used. The rst one, denoted by DAT1, consists of 90 pages whic h are selected from the 260,763 bro wsed pages. Table 1 gives the query num bers as-sociated with eac h page of DAT1. Three human evaluators were emplo yed to summarize these pages. Eac h evaluator was requested to extract the sen tences whic h he/she deemed to be the most imp ortan t ones for a Web page. There is no constrain t on the num ber of sen tences to be extracted. Table 2 describ es the overall consistencies among the three eval-uators. For example, for the Evaluator1:Ev aluator2 pair, 0.45 means 45% sen tences in evaluator1's summary are also included in evaluator2's summary . Eac h num ber given in this table is an average result over all Web pages. From Table 2, we can nd the three evaluators have a relativ ely high disparit y on the 90 pages. Similar observ ations of high disparities between human evaluators are also observ ed in previous works, suc h as [7]. Thus, in this pap er, the exp eri-men ts are evaluated using the annotation results of all three evaluators. The average results are also rep orted. We also use a relativ ely large scale data set, denoted by DAT2, to evaluate our summarization metho ds. We pre-pro cess the 260,763 pages con tained in the clic kthrough data using a layout analysis algorithm. After the con ten t body of eac h page is extracted, the textual con ten t is segmen ted into sen tences by a sen tence segmen ting program implemen ted in our group. In addition, descriptions of eac h page (\DE-SCRIPTION" metadata) are also extracted. We keep the Web pages with a description of over 200 characters and con-taining at least 10 sen tences, from whic h 10,000 pages are randomly selected and constitutes DAT2 data set. Since the description is pro vided by the page editor to give a general description of this page, we use it as the ideal summary .
Both intrinsic and extrinsic metho des are prop osed for automatic summarization evaluation [11, 14]. In this pap er, we emplo y two intrinsic evaluation approac hes to evaluate the prop osed approac hes.
Precision, recall and F 1 are straigh tforw ard measures widely used in summarization evaluation. For eac h documen t, the man ually extracted sen tences are considered as the reference Table 1: Num ber of Queries Asso ciated with Anno-tated Pages Table 2: Consistiencies among the Human Evalua-tors summary . This approac h compares the candidate summary with the reference summary and computes the precision, re-call and F 1 values: where S cand and S ref denotes the sen tences con tained in the candidate summary and the reference summary resp ectiv ely.
ROUGE 1 is a soft ware pac kage adopted by DUC 2 for au-tomatic summarization evaluation [11]. It measures summa-rization qualit y by coun ting overlapping units suc h as the n-gram, word sequences, and word pairs between the can-didate summary and the reference summary . ROUGE-N is an n-gram recall measure whic h is de ned as follo ws: In Equation 6, N stands for the length of the n-gram, Count match ( gram n ) is the maxim um num ber of n-grams co-o ccurring in the candidate summary and the reference summary , Count ( gram n ) is the num ber of n-grams in the candidate summary .

According to [11], among the evaluation metho ds imple-men ted in ROUGE, ROUGE-N (N=1, 2) is relativ ely simple and works well in both single documen t summarization eval-uation tasks and evaluation of very short summaries. In this pap er, we evaluated our exp erimen ts using all the meth-ods pro vided by the ROUGE pac kage and only rep orted ROUGE-N where N=1, since the conclusions dra wn from di eren t metho ds are quite similar.
In this subsection, we conduct exp erimen ts to evaluate the summarization metho ds prop osed in Section 3.2. On 1 http://www.isi.edu/ cyl/R OUGE/ the DAT1 dataset, in order to evaluate our metho ds on summaries of di eren t evaluators, we keep the num ber of sen tences extracted be equal with that of the human sum-mary . In this case, the precision, recall and F 1 measure are all equal. On both the data sets, the summarization meth-ods are evaluated by the ROUGE soft ware.
We rst conduct exp erimen ts to investigate whether the adapted summarizers can bene t from query terms asso ci-ated with eac h page. For the ASW metho d, we vary from 0 to 1 (in step 0.1) to change the in uence of query terms on signi can t word selection. The summarization results are listed in Figure 1, measured by precision and ROUGE-1 re-spectiv ely. When is 0, the clic kthrough data is ignored and only the local features of a Web page are used to se-lect signi can t words. When is 1, only query words are used. For ALSA, the parameter is also varied from 0 to 1. The results are given in Figure 2. From the two gures, we can nd that both metho ds achiev e signi can t impro vemen ts when evaluated on the man ual summaries of di eren t evalu-ators. For ASW metho d, the average precision of the three evaluators got a relativ e impro vemen t of 20.7%, from 0.29 (when no query terms are used) to 0.35 (when query terms are used and = 0 : 5). The average ROUGE-1 measure got a relativ e impro vemen t of 11.5% (from 0.52 to 0.58). The ALSA metho d achiev es a relativ e impro vemen t of 12.9% and 11.5% when measured by precision and ROUGE-1 resp ec-tively, compared with without using the clic kthrough.
On this data set, we also evaluate our summarization metho ds using the thematic lexicon approac h. We rst use the ODP directory and the clic kthrough data to build a thematic lexicon. Since the clic kthrough data con tains only 260,763 pages, our lexicon con tains 141,869 categories, whic h is a subset of the ODP category structure. When we summa-rize a page, we ignore the query words asso ciated with it. As describ ed in section 3.3, the thematic lexicon is used to help generate query words. We select query terms from the lexi-con based on the category of the page to be summarized. If terms under this category have more than P % overlap with distinct terms in the Web page, then they are used for sum-marization. Otherwise, we try to use lexicon terms of its paren t category . This pro cess con tinues until we nd a cat-egory whic h covers enough query terms or until we reac h the root of the thematic lexicon. In this exp erimen t, P is em-pirically set to 6. Evaluation results are describ ed in Figure 3 and Figure 4. We can nd that the ASW metho d achiev es an impro vemen t of 7.8% and 5.3% measured in precision and ROUGE-1 resp ectiv ely. The ALSA metho d achiev es an impro vemen t of 7.6% and 6.7% when measured in precision and ROUGE-1 resp ectiv ely. From the above exp erimen tal results, we can nd the ROUGE-1 measure and the precision measure are consis-ten t with eac h other in most cases, esp ecially when the av-erage results of the three evaluators are averaged. On this data set, only ROUGE-1 measure is used for evaluation. The extract-summary of eac h page is generated by our pro-posed summarization algorithms and the leading characters of length equal with the description sen tences are used for evaluation. The results are illustrated in Figure 5. In Figure 5, \Text" denotes summarization based on textual con ten t Figure 5: Summarization results on DAT2, evalu-ated by ROUGE-1 measure of Web pages. \Query" denotes summarization using query words issued to locate Web pages. \Lexicon" denotes the page queries are ignored while the thematic lexicon is used to help summarization. Since the description length is com-monly short and the ROUGE-1 measure is recall based, the summarization results are relativ ely poor. From the results in Figure 5, we can nd that the clic kthrough data can im-pro ve the Web-page summarization. Even the real queries of Web pages are dropp ed, the thematic lexicon-based metho ds can still lead to better summaries compared with local tex-tual con ten t based summarizers, although the performance is not so good as that of page-query based metho ds.
All the above exp erimen ts indicate the clic kthrough data are helpful for generic Web-page summarization and both our prop osed metho ds can leverage this kno wledge source well. When the thematic lexicon is used to help summa-rize the Web pages whic h are not covered by the clic k-through data, the impro vemen ts are not as signi can t as when queries of a page are directly used. However, the lexicon-based approac h achiev es better results compared with pure-text-based summarizers. This is because the thematic lexicon built from clic kthrough data can disco ver the topic terms asso ciated with a speci c category and the ICF-based approac h can e ectiv ely assign weigh ts to terms of this cat-egory . As illustrated in Table 3, the top 10 terms of two categories are listed. From this table, we can nd that ICF-based re-w eigh ting can help disco ver topic terms of a spe-ci c category , while terms like \com", \www", \do wnload" are assigned with lower weigh ts. Although there are high disparities between di eren t human summarizers, impro ve-men ts can be achiev ed when the results are evaluated on summaries of eac h evaluator. This indicates our metho ds can help pro duce summaries whic h meet general Web users. In most cases, our metho ds achiev e optimal results when the local con ten ts of a Web page and the clic kthrough data are com bined together, whic h verify our hypothesis that the clic kthrough data can complemen t the textual con ten ts of Web pages for summarization tasks.
We leverage extra kno wledge from clic kthrough data to impro ve Web-page summarization. Tw o extract-based meth-ods are prop osed to pro duce generic Web-page summaries. For the pages whic h are not covered by the clic kthrough Table 3: Top 10 Terms in Thematic Lexicon, before and after ICF Re-W eighting Approac h is Applied (after Stemming)
Before ICF After ICF Before ICF After ICF data, we build a thematic lexicon using the clic kthrough data in conjunction with an available hierarc hical Web di-rectory . The exp erimen tal results sho w that signi can t im-pro vemen ts are achiev ed compared with summarizers with-out using clic kthrough logs.

Our exp erimen ts indicate the trade-o parameter can in-uence the summarization result when either prop osed metho d is used. Therefore it would be interesting to prop ose a metho d to determine its value automatically . Besides, we will also study how to leverage other types of kno wledge, suc h as word clusters and thesaurus, hidden in the clic k-through data to enhance Web page summarization. We also plan to evaluate our metho ds using extrinsic evaluation met-rics and much larger data sets.
The authors would like to thank Ya-Bin Kang for his help on organizing the Web page annotation pro cess and try-ing some exp erimen ts in this work, and Xue-Mei Jiang for preparing the clic kthrough data. The sudy was funded in part by Natural Science Foundation under the gran t num-ber 60473115 and 60403021. Qiang Yang and Dou Shen are supp orted by Hong Kong RGC HKUST6180/02E. [1] E. Amita y and C. Paris. Automatically summarising [2] M. W. Berry , S. T. Dumais, and G. W. O'Brien. [3] O. Buyukk okten, H. Garcia-Molina, and A. Paep cke. [4] S. Chuang and L. Chien. Enric hing web taxonomies [5] J.-Y. Delort, B. Bouc hon-Meunier, and M. Rifqi. [6] J. Goldstein, M. Kan trowitz, V. Mittal, and [7] Y. Gong and X. Liu. Generic text summarization [8] C.-C. Huang, S.-L. Chuang, and L.-F. Chien. Using a [9] A. Hulth, J. Karlgren, A. Jonsson, H. Bostrom, and [10] K. Kummam uru, R. Lotlik ar, S. Roy, K. Singal, and [11] C. Y. Lin and E. H. Hovy. Automatic evaluation of [12] F. Liu, C. Yu, and W. Meng. Personalized web searc h [13] H. Luhn. The automatic creation of literature [14] I. Mani, D. House, G. Klein, L. Hirsc hman, T. Firmin, [15] I. Mani and M. T. Ma ybury . Advanc es in Automatic [16] D. Shen, Z. Chen, Q. Yang, H.-J. Zeng, B. Zhang, [17] J.-T. Sun, H.-J. Zeng, H. Liu, Y. Lu, and Z. Chen.
