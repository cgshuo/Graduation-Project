 shown that regularization with the  X  example [4, 12, 15] and references therein.
 on an extension of the  X  see, a key property of our approach is that the penalty functi on equals the  X  when |  X  | X   X  and it is strictly greater than the  X  the penalty function encourages the desired structured spa rsity property.  X  of model sparsity pattern forming multiple connected regions of coefficients. showing the advantage offered by our approach. associated penalty function. We let R integers up to n . We prescribe a convex subset  X  of the positive orthant R n solution of the convex optimization problem where kk and the function  X  : R n  X  R n by the arithmetic-geometric mean inequality, namely ( a + b ) / 2  X  ity it also follows that the Lasso problem corresponds to (2. 1) when  X  = R n differentiable on its domain.
 tured sparsity and establish circumstances for which the pe nalty function is a norm. Proposition 2.1. For every  X   X  R n , it holds that k  X  k only if |  X  | := ( |  X  { e k : k  X  N n } Proof. By the arithmetic-geometric inequality we have that k  X  k sertion. If |  X  | X   X  , there exists a sequence {  X  k : k  X  N } in  X  , such that lim Since  X (  X  |  X )  X   X (  X , X  k ) it readily follows that  X (  X  |  X )  X k  X  k there is a sequence {  X  k : k  X  N } in  X  , such  X  (  X , X  k )  X k  X  if and only if  X  = max {  X (  X  |  X ) : k  X  k an extreme point of the  X  will encourage vectors which not only are sparse but also hav e sparsity patterns (1 N n )  X   X  in our setting as a special case. If { J corresponding group Lasso penalty is defined as  X  for  X  = {  X  :  X   X  R n We proceed to discuss some examples of the set  X   X  R n that is  X  = {  X  :  X   X  R n the function  X ( |  X ) is a norm.
 regression vector should be in some prescribed intervals.

N functions. To state our result, we define, for every t  X  R , the function ( t ) Theorem 3.1. We have that equations  X  Proof. Since  X (  X  | B [ a,b ]) = P n = 1 . We shall show that if a,b, X   X  R , a  X  b then has a unique minimum in R  X  (  X  ) . Consequently, we have that Equation (3.1) now follows by a direct computation.
 defined by a limiting argument.
 in a non increasing fashion.
 Example 3.2. We define the wedge as W = {  X  :  X   X  R n We say that a partition J = { J are contiguous but {{ 1 , 3 } , { 2 }} is not.
 Theorem 3.2. For every  X   X  ( R \{ 0 } ) n there is a unique contiguous partition J = { J of N n , k  X  N n , such that Moreover, the components of the vector  X  (  X  ) = argmin {  X (  X , X  ) :  X   X  W } are given by and, for every  X   X  N The partition J appearing in the theorem is determined by the set of inequali ties  X  associated with a vector  X  . We explain how to do this in Section 4. constraints on the vector  X  . For example, for n = 2 we obtain that  X (  X  | W ) = k  X  k and  X (  X  | W ) =  X  2 k  X  k  X (  X  | W ) = where we have also reported the partition involved in each ca se. associated with a line graph.
 R ++ ,A X   X  0 } is yet to be understood. We postpone this discussion to a futu re occasion. vector D 1 (  X  ) := (  X   X  Example 3.4. For every k  X  N concentrated either on the first elements of the vector, on th e last, or on both. penalty functions from available ones. It is obtained by com posing a set  X   X  R k partition { P use this construction in the composite wedge experiments in Section 5. algorithm we choose  X  &gt; 0 and introduce the mapping  X   X  : R n  X  R n k  X  N , define the iterates plementary material.
 Theorem 4.1. If the set  X  is convex and, for all a,b  X  R with 0 &lt; a &lt; b , the set  X  of the sequence {  X  1 additional variable t  X  R n and note that every k  X  N The proof of the next lemma is straightforward and we do not el aborate on the details. Lemma 4.1. If  X   X  R n and  X   X  R p are admissible and k  X  k admissible.
 The iterative algorithm presented in Figure 2 can be used to fi nd the partition J = { J components, t , is put in a new set J satisfies, for every  X   X  N of the algorithm is linear. such that their  X  resorted to the solver CVX ( http://cvxr.com/cvx/ ) in the other cases. that each component |  X   X  sizes, namely a the amount of information about the true model increases, an d the performance improves.  X  j = 11  X  j we choose  X (  X  ) = P more sensitive to such a perturbation.
 sparse within different contiguous regions P the  X  a random position in P  X  =  X   X  R 100 : ||  X  P across the sets P first is a standard group Lasso with the nonoverlapping group s J variation of the hierarchical group Lasso discussed above w ith J with these approaches is that the  X  patterns within each of the P formed by all sequences of q  X  N sets P than half the number of dimensions.
 that is decreasing: the Wedge and the group Lasso with J provides a better estimate than the Lasso in both cases.
 algorithm presented here. [3] D. Bertsekas. Nonlinear Programming . Athena Scientific, 1999. [5] S. Boyd and L. Vandenberghe. Convex Optimization . Cambridge University Press, 2004. In this appendix we provide the proof of Theorems 3.2 and 4.1. A.1 Proof of Theorem 3.2 J,K  X  N n we define the region degree two. We also need the following construction.
 Definition A.1. For every subset S  X  N increasing order as S = { j of j = n .
 A subset S of N This is defined to be the set In other words,  X   X  O induced by the partition J ( S ) . This region requires the notation J defined by the equation property that, for every set J of above notation should be interpreted as O We also introduce, for every S  X  N We shall prove the following slightly more general version t he Theorem 3.2 Theorem A.1. The collection of sets U := { U each  X   X  ( R \{ 0 } ) n there is a unique S  X  N given by the equations  X  vector  X  = (  X  where we set  X  true To unravel these equations, we let S := { j :  X  { j As explained in Definition A.1, the set S induces the partition J ( S ) = { J only of N j  X  N q we obtain that  X  q =  X  P j  X  N J , which we denote by decreasing and equation (A.6) implies that  X  (A.5) over j  X  J from which equation (A.4) follows. Since the Moreover, choosing q  X  J which implies that  X   X  Q  X   X  I S and therefore, it follows that  X   X  U S .
 In summary, we have shown that  X   X  U unique solution. Now, if  X   X  U decreasing from one element to the next in those partition, i t must be the case that S the function  X ( |  X ) is continuous.
 Proof of Theorem 4.1 We divide the proof into several steps. To this end, we define and let  X  (  X  ) := argmin { E Step 1. We define two sequences,  X  k  X  2 , that and (4.2).
 Step 2. We define the compact set B = {  X  :  X   X  R n , k  X  k Proposition 2.1, k  X  k Step 3. We define a function g : R n  X  R at  X   X  R n as  X  1 , X  2  X  B , it holds that  X  Danskin X  X  Theorem [6].
  X   X  R n and  X   X   X  , it holds that Indeed, from step 1 we conclude that there exists  X   X  R From this inequality we obtain, passing to limit, inequalit ies (A.9). 4.
 Step 6. The alternating algorithm converges. This claim follows fr om the fact that E convex. Hence, E attained at (  X   X , X  (  X   X  (  X   X  ))) .
 function  X  (  X  ) is continuous.
