 In Section 4 of the main paper, we present results on two benchmarks in terms of average ranking, since classification datasets may not be commensurable in terms of raw validation error. For the sake of com-pleteness, we present here results in terms of average meta-test error. Meta-test error is defined slightly dif-ferently in our two experiments. We also present a PCA of our data in the MLP experiment. 1.1. A case study on AdaBoost In this experiment, meta-test error is obtained by a 5-fold CV on the set of datasets. Figure 1 shows the average meta-test error as a function of the number of iterations. The curves are (obviously) similar in the beginning and at the end of the experiment, but be-tween step 20 and 50, the speedup of reaching a given error level can be more than two-fold wrt. separate tuning, and more than three-fold wrt. random search. 1.2. A controlled experiment with MLPs First, Figure 2 presents the PCA in D of the 20 datasets mentioned in Section 4.2.1, showing non-degeneracy but clustering, as expected. Second, unlike in the case study on AdaBoost , we did not perform meta-cross-validation, but rather acted as if we used SCoT to tune neural networks simultaneously on the
