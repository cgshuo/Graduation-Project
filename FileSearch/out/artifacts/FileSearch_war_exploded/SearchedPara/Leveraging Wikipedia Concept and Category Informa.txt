 As a prevalent type of Web advertising, contextual advertis-ing refers to the placement of the most relevant ads into a Web page, so as to increase the number of ad-clicks. Howev-er, some problems of homonymy and polysemy, low intersec-tion of keywords etc., can lead to the selection of irrelevant ads for a page. In this paper, we present a new contextual advertising approach to overcome the problems, which uses Wikipedia concept and category information to enrich the content representation of an ad (or a page). First, we map each ad and page into a keyword vector, a concept vector and a category vector. Next, we select the relevant ads for a given page based on a similarity metric that combines the above three feature vectors together. Last, we evaluate our approach by using real ads, pages, as well as a great num-ber of concepts and categories of Wikipedia. Experimental results show that our approach can improve the precision of ads-selection e ectively.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Theory
PwC 1 predicts that Web advertising will become the 2nd largest advertising medium in America after television with-in the next 4 years, and its spending will increase from 24.2 billion dollars in 2009 to 34.4 billion dollars in 2014. As an Pric ewaterhouseCoopers -http://www.pwc.com important type of Web advertising, contextual advertising aims to embed the most relevant ads into a page, so as to increase the number of ad-clicks. Most contextual advertis-ing approaches were based on the keyword matching, which estimated the ad relevance based on the co-occurrence of the same keywords between pages and ads [1, 2]. However, as pointed out in [3, 4], the keyword matching may lead to the problems such as homonymy and polysemy, low intersection of keywords, and context mismatch, consequently, degrading its e ectiveness.

To solve these problems, in the area of text classi cation, a new approach called Wikipedia matching was proposed in [5, 6, 7], which uses Wikipedia, the largest knowledge base, as a reference model to enrich the content representation of text documents, so as to improve the accuracy of similarity computation among documents. It was then applied into contextual advertising in [4], and has been proved its e ec-tiveness on solving the problems encountered in the keyword matching. However, this approach may lead to the following problems that decrease its e ectiveness in practical contex-tual advertising. (1) Its limited coverage over semantic con-cepts. To enhance performance, it only chooses a small part of articles from Wikipedia as a reference model. As a result, for many pages not properly characterized by the reference articles, it is impossible to nd out the articles that share the same topics with the pages, thus, leading to the returning of irrelevant ads for the pages. (2) Its time-consuming perfor-mance. To solve the limited coverage over semantics, a very straightforward way is to choose a sucient number of refer-ence articles from Wikipedia. However, this will result in a seriously decreased performance, because the time spending of fulltext matching between all the reference articles and the ads (or pages) is very high.

In order to better balance e ectiveness and eciency, we in this paper present a new contextual advertising approach by combining Wikipedia knowledge with the keyword match-ing, which considers two aspects of similarity between ads and pages, where the keyword-based similarity captures the textual commonness, while the Wikipedia-based similarity measures the relatedness from the semantic perspectives of concepts and categories. Its process consists of the follow-ing three steps. First, we choose a sucient number of arti-cles from Wikipedia, to cover as many concepts as possible. Next, we map each ad (as well as each page) into a keyword v ector, a concept vector and a category vector. Last, com-bining the three feature vectors together, we make the top-N ads selection. The experimental results show that our ap-proach can improve the accuracy of ads selection e ectively. And, due to avoiding time-consuming fulltext matching be-tween all the reference articles and the pages (or ads), our approach also obtains a good running performance.
The keyword matching approach estimated the ad rele-vance by analyzing the co-occurrence of the same keywords within ads and within pages. One of the recent results on applying the keyword matching into contextual advertising was presented in [2], whose main idea was to use a tech-nique called \Impedance Coupling Strategy" to augment a page with additional keywords from other similar pages, so as to overcome the problem of low intersection of keywords between pages and ads. Under the assumption that an ad can be seen as a noisy translation of a page, it was proposed in [8] to select the ads that provide the best translation for a page. In [9], it was proposed to leverage sentiment detec-tion to improve contextual advertising. In [10], the authors proposed to use lexical graphs created from web corpora as a means of computing improved content similarity metrics between ads and pages. However, as pointed out in [4, 5, 6], the main drawback of the keyword matching approach is that it may lead to the problems of homonymy and polyse-my etc., resulting in degrading the relevance of selected ads to their pages.

For solving the problems of homonymy and polysemy etc., the Wikipedia matching was proposed, whose main idea is to leverage the Wikipedia as an intermediate reference mod-el to enhance the semantic representation of text documents and thus improve the precision of similarity measure among documents. In [4], a solution to contextual advertising was proposed by using the Wikipedia matching. In this solution, a group of reference articles is rst chosen from Wikipedia. Next, through using the articles as the intermediate refer-ence model on which the ads and the page is re-expressed as feature vectors, the ads that exhibit more relevance to a targeted page are determined, and a ranking function to se-lect the most relevant ads to the page is constructed. In [5, 6, 7], a similar method was proposed aiming to textual doc-ument clustering. It was proposed in [11] to use Wikipedia to understand a user's query intent, without the need to col-lect large quantities of examples to train an intent classi er. However, the main drawback of the traditional Wikipedia matching is due to the problems that we have mentioned in Section 1 (i.e., the problems of limited coverage of semantic concepts and time-consuming performance) can dramatical-ly degrade the relevance of selected ads with their pages.
Figure 1 shows the framework of our approach to improv-ing contextual advertising using the Wikipedia knowledge, where each ad or page is mapped into a keyword vector, a concept vector and a category vector, and then the three vectors are combined together to measure the similarity be-tween a page and an ad. This process is similar to that used in [6] for clustering, but has a di erent way for construct-ing concept vector and category vector. We below introduce how to construct such two types of feature vectors.
As shown in Figure 1, the process of generating a concept vector includes the following three steps: (1) search related concepts appearing in an ad (or a page); (2) add concepts semantically related to the previous ones; and (3) generate a concept vector based on the frequency values of the concepts that are semantically related to the ad. We below introduce these steps, respectively. (1) Searching Related Concepts . This step aims to determine a set of related concepts for an ad (or a page) and to count their frequency values in the ad. First, we scan a given ad to nd all the Wikipedia titles that appear in the ad. Such titles are called related titles, and the concepts associated with them are called related concepts. We search related titles using a similar method mentioned in [7]. Once we determine a set of titles related to an ad, actually, we determine a set of concepts related to the ad. Next, we compute the frequency of each related concept to the ad. Let count ( t ; a ) be the frequency of a related title t appearing in a page a , which is determined above. Let cots ( t ) be a set of concepts associated with the title t , i.e., each of which satisfying that at least one of its titles is identical to t . Then, the frequency of occurrences of any concept c related to t in a is computed as follows: if t is not a title of the concept c , then count ( c ; t ; a ) = 0; and otherwise, count ( c ; t ; a ) = where sim k ( c ; a ) denotes the similarity between a and c computed using the traditional keyword matching.

In Wikipedia, a concept may have several titles [11], and thus to count the actual frequency of occurrences of a con-cept in a given ad, we need to sum up the frequency value of the concept related to each one of its titles appearing in the ad. Let titles ( a ) be all the titles appearing in an ad a (i.e., the related titles of a ). Then, the frequency of any concept c appearing in a is computed as follows:
Now, we determine a set C ( a ) = f count ( c j ; a ) g N sisting of frequency of each related concept in an ad a , where N a is the number of related concepts in a . Similarly, we de-termine a set C ( p ) = f count ( c j ; p ) g N c p j =1 for a page p . (2) Expanding Related Concepts . There are fewer keywords in an ad a than a generic document, due to its limited size, so C ( a ) is of a smaller size, resulting in the d ecreased precision of similarity computation between pages and ads. However, two Wikipedia concepts are hyperlinked to each other, generally, denoting that the two concepts are semantically related to each other [11]. So, this step aims to expand and enrich C ( a ), based on the hyperlinks within concepts. Actually, the expansion is a process of breadth-rst graph traversal that starts from the concepts associated with C ( a ). First, for each concept c in C ( a ), we obtain all the concepts hyperlinking to c , noted as cots ( c ). Second, we calculate the frequency value of each concept e in cots ( c ): count ( e ; a ) = where num k ( e ; c ) denotes the number of hyperlinks between the concepts e and c .

Third, if count ( e ; a ) is greater a given parameter that is assigned by users to control the depth of graph traversal, then count ( e ; a ) would be added into C ( a ). Such a process is kept on until all the concepts (including the new added ones) associated with C ( a ) are traversed. Similarly, we can expand C ( p ) for a page p . (3) Generating Concept Vector . After the above step, C ( a ) would be expanded with more concepts that are all semantically related to the ad a , i.e., obtaining a set of fre-quency values of concepts for a . Similarly to the traditional keyword matching, based on C ( a ), we can generate a con-cept vector for a , which consists of the tf-idf values [12] of all the concepts in C ( a ). We also generate a concept vector for a page p . Last, we compute the concept-based seman-tic similarity sim c ( a ; p ) between p and a , by using the two concept vectors.
In this subsection, by combining concept vector and the hierarchical relation between concepts and categories or with-in categories, we describe how to generate a category vector for an ad (or a page), and leverage it further to enrich the semantic representation for pages and ads.

Let cots ( d ) be all the concepts that belong to a category d , and cats ( d ) all the immediate subcategories that belong to d (i.e., there is a hyperlink from each category in cats ( d ) to d ). cots ( d ) and cats ( d ) are determined by the hierar-chical categorization system in Wikipedia. Then, we de ne the related frequency of any category d appearing in an ad (or a page) a as follows: count ( d ; a ) = wh ere 1 and 2 are two attenuation coecients, used to balance importance of frequency values of categories in dif-ferent depths.

Now, we determine a set D ( a ) = f count ( d j ; a ) g N sisting of frequency of each category related to an ad a , where N d a is the number of related categories in a . The gen-eration of D ( a ) for an ad a is also a process of breadth-rst graph traversal starting from the concepts associated with C ( a ). However, the traversed graph consists of (1) nodes, concepts and categories, and (2) edges, the hyperlinks with-in concepts and categories, as well as the hyperlinks within Ite m Num ber Gen eral pages in dataset 5 0 Am biguous pages in dataset 2 7 T extual ads in dataset 1 0,244 Wik ipedia articles (Wikipedia concepts) 2 60,000 Wik ipedia categories 1 2,000 Ite m Ex planation K so lely based on keyword vectors C so lely based on concept vector D so lely based on category vector K C b ased on keyword vector and concept vector KD b ased on keyword vector and category vector CD b ased on concept vector and category vector K CD b ased on keyword vector, concept vector and category vector T able 2: 7 contextual advertising strategies based on di erent combinations of feature vectors categories. Furthermore, based on D ( a ), we can generate a category vector for the ad a , which consists of the tf-idf values of all the categories in D ( a ). Similarly, we also gen-erate a category vector for a page p . Last, we can compute the category-based semantic similarity sim d ( a ; p ) between p and a , by using the two category vectors.
Now, each ad (or page) has been represented as three fea-ture vectors: a keyword vector, a concept vector and a cat-egory vector. So, when measuring similarity between an ad and a page, we combine the similarity values calculated us-ing the three feature vectors. For a given page p and an ad a , the similarity between p and a can be computed as follows: sim ( a ; p ) = sim k ( a ; p ) + sim c ( a ; p ) + sim d where the coecients , and indicate the importance of concept vector and category vector in measuring the seman-tic similarity between the page and the ad, which also can be used to balance the keyword matching and the Wikipedia matching. And + + = 1.
We evaluated experimentally our approach using a dataset that contains 50 generic pages, 27 ambiguous pages and 10, 224 ads, more detailed characteristics of which are shown as Table 1. For each page, we collected human judgment scores that describe the relevance of ads selected by each of the candidate strategies (see Table 2). The human judgment St rategy K C D K C K D CD K CD Ti me (ms) 4 5 29 0 76 5 2 93 7 75 75 8 78 0
T able 3: Running performance for the 7 strategies Fi gure 2: Average relevance for the ads selected by the 7 candidate strategies for general pages Fi gure 3: Average relevance for the ads selected by the 7 strategies for general or ambiguous pages scores for the relevance of embedded ads to a page were determined by using a similar method in [5, 6], and were completed by at least two human assessors in a scale between 0.0 to 1.0.

The rst group of experiments aimed to evaluate the ex-ecution time of selecting relevant ads for pages. Here, the work of generating feature vectors for all the ads, has been completed in advance (i.e., completed oine). In each ads-selection for a page, we only concern the execution time con-sumed by (1) generating the feature vectors for the page, and (2) computing the similarity between the page and each ad to choose the most relevant ads. The results are presented in Table 3. The second group of experiments aimed to evaluate the relevance scores of embedded ads to their pages. In our experiments, we invited evaluation assessors to mark score for each ad based on the relevance of the ad to its page, and then averaged the relevance scores given by the evaluation assessors. The results are shown in Figure 2. In the third group of experiments, we have chosen a special dataset that consists of 27 ambiguous pages. In the pages, there are many ambiguous keywords, such as Puma (company versus lion), Rock (person versus music), Driver (software versus car), Game (software versus sports), Window (OS versus glass) and so on. The experimental results of running the seven candidate strategies over the ambiguous pages are shown in Figure 3.
In order to improve contextual advertising, we in this pa-per presented a new approach by incorporating the Wikipedi-a concept and category information into the traditional key-word matching to enrich the content representation of pages and ads. We described how to map each ad (or page) into a keyword vector, a concept vector and a category vector, as well as how to combine the three feature vectors together for making the top-N ads selection.

From the experimental results in Section 4, we have the following conclusions. (1) Our approach obtains a satisfacto-ry running performance (the time spending of ads-selection for a page is less than 1000 ms). This is due to avoiding to conduct the time-consuming fulltext matching operation be-tween pages and all the referenced articles, which is used in the previously published Wikipedia contextual advertising approach. (2) Our approach obtains a better e ectiveness, i.e., it can well improve the accuracy of ads-selection (the relevance score of embedded ads to their pages is generally greater than 0.8). This is due to that we use the Wikipedi-a knowledge to enrich the semantic representation of pages and ads, and use them to measure the semantic similari-ty between pages and ads; while, compared to the surface text information contained in pages and ads, the semantic information has a better stability, i.e., it can re ect out the similarity between pages and ads more accurately. Funding : This work is supported by the National Science Foundation of the Zhejiang Province of China, under grant Y1100137, the Education Research Program of the Zhejiang Province of China, under grant Y201016197, the Science and Technology Program of the Wenzhou City of China, under grant 2009G0339 and grant S20100055. [1] A. Lacerda, M. Cristo, M. G. Andre et al. Learning to [2] B. Ribeiro-Neto, M. Cristo, P. B. Golgher et al. [3] A. Anagnostopoulos, A. Broder, E. Gabrilovich et al. [4] A. N. Pak, and C.-W. Chung. A Wikipedia matching [5] J. Hu, L. J. Fang, Y. Cao, H.-J. Zeng et al. Enhancing [6] X. H. Hu, X. D. Zhang, C. M. Lu et al. Exploiting [7] P. Wang, J. Hu, H.-J. Zeng et al. Using Wikipedia [8] V. Murdock, M. Ciaramita and V. Plachouras. A [9] T.-K. Fan and C.-H. Chang, \Sentiment-oriented [10] S. Papadopoulos, F. Menemenis, Y. Kompatsiaris et [11] J. Hu, G. Wang, F. Lochovsky et al. Understanding [12] H. C. Wu, R. W. P. Luk, K. F. Wong et al.

