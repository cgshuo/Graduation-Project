 Modeling the beyond-topical aspects of relevance are currently gaining popularity in IR evaluation. For example, the discounted cumulated gain (DCG) measure imp licitly models some aspects of higher-order relevance via diminishing the value of relevant documents seen later during retrieval (e.g., due to information cumulated, redundancy, and effort). In this paper, we focus on the concept of negative higher-order relevance (NHOR) made explicit via negative gain values in IR evaluation. We extend the computation of DCG to allow ne gative gain values, perform an experiment in a laboratory setting, and demonstrate the characteristics of NHOR in evaluation. The approach leads to intuitively reasonable performance curves emphasizing, from the user X  X  point of view, the progressi on of retrieval towards success or failure. We discuss normalization issues when both positive and negative gain values are allowed and conclude by discussing the usage of NHOR to characterize test collections.
 H.3.3 [ Information Search and Retrieval ]: Selection process General Terms : Measurement, Performance, Theory. Evaluation, relevance, test coll ections, discounted cumulated gain Information retrieval (IR) evaluations using the Cranfield methodology [1] and its modern ma nifestations, like TREC [16] build upon a limited view of relevance as being topical -and often binary -measure; relevance of a specific document is independent from other documents and the rele vance judgments are assumed to be consistent and stable over tim e. However, when relevance is evaluated by a real user, motivation, cognitive and affective states are involved, and the selection of documents is geared towards maximization of results and/or mini mization of effort [13]. In this paper, we propose a more fine-g rained view on operationalizing these tendencies in IR evaluation. Saracevic [12] discussed a range of relevance levels from algorithmic to topical, cognitive, situational, and emotional (see also [3][11][13]). We utilize the dichotomy topical versus higher-order relevance [5] where the latter refers to the beyond-topical relevance criteria. Generally speaking, the IR evaluation process involves, among others, valuing benefits, co sts and drawbacks, which is a situation dependent process. Ev aluation based on binary topical relevance makes the simplistic assumption that all relevant documents are equally valuable. To gain more realism, it is desirable to use graded relevance assessments to give a richer view of the evaluation case. Weighting schemes can be used to reflect different values the user gives to the documents belonging to different relevance levels [4]. Currently, modeling the higher-order aspects of relevance is gaining popularity in evaluation. For ex ample, the rank-based discount factor in the discounted cumulate d gain (DCG) meas ure models the phenomenon that a relevant document with a greater ranked position is less valuable for the user. The greater the ranked position, the less likely it is that the user will ever examine the document due to time, effort, and cumulated information from documents already seen. These aspects of higher-order rele vance are implicitly included into the discount factor, whose effect can be varied. The concept of discount factor is also valuable fo r the system designer as retrieval techniques placing relevant documen ts later in the ranked results should not be credited as much as techniques ranking them earlier [4]. Although test collections are c onstructed using topical relevance criteria, they can be used in e xperiments which take into account relevance beyond topicality. Indeed, an increasing number of studies utilize weighting schemes and measures diminishing the values of relevant documents s een later during retrieval thus modeling higher-order relevance with in test collections defined by topical relevance criteria (see, e.g., [10][15]). We set forth the following claims: (i) both gains and costs are present in retrieving documents; ( ii) they belong partially to the higher-order relevance domain  X  above topical relevance; (iii) the traditional IR effectiveness visualizations focus on representing the positive aspect of relevance and therefore only partially support understanding users X  searching behavior emerging from the negative higher-order relevance aspects; (iv) (D)CG models higher-order relevance to some degree, but so far has no explicit stance regarding negative higher-order relevance; (v) the current evaluation approaches insufficiently represent the user X  X  viewpoint for the evaluator to allow analyzing, e.g., why the user continues searching or why and when he stops; (vi) the traditional metrics support the idea that searching could be continued for extended lengths because there may be something relevant to be found. The explicit negative gains, instead, make the early stopping understandable, even desirable and rational. Therefore, in Section 2, we argue that representing the retrieval result honest to the user X  X  expe rience requires taking into account both the user X  X  gains and costs/frust rations. In Section 3 we define an extension to the (normalized) DCG measure involving explicit negative gain values representing the frustration inherent in searchers encountering non-relevant documents. In Section 4 we demonstrate the effects of using negative gains in IR evaluation through traditional binary-scale TREC data and graded relevance TREC data, based on both average results over topics and through the evaluation of an individual query demonstrating the ups and downs in user X  X  experience in a session. We also demonstrate the usefulness of negative weights in ch aracterizing test collections. In Sections 5 and 6 we close by discussing the significance of the proposed approach -modeling of user X  X  viewpoint; the possibility of fitting the parameters to empirical findings; hypothesizing; and characterizing reasonable behavior in different types of collections. During the time period from 1959 to 1976 relevance studies in IR were more oriented toward releva nce inherent in the documents and in the query [7]. When document weighting in relevance assessments is made to reflect th e  X  X mount of X  relevant information in the documents, the minimum we ight to consider is obviously impose going to values below zero. The lowest possible amount of relevant information in a document is represented by the numerical value zero. Since 1977 IR researchers have tried to understand, formalize and measure a more subjective, dynamic and multidimensional relevance in the user X  X  context [7]. These goals are served, in part, by the DCG-based measures [4] which allow modeling higher-order relevance aspects related to the c ognitive relevance -user X  X  state of knowledge -as cumulated information affects the value of a document; and situational and motivational relevance related to, e.g., time and effort -how far the user will browse the list. Adhering to document weighting sc hemes always based on values having the same sign may suppre ss the prospects of modeling, observing and measuring effects caused by browsing  X  X nwanted X  documents. Myaeng and Korfhage [8] and Korfhage [6] recognize the need for a bi-directional measur e and define a weighting scheme in terms of the user X  X  satisfaction (in receiving relevant documents) and frustration (in having to examine non-relevant documents); the total measure is defined, e.g., as the satisfaction minus frustration. Cooper [2] argues for the significance of the concept utility , which involves relatedness, quality, novelty, importance and credibility among other things, in addition to topic-relatedness, used to measure the  X  X ltimate worth of the user X  X  encounter with a document X . Cooper X  X  utility also is a bi-directional measure, which may be assigned positive, zero, or negative values. Yang et al. [17] propose an extension of the nDCG metric for assessing the utility of ranked passages, but they do not discuss the visualization point of view, which is our focus in this paper. documents may actually be negative for most users. For example, assume a user inspecting a large se t of entirely off-target documents (zero amount of topical informa tion). The documents may have, individually and as a whole, a nega tive value for the user (that is, not a zero value) because of the effort wasted. Thus, even the whole retrieval session may have a negativ e value yielding less gain than the effort spent  X  the session may fail. A long sequence of off-target documents is bound to make the us er quit and/or switch search strategy. Price et al. [9] give an example of physicians searching information under time pressure. Fo r them, explicit negative values for non-relevant documents may honestly reflect reality, although the IR evaluation tradition consid ers modeling this pressure as secondary. The term negative higher-order relevance (NHOR) refers to the negative aspects of beyond-topical relevance, for example, dissatisfaction, frustration, and un certainty. Negative aspects may be related to the motivational relevance  X  inferred by criteria like (lack of) satisfaction, success and accomplishment  X  and affective relevance  X  the relation between the information and the intents, goals, and emotions of the user [ 13]. While the concept of NHOR is unavoidably at least as elusive as the concept of higher-order the importance of making the negativ e user sentiments visible in evaluation. Consequences of allowing negative gain values include the mindset that systems, methods and sessions can succeed or fail -more or less -both for individual queries and for sets of queries. Simply, negative cumulated value (or discounted negative cumulated value) at a given rank, by definition, i ndicates failure assuming that the values used reasonably represen t reality, while positive values indicate success. An essential characteristic of DCG curves is that they may alternate up and down if negative document values are allowed. Moreover, the curve of any topic will finally turn negative (a failure) if sufficiently many ranks are considered. In other words, success in ranked IR is, by nature, temporary in a retrieval session -unlike traditional IR evaluation suggests. Because of the cumulating costs caused by browsing through sequences of non-relevant documents, searchers in real life consider whether to stop or continue a search session in orde r to optimize its success and avoid failure. This kind of behavior can be modeled in the laboratory setting if we allow negative gain values. In this paper, cumulated gain (CG) and its discounted version, DCG, are computed as in [4]. However, the gain vector of the retrieved result is constructed so that bot h positive and negative values are allowed. The original formulations were intended for positive values only. With no loss on generality, we first discuss negative values and normalization regarding CG. We operationalize negative higher-order relevance (NHOR) by allowing negative gain values in CG -based measures (Figure 1). Let us assume a four-point rele vance scale from non-relevant (represented by n), to marginally (M), fairly (F), and highly (H) relevant documents. Let us denote the set of gain values given to documents by an ordered sequence -5/0/5/10 corresponding to scores n/M/F/H. Figure 1. Schematic picture of the effect of negative gain values on cumulated gain (CG). The topmost curve: the best possible CG based on values -5/0/5/10 for the corresponding scores n/M/F/H and recall base sizes N-6/3/2/1 (see text below for explanation). The middle curve: the actual CG result based on ranked result &lt;n, n, H, F, F, M, n, n, ...&gt;. The bottommost curve: the worst possible CG. Let the corresponding recall base sizes be N -6/3/2/1 for the sample topic. In other words, highly rele vant documents (H) are given gain value 10, and one such document ex ists in the recall base for the sample topic. Fairly relevant docum ents (F) are given gain value 5; their recall base size is two. Marg inally relevant documents (M) are given value 0; their recall base size is three; all the remaining documents are non-relevant (n) ( N -6 documents assuming a database of N documents) and they are given the negative value -5. In other words, non-relevant docum ents retrieved and seen during retrieval are penalized. The ideal gain vector is constructed by arranging the gain values in descending order, i.e., &lt;10, 5, 5, 0, 0, 0, -5, ...&gt;; the worst case gain vector by taking the reverse order &lt;-5, these two boundaries. starts to descend after rank 6.  X  X ravity X  caused by browsing through sequences of negative values exis ts and therefore the user cannot avoid ending up with a negative fina l CG value (a failure) sooner or later (leading to stopping or revising the query). To normalize CG at any rank one divides it by the corresponding ideal value, e.g., assuming actual value 35 and ideal value 55, the normalized CG is 35/55 = 64 %. If only the conventional non-negative gain values are allowed, the normalized values are in the range [0, 1] and they express the share of ideal performance cumulated [4] by the technique evaluated. If negative values are allowed, this approach gives incorrect results, e.g., in Figure 1 at rank 11 the formula gives a fallacious normalized value -15/-5 = +300 %. To approach the normalization probl em in the case of negative gain values, let us briefly denote the focal points related to CG at any rank j ( j  X  1), as follows (Figure 1): icg[j] ideal CG value at rank j cg[j] actual cumulated gain value at rank j wcg[j] worst possible CG value at rank j Negative CG values could be normalized in relation to the worst possible CG value and by adding a minus sign, in other words, by  X  X irroring X  the case of negative valu es in the spirit of the original formula. Here, the normalized va lues would be between -100 % ...100 %. For the positive CG values, +100 % would denote a  X  X ery good X  result; for the negative values, -100 % would denote a  X  X ery bad X  result in relation to the worst case, and the values close to 0 % would denote  X  X eak X  results. Howe ver, such values might be misleading as the formula may give high negative values at low ranks, and hence the fo rmula is discarded. Our proposed solution in this paper is to normalize using the worst possible result at a given rank as the base of comparison. The horizontal axis of the original normalization formula used as the base of comparison is simply replaced by the worst possible CG level. In other words, the distances between the focal curve points at rank j are used, namely cg[j] -wcg[j] and icg[j] -wcg[j]. The normalized cumulated gain value ncg[j] at rank j is computed as follows: This normalization method gives  X  X ptimistic X  normalized values due to the dominant negative baseline. Therefore, also the actual CG (or DCG) values should be presente d. For example, at rank 9, we get nCG[9] = (-5-(-45)/(+5-(-45)) = 40/50 = 80 % while the actual score is negative (-5). Note that the normalized CG curve (averaged over topics) based on both conve ntional and negative document weights does not model explicitly such a negative net result  X  the normalized curve typically keeps on going (even if only slightly) upwards even at high ranks. The purpose of discounting is to  X  X une X  the CG curves: discounting models the phenomenon of gain and loss related to the higher-order relevance. Discounting of negative values can be performed in a similar way as for the positive gain values, because the greater the the non-relevant document due to time, effort, or cumulated information and frustration from documents already seen. Computing the normalized DCG value ndcg[j] at rank j is analogous to computing ncg[j ] above. We simply replace the CG curve points icg[j], cg[j], and wcg[j] in the formula above by the corresponding DCG curve points. Di scounting in DCG is based on the logarithm of the rank of each document. It is controlled by the base of the logarithm, a small base causing discount to be rapid [4]. In our experiments, we will use th ree log bases: the base 2 modeling an impatient user, the base 4 a mode rately patient user, and the base 10 a patient user. Our purpose is to explore what ar e the consequences if NHOR is modeled by explicit negative weights. We illustrate the difference in results between allowing (NHOR-bas ed modeling) versus not using negative gain values (the conventi onal modeling). First, we use one of the five best performing runs of the TREC 8 ad hoc track, based on binary relevance judgments (50 topics). Second, we experiment with non-binary relevance test da ta (41 topics from TREC 7 and 8 having graded relevance assessmen ts) -the documents are highly, fairly or marginally relevant, or non-relevant (for details, see [14]) in respect to the topic. In the latte r case the retrieval system used is Lemur 1 with language modeling and two-stage smoothing options. DCG-based evaluation allows modeli ng the effects of various user scenarios via selection of the va lues given to documents belonging to various levels of relevance, and the logarithm base used during discounting. Figure 2 presents average results for 50 topics for the TREC 8 ad hoc track using binary relevance da ta (Run A). The query keys were selected from the topic and descriptor fields. The upper DCG curve visualizes the conventional situa tion based on 0/1 weighting for the non-relevant and relevant documen ts. The lower curve is based on the same retrieved result but evalua ted from the viewpoint of a user defining negative values for enc ountering non-relevant documents (-1/1 weighting). Negative values imply that a turning point is rapidly reached after which the DCG curve starts to progress towards failure -around rank 18 the average curve goes below the horizontal axis. Figure 2. Average DCG results for 50 topics (TREC 8) based on binary relevance data. Visualization of the same retrieval result is compared using binary weighting 0/1 corresponding to non-relevant/relevant documents (the upper curve), and the corresponding -1/1 weighting (the lower curve). The log base for discounting is 4 (a moderately patient user). We compare the curves in Figure 2 to the traditional binary precision/recall graph (Figure 3). The traditional precision/recall result does not explicitly model negative relevance aspects. As negative values are not allowed, an incentive is sustained that Lemur is an open-source toolkit de signed to facilitate research in language modeling and information retrieval. See http://www.lemurproject.org searching should be continued for extended lengths because there may be something relevant to be found there -whereas the explicit incorporation of negative gains makes early stopping understandable, even desirable and rational. Figure 3. Binary precision/recall (TREC 8, N=50). Figure 4 illustrates the properties of negative gain values using graded relevance data. Average DCG results are presented over topics (N=41) based on TREC 7 and 8 data (Run B). The query keys were selected from the topic and descriptor fields. A patient user (log base 10) is modeled in discounting. Figure 4. Average DCG results for 41 topics (TREC 7 and 8) using graded relevance data. The same retrieved result is illustrated based on non-negative weighting 0/0/5/10 with logarithm base 10 (the upper curve), and with values -2/0/5/10 (the lower curve) with the same logarithm base, emphasizing a  X  X urning point X . The lower curve illustrates that when negative gain values are used the DCG value first increases (up to rank 16) where the maximum sink, thereby illustrating progression towards failure . After the turning point the negative cumulated values start outweighing the positive values. Eventually the w hole DCG value turns negative (a failure ) -on the average around rank 50. Note the qualitative concepts (shown in italics) s uggested by accepting NHOR-based weighting, based on the quantitative data. These concepts are applicable for averaged curves over topics, and for analyzing individual topics. Importantly , the upper curve (based on conventional weights) does not invite one to recognize such concepts. Conventional DCG wei ghts do not model bi-directional progression towards success and failure, thus the upper curve keeps improving. Next we look at the average DCG curves assuming the same retrieval output viewed by an im patient user (e.g., a searcher working under hard time pressure, Figure 5). The results in Figure 5 look very different from the results in the previous figure  X  while the run result is the same. For an impatient user encountering non-relevant documents causes much fru stration and dissatisfaction. The lower curve in Figure 5 shows the results of modeling this tendency via larger negative gain values for the non-relevant documents. The discounted gain turns negative ra pidly (around rank 8). Again here, conventional weighting (the uppe r curve) shows ascending gain towards success for the impatient user  X  probably not modeling the phenomenon really correctly as th e negative aspects of NHOR are not represented explicitly. Comparing Figures 4 and 5 underlines that the user X  X  perspective in evaluation really matters. Figure 5. Average DCG results modeling an impatient user via traditional weighting 0/0/0/10 (log base 2) (the upper curve) and allowing negative values -5/0/0/10 with the same logarithm base (the lower curve). Next we illustrate the case of an individual topic using two types of weightings for a patient user (Figur e 6). The lower curve visualizes the likely user sentiment that the re trieval result first gets worse and then improves. Figure 6. Comparing two DCG vi sualizations for one topic (TREC topic #387) for a patient user and traditional weighting 0/0/5/10 with logarithm base 10 (the upper curve), and using values -2/0/5/10 with the same l ogarithm base (the lower curve). A patient user might be able to st and the initial failure and continue searching despite the fact that the DCG curve keeps sinking at first. The upper curve (conventional weighti ng) could also be interpreted this way, but it does not naturally explicate descending expectations or negative sentiments. Next we illustrate a different case for the same topic  X  now for an impatient user (Figure 7). Figure 7. Same retrieval result as in previous picture (TREC topic #387) but an impatient user and traditional weighting 0/0/0/10 with logarithm base 2 (the upper curve), and weighting -5/0/0/10 with the same logar ithm base (the lower curve). For an impatient user the retrieve d result looks depressing. He will probably stop browsing the results rapidly. The lower curve emphasizes the severe, growing frustration and dissatisfaction experienced by a user encounter ing non-relevant documents. The conventional result (the upper curve) could also be interpreted this way, but it does not really support this kind of interpretation  X  the researcher must be experienced to interpret it that way. Next we show the effects of the corpus on DCG result by using log base 10 (a patient user) and doc ument value set -2/0/5/10. We divided the 41 TREC topics with gr aded assessments into two sets. The Small RB refers to the set of 20 topics having a small recall base (below median: avg = 12.3 relevant docs). The Large RB refers to the set of 21 topics having a reca ll base size above median (avg = 45.7 relevant docs) (Figure 8). Figure 8. Corpus analysis: run results, ideal curves and the worst possible curve related to two separate sets of topics. Figure 8 exhibits two ideal DCG cu rves -the topmost one for the 21 Large RB Topics and the next curve for the 20 Small RB Topics. Below them, curves for Run B for the Large and Small RB Topics are presented, respectively. The bottommost curve presents the worst possible DCG result (identical for both two topic sets). As we can see, the average ideal DCG for the Large RB Topics peaks late, around rank 65, while the average ideal for the Small RB Topics peaks soon, around rank 20. Even at rank 100 the former curve keeps a high value; the latter curve is about to turn negative. This suggests that if a test collection has only few relevant documents per even if the conditions were ideal. This kind of analysis can be used to characterize test collections and predict user behavior in them utilizing negative gain values. The general finding is echoed in the Run B curves: average DCG result regarding the Large RB topic set persists positive much longer (until rank 69) than the Small RB results (until rank 41). Interestingly, average DCG results for both topic sets peak equally high, albe it at different ranks (ranks 27 vs. 16). Maybe the Large RB Topics are tougher for Run B? nDCG has become popular in IR evaluation recently. As the last example we illustrate nDCG results using conventional and negative weighting (Figure 9). (Note that the corresponding DCG results are presented in Figure 4.) The normalized curves look surprisi ngly alike. Normalization loses information regarding the bi-directional effectiveness measure; while it serves the system designe r, it hardly helps in understanding the user behavior. Therefore, if user-oriented evaluation is used in DCG-based evaluation, not only nDCG values should be presented, but also DCG values should be shown because they are able to indicate user X  X  frustration as well. Figure 9. nDCG results for 41 topics (TREC 7 and 8) with CG values 0/0/5/10 and -2/0/5/10, with logarithm base 10. Information regarding the effectiveness from the user viewpoint is obviously lost (compare to Fig. 4). Visualizing the user X  X  viewpoint regarding the retrieved result requires taking into account both the user X  X  gains and costs belonging partially to the higher-order relevance domain  X  which is not explicitly supported by the traditional IR evaluation. Therefore, we defined an extension to the (normalized) DCG measure involving explicit negative gain values, which may be used to represent the negative higher-order relevance aspects, e.g., frustration, dissatisfaction, and un certainty. We demonstrated the effects of negative gains in IR evaluation in both traditional binary-scale TREC test collection and graded relevance TREC test collection. We showed both average results over topics, and demonstrated the visualization of an individual query emphasizing the ups and downs in user X  X  expe rience in a session. Finally we demonstrated the usefulness of negative weights in characterizing test collections. The resulting alternating DCG curves emphasize the viewpoint of IR as a risk-taking behavior, where the systems, methods and sessions may succeed or fail  X  both for the individual queries and for sets of queries. For example, the DCG value averaged over topics typically increases at firs t (up to a specific rank) until the maximum utility is reached; after this turning point the DCG starts to sink, thereby illustrating progre ssion towards failure. Positive and negative DCG values at each rank express the magnitude of success and failure. After the turning point the negative cumulated values start outweighing the positive ones. Finally, the  X  X ravity X  caused by browsing through sequences of negativ e values means that the user cannot avoid ending up with a failure (negative final CG and DCG) if he keeps on browsing a very long sequence of ranks. Negative values make visible the qualitative concepts of maximum (and minimum) utility, turning point(s), and progression towards success or failure, which all are useful in analyzing the averaged results over topics -or results regarding individual topics. Note that the conventional measures like MAP, or DCG-based measures based on conventional weights do not invite one to recognize these concepts. If bi-directional progression towa rds success and failure is not modeled in DCG, the performan ce curves keep going upwards which may be considered counter-intuitive  X  when real-life suggests stopping. We are aware of the fact that we did not motivate any specific positive and negative values. However, document values (also context dependent. In this paper we remained at the level of principle  X  but the parameter va lues can be calibrated based on empirical findings: e.g., users can be interviewed and their behavior monitored in order to identify parameter values that make the (D)CG curves closely match real user experience in a specific retrieval situation. Conversely, one may hypothesize the behavior of users facing a given test collection through a user scenario. Finally, if negative gain values feel elus ive, one may find consolation by thinking that the positive-only values used so far are even more remote to real life. Negative values facilitate also cor pus analysis of (graded) test collections. The ideal and worst-case (D)CG curves can be drawn -averaged over topics -for each separate test collection using some specific parameters (discounts a nd relevance weighting). For each collection, the size and form of the area above vs. below the horizontal axis determined by the ideal curve characterizes the collection. While this information is hidden in the recall bases of the test collections, the proposed analysis makes it explicit and allows inspection with varying user scenarios. Both the gains and costs of using systems may be important for real users retrieving documents. They belong partially to the higher-order relevance domain, above topical relevance. Traditional IR evaluation insufficiently represents the user X  X  viewpoint and thus, insufficiently allows analyzing, e.g., why the user continues searching or why and when he stops. Therefore, traditional IR evaluation only partially supports understanding users X  searching behavior. DCG-based measurements model hi gher-order relevance to some degree, but so far it has not had an explicit stance regarding negative higher-order relevance. We argued that honest evaluation regarding user X  X  experience requires taking in to account both the user X  X  gains and costs/frustrations. Thus we defined an extension to the (normalized) DCG measure involving explicit negative gain values and utilized it to visualize the frustration inherent in encountering non-relevant documents. We demons trated the effects of using negative gains in IR evaluation through traditional binary-scale TREC data and through graded relevance TREC data, based on average results over topics, and ba sed on using individual queries to demonstrate the ups and downs in user X  X  experience in sessions. Finally we demonstrated the usefulness of our approach in characterizing test collections. The authors are grateful to the members of the FIRE research group for their useful comments. This work was supported, in part, by Academy of Finland under grants #1124131, and #204978. [1] Cleverdon, C. W. (1991) The Significance of the Cranfield Tests [2] Cooper, W.S. (1973) On Selecting a Measure of Retrieval [3] Cosijn, E. &amp; Ingwersen, P. (2000) Dimensions of Relevance. [4] J X rvelin, K. &amp; Kek X l X inen, J. (2002) Cumulated Gain-Based [5] Kek X l X inen, J. &amp; J X rvelin, K. (2002) Evaluating Information [6] Korfhage, R. R. (1997), Information Storage and Retrieval , [7] Mizzaro, S. (1996) Relevance: The Whole (Hi)story. Journal of [8] Myaeng, S. H. &amp; Korfhage, R. R. (1990) Integration of User [9] Price, S.L., Nielsen, M.L., Delcambre, L.M.L., &amp; Vedsted, P. [10] Sakai, T. (2007) On Penalis ing Late Arrival of Relevant [11] Saracevic, T. (1975) Relevance: A Review of and a Framework [12] Saracevic, T. (1996) Relevance Reconsidered  X 96. In [13] Saracevic, T. (2006) Relevance: A Review of the Literature [14] Sormunen, E. (2002). Liberal Relevance Criteria of TREC -[15] Voorhees, E. M. (2001) Evaluation by Highly Relevant [16] Voorhees, E. M. (2007) TREC: Continuing Information [17] Yang, Y., Lad, A., Lao, N., Harpale, A., Kisiel, B., &amp; Rogati, 
