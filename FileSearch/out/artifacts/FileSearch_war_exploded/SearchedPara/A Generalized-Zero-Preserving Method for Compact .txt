 The use of bit vectors is almost as old as HPSG parsing itself. Since they were first suggested in the programming languages literature (A  X   X t-Kaci et al., 1989) as a method for computing the unifica-tion of two types without table lookup, bit vectors have been attractive because of three speed advan-tages:  X  The classical bit vector encoding uses bitwise  X  Hash tables, the most common alternative,  X  With large type signatures, the table that in-
Why isn X  X  everyone using bit vectors? For the most part, the reason is their size. The classical encoding given by A  X   X t-Kaci et al. (1989) is at least as large as the number of meet-irreducible types, which in the parlance of HPSG type signatures is the number of unary-branching types plus the number of maximally specific types. For the En-glish Resource Grammar (ERG) (Copestake and Flickinger, 2000), these are 314 and 2474 respec-tively. While some systems use them nonetheless (PET (Callmeier, 2000) does, as a very notable ex-ception), it is clear that the size of these codes is a source of concern.
 Again, it has been so since the very beginning: A  X   X t-Kaci et al. (1989) devoted several pages to a discussion of how to  X  X odularize X  type codes, which typically achieves a smaller code in ex-change for a larger-time operation than bitwise AND as the implementation of type unification. However, in this and later work on the subject (e.g. (Fall, 1996)), one constant has been that we know our unification has failed when the imple-mentation returns the zero vector. Zero preserva-tion (Mellish, 1991; Mellish, 1992), i.e., detect-ing a type unification failure, is just as important as obtaining the right answer quickly when it suc-ceeds.

The approach of the present paper borrows from recent statistical machine translation re-search, which addresses the problem of efficiently representing large-scale language models using a mathematical construction called a Bloom filter (Talbot and Osborne, 2007). The approach is best combined with modularization in order to further reduce the size of the codes, but its novelty lies in the observation that counting the number of one bits in an integer is implemented in the basic in-struction sets of many CPUs. The question then arises whether smaller codes would be obtained by relaxing zero preservation so that any resulting vector with at most  X  bits is interpreted as failure, with  X   X  1 .

Penn (2002) generalized join-preserving encod-ings of partial orders to the case where more than one code can be used to represent the same ob-ject, but the focus there was on codes arising from successful unifications; there was still only one representative for failure. To our knowledge, the present paper is the first generalization of zero preservation in CL or any other application do-main of partial order encodings.
 We note at the outset that we are not using Bloom filters as such, but rather a derandomized encoding scheme that shares with Bloom filters the essential insight that  X  can be greater than zero without adverse consequences for the required al-gebraic properties of the encoding. Deterministic variants of Bloom filters may in turn prove to be of some value in language modelling. 1.1 Notation and definitions A partial order  X  X, v X  consists of a set X and a reflexive, antisymmetric, and transitive binary re-lation v . We use u t v to denote the unique least upper bound or join of u,v  X  X , if one exists, and u u v for the greatest lower bound or meet . If we need a second partial order, we use for its order relation and g for its join operation. We are espe-cially interested in a class of partial orders called meet semilattices , in which every pair of elements has a unique meet. In a meet semilattice, the join of two elements is unique when it exists at all, and there is a unique globally least element  X  ( X  bot-tom  X ).

A successor of an element u  X  X is an element v 6 = u  X  X such that u v v and there is no w  X  X with w 6 = u,w 6 = v, and u v w v v , i.e., v fol-lows u in X with no other elements in between. A maximal element has no successor. A meet irre-ducible element is an element u  X  X such that for any v,w  X  X , if u = v u w then u = v or u = w . A meet irreducible has at most one successor.
Given two partial orders  X  X, v X  and  X  Y,  X  , an embedding of X into Y is a pair of functions f : X  X  Y and g : ( Y  X  Y )  X  { 0 , 1 } , which may have some of the following properties for all u,v  X  X :  X  defined ( u t v )  X  g ( f ( u ) ,f ( v )) = 0 (3)
With property (1), the embedding is said to pre-serve order; with property (2), it preserves suc-cess; with property (3), it preserves failure; and with property (4), it preserves joins. Intuitively, taking the join of two types in a type hi-erarchy is like taking the intersection of two sets. Types often represent sets of possible values, and the type represented by the join really does repre-sent the intersection of the sets that formed the in-put. So it seems natural to embed a partial order of types  X  X, v X  into a partial order (in fact, a lattice) of sets  X  Y,  X  , where Y is the power set of some set Z , and is the superset relation  X  . Then join g is simply set intersection  X  . The embedding function g , which indicates whether a join exists, can be naturally defined by g ( f ( u ) ,f ( v )) = 0 if and only if f ( u )  X  f ( v ) =  X  . It remains to choose the underlying set Z and embedding function f .
A  X   X t-Kaci et al. (1989) developed what has be-come the standard technique of this type. They set Z to be the set of all meet irreducible elements in X ; and f ( u ) = { v  X  Z | v w u } , that is, the meet irreducible elements greater than or equal to u . The resulting embedding preserves order, suc-cess, failure, and joins. If Z is chosen to be the maximal elements of X instead, then join preser-vation is lost but the embedding still preserves or-der, success, and failure. The sets can be repre-sented efficiently by vectors of bits. We hope to minimize the size of the largest set f (  X  ) , which determines the vector length.

It follows from the work of Markowsky (1980) that the construction of A  X   X t-Kaci et al. is optimal among encodings that use sets with intersection for meet and empty set for failure: with Y defined as the power set of some set Z , v as  X  , t as  X  , and then the smallest Z that will preserve order, suc-cess, failure, and joins is the set of all meet irre-ducible elements of X . No shorter bit vectors are possible.

We construct shorter bit vectors by modifying the definition of g , so that the minimality results no longer apply. In the following discussion we present first an intuitive and then a technical de-scription of our approach. 2.1 Intuition from Bloom filters Vectors generated by the above construction tend to be quite sparse, or if not sparse, at least bor-ing. Consider a meet semilattice containing only the bottom element  X  and n maximal elements all incomparable to each other. Then each bit vector would consist of either all ones, or all zeroes ex-cept for a single one. We would thus be spending n bits to represent a choice among n + 1 alterna-tives, which should fit into a logarithmic number of bits. The meet semilattices that occur in prac-tice are more complicated than this example, but they tend to contain things like it as a substruc-ture. With the traditional bit vector construction, each of the maximal elements consumes its own bit, even though those bits are highly correlated.
The well-known technique called Bloom fil-tering (Bloom, 1970) addresses a similar issue. There, it is desired to store a large array of bits subject to two considerations. First, most of the bits are zeroes. Second, we are willing to accept a small proportion of one-sided errors, where ev-ery query that should correctly return one does so, but some queries that should correctly return zero might actually return one instead.

The solution proposed by Bloom and widely used in the decades since is to map the entries in the large bit array pseudorandomly (by means of a hash function) into the entries of a small bit ar-ray. To store a one bit we find its hashed location and store it there. If we query a bit for which the answer should be zero but it happens to have the same hashed location as another query with the an-swer one, then we return a one and that is one of our tolerated errors.

To reduce the error rate we can elaborate the construction further: with some fixed k , we use k hash functions to map each bit in the large array to several locations in the small one. Figure 1 il-lustrates the technique with k = 3 . Each bit has three hashed locations. On a query, we check all three; they must all contain ones for the query to return a one. There will be many collisions of indi-vidual hashed locations, as shown; but the chances are good that when we query a bit we did not in-tend to store in the filter, at least one of its hashed locations will still be empty, and so the query will return zero. Bloom describes how to calculate the optimal value of k , and the necessary length of the hashed array, to achieve any desired bound on the error rate. In general, the hashed array can be much smaller than the original unhashed ar-ray (Bloom, 1970).

Classical Bloom filtering applied to the sparse vectors of the embedding would create some per-centage of incorrect join results, which would then have to be handled by other techniques. Our work described here combines the idea of using k hash functions to reduce the error rate, with perfect hashes designed in a precomputation step to bring the error rate to zero. 2.2 Modified failure detection In the traditional bit vector construction, types map to sets, join is computed by intersection of sets, and the empty set corresponds to failure (where no join exists). Following the lead of Bloom filters, we change the embedding function  X  for some constant  X  . With  X  = 0 this is the same as before. Choosing greater values of  X  allows us to re-use set elements in different parts of the type hierarchy while still avoiding collisions.

Figure 2 shows an example meet semilattice. In the traditional construction, to preserve joins we must assign one bit to each of the meet-irreducible elements { d,e,f,g,h,i,j,k,l,m } , for a total of ten bits. But we can use eight bits and still pre-serve joins by setting g ( f ( u ) ,f ( v )) = 0 if and only if | f ( u )  X  f ( v ) | X   X  = 1 , and f as follows. Figure 2: An example meet semilattice;  X  is the most general type.

As a more general example, consider the very simple meet semilattice consisting of just a least element  X  with n maximal elements incompara-ble to each other. For a given  X  we can represent this in b bits by choosing the smallest b such that distinct choice of the bits. With optimal choice of  X  , b is logarithmic in n . 2.3 Modules As A  X   X t-Kaci et al. (1989) described, partial or-ders encountered in practice often resemble trees. Both their technique and ours are at a disadvantage when applied to large trees; in particular, if the bottom of the partial order has successors which are not joinable with each other, then those will be assigned large sets with little overlap, and bits in the vectors will tend to be wasted.

To avoid wasting bits, we examine the partial order X in a precomputation step to find the mod-ules , which are the smallest upward-closed sub-sets of X such that for any x  X  X , if x has at least two joinable successors, then x is in a mod-ule. This is similar to ALE X  X  definition of mod-ule (Penn, 1999), but not the same. The definition of A  X   X t-Kaci et al. (1989) also differs from ours. Under our definition, every module has a unique least element, and not every type is in a module. For instance, in Figure 2, the only module has a as its least element. In the ERG X  X  type hierarchy, there are 11 modules, with sizes ranging from 10 to 1998 types.

To find the join of two types in the same mod-ule, we find the intersection of their encodings and check whether it is of size greater than  X  . If the types belong to two distinct modules, there is no join. For the remaining cases, where at least one of the types lacks a module, we observe that the mod-ule bottoms and non-module types form a tree, and the join can be computed in that tree. If x is a type in the module whose bottom is y , and z has no module, then x t z = y t z unless y t z = y in which case x t z = x ; so it only remains to compute joins within the tree. Our implementa-tion does that by table lookup. More sophisticated approaches could be appropriate on larger trees. Ideally, we would like to have an efficient algo-rithm for finding the best possible encoding of any given meet semilattice. The encoding can be rep-resented as a collection of sets of integers (repre-senting bit indices that contain ones), and an opti-mal encoding is the collection of sets whose over-all union is smallest subject to the constraint that the collection forms an encoding at all. This com-binatorial optimization problem is a form of set programming ; and set programming problems are widely studied. We begin by defining the form of set programming we will use.
 Definition 1 Choose set variables S 1 ,S 2 ,...,S n to minimize b = | S n i =1 S i | subject to some con-straints of the forms | S i |  X  r i , S i  X  S j , S i + S | S i  X  S j |  X   X  , and S i  X  S j = S k . The constant  X  is the same for all constraints. Set elements may be arbitrary, but we generally assume they are the integers { 1 ...b } for convenience.

The reduction of partial order representation to set programming is clear: we create a set variable for every type, force the maximal types X  sets to contain at least  X  + 1 elements, and then use sub-set to enforce that every type is a superset of all its successors (preserving order and success). We limit the maximum intersection of incomparable types to preserve failure. To preserve joins, if that property is desired, we add a constraint S i + S j for every pair of types x i 6v x j and one of the form S i  X  S j = S k for every x i ,x j ,x k such that x i t x j = x k ..

Given a constraint satisfaction problem like this one, we can ask two questions: is there a feasi-ble solution, assigning values to the variables so all constraints are satisfied; and if so what is the optimal solution, producing the best value of the objective while remaining feasible? In our prob-lem, there is always a feasible solution we can find by the generalized A  X   X t-Kaci et al. construc-tion (GAK), which consists of assigning  X  bits shared among all types; adding enough unshared new bits to maximal elements to satisfy cardinal-ity constraints; adding one new bit to each non-maximal meet irreducible type; and propagating all the bits down the hierarchy to satisfy the subset constraints. Since the GAK solution is feasible, it provides a useful upper bound on the result of the set programming.

Ongoing research on set programming has pro-duced a variety of software tools for solving these problems. However, at first blush our instances are much too large for readily-available set program-ming tools. Grammars like ERG contain thou-sands of types. We use binary constraints be-tween every pair of types, for a total of millions of constraints X  X nd these are variables and con-straints over a domain of sets, not integers or re-als. General-purpose set programming software cannot handle such instances. 3.1 Simplifying the instances First of all, we only use minimum cardinality con-straints | S i |  X  r i for maximal types; and every r  X   X  + 1 . Given a feasible bit assignment for a maximal type with more than r i elements in its set S , we can always remove elements until it has ex-actly r i elements, without violating the other con-straints. As a result, instead of using constraints | S i |  X  r i we can use constraints | S i | = r i . Doing so reduces the search space.
 Subset is transitive; so if we have constraints S i  X  S j and S j  X  S k , then S i  X  S k is implied and we need not specify it as a constraint. Simi-larly, if we have S i  X  S j and S i * S k , then we have S j * S k . Furthermore, if S i and S j have maximum intersection  X  , then any subset of S i also has maximum intersection  X  with any subset of S k , and we need not specify those constraints either.

Now, let a choke-vertex in the partial order  X  X, v X  be an element u  X  X such that for ev-ery v,w  X  X where v is a successor of w and u v v , we have u v w . That is, any chain of suc-cessors from elements not after u to elements after u , must pass through u . Figure 2 shows choke-vertices as squares. We call these choke-vertices by analogy with the graph theoretic concept of cut-vertices in the Hasse diagram of the partial or-der; but note that some vertices (like j and k ) can be choke-vertices without being cut-vertices, and some vertices (like c ) can be cut-vertices without being choke-vertices. Maximal and minimal ele-ments are always choke-vertices.

Choke-vertices are important because the op-timal bit assignment for elements after a choke-vertex u is almost independent of the bit assign-ment elsewhere in the partial order. Removing the redundant constraints means there are no con-straints between elements after u and elements before, or incomparable with, u . All constraints across u must involve u directly. As a result, we can solve a smaller instance consisting of u and everything after it, to find the minimal number of bits r u for representing u . Then we solve the rest of the problem with a constraint | S u | = r u , ex-cluding all partial order elements after u , and then combine the two solutions with any arbitrary bi-jection between the set elements assigned to u in each solution. Assuming optimal solutions to both sub-problems, the result is an optimal solution to the original problem. 3.2 Splitting into components If we cut the partial order at every choke-vertex, we reduce the huge and impractical encoding problem to a collection of smaller ones. The cut-ting expresses the original partial order as a tree of components , each of which corresponds to a set programming instance. Components are shown by the dashed lines in Figure 2. We can find an op-timal encoding for the entire partial order by opti-mally encoding the components, starting with the leaves of that tree and working our way back to the root.

The division into components creates a collec-tion of set programming instances with a wide range of sizes and difficulty; we examine each in-stance and choose appropriate techniques for each one. Table 1 summarizes the rules used to solve an instance, and shows the number of times each rule was applied in a typical run with the modules ex-tracted from ERG, a ten-minute timeout, and each  X  from 0 to 10.

In many simple cases, GAK is provably opti-mal. These include when  X  = 0 regardless of the structure of the component; when the component consists of a bottom and zero, one, or two non-joinable successors; and when there is one element (a top ) greater than all other elements in the com-ponent. We can easily recognize these cases and apply GAK to them.

Another important special case is when the
Condition Succ. Fail. Method  X  = 0 216 GAK (optimal)  X  top 510 GAK (optimal) 2 successors 850 GAK (optimal) 3 or 4 successors only ULs 420 b -choose-(  X  +1) before UL removal after UL removal remaining 50 GAK Table 1: Rules for solving an instance in the ERG component consists of a bottom and some num-ber k of pairwise non-joinable successors, and the successors all have required cardinality  X  + 1 . Then the optimal encoding comes from finding the smallest b such that b  X  +1 is at least k , and giving each successor a distinct combination of the b bits. 3.3 Removing unary leaves For components that do not have one of the spe-cial forms described above, it becomes necessary to solve the set programming problem. Some of our instances are small enough to apply constraint solving software directly; but for larger instances, we have one more technique to bring them into the tractable range.
 Definition 2 A unary leaf (UL) is an element x in a partial order  X  X, v X  such that x is maximal and x is the successor of exactly one other element.
ULs are special because their set programming constraints always take a particular form: if x is a UL and a successor of y , then the constraints on its set S x are exactly that | S x | =  X  + 1 , S x  X  S y , and S x has intersection of size at most  X  with the set for any other successor of y . Other constraints disappear by the simplifications described earlier.
Furthermore, ULs occur frequently in the par-tial orders we consider in practice; and by increas-ing the number of sets in an instance, they have a disproportionate effect on the difficulty of solv-ing the set programming problem. We therefore implement a special solution process for instances containing ULs: we remove them all, solve the re-sulting instance, and then add them back one at a time while attempting to increase the overall num-ber of elements as little as possible.

This process of removing ULs, solving, and adding them back in, may in general produce sub-optimal solutions, so we use it only when the solver cannot find a solution on the full-sized prob-lem. In practical experiments, the solver gener-ally either produces an optimal or very nearly op-timal solution within a time limit on the order of ten minutes; or fails to produce a feasible solu-tion at all, even with a much longer limit. Testing whether it finds a solution is then a useful way to determine whether UL removal is worthwhile.
Recall that in an instance consisting of k ULs and a bottom, an optimal solution consists of find-ing the smallest b such that b  X  +1 is at least k ; that is the number of bits for the bottom, and we can choose any k distinct subsets of size  X  + 1 for the ULs. Augmenting an existing solution to include additional ULs involves a similar calculation.
To add a UL x as the successor of an element y without increasing the total number of bits, we must find a choice of  X  + 1 of the bits already as-signed to y , sharing at most  X  bits with any of y  X  X  other successors. Those successors are in general sets of arbitrary size, but all that matters for as-signing x is how many subsets of size  X  + 1 they already cover. The UL can use any such subset not covered by an existing successor of y . Our al-gorithm counts the subsets already covered, and compares that with the number of choices of  X  + 1 bits from the bits assigned to y . If enough choices remain, we use them; otherwise, we add bits until there are enough choices. 3.4 Solving For instances with a small number of sets and rela-tively large number of elements in the sets, we use an exponential variable solver. This encodes the set programming problem into integer program-ming. For each element x  X  { 1 , 2 ,...,b } , let c ( x ) = { i | x  X  S i } ; that is, c ( x ) represents the indices of all the sets in the problem that contain the element x . There are 2 n  X  1 possible values of c ( x ) , because each element must be in at least one set. We create an integer variable for each of those values. Each element is counted once, so the sum of the integer variables is b . The constraints translate into simple inequalities on sums of the variables; and the system of constraints can be solved with standard integer programming tech-niques. After solving the integer programming problem we can then assign elements arbitrarily to the appropriate combinations of sets.

Where applicable, the exponential variable ap-proach works well, because it breaks all the sym-metries between set elements. It also continues to function well even when the sets are large, since nothing in the problem directly grows when we increase b . The wide domains of the variables may be advantageous for some integer program-ming solvers as well. However, it creates an in-teger programming problem of size exponential in the number of sets. As a result, it is only applica-ble to instances with a very few set variables.
For more general set programming instances, we feed the instance directly into a solver de-signed for such problems. We used the ECL i PS e logic programming system (Cisco Systems, 2008), which offers several set programming solvers as libraries, and settled on the ic sets library. This is a straightforward set programming solver based on containment bounds. We extended the solver by adding a lightweight not-subset constraint, and customized heuristics for variable and value selec-tion designed to guide the solver to a feasible so-lution as soon as possible. We choose variables near the top of the instance first, and prefer to as-sign values that share exactly  X  bits with exist-ing assigned values. We also do limited symme-try breaking, in that whenever we assign a bit not shared with any current assignment, the choice of bit is arbitrary so we assume it must be the lowest-index bit. That symmetry breaking speeds up the search significantly.

The present work is primarily on the benefits of nonzero  X  , and so a detailed study of gen-eral set programming techniques would be inap-propriate; but we made informal tests of several other set-programming solvers. We had hoped that a solver using containment-lexicographic hybrid bounds as described by Sadler and Gervet (Sadler and Gervet, 2008) would offer good performance, and chose the ECL i PS e framework partly to gain access to its ic hybrid sets implementation of such bounds. In practice, however, ic hybrid sets gave consistently worse performance than ic sets (typi-cally by an approximate factor of two). It appears that in intuitive terms, the lexicographic bounds rarely narrowed the domains of variables much un-til the variables were almost entirely labelled any-way, at which point containment bounds were al-most as good; and meanwhile the increased over-head of maintaining the extra bounds slowed down the entire process to more than compensate for the improved propagation. We also evaluated the Cardinal solver included in ECL i PS e , which of-fers stronger propagation of cardinality informa-tion; it lacked other needed features and seemed no more efficient than ic sets. Among these three solvers, the improvements associated with our custom variable and value heuristics greatly outweighed the baseline differences between the solvers; and the differences were in optimization time rather than quality of the returned solutions.
Solvers with available source code were pre-ferred for ease of customization, and free solvers were preferred for economy, but a license for ILOG CPLEX (IBM, 2008) was available and we tried using it with the natural encoding of sets as vectors of binary variables. It solved small in-stances to optimality in time comparable to that of ECL i PS e . However, for medium to large in-stances, CPLEX proved impractical. An instance with n sets of up to b bits, dense with pairwise constraints like subset and maximum intersection, requires  X ( n 2 b ) variables when encoded into in-teger programming in the natural way. CPLEX stores a copy of the relaxed problem, with signifi-cant bookkeeping information per variable, for ev-ery node in the search tree. It is capable of storing most of the tree in compressed form on disk, but in our larger instances even a single node is too large; CPLEX exhausts memory while loading its input. The ECL i PS e solver also stores each set variable in a data structure that increases linearly with the number of elements, so that the size of the prob-lem as stored by ECL i PS e is also  X ( n 2 b ) ; but the constant for ECL i PS e appears to be much smaller, and its search algorithm stores only incremental updates (with nodes per set instead of per element) on a stack as it explores the tree. As a result, the ECL i PS e solver can process much larger instances than CPLEX without exhausting memory.

Encoding into SAT would allow use of the so-phisticated solvers available for that problem. Un-fortunately, cardinality constraints are notoriously difficult to encode in Boolean logic. The obvi-ous encoding of our problem into CNFSAT would require O ( n 2 b X  ) clauses and variables. Encod-ings into Boolean variables with richer constraints than CNFSAT (we tried, for instance, the SICS-tus Prolog clp(FD) implementation (Carlsson et al., 1997)) generally exhausted memory on much smaller instances than those handled by the set-
Module n b 0  X  b  X  mrs_min 10 7 0 7 conj 13 8 1 7 list 27 15 1 11 local_min 27 21 1 10 cat_min 30 17 1 14 individual 33 15 0 15 head_min 247 55 0 55 * sort * 247 129 3 107 synsem_min 612 255 0 255 sign_min 1025 489 3 357 mod_relation 1998 1749 6 284 entire ERG 4305 2788 140 985 Table 2: Best encodings of the ERG and its mod-ules: n is number of types, b 0 is vector length with  X  = 0 , and  X  is parameter that gives the shortest vector length b  X  . variable solvers, while offering no improvement in speed. Table 2 shows the size of our smallest encodings to date for the entire ERG without modularization, and for each of its modules. These were found by running the optimization process of the previ-ous section on Intel Xeon servers with a timeout of 30 minutes for each invocation of the solver (which may occur several times per module). Un-der those conditions, some modules take a long time to optimize X  X s much as two hours per tested value of  X  for sign_min . The Xeon X  X  hyper-threading feature makes reproducibility of timing results difficult, but we found that results almost never improved with additional time allowance be-yond the first few seconds in any case, so the prac-tical effect of the timing variations should be min-imal.

These results show some significant improve-ments in vector length for the larger modules. However, they do not reveal the entire story. In particular, the apparent superiority of  X  = 0 for the synsem_min module should not be taken as indicating that no higher  X  could be better: rather, that module includes a very difficult set programming instance on which the solver failed and fell back to GAK. For the even larger modules, nonzero  X  proved helpful despite solver failures, because of the bits saved by UL removal. UL re-moval is clearly a significant advantage, but only Encoding length time space Lookup table n/a 140 72496 Modular, best  X  0 X 357 321 203 Modular,  X  = 0 0 X 1749 747 579 Non-mod,  X  = 0 2788 4651 1530 Non-mod,  X  = 1 1243 2224 706 Non-mod,  X  = 2 1140 2008 656 Non-mod,  X  = 9 1069 1981 622 Non-mod,  X  = 140 985 3018 572 Table 3: Query performance. Vector length in bits, time in milliseconds, space in Kbytes. for the modules where the solver is failing any-way. One important lesson seems to be that further work on set programming solvers would be bene-ficial: any future more capable set programming solver could be applied to the unsolved instances and would be expected to save more bits.

Table 3 and Figure 3 show the performance of the join query with various encodings. These re-sults are from a simple implementation in C that tests all ordered pairs of types for joinability. As well as testing the non-modular ERG encoding for different values of  X  , we tested the modularized encoding with  X  = 0 for all modules (to show the effect of modularization alone) and with  X  cho-sen per-module to give the shortest vectors. For comparison, we also tested a simple lookup table. The same implementation sufficed for all these tests, by means of putting all types in one mod-ule for the non-modular bit vectors or no types in any module for the pure lookup table. The times shown are milliseconds of user CPU time to test all join tests (roughly 18.5 million of them), on a non-hyperthreading Intel Pentium 4 with a clock speed of 2.66GHz and 1G of RAM, run-ning Linux. Space consumption shown is the total amount of dynamically-allocated memory used to store the vectors and lookup table.

The non-modular encoding with  X  = 0 is the basic encoding of A  X   X t-Kaci et al. (1989). As Ta-ble 3 shows, we achieved more than a factor of two improvement from that, in both time and vec-tor length, just by setting  X  = 1 . Larger values offered further small improvements in length up to  X  = 140 , which gave the minimum vector length of 985. That is a shallow minimum; both  X  = 120 and  X  = 160 gave vector lengths of 986, and the length slowly increased with greater  X  .

However, the fastest bit-count on this architec-ture, using a technique first published by Weg-ner (1960), requires time increasing with the num-ber of nonzero bits it counts; and a similar effect would appear on a word-by-word basis even if we used a constant-time per-word count. As a result, there is a time cost associated with using larger  X  , so that the fastest value is not necessarily the one that gives the shortest vectors. In our experiments,  X  = 9 gave the fastest joins for the non-modular encoding of the ERG. As shown in Figure 3, all small nonzero  X  gave very similar times.

Modularization helps a lot, both with  X  = 0 , and when we choose the optimal  X  per module. Here, too, the use of optimal  X  improves both time and space by more than a factor of two. Our best bit-vector encoding, the modularized one with per-module optimal  X  , is only a little less than half the speed of the lookup table; and this test favours the lookup table by giving it a full word for every entry (no time spent shifting and masking bits) and testing the pairs in a simple two-level loop (almost purely sequential access). We have described a generalization of conven-tional bit vector concept lattice encoding tech-niques to the case where all vectors with  X  or fewer one bits represent failure; traditional encodings are the case  X  = 0 . Increasing  X  can reduce the over-all storage space and improve speed.

A good encoding requires a kind of perfect hash, the design of which maps naturally to con-straint programming over sets of integers. We have described a practical framework for solving the instances of constraint programming thus cre-ated, in which we can apply existing or future con-straint solvers to the subproblems for which they are best suited; and a technique for modularizing practical type hierarchies to get better value from the bit vector encodings. We have evaluated the re-sulting encodings on the ERG X  X  type system, and examined the performance of the associated unifi-cation test. Modularization, and the use of nonzero  X  , each independently provide significant savings in both time and vector length.

The modified failure detection concept suggests several directions for future work, including eval-uation of the new encodings in the context of a large-scale HPSG parser; incorporation of further developments in constraint solvers; and the possi-bility of approximate encodings that would permit one-sided errors as in traditional Bloom filtering.
