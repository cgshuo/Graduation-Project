 The increasing complexity of today X  X  systems makes fast and ac-curate failure detection essential for their use in mission-critical applications. Various monitoring methods provide a large amount of data about system X  X  behavior. Analyzing this data with advanced statistical methods holds the promise of not only detecting the er-rors faster, but also detecting errors which are difficult to catch with current monitoring tools. Two challenges to building such detection tools are: the high dimensionality of observation data, which makes the models expensive to apply, and frequent system changes, which make the models expensive to update. In this paper, we present al-gorithms to reduce the dimensionality of data in a way that makes it easy to adapt to system changes. We decompose the observation data into signal and noise subspaces. Two statistics, the Hotelling T 2 score and squared prediction error ( SPE ) are calculated to rep-resent the data characteristics in signal and noise subspaces respec-tively. Instead of tracking the original data, we use a sequentially discounting expectation maximization (SDEM) algorithm to learn the distribution of the two extracted statistics. A failure event can then be detected based on the abnormal change of the distribution. Applying our technique to component interaction data in a simple e-commerce application shows better accuracy than building inde-pendent profiles for each component. Additionally, experiments on synthetic data show that the detection accuracy is high even for changing systems.
 K.6.4 [ Management of Computing and Information Systems ]: System Management; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Management Subspace decomposition, online tracking, statistics, failure detec-tion, distributed computing, Internet services Copyright 2005 ACM 1-59593-135-X/05/0008 ... $ 5.00.
Recent increases in the complexity of software systems coupled with a demand that they function 24  X  7 with only minutes of down-time per year, require significant advances in management capabil-ities of these systems. While designers spend significant effort in building-in resilience to failures, the complexity of these systems makes anticipating all the failure modes all but impossible. A com-plementary way of increasing availability is to shorten the time to recover from failures when they do occur. This paper proposes a new method for online detection and localization of failures, thus contributing to a significant reduction of the recovery duration.
Detection of failures in large dynamic systems is a challenging task because failure events appear rarely and may not have fixed be-havior. The high dimensionality of observation data, together with the changes in normal system behavior, due to software upgrad-ing, web content and user behavior changes, make detection even more difficult. One main contribution of this paper is to propose an online dynamic tracking approach for high dimensional data , and apply it to monitor the component interactions for detecting system failures. Since the tracking strategy does not require human inter-vention, our algorithm can be applied to achieve automatic adapta-tion to system changes.

In recent years, component based architectures (J2EE, .Net) have become prevalent in developing large scale Web applications. Ac-cording to the program logic, a component dynamically calls other components to fulfill service requests. We collect the frequencies of interactions between components as a feature to characterize a sys-tem X  X  health. Typically, when a component fails, its communication pattern with others will change. By tracking their interactions over time, we can detect system failures.

One main obstacle in applying component interaction tracking is the high dimensionality of observations. For a system with components, the dimension of interaction data is l 2 . However, we observe that in most real systems, each component only interacts with a small number of other components. This allows us to de-compose the original high dimensional space into signal and noise subspaces. Two important statistics, the Hotelling T the squared prediction error (SPE) [5], are computed to represent the characteristics of data distribution in the two subspaces. An online tracking algorithm, called sequentially discounting expec-tation maximization (SDEM) algorithm, is employed to learn the distribution of these two statistics. The anomaly of a new sam-ple is then determined by comparing the distribution model before and after that sample is learned. Meanwhile, the signal and noise subspaces are also updated along time. In both SDEM algorithm and subspace updating, an exponentially weighted moving ave rage (EWMA) filter is employed to enhance the adaptivity of our algo-rithm to system changes. If an anomaly is detected, we identify the likely faulty components by statistically comparing the abnormal observation with normal ones.

In order to verify how good our method of modeling component interaction is at detecting failures in componentized applications, we have performed experiments on Pet Store , an open source application based on the J2EE architecture. We have used an instru-mented application server to collect observations and also to inject a variety of failures. The detection and localization results show that our data reduction and tracking strategy gives good results for failure detection in real applications. Furthermore, we believe that our method is applicable to observations other than component in-teraction (e.g. number of hits to individual web pages), and in gen-eral to applications where quantitative information is represented by high-dimensional data. In order to test our method X  X  applicabil-ity to systems that change over time, we have used synthetic data in the experiment. This allows us to arbitrarily manipulate the dynam-icity of data, and thus test the performance of our approach under scenarios difficult to simulate with real applications.
We extract the frequencies of component interactions from re-quest traces as a feature used to build a model of the system. For a system consisting of l components, the interaction frequencies are represented by a normalized vector x  X  R l 2 . The normalization step allows us to reduce the false positives introduced by workload changes. There are many techniques that enable us to collect the component interaction information. In [3], Chen et al. modified the JBoss middleware to monitor component interactions; we have used the same mechanism in our experiments. Aguilera et al.[1] proposed a passive monitoring approach in both RPC-like systems and message based systems. Several commercial software, such as HP X  X  OpenView Transaction Analyzer, can also be used to moni-tor transaction flows in distributed J2EE and .NET systems. We will not introduce these techniques in detail, but instead focus on techniques for reducing dimensionality and online tracking which are useful in detecting and localizing service failures based on col-lected interaction data.

In our approach, the activity of component interactions is tracked over time and its normal behavior is dynamically learned. Since the dimensionality of the data is high, it is difficult to model the system X  X  normal behavior by applying directly distribution based methods, such as Gaussian mixture models . We observe that in many systems each component interacts only with a limited num-ber of other components to exchange information. That means the observed data are actually located in a low dimensional subspace of the original space. Therefore, a reasonable alternative is to de-compose the original data space into a small number of subspaces. Although in this paper we have used only two subspaces, signal and noise, the method can be easily generalized as described in Section 3.1. Two statistics, the Hotelling T 2 score and squared predic-tion error (SPE) are extracted to represent the characteristics of the two subspaces. While the Hotelling T 2 score measures the Maha-lanobis distance from projected sample to the origin in the signal space, the SPE of an observation indicates its residual error in the noise space. Note that we do not ignore the information from any of the two subspaces since both could provide clues about system failure.
 manufacture processes has been described in [8][6]. In that work, a control limit is set on these two statistics, based on the assumption that all the data obey multivariate normal distribution. However, in many real cases (including software systems) the data distribu-tions are arbitrary and unknown, making it hard to obtain reliable thresholds to distinguish normal and abnormal behaviors. Instead of relying on such an assumption, we employ an online distribu-tion learning algorithm, called sequentially discounting expectation maximization (SDEM) algorithm, to dynamically learn the distribu-tion of the two statistics. An important feature of SDEM is that it uses an exponentially weighted moving ave rage (EWMA) filter to adapt to system changes. For instance, given a set of observations { x 1 ,x 2 ,  X  X  X  ,x n ,  X  X  X } , an online EWMA filter of the mean expressed as where the constant  X  dictates the degree of filtering. When we choose  X  = 1 erage (MA) estimation. In the EWMA filter, the parameter fixed so that  X  n +1 can  X  X ge out X  old observations and put more importance to the recent data. This allows the algorithm to auto-matically adapt to system changes. Once the normal behavior has been modeled from current data, the abnormality of new observa-tion is determined based on how much the model has shifted after learning the new observation.

In most time varying systems, it is quite possible that the signal and noise subspaces are changing over time. For this purpose, we update the subspaces for every new observation or a batch of obser-vations. The updating is based on the eigen decomposition of data correlation matrix. The EWMA filter is employed again in the pro-cess of updating to discount the influence of previous observations. In order to speed up computation, some restricted rank modifica-sample-wise subspace updating is obtained by the first rank modi-fication of data correlation matrix. Similar techniques are used in the block-wise updating.

After one (or more) sample has been detected abnormal, the faulty components are identified by comparing the failed observa-tion against normal ones. We first transform the component inter-action observation into a set of vectors. Each vector represents the interaction behavior of a specific component. An anomaly score is then computed for each component by summing up the weighted deviation of the component X  X  link usages with respect to its normal model. Those with highest scores are the most suspicious compo-nents. The building blocks of our online failure detector is shown in Figure 1. In this section, we provide a detailed description of each procedure in our failure detector.
Given a set of observations { x 0 , x 1 ,  X  X  X  , x n  X  1 } , with x observed at time t 0 ,t 1 ,  X  X  X  ,t n  X  1 , the signal subspace subspace S n spanned by those observations can be obtained either by singular value decomposition (SVD) of the data matrix X or by eigen decomposition of the data correlation matrix C = 1 The SVD of data matrix X =[ x 0  X  X  X  x n  X  1 ] is expressed as X = U  X  V , where  X = diag (  X  1 ,  X  X  X  , X  r , X  r +1 ,  X  X  X  R two orthogonal matrices U and V are called the left and right eigen matrices of X . Based on the singular value decomposition, the subspace decomposition of X is expressed as where the diagonals of  X  s are large singular values {  X  1 and {  X  r +1 ,  X  X  X  , X  n } belong to the diagonals of  X  n with  X  +1 . The set of orthonormal vectors U s =[ u 1 , u 2 ,  X  X  X  u the bases of signal space S s . The projection matrix P signal space would be given by P s = U s U subspace is the orthogonal complement of signal subspace, S I two projection vectors from two subspaces S s and S n The subspace decomposition can also be accomplished by eigen analysis of the correlation matrix C , which is expressed as where the columns of U are actually the eigenvectors of C eigenvalues of C are related to the diagonals of matrix  X  .
Once the observations have been decomposed into signal and noise subspaces, we extract some statistics to describe the data dis-tributions in two subspaces. One is the Hotelling T 2 score, which measures the variation of each sample in signal subspace. For a new sample vector x , it is expressed as Another statistic, the squared prediction error (SPE), indicates how well each sample conforms to the model, measured by the projec-tion of sample vector on the residual space The geometric interpretation of these two statistics is shown in Figure 2. In this figure, the signal subspace is constructed by 1000 normally distributed 3D samples in a 2D plane. Given a new sam-ple x , its projection onto the signal subspace (the plane) is denoted from x s to the origin in the plane. SPE measures the distance from the sample x to its projection in the signal space x s . Note in this example the data are centered, then the value T 2 is also related to the Mahalanobis distance from the projected sample to the mean of all samples.
 If the data obey a multivariate normal distribution, the Hotelling T this statistic based on a significance level. Similarly if we assume the noise x n is normally distributed, a control limit for SPE can be Figure 2: Geometric interpretation of Hotelling T 2 and SPE. x s is the projection of x onto the signal space (a plane) spanned by the cluster of points. The Hotelling T 2 score indicates the Mahalanobis distance from x s to the origin in the signal space. SPE measures the distance between x and x s . obtained to distinguish outliers from inliers. The work in [8][6] fol-lows these assumptions in the field of process control. However, in real situations the data are arbitrarily distributed and unknown. It is hard to determine those thresholds. For example, if the data are bi-modally distributed in the signal subspace with large gap between two distributions, then the threshold for T 2 is meaningless. Fur-thermore, the data distribution is sometimes changing over time, the thresholds that are determined during training would become invalid after a certain time period. In order to avoid any prior as-sumptions, we present the SDEM algorithm to dynamically track the distribution of those two statistics. Note here we only employ two statistics to represent the original data. Even though such rep-resentation works well in our failure detection experiments, it is our future research to apply more statistics to sufficiently reveal the original high dimensional data distribution. For instance, we can decompose the original data space into several subspaces instead of only two, and extract statistics to represent the distribution of data projections in every subspace.
The sequentially dynamic expectation maximization (SDEM) is a sub-algorithm of SmartSifter, an online unsupervised outlier de-tector developed by Yamanishi et al.[7]. It uses a Gaussian mixture model to represent the probability density over the domain of con-tinuous variables z , p ( z |  X  )= k positive integer, c i  X  0 , k dimensional Gaussian distribution with density specified by mean  X  , and covariance matrix  X  i p ( z |  X  where i =1 ,  X  X  X  ,k and d is the dimension of each datum. In our failure detection algorithm, z is the vector of two statistics as described in Section 3.1 and d =2 . We set the parameter vector
Every time the datum is input, the SDEM algorithm, as described in Figure 3, estimates the parameter  X  and hence learns the distribu-tion model. The EWMA filter is employed in the parameter estima-tion in order to discount past examples. The forgetting parameter  X  is related to the degree of discounting. Intuitively, the smaller  X  is, a larger effect the SDEM algorithm has from past examples. Such mechanism makes the SDEM adaptive to non-stationary data sources, e.g., when drifting sources of time series are tackled.
Another parameter  X  is introduced in the SDEM algorithm in order to improve the stability of the estimates of c i , which is set between [1 . 0 , 2 . 0] . Usually c (0) they are uniformly distributed over the data space. The computation time at each round is O ( d 3 k ) where d is the dimension of z and is the number of Gaussian distributions. We suggest the value of integer k to be chosen between 2 and 5 in usual cases.

The anomaly of a new observation z n is determined based on the statistical deviation of distribution before and after z n If we denote the two distributions as p ( n  X  1) ( called Hellinger score is defined by Intuitively, this score measures how much the probability density function p ( n ) ( score indicates that z n is an outlier with high probability. For the efficient computation of Hellinger score please see [7].
It does not bring enough benefits to update z by keeping the subspaces fixed since the two subspaces may also change. There-fore we update the subspaces for every new observation or ev-ery batch of observations. Both the sample-wise and block-wise subspace updating algorithms are based on the eigen decomposi-tion of data correlation matrix as shown in equation (4). If we choose to update the subspaces after k data samples are obtained, B =[ x n +1 , x n +2 ,  X  X  X  , x n + k ] , the new correlation matrix is esti-mated by using the EWMA filter where the forgetting parameter  X  is introduced to discount the pre-vious samples.

The new subspace is obtained by finding the modified eigen sys-tem C n + k =  X  U  X   X   X  U , where  X   X = diag (  X   X  1 , column vectors of  X  U form the bases of the new signal subspace. Suppose  X   X ,  X  u are the eigen pairs of C n + k , we have
Common methods of solving (9) such as QR iteration would take computation complexity with magnitude O ( p 3 ) , where p mension of data. However since we already have the eigen decom-position of C n , such information can be utilized to speed up the computation. Substituting (8) into (9) gives the following expres-sion where  X  B =  X  1  X  the following system of equations is induced Solving  X  u in terms of y in (11) and substituting  X  u into (12) gives the matrix F (  X   X  ) to find the eigen pairs of C n + k . For the sample-wise subspace updating ( k =1 ), F (  X   X  ) is a scalar. The eigenvalues  X   X  i of C n + k can be obtained by finding the roots of equation F (  X   X  )=0 . Various iterative search algorithms such as the bisection method can be applied. Once the eigenvalues are known, the corresponding eigenvectors {  X  u i } are calculated from the equation (10).

For the block-wise subspace updating ( k&gt; 1 ), the eigenvalues of C n + k can be located by means of the inertia [4] of matrix which are the numbers of negative, zero and positive eigenvalues of
F (  X   X  ) . The eigenvectors of C n + k are evaluated efficiently in two steps. First, we solve the intermediate vector y from the equation (13). The eigenvector  X  u is then computed explicitly using (11). Ap-plying the sample-wise or block-wise subspace updating algorithm reduces the computation complexity from O ( p 3 ) to O ( p
In order to localize the most suspicious components, we first transform the component interaction vector into a set of vectors, one for each component. A score is then evaluated for each com-ponent based on the deviation of failure observation from its nor-mal model. The components with the highest scores are the most suspicious. For instance, as shown in Figure 4, the behavior of component A is represented as a set of links by which the paths enter A from other components or leave A to other components. We use a vector v A  X  R 2 l to represent this behavior, v [ v ponents in the system. Under the assumption that all the variables in v A are mutually independent, given a set of normal observations, we obtain the mean and standard deviation of each variable in v Given the failure observation, the anomaly score of component A is calculated as the sum of weighted deviation of each link frequency with respect to its normal mean In the same way we calculate the scores of all other components. In fact the score s A satisfies  X  2 distribution with degree of freedom
Figure 4: The interaction behavior model for component A. 2 l . By choosing certain significance level, we obtain a threshold to separate faulty and normal components. Note here we only use one failure observation x f for localization. If more failure observa-tions from the same accident are available, we can use some voting strategies [2] to increase the confidence of localization.
In this section, we first use some synthetic data to test our al-gorithm. The performances of tracking with and without subspace updating are compared. Then we apply the algorithm to a real sys-tem developed on the J2EE platform. A variety of failures have been injected. The detection and diagnosis results show the effec-tiveness of our algorithm.
The advantage of using synthetic data is that we can arbitrarily manipulate the dynamicity of the data that are used to compare the performances of online detectors with and without subspace updat-ing. We call the detection technique without subspace updating as  X  X ixed subspace tracking X , and detection with subspace updating as  X  X ynamic subspace tracking X .

Each normal observation is obtained by sampling a harmonic si-nusoid curve with random shift u 0 , x ( u )= Asin 2  X f ( where A is the maximum amplitude, f is the frequency, T 0 period of the signal, u 0 is a uniformly generated random num-ber between [0 ,T 0 ) , and -is the noise. In the experiment we choose A =10 ,T 0 =100 , and the curve is sampled at position u =1 , 2 ,  X  X  X  ,T 0 . Then each observation is with one hundred di-mensions. It is easy to see that the signal space of those observa-tions is spanned by samples from two curves { sin 2  X fu The abnormal data are also generated from sinusoid functions, but the function is truncated when its absolute amplitude is larger than a threshold, which is randomly generated between [3 ,A ] . We added Gaussian noise with standard deviation  X  =0 . 5 on both normal and abnormal signals. Figure 5: ROC curves of online detectors with and without sub-space updating.

We generate 1000 normal observations as the training data and 1100 test samples. Whereas the training data are obtained by sam-pling curves with fixed frequency f =1 , we generate the test curves with a slowly shifting frequency to simulate the situation of dynamicity. Twenty abnormal observations are created in the test data which appear every 50 observations between index 50 and 1000. By conducting the above random data generation 100 times and executing performance comparison between fixed and dynamic space tracking for each data set, we plot the average ROC curves for two algorithms in Figure 5. It is clear that the dynamic subspace tracking performs much better than fixed subspace tracking in case of non-stationary observations.
We test our algorithms on a simple e-commerce application to detect and localize actual failures. The ROC group [3] modified the JBoss application server to collect the component interaction activity. We use their method in this experiment. JBoss is an open source implementation of J2EE which is a widely adopted middleware standard for constructing enterprise applications from reusable Java modules, called Enterprise Java Beans (EJBs). Pet Store 1.3.1 is deployed as our testbed application. Its functional-ity consists of store front, shopping cart, purchase tracking and so on. There are 47 components in Pet Store , including EJBs and Servlets. We built a client emulator to generate workloads similar to that created by typical user behavior. The emulator generates a varying number of concurrent client connections. Each client simulates a session, which consists of a series of requests such as creating new accounts, searching, browsing for item details, updat-ing user profiles, placing order and checking out. The component interactions are monitored under these simulated workloads.
Two types of component faults are simulated in the experiment, expected exception and null call [3]. The first failure happens when a method declaring exceptions (which appear in the method X  X  sig-nature) is invoked; in this situation the exception is thrown without the method being executed. Applications are expected to handle gracefully and/or mask such exceptions from end users. Null call failures cause all methods in the affected component to return a null value, again without executing the methods. We injected each type of failure into 15 EJB components of Pet Store to simu-late a variety of errors. For example, injecting a null call fault into component  X  X ccountEJB X  would prevent a customer from seeing his account information on the related web page. Similarly inject-ing an expected exception fault into the component  X  X ignOnEJB X  would prevent a client from creating a new account. Figure 6: Experimental results in a real Internet service. (a) scores. (b) SPE scores. (c) online tracking scores. (d) maximum  X 
To test the effectiveness of our failure detection algorithm, we collect 1000 observations under system X  X  normal operation as the training data. Each observation reflects the number of compo-nent interactions in Pet Store under a simulated workload which obeys the Gaussian distribution with mean 30 and standard devia-tion 6, N (30 , 6) . The workloads for the failure cases have the same distribution. Each time after a simulated failure is injected, we col-lect one failure observation. Note the system is restarted before every failure injection in order to remove the impacts of previous injected failures. We then collected 30 failure observations. Each observation corresponds to a specific failure. We also collect 770 normal observations to form 800 test samples. The failure samples appear every 20 observations in the test data between index 110 and 690. Figure 6 illustrates the detection results. The Hotelling T Figure 6(c) shows the online tracking scores. Obviously the track-ing scores provide more reliable results than those directly relying on the two statistics. Actually if we choose the threshold 50, we can detect 28 failures with only 1 false positive.

The performance of our algorithm is also compared with the scores proposed in [3], in which the detection is based on the sta-tistical test for each component. The authors in [3] built a template of normal behavior for every component, which is expressed as the expected interactions of all its links. We use the average of training samples to build the template of each component. For every test observation, the Q values [3] of all components are calculated. We extract the maximum Q value for every test sample and plot them in Figure 6(d). It can be seen from Figures 6(c) and (d) that our technique got much higher detection rate with fewer false positives. There are several reasons to account for this. First, while in [3] all the interaction links are regarded as independently distributed, our approach captures the dependencies between them. For instance, if there exists a rule that whenever component A calls component B, B always calls component C and no other components call B and C , then the frequencies of the interaction  X  X  calls B X  and  X  X  calls C X  should be the same. Such information is taken into account in the subspace decomposition and implicitly embedded in the two statistics, whereas it is lost when separate profiles are built. An-other advantage is that our algorithm can dynamically adapt to the changes of simulated workload. Figure 7: The localization score of the failure injected compo-nent in each failure case.

After each failure has been detected, every component will get an anomaly score for that failure. Figure 7 shows the localization scores of the failure injected components for all the failure cases. According to equation (16), the localization score satisfies tribution with a degree of freedom 2 l . If we choose a level of signif-icance 0 . 005 to determine the threshold, we can localize the fail-ure injected components in 22 of the 30 simulated failure cases. However, we also notice that for some simulated failures, the in-jected component is not the component with the highest anomaly score. This is because of the cascading effects of failures. That is, when a component is failure injected, not only itself but also other components, e.g. its neighbors, will change the interaction behav-iors significantly. Our strategy can only localize a cluster of sus-picious components. In order to further localize the faulty compo-nents, other information about the system has to be utilized. For in-stance, in the Pet Store testbed system, when we found both the  X  X ignOnEJB X  and  X  X ervlet X  components have highest localization scores, usually the  X  X ignOnEJB X  component will be analyzed first because all of the business logic of Pet Store is implemented in the EJB components.
In this paper we proposed a new approach to detecting and local-izing service failures in component based systems. An online track-ing of the information about component interactions is the key to our failure detection. Due to the high dimensionality of interaction data, we decomposed the observations into signal and noise sub-spaces, and extracted two statistics that reflect the data distribution in subspaces for tracking. A forgetting mechanism was employed in the tracking to discount old samples. We also proposed meth-ods for updating the signal and noise subspaces. The experimental results demonstrated the satisfactory detecting performance of our approach. In the paper we used two statistics in the tracking. For the future, we plan to apply our approach to more complex systems, and investigate the trade-offs involved in extracting more statistics from high dimensional data.
The authors would like to thank the Berkley/Stanford ROC group for providing Pinpoint [3], which was used to collect observations from Pet Store . We also would like to thank Kenji Yamanishi for the software implementing the SDEM algorithm. Finally, we would like to thank Pranav Ashar, Hans Peter Graf and Kenji Ya-manishi for their comments and suggestions for improvement of this paper. [1] M. K. Aguilera, J. C. Mogul, J. L. Wiener, P. Reynolds, and [2] E. Bauer and R. Kohavi. An empirical comparison of voting [3] M. Chen, E. Kiciman, E. Fratkin, A. Fox, and E. Brewer. [4] G. H. Golub and C. F. Van Loan. Matrix Computations . The [5] I. T. Jolliffe. Principal Component Analysis . New York: [6] V. Kumar, U. Sundararaj, S.L. Shah, D. Hair, and L.J. Vande [7] K.Yamanishi, J.Takeuchi, G.Williams, and P.Milne. On-line [8] E.B. Martin, A.J. Morris, and C. Kiparrisides. Manufacturing
