 The recent success and proliferation of statistical machine translation (MT) systems raise a number of important questions. Prominent among these are how to evaluate the quality of such a system efficiently and how to detect the output of such systems (for example, to avoid using it circularly as input for refining MT systems).

In this paper, we will answer both these ques-tions. First, we will show that using style-related linguistic features, such as frequencies of parts-of-speech n-grams and function words, it is pos-sible to learn classifiers that distinguish machine-translated text from human-translated or native English text. While this is a straightforward and not entirely novel result, our main contribution is to relativize the result. We will see that the suc-cess of such classifiers are strongly correlated with the quality of the underlying machine translation system. Specifically, given a corpus consisting of both machine-translated English text (English be-ing the target language) and native English text (not necessarily the reference translation of the machine-translated text), we measure the accuracy of the system in classifying the sentences in the corpus as machine-translated or not. This accu-racy will be shown to decrease as the quality of the underlying MT system increases. In fact, the correlation is strong enough that we propose that this accuracy measure itself can be used as a mea-sure of MT system quality, obviating the need for a reference corpus, as for example is necessary for BLEU (Papineni et al., 2001).

The paper is structured as follows: In the next section, we review previous related work. In the third section, we describe experiments regarding the detection of machine translation and in the fourth section we discuss the use of detection tech-niques as a machine translation quality estimation method. In the final section we offer conclusions and suggestions for future work. 2.1 Translationese The special features of translated texts have been studied widely for many years. Attempts to de-fine their characteristics, often called  X  X ranslation Universals X , include (Toury, 1980; Blum-Kulka and Levenston, 1983; Baker, 1993; Gellerstam, 1986). The differences between native and trans-lated texts found there go well beyond systematic translation errors and point to a distinct  X  X ransla-tionese X  dialect.

Using automatic text classification methods in the field of translation studies had many use cases in recent years, mainly as an empirical method of measuring, proving or contradicting translation universals. Several works (Baroni and Bernar-dini, 2006; Kurokawa et al., 2009; Ilisei et al., 2010) used text classification techniques in order to distinguish human translated text from native language text at document or paragraph level, us-ing features like word and POS n-grams, propor-tion of grammatical words in the text, nouns, fi-nite verbs, auxiliary verbs, adjectives, adverbs, nu-merals, pronouns, prepositions, determiners, con-junctions etc. Koppel and Ordan (2011) classi-fied texts to original or translated, using a list of 300 function words taken from LIWC (Pen-nebaker et al., 2001) as features. Volanski et al. (2013) also tested various hypotheses regarding  X  X ranslationese X , using 32 different linguistically-informed features, to assess the degree to which different sets of features can distinguish between translated and original texts. 2.2 Machine Translation Detection Regarding the detection of machine translated text, Carter and Inkpen (2012) translated the Hansards of the 36th Parliament of Canada us-ing the Microsoft Bing MT web service, and conducted three detection experiments at docu-ment level, using unigrams, average token length, and type-token ratio as features. Arase and Zhou (2013) trained a sentence-level classifier to distinguish machine translated text from human generated text on English and Japanese web-page corpora, translated by Google Translate, Bing and an in-house SMT system. They achieved very high detection accuracy using application-specific fea-ture sets for this purpose, including indicators of the  X  X hrase Salad X  (Lopez, 2008) phenomenon or  X  X appy-Phrases X  (Bansal et al., 2011).

While Arase and Zhou (2013) considered MT detection at sentence level, as we do in this pa-per, they did not study the correlation between the translation quality of the machine translated text and the ability to detect it. We show below that such detection is possible with very high accuracy only on low-quality translations. We examine this detection accuracy vs. quality correlation, with various MT systems, such as rule-based and sta-tistical MT, both commercial and in-house, using various feature sets. 3.1 Features We wish to distinguish machine translated En-glish sentences from either human-translated sen-tences or native English sentences. Due to the sparseness of the data at the sentence level, we use common content-independent linguistic fea-tures for the classification task. Our features are binary, denoting the presence or absence of each of a set of part-of-speech n-grams acquired using the Stanford POS tagger (Toutanova et al., 2003), as well as the presence or absence of each of 467 function words taken from LIWC (Pennebaker et al., 2001). We consider only those entries that ap-pear at least ten times in the entire corpus, in order to reduce sparsity in the data. As our learning al-gorithm we use SVM with sequential minimal op-timization (SMO), taken from the WEKA machine learning toolkit (Hall et al., 2009). 3.2 Detecting Different MT Systems In the first experiment set, we explore the ability to detect outputs of machine translated text from different MT systems, in an environment contain-ing both human generated and machine translated text. For this task, we use a portion of the Cana-dian Hansard corpus (Germann, 2001), containing 48,914 parallel sentences from French to English. We translate the French portion of the corpus using several MT systems, respectively: Google Trans-late, Systran, and five other commercial MT sys-tems available at the http://itranslate4.eu website, which enables to query example MT systems built by several european MT companies. After trans-lating the sentences, we take 20,000 sentences from each engine output and conduct the detection experiment by labeling those sentences as MT sen-tences, and another 20,000 sentences, which are the human reference translations, labeled as ref-erence sentences. We conduct a 10-fold cross-validation experiment on the entire 40,000 sen-tence corpus. We also conduct the same exper-iment using 20,000 random, non-reference sen-tences from the same corpus, instead of the ref-erence sentences. Using simple linear regression, we also obtain an R 2 value (coefficient of deter-mination) over the measurements of detection ac-curacy and BLEU score, for each of three feature set combinations (function words, POS tags and mixed) and the two data combinations (MT vs. reference and MT vs. non reference sentences). The detection and R 2 results are shown in Table 1.
As can be seen, best detection results are ob-tained using the full combined feature set. It can also be seen that, as might be expected, it is easier to distinguish machine-translated sentences from a non-reference set than from the reference set. In Figure 1, we show the relationship of the observed detection accuracy for each system with the BLEU score of that system. As is evident, regardless of the feature set or non-MT sentences used, the correlation between detection accuracy and BLEU detection accuracy (%) Figure 1: Correlation between detection accu-racy and BLEU score on commercial MT systems, using POS, function words and mixed features against reference and non-reference sentences. values in Table 1. 3.3 In-House SMT Systems Table 3: Details for Moses based SMT systems
In the second experiment set, we test our de-tection method on SMT systems we created, in which we have control over the training data and the expected overall relative translation quality. In order to do so, we use the Moses statistical ma-chine translation toolkit (Koehn et al., 2007). To train the systems, we take a portion of the Europarl corpus (Koehn, 2005), creating 7 different SMT systems, each using a different amount of train-ing data, for both the translation model and lan-guage model. We do this in order to create dif-ferent quality translation systems, details of which are described in Table 3. For purposes of classifi-cation, we use the same content independent fea-tures as in the previous experiment, based on func-detection accuracy (%) Figure 2: Correlation between detection accu-racy and BLEU score on in-house Moses-based SMT systems against non-reference sentences us-ing content independent features. tion words and POS tags, again with SMO-based SVM as the classifier. For data, we use 20,000 ran-dom, non reference sentences from the Hansard corpus, against 20,000 sentences from one MT system per experiment, again resulting in 40,000 sentence instances per experiment. The relation-ship between the detection results for each MT system and the BLEU score for that system, re-sulting in R 2 = 0 . 774 , is shown in Figure 2. 4.1 Human Evaluation Experiments As can be seen in the above experiments, there is a strong correlation between the BLEU score and the MT detection accuracy of our method. In fact, results are linearly and negatively correlated with BLEU, as can be seen both on commercial systems and our in-house SMT systems. We also wish to consider the relationship between detection accu-racy and a human quality estimation score. To do this, we use the French-English data from the 8th Workshop on Statistical Machine Translation -WMT13 X  (Bojar et al., 2013), containing out-puts from 13 different MT systems and their hu-man evaluations. We conduct the same classifi-cation experiment as above, with features based on function words and POS tags, and SMO-based SVM as the classifier. We first use 3000 refer-mixed MT/ref 59.51 69.47 69.77 75.86 78.11 79.24 88.85 0.944 func. w. MT/ref 57.27 66.05 67.48 67.06 68.58 73.37 84.79 0.779 detection accuracy (%) Figure 3: Correlation between detection accuracy and human evaluation scores on systems from WMT13 X  against reference sentences. detection accuracy (%) Figure 4: Correlation between detection accu-racy and human evaluation scores on systems from WMT 13 X  against non-reference sentences. detection accuracy (%) Figure 5: Correlation between detection accu-racy and human evaluation scores on systems from WMT 13 X  against non-reference sentences, using the syntactic CFG features described in section 4.2 ence sentences from the WMT13 X  English refer-ence translations, against the matching 3000 out-put sentences from one MT system at a time, re-sulting in 6000 sentence instances per experiment. As can be seen in Figure 3, the detection accuracy is strongly correlated with the evaluations scores, yielding R 2 = 0 . 774 . To provide another mea-sure of correlation, we compared every pair of data points in the experiment to get the proportion of pairs ordered identically by the human evalu-ators and our method, with a result of 0 . 846 (66 of 78). In the second experiment, we use 3000 random, non reference sentences from the new-stest 2011-2012 corpora published in WMT12 X  (Callison-Burch et al., 2012) against 3000 output sentences from one MT system at a time, again re-sulting in 6000 sentence instances per experiment. While applying the same classification method as with the reference sentences, the detection accu-racy rises, while the correlation with the transla-tion quality yields R 2 = 0 . 556 , as can be seen in Figure 4. Here, the proportion of identically or-dered pairs is 0 . 782 (61 of 78). 4.2 Syntactic Features We note that the second leftmost point in Figures 3, 4 is an outlier: that is, our method has a hard time detecting sentences produced by this system although it is not highly rated by human evalu-ators. This point represents the Joshua (Post et al., 2013) SMT system. This system is syntax-based, which apparently confound our POS and FW-based classifier, despite it X  X  low human evalu-ation score. We hypothesize that the use of syntax-based features might improve results. To ver-ify this intuition, we create parse trees using the Berkeley parser (Petrov and Klein, 2007) and ex-tract the one-level CFG rules as features. Again, we represent each sentence as a boolean vector, in which each entry represents the presence or ab-sence of the CFG rule in the parse-tree of the sen-tence. Using these features alone, without the FW and POS tag based features presented above, we obtain an R 2 = 0 . 829 with a proportion of iden-tically ordered pairs at 0 . 923 (72 of 78), as shown in Figure 5. We have shown that it is possible to detect ma-chine translation from monolingual corpora con-taining both machine translated text and human generated text, at sentence level. There is a strong correlation between the detection accuracy that can be obtained and the BLEU score or the human evaluation score of the machine translation itself. This correlation holds whether or not a reference set is used. This suggests that our method might be used as an unsupervised quality estimation method when no reference sentences are available, such as for resource-poor source languages. Further work might include applying our methods to other language pairs and domains, acquiring word-level quality estimation or integrating our method in a machine translation system. Furthermore, ad-ditional features and feature selection techniques can be applied, both for improving detection ac-curacy and for strengthening the correlation with human quality estimation.
 We would like to thank Noam Ordan and Shuly Wintner for their help and feedback on the early stages of this work. This research was funded in part by the Intel Collaborative Research Institute for Computational Intelligence.
