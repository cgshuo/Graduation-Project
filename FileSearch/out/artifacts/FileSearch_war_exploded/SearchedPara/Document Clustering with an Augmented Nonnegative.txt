 Document clustering is an unsupervised machine learning technique aims to discover the classification of documents according to their similarities. So far many document Analysis (PLSA) [5] etc. to extract features of the data set. However, recent research has shown that most data reduction approach that tries to discover the low dimensional structure for the data in Isomap [7][8][9], Locally Linear Embedding [10], and Laplacian Eigenmaps [11] etc. (GNMF). GNMF embedded manifold learning into nonnegative matrix factorization learning can significantly improve the clustering results. 
One defect of the aforementioned clustering methods is that they only focus on one explore the data structure from two dimensi ons, because the geometrical structures of vectors in two dimensions of the matrix are independent to each other. For example, of the term vectors. augmented nonnegative matrix factorization (ANMF), which incorporates both matrix factorization and manifold learning on both dimensions of the data matrix. Then, we challenges here is how to establish a reliable clustering evaluation method. Since most effectiveness of algorithms. 2.1 NMF convenient for data analysis. The relationship between the data points can be regarded as their distance of similarity on the graph. 
Given a data matrix  X  X  X  X  X   X   X ,...,  X   X  X  X   X  X  X  X  , the goal of nonnegative matrix can best approximate the original matrix  X  : indicate that NMF method outperforms the latent semantic indexing and the spectral represents maybe dissimilar after factorization. 2.2 GNMF the two dimensions respectively. GNMF model applies the local invariance assumption the NMF objective function. The main idea of the local invariance assumption is that if close in the new representation after factorized [12]. This is formulated as follows: defined by  X  X  X  X  X  X  [15].  X  is a diagonal matrix and its diagonal entries are the  X 
In the experiment, two image data sets and one document corpus were selected for clustering and showed GNMF outperforms NMF model, k-means and SVD methods. A flaw of this model is that it only considers the local invariance assumption from one dimension of the data. 2.3 Document Clustering on the Web Many document clustering methods including the aforementioned ones are developed information. Sometimes even the textual documents are no longer available due to the algorithms such as k-means and NMF ignore the semantic relations among terms. As research. 
Ramage and Heymann proposed two methods in Web clustering that include both includes both term and tag information; the other used the term and tag information in shows that including tagging data can significantly improve the clustering quality. webpage clustering [17]. The authors integrated the tripartite information together for better clustering performance. Three different methods are proposed in the paper. The has better performance. algorithm in detail. 
The objective function of the ANMF model is: 3.1 Notations and Problem Formalization Before describing the model, some useful definitions are introduced. Given a data set, the data feature  X   X  for the data point  X   X  . Table 1. the nonnegative matrix factorization can be applied to decompose the matrix  X  assumption to obtain two regularization terms, Regularizer I and Regularizer II. 3.2 Local Invariance Assumption similarities between points of the original data, we construct two adjacency matrix  X  as 0-1 weighting, heat kernel weighting and dot-product weighting, the definitions of the three weighting modes can be referenced in [18]. For each data point, only the  X  nearest neighbors are considered. 
In the NMF decomposition, let  X  be the number of latent component, and  X  dimensional Euclidean space. From a geometric point of view, their similarity can be easily compared by Euclidean distance. standard basis. Under the matrix decomposition, original vector  X   X  is approximated by the linear combination of vectors  X   X  X :  X  coordinates. To quantize this information, we use the Euclidean distance in the latent  X  weighting. Also as in [12], we define the Regularizer I as we simplify the regularizer as following.  X   X  X  X   X   X   X   X  X  X  .  X   X  X  X  X  The Laplacian matrix  X  is defined by  X  X  X  X  X  X  [15]. 
Incorporating this information to the NMF model, the objective function now becomes is the model considered in [1], called the graph regularized NMF method (GNMF). assumption applies to the other piece of data, the features. 
To reflect the local invariance of the data, a second regularization term is added: function can be defined as 
We call this new model the augmented NMF (ANMF). Here the two regularization . X  We will discuss the algorithms in next section. 3.3 Iterative Algorithm minima. 
The cost function can be rewritten as  X  X  X  (  X  X   X  )  X  X  X 2 (  X  X  X   X  )  X  X  X  (  X  X   X   X  X   X  )  X  X  X  X  (  X   X   X  X  ) +  X  X  X  X  X   X   X   X   X  X 
Here the basic properties  X ( X  X   X  ) X ( X  X   X  ) and ) X  X ( X  X   X  ) X  X ( X  X  are used for  X  Lagrangian is  X  X  X  X  X  (  X  X   X  )  X  X  X 2 (  X  X  X   X  )  X  X  X  (  X  X   X   X  X   X  )  X  X  X  X  (  X   X   X  X  )  X  X  X  X  X  X   X   X   X   X  X  X   X  X ( X  X  X ) X U( X  X   X  ) (4) where  X ( X  X   X  X  X  )  X  X  X  X  and  X ( X  X   X  X  X  )  X  X  X  X  . The partial derivatives are 
The derivatives vanish at local minima. Using the Karush-Kuhn-Tucker (KKT) condition,  X   X  X  X   X   X  X  X   X , X 0  X  X  X   X   X  X  X   X 0 . The equations (5) and (6) become 
These equations give the following updating rules 
The updating rules of our model actually lead to convergence sequences, which are justified by Theorem 1 and its proof below. Theorem 1. Under the updating rules (9) and (10), the objective function (3) is non-increasing. 
As in [12] and [18], the proof of Theorem 1 is essentially based on the existence of a proper auxiliary function for the ANMF. We give a simple proof on the ground of the following results from [12]. Lemma 2. Under the updating rule 
The cost function  X   X  in GNMF, i.e. is non-increasing.  X  consider increasing under the updating of  X  by (9).  X  matrix  X  as one. This approach can be achieved by Table 2 shows the simple algorithm of ANMF model. Input: the data matrix  X  , regularization parameter  X  and  X  . Output: the data-topic matrix  X  , and the topic-feature matrix  X  . Method: Random initialize U and V; Repeat (9) and (10) until convergence;
Normalize U and V using (13) and (14). 3.4 Complexity Analysis is ) X  X  X  X ( X  . For GNMF, the adjacency matrix needs  X ( X   X  ) X  to construct, so the overall cost for GNMF is  X  X  X  X  X  X ( X   X  ) X  . As ANMF adds one more adjacency matrix on the other dimension, so the overall cost for ANMF is  X  X  X  X  X  X ( X   X   X  X   X   X  ) X  . 4.1 Data Sets and Evaluation Metrics which were Coil20, ORL, TDT2, and Reuters-21578. Two of them are image data and the other two are text data. normalized mutual information (NMI) [21]. Both of the evaluation metrics range from zero to one, and a high value indicates better clustering result. 4.2 Parameter Settings means [22], NMF [4], and GNMF [12]. For both GNMF and ANMF, we normalized the vectors on columns of  X  and  X  . settings, and the best results were selected to compare with each other. The number of and clustering algorithms. The 0-1 weighting was applied in GNMF and ANMF algorithms for convenience.  X  which lies on the assumption that the neighboring data points share the same topic. So which was verified by [12] for GNMF. There is only one regularization parameter in them were set by the grid  X 10  X  X  ,10  X  X  ,10  X  X  ,1,10,10  X  ,10  X  ,10  X   X  . 
The aforementioned algorithms were repeated 20 times for each parameter setting, the average results were computed. The best average results are shown in Table 3 and Table 4. 4.3 Clustering Results respectively. We can see that overall both GNMF and ANMF performed much better than K-means and NMF algorithms. Note that both GNMF and ANMF consider the imply the importance of the geometrical structure in mining the latent features of the indicates that by adding the geometrical structure for the two dimensions of the data, the algorithm can achieve better performance. 5.1 Data Processing CiteULike is a social bookmarking platform that allows researchers to share scientific CiteULike data was crawled during January-December 2008. We extracted the article processing the dataset, we unified the format of the tags. Tags such as  X  X ata_mining X ,  X  X ata-mining X ,  X  X ata.mining X ,  X  X atamining X , etc. were all considered as the same one. evaluate the CiteULike dataset, we utilized the subject categories in Web of Science [23]. There are a total of 176 top-level subject categories for science journals. Under journals of all articles from CiteULike with the journals under the categories in Web 2406 articles, 1220 users and 4593 tags. 5.2 Clustering Results Just as the experiments in section 4, we compare the clustering results of ANMF with GNMF, NMF and k-means based on the Clustering Accuracy and NMI. The settings section 4. dataset. The experiments reveal several interesting points:  X  improvement is significant in NMI results. This shows that ANMF is efficient not only in image and text data, but also in the data from social tagging systems, which suggests the potential of ANMF in collaborative filtering area.  X  NMF algorithms for the article-user matrix and article-tag matrix. For GNMF and ANMF, their NMI scores are better than the other two matrices, while the 
Clustering Accuracy scores are a little lower. In this paper, we have explored a graph regularized nonnegative matrix factorization data sets, and compared it with three canonical algorithms to evaluate its performance in clustering. Then the algorithm was used in CiteULike dataset by applying user and outperforms GNMF, NMF and k-means models in both benchmark data sets and CiteULike data set. 
