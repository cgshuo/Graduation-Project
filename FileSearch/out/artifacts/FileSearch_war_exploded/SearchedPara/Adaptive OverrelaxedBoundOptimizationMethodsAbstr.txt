 Ruslan Salakhutdino v RSALAKHU @ CS . TORONTO . EDU Department of Computer Science, Uni versity of Toronto 6 King X  s Colle ge Rd, M5S 3G4, Canada Man y problems in machine learning and pattern recogni-tion ultimately reduce to the optimization of a scalar valued function L ( j data ) of a free parameter vector . For ex-ample, in (supervised or) unsupervised probabilistic mod-eling the objecti ve function may be the (conditional) data lik elihood or the posterior over parameters. In discrimi-nati ve learning we may use a classification or regression score; in reinforcement learning we may use average dis-counted reward. Optimization may also arise during infer -ence; for example we may want to reduce the cross entrop y between two distrib utions or minimize a function such as the Bethe free ener gy.
 Bound optimization tak es adv antage of the fact that man y objecti ve functions arising in practice have a special struc-ture. We can often exploit this structure to obtain a bound on the objecti ve function and proceed by optimizing this bound. Ideally , we seek a bound that is valid everywhere in parameter space, easily optimized, and equal to the true objecti ve function at one (or more) point(s). A general form of a bound maximizer which iterati vely lower bounds the objecti ve function is given belo w: Man y popular iterati ve algorithms are bound optimiz-ers, including the EM algorithm for maximum lik eli-hood learning in latent variable models[3 ], iterati ve scaling (IS) algorithms for parameter estimation in maximum en-trop y models[2 ] and the recent CCCP algorithm for min-imizing the Bethe free ener gy in approximate inference problems[19 ]. Bound optimization algorithms enjo y a strong guarantee; the y never worsen the objecti ve function. To guarantee an increase in the objecti ve function at each iteration, BO methods must sometimes construct very conserv ative bounds, resulting in extremely slo w con vergence beha vior . Belo w, we analyze a family of overr elaxed BO algorithms called BO ( ) algorithms with denoting the overrelaxation learning rate: Clearly , for = 1 BO ( ) algorithms become just regular bound optimizers. Several authors have studied a particular variant of this idea as applied to Expectation Maximization. In particular , Helmbold et al. (1995) [6] investigated the problem of estimating the component priors for a mixture of given densities, and disco vered that an EM ( ) update rule can be vie wed as a first order approximation to the ex-ponentiated gradient EG( ) update. Follo wing this, Bauer et al. (1997) [1] presented an analysis of EM ( ) similar to [6] and deri ved the update rules for parameter estima-tion in discrete Bayesian netw orks. Ho we ver, more general BO ( ) methods have not been widely used for several rea-sons. First, only one particular variant of BO ( ) , EM ( ) well studied, and update rules have been published only in the special case of discrete Bayesian netw orks. Second, if a learning rate lar ger than optimal is used, BO ( ) algorithms cannot guarantee con vergence to even a local optimum of their objecti ve function, in contrast to standard bound opti-mizers. Finally , it is computationally very dif ficult to obtain the optimal learning rate .
 In this paper , we analyze a broad class of BO ( ) algorithms beyond EM ( ) and sho w how one can design a simple adapti ve algorithm that, in general, will possess superior con vergence rates over standard BO (1) methods while at the same time guaranteeing con vergence and avoiding the need to calculate or estimate an optimal learning rate . Standard bound optimization methods implicitly define a mapping: M : ! 0 from parameter space to itself, such that t +1 = M ( t ) . If the iterates t con verge to the neighborhood of , by Taylor series expansion: where M 0 ( ) = @M nonzero, then any bound optimizer is essentially a linear it-eration algorithm with a con vergence rate matrix M 0 ( ) M 0 ( ) can be vie wed as an operator that forms a contrac-tion mapping around . 1 For multidimensional vector , a measure of the actual observ ed con vergence rate is the  X  X lobal X  rate, defined as: with k k being Euclidean norm[12 ]. It is also well-kno wn that under some regularity conditions r = the lar gest eigen value of M 0 ( ) . Lar ger values of (as the y approach unity) imply slo wer con vergence. BO ( ) methods, just as standard bound optimizers, implic-itly define a mapping: : ! from parameter space to itself, such that t +1 = ( t ) . In particular , which means that all fix ed points of BO (1) are also fix ed points of BO ( ) , although the radius of con vergence may change.
 We can now analyze con vergence beha vior of BO ( ) meth-ods and their relationship to standard BO (1) algorithms. Lemma: If BO ( ) iterates con verge to and () and M () are dif ferentiable in the parameter space , then: with I being identity matrix and 0 the input-output deri vative matrix for the BO ( ) mapping. This can be sho wn by dif ferentiating both sides of (3) with respect to .
 Equation (4) sho ws a very interesting relationship between con vergence properties of BO ( ) and its standard BO (1) counterparts. If the eigen values of M 0 ( t ) approach unity in the neighborhood of , BO (1) algorithm will exhibit extremely slo w con vergence. In this case, lar ger values of will in fact force the eigen values of 0 ( t ) to decrease, and thus result in faster global rate of con vergence of the BO ( ) algorithm.
 Pr oposition 1: If BO (1) iterates con verge to , then within some neighborhood of , BO ( ) algorithm will also con verge to the same local maximum of the objecti ve function for any 0 &lt; &lt; 2 , although the radius of con ver-gence for BO ( ) may be dif ferent than for BO (1) . Pr oposition 2: The optimal learning rate is: with values of M 0 ( ) . Moreo ver, 1 .
 We sketch proofs of both propositions in the appendix. The important consequence of the abo ve analysis is that for the typical real problems with rate is &gt; 1 . Moreo ver, the global rate of con vergence of BO ( ) algorithms is upper bounded by the spectral radius r = max fj 1 (1 max ) j ; j 1 (1 min ) jg
This implies that, within some neighborhood of , BO ( ) methods can significantly outperform standard BO (1) al-gorithms in terms of con vergence. Indeed, after M itera-tions BO ( ) will shrink the distance k k by a fac-tor of M , whereas standard BO (1) will shrink it by M This clearly constitutes exponential gain of ( = the vicinity of the local optimum. Ho we ver, setting to lar ger values may result in BO ( ) not con verging or con-verging to a dif ferent local optimum than BO (1) . The abo ve con vergence results for the family of the BO ( ) algorithms are only valid within some neighborhood of , whereas BO (1) methods are guaranteed to con verge from any point in the parameter space. In the next section we sho w how we can easily overcome this problem by describ-ing a simple adapti ve BO algorithm. Computing the optimal learning rate is dif ficult, since it requires kno wledge of the minimum and maximum eigen values that depends on the algorithm details, data set, and current parameters. Furthermore, this calculation is only valid in a very small neighborhood around a local optimum. Ideally , we would lik e to find the optimal learning rate in an adapti ve fashion that is computationally ine xpensi ve and valid everywhere. It is possible to perform a line search at each step to determine [7]; howe ver, this is quite expensi ve. We now describe a very simple adapti ve overrelax ed bound optimization (ABO) algorithm that is guaranteed not to decrease the objecti ve function at each iteration and requires only a very slight overhead in computation over regular BO (1) methods yet can often be man y times faster .
 Note that for man y objecti ve functions, computing arg max G ( ; t ) also evaluates the function L ( t )  X  X or free X , so that step 3 abo ve can be efficiently inter -lea ved between steps 1 and 2 with essentially no extra computation (except when the optimizer oversteps). 4.1. Reparameterization of Constrained Quantities The description of the adapti ve algorithm abo ve assumes that the parameters being optimized are unconstrained. In man y cases, parameters must remain non-ne gati ve (or pos-itive definite), sum to unity , respect symmetries or other parameter tying constraints. In these situations, the appro-priate update rules can be deri ved by first reparameteriz-ing the optimization using unconstrained variables which are related to the original variables through a fix ed (possi-bly nonlinear) mapping. As examples, we develop several cases that arise often in practice.
 If parameter values the overrelaxation step can be deri ved from the reparame-terization For parameter values ~ bution (e.g. mixing proportions in a mixture model, con-ditional probability tables for discrete quantities, or state transition probabilities in dynamic models), we reparam-eterize ~ perform overrelaxation in the unconstrained ~ space. In the constrained space this corresponds to the update: using elementwise multiplication and division operations and with Z being an appropriate normalizing constant. 4.2. Adapti ve Ov err elaxed EM We now consider a particular bound optimizer , the popular Expectation-Maximization (EM) algorithm and present its adapti ve overrelax ed version. As an example, consider a probabilistic model of continuous observ ed data x which uses continuous latent variables y . Then: L () = ln p ( x j ) = R p ( y j x ; ) ln p ( x j ) d y =
R p ( y j x ; ) ln p ( x ; y j ) d y R p ( y j x ; ) ln p ( = Q ( ; ) H ( ; )
Setting = t , it can be easily verified that the follo wing is the lower bound function:
The EM algorithm is nothing more than coordinate ascent in the functional G ( ; ) , alternating between maximizing G with respect to for fix ed (E-step) and with respect to for fix ed (M-step). Our new adapti ve overrelax ed version of EM is given belo w: Dempster , Laird, and Rubin [3] sho wed that if EM iterates con verge to , then which can be interpreted as the ratio of missing informa-tion to the complete information near the local optimum. According to Lemma 2, the con vergence rate matrix of EM ( ) algorithm can be represented as follo ws: In the neighborhood of a solution (for suf ficiently lar ge t ):
This vie w of the EM ( ) algorithm has a very interesting interpretation: An incr ease in the proportion of missing information corr esponds to higher values of the learning rate . If the fraction of missing information approaches unity , standard EM will be forced to tak e very small, conserv ative steps in parameter space, therefore higher and more aggressi ve values of will result in much faster con vergence. When the missing information is small compared to the complete information, the potential adv antage of EM ( ) over EM (1) becomes much less. 4.3. Generalized Iterati ve Scaling (GIS) Algorithm The Generalized Iterati ve Scaling algorithm is widely used for parameter estimation in maximum entrop y models [2]. Its goal is to determine the parameters of an expo-nential family distrib ution p ( x j ) = 1 such that certain generalized mar ginal constraints are pre-serv ed: P is the normalizing factor , p ( x ) is a given empirical distri-bution and F ( x ) = [ f vector . These types of problems can be expressed in a stan-dard form, with f x . The log-lik elihood function is: L () = P x p ( x ) ln p ( x j ) = P x p ( x ) T F ( x ) ln Z ()
By noting that ln Z () Z () =Z ( ) + ln Z ( ) 1 for all , and exp P construct a lower bound on L () : L () X We can now deri ve an adapti ve overrelax ed version of GIS: It has been observ ed that the con vergence of GIS algo-rithms depends on the correlation between features. In general, an incr ease in the featur e corr elation corr esponds to higher values of the learning rate . If feature vectors are highly correlated, GIS will tak e very small conserv ative steps in the parameter space. Thus higher and more ag-gressi ve values of will result in much faster con vergence. 4.4. Non-Negati ve Matrix Factorization (NMF) Given non-ne gati ve matrix V, we are interested in finding non-ne gati ve matrices W and H, such that V W H [9]. Posed as an optimization problem, we are interested in maximizing a negati ve divergence L () = D ( V jj W H ) , subject to = ( W; H ) 0 elementwise, where: We use ln P to one. Defining = ( W; H ) and = ( W t ; H t ) , we can construct a lower bound on the cost function:
L () X Adapti ve overrelax ed NMF algorithm is then deri ved as: For man y models overrelaxation is straightforw ard to im-plement and does not require significant computational overhead. As we will see in the next section, it can substan-tially outperform standard bound optimization algorithms. We compared the performance of adapti ve overrelax ed bound optimizers to standard BO (1) algorithms for learn-ing parameters of various models on both synthetic data sets (which are easier to interpret, display and analyze) and on real world data sets, supporting the analysis presented abo ve. Though not reported, we confirmed that the con ver-gence results did not vary significantly for dif ferent initial starting points in the parameter space. For all of the exper -iments reported belo w, we used tol = 10 8 and = 1 : 1 (for which a few percent of steps are rejected). Of course the stepsize is set adapti vely . We did not observ e these results to be very sensiti ve to the setting of , if close to but greater than unity . 5.1. Synthetic Data Sets To compare AEM and EM algorithms, we considered sev-eral latent variable models. As predicted by theory , a high proportion of missing information in these models will re-sult in slo w con vergence of EM, more aggressi ve learning rates and thus superior performance of AEM.
 First, consider a mixture of Gaussians (MoG) model. We generated data from a 5 component Gaussian mixture (see Fig 1). In this model the proportion of missing informa-tion corresponds to how  X  X ell X  or  X  X ot-well X  data is sep-arated into distinct clusters. In our data, mixture compo-nents overlap in one contiguous region, which constitutes a lar ge proportion of missing information. Figure 1 sho ws that AEM outperforms standard EM algorithm by almost a factor of three. We also experimented with the Probabilis-tic Principal Component Analysis (PPCA) latent variable model[15 , 17], which has continuous rather than discrete hidden variables. Here the concept of missing information is related to the ratios of the leading eigen values of the sam-ple covariance, which corresponds to the ellipticity of the distrib ution. We observ e that even for  X  X ice X  data, AEM outperforms EM by almost a factor of four . Similar results are displayed in figure 1 for the MF A [5] model. In figure 3 we sho w the evolution of the adapti ve learn-ing rate and the  X  X ptimal X  learning rate while learning the means in the MoG model (holding the mixing propor -tions and covariances fix ed). The  X  X ptimal X  learning rate ues of M 0 ( ) matrix and applying equation 5; since this bound is only valid under a local quadratic approximation, the adapti ve algorithm is sometimes able to exceed it and still impro ve the objecti ve before overstepping. We then applied our algorithm to the training of Hidden Mark ov Model. Missing information in this model is high when the observ ed data do not well determine the under -lying state sequence (gi ven the parameters). A simple 5-state fully-connected model was trained on 41 character sequences from the book  X  X ecline and Fall of the Roman Empire X  by Gibbon, with an alphabet size of 30 characters (parameters were randomly initialized). We observ e that even for the real, structured data AEM is superior to EM. To compare IS and adapti ve IS algorithms, we applied both methods to the simple 2-class logistic regression model: p ( y = 1 j x; w ) = 1 = (1 + exp ( yw T x )) [13 ]. Feature vectors of dimensionality d were dra wn from standard nor -mal: x N (0 ; I randomly chosen on surf ace of the d-dimensional sphere with radius modified by adding 10 to all feature values. This in tern introduces significant correlation, and thus results in slo w con vergence of IS. To insure that w t x is unchanged, an ex-tra feature was added. Figure 1 reveals that for d=2, AIS is superior to standard IS by at least a factor of 3. Similar re-sults are obtained if dimensionality of the data is increased. Finally , to compare ANMF and NMF , we randomly initialized the non-ne gati ve matrix V both algorithms to perform non-ne gati ve factorization: V the fact that overrelax ed methods can give speedup over con ventional bound optimizers by several times. 5.2. Real World Data Sets To compare AEM and EM, our first experiment consisted of training Aggre gate Mark ov models AMM [16 ] on the ARP A North American Business Ne ws (N AB) corpus. AMMs are class-based bigram models in which the map-ping from words to classes is probabilistic. The task of AMMs is to disco ver  X  X oft X  word classes. The experi-ment used a vocab ulary of sixty-thousand words, includ-ing tok ens for punctuation, sentence boundaries, etc. The training data consisting of approximately 78 million words (three million sentences), with all sentences dra wn with-out replacement from the NAB corpus. The number of classes was set C=2,4,6 and all parameter values were ran-domly initialized. 2 Figure 2 (upper panels) reveals that AEM outperforms EM by at least a factor of 1.5. The considered data sets contain rather structured real data, suggesting a relati vely small fraction of the missing in-formation. Ne vertheless, to perform fair comparison, we have run both algorithms until the con vergence criterion: ( L ( t +1 ) L ( t )) = abs ( L ( t +1 )) 10 8 is met. For ex-ample, with 2 classes, EM has con verges in 164 iterations, whereas AEM has con verges to exactly the same lik elihood after only 72 iterations, a speedup by a factor of more than two. Our second experiment consisted of training a fully con-nected HMM to model DN A sequences. For the training, we used publicly available  X  X ENIE gene finding data set X , pro vided by UCSC and LBNL [4], that contains 793 un-related human genomic DN A sequences. We applied our AEM algorithm to 66 DN A sequences from multiple exon and single exon genes, with length varying anywhere be-tween 200 to 3000 nucleotides per sequence. The number of states was set to 5,7, and 10 and all the parameter values were randomly initialized. Figure 2 sho ws superior con ver-gence of AEM over EM algorithm. This data contains very little Mark ovian structure, which constitutes a high propor -tion of missing information. (W e also observ ed that for 7 and 10 state HMMs, AEM con verged to a better local op-timum; howe ver in general, we can not say anything about the quality of the local optimum to which EM or AEM will con verge.) We also applied the MF A model to the block transform im-age coding problem. A data set of 360 496 pix el images (see fig 2 bottom left panel) were subdi vided into nono ver-lapping blocks of size 8 8 pix els. Each block was regarded as a d=8 8 dimensional vector . The blocks (total of 2,790) were then compressed down to five dimensions using 10 mixture components. 3 Again, AEM beats EM by a factor of over two, con verging to the better lik elihood. To present the comparison between GIS and AGIS, we trained Maximum Entrop y Mark ov Model [10 ] on the Fre-quently Ask ed Questions (FAQ) data set. The data set con-sisted of 38 files belonging to 7 Usenet groups. Each file contains header , follo wed by a series of one or more ques-tion/answer pairs, and ends with tail. The goal is to auto-matically label each line according to whether it is header , question, answer , or tail by using 24 boolean features of lines, lik e begin-with-number , contains-http, etc. 4 We ob-serv e that AGIS outperforms GIS by several times. We have also obtained analogous results training Conditional Random Fields [8].
 In our last experiment, we trained NMF and adapti ve NMF on the data set of facial images to learn part-based repre-sentation of faces [9]. The data set 5 consisted of m=2,429 facial images, each consisting of n=19 19 gray pix els, thus forming an n m matrix V; the number of learned basis im-ages was set to 49. Once again, figure 2 reveals that ANMF substantially outperforms standard NMF algorithm. In par -ticular , ANMF tak es about 3,500 iterations until the con ver-gence criterion is met, whereas regular NMF con verged in approximately 13,500 iterations to exactly the same value of the cost function, loosing to ANMF by a factor of almost four . In this paper we have analyzed the con vergence properties of a lar ge family of overrelax ed bound optimization BO ( ) algorithms. Based on this analysis, we introduced a novel class of simple, adapti ve overrelax ed bound optimization (ABO) methods and pro vided empirical results on several synthetic and real-w orld data sets sho wing superior con ver-gence of ABO methods over standard bound optimizers. In all of our experiments with adapti ve algorithms we found that the value of objecti ve function at any iterate was bet-ter than the value at the same iterate of the standard bound optimizer: L ( t have never found a disadv antage to using adapti ve meth-ods; and often there is a lar ge adv antage, particularly for comple x models with lar ge training data sets, where due to the time constraints one could only afford to perform a few number of the BO iterations.
 We are experimenting with models where parameter values represent symmetric positi ve definite matrices (e.g. co-variance matrices in the MoG model). We can use the ma-trix exponential = exp to perform overrelaxation in the unconstrained space. In particular , we use a spectral decomposition: = V DV T , with D being the diagonal matrix of the eigen values, and V being the orthogonal ma-trix of the corresponding eigen vectors. The matrix func-tions ln and exp are then defined: ln = V ln ( D ) V T , exp = V exp ( D ) V T . When the matrix is diagonal, overrelaxation corresponds to eq. (6).
 Bauer et al. (1997) [1] presented empirical results using non-adapti ve EM ( ) algorithm with fix ed &lt; 2 . Our anal-ysis and empirical results indicate that in the cases when the con vergence rate matrix has lar ge eigen values, (e.g. miss-ing information is high while learning with EM, or feature vectors are highly dependent while estimating parameters with GIS or NMF), adapti ve bound optimizers can exhibit con vergence rates substantially superior to BO (2) by using &gt; 2 , while at the same time maintaining con vergence guarantees and setting automatically .
 Ackno wledgments We would lik e to thank Zoubin Ghahramani and La wrence Saul for man y useful discussions and for pro viding us with some data sets.
 Appendix
