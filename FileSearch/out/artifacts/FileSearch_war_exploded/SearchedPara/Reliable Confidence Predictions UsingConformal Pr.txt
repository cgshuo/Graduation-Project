 Henrik Linusson 1( Conformal classifiers [ 13 ] are classification models that associate each of their predictions with a measure of confidence; each prediction consists of a set of class labels, and the probability of including the true class label is bounded by a predefined level of confidence. Conformal predictors are automatically valid for any exchangeable sequence of observations, in the sense that the probability of excluding the correct class label is well-calibrated by default.
 efficiency , i.e., the size of the prediction regions produced should be kept small, as they limit the number of possible outputs that need to be considered. For conformal classifiers, efficiency can be expressed as a function of the number of class labels included in the prediction regions, given a specific confidence level [ 12 ].
 In order to make use of the confidence predictions provided by conformal classifiers, it is necessary that the prediction regions are both small and reliable. The automatic validity of conformal classifiers effectively ensures their reliability for appropriate, i.e., exchangeable, data streams, and much research has been ever, there is a need for addressing the problem of making predictions that are simultaneously small and reliable. The probability of making an incorrect pre-diction is only valid prior to making said prediction, i.e., we know the probability of the next prediction being incorrect. After classifying a sequence of test pat-terns, however, the a posteriori error probability of each particular prediction is dependent on its size; this can easily be seen by noting that an empty prediction region is always incorrect, whereas a prediction region containing all possible outputs is always correct.
 This paper proposes a method for utilizing posterior information, i.e., the size of prediction regions produced for a sequence of test patterns, in order to more reliably estimate the error probability of singleton predictions, i.e., predictions containing only a single class label, for binary classification problems. In order to output prediction sets, conformal classifiers combine a nonconformity function , which ranks objects based on their apparent strangeness (compared to other observations from the same domain), together with a statistical test that can potentially reject unlikely patterns.
 The nonconformity function can be any function on the form f : X R , but is typically based on a traditional machine learning model according to where h Z is a predictive model trained on the problem, Z ,and  X  is some function that measures the prediction errors of h Z . For binary classification problems, a common choice of error function is where  X  P h Z ( y i | x i ) is a probability estimate for class y applied on x i .
 In order to construct an inductive conformal classifier [ 9 , 10 , 13 ], the following training procedure is used: 1. Divide the training set Z into two disjoint subsets:  X  X  proper training set Z t .  X  X  calibration set Z c , where | Z | = q . 2. Train a classifier h (the underlying model) on Z t . 3. Let {  X  1 ,..., X  q } = { f ( h, z i ) ,z i  X  Z c } .
 When a new test pattern, x j , is obtained, its output can be predicted as follows: 1. Fix a significance level  X  (0 , 1). 2. For each class  X  y  X  Y : (b) Let  X   X  y j = f [ h, ( x j ,  X  y )]. (d) Let  X  j =  X  y  X  Y : p  X  y j &gt; .
 1  X  . An error occurs whenever y made by a conformal classifier is k , where k is the number of test patterns. Conformal predictors are unconditional by default, i.e., while the probability of making an error for an arbitrary test pattern is , it is possible that errors are distributed unevenly amongst different natural subgroups in the test data, e.g., is easily predicted, e.g., because it belongs to the majority class, the probability of making an erroneous prediction on that test pattern might be lower than , while the opposite might be true for difficult test patterns, e.g., those belonging to the minority class. Hence, we can express the expected number of errors made by a binary conformal classifier as where 0 and 1 are the (unknown) probabilities of making an erroneous predic-tion for test patterns that belong to class c 0 and c 1 respectively. expected behaviour of an unconditional conformal classifier for binary classi-fication problems where the two classes are of unequal difficulty. The easier (majority) class  X  X IVE X  shows an error rate below , while the error rate of the more difficult (minority) class  X  X IE X  far exceeds . 3.1 Class-Conditional Conformal Classification Conditional (or Mondrian ) conformal classifiers [ 11 , 13 ] effectively let us fix and 1 such that = 0 = 1 by making the p -values conditional on the class labels of the calibration examples and test patterns. This is accomplished by slightly modifying the p -value equation, so that only calibration examples that share output labels with the test pattern (which is tentatively labeled as  X  y )are considered, i.e., where Z  X  = { ( x i ,y i )  X  Z c : y i =  X  y } and  X  j  X  U [0 , 1].
 Figure 2 shows the error rates of a class-conditional conformal classifier for the two classes of the hepatitis dataset. Here, a much more preferable behaviour is observed: the error rate of the  X  X IE X  and  X  X IVE X  classes both correspond well to the expected error rate .
 3.2 Utilizing Posterior Information The overall error probability of a conformal classifier is , and class-conditional conformal classifiers extend this guarantee to apply to each class individually such that (for a binary classification problem) = 0 = 1 . This effectively handles the issue of making sure that conformal predictors can provide us with reliable predictions, regardless of class (im)balance. However, we have yet to address the task of making reliable predictions that are also small. arguably, those containing only a single class label, i.e., the singleton predictions, since empty predictions and double predictions provide us with little actionable information. As illustrated by Fig. 3 , conformal classifiers, unfortunately, pro-vide no guarantees regarding the error rate of singleton predictions; as can be seen, for the hepatitis data set, the error rate of singleton predictions (OneE) is substantially greater than for low values of .
 diction being correct, without requiring knowledge of the true labels of the test patterns. To accomplish this, we are required to slightly shift our point-of-view: rather than guaranteeing the probability of making an erroneous prediction, we need to express the probability of having made an erroneous prediction. In the case of a binary classification problem, once k predictions have been made, we can state the expected number of errors as where P ( e ), P ( d )and P ( s ) are the probabilities of making empty, double and singleton predictions respectively. It is clear that we are required to make pre-dictions (at any significance level ) in order to estimate these probabilities, however, we are not required to know the true output labels of the test pat-terns. Once values for P ( e ), P ( d )and P ( s ) have been found, we can leverage three pieces of information regarding conformal classifiers and their prediction regions: the overall error rate on the k test patterns is ; double predictions are never erroneous; and, empty predictions are always erroneous. This lets us state the following, where  X  is the expected error rate of the kP ( s ) singleton predictions made. Alternatively, we can define a smoothed estimate, where the confidence in a singleton prediction is never allowed to exceed 1 Figure 4 shows, again using the hepatitis data set, that the estimates  X  and  X  correspond well with the observed error rates on singleton predictions. From Fig. 4 a, it is clear that both estimates are better indicators for the OneE scores than the significance level , however, Fig. 4 b displays an obvious issue with both estimates: singleton predictions that indicate that the true class label is  X  X IE X  are incorrect much more often than expected from both  X  and  X  is true for singleton predictions consisting only of the  X  X IVE X  class label. Thus, it seems that we have effectively undone the efforts in making sure that the overall error rates are equal for both classes. Indeed, we would ideally want to express a reliable confidence estimate in singleton predictions for each class separately, and thus need to expand on our definition of  X  .
 For our binary classification problem, we can write the expected error rate for examples belonging to class c i as where, P ( s j = i | c i ) is the probability of (erroneously) making a singleton pre-diction that does not include the true class c i ,and P ( e of producing an (automatically incorrect) empty prediction for test patterns belonging to class c i . From this we can obtain taining only class c j being erroneous. Unfortunately, this assumes that P ( e is known X  X omething that requires us to obtain the true class labels of our test set X  X owever, if we assume that no empty predictions are made, we can define the estimate Using our previous notation, we can express the estimate where  X  j is the error probability of a singleton prediction containing only class c . It is clear that this is a conservative estimate, since the presence of empty predictions can only decrease the true expected error rate on singleton predic-tions. We note also that P ( c i = j ) can be estimated from the set of calibration examples.
 the  X  X IE X  and  X  X IVE X  classes, respectively, together with the estimates  X   X 
LIV E . In both cases, the true OneE rate is approximately equal to, or lower than, the conservative estimate  X  j . To evaluate the proposed method of obtaining improved error rate estimates of singleton predictions, an experimental evaluation was conducted using 10 fold cross-validation on 20 binary classification data sets, obtained from the UCI was used as the underlying model, and the calibration set size was set to 25 % of the training data for all data sets. Equation 2 was used as the nonconformity function.
 dictions (OneC) as well as the error probability of singleton predictions (OneE) of a class-conditional conformal classifier on all 20 data sets at =0 . 1. Error rates in bold indicate that OneE &gt; 1 . 05 , i.e., where the one-sided margin of error is greater than 5 %. This error margin is due to the asymptotic validity of conformal predictors X  X e expect some statistical fluctuations in the observed error rate on a finite data set. For several of the data sets, e.g., breast-cancer, haberman and liver-disorders, the total error probability of singleton predictions ( s  X  s 1 ) is much greater than . This does not appear sufficient, as the singleton predictions would typically be those that are of interest to an analyst. Looking at the error rates of the individual classes, i.e., singleton predictions containing only c 0 ( s 0 ) and singleton predictions containing only c 1 ( s more pronounced X  X he error rate of singleton predictions containing a specific class is, for some data sets, several times greater than . So, while a conformal classifier does indeed provide us with a guarantee on the overall error probabil-ity of its predictions (when considering singleton predictions, double predictions as well as empty predictions), and even though a class-conditional conformal predictor extends this guarantee to each class separately, we cannot state any particular confidence in those prediction regions that would be of most use. exact estimate of singleton error probability  X  (Eq. 7 ), the smoothed estimate  X  (Eq. 8 ) and the class-conditional estimate  X  j (Eq. 13 ). Estimates in bold indi-empirical error rate of singleton predictions. Although the estimate does exceed the true singleton error rate occasionally, we should expect it to converge with an increasing number of calibration examples and test patterns. The smoothed estimate is automatically conservative whenever the true singleton error rate is lower than the significance level , and does not substantially underestimate the true singleton error probability for any of the data sets tested on. The class-conditional estimate,  X  j , is often conservative, in particular for the data sets where the conformal classifier outputs a relatively large number of empty pre-dictions, e.g., balance-scale, breast-w, kr-vs-kp; see Table 2 . Again, on the data sets used for evaluation, this estimate never underestimates the singleton error probability substantially; however, for the sick data set in particular, the esti-mate is extremely conservative on the s 0 predictions (indicating that they are all likely to be incorrect), which is likely a result of the low rate of s (see Table 2 ).
 Overall, it does indeed appear as though these three estimates are better able to more accurately express the true error probability of the singleton pre-dictions than the original significance level . The smoothed estimate  X  the class-conditional estimate  X  j , in particular, tend to overestimate rather than underestimate the true singleton prediction error rate, while the exact estimate  X  should be expected to converge to the true error probability given enough data. In this paper, a method is proposed for providing well-calibrated error proba-bility estimates for confidence prediction regions from a class-conditional binary conformal classifier. In particular, three estimates are proposed that express the error probability of prediction regions containing only a single class label more accurately than the original significance level, i.e., the acceptable error rate . The three estimates proposed are: an exact estimate  X  , that expresses the error probability of singleton predictions; a smoothed estimate  X  the same probability in a conservative manner (it never falls below the original expected error rate ); and, a conservative class-conditional estimate  X  expresses the error probability of a singleton prediction containing only class c All three estimates are evaluated empirically with good results.
 edge of the true outputs of the test set, however, it is necessary that several predictions are made before the estimates can be calibrated, as they require knowledge of the probabilities of making empty, singleton and double predic-tions respectively. An alternative approach, left for future work, is to obtain these probabilities from an additional validation set, or, from the calibration set itself. This could, potentially, also allow us to refine the class-conditional estimate, as it would enable us to estimate additional parameters, i.e., the prob-ability of making an empty prediction for a test pattern belonging to a certain class, that are required to express an exact class-conditional estimate rather than a conservative one.
 the proposed method in an on-line setting. As it stands, the method is best suited for use in a batch prediction setting, due to the requirement of making predictions before calculating the error probability estimates.
 multi-class problems as well as regression problems.

