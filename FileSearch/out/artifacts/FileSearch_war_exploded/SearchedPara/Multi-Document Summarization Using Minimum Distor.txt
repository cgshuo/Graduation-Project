 heuristic features of the sentences such as their positions in the text, the frequency of the words they contain, or some key phrases indicating the importance of the sentences [19]. More advanced techniques consider the rhetorical structure [21] and semantic relationships [9]. Researchers also leverage these features in some machine learning models [13, 30]. However, these techniques seem to ignore or belittle the redundancy and coverage of the summary. marginal relevance (MMR) measure which combines query relevance and information novelty in topic-driven summarization. The relevance and the redundancy are simultaneously considered in this model. Moreover, cluster-based [32] and centroid-based techniques [23] have been investigated in these years. They employ clustering to avoid redundancy and then choose the most important or representative sentences by the centrality, LexRank scores [7], and some other features. More recently, Li et al. [14] enhance the diversity, coverage and balance of summaries in supervised single-document summarization. by borrowing a concept in information theory: distortion [5]. Our main idea is to use the distortion measures to take place of the above summary standards which cannot be easily quantified integrally, and by minimizing the distortion our final summary can achieve the same or better effects. We see summarization as a data transmission system and assume that the output summary sentences represent the input document sentences. The distortion of the  X  X epresentation X  is used as a measure to evaluate the summary quality. Based on different methods of the representation and the algorithms of minimizing the distortion, we propose three summarization models: p-median model, facility location model and linear representation model. First we adopt the one-to-one representation like the clustering technique (i.e. one original sentence is represented by one summary sentence). Under this assumption we get the first model -p-median model. Then the p-median model is improved by adding constraints or features and we propose the facility location model. At last, we jump out of the idea of clustering and replace the one-to-one representation to many-to-one representation (i.e. we use a linear combination of output sentences to represent one input sentence). Our final approach takes the linear representation model combined with the facility location including the cluster centroids, the position, the TFIDF value, etc. Harabagiu and Lacatusu [10] add information to the clusters via various topic representations. Graph-based model [28, 20] is another extractive summarization model and it is often integrated with cluster-based methods [7, 27]. Erkan and Radev [7] assess the graph-based centrality (LexRank) instead of the centroid score. Wan and Yang [27] improve the Markov random walk model by using cluster-level information. an optimization objective from the information-theoretic perspective, and then consider the summarization task as an optimization problem. The previous information-theoretic methods for document summarization are based on information distance [17] and entropy estimates [24]. [24] ranks sentences by calculating entropies of symbol units. [17] uses the theory of Kolmogorov complexity and generates summaries that have the minimum information distance with the original documents. It approximates Kolmogorov complexity by compression or the coding theory and then chooses one sentence from each document [18]. The distortion measure in our models is also related to the coding theory, but it is a more accountable concept by seeing the summarization as a data transmission system. In addition, our models are all based on integral optimization of the whole summary sentences, which is different from the information distance method. Summarization models based on optimization methods are rare. Li et al. [14] improve the SVM model by directly optimizing three aspects of good summaries: diversity, coverage and balance, but it is a supervised model and used for single-document summarization. extractive summarization is to select several representative sentences  X  {}
 X  by some measures and select the sentences with the highest ranks. The ranking system can easily integrate various features of the sentence, but it cannot sufficiently leverage the correlation with the original document(s) if we only consider the word occurrence information, for it calculates the similarity between only one sentence with the whole set. The coverage of the summary is hardly considered in the ranking model either. disadvantage. The difference is that they can remove some redundancies and noises by clustering first, hence achieving some extent of diversity. But the subsequent selection in the clusters is essentially another ranking method between one sentence and a subset of the original document(s). advantage of the relevance, or on the contrary, the information loss, between the whole summary sentences and the whole set of original sentences is a problem deserved to where y indicate a kind of value of a word. In Hamming distortion y indicates the word itself, while in squared error distortion y indicates the frequency of the word. A. Hamming Distortion : this distortion mainly evaluate the number of common words between two sentences. In this case, the optimization of the summary can be intuitively explained as sharing the most words with the source without taking into account the weights of words. B. Squared Error Distortion: continuous alphabets [5]. Although there are some disadvantages, it is widely used in image and speech coding. The distortion measure has many useful characteristics: non-negative, non-decreasing, symmetry. distance, the optimization of  X  (, ) x D is d x x  X   X  can be seen as a p-median problem. With respect to this assumption, we can use heuristic algorithms for p-median problem to determine which points can be chosen as the reconstruction points. The process will be elaborated in the next section. clusters , but it is based on the K-means method which is only used to form the clusters and calculate a virtual centroid, instead of real sentences in the original texts. C. Information Divergence (KLD): (KLD), or relative entropy. It measures the expectation number of extra bits required to code when we use the distribution seen as a memory-less source of words Y , and the summary is the corresponding output. It is a good measure to evaluate the degree of representation from the information-theoretic perspective. But it has a problem that it is not symmetrical, and it does meet the triangle relation. So it cannot be handled as same as the squared error distortion sometimes. (, ) Pxy in our approach, because we want to add the distortion of each sentence and reflect the integral distortion loss of mutual information like a distortion measure. [11] demonstrates the rate distortion theory using information divergence distortion is equal to the information bottleneck method in clustering. However, in the document summarization, the two algorithms are not the same. A simple example is that when we represent all the source sentences using the sentence x with the highest T(x,Y) as not the best summary. the global information ( Y ), but ignores the information loss between the sentences. In clustering the new representation summarization the representation is a new sentence which has nothing to do with the original sentence if we use the mutual information loss as the distortion. A. P-median Clustering Model summarization problem. If the summary has a definite number ( N ) of sentences, there is no need to consider the rate region. So the optimization problem is as follows in (9). the agglomerative approach and the interchange approach. sentences in  X  are the representative sentences. Then one sentence is merged into a partition region in every step until the number of the sentences in the summary is N . The process is in fact a kind of hierarchical clustering, and this method can also serve as the base clustering method for traditional cluster-based summarization. as the initial points and then starts an iteration process to replace the former point with a new point and gain a lower cost of the objective. In this approach, the problem is seen as information distortion as the standard of summary selection, and it can integrate other features (such as features in the centroid-based method) or constraints in our model. The optimization algorithm is similar to p-median clustering, and we both use the simple local search method. Thus, our model gains a good extensibility without adding much complexity. A. Motivation original sentence is represented by a new sentence. However, it is not an optimal representation. Intuitively, if a sentence is represented by more sentences instead of a single  X  X enter X , the information loss may be less.  X 
X to represent X . Thus the distortion function is changed to: where  X  ( ) denotes the linear generative space of . system in Fig. 1 is not changed. The change can be respected as only adopting a different transmission process. one representation along the distortion measure of J-S divergence in Appendix A , i.e. And if there exists at 0 and 0( )  X  X  == X  , (11) will become: sentences assigned to  X  its initial value. tradeoff between accuracy and computational complexity. constant or larger than the last step. C. Comparison with Soft Partition the partition is  X  X ard X , i.e.  X  (|) {0,1} px x = (see (9)). The model can be improved by using a soft partition (soft assume  X  (|) [0,1] px x  X  . Thus every sentence can be represented by several sentences with a serial of probabilities: 
D is px px xdxx px dxx have similar effects. Now we compare the two ideas. As J-S divergence has similar characteristics with K-L divergence in these inequalities, we need only to take K-L divergence as the example. attain a smaller distortion than soft partition. when two sentences have less than two common words, we assign the KLDS and JSDS with a large value (1.0) and stop the clustering process according to this value. The results in Table I show that JSDS is the best measure in the interchange approach, while in the agglomerative approach different distortion measures achieve similar results. On DUC2002, we do not test all distortion measures but only use the best measure-JSDS to demonstrate the effectiveness of the improved models. KLDS, we add the length punishment function and solve the optimization problem using the facility location model. We also tried this model with the JSDS measure. On the DUC2004 dataset, we punish sentences whose lengths are more than 100 bytes or less than 50 bytes. And on the DUC2002 dataset, we assume the length of a good sentence is between 7 and 20 words. In the listed results, we find that in most cases the length constraint leads to performance improvement. The model can be further extended by adding more features like the positions and structure features; however, we do not investigate other features in this work. Our main aim here is to demonstrate the extendibility of our model. representation. As our method usually gains a local optimization, the selection of the initial sentences is crucial and it can greatly impact the final result. Fortunately, we always obtain performance improvement when using results of former runs (the interchange approach of the p-median model and the facility location model) as the initial sentences. We do not conduct experiments using distortion measures other than KLDS and JSDS at this step. method (i.e. we use the result of facility location model to initiate the linear representation model and add length punishment to the summary sentences.) achieves the best performance on both DUC2004 and DUC2002. This indicates the effectiveness of the two techniques, and also proves the distortion is a good standard to estimate the quality of summaries. models in Table III and Table IV. (As all the results of these models are cited from their original papers which maybe experiment only on one of our datasets, finally we have different control groups on DUC2002 and DUC2004, and  X - X  indicates there is no reported score in this term.) First, we list the best performance values of the DUC2002 and DUC2004 participants. Moreover, on DUC2004, the human summaries are also evaluated and the official ROUGE scores are given. In comparison with the results provided by [7], we can see the advantage of our model over the traditional cluster-based models, such as MEAD and LexRank. The topic theme method [10], the language independent graph-based model [22], and a semi-supervised model [29] are also included, Length Punishment) exceeds most of popular models and the participating systems. Especially, we have achieved a result close to the human-annotated result on the DUC2004 dataset. The result of our interchange approach is better than the centroid method, and LexRank (a graph-based method). It shows that the traditional selection methods in a cluster are not good enough and our optimization approach is a better choice, for our method conveys more integral information from the perspective of information theory. The information distance model is not very effective on the DUC2002 dataset, the reason may be that it is a model which is more suitable for topic-focused summarization. optimization of an information theoretic measure: distortion. The p-median model respects the optimization as a p-median problem and conveys as more information between the whole summary and the whole original documents as possible. The facility location model adds features to the p-median model, and the linear representation model jumps out of the idea of clustering, and modify the representation method. Linear representation is proved to be effective both theoretically and experimentally. The experimental results of our final model which combine the facility location model and linear representation model exceed most of current popular models on the DUC2002 and DUC2004 datasets. document summarization without adding many features. We focus on their theoretic founda tion and extensibility. Though the final model gains a good result in the comparison with other systems, however, more summary features (e.g. positions and structural features) should be integrated in the model in future. topic-focused and updated summarization. Researchers must take into account more factors in the new tasks. Fortunately, it is convenient to adapt our model to these new tasks. In the topic-focused task, topic relevance can be added to the cost in the facility locatio n model. In the updated summarization, we can follow the solution in information distance method [17] . where a document which can be chosen by a distortion threshold. A. Proof 1 : Linear representaion is better than one-to-one f dx x  X  =  X  with respect to  X  is taken as an example of the distortion measure. representation, will have a smaller distortion. correct in J-S divergence and we can gain the same conclusion when using J-S divergence as the distortion measure. (according to (6)) Dis D p y x p y x n =+  X  X  X  + =
