 This paper investigates the effect that text pre-processing approaches have on the estimation of the readability of web pages. Readability has been highlighted as an important as-pect of web search result personalisation in previous work. The most widely used text readability measures rely on sur-face level characteristics of text, such as the length of words and sentences. We demonstrate that different tools for ex-tracting text from web pages lead to very different estima-tions of readability. This has an important implication for search engines because search result personalisation strate-gies that consider users reading ability may fail if incorrect text readability estimations are computed.
 Categories and Subject Descriptors: H.3 [Information Systems]: Information Storage and Retrieval General Terms: Algorithms, Experimentation.
 Keywords: Readability, Text pre-processing
Search result personalisation is an area of active research within information retrieval [5]. User location, their search history, time the query is issued and type of device used to query are among the many features that current web search engines use to contextualize and personalise the search, aim-ing to increase the user X  X  satisfaction with the retrieved re-sults. In this paper we investigate one important contextual aspect: the readability of information as presented to users.
The readability of a document is often referred to as the the minimal required level of knowledge to comprehend the text, often measured using the U.S. grade level system. For example, a text with a score of 1 would be suitable for a 6-7 year old child, while a score of 13 requires the knowledge of a freshman undergrad student. In general, the higher the readability score, the harder it is to understand the text.
Within information retrieval, using readability as a way to personalise search results has received substantial atten-tion [3, 15, 18, 9, 17]. For example, Collins-Thompson et  X  al. [3] have investigated methods for estimating user profi-ciency and readability of results, as well as for re-ranking results according to this information. In health web search, accounting for the readability of the retrieved information is a core requirement to effectively support users (see for example [16]). Health consumers may have a limited un-derstanding of the medical terminology and processes, and thus they should be shown text that is simple to understand and limits expert terminology. Notwithstanding, if experts were to query, the search engine should instead provide more advanced material and detailed information.

Numerous studies have proposed and analysed methods to accurately measure the level of knowledge required to read a text [4]. While recent research has proposed sophisticated readability estimation methods [3, 7], often tailored to spe-cific domains [17], traditional readability measures such as the Automated Readability Index and the Gunning Fog In-dex are extensively used for assessing information on the web (see for example [16, 18]). These long-established readabil-ity measures consider the surface level of the text contained in web pages, that is, the wording and the syntax of sen-tences. In this framework, the presence of long sentences, words containing many syllables and unpopular words, are all indicators of difficult text to read.

Because traditional readability measures are based on sur-face level characteristics of text, the accurate parsing of web pages is fundamental to ensure readability is accurately es-timated and taken into account for search result person-alisation. For example, text contained in different HTML fields, tables, lists, etc., should be adequately processed so as to determine the wording and the syntax of sentences, including sentence length. This pre-processing step is often omitted or simplified (see for example [18]) and the influence of parsing errors on the readability estimation of web pages is unknown. On the other hand, the cleansing of web pages X  text has been recognised as an important issue in linguistics and language technology research [1].

In this work we contribute an understanding of:
In this paper we consider a number of traditional readabil-ity measures to study how web page pre-processing affects the corresponding estimations of readability. More recent and sophisticated readability measures are not considered here because they usually introduce additional complexity to the estimation process and thus introduce more degrees of variation which are difficult to control for and compare across. For example, Yan X  X  et al. [17] measure requires ac-cess to a domain knowledge resource and a method to map text to named entities in such resource.
 The readability measures we consider are listed in Table 1; To estimate readability, these measures take into account the surface level of text (i.e., the wording and the syntax of sentences). We do not consider the Dale-Chall and the Flesch Reading Ease (FRE) measures, which share similari-ties with the measures in Table 1, as their readability score scales widely differ from that of the others, and thus make their comparison more difficult to interpret. An easier way to compare different metrics was also one motivation to up-date the FRE formula to the FKGL [10].

There are two main factors that have been identified as affecting the user perception of text difficulty and that thus characterise the readability measures of Table 1: Word Length: short words are commonly used and un-Sentence Length: while short sentences are usually sim-Each measure differs from the others in the way these factors are combined, usually via a coefficient that has been tuned through comprehensive readability experiments [4].

Web pages contain text strings that do not belong to the information content of pages, but are instead used to format, structure and layout (e.g., tags) and to embed functionali-ties (e.g., scripts). The presence of these strings affect both word length and sentence length. While the use of HTML parsers allow to remove all strings not associated with the actual informative content of the pages (and thus reducing the errors in estimating word lengths), the estimation of sen-tence lengths is heavily affected by how the text is extracted from web pages, as we show with a concrete example in Sec-tion 3. This is because web pages are rich in tables, menus, lists, figures, captions, titles and subtitles: these are often part of the information content of the pages, but do not fol-low the expected structure of a sentence as assumed by the traditional readability measures. For example, often titles, menus and lists do not end with a punctuation mark that delimits the end of the sentence. In this paper we determine the effects that different ways of pre-processing web pages to extract the text associated with their information content have on the estimation of readability scores.

It is interesting to note that, already in the 1960s, the precise identification of sentence boundaries was a topic of concern for evaluating the readability of text. For example, Smith and Senter [14], authors of the Automated Readabil-ity Index (ARI), recommend typists to add to the end of each sentence an equal sign, aligned with a full stop, so as to explicitly demarcate sentence boundaries.
Before it is possible to estimate the readability of web pages, it is required to remove the HTML tags and the boil-erplate text, so as to maintain only the text associated with Table 1: Five of the most used readability measures. W is the number of words in the text, Sy the number of syllables, S the number of sentences, C the num-ber of characters, PW the number of polysyllables words (words with more than 3 syllables).
 the information content of the web pages. One approach to perform this text cleansing process is to use standard HTML parsing tools such as JSoup 1 for Java or Beautiful Soup 2 Python. We used Beautiful Soup version 4.3.2, and we term this approach as Na  X   X ve .

We also consider two open sourced tools developed specif-ically for removing the boilerplate from HTML pages: Boil-erpipe [11] and Justext [13]. We used the Python version 1.2.0.0 of Boilerpipe 3 and version 2.1.1 of Justext 4 .
Figure 1 shows the output of the Na  X   X ve approach applied to the fist paragraph of a web page from Wikipedia. The extracted text often presents an interesting characteristic: it lacks the punctuation marks to delimit the sentence bound-aries. This has a clear effect on the readability measures that consider sentence length as an indication of text dif-ficulty. To better understand the effect of this, we explore two possible approaches: 1. a sentence boundary (full stop) is added at the end 2. no sentence boundary is added, possibly resulting in Note that the Na  X   X ve -Short approach is often used when pro-cessing web pages to automatically estimate readability mea-sures, see for example [18].
To better understand the impact that parsing methods and approaches to sentence boundaries have when process-ing web pages, we consider the CLEF 2014 eHealth Task 3 collection, along with the 50 topics used in 2014 [6]. This collection contains web pages related to the medical domain and is used as a resource to evaluate search engines tai-lored to health consumers. We use this collection because of the importance the readability (and, more generally, the understandability) of web pages presenting medical advice has within consumer health search [16, 18]. Table 2: Number of words and sentences (mean and standard deviation) for documents CLEF 2014 eHealth Task 3 collection, as obtained by the three pre-processing tools and the two approaches to sen-tence boundaries (see Section 3).
 Boilerpipe 364.2  X  884 24.4  X  55 18.6  X  49
Table 2 reports the average number of words and sen-tences in the CLEF 2014 collection as extracted by the dif-ferent pre-processing methods. These statistics are at the basis of the readability measures of Table 1. From the ta-ble, we can observe that the Na  X   X ve method produces one order of magnitude more words and sentences (except when using Long ) than the other two methods. While small, the differences between Boilerpipe and Justext are still signifi-cant in that they can influence the estimations of readability measures. Similarly, the use of the Short approach for sen-tence boundary rather than the Long produces significant differences among all text pre-processing approaches. We use the default vector space retrieval model of Apache Lucene 4.8 to retrieve the top 1,000 documents per query, using the query titles of the topics in the CLEF 2014 col-lection. For each query, we compute the readability scores of the retrieved documents according to the different set-tings considered here in terms of pre-processing tools and the approaches to sentence boundaries.

Figure 2 reports the mean values of readability scores averaged over the 50 topics for each combination of pre-Figure 2: Readability scores for each measure based on different pre-processing and sentence boundary methods. Error bars indicate 95% confidence inter-vals around the mean. processing and sentence boundary approaches. The results suggest that the choice of approach to use for sentence bound-ary has a significant influence on readability measures: the variance between the readability scores obtained with Long and Short is large across all methods, apart for CLI that appears to be the most robust readability formula in this aspect. For example, when using the Na  X   X ve pre-processing method, the mean readability score of ARI can vary more than 100%, from 10 . 9  X  0 . 4, when using Short sentences to 22 . 5  X  1 . 8, when using Long sentences. This high variabil-ity in the estimation of readability measures influences the conclusions one would infer about the difficulty of the re-trieved documents: pages that could be readable by high school students (grade 11)  X  when sentence boundaries are detected with Short  X  become suddenly intractable for peo-ple with level of education below that of a PhD student (grade 22)  X  according to the readability measures computed using the Long method. In addition, note that different pre-processing methods (i.e., Na  X   X ve , Boilerpipe , Justext ) lead to different conclusions about the readability of text. For ex-ample, Figure 2 suggests that, when ARI is used as readabil-ity measure and the Long approach is employed to identify sentence boundaries, Na  X   X ve and Justext provide contrasting results, with the mean readability of text assessed as being 22 . 5  X  1 . 8 according to Na  X   X ve and 14 . 1  X  0 . 6 according to Justext . These results highlight the significance that choices of pre-processing tool and sentence boundary identification approach have on the estimation of readability scores for web pages, when using the commonly adopted readability measures considered in this study.

The results in Figure 2 also suggest that CLI is the most stable readability measure among those considered in this paper. In particular, variations in pre-processing tool and sentence boundary identification have little impact on the estimated readability scores for this measure. The stability of CLI is due to the fact that W S and thus S W &lt; 0, dampening the effect of the relation between the number of words and sentences (in our experiments, 1 &lt; 30 . 0  X  S and ensuring stability across different values of S . This is unlike measures such as ARI, where 3 &lt; 0 . 5  X  W S &lt; 13.
Next, we consider how similar document rankings ob-tained from readability measure estimations are when using different pre-processing and sentence boundary approaches. This is interesting for information retrieval because it is of-ten these differences between rankings, rather than the ac-tual absolute value of the readability estimation, that are used to demote or promote web pages when taking into ac-Figure 3: Kendall  X  correlation and 95% confidence intervals between the Short and Long approaches for sentence boundary identification.
 Figure 4: Kendall  X  correlation and 95% confi-dence intervals between the approaches for HTML pre-processing, under different settings for sentence boundary identification. count reading levels. For example, a high correlation be-tween two different pre-processing settings would suggest that, although the actual readability scores may be very dif-ferent, the preference ordering obtained by the readability measures (i.e., the ranking according to readability scores) are similar and therefore these two pre-processing settings would lead to little difference in terms of impact on retrieval.
To this aim, we consider the Kendall  X  ranking correlation between different settings of sentence boundary identifica-tion (Figure 3) and pre-processing tool (Figure 4).
Figure 3 shows that, independently of the pre-processing tool used, the correlation between Long and Short rankings obtained when using Boilerpipe or Justext as pre-processing tools is generally high, with the maximum achieved using CLI (  X  = 0 . 92  X  0 . 01). However, if the Na  X   X ve approach to text pre-processing is used, then correlations deteriorate, with the SMOG measure exhibiting only marginal correla-tion (  X  = 0 . 20  X  0 . 03). CLI exhibits the least variance in correlation among the three pre-processing approaches (and indeed, the highest correlations)  X  a stability that was al-ready observed when analysing Figure 2.

The results of Figure 4 suggest that different pre-processing tools produce different document rankings (when using read-ability to rank). Specifically, the highest correlation between two of these tools is achieved by the Boilerpipe -Justext pair  X  but these exhibit correlations of only about 0.5, indepen-dently of the readability measure or the sentence boundary approach (the highest correlation is achieved for SMOG by Boilerpipe -Justext using Long :  X  = 0 . 57  X  0 . 02). When comparing these methods to the Na  X   X ve approach, correla-tion sensibly decreases (apart for CLI that once again shows the smallest difference between settings).
This paper analysed the influence pre-processing and sen-tence boundary identification choices have on the estimation of readability measures for web pages. The experimental re-sults show that these choices have a large impact on the es-timation of readability scores, which in turn can drastically influence the order relations among documents that can be obtained from the readability scores. Our findings suggest that attention should be put on the choice of pre-processing settings when measuring readability for web pages. Ad-vanced HTML cleansing tools such as Boilerpipe and Justext provide more stable results across settings. In addition, the use of the Coleman-Liau Index (CLI) as readability mea-sure leads to the most stable results across choices of pre-processing tools and sentence boundary identification strate-gies (although we could not assess the quality of CLI for cor-rectly estimating the readability of documents). In future work, we plan to study which combination of pre-processing settings and readability measure lead to estimations of read-ability that most agree with user assessments. The data and source code used in this work can be found online at: https: //github.com/joaopalotti/cikm_readability_2015 .
 JP and AH are supported in part by the European Union Seventh Framework Programme (FP7/2007-2013) n o 257528 (KHRESMOI), by Horizon 2020 program (H2020-ICT-2014-1) n o 644753 (KCONNECT), and by the Austrian Science Fund (FWF) n o I1094-N23 (MUCKE).
