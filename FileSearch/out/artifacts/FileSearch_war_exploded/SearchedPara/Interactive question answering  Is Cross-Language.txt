 1. Introduction
Cross-language question answering (CL QA) is defined as a variant of the Question Answering task where the questions and the documents (containing the answers) are written in different languages. Comparative evaluation exercises devoted to this problem ( Magnini et al., 2005 ) have shown that the Cross-Language prob-lem is significantly harder for machines than its monolingual counterpart.

Little is yet known, however, on the nature and difficulty of the cross-language QA task from an interactive locate answers in the web using standard Information Retrieval engines, without the assistance of specific
QA machinery. In this search scenario, the ability of the search engine to scan large numbers of documents combines gracefully with the ability of the user to state and refine his queries and quickly recognize the answer to the question within the contents of potentially relevant documents.

The Cross-Language version of this search scenario is also a realistic task: the user wants to know the answer to a question, and if the answer is out there in some web document, he wants to find it as fast and easily as possible, even if it is written in a language he does not understand. This is perhaps a more common information need in web searches than its document retrieval counterpart (where the user is interested in get-ting several documents covering a topic of interest in a language he cannot read). For instance, a user prepar-ing a trip to a foreign-language region will probably have general travel documentation in his native language, guage. This is a usual searching problem (specially for non-English language speakers).

Intuitively, it seems that when the answer has to be found in an unknown language the task becomes sig-nificantly harder. Machine Translation (MT) plays a key role, and the output of general purpose MT systems tends to be noisy and hard to read. Users must be able to recognize the answer when it is present, and to iden-tify the failure reasons when it is not. But how much harder is it? There is no empirical evidence, to our know-ledge, substantiating and quantifying this intuition.

In this paper we aim, then, to provide an initial answer to this question: Is it Cross-Language QA signifi-cantly harder than monolingual QA from an interactive point of view? In other words, is the cross-language problem a significant barrier for users when searching answers?
We have focused on the following issues: (2) Is more context needed to answer questions correctly when searching cross-language? Intuitively, if the
Therefore, we want to study two facets (language mismatch and amount of context) with two conditions each (mono/cross-lingual, paragraphs/documents), which gives four configurations to be compared in an empirical test involving users (see Fig. 1 ).

Comparing these four combinations in an experiment involving users is not trivial. Unlike batch systems, we cannot ask a user to search an answer for the same question more than once (with different systems), this change will also have an effect on the results. The standard way to avoid this problem in interactive search experiments is using a latin-square matrix design which alternates users, topics and systems to eliminate user/ topic effects from the comparison of two different systems (see Section 2 for details).

The latin-square design can be applied to compare the document/paragraph search conditions. But the mono/cross-language comparison is harder. As we must keep the same set of questions and the same docu-ment collection for both conditions, inevitably we must change the set of users, because in the monolingual setting they must be native speakers in the document language, and in the cross-language setting they must be unfamiliar with the document language.

Overall, the only possibility to apply a standard interactive retrieval evaluation design is doing two exper-iments: an experiment with a latin-square to compare the paragraph/document conditions, with a set of native speakers in the document language, and then a duplicate of that experiment with a set of users unfamiliar with the document language. We have to be cautious, therefore, with the conclusions that we extract by comparing two different sets of users. Even so, the outcome of our experiments has proved useful and informative, both to offer initial answers to the research questions posed and as a case study to discuss the problems of the latin-square design for studies involving cross-language searching issues.
 The remainder of the paper is organized as follows: in Section 2 , we describe our experimental design. In
Section 3 , we present and discuss the quantitative and qualitative results of our study. Section 4 points out some weaknesses of the standard evaluation methodology for the task being studied. Finally, Section 5 dis-cusses related work and conclusions are presented in Section 6 . 2. Experiment design
Our experiment design follows the iCLEF 2004 guidelines ( Gonzalo &amp; Oard, 2005 ), which are, to our knowledge, the only standard experimental setting for interactive cross-language question answering evalua-tion used in an evaluation campaign. Basically, users are given 5 min to find an answer for each of 16 ques-tions, half with each of two systems being compared. Questions are a sample of the question set given to the participating systems in the CLEF QA 2004 ( Magnini et al., 2005 ) task, and the answers provided by the users are evaluated with the same methodology and by the same assessors used in the (batch) QA task (see Section 2.5 ). In the cross-language condition, users write the answer in their native language, then they are manually translated into the target language, and then the translations are evaluated by the assessors.
In this section we summarize that design and how we have particularized it to our experiment. 2.1. Test data
We have used the iCLEF 2004 question set (see Table 1 ) and the English document collection ( Peters, 2005 ) (news from Los Angeles Times 1994 and The Glasgow Herald 1995) previously translated into Spanish using Systran Professional 3.0 by the iCLEF organization.
 The iCLEF 2004 question set is available in English (used for our monolingual search condition) and
Spanish (used for our cross-language search condition). Note that questions were chosen in a way that minimizes the chances that the user knows the answer beforehand, and the chances that the user elaborates the answer (as in definition questions), because both cases could cause problems in the evaluation phase.
NIL questions (i.e. those which cannot be answered with the document set) were excluded from the iCLEF question set.

We asked our users if they were familiar with the questions before starting the search process; only in two occasions (out of 16  X  16 possibilities) a user stated that he might know the answer to a question before start-ing the search. 2.2. User profiles
For the cross-language experiment, we recruited eight native Spanish speakers between 20 and 43 years old, with different levels of education and low or medium X  X ow English skills. All were familiar with graphical inter-faces and search engines, but had little or no familiarity with Machine Translation systems. This experiment was carried out in Madrid at the UNED campus.

For the monolingual experiment, all eight native English participants came from an academic environment, were between 19 and 31 years old and, either had a degree or were pursuing it. All were familiar with graphical interfaces and search engines. This experiment was carried out in Maryland at the University of Maryland campus.

User profiles in both experiments do not exactly match, mainly for reasons of availability; for the cross-language experiment, we had to find users with low English skills, a feature that was difficult to match in a computer science faculty. 2.3. Search interfaces
Our Documents system is a simple document retrieval engine. Users type in queries, and monolingual retrie-val is performed using the Inquery X  X  API ( Callan, Croft, &amp; Harding, 1992 ).

To make the monolingual and cross-language conditions identical from the point of view of interface design and system delays, the cross-language version of the system is identical; the only difference is that the docu-ment set is the English to Spanish Systran version of the original documents.

The system displays the results as a standard ranked list of document titles and dates. Then, the user can access the full documents by a single click. The interface has additional buttons to store a document, visualize documents previously saved and finish the search marking a document as supporting the answer.
The Paragraphs system starts by asking the user to select which kind of answer is appropriate for the ques-tion: a proper noun, a temporal reference or a quantity. Then, the search interface is similar to the Documents system, with a few differences: (i) it retrieves paragraphs instead of documents. The possibility of examining the context of a paragraph is intentionally excluded to test whether context is necessary or not to find and validate answers; (ii) paragraphs containing the selected kind of answers are promoted in the rank by the Inquery X  X  standard weighting schema.

The retrieval engine is identical to the first system (Inquery X  X  API); only the way in which the collection is indexed changes, because now each paragraph is indexed as a document.

The filter that classifies paragraphs is intended to be non-aggressive, because we do not want to exclude a paragraph containing the answer, and quite straightforward:
A paragraph contains a proper noun if there are expressions in uppercase where uppercase is not prescribed by punctuation rules. Locations are looked up in a gazetteer and filtered out, because  X  X  X ocation X  X  questions are excluded from the iCLEF question set (see Table 1 ). In 75% of the paragraphs some proper noun was found.

A paragraph contains a temporal reference if there is a match with a list of words denoting dates or a num-ber between 1900 and 1999. This temporal restriction is set ad hoc for CLEF data. This happened for the 21% of the paragraphs.

Similarly, a paragraph contains a quantity if there is a number or a word from a given list (such as dos , tres , This condition was met in 43% of the cases.

Note that this filter makes a binary decision as to whether there are proper nouns, dates or quantities, but it does not find them, making the recognition task much easier. 2.4. Search sessions
Every user searches all 16 questions, half with each system, following the iCLEF latin-square matrix shown in Table 2 to factor out user/question effects on the results. The maximum search time per question is 5 min. Once time expires, the system stops the search process and the user is given a last chance to write an answer.
Participants are asked to fill in a number of pre-and post-search questionnaires about their previous expe-rience using computers and search engines, the usability of each system and their opinion about the whole task. In the cross-language experiment there are additional questions about their English skills and the perceived quality of the translations. 2.5. Evaluation metrics Answers provided by users are evaluated with the same criteria and by the same assessors used in the CLEF QA track. Every answer is marked as: Right (R), when the answer is correct and supported by the document.

Inexact (X), when the answer is supported by the document but the answer snippet is not completely accu-rate (e.g. there is some missing information or there is some additional text).
 Unsupported (U), when the answer is correct but is not supported by the document.
 Wrong (W), when the answer is not correct.

Using these assessments, two overall measures are computed: strict accuracy (fraction of right answers) and lenient accuracy (fraction of right plus unsupported answers).
 All assessments are performed by native speakers in the original language of the documents (English). Spanish and English versions of the questions were provided by the iCLEF organization. For the Cross-
Language experiments, we had to provide a manual translation of the Spanish answers into English, which was then used by the assessors to make the judgements. 3. Results and discussion
Table 3 shows the results of both experiments in terms of strict and lenient accuracy and additional data about users performance including average time taken with each retrieval system, level of confidence in the answers and number of query refinements.
 3.1. Is cross-language searching harder than monolingual searching? 3.1.1. Differences in accuracy
Our first (and main) research question was whether cross-language answer finding is indeed harder than monolingual answer finding.

From a quantitative point of view, the accuracy obtained by cross-language users is around 10% worse than the monolingual ones, and both sets of users perform the task reasonably well (above 0.7). This is a small dif-ference and suggests that the cross-language barrier can be overcome with standard MT machinery. Of course, this result has to be taken with caution, given that the user sets are different and cannot be compared directly. case, the high absolute figures in both cases also support our conclusion.
 In Fig. 2 , we can see the absolute and relative accuracy increments for both systems within each experiment.
It is worth mentioning that the strict accuracy is about 4% higher for the Documents system in both condi-tions, suggesting that their relative performance is not affected by cross-language issues. The monolingual experiments, in addition, show similar increments with respect to their cross-language counterparts (+11.4% for Documents system and + 11.9% for Paragraphs system).

Observing more carefully the accuracy per question, as Fig. 3 shows, in the monolingual environment all questions obtained, at least, one correct answer. The cross-language experiment is similar except for question number 12. Given that millo  X  n is a common noun in Spanish (meaning  X  X  X illion X  X ), when launching queries such as  X  X  X harles Millon political party X  X , the users obtained documents containing instances of this word with this numerical meaning. It seems that finding documents about  X  X  X harles Millon X  X  using this strategy became a rather difficult task under the tight time constraints of the experiment.

Also notice that questions 1 ( what year was Thomas Mann awarded the Nobel Prize ) and 15 ( how many people live in Bombay ) turned out to be quite easy and all participants in both experiments found the right answers. One possible explanation is that there were not many documents discussing these two topics, and it was not hard to find the correct answer in the top ranked documents retrieved. A 100% accuracy would have been obtained also on topic 8 ( who is the managing director of the international mone-tary fund ) if one of the cross-language users had not made a cut-and-paste mistake when writing the answer.

Regarding question number 11 ( who is the president of Burundi ), the only two users who found a wrong answer submitted  X  X  X elchior Ndadaye de Frodebu X  X , who was mentioned in some documents as a former pres-ident of Burundi.

In a few questions the accuracy was surprisingly higher for the cross-language experiment. Let us analyze them in detail:
Questions 2 ( amount of human genes ), 9 ( Lenin death date ) and 13 ( Bobby Robson X  X  team ): in these three cases the increment in accuracy is caused by one more correct answer in the cross-language side. The users seem to be the main cause of the differences. For instance, on question 13, some users were unable to iden-tify soccer teams X  names because they were not familiar with this sport.

Question 16 ( Nobel prize for literature in 1994 ): in the cross-language experiment the users apparently did not have any problem to find the correct answer, since all of them found  X  X  X enzaburo Oe X  X . Nevertheless, in the monolingual environment two users answered former Nobel winners X  names, mentioned in documents dated in 1994.

Question 3 ( German minister for economic affairs ): in a cross-language environment, with documents trans-lated using a MT system, users need to read, interpret and understand the documents. Since they knew that the translation was not perfect, they tended to understand  X  X  X conomic Affairs X  X  and  X  X  X inances X  X  as synonyms.
In the remaining questions, as expected, the monolingual accuracy was higher. This is the case, in parti-cular, of questions 4 ( terrorist attack in Tokyo underground )and6( independence of Latvia ), where the main problem to locate the answers was Systran X  X  incorrect translations (see discussion in Section 4 ). 3.1.2. Differences in searching behavior
The highest difference between the monolingual and cross-language conditions is the average search time (see Fig. 4 ). Monolingual searchers were able to find answers much faster that their monolingual counterparts (2 min versus 3.5 min in average).

In both experiments, users were generally more confident about the answers they found using the Docu-ments systems, although the differences are not significant (see Table 3 ).

Finally, as far as the number of refinements is concerned, differences in the monolingual experiment are more noticeable, and it seems that users, regardless of the language barrier, need to make more refinements when browsing with the Paragraphs system. 3.1.3. Questionnaires and observational study
Comparing the qualitative data obtained from the questionnaires and the observations made along the search sessions, some facts can be highlighted:
Users in the monolingual experiment perceived the task as easier and the differences between systems as smaller. System X  X  evaluation figures are on average higher in this experiment while variations when compar-ing both systems are very small (see Figs. 5 and 6 ).

Participants of the cross-language experiment found the paragraph system easier to learn. This may be explained because it is harder to find a potential answer reading a large document automatically translated than only a short paragraph. This problem does not exist in the monolingual environment and thus, user preferences were reversed (see Figs. 7 and 8 ).

Both monolingual and cross-language users declared that the Paragraphs system was easier to use because it helped focus on the question target. In some cases, this feature was anyway harmful, as not enough con-text was provided in order to confirm whether a possible answer was indeed correct.

Users found the translations usable to recognize answers (specially when looking at isolated paragraphs, 3.8/5) but the perceived quality of the Systran translations was lower (2.5/5). 3.2. Is more context needed to provide accurate answers?
Now we compare the performance of the document and paragraph searching conditions, both in cross-language and monolingual settings, in order to analyze how the access to the context of a given paragraph may affect the answers X  accuracy.

First, in the cross-language environment, the average strict accuracy is 4.5% higher for the Documents sys-tem, and the search behavior (i.e. average searching time, confidence in the answers, average number of refine-ments) is very similar for both systems (see Table 3 ). The absolute performance (between 0.67 and 0.70 strict accuracy) leaves room for improvement, but we can conclude that users can find answers efficiently without
QA-specific machinery. This accuracy is obtained in an average time of 3.5 min per search, using only 1.6 refinements per question in average.

It is worth noticing that the Paragraphs system, which has some search features specifically oriented towards Q&amp;A, performs worse than the Documents system, although the difference is not statistically signi-ficant. Our observational studies, together with the questionnaires filled in by the users, provide some explan-ations for this result:
Fig. 5 shows the answers to questionnaires after the searching process with each system. According to our users, the Paragraphs system was easier to search with, faster to use and it was easier to find and recognize answers with it. The translation quality X  X hich was identical for both systems X  X as indeed perceived as similar by the users. In principle, we can conclude that users felt more comfortable when searching with the Paragraphs system.
 The data of the final questionnaire, where users were explicitly asked to compare systems, can be seen in Fig. 7 . In this comparison, the Paragraphs system is perceived as  X  X  X asier to learn X  X  and  X  X  X asier to use X  X . However, when asked for the better system overall, both systems received half of the votes.
We can find a possible answer to the differences between systems looking at the comments made by our subjects: most users declared that the Paragraphs system was easier and faster to use, but they often wanted to look at the whole text of the document to ensure that a potential answer was correct. This is a factor which is related also to the poor translation quality, which often makes users hesitate about the validity of a potential answer found in a paragraph.

From the comments made by our subjects and from their search behavior, it seems clear that the preferred search mode is paragraph retrieval, but adding the possibility of visualizing the full content of the documents X  as in Figuerola, Zazo, Alonso Berrocal, and Rodr X   X  guez Va  X  zquez de Aldana (2005)  X  X s a desired feature.
Regarding the monolingual experiment, the average strict accuracy for the Documents system is higher than for the Paragraphs approach (+4%), reaching respectively 0.78 and 0.75 (see Table 3 ). The average search time is slightly over 2 min for both systems. Also, using Documents, our users consider that their answers are more reliable, and also find answers with fewer query refinements.

In Fig. 6 the data of the questionnaires filled in by our users are shown. Even though the users declared that the search task was easier and faster using the Paragraphs system, they felt that the Document system offered more reliability. Overall, however, users did not find substantial differences between both systems.
Analyzing the final questionnaires (see Fig. 8 ), we see that the users thought that the Documents system was easier to learn, but harder to use. As in the previous experiment, the overall impression was similar for both systems.

Along the search sessions, all users asked to take a look at the full text of the documents when searching with the Paragraphs system. Thus, it seems that knowing the context of a given paragraph is helpful to con-firm the correctness of a possible answer, even if paragraphs do not have automatic translation problems.
Unlike the cross-language experiment, it is not clear whether the preferred system is Paragraph retrieval, given that the differences between systems are smaller. Anyway, monolingual users also found desirable to access the full content of the paragraph. Note that all users in both conditions required this facility, even though most automatic Q&amp;A systems only use individual paragraphs to find answers. 3.3. Failure analysis
Next, we will take a deeper look at the assessments in order to study whether the mistakes made by our users are specific for a given environment or directly related to a task of this kind.

On one hand, the assessments for the cross-lingual experiment are shown in Fig. 9 . The bars represent the number of answers for each possible category of evaluation, i.e. right (R), wrong but non-NIL (WNN), inex-act (X), unsupported (U) and NIL ( Gonzalo &amp; Oard, 2005 ). The official evaluation does not distinguish between a wrong answer and no answer, but we have separated both cases for a better understanding of users X  behavior.

Out of 128 answers, 32 were judged as wrong, 7 as inexact, and only 1 as unsupported. Out of the set of 32 wrong answers, 21 were time outs in which the users were not able to find an answer in the available time. In the remaining 11 + 7 + 1 = 19 cases, the users gave an answer that was not judged as correct. Among the pos-sible causes of errors, we can mention:
Misleading translations . In every experiment involving automatic translations, one of the causes responsible for an incorrect answer is an incorrect translation of the original source of information. In some cases, our users were doubtful about an answer and kept looking for additional evidence in order to support the answer.
However, once time had expired, they preferred to give an answer assigning a low level of confidence than no answer at all.

Notice that, while in automatic cross-language QA systems documents translation may be avoided (in these cases, a query translation approach is faster), in our interactive experiment, we needed to translate the whole collection in order for our monolingual Spanish users to understand the documents. Thus, accurate transla-tion should be a specific requirement for any interactive CL-QA systems.

Human errors . Needless to say that every experiment involving human beings can be influenced by human errors.

Typing errors : In a few cases, the user just made a mistake when writing the answer. For instance, one of our users stated that the Channel Tunnel cost  X  X 15,000 million X  X  without specifying the currency unit, so the assessors judged this answer as inexact. In other example, a cut-and-paste error repeated the answer given for a former question.

Question misunderstanding : In most cases, the user misunderstood the target of the question and answered something related with the question but wrong. For instance, a user, when asked about the consequences of typhoon  X  X  X ngela X  X  or a concrete terrorist attack, answered about a different typhoon or blast.
Different responsiveness criteria . Occasionally, the user and the assessor had different opinions about the responsiveness of a given answer. Considering the goal of this task (finding an answer and a document supporting it), questions like How many people were declared missing in the Philippines after the typhoon  X  X  X ngela X  X ? , which can have different right answers in different moments (e.g. the number of casualties of a disaster usually grows as time goes by) show more disagreements between assessors and users.

Another example can be found in question 14 ( date of attack in Paris underground ) in which some users did not consider necessary to specify the day of the attack, although the full date could be found in the supporting document. In this cases, the assessor judged the answer as inexact.

On the other hand, Fig. 10 shows the assessment for the monolingual experiment. In this case, out of 128 answers, the assessors judged as wrong 24 (8 of which were due to time outs), 4 as inexact and only 2 as unsup-ported. The sources of error are:
Human errors. Since this experiment does not involve automatic translations, users were the major source of errors:
Typing errors : Notice that in the monolingual experiment only four answers were considered as inexact. The only one that may be considered as a typo appears on question 3 ( German minister for economic affairs )in which one user misspelled  X  X  X heo Waigel X  X  as  X  X  X heo Weigel X  X .

Question misunderstanding : Occasionally, users misunderstood concepts in the question, such as interpret-ing wrongly chromosomes instead of genes in question 2, political position instead of political party in question 12 ( Charles Millon X  X  political party ) and people that lost their houses instead of people declared missing in question 7 ( missing people after typhoon Angela ).

Different responsiveness criteria . The remaining inexact answers are found in question 4 ( authors of Tokyo underground terrorist attack ), where one of the possible answers was a person and one user did not submit the of the attack.

It is also interesting to study whether the paragraph filters had any effect on the quantitative differences between both systems. Recall that filters are used in the paragraph system to promote in the ranking the para-graphs more likely to contain a possible valid answer.

On one hand, Fig. 11 shows the Precision at Recall 100% per answer type in both experiments. To compute it, first, we manually checked the documents that had been assessed as relevant in the official iCLEF 2004 evaluation and we compiled a list of paragraphs that actually contained the answer for each topic. Then, we collected all queries submitted by our users during the experiments and, for each query, we retrieved up to 10,000 paragraphs, both enabling and disabling the filter in the retrieval process.

The differences in precision between using and ignoring the filter are negligible so we can conclude that the filter did not have visible effects in the final results.

On the other hand, Fig. 12 shows the Mean Reciprocal Ranking, also for each answer type in both exper-iments. MRR is computed by averaging the inverse of the position of the first retrieved document judged as relevant. Again, the differences were not significant.

The effect of the filter was not the expected, but looking at the figures it is clear that its influence in the results was not relevant. So it is reasonable to conclude that the differences between systems are not due to the filter. 4. Is the experimental setting appropriate for interactive QA? comparison between the monolingual and cross-language conditions cannot be made directly; this is a lim-itation hard to overcome.

The latin-square design is scientifically sound, provides direct evidence on the problem under study, and prevents from reaching inaccurate conclusions from the experimental data. But we have found it too restrictive hard to obtain statistically significant differences. Perhaps an observational study with many more users, in a more realistic scenario (real user needs rather than predefined questions) and one single search interface (with alternative or complementary facilities), would provide richer data to understand the nature of the cross-language searching problem.

The second limitation of our experimental setting is that using the same assessments for humans and for machines do not reveal the nature of the problems faced by humans and their strategies to overcome them when seeking answers.

Regarding solutions provided by humans, in the cross-lingual environment users were able to jump over significant translation problems. For example, the word  X  X  X atvia X  X  was not translated by Systran. Users ini-tially searched using the Spanish term, Letonia , and found no information. Nevertheless, a few documents to infer the meaning of Latvia from the context. Our evaluation setting does not distinguish between cases like this and cases where the translation is good enough for a direct answer extraction; only the observational study raises these issues qualitatively.

Another problem is how to incorporate the previous knowledge of the user into the evaluation. For instance, one person, using her previous knowledge about the independence of the three Baltic countries, found the right answer for the independence of Latvia, but it was judged as unsupported because the submit-ted document did not mention Latvia but only Estonia. This is an example that shows how applying the same assessments to humans and machines may lead to misinterpretation of results.

It is also worth mentioning that users were able to make more inferences than current QA systems. An interesting example is the question 9 When did Lenin die? Some users, in both cross-lingual and monolingual experiments, found a 1994 document referring to the beginning of the celebrations of the 70th anniversary of Lenin X  X  death and correctly deduced  X  X 1924 X  X .

In a similar way, some users found the right answer for question 14 ( Paris underground attack date ) deduc-ing it from the date of the document and the temporal references of the text. For example, a document dated Metro station in the heart of Paris [ ... ].

Regarding the problems encountered by humans, we would need an assessment procedure that distin-guishes between failure sources: we must be able to tell cases where the human has performed the search per-fectly, but made a typo or cut-and-paste error, from cases where the translation was incorrect and led to an inexact or wrong answer. In other words, the typology of assessments must be rich enough to provide (at least partially) a failure analysis adapted to human searching behavior.

A third problem is related to assessment itself: roughly speaking, the assessors X  task is to judge if a given answer can be supported by the attached document ( Magnini et al., 2005 ). This task is identical to the one performed by our monolingual users: to give an answer and a supporting document. Having reached levels of accuracy of about 0.75, it seems necessary to consider the different opinions and interpretations of the peo-ple involved in the experiment and evaluation processes as a whole.

When examining the judgements, we have found that there exists some conflicts not only between assessors and users, but also among the assessors themselves. For example, on question 9, faced with the answers  X  X 1924 X  X  and  X  X  X n the year 1924 X  X  supported by the same document, the first one was judged as right and the second one as inexact. Given the high overall results obtained in the experiment, it is necessary to study inter-annotator agreement to find out which is the practical ceiling on performance with this evaluation methodology.

Following the current iCLEF guidelines, it is not possible to take into account the previous knowledge con-tributed by the user. We think that it would be necessary to consider it together with the information found in the collection. Now a correct but unsupported answer is discarded, regardless of the a priori knowledge that may be used as an intermediate tool during the search process, as in the Latvia/Estonia example explained above.

In summary, the iCLEF experiment design is focused on the output of the search process, and meant to be directly comparable with the output obtained by automatic Q&amp;A systems. After our analysis, our impression is that the evaluation should better focus on the search process, centering on the observational analysis and including failure and success reasons in the quantitative analysis. 5. Related work
Most research in interactive question answering has been done in monolingual scenarios. The experiments which are closer to our methodology have been performed in TREC and have inspired the interactive CLEF experiment design ( Hersh &amp; Over, 2000, 2001 ). In general, TREC experiments involved more complex ques-tions than those used in our experiments. For instance, questions that involve assembling information from different documents, questions that require listing a number of items, or questions that require comparison of items for an answer. We wanted, however, to start with the simplest kind of questions in order to discern whether the cross-language search condition was a significant burden on the search task.

More recently, the interest in interactive Q&amp;A has focused on the so-called  X  X  X omplex question answering X  X  or  X  X  X uestion answering in complex scenarios X  X , where users have to perform complex information synthesis tasks which can be decomposed in series of simpler related questions ( Harabagiu &amp; Lacatusu, 2004; Hickl,
Lehmann, Williams, &amp; Harabagiu, 2004; Small et al., 2004 ). This task is, however, closer to summarization than to the simple Q&amp;A attempted in our experiments, and is much more difficult to evaluate from an inter-active perspective.

The only cross-language experiments in interactive Q&amp;A known to us have been conducted at iCLEF 2004/ 2005, using the same experiment design and test bed of our study. The main difference is that we have com-plemented the cross-language study with a monolingual counterpart, which helps factoring out Q&amp;A intrinsic features from cross-language problems. Among iCLEF participants, the University of Salamanca performed an experiment ( Figuerola et al., 2005 ) in which one of the conditions being tested was whether the users were allowed to see full documents or not. The results of this experiment agree with our observation that users want to check the content of the documents to attest potential answers. 6. Conclusions
In this work, we have performed two Interactive Question Answering experiments. We intended to study the inherent difficulties in a cross-language environment when searching answers for general purpose questions.

Just by coupling standard Document Retrieval and Machine Translation systems, our users have been able to find a correct answer in 70% of the questions within a five minute time limit. This suggests that it is not necessary to use complex machinery for this interactive task, even in a cross-language setting.
In the monolingual experiment (without Machine Translation), users found a correct answer in 78% of the cases, an increment of only 11.4% with respect to its cross-language counterpart. As well as this accuracy improvement, users performed the task 40% faster on average.

These results suggest that, without time restrictions, the cross-language aspect is not a significant burden on the ability to find answers using relatively conventional Information Retrieval facilities. Finding cross-language answers as fast as in a monolingual setting seems, nevertheless, a difficult technical challenge for Q&amp;A search assistants.

Concerning the differences between the systems (document versus paragraph conditions), it is worth notic-ing that the variation between them was the same in the monolingual and cross-language experiments (around 4% higher for documents in both cases). In spite of the fact that most of our users preferred searching para-graphs, they performed better when browsing full documents. Indeed all users demanded the possibility to access the full context of the paragraphs in order to validate a possible answer. This suggests that users need a wider context than what automatic systems usually consider to extract answers. Indeed, a human is able to take advantage of a larger context to understand the meaning of the text and make more complex inferences.
This leads us to the lack of an appropriate evaluation methodology for the interactive task. As we have discussed above, the current evaluation metrics, designed for automatic systems, cannot capture specific inter-active aspects that are of special interest in this kind of task. We think it would be necessary to take into account the full searching and reasoning process of the user, instead of considering just the final answer and its supporting document. Besides, the measures used in evaluations strongly penalize subtle human mis-takes such as cut-and-paste errors or writing typos.
 Acknowledgements
This work has been partially supported by the Spanish Government under the project R2D2-Syembra (TIC2003-07158-C04-02). V X   X  ctor Peinado holds a Ph.D. grant by UNED ( Universidad Nacional de Educacio  X  n a Distancia ).
 References
