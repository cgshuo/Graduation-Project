 Online reputation management is a task that basically involves  X  X onitoring and handling the public image of entities (people, products, organizations, companies or brands) on the Web X  [ 7 ]. On today X  X  World Wide Web Twitter has assumed the role of an effective marketing platform with almost all the major compa-nies maintaining their Twitter accounts. On the other hand, Twitter users often express their opinions about companies via short Twitter messages (around 140-character long) called tweets. Companies are highly interested in monitoring their online reputation; this, however involves the significant challenge of disam-biguating company names in texts. This task becomes even more challenging if company names have to be identified in tweets due to their short length, to the huge amount of noise in tweets, and to the lack of context that could help in company name disambiguation [ 6 ]. lenges related to company name disambiguation in tweets; while this technique has recently been evaluated in the context of the RepLab task at CLEF 2012, its formal definition is presented for the first time in this paper. The task provided a set of companies and for each company a set of tweets, a subset of which was relevant to the company, and a subset was irrelevant.
 use of Wikipedia as an external knowledge resource in the first step, and of a concept term score propagation mechanism in the second step. The first step is precision-oriented where the aim is to reduce as much as possible the noisy tweets to a minimum. The second step has been defined to enhance recall via a score propagation technique, and through the utilization of other knowledge resources. Our technique has shown high accuracy figures over the given dataset, and it was classified as the second best algorithm. There has been an increasing interest in research on applying natural language processing techniques to tweets over the past few years. However, in spite of the immense significance of extracting commercially useful information from tweets, the amount of research dedicated to company name disambiguation in tweets is very limited. The only two serious efforts which have been undertaken to stimulate this research task are represented by the WePS online reputation management evaluation campaign at CLEF 2010 [ 1 ], and by the RepLab online reputation management evaluation campaign at CLEF 2012 [ 2 ]. The best two teams in the WePS online reputation management evaluation campaign were LSIR-EPFL [ 9 ] and ITC-UT [ 10 ]. The LSIR-EPFL system builds profiles for each company relying on external resources such as WordNet or the company homepage in addition to a manual list of keywords for the company and the most frequent unrelated senses for the company name. The profiles are then used for extraction of tweet-specific features for use in an SVM classifier. The ITC-UT system is based on a two-step algorithm. In the first step, the algorithm categorizes queries by predicting the class of each company ( X  X rganization-like names X  or  X  X eneral-word like names X ) using a Naive Bayes classifier with six binary features (for example, is the query an acronym?, is the query an entry of a dictionary? etc.). They use thresholds manually set by looking at the training data results for this categorization. The second step consists in categorizing the tweets using a set of heuristics.
 UT indicate heavy reliance on manual selection of both terms and thresholds for the company name disambiguation task. Moreover, their reliance on a large amount of training data makes them infeasible for real-world  X  X ompany name disambiguation X  tasks, and this is particularly true for tweets where obtaining training data is extremely hard. This was particularly evidenced during the RepLab 2012 online reputation management evaluation campaign, where even the best performing team relied on hand-coded rules [ 2 ] for the filtering task. Instead of relying on semi-automatic and supervised methods, we have defined a completely automatic two-step algorithm for this task that relies on Wikipedia as an external knowledge resource of evidence for the first step, and on a score propagation mechanism for the second step. This section describes the proposed filtering method in detail. We first explain how we make use of Wikipedia as an external knowledge resource, and how only portions of a company X  X  Wikipedia page are used. Next we explain the two steps of our algorithm. 3.1 Wikipedia as an External Knowledge Resource Recently researchers have begun to make use of Wikipedia as an external knowledge resource for understanding social media content [ 5 ], the work by Meij et al. [ 6 ] is particularly significant with respect to linking tweets with con-cepts 1 . As the authors point out, a simple matching between terms in tweets and in Wikipedia text would produce a significant amount of irrelevant and noisy concept terms, and that this noise can be removed on either the Wikipedia or on the textual side. In line with this argument and on the basis of the intuition that the Wikipedia page of a company contains significant information about the company in certain portions of the Wikipedia article (i.e., concept terms exist in some portions of text), we perform the cleaning on the Wikipedia side as follows:  X  The text inside the category information within a Wikipedia article contains significant information about a company. We make use of this text by splitting it into single terms, and by using these single terms as concept terms of a company to be matched with the tweets X  dataset.  X  The information inside Wikipedia infoboxes is highly significant with respect to any Wikipedia entity [ 4 , 8 ], and hence we make use of this information. For example, let X  X  say on the Wikipedia page of the company  X  X pple Inc. X , we have the following information in the infobox: 1. Founder(s): Steve Jobs, Steve Wozniak, Ronald Wayne 2. Industry: Computer hardware, Computer software, Consumer electronics, We extract Steve Jobs, Steve Wozniak, Ronald Wayne, Computer hardware, Computer software, Consumer electronics and Digital distribution from the
Wikipedia infoboxes. These are then split into single terms and used as concept terms of a company to be matched with the tweets X  dataset.  X  We parse the paragraphs in Wikipedia and apply POS tagging to these para-graphs. After the application of POS tagging, we extract significant unigrams, bigrams and trigrams for proper nouns (NNP and NNPS). It is important to note that a bigram or trigram will only be extracted if all of its terms are tagged as a proper noun e.g., on the Wikipedia page of the company  X  X pple
Inc. X , the bigram  X  X im Cook X  is extracted while the bigram  X  X nveiled iPad X  is not extracted (this is because POS tag of unveiled does not correspond to that of a proper noun but to that of a verb). After extraction, the significant unigrams, bigrams and trigrams are split into single terms to serve as concept terms for linking with the tweets. 3.2 First Pass We collect all concept terms extracted from Wikipedia (through the method outlined in Sect. 3.1 ) adding them to the corpus of wiki terms to check . Further-more, corresponding to every term in wiki terms to check we store the inverse-document frequency of that term (note that each phrase that we extract from Wikipedia is treated as a separate document when computing the idf scores; as an example the phrases  X  X omputer hardware X  and  X  X omputer software X  from Sect. 3.1 would be treated as two separate documents with the term  X  X omputer X  having a document frequency of two). The computed idf score is referred to as weight of a term. We check for the occurrence of these concept terms in the tweets and number of occurrences per term is multiplied by the weight of that particular concept term to constitute a score for the tweet. Tweets that have score above a certain threshold are considered to be relevant. 3.3 Second Pass The second pass makes use of the idea of concept term score propagation in order to discover more tweets relevant to a particular company i.e., to increase the recall. The score propagation technique rests upon the intuition that terms co-located with significant concept terms may have some relevance to that concept. The scores for concept terms in a relevant tweet obtained from the first pass are redistributed among co-located terms as follows.
 Here, W r denotes the set of concept terms found in a tweet, of terms in a tweet that do not fall under the definition of concept terms (based on the discovery process from Wikipedia that we described previously) and finally, S denotes the score assigned to a term w in W n . We further illustrate the score propagation mechanism with the help of the example in Table 1 . Consider the two tweets in Table 1 with first tweet having t 1 , t 3 and t tweet having an additional t 11 as concept term ( t 1 and in the second tweet since they were already discovered as concept terms for the first tweet). The tweet score for the first and second tweet (computed as described in Sect. 3.2 ) is first decomposed between each tweet X  X  discovered concept terms to constitute a Redistribution Score as shown in Eq. 1 . This Redistribution Score is then distributed among the terms in W n asshowninEq. 2 . After the score redistribution, the second pass computes a new score for each tweet which now takes into account the scores of non-concept terms as well.
 Moreover, the second phase also makes use of additional sources of evidence, these are:  X  POS tag of the company name occurring within the tweets: We apply POS tagging to each tweet after which the POS tag corresponding to the company name occurring within a tweet is checked and if it corresponds to a proper noun (i.e., POS tag being NNP) we perform an increment in the tweet score.  X  URL occurring within the tweets: We get the extended URL contain a URL and if the company name occurs in extended URL within a tweet we perform an increment in the tweet score 3 .  X  Twitter username occurring within the tweets: Using the Twitter API we extract the description field of the tweet author 4 and if the company name occurs in that description field we perform an increment in the tweet score.  X  Hashtag occurring within the tweets: We extract all hashtags in each tweet and if the hashtag occurring within a tweet contains company name we perform an increment in the tweet score.
 Note that the score increment values for each of the mentioned additional sources of evidence are set empirically. Each of the above-mentioned sources of evidence then contribute towards computing the final score of a tweet. At the end of the second pass, the score propagation technique along with the extra sources of evidence enable extraction of more tweets relevant to the company, thus increasing the recall. We show more specific results for precision and recall in the next section. As mentioned in Sect. 2 the task comprises binary classification and hence we report the effectiveness of our algorithm through the standard evaluation metrics of precision and recall. We were given a very small trial dataset (six companies) and a considerably larger test dataset (31 companies). For each company a few hundred tweets were provided with the language of the tweets being English and Spanish; furthermore, most of the tweets are in Spanish. It is important to note that we translate a tweet that is not written in English into the English Language by using the Bing Translation API 5 .
 the gold standard provided by the lab organizers which comprised of individual tweet messages tagged for each company. Table 2 shows the precision and recall figures after the application of the first pass, and after the application of the second pass of our algorithm.
 recall. The application of the second pass increases the recall by a large degree, while not overly reducing the precision. The significantly large increase in recall proves the effectiveness of the score-propagation technique combined with the use of multiple sources of evidence.
 ity for evaluation purposes, these are described in detail in [ 3 ]. Table 4 presents a snapshot of the official results for the Filtering task of RepLab 2012, where CIRGIRDISCO is the name of our team. Table 4 shows that our algorithm per-formed competitively. It is the second best among the submitted systems and fourth best among the submitted runs. The baseline system marked all tweets as relevant; note that the measure of Accuracy Filtering does not reveal true performance which is why lab organizers used Reliability and Sensitivity [ 3 ]for the purpose of comparing team performances. It is also important to note that all the runs submitted by  X  X aedalus X  relied on manually crafted rules whereas our system is completely unsupervised and automatic (i.e., does not require any training data or manual efforts).
 Another interesting observation we made with respect to per entity (com-pany) evaluation results is that our algorithm suffers due to translation issues This is particularly obvious for entities with high percentage of English tweets. Table 3 shows results for three entities with high percentage of English tweets: the Reliability and Sensitivity scores exhibited by our algorithm are consider-ably high. In fact for the entities Bing and Volkswagen our algorithm showed the best results (out of all submitted runs by all teams). This demonstrates the high performance of our algorithm specially for English tweets. We proposed a two-pass algorithm for company name disambiguation in tweets. Our algorithm makes use of Wikipedia as a primary knowledge resource in the first pass of the algorithm, and the tweets are matched across Wikipedia terms. The matched terms are then used for score propagation in the second pass of the algorithm that also makes use of multiple sources of evidence. Our algo-rithm showed competitive performance and demonstrates the effectiveness of the techniques employed in the two passes. We believe that the two-step fil-tering approach may open a promising new dimension to address the problem of entity name disambiguation in tweets as can be evidenced by the evaluation results.

