 REGULAR PAPER Hiroshi Mamitsuka Abstract We propose a new data-mining method that is effective for learning from extremely high-dimensional data sets. Our proposed method selects a subset of features from a high-dimensional data set by a process of iterative refinement. of instances, to which predictions by hypotheses previously obtained are most unreliable, from the data set. The second step selects a subset of features whose values in the selected instances vary the most from those in all instances of the database. We empirically evaluate the effectiveness of the proposed method by comparing its performance with those of four other methods, including one of the latest feature-subset selection methods. The evaluation was performed on a real-world data set with approximately 140,000 features. Our results show that the performance of the proposed method exceeds those of the other methods in terms of prediction accuracy, precision at a certain recall value, and computation time to reach a certain prediction accuracy. We have also examined the effect of noise in the data and found that the advantage of the proposed method becomes more pronounced for larger noise levels. Extended abstracts of parts of the work presented in this paper have appeared in Mamitsuka [14] and Mamitsuka [15]. Keywords Query learning  X  Feature-subset selection  X  High-dimensional data set  X  Uncertainty sampling  X  Drug design 1 Introduction As the fields to which mining/learning systems are applied expand, the types of data sets dealt with have also been increasing. In particular, the growth of the data-set size in a variety of real-world applications has been extremely pro-nounced. In this paper, we focus on mining/learning from high-dimensional data sets, that is, data sets with a large number (say 1,000,000) of features (attributes), to which a single standard induction algorithm cannot be applied using normal hardware. Our goal is to mine/learn prediction rules efficiently from such a very high-dimensional data set. This type of data set has appeared, for example, in the process of drug design (or drug discovery) [ 5 , 17 ]. In such a drug data set, each record corresponds to a chemical compound and this data set has an enormous number of features characterizing a compound and its label, such as drugability or toxicity.
 dimensional data set is to reduce the number of features of the data set by selecting a feature-subset from all of the given features. There are two main techniques for feature-subset selection: filter and wrapper methods [ 8 , 10 , 13 ]. In both methods, there are two different approaches in terms of incremental or decremental selec-tion of features. More concretely, the incremental approach starts with a set of no features and adds features one by one to this set, and the decremental approach starts with the set of all features and reduces the number of features in the set. The performance of an incremental approach must depend on its initially selected set of features, and thus, a set of features selected by a decremental approach can be generally said to be more reliable than that obtained by an incremental ap-proach. In fact, recently proposed feature-subset selection methods [ 2 , 9 , 18 , 25 ] belong to the decremental approach. When we combine the decremental approach with the wrapper method, an inductive algorithm has to be fed with the whole of a given high-dimensional data set during the initial process, and so the filter method is more practical than the wrapper method. Therefore, the decremental filter approach can be considered the most practical for selecting features from a high-dimensional data set to be considered here.
 data set. Our method is different from the existing methods of feature-subset selec-tion, and it iteratively selects a feature-subset, instead of selecting a feature-subset once only. Our method is closely related to a general sampling method called  X  X equential multi-subset learning with a model-guided instance selection X , which appears in the comprehensive survey by Provost and Kolluri [ 19 ]onthemeth-ods for scaling up inductive algorithms. This approach repeats the following two steps: selecting a small subset of a large database using previously obtained pre-dictive hypotheses, and training a component induction algorithm with this subset to obtain a new hypothesis. We note that a number of methods that belong to this category have been proposed to improve predictive performance, including windowing [ 20 ], boosting [ 3 ], Ivotes [ 1 ]andQbagS[ 16 ]. Our new method also repeats the same two steps, but selects a subset of features from a given data set, although the general sampling approach selects a subset of instances.
 mance of our method, using a real high-dimensional data set. In our experiments, we used a data set with approximately 140,000 binary features, and compared the performance of our new method with those of four other methods. We used two different component inductive learning algorithms, C4.5 [ 21 ] and the support vector machine (SVM) [ 7 ] to test the performance of each of the five methods tested.
 the other four methods, when using either C4.5 and SVM as the component learn-ing algorithm. We further varied a noise level of this data set, and evaluated the performance of the five methods when the noise level was 10% and 20%. We found that, for higher noise levels, the significance level by which our method outperformed each of the other four methods increased. In particular, for the noise level of 20%, our method outperformed all of the other four methods, being sta-tistically significant, when SVM was used as a component learning algorithm. We then checked how much computation time it took for each of our and other two methods to reach the lowest final prediction accuracy of these three methods, and found that our method achieved a time-efficiency factor of approximately two to ten for all cases. All of these experiments have shown that our proposed method is a highly predictive noise-robust and time-efficient method for an extremely high-dimensional data set. 2 The query-learning-based iterative feature-subset selection (Qifs) method 2.1 Maximizing information gain Our method alternately repeats selection of a subset of a given input database and training of a component learning algorithm using this subset. Selection of a feature-subset has the following two steps: 1. We first select a subset of instances, based on the idea of a method for query learning called  X  X uery by committee X  [ 4 , 23 ]. The query-by-committee algo-rithm predicts a label for each instance of the data set with previously ob-tained hypotheses and selects the instances to which the predictions are dis-tributed (or split) most evenly. This algorithm, more generally called uncer-tainty sampling [ 11 , 12 ], selects queries (instances) with maximum uncertainty for predicting labels. In other words, this sampling criterion maximizes the in-formation to be gained in the process of instance selection. This instance selec-tion does not use the label information in a given database, nor does it consider any weight for each hypothesis when combining obtained hypotheses. Because of these characteristics of uncertainty sampling, our sampling approach is not easily affected by noise. On the other hand, adaptive sampling, such as that of AdaBoost [ 3 ], uses the label information and combines hypotheses with weights, each of which is computed considering the predictive performance of each hypothesis. Such adaptive sampling has often been said to be sensitive to noise [ 22 ]. Our uncertainty sampling can be more noise-robust than adap-tive sampling, or more generally the approach of combining hypotheses with weights, computed by predictive errors [ 16 ]. 2. The instances selected by the query-by-committee algorithm have the maxi-mum information, and so our focus then turns to the features (attributes) that provide the maximum information, in the selected instances. In order to find such informative features, a natural idea is to check the difference of feature values between the selected informative instances and all given instances, be-cause the selected instances are the most informative among all instances. We then make a natural assumption that the more different the values of a feature for the selected instances are from those for all instances, the more information this feature has. We finally focus on the features whose values in the selected instances differ the most from those in the input database. More concretely, we measure the distance of feature values between the selected instances and all in-stances, for each of the features, and select features having the largest distances as a feature subset.
 on the idea of maximizing the information gain in query learning. We thus call our method query-learning-based iterative feature-subset selection (Qifs). 2.2 Algorithm Let S and x be an input database and an instance, respectively. Note that an in-stance is a vector of real values, and S is a set of instances, i.e. x  X  S .Let z and y be a feature and a label, respectively. Note that a feature is a vector of real values and a label is a vector of discrete values, and S can again be a set of more than one feature and a label. Let S + be the data set to be selected at each iteration, and let x + , y + and z + be an instance, the label and a feature of S + , respectively. Thus S + is a set of x + and can be a set of more than one y + and z + .Let n number (count) of all instances, in which the value of the feature z is i ,in S .Sim-ilarly, let n + z ( i ) be the number of all instances, in which the value of the feature z is i ,in S + .Let h i + 1 be the hypothesis obtained at the i -th iteration of Qifs ( h 1 is obtained in the initialization.).
 using this pseudocode. The inputs of Qifs are the following: a database S ,acom-ponent learning (prediction) algorithm A ( A ) and a component feature-subset selection method B . We can use an arbitrary feature-subset selection method as B , and in our experiments, we adopt the method used by Xing et al., which will be explained in Sect. 3.1.2 . The parameters for Qifs are the following: the number of iterations ( T ), the number of instances to be selected at each iteration ( M )and the number of features to be selected at each iteration ( Q ).
 Qifs computes the counts n z ( i ) for all z and i in S (line 1 of the Initialization). Second, it obtains a feature-subset from S by using the input feature-subset se-lection method B and applies the input component learning algorithm A to the feature-subset to obtain the first hypothesis h 1 (lines 2 and 3 of the Initialization). This hypothesis is used to predict the label of each of all instances in an input database S , and the predicted label is stored (line 4 of the Initialization). three steps: selecting instances, selecting features and running a component learn-ing algorithm. In the first step, Qifs first computes the  X  X argin X  of each of the instances from an input data set (line 1 of Step 1). At the i -th iteration, the margin m for an instance x is defined as follows: ence between the number of votes by the past hypotheses for the most  X  X opular X  label, and that for the second-most-popular label. We note that, to compute this margin in Eq. ( 1 ), we need the predicted labels for each of the instances by using previously obtained hypotheses, and so we store the label predicted for each in-stance at each iteration. Qifs then sorts all instances of the given input data set S into ascending order of computed margin (line 2 of Step 1), and selects the top M instances out of all the sorted instances as selected data set S + .
 selected data set S + . It then computes the  X  X ifference X  (distance) d of the feature values for the feature z between the input data set and the selected data set, using n ( i ) and n + a measure of the difference, and we chose the following square distance in our experiments because it is one of the most popular distance measures: We note that this distance, which is the squared sum of block distances, can be used for any discrete feature z . If feature z takes continuous values, we need to modify it to take discrete values, using a discretization method such as uncondi-tional mixture modeling [ 25 ]. We further note that, when we deal with a data set of only binary feature values (such as i = 0 or 1), d z is dependent on only one value (say i = 0) as follows: This indicates that the simple block distance gives the same result as that obtained by the square distance for this binary case, because this measure is used for com-paring one feature with another. More generally, any power of the sum of the block distances can be replaced with the square sum, keeping the result the same, for the binary case. This case is true of the data set used in our experiments. The Kullback X  X eibler (KL) divergence is also a possible candidate. However, we did not use this metric in our experiments because it is a nonsymmetric measure, and when computing the divergence. Qifs then sorts all features into descending order of the difference d z (line 3 of Step 2), and selects the relatively small number of top Q features as a new selected data set, S + (line 4 of Step 2). In the third step, Qifs first applies the component learning algorithm to the obtained S + and obtains a new hypothesis h i + 1 (line 1 of Step 3). It then runs the component prediction algorithm to predict the label of each instance in S , and this label is stored (line 2 of Step 3).
 over all hypotheses obtained in the above iterative process as follows: 2.3 Parameter setting The optimal value of each of three input parameters, T , M and Q , depends on the data set. We explain here how we can set these parameter values practically. Usually the value of T is set to be very large, and the iteration is repeated until a certain condition is satisfied, such as that the predictive performance for a test data set is saturated. The value of M should be relatively large, since we need to compute counts n + z ( i ) from M instances. However, if it is too large, say 80% of the number of all instances, the counts n + z ( i ) will be almost the same as n z ( i ) . Thus, a half of all instances is a natural choice for M .Thevalueof Q must be significantly smaller than the number of all features in S , because in our settings the number of all features in S is extremely large. Also, the value of Q depends on the number of instances in S . More concretely, the number of features should be smaller than the number of instances in S . This is because features are used to classify instances in an inductive learning algorithm, and it is natural that we do not need a larger number of features than that of instances. Thus, the value of Q can be considerably smaller than the number of instances in S . In fact, for example, it has already been reported that around 200 features are enough for a data set which has more than 100,000 features and around 2,000 instances. 1 2.4 Time complexity Let N and K be the number of instances and features, respectively, in S .Let L be the number of types of feature values in S . For simplicity, let L also be the number of types of label in S .Let K + be the number of features in S + . We denote by a ( N , K )( a ( N , K )) the time complexity of a component learning (prediction) algorithm, taking a data set of N instances and K features as input. Similarly, we denote by b ( N , K ) the time complexity of a component feature-subset selection algorithm, taking a data set of N instances and K features as input.
 is generally larger than L . Furthermore, in the situation we consider here, K is much larger than N . For example, K = 1 , 000 , 000, N = 10 , 000 and L = 10, and in fact, K = 139 , 351, N = 1560 and L = 2 in our experiments. Thus, O ( T  X  N  X  K ) is clearly the maximum among the complexities listed in the table, except cases in which a or b is involved. This indicates that the computation time of Qifs is linearly related to the time for simply scanning all values in a given data set, otherwise it depends on the time for the component learning algorithm and/or the component feature-subset selection algorithm. From this analysis, we can say nature.
 nent feature-subset selection method, because an existing feature-subset selection algorithm generally needs much more computation time than that of scanning all values in a given data set. 3 Empirical evaluation 3.1 Methods compared with Qifs 3.1.1 Rand: Ho X  X  random subspace method We tested Ho X  X  random subspace method [ 6 ]( Rand for short) as a competing method. As in Qifs, the method repeats the two steps of selecting a feature-subset and running a component learning algorithm with the subset, but it chooses fea-tures in the subset randomly. Figure 2 shows the pseudocode of Rand.
 features from a given database (line 1), feeds the selected features to the compo-nent learning algorithm to obtain a new hypothesis (line 2), and predicts the label of each instance using the hypothesis obtained (line 3). The final hypothesis is, as in Eq. ( 3 ) for Qifs, defined by majority vote over all the hypotheses obtained in the above iterative process. Rand does not use the previously obtained hypotheses in selecting a feature-subset and simply repeats random sampling and hypothesis building. 3.1.2 FSS: Xing et al. X  X  feature-subset selection method We tested Xing et al. X  X  feature-subset selection method [ 24 , 25 ], which hereafter we call FSS . We note that FSS is one of the most recent feature-subset selection methods, which belong to the decremental filter approach. FSS has two steps, i.e. information gain ranking (IGR) and Markov blanket filtering (MBF). More con-cretely, FSS first calculates the information gain for all given features and selects those that have a high information gain. Then, the selected features are reduced one by one using the idea of Markov blanket filtering proposed by Koller and Sahami [ 9 ]. We can interpret the two steps roughly as follows: IGR selects a set of features, each of which is strongly relevant to the label of a given database. Then, out of the obtained set, MBF removes one by one the features for which a similar feature is contained in the set. Our implementation follows Xing et al. [ 25 ] exactly. Note that in IGR the information gain of a feature is calculated using only two columns (the feature and the label) of the data set. However, in MBF, all features (i.e. all columns) must be checked to obtain a similarity between a feature and one of the other features, and MBF is highly time-consuming. More concretely, regarding the number of features, K , in an input database, IGR takes O ( K ) in computation time, while MBF needs O ( K 2 ) because of the need to check all possible feature pairs. 3.1.3 Efs: ensemble of feature-subset selection methods We further tested an ensemble of feature-subset selection methods ( Efs for short) to improve the predictive performance of FSS. Figure 3 shows the pseudocode of Efs.
 chooses a relatively large (say a half) feature-subset from a given data set (line 1), (2) it then selects a small feature-subset from this large feature-subset using FSS (line 2), (3) it then obtains a new hypothesis by applying a component learn-ing algorithm to this small feature-subset (line 3), and then (4) it predicts the label of each instance by using the new hypothesis obtained (line 4). The final hypoth-esis is defined by majority vote over all the hypotheses obtained in the above pro-cess, as done in Eq. ( 3 ) of Qifs, but Efs and Rand do not use previously obtained hypotheses in selecting a feature-subset. 3.1.4 Eifs: ensemble of information-gain-ranking-based feature-subset As mentioned in Sect. 3.1.2 , MBF is a time-consuming step in FSS, and EFS, an ensemble of FSSs, is expected to be more time-consuming. We then tested iteratively the use of IGR, a relatively time-efficient part in FSS, and we call this method Eifs , standing for an ensemble of information-gain-ranking-based feature-subset selection methods. Eifs follows the method of Efs, except that it uses IGR instead of FSS in Efs. The pseudocode for Eifs is shown in Fig. 4 .
 3.2 Data We need to evaluate our method using a data set which has a large number (say 100,000 or more) of features and to which we cannot apply a single standard in-ductive algorithm using normal hardware. It is, however, difficult to find such a publicly available large-scale data set; the only one we found is a data set used in KDD Cup 2001, consisting of 139,351 features. The KDD Cup data set is a real-world data set used for discovering drugs, i.e. small organic molecules which bind to a protein. The data set is a table in which each record, corresponding to a chemical molecule, has a number of binary (0 or 1) features characterizing it and a binary class value of binding to a protein called thrombin. A class value of  X  X  X  for active (binding) and  X  X  X  for inactive (non-binding) is given to each chemical com-pound. The sizes of the originally given training and test data sets are 1,909 and 636, respectively. Out of them we obtain a total of 1,950 records (compounds) by mixing them, while removing the records in which all feature values are zero. Of these 1,950 records, 190 are active compounds. That is, the percentage of the com-pounds binding to thrombin is 9.74%. We call this data set the  X  X hrombin data set X . That is, we split the data set into five blocks of roughly equal size, and in each trial four out of these five blocks were used as training data, and the last block was reserved for test data. The results (learning curves, final accuracies, precisions and recalls) were then averaged over the five runs.
 are shown in Table 2 . 3.3 Parameter settings In our experiments, we used the C4.5 [ 21 ] and support vector machine (SVM) 2 methods for the component learning algorithm. We used them with no particular options and equal conditions for all five methods, i.e. Qifs, Rand, FSS, Efs and Eifs. Table 3 shows the parameter settings for Qifs, Rand, Efs and Eifs that were used in our experiments. The winner of KDD Cup 2001 used around only 200 features out of the 139,351 features. Thus, we set Q at 500, although a smaller number may be enough. We then set N to be a half of all the training instances, as mentioned in Sect. 2.3 . For Qifs, T was set to be large, and the algorithm was stopped when the predictive performance was almost saturated. We fixed T = 10 for Efs and T = 20 for Eifs, 3 since these two methods are extremely time-consuming. In all our experiments, we ran FSS (and FSS in Efs) as follows. We first reduced the number of features to 1,000 by choosing the top features ranked by information gain, and then we reduced the number one by one to 500 by Markov blanket filtering. The prediction accuracy for FSS was measured by a subset with these 500 features. 3.4 Learning curves and final prediction accuracies We show the results of the cross-validation on the Thrombin data set in the form of learning curves in Figs. 5 and 6 . 4 component learning algorithm, respectively. Note that, in these curves, the average prediction accuracy (on separate test data sets) is plotted against the total computation time, including disk access time. In these figures, we add prediction accuracies for FSS for reference. We show the results of FSS as first step (information gain ranking) of FSS and thus learning curves were not obtained. From these figures, we can see that the prediction accuracies obtained by Qifs were better than those obtained using the other four methods.
 final accuracy, we mean an accuracy level large enough that the predictive perfor-mance appears to be saturating. 5 Ta b l e s 4 and 5 (noise level: 0%) show the results for the final prediction accuracy obtained by the five methods and the t values of the mean difference significance (pairwise) test for the respective cases. The t values are calculated using the following formula: where D denotes the difference between the accuracies of two methods for each data set in our cross-validation, ave ( X ) is the average of X ,var ( X ) is the variance of X ,and n is the number of data sets (five in our case). For the case n = 5, if t is greater than 4.604 then it is more than 99% statistically significant that Qifs achieved higher accuracy than the other method.
 eight cases for the 0% noise level. As shown in Table 5 , for the Thrombin data set, the t values range from 1.15 to 4.78. We can statistically see that the performance of Qifs was slightly (insignificantly) better than those of the other four methods. 3.4.1 Adding noise to the Thrombin data set To better understand under what conditions our method works well, we added a kind of noise to Thrombin data set, varying the noise level. More concretely, we randomly reversed binary feature values of the data set, while keeping the percentage of the number of these reversed features at a certain level, specifi-cally 10% or 20%. Figure 5 b and c show the learning curves of the 10% and 20% noise levels, respectively, using C4.5 as a component algorithm. Figure 6 b and c also show the learning curves of these two noise levels, using SVM as a component algorithm. As shown in these figures, in terms of the final predic-tion accuracy results, Qifs performed better than all of the other four methods again.
 mean difference significance test for the noise levels of 10% and 20%. When the noise level is 10% and 20%, Qifs outperformed the other four methods in all 16 cases except for only two cases, being statistically significant in ten out of the 16 cases. We can see, in these results, that for a higher noise level, the significance of the difference in the predictive performance between Qifs and the other four methods becomes more pronounced.
 how the t values of the mean difference significance test vary as the noise level is changed from 0 to 20%. We also found that noise effects obtained by using SVM as the component learning algorithm are more clearly shown than those obtained by the C4.5 case.
 Qifs for all noise levels, using C4.5 as a component learning algorithm. However, as shown in Figs. 5 and 6 , at a very early stage in learning, the prediction accuracy of Qifs reaches the level of the final accuracies of Efs and Eifs. Table 6 gives concrete figures that quantify this observation. Here, the target accuracy is fixed to the lowest of the three final accuracies obtained by Qifs, Efs and Eifs, which are all shown in Table 4 . Then, we checked to see how much computation time it took for each of these three methods to reach that accuracy. In parentheses, we also show the ratio of the computation time required by each method to the target accuracy. We can easily see that the difference of time efficiency reaches a factor of approximately two to ten, except in only one case. From these results, we can say that Qifs is a noise-robust and time-efficient method for learning from very high-dimensional data sets. 3.5 Precision X  X ecall curves In order to investigate the empirical findings obtained further, we also used the measures of precision and recall, standard performance measures in the field of information retrieval. Note that recall is defined as the probability of correct pre-diction given that the actual label is  X  X  X , and precision is defined as the probability of correct prediction given that the predicted label is  X  X  X .
 and SVM as the component learning algorithm, respectively.
 tained after approximately (a) 6,000 (b) 2,000 and (c) 5,000 s of computation time. The curves in Fig. 9 are those attained after approximately (a) 7,000 (b) 18,000 and (c) 20,000 s of computation time using SVM as a component algorithm. For Efs and Eifs, the precision X  X ecall curves shown in these figures are those obtained after finishing their iterations, i.e., approximately 25,000 and 40,000 s of compu-tation time for Eifs and Efs, respectively. In FSS, prediction is done by a single inductive learning algorithm, and only a single pair of recall and precision values is obtained.
 value of Qifs at a certain recall value and that of each of the other four methods at the same recall value is larger. In particular, as shown in Figs. 8 cand 9 c, when the noise level reaches 20%, the precision of Qifs is approximately 20 X 40% better than that of FSS at the equal recall value given by FSS. One more item of note is that the performance of Rand is better than that of FSS when using C4.5 as the component learning algorithm. This shows that there is a case in which multiple hypotheses built by sets of randomly selected features achieve a better predictive performance than a single hypothesis built by a set of features carefully selected from all given features.
 ods when using either C4.5 or SVM as the component learning algorithm. The table shows, for each noise level and component algorithm, the precision values of the three methods at a recall value given by FSS. This table confirms the per-formance advantage of Qifs over the other four methods, and that this advantage is more pronounced for higher noise levels.
 how the ratio of precision values of each of the four other methods to that of Qifs varies as the noise level changes from 0 to 20%. The graphs visibly show that, in terms of precision, the difference between the performance of Qifs and those of the other four methods became larger for higher noise levels. 4 Concluding remarks We have proposed a new method for learning from very high-dimensional, noisy data sets. Using a data set having 140,000 features, we have empirically shown that the performance of our proposed method is clearly better than those of other four methods, including one of the latest feature-subset selection methods and a variety of ensemble methods. The advantage of our method becomes more pronounced for more noisy data sets. The key property of our method that contributes to this advantage is its iterative feature-subset sampling strategy, based on the idea of query learning.
 having an extremely large number of both features and instances. For this type of database, a possible way to modify our proposed method is to use only a subset containing instances selected by the idea of query by committee at each iteration. That is, a possible new method iteratively selects a subset of both instances and features from such a large database. It would also be interesting to investigate un-der what conditions (noise level and number of features and instances) this method works better than other methods, if such a large database is available. References
