 Many modern applications are faced with the task of knowl-edge discovery in entity-relationship graphs, such as domain-specific knowledge bases or social networks. Mining an  X  X n-formative X  subgraph that can explain the relations between k (  X  2) given entities of interest is a frequent knowledge dis-covery scenario on such graphs. We present MING, a prin-cipled method for extracting an informative subgraph for given query nodes. MING builds on a new notion of infor-mativeness of nodes. This is used in a random-walk-with-restarts process to compute the informativeness of entire subgraphs.
 H.0 [ Information Systems ]: General X  knowledge discov-ery Algorithms, Design
Many modern applications exploit information organized in entity-relationship (ER) graphs, such as domain-specific knowledge bases (e.g. metabolic or regulatory networks in biology, criminalistic networks for crime investigation, etc.) or social networks (such as data sharing or business-customer networks). One can represent them by relational or ER models, XML with XLinks, or RDF triples. An ex-ample of an ER graph is YAGO [17], which has been con-structed by systematically harvesting semi-structured assets of Wikipedia (e.g., infoboxes, categories, lists, etc.). The YAGO graph consists of millions of nodes (representing en-tities, e.g. persons, movies, locations, dates, etc.) and tens of millions of labeled edges, representing facts about entity pairs, such as Max Planck fatherOf Erwin Planck , Max Planck isA Physicist , Max Planck bornIn Germany , etc. YAGO supports more than 100 relationship labels such as isA, bornIn, citzenOf, marriedTo, etc.
 Other examples for ER graphs are GeneOntology or UMLS (in the biomedical domain), the graphs represented by IMDB (in the domain of movies and actors), DBLP (in the domain of Computer Science publications), and LOD [3] (for publishing interlinked Web data sets as RDF graphs), etc.

A knowledge discovery task on such graphs is to determine an  X  X nformative X  subgraph that can explain the relations be-tween k (  X  2) entities of interest. Examples are queries that aim at finding the relations between k given biomedical en-tities, the connections between k criminals, the most rele-vant data shared by k Web 2.0 users, etc. Formally, the general problem that motivates this work is: given a set from an entity relationship graph G and an integer b &gt; k (representing a node budget), find a connected subgraph S of G with at most b nodes that contains all query nodes and maximizes an  X  X nformativeness X  function g info ( S, Q ).
As a concrete example, consider the query that asks for the relation between Max Planck, Albert Einstein, and Niels Bohr . An informative subgraph that captures their re-latedness should reveal that all three of them are physicists, scientists, Nobel Prize winners, etc., and should discourage long or obscure connections (e.g. connections through per-sons with same nationalities or same birth or death places as some of the query entities). Figure 1 depicts a possible answer.
 Figure 1: Answer returned by MING on YAGO
The above problem comes with two subproblems: (1) what is a good measure for representing the informative-ness of relations between entities in ER graphs? (2) how to determine the most informative subgraph for the given query nodes? We address both problems in this work.
In previous approaches [16, 5, 7, 18, 8, 15], the notion of subgraph importance is mainly based on structural proper-ties of the underlying graph (e.g. indegree or outdegree of a node, density or edge connectivity of a subgraph, etc.). More related to our approach are techniques based on in-fluence propagation like [7] or [18]. The approach of [7] exploits a current-flow algorithm to determine an impor-tant subgraph for two query nodes. The approach of [18], CEPS, can handle more than two query nodes, and gives a random-walk-based solution for retrieving the most  X  X en-tral X  X odes with respect to the query nodes, so called center-pieces : nodes that are closely connected to most of the query nodes. The centerpieces are exploited to mine an important subgraph for the query nodes. All mentioned approaches leave aside the problem of deriving measures for capturing the semantic importance of nodes and edges in ER graphs. Other, Steiner-tree-based, approaches [1, 11, 2, 12, 10, 6, 9, 13] have addressed the problem of retrieving the top-k minimum-cost subtrees that closely interconnect the given query nodes. Their result paradigm is tree-based. Hence, these approaches are not directly applicable to our problem of retrieving informative subgraphs.

Our approach gives a principled solution, while making the semantic aspect of entities and relationships in ER graphs a key ingredient for the measure of informativeness. Our main contributions are the following: 1. We give a clean notion of informativeness for nodes 2. We present MING, a robust algorithm for mining 3. Based on user assessments, we demonstrate the quality
In the following definition we introduce ER graphs as multi-graphs.

Definition 1 (ER graph). Let Ent and Rel be finite sets of entity and relationship labels respectively. An ER graph over Ent and Rel is a tuple G = ( V, l Ent , E Rel ) , where V is a set of nodes, l Ent : V  X  Ent is an injective function, E Rel  X  l Ent ( V )  X  Rel  X  l Ent ( V ) is a set of labeled edges.
Since the direction of a relationship between two entities can always be interpreted in the converse direction, we view the edges of an ER graph as bidirectional. That is, we as-sume that for each edge ( u, r, v )  X  E Rel there is an edge ( v, r  X  , u )  X  E Rel , where r  X  represents the inverse relation label of r .
 For any subgraph S of an ER graph G , we denote by Ent ( S ) the set of its labeled nodes (i.e., entities), and by F ( S ) the set of its labeled edges (i.e., facts). Note that F ( S ) contains edges of the form (  X ,  X ,  X  ), and that both  X ,  X   X  Ent ( S ).
 Discussion We believe that in order to compute the infor-mativeness of nodes in ER graphs, the link structure has to be taken into account. But, as a matter of fact, edge direc-tions in ER graphs do not always reflect a  X  X lear X  endorse-ment. For example, the fact Albert Einstein isA Physicist can be represented as Physicist hasInstance Albert Einstein . Consequently, measures that build on the link-based en-dorsement hypotheses such as PageRank [4] or HITS [14] are not applicable in a straight-forward way. In general, we argue that measures that rely on the graph structure alone are not sufficient, since ER graphs represent only a limited fraction of the real world.

Our informativeness measure for nodes overcomes these problems by building on edge weights that are based on co-occurrence statistics for entities and relationships. These statistics are derived from the domain represented by the ER graph. They will guide a random walk process on the adjacency matrix of the ER graph.
For each fact represented by an edge, we compute two weights; one for each direction of the edge. Each of these weights will represent a special kind of endorsement, ob-tained from co-occurrence statistics for entities and relation-ships.
 Definition 2 (Fact Pattern, Match, Binding). Let X be a set of entity variables (placeholders for entities). A fact pattern from an ER graph G = ( V, l Ent , E Rel ) is a triple (  X ,  X ,  X  )  X  ( Ent  X  X )  X  Rel  X  ( Ent  X  X ) , in which either  X   X  X or  X   X  X , such that if  X   X  X then there exists (  X  0 ,  X ,  X  )  X  E Rel , and if  X   X  X there exists (  X ,  X ,  X  0
Without loss of generality, let  X   X  X . The edge (  X  0 ,  X ,  X  ) from G is called a match to the fact pattern (  X ,  X ,  X  ) , and the entity  X  0 is called a binding to the variable  X  .
Consider the fact pattern x isA Physicist , x  X  X . The facts Albert Einstein isA Physicist and Bob Unknown isA Physicist are matches to the above fact pattern. In our ex-ample, the fact Albert Einstein isA Physicist should have a higher informativeness than Bob Unknown isA Physicist , since Einstein is an important individual among the scien-tists. More precisely, the binding Albert Einstein should be more informative than Bob Unknown . To capture this no-tion of informativeness, we introduce a probabilistic model.
Let (  X ,  X ,  X  ) be a fact pattern, where  X   X  X . Let  X  0 be a binding of  X  . We estimate the informativeness of  X  0 given the relationship  X  and the entity  X  as: where W (  X  0 ,  X ,  X  ) denotes the number of domain witnesses for the fact  X  0  X   X  , i.e., the number of its occurrences in the underlying domain of the ER graph. Analogously, W (  X ,  X  ) stands for the number of witnesses for the pattern (  X  ,  X ,  X  ), where the wild card  X   X   X  can be any entity. The value P info (  X  0 |  X ,  X  ) is assigned as a weight to the edge  X 
In practice, these values can be estimated by means of inverted indexes on a background corpus, e.g. a large Web sample representing the domain of the ER graph. From the indexes one can compute (co-)occurrence statistics for (pairs of) entity names and estimate the needed parameters.
Our aim is an informativeness measure for nodes based on random walks on the  X  now weighted  X  ER graph. Our measure, coined IRank (Informativeness Rank), is related to PageRank [4].

Let G = ( V, l Ent , E Rel ) be an ER graph. Let u  X  l Ent be an entity and let P ( u ) be the probability of encounter-ing the entity u in the domain from which G was derived. This value can be estimated as P ( u )  X  W ( u ) P again W ( u ) denotes the number of occurrences of the en-tity u in the underlying domain. P ( u ) can be viewed as an importance prior for u .

In IRank, the random surfer may decide to restart his walk from an entity u  X  l Ent ( V ) with probability propor-tional to P ( u ). Alternatively, the surfer may reach u from any neighboring entity v that occurs in an edge of the form ( v, r, u )  X  E Rel (given that the surfer is at one of these neigh-boring entities of u ).
 Let N ( u ) denote the set of neighboring entities of u in G . The probability of reaching u via one of its neighbors would be proportional to: where IR ( v ) denotes the probability that the surfer is at v .
Finally, the accumulated informativeness at a node u  X  l
Ent ( V ) is given by:
IR ( u ) = (1  X  q ) P ( u )+ q X For practical reasons, the outgoing edge weights (i.e., the P info weights) for each entity u are normalized by the sum of all outgoing edge weights of u . With this normalization step, Equation (3) represents an aperiodic and irreducible finite-state (i.e., an ergodic) Markov Chain. This guarantees the convergence and the stability of IRank. Although IRank is related to PageRank, the P info values are crucial and make a big difference in the random walk process.
As a first step, MING extracts a subgraph C of G that contains many connections between the query nodes. This is a rather recall-oriented step; most of the spurious regions of G are removed. The subgraph C can be extracted with heuristics similar to the ones presented in [7]. In a second step, we run the STAR algorithm [13] to determine a subtree T of C that closely interconnects all entities from Q . As-suming that T already captures some relatedness between the query entities, each node v  X  Ent ( T ) is viewed as infor-mative; these nodes are assigned the label  X + X . Nodes on the  X  X im X  of C that do not represent query entities and have de-gree 1, i.e., nodes that do not contribute to any connection between query entities, are viewed as uninformative, and are labeled  X   X   X .
 For each unlabeled node v  X  Ent ( C ), we compute a score P  X  ( v ), representing how uninformative v is, and a score P ( v ), representing how informative v is, with respect to the query entities.

The informative subgraph mining problem can be stated as follows.
 Definition 3 (Informative Subgraph Mining).
 Given the connected subgraph C , the set Q of query nodes, and an integer node budget b &gt; = | Ent ( T ) | , solve the tasks: 1 . Determine for each v  X  Ent ( C ) a label lab ( v )  X  X  X  , + } as lab ( v ) = arg max l  X  X  X  , + } P l ( v ) . 2 . Extract a connected subgraph S of C that contains T and has the following properties: (1) every v  X  Ent ( S ) is labeled  X  +  X , (2) S contains at most b nodes, (3) S maximizes P
With the requirement that T be a subgraph of S , we guar-antee that all query nodes are interconnected in the result graph.
 Classification Method Our intuition is the following. Let l  X  X  X  , + } . Consider all paths in C that connect any two nodes labeled l and cross an unlabeled node v . The higher the number of such paths, the higher the probability that v is also labeled l . On the other hand, the longer these paths, the smaller the probability that v is labeled l . To estimate P l ( v ), we need methods that capture and reward robust structural connectivity and discourage long and loose connections.

Consider a random walker that starts at a node labeled l in C and finishes his walk again at a node labeled l . For an unlabeled node v  X  Ent ( C ), let P l ( v ) denote the probability that v is visited during this random walk. As depicted in Figure 2, we estimate this probability as the composition probability that the random walker starts at any l -labeled node and reaches v . P 2 l ( v ) represents the probability that any l -labeled node is reached when the random walker starts his walk at v . It is straightforward to see that P l ( v ) = P ( v )  X  P 2 l ( v ).

In order to estimate P 1 l ( v ), we extend IRank into a Ran-dom Walk with Restarts (RWR) process that restarts from the nodes labeled l . The walk starts at any l -labeled node v and follows the outgoing edges of v with a probability that is proportional to the edge weights (as edge weights on C we consider the P info values from Equation (1)). The prob-ability that our walk follows the outgoing edges of nodes is dampened by a factor q . With a probability (1  X  q ) the random walk restarts at any node labeled l .
 Figure 2: Probability P l composed of P 1 l and P 2 l .
In RWR processes long connectivity paths are discour-aged in a natural way (by the restart probability). Further-more, as reported in [19] and [18], RWRs have nice proper-ties when it comes to capturing the structural connectivity between nodes. They overcome several limitations of tra-ditional graph distance measures such as maximum flow, shortest paths, etc.

In order to compute P 2 l for an unlabeled node v , we could use again RWRs. More precisely, we could run an RWR for every unlabeled node v and compute P 2 l ( v ) as P 2 P probability of u as determined by the RWR starting at v . However, there might be several hundreds of unlabeled nodes in C , and running an RWR for each of the unlabeled nodes is highly inefficient in practice. Hence, we estimate the P in a more relaxed but more efficient way.

Let u be an unlabeled node in C . The probability of having been at node u one step before reaching any node v labeled l is given by: where N ( u ) denotes the set of neighboring nodes of u in C , and P info ( v | u ) is defined as: Let L  X  Ent ( C ) denote the set of nodes labeled l in C . Now, one can recursively define the probability that u is reached s &gt; 1 steps before any node labeled l as:
Intuitively, s represents the depth of the recursion. As shown in Algorithm 1, the above recursion can be computed in an iterative manner in time O ( | F ( C ) | ). Algorithm 1 p2lEstimation ( C ) 1: X := { v | lab ( v ) = l } 2: for all v  X  X do 4: end for 5: Y :=  X  ; U := Ent ( C ) \ L 6: while U is not empty do 7: for all adjacent nodes u, v with u  X  U, v  X  X do 9: insert u into Y 10: end for 11: U := U \ Y 12: X := Y ; Y :=  X  13: end while
In lines 1 -4 of Algorithm 1, all nodes in X (which are exactly the nodes labeled l ) are assigned the same P 2 l | X | . The set U (line 5) contains in each iteration (lines 6 -13) all unlabeled nodes that have no P 2 l value. In each iteration, we exclude from U (line 11) all nodes for which a P l value was determined during the iteration (represented by the set Y , line 5). At the end of each iteration the set X is set to Y . In lines 7 -10, for all adjacent nodes u, v with u  X  U and v  X  X we compute P 2 l ( u ) (line 8). The algorithm terminates when the set U is empty.

At this point, each node v of C has for each l  X  X  X  , + } node in v  X  Ent ( C ) can now be easily determined by lab ( v ) = arg max l  X  X  X  , + } P l ( v ). Finally, the most informa-tive subgraph of C is the one that consists of all nodes v for which lab ( v ) = +. In case this subgraph has more than b nodes, we successively remove from it the node v that does not belong to T and has minimal P + ( v ). By the construc-tion of our mining method, it is easy to see that S fulfills the desired properties of Definition 3. Setting The focus of our evaluation has been on the user perceived quality of MING X  X  answers. Therefore, in a user evaluation, we compared the answers of MING to those re-turned by CEPS [18]. As data set we used YAGO. The P info edge weights (see Equation (1)) for YAGO were approxi-mated on the Wikipedia corpus, based on co-occurrences of entity names in Wikipedia articles. In general, it is quite dif-ficult for users to decide whether an ER graph that intercon-nects a given set of query entities is informative, because: (1) informativeness is an intuitive and also subjective notion, (2) a user X  X  intuition has to be supported by the data in the un-derlying ER graph, and (3) a user needs to have very broad knowledge to assess the informativeness of a result graph for any set of given query nodes (especially when the query nodes represent rather obscure entities). Therefore, for this evaluation, we generated queries in which the query nodes represented famous individuals. YAGO is very rich in terms of famous individuals and contains plenty of interesting facts about them. In order to generate our queries, we extracted from the Wikipedia lists, a list of famous physicists, a list of famous philosophers, and a list of famous actors. From each of these lists we randomly generated 20 queries, each of them consisting of 2 or 3 query entities, resulting in a set of 60 queries in total. For each of the 60 queries, we presented the results produced by CEPS and MING (on the same sub-graph C ) to human judges (not familiar with the project) on a graph-visualization Web interface, without telling them which method produced which graph. For visualization pur-poses, the result graphs of CEPS and MING were pruned, whenever they had more than 15 nodes. By restricting the result graphs to such a small number of nodes, both methods were challenged to maintain only the most important nodes in the result graphs. CEPS comes with its own pruning pa-rameter (i.e., visualization parameter). For each query, the users were given the possibility to decide which of the pre-sented subgraphs they perceived as more informative. That is, one of the results could be marked informative . We also allowed users to mark both result graphs as informative , if they perceived them both as equally informative. Addition-ally, the results of both methods could be left unmarked, meaning that they both did not suit the user X  X  intuition. The results are presented in Table 1.
 # times preferred over competitor 182 4 # times marked informative 185 7 # times both marked informative 3 # times both left unmarked 21 Results There were 210 assessments in total, correspond-ing to more than 3 assessments per query. The result graphs produced by MING were marked 185 times as informative, and out of these, 182 times, they were perceived more in-formative than the results produced by CEPS. On the other hand, the MING results were left 25 times unmarked, and out of these, only 4 times they were perceived to be less informative than the results produced by CEPS.

The fundamental factor for the superiority of MING is its subgraph learning method. It learns informative and structurally robust paths between the nodes of an initial tree T that closely interconnects the query nodes.
The motivation for this work has been to provide new techniques for exploring and discovering knowledge in ER graphs. Our method, MING, is a significant step forward in this realm. It contributes to new semantic measures for the relatedness between entities. MING exploits such measures for extracting informative subgraphs that connect two or more given entities. The results of the user study fortify our assumption that MING indeed captures the intuitive notion of informative subgraphs in most of the cases.
