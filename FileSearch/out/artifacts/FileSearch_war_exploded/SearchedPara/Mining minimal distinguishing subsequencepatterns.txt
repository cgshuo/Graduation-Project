 REGULAR PAPER Xiaonan Ji  X  James Bailey  X  Guozhu Dong Abstract Discovering contrasts between collections of data is an important task in data mining. In this paper, we introduce a new type of contrast pattern, called a Minimal Distinguishing Subsequence (MDS). An MDS is a minimal subsequence that occurs frequently in one class of sequences and infrequently in sequences of another class. It is a natural way of representing strong and succinct contrast infor-mation between two sequential datasets and can be useful in applications such as protein comparison, document comparison and building sequential classification models. Mining MDS patterns is a challenging task and is significantly different from mining contrasts between relational/transactional data. One particularly im-gap constraint. We present an efficient algorithm called ConSGapMiner (Con-trast Sequences with Gap Miner), to mine all MDSs satisfying a minimum and maximum gap constraint, plus a maximum length constraint. It employs highly efficient bitset and boolean operations, for powerful gap-based pruning within a prefix growth framework. A performance evaluation with both sparse and dense datasets, demonstrates the scalability of ConSGapMiner and shows its ability to mine patterns from high dimensional datasets at low supports.
 Keywords Data mining algorithm  X  Sequential pattern  X  Frequent pattern  X  Emerging pattern  X  Gap constraint  X  Contrast pattern 1 Introduction Contrasting collections of data is an important objective in data mining and se-quences are a particularly important form of data. In this paper, we introduce a new type of pattern that is useful for contrasting collections of sequences, called a Minimal Distinguishing Subsequence (MDS). A distinguishing subsequence is a subsequence that appears frequently in one class of sequences, yet infrequently in another. A distinguishing subsequence is minimal if none of its subsequences is distinguishing. A key property of an MDS is that its items do not have to ap-pear consecutively X  X here may be gaps between them. As mentioned in [ 7 ], in the analysis of purchase behaviours, web-logs and biochemical data (e.g. motifs research), sequence patterns with gaps are often much more useful than ones with no gaps.
 classification models. We give two specific examples to highlight the idea. Example 1.1 When comparing the two protein families 1 zf-C2H2 and zf-CCHC, we discovered a protein section CLHH appearing as a subsequence 141 times among a total of 196 protein sequences in zf-C2H2, but never appearing among the 208 sequences in zf-CCHC. This subsequence represents a very strong con-spective, an unknown protein sequence containing CLHH as a subsequence seems unlikely to be a member of the zf-CCHC family.
 in [ 26 ], where it is observed that biologists are very interested in identifying the most significant subsequences that discriminate between outer membrane proteins and non-outer membrane proteins. Furthermore, the higher dimensional structure of proteins makes allowing gaps in a subsequence particularly important. Ele-ments which have a gap between them in the sequence, may in fact be spatially very close in the three-dimensional protein.
 Example 1.2 Comparing the first and last books from the Bible, we found that the subsequences  X  X aving horns X ,  X  X aces worship X ,  X  X tones price X  and  X  X rnaments price X  appear multiple times in sentences in the Book of Revelation, but never in the Book of Genesis. (The gap between the two words of each pair is  X  6 non-trivial words.) Such pairs might be seen as a fingerprint associated with the Book of Revelation and may be of interest to Biblical scholars.
 other in the original sequences. However, subsequences in which items are far away from each other are likely to be less meaningful than those whose items are close in the original sequence. A key focus, therefore, is to set a gap constraint when mining the MDS set. This restricts the distance between neighbouring elements of the subsequence. The benefits are that the mining output is smaller and more intuitive and the mining process can be faster. 1.1 Challenges Several challenges arise in the mining of MDS. The first is that the Apriori prop-erty does not hold for distinguishing subsequences, meaning that the subsequences contrast, the property does hold for frequent subsequence patterns.) Hence, any bottom up mining strategy needs to employ extra techniques for pruning the search space. This is especially important, since the search space is exponential and the number of MDS patterns present in the data may also be very large.
 databases, the thresholds may need to be set to at least 80% [ 29 ]. Using the same thresholds for MDS mining is likely to result in empty output. In MDS mining, thresholds below 30% are needed for dense databases.
 have been considered in other contexts, such as episode pattern mining [ 6 , 21 , 34 ]. Techniques there rely upon storing all possible occurrences in a list. For each can-didate, a scan through the list is performed to test if it fulfills the gap constraint. This may be workable in pure frequent pattern mining under high frequency thresholds. However, since the gap constraint is not class preserved (see Sect. 4 for a brief explanation) [ 32 ] and the search space is potentially larger in MDS mining, the occurrence list may be very large and thus such scans become very costly. 1.2 Our contributions Besides introducing the concept of minimal distinguishing subsequences, we de-scribe a new algorithm called ConSGapMiner (Contrast Sequences with Gap Miner), to efficiently mine the complete MDS set for a (minimum, maximum) gap constraint. We employ a novel technique using efficient bitset and boolean opera-tions, to determine whether a candidate subsequence can satisfy the gap constraint. We also employ several other pruning strategies.
 cation of a length constraint and how it can be easily modified to mine other kinds of patterns, such as frequent sequential patterns with gap constraints. mine MDSs from some very dense real-world databases, using a relatively low frequency threshold. Indeed, using the gap constraints, it is able to mine patterns for some very long proteins, in circumstances that would challenge the current generation of frequent subsequence miners. 1.3 Organization The rest of the paper is organised as follows. Section 2 surveys related work in the area. Section 3 introduces the basic concepts used, as well as terminologies and notations used throughout the paper. Section 4 describes the basic ConSGapMiner algorithm. Discussion of extensions of the basic algorithm to include minimum gaps, length constraints and more complex minimization is provided in Sect. 5 . Experimental and performance results are given in Sect. 6 . Finally, future work is discussed in Sect. 7 and a summary of our results is given in Sect. 8 . 2 Related work Emerging patterns, introduced by Dong and Li [ 9 , 10 ], can be used to build high databases, since the order in which items occur in sequential data is significant and items may also occur multiple times. Contrasts for relational data have been considered in other work as well (see [ 5 , 30 ] for details).
 strings of items used to differentiate between two classes of sequences. A suffix tree is used to store all the substrings. Version space trees have also been used Because substrings are a special case of subsequences using maximum gap as 0, our framework can also be used to mine minimal distinguishing substrings. However, since the items in subsequences may not necessarily appear consec-trees is unsuitable for mining them. Also, the search space for subsequence pat-terns with gap constraints is larger and consequently the mining problem is more difficult.
 imizing some function, which describes pattern goodness (and can describe con-trasts). It does not produce a collection of patterns.
 algorithm used is SPADE [ 33 ], relying on the Apriori property. Thus, any con-trast patterns it finds must have all their subsequences being contrast as well. This assumption is not true for MDS patterns.
 However, these algorithms store all occurrences for a given candidate in a list, which needs to be scanned when checking the gap constraint. This idea becomes less effective in situations where the alphabet size and support thresholds are small and many long sequences need to be checked (such as in protein datasets). Gap constraints have also been considered for repetitive pattern mining with a single long sequence. Refer to [ 6 , 21 , 34 ] for details.
 quences (see, e.g., [ 23 ]). Such patterns are related to MDSs, but are quite different and thus require different mining techniques. They also take into account various biological constraints and usually have 100% support.
 When computing the optimal alignment of two sequences, scoring functions can be adjusted to give penalties for insertions or deletions. Under this regime, align-automatically determined. 3 Definitions and terminology where e i  X  I for 1  X  i  X  n . For example, DNA sequences are sequences over the alphabet of { A , C , G , T } , and the Declaration of Independence document is a sequence over the alphabet consisting of English words. We write S [ i ] to denote quences, i.e. each element of the sequence is a single item. Although more general sequence definitions exist, the univariate representation is able to capture some of the most important and popular sequences, such as DNA, proteins, documents and Web-logs.
 is a supersequence of S ), written as S  X  S ,if S = e i 1 e i 2 ,..., e i m such that 1  X  i 1  X  j &lt; m . For example, AB is a subsequence of ACBC but BA is not, and CBC is a substring of ACBC .
 Definition 3.1 (Max-Prefix) The max-prefix of the sequence e 1 , e 2 , e 3 ,..., e n is e , e Example 3.1 ABC is the max-prefix of ABCD while AB is not. According to our definition, a sequence has exactly one max-prefix.
 Definition 3.2 (Subsequence occurrence) Given a sequence S = e , e e = e Example 3.2 For the sequence S=ACACBCB and subsequence S =AB , there are four occurrences of S in S: { 1,5 } , { 1,7 } , { 3,5 } and { 3,7 } . We now define the gap constraints, which restrict the allowed distance between items of subsequences in sequences.
 Definition 3.3 (Gap constraint and satisfaction) A (maximum) gap constraint an occurrence o s = i 1 i 2 ,..., i m of a subsequence S ,ifi k + 1  X  i k  X  g + 1  X  k  X  X  1 ,..., m  X  1 } , then we say the occurrence o s fulfills the g-gap constraint. rence of a subsequence S fulfilling the g-gap constraint, we say S fulfills the g-gap constraint. Otherwise S fails the g-gap constraint. We will later consider the minimum gap constraint.
 Example 3.3 In Example 3.2 , only the occurrence { 3,5 } fulfills the 1 -gap con-straint. Thus, the subsequence S fulfills the 1 -gap constraint since at least one of its occurrences does. No occurrence of S fulfills the 0 -gap constraint and so S fails the 0 -gap constraint. the count of p in D with g -gap constraint, denoted as count D ( p , g ) , is the num-ber of sequences in D in which p appears as a subsequence fulfilling the g -gap constraint. The (relative) support of p in D with g -gap constraint is defined as say p is frequent in D with g -gap constraint. Otherwise p is infrequent. Definition 3.4 ( g -MDS and the g -MDS mining problem) Given two classes of sequences pos (the positive) and neg (the negative), two support thresholds  X  and  X  , and a maximum gap 2 g, a pattern p is called a Minimal Distinguishing Subsequence with g-gap constraint (g-MDS for short), if and only if the following conditions are true: 1. Frequency condition: su pp pos ( p , g )  X   X  ; 2. Infrequency condition: su pp neg ( p , g )  X   X  ; 3. Minimality condition: There is no subsequence of p satisfying 1 and 2 . Given pos, neg,  X  ,  X  and g, the g-MDS mining problem is to find all the g-MDSs. The minimality condition is important, since it both reduces output size and im-proves performance, as well as making patterns shorter (more succinct). This is especially important for datasets with long sequences, where the number of pat-terns output may be huge. Similar issues regarding concise representations arise in frequent and emerging pattern mining as well. In frequent pattern mining, mining minimal and maximal borders are mined, rather than the entire space of emerging patterns.
 may be minimal, we will use the following definition of Semi-Minimal Distin-guishing Subsequence.
 Definition 3.5 ( g -SMDS set) Any superset of the g-MDS set containing patterns that satisfy the frequency and infrequency conditions, but not necessarily the min-imality condition, is called a Semi-Minimal Distinguishing Subsequence set with g maximum gap constraint, g-SMDS set for short.
 Example 3.4 Given the two sets of sequences shown in Table 1 , suppose  X  = 1 / 3 (and  X  = 0 ) and g = 1 .The 1 -MDSs are { BB, CC, BAA, CBA } . Notice that BB is a subsequence of all the negative sequences, if no gap constraint is used. However, all the occurrences of BB in the negative fail the 1 -gap constraint, so BB becomes a distinguishing subsequence when g = 1 . Observe that every supersequence of a 1 -MDS fulfilling the 1 -gap constraint and support threshold is also distinguishing. However, these are excluded from the MDS set, since they are non-minimal and contain redundant information. 4The ConSGapMiner algorithm We now introduce our algorithm known as ConSGapMiner , for solving the g -MDS mining problem. Extensions of this basic algorithm will be provided later in Sect. 5 . It operates in three stages: (i) candidate generation, (ii) support and gap calculation and (iii) post-processing (minimization). In the first stage, a candidate then c is retained. Finally, in the third stage, post-processing is used to remove all the non-minimal patterns and yield the final g -MDS set. We now discuss each of these stages in turn. 4.1 Candidate generation ConSGapMiner performs a depth-first search in a lexicographic sequence tree, similar to frequent subsequence mining techniques such as [ 3 , 31 ]. In the lexico-graphic sequence tree, each node contains a sequence s (we will interchangeably refer to nodes and the sequences they represent), a value for count pos ( s , g ) and a ing the depth-first search, we extend the current node by a single item from the alphabet, according to a certain lexicographic order. For (the sequence of) each newly generated node n , we calculate its supports from pos and from neg . Example 4.1 Part of the lexicographic tree for mining the database from Table 1 is given in Fig. 1 . Observe that the branches of the lexicographic tree terminate at nodes whose count pos = 0 .
 Two basic pruning strategies can be applied to reduce the size of the search space of the tree. These will be applied in the candidate generation process. 4.1.1 Non-minimal distinguishing pruning This strategy is based on the fact that any supersequence of a distinguishing se-quence cannot be a minimal one. Suppose we encounter a node representing se- X  . Then (i) we need never extend s and (ii) need never extend any of the sibling nodes of s by the item c . Such an extension would lead to a supersequence of s and would not be an MDS.
 Example 4.2 In Figure 1 , because su pp pos ( AACC )&gt; 0 and su pp neg ( AACC ) = 0 , AACC must be distinguishing. Moreover, we know in the subtree of its sibling AACB ,supp neg ( AACBC ) must be 0 too. So AACBC cannot be an MDS. 4.1.2 Max-prefix infrequency pruning the tree can be frequent. Thus, whenever we come across a node s ,where supp pos ( s , g )&lt; X  , we do not need to extend this node any further. For exam-pos ), since no frequent sequence can be found in its subtree.
 pruning  X   X  X f a subsequence is infrequent in pos , then no supersequence of it can be frequent X . Such a statement is not true, because the gap constraint is not class necessarily infrequent; this consequently increases the difficulty of our problem. Indeed, extending an infrequent subsequence by appending will not lead to a fre-quent sequence, but extensions by inserting items in the middle of the subsequence may lead to a frequent subsequence. An example situation is given next. Example 4.3 For Figure 1 , suppose  X  = 1 / 3 and g = 1 .Then AAB is not a fre-quent pattern because count pos ( AAB , 1 ) = 0 . But looking at AAB  X  X  sibling, the subtree rooted at AAC , we see that count pos ( AACB , 1 ) = 1 . So a supersequence AACB is frequent, but its subsequence AAB is infrequent.
 4.2 Support calculation and gap checking be computed. The main challenge comes in checking satisfaction of the gap con-straint. A candidate can occur many times within a single positive sequence. A straightforward idea for gap checking would be to record the occurrence of each candidate in a separate list. When extending the candidate, a scan of the list deter-mines whether or not the extension is legal, by checking whether the gap between Algorithm 1 Candidate Gen( c , g , I ,  X  ,  X  ): Generate new candidates from sequence c the end position and the item being appended is smaller than the (maximum) gap constraint value for each occurrence. This idea becomes ineffective in situations with small alphabet size and small support threshold and many long sequences needing to be checked, since the occurrence list becomes unmanageably large. In-stead, we use a new method for gap checking, based on a bitset representation of subsequences and the use of boolean operations. This technique is described next. Definition 4.1 (Bitset) A bitset is a sequence of bits which each takes the value 0 or 1 . An n-bitset X contains n bits, and X [ i ] refers to the i th bit of X . Suppose we have a sequence S = e 1 , e 2 , e 3 ,..., e n , and another sequence S , which is no longer than S . The occurrence(s) of S in S can be represented by an BS [ i ] is set to 1; otherwise it is set to 0. For example, if S = BACACBCCB ,the 9-bitset representing S = AB is 000001001. This indicates how the subsequence AB can occur in BACACBCCB , with a  X 1 X  being turned on in each final position where the subsequence AB could be embedded. If S is not a subsequence of S , then the bitset representing the occurrences of S consists of all zeros. to 1 if e i = e . In the last example, the 9-bitset representing the single item C is 001010110.
 sequences. In this case, the subsequence will have associated with it an array of bitsets, where the k th bitset describes the occurrences of S in the k th sequence. 4.2.1 Initial bitset construction Before mining begins, it is necessary to construct the bitsets that describe how each item of the alphabet occurs in each sequence from the pos and neg datasets. So, each item i has associated with it an array of | pos |+| neg | bitsets. For a given item, the number of bitsets in its array which contain one or more 1 X  X  is equal to count ( i , g ) .
 Example 4.4 Consider the database in Table 1 . The bitset array for count pos ( A , g ) = 3 and count neg ( A , g ) = 2 . 4.2.2 Bitset checking Each candidate node c in the lexicographic tree has a bitset array associated with it, which describes how the sequence for that node can occur in each of the | pos |+ | neg | sequences. This bitset array can be directly used to compute count pos ( c , g ) equal to zero, that describe positive sequences). During mining, we extend a node c to get a new candidate c , by appending some item i . Before we can compute c . The bitset array for c is calculated using the bitset array for c and the bitset array for item i , and is done in two stages.
 Stage 1: Using the bitset array for c , we generate another array of corresponding mask bitsets. Each mask bitset captures all the valid extensions of c , with respect to the gap constraint, for a particular sequence in pos  X  neg . Suppose the maximum gap is g ,foragivenbitset b in the bitset array of c . We perform g + 1 times of intermediate bitsets, one for each stage of the shift. By ORing together all the intermediate bitsets, we obtain the final mask bitset m derived from b .Themask bitset array for c consists of all such mask bitsets.
 Example 4.5 Taking the last bitset 10100 in the previous example and setting g = 1 , the process is 01111 is the mask bitset derived from bitset 10100 .
 setting them to 0) and opens the following g + 1 bits (by setting them to 1). In this way, m can accept only 1s within a g + 1 distance from the 1s in b .
 calculate the bitset array for c which is the result of appending i to c . Consider a sequence s in pos  X  neg and suppose the mask bitset describing it is m and the bitset for item i is t . The bitset describing the occurrence of c in s is equal to m AND t . If the bitset of the new candidate c does not contain any 1, we can conclude that this candidate is not a subsequence of s with g -gap constraint. Example 4.6 ANDing 01111 (the mask bitset for sequence A) from the last ex-ample with C  X  X  bitset 00010 , gives us AC  X  X  bitset 00010 .
 mask 5 -bitset is So BB  X  X  bitset is 00110 AND 01001 = 00000 . This means BB is not a subsequence of ABACB with 1 -gap constraint.
 Example 4.7 Figure 2 shows the process of getting the bitset array for BB from that for B . From the figure we can see count pos ( BB , 1 ) = 2 and count neg ( BB , 1 ) = 0 .
 puter architectures have very fast implementations of shift operations and logical operations. Since the maximum gaps are usually small (e.g. less than 20), the total number of right shifts and logical operations needed is not too large. Consequently, algorithm for support counting is given in Algorithm 2 . 4.3 Minimization We have already seen how non-minimal distinguishing pruning eliminates non-minimal candidates during tree expansion. However, the pattern set returned by Algorithm 1 is only semi-minimal, i.e. an SMDS set. For example, in Figure 1 , we will get ACC , which is a supersequence of the distinguishing sequence CC . Thus, in order to get the g -MDS set, a post-processing minimization step is needed.
 sequences, this leads to an O ( n 2 ) algorithm, which is expensive if n is large. patterns are clustered according to their length, when they are output during min-ing.
 inserted into the tree in ascending order of length. Each sequence p to be inserted into the tree is compared against the sequences already there. This is easily done by stepping through each prefix of p , at each stage identifying the nodes of the Algorithm 2 Support Count( c , g , D ): calculate supp D ( c , g ) tree which are subsequences of the prefix so far. The process terminates when a leaf node or the end of p is reached. If a subsequence of p in the tree is found, then p is discarded. Otherwise, p must be minimal and it is inserted. duplicate comparisons, particularly for situations where there is substantial simi-larity between the sequential patterns, since each sequential pattern prefix is only stored once. For example, consider two shorter patterns P 1 = ABCC , P 2 = ABCF and a longer pattern P 3 = ABCDE . To check whether P 3 is minimal by using the naive way, we compare P 3 with P 1 itemwise for five comparisons and with P 2 itemwise for five comparisons to conclude that P 3 is minimal. By using the prefix tree, ABC is built once and compared once, which takes itemwise three compar-isons and then another two comparisons to check the other two items D and E . Finally, we know that P 3 is minimal, because no leaf is found. This takes item-wise 5 comparisons total, rather than 10 comparisons using the naive way. algorithm of ConSGapMiner is provided in Algorithm 4 .
 Algorithm 3 Minimization( ds ): minimize the distinguishing subsequence set ds Algorithm 4 ConSGapMiner ( pos , neg , g ,  X  ,  X  ): mine all the g -MDS from pos to neg 5 Extending the basic ConsGapMiner algorithm We now examine several extensions to the basic approach just described. These involve handling minimum gaps, length constraints and performing more complex mining frequent sequential patterns. 5.1 Allowing minimum gap constraints The first extension is to add a minimum gap constraint along with the maximum one.
 Definition 5.1 (Minimum gap constraint) A minimum gap constraint is specified by a positive integer q . Given a sequence S = e 1 , e 2 ,..., e n and an occurrence o = ( i then we say the occurrence o s fulfills the q -minimum gap constraint. Otherwise we say o s fails the q -minimum gap constraint. If there is at least one occurrence of a subsequence S fulfilling the q -minimum gap constraint, we say S fulfills the q -minimum gap constraint. Otherwise S fails it.
 Example 5.1 Consider a sequence S = ACEBE . If a maximum gap g = 3 is given, AE is a subsequence with two occurrences in S. The first is ( 1 , 3 ) and the other is ( 1 , 5 ). If a minimum gap q = 2 is given, AE is a subsequence with one occurrence ( ( 1 , 5 ) ). Considering only the above maximum gap constraint, AC is a subsequence of S, but in conjunction with the above minimum gap constraint, it is not.
 for maximum gap. Minimum gaps can be useful for applications where we re-quire items in a sequence to be at least certain distance apart from one another. For example, in scenarios where the items in the sequence represent values being sampled over time, such as a waveform, items that are too close to each other may represent information that is overly similar. Minimum gaps may then be specified special case is when the value of the minimum gap is specified to equal the value for the maximum gap. This will result in patterns whose items are distributed in equal distance in the original dataset. This could be viewed as a kind of quasi-periodicity.
 constraints g and q . The count of p in D with g as the maximum gap con-straint and q as the minimum gap constraint, denoted as count D ( p , g , q ) ,isthe number of sequences in D in which p appears as a subsequence fulfilling both the g -gap constraint and the q -minimum gap constraint. The (relative) support of p in D with g -gap constraint and q -minimum gap constraint is defined as mum gap constraints: Definition 5.2 (Extended MDS mining problem) Given two classes of sequences pos (the positive) and neg (the negative), two support thresholds  X  and  X  ,amaxi-mum gap g and a minimum gap q , a pattern p is called a Minimal Distinguishing following conditions are true: 1. Frequency condition: su pp pos ( p , g , q )  X   X  ; 2. Infrequency condition: su pp neg ( p , g , q )  X   X  ; 3. Minimality condition: There is no subsequence of p satisfying 1 and 2 . Given pos, neg,  X  ,  X  , g and q , the ( g , q ) -MDS mining problem is to find all the ( g , q ) -MDSs.
 minimum gap constraint. The only part requiring modification is the construction of the mask bitset. Recall that for a maximum gap constraint g , to construct the mask bitset, we perform g + 1 times of right shift by distance 1 and OR the g + 1 intermediate bitsets together. The resulting bitset is the mask of the given bitset. When a minimum gap constraint q is added, we initially right shift the given bitset q times and discard the intermediate bitsets. Then, we perform another g + 1  X  q right shift operations and OR the g + 1  X  q intermediate bitsets to obtain the mask bitset.
 Example 5.2 Consider the last sequence in Table 1 as an example. We know that B  X  X  bitset is 00111 w.r.t. g = 2 .So BC  X  X  bitset should be 00111 AND 00010 = 00010 . Consider q = 2 , we discard the first two intermediate bitsets which are 00100 and 00010 . Then we right shift g + 1  X  q = 1 times and the mask bitset is 00001 . From this bitset we can see that the adjacent two positions, which are 3 and 4 , are closed and the third position 5 is open. This mask bitset expresses the minimum gap constraint. By ANDing this mask bitset with the single item C  X  X  bitset 00010 ,weget 00000 ;so BC is not a subsequence of ABACB ,becauseit fails the minimum gap constraint q = 2 . 5.2 Sequence length constraints The use of a maximum length constraint is a popular way of reducing the search space and size of the output for sequential pattern mining algorithms (e.g. [ 32 ]). ConSGapMiner can be easily extended to mine patterns with lengths less than a given threshold l . Here, by length we mean the number of items that appear in the pattern (as distinct from window size, which refers to the maximum gap between the first and the last items in the pattern). We supplement the existing pruning strategies with another called max-length pruning. 5.2.1 Max-length pruning extended.
 straightforward to determine the current length of each candidate for performing the comparison. Using length constraint pruning is likely to result in a much shal-lower lexicographic tree and thus faster mining time. 5.3 Coverage and prefix-based pattern minimization In order to discover patterns which satisfy the minimality condition from Defi-nitions 3.4 and 5.2 , we have described strategies for pattern minimization which aim to determine whether a pattern (or candidate) is a supersequence of some other pattern (or candidate). For some situations, pursuing this kind strategy may be too aggressive and useful patterns may be eliminated, since the gap constraint is not class preserved. Consider the following example.
 Example 5.3 Suppose pos = s { ACCBD , ACDBD , ABD } and two distinguishing patterns ACBD and ABD have been found using  X  = 1 / 2 ,g = 1 ,q = 0 .We observe that count pos ( AC B D , 1 , 0 ) = 2 and count pos ( ABD ) = 1 .Ifwere-move ACBD because it is a supersequence of ABD , we will lose a pattern which has higher frequency than its subpattern and thus is arguably a more important feature.
 niques. The first is based on comparisons using coverage in pos , the second is based on prefix comparisons. Both are less aggressive and remove fewer patterns than does the minimization we have previously described (which will be called basic minimization from now on).
 in Definitions 3.4 and 5.2 . Given two MDSs, p 1 and p 2 and a reference dataset D , we wish to remove p 2 due to p 1 ,if p 1 occurs in every sequence from D that p 2 does ( p 2 : D p 1 ).If p 1 occurs in every sequence from pos that p 2 does, then it is guaranteed that ( p 2 : D p 1 ) only if D = pos .Thisis coverage-based minimization . is prefix-based minimization . 5.3.1 Coverage-based minimization To describe the implementation of coverage-based minimization, we first begin by formally defining the notion of coverage. Definition 5.3 (Coverage set) Given a sequence p, gap parameters g and q and the datasets pos and neg , the coverage set of p is equal to the set of sequences from pos in which p appears as a subsequence fulfilling both the g-gap constraint and the q -minimum gap constraint. The coverage set can be represented by a bitset, containing as many bits as there are sequences in pos. Bit i is turned on zero.
 nated by a sequence p 2 iff: 1. p 2  X  p 1 and 2. the coverage set of p 1 is a subset of the coverage set of p 2 .
 two techniques, the first is the non-minimal distinguishing pruning step and the other is the post-processing minimization.
 is attached. This coverage bitset contains as many bits as the total of sequences in pos . A newly generated candidate X  X  coverage bitset can be set by its bitset array. If the i th bitset in the array contains at least one 1, this candidate X  X  count pos is increased and the i th bit in the coverage bitset is set to 1. The rule for the pruning is then changed to the following.
 Non-minimal distinguishing pruning (adjusted) : Suppose we encounter a node representing sequence s ,where c is the last item in s and supp pos ( s , g )  X   X  and nodes s ,weAND s  X  X  coverage bitset with s  X  X  coverage bitset and then XOR the resulting bitset with s  X  X  coverage bitset. If the resulting bitset does not contain any 1, then we never need extend s by the item c . If the resulting bitset contains at least one 1, we must extend s by the item c .
 s  X  X . If this is the case, then no candidate in the subtree of s has a coverage which is not a subset of s  X  X . In this situation, any extension of the nodes in the subtree of s , with item c can give a supersequence of s with a subset of s  X  X  coverage, which means it is not minimal. If s has a non-subset coverage of s , then there may be an extension with item c , which gives a distinguishing subsequence with a non-subset coverage of s  X  X  and this may be minimal, so we need to generate and keep it.
 order of their lengths. For each pattern we still keep a coverage bitset with the same meaning as described above. For each pattern p , we find all the patterns which have shorter lengths and with a coverage that is a superset of that of p . If such a pattern is found, the standard way of checking whether this pattern is a subsequence of p or not is performed. If it is a subset, p is eliminated. If no pattern can be found to eliminate p , then we retain it in the MDS set. 5.3.2 Prefix-based minimization Performing prefix-based minimization is substantially simpler than performing coverage-based minimization. Two modifications are needed to the basic ConsS-GapMiner algorithm. First of all, the non-minimal distinguishing pruning step is adjusted to the following.
 Non-minimal distinguishing pruning (adjusted) : Suppose we encounter a node representing sequence s ,where c is the last item in s and supp pos ( s , g )  X   X  and supp neg ( s , g )  X   X  . Then we need never extend s .
 guishing sequences produced are guaranteed to be prefix minimal. 5.3.3 Comparison of the three minimization techniques We now compare the three minimization techniques we have described: basic minimization, prefix-based minimization and coverage minimization. We com-pare them in terms of output size, running time and classification capability. MDS , MDS pr e and MDS co v . Observe that MDS  X  MDS co v  X  MDS pr e , hence |
MDS | X | MDS co v | X | MDS pr e | . In terms of running time, the search space for mining MDS will be the smallest and the search space to mine MDS pr e will be the largest. With regard to classification ability, we would expect this to be corre-lated with the number of patterns that are contained in an unknown test sequence (according to the gap constraints). Thus, MDS pr e would have the most useful clas-sification capability, since it (i) includes all the patterns that MDS and MDS co v do, (ii) may include some patterns which do not have a subsequence that both exists in MDS or MDS co v and is contained in the test sequence. This means MDS pr e will contain most of the patterns which are contained in an unknown test sequence during classification. In a similar way, we would expect MDS to have the least powerful classification capability. 5.4 Applicability of the approach to frequent sequential patterns Frequent sequential pattern mining with gap constraints has been identified as a useful task [ 32 ]. ConSGapMiner can be adapted to mine this kind of pattern too. dataset. The method for checking maximum and/or minimum gap constraints re-mains the same. The main difference is that only max-prefix infrequency pruning can be used. The non-minimal distinguishing pruning and post-processing mini-mization techniques are no longer needed, since a succinct representation is not required. 6 Performance study We have evaluated the performance of ConSGapMiner in a number of ways. Sec-tion 6.1 focuses on mining a special kind of MDS which has zero support in the It captures very sharp contrasts between the datasets. Section 6.2 then examines the mining of patterns with non-zero support in the neg . In Sect. 6.3 we then show the distribution of the MDSs and look at the effect of using a maximum length constraint. We use a minimum gap q = 0 for all experiments.
 other work that is suitable for mining g -MDSs. We use two kinds of sequences, one from protein families and the other from books of the Bible. These two se-quence types represent some interesting real-world applications. On the one hand, protein families use a relatively small alphabet (20 amino acids), each containing relatively few sequences with long average length. On the other hand, books of the Bible are built on a large alphabet (several thousand words), and have thou-sands of sentences of small average length. The protein families were selected from PFam: Protein Family Database (http://www.sanger.ac.uk/Software/Pfam/?) and the Bible books were downloaded from http://www.o-bible.com/dlb.html?. memory, running UNIX.
 tein families that we used are listed in Table 2 . These represent some challenging situations and the dataset sizes are representative for protein families. (2) We ex-tracted sequences from the books of the Bible. This kind of sequential data differs from protein data, due to its large alphabet size, much smaller sequence length and larger number of sequences. We used all sentences in the first four books of the New Testament (Matthew, Mark, Luke and John) as the positive class and all sentences in the first four books of the Old Testament (Genesis, Exodus, Leviticus and Numbers) as the negative class. In order to obtain meaningful patterns, we removed all the punctuation and frequently appearing words such as  X  X nd X ,  X  X he X ,  X  X f X . Each sentence corresponds to a separate sequence. There are 3768 sequences in pos , 4893 sequences in neg , and a total (alphabet size) of 3344 unique words. Average sentence length is 7 words and the maximum is 23. 6.1 Mining MDSs with zero support in neg 6.1.1 Protein families In Figure 3 , we give the running time for varying frequency thresholds (refer to a) and (maximum) gap size (refer to b).  X  is set to 0 for all cases. We can see that as the maximum gap becomes larger, or as the frequency threshold  X  becomes lower, more time is required for mining. An important reason for this is that the MDS output size increases dramatically in both situations. For example, consider the final pair of protein families in Table 2 .When g = 5and  X  = 24 . 3%, there are 20,936 5-MDSs output. Changing  X  to 5.4%, the output size jumps to 3,600,822. For the same dataset with  X  = 27% and g = 3, the output size is 536, whereas for  X  = 27% and g = 7, it is 314,791. Some explanations for these are the following: The smaller the maximum gap is, the earlier a candidate is likely to become dis-tinguishing (being less likely to appear in the neg ) and so earlier pruning of the search space is possible. Similarly, earlier pruning is possible for high values of the frequency constraint  X  , since it is more difficult to satisfy for longer (and thus lower) sequence nodes in the tree. Furthermore, the longer sequences in the pos and neg are, the more time ConSGapMiner needs, since it has to search to deeper levels in the lexicographic tree. 6.1.2 Books of the Bible Experimental results concerning the Bible books are shown in Fig. 3 . Looking at these figures, we can see that ConSGapMiner operates much faster on this kind of data. The larger alphabet means that non-minimal distinguishing pruning hap-pens very early in the lexicographic tree, while the small average length means the tree cannot become too deep. Table 3 lists some of the patterns returned when mining the 6-MDS. Both contiguous patterns (substrings) and non-contiguous pat-terns (subsequences) are shown, with the number of times they occur. Obviously, for human understanding of the patterns, the meaning of the substrings is more straightforward than that for the subsequences. However, subsequence contrasts can sometimes capture combinations of interesting words that are not found by substrings. 6.2 Mining MDSs with non-zero support in negative Figure 4 shows experiments of the four protein family pairs with varying  X  ,fixed  X  and g . ConSGapMiner runs faster when  X  is bigger. This is because, as  X  in-creases, it becomes easier for a candidate pattern to become distinguishing, be-cause it is easier for its support in neg to be below  X  . When this happens, the sub-tree of the candidate pattern can be pruned and the search space becomes smaller. But as  X  increases, the contrast information carried by the mined patterns may be weaker because potentially more negative sequences can match them as well. From Fig. 4 we can also see the time used by minimization procedure. This post-processing step only takes a small proportion of the total running time. 6.3 Distributions of the MDS patterns and effect of the length constraint We also examined the distribution of the MDSs mined from the four protein family pairs. From Fig. 6 we can see that the number of patterns drops while the support increases. The patterns with high support in pos and low (even 0) support in neg are considered to capture the most significant contrast information and considered to be more valuable.
 with smaller lengths are more easily found in neg ; so a pattern needs to be rea-sonably long to capture enough information to distinguish pos from neg .Alsothe minimization post-process removes a lot of long patterns because some shorter subsequences are already contained.
 ond pair of the protein families in Table 2 are now shown. From Fig. 5 we can see that as the maximum length goes down, the mining process achieves a consider-able speedup, due to the depth of the lexicographic tree being no more than the maximum length parameter.
 port in pos , low support in neg and small lengths. These patterns capture strong sequence and contribute to the identification of the most likely class (in a classi-fication scenario). But as we can see from the diagrams, these  X  X seful X  patterns are relatively few. In order to get enough  X  X seful X  patterns, parameters need to be carefully set. The relationship between parameter settings and the accuracy of classifiers is likely to depend on specific dataset properties, but is an interesting topic for future work. 7 Discussion and future work performed. We also tested ConSGapMiner on a number of other protein datasets, with overall performance being similar and pleasing overall. We now look at the complexity of the algorithm, discuss some limitations and consider future work. 7.1 Complexity analysis We analyze the time and space complexity of ConSGapMiner. Let L be the average length of the sequences; A be the size of the alphabet; N be the total number of the sequences in pos and neg . 7.1.1 Space complexity candidate generation, a depth-first search is performed in the lexicographic tree. Suppose the longest pattern X  X  length is l , then the upper bound of the memory usage is (without any pruning of the search space): ( l  X  A  X  L  X  N )/ 8 bytes. At runtime, the deeper the candidate is located in the tree, the more branch pruning is possible so the actual usage of memory is always less than ( l  X  A  X  L  X  N )/ 8. For the minimization, suppose the average length of the patterns is l a v g and each item in the sequence can be stored in 1 byte (that means A is less than 256 which is sufficient in most bio-sequence mining problems) and the total number of minimal distinguishing subsequences is T pat . In the prefix tree, each node contains 1 byte for the item and 4 A bytes for the children links (suppose the machine is 32-bit one). The upper bound memory used to store the tree is l a v g  X  T pat  X  ( 4 A + 1 ) considering no sharing of the prefix. So the memory usage by ConSGapMiner memory, a reasonable assumption for today X  X  hardware; suppose A is 20, which is the case for amino acids in proteins; and suppose the longest pattern contains no more than 20 amino acids and that the average length of the MDSs is 10. Then ConSGapMiner can handle a dataset having an average length L of 1000 and the number of sequences N of 20 , 000, within ( 20  X  20  X  1000  X  20 , 000 )/ 8 = 953 MB, assuming an output size 1 , 000 , 000 of MDSs. ConSGapMiner is thus likely to be practical for many bioinformatic applications. 7.1.2 Time complexity Suppose N exp nodes in the lexicographic tree are explored by ConSGapMiner and the maximum gap constraint is g . A pre-condition of the following analysis is that byte-wise bit operations can be finished in constant time and checking for whether a bitset contains 1 or not can also be done in constant time. For each bitset, g + 1 right shifts and ORing the intermediate bitsets require 2 ( g + 1 ) L / 8 time to finish. The last AND operation takes another L / 8 time. So for each node, 2  X  N  X  ( g + 2 )  X  L / 8 + N time is used to generate the bitset array and determine Count pos and Count neg . For the total N exp nodes, the time used to mine all the MDSs is roughly N exp  X  ( 2  X  N  X  ( g + 2 )  X  L / 8 + N ) . For short protein segments with maximum length no more than 64 or 32, all bit operations can be finished in constant time and so ConSGapMiner will be extremely fast. 7.2 Limitations We now discuss some of the limitations of ConSGapMiner . Firstly, for very large maximum gap constraint, the shift operation of the bitsets may become costly. Secondly, for extremely large datasets with many sequences longer than 10 K, there may be insufficient main memory to use ConSGapMiner . From this point of view, it may be worthwhile to examine schemes for bitset compression. Finally, the parameters of  X ,  X , g and q all have to be chosen by the user in order to give useful patterns. For users who lack knowledge of the structure of the datasets, this may be a very difficult task. Hence, it may be worthwhile to instead focus on  X  X arameter free X  mining [ 28 ] mining the top k most distinguishing sequences. 7.3 Window size constraint As mentioned earlier, the number of MDS patterns present for high dimensional datasets can be very large. Gap size is certainly an important way of reducing this output size. A window size constraint (limiting the maximum gap between the first and last item in an MDS) appears to be more difficult to deal with and to require the maintenance of more involved data than bitsets. 7.4 Suitability for classification The focus of this paper has been presenting an efficient algorithm for mining MDS patterns. Of course, there is the important related question of how such patterns may be used. We believe these patterns are interesting, due to their intuitive, hu-man understandable form and ability to capture strong contrasts. Such contrast patterns can be used directly by humans, and they can be used to build accurate classifiers. We have carried out some preliminary experiments (not included in this paper) which indicate that a simple classifier model built from these patterns is able to give very promising predictive power for determining the correct protein family of an unknown sequence, an important research topic in bioinformatics. an interesting direction. One possibility is to form a high dimension feature space with the MDSs and then use an SVM to predict in this space dimension. This kind of direction has been followed in [ 26 ], although only frequent substrings were used. 7.5 Using different gaps for pos and neg datasets As mentioned earlier, ConSGapMiner can be easily modified to mine patterns using a gap constraint for the neg dataset which is different than that used for the pos dataset. For example, mine all patterns using g = 0in pos and g = 20 in neg . This will yield MDSs which appear as substrings in pos and which have a very strict definition of non-appearance in neg  X  X hey must not appear as a substring, or as any subsequence with gap less than 20. These can be thought of as being contrasts that are much sharper than those that would be obtained if g = 0were used for both datasets. 8 Concluding remarks We have introduced the data mining problem of minimal distinguishing subse-quences . These patterns can capture essential contrast information between differ-ent classes of sequences.
 subsequences and made the following major contributions: (a) A prefix growth framework for mining MDSs, utilizing a number of pruning techniques. (b) A bitset-manipulation-based technique for checking gap constraints. Analysis and experiments show that our approach works well for a number of datasets, particularly high dimensional proteins. (c) Some extensions of our algorithm ConSGapMiner and the properties of the MDSs.
 References Author Biographies
