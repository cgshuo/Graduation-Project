 In decision problems where errors incur a severe loss, one may have to build classifiers that abstain from classifying ambiguous examples. Rejecting these examples has been investigated since the early days of pattern recognition. In particular, Chow (1970) analyses how the error rate may be decreased thanks to the reject option.
 There have been several attempts to integrate a reject option in Support Vector Machines (SVMs), using strategies based on the thresholding of SVMs scores (Kwok, 1999) or on a new training cri-terion (Fumera &amp; Roli, 2002). These approaches have however critical drawbacks: the former is not consistent and the latter leads to considerable computational overheads to the original SVM algorithm and lacks some of its most appealing features like convexity and sparsity. We introduce a piecewise linear and convex training criterion dedicated to the problem of classi-fication with the reject option. Our proposal, inspired by the probabilistic interpretation of SVM Hence, we generalize the loss suggested by Bartlett and Wegkamp (2008) to arbitrary asymmetric misclassification and rejection costs. For the symmetric case, our probabilistic viewpoint motivates another decision rule. We then propose the first algorithm specifically dedicated to train SVMs with a double hinge loss. Its implementation shows that our decision rule is at least at par with the one of Bartlett and Wegkamp (2008).
 The paper is organized as follows. Section 2 defines the problem and recalls Bayes rule for binary classification with a reject option. The proposed double hinge loss is derived in Section 3, together with the decision rule associated with SVM scores. Section 4 addresses implementation issues: it formalizes the SVM training problem and details an active set algorithm specifically designed for training with the double hinge loss. This implementation is tested empirically in Section 5. Finally, Section 6 concludes the paper. Classification aims at predicting a class label y  X  Y from an observed pattern x  X  X . For this purpose, we construct a decision rule d : X  X  X  , where A is a set of actions that typically consists in assigning a label to x  X  X  . In binary problems, where the class is tagged either as +1 or  X  1 , the In general, the goal of classification is to predict the true label for an observed pattern. However, patterns close to the decision boundary are misclassified with high probability. This problem be-comes especially eminent in cases where the costs, c  X  or c + , are high, such as in medical decision making. In these processes, it might be better to alert the user and abstain from prediction. This motivates the introduction of a reject option for classifiers that cannot predict a pattern with enough confidence. This decision to abstain, which is denoted by 0 , incurs a cost, r  X  and r + for examples labeled  X  1 and +1 , respectively.
 The costs pertaining to each possible decision are recapped on the right-hand-side. In what follows, we assume that all costs are strictly positive: Furthermore, it should be possible to incur a lower expected loss by choosing the reject option instead of any prediction, that is Bayes X  decision theory is the paramount framework in statistical decision theory, where decisions are taken to minimize expected losses. For classification with a reject option, the overall risk is where X and Y denote the random variable describing patterns and labels.
 The Bayes classifier d  X  is defined as the minimizer of the risk R ( d ) . Since the seminal paper of Chow (1970), this rule is sometimes referred to as Chow X  X  rule : Note that, assuming that (1) and (2) hold, we have 0 &lt; p  X  &lt; p + &lt; 1 .
 One of the major inductive principle is the empirical risk minimization, where one minimizes the empirical counterpart of the risk (3). In classification, this principle usually leads to a NP-hard problem, which can be circumvented by using a smooth proxy of the misclassification loss. For example, Vapnik (1995) motivated the hinge loss as a  X  X omputationally simple X  (i.e., convex) surro-for classification with a reject option. One method to get around the hardness of learning decision functions is to replace the conditional probability P( Y = 1 | X = x ) with its estimation b P( Y = 1 | X = x ) , and then plug this estimation back in (4) to build a classification rule (Herbei &amp; Wegkamp, 2006). One of the most widespread Figure 1: Double hinge loss function ` p  X  ,p + for positive (left) and negative examples (right), with p  X  = 0 . 4 and p + = 0 . 8 (solid: double hinge, dashed: likelihood). Note that the decision thresholds f + and f  X  are not symmetric around zero. representative of this line of attack is the logistic regression model, which estimates the conditional probability using the maximum (penalized) likelihood framework.
 As a starting point, we consider the generalized logistic regression model for binary classification, where and the function f : X  X  R is estimated by the minimization of a regularized empirical risk on the training sample T = { ( x i , y i ) } n i =1 regression procedure, ` is the negative log-likelihoood loss This loss function is convex and decision-calibrated (Bartlett &amp; Tewari, 2007), but it lacks an ap-pealing feature of the hinge loss used in SVMs, that is, it does not lead to sparse solutions. This drawback is the price to pay for the ability to estimate the posterior probability P( Y = 1 | X = x ) on the whole range (0 , 1) (Bartlett &amp; Tewari, 2007).
 However, the definition of the Bayes X  rule (4) clearly shows that the estimation of P( Y = 1 | X = x ) does not have to be accurate everywhere, but only in the vicinity of p + and p  X  . This motivates the construction of a training criterion that focuses on this goal, without estimating P( Y = 1 | X = x ) on the whole range as an intermediate step. Our purpose is to derive such a loss function, without sacrifying sparsity to the consistency of the decision rule.
 Though not a proper negative log-likelihood, the hinge loss can be interpreted in a maximum a posteriori framework: The hinge loss can be derived as a relaxed minimization of negative log-likelihood (Grandvalet et al., 2006). According to this viewpoint, minimizing the hinge loss aims at deriving a loose approximation to the the logistic regression model (5) that is accurate only at f ( x ) = 0 , thus allowing to estimate whether P( Y = 1 | X = x ) &gt; 1 / 2 or not. More generally, one can show that, in order to have a precise estimate of P( Y = 1 | X = x ) = p , the surrogate loss should be tangent to the neg-log-likelihood at f = log( p/ (1  X  p )) .
 Following this simple constructive principle, we derive the double hinge loss , which aims at reliably estimating P( Y = 1 | X = x ) at the threshold points p + and p  X  . Furthermore, to encourage sparsity, Figure 1. Formally, for the positive examples, the double hinge loss satisfying the above conditions can be expressed as and for the negative examples it can be expressed as symmetry with respect to the labels.
 rule can be expressed in terms of the function f as follows is optimal in the sense that the risk for the learned decision rule converges to the Bayes X  risk. Theorem 1. Let H be a functional space that is dense in the set of continuous functions. Suppose that we have a positive sequence {  X  n } with  X  n  X  0 and n X  2 n / log n  X  X  X  . We define f  X  n as d Proof. Our theorem follows directly from (Steinwart, 2005, Corollary 3.15), since ` p  X  ,p + is regular (Steinwart, 2005, Definition 3.9). Besides mild regularity conditions that hold for ` p  X  ,p + , a loss function is said regular if, for every  X   X  [0 , 1] , and every t  X  such that we have that d p  X  ,p + ( t  X  , x ) agrees with d  X  ( x ) almost everywhere.
 t which is the desired result.
 desirable regarding sparsity, since sparseness does not occur when the conditional probabilities can be unambiguously estimated .
 Note on a Close Relative A double hinge loss function has been proposed recently with a dif-ferent perspective by Bartlett and Wegkamp (2008). Their formulation is restricted to symmetric classification, where c + = c  X  = 1 and r + = r  X  = r . In this situation, rejection may occur only if 0  X  r &lt; 1 / 2 , and the thresholds on the conditional probabilities in Bayes X  rule (4) are p  X  = 1  X  p + = r .
 For symmetric classification, the loss function of Bartlett and Wegkamp (2008) is a scaled version of our proposal that leads to equivalent solutions for f , but our decision rule differs. While our of Bartlett and Wegkamp (2008) has a free parameter (corresponding to the threshold f + =  X  f  X  ) whose value is set by optimizing a generalization bound.
 Our decision rule rejects more examples when the loss incurred by rejection is small and fewer examples otherwise. The two rules are identical for r ' 0 . 24 . We will see in Section 5 that this difference has noticeable outcomes. In this section, we show how the standard SVM optimization problem is modified when the hinge loss is replaced by the double hinge loss. The optimization problem is first written using a compact notation, and the dual problem is then derived. 4.1 Optimization Problem Minimizing the regularized empirical risk (6) with the double hinge loss (7 X 8) is an optimization problem akin to the standard SVM problem. Let C be an arbitrary constant, we define D = C ( p +  X  p ) , C i = C (1  X  p + ) for positive examples, and C i = Cp  X  for negative examples. With the introduction of slack variables  X  and  X  , the optimization problem can be stated as for negative examples t i = H ( p  X  ) /p  X  ,  X  i = ( H ( p  X  )  X  H ( p + )) / ( p  X   X  p + ) . optimization algorithms can be drawn from the dual formulation: problem under box constraints. Compared to the standard SVM dual problem, one has an additional vector to optimize, but, with the active set we developed, we only have to optimize a single vector of R n . The primal variables f and b are then derived from the Karush-Kuhn-Tucker (KKT) conditions. below. 4.2 Solving the Problem To solve (11), we use an active set algorithm, following a strategy that proved to be efficient in SimpleSVM (Vishwanathan et al., 2003). This algorithm solves the SVM training problem by a greedy approach, in which one solves a series of small problems. First, the repartition of training examples in support and non-support vectors is assumed to be known, and the training criterion is of examples in support and non-support vectors. These two steps are iterated until some level of accuracy is reached.
 Partitioning the Training Set The training set is partitioned into five subsets defined by the ac-tivity of the box constraints of Problem (11). The training examples indexed by: When example i belongs to one of the subsets described above, the KKT conditions yield that  X  i I , I C , I  X  and I D is known, we only have to consider a problem in  X  . Furthermore,  X  i has to be computed only for i  X  I t  X  I  X  . Updating Dual Variables Assuming a correct partition, Problem (11) reduces to the considerably smaller problem of computing  X  i for i  X  I T = I t  X  I  X  : P because we assumed the partition to be correct.
 The solution of Problem (12) is simply obtained by solving the following linear system resulting from the first-order optimality conditions: where  X  , which is the (unknown) Lagrange parameter associated to the equality constraint in (12), is computed along with  X  . Note that the | I T | equations of the linear system given on the first line primal variable b is equal to  X  .
 Algorithm The algorithm, described in Algorithm 1, simply alternates updates of the partition of standard SVMs, the initialization step consists in either using the solution obtained for a different hyper-parameter, such as a higher value of C , or in picking one or several examples of each class to Algorithm 1 SVM Training with a Reject Option input { x i , y i } 1  X  i  X  n and hyper-parameters C , p + , p  X  initialize  X  old I T = { I t , I  X  } , I T = { I 0 , I C , I D } , repeat until convergence output f , b .
 The exact convergence is obtained when all constraints are fulfilled, that is, when all examples be-long to the same subset at the begining and the end of the main loop. However, it is possible to relax the convergence criteria while having a good control on the precision on the solution by monitor-ing the duality gap, that is the difference between the primal and the dual objectives, respectively provided in the definition of Problems (10) and (11). Table 1: Performances in terms of average test loss, rejection rate and misclassification rate (re-jection is not an error) with r + = r  X  = 0 . 45 , for the three rejection methods over four different datasets. Theorem 2. Algorithm 1 converges in a finite number of steps to the exact solution of (11). Proof. The proof follows the ones used to prove the convergence of active set methods in general, and SimpleSVM in particular, see Propositon 1 in (Vishwanathan et al., 2003)). We compare the performances of three different rejection schemes based on SVMs. For this purpose, we selected the datasets from the UCI repository related to medical problems, as medical decision making is an application domain for which rejection is of primary importance. Since these datasets are small, we repeated 10 trials for each problem. Each trial consists in splitting randomly the examples into a training set with 80 % of examples and an independent test set. Note that the training examples were normalized to zero-mean and unit variance before cross-validation (test sets were of course rescaled accordingly).
 In a first series of experiments, to compare our decision rule with the one proposed by Bartlett and Wegkamp (2008) (B&amp;W X  X ), we used symmetric costs: c + = c  X  = 1 and r + = r  X  = r . We also chose r = 0 . 45 , which corresponds to rather low rejection rates, in order to favour different behaviors between these two decision rules (recall that they are identical for r ' 0 . 24 ). Besides the double hinge loss, we also implemented a  X  X aive X  method that consists in running the standard SVM algorithm (using the hinge loss) and selecting a symmetric rejection region around zero by cross-validation.
 For all methods, we used Gaussian kernels. Model selection is performed by cross-validation. This includes the selection of the kernel widths, the regularization parameter C for all methods and additionally of the rejection thresholds for the naive method. Note that B&amp;W X  X  and our decision rules are based on learning with the double-hinge loss. Hence, the results displayed in Table 1 only differ due to the size of the rejection region, and to the disparities that arise from the choice of hyper-parameters that may arise in the cross-validation process (since the decision rules differ, the cross-validation scores differ also).
 Table 1 summarizes the averaged performances over the 10 trials. Overall, all methods lead to equivalent average test losses, with an unsignificant but consistent advantage for our decision rule. We also see that the naive method tends to reject fewer test examples than the consistent methods. This means that, for comparable average losses, the decision rules based on the scores learned by minimizing the double hinge loss tend to classify more accurately the examples that are not rejected, as seen on the last column of the table.
 For noisy problems such as Liver and Pima, we observed that reducing rejection costs considerably decrease the error rate on classified examples (not shown on the table). The performances of the two learning methods based on the double-hinge get closer, and there is still no significant gain compared to the naive approach. Note however that the symmetric setting is favourable to the naive approach, since we only have to estimate a single decision thershold. We are experimenting to see whether the double-hinge loss shows more substantial improvements for asymmetric losses and for larger training sets. In this paper we proposed a new solution to the general problem of classification with a reject option. The double hinge loss was derived from the simple desiderata to obtain accurate estimates of posterior probabilities only in the vicinity of the decision boundaries. Our formulation handles asymmetric misclassification and rejection costs and compares favorably to the one of Bartlett and Wegkamp (2008).
 We showed that for suitable kernels, including usual ones such as the Gaussian kernel, training a kernel machine with the double hinge loss provides a universally consistent classifier with reject option. Furthermore, the loss provides sparse solutions, with a limited number of support vectors, similarly to the standard L1-SVM classifier.
 We presented what we believe to be the first principled and efficient implementation of SVMs for classification with a reject option. Our optimization scheme is based on an active set method, whose complexity compares to standard SVMs. The dimension of our quadratic program is bounded by the number of examples, and is effectively limited to the number of support vectors. The only computational overhead is brought by monitoring five categories of examples, instead of the three ones considered in standard SVMs (support vector, support at bound, inactive example). Our approach for deriving the double hinge loss can be used for other decision problems relying on conditional probabilities at specific values or in a limited range or values. As a first example, one may target the estimation of discretized confidence ratings, such as the ones reported in weather forecasts. Multi-category classification also belongs to this class of problems, since there, decisions rely on having precise conditional probabilities within a predefined interval.
 Acknowledgements This work was supported in part by the French national research agency (ANR) through project GD2GS, and by the IST Programme of the European Community through project DIRAC.

