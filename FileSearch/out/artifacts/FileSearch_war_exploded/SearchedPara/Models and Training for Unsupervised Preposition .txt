 Reliable disambiguation of words plays an impor-tant role in many NLP applications. Prepositions are ubiquitous X  X hey account for more than 10% of the 1.16m words in the Brown corpus X  X nd highly ambiguous. The Preposition Project (Litkowski and Hargraves, 2005) lists an average of 9.76 senses for each of the 34 most frequent English preposi-tions, while nouns usually have around two (Word-Net nouns average about 1.2 senses, 2.7 if monose-mous nouns are excluded (Fellbaum, 1998)). Dis-ambiguating prepositions is thus a challenging and interesting task in itself (as exemplified by the Sem-Eval 2007 task, (Litkowski and Hargraves, 2007)), and holds promise for NLP applications such as Information Extraction or Machine Translation. 1 Given a sentence such as the following: we ultimately want to be able to annotate it as Here, the preposition in has two distinct meanings, namely a temporal and a locative one. These mean-ings are context-dependent. Ultimately, we want to disambiguate prepositions not by and for them-selves, but in the context of sequential semantic la-beling. This should also improve disambiguation of the words linked by the prepositions (here, morn-ing , shopped , and Rome ). We propose using un-supervised methods in order to leverage unlabeled data, since, to our knowledge, there are no annotated data sets that include both preposition and argument senses. In this paper, we present our unsupervised framework and show results for preposition disam-biguation. We hope to present results for the joint disambiguation of preposition and arguments in a future paper.

The results from this work can be incorporated into a number of NLP problems, such as seman-tic tagging, which tries to assign not only syntac-tic, but also semantic categories to unlabeled text. Knowledge about semantic constraints of preposi-tional constructions would not only provide better label accuracy, but also aid in resolving preposi-tional attachment problems. Learning by Reading approaches (Mulkar-Mehta et al., 2010) also cru-cially depend on unsupervised techniques as the ones described here for textual enrichment.

Our contributions are:  X  we present the first unsupervised preposition  X  we compare the effectiveness of various models  X  we present ways to extend this work to prepo-A preposition p acts as a link between two words, h and o . The head word h (a noun, adjective, or verb) governs the preposition. In our example above, the head word is shopped . The object of the preposi-tional phrase (usually a noun) is denoted o , in our example morning and Rome . We will refer to h and o collectively as the prepositional arguments . The triple h,p,o forms a syntactically and semantically constrained structure. This structure is reflected in dependency parses as a common construction. In our example sentence above, the respective struc-tures would be shopped in morning and shopped in Rome . The senses of each element are denoted by a barred letter, i.e.,  X  p denotes the preposition sense,  X  denotes the sense of the head word, and  X  o the sense of the object. We use the data set for the SemEval 2007 PSD task, which consists of a training (16k) and a test set (8k) of sentences with sense-annotated preposi-tions following the sense inventory of The Preposi-tion Project , TPP (Litkowski and Hargraves, 2005). It defines senses for each of the 34 most frequent prepositions. There are on average 9.76 senses per preposition. This corpus was chosen as a starting point for our study since it allows a comparison with the original SemEval task. We plan to use larger amounts of additional training data.

We used an in-house dependency parser to extract the prepositional constructions from the data (e.g.,  X  X hop/VB in/IN Rome/NNP X ). Pronouns and num-bers are collapsed into  X  X RO X  and  X  X UM X , respec-tively.

In order to constrain the argument senses, we con-struct a dictionary that lists for each word all the possible lexicographer senses according to Word-Net. The set of lexicographer senses (45) is a higher level abstraction which is sufficiently coarse to allow for a good generalization. Unknown words are as-sumed to have all possible senses applicable to their respective word class (i.e. all noun senses for words labeled as nouns, etc). Figure 1: Graphical Models. a) 1 st order HMM. b) variant used in experiments (one model/preposition, thus no conditioning on p ). c) incorporates further constraints on variables
As shown by Hovy et al. (2010), preposition senses can be accurately disambiguated using only the head word and object of the PP. We exploit this property of prepositional constructions to represent the constraints between h , p , and o in a graphical model. We define a good model as one that reason-ably constrains the choices, but is still tractable in terms of the number of parameters being estimated.
As a starting point, we choose the standard first-order Hidden Markov Model as depicted in Figure 1a. Since we train a separate model for each preposi-tion, we can omit all arcs to p . This results in model 1b. The joint distribution over the network can thus be written as
We want to incorporate as much information as possible into the model to constrain the choices. In Figure 1c, we condition  X  p on both  X  h and  X  o , to reflect the fact that prepositions act as links and determine their sense mainly through context. In order to con-strain the object sense  X  o , we condition on  X  h , similar to a second-order HMM. The actual object o is con-ditioned on both  X  p and  X  o . The joint distribution is equal to Though we would like to also condition the prepo-sition sense  X  p on the head word h (i.e., an arc be-tween them in 1c) in order to capture idioms and fixed phrases, this would increase the number of pa-rameters prohibitively. The training method largely determines how well the resulting model explains the data. Ideally, the sense distribution found by the model matches the real one. Since most linguistic distributions are Zipfian, we want a training method that encourages sparsity in the model.

We briefly introduce different unsupervised train-ing methods and discuss their respective advantages and disadvantages. Unless specified otherwise, we initialized all models uniformly, and trained until the perplexity rate stopped increasing or a predefined number of iterations was reached. Note that MAP-EM and Bayesian Inference require tuning of some hyper-parameters on held-out data, and are thus not fully unsupervised. 5.1 EM We use the EM algorithm (Dempster et al., 1977) as a baseline. It is relatively easy to implement with ex-isting toolkits like Carmel (Graehl, 1997). However, EM has a tendency to assume equal importance for each parameter. It thus prefers  X  X eneral X  solutions, assigning part of the probability mass to unlikely states (Johnson, 2007). We ran EM on each model for 100 iterations, or until the perplexity stopped de-creasing below a threshold of 10  X  6 . 5.2 EM with Smoothing and Restarts In addition to the baseline, we ran 100 restarts with random initialization and smoothed the fractional counts by adding 0.1 before normalizing (Eisner, 2002). Smoothing helps to prevent overfitting. Re-peated random restarts help escape unfavorable ini-tializations that lead to local maxima. Carmel pro-vides options for both smoothing and restarts. 5.3 MAP-EM with L 0 Norm Since we want to encourage sparsity in our mod-els, we use the MDL-inspired technique intro-duced by Vaswani et al. (2010). Here, the goal is to increase the data likelihood while keeping the number of parameters small. The authors use a smoothed L 0 prior, which encourages probabil-ities to go down to 0 . The prior involves hyper-parameters  X  , which rewards sparsity, and  X  , which controls how close the approximation is to the true L 0 norm. 2 We perform a grid search to tune the hyper-parameters of the smoothed L 0 prior for ac-curacy on the preposition against , since it has a medium number of senses and instances. For HMM, we set  X  trans = 100.0,  X  trans = 0.005,  X  emit = 1.0,  X  emit = 0.75. The subscripts trans and emit de-note the transition and emission parameters. For our model, we set  X  trans = 70.0,  X  trans = 0.05,  X  emit = 110.0,  X  emit = 0.0025. The latter resulted in the best accuracy we achieved. 5.4 Bayesian Inference Instead of EM, we can use Bayesian inference with Gibbs sampling and Dirichlet priors (also known as the Chinese Restaurant Process, CRP). We follow the approach of Chiang et al. (2010), running Gibbs sampling for 10,000 iterations, with a burn-in pe-riod of 5,000, and carry out automatic run selec-tion over 10 random restarts. 3 Again, we tuned the hyper-parameters of our Dirichlet priors for accu-racy via a grid search over the model for the prepo-sition against . For both models, we set the concen-tration parameter  X  trans to 0.001, and  X  emit to 0.1. This encourages sparsity in the model and allows for a more nuanced explanation of the data by shifting probability mass to the few prominent classes. Given a sequence h,p,o , we want to find the se-quence of senses  X  h,  X  p,  X  o that maximizes the joint probability. Since unsupervised methods use the provided labels indiscriminately, we have to map the resulting predictions to the gold labels. The pre-dicted label sequence  X  h,  X  p,  X  o generated by the model via Viterbi decoding can then be compared to the true key. We use many-to-1 mapping as described by Johnson (2007) and used in other unsupervised tasks (Berg-Kirkpatrick et al., 2010), where each predicted sense is mapped to the gold label it most frequently occurs with in the test data. Success is measured by the percentage of accurate predictions. Here, we only evaluate  X  p .

The results presented in Table 1 were obtained on the SemEval test set. We report results both with and without against , since we tuned the hyper-parameters of two training methods on this preposi-tion. To test for significance, we use a two-tailed t -test, comparing the number of correctly labeled prepositions. As a baseline, we simply label all word types with the same sense, i.e., each preposition to-ken is labeled with its respective name. When using many-to-1 accuracy, this technique is equivalent to a most-frequent-sense baseline.

Vanilla EM does not improve significantly over the baseline with either model, all other methods do. Adding smoothing and random restarts increases the gain considerably, illustrating how important these techniques are for unsupervised training. We note that EM performs better with the less complex HMM.

CRP is somewhat surprisingly roughly equivalent to EM with smoothing and random restarts. Accu-racy might improve with more restarts.

MAP-EM with L 0 normalization produces the best result (56%), significantly outperforming the baseline at p &lt; . 001 . With more parameters (9.7k vs. 3.7k), which allow for a better modeling of the data, L 0 normalization helps by zeroing out in-frequent ones. However, the difference between our complex model and the best HMM (EM with smoothing and random restarts, 55%) is not signifi-cant.

The best (supervised) system in the SemEval task (Ye and Baldwin, 2007) reached 69% accuracy. The best current supervised system we are aware of (Hovy et al., 2010) reaches 84.8%. The semantics of prepositions were topic of a special issue of Computational Linguistics (Baldwin et al., 2009). Preposition sense disambiguation was one of the SemEval 2007 tasks (Litkowski and Hargraves, 2007), and was subsequently explored in a number of papers using supervised approaches: O X  X ara and Wiebe (2009) present a supervised preposition sense disambiguation approach which explores different settings; Tratz and Hovy (2009), Hovy et al. (2010) make explicit use of the arguments for preposition sense disambiguation, using various features. We differ from these approaches by using unsupervised methods and including argument labeling.

The constraints of prepositional constructions have been explored by Rudzicz and Mokhov (2003) and O X  X ara and Wiebe (2003) to annotate the se-mantic role of complete PPs with FrameNet and Penn Treebank categories. Ye and Baldwin (2006) explore the constraints of prepositional phrases for semantic role labeling. We plan to use the con-straints for argument disambiguation. We evaluate the influence of two different models (to represent constraints) and three unsupervised train-ing methods (to achieve sparse sense distributions) on PSD. Using MAP-EM with L 0 norm on our model, we achieve an accuracy of 56%. This is a significant improvement (at p &lt; .001) over the base-line and vanilla EM. We hope to shorten the gap to supervised systems with more unlabeled data. We also plan on training our models with EM with fea-tures (Berg-Kirkpatrick et al., 2010).

The advantage of our approach is that the models can be used to infer the senses of the prepositional arguments as well as the preposition. We are cur-rently annotating the data to produce a test set with Amazon X  X  Mechanical Turk, in order to measure la-bel accuracy for the preposition arguments.
 We would like to thank Steve DeNeefe, Jonathan Graehl, Victoria Fossum, and Kevin Knight, as well as the anonymous reviewers for helpful comments on how to improve the paper. We would also like to thank Morgan from Curious Palate for letting us write there. Research supported in part by Air Force Contract FA8750-09-C-0172 under the DARPA Ma-chine Reading Program and by DARPA under con-tract DOI-NBC N10AP20031.

