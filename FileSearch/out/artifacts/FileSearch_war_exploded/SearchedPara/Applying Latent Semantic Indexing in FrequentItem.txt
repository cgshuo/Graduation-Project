
Thanaruk Theeramunkong 1 , Kritsada Sriphaew 1 , 2 , and Manabu Okumura 2 Fast increasing of research publication has caused the difficulty for researchers to grasp movement or change in their area of interest. Such information overload becomes serious hindrance for researchers to position their own works against existing ones, or to find useful relations (or connections) among them. Although the publication of each work may include a list of related articles (documents) as its reference (called citation), it is still impossible to include all related works due to either intentional reasons (e.g., limitation of paper length) or unintentional reasons (e.g., na  X   X vely unknown). Enormous meaningful connections that perme-ate the literatures may remain hidden. Recently, there have been two different approaches to find relations among research documents. As the first approach, the citation-based method uses expansion of bibliography or citation informa-tion in scientific publication to find indirect relations, including measurement of impact factor [1], characterization of the citation [2], support of browsing citation graph [3] and so forth. For the task of relation discovery, two basic properties of citation, called bibligraphic coupling [4] and co-citation [5], can be focused. Those previous works stated that any two documents tend to have relation with each other if they are citing to one or more documents in common (bibliographic coupling) or they are both cited by one or more documents in common (co-citation). As the second approach, the word-or term-based method exploits words or terms in a document a s potential clues to detect relations between the document and other related documents. This method (later called word-based approach) discovers a set of documents with similar contents (topics) using either word co-occurrences or shared vocabularies, such as done in infor-mation retrieval, text categorization and text clustering. However, the process to find relations among two documents is computationally expensive since all combinations need to be co nsidered for any possible relation [6]. Towards this problem, some recent works [7,8] have applied association rule mining (ARM) techniques to find n-ary document relations where a support can be set to avoid exploring all document combinations. Even such works could achieve discovery of high-quality relations to some extents, they still have some limitations due to direct use of words and terms in documents.

In this paper, we propose a method to apply latent semantic indexing in the process of discovering hidden relations among two documents. Two main objectives are (1) to study how well the word-based approach with different weighting (tf and tfidf) performs in finding relations among documents using ARM techniques, and (2) to study how much latent semantic indexing improves the conventional approach in finding useful hidden relations. In the past, association rule mining (ARM) and frequent itemset mining (FIM) was known as a process to find co-occurren ces (frequent patterns) in a database. In general, the conventional transactional database is presented in the term of item existences in the transaction. Although most ARM works deal with a this kind of databases, there are some attempts to extend the original framework to be able to assign the weights for items or transactions in the database, called weighted association rule mining [9]. In those works, items or transactions are independently weighted regarding to which type of discovered rules we would like to find. The higher weighted items or transactions will obtain higher priority for user interests. However, this approach gives a fixed weight to each item regardless of the transaction such item occurs. Unlike those works, our approach utilizes the term-document orientations, where the discovered frequent itemset is a set of documents which share a large number of terms as done in [7,8]. Note that a transaction corresponds to a term while an item corresponds to a document. Therefore, a  X  X ocset X  (document set) is used in place of the term  X  X temset X  in the traditional FIM approaches. The discovered results can be assumed as a term-based relation among documents where th e relation is introduced by coincident terms. In Figure 1, two examples of the real-valued databases are defined in the form of well-known vector space model (VSM). The left part indicates how often a term occurs in each document (called t erm frequency -tf) while the right part shows term frequency multiplied by the inverse document frequency (tfidf).
Traditionally, the support of a docset is defined by a ratio between the number of terms that exist in all documents in the docset and the total number of distinct terms in a database. To expand this concept to a real-valued database, the definition of support is generalized as follows. Let D be a set of documents (items) where D = { d 1 ,d 2 , ..., d m } ,and T be a set of terms (transactions) where T d . A subset of D is called a docset whereas a subset of T is called a termset. Furthermore, a docset X k = { x 1 ,x 2 , ..., x k } X  X  with k documentsiscalled k -docset. The support of X k is defined as follows.
By representing the data to be mined as shown in Figure 1, the new definition of support employs the min operation to find the weight of each term for a docset by selecting a minimum weight of such term among all documents in the docset. The max operation is applied for finding the maximum weight of each term in the database. The suppor t of a docset will then be calculated from the ratio between the sum of all term weights for a docset and the sum of maximum weights of all terms in the database. While this definition can be applied for general real-valued databases, it also can used for the traditional FIM on boolean-valued databases with the same result. An example of docsets and their supports, for tf and tfidf databases, can be computed as shown in Figure 2. Besides support, a so-called confidence is used for generating confident association rules. Here, the confidence is left since it is out of scope in this work. Note that similar to conventional ARM, these generalized supports preserve two closure properties, i.e., downward closure property ( X  X ll subsets of a frequent itemset are also frequent X ), and upward closure property ( X  X ll supersets of an infrequent itemset are also infrequent X ). For example, sup ( d 1 )  X  sup ( d 1 d 2 )and sup ( d 2 )  X  sup ( d 1 d 2 ). The mathematical proof can be found in [8]. To represent document representation, term weighting can be performed to set importance level of a term in a document. This work uses two most common non-binary weightings: term-frequency (tf) and term-frequency-inverse-document-frequency (tfidf). Moreover, latent semantic indexing is applied to reveal hidden meaning in a document or a query. In this latent semantic space, a query and a document may have high cosine similarity even if they do not share any common words or terms but their terms are seman tically similar. Applied the concept of Singular Value Decomposition (SVD), LSI can also be viewed as a method for dimensionality reduction by a least-squared method [10]. SVD (also LSI) translates an input matrix A and represents it as A in a lower dimensional space such that the  X  X istance X  between the two matrices as measured by minimizing the 2-norm (Euclidean distance), || A  X  A || 2 . It is possible to project an n-dimensional space of word-document matrices onto a k-dimensional space where n is the number of word types in the collection and k is relatively very small compared to n, say 100 and 150. The SVD projection is done by decomposing a document-by-term matrix A t  X  d into the product of three matrices, T t  X  n , S n  X  n and D d  X  n as follows.
 Here, t is the number of terms, d is the number of documents, n =min( t, d ), T and D have orthonormal columns, i.e. T  X  T T = I and D T  X  D = I ,and S is a diagonal matrix, where si, j =0for i = j . Moreover, in some situations rank ( A )= r where r  X  n . In these situations, the diagonal elements of S are  X  , X  2 , ...,  X  n where  X  i &gt; 0for1  X  i  X  r and  X  i =0for r&lt;i  X  n . For details of how to derive T t  X  n , S n  X  n and D d  X  n , can be found in [10]. In this work, we investigate the best combination of the four schemes. To evaluate the result, we introduce an automatic evaluation where citation graph is used to evaluate our system based on its ability to find the relations that exist in the citation graph. Although human judgment is the best method for evaluation, it is a labor-intensive and time-consuming task. To do this, a citation graph is applied. Conceptually citations among documents in scientific publication collection form a citation graph, where a node corresponds to a document and an arc corresponds to a dir ect citation of a document to another document. Based on this citation graph, an indirect citation can be defined using the concept of transitivity. The formulation of direct and indirect citations can be given in the terms of the u -th order citation and the v -th order accumulative citation matrix as follows.
 Definition 1 ( the u -th order citation ) . For x, y  X  X  , y is the u -th order citation of x iff the number of arcs in the shortest path between x to y in the citation graph is u (  X  1). Conversely, x is called the u -th order citation of y . Definition 2 ( the v -th order accumulative citation matrix ) . Given a set of n distinct documents, the v -th order accumulative citation matrix (for short, v -OACM) is an n  X  n matrix, each element of which represents the citation relation  X  v between two documents x , y where  X  v ( x, y )=1when x is the u -th order citation of y and u  X  v ,otherwise  X  v ( x, y ) = 0. Note that  X  v ( x, y )=  X  ( y, x )and  X  v ( x, x )=1.

For example, given a set of six documents d 1 ,d 2 ,d 3 ,d 4 ,d 5 ,d 6  X  X  and a set of d 3 and d 5 is the second, d 4 is the third, and d 6 is the fourth order citations of the document d 1 . The 1-, 2-and 3-OACMs can be created as shown in Figure 3. The 1-OACM can be straightforwardly constructed from the set of the first-order citation (direct citation). The ( v + 1)-OACM (mathematically denoted by amatrix A v +1 ) can be recursively created from the operation between v -OACM ( A v )and1-OACM( A 1 ) according to the following formula. where  X  is an OR operator,  X  is an AND operator, a v ik is the element at the i -th row and k -th column of the matrix A v and a 1 kj is the element at the k -th row and j -thcolumnofthematrix A 1 . Here, a v -OACM is a symmetric matrix.
The shorter the specific range is, the more restrict the evaluation is. With the concept of v -OACM stated in the previous sect ion, we can realize this general-ized evaluation by a so-called v -th order validity (for short, v -validity ), where v corresponds to the specific range menti oned above. The formulation of the v -validity of a docset X ( X  X  X  ), denoted by S v ( X ), is defined as follows. Here,  X  v ( x, y ) is the citation relation defined by Definition 2. In the equation, we can observe that the v -validity of a docset is ranging from 0 to 1, i.e., 0  X  S ( X )  X  1. The v -validity achieves the minimum (i.e., 0) when there is no citation relation among any document in the docset. On the other hand, it achieves the maximum (i.e., 1) when th ere is at least one document that has a citation relation with all documents in a docset. Intuitively, the validity of a bigger docset tends to have lower validity than a smaller one. Moreover, given a set of discovered docsets F ,its v -validity (later called set v -validity )), denoted by S v ( F ),canbedefinedasfollows.
 where w X is the weight of a docset ( X ). In this work, w X is set to | X | X  1, the maximum value that the validity of a docset X can gain. For example, given the 1-OACM in Figure 3 and F = { d 1 d 2 ,d 1 d 2 d 3 } , the set 1-validity of F (i.e., A set of experiments are made to investi gate how efficiently universal frequent itemset mining helps in discovering document relation among scientific research publications. In this work, an evaluation material is constructed from a collection of scientific research publications in the ACM Digital Library 1 . This dataset was originally used in [7]. As a seed of evaluation dataset, 200 publications are re-trieved from each of the three computer-r elated classes, coded by B (Hardware), E (Data) and J (Computer) classes. Then the publications referred by these newly collected publications are also gathered and appended into the dataset. In total there are 10,817 publications collected as the evaluation material and used to generate citation graph under 1-OACM. As the result, only 36,626 citation edges are remained with an average of 7 citations (including both cite to and cited from other publications) per publication. For mining, we applied FP-tree algorithm, originally introduced in [11] and used the BOW library [12] as a tool for constructing an attribute-value database. The 524 stopwords and terms with very low frequency (less than 3 times) are omitted. Table 1 shows the validity of discovered document relations when either tf or tfidf are considered and LSI is applied with a thresholds of either 0.5, 0.7 or 1.0.

From the result shown in Table 1, some interesting characteristics can be observed. First, in most cases of the original space (w/o LSI), tfidf performs better than tf even there are few exceptions. The result implies that tfidf helps us obtain good representation for docum ent relation discovery. Moreover, the result of 1-OACM becomes lower when N increases. This implies that better relations are located at higher ranks. In addition, with a higher-OACM, the method can achieve up to 90-100 % validity and has the same trend that the validity drops when N increases. Second, for both tf and tfidf, the 1-OACM performance of discovering document relations improves from 14.29 % to around 40 % for top-1000 documents when LSI is applied. Focusing on the 2-OACM and 3-OACM performance, LSI is helpful to improve the validity of the discovered relations, especially for the cases of t f. In the cases of tfidf, LSI is helpful to improve validity of discovered document relations especially in the case of the 1-OACM. However, it is not useful for the 2-OACM and 3-OACM performance. This implies that LSI is helpful to increase the performance of discovering direct citations but not indirect citations. One implication is that the tfidf seems to be a good representation. Third, a stronger LSI (LSI with a higher threshold) performs better than a softer LSI (LSI with a lower threshold). This implies that LSI is useful to grasp the semantics of documents and then help increasing the discovery performance. This work presents a new approach to dis cover document relations using asso-ciation rule mining techniques with latent semantic indexing. Extended from the conventional frequent itemset mining , a so-called genera lized support is pro-posed. The generalized support can serv e a mining process of frequent itemsets from an attribute-value database where the values are weighted by real values, instead of boolean values as done in conventional methods. The quality of dis-covered document relations is measured under the concepts of the u -th order citation and the v -th order accumulative citati on matrix. By experiments, we found out that tfidf seems better than tf and latent semantic indexing is helpful in discovering meaningful document rel ations. As future works, it is necessary to explore other suitable term weightings and normalization techniques. More explorations are needed for d ifferent data collections.
 This work has been supported by Thailand Research Fund under project number BRG50800013. Th is work was also supported by NECTEC under project number NT-B-22-I4-38-49-05 and Royal Golden Jubilee (RGJ) Ph.D. program of the Thailand Research Fund (TRF).

