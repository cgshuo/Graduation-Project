
School of Computer Science, McGill University, Montreal, Canada Regularization is critical to controlling the complexity of hypothesis spaces and achieving favourable bias-variance trade-offs. Some machine learning methods even owe most of their success to an effective use of regularization. For ex-ample, a major reason for the success of SVMs is arguably their use of regularization that is  X  X atural X  in the RKHS tied to their kernel. For some choices of kernels, this Tikhonov-like regularization has a smoothness-inducing interpreta-tion (Sch  X  olkopf &amp; Smola, 2002). For example, the RKHS norm induced by the popular Gaussian RBF kernel penal-izes all orders of derivatives of the learned function (Yuille &amp; Grzywacz, 1988). Spline-based methods, which are ubiquitous in statistics but less common in machine learn-ing, also rely on smoothness-inducing, derivative-based penalties. In particular, for univariate inputs or addi-tive models, a second-order derivative penalty can be ap-plied exactly in the nonparametric setting, leading to cubic smoothing splines (Wahba, 1990). But, this exact penalty quickly becomes intractable as the training set grows or the order of modeled interactions increases. While attempts have been made to produce computationally-efficient ap-proximations of spline-like penalties (Eilers &amp; Marx, 1996; Wood, 2003), full spline-based methods generally scale un-favorably.
 In this paper, we introduce a method for efficiently approx-imating a general class of derivative-based penalties, which we call Sample-based Approximate Regularization (SAR). This approach applies to hypothesis spaces that are linearly parameterized, i.e., in which the input x is transformed into a feature space  X  ( x ) and the output is approximated by  X  ( x ) &gt; w . This type of hypothesis space includes SVM-style approximators, feedforward neural networks, and var-ious other types of regressors using features that are useful for particular application domains, such as SIFT, MFCC, etc (Lowe, 1999; Davis &amp; Mermelstein, 1980). Based on the success of derivative-based penalties in the related RKHS and spline settings (Pearce &amp; Wand, 2006), and on the empirical success of problem-specific features, it is desirable to obtain derivative-based regularizers that work with a wide range of feature transformations, e.g., ones not restricted to explicitly-computable RKHS kernels. The SAR method can be used to augment the standard l and l 1 regularizers that are commonly used with  X  X en-eral X  feature transforms (i.e., non-kernel transforms). Con-veniently, the regularizers produced by SAR are of the Tikhonov-type (i.e., J ( f w ) = w &gt;  X  w for some  X  ), and can thus be applied efficiently with standard software. The computational complexity of SAR depends only loosely on the complexity of  X  and not at all on the size of the train-ing set, thus improving on costs of spline-based approaches to regularizing derivatives. We prove that the regularizers produced by SAR converge efficiently to the exact penal-ties they approximate. We also compare the loss of a SAR-regularized regression estimator to the loss of an estimator shaped by the exact regularizer approximated by SAR. In the rest of this paper, we present our generalization of smoothness-inducing, derivative-based regularizers (Sec-tion 2), present our approach for approximating them effi-ciently (Section 3), analyze its theoretical properties (Sec-tion 4), and present empirical results illustrating the power of the proposed approach (Section 5).
 Consider a function f : X  X  R and a measure  X   X  X  ( X ) . If f 0 ( x ) exists and is L 2 (  X  ) -integrable, then for X = natural measure of the smoothness of f is: One typically extends (1) to multi-dimensional domains X = R d by integrating the squared norm of the gradient. We propose instead a more general form, expressible as: The inner integral is over the surface S x of a hyper-sphere centred at x , according to a location-dependent measure over directions s x  X  M ( S x ) . Each s  X  S x corresponds to a unit-length vector pointing away from x in some direc-tion. If s x is set to a uniform distribution over the unit hyper-sphere for all x  X  X , then J 1 ( f ) is proportional to the integrated squared norm of  X  x f , due to the linear-ity of the dot-product. If s x is the set of delta functions on the coordinate vectors of X , J 1 ( f ) penalizes the inte-grated squared norm of  X  x f exactly, as in the typical multi-dimensional extension of (1). But, the generalized deriva-tive penalty J 1 ( f ) allows flexibility in assigning location-dependent importance to the variation of f along particu-lar directions. Moreover, as we will see, it is amenable to sample-based approximation.
 Another reasonable measure of the smoothness of f uses its second-order derivatives. In one dimension, if f 00 ( x ) exists and is L 2 (  X  ) -integrable, then gives the standard penalty used in, e.g., cubic smoothing splines (Wahba, 1990). We extend the penalty in (3) to multiple dimensions as follows: where s  X  S x are again distinct unit-length vectors jointly covering all directions pointing away from x , and H x f is the Hessian of f evaluated at x . When s x is uniform over S , J 2 ( f ) penalizes the squared Frobenius norm || H x f || w.r.t.  X  , which provides regularization that has proven use-ful in previous work (Rifai et al., 2011; Kim et al., 2009). As with (2), the generalized form in (4) encompasses a broad range of regularizers, due to flexibility in the choice of  X  and s x , and is amenable to approximation. The goal of our approach is to efficiently approximate reg-ularizers of the form (2) and (4). The functionals J 1 and J Algorithm 1 SAR(  X  p x ,  X  p s x , N ,  X  , i , ) 1:  X   X  i := zero matrix of size p  X  p . 2: for j = 1 to N do 3: Sample X j from  X  p x . 4: Sample S j from  X  p s X 5: Compute  X  i ( X j ,S j , X  ) (see: (9) for defn.) 6:  X   X  i :=  X   X  i +  X  i ( X j ,S j , X  )  X  i ( X j ,S j , X  ) 7: end for 8: return 1 N  X   X  i . both involve integrating some quantity w.r.t.  X  and s x . SAR approximates the integrands in J 1 and J 2 efficiently using finite-difference approximations of directional derivatives and estimates the integrals using a Monte-Carlo approach based on samples from  X  and s x . We call methods to sam-ple from  X  point samplers and methods to sample from s x direction samplers .
 We focus on linearly-parameterized functions f w ( x ) =  X  ( x ) &gt; w , where  X  : X  X  R p is a fixed feature transform, whose components are one-dimensional measurable func-tions {  X  i } p i =1 , and w  X  R p is a parameter vector. We de-note the function space defined by the span of  X  as F . Given a point sampler  X  p x , a direction sampler  X  p s size N , and a derivative order i , Algorithm 1 produces a matrix  X   X  i that defines SAR with:  X  J i ( f w ) = w &gt;  X  functions f w  X  F . To simultaneously regularize multiple derivative orders, their corresponding  X   X  i can be combined via element-wise summation.
 Once an approximate regularizer  X   X  i has been produced by SAR, any method for estimating Tikhonov-regularized lin-ear models can be applied. The computational cost of SAR comes from lines 3-6 of Algorithm 1. Assuming efficient line 5 and the outer products in line 6 dominate the cost of SAR. If the expected cost of computing  X  ( x ) is c  X  , the tar-get derivative order is i , and  X  ( x )  X  R p , then line 5 costs c ( i + 1) per sample and line 6 costs p 2 per sample. Lines 3-6 each execute N times when using N samples to com-pute  X   X  i . Depending on c  X  and p , either line 5 or 6 may dominate the overall cost. The discussion section further considers computation costs.
 We now describe approaches for approximating the direc-tional derivatives and for constructing samplers  X  p x /  X  p 3.1. Approximating directional derivatives For functions f w  X  F , the first-order forward finite differ-ence is given by 1 w &gt; (  X  ( x + s )  X   X  ( x )) , thus: in which we introduce the first-order finite difference vector  X  ( x,s, X  ) , 1 (  X  ( x + s )  X   X  ( x )) .
 For a point x and direction s , the term s &gt; ( H x f ) s in (4) is equivalent to the second-order directional derivative of f , at x , in direction s , with finite difference approximation: For f w  X  X  , the square of this term is given by: in which we use the second-order finite difference vector  X  ( x,s, X  ) , 1 2 (  X  ( x )  X  2  X  ( x + s ) +  X  ( x + 2 s )) . Based on (5) and (7), the key to SAR is that the integrals in J 1 and J 2 can be approximated by: in which we use finite difference vectors and i  X  { 1 , 2 } indicates the derivative order to regularize.
 To regularize higher-order derivatives with SAR, only the finite difference vectors used in (8) need to change: When regularizing a single order i with fixed , the denom-inator i in (9) can be ignored, as it is constant for all  X  In this case, numerical precision (for i  X  0 ) is not an is-sue. A similar idea can be applied when regularizing across multiple orders. Principled approaches to select and min-imize the side-effects from finite precision are subject for future work. We note that we have not run into any numer-ical problems in the experiments. 3.2. Sampling from  X  and s x We now describe concrete methods to sample from  X  and s . Suppose we are given a set D n = { X 1 ,X 2 ,...,X n } of  X  X raining X  input observations X i  X  R d , drawn from the source distribution p ( x ) . We will approximate  X  using N samples, contained in a set D 0 N . 1 A natural choice for the sampler is to draw values from an approximation to p ( x ) obtained by perturbing the existing points D n , an approach based on the manifold/cluster assumption underlying most work on semi-supervised learning. Let L be a distribu-tion over lengths, which determines the degree of  X  X mooth-ing X  to apply during sampling. Algorithm 2 samples from the empirical approximation to p ( x ) , convolved with the isotropic distribution with length distribution L . Algorithm 2 FuzzyPointSampler( D n , N , L ). 1: for j = 1 to N do 2: Sample X j from D n uniformly at random. 3: Sample a direction S j uniformly at random. 4: Sample a perturbation length j from L . 5: Add  X  X j = X j + j S j to D 0 N . 6: end for 7: return D 0 N .
 Algorithm 3 BlurryBoxSampler( D n , N , L ) 1: Compute the minimal bounding box for the D n . 2: for j = 1 to N do 3: Sample X j uniformly from within the box. 4: Sample a direction S j uniformly at random. 5: Sample a step length j from L . 6: Add  X  X j = X j + j S j to D 0 N . 7: end for 8: return D 0 N .
 The second method samples approximately from the uni-form distribution over X , to mimic the distribution implicit in the RKHS regularization accompanying Gaussian RBFs. Algorithm 3 samples from a uniform distribution over the smallest axis-aligned box enclosing D n convolved with the isotropic distribution with length distribution L . We propose two methods to sample directions from s x . The first is to sample a unit direction uniformly at random. The second is to sample a unit direction uniformly at random, transform it by some matrix, and then rescale it to unit length. The first method produces a regularizer that pe-nalizes derivatives in all directions equally, and the second biases the penalty based on the eigenvectors and eigenval-ues of the transform matrix. Developing direction samplers with location-dependent biases, e.g., to emphasize invari-ance w.r.t. small translations/rotations in an object recogni-tion task, is an interesting topic for future work. The goal of this section is twofold. First, we study the be-haviour of a SAR-based regularized least-squares regres-sion estimator (Theorem 1). Second, we focus on the con-vergence behaviour of the sample-based approximate reg-results, one in the form of the supremum of the empir-ical process (Theorem 2) and the other in the form of the supremum of the modulus of continuity of the empir-ical process (Theorem 3). For simplicity, we only study the 1 st -order derivative-based regularizer and its central difference-based SAR.
 Let us first define some notations. The gradient of a function f : R d  X  R is denoted by  X  f ( x ) . We denote the central difference approximation of the gra-dient by ( 4  X  f )( x ) = [( 4  X  f ) 1 ( x )  X  X  X  ( 4 ordinate vectors.
 Given a probability distribution  X   X  M ( X ) , the 1 -order derivative-based regularizer 2 is J ( f ) = R k X  f ( x ) k 2 d  X  ( x ) . Given D 0 with X 0 i i.i.d.  X   X  , we define the sample-based approxi-mate regularizer as:  X  J N ( f ) = 1 N P N i =1 k4  X  We also define J N ( f ) = 1 N P N i =1 k X  f ( X 0 i ) k that for f w  X  F , we have J ( f w ) = w &gt;  X  w with Similarly, we have  X  J N ( f w ) = w &gt;  X   X  N w with the approximate empirical Grammian  X   X  N , L &gt; 0 , the truncation operator  X  L : F  X  F is de-fined as (  X  L f )( x ) , f ( x ) when | f ( x ) |  X  L and (  X  L f )( x ) , sgn f ( x ) L otherwise.
 The regression setup is as follows. Let D n = the probability distribution generating the data is such that | Y |  X  L (almost surely) with L &gt; 0 . Denote by f ( x ) = E [ Y | X = x ] the regression function, which in general does not belong to F . Given D n and an indepen-dent dataset D 0 N , the SAR-based regression estimator  X  defined as the L -truncated estimator  X  f n ,  X  L  X  f n , with We now provide an upper bound on the performance of this estimator. To state our result, for k  X  1 , we define If the k -th partial derivatives are not defined, we set D k (  X  ) =  X  . For our results, we require the existence of D 1 (  X  ) and D 3 (  X  ) . All proofs are deferred to the Supple-mentary Material. Theorem 1. Assume that all {  X  i } p i =1 are three-time dif-ferentiable and sup x  X  X  k  X  ( x ) k 2  X  R . Moreover, suppose that  X  min (  X   X  N ) , the smallest eigenvalue of  X   X  N away from zero. There exist constants c 1 ,c 2 &gt; 0 such that for any fixed  X  &gt; 0 , with probability at least 1  X   X  , we have: This result shows the effects of function approximation and estimation errors, the way regularization coefficient  X  and J ( f w ) determine their tradeoff, and the error caused by SAR. The term min w  X  R p R | f w  X  f  X  ( x ) | 2 d  X  ( x ) +  X J ( f is the [regularized] approximation error and indicates how well the target function f can be approximated in a subset of F . The subset is determined by the true regularization functional J ( f w ) = w &gt;  X  w and  X  . As usual in regular-ized estimators, increasing  X  might increase the approxi-mation error, but it decreases the estimation error O ( log( n ) on the other hand, and vice versa. If F as defined by the basis functions  X  X atches X  the target function (i.e., f  X  can be well-approximated with a function in F with a small J ( f ) ), we can learn the target function fast. This is how feature-engineering or data-dependent feature generation show their benefits. It is noticeable that this result does not depend on the dimension of the feature space p . Results similar to this part of the theorem are known in the supervised learning literature, cf. Theorem 21.1 of Gy  X  orfi et al. (2002) for regularized regression in C k ( R ) (splines), Theorem 7.23 of Steinwart &amp; Christmann (2008) for reg-ularized loss in an RKHS, and Sridharan et al. (2009) for strongly convex objectives (which is satisfied for a convex loss and the l 2 regularizer) and linear function spaces. The effect of using  X  J N ( f ) instead of the true regularizer curious observation here is that the effect depends on the size of w , so if the true function can be well-approximated by a  X  X imple X  function (measured according to k w k 2 ), we would not suffer much from the error caused by SAR. To better understand the behaviour of the bound, consider the case that J ( f w ) = k w k 2 2 and the target function f longs to F , i.e., f  X  = f w  X  for some w  X  . Ignoring the con-stants and the logarithmic terms, by choosing  X  = 1 k w  X  k to optimize the tradeoff between  X J ( f  X  w ) and 1 n X  , we get the upper bound of O ( k w  X  k 2  X  n [1 + 1 N +  X  2 ]) . Remark 1 . One could get R | f w  X  f  X  ( x ) | 2 d  X  ( x ) +  X J ( f inside the minimizer instead of the current one, which has a multiplicative constant of 2 , at the price of having whether we use Bernstein X  X  inequality or Hoeffding X  X  in-equality in the proofs.
 Remark 2 . The quantity  X  min (  X   X  N ) in the theorem is a ran-dom function of D 0 N and can be calculated given D 0 N . We now depart from the context of regression and focus on the SAR procedure itself. The first result is a uniform upper bound on the difference between J ( f ) and  X  J N ( f ) for any function f  X  F B ,  X  &gt; w : w  X  R p , k w k 2  X  B , i.e., the ball with radius B w.r.t. the l 2 -norm of w . Theorem 2 (Supremum of the Empirical Process |  X  J
N ( f )  X  J ( f ) | ) . Assume that all {  X  i } differentiable. For any fixed  X  &gt; 0 and B &gt; 0 , we have: with probability at least 1  X   X  .
 This theorem shows the effects of the estimation error and the finite difference approximation error. The simplified behaviour of the estimation error is O ( B 2 p p N ) . The de-pendence on N and p is common to the usual uniform de-viation bounds in statistical learning for functions from a p -dimensional linear vector space. The effect of the size of the function space also manifests itself through B 2 . The effect of the finite difference approximation error is O ( B 2  X  2 )  X  neglecting terms depending on the smoothness of the basis functions. The  X  2 dependence is the usual residual error from the central difference approximation of a derivative. If instead we used a forward (or backward) estimate of the derivative, we would get  X  behaviour. The dependence on B is because functions  X  &gt; w with larger k w k 2 might have a larger derivatives, so their finite differ-ence approximation would have a larger residual error. Theorem 2 provides an upper bound for the supremum of the empirical process only over a subset F B of F , but it does not provide a non-trivial result for the supremum of |  X  J
N ( f )  X  J ( f ) | over F . This is expected as for large w , the true regularizer J ( f w ) would be large too, and the devi-ation of  X  J N ( f w ) around it can also be large. Nonetheless, we can still study the behaviour of the empirical process as a function of J ( f ) . This is known as the modulus of continuity result in the empirical process theory (or rela-tive deviation of error). The following theorem provides such a result. Here we denote a  X  b = max { a,b } . Theorem 3 (Modulus of Continuity for the Empirical Pro-cess |  X  J N ( f )  X  J ( f ) | ) . Assume that all {  X  i time differentiable. Suppose that  X  min ( X ) , the smallest eigenvalue of  X  , is bounded away from zero. W.l.o.g., as-sume that 256 dD 2 1 (  X  )  X  1 . Let  X  &gt; 0 . There exists c such that for any fixed  X  &gt; 0 , we have sup with probability at least 1  X   X  . Here c 1 (  X  ) can be chosen as follows: For 0 &lt;  X   X  1 4 e log(2)  X  0 . 1327 , c 1 of Lambert W -function), and c 1 (  X  ) = 16 otherwise. We can elucidate this result by seeing how it works in the context of Theorem 2, by restricting F to F B . In this case, J ( f )  X  O ( B 2 ) , so we get sup f  X  X  c c major difference is in the exponent of B . When  X  goes to zero, B 2(1+  X  ) decreases, but the term c 1 (  X  ) inside the logarithm increases. As can be seen from the definition of c (  X  ) , when  X   X  0 , c 1 (  X  ) blows up. Overall, even though Theorem 2 provides a slightly tighter upper bound on the error for F B , Theorem 3 can be considered a stronger result as it holds for all functions in F .
 Remark 3 . The effect of the input space dimension d on SAR X  X  statistical properties, as can be seen in all results, is quite mild, and only appears in constants. SAR X  X  sampling is a typical Monte Carlo integration, for which convergence rate is dimension-independent. The minor effect of d is due to using finite differences and the way we have defined D Finally it is worth mentioning that in the manifold regular-ization literature, there are results similar to Theorem 2. In particular, they provide conditions that the error between the various variants of the graph Laplacian-based and the Laplace-Beltrami-based regularizers goes to zero. For ex-ample, Bousquet et al. (2004) proved the asymptotic con-vergence for a fixed function. This should be compared to our much stronger uniform convergence rate over a func-tion class  X  albeit the regularizers are different. Belkin &amp; Niyogi (2008) showed the asymptotic uniform convergence over a class of functions, but did not provide a convergence rate. Hein (2006) extended that result and provided a con-vergence rate over a subset of H  X  older-continuous functions. In contrast to Theorem 1, the results in those papers did not consider the effect of error in regularization to the estima-tor (e.g., classifier or regression estimator), though Hein (2006) mentioned that his result could be used to prove consistency for algorithms that use graph Laplacian-based regularizers. This would be similar to using Theorem 2 to prove error bounds, which is a path that we did not take. In the different context of transductive learning (or semi-supervised learning on graphs), Belkin et al. (2004) provided a generalization error result for regularized algo-rithms on graphs, with a graph Laplacian-based regularizer being one possible choice, using tools from algorithmic sta-bility. None of these papers provides a modulus of conti-nuity result similar to Theorem 3. Our first tests involved least-squares regression with inputs x  X  R and outputs y  X  R . The data distribution was de-signed to emphasize SAR X  X  ability to regularize heteroge-nous basis functions. This contrasts with standard RKHS regularization, which uses more restricted collections. The joint distribution over ( x,y ) was set so four cycles of a sin wave occurred over the input domain, each with a wave-length 2 . 5 times longer than the previous one. The wave amplitude was scaled linearly from 1 to 2 over the input do-main. The density of x was set so the expected number of observations seen for each cycle was the same. The train-ing y values were corrupted by zero-mean Gaussian noise with standard deviation scaling linearly from 0 . 2 to 0 . 4 over the input domain. Performance was measured using uncor-rupted y values. We call this distribution SynthSin1d. The smooth sinusoid underlying SynthSin1d seems amenable to RKHS-regularized RBF regression, but causes problems due to large changes in the length scale of use-ful correlations over the input domain. When restricted to fixed bandwidth RBFs, the RKHS approach will always un-derperform on some part of the function not suited to the chosen bandwidth, as shown by results in Figure 1a. Using SynthSin1d, we compared the performance of SAR2 regularization with L2 regularization and RKHS regular-ization of Gaussian RBFs. SAR2 and L2 regularization were applied to four RBFs anchored at each training point, with bandwidths  X   X  { 2 , 4 , 8 , 16 } . RKHS regularization was applied independently at each bandwidth, using the same RBFs, i.e., four RKHS-regularized solutions were learned for each train/test set. We compared the perfor-mance of the three methods on training sizes  X  [50 ... 100] . For each training size, 100 training sets were sampled from SynthSin1d (with output noise) and, for each set, the func-tion learned with each regularizer was tested on 5000 points sampled from SynthSin1d (without output noise). Regular-ization weights for each method were set independently for each training size, to maximize measured performance. We measured performance as the percentage of variance in the true function recovered by the learned approximation: in which  X  y i gives the value of the learned approximation at test point x i , y i gives the value of the true function at x , and  X  y gives the mean of the true function. The value of (11) approaches 1 as the approximation approaches the true function (i.e., larger values are better).
 Figure 1a plots the mean performance of each regulariza-tion method for each considered training set size, with error bars indicating the upper and lower quartiles over the 100 tests at each size. The performance of RKHS regulariza-tion at each bandwidth is plotted in gray and the maximum performance is in red. In these tests, SAR2 significantly outperformed both L2 regularization using the same basis functions and RKHS regularization using any of the fixed-bandwidth subsets of the basis functions.
 Our second tests extended the form of SynthSin1d to inputs ( x 1 ,x 2 )  X  R 2 and outputs y  X  R . We call this distribution SynthSin2d. Importantly, the value of y depended most strongly on x 1 , making x 2 relatively uninformative. We performed 100 tests at each of the same training sizes as for SynthSin1d. SAR2 and L2 regularization were applied to collections of three Gaussian RBFs anchored at each training point, with bandwidths  X   X  { 0 . 5 , 2 , 8 } . RKHS regularization was applied independently for each fixed-bandwidth RBF subset. Regularization weights for each method were set at each training size, to maximize mea-sured performance. We also measured the performance of SAR2 regularization with direction sampling biased as follows: select a direction ( x 1 ,x 2 ) uniformly, multiply its x 2 by 10, and then rescale ( x 1 ,x 2 ) to the desired length. A SAR regularizer computed subject to this bias more severely penalizes change in the estimated function along the x 2 axis, which was known to be less informative. Figure 1b shows that SAR2 significantly improves on the performance of strong RKHS regularization applied to a more restricted set of basis functions and simple L2 regu-larization applied to an equally flexible set of basis func-tions. Adding a  X  X orrect X  bias during regularizer construc-tion further improves the advantage of SAR2, particularly for small training sets. Figure 1c/d qualitatively compares the behavior of L2 and biased SAR2 regularization. Biased SAR2  X  X nterpolates X  noticeably better than L2. 5.1. Natural Data We write  X  X ull RBF X  for RBFs based on the values of all features of an observation, and we write  X  X nivariate RBF X  for RBFs based on the value of a single feature of an ob-servation. RBFs were Gaussian and RKHS regularization was applied during estimation, unless noted otherwise. We tested SAR with the  X  X oston housing X  dataset from UCI/StatLib, which comprises 506 observations x  X  R 13 describing features of neighborhoods in the Boston area (circa 1978), with the prediction target being the median value of homes in each neighborhood. We preprocessed the observations by setting features to zero mean and unit variance, and setting the targets to zero mean. We com-pared six methods: L2, SAR4, Gaussian RBFs, 4th-order B-spline RBFs, additive P-splines, and boosted Trees. We measured performance with respect to (11).
 We performed tests with training sets of size 150-450. For each size, 100 rounds of randomized cross validation were performed, with non-training examples used for evaluating performance. When boosting trees, we set the maximum depth to 3 and performed 250 rounds of boosting with a shrinkage factor of 0.1, which maximized measured per-formance. For other methods, we set regularization weights separately for each training size to maximize measured per-formance. Kernel bandwidths were selected to maximize performance with 300 training samples.
 L2, SAR4, and Gauss all used full Gaussian RBFs cen-tered on each training point with bandwidth  X  = 0 . 05 fixed across all tests. B-spline used 4th-order B-spline RBFs centered on each training point with bandwidth  X  = 0 . 2 . P-spline applied 4th-order regularization to 2nd-order ad-ditive B-spline bases with 30 knots per dimension. In ad-dition to full RBFs, L2 and SAR4 used a collection of uni-variate RBFs, with the RBFs on each axis centered on the empirical deciles of the corresponding features. The stan-dard deviation of each univariate RBF was set to the max-imum of the distances to its upper and lower  X  X eighbors X . The single binary feature in this dataset was represented by just two univariate RBFs, centered on its min/max values. Univariate RBF structure was not optimized.
 SAR4 estimated approximate regularizers for first through fourth-order derivatives and combined the resulting matri-ces naively, by an unweighted sum. SAR4 used a com-pound point sampler which drew 75% of its samples from the fuzzy point sampler in Algorithm 2 and 25% of its sam-ples from the blurry box sampler in Algorithm 3. Both samplers were constructed strictly from the training set during each round of CV. An unbiased direction sampler with stochastic lengths was used. The length distributions L in point/direction sampling were set to the non-negative half of a normal distribution, with standard deviation set to 0.5/0.2 times the median nearest-neighbor distance in the training set. A lower bound of 0.05 was set on the effective step length . 4 The sampler parameters were not optimized. Figure 2 presents these tests. SAR4 consistently outper-formed the other methods, as seen in 2a. Figure 2b exam-ines relative performance more closely, by plotting results on individual train/test splits for training sets of size 300. SAR4 outperformed boosted trees and Gauss-RBF on most splits. Figure 3 examines SAR4 X  X  convergence in this set-ting.
 Our final tests used the standard USPS/MNIST digit recog-nition datasets. We tested on 100 randomly sampled training sets of size 500/2500 and tested on points not seen in training. We compared standard L2, RKHS, and SAR4 regularization using sampler parameters matching those used for tests on the housing data. Each method used full Gaussian RBFs at each training point (as for an SVM), with bandwidth  X  = 0 . 015 / 0 . 025 , which were selected to maximize performance of RKHS reg-ularization. We optimized the 1-vs-all squared hinge loss. Regularization weights were set to maximize mea-sured performance. For L2/RKHS/SAR4 the mean and standard deviation of classification accuracy in these tests was 92.7(0.5)/94.0(0.4)/94.1(0.4) for USPS and 94.5(0.02)/95.2(0.01)/95.4(0.02) for MNIST. Both RKHS and SAR4 significantly outperformed L2 on USPS. All pairwise comparisons were significant on MNIST. Figure 3 illustrates convergence of SAR on the USPS data. Note that MNIST tests used  X  ( x )  X  R 2500 for x  X  R 784 . SAR provides a general approach to controlling complex-ity in a broad class of functions, i.e., those representable by linear combinations of a fixed set of basis functions, by minimizing the n th -order derivative. For n = 1 , we pro-vided bounds on the error in the regularizer produced by SAR and showed that the approximation process is reason-ably sample-efficient. The main benefit of SAR is its flexi-bility, as can be seen from the empirical examination. Some other work in the manifold learning literature uses the data distribution to define data-dependent regularizers. For instance, Bousquet et al. (2004) defines a density-based regularizer. But, their practical implementation only con-siders a first-order derivative-based regularizer using Gaus-sian basis functions. SAR provides a more general frame-work to regularize higher-order derivatives, without requir-ing analytically tractable integrals.
 When the data belongs to a low-dimensional manifold, a common choice is to use the norm of the Laplace-Beltrami operator on the manifold. However, this norm cannot be computed analytically in most cases, so sample-based ap-proximations are used, e.g., the graph Laplacian opera-tor Zhu et al. (2003); Belkin et al. (2006). 5 SAR is more general and is not designed with the goal of approximating the Laplace-Beltrami-based regularizer.
 SAR raises a number of other interesting questions. On the theoretical side, it would be interesting to analyze SAR for higher-order derivatives, establish the influence of structure in the point measure  X  and direction measure s x , or make precise the relation between SAR and Laplacian-based reg-ularization. On the practical side, developing heuristic approaches to reduce the effective sample complexity, as well as point and direction samplers that better leverage prior knowledge is desirable. Reducing the per-sample cost of SAR by leveraging techniques for reduced-rank kernel approximation in SVMs and implementing SAR so as to take advantage of sparsity in  X  ( x ) both seem worthwhile, as they could significantly reduce the cost of the outer-products in line 6 of Algorithm 1.
 This work was supported by NSERC. Belkin, Mikhail and Niyogi, Partha. Towards a theoretical foundation for laplacian-based manifold methods. 74(8): 1289 X 1308, 2008. 5 Belkin, Mikhail, Matveeva, Irina, and Niyogi, Partha.
Regularization and semi-supervised learning on large graphs. In Shawe-Taylor, John and Singer, Yoram (eds.),
COLT , volume 3120 of Lecture Notes in Computer Sci-ence , pp. 624 X 638. Springer, 2004. 6 Belkin, Mikhail, Niyogi, Partha, and Sindhwani, Vikas.
Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. Journal of Machine Learning Research (JMLR) , 7:2399 X 2434, 2006. 8 Bousquet, Olivier, Chapelle, Olivier, and Hein, Matthias. Measure based regularization. In Thrun, Sebastian, Saul,
Lawrence, and Sch  X  olkopf, Bernhard (eds.), Advances in Neural Information Processing Systems (NIPS -16) . MIT Press, 2004. 5, 8 Davis, Steven B. and Mermelstein, Paul. Comparison of parametric representations for monosyllabic word recog-nition in continuously spoken sentences. IEEE Transac-tions on Acoustics, Speech, and Signal Processing , 28 (4), 1980. 1 Eilers, Paul H. C. and Marx, Brian D. Flexible smoothing with B-splines and penalties. Statistical Science , 11(2), 1996. 1
Walk, Harro. A Distribution-Free Theory of Nonpara-metric Regression . Springer Verlag, New York, 2002. 4 Hein, Matthias. Uniform convergence of adaptive graph-based regularization. In Proceedings of the 19th an-nual conference on Learning Theory (COLT) , pp. 50 X 64, Berlin, Heidelberg, 2006. Springer-Verlag. 5, 6 Kim, Kwang In, Steinke, Florian, and Hein, Matthias.
Semi-supervised regression using Hessian energy with an application to semi-supervised dimensionality reduc-tion. In Bengio, Y., Schuurmans, D., Lafferty, J., Williams, C. K. I., and Culotta, A. (eds.), Advances in
Neural Information Processing Systems (NIPS -22) , pp. 979 X 987. 2009. 2, 8 Lowe, David G. Object recognition from local scale-invariant features. In Proceedings of the seventh IEEE
International Conference on Computer Vision (ICCV) , 1999. 1 Nadler, Boaz, Srebro, Nathan, and Zhou, Xueyuan. Semi-supervised learning with the graph laplacian: The limit of infinite unlabelled data. In Bengio, Y., Schuurmans,
D., Lafferty, J., Williams, C. K. I., and Culotta, A. (eds.), Advances in Neural Information Processing Sys-tems (NIPS -22) , pp. 1330 X 1338, 2009. 8 Pearce, N. D. and Wand, M. P. Penalized splines and re-producing kernel methods. The American Statistician , 60(3), 2006. 1 Rifai, Salah, Mesnil, Gregoire, Vincent, Pascal, Muller, Xavier, Bengio, Yoshua, Dauphin, Yann, and Glorot, Xavier. Higher-order contractive auto-encoder. In
European Conference on Machine Learning (ECML) and Principles and Practice of Knowledge Discovery in Databases (PKDD) , 2011. 2 Sch  X  olkopf, Bernhard and Smola, Alexander J. Learning with Kernels . MIT Press, Cambridge, MA, 2002. 1 Sridharan, Karthik, Srebro, Nathan, and Shalev-Shwartz, Shie. Fast rates for regularized objectives. In Koller, D., Schuurmans, D., Bengio, Y., and Bottou, L. (eds.),
Advances in Neural Information Processing Systems (NIPS -21) , 2009. 4 Steinwart, Ingo and Christmann, Andreas. Support Vector Machines . Springer, 2008. 4 Wahba, Grace. Spline Models for Observational Data .
SIAM [Society for Industrial and Applied Mathematics], 1990. 1, 2 Wood, Simon N. Thin plate regression splines. Journal of the Royal Statistical Society, Series B , 65, 2003. 1 Yuille, Alan L. and Grzywacz, Norberto M. The motion coherence theory. In Proceedings of the International Conference on Computer Vision , 1988. 1 Zhou, Xueyuan and Belkin, Mikhail. Semi-supervised learning by higher order regularization. In International
Conference on Artificial Intelligence and Statistics , pp. 892 X 900, 2011. 8 Zhu, Xiaojin, Ghahramani, Zoubin, and Lafferty, John.
Semi-supervised learning using Gaussian fields and har-monic functions. In Proceedings of the 12th Interna-tional Conference on Machine LearningProceedings of the 27th International Conference on Machine Learning
