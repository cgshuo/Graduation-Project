 1. Introduction User interfaces for software download on the Internet are the front end of online software publishers for remote users.
One important component of such interfaces is the software download interface , namely the interface that prompts the user if he wishes to actually complete its download, after that he/she has browsed on a web site and selected the desired product to download. The design of such an interface is critical, because it usually shows product related information that should guide the user in its final choice. Software download interfaces are critical for users who may take unconscious or misguided 2005 ). Software download interfaces are critical for publishers too, if the interfaces reduce user trust and satisfaction and software downloading has security implications too. Brustoloni &amp; Villamarin-Salomon have shown that a poorly designed software download interface may cause unjustified risks in terms of breaches in a per-user derived policy adopted to classify risks ( Brustoloni &amp; Villamarin-Salomon, 2007 ).

The Security Warning Dialog Box in the Microsoft Authenticode interface is a reference, largely-used software download interface for Internet Explorer. Prior works have highlighted weaknesses of such an interface ( Dini, Foglia, Prete, &amp; Zanda,  X  2006; Cranor, 2007 ), as well as the attitude of the users to seldom act appropriately when presented with such a download interface ( Wood2010 ). In particular, in line with Brustoloni and Villamarin-Salomon (2007) , incoherent behaviors in users interacting with the Microsoft Authenticode interface were observed in Dini et al. (2006) , where an incoherent behavior con-sists in a download decision that is incoherent with the motivation given a posteriori. According to Shin (2010) , the rate of online visitors who buy a product or download software is influenced, among other factors, by trust and attitude, with trust also depending on user interface design quality ( Flavian et al., 2006; Hoffman et al., 2006 ). As a consequence, software down-load interfaces should have a good design that minimizes incoherent behaviors, increases user trust and, then, supports the user intention to purchase.

In prior works we have proposed a Question-&amp;-Answer (Q&amp;A) interface, i.e. a software download interface aimed at reduc-ing incoherent behaviors by asking the user questions about specific product download issues, e.g., the product cost ( Dini et al., 2007 ). Furthermore, we have suggested the use of a Reputation System (RS) X  X  service like the eBay Feedback Forum X  X o increase the user trust on a given software product ( Dini, Foglia, Prete, &amp; Zanda, 2013 ).

In this paper we experimentally compare the Authenticode and Q&amp;A software download interfaces and evaluate the effects of their extension by means of a reputation system. The overall result is that any solution that increases users atten-tion to the interface contents also significantly reduces incoherent behaviors. Nonetheless, the three design choices are not equivalent as they make users focus on different aspects of the proposed products. For example, RS make users to focus on the aggregated social feedback from previous users whereas Q&amp;A makes users to focus on specific software download issue, such as the cost . Even if the minimization of incoherent behaviors through a Q&amp;A-enhanced interface has positive effects on users trust, it causes a strong reduction in all software downloads because users tend only to accept free software, refusing if the software offered entails a charge. However, embedding a reputation mechanism side by side to a Q&amp;A dialog reduces the relevance of cost, and reduces incoherent behaviors at an intermediate level. Showing aggregated feedback from previ-ous users has a mitigating role between the common interface and the Q&amp;A interface. From these findings we indicate how designers can better tackle the design tradeoffs in these user interfaces.

The paper is structured as follows. Section 2 presents the background of the problem, and shows how the macro-hypoth-eses under test have been developed. Sections 3 and 4 show the graphical interfaces that have been tested in the experi-ments and present the experimental settings. Then, the experimental results are given with a concluding discussion on the observed trends. 2. Background and hypotheses 2.1. Issues in online software delivery
Online software delivery, like common e-commerce, must solve concerns especially regarding authentication and trust, in order to make users confident that they are not being cheated ( CommerceNet., 2000; Corritore, Kracher, &amp; Wiedenbeck, 2003 ). Complex Web interfaces and fraudulent software houses have caused serious problems such as spreading dialers, spy-wares and other threats ( Shukla &amp; Nah, 2005 ).

In this environment users cannot relate with a merchant and understand whether the software is in line with their expec-tations. In an ordinary shop, clients relate with a merchant and establish a trust relationship, and a similar relationship should be established also online ( Corritore et al., 2003; Cheskin research, 1999 ). Easy-to-use interfaces and complete infor-
Caird, 2000; Lanford, 2006 ), and a trustworthy interface is more likely to make users trust a vendor on the Internet ( Fogg et al., 2001 ).

A trustworthy, usable interface would not solve two relevant questions for users dealing with software download from the Internet: first, the need to know who really published the file offered, second, supporting the user in understanding what to expect from the executable file. As a solution to the first problem (authentication), major software vendors have devel-oped frameworks for code signing ( Jansen, 2000; Thawte., 2007 ), such as the Microsoft Authenticode that has been taken as reference in the current study due to its widespread diffusion on the Internet: 53% of the market share ( BMS, 2012 ).
By signing code, publishers seek to build a relationship of trust with users, satisfying the matter of accountability at the same time. Software publishers sign the piece of software they are releasing. The publisher X  X  certificate, the certification authority certificate, and the signed code are then packaged together.

At the client side, the browser verifies the publisher X  X  signature on the code, and, if verifications are successful, the Secu-rity Warning dialog box (SWDB) is presented to the user ( Fig. 1 ). If some problems arise, the user is notified with a different dialog box. The SWDB presents the certification authority name, the software publisher name and the software name, as well as other details such as the cost and other optional information pieces. This information is shared by means of the different Authenticode versions that have been released over the years.

The further issue to be faced in software downloading concerns the user X  X  expectations of the file. The Authenticode framework, like other code signing frameworks, ties a publisher to a file via its certification authority, but it does not help the user understand what to expect from the file. Predictability in this scenario would be important as it influences trust ( Corritore et al., 2003 ). Current software download frameworks do not give any safe information about what and how the executable file will behave once run on a computer system. Given that predictability cannot be fully ensured with the above approaches, in recent years Reputation Systems (RS) have emerged as a method for fostering trust among strangers cooper-ating online by gathering, distributing, and aggregating the feedback of previous consumers ( Resnick, Kuwabara, Zeckhauser, &amp; Friedman, 2000 ). 2.2. Interaction paradigms for software download
Deciding whether to accept or refuse a software download is a context-dependent decision:  X  X  X n such a situation, an appli-cation usually needs user input, because the application cannot determine automatically all the context relevant to the deci-and should be denied if the sender is a malicious user. Based on Brustoloni and Villamarin-Salomon (2007) classification of interaction paradigms that support such decisions, in this paper we analyze the warn-and-continue and the context sensitive guidance . A context sensitive guidance interface asks the user to provide context information necessary for a security deci-sion, while in the warn-and-continue approach the application warns the user of the risk and asks whether the user wants to accept it or not. Interfaces for software download (e.g.: the Authenticode Security Warning Dialog Box) follow the warn-and-continue approach.

In Dini et al. (2006) users claimed that they wished to download free software only, but they largely accepted software from Well-Known (WK) publishers, showing that the brand effect was more important than cost. Thus, users did not focus their attention on the whole interface, but they accepted or refused software downloads according to the publisher name, not the item cost. In addition, the Authenticode Security Warning Dialog Box prevented Common Name (CN) publishers with little or null brand name from having their software downloaded. Similar considerations were done by Brustoloni and
Villamarin-Salomon (2007) and , Wood2010 highlighting the problems deriving from the warn-and-continue approach: users policy adopted to classify risks) ( Brustoloni &amp; Villamarin-Salomon, 2007 ). As a countermeasure to these problems, ( Brustoloni and Villamarin-Salomon2007 ) propose the adoption of polymorphic dialogs that change the form of the required interface designed to increase user attention on the contents of the interface. Both the designs proved effective in reducing unjustified risks or incoherent behaviors compared to other interaction paradigms. 2.3. Trust and reputation systems
Trust for Mayer, Davis, &amp; Schoorman (1995) is the willingness of a trustor to be vulnerable to the actions of a trustee based on the expectation that the trustee will perform a particular action important to the trustor, irrespective of the ability to monitor or control the trustee. Trust is driven by many variables, and upon deciding whether to accept or refuse a soft-ware download, the user should be able to predict if the software meets with his/her expectations. Predictability influences trust:  X  X  X redictability is a trustor X  X  expectation that an object of trust will act consistently based on past experience X  X  ( Corritore et al., 2003 ). To better understand and solve trust issues, Egger (2000) proposed a model of trust for electronic commerce, which includes interface properties such as usability and familiarity, and cooperation between consumers. Coop-noting their behavior X  X . In the online domain, the end user has limited possibilities to fully try a software product or a service before buying, while the software publisher knows what he gets as long as he receives money.  X  X  X he inefficiencies resulting from this information asymmetry can be mitigated through trust and reputation X  X  ( Josang, Ismail, &amp; Boyd, 2007 ). Reputation
Systems help end users in predicting whether a product can be trusted or not. Enforcing a trust belief in the end users towards a remote software provider is a subjective phenomenon influenced by a set of factors, and in absence of personal experience, trust must be based on third party reviews. For Josang et al. (2007) , reputation is a collective measure of trust-worthiness based on reviews from members in a community. 2.4. Following proven research models
Lin, Rong, &amp; Thatcher (2009) validate a research model wherein purchase intention and swift trust are influenced by a set of parameters, including perceived social presence, awareness of 3rd party assurance, and perceived security ( Fig. 2 ). Swift ence refers to consumers X  perception that there is personal, sociable, and sensitive human contact and/or peer community on the Web site X  X . This type of reassurance for online entities is similar to the reassurance provided by brick and mortar busi-nesses ( Gefen &amp; Straub, 2004 ). The model by Lin et al. infers users opinions and intentions by looking at the answers from a survey. The users answers are analyzed to derive how  X  X  X urchase intention X  X  is influenced by a set of factors. The problem with survey-only studies is that relevant aspects of the user interaction are not differentiated qualitatively. In an everyday
Internet scenario users can make incoherent decisions, which happen to differ significantly from the desired behaviors. For instance, it is clear that asking a user  X  X  X o you feel safe to provide your login and password to a phishing Website? X  X  will cause a negative answer. Though in real life the user could behave differently because of badly designed interactions, fraudulent designs, lack of attention.
 The results of this paper help to understand how interfaces can be better designed to meet the users needs, in a safe way.
Thus, our study is orthogonal to the above research model: we investigate what happens in some building blocks of the above model, when real interfaces must be designed. The block  X  X  X ocial presence X  X  suggests the adoption of a Reputation Sys-tem as aggregator of reputation scores from a community, also to improve the efficiency in the process of software down-loading. Resnick et al. (2000) state that these mechanisms can help people make decisions about who to trust and provide incentive for honest behavior, while deterring dishonest parties from participating. Despite many problems in theory ( Resnick et al., 2000 ), in practice RSs proved to work rather well as a means to provide relevant information on the quality of merchants and products in auction sites. Successful examples are represented by iTunes and its reputation system for both desktop and mobile users, by the eBay auction site, or online book sellers like Amazon ( Chia et al., 2010; Dellarocas C., 2003; Chen and Liu, 2011 ). RSs have been proposed as a means to identify inauthentic content in file sharing applications ( Walsh &amp; Sirer, 2006 ), and similarly they could be effective in helping users to identify deceptive software publishers.
The model by Lin also includes  X  X  X erceived security X  X  which, as shown in Section 2.1 , can be managed by designing a con-security in the interaction paradigm, as also observed in questionnaire answers ( Dini et al., 2007 ). In our design, the Q&amp;A interface asks for an explicit feedback on cost, and this results in minimizing the incoherent behaviors, but also in focusing users attention on cost.

The Reputation System makes users focus on aggregated scores from previous users, and by doing so it also increases users overall attention in the interface. With the RS, the answer given in the Q&amp;A interface becomes one of the possible driv-ing factors together with the scores shown in the RS box. This is tested with the hypothesis H1 : H1. The RS reduces the relevance of cost aspects when compared to the Q&amp;A interface.

The RS does not ask for an explicit feedback in order to let the user go further with the download, so it should be expected an increase in incoherent behaviors when compared with a Q&amp;A interaction (hypothesis H2 ).
 H2. The Q&amp;A interface minimizes incoherent behaviors when compared to the other interfaces.
 From these considerations, combining the Q&amp;A with the RS should increase incoherent behaviors when compared to plain Q&amp;A (hypothesis H3 ), and reduce them when compared with the common Authenticode interface (hypothesis H4 ). H3. The RS with Q&amp;A increases incoherent behaviors when compared to the Q&amp;A.
 H4. The RS with Q&amp;A reduces incoherent behaviors when compared to the SWDB.
 And we also test H5, H6 and H7 : H5. The RS with Q&amp;A reduces the relevance of cost aspects when compared to the Q&amp;A interface.
 H6. The RS with Q&amp;A increases the relevance of trust when compared to the Q&amp;A interface.

H7. The RS with Q&amp;A increases the relevance of trust when compared to the SWDB interface. 3. Graphical designs replicates the information of the common SWDB, while the second step asks the cost of the software under download, and the third steps allows to download the software if the given answer is correct ( Fig. 3 ). The reason why we choose to focus on the cost information is the following. In a previous work ( Dini et al., 2007 ) we found incoherent behaviors of users with respect to cost, namely users stated they wanted to download free software whereas in practice they actually downloaded expensive one (or vice versa). We also proved that the Q&amp;A interface mitigates such a behavior. Therefore, in this work, we decided to start from such consolidated results. Conceptually, the Q&amp;A interface may present questions on any other infor-mation that is considered relevant, such as name of the producers, name of the software, et cetera.

Fig. 4 shows the other interfaces used in our experiment. They are, respectively, the Security Warning Dialog Box (left) and the Q&amp;A interface (right), both augmented by a reputation systems which display the aggregation of users rankings regarding the software to be downloaded ( Dini et al., 2013 ). As shown in Fig. 4 , the RS is placed at the bottom of the dialog box, showing the total number of feedback reviews, as well as the rate of positive, neutral, and negative reviews. According to the classification of RS ranking aggregation mechanisms proposed by Josang et al. (2007) , the interface, like that of the eBay Feedback Forum, is a simple summation system. In a simple summation RS, the system accumulates all given ratings to get the overall reputation. The RS is simple and readable at a glance, providing a concise view of the distribution of users X  feedback ( Dini et al., 2013 ). The implementation aspects of the RS are out the scope of this work. Anyway, the RS can be
Prete, 2002 ) in CMP systems ( Foglia &amp; Solinas, 2014 ). 4. Method 4.1. Populating the reputation system
The Reputation System was inspired by the eBay Feedback Forum also described by Resnick et al. (2000) , with the pur-pose to influence user trust beliefs about software publishers while dealing with software download. The RS effect is assessed by looking at the software download acceptances rates and by considering the motivations given for each accep-tance or refusal. The RS must be populated meaningfully, in order to simulate an every day scenario for all three classes of publishers tested in the experiment. Looking at the eBay Feedback Forum, often well-known and established merchants have a high and positive reputation ranking, unless they decide to have no ranking at all as their brand effect is strong enough. On the other hand, deceptive merchants who may provide an RS present few comments mostly negative. Finally, common name merchants tend to support an RS as they want to achieve visibility and, contextually, be trusted by end users.
Thus, in our experiments we do not test patterns in the population of the RS, but instead we choose to assume that the RS is already populated in line with the above observations. In particular: well-known publishers have a high and positive RS, or do not support it ; deceptive publishers have a low and negative RS, or do not support it ; publishers with a common name have either high and positive or low and negative RS ranking. The reason is twofold: reducing the number of test cases for each user, and simulating real life scenarios by associating appropriate reputation ranks to the three classes of publishers, while opti-mizing the time to complete the experiment.

In detail, the rankings present in the experiments are shown in Table 1 : a high ranking means that the number of reviews is greater than 400, with 80% positives, 10% neutrals, 10% negatives; a low ranking means that the number of feedback is smaller than 40, with 30% positives, 30% neutrals, 40% negatives; and finally the publisher can decide to not support the RS. 4.2. Test procedure
The study included the following independent variables.  X  Software publisher name. Software publishers were varied according to three classes: (i) well-known software publishers (WK); (ii) common name publishers (CN), and (iii) companies whose name was deceptive (DN).  X  Software cost. It was varied between free and charge entailed (above 20  X  ).  X  Reputation System ranking. The RS was varied with the following scheme:  X  Interface for software downloading. Participants interacted with four interfaces to perform differential analysis: the ori-ginal SWDB, the SWDB enriched with the RS, the Q&amp;A enriched with the RS and the plain Q&amp;A.

Each participant was shown a complete combination of the independent variables. More in details, each participant com-pleted 36 test cases ( Fig. 5 ), obtained from combining the independent variables with: ( i ) the SWDB interface, publisher name (3 values), and cost details (2 values); ( ii ) the SWDB + RS interface, publisher name, cost details, and RS ranking (2 val-name and cost details. In particular, we defined a pool of 36 pages divided in 4 groups as described in Fig. 5 . Such pages are then presented each participant in a random order.

The test consists of a sequence of test cases, where each test case consists of three stages. In the first stage we display a web page that proposes a software download. In the second stage, the participant decides whether to accept or not the soft-ware product by using one of the displayed interfaces. In the third stage, the participant is required to motivate his/her download choice with a motivation radio-button. The given motivations, together with the acceptance or refusal of a piece of software, are dependent variables in our experiments. The motivation radio-button presents a set of options for partici-pants to choose. The content of this window is described in Table 2 (column Motivation). If the participant accepts to down-load, then he/she is presented a list of motivations mainly related to interest (I), trust (T), free cost (F) and other minor motivations (O). In contrast, if the participant refuses to download, he/she is shown another list of motivations related to no interest (NI), distrust (DT), high cost (C) and other minor ones (O).

At the end of the 36 test cases, participants answered a multiple-choice questionnaire containing personal questions and questions about the used interface. The full questionnaire is reported in Appendix B . As each participant had to complete 36 test cases, we minimized learning effects by not giving participants feedback on their actions during the experiment.
In a previous work ( Dini et al., 2006 ), it was observed that user intentions and proper behaviors for each single test case could not be derived by just looking at the final questionnaire answers, as each test case needed a precise motivation. For this reason, we let participants choose the most suitable motivation out of a set (see Table 2 ), after either accepting or refusing a downloading. By doing this, participants can motivate their decisions with precision. In addition, we used motivations to clearly identify participants X  coherent and incoherent behaviors. In fact, in the cited previous works, it was observed that users interacted with interfaces incoherently at times: a participant behaves incoherently if his/her motivation does not match with the features of a piece of software downloaded or refused. In Table 2 , column  X  X  X ncoherent, if in combination with X  X  shows when motivations not matching actual software features identify incoherent behaviors. An example of an inco-herent behavior is if a participant refuses a download picking the motivation  X  X  X  could be interested, but it was expensive X  X , while the software is free.

At the beginning of a test session each participant was asked to  X  X  X ither accept or refuse the proposed software as if you were using your own computer X  X , since we want to simulate the scenario of a typical Web user who browses the Internet and downloads software according to its own interests. Participants were not rewarded for their time, but they were told:  X  X  X ou are participating to the design of a new user interface for e-government web applications X  X . A gender balanced group of 40 participants was recruited, with ages ranging from 18 to 40 (mean age 23). 34 participants were high school students or undergraduate students, while 6 participants were employed in public administration. Participants attended the tests in per-son in our labs or in their office (rather than remotely via web). The recruited group had good Internet experience, as shown in Table 3 . All the experiment were performed on a windows based platform.

All participants actions were logged on a database for subsequent analyses. Results presented in the paper are given reporting raw percentages, or Chi-Square v 2 statistics. Chi-square was adopted to compare groups with different cardinal-ities, and we also considered the Yates correction for one degree of freedom. It provides a p-value that indicates the prob-ability that the observed difference is due to chance, where p &lt; 0.05 is commonly accepted to indicate a statistically significant difference in the sample ( Glantz, 2005 ). 4.3. Validity
We minimized learning effects by not giving the users any feedback on each single test cases (e.g., users performing free acceptances or acceptances entailing a charge without highlighting, during the test, incoherent motivations). We observed that the actions taken by the users with tested interfaces were very conservative: they tried to minimize possible risks (even if they were aware that the experiment could not harm their terminal). Thus, we believe that the test environment had little impact on our results. In addition, we did not observe a lack of motivation in the participants, also because after initial tests, we minimized the duration of a single test at less than 30 min on average.

There were three ranking categories shown in the Reputation System in our test: high and positive rankings, low and neg-ative rankings, and rankings not supported. Although we observed that rankings tend to be classified mainly in line with these three categories, other studies should investigate how users behave while dealing with other reputation patterns. 5. Results
In this Section we verify the hypotheses under test specified in Section 2.4 and give further significant findings. Tables with full results in details are reported in Appendix A . 5.1. Tested hypotheses
Presenting aggregated social-feedback with an RS influences user decisions even in presence of a Q&amp;A interface asking for cost details: users X  focus shifts from cost to rankings given from previous users ( H1 , H6 ). Such a shift of focus can cause an effect that we identify with a soft information overload, since incoherent behaviors are minimized only with the plain Q&amp;A interface ( H2 ). The presence of more information provided by the RS in the Q&amp;A interface makes users to focus on aspects not related to cost but also causes an increase in incoherent behaviors ( H3 ).

H1. The RS reduces the relevance of cost aspects when compared to the Q&amp;A interface.  X  Confirmed ( v power &gt; 0.8).

H2. The Q&amp;A minimizes incoherent behaviors when compared to the other interfaces.  X  Confirmed ( v power &gt; 0.8 ).

H3. The RS with Q&amp;A increases incoherent behaviors when compared to the Q&amp;A.  X  Confirmed ( v power &gt; 0.8).

The RS not only shifts users X  focus from cost, but it also reduces incoherent behaviors observed in the SWDB ( H4 ).  X  X  X low X  X  is a concept that characterizes the human X  X omputer experience as playful and exploratory ( Trevino &amp; Webster, 1992 ). We believe that the RS presence increases users  X  X  X low X  X , making them more active in exploring the information shown in the dia-log box, and by doing so they make fewer incoherent behaviors .

The RS presence in the Q&amp;A manages to increase the relevance of trust, reducing the focus on cost, when compared with of information and influences users.

H4. The RS with Q&amp;A reduces incoherent behaviors when compared to the SWDB.  X  Confirmed ( v power &gt; 0.8).

H5. The RS with Q&amp;A reduces the relevance of cost aspects when compared to the Q&amp;A interface.  X  Not confirmed ( v p &gt; 0.05, power &gt; 0.6).

H6. The RS with Q&amp;A increases the relevance of trust when compared to the Q&amp;A interface  X  Confirmed ( v power &gt; 0.8).

H7. The RS with Q&amp;A increases the relevance of trust when compared to the SWDB interface  X  Not confirmed ( v p &gt; 0.05, power &gt; 0.8).
 The RS in the Q&amp;A did not show statistically significant differences ( H5 not confirmed) when compared with the plain
Q&amp;A even if raw percentages indicate a difference, but looking at the statistical power, the non-significance can be due to the limited number of participants. No statistical significance on trust motivations was observed when the Q&amp;A + RS is com-pared with the SWDB interface ( H7 not confirmed). This is due to the fact that with the SWDB users gather mainly the pub-lisher name, and base their decisions on the trust that they have in it. Thus, trust motivations with SWDB are high, but they are high because users do not find the relevant information and as a consequence base their decisions on the publisher name.
The macro-effect of the RS on motivations ( Figs. 8 and 9 ) is reduced as results by the aggregation of positive, negative or null rankings in the classes of publishers, but different ranking influence downloads acceptances and refusals as shown on Table 6 in Appendix A .

The Q&amp;A has few graphical effects, and asks for explicit feedback on cost, to ensure that the user gathers it. The results show that when users are pushed to think about cost details, cost becomes the driving factor for software acceptance/refusal.
In fact, with the plain Q&amp;A interface users prefer to download free software, and download software entailing a charge only if they are interested in it ( Table 4 ).
 5.2. General trends
We investigated the rates of incoherent behaviors for all the tested interfaces. Minimizing the rate of the observed inco-herent behaviors can be considered as an indicator of interface quality. By observing the decreasing tendency line of incoher-ent behaviors in Fig. 6 , we derive that the plain Q&amp;A performs better than the other proposed interfaces, ensuring the safest interaction paradigm. An SWDB-based design results more prone to incoherent behaviors, causing higher acceptances of costly pieces of software (often not desired by the users). Adding graphical elements, like the RS, causes a sort of information overload ( Farhoomand &amp; Drury, 2002 ), as incoherent behaviors with the plain Q&amp;A are lower than with the Q&amp;A + RS.
The analyses of the three novel interfaces (SWDB + RS, Q&amp;A + RS, and Q&amp;A) compared with the SWDB show a global reduction of accepted costly software, in particular concerning software by WK publishers. Fig. 7 shows the acceptance trends of costly pieces of software (data are taken from Tables 5 and 6 in Appendix A ). The effect of introducing the Q&amp;A and the RS is clear compared to the basic SWDB: the Q&amp;A reduces significantly the acceptances of costly code, while the RS reduces the relevance of cost providing higher costly acceptances than the basic Q&amp;A.

The motivations given by the users for either accepting or refusing software download are shown in Figs. 8 and 9 , respec-mitigating role played by the RS between the SWDB and the Q&amp;A. Aligning acceptance motivations from the four interfaces, emphasizes the influence of motivations related to high Cost ( Fig. 9 ), still reducing the other motivations, namely non-inter-est (NI) and distrust (DT), from the average rows. Looking back at the histograms about the incoherent behaviors ( Fig. 7 ), the
Q&amp;A related acceptances and motivations match more closely with actual users intentions compared to the other tested interfaces, as the Q&amp;A nullifies incoherent behaviors. 6. Concluding discussion
The usability studies as a whole have shown stable trends in user behaviors while interacting with the different graphical interfaces. The Question-&amp;-Answer interface focuses very significantly the user attention on cost aspects, driv-ing users to largely accept free software. Adding a Reputation System in the Q&amp;A interface causes a reduced relevance of cost, while in the classic SWDB interface it reduces a little incoherent user behaviors and influences a little their trust beliefs.

The overall results, as shown in the General Trends section, suggest guidelines to be considered while choosing the best interface depending on the brand effect of the software publisher and/or the need to increase the software market penetration. From our design, where the Q&amp;A asked for cost information, it derives that a Q&amp;A-like interface is to choose if software is free, while, if software entails a charge, such an interface should be proposed if the publisher wants to ensure that users fully understand cost aspects of the product offered. In detail, our Q&amp;A interface prevents from having users who in the future give negative feedback about a publisher because the interface is very robust in not causing incoherent behaviors . However, the Q&amp;A can increase the attention level on any content of the interface, so if the pub-lisher wants to increase the attention on aspects other than cost, like brand name, this can be an option, though not investigated in our tests.

The classic SWDB interface results now somehow out-of-date because of the spreading of social aspects in a plethora of web based activities. Providing a feedback aggregation mechanism (such as a reputation system) upon software download is a feature that is not unexpected by the users, as also shown in our questionnaire answers. For this reason the publisher should not provide an RS only if negative feedback is likely to be received. This might be the case of software out as alpha-or beta-version, that still has not the desired quality. When the publisher has not a strong brand effect, but the product is valid, providing an RS has a positive effect on software downloads. When the RS is embedded in the Q&amp;A we designed, it increases trust-based motivations, reducing those related to cost aspects. However, enriching the Q&amp;A with the RS shows incoherent behaviors similar to the common SWDB interface with RS, and an acceptance rate similar to the plain Q&amp;A for well-known publishers, so its adoption has little advantages. Conversely, the Q&amp;A with RS should be preferred compared to the plain Q&amp;A by common-name publishers as it provides higher acceptances for software entailing a charge, keeping incoherent behaviors low.

As for future works, we plan to extend our analysis to different application domains ( Foglia, Giuntoli, Prete, &amp; Zanda, 2007 ), and to investigate the opportunity of assessing usability measures via physiological signals ( Foglia, Prete, &amp; Zanda, 2008 ).
 Appendix A. Appendix A  X  Tables with full results See Appendix Tables 5 X 8
Appendix B. Appendix B  X  Final Questionnaire (1) How often do you use the Internet? (Rarely, Once a month, once a week, Every day) (2) Why do you use the Internet? (Work, Study, Entertainment, Other) (3) Do you usually download software? (Never, At times, Frequently) (4) As a Internet user, have you ever seen the  X  X  X ecurity Warning X  X  dialog box? (Yes, No, Don X  X  remember) (5) How do you usually behave when confronted with a  X  X  X ecurity Warning X  X  dialog box? (Click Yes, Click No, It depends) (6) Do you usually pay attention to what is written in the  X  X  X ecurity Warning X  X  dialog box? (Yes, No, At times) (7) While deciding whether to accept or refuse a download, the publisher name matters to me. (Agree completely, Agree (8) Do you trust more well-known publishers? (A lot, Not much, Yes/no, Little, Very little) (9) Did you notice publisher names similar to famous ones? (Yes, No) (10) How did you behave when dealing with publishers with a name similar to those of well-known ones? (Always (11) Do you know what a premium rate connection is? (Yes, No) (12) Is it a risk for you that software download entails a charge? (A lot, Not much, Yes/no, Little, Very little) (13) Is it a risk for you that software usage implies a premium rate connection? (A lot, Not much, Yes/no, Little, Very little) (14) How much are you willing to spend to use a piece of software? (0 -I don X  X  want to spend, 3  X  ,10  X  ,30  X  for full down-(15) Would you feel safer if software which entails a charge were guaranteed by well-known entities? (A lot, Not much, (16) Before this test, had you already dealt with a Reputation System and feedback systems? (Yes, No) (17) How much were you influenced by the extra information provided by the reputation system? (A lot, Not much, Yes/no, (18) How much do you trust a publisher with no reputation system? (A lot, Not much, Yes/no, Little, Very little) (19) Do you consider it useful to know the precise number of users who gave feedback comments? (A lot, Not much, Yes/ (20) Do you feel safer having a reputation system in the dialog box? (A lot, Not much, Yes/no, Little, Very little) References
