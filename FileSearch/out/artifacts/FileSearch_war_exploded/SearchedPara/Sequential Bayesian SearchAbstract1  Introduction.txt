 Zheng Wen zhengwen@stanford.edu Branislav Kveton Branislav.Kveton@technicolor.com Brian Eriksson Brian.Eriksson@technicolor.com Sandilya Bhamidipati Sandilya.Bhamidipati@technicolor.com Millions of people search daily for movies, books, and music on the Internet. Existing recommender systems, like Netflix and Pandora, typically allow their users to browse their collections of content in some predefined taxonomy, from general item categories, such as movie genres, to more specific ones. When the users look for items in obscure and niche categories, they are usually hard to find. This is unfortunate since such items are often of the highest value. The ability to find relevant content fast is important, and would ultimately lead to better recommendations, user experience, and content monetization.
 In modern recommender systems, the number of item categories is often large, and they can be very general or specific. In the movie recommender system Netflix, the categories range from Romance to Nature Family-Time TV . In the music recommender system Pandora, the categories range from Classical to East Coast Hip Hop . To navigate through these categories, the user is first asked to choose a broad area, such as Drama or Comedy , and then chooses from more specific options, such as Independent or Romantic Drama . This policy for navigating the space of content can be represented by a tree, where the nodes and branches are the user X  X  options and choices, respectively. We refer to this tree as a taxonomic tree .
 In this work, we study how to learn better taxonomic trees over time based on the user X  X  behavior. Suppose that the user enjoys Cult Horror . This movie genre is not widely popular. So in a taxonomic tree that is not personalized, the user would have to navigate through many options before getting to relevant movies. If the tree was personalized, Cult Horror could be one of the options at the root, and therefore the user would find relevant content faster.
 Our learning problem is a repeating interactive game, which comprises of episodes . In each episode, we navi-gate the user to the item of interest by asking a series of questions . The questions depend on searched items in the past episodes and the answers to the questions in the current episode. We interact with the user until we can identify a single target item , the item of user X  X  interest. Then we update our estimator of user pref-erences P t and the game proceeds to the next episode. Assuming that the user repeatedly searches for similar content, we expect to learn a better policy for content search over time.
 We solve our problem by Bayesian learning and hence we refer to our solution as Sequential Bayesian Search (SBS) . Our solution has several nice properties. First, it is computationally efficient . In particular, we show that the posterior P t over user preferences  X   X  can be updated by counting. Even more importantly, nearly optimal policies for finding items with respect to P t , a distribution over user preferences, can be derived as efficiently as with respect to any particular preference  X  . Second, we show that our solution is Bayesian op-timal . Third, we analyze it from the frequentist point of view and prove that its regret , additional questions asked due to not knowing the true user preferences, is O ( p t ln( t )), sublinear in the number of episodes t . Finally, we evaluate our solution on a movie discov-ery problem and show how it improves with training episodes.
 This paper is organized as follows. In Section 2, we in-troduce the notion of interactive search. In Section 3, we discuss related work. In Section 4, we propose se-quential Bayesian search, discuss how to implement it, and show its Bayesian optimality. In Section 5, we an-alyze our approach from the frequentist point of view and bound its regret. In Section 6, we evaluate it on a real-world problem. Finally, we conclude in Section 7. In this section, we discuss how the problem of content discovery can be viewed as a game, where the content discovery system asks the user a sequence of questions to find the target item of interest. We assume that the user has a target item i  X  in mind. This item belongs to a set of M items I = { 1 ,  X  X  X  ,M } . The preferences of the user for the items are modeled by a probability distribution  X   X  over I and the target item i  X  is drawn i.i.d. from this distribution.
 Our goal is to identify the item i  X  by asking the user a sequence of yes-or-no questions q . The questions are modeled as functions q : I  X  X  0 , 1 } such that q ( i ) = 1 when the item i has the attribute q and q ( i ) = 0 when it does not. The set of all questions is referred to as the question pool Q . We assume that all items can be uniquely identified by a subset of questions from Q . The search for the target item i  X  proceeds as follows. First, we ask a question q 1 and the user replies q 1 ( i Second, we ask another question q 2 and get an answer q ( i  X  ). Note that the choice of the second question q 2 may depend on the answer q 1 ( i  X  ) to the first question. In general, at time ` we choose a question q ` based on all questions and answers up to time ` , ( q j ,q j ( i  X  Formally, a policy for choosing questions is a function: where 2 Q is the index set of the question pool Q and { 0 , 1 } |Q| is a vector of | Q | answers. The policy can be represented by a decision tree T , where the nodes and branches correspond to the asked questions and user X  X  answers, respectively (Golovin &amp; Krause, 2011). The search continues until only one item in I satisfies all answers thus far. In other words, the cardinality of the version space U ` at time ` : is one. We let: be the number of questions after which the item i is uniquely identified by the policy T .
 Interaction with the user is usually costly. As a result, good content discovery systems should be able to find target items in as few interactions as possible. In this work, we focus on learning systems that minimize the expected number of interactions with the user: given user preferences  X   X  .
 Consider the case where we want to determine the best decision tree T given prior knowledge of the user pref-erences  X   X  . This case is well studied in stochastic con-trol, where it can be formulated as a Markov decision process (MDP) and solved by dynamic programming (Bertsekas, 2012). While dynamic programming can be used to learn the optimal policy T  X  , it is computa-tionally intractable for large question pools due to the curse of dimensionality (Bertsekas, 2012). If the sys-tem is allowed to ask arbitrary questions, not confined to a specific question pool Q , then the optimal policy T  X  can be found efficiently by Huffman coding (Cover &amp; Thomas, 2006). Note that this approach cannot be applied to our problem due to the restricted question set. Our content discovery problem is an instance of gener-alized binary search (GBS) (Dasgupta, 2005; Nowak, 2011). In GBS, the goal is to learn a policy that finds items in a collection of items in the minimum number Algorithm 1 GBS ( I , Q , X   X  ) Greedy generalized binary search.

Input: Items I , questions Q , and user preferences
U 1  X  X  `  X  1 while | U ` | &gt; 1 do end while
Output: Target item i  X   X  U ` of queries. The optimal policy is NP-hard to compute, both in the worst and average cases. A nearly-optimal policy can be computed greedily by an algorithm that always selects the query that divides the version space closest to two halves. We refer to this solution as GBS (Algorithm 1).
 The number of queries in Algorithm GBS was bounded both in the worst (Dasgupta et al., 2003) and average (Dasgupta, 2005) cases. The average-case analysis was later improved (Golovin &amp; Krause, 2011) based on the notion of adaptive submodularity. The resulting upper bound is presented below.
 Lemma 1. [Theorem 5.8 (Golovin &amp; Krause, 2011)] Let  X   X  be a probability distribution over items. Let T  X  be the optimal policy with respect to  X   X  and T g be the greedy policy generated by Algorithm GBS . Then: In practice, the distribution  X   X  is usually unknown. In our paper, we study the problem where  X   X  is revealed sequentially through interactions with the user. This is the first work that derives performance guarantees for this setting and shows how the quality of GBS policies improves over time with more interactions.
 Yue &amp; Guestrin (2011) show how to learn policies that cover items of user X  X  interest based on interaction with the user. Our paper is related since both problems are submodular and involve learning of user preferences. Our work differs from Yue &amp; Guestrin (2011) in several aspects. First, we study a variant of the minimum set cover problem. The problem in Yue &amp; Guestrin (2011) is a maximum coverage problem. Second, we guide the user to the items of interest by questions, rather than recommending k items at once. So naturally, both our solution and its analysis differ significantly from those of Yue &amp; Guestrin (2011).
 The problem of learning trees in the online setting has been studied before, for instance in the framework of experts (Section 5.3 in Cesa-Bianchi &amp; Lugosi (2006)). In this setting, the best tree can be found by learning | Q | (2 D +1  X  1) weights, where | Q | is the number of the experts at each node of the tree and D is the depth of the tree. In our setting (Section 2), the trees can be as deep as min { M, | Q |} . Therefore, online learning with tree experts would be exponential in the quantities of interest, min { M, | Q |} , and impractical for solving our problem. In most real-world problems, the user preferences  X   X  are initially unknown. By repeatedly interacting with the user, we can learn the preferences and reduce the number of asked questions to find target items. In this section, we propose a novel algorithm for interactive search that learns user preferences. We refer to it as Sequential Bayesian Search ( SBS ).
 We employ Bayesian learning for two reasons. First, note that the choice of the target item is not affected by how we ask questions (Section 2). Therefore, we do not need to explore , which implies that learning and optimization can be separated. Second, high-quality prior knowledge can significantly reduce the expected number of interactions with the user in practice. So Bayesian learning seems like an appropriate approach to our problem.
 Specifically, we assume that the system has a prior be-lief P 0 over the user preferences  X   X  , which is a proba-bility density function over all the possible realizations of  X   X  . The system updates its belief at the end of each episode after observing the target item in that episode, which is sampled i.i.d. from  X   X  . During episode t , the system uses its current belief P t to derive a policy that minimizes the expected number of questions to find the target item: In summary, we find the best policy T  X  t with respect to the system X  X  belief P t . The optimization problem in Equation 5 can be simplified based on the notion of the certainty-equivalent user preference  X   X  t . Definition 1. Let P t be the system X  X  belief over  X   X  in episode t . Then the certainty-equivalent (CE) user i  X  X  .
 Algorithm 2 SBS  X  ( I , Q , P 0 ) Optimal sequential Bayesian search.

Input: Items I , questions Q , and prior belief P 0 on user preferences for episode t = 0 , 1 , 2 ,... do end for Based on this definition, we make the following obser-vation. Computing the optimal policy with respect to belief P t is equivalent to computing the optimal policy with respect to the CE user preference  X   X  t . This ob-servation is formalized in Lemma 2. We also note that finding the optimal tree, where  X   X  is replaced by  X   X  t . Lemma 2. The problem min T E  X   X  P t [ E i  X   X  [ N ( T,i )]] is equivalent to min T E i  X   X   X  Proof. For any policy T :
X 4.1. Algorithm SBS  X  In this section, we propose a novel content discovery algorithm SBS  X  (Algorithm 2). In each episode t , the algorithm involves three steps: (1) estimating the CE user preference  X   X  t , (2) solving the optimization prob-the belief P t based on Bayes X  rule. Due to Lemma 2, finding the optimal tree. This problem can be solved efficiently when the question pool Q is small, or if the system can ask arbitrary questions (Section 2). Note that the user preferences  X   X  are a multinomial distribution over items I . So the computation of the CE user preference  X   X  t , and updating the belief over  X  , can be implemented efficiently when the prior P 0 is the Dirichlet distribution, the conjugate prior of the multinomial distribution. Specifically, we assume that Algorithm 3 SBS ( I , Q , P 0 ) Greedy sequential Bayesian search.

Input: Items I , questions Q , and prior belief P 0 on user preferences for episode t = 0 , 1 , 2 ,... do end for the system X  X  prior belief is modeled as a Dirichlet dis-tribution with parameters  X   X &lt; M + , denoted as Dir(  X  ), and we define an indicator vector Z t  X  &lt; M such that Z ( i ) = 1 { i = i t } , where i t represents the target item in episode t .
 From Bayes X  rule, the posterior belief at the beginning of episode t is: Furthermore, from the properties of the Dirichlet dis-tribution, we have: where  X  ( i ) is the i -th entry of  X  , which corresponds to item i . So the CE user preference  X   X  t can be updated by counting and re-normalization.
 By design and Lemma 2, Algorithm SBS  X  is Bayesian optimal.
 Proposition 1. Algorithm SBS  X  is Bayesian optimal. That is, for any episode t :  X  P t is the posterior belief over  X   X  at the beginning 4.2. Algorithm SBS Unfortunately, for a general question set Q , computa-tion of the optimal policy T  X  t is intractable. However, the policy can be approximated efficiently by a greedy algorithm (Section 3). This motivates us to propose a near-optimal algorithm SBS (Algorithm 3) for content discovery.
 The policy generated by Algorithm SBS in episode t is denoted by T g t . Similarly to Algorithm SBS  X  , the new method is computationally efficient when its belief P t is modeled by a Dirichlet distribution. The following proposition claims that our method is Bayesian near-optimal.
 Proposition 2. Algorithm SBS is Bayesian near op-timal. That is, for any episode t :  X  P t is the posterior belief over  X   X  at the beginning  X  If min i  X  X   X   X  t ( i ) &gt; 0 , then: 1 Proof. From Algorithm SBS , P t is the posterior belief over  X   X  . From Lemma 1, we have: Furthermore, similarly to Lemma 2, we have: Our second claim follows directly from combining the above three equations. In Section 4, we showed that Algorithms SBS  X  and SBS are Bayesian optimal and Bayesian near-optimal, re-spectively. In other words, both SBS  X  and SBS achieve good performance with respect to the system X  X  current belief. However, such results do not reflect the cost of learning user preferences  X   X  . In particular, since SBS and SBS learn  X   X  while interacting with the user, they incur a performance loss, ask more questions than the optimal policy T  X  . We show that these losses are small by analyzing our algorithms from the frequentist per-spective. In particular, we show that the cumulative regret of Algorithm SBS  X  and the [1  X  ln(  X   X  min )] -scaled cumulative regret of Algorithm SBS are sublinear. Note that from the frequentist perspective,  X   X  is determin-istic but unknown.
 We assume that the system X  X  prior belief is a Dirichlet distribution Dir(  X  ). To simplify notation, we denote the minimum expected number of interactions under the user preferences  X   X  by N  X  = min T E i  X   X   X  [ N ( T,i )]. Furthermore, we define the minimum user preference probability  X   X  min = min i  X  X   X   X  ( i ) and  X  0 = P i  X  X  where  X  ( i ) is the i -th entry of  X  . 5.1. Analysis of Algorithm SBS  X  The performance of Algorithm SBS  X  is measured by its cumulative regret.
 Definition 2. For any episode  X  , the cumulative re-gret of Algorithm SBS  X  is defined as:
Reg  X  (  X  ) = The cumulative regret Reg  X  (  X  ) is the expected cumu-lative performance loss of Algorithm SBS  X  in the first  X  + 1 episodes, relatively to the optimum (  X  + 1) N  X  . In Theorem 1, we prove that Reg  X  (  X  ) is sublinear in  X  when  X  is sufficiently large.
 Theorem 1. Let P 0  X  Dir(  X  ) and  X  E be the sample threshold of Algorithm SBS  X  defined as:  X  E = min Then, for  X  &lt;  X  E , Reg  X  (  X  )  X |Q| (  X  + 1) . In addition, for  X   X   X  E :
Reg  X  (  X  )  X  |Q|  X  E + The proof of Theorem 1 is sketched below. The com-plete proof can be found in Appendix.
 Proof. If the CE user preference  X   X  t is close enough to the true user preference  X   X  , the policy T  X  t in episode t performs near optimally. Specifically, if k  X   X   X   X   X  t k  X  min , then: Based on Hoeffding X  X  inequality, k  X   X  t  X   X   X  k  X  is small with high probability for t  X   X  E . Hence, the expected regret in episode t is small. Specifically, for all t  X   X  we have: E For t &lt;  X  E , the regret can be bounded naively as: Our bound on the cumulative regret up to episode  X  is proved by summing up the above upper bounds.
 Theorem 1 states that Reg  X  (  X  ) = O ([  X  ln(  X  )] 1 2 ) when  X   X   X  E . Therefore,  X  E can be viewed as the number of episodes after which Algorithm SBS  X  achieves good performance. Note that  X  E depends on both  X   X  min and the choice of P 0 . In particular, smaller  X   X  min or a poor choice of P 0 result in larger  X  E . 5.2. Analysis of Algorithm SBS Note that the cumulative regret (Definition 2) is not a good metric for evaluating the performance of Algo-rithm SBS . Specifically, since the algorithm computes a suboptimal solution, it can incur a large loss in com-parison to N  X  in each episode t and this loss does not diminish as t  X   X  . Lemma 1 suggests that the algo-rithm should be compared to the [1  X  ln(  X   X  min )] factor of the minimum expected number of interactions N  X  . This motivates us to generalize the notion of the cu-mulative regret to a C -scaled cumulative regret. Definition 3. For any episode  X  and C  X  1 , the C -scaled cumulative regret of Algorithm SBS is defined as: Reg (  X ,C ) = The regret Reg (  X ,C ) measures the expected cumula-tive loss of Algorithm SBS in the first  X  + 1 episodes, relatively to the C -scaled optimum (  X  + 1) CN  X  . Note that C = 1 yields the classical cumulative regret. We would like to bound Reg (  X , 1  X  ln(  X   X  min )) from above. In Theorem 2, we prove that Reg (  X , 1  X  ln(  X   X  min )) is sublinear in  X  when  X  is sufficiently large.
 Theorem 2. Let P 0  X  Dir(  X  ) and  X  G be the sample threshold of Algorithm SBS defined as:  X  G = min Moreover, for  X   X   X  G : where f (  X   X  min ,N  X  , X , X  G ) = The proof of Theorem 2 is sketched below. The com-plete proof can be found in Appendix.
 Proof. If the CE user preference  X   X  t is close enough to the true user preference  X   X  , the policy T g t in episode t performs near optimally. Specifically, if k  X   X   X   X   X  t k  X  min , then:
E Based on Hoeffding X  X  inequality, k  X   X  t  X   X   X  k  X  is small with high probability for t  X   X  G . Hence, the expected scaled regret in episode t is small. Specifically, for all t  X   X  G , we have: E For t &lt;  X  G , the regret can be bounded naively as: Our bound on the cumulative regret up to episode  X  is proved by summing up the above upper bounds.
 The above theorem says that Reg (  X , 1  X  ln(  X   X  min )) = O ([  X  ln(  X  )] 1 2 ) when  X   X   X  G . The value of  X  G depends on  X   X  min and the choice of P 0 . Specifically, smaller  X  or a poor choice of P 0 result in larger  X  G . Note that the 1  X   X  orems 1 and 2 originates in Lemma A-1 in Appendix and is due to bounding the terms that grow with time  X  using N  X  , as opposing to looser |Q| . This factor is hard to eliminate because the bound in Lemma A-1 is tight. Algorithm SBS is evaluated on a real-world movie dis-covery problem. We perform two experiments. First, we study how the number of questions asked by SBS decreases over time, and compare the approach to two baselines. Second, we show how the choice of the prior P 0 affects the quality of SBS policies.
 6.1. Experimental Setup Our experiment is conducted on a dataset of one mil-lion ratings from the MovieLens project (Lam &amp; Her-locker, 2012). In this dataset, 6k users rate 4k movies for more than one year. The movies are annotated by 19 genres (Table 1). Only 302 unique combinations of genres exist in our dataset. In our experiments, these are the individual items in I . Movies with the same genre descriptors are equivalent in the sense that the user has equal preferences for all of these movies. We assume that each rating event is a movie viewing event, where Algorithm SBS is used to find the target movie, the movie that was actually watched. We ask questions of the form  X  X ould you like to watch X ? X , where X represents a genre, and measure the number of asked questions until the target movie is identified. We learn a single preference distribution for all users. We opt for this setup since we want to show that SBS converges to the optimal solution when the number of episodes is large. None of our individual users rated enough movies to illustrate this trend.
 We focus on two subsets of our data, movies belonging to at least two, and at least three, genres. The movies in the former set are harder to identify since they are described by fewer attributes. These movies are rated over 700k times, and the number of unique movies is M = 283. The movies in the latter set are rated 300k times, and the number of unique movies is M = 203. To avoid bias, we partition both datasets into 30 time periods with the same number of ratings, evaluate the policy in each time period, and finally average the re-sults over the periods. 6.2. Optimality In the first experiment, we study how SBS policies im-prove over time. Algorithm SBS is initialized with an uninformative prior P 0 = Dir( 1 ), where 1 is a vector of all ones. We refer to this policy as SBS 0. Our approach is compared to two baselines. The first baseline GBS 0 assumes that all items are drawn with the same probability, GBS ( I , Q , X  0 ) where  X  0 ( i ) = This can be viewed as an upper bound on the number of questions asked by SBS 0. The second baseline GBS  X  knows the probability  X   X  with which target items are drawn, GBS ( I , Q , X   X  ). This can be viewed as a lower bound on the number of questions asked by SBS 0. The performance of all policies is measured by the average number of questions asked up to episode  X  . The lower the number, the better.
 Our results are reported in Figure 1. In Figure 1a, the average number of questions asked by SBS 0 decreases over time to 9.5. This is 0.7 questions less than GBS 0, which asks 10.2 questions, and similar to the best solu-tion in hindsight GBS  X  . Note that the Shannon entropy H (  X   X  ) of  X   X  is a lower bound on the expected number of questions to find a randomly chosen item from  X   X  . In our case, H (  X   X  )  X  6 . 8. Relatively to this baseline, the improvement from 10.2 to 9.5 is 21%.
 Similar trends can be observed in Figure 1b. The aver-age number of questions asked by SBS 0 decreases over time to 7.8. This is 0.7 questions less than GBS 0, which asks 8.5 questions, and similar to the best solution in hindsight GBS  X  . Relatively to H (  X   X  ), the improvement from 8.5 to 7.8 is 41%. 6.3. Impact of the Prior P 0 In the second experiment, we study how a well-chosen prior P 0 helps in speeding up the convergence of SBS to the lower bound GBS  X  . We compare three variants of SBS . In the first variant SBS 0, the prior P 0 = Dir( 1 ) is uninformative. In the second and third variants, the prior is P 0 = Dir( 1 + c ), where c  X &lt; M and c ( i ) is the number of occurrences of item i in 100 and 1k random movie viewing events, respectively. We refer to these variants as SBS 100 and SBS 1 k , respectively. Our results are summarized in Figure 2. In both plots, we observe that SBS 1 k asks less questions on average than SBS 100, which asks less questions than SBS 0. In other words, SBS performs better when the prior P 0 is more accurate. All variants of SBS approach the lower bound GBS  X  as time increases. In this work, we propose two user-adaptive algorithms for interactive content discovery, SBS  X  and SBS . Both algorithms learn user preferences over time as the user interacts with them. We proved that SBS  X  is Bayesian optimal and achieves sublinear cumulative regret; and that SBS is Bayesian near-optimal and achieves sublin-ear [1  X  ln (  X   X  min )]-scaled cumulative regret. We evalu-ate SBS on a real-world problem and demonstrate that its performance is similar to the best solution in hind-sight.
 We assume that user X  X  target items i  X  are drawn i.i.d. from a stationary distribution  X   X  (Section 2) to sim-plify the exposition of our work. This assumption can be quite easily relaxed. In particular, both of our al-gorithms can be adapted to the target items that are drawn from time-varying distributions or follow an ex-ogenous Markov chain. As long as the user preferences are exogenous, our algorithms would work. The only difference would be that learning of more complicated models takes more time.
 This is the first work that studies learning in adaptive submodularity (Golovin &amp; Krause, 2011). In particu-lar, we show that a nearly optimal policy for solving an adaptive submodular problem whose model is ini-tially unknown, identifying M  X  1 non-target items by asking the minimum number of questions on average, can be learned from solving the problem repeatedly. A key observation in our solution is that the expected gain of asking questions, conditioned on any sequence of questions and answers, can be estimated efficiently from another statistic, the probability of drawing tar-get items  X   X  . Therefore, our learning problem reduces to estimating  X   X  . We believe that this decomposition could be useful in solving other adaptive submodular problems.
 Perhaps surprisingly, it can be shown that the cumu-lative regret of Algorithm SBS  X  is O (1). In particular, learning of the minimum-depth tree can be viewed as choosing one tree out of |Q| M M ! trees of at most |Q| depth. Based on the union bound and Hoeffding X  X  in-equality, the regret of the policy that chooses in each episode the minimum-depth tree given all past target items is  X ( N |Q| 3 ). This regret bound is problematic since |Q| is often large. We believe that our bound is much more practical because it is linear in M and |Q| , and deriving such a bound is challenging. Even when t is relatively large, our bound is expected to be tighter than a higher-order polynomial O (1) bound. We leave derivation of practical O (1) bounds for future work. Bertsekas, Dimitri. Dynamic Programming and Opti-mal Control . Athena Scientific, Nashua, New Hamp-shire, 2012.
 Cesa-Bianchi, Nicolo and Lugosi, Gabor. Prediction, Learning, and Games . Cambridge University Press, New York, NY, 2006.
 Cover, Thomas and Thomas, Joy. Elements of Infor-mation Theory . John Wiley &amp; Sons, Hoboken, New Jersey, 2006.
 Dasgupta, Sanjoy. Analysis of a greedy active learning strategy. In Advances in Neural Information Pro-cessing Systems 17 , pp. 337 X 344, 2005.
 Dasgupta, Sanjoy, Lee, Wee Sun, and Long, Philip.
A theoretical analysis of query selection for collab-orative filtering. Machine Learning , 51(3):283 X 298, 2003.
 Golovin, Daniel and Krause, Andreas. Adaptive sub-modularity: Theory and applications in active learn-ing and stochastic optimization. Journal of Artificial Intelligence Research , 42:427 X 486, 2011.
 Lam, Shyong and Herlocker, Jon. MovieLens 1M Dataset. http://www.grouplens.org/node/12, 2012. Nowak, Robert. The geometry of generalized binary search. IEEE Transactions on Information Theory , 57(12):7893 X 7906, 2011.
 Yue, Yisong and Guestrin, Carlos. Linear submodular bandits and their application to diversified retrieval.
In Advances in Neural Information Processing Sys-
