 In some cases, such as federated retrieval, aggregated search or blog search, it is necessary to perform retrieval over multip le sources of information, such as feder-ated collections, verticals or blogs. For a g iven user X  X  query often only few sources contain relevant information. Therefore, it is useful to filter out all non-relevant sources and search only those that are lik ely to contain relevant documents. The task of selecting such sources is referred to as resource selection and is usu-ally performed by a centra lized broker. Resource selection was introduced and extensively studied in the area of distributed information retrieval [1,2,3,4,5].
Due to operational constraints (e.g., s ources do not provide direct access to their content) and for efficiency reasons resource selection is often performed based on incomplete samples of documents obtained from each federated sour-ce [6,7,8]. This introduces uncertainty into the selection process making it risky [9]. In this paper we approach the problem of risk-aware resource selection in the following way. First, we propose to perfo rm resource selection based on the dis-tributionofdocumentscoresineachfed erated source. Second, we derive a gen-eral formula of the variance of source scor es, thus capturing their uncertainty. Third, following the risk-aware approach to IR [10], we incorporate the variance of source scores into the resource select ion process and develop a risk-aware re-source selection technique, where the am ount of allowed risk is controlled by a risk-adjusting parameter.

We analyze two recently proposed distri buted retrieval scenarios, namely fed-erated Web search [11] and shard ranking [12], and show that many queries are risk-sensitive, i.e. require a risk-aware component when performing resource se-lection. We also show that the proposed r isk-aware resource selection technique with appropriate values of a risk-adjusting parameter gives significant improve-ments of resource selection performance. Many resource selection techniques ha ve been proposed in the literature. Large-document approaches (LD) represent each federated source as a concatenation of its documents and, given a user X  X  query, rank these large documents using standard IR ranking functions, such as INQUERY in CORI [1] and language modelling in [13]. Small-document techniques (SD), instead, make use of a cen-tralized index of documents sampled fro m federated sources (denoted as CSI for centralized sample index ). These documents are ranked with respect to a user X  X  query and sources are select ed based on the number of documents they contribute to the top of such ranking [2,3,4,5], [12], [14]. SD techniques were shown to out-perform LD methods [2,3], [5]. Therefore, we will follow the small-document idea in this paper. There is also a large volume of literature on efficient, supervised and theoretical methods for resource selection [8], but we do not describe them here as they are not directly related to the current work.

Despite many studies performed on resource selection, still little is understood about its uncertainty and the risk, associ ated with the selection process. Markov et al. [9] addressed the uncertainty in re source selection by p erturbing various components of SD techniques, such as the document retrieval function, query and ranking of sampled documents. This approach managed to improve the performance of SD methods in some cases. However, this was done at the cost of performing resource selection multipl e times, thus, hurting its efficiency. In this paper we propose a resource select ion technique, which allows estimation of uncertainty, associated with source sc ores, using a single r esource selection run. It then incorporates th e estimated risk directly into the selection process, resulting in risk-aware resource selection.

Our method is based on the distribution of document scores, similarly to [15,16,17]. Baumgarten [16] models document scores using the Gamma distri-bution and estimates the parameters of the distribution assuming a particular retrieval model, namely probabilistic retrieval. The recently proposed Taily re-source selection technique [15] infers score distributions from term statistics, cooperatively provided by federated so urces, and also assumes a specific docu-ment retrieval function. Then both studies calculate source scores as the area under the right tail of the estimated score distribution. However, the approach in [16] is tied to a particular retrieval model and particular score distribution, while Taily requires certain cooperation f rom federated sources, which is rarely available in practice. Our approach, instead, is completely uncooperative and does not depend on a specific retrieval method and/or score distribution model. But the main novelty of our technique is that it considers the risk associated with source scores and incorporates i t into the resource selection process. In this section we first describe the idea of using score distributions for resource selection (called DSDE in this paper, which stands for Document Score Dis-tribution Estimation). Then we show how the variance of source scores can be computed and incorporated into the selection process, resulting in a risk-aware resource selection (RA-DSDE). Finally, we instantiate our general approach us-ing a normal distribution. 3.1 Score Distributions for Resource Selection Small-document resource se lection techniques aim at s electing those sources that contain the largest number of relevant documents for a given user X  X  query. Since the actual relevant documents are not known, the top-ranked documents are used instead [4]. Thus, SD resource selectio n methods aim at selecting the sources that have the largest number of top-ranked documents for the query.

Following the SD approach, we first define a threshold  X  and calculate the number of documents in a source R that have scores higher than  X  (we consider these documents as high scoring): where d denotes a document and q denotes a query.

We assume that the distribution of doc ument relevance scores in each source can be approximated by a probability distribution with a distribution function F and a density function f . The area under the density function to the right of the threshold  X  is the approximate fraction of high scoring documents in R :
By combining Equations 1 and 2 we can calculate the expected number of high scoring documents per source with a closed-form expression and use it as a score for ranking sources: where | R | = | d  X  R | is the number of documents in R and  X  is the parameter of the DSDE approach, which can be used for tuning the algorithm according to user needs. Since relevance scores are c omparable across diff erent sources, the fitted models are also comparable and, th erefore, we can use the same threshold  X  for all sources. The value of  X  used in our experiments is discussed in Section 4. 3.2 Expectation and Variance of Source Scores Equation 3 gives an estimation of a score of a source R based on its distribution function F ( x ). Since the exact function F ( x ) is not known and has to be ap-proximated for a given query, it becomes also a function of a set of parameters  X  : F = F ( x ;  X  ).

The parameters  X  are approximated based on the observed scores of docu-ments sampled from R with the maximum likelihood estimation (MLE). Using the delta method, it can be shown that in this case the expectation and variance of the distribution function F (  X  ;  X   X  ) are calculated as follows: where I (  X   X  ) is the Fisher information matrix and n is the number of documents sampled from R .

Based on the above discussion, Equation 3 can be rewritten as follows: This means that score ( R | q ) is just an approximation of the true score ( R | q ). Combining this equation with Equations 4 and 5 we can calculate the expectation and variance of source score estimates: In particular, Equation 7 means that the proposed source score estimate is un-biased and asymptotically co nverges to the true score. 3.3 Risk-Aware Resource Selection Following the risk-aware approach to IR [10], we combine expectation and vari-ance of source scores in the following way: Here b is a risk-adjusting parameter, where b&gt; 0 produces the risk-averse rank-ing and b&lt; 0 gives the risk-inclined ranking of sources.

The final formula for calculating risk-aware source scores based on score dis-tribution models is the following: where the values of F ,  X  X   X  X  and I  X  1 are calculated at the point (  X  ;  X   X  ). Note that this formula is generic with regards to a score distribution model F and any appropriate distribution can be used in practice. 3.4 Choosing a Score Distribution Model Various combinations of probability distributions were proposed to model scores of relevant and non-relevant documents: two normal, two exponential, two Pois-son, two Gamma and two log-normal distributions [18]. Also a number of mix-tures were proposed, where the scores of non-relevant documents were modeled by an exponential or a Gamma distribution, while the scores of relevant doc-uments were modeled with a normal, a Gamma or a mixture of normals [18]. However, the most widely used score distribution model is a mixture of an ex-ponential and normal distributions [18,19]. Therefore, we use this model in our work.

Since our resource selection method is model-agnostic and does not necessarily require a mixture of distributions, we also use a number of single distributions to model document scores, because they h ave less parameters, are easier to es-timate and simpler to work with. In particular, we use a Gamma, a normal and an exponential distributions. Preliminary experiments showed that the mixture, Gamma and normal models performed sim ilarly to each other, while the expo-nential distribution performed significantly worse than others. This is because we model the scores of top-ranked documents and, therefore, the Gamma and normal provided better fit than the exponential model, while the mixture re-duced to a single normal in most cases. Since most of the chosen models provide similar performance, we will use a normal distribution in this work, as it is the simplest to operate with. Note, however, that the following discussion is fully applicable to other models that provide a good fit of document scores. 3.5 Normal-Based Instantiation The normal distribution is defined with a probability density function f and a distribution function F as follows: where erf( x )= 2  X   X  x 0 e  X  t 2 dt is known as the error function . The MLE estimators of the mean  X  and variance  X  2 for each source are the following:
The Fisher information of a normal distribution and its inverse are given by the following matrices:
Finally, the partial derivatives of the distribution function F with respect to the parameters  X  and  X  are the following:
Substituting the generic expressions in Equation 10 with the above formulas for the normal distribution, we obtain the following equation for source scores:
Below we evaluate the proposed risk-aware formula against its basic version ( b = 0) and other state-of-the-art r esource selection techniques. In this section we describe evaluation scen arios, testbeds, parameter values and the overall DIR setup used in our experiments.
 Scenarios and Testbeds. We consider the following state-of-the-art cases, where resource selection has a direct app lication: federated Web search [11] and shard ranking [12].

In order to simulate these scenarios we adopt three testbeds, as in [20], which are the different splits of the TREC GOV2 dataset and contain about 22 million documents. The largest 1000 domains of GOV2 are used as 1000 Web search en-gines, constituting the gov2.1000 testbed. Since the GOV2 dataset is the crawl of the real Web, gov2.1000 can be used to simulate the federated Web search scenario. The domains of gov2.1000 are clustered into 250 partitions using the average-link agglomerative clustering (the gov2.250 testbed), making them more topically homogeneous. Therefore, gov2 .250 can be seen as an intermediate sce-nario between federated Web search and shard ranking. Then gov2.1000 is also clustered into 30 partitions, creating the gov2.30 testbed. This testbed has topi-cally homogeneous sources a nd, therefore, represents th e shard ranking scenario. We use the titles of the TREC topics 701-850 from the Terabyte Track 2004-2006 as queries.
 Parameter Settings. We compare the DSDE and RA-DSDE techniques to ReDDE [4] and CRCS [3], which were shown to be effective unsupervised SD resource selection methods [3], [5]. We co nsider the exponential and linear ver-sions of CRCS denoted as CRCS(e) and CRCS(l) respectively. Based on our preliminary experiments, t he original papers and recen t resource selection stud-ies, the parameters of the methods are se t as follows: ReDDE considers the top 20 documents to be relevant, CRCS(e) uses  X  =0 . 28, CRCS(l) considers  X  =20 documents to be relevant.

DSDE needs to estimate the distribution of document scores for each feder-ated source. For this purpose we use the top 100 documents retrieved from a centralized sample index (CSI) for a give n query for each source. The threshold  X  is set to the score of the 10th document in a centralized ranking produced by CSI. The risk-adjusting parameter b of the RA-DSDE approach will be discussed in detail in Section 5.
 DIR Setup. Both the federated Web search a nd shard ranking scenarios as-sume a low-cost (and sometimes uncooperative) environment, where the full centralized index of all documents is not available. In this case a number of doc-uments are sampled from each federated source and stored in CSI. In cooperative scenarios, such as shard ranking for topically partitioned collections, a uniform sample of documents can be obtained [12] . In uncooperative environments the query-based sampling technique [21] can be used instead and we follow this ap-proach here. Since the size of the three co nsidered testbeds is the same, i.e. 22 million documents, we make their CSIs to be also of the same size, i.e. 300K documents for each testbed. This resu lts in 300 sampled documents from each source of the gov2.1000 testbed, 1200 documents from each source of gov2.250 and 10000 documents from each source of gov2.30.

Since the actual size of sources may be unknown in uncooperative environ-ments, we estimate it using the multiple capture-recapture algorithm with 80 queries [22]. The Terrier 1 implementation of BM25 is used as a document scor-ing function for CSI. After resource sel ection is performed, the query is sent to the selected sources and 100 documents are retrieved from each of them. The ob-tained results are merged using CORI [1] as presented in [23], because it showed the best performance in preliminary e xperiments. P@10 and MAP are used to evaluate the quality of merged results. In this section we perform an empirical e valuation of the proposed risk-aware resource selection technique. We start b y analyzing the optimal values of the risk-adjusting parameter b and studying the sensitivity of queries to risk. We then compare the performance of the RA-DSDE approach to basic DSDE and other state-of-the-art resource sel ection methods, namely ReDDE and CRCS. 5.1 Risk-Adjusting Parameter Intuitively, the sensitivity to risk vari es across queries: some queries tend to be risk-averse, i.e. require reducing the risk ( b&gt; 0), while others are risk-inclined, i.e. take risks and gain performance improvement ( b&lt; 0) [24]. Examples of different behaviors are given in Fig. 1. Here the improvement of p@10 is plotted against the values of b on a log-scale for the shard ranking scenario (the gov2.30 testbed).

Fig. 1 shows that the topic 819,  X 1890 Census X , exhibits a risk-averse behavior and has higher p@10 when the uncertainty is reduced. On the other hand, the topic 826,  X  X lorida Seminole Indians X , shows better performance for negative values of b . The differences in risk preferences can be attributed to the specificity of query terms and the overall difficulty of a query.

Table 1 presents the number of risk-sensitive queries, i.e. queries that benefit from a risk-aware component in terms of p@10 and MAP 2 . When consider-ing p@10 as a performance measure, th e federated Web search scenario (the gov2.1000 testbed) benefits from accounting for the risk in 63 cases out of 150 (42%). The intermediate scenario (the gov2.250 testbed) has half of queries be-ing risk-sensitive, while in the shard ranking scenario (the gov2.30 testbed) 87 out of 150 queries (58%) benefit from a risk-aware component. Thus, when opti-mizing for p@10, on average half of the queries are risk-sensitive across various scenarios. The shard ranking scenario, having larger sources, tend to have more risk-sensitive queries then the federa ted Web search scenario. This is because larger sources contain more relevant doc uments and, therefor e, are more sensi-tive to reranking.

Table 1 shows that more queries are risk-sensitive with respect to the MAP performance measure compared to p@10. 97 queries out of 150 (65%) benefit from a risk-aware component in terms of MAP for the federated Web search scenario, 125 queries (83%)  X  for the intermediate case and 133 queries (89%)  X  for the shard ranking scenario. This behavior is intuitive, because even if accounting for risk does not affect precision at rank 10, it often improves precision at other ranks, thus improving MAP. This suggests that aggregate performance measures are more sensitive to risk and may benefit a lot from risk-aware resource selection. Also note that here, similar to p@10, the number of risk-sensitive queries increases for testbeds with larger sources.

The number of risk-averse ( b&gt; 0) and risk-inclined ( b&lt; 0) queries are also shown in Table 1. For p@10, on average, around 60% of risk-sensitive queries are risk-averse and 40% are risk-inclin ed. This means that more queries benefit from reducing the uncertainty in source ranking. Still there is a large portion of queries that improve performance by taking risks. For MAP these numbers are similar having slightly more risk-inclined queries. This can be explained by the aggregate nature of MAP: the risk-inclined behavior, even if hurting p@10, may improve precision at other ranks, thus improving MAP. 5.2 Retrieval Results Resource selection performance is sh own in Figs. 2  X  4. Here we plot p@10 and MAP against the number of selected sources. The performance of the RA-DSDE approach is given when optimized for p@10 and for MAP.

First, note that the basic DSDE approach ( b = 0) outperforms the baseline methods for the shard ranking scenario in terms of p@10 in almost all cases (see Fig. 4). It also shows better p@10 when 1 and 2 sources are selected for the intermediate scenario (Fig. 3) and the f ederated Web search scenario (Fig. 2). However, the improvements are minor and not statistically significant.
On the other hand, as expected the optimized RA-DSDE technique signifi-cantly improves p@10 and MAP in all cases. This shows the potential of the risk-aware approach and the amount of improvement that can be achieved by considering the risk in resource selection. In particular, the average improve-ment of RA-DSDE(p@10) over the basic DSDE method in terms of p@10 is 28% for the gov2.1000 testbed, 32% for gov2.250 and 24% for gov2.30. In terms of MAP, the RA-DSDE(MAP) method gives 17% improvement for the gov2.1000 testbed, 20% improvement for gov2.250 and 13% for gov2.30 over the DSDE baseline. These results show that both in the federated Web search and shard ranking scenarios significant improvements are possible by considering the risk-aware component during resource selection and optimizing the risk-adjusting parameter b .

Finally, note that RA-DSDE optimized for MAP also performs well in terms of p@10, although not as good as RA-DSDE(p@10). However, the opposite is not true, i.e. RA-DSDE(p@10) does not improve the performance of DSDE in terms of MAP in about half of the cases. These results are intuitive, as MAP optimizes the average performance of res ource selection, including precision at rank 10, while p@10 optimizes only early precision. This suggests that aggregate performance measures, such as MAP, must be used for tuning the risk-adjunsting parameter and optimizing the RA-DSDE approach. In this paper we approached the problem of risk in resource selection. To this end we performed resource selection based on the distribution of document scores in each federated source. This method, being as effective as state-of-the-art resource selection techniques, provides a closed-form solution for the variance of source scores. Following the risk-aware approach to IR, we incorporated the variance into the source score estimates, thus, dev eloping a risk-aware resource selection method. The amount of risk in this method is controlled by a risk-adjusting parameter that has to be set on a per query basis. Finally, we showed how to in-stantiate the proposed approach based on a normal distribution. Note, however, that since our method is model-agnostic, any other suitable distribution model canbeusedinasimilarway.

The empirical study of the federated Web search and shard ranking scenarios revealed that many queries are, in fact, risk-sensitive. The experimental results suggest that aggregate measures, such as MAP, are more sensitive to risk, than measures like precision at a certain rank. A lso the number of risk-sensitive queries varies slightly across testbeds and is higher for those with larger sources. The experimental evaluation also showed that by considering risk in resource selection its performance could be improved up to 32% in terms of p@10 and up to 20% in terms of MAP.

As for future work, we plan to study what features affect the risk preference of a particular query. Then those features can be used to predict an optimal value of the risk-adjusting parameter for a given u ser X  X  query. The second direction is to apply portfolio theory to resource selection [25]. To this end, we plan to estimate the correlation between different sources and incorporate this correlation into the resource selection process.

