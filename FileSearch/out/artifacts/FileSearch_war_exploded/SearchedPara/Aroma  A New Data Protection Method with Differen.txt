 We propose a new local data pert urbation method called Aroma. We first show that Aroma is so und in its privacy protection. For that, we devise a rea listic privacy game, call ed the exposure test. We prove that the  X  X  algorithm, a previously proposed method that is most closely related to Aroma, performs poorly under the exposure test and fails to provide sufficient privacy in practice. Moreover, any data protec tion method that satisfies  X  -differential privacy will succeed in the test. By proving that Aroma satisfies  X  -differential privacy, we show that Aroma offers strong privacy protection. We then demonstrate the utility of Aroma by proving that its estimator has significantly smaller errors than the previous state-of-the-art algorithms such as  X  X  , AM, and FRAPP. We carry out a systematic empirical study using real-world data to evaluate Aroma, which shows its clear ad vantages over previous methods. H.2.8 [ Database Management ]: Database Applications Data perturbation; Differential privacy; Query. 
With the growth of modern technologies such as the Internet and sensor networks, the amount of data that is made available for analysis and mining is increasing at an amazing speed. Business-es, government agencies, and research agencies all greatly benefit from analyzing and making use of the data. However, this bur-geoning data has also caused seri ous concerns on privacy breach-es. Consequently, it is increasi ngly crucial to provide rigorous privacy protections for individuals while enabling data utilization as effectively as possible for various parties. 
A common example is a table that stores individuals X  medical records. While it is much needed for researchers or decision-makers to query aggregate inform ation about the data (say, dis-eases over a geographical region, or within a company), it is even more important that personal privacy be protected. In this case, an individual would not want her disease informati on to be public. Clearly, there are two key issues: (1) the privacy of individuals and (2) the utility of data after protection techniques are applied. 
Privacy for published data is an elusive notion that seemingly resists any definition that is commonly useful in practice. A lot of privacy definitions have been proposed in recent years, e.g., k -anonymity [30], l -diversity [19], t -closeness [17], (  X  [13],  X  -indistinguishability [12],  X  -amplification [5], and ( d ,  X  )-privacy [26]. However, various problems and privacy holes have been found in some of these definitions, which is partly the reason why one is proposed after anot her. A promising one is the  X  -differential privacy from the cryptography community [11, 12]. It is a rigorous and strong definition that is independent of the com-putational power and auxiliary information (a.k.a. background knowledge [20]) available to the adversary. 
In this work, we design a realistic privacy game, called an ex-posure test . It is a mental game between a guard of a privacy preservation scheme and an adversary . The design of the exposure test is based on a practical scenario called the linking attack [30], which is the root motivation for a whole line of work in the data privacy literature. In fact, Sweeney showed that, using the linking attack, she could discover the medical record of a previous Mas-sachusetts governor [30]. 
One promising data perturbation me thod in previous work is the  X  X  algorithm [26]. It is shown to satisfy the definition of ( d ,  X  )-privacy while offering an improved accuracy over the previously best local data perturbation method FRAPP in its matrix-theoretic framework [5]. However, we show that the  X  X  algorithm fails our exposure test and therefore it do es not provide sufficient privacy protection in practice. Furthermore, we prove that any scheme that satisfies the  X  -differential privacy will succeed in the exposure test. This result supports the sign ificance of the differential priva-cy definition. 
The main result of this work is the proposal of a new local data perturbation method, called Ar oma (a random matching ap-proach). In addition to showing that Aroma satisfies the ( n , t )-closeness [18] definition of privacy, we prove that it satisfies the  X  -differential privacy (hence it succeeds in the exposure test). The high level idea of Aroma is to weaken the link between a sensitive attribute and the non-sensitive attributes of a relation. This is illus-trated pictorially in Figure 1. 
Here we informally explain how Aroma perturbs data; we will describe it more precisely in Sec tion 3. In Figure 1, each node on the left-hand side of the graph indicates the collection of all non-sensitive attributes of a record, wh ich includes both identifiers and quasi-identifiers [30]. The non-sensitive attributes of Alice X  X  rec-ord link to four sensitive values (for the disease attribute) in a value pool P . P is actually a probability distribution, which we call the pool distribution. The true sensitive attribute value of Alice is the one connected by a solid arrow (i.e., diabetes), while the other three sensitive values (i .e., pneumonia, etc.) are random-ly drawn from P according to its distribution. Bob X  X  record as shown in Figure 1 is similarly linked. Clearly in the published view, we do not disclose which link is the solid one. At this point, it becomes apparent that a parameter of Aroma is the number of links that a record has, which we denote as k . k plays a crucial role in Aroma X  X  privacy guarantee and is a knob for a tradeoff between privacy and utility. 
As in previous work [16, 26, 37, 39, 12, 4], when we discuss utility, we mainly focus on the accuracy of the estimator for COUNT queries. We analyze and compare the accuracy of Aro-ma X  X  estimator with three other st ate-of-the-art data perturbation methods: (1) the  X  X  algorithm (non-local data perturbation), (2) the FRAPP (local perturbation), and (3) an output perturbation method in [16] which we call AM (an adaptive mechanism). We show that Aroma also has superior accuracy than those algorithms. 
In a general sense, utility br oadly means how useful the per-turbed data is for its users. Besides the estimator accuracy, there are other aspects in us age that makes Aroma an appealing method: (1) It uses a local perturbation (whi ch is the most widely appli-(2) It is easy to handle data upda tes. Privacy is trivially pre-(3) We can easily have multiple privacy levels. That is to say, Related Work. Privacy protection methods broadly have two categories: (1) data perturbation (a.k.a. input perturbation) in which some form of noise is added to the data itself [2], and (2) output perturbation , in which noise is added to the otherwise ac-curate query answers. Data perturbation methods can further be classified into (1a) local perturbation in which individuals do not need to trust anyone and can anonymize their own data, and (1b) non-local perturbation , in which a central server trusted by all individuals is required. 
There are a few algorithms for local perturbation (category 1a) [3, 28, 13, 14, 4, 23, 5]. Agrawal and Haritsa [5] define a matrix-theoretic framework that generali zes all algorithms proposed thus far in this category, and propose one (called FRAPP) that achieves the provably optimal privacy/utility within that framework. FRAPP anonymizes data as follows. For each tuple in the original table T , with probability  X  (a parameter) it stays the same in the output view V ; with the remaining probabili ty it is replaced by a tuple chosen uniformly at random from the whole domain D . We show (in Sec. 5) that, under th e same privacy, Aroma achieves better utility. This result does not contradict FRAPP X  X  optimality proof because Aroma does not fall in the matrix-theoretic frame-work [5], although Aroma is a local perturbation method. 
Non-local data perturbation methods (category 1b) include [30, 15, 35, 27] and it is shown in [26] that the state-of-the-art algo-rithm in this category is the  X  X  algorithm. It proceeds as follows. (1)  X  -step. For each tuple in the original table T , insert it into the output view V with probability  X  +  X  ; (2)  X  -step. For each tuple in the whole tuple domain D but not in T , insert it into V with proba-bility  X  .  X  and  X  are two parameters. We show that the  X  X  algo-rithm fails our exposure test and does not provide sufficient priva-cy in practical settings, while Aroma satisfies differential privacy and succeeds in the exposure test. Moreover, we show that Aroma also has better utility than the  X  X  algorithm. 
The state-of-the-art output pert urbation method (category 2) achieves  X  -differential privacy by addi ng a noise term that follows a Laplace distribution [12, 11]. Ou tput perturbation requirements for the basic setting where a databa se server has to answer multi-ple queries during its lifetime are also studied [12, 37]. We exper-imentally compare Aroma with this output perturbation method and show that, in practice, Arom a provides much better utility overall as the database needs to process multiple queries. 
As mentioned earlier, a privacy definition that is useful in prac-tice and that ensures good privacy under realistic attack s is elusive. Differential privacy has seen wi de usage since its introduction (e.g., [16, 34, 22, 37, 21, 39]). We prove Aroma X  X  differential privacy in this work. Among thos e methods that satisfy differen-tial privacy, AM [16] is the most state-of-the-art work and shows better accuracy than other methods. Hence, we compare Aroma X  X  estimator accuracy with AM under different  X  (a parameter in differential privacy). The main AM algorithm approximates the optimal strategy for any workload of linear counting queries. AM has a mechanism that automatically adapts to the set of submitted queries and provides significantly improved utility over other work. It focuses on batch query answering and provides a matrix based approach which achieves differential privacy by adding noise to output answers. AM divi des each sensitive attribute into several ranges so the whole dataset maps to a multi-dimension matrix and each combination of th e sensitive attributes maps to a cell in that matrix. Accordingly, however, query predicate ranges, even for non-sensitive attributes, must be on those cell boundaries (otherwise accuracy can be lost significantly). 
Finally, data updates and multiple privacy levels have been ad-dressed for other privacy protecti on methods before (e.g. [32, 7, 36, 38]). Privacy protection for different kinds of datasets and models has also been studied (e.g . [10, 40, 8, 9, 41, 31]). While these are not the focus of our work, we show that it is remarkably easier for Aroma to achieve th em than previous methods. Our Contributions. The contributions of this paper are as follows:  X  We devise a novel data pert urbation method Aroma and its  X  We thoroughly show the privacy of Aroma. We prove its ( n ,  X  We analyze the accuracy of the Aroma estimator and show  X  We experimentally validate the utility of Aroma on two real ( n , t )-closeness. The ( n , t )-closeness principle [18] is a generaliza-tion of the previous t -closeness that the same authors propose [17]. This is because the authors of [18] demonstrate that t -closeness is often too restrictive in practice (hard to satisfy), and it limits the release of useful information. Definition 2.1 (The ( n , t )-closeness Principle) [18]. An equiva-lence class is the smallest unit (i.e., set of records) in a perturbed table that an adversary can loca te an individual. An equivalence sensitive values in the population distribution that is a superset of distance between the two distributions (sensitive values in E and N) is no more than a threshold t. A table is said to have (n, t)-closeness if all equivalence classes have (n, t)-closeness.

The distance t can be based on a standard distance metric be-tween two distributions (e.g., the Earth Mover X  X  distance [18]). The intuition is that the distribution of the sensitive attribute over a set of individuals with a sufficiently large size ( n ) should be public knowledge for the purpose of utility. We would only like to guard against finding out the real distribution of a small number of individuals. Thus, ( n , t )-closeness requires that the apparent distribution of the sensitive attri bute within any equivalence class ( the smallest unit that the adversary can locate an individual ) is close to that of a large population (size n ). n and t are two param-eters of the definition. The larger n is, or the smaller t is, the more privacy we get. Example 1. Table 1 [18] shows a toy example, where, based on the quasi-identifiers ZIP Code and Age, there are three equiva-lence classes (two tuples each) which we call E 1 , E 2 , and E 3 in the order that they appear in the table. The Disease attribute is sensitive and the Count attribute indicates the number of individ-uals. E 2 satisfies (1000, 0.1)-closene ss because it contains 2000 &gt; 1000 individuals and thus meets the privacy requirement (by set-closeness because both have exactly the same distribution (the distribution is (0.5, 0.5)) as N = E 1  X  E 3 , which has 1000 individu-als. In fact, clearly the table also satisfies (1000, 0)-closeness. Differential Privacy . A rigorous privacy preservation definition that has been recently proposed in cryptography is the concept of differential privacy [12, 11]. Definition 2.2 (Differential Privacy) [12]. An algorithm A gives  X  -differential privacy if for all database instances D and D  X  differ-ing on at most one record, and all S  X  Range(A), 
The privacy parameter  X  controls the amount by which the dis-tributions induced by the two databases may differ under algo-rithm A . Smaller  X  values enforce a stronger privacy guarantee (when  X  is close to 0, exp(  X  ) is close to 1). Intuitively, an algo-rithm A that satisfies differential privacy implies that even if any individual arbitrarily changes her sensitive attribute value (thus making the database instance ch ange by only one record), the query (algorithm A ) will still return about the same result. This clearly protects each individual X  X  privacy. Differential privacy offers a very strong theoretical gua rantee, since it is a statistical property about the behavior of the algorithm A and therefore is independent of the computational power and auxiliary information (a.k.a. background knowledge [20]) available to the adversary. 
A table T contains a set of l non-sensitive attributes { A  X  l ) and a set of s sensitive attributes { S i } (1  X  i  X  s ). For simplici-ty, we call all non-sensitive attributes quasi-identifiers (or qid X  X  ) [30]. Thus, a quasi-identifier is a general term and an identifier (if it exists) is part of a quasi-identifier. Usually s is small (e.g., 1 or 2). Without loss of generality, we only consider the case of s = 1, because when s &gt; 1, Aroma anonymizes each sensitive attribute separately. We denote the sensitive attribute as S . Let the domain of S be D s and the domain of { A i } (1  X  i  X  l ) be D present (a.k.a., publish) a view V of table T , such that individual X  X  sensitive data is protected, while V is useful for data analysis. We will be more precise on privacy and utility shortly (Sec. 4 and 5). 
Aroma is a randomized algorithm. Its local perturbation method is shown in Fig. 2. The intuition is as follows. For each tuple, we replace its sensitive value by k values; one of them is the original true value, while the other k  X  1 values are randomly chosen from a distribution P . We also ensure that the distribution of these k values is somewhat close to P (e.g., they are not all the same value, but have some diversity). The i nput includes the original table T and a probability distribution P on the sensitive values, called the pool distribution . P is encoded as a set of  X  values in the distribu-tion and their corresponding integer weights  X   X   X  X  X   X  X 1 X  , which indicate the corresponding va lue X  X  frequency. In other words, if we let  X  X   X   X   X   X   X  X  X  X  , then m i / m is the probability of v
View V is initially an empty set of tuples (line 1). For each tuple in the original table T , we sample k  X  1 times (with replacement) mixing these k  X  1 values with the true sensitive value of the tuple, s (line 7). As we show in Sec. 4.1, lines 7 to 12 are to ensure the ( n , t )-closeness property. If it is violated, we go to line 3 and sam-ple again (line 10). We show in Sec. 4.1 and in the experiments that resampling is rare for a reasonable setting of k . Finally, we replace the original true sensitive value of the tuple t by the set of k values that we obtain, and add this perturbed tuple to the output view V (lines 13 and 14). An illust rative example is shown in Sec. 1 and Fig. 1. 
As in previous work [26, 37, 39, 12, 4], we examine how well one can estimate count queries of the following form with Aroma: The estimation algorithm is shown in Figure 3. 
Many query optimizers of DBMS X  X  convert an arbitrary predi-cate list to a conjunctive normal form (CNF). We assume the input count query is already normalized into CNF. In line 1, we parti-tion the predicate_list into two parts: P 1 contains a maximum number of clauses that does not involve the sensitive attribute S do since it is in CNF. We then set Q to be the set of records in V that satisfies P 1 (line 2). This can be done because P tains qid X  X  and Aroma does not cha nge any qid X  X . Note that keep-ing the qid part of the table as it is enhances estimation accuracy. This is in contrast with other methods such as the generalization-based algorithms [30, 15] and the  X  X  algorithm [26] (which adds fake tuples with random qid X  X ). 
The notation in line 3 means the probability that a random sen-sitive value chosen from P , conditioned on that the qid part is randomly chosen from Q (thus we get a whole random tuple), will Then in line 4, we just observe from V what fraction of the total of kq sensitive values in Q actually satisfies P 2 . The final step (line 5) simply returns an estimate based on these values. Example 2. Consider a simple example. Table T only contains three attributes: name, company, and disease. The first two are qid X  X  and the last one is S (sensitive attribute). Now we use the Aroma perturbation algorithm (in Figure 2) and get a view V. We employees in company A that has cancer (because we suspect that company A has a dangerous working environment and does not follow the rules to protect employees). Thus, we are interested in the result of Q 1 divided by the result of Q 2 : V in total has 5,000,000 records (individuals), among which 10,000 are company A X  X  employees. Our estimation has to be based on V, but not T. Clearly we can get the true answer for Q 2 (10,000) since qid X  X  remain unchanged in V. We use the estima-tion algorithm for Q 1 . P 1 is company =  X  X  X  and P  X  X ancer X . Thus, in line 2 of the algorithm, Q is the 10,000 records of company A (q = 10,000 ). Suppose in the pool distribution P, the fraction of value  X  X ancer X  is 0.1 . Hence, in line 3, f = 0.1 , as P does not involve any qid X  X . In line 4, f* is based on the kq (i.e., 50,000 ) actual random values in Q. Suppose we find out that f* = 0.14 . Then, in the final step, the estimator returns 10,000  X  [5  X  0.14  X  4  X  0.1] = 3,000 . Dividing this value by Q 2  X  X  result, we get that the estimated fraction of em ployees in company A that have cancer is 0.3 , which is significantly higher than the fraction in P.
It is not hard to understand line 5 of the estimator. Within the q records in Q , kqf* is the total number of sensitive values that satis-fy P 2 , while ( k  X  1) qf is the expected number of random values from distribution P that satisfy P 2 . Thus, the difference of the two values is the expected number of true values that satisfy P it is an unbiased estimator, by calculating its variance, we can reason on its accuracy (in Section 5). In this section, we demonstrat e the strong privacy guarantees of Aroma. We first show that it satisfies the ( n , t )-closeness property in a general sense. We then introduce a privacy game, which ex-poses a practical and serious privacy leak of a previously pro-posed data perturbation scheme. On the other hand, we prove that any scheme that satisfies differential privacy will win this game; this verifies the strong privacy requi rement of differential privacy. Finally, we show that Aroma satisfies differential privacy, which is a formal guarantee of its privacy. 
Aroma does not use data generali zation and it keeps all identifi-ers and quasi-identifiers; thus each equivalence class is just one record (with k sensitive values). The scheme hides an individual X  X  sensitive value in a pool distribution P = {( v 1 , m 1 m )}, where d = | D s |. Essentially P describes a population with m individuals that have value v 1 , m 2 individuals that have value v and so on. Let  X  X   X   X   X   X   X  X  X  X  . Moreover, this population can be of an arbitrarily large size as we scale up all m i  X  X  and m . For example, we can use a uniform distribution in which all m i  X  X  are equal. Thus, the parameter n in ( n , t )-closeness satisfies n  X  m . Specifically we of a record is exactly the same as the distribution in N . Example 3. We may have P = {(  X  X lu X , 1000) , (  X  X neumonia X , 1000) , (  X  X ancer X , 1000) , (  X  X eart disease X , 1000) , (  X  X yspepsia X , 1000) , (  X  X iabetes X , 1000)} and n = 3000 . Since all m 6000 ), a necessary (but not sufficient) condition of ( 3000 , 0 )-closeness is that the k values of a record must contain at least three distinct values (similar to l-diversity). Note that the privacy property stays the same if we multiply each of m i  X  X , m, and n by a factor (e.g., m i = 1 , m = 6 , and n = 3 ). Thus, n is relative to m. We have the following result: Theorem 4.1 Aroma ensures ( n , 0)-closeness in the output view. Furthermore, consider one tuple t. Suppose its true sensitive value is v a (1  X  a  X  d ) in P. Let  X  X   X   X  X 1 X   X   X   X  above. Then the probability that line (10) of the algorithm is ever executed is no more than 1  X  p, and the expected number of times that the loop between lines (3) and (5) is executed is no more than 1 /p. As k increases, this num ber can be arbitrarily small. Proof. We first show that Aroma ensures ( n , 0)-closeness. When the Aroma algorithm finishes, for each tuple, it must be true that because otherwise the condition in line (9) would be satisfied and the algorithm would not finish yet. Now, let c = min each distinct value v that is v i in P (as in line 8 of the algorithm), it must be true that c  X  k i  X  m i (because c is the minimum ratio be-tween m i and k i ). Thus, if we scale up each distinct value in S factor of c , the multi-set will still be contained in the m values of c  X  k  X  n ) within the m values of P , such that the distribution in S exactly the same as the distribution in N . This proves that Aroma ensures ( n , 0)-closeness in the output view. 
We now prove the second part of the theorem. Consider one tuple t and d events as follows: When any of the E i happens, line (10) of the algorithm will be executed. Define d  X ( k  X  1) random variables: where 1  X  i  X  d and 1  X  j  X  k  X  1 ( j iterates over the random choices of the tuple t ). We further define d random variables: where 1  X  i  X  d . Thus, X i is the count of v i within S now have E( X ij ) = Pr( X ij = 1) = m i / m . From the linearity of expec-tation, for 1  X  i  X  d and i  X  a , E( X i ) = ( k  X  1) m E( X i  X  1) = ( k  X  1) m i / m . Let  X  i = ( k  X  1) m the case i  X  a and 1  X  i  X  d . Define: Thus, Pr( E i ) = Pr( k i &gt; km i /n ) = Pr( X i &gt; km As X i is the sum of 0/1 random variables, we can use Chernoff bounds [24] to bound the probability of event E i : Similarly, for the case of i = a , we consider X variable and let  X  be the following value instead: so that Pr( E i ) = Pr( X i  X  1 &gt; km i /n  X  1) = Pr( X Now we can use Chernoff bound on X i  X  1: with the  X  value in (2), we have the same inequality as (1). Let p i be the r.h.s. of (1), we have that the probability that none of the E i  X  X  happens is at least p , and hence line 10 of the algorithm is ever executed with probabil-ity no more than 1  X  p . The number of times that the loop between lines 3 and 5 is executed is a ge ometric distribution with a param-eter at least p ; thus the expectation is no more than 1/ p . It is also easy to check that as k increases, the bound 1/ p decreases. 
Let us look at an example. Let d = 10, m i =1,000 (for 1  X  i  X  10), m = 10,000, n = 3,000. By choosing k = 30, we have that the probability that line 10 of the algorithm is ever executed is no more than 0.056. Note that the constant factors in m i , m , and n do not affect the results; if we had m i =1, m = 10 and n = 3, we would get the same bounds. By making 1  X  p low, we can ensure the effi-ciency of the Aroma algorithm. Let us look at a privacy game, which we call a p-exposure test. Definition 4.1 [p-exposure] Consider a data perturbation meth-od M that converts an or iginal table T into a perturbed view V. Let the domain size of the sensitive attribute S be d. Consider the following privacy game, which has two players, the adversary A and the guard G. (1) A picks an individual t X  X  qid and passes it to G. (2) G composes a table T. First, t is in T and G picks a sensitive (3) G then runs M on T and gets an output view V. We assume (4) G passes V to A, who will then try to tell t X  X  sensitive value Let the probability that A succeeds be p. We say that M is p-exposed. 
Clearly, the most private method is 1/ d -exposed since A can at least make a choice uniformly at random and the probability of success is 1/ d . A method with good privacy should have a p value that is low (close to 1/ d ). The privacy game is based on a very realistic setup that the adversar y recognizes someone in the per-turbed data through either identifiers (if they are kept in V ) or quasi-identifiers (say, by a join w ith another table [30]). In fact, this is the motivation for most of the previous work in this area (e.g., [30, 15, 19, 18, 35]). Thus, the p -exposure test is a basic one that all practical pr ivate data perturbation methods should pass. 
We next show that the  X  X  algorithm [26] (described in Sec. 1.1) is not private under the p -exposure test. Theorem 4.2 The  X  X  algorithm is at least  X  X  X  X  where d is the domain size of S, as in Definition 4.1 . Proof. In the privacy game, regardless of what sensitive value that G picks for t , A  X  X  strategy is very simple : she just returns the sen-sitive value in a record that she finds in V that matches t  X  X  qid. We now compute A  X  X  success probability: preserve the original sensitive value and thus A will succeed. The first inequality is based on the definition of conditional probability and the union bound [24]. The second inequality is because there are d tuples in D (set of all possible tuples ) that have the same qid as t (but only differ in S ). Then again due to union bound, we have the second inequality. This concludes the proof. 
The  X  X  algorithm requires that  X  +  X  should not be too small and  X  should be very small in order to have a good utility (which is the main advantage of the algorithm compared to its previous work). For example, the authors [26] recommend choosing  X  +  X  =  X  and  X  = 0.002325 (as in their experiments) in order to satisfy their privacy and utility goal s. However, based on Theorem 4.2, this means that the algorithm is at least 0.91 -exposed (say, for d = 20). Therefore, it is not private. By contrast, we show that any pertur-bation method that satisfies di fferential privacy performs well under the exposure test. Theorem 4.3 An  X  -differential privacy method is at most Proof. We show that there exists a strategy for G to choose a sen-sitive value for t in step (2) of the game such that, in the end, A  X  X  success probability is at most  X  ly to choose a sensitive value uniformly at random from the do-main. That is, each sensitive value v i (1  X  i  X  d ) is chosen with probability 1/ d . 
We now construct d tables T 1 , T 2 , ..., T d . They are identical ex-cept for one record, namely t ; table T i has sensitive value v  X  i  X  d ). Let the probability that A is successful when the game uniformly at random (thus choosing one of the T i random), the success probability of A is: Now consider the case when the game proceeds with T 1 . Clearly, Consider Pr( A returns v 2 | T 1 ). Recall that T 1 and T one record and Pr( A returns v 2 | T 2 ) = p 2 . From the definition of  X  -differential privacy, Likewise, Because Pr( A returns v 1 | T 1 ) + ... + Pr( A returns v add up the r.h.s. of (2), (3), (4), etc., we have: In the same manner, if we consider how the game proceeds with T , we have: Similarly, for T 3 ..., we also have such inequalities, until for T Now we add up these d inequalities (5), (6), etc., we have: Dividing both sides by d , we have: Finally, combining (1) and (7), we have: 
For  X  -differential privacy,  X  is meant to be small (e.g., 0.3), which makes exp(  X  ) close to 1. As  X  approaches 0, exp(  X  ) ap-proaches 1, and 1/[1+( d  X  1)/exp(  X  )] approaches 1/ d , the smallest possible exposure. Theorem 4.3 verifi es that differential privacy is a strong guarantee and will expe ctedly pass the exposure test. 
We now prove the differential pr ivacy of Aroma, demonstrating that it has a strong privacy property. Theorem 4.4 Let k and f be the same as in the estimation algo-rithm of Figure 3. Aroma satisfies the  X  -differential privacy if k satisfies the following condition: Proof. Recall the estimation algo rithm of Aroma. Among the q records that satisfy P 1 , suppose c records actually satisfy P count query returned by Aroma. Then, where B follows a binomial distribution B(( k  X  1) X  q , f ). Define: From (1) and (2), we have: Now towards the goal of analyzing differential privacy, we have E ( E 2 ) to be  X  X roma returns a count value v under table T Consider E 1 first. Using (3), we have: 
Without loss of generality, we analyze T 2 under three cases: (a) q is the same (as with T 1 ), while c increases by one; (b) q increas-es by one, while c stays the same; and (c) both q and c increase by one. These correspond to the different outcomes when the two tables differ by only one record. Note that the remaining case where both q and c stay the same would make Pr( E thus trivially satisfying  X  -differential privacy. First consider case (a). From (2), when c increases by one, b decreases by one. There-fore, based on (4), we have Now divide (4) by (5): Plugging in the definition of b in (2), we have Since 0  X  v , c  X  q (any count value outside the valid range should have probability 0; if Aroma obta ined such a value, it should be trivially truncated to 0 or q ), we know that  X  q  X  v  X  c  X  q . Observe that decreasing v  X  c causes (6) to increase. Hence, where the last inequality is because when q is at its minimum ( q = 1), the expression reaches its maxi mum. Thus, with case (a), it suffices for  X  -differential privacy if the r.h.s. of inequality (7) is no more than exp(  X  ), which is to say, Let us now consider case (b), in which q increases by one, while c stays the same. From (2), this implies that b increases by ( k  X  1) X  f excellent approximation of B(( k  X  1) X  q , f ) [6, 25], we have: Dividing the two equalities above and simplifying the result using (2), we have: Since 0  X  v , c  X  q , we have 0  X  ( v  X  c ) 2  X  q 2 . Hence, from (9), where the last inequality is due to q /( q +1)  X  1. Thus, from (10),  X  -differential privacy for case (b) is satisfied if: (b), we eventually have: The inequality above indicates that  X  -differential privacy for case (c) is satisfied if: Apparently, (11) is subsumed by (12). Thus, based on (8) and (12), we can choose a k value that ensures  X  -differential privacy. 
Let us see an example. If f = 0.2 and  X  = 0.5, inequality (8) re-quires k  X  18 and inequality (12) requires k  X  20. Therefore, choosing k = 20, we achieve 0.5-differential privacy. 
We compare the accuracy of Aroma with the state-of-the-art da-ta perturbation methods and show that Aroma has be tter accuracy. It is shown in [26] that, under the same privacy level, the  X  X  algo-rithm yields smaller errors than the previous state-of-the-art algo-rithm, FRAPP, which is optimal among local perturbation algo-rithms that fit in their matrix-theoretic framework [5]. Like FRAPP (and unlike  X  X  ), Aroma is a local perturbation method; but it does not fit in the matrix-theoretic framework in [5]. 
We have shown that  X  X  fails the exposure test and it is not pri-vate; thus it is not comparable with Aroma on accuracy under the same privacy level . However, even if we followed the same priva-cy perspective as in [26], namely the posterior probability of the adversary Pr[ t  X  T | V ], we could show that Aroma has better accu-racy than both  X  X  and FRAPP, given that they have the same pos-terior probability for the adversary. 
Since all three estimators are unbiased, like in [5], we use the variance of the estimation to compare the accuracy of different methods. Intuitively,  X  X  has a larger error for two reasons: (1) It samples with a tiny probability  X  from the whole domain D which is likely to be huge (the cross product of the cardinality of all attributes of T ); thus the variance of this part tends to be large. (2) Only with probability  X  an individual X  X  data in T is actually kept in V . By contrast, Aroma does not discard any individual X  X  data, and the fake values are more focused on protecting the true rec-ords. These differences make Aroma not only have stringent pri-vacy guarantees, but also have excellent accuracy in its estimation. Theorem 5.1 Under the same posterior probability Pr[ t for the adversary, Aroma X  X  estimation has a smaller variance than both the  X  X  and the FRAPP algorithms. Proof. We first calculate the variance of the Aroma estimator in Figure 3. Recall Equation (1) in the proof of Theorem 4.4: where C is the estimator result and B follows a binomial distribu-Let us calculate the variance of  X  X   X  X  estimator [26]. We rewrite the estimator with uppe r-case letters denoting random variables: where N is the estimator result, N V is the number of records in view V that satisfy the predicate list, and n D is the number of rec-ords in the whole domain D (all possible tuples) that satisfy the predicate list. Note that the only random variable on the r.h.s. N consists of two parts: where N 1 ( N 2 , resp.) is the number of records sampled from the original table T (whole domain D , resp.) that satisfy the predicate list. Thus, N 1 follows B( n T ,  X  ) and N 2 follows B( n the number of records in T that satisfy the predicate list. Therefore, together with (2) and (3), we have: We now compare the variances of the two estimators, Equations that satisfy the predicate list in Aroma X  X  view, while n Since the ratio of true values between Aroma and  X  X   X  X  views are 1/  X  , in order to maintain the same posterior probability, we have ( k  X  1) X  q  X  f = n D  X  /  X  . Combining this with (1) and (4), we get: rem 4.3 in [26]. As 0 &lt; 1  X  f &lt; 1, from (5), it is clear that Var( N ) &gt; Var( C ), i.e., the  X  X  estimator has a larger variance than Aroma. Its result can be written as: where p is the probability that a tuple in T stays put in V , E number of records in T that satisfy the predicate list and appear in V , and E 2 is the number of records that satisfy the predicate list in V , but not in T (because they are replaced by random choices). It is not hard to see that E 1 follows B( f 1 q , p +(1  X  p ) f ) while E B((1  X  f 1 ) q , (1  X  p ) f ). Thus, from (6), after some algebraic manipula-tion, we have: 1/ p . Thus, comparing (7) with (1), we can see that the variance of the FRAPP estimator is more than a factor of k  X  1 larger than that of Aroma. This completes the proof. appealing data perturbation method. Local perturbation. Unlike the  X  X  algorithm or data generaliza-tion, Aroma is a local perturbation method. That is, each individu-al (i.e., each record) in the original table T can generate its own share of data (i.e., perturbed record) in the output view V ; there is no dependency between two individuals during data perturbation, and the final view V contains nothing more than these perturbed records from each individual. vidual and other entities and does not require a trusted central server to gain information other than s/he can observe from the final view V . Therefore, local perturbation is the most widely applicable in practice. For exam ple, when individuals want to share their sensitive data for data mining purpose in a distributed environment, it may be very difficul t to find a trusted third party. The individuals are clearly more motivated to (honestly) share their data if they can perturb their own data. Local perturbation also leads to the next advantag e of Aroma, ease of updates. Update capability. A side effect of a local perturbation method is When a record r is inserted into or deleted from the original table T , if r  X  X  corresponding piece of perturbed data in the view V is r one can simply insert r V into V , or delete r V from V , respectively. This is because V can be partitioned into independently generated pieces, one for each individual. Note that when the same record r appears in multiple versions of V (e.g., if it is deleted and then inserted back), we must ensure that r V stays the same (i.e., no re-randomization; otherwise the adve rsary can narrow down the true value by finding the intersection of the two versions of r updates. The ease of handling updates in Aroma is in sharp con-trast with non-local perturbation me thods such as data generaliza-tion and the  X  X  algorithm (which requires a trusted central server), where handling updates is much more complicated, and usually requires sacrificing the accuracy of their estimators. Multiple privacy levels. Multiple privacy levels are often desira-ble in many applications. For ex ample, imagine a data warehouse contains medical insu rance data of employ ees in various compa-nies in a big region. There are different types of users of the data-base. Company managers may have a perturbed view V ance agents may have a perturbed view V 2 ; and doctors may have a perturbed view V 3 , etc. These views may have different privacy requirement; thus by adjusting a data perturba tion method X  X  pa-rameters, we may get different privacy vs. estimation accuracy tradeoffs. This issue is also recently studied in [38], where more motivations are discussed. 
One important issue here is to ensure that privacy is not com-promised when there is collusion between two clients that have different privacy levels. Let us examine this issue with Aroma. Clearly, by having different k parameter values for different cli-ents while generating the perturbed views, we get different priva-cy levels. Imagine client 1 has a view V 1 generated with parameter k , while client 2 has a view V 2 generated with parameter k these two clients collude, for a record r (corresponding to the same individual) in their views, its true sensitive value must be in the intersection of the set of k 1 values at client 1 and the set of k values at client 2, which gives th em a higher level of privacy priv-ilege than either of them alone. 
The solution of this problem is very simple under Aroma. We require that, for two views ( V 1 , V 2 ) with k 1  X  k 2 sensitive values in V 1 must be a subset of the k 2 values in V we ensure that even if two clients collude, their combined view is merely the same as one of their original views (i.e., V 1 while this problem is also discussed in [38], our solution based on Aroma is much simpler. Moreover, their solution is based on a perturbation method in the same framework as FRAPP, which has a larger estimation errors as shown in Sections 5.1 and 6. experiment with a real dataset, the U.S. census data available at the Minnesota Population Center web site [43]. The dataset is commonly used in the literature of data privacy protection (e.g., [37, 38, 39]). The table that we use has 10 million records and its schema contains the following attributes: age , gender , education , and occupation . As in previous work, we treat occupation as a sensitive attribute to be protected, and it contains 15 categories of occupations. In addition, to compare with [16], we use an adult dataset [42] which was also used in [16]. The table has 33K tuples and contains 14 attributes. As in [16], we treat occupation as the sensitive attribute. The experiments were run on a 2.4 GHz Intel Core 2 Quad CPU machine with 1 GB memory. views. In Section 4.3, we analyzed the  X  -differential privacy of Aroma. One of the key parameters of Aroma is k , the number of sensitive values associated with a record. We first evaluate the necessary k values in order to achieve various  X  values (as in dif-ferential privacy) in a real set ting. We use a uniform pool distribu-tion P , and our target query asks for the fraction of a specific sen-sitive value in a group of records. Based on Theorem 4.4, we plot the relationship between  X  and k in Figure 4. As can be seen, a bigger  X  in differential privacy indicates that less privacy is re-quired (and hence more utility can be obtained), which corre-sponds to a smaller k value for Aroma. R ecall from Equation (1) in Theorem 5.1 that a smaller k value implies smaller variance in the estimation result, i.e., more accuracy. (i.e., line 10 of the Aroma algorithm in Fig. 2 is executed) in order to satisfy the ( n , 0)-closeness as analyzed in Theorem 4.1. The results are shown in Figure 5. First, in Figure 5(a), we examine the amount of re-sampling that is necessary for various k values which we observe in Figure 4 (corresponding to  X  = 0.3, 0.5, ...). We fix n = 4 m i for this figure. Recall that m i is the number of i 'th distinct value in the pool distribution P . Since P is a uniform dis-tribution, all m i  X  X  are the same. We also show the theoretical bounds that we obtain in Theorem 4. 1. Note that a necessary (but insufficient) condition for n = 4 m i is that at least four distinct val-ues must be present in the k values associated with a record. small fraction of records that need re-sampling. This fraction is virtually 0 when k is above 35. Moreover, the re-sampling need in practice is much lower than the theoretical bounds, which makes them a safe guarantee. Fig. 5( b) shows how the re-sampling frac-tion changes with n ( n / m i = 4 is equivalent to n = 4 m be 49, which corresponds to  X  = 0.5 in Fig. 4. When n gets bigger, clearly ( n , 0)-closeness becomes harder to satisfy. sumption. Denote the number of distinct values in distribution indicating the number of occurrence of a value among the k values of a record. Thus, each record requires  X   X   X  X  X   X  the values in P and each record requires  X   X   X   X  X  X  sensitive attribute. This is a different format (by listing the k val-ues) than the case of k &gt; | P |, where we list the counts of each value in P . It is easy to see that in both cases we choose the more com-ma X  X  estimation on query results as it relates to various parameters. We issue a query that asks for the fraction of a specific sensitive value in a group of records of size q . The results that we show are based on the average of 20 runs over different instances of Aroma perturbed views. We show the resu lts in Figure 6. From the origi-nal (unperturbed) data, we know the true answer to the query, from which we can then find out the error of Aroma X  X  estimation. plain this using our analysis of variance. Equation (1) in Theorem average error is the square root of the variance (i.e., the standard deviation). Thus, the analysis is c onsistent with Fig. 6(a). We also show this theoretical average error in the same figure. Here we fix k to be 49. The actual errors are very close to the standard devia-tion. In fact, in our experimental results, we find a consistent pat-tern that the errors we get are almost always slightly smaller than the standard deviation, reflecting a small systematic error. privacy) offers more privacy but lower utility. Finally, in Fig. 6(c), we look at how errors change as we vary the pool distribution P , from which we draw the k values for a record. The parameter here is f , the probability that we obtain the sensitive value in question as we draw samples from P. We pick a few f values between 0 and 1. The figure shows that the error is highest when f is about 0.5. This again verifies our variance analysis. Specifically, in Equation (1) of Theorem 5.1, the factor  X  X  X  X 1 X  X  reaches its max-imum at  X 0.5 X  and its minimum at the two ends. Accuracy comparisons with output perturbation. We now compare Aroma with Output Perturbation (OP), a classical tech-nique that achieves differential priv acy [12, 37]. Note that we do not compare with the additional technique of query relaxation as proposed in [37] because query relaxation gives the result of a modified query, which has much dependency on which queries have already been issued and is not directly comparable with Aroma. The results are shown in Figure 7. In Figure 7(a), we compare OP with Aroma as the database system continuously answers queries. We set the  X  parameter (as in di fferential privacy) for both schemes to be 0.5. For Aroma, this implies that k = 49, as given in Figure 4. The query workload is scattered across the whole table, focusing more on 30 hot areas of the table, thus imi-tating the uneven range query regi ons in reality. A hot area around a tuple is modeled with a normal distribution with a stand-ard deviation of 5000 tuples. That is, when a hot area is chosen, a random tuple is selected from this distribution and is used as the center of the next range query. We implement the technique pre-sented in [37] that uses a histogram approach to approximate the sensitivity of a set of queries S( Q ), which is necessary to deter-mine the added noise for OP. deteriorates as the system answers more queries. The error will eventually be so large that the system is useless. By contrast, Aroma X  X  result error fluctuates around a low value, regardless of how many queries the system has answered. Then we look at the errors of both Aroma and OP for various  X  parameter values, as shown in Figure 7(b). For OP, we show the error when the data-base system is answering the first query, the 1000 X  X h query, and the 2000 X  X h query, respectively. When  X  decreases, more privacy is required, both OP and Aroma incur bigger errors, hence a lower utility. Moreover, although OP in itially has a smaller error than Aroma, as more queries are an swered, it soon has much greater errors than Aroma. Accuracy comparisons with  X  X  X  and FRAPP. In this set of ex-periments, we compare the accuracy of Aroma X  X  estimator with that of the  X  X  and FRAPP estimators. For the  X  X  algorithm, we follow the discussion on choice of parameters in Section 4.2 of [26] and set  X  +  X  =  X . We then choose  X  such that the algorithm has the same posterior probabilit y as Aroma per our discussion in Section 5.4. We use k = 49 for Aroma in Figure 8. Note that the  X  X  algorithm is not really comparable with Aroma since it fails the exposure test as shown in Theore m 4.2. Nonethel ess, we still compare its accuracy with Aroma to verify our analysis in Section 5. The result of this comparison is shown in Figure 8 for various q values similar to Figure 6(a). We can see that Aroma is consist-ently more accurate than the  X  X  estimator. This verifies Theorem 5.1. The theoretical errors refe r to their standard deviations. ous k values. We choose the corre sponding parameter values for FRAPP based on the condition that the two algorithms have the same posterior probability as used in Theorem 5.1. Figure 9 shows that FRAPP has much bi gger errors, especially as k (priva-cy) increases. This verifies our result of Theorem 5.1 that FRAPP has about O( k 1/2 ) larger errors. query, namely the AVG query. Th e query asks fo r the average age of people whose occupation (sensitive attribute) is a certain value. The estimation algorithm for Aroma is similar to the one in Figure 3. As with COUNT queries, we analyze separately the true values and the fake values. The result of an AVG query is simply an estimate of the sum divided by an estimate of the count. It is also easy to see how to answer such an AVG query under FRAPP. We omit the details in the interest of space. The result is shown in Figure 10. Once again, we see that for AVG queries, too, Aroma has significantly smaller errors. estimator, as compared with FRAPP. Aroma views are imple-mented based on the strategy disc ussed in Section 6.1. Compared to FRAPP, the query processing of Aroma requires some addi-tional overhead on reading out the extra bytes of sensitive values and unpacking them. However, si nce the representation is so compact, the execution overhead ratio over FRAPP is low. This is shown in Figure 11. The execution overhead is clearly accepta-ble. Accuracy of estimation and privacy are our main concerns. Accuracy comparisons with AM. In this experiment, we com-pare the accuracy of Aroma X  X  estimator with AM [16]. We use the Adult dataset [42] since it is also used in [16]. AM is the most state-of-the-art method which has the same goal as ours  X  to pro-tect data under differential priv acy while provide query answers as accurate as possible. In [16], the authors compare AM with some other data protection methods and show that AM has a bet-ter accuracy under the same level of  X  -differential privacy. ar counting queries, we execute a query SELECT COUNT(*) FROM adult WHERE age &gt; 30 AND occupation =  X  X raft-repair X , for the counts of records that satisfy a predicate that contains both a non-sensitive and a sensitive attr ibute. We use relative error to measure the answer accuracy, and compare the accuracy at five different  X  -differential privacy levels. For each  X  , we run the que-ry for five times to get the average error.
 error of Aroma is 1.5 to 3.5 times lower than AM, while their accuracies are very close when  X  is large. The relative error of Aroma is less than 0.1 even when  X  is around 0.2. This indicates that Aroma can afford to provide high privacy while maintaining low accuracy loss to query answering. This result is also con-sistent with the evaluation in [16].
In this paper, we devise a nove l local data perturbation method called Aroma. We show that Ar oma has rigorous privacy guaran-tees. It satisfies  X  X , X  X  -closeness and differential privacy. Moreo-ver, we propose a realistic privacy game called the exposure test, and show that a previously proposed method fails the test while any method with differential priv acy passes the test. Furthermore, we analyze, as well as empirically demonstrate, the superior query answer accuracy and utility of Aroma. Acknowledgments. This work was supported in part by the NSF, under the grants IIS-1149417, IIS-1239176, and IIS-1319600. 
