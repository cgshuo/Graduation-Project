 The kernel function plays a central role in kernel methods. In this paper, we consider the automated learning of the kernel matrix over a convex combination of pre-specified kernel matrices in Regularized Kernel Discriminant Anal-ysis (RKDA), which performs linear discriminant analysis in the feature space via the kernel trick. Previous studies have shown that this kernel learning problem can be for-mulated as a semidefinite program (SDP), which is however computationally expensive, even with the recent advances in interior point methods. Based on the equivalence rela-tionship between RKDA and least square problems in the binary-class case, we propose a Quadratically Constrained Quadratic Programming (QCQP) formulation for the ker-nel learning problem, which can be solved more efficiently than SDP. While most existing work on kernel learning deal with binary-class problems only, we show that our QCQP formulation can be extended naturally to the multi-class case. Experimental results on both binary-class and multi-class benchmark data sets show the efficacy of the proposed QCQP formulations.
 H.2.8 [ Database Management ]: Database Applications -Data Mining Algorithms Quadratically constrained quadratic programming, kernel learning, model selection, kern el discriminant analysis, con-vex optimization
Kernel methods [21, 22] have been applied successfully in various machine learning tasks, such as dimensionality Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. reduction, clustering, classi fication, and regression. They work by embedding the data into some high-dimensional fea-ture space through the so-called kernel function .Thekey fact underlying the success of kernel methods is that the em-bedding into feature space can be uniquely determined by specifying the kernel function that computes the dot prod-ucts between data points in the feature space. In other words, the kernel function implicitly defines the nonlinear mapping to the feature space and expensive computations in the high-dimensional space can be avoided by evaluating the kernel function. Therefore, one of the central issues in kernel methods is the problem of kernel selection.
The problem of automated kernel learning has been an ac-tive area of research recently. Lanckriet et al. [12] pioneered the work of learning a linear combination of pre-specified kernels for Support Vector Machines (SVM) [5, 26] using convex programming and it has been improved in [3] using the Sequential Minimal Optimization (SMO) algorithm [20]. Reformulation of this problem as semi-infinite linear pro-gram has been proposed in [23] along with extensions to re-gression and one-class classi fication. While most approaches produce stationary combinations, Lewis et al. [13] proposed to use different combinations for different inputs. Argyriou et al. [2] proposed to combine potentially an infinite number of kernels, which was formulated as a difference of convex (DC) program. In general, approaches based on learning a convex combination of kernels facilitate heterogeneous data integration from multiple data sources and it has been ap-plied for integrating various types of biological data, e.g. , amino acid sequences, hydropathy profiles, and gene ex-pression data for enhanced biological inference [11]. Jebara [9] considered the kernel selection problem in the context of multi-task learning. Zien and Ong [30] recently studied multi-class multiple kernel learning.

This paper addresses the issue of kernel learning for reg-ularized kernel discriminant analysis [16, 17]. The proposed algorithms learn an optimal kernel matrix as a convex com-bination of a set of pre-specified kernel matrices. In this sense, our proposed methods are similar to those proposed in [10, 12] and can thus be used for heterogeneous data in-tegration. The problem of kernel learning for discriminant analysis has been addressed originally in [6]. Kim et al. [10] formulated this problem as a semidefinite program (SDP), and hence the globally optimal solution can be obtained. The algorithms in [6, 10] deal with binary-class problems only. They were extended to the multi-class case in [28]. One limitation of the SDP formulation is its high computa-tional cost, even with the recent advances in interior point methods. In this case, the overhead of model (kernel) se-lection may dominate the learning process. We address this issue in this paper. Key contributions of this paper include:
The rest of this paper is organized as follows: Section 2 provides background information on kernel methods, as well as the SDP formulation for kernel learning. Section 3 derives the QCQP formulation for the binary-class case. Section 4 extends the QCQP formulation to the multi-class case. Sec-tion 5 presents our experimental study and this paper con-cludes with discussion and conclusion in Section 6.
Let X denote the input or instance space, which is a sub-space of IR d ,and Y = { X  1 , +1 } denote the output or class where x  X  X  and y  X  X  , is called positive (negative) if its class label y is +1 (  X  1). We assume that the examples are drawn randomly and independently from a fixed, but unknown, underlying probability distribution over X X Y .
A symmetric function K : X X X X  R is called a ker-definite property: for any x 1 ,  X  X  X  ,x m  X  X  ,the Gram matrix G  X  IR m  X  m , defined by G ij = K ( x i ,x j ) is positive semidefi-nite. Any kernel function K implicitly maps the input set to a high-dimensional (possibly infinite) Hilbert space H equipped with the inner product (  X  ,  X  ) H K through a mapping  X 
In kernel-based classification, the algorithms learn a clas-sifier f : X X  X  X  1 , +1 } whose decision boundary between the two classes is affine in the feature space H K : where w  X  X  K is the vector of feature weights, b  X  IR i s t h e intercept, and sgn( u )=+1,if u&gt; 0, and  X  1otherwise. tions of data points from the positive and negative classes, respectively. let X =[ x + 1 ,  X  X  X  ,x + m + ,x  X  1 ,  X  X  X  data matrix. The total number of data points in the train-ing set is m = m + + m  X  . For a given kernel function K , the basic idea of RKDA in the binary-class case is to find a direction in the feature space H K onto which the projec-tions of the two sets {  X  ( x + i ) } m + i =1 and {  X  ( x separated. Define the centroids of the two classes as follows: and the two sample class covariance matrices as follows: Specifically, in RKDA the separation between two classes is measured by the ratio of the variance ( w T (  X  + K  X   X  between the classes to w T m + /mS + K + m  X  /mS  X  K w ,the variance within the classes. Thus, the following criterion is commonly maximized in RKDA: where  X &gt; 0 is a regularization parameter. For a fixed ker-nel function K and regularization parameter  X  ,theoptimal weight vector w  X   X  argmax w { F 1 ( w, K ) } , that maximizes the objective in Eq. (6) is given by The maximum value F  X  1 ( K )  X  max w { F 1 ( w, K ) } of the ob-jective function in Eq. (6) achieved by the optimal weight vector w  X  is given by F ( K )=(  X  + K  X   X   X  K ) T It can be shown [10] that the optimal value F  X  1 ( K )inEq.(8) can be simplified to the following: where I is the identity matrix, a is defined as a =[1 /m + ,  X  X  X  , 1 /m + ,  X  1 /m  X  ,  X  X  X  ,  X  1 /m  X  ] T  X  the matrix J is defined as: J = and e m + and e m  X  are vectors of all ones of length m + m  X  , respectively.

In [10], G is restricted to be a linear combination of the p given kernel matrices G 1 ,  X  X  X  ,G p as
G  X  X  = G G = It was shown in [10] that, for a fixed regularization parame-ter  X  , the optimal Gram matrix G computed from the kernel function K that maximizes F  X  1 ( K ) given in Eq. (9) can be obtained by solving a SDP [4, 25]. General-purpose op-timization packages such as SeDuMi [24] use the interior-point methods [18] to solve SDP. However, it is known that SDP is expensive to solve practically, and thus for problems of moderate size, this overhead of optimal kernel learning is large. In order to alleviate this computational problem, we propose in the next section a convex QCQP formulation for this kernel learning problem. Interior point methods for solving QCQP is more efficient than its SDP counterpart. Furthermore, the proposed QCQP formulation can be ex-tended naturally to the multi-class case.
In the following discussion, we work on the centered ver-sion of kernel matrices. This is equivalent to a data center-ing process. More precisely, given a set of p kernel matrices G ,  X  X  X  ,G p , the proposed algorithms learn an optimal kernel matrix  X  G  X   X  G ,where  X  G i = PG i P , r i =trace(  X  G i ), and P  X  IR m ing matrix defined as and e m is the vector of all ones of size m .

Consider the maximization of the following objective func-tion [28]: where  X  K is defined as follows:  X 
K ( X ) is the data matrix in the feature space given by  X 
K ( X )=  X  K ( x P is defined in Eq. (13), and is the global centroid of the data in the feature space. It is easy to verify that for fixed K and  X  , Eqs. (6) and (14) are equivalent for the computation of the optimal w .
Consider the regularized least squares problem, which min-imizes the following objective function: It is known that discriminant analysis and least square prob-lems are equivalent in terms of the computation of the op-timal w for fixed K and  X  in the binary-class case [15]. We show in the following lemma that discriminant analysis and least square problems are also equivalent in terms of the computation of the optimal kernel function K .

Lemma 3.1. Let w  X  , K  X  ,  X  w  X  ,and  X  K  X  solve the following optimization problems: Then w  X  =  X  w  X  and K  X  =  X  K  X  .

Proof. The optimal vector w  X   X  argmax for a fixed K is given by The maximum value of the objective function in Eq. (14) achieved by w  X  is given by F ( K )  X  F 2 ( w  X  ,K )=(  X  + K  X   X   X  K ) T ( X  K +  X I )  X  1 It follows from the Sherman-Woodbury-Morrison formula [7] that and thus Since the vector a defined in Eq. (10) is of zero mean, i.e., Pa = a ,wehave where  X  G = PGP is derived from G with both rows and columns centered. Since  X 
G  X   X  G (  X I +  X  G )  X  1  X  G =  X  G  X   X  G (  X I +  X  G ) the optimal value F  X  2 ( K )inEq.(23)canbesimplifiedas For a fixed K and  X  ,theoptimal  X  w  X  that minimizes the objective function in Eq. (17) is given by  X  w  X  =  X I +  X  K ( X ) P X  K ( X ) T The optimal value of the objective function in Eq. (17) is therefore given by Thus, the maximization of F  X  2 ( K ) in Eq. (25) is equivalent to the minimization of F  X  3 ( K ) in Eq. (27). This completes the proof.

Based on this equivalence result, we can formulate the ker-nel learning problem for RKDA as a convex QCQP problem, as summarized in the following theorem.

Theorem 3.1. Given a set of p centered kernel matrices  X  G ,  X  X  X  ,  X  G p , the optimal kernel matrix, in the form of linear combination of the given p kernel matrices, that optimizes the criterion in Eq. (17) can be found by solving the following convex QCQP problem: where r i = trace (  X  G i ) and r =[ r 1 ,r 2 ,  X  X  X  ,r p
Proof. We consider the dual formulation of the mini-mization of F 3 ( w, K )intermsof w . Denote It follows that This leads to the following optimization problem: Define its Lagrangian function as follows: L (  X , w,  X  )= ||  X  || 2 +  X  || w || 2  X   X  T ((  X  K ( X ) P ) where  X  is the vector of Lagrangian dual variables. Taking the derivative of L (  X , w,  X  )withrespectto  X  and w and setting them equal to zero, we get It follows that Thus we obtain the following Lagrangian dual function: g (  X  )=min The optimal  X   X  is computed by maximizing g (  X  )as Since strong duality holds, the optimal kernel is given by solving the following optimization problem: We can rewrite the above optimization problem as =max =max =max =max The exchange of minimization and maximization in deriving the second equation from the first holds since the objective function is convex in  X  and concave in  X  , the minimization problem is strictly feasible in  X  and the maximization prob-lem is strictly feasible in  X  . Therefore, Slater X  X  condition [4] follows and strong duality holds [12, 4]. By simply changing the last term in the above equation to t and moving it to the constraint, we prove this theorem.

Note that general-purpose optimization software packages like SeDuMi [24] and MOSEK [1] also solve the dual prob-lem. Thus the coefficients  X  1 ,  X  X  X  , X  p can be obtained from the dual variables by dividing the corresponding traces of centered kernel matrices. The formulation in Eq. (28) is a QCQP, which is a special form of Second Order Cone Pro-gram (SOCP) [14] and SDP. Theoretical results on interior point method [18] show that QCQP can be solved more effi-ciently than SDP and it is therefore more scalable to large-scale problems. The worst-case complexity of this QCQP formulation is O ( pn 3 ). Similar ideas have been used in [12] to formulate the kernel learning problem of SVM as a non-negative linear combination of some given kernel matrices.
Recall that all kernel learnin g formulations discussed in [10, 12] are constrained to binary-class problems. We show in the next section that our formulation in this section can be extended naturally to deal with multi-class problems.
In multi-class classifications, we are given a data set that consists of m samples { ( x i ,y i ) } m i =1 ,where x i  X  { 1 , 2 ,  X  X  X  ,k } is the class label of the i -th sample. Similar to the binary-class case, let X =[ x 1 ,  X  X  X  ,x m ] be the data matrix.

In the multi-class RKDA formulation, the maximization of the following objective function is commonly used [27]:
F 4 ( W, K )=trace W T ( X  K +  X I ) W where W =[ w 1 ,w 2 ,  X  X  X  ,w ]( &lt;k ) is the transformation matrix, and B K , the so-called between-class scatter matrix is defined as given by The optimal W is given by computing the top eigenvectors of the following matrix: Since the weight vectors are in the span of the images of the data points in the feature space, we can express W as W =  X  K ( X ) A for some matrix A  X  IR m  X  ,where A = [  X  1 ,  X  X  X  , X  ]. Then F ( W, K )=trace A T ( GP G +  X G ) A Define two matrices S K t and S K b as follows: Since the null space of S K t lies in the null space of S exists a nonsingular matrix Z such that [7] where  X  b is diagonal with the diagonal entries sorted in non-decreasing order. The optimal A  X  is given by where Z q consists of the first q columns of Z ,and q = rank( S K b ). It follows that the optimal value of F 4 ( W, K ) achieved by the optimal A  X  is given by Note that we have assumed S K t = GP G +  X G is nonsingular in the above discussion. In case it is singular, we could use the pseudo-inverse, and all t he following derivations still hold.
 Thus, in the multi-class case, the optimal kernel function K can be computed by maximizing F  X  4 ( K ) in Eq. (46), which is however highly nonlinear and difficult to solve. In [28], we present an equivalent formulation as the one in Eq. (46), which is more tractable computationally. The main result is summarized in the following lemma: Lemma 4.1. Let F 4 be defined as in Eq. (39) and where W =[ w 1 ,  X  X  X  ,w k ] ,and h i is defined in Eq. (41). Let W  X  , K  X  ,  X  W  X  ,and  X  K  X  solve the following optimization prob-lems: Then K  X  =  X  K  X  .

It is interesting to note that, in general, the optimal W and  X  W  X  for the optimization problems in Eqs. (48) and (49), respectively, are different. Furthermore, the objective func-tion in Eq. (47) is closely related to its binary counterpart in Eq. (14). Note that a variant of the Fisher Discriminant Ratio (FDR) [10] can be written as: Thus F 5 ( W, K ) in Eq. (47) can be interpreted as the weighted summation of the FDRs between the samples in the i -th class and the rest where i =1 ,  X  X  X  ,k .Theweightscanbe computed from the definition of H in Eq. (41) as follows: where a ( i ) is obtained from Eq. (10) by taking the samples from the i -th class as positive and the rest as negative. Thus the weight for the i -th binary classification problem is: ( n n Next, we propose a QCQP formulation for the multi-class RKDA kernel learning problem. Consider the minimization of the following objective function:
F 6 ( W, K )= where W =[ w 1 ,  X  X  X  ,w k ]. It is clear that for a fixed K ,the computations of w i and w j for i = j are independent of each other. By extending the results from Lemma 3.1 and Lemma 4.1, it is easy to show that the optimal kernel func-tion K minimizing the objective function in Eq. (47) coin-cides with the minimizer of F 6 ( W, K ) in Eq. (51). Motivated by this equivalence result, we derive an efficient QCQP for-mulation for the multi-class RKDA kernel learning problem, as summarized in the following theorem.

Theorem 4.1. Given a set of p centered kernel matrices  X  G ,  X  X  X  ,  X  G p , the optimal kernel matrix, in the form of linear combination of the given p kernel matrices, that optimizes the criterion in Eq. (51) can be found by solving the following convex QCQP problem: where r i = trace (  X  G i ) .

Proof. We first consider the dual formulation of the min-imization of F 6 ( W, K )intermsof W for a fixed K . Denote It follows that Define the Lagrangian function as follows: where the  X  i  X  X  are the vectors of Lagrangian dual variables. Taking the derivative of L with respect to  X  i and w i for all i , and setting them equal to zero, we get Thus we have and we obtain the following Lagrangian dual function: g (  X  1 ,  X  X  X  , X  k )= min The optimal  X   X  1 ,  X  X  X  , X   X  k can be computed by maximizing g (  X  1 ,  X  X  X  , X  k )as Since strong duality holds, the optimal kernel function K is given by solving the following optimization problem: min Similar to the binary-class case, the above optimization prob-lem can be written as where the second equality follows since Adding a variable t , we prove the formulation in Eq. (52).
In this section, we empirically evaluate the proposed QCQP formulations in comparison with several existing kernel learn-ing approaches. The experimental setup is similar to the one in [10]. We used the MOSEK package [1] to solve the QCQP formulations.
In the binary-class case, we compared our formulations with the SDP formulation proposed in [10], 1-norm soft mar-gin SVM, 2-norm soft margin SVM with and without the regularization parameter C optimized jointly as proposed in [12]. We also employed double cross-validation to choose kernels and regularization parameters for SVM and RKDA. Four data sets were used in the binary case. The sonar , ionosphere ,and breast cancer data sets were retrieved from the UCI Machine Learning Repository [19]. The heart data set was obtained from the STATLOG project 1 .Foreach data set, we randomly partitioned the entire data set into training and test sets with 80% of the data in the training set and 20% in the test set. Following [10], we focus on learning a convex combination of ten Gaussian kernels: The values of  X  i are chosen uniformly on the logarithmic scale over the interval [10  X  1 , 10 2 ], as in [10]. Then the ten kernels were fed into the optimization packages to obtain the corresponding coefficients for each kernel. Finally, the combined kernel was used for classification.

Tables 1 and 2 show the corresponding experimental re-sults on the sonar, heart, ionosphere ,and breast cancer data sets. The  X  value is fixed to 10  X  8 for the proposed QCQP formulation, as used in [10]. Following [12], we fix C to 1 for SM1 and SM2. We can observe from the tables that the pro-posed QCQP formulation is co mparable to three SVM-based approaches, while they are very competitive with other three approaches, including SDP Kim ,RKDA K, X  , and SVM K,C Recall that the first five methods in the tables avoid cross-validation and they can be used for heterogeneous data inte-gration from multiple sources, while RKDA K, X  and SVM K,C choose the single best kernel.

To understand the relative importance of each kernel when they are used individually, we fix the kernel to each of the ten pre-specified kernels and tune the  X  in RKDA and C in SVM using cross-validation and the accuracy of each kernel is recorded (RKDA  X  and SVM C ). We recorded the num-ber of times that a particular kernel has been selected by RKDA K, X  and SVM K,C in double cross-validation. We ex-pect these quantities to have certain relationship with the coefficients learned by convex optimizations.

For the sonar data, cross-validated RKDA achieves the best performance on kernels corresponding to  X  6 and  X  while cross-validated SVM achieves the highest accuracy on  X  ,  X  7 ,and  X  8 . On the other hand, methods using linear combination of kernels seem to favor kernels corresponding to  X  5 ,  X  6 ,and  X  7 . For the heart data, cross-validated SVM favors kernels corresponding to  X  8 ,  X  9 ,and  X  10 (they were  X  (  X  1 to  X  10 ) on the same scale for ease of presentation. chosen 10 , 9 , 9 times out of 30, respectively) while cross-validated RKDA uses kernels corresponding to  X  6 ,  X  7 ,and  X  most frequently. Our QCQP formulation gives the kernel corresponds to  X  10 a large weight while all other methods using linear combination of kernels give this kernel a zero weight. Another interesting observation is that SM1, SM2, and SM2 C give the first kernel a large weight while it per-forms poorly when used separately. Similar behavior has been observed for the breast cancer data where large weights have been assigned to the second kernel while it is not the best individual kernel. This implies that the best individual kernel may not lead to a large weight when used in combi-nation with others and poorly-performed individual kernel may contain complementary information that is useful when combined with other kernels. Such complementary informa-tion can not be incorporated when cross-validation is used to choose a single best kernel. For the ionosphere data, the best three individual kernels chosen by cross-validation are kernels corresponding to  X  5 ,  X  6 ,and  X  7 . Interestingly, the kernel corresponding to  X  5 has a zero weight for all methods using a linear combination of kernels. Overall, the pattern of the coefficients learned from our QCQP formulation is similar to those of the SVM-based methods.

We compared the running time of the proposed QCQP formulation with two other RKDA-based kernel learning al-gorithms including the SDP formulation in [10] and doubly cross-validated RKDA (RKDA K, X  ). Figure 1 shows the run-ning time of 30 different runs of these three methods, where the x -axis denotes the 30 different splits of the data into the training and test sets and the y -axis denotes the running time (in seconds). Table 3 summarizes the average running time over 30 runs. It is clear that the proposed QCQP for-mulation is much more efficient than the SDP formulation. For example, for the breast cancer data set the average run-ning time over 30 splits for SDP Kim is about 2 , 000 seconds while the QCQP formulation takes about 6 . 4 seconds. Re-sults also show that the QCQP formulation is much more efficient than doubly cross-validated RKDA.
In the multi-class experiment, we compared our formu-lations with KRDA and SVM with kernels and regulariza-tion parameters tuned using double cross-validation. The methods proposed in [10, 12] are restricted to binary-class problems only. We used a total of five data sets for this experiment. The USPS handwritten digits database was described in [8]. We choose the first 3, 6, and 8 classes with 100 samples in each class for the experiments. The wine and waveform data were obtained from UCI Machine Learning Repository and the satimage and segment were obtained from the STATLOG project. We used the first 3, 5, and 6 classes for the satimage data and the first 3 and 4 classes for the segment data. For each data set, we randomly partitioned the entire set into two subsets with 60% in the training set and 40% in the test set. Ten RBF kernels, with the  X  assigned the same values as in the binary case were constructed from the training set.

Figure 2 shows the classification results (measured in error rate) on ten data sets. We can observe from the figure that the proposed QCQP formulation is very competitive with  X   X   X  Table 3: Comparison of running time (in seconds) of three methods averaged over 30 random partitions. the other two methods based on cross-validation. Compared with the other two methods, the proposed method learns a convex linear combination of kernels by avoiding the cross-validation. It is expected to perform even better for hetero-geneous data integration, when kernels from multiple data sources contain complementary information.
We addressed the issue of automated kernel learning for discriminant analysis in this paper. We formulate this kernel learning problem as a convex program and thus a globally optimal solution is guaranteed. Practically, some convex optimization problems, such as SDP are computationally expensive and we proposed a QCQP formulation for kernel learning, which is much more efficient to solve than SDP. While most existing work on kernel learning only deal with binary-class problems, we show that our binary formulation can be extended naturally to the multi-class setting.
We conducted extensive experiments to evaluate the pro-posed algorithms. The proposed formulations are competi-tive with the SDP-based formul ation, SVM-based methods, and doubly cross-validated SVM and RKDA in classifica-tion. In terms of running time, the QCQP formulation is much more efficient than its SDP counterpart. When evaluating the relative importance of each kernel (either used separately or in linear combination), we found that the best individual kernel sometimes coincides with the highly-weighted kernels in linear combination and sometimes dis-agrees considerably. If complementary information exists among different kernels, the proposed formulations can po-tentially utilize such information.

There are several directions for future work. Most pre-vious formulations for learning SVM kernels are restricted to the binary-class case. The idea from this paper may be useful for kernel learning in multi-class SVM. Lanckriet et al. [12] considered the problem of optimizing the kernel and regularization parameter simultaneously. We plan to inves-tigate the effectiveness of incorporating the learning of the regularization parameter in the framework. The proposed QCQP formulations are still computationally expensive, es-pecially for problems with a large number of samples and a large number of classes. We plan to examine other opti-mization techniques [23, 30] for efficient multi-class multi-ple kernel learning in discriminant analysis. The proposed formulations can be applied for heterogeneous data integra-tion from multiple data sources. We plan to apply these approaches to the analysis of biological images [29]. This research is sponsored by t he Center for Evolutionary Functional Genomics of the Biodesign Institute at Arizona State University and by the National Science Foundation Grant IIS-0612069. [1] E. D. Andersen and K. D. Andersen. The MOSEK [2] A. Argyriou, R. Hauser, C. Micchelli, and M. Pontil. [3] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. [4] S. Boyd and L. Vandenberghe. Convex Optimization . [5] N. Cristianini and J. Taylor. An Introduction to [6] G. Fung, M. Dundar, J. Bi, and B. Rao. A fast [7] G. H. Golub and C. F. Van Loan. Matrix [8] J. J. Hull. A database for handwritten text [9] T. Jebara. Multi-task feature and kernel selection for classes used in each of the data sets. [10] S.-J. Kim, A. Magnani, and S. Boyd. Optimal kernel [11] G. Lanckriet, T. D. Bie, N. Cristianini, M. Jordan, [12] G. Lanckriet, N. Cristian ini, P. Bartlett, L. E. Ghaoui, [13] D. Lewis, T. Jebara, and W. S. Noble. Nonstationary [14] M. S. Lobo, L. Vandenberghe, S. Boyd, and H. Lebret. [15] S. Mika. Kernel Fisher Discriminants .PhDthesis, [16] S. Mika, G. R  X  atsch, and K.-R. M  X  uller. A [17] S. Mika, G. R  X  atsch, J. Weston, B. Sch  X  olkopf, and [18] Y. Nesterov and A. Nemirovskii. Interior-point [19] D.Newman,S.Hettich,C.Blake,andC.Merz.UCI [20] J. C. Platt. Fast training of support vector machines [21] S. Sch  X  olkopf and A. Smola. Learning with Kernels: [22] J. Shawe-Taylor and N. Cristianini. Kernel Methods [23] S. Sonnenburg, G. R  X  atsch, C. Sch  X  afer, and [24] J. F. Sturm. Using SeDuMi 1.02, a MATLAB toolbox [25] L. Vandenberghe and S. Boyd. Semidefinite [26] V. Vapnik. Statistical learning theory . Wiley, New [27] J. Ye. Characterization of a family of algorithms for [28] J. Ye, J. Chen, and S. Ji. Discriminant kernel and [29] J. Ye, J. Chen, Q. Li, and S. Kumar. Classification of [30] A. Zien and C. Ong. Multiclass multiple kernel
