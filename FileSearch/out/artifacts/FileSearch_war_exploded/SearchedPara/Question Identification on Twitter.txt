 In this paper, we investigate the novel problem of auto-matic question identification in the microblog environment. It contains two steps: detecting tweets that contain ques-tions (we call them  X  X nterrogative tweets X ) and extracting the tweets which really seek information or ask for help (so called  X  X weets X ) from interrogative tweets. To detect inter-rogative tweets, both traditional rule-based approach and state-of-the-art learning-based method are employed. To extract qweets, context features like short urls and Tweet-specific features like Retweets are elaborately selected for classification. We conduct an empirical study with sampled one hour X  X  English tweets and report our experimental re-sults for question identification on Twitter.
 H.3.5 [ Information Systems ]: Online Information Ser-vices X  Web-based services Experimentation, Performance Question Identification, Microblogs, Twitter
Twitter 1 , as the first and one of the most popular mi-croblog services, has become a platform of question asking. Morris et al. [3] reported that more than 10% of Twitter users once asked questions on Twitter. Furthermore, Efron  X 
This work was done when the first author was on internship at Google.  X  Irwin King is currently on leave from CUHK to be with AT&amp;T Labs. http://www.twitter.com/ and Winget [2] found that 13% of their randomly sampled 2-million tweets corpus were questions. According to the blog of Twitter, on average 1,620 tweets were posted every sec-ond in March 2011 2 , which means each second 210 questions appeared on Twitter.

However, not all of them need to be answered. According to the taxonomy in [2], many questions are not request-ing information but providing information, like suggesting a Q&amp;A pair or expressing an opinion. Different from the meanings of questions in [2] and [3], we restrict the scope of questions to be those tweets which require some informa-tion or help and thus need to be answered. In the following, we use the term  X  X weets X  to represent these tweets. Addi-tionally, we call the tweets which contain question sentences as  X  X nterrogative tweets X . It is worth noting that  X  X weets X  are  X  X nterrogative tweets X  but  X  X nterrogative tweets X  are not necessary  X  X weets X .

To our best knowledge, few attempts have been made on identifying qweets in vast tweets. Thus, this paper asks such research question: Can we automatically identify qweets from tweets? We know that microblog provides an instant message publishing service, so identifying these questions automatically will help question askers get answers efficiently through approaches such as question routing and automatic question answering system [5]. Identifying qweets also lays a foundation for question analysis (such as classification and clustering) in the microblog environment.

We argue that qweet identification is different from tra-ditional question finding [1, 6]. On the one hand, a tweet contains less than 140 characters, and thus less information (features) is provided; On the other hand, a tweet usually in-cludes some special features, such as Retweets (repost other users X  tweets, with  X  X T @username X  as the marker),  X  X user-name X  (mention or reply to some user), hashtags (terms starting with the charachter  X # X , usually denote topics) and short urls, which may contribute to qweet identification.
In this paper, we cast the qweet identification problem into a two-phase cascade process: In the first step, we detect the tweets which contain questions (interrogative tweets). We adopt both traditional methods like question marks, 5W1H words, and state-of-the-art approach [1, 6] which utilizes sequential question patterns to detect interrogative tweets. In the second step, we extract qweets by splitting each interrogative tweet into question part and context part and employing four kinds of features (question features, con-text features, question-context features and tweet-specific features) to build a binary classifier. As an empirical study, http://blog.twitter.com/2011/03/numbers.html. we experiment with a data set sampled from one-hour tweet stream and the experimental results demonstrate that: 1) 11% of tweets contain questions and 6% of tweets are qweets; 2) Simple rule-based approach produces satisfactory results in detecting interrogative tweets; 3) Context features and Tweet-specific features are ver y useful for extracting qweets from interrogative tweets.

The paper proceeds as follows. Section 2 presents the related work. Section 3 details the work of qweet identifica-tion. Experiments are described in Section 4 and a conclu-sion is given in Section 5.
Detecting Questions in User Generated Content (UGC). Interrogative tweets detection is closely related to question (subquestion) finding in UGC such as forums, com-munity question answering (CQA) portals and microblogs. There are two divisions for question finding in UGC: rule-based approach [2] and learning-based approach [1, 6]. Rule-based approach usually designs several rules from heuristics or observations to check whether a thread or tweet is ques-tion or not, while learning-based approach constructs a bi-nary classifier with lexical and/or syntactic features.
Analyzing Questions in Tweets. The study of ques-tions on Twitter is in its infancy and previous work mainly focused on analyzing the types and motivations of questions people asked. Efron and Winget [2] built a taxonomy of questions on Twitter and found that people asked questions on Twitter to both seek information (facts, opinions, etc.) and recommend information (external resources, invitations, QA pairs,etc.). Morris et al. [3] conducted a survey study to reveal the types, and motivations of questions people ask andansweronTwitterandFacebook.
As mentioned, we break qweet identification into the fol-lowing two sub-problems: (1) Detecting tweets which con-tain questions (interrogative tweet detection); (2) Extract-ing qweets from the previous result (qweet extraction). De-tail will be given one by one.
Previous work showed that learning-based approach out-performed rule-based approach in question detection in fo-rums [1] and CQA portals [6], so we attempt both rule-based approach and learning-based approach to investigate whether the latter one still outperforms the former in the microblog environment.
We apply question marks, 5W1H words, refined 5W1H and the rules introduced in [2] to detect interrogative tweets, detail will be presented in Section 4.2.1.
This approach utilizes sequential question patterns to train a binary classifier, which needs manually labeling data. To avoid time-costing labeling, we utilize corpus from CQA por-tals to get mountains of  X  X ell labeled X  sequential question patterns based on the following two considerations:
We apply the Prefixspan algorithm proposed by Pei et al. [4] to mine frequent question patterns. This algorithm explores prefix projection to speed up sequential pattern mining. To get high quality question patterns, we also set extra constrains to the original algorithm empirically: The minimum support and minimum confidence are set to be 0.5% and 0.7; The minimum and maximum length of pat-terns are set to be 2 and 5; The maximum length between two neighbor terms in any pattern is set to be 3. In or-der to calculate the confidence, we choose the corresponding best answers as negative examples. Furthermore, to avoid long question titles and unbalanced Q&amp;A pair, we limit the length of each question title to be within 5 and 20 words and the maximum length ratio between question title and answer to be 1:5. One-class SVM is employed afterwards to predict whether a tweet contains question(s) or not.
We present qweet extraction in this section. Firstly, we need to understand which kinds of interrogative tweets are not qweets. From the observations, we construct a taxon-omy of interrogative tweets (Section 3.2.1). Afterwards, we extract four types of features according to the characteris-tics of interrogative tweets and convert qweet extraction to a binary classification problem (Section 3.2.2). We classified interrogative tweets to the following 6 types. Only those tweets belonging to the last type are qweets. 1. Advertisement. This kind of tweets ask questions to the reader and deliver advertisements in the following. E.g., 2. Article or News Title on the Web. These tweets post article names or news titles together with the links to the webpage. E.g., 3. Question with Answer. These tweets contain ques-tions followed by their answers. E.g., 4. Question as Quotation. These tweets contain ques-tions in quoted sentences as references to what other people said. E.g., 5. Rhetorical Question. This kind of tweets include rhetorical questions, which seem to be questions but with-out the expectation of any answer. In another words, these tweets encourage readers to think about the obvious an-swers. E.g., 6. Qweet. These kinds of tweets ask for some informa-tion or help. E.g., Another special type is  X  X uestion on The Web X  which the Tweet author posts a question asked by someone on the web, e.g., CQA portals, forums, etc. The following is an example: However, even for human it is hard to infer the author X  X  in-tention. Therefore, we classify it to Type 2 for convenience. Different from traditional short texts like forum posts and CQA questions, tweets own some special characteristics, such as  X  X username X , Retweets, and hashtags. Apart from that, plenty of tweets contain (shortened) links which provide ex-tra information to determine these tweets X  types. We also note that an interrogative tweet usually can be split into two parts: question part and context part. Although it is not easy to judge whether a tweet is qweet from the ques-tion itself, the context helps to distinguish qweets from non-qweets.

Based on the above findings, we extract four kinds of fea-tures from each tweet, which are reported in Table 1. Not-ing that word overlap similarity is employed to calculate the value of question-url sameness. To be specific, if the overlap similarity between any question sentence and the title of url is above a certain threshold, the value is 1; otherwise we treat them as different content and set the value to 0. In our experiments, we set the threshold to 0.8.

Using these features, we leverage one Random Forrest classifier to predict whether a interrogative tweet is a qweet or not.
We sampled one hour X  X  tweets from 11:00am to 12:00am on April 18, 2011 using Twitter API 3 , getting 2,045 English tweets in total. Two raters were asked to label whether each tweet was interrogative tweet. They worked individually and the labels were finalized through discussions between them.
Table 2 reports the statistic of the data, from which we estimate that on Twitter there are about 11% of tweets con-taining questions (similar to 13% reported in [2]) and 6% of tweets having information needs. This observation confirms the statement that Twitter is no longer a pure social media http://dev.twitter.com. which let user publish  X  X hat X  X  happening X  X  to provide in-formation but also becomes an online questioning platform where users post  X  X oes anyone know...? X  X  to seek informa-tion.

To extract sequential patterns, we collected over 850,000 question titles and the corresponding best answers from Ya-hoo! Answers 4 and WikiAnswers 5 .

Table 3 presents the accuracy of various methods on de-tecting interrogative tweets. We find that simple question patterns give satisfactory performance. Specifically, using question mark alone gets the precision of nearly 0.97 but captures less than 85% of all interrogative tweets. Adding 5W1H question words remedies the drawback of low recall, but substantially decreases precision. To reduce the number of false positives, we set up the following two heuristics for 5W1H: 1. They must appear at the beginning of one sentence. 2. Auxiliary words are added to the original words. For The results demonstrate that the above rules boost the per-formance significantly. The first heuristic improves preci-sion of 5W1H by 60.5% although reduces 5.9% of recall. For F , it improves 28.4% comparing with original 5W1H. The second heuristic gives similar results. When applying the above two together, it makes the precision higher than 0.95 and recall higher than 0.9, which increases precision and http://answers.yahoo.com. http://wiki.answers.com. by 7.2% and 3.0% comparing to using question mark with slight decrease on recall (1.5%). We also try the artificial rules designed in [2], which keep high precision but do not increase recall too much.

Learning-based approach, however, does not improve the accuracy of detected questions as expected. Patterns with low confidence get low precision while patterns with high confidence fail to increase recall. We conjecture that for one reason, most of question patterns are  X  X verlapped X  with 5W1H, such as  X  X ow to X  and  X  X here can I X . For the other, sequential question patterns in CQA may not well capture the questions on Twitter as many patterns fail to match any tweet. In our future study, we plan to label large number of real tweets to further explore this problem.
 Table 3: Accuracies of interrogative tweet detection for various methods (QM: question mark; best re-sults are in bold) Table 4: Effects of feature sets on qweet extraction
In our experiments, we employed a Random Forrest clas-sifier 6 with 1000 trees to extract qweets and adopted 10-fold cross validation. Table 4 presents the results of qweet extrac-tion using different sets of features. We find that: 1) Con-text features are of great importance in distinguish-ing qweets from non-qweets. Comparing the classifier X  X  performance with feature sets  X  X  X  and  X  X +QC+C X , it is ob-vious that context features boost the performance greatly. To be specific, precision is improved by 22% with a cost of slight decrease in recall (We consider precision as more im-portant than recall since high recall is easier to reach if the classifier predicts all interrogative tweets as qweets). How-ever, unigrams in context seem useless since  X  X +QC+non-word C X  gives even higher precision than  X  X +QC+C X . We conjecture that although some words like  X  X ree X ,  X  X lick X , etc. are useful in judging advertisement and online articles, much more irrelevant words are introduced when using word fea-tures. 2) Tweet-specific features also help in qweet identification. Comparing the results using features sets  X  X +QC+non-word C X  and  X  X +QC+non-word C+T X , we find that Tweet-specific featu res improve the precision of qweet extraction. In addition, without Tweet-specific fea-tures, the classifier X  X  recall increases, which means it tends to We attempted Random Forrest, SVM, J48, and Logistic Regression in our experiments and Random Forrest per-formed the best among them. predict a interrogative tweet as qweet when such information is missing. From the last three rows of Table 4 we can further understand that Retweet is the most significant feature, fol-lowed by  X  X username X , while Hashtag is not so helpful. The possible explanation is that most Retweets are not qweets as they just rebroadcast other users X  tweets, which are usu-ally providing rather than asking for information. On the contrary,  X  X username X  always denotes a interrogative tweet being a qweet as it implies a dialog between two users. As our data contain no obvious qweet signals in Hashtags (like  X #lazyweb X ) and Hashtags are usually used to represent the topics, they are not so influential. 3) Qweet extraction is a non-trivial problem. At present, the performance is still not so satisfactory. Through examining the failed exam-ples, we find that tweets containing rhetorical questions and complicated self-ask-self-answer sentences are always being misclassified. E.g.,  X  X f I X  X  that mean/nasty why do u con-tinue to talk to me X  and  X  X ee what you did? Poor 4 minute. They... X . Potentially this problem may be solved through utilizing syntax features to capture the structure of tweets and employing more training data.
This paper explored the problem of automatic question identification in the microblog environment. To identify qweets, we designed a cascade process which first detected interrogative tweets and then extracted qweets. For in-terrogative tweet detection, both rule-based approach and learning-based approach were employed and experimental results showed that on Twitter rule-based approach per-formed well while learning-based approach did not improve the performance significantly. For qweets extraction, we leveraged the special characteristics of Tweets like Retweet and @username, and context of questions to build a binary classifier. Experimental results demonstrated that such fea-tures were of great importance in extracting qweets from interrogative tweets. Looking forward, we plan to enlarge our dataset and utilize syntax features in qweet extraction. This work is supported by two grants from the Research Grants Council of the Hong Kong SAR, China (Project No. CUHK 413210 and Project No. CUHK 415410) and a grant from Google Focused Grant Project  X  X obile 2014 X .
