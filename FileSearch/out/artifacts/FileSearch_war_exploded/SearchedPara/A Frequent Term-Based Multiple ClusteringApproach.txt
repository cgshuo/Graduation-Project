 Data mining in database provides data owners with new information and pat-terns in their data. Clustering is a traditional data mining task for automatically grouping data. However, groups may be h idden in different perspectives of the data. An item may belong to different groups with different perspectives. Tra-ditional clustering approaches only provide either a hard partitioned result or a hierarchical result. In these clustering results, an item can only belong to one group. To discover hidden groups in various perspectives, we need to apply mul-tiple clustering approaches. Multiple clustering methods can assign one item to different groups with respect to different pe rspectives. Generally speaking, mul-tiple clustering approaches have to deal with two challenges including high time complexity and redundant result.

On the other hand, as the web continues to grow rapidly, huge number of text documents have been generated. To organize and do data mining work on these text documents, text clustering becomes a very important application of clus-tering algorithms. However, compared to other applications of clustering, three major challenges including high dimensionality, large data and incomprehensible results should be addressed for text clustering:
Although applying a multiple clustering approach to text documents can help us significantly while doing text mining tasks, to our best knowledge there is no existing feasible multiple clustering approach for text documents since now. The high dimensionality of text documents makes multiple clustering approaches not scalable while applied to text documents. In this paper, we propose the first feasible multiple clustering approach for text documents called FTMTC( frequent term-based multiple text clustering approach). FTMTC represents a cluster with a set of terms to deal with the high dimensionality challenge. We also introduce WordNet[1] to improve the quality of redundancy removal process.
This paper is structured as follows: We review existing text clustering and multiple clustering approaches in section 2. In section 3, we introduce a series of notations to define the problem we are going to solve. We describe details of FTMTC with sequence charts in section4. Then, we prove our approach is feasible and outstanding with a series of experiments in section 5. Finally, we make a conclusion and introduce our future work in section 6. 2.1 Multiple Clustering Approaches The main difference between multiple clustering approaches and traditional clus-tering approaches is that multiple clustering X  X  result contains clusters discovered with various perspectives. Clusters in mul tiple clustering result can overlap to each other while clusters in traditional clustering result can X  X . Traditional mul-tiple clustering approaches tend to generate a quite large amount of clusters. The result contains a lot of redundant clusters. OSCLU[2] is a recent proposed non-redundant multiple clustering approach, which is based on the idea that a pair of clusters which share more than a certain amount of overlapped dimen-sions and items should be regarded as similar to each other. ASCLU[3] applies OSCLU to an alternative clustering way. 2.2 Text Clustering Approaches Most text clustering approaches rely on a vector-space model ,inwhich,each text document d is represented by a vector of frequencies of all terms: d = ( tf 1 ,tf 2 ,...,tf m ). Based on this model, standard clustering approaches like k-means[4] can be applied to text documents directly. But they can X  X  handel the high dimensional and incomprehensible result challenges well.

In this paper, we propose a multiple clustering solution for text documents based on frequent term model. This model can help us get avoid of the high dimensionality challenge of text documents. For consistent notations in the following sections, we define some notations here. First of all, we make a formal definition of our problem: Given a set of text the terms that appear in DS and T ( d ) denote terms that appear in d .Ourtarget is to generate a set of clusters R = { C 1 ,C 2 ,...,C n } . In this procedure, three main challenges need to be addressed: Challenge 1: Incomprehensible Results: Traditional clustering results do not provide explanations for clusters. To give each cluster a explanation, we associate each cluster in R with a term set. To associate terms with documents, we introduce the following definitions: As document d contains a set of terms, a term t can also  X  X over X  a set of documents. We define the set of documents in DS that contain term t as Cover ( t ): terms in T : So, if Cover ( T ) is the documents grouped by a cluster, T will give an explanation for the cluster.
 Challenge 2 High Dimensional Data: To deal with the high dimensional challenge, we control the number of term sets that are associated to clusters. We only associate frequent term sets to c lusters. We can judge whether a term set T is a frequent set with Cover ( T ), we define the set of all frequent term sets as FTS ( DS ): Where  X  is the threshold of frequent term set. So, a cluster is composed with a frequent term set T as explanation and a document set D as members. Where T  X  FTS ( DS )and D = Cover ( T ).
 Challenge 3 Redundant Clustering Results: To prevent a redundant clus-tering result, the size of R should be reasonable. Each cluster in R should bring novel information. We will introduce a cluster picking algorithm in section 4 to handle this challenge. Based on the notations above, we propose a multiple clustering approach for text documents called FTMTC( Frequent-t erm based multiple text clustering). Generally speaking, FTMTC is composed of three steps as shown in Fig. 1: 4.1 Preprocess Step To preprocess the document data, we conduct several steps including stop words removing, stemming and indexing. First of all, a stop word list 1 is employed to remove the stop words. Secondly, we apply Poter stemming algorithm for word stemming. To process the document efficiently, we apply a tool named Lucene to build index files for the documents.

For an efficient algorithm, we extract k important words from T all as key terms. The key term set is noted as KT . Since nouns with high TFIDF value tend to be representative in general, we pick nouns with high TFIDF from T all and add them to KT . 4.2 Candidate Generating Step In this step, we generate FTS ( DS ) with Aprior algorithm[5] algorithm. For each T in FTS ( DS ), we build a corresponding cluster C =( T,Cover ( T )) and add it to the candidate set Cand . Since term sets with more terms tend to cover less documents than those with less terms, we set the document coverage threshold as follows: Assuming that  X  is the threshold of frequent term set with one term, the threshold of a frequent term set with N terms will be  X   X  0 . 9 ( N  X  1) 4.3 Candidate Picking Step In this step, we pick clusters from Cand and add them to the result set R grad-ually. First of all, we rank clusters in Cand based on clusters X  quality. Generally speaking, clusters with large number of documents or terms tend to have high quality. Besides, if the terms are closely related to the documents, the clus-ter X  X  quality is high. We judge the relationship between terms and documents with average TFIDF value. Therefore, we define the the quality of a cluster C ( T,D )as Quality ( C )= | D | a  X | T | b  X  AV G TFIDF ( C ) c .where a + b + c =1 and AV G TFIDF ( C ) denotes the average TFIDF value between documents and terms.
 Where m denotes the size of D and n denotes the size of T
As clusters in Cand are sorted by Quality ( C ) in descending order, we remove redundant clusters from Cand to deal with challenge 3. Inspired by OSCLU, we consider clusters either have dissimila r term sets or group dissimilar documents to be non-redundant to each other.

Obviously we can define the similarity between two term sets with overlap percentage. However, terms contain sem antic meanings. It makes similarity be-tween term sets more complex than simila rity between mathematic vectors. For example, term set {  X  X SA X ,  X  X resident X ,  X  X istory X  } and term set {  X  X merica X ,  X  X hairman X ,  X  X ast X  } share no term, but they do re present similar concepts.
To adapt the redundancy definition to text clustering, we introduce WordNet as external knowledge. WordNet is a lexical database which can be used to calcu-late the similarity between two terms. We use Jiang and Conrath X  X  word similar-ity algorithm JNC[6] to judge the similarity between terms. We define semantic Similarity ( T,  X  T ):
Similarity ( T,  X  T )=
With a similarity threshold  X  , we can get a group of clusters in R that are similar to a given cluster C . The similar group of C ( T,D )in R with threshold  X  is defined as:
Although C has similar term set with clusters in SimGroup  X  ( C,R ), if C groups dissimilar documents, we also consider C as non-redundant to clusters in R . Given a cluster set CS = { C 1 ,C 2 ,...,C n } , we define the coverage of CS  X  X  document as Coverage ( CS )= C
At last, we define an interest value Interest ( C,R ) to judge whether C is novel to R . Given a threshold  X  ,weadd C to R if Interest ( C,R ) is larger than  X  . Since we already sort Cand with regard to cluster X  X  quality by descending order, it X  X  obvious that our algorithm is a greedy algorithm and thus can maximize the summation of clusters X  qualities under the premise that R is none-redundant. We define the interest of C =( T,D )to R as Interest ( C,R ): 5.1 Experiment Setup To build a multi-label data set, we download 4505 biography pages from Wikipedia with two different perspectives. The biography pages are downloaded from four country categories and three occupation categories.

We measure clustering results with three aspects. First of all, we list the 5.2. Secondly, we evaluate the scalability of our algorithm in section 5.3. At last, in section 5.4, we evaluate the quality of clustering result with multiple clustering evaluation measurements introduced in [7], including purity, entropy and F1-value. 5.2 Experiment Result Table 1 shows the result of FTMTC. It handles Challenge 1 and Challenge 2 well. The term set associated to a cluster explains the cluster X  X  topic well. We mark categories in nationality perspective with black font and mark categories in occupation perspective with norma l font. We found the result covers every known category in two perspectives. B esides, we are glad to see that FTMTC also can discover clusters we do not know in advance like X  X ar X  and detailed category like  X  X wim X . We mark them with italics font. 5.3 Scalability Evaluation Multiple clustering approaches that are based on term vector model X  X  running time grows fast as the database grows. Since FTMTC applies similar redundancy removal process with OSCLU and ASCLU, we compare FTMTC with OSCLU, ASCLU and FTC. We do experiments as the database X  X  size grows from 100 to 5000 (add 200 documents each time). Each time, we run the clustering algorithms ten times and calculate the average running time. From Fig.2 and Fig.3, we can see that FTMTC and FTC X  X  running time grows linearly as the database X  X  size grows while OSCLU and ASCLU X  X  running time grows exponentially. It X  X  obvious that FTMTC outperforms OSCLU and ASCLU with regard to scalability. 5.4 Clustering Quality Evaluation Since there is no existing multiple clustering approaches for text documents, we compare FTMTC with FTC, OSCLU and ASCLU on multi-label text docu-ments. We choose FTC as the baseline because FTMTC shares the same cluster definition with it. We choose multi-label text documents as test set to focus on discovering clusters in v arious perspectives.

We compare clustering quality with different database sizes. . For a fair com-parison, we set the FTMTC X  X  key word number equals to FTC X  X . From Fig.4 to Fig.6 we can see that FTMTC can obviously outperform FTC with regard to purity, F1 value and entropy. In this paper, we propose a feasible multiple clustering approach for text docu-ments based on a frequent term model. We also introduce WordNet as external knowledge to help removing results X  redundancy. With a series of experiments, we prove that FTMTC can provide an understandable clustering result which contains clusters in various perspectives. FTMTC can also excavate hidden and more detailed clusters, which helps many tasks of data mining. With compari-son, we prove that FTMTC is more scalable than traditional multiple clustering approaches and achieves a better clustering result than FTC while applied to multi-label text documents. In the future, we will exploit more external knowl-edge, such as Cyc Ontology 2 and Wikipedia 3 , to improve our clustering results. Acknowledgments. This research is supported by the 863 project of China (2013AA013300), National Natural Sc ience Foundation of China (Grant No. 61375054) and Tsinghua University Initiative Scientific Research Pro-gram(20131089256).

