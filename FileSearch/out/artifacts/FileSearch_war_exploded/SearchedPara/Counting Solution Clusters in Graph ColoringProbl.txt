 Message passing algorithms, in particular Belief Propagat ion (BP), have been very successful in efficiently computing interesting properties of succinctl y represented large spaces, such as joint probability distributions. Recently, these techniques ha ve also been applied to compute properties of discrete spaces, in particular, properties of the space o f solutions of combinatorial problems. For example, for propositional satisfiability (SAT) and graph c oloring (COL) problems, marginal prob-ability information about the uniform distribution over so lutions (or similar combinatorial objects) has been the key ingredient in the success of BP-like algorit hms. Most notably, the survey propa-gation (SP) algorithm utilizes this information to solve ve ry large hard random instances of these problems [ 3 , 11 ].
 Earlier work on random ensembles of Constraint Satisfactio n Problems (CSPs) has shown that the computationally hardest instances occur near phase bounda ries, where instances go from having many globally satisfying solutions to having no solution at all (a  X  X olution-focused picture X ). In recent years, this picture has been refined and it was found th at a key factor in determining the hard-ness of instances in terms of search algorithm (or sampling a lgorithm) is the question: how are the solutions spatially distributed within the search space? This has made the structure of the solution space in terms of its clustering properties a key factor in de termining the performance of combina-torial search methods (a  X  X luster-focused picture X ). Can B P-like algorithms be used to provide such cluster-focused information? For example, how many cluste rs are there in a solution space? How big are the clusters? How are they organized? Answers to such questions will shed further light into our understanding of these hard combinatorial problems and lead to better algorithmic approaches for reasoning about them, be it for finding one solution or ans wering queries of probabilistic infer-ence about the set of solutions. The study of the solution spa ce geometry has indeed been the focus of a number of recent papers [e.g. 1 , 2 , 3 , 7 , 9 , 11 ], especially by the statistical physics community, which has developed extensive theoretical tools to analyze such spaces under certain structural as-sumptions and large size limits. We provide a purely combina torial method for counting the number of clusters, which is applicable even to small size problems and can be approximated very well by message passing techniques.
 Solutions can be thought of as  X  X eighbors X  if they differ in t he value of one variable, and the transitive closure of the neighbor relation defines clusters in a natura l manner. Counting the number of clusters is a challenging problem. To begin with, it is not even clear w hat is the best succinct way to represent clusters. One relatively crude but useful way is to represen t a cluster by the set of  X  X ackbone X  variables in that cluster, i.e., variables that take a fixed v alue in all solutions within the cluster. Interestingly, while it is easy (polynomial time) to verify whether a variable assignment is indeed a solution of CSP, the same check is much harder for a candidate cluster represented by the set of its backbone variables.
 We propose one of the first scalable methods for estimating th e number of clusters of solutions of graph coloring problems using a belief propagation like alg orithm. While the na  X   X ve method, based on enumeration of solutions and pairwise distances, scales to graph coloring problems with 50 or so nodes and a recently proposed local search based method prov ides estimates up to a few hundred node graphs [ 7 ], our approach X  X eing based on BP X  X asily provides fast estima tes for graphs with 100 , 000 nodes. We validate the accuracy of our approach by also provi ding a fairly non-trivial exact counting method for clusters, utilizing advanced kno wledge compilation techniques. Our approach works with the factor graph representation of the g raph coloring problem. Yedidia et al. [ 12 ] showed that if one can write the so-called  X  X artition funct ion X , Z , for a quantity of interest in a factor graph with non-negative weights, then there is a f airly mechanical variational method derivation that yields belief propagation equations for es timating Z . Under certain assumptions, we derive a partition function style quantity, Z (  X  1) , to count the number of clusters. We then use the variational method to obtain BP equations for estimatin g Z (  X  1) . Our experiments with random graph coloring problems show that Z (  X  1) itself is an extremely accurate estimate of the number of clusters, and so is its approximation, Z BP (  X  1) , obtained from our BP equations. The graph coloring problem can be expressed in the form of a factor graph , a bipartite graph with  X , . . . , with associated factor functions f  X  , . . . , represent the constrains of the problem (no two adjacent vertices have the same color). Each factor functio n is a Boolean function with arguments ~x constraint is satisfied. An edge connects a variable x i with factor f  X  iff the variable appears in the constraint represented by the factor node, which we denote b y i  X   X  . In the graph coloring problem, each factor function has exactly two variables.
 In the factor representation, each variable assignment ~x is thought of as having a weight equal to the product of the values that all factors evaluate to. We denote this product by F ( ~x ) := Q  X  f  X  ( ~x  X  ) . In our case, the weight of an assignment ~x is 1 if all of the factors have value of 1 , and 0 otherwise. The assignments with weight 1 correspond precisely to legal colorings, or solutions to the problem. The number of solutions can thus be expressed as the weighted sum across all possible assignments. We denote this quantity by Z , the so-called partition function : We define the solution space of a graph coloring problem to be the set of all its legal color ings. Two legal colorings (or solutions) are called neighbors if they differ in the color of one vertex. Definition 1 (Solution Cluster) . A set of solutions C  X  S of a solution space S is a cluster if it is a maximal subset such that any two solutions in C can be connected by a sequence from C where consecutive solutions are neighbors.
 In other words, clusters are connected components of the  X  X o lution graph X  which has solutions as nodes and an edge between two solutions if they differ in the v alue of exactly one variable. In this section we consider a method for estimating the numbe r of solution clusters of a graph coloring problem. We briefly describe the concepts here; a mo re in-depth treatment, including formal results, may be found in [ 8 ]. First let us extend the definition of the function F so that it may be evaluated on an extended domain DomExt := P ( { c 1 , . . . , c k } ) \ X  where c 1 , . . . , c k are the k domain values (colors) of each of the problem variables, and P is the power set operator (so | DomExt | = 2 k  X  1 ). Each generalized assignment ~y  X  DomExt n thus associates a (non-empty) set of values with each original variable, defining a hypercube in the search space for F . We generalize F and f  X  to this extended domain in the natural way, F  X  ( ~y ) := Q ~x  X  ~y F ( ~x ) , and f ( ~y  X  ) := Q ~x relational operators used on vectors in this text. This mean s that F  X  evaluates to 1 on a hypercube iff F evaluates to 1 on all points within that hypercube.
 Let us first assume that the solution space we work with decomp oses into a set of separated hy-percubes , so clusters correspond exactly to the hypercubes; by separ ated hypercubes, we mean that points in one hypercube differ from points in others in a t least two values. E.g., ~y 1 = This allows us to develop a surprisingly simple expression f or counting the number of clusters, and we will later see that the same expression applies with high p recision also to solution spaces of much more complex instances of graph coloring problems. Conside r the indicator function  X  ( ~y ) for the property that ~y  X  DomExt n is a maximal solution hypercube contained in the solution space: fact hypercubes, then variable values that can be  X  X xtended  X  independently can also be extended all y observations,  X  ( ~y ) can be reformulated by factoring out the product as follows. Here # o ( ~y ) denotes the number of odd-size elements of ~y , and # e ( ~y ) the number of even-size ones. Finally, to count the number of maximal hypercubes fitting in to the set of solutions, we sum the indicator function  X  ( ~y ) across all vectors ~y  X  DomExt n : The expression above is important for our study, and we denot e it by Z (  X  1) : The notation Z (  X  1) is chosen to emphasize its relatedness to the partition func tion ( 1 ) denoted by Z , and indeed the two expressions differ only in the (  X  1) term. It is easily seen that if the solution space consists of a set of separated hypercubes, then Z (  X  1) exactly captures the number of clusters (each separated hypercube is a cluster). Surprisingly, thi s number is remarkably accurate even for random coloring problems as we will see in Section 6 , Figure 1 . Obtaining the exact number of clusters for reasonable size p roblems is crucial for evaluating our proposed approach based on Z (  X  1) and the corresponding BP equations to follow in Section 5 . A na  X   X ve way is to explicitly enumerate all solutions, compute th eir pairwise Hamming distances, and infer the cluster structure. Not surprisingly, this method does not scale well because the number of solutions typically grows exponentially as the number of va riables of the graph coloring problems increases. We discuss here a much more scalable approach tha t uses two advanced techniques to this effect: disjunctive negation normal form (DNNF) and bi nary decision diagrams (BDDs). Our method scales to graph coloring problems with a few hundred v ariables (see experimental results) for computing both the exact number of clusters and the exact value of Z (  X  1) .
 Both DNNF [ 6 ] and BDD [ 4 ] are graph based data structures that have proven to be very e ffective in  X  X nowledge compilation X , i.e., in converting a 0-1 funct ion F into a (potentially exponentially long, but often reasonably sized) standard form from which v arious interesting properties of F can be inferred easily, often in linear time in the size of the DNN F formula or BDD. For our purposes, we use DNNF to succinctly represent all solutions of F and a set of BDDs to represent solution clusters that we create as we traverse the DNNF representati on. The only relevant details for us of these two representations are the following: (1) DNNF is rep resented as an acyclic directed graph with variables and their negations at the leaves and two kind s of internal nodes,  X  X r X  and  X  X nd X ;  X  X r X  nodes split the set of solutions such that they differ in the v alue of the variable labeling the node but otherwise have identical variables;  X  X nd X  nodes partition the space into disjoint sets of variables; (2) BDDs represent arbitrary sets of solutions and support effic ient intersection and projection (onto a subset of variables) operations on these sets.
 We use the compiler c2d [ 5 ] to obtain the DNNF form for F . Since c2d works on Boolean formulas and our F often has non-Boolean domains, we first convert F to a Boolean function F  X  using a unary encoding, i.e., by replacing each variable x i of F with domain size t with t Boolean variables x i,j , 1  X  j  X  t , respecting the semantics: x i = j iff x i,j = 1 . In order to ensure that F and F similar cluster structure of solutions, we relax the usual c ondition that only one of x i, 1 , . . . , x i,t may be 1, thus effectively allowing the original x i to take multiple values simultaneously. This yields a generalized function : the domains of the variables of F  X  correspond to the power sets of the domains of the respective variables of F . This generalization has the following useful property: if two solutions ~x (1) and ~x (2) are neighbors in the solution space of F , then the corresponding solutions ~x  X  (1) and ~x  X  (2) are in the same cluster in the solution space of F  X  .
 Computing the number of clusters. Given F  X  , we run c2d on it to obtain an implicit representation of all solutions as a DNNF formula F  X  X  . Next, we traverse F  X  X  from the leaf nodes up, creating clusters as we go along. Specifically, with each node U of F  X  X  , we associate a set S U of BDDs, one for each cluster in the sub-formula contained under U . The set of BDDs for the root node of F  X  X  then corresponds precisely to the set of solution clusters o f F  X  , and thus of F . These BDDs are computed as follows. If U is a leaf node of F  X  X  , it represents a Boolean variable or its negation and S
U consists of the single one-node BDD corresponding to this Bo olean literal. If U is an internal node of F  X  X  labeled with the variable x U and with children L and R , the set of BDDs S U is computed as follows. If U is an  X  X r X  node, then we consider the union S L  X  S R of the two sets of BDDs and merge any two of these BDDs if they are adjacent, i.e., have tw o solutions that are neighbors in the solution space (since the DNNF form guarantees that the BDDs in S L and S R already must differ in the value of the variable x U labeling U , the adjacency check is equivalent to testing whether the two BDDs, with x U projected out, have a solution in common; this is a straightf orward projection and intersection operation for BDDs); in the worst case, thi s leads to | S L | + | S R | cluster BDDs in S U . Similarly, if U is an  X  X nd X  node, then S U is constructed by considering the cross product { b
L and b R | b L  X  S L , b R  X  S R } of the two sets of BDDs and merging adjacent resulting BDDs as before; in the worst case, this leads to | S L || S R | cluster BDDs in S U .
 Evaluating Z (  X  1 ) . The exact value of Z (  X  1) on F  X  can also be evaluated easily once we have the DNNF representation F  X  X  . In fact, as is reflected in our experimental results, evalua tion of Z (  X  1) is a much more scalable process than counting clusters becau se it requires a simple traversal of F  X  X  without the need for maintaining BDDs. With each node U of F  X  X  , we associate a value V U which equals precisely the difference between the number of solut ions below U with an even number of positive literals and those with an odd number of positive literals; Z (  X  1) then equals (  X  1) N times the value thus associated with the root node of F  X  X  . These values are computed bottom-up as follows. If U is a leaf node labeled with a positive (or negative) literal, then V U =  X  1 (or 1 , resp.). If U is an  X  X r X  node with children L and R , then V U = V L + V R . This works because L and R have identical variables. Finally, if U is an  X  X nd X  node with children L and R , then V U = V L V R . This last computation works because L and R are on disjoint sets of variables and because of the following observation. Suppose L has V e L solutions with an even number of positive literals and V o L solutions with an odd number of positive literals; similarl y for R . Then V We present a version of the Belief Propagation algorithm tha t allows us to deal with the alternating signs of Z (  X  1) . The derivation follows closely the one given by Yedidia et a l. [ 12 ] for standard BP, i.e., we will write equations for a stationary point of KL div ergence of two sequences (not necessarily probability distributions in our case). Since the Z (  X  1) expression involves both positive and negative terms, we must appropriately generalize some of the steps.
 Given a function p ( ~y ) (the target function, with real numbers as its range) on DomExt n that is known up to a normalization constant but with unknown margin al sums, we seek a function b ( ~y ) (the trial function) to approximate p ( ~y ) , such that b  X  X  marginal sums are known. The target function p ( ~y ) is defined as p ( ~y ) := 1 Z ~y in ~y except y i . The marginal sums can be extended in a similar way to allow fo r any number of variables fixed in ~y , specified by the subscript. When convenient, we treat the sym bol  X  as a set of indices of variables in f  X   X  , to be able to index them. We begin by listing the assumptions used in the derivation, both the ones that are used in the  X  X tandard X  BP, and two additional ones needed for the generalization. An assumption on b ( ~y ) is legitimate if the corresponding condition holds for p ( ~y ) . Assumptions: The standard assumptions, present in the derivation of standard BP [ 12 ], are:  X  Marginalization: b i ( y i ) = P ~y  X  Normalization: P y  X  Consistency:  X   X , i  X   X , y i : b i ( y i ) = P ~y  X  Tree-like decomposition: says that the weights b ( ~y ) of each configuration can be obtained from To appropriately handle the signs of b ( ~y ) and p ( ~y ) , we have two additional assumptions . These are necessary for the BP derivation applicable to Z (  X  1) , but not for the standard BP equations.  X  Sign-correspondence: For all configurations ~y , b ( ~y ) and p ( ~y ) have the same sign (zero, being a The Sign-alternation assumption can be viewed as an applica tion of the inclusion-exclusion prin-ciple, and is easy to illustrate on a graph coloring problem w ith only two colors. In this case, if F ( ~y ) = 1 , then y i = { c 1 } means that y i can have color 1 , y i = { c 2 } that y i can have color 2 , and y i = { c 1 , c 2 } that y i can have both colors. The third event is included in the first two, and its probability must thus appear with a negative sign if the sum o f probabilities is to be 1 . Kullback-Leibler divergence: The KL-divergence is traditionally defined for probability distribu-tions, for sequences of non-negative terms in particular. W e need a more general measure, as our sequences p ( ~y ) and b ( ~y ) have alternating signs. But using the Sign-correspondence assumption, we observe that the usual definition of KL-divergence is still a pplicable, since the term in the logarithm over, the following Lemma shows that the two properties of KL -divergence that make it suitable for distance-minimization are still valid.
 Lemma 1. Let b ( . ) and p ( . ) be (possibly negative) weight functions on the same domain D , with the they sum to the same constant (i.e., P ~y b ( ~y ) = P ~y p ( ~y ) = c ). Then the KL-divergence D ( b k p ) satisfies D ( b k p )  X  0 and D ( b k p ) = 0  X  b  X  p .
 The proof is essentially identical to the equivalent statem ent made about KL-divergence of proba-bility distributions. We omit it here for lack of space.
 us to isolate the signs, and the minimization follows exactl y the steps of standard BP derivation, namely we write a set of equations characterizing stationar y points of D ( b k p ) . At the end, using the Sign-alternation assumption, we are able to implant the signs back.
 BP equations: The resulting modified BP updates (denoted BP (  X  1) ) are, for y i  X  DomExt : (Almost equivalent to standard BP, except for the (  X  1) term.) One would iterate these equations estimates of marginal sums) using the Sign-alternation ass umption and the standard BP relations: To approximately count the number of clusters in large probl ems for which exact cluster count or exact Z (  X  1) evaluation is infeasible, we employ the generic BP (  X  1) scheme derived above. We sub-point to find a fixed point, and then use Equations ( 5 ) to compute the beliefs. The actual estimate of Z (  X  1) is obtained with the standard BP formula (with signs properl y taken care of), where d i is the degree of the variable node y i in the factor graph: We empirically evaluate the accuracy of our Z (  X  1) and Z BP (  X  1) approximations on an ensemble of random graph 3 -coloring instances. The results are discussed in this sect ion.
 Z (  X  1 ) vs. the number of clusters. The left panel of Figure 1 compares the number of clusters (on the x-axis, log-scale) with Z (  X  1) (on the y-axis, log-scale) for 2 , 500 colorable random 3-COL instances on graphs with 20 , 50 , and 100 vertices with average vertex degree ranging between 1 . 0 and 4 . 7 (the threshold for 3-colorability). As can be seen, the Z (  X  1) expression captures the number of clusters almost exactly. The inaccuracies come mostly from low graph density regions; in all instances we tried with density &gt; 3 . 0 , the Z (  X  1) expression was exact. We remark that although uncolorable instances were not considered in this comparison, Z (  X  1) = 0 = num-clusters by construction. It is worth noting that for tree-structured graphs (with mor e than one vertex), the Z (  X  1) expression gives 0 for any k  X  3 colors although there is exactly one solution cluster. More over, given a disconnected graph with at least one tree component, Z (  X  1) also evaluates to 0 as it is the product of Z (  X  1) values over different components. We have thus removed all t ree components from the generated graphs prior to computing Z (  X  1) ; tree components are easily identified and removing them does not change the number of clusters. For low graph den sities, there are still some instances Figure 1: Left: Z (  X  1) vs. number of clusters in random 3-COL problems with 20 , 50 and 100 vertices, and average vertex degree between 1 . 0  X  4 . 7 . Right: cluster marginals vs. Z (  X  1) -marginals for one instance of random 3-COL problem with 100 vertices. for which Z (  X  1) evaluates to 0 ; these instances are not visible in Figure 1 due to the log-log scale. In fact, all our instances with fewer than 5 clusters have Z (  X  1) = 0 . This is because of other substructures for which Z (  X  1) evaluates to 0 , e.g., cordless cycles of length not divisible by 3 (for k = 3 coloring) with attached trees. These structures, however, become rare as the density increases. Z (  X  1 ) marginals vs. clusters marginals. For a given problem instance, we can define the cluster marginal of a variable x i to be the fraction of solution clusters in which x i only appears with one particular value (i.e., x i is a backbone of the cluster). Since Z (  X  1) counts well the number of clusters, it is natural to ask whether it is also possible to obtain the m arginals information from it. Indeed, Z (  X  1) does provide an estimate of the cluster marginals, and we cal l them Z (  X  1) -marginals . Recall that the semantics of factors in the extended domain is such t hat a variable can assume a set of values only if every value in the set yields a solution to the problem. This extend s to the Z (  X  1) estimate of the number of clusters, and one can therefore use the princip le of inclusion-exclusion to compute the number of clusters where a variable can only assume one parti cular value. The definition of Z (  X  1) conveniently provides for correct signs, and the number of c lusters where x i is fixed to v i is thus estimated by P y is obtained by dividing this quantity by Z (  X  1) .
 The right panel of Figure 1 shows the results on one random 3-COL problem with 100 vertices. The plot shows cluster marginals and Z (  X  1) -marginals for one color; the points correspond to individ-ual variables. The Z (  X  1) -marginals are close to perfect. This is a typical situation , although it is important to mention that Z (  X  1) -marginals are not always correct, or even non-negative. Th ey are merely an estimate of the true cluster marginals, and how wel l they work depends on the solution space structure at hand. They are exact if the solution space decomposes into separated hypercubes and, as the figure shows, remarkably accurate also for random coloring instances.
 for the 3-COL problem on colorable random graphs of various s izes and graph densities. It com-pares Z (  X  1) (on the x-axis, log-scale) with Z BP (  X  1) (y-axis, log-scale) for 1 , 300 colorable 3-COL instances on random graphs with 50 , 100 , and 200 vertices, with average vertex degree ranging from 1 . 0 to 4 . 7 . The plots shows that BP is quite accurate in estimating Z (  X  1) for individual instances, which in turn captures the number of clusters. Instances whi ch are not 3-colorable are not shown, and BP in general incorrectly estimates a non-zero number of clusters for them.
 Estimates on very large graphs and for various graph densiti es. Figure 2 shows similar data from a different perspective: what is shown is a rescaled ave rage estimate of the number of clusters (y-axis) for average vertex degrees 1 . 0 to 4 . 7 (x-axis). The average is taken across different colorable instances of a given size, and the rescaling assumes that the number of clusters = exp( | V |  X ) where  X  is a constant independent of the number of vertices [ 3 ]. The three curves show, respectively, BP X  X  estimate for graphs with 100 , 000 vertices, BP X  X  estimate for graphs with 100 vertices, and Z (  X  1) for the same graphs of size 100 . The averages are computed across 3 , 000 instances of the small graphs, and only 10 instances of the large ones where the instance-to-instance variability is practically non-existent. The fact that the curves nicely overlay shows that BP (  X  1) computes Z (  X  1) very accurately Figure 3: Z BP (  X  1) compared to Z (  X  1) for 3-COL problem on random graphs with 50 , 100 and 200 vertices and average vertex degree in the range 1 . 0  X  4 . 7 . on average for colorable instances (where we can compare it w ith exact values), and that the esti-mate remains accurate for large problems. Note that the Surv ey Propagation algorithm developed by Braunstein et al. [ 3 ] also aims at computing the number of certain clusters in the solution space. However, SP counts only the number of clusters with a  X  X ypica l size X , and would show non-zero values in Figure 2 only for average vertex degrees between 4 . 42 and 4 . 7 . Our algorithm counts clusters of all sizes, and is very accurate in the entire rang e of graph densities. We discuss a purely combinatorial construction for estimat ing the number of solution clusters in graph coloring problems with very high accuracy. The techni que uses a hypercube-based inclusion-exclusion argument coupled with solution counting, and len ds itself to an application of a modified belief propagation algorithm. This way, the number of clust ers in huge random graph coloring instances can be accurately and efficiently estimated. Our p reliminary investigation has revealed that it is possible to use combinatorial arguments to formal ly prove that the cluster counts estimated by Z (  X  1) are exact on certain kinds of solution spaces (not necessari ly only for graph coloring). We hope that such insights and the cluster-focused picture wil l lead to new techniques for solving hard combinatorial problems and for bounding solvability trans itions in random problem ensembles. References
