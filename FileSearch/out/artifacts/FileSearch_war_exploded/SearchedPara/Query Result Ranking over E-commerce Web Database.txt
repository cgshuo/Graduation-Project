 To deal with the problem of too many results returned from an E-commerce Web database in response to a user query, this paper proposes a novel approach to rank the query results. Based on the user query, we speculate how mu ch the user cares about each attribute and assign a corresponding weight to it. Then, for each tuple in the query result, each attribute value is assigned a score according to its  X  X esirableness X  to the user. These attribute value scores are combined according to the attribute weights to get a final ranking score for each tuple. Tuples with the top ranking scores are presented to the user first. Our ranking method is domain independent and requires no user feedback. Experimental results demonstrate that this ranking me thod can effectively capture a user X  X  preferences. H.3.5 [ INFORMATION STORAGE AND RETRIEVAL ]: Online Information Services -Web-based services Algorithms, Design, Experime ntation, Human Factors. E-commerce, Query result ranking, Attribute weight assignment With the rapid expansion of the World Wide Web, more and more Web databases are available. At the same time, the size of existing Web databases is growing rapidly. One common problem faced by Web users is that there is usually too many query results returned for a submitted query. For example, when a user submits a query to autos.yahoo.com to search for a used car within 50 miles of New York with a price between $5,000 and $10,000, 10,483 records are returned. In order to find  X  X he best deal X , the user has to go through this long list and compare the cars to each other, which is a tedious and time-consuming task. Most Web databases rank their query results in ascending or descending order according to a single attribute (e.g., sorted by date, sorted by price, etc.). Howeve r, many users probably consider multiple attributes simultaneously when judging the relevance or desirableness of a result. While so me extensions to SQL allow the user to specify attribute weights according to their importance to him/her [21], [26], this approach is cumbersome and most likely hard to do for most users since they have no clear idea how to set appropriate weights for different a ttributes. Furthermore, the user-setting-weight approach is not applicable for categorical attributes. In this paper, we tackle the many-query-result problem for Web databases by proposing an au tomatic ranking method, QRRE ( Query Result Ranking for E-commerce ), which can rank the query results from an E-commerce Web database without any user feedback. We focus specifically on E-commerce Web databases because they comprise a large part of today X  X  online databases. In addition, most E-commerce custom ers are ordinary users who may not know how to precisely express their interests by formulating database queries. The carDB Web da tabase is used in the following examples to illustrate the intu itions on which QRRE is based. Example 1 : Consider a used Car-selling Web database D with a single table carDB in which the car instances are stored as tuples with attributes: Make, Model, Ye ar, Price, Mileage and Location. Each tuple t i in D represents a used car for sale.
 Given a tuple t i in the query result T q for a query q that is submitted values, which indicates its desirableness, from an E-commerce viewpoint, to the buyer. For instance, it is obvious that a luxury, new and cheap car is globally popular and desired in the used car market. However, sometimes the desired attribute values conflict with each other. For example, a new luxury car with low mileage is unlikely to be cheap. Hence, we need to decide which attributes are more important for a buyer. Some buyer may care more about the model of a car, while some other buyer may be more concerned about its price. For each attribute, we use a weight to denote its importance to the user. In this work, we assume that the attributes about which a user cares most are present in the query he/she submits, from which the attribute importance can be inferred. We define specified attributes to be attributes that are specified in a query and unspecified Furthermore, we also consider that a subset of the unspecified attributes, namely, those attributes that are closely correlated to the query, is also important. Example 2 : Given a query with condition  X  X ear &gt; 2005 X , the query condition suggests that the user wants a relatively new car. Intuitively, besides the Year attri bute, the user is more concerned about the Mileage than he/she is concerned about the Make and Location, considering that a relatively new car usually has low mileage.
 Given an unspecified attribute A i , the correlation between A user query q is evaluated by the difference between the distribution whole database D . The bigger the difference, the more A to the specified attribute value(s). Consequently, we assign a bigger attribute weight to A i . Example 3 explains our intuition. Example 3 : Suppose a used car database D contains car instances for which the Year has values 1975 and onwards and D returns a subset d containing the tuples that satisfy the query with condition  X  X ear &gt; 2005 X . Intuitively, Mileage values of the tuples in d distribute in a small and dense range with a relatively low average, while the Mileage values of tuples in D distribute in a large range with a relatively high average. The distribution difference shows a close correlation between the uns pecified attribute, namely, Mileage, and the query  X  X ear &gt; 2005 X . Besides the attribute weight, we also assign a preference score to each attribute value, including the values of both specified and unspecified attributes. In the E-commerce context, we first assume that an expensive product is less preferred than a cheap product if other attribute values are equal. Hence, we assign a small preference score for a high Price value and a large preference score for a low Price value. We further assume th at a non-Price attribute value with high desirableness, such as a l uxury car or a new car, correlates positively with a high Price value. Thus, a luxury car is more expensive than a standard car and a new car is usually more expensive than an old car. Based on this assumption, we convert a value a i of a non-Price attribute A i to a Price value p X  the average price of the product for A i = a i Consequently, the preference score for a i will be large if p X  because a large Price value denotes a high desirableness for the user. Finally, the attribute weight and the value preference score are combined to get the final ranking score for each tuple. The tuples with the largest ranking scores are presented to the user first. The contributions of this paper include the following: 1. We present a novel approach to rank the tuples in the query 2. We propose a new attribute importance learning approach, 3. We also propose a new attribute-value preference score In the entire ranking process, no user feedback is required. The rest of the paper is organi zed as follows. Section 2 reviews some related work. Section 3 gives a formal definition of the many-query-result problem and presents an overview of QRRE. Section 4 proposes our attribute importance l earning approach while Section 5 presents our attribute preference score assignment approach. Experimental results are reporte d in Section 6. The paper is concluded in Section 7. Query result ranking has been investigated in information retrieval for a long time. Cosine Similarity with TF-IDF weighting of the vector space model [2] and [26], the probabilistic ranking model [30] and [31] and the statistical language model [12] have been successfully used for ranking purposes. In addition, [10], [11], [14] and [15] explore the integrati on of database and information retrieval techniques to rank tuples w ith text attributes. [1], [5] and [17] propose some keyword-query based retrieval techniques for databases. However, most of these techniques focus on text attributes and it is very difficult to apply these techniques to rank tuples with categorical or numerical attributes. Some recent research addresses the problem of relational query result ranking. In [9], [26], [28] and [33], user relevance feedback is employed to learn the similarity between a result record and the query, which is used to rank the query results in relational multimedia databases. In [21] a nd [26], the SQL query language is extended to allow the user to specify the ranking function according to their preference for the attributes. In [18] and [19], users are required to build profiles so that the query result is ranked according to their profile. Compared with the above work, our approach is fully automatic and does not require user feedback or other human involvement. In [1] and [12], two ranking methods have been proposed that take advantage of the links (i.e., associ ations) among records, such as the citation information between papers. Unfortunately, linking information among records does not exist for most domains. The work that is most similar to ours is the probabilistic information retrieval (PIR) model in [8], wh ich addresses the many-query-result problem in a probabilistic framework. In PIR, the ranking score is composed of two factors: global score, which captures the global importance of unspecified values , and conditional score, which captures the strength of the depe ndencies between specified and unspecified attribute values. The two scores are combined using a probabilistic approach. Our approach differs from that in [8] in the following aspects: 1. PIR only focuses on point queries, such as  X  A i = a 2. PIR focuses on the unspecified attributes during query result 3. A workload containing past user queries is required by PIR in The experimental results in Sec tion 6 show that QRRE produces a better quality ranking than does PIR. The attribute-importance learning problem was studied in [23] and [24], in which attribute importance is learned according to the attribute dependencies. In [23], a Bayesian network is built to discover the dependencies among attri butes. The root attribute is the most important while the leaf attributes are less important. In [24], an attribute dependency graph is built to discover the attribute dependencies. Both of these methods learn the attribute importance based on some pre-extracted data and their result is invariant to the user queries. Fu rthermore, both methods can only determine the attribute importance sequence. They are incapable of giving a specific value to show how important each attribute is. In contrast, the attribute-importance learning method presented in this paper can be adapted to the user X  X  query and thus can be tailored to take into account the desire of different users, since each attribute is assigned a weight that denotes its importance for the user. To our knowledge, this is the first work that generates attribute weights that are adaptive to the query the user submitted. In this section, we first define the many-query-result problem and then present an overview of QRRE. Consider an autonomous Web database D with attributes A ={ A ..., A m } and a selection query q over D with a conjunctive selection condition that may include point queries, such as  X  A i = a queries, such as  X  a i 1 &lt; A i &lt; a i 2  X . Let T ={ t result tuples returned by D for the query q . In many cases, if q is not a selective query, it will produce a large number of query results (i.e., a large T ). The goal is to develop a ranking function to rank the tuples in T that captures the user X  X  preference, is domain-independent and does not require any user feedback. Initially, we focus on E-commerce Web databases because E-commerce Web databases comprise a large proportion of the databases on the Web. We further assume that each E-commerce Web database has a Price attribute, which we always assume to be A . The Price attribute A 1 plays an intermediate role for all attributes during the attribute preference score assignment. Example 4 : Consider the tuples in Table 1 that represent an example query result set T. It can be seen that most tuples have their own advantages when compared with other tuples. For example, t is a relatively new car while t 2 is a luxury car and t 3 among all cars. Hence, depending on a user X  X  prefer ences, different rankings may be needed for differe nt users. Assuming that a user would prefer to pay the smallest amount for a car and that all other attribute values are equal, then the only certainty is that t always be ranked after t 3 because its mileage is higher than t it is more expensive than t 3 . According to Example 4 , two problems need to be solved when we assign a ranking score for a tuple t i ={ t i 1 , t result T : 1. How can we surmise how much a user cares about an attribute 2. How do we assign a preference score v ij for an attribute value t For example, when assigning the score for the attribute value  X  X ear = 2005 X  in t 1 , should the score be larger than the score assigned for attribute value  X  X ear = 2002 X  in t 2 and how much larger is reasonable? The first problem will be discussed in Section 4. The second problem will be discussed in Section 5. Having assigned a preference score v ij (1  X  j  X  m ) to each attribute-value of t i and a weight w j to the attribute A scores v ij are summed to obtain the ranking score s the attribute importance for the user. That is: The overall architecture of a system employing QRRE is shown in Figure 1. Such a system include s two components: pre-processing component and online processing component. The pre-processing component collects statistic s about the Web database D using a set of selected queries. Two kinds of histograms are built in the pre-processing step: single-attribut e histograms and bi-attribute histograms. A single-attribute histogram is built for each attribute A A bi-attribute histogram is built for each non-Price attribute (i.e., A in which i &gt;1) using the Price attribute A 1 . The online-processing component ranks the query results given the user query q . After getting the query results T from the Web database D for q , a weight is assigned for each attribute by the same time, the preference scor e for each attribute value in the query result is determined using the information from the bi-attribute histograms. The attribute weights and preference scores are combined to calculate the ranking score for each tuple in the query result. The tuples X  ranking scores are sorted and the top K tuples with the largest ranking scores are presented to the user first. In the real world, different user s have different preferences. Some people prefer luxury cars while some people care more about price than anything else. Hence, we need to surmise the user X  X  preference when we make recommendations to the user as shown by Example 4 in Section 3. The difficulty of this problem lies in trying to determine what a user X  X  preference is (i.e., which attributes are more important) when no user feedback is provided. To address this problem, we start from the query the user submitted. We assume that the user X  X  preference is refl ected in the submitted query and, hence, we use the query as a hint for assigning weights to attributes. The following example provides the intuition for our attribute weight assignment approach. Example 5 : Consider the query q with condition  X  X ear &gt; 2005 X , which denotes that the user pref ers a relatively new car. It is obvious that the specified attribute Year is important for the user. However, all the tuples in the query result T satisfy the query condition. Hence, we need to look beyond the specified attribute and speculate further about what the user  X  X  preferences ma y be from the specified attribute. Since the user is interested in cars that are made after 2005, we may speculate that the user cares about the Mileage of the car. Considering the distri bution of Mileage values in the database, cars whose model year is greater than 2005 usually have a lower mileage when compared to all other cars. In contrast, attribute Location is less important for the user and its distribution in cars whose model year is greater than 2005 may be similar to the distribution in the entire database. According to this intuition, an attribute A j that correlates closely with the query will be assigned a large weight and vice verse. Furthermore, as Example 3 in S ection 1 shows, the correlation of A and the query can be measured by the data distribution difference of A in D and in T . It should be noted that the specified attribute is not always important, especially when the condition for the specified attribute is not selective. For example, for a query with condition  X  X ear &gt; 1995 and Make = BMW X , the specified attribute Year is not important because almost all tuples in the database satisfy the condition  X  X ear &gt; 1995 X  and the Year distribution in D and in T is similar. A natural measure of the distribution difference of A j in D and in T is the Kullback-Leibler distance or Kullback-Leibler (KL) divergence [13]. Suppose that A j is a categorical attribute with value set { a j 1 , a j 2 , ..., a jk }. Then the KL-divergence of A in which prob(A j =a jl | D) refers to the probability that A and prob(A j =a jl | T) refers to the probability that A a numerical attribute, its value range is first discretized into a few value sets, where each set refers to a category, and then the KL-divergence of A j is calculated as in (1). To calculate the KL-divergence in equation (1) we need to obtain the distribution of attribute values over D . The difficulty here is that we are dealing with an autonomous database and we do not have full access to all the data. In [24], the attribute value distribution over a collection of data crawled from D is used to estimate the actual attribute value distribution over D . However, it is highly likely that the distribution of the crawled data can be different from that of D because the crawled data may be biased to the submitted queries. In this paper, we propose a pr obing-and-count based method to build a histogram for an attribute over a Web database 1 . We assume that the number of query results is available in D for a given query. After submitting a set of selected queries to D , we can extract the number of query results, instead of the actual query results, to get the attribute value distribution of A i . An equi-depth histogram [27] is used to represent the attribute va lue distribution, from which we will get the probability required in Equation (1). The key problem in our histogram construction for A i is how to generate a set of suitable queries to probe D . Figure 2 shows the algorithm for building a histogram for attribute A . For each attribute A i , a histogram is built in the preprocessing stage. We assume that one attribute value of A i is enough to be a query for D . If A i is a categorical attribute, each category of A used as a query to get its o ccurrence count (Lines 2-3). If A numerical attribute, an equal-depth histogram is built for A decide the occurrence frequency threshold t for each bucket by dividing | D |, namely, the number of tuples in D , with the minimum bucket number n that will be created for a numerical attribute A our experiments, n is empirically set to be 20. Then we probe D using a query with condition on A i such that low  X  A i the number of instances in that range (Line 8). If c is smaller than t , a bucket is added for it in H Di (Line 10) and another query probe is prepared (Line 11). Otherwise, we update the query probe condition on A i by reducing the size of the bucket (Line 13) and a new iteration begins. The iteration continues until each value in the value range is in a bucket. It is obvious that there are some improvements that can be made to the algorithm to accelerate the histogram construction. The improvements are not described here because histogram construction is not th e major focus of this paper. Considering that only a single-attri bute histogram is constructed, the process should complete quickly. Although both our histogram construction method and the histogram construction methods in [1] and [5] are probing-based, they have different goals. The goal in [1] and [5] is to build a histogram that precisely describes the regions on which the queries concentrate, while our purpose is to build a histogram that summarizes the data distribution of D as precisely as possible with a number of query probes. A histogram H Ti also needs to be built for A i over T (the result set) to get its probability distribution over T . For each bucket of H bucket with the same bucket boundary is built in H frequency is counted in T . converted to a probability distribution by dividing the frequency in each bucket of the histogram by the bucket frequency sum of the histogram. That is, the probability distribution of A i for D , P in which c Dk is the frequency of the k th bucket in H probability distribution of A i for T, P Ti , is in which c Tk is the frequency of the k th bucket in H Next, for the i th attribute A i , we assign its importance w The attribute weight assignment is performed not only on the unspecified attributes, but also on the specified attributes. If a specified attribute is a point condition, its attribute weight will be the same for all tuples in the query result. If a specified attribute is a range condition, its attribute weight will be different for the tuples in the query result. Example 6 illustrates this point. Example 6: Consider a query q with condition  X  X ake = 2004 and attribute, the attribute weight a ssigned to it is useless because all the query results have the same value for Make. On the other hand, tuples is an important factor to consider during ranking. In our experiments, we found that the attribute weight assignment was intuitive and reasonable for the given queries. Table 2 shows the attribute weight assigned to di fferent attributes corresponding to different queries in our experime nts for the carDB. Given a query with condition  X  X ileage &lt; 20000 X , which means that the user prefers a new car, as expected the attribute  X  X ileage X  is assigned a large weight because it is a specified attribute and the attribute  X  X ear X  is assigned a large weight too. The attribute  X  X odel X  is assigned a large weight because a new car usually has a model that appears recently. In contrast, Consider the query with condition  X  X ake = BMW &amp; Mileage &lt; 100000 X . The sub-condition  X  X ileage &lt; 100000 X  possesses a very weak selective capability because almost all tuples in the database satisfy it. The buyer is actually just concerned about the Make and the Model of the car. As expected, the attribute Make and Model are assigned large weights, while Year and Mileage are no longer assigned large weights. In addition to the attributes them selves, different values of an attribute may have different attrac tions for the buyer. For example, a car with a low price is obviously more attractive than a more expensive one if other attribute values are the same. Similarly, a car with low mileage is also obvious ly more desirable. Given an attribute value, the goal of the attribute preference score assignment desirableness for the buyer. To facilitate the combination of scores of different attribute values, all sc ores assigned for different attribute values are in [0, 1]. Instead of requiring human involvement for attribute value assignment, given a normal E-co mmerce context, we make the following two intuitive assumptions: 1. Price assumption: A product with a lower price is always more 2. Non-Price assumption: A non-Price attribute value with higher 
Figure 2: Probing-based histogr am construction algorithm. With the above two assumptions, we divide the attributes into two sets: Price attribute set, which onl y includes the attribute Price, and non-Price attribute set, which incl udes all attributes except Price. The two sets of attributes are handled in different ways. According to the Price assumption, we assign a large score for a low price and a small score for a high price. To avoid requiring human involvement to assign a suitable score for a Price value, the Price distribution in D is used to assign the scores. Given a Price value t , a score v t is assigned to it as the per centage of tuples whose Price value is bigger than t i in D : in which S t denotes the number of tuples whose Price value is bigger than t . In our experiments, the histogram for the attribute Price A whose construction method is describe d in Section 4, is used for the Price preference score assignment. Figure 3 shows the algorithm used to assign a score v for a Price value t using the Price histogram. Given the Price histogram H the frequency sum is fist calculated (Line 1). Then we count the number S t of tuples whose Price value is bigger than t . For each bucket in H D1 , if the lower boundary of the bucket is bigger than t , it means that all the tuples for this bucket have a Price value bigger than t and the frequency of this bucket is added to S t within the boundary of the bucket, we assume that the Price has a uniform distribution in the bucket and a fraction of the frequency in this bucket is added to S t (Line 5). If the upper boundary of the have a Price value lower than t and the bucket is ignored. Finally the ratio is generated by dividing S t with the frequency sum. For a value a i of a non-Price attribute A i , the difficulty of assigning it a score is two fold: 1. How to make the attribute preference score assignment adaptive 2. How to establish the correspondence between different We solve the problem in two steps. First, based on the non-Price assumption, we can convert a non-Price value a i to a Price attribute value t i :  X  If A i is a categorical attribute, t i is the average price for all tuples  X  If A i is a numerical attribute, v i is the average price for all tuples In our experiments, a bi-attribute histogram ( A 1 , A i is converted to a Price value. The bi-attribute histograms are built in the pre-processing step in a way similar to the histogram construction described in Section 4. Second, after converting all non-Price attribute values to Price values, we use a uniform mechanism to assign them a preference score. We assign a large score for a large Price value according to the non-Price assumption. That is, given a converted Price value t preference score v i is assigned to it as the percentage of Price values preference score assignment can be easily adapted from the algorithm in Figure 3. Table 3 shows the average Price and assigned score for different Make values for the carDB database used in our experiments. It can be seen that the prices for different car makes fit our intuition well. Luxury cars are evaluated to have a higher price than standard cars and, consequently, are assigned a larger preference score. We found that the attribute preference assignments for other attributes in carDB are intuitive too. In this section, we describe our experiments, report the QRRE experimental results and compare them with some related work. We first introduce the databases we used and the related work for comparison. Then we informally give some examples of query result ranking to provide some intu ition for our experiments. Next, a more formal evaluation of the ranki ng results is presented. Finally, the running time statistics are presented. To evaluate how well different ra nking approaches capture a user X  X  preference, five postgraduate student s were invited to participate in the experiments and behave as buyers from the E-commerce databases. For our evaluation, we set up two databases from two domains in E-commerce. The first database is a used car database carDB(Make, Model, Year, Price, Mileage, Location) containing 100,000 tuples extracted from Yahoo! Autos. The attributes Make, Model, Year and Location are categorical attributes and the attributes Price and Mileage are numerical attributes. The second database is a real estate database houseDB(City, Loca tion, Bedrooms, Bathrooms, Sq Ft, Price) containing 20,000 tuples extracted from Yahoo! Real Estate. The attributes City, Loca tion, Bedrooms and Bathrooms are categorical attributes and the attributes Sq Ft and Price are numerical attributes. To simula te the Web databases for our experiments we used MySQL on a P4 3.2-GHz PC with 1GB of RAM . We implemented all algorith ms in JAVA and connected to the RDBMS by DAO. Besides QRRE describe d above, we implemented two other ranking methods, which are described briefly below, to compare with QRRE. RANDOM ranking model : In the RANDOM ranking model, the tuples in the query result are presen ted to the user in a random order. The RANDOM model provides a base line to show how well QRRE can capture the user behavior over a random method. Probabilistic Information Retr ieval (PIR) ranking model : A probabilistic information retrieval (PIR) technique, which has been successfully used in the Information Retrieval field, is used in [8] for ranking query results. This technique addresses the same problem as does QRRE. In PIR, given a tuple t , its ranking score is given by the following equation: in which X is the specified attributes, Y is the unspecified attributes, W is a past query workload and p denotes the probability. As mentioned in Section 2, PI R work focuses on point queries without considering range queries. Therefore, when applying the PIR ranking model, the numerical attributes Price and Mileage in carDB and Sq Ft and Price in houseDB are discretized into meaningful ranges as categories, which in reality requires a domain expert. In PIR, a workload is required to obtain the conditional probability used to measure the correlation between specified attribute values present in the query and the uns pecified attributes. In our experiments, we requested 5 subjects to behave as different kinds of buyers, such as rich people, cler ks, students, women, etc. and post queries against the databases. We collected 200 queries for each database and these queries are used as the workload W for the PIR model. When we examine the query result rankings, we find that the ranking results of both QRRE and PIR are much more reasonable and intuitive than that of RANDOM. However, there are some interesting examples that show th at the QRRE rankings are superior to those of PIR. We found that the ranking result of QRRE is more reasonable than that of PIR in several ways:  X  QRRE can discover an assumption that is implicitly held by a  X  In PIR, given a numerical attribute, its value range needs to be  X  QRRE considers the value difference of the specified attributes Likewise, QRRE often produces a ranking better than does PIR for houseDB. The actual evaluation in the following section confirms these observations. We now present a more formal evaluation of the query result ranking quality. A survey is conducted to show how well each ranking algorithm captures the user X  X  preference. We evaluate the query results in two ways: average precision and user preference ranking. In this experiment, each subject was asked to submit three queries for carDB and one query for houseDB according to their preference. Each query had on average 2.2 specified attributes for carDB and 2.4 specified attributes for houseDB. We found that all the attributes of carDB and houseDB were specified in the collected queries at least once. On average, for carDB, each query had a query result of 686 tuples, with the maximum bei ng 4,213 tuples and the minimum 116 tuples. It can be seen that the many-query-result problem is a common problem in reality. Each query for houseDB has a query result of 166 tuples on average. result for a query, we adopt the following strategy to compare the performance of different ranking approaches. For each implemented ranking algorithm, we collected the first 10 tuples that it recommended. Hence, thirty tuples are collected in total. If there is overlap among the recommended tupl es from different algorithms, we extract more tuples using the RANDOM algorithm so that thirty unique tuples are collected in total. Next, for each of the fifteen queries, each subject was asked to rank the top 10 tuples as the relevant tuples that they preferre d most from the thirty unique tuples collected for each query. During ranking, they were asked to behave like real buyers to rank the record s according to their preferences. 
Table 4: Average precision for different ranking methods for We use the Precision/Recall metrics to evaluate how well the user X  X  preference is captured by the different ranking algorithms. Precision is the ratio obtained by dividing the number of retrieved tuples that are relevant by the total number of retrieved tuples. Recall is the ratio obtained by dividing the number of relevant tuples by the number of tuples that are retrieve d. In our experiments, both the relevant tuples and the retrieve d tuples are 10, which make the Precision and Recall to be equal. Table 4 shows the average precision of the different ranking methods for each query. It can be seen that both QRRE and PIR cons istently have a higher precision than RANDOM. For 11 queries out of 15, the precision of QRRE is higher than that of PIR. The precision of QRRE is equal to that of PIR for two queries and is lower than that of PIR for the remaining two queries. QRRE X  X  average precisi on is 0.18 higher than that of PIR. QRRE has a precision higher than 0.5 for each query while PIR has a precision as low as 0.22 for q3. It should be noted that there is some overlap between the top-10 ranked results of QRRE and top-10 ranked results of PIR for most queries. Figure 4 and Figure 5 show the average precision of the three ranking methods graphically for both carDB and houseDB. 
Figure 4: Average prevision for different ranking methods for 
Figure 5: Average precision for different ranking methods for In this experiment, 10 queries were collected from the 5 subjects for carDB and 5 queries were collected for houseDB. After getting the query results, they were ranked using the three ranking methods. The ranking results were then provided to the subjects in order to let them select which result they liked best. Table 5 and Table 6 show the user preference ranking (UPR) of the different ranking methods for each query for carDB and houseDB, respectively. It can be seen that again both QRRE and PIR greatly outperform RANDOM. In most cases, the subjects preferred the ranking results of QRRE to that of PIR. While these preliminary experiments indicate that QRRE is promising and better than the existing work, a much larger scale user study is necessary to conc lusively establish this finding. Using QRRE, histograms need to be constructed before the query results can be ranked. The hist ogram construction time depends on the number of buckets and the tim e to query the web to get the number of occurrences for each bucket. However, in most cases the histogram usually does not change very much over time and so needs to be constructed only once in a given time period. The query result ranking in the online processing part includes four modules: the attribute weight assi gnment module, the attribute-value preference score assignment module, the ranking score calculation module and the ranking score sorting module. Each of the first three modules has a time complexity of O( n ) after constructing the histogram, where n is the number of query results, and the ranking score sorting module has a time complexity of O( n log( n )). Hence, the overall time complexity for the online processing stage is O( n log( n )). Figure 6 shows the online execution time of the queries over carDB seen that the execution time of Q RRE grows almost linearly with the number of tuples in the query result. This is because ranking score sorting is fairly quick even for a large amount of data and thus most of the running time is spen t in the first three modules. Figure 6: Execution times for different numbers of query results In this paper, a novel automated ranking approach for the many-query-result problem in E-commerce is proposed. Starting from the user query, we assume that the specified attributes are important for the user. We also assume that the attributes that are highly correlated with the query also are important to the user. We assign a weight for each attribute according to its importance to the user. Then, for each value in each tuple of the query result, a preference score is assigned according to its desirableness in the E-commerce context, where users are assumed to more prefer products with lower prices. All preference scores are combined according to the attribute weight assigned to each attribute. No domain knowledge or user feedback is required in the whole process. Preliminary experimental results indicate that QRRE captures the user preference fairly well and better than existing works. We acknowledge the following shortcoming of our approach, which string attributes, such as book titles or the comments for a house, contained in many Web databases. It would be extremely useful to find a method to incorporate string attributes into QRRE. Second, QRRE has only been evaluated on small-scale datasets. We realize that a large, comprehensive benchmark should be built to extensively evaluate a query re sult ranking system, both for QRRE and for future research. Finally, QRRE has been specifically tailored for E-commerce Web databases. It would be interesting to extend QRRE to also deal with non-E-commerce Web databases. This research was supported by the Research Grants Council of Hong Kong under grant HKUST6172/04E. [1] A. Aboulnaga and S. Chaudhur i.  X  X elf-tuning Histograms: [2] S. Agrawal, S. Chaudhuri and G. Das.  X  X BXplorer: A System [3] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information [4] A. Balmin, V. Hristidis and Y. Papakonstantinou. [5] G. Bhalotia, C. Nakhe, A. Hulg eri, S. Chakrabarti and S. [6] N. Bruno, S. Chaudhuri and L. Gravano.  X  X THoles: A [7] K. Chakrabarti, S. Chaudhur i and S. Hwang.  X  X utomatic [8] S. Chaudhuri, G. Das, V. Hristidis and G. Weikum. [9] K. Chakrabarti, K. Porkaew and S. Mehrotra.  X  X fficient Query [10] W. Cohen.  X  X ntegration of He terogeneous Databases Without [11] W. Cohen.  X  X roviding Database -like Access to the Web Using [12] W.B. Croft and J. Lafferty. Language Modeling for [13] R.O. Duda, P.E. Hart and D.G. Stork, Pattern Classification . [14] N. Fuhr.  X  X  Probabilistic Framework for Vague Queries and [15] N. Fuhr.  X  X  Probabilistic Relational Model for the Integration [16] F. Geerts, H. Mannila and E. Terzi.  X  X elational Link-based [17] V. Hristidis and Y. Papakonsta ntinou.  X  X ISCOVER: Keyword [18] G. Koutrika and Y.E. Ioannidis.  X  X ersonalization of Queries in [19] G. Koutrika and Y.E. Ioannidi s.  X  X onstrained Optimalities in [20] Y.E. Ioannidis.  X  X he Histor y of Histograms (abridged), X  Proc. [21] W. Kie X ling.  X  X oundations of Preferences in Database [22] R. Kooi. The Optimization of Queries in Relational Databases . [23] I. Muslea and T. Lee.  X  X nline Query Relaxation via Bayesian [24] U. Nambiar and S. Kambhamp ati.  X  X nswering Imprecise [25] Z. Nazeri, E. Bloedorn and P. Ostwald.  X  X xperiences in [26] M. Ortega-Binderberger, K. Ch akrabarti and S. Mehrotra.  X  X n [27] G. Piatetsky-Sharpiro and C. Connell.  X  X ccurate Estimation of [28] Y. Rui, T.S. Huang and S. Merhotra.  X  X ontent-Based Image [29] G. Salton, A. Wong and C.S. Yang.  X  X  Vector Space Model [30] K. Sparck Jones, S. Walker and S.E. Robertson.  X  X  [31] K. Sparck Jones, S. Walker and S.E. Robertson.  X  X  [32] E.M. Voorhees.  X  X he TREC-8 Question Answering Track [33] L. Wu, C. Faloutsos, K. Sycara and T. Payne.  X  X ALCON: 
