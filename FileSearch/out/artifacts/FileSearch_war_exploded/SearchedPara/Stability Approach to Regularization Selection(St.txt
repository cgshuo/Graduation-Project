 Undirected graphical models have emerged as a useful tool because they allow for a stochastic description of complex associations in high-dimensional data. For example, biological processes in a cell lead to complex interactions among gene products. It is of interest to determine which features of the system are conditionally independent. Such problems require us to infer an undirected graph from i.i.d. observations. Each node in this graph corresponds to a random variable and the existence of an edge between a pair of nodes represent their conditional independence relationship. Gaussian graphical models [4, 23, 5, 9] are by far the most popular approach for learning high di-mensional undirected graph structures. Under the Gaussian assumption, the graph can be estimated using the sparsity pattern of the inverse covariance matrix. If two variables are conditionally inde-pendent, the corresponding element of the inverse covariance matrix is zero. In many applications, estimating the the inverse covariance matrix is statistically challenging because the number of fea-tures measured may be much larger than the number of collected samples. To handle this challenge, the graphical lasso or glasso [7, 24, 2] is rapidly becoming a popular method for estimating sparse undirected graphs. To use this method, however, the user must specify a regularization parameter  X  that controls the sparsity of the graph. The choice of  X  is critical since different  X   X  X  may lead to different scientific conclusions of the statistical inference. Other methods for estimating high dimen-sional graphs include [11, 14, 10]. They also require the user to specify a regularization parameter. The standard methods for choosing the regularization parameter are AIC [1], BIC [19] and cross validation [6]. Though these methods have good theoretical properties in low dimensions, they are not suitable for high dimensional problems. In regression, cross-validation has been shown to overfit the data [22]. Likewise, AIC and BIC tend to perform poorly when the dimension is large relative to the sample size. Our simulations confirm that these methods perform poorly when used with glasso. A new approach to model selection, based on model stability, has recently generated some interest in the literature [8]. The idea, as we develop it, is based on subsampling [15] and builds on the approach of Meinshausen and B  X  uhlmann [12]. We draw many random subsamples and construct a graph from each subsample (unlike K -fold cross-validation, these subsamples are overlapping). We choose the regularization parameter so that the obtained graph is sparse and there is not too much variability across subsamples. More precisely, we start with a large regularization which corresponds to an empty, and hence highly stable, graph. We gradually reduce the amount of regularization until there is a small but acceptable amount of variability of the graph across subsamples. In other words, we regularize to the point that we control the dissonance between graphs. The procedure is named StARS: Stability Approach to Regularization Selection. We study the performance of StARS by simulations and theoretical analysis in Sections 4 and 5. Although we focus here on graphical models, StARS is quite general and can be adapted to other settings including regression, classification, clustering, and dimensionality reduction.
 In the context of clustering, results of stability methods have been mixed. Weaknesses of stability have been shown in [3]. However, the approach was successful for density-based clustering [17]. For graph selection, Meinshausen and B  X  uhlmann [12] also used a stability criterion; however, their approach differs from StARS in its fundamental conception. They use subsampling to produce a new and more stable regularization path then select a regularization parameter from this newly created path, whereas we propose to use subsampling to directly select one regularization parameter from the original path. Our aim is to ensure that the selected graph is sparse, but inclusive, while they aim to control the familywise type I errors. As a consequence, their goal is contrary to ours: instead of selecting a larger graph that contains the true graph, they try to select a smaller graph that is contained in the true graph. As we will discuss in Section 3, in specific application domains like gene regulatory network analysis, our goal for graph selection is more natural. Let X = ( V,E ) associated with P has vertices V = { X (1) ,...,X ( p ) } and a set of edges E corresponding to pairs of vertices. In this paper, we also interchangeably use E to denote the adjacency matrix of the independent given the other coordinates of X . The graph estimation problem is to infer E from i.i.d. observed data X 1 ,...,X n where X i = ( X i (1) ,...,X i ( p )) T .
 Suppose now that P is Gaussian with mean vector  X  and covariance matrix  X  . Then the edge corresponding to X ( j ) and X ( k ) is absent if and only if  X  jk = 0 where  X  =  X   X  1 . Hence, to estimate the graph we only need to estimate the sparsity pattern of  X  . When p could diverge with n , estimating  X  is difficult. A popular approach is the graphical lasso or glasso [7, 24, 2]. Using glasso, we estimate  X  as follows: Ignoring constants, the log-likelihood (after maximizing over  X  ) can be written as  X  ( X ) = log |  X  | X  trace regularization parameter  X  , the glasso estimator b  X (  X  ) is obtained by minimizing the regularized negative log-likelihood where ||  X  || 1 = the corresponding entry in b  X (  X  ) is nonzero. Friedman et al. [7] give a fast algorithm for calculating b  X (  X  ) over a grid of  X  s ranging from small to large. By taking advantage of the fact that the objec-tive function in (1) is convex, their algorithm iteratively estimates a single row (and column) of  X  in each iteration by solving a lasso regression [21]. The resulting regularization path b  X (  X  ) for all  X  s has been shown to have excellent theoretical properties [18, 16]. For example, Ravikumar et al. [16] show that, if the regularization parameter  X  satisfies a certain rate, the corresponding estimator b  X (  X  ) could recover the true graph with high probability. However, these types of results are either asymptotic or non-asymptotic but with very large constants. They are not practical enough to guide the choice of the regularization parameter  X  in finite-sample settings. of  X  tend to yield sparser graphs and smaller values of  X  yield denser graphs. It is convenient to define  X  = 1 / X  so that small  X  corresponds to a more sparse graph. In particular,  X  = 0 corresponds to the empty graph with no edges. Given a grid of regularization parameters G n = {  X  1 ,...,  X  K } , our goal of graph regularization parameter selection is to choose one b  X   X  G n , such that the true graph E is contained in b E ( b  X ) with high probability. In other words, we want to  X  X verselect X  instead of  X  X nderselect X . Such a choice is motivated by application problems like gene regulatory networks reconstruction, in which we aim to study the interactions of many genes. For these types of studies, we tolerant some false positives but not false negatives. Specifically, it is acceptable that an edge presents but the two genes corresponding to this edge do not really interact with each other. Such false positives can generally be screened out by more fine-tuned downstream biological experiments. However, if one important interaction edge is omitted at the beginning, it X  X  very difficult for us to re-discovery it by follow-up analysis. There is also a tradeoff: we want to select a denser graph which contains the true graph with high probability. At the same time, we want the graph to be as sparse as possible so that important information will not be buried by massive false positives. Based on this rationale, an  X  X nderselect X  method, like the approach of Meinshausen and B  X  uhlmann[12], does not really fit our goal. In the following, we start with an overview of several state-of-the-art regularization parameter selection methods for graphs. We then introduce our new StARS approach. 3.1 Existing Methods The regularization parameter is often chosen using AIC or BIC. Let b  X ( X ) denote the estimator corresponding to  X  . Let d ( X ) denote the degree of freedom (or the effective number of free pa-rameters) of the corresponding Gaussian model. AIC chooses  X  to minimize  X  2  X  and BIC chooses  X  to minimize  X  2  X  these methods assumes that the dimension p is fixed as n increases; however, in the case where p &gt; n this justification is not applicable. In fact, it X  X  even not straightforward how to estimate the degree of freedom d ( X ) when p is larger than n . A common practice is to calculate d ( X ) as d ( X ) = m ( X )( m ( X )  X  1) / 2 + p where m ( X ) denotes the number of nonzero elements of b  X ( X ) . As we will see in our experiments, AIC and BIC tend to select overly dense graphs in high dimensions. Another popular method is K -fold cross-validation ( K -CV). For this procedure the data is parti-tioned into K subsets. Of the K subsets one is retained as the validation data, and the remaining K  X  1 ones are used as training data. For each  X   X  G sets and evaluate the negative log-likelihood on the retained validation set. The results are averaged over all K folds to obtain a single CV score. We then choose  X  to minimize the CV score over he whole grid G n . In regression, cross-validation has been shown to overfit [22]. Our experiments will confirm this is true for graph estimation as well. 3.2 StARS: Stability Approach to Regularization Selection The StARS approach is to choose  X  based on stability. When  X  is 0, the graph is empty and two datasets from P would both yield the same graph. As we increase  X  , the variability of the graph increases and hence the stability decreases. We increase  X  just until the point where the graph becomes variable as measured by the stability. StARS leads to a concrete rule for choosing  X  . Let b = b ( n ) be such that 1 &lt; b ( n ) &lt; n . We draw N random subsamples S 1 ,...,S N from X 1 ,...,X n , each of size b . There are samples. However, Politis et al. [15] show that it suffices in practice to choose a large number N of subsamples at random. Note that, unlike bootstrapping [6], each subsample is drawn without replacement. For each  X   X  G n , we construct a graph using the glasso for each subsample. This value of  X  . Let  X   X  (  X  ) denote the glasso algorithm with the regularization parameter  X  . For any use a U-statistic of order b , namely, b  X  b st ( X ) = its estimate. Then  X  b st ( X ) , in addition to being twice the variance of the Bernoulli indicator of the edge ( s,t ) , has the following nice interpretation: For each pair of graphs, we can ask how often they disagree on the presence of the edge:  X  b st ( X ) is the fraction of times they disagree. For  X   X  G n , we Define the total instability by averaging over all edges: b D b ( X ) = boundary b D b (0) = 0 , and b D b ( X ) generally will increase as  X  increases. However, when  X  gets very large, all the graphs will become dense and b D b ( X ) will begin to decrease. Subsample stability for large  X  is essentially an artifact. We are interested in stability for sparse graphs not dense graphs. approach chooses  X  by defining b  X  s = sup It may seem that we have merely replaced the problem of choosing  X  with the problem of choosing  X  , but  X  is an interpretable quantity and we always set a default value  X  = 0 . 05 . One thing to note subsampling, the effective sample size for estimating the selected graph is b instead of n . Compared with methods like BIC and AIC which fully utilize all n data points. StARS has some efficiency loss in low dimensions. However, in high dimensional settings, the gain of StARS on better graph selection significantly dominate this efficiency loss. This fact is confirmed by our experiments. The StARS procedure is quite general and can be applied with any graph estimation algorithms. Here, we provide its theoretical properties. We start with a key theorem which establishes the rates of convergence of the estimated stability quantities to their population means. We then discuss the implication of this theorem on general gaph regularization selection problems.
 Let  X  be an element in the grid G n = {  X  1 ,...,  X  K } where K is a polynomial of n . We denote D b ( X ) = D b ( X ) . Standard U -statistic theory guarantees that these estimates have good uniform convergence properties to their population quantities: Theorem 1. (Uniform Concentration) The following statements hold with no assumptions on P . For any  X   X  (0 , 1) , with probability at least 1  X   X  , we have [20], we have, for any  X  &gt; 0 , Now b  X  b st ( X ) is just a function of the U -statistic b  X  b st ( X ) . Note that we obtain: for each  X   X  X  n , Using two union bound arguments over the K values of  X  and all the p ( p  X  1) / 2 edges, we have: Equations (2) and (3) follow directly from (11) and the above exponential probability inequality. Theorem 1 allows us to explicitly characterize the high-dimensional scaling of the sample size n , dimensionality p , subsampling block size b , and the grid size K . More specifically, we get by setting  X  = 1 /n in Equation (3). From (14), let c 1 ,c 2 be arbitrary positive constants, if b = c 1 to its mean D b ( X ) uniformly over the whole grid G n .
 We now discuss the implication of Theorem 1 to graph regularization selection problems. Due to the generality of StARS, we provide theoretical justifications for a whole family of graph estimation procedures satisfying certain conditions. Let  X  be a graph estimation procedure. We denote b E b ( X ) as the estimated edge set using the regularization parameter  X  by applying  X  on a subsampled dataset with block size b . To establish graph selection result, we start with two technical assumptions: Note that  X  o here depends on the sample size n and does not have to be unique. To understand the above conditions, (A1) assumes that there exists a threshold  X  o  X  G n , such that the population quantity D b ( X ) is small for all  X   X   X  o . (A2) requires that all estimated graphs using regularization parameters  X   X   X  o contain the true graph with high probability. Both assumptions are mild and should be satisfied by most graph estimation algorithm with reasonable behaviors. More detailed analysis on how glasso satisfies (A1) and (A2) will be provided in the full version of this paper. There is a tradeoff on the design of the subsampling block size b . To make (A2) hold, we require b to be large. However, to make b D b ( X ) concentrate to D b ( X ) fast, we require b to be small. Our suggested value is b =  X  10 well. The next theorem provides the graph selection performance of StARS: Theorem 2. (Partial Sparsistency): Let  X  to be a graph estimation algorithm. We assume (A1) and (A2) hold for  X  using b =  X  10 be the selected regularization parameter using the StARS procedure with a constant cutting point  X  . Then, if p  X  exp ( n ) for some  X  &lt; 1 / 2 , we have Proof. We define A n to be the event that max  X   X  X  n | b D b ( X )  X  D b ( X ) |  X   X / 2 . The scaling of n,K,b,p in the theorem satisfies the L.H.S. of (14), which implies that P ( A n )  X  1 as n  X  X  X  . Using (A1), we know that, on A n , This implies that, on A n , b  X  s  X   X  o . The result follows by applying (A2) and a union bound. We now provide empirical evidence to illustrate the usefulness of StARS and compare it with several state-of-the-art competitors, including 10 -fold cross-validation ( K -CV), BIC, and AIC. For StARS we always use subsampling block size b ( n ) =  X  10  X  quantitatively evaluate these methods on two types of synthetic datasets, where the true graphs are known. We then illustrate StARS on a microarray dataset that records the gene expression levels from immortalized B cells of human subjects. On all high dimensional synthetic datasets, StARS significantly outperforms its competitors. On the microarray dataset, StARS obtains a remarkably simple graph while all competing methods select what appear to be overly dense graphs. 5.1 Synthetic Data To quantitatively evaluate the graph estimation performance, we adapt the criteria including pre-cision, recall, and F 1 -score from the information retrieval literature. Let G = ( V,E ) be a p -Precision is the number of correctly estimated edges divided by the total number of edges in the estimated graph; recall is the number of correctly estimated edges divided by the total number of edges in the true graph; the F 1 -score can be viewed as a weighted average of the precision and recall, where an F 1 -score reaches its best value at 1 and worst score at 0. On the synthetic data where we know the true graphs, we also compare the previous methods with an oracle procedure which selects the optimal regularization parameter by minimizing the total number of different edges between the estimated and true graphs along the full regularization path. Since this oracle procedure requires the knowledge of the truth graph, it is not a practical method. We only present it here to calibrate the inherent challenge of each simulated scenario. To make the comparison fair, once the regulariza-tion parameters are selected, we estimate the oracle and StARS graphs only based on a subsampled dataset with size b ( n ) =  X  10 the full dataset. More details about this issue were discussed in Section 3.
 We generate data from sparse Gaussian graphs, neighborhood graphs and hub graphs , which mimic characteristics of real-wolrd biological networks. The mean is set to be zero and the covariance matrix  X  =  X   X  1 . For both graphs, the diagonal elements of  X  are set to be one. More specifically: We generate synthetic datasets in both low-dimensional ( n = 800 ,p = 40 ) and high-dimensional ( n = 400 ,p = 100 ) settings. Table 1 provides comparisons of all methods, where we repeat the experiments 100 times and report the averaged precision, recall, F 1 -score with their standard errors. For low-dimensional settings where n  X  p , the BIC criterion is very competitive and performs the best among all the methods. In high dimensional settings, however, StARS clearly outperforms all the competing methods for both neighborhood and hub graphs. This is consistent with our theory. At first sight, it might be surprising that for data from low-dimensional neighborhood graphs, BIC and AIC even outperform the oracle procedure! This is due to the fact that both BIC and AIC graphs are estimated using all the n = 800 data points, while the oracle graph is estimated using only the subsampled dataset with size b ( n ) =  X  10  X  is an advantage of model selection methods that take the general form of BIC and AIC. In high dimensions, however, we see that even with this advantage, StARS clearly outperforms BIC and AIC. The estimated graphs for different methods in the setting n = 400 ,p = 100 are provided in Figures 1 and 2, from which we see that the StARS graph is almost as good as the oracle, while the K -CV, BIC, and AIC graphs are overly too dense.
Figure 1: Comparison of different methods on the data from the neighborhood graphs ( n = 400 ;p = 100 ). 5.2 Microarray Data We apply StARS to a dataset based on Affymetrix GeneChip microarrays for the gene expression levels from immortalized B cells of human subjects. The sample size is n = 294 . The expression levels for each array are pre-processed by log-transformation and standardization as in [13]. Using a sub-pathway subset of 324 correlated genes, we study the estimated graphs obtained from each method under investigation. The StARS and BIC graphs are provided in Figure 3. We see that the StARS graph is remarkably simple and informative, exhibiting some cliques and hub genes. In contrast, the BIC graph is very dense and possible useful association information is buried in the large number of estimated edges. The selected graphs using AIC and K -CV are even more dense than the BIC graph and will be reported elsewhere. A full treatment of the biological implication of these two graphs validated by enrichment analysis will be provided in the full version of this paper. The problem of estimating structure in high dimensions is very challenging. Casting the problem in the context of a regularized optimization has led to some success, but the choice of the regu-larization parameter is critical. We present a new method, StARS, for choosing this parameter in high dimensional inference for undirected graphs. Like Meinshausen and B  X  uhlmann X  X  stability se-lection approach [12], our method makes use of subsampling, but it differs substantially from their approach in both implementation and goals. For graphical models, we choose the regularization pa-rameter directly based on the edge stability. Under mild conditions, StARS is partially sparsistent. However, even without these conditions, StARS has a simple interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. Empirically, we show that StARS works significantly better than existing techniques on both syn-thetic and microarray datasets. Although we focus here on graphical models, our new method is generally applicable to many problems that involve estimating structure, including regression, clas-sification, density estimation, clustering, and dimensionality reduction. [1] Hirotsugu Akaike. Information theory and an extension of the maximum likelihood principle. [2] Onureena Banerjee, Laurent El Ghaoui, and Alexandre d X  X spremont. Model selection through [3] Shai Ben-david, Ulrike Von Luxburg, and David Pal. A sober look at clustering stability. In [4] Arthur P. Dempster. Covariance selection. Biometrics , 28:157 X 175, 1972. [5] David Edwards. Introduction to graphical modelling . Springer-Verlag Inc, 1995. [6] Bradley Efron. The jackknife, the bootstrap and other resampling plans . SIAM [Society for [7] Jerome H. Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estima-[8] Tilman Lange, Volker Roth, Mikio L. Braun, and Joachim M. Buhmann. Stability-based vali-[9] Steffen L. Lauritzen. Graphical Models . Oxford University Press, 1996. [10] Han Liu, John Lafferty, and J. Wainwright. The nonparanormal: Semiparametric estimation of [11] Nicolai Meinshausen and Peter B  X  uhlmann. High dimensional graphs and variable selection [12] Nicolai Meinshausen and Peter B  X  uhlmann. Stability selection. To Appear in Journal of the [13] Renuka R. Nayak, Michael Kearns, Richard S. Spielman, and Vivian G. Cheung. Coexpression [14] Jie Peng, Pei Wang, Nengfeng Zhou, and Ji Zhu. Partial correlation estimation by joint sparse [15] Dimitris N. Politis, Joseph P. Romano, and Michael Wolf. Subsampling (Springer Series in [16] Pradeep Ravikumar, Martin Wainwright, Garvesh Raskutti, and Bin Yu. Model selection in [17] Alessandro Rinaldo and Larry Wasserman. Generalized density clustering. arXiv/0907.3454 , [18] Adam J. Rothman, Peter J. Bickel, Elizaveta Levina, and Ji Zhu. Sparse permutation invariant [19] Gideon Schwarz. Estimating the dimension of a model. The Annals of Statistics , 6:461 X 464, [20] Robert J. Serfling. Approximation theorems of mathematical statistics . John Wiley and Sons, [21] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal [22] Larry Wasserman and Kathryn Roeder. High dimensional variable selection. Annals of statis-[23] Joe Whittaker. Graphical Models in Applied Multivariate Statistics . Wiley, 1990. [24] Ming Yuan and Yi Lin. Model selection and estimation in the Gaussian graphical model.
