 Unsupervised grammar induction is a fundamental challenge of statistical natural language processing (Lari and Young, 1990; Pereira and Schabes, 1992; Carroll and Charniak, 1992). The constituent con-text model (CCM) for inducing constituency parses (Klein and Manning, 2002) was the first unsuper-vised approach to surpass a right-branching base-line. However, the CCM only effectively models short sentences. This paper shows that a simple re-parameterization of the model, which ties together the probabilities of related events, allows the CCM to extend robustly to long sentences.

Much recent research has explored dependency grammar induction. For instance, the dependency model with valence (DMV) of Klein and Manning (2004) has been extended to utilize multilingual in-formation (Berg-Kirkpatrick and Klein, 2010; Co-hen et al., 2011), lexical information (Headden III et al., 2009), and linguistic universals (Naseem et al., 2010). Nevertheless, simplistic dependency models like the DMV do not contain information present in a constituency parse, such as the attachment order of object and subject to a verb.

Unsupervised constituency parsing is also an ac-tive research area. Several studies (Seginer, 2007; Reichart and Rappoport, 2010; Ponvert et al., 2011) have considered the problem of inducing parses over raw lexical items rather than part-of-speech (POS) tags. Additional advances have come from more complex models, such as combining CCM and DMV (Klein and Manning, 2004) and model-ing large tree fragments (Bod, 2006).

The CCM scores each parse as a product of prob-abilities of span and context subsequences. It was originally evaluated only on unpunctuated sentences up to length 10 (Klein and Manning, 2002), which account for only 15% of the WSJ corpus; our exper-iments confirm the observation in (Klein, 2005) that performance degrades dramatically on longer sen-tences. This problem is unsurprising: CCM scores each constituent type by a single, isolated multino-mial parameter.

Our work leverages the idea that sharing infor-mation between local probabilities in a structured unsupervised model can lead to substantial accu-racy gains, previously demonstrated for dependency grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick et al., 2010). Our model, Log-Linear CCM (LLCCM), shares information between the probabilities of related constituents by expressing them as a log-linear combination of features trained using the gradient-based learning procedure of Berg-Kirkpatrick et al. (2010). In this way, the probabil-ity of generating a constituent is informed by related constituents.

Our model improves unsupervised constituency parsing of sentences longer than 10 words. On sen-tences of up to length 40 (96% of all sentences in the Penn Treebank), LLCCM outperforms CCM by 13.9% (unlabeled) bracketing F1 and, unlike CCM, outperforms a right-branching baseline on sentences longer than 15 words. The CCM is a generative model for the unsuper-vised induction of binary constituency parses over sequences of part-of-speech (POS) tags (Klein and Manning, 2002). Conditioned on the constituency or distituency of each span in the parse, CCM generates both the complete sequence of terminals it contains and the terminals in the surrounding context.
Formally, the CCM is a probabilistic model that jointly generates a sentence, s , and a bracketing, B , specifying whether each contiguous subsequence is a constituent or not, in which case the span is called a distituent. Each subsequence of POS tags, or S PAN ,  X  , occurs in a C ONTEXT ,  X  , which is an ordered pair of preceding and following tags. A bracketing is a boolean matrix B , indicating which spans ( i,j ) are constituents ( B ij = true ) and which are distituents ( B ij = false ). A bracketing is con-sidered legal if its constituents are nested and form a binary tree T (B) .
 The joint distribution is given by: P( s, B) = P T (B)  X  The prior over unobserved bracketings P T (B) is fixed to be the uniform distribution over all legal bracketings. The other distributions, P S (  X  ) and P
C (  X  ) , are multinomials whose isolated parameters are estimated to maximize the likelihood of a set of observed sentences { s n } using EM (Dempster et al., 1977). 1 2.1 The Log-Linear CCM A fundamental limitation of the CCM is that it con-tains a single isolated parameter for every span. The number of different possible span types increases ex-ponentially in span length, leading to data sparsity as the sentence length increases.
The Log-Linear CCM (LLCCM) reparameterizes the distributions in the CCM using intuitive features to address the limitations of CCM while retaining its predictive power. The set of proposed features includes a B ASIC feature for each parameter of the original CCM, enabling the LLCCM to retain the full expressive power of the CCM. In addition, LL-CCM contains a set of coarse features that activate across distinct spans.

To introduce features into the CCM, we express each of its local conditional distributions as a multi-class logistic regression model. Each local distri-bution, P t ( y | x ) for t  X  { S PAN , C ONTEXT } , condi-tions on label x  X  { true,false } and generates an event (span or context) y . We can define each lo-cal distribution in terms of a weight vector, w , and feature vector, f xyt , using a log-linear model:
This technique for parameter transformation was shown to be effective in unsupervised models for part-of-speech induction, dependency grammar in-duction, word alignment, and word segmentation (Berg-Kirkpatrick et al., 2010). In our case, replac-ing multinomials via featurized models not only im-proves model accuracy, but also lets the model apply effectively to a new regime of long sentences. 2.2 Feature Templates In the S PAN model, for each span y = [  X  1 ,..., X  n ] and label x , we use the following feature templates:
Just as the external C ONTEXT is a signal of con-stituency, so too is the internal  X  X ontext. X  For exam-ple, there are many distinct noun phrases with differ-ent spans that all begin with DT and end with NN; a fact expressed by the B OUNDARY feature (Table 1).
In the C ONTEXT model, for each context y = [  X  1 , X  2 ] and constituent/distituent decision x , we use the following feature templates:
Consider the following example extracted from the WSJ:
Both spans (0, 3) and (4, 6) are constituents corre-sponding to noun phrases whose features are shown in Table 1:
Notice that although the B ASIC span features are active for at most one span, the remaining features fire for both spans, effectively sharing information between the local probabilities of these events.
The coarser C ONTEXT features factor the context pair into its components, which allow the LLCCM to more easily learn, for example, that a constituent is unlikely to immediately follow a determiner. In the EM algorithm for estimating CCM parame-ters, the E-Step computes posteriors over bracket-ings using the Inside-Outside algorithm. The M-Step chooses parameters that maximize the expected complete log likelihood of the data.

The weights, w , of LLCCM are estimated to max-imize the data log likelihood of the training sen-tences { s n } , summing out all possible bracketings B for each sentence: We optimize this objective via L-BFGS (Liu and Nocedal, 1989), which requires us to compute the objective gradient. Berg-Kirkpatrick et al. (2010) showed that the data log likelihood gradient is equiv-alent to the gradient of the expected complete log likelihood (the objective maximized in the M-step of EM) at the point from which expectations are com-puted. This gradient can be computed in three steps. First, we compute the local probabilities of the CCM, P t ( y | x ) , from the current w using Equa-tion (1). We approximate the normalization over an exponential number of terms by only summing over spans that appeared in the training corpus.
 Second, we compute posteriors over bracketings, P( i,j | s n ) , just as in the E-step of CCM training, 2 order to determine the expected counts: where  X  ( true ) = P( i,j | s n ) , and  X  ( false ) = 1  X   X  ( true ) .

We summarize these expected count quantities as:
Finally, we compute the gradient with respect to w , expressed in terms of these expected counts and conditional probabilities:
Following (Klein and Manning, 2002), we initialize the model weights by optimizing against posterior probabilities fixed to the split-uniform distribution, which generates binary trees by randomly choosing a split point and recursing on each side of the split. 3 3.1 Efficiently Computing the Gradient The following quantity appears in G ( w ) : Which expands as follows depending on t :  X 
In each of these expressions, the  X  ( x ) term can be factored outside the sum over y . Each fixed ( i,j ) and s n pair has exactly one span and con-text, hence the quantities P y I [  X  ( i,j,s n ) = y ] and P y I [  X  ( i,j,s n ) = y ] are both equal to 1. This expression further simplifies to a constant. The sum of the posterior probabilities,  X  ( true ) , over all positions is equal to the total number of con-stituents in the tree. Any binary tree over N ter-minals contains exactly 2 N  X  1 constituents and ( N  X  2)( N  X  1) distituents.  X  ( x ) = where | s n | denotes the length of sentence s n .
Thus, G ( w ) can be precomputed once for the en-tire dataset at each minimization step. Moreover,  X  ( x ) can be precomputed once before all iterations. 3.2 Relationship to Smoothing The original CCM uses additive smoothing in its M-step to capture the fact that distituents outnumber constituents. For each span or context, CCM adds 10 counts: 2 as a constituent and 8 as a distituent. 4 We note that these smoothing parameters are tai-lored to short sentences: in a binary tree, the number of constituents grows linearly with sentence length, whereas the number of distituents grows quadrati-cally. Therefore, the ratio of constituents to dis-tituents is not constant across sentence lengths. In contrast, by virtue of the log-linear model, LLCCM assigns positive probability to all spans or contexts without explicit smoothing. LLCCM Right We train our models on gold POS sequences from all sections (0-24) of the WSJ (Marcus et al., 1993) with punctuation removed. We report bracketing F1 scores between the binary trees predicted by the models on these sequences and the treebank parses.
We train and evaluate both a CCM implementa-tion (Luque, 2011) and our LLCCM on sentences up to a fixed length n , for n  X  { 10 , 15 ,..., 40 } . Fig-ure 1 shows that LLCCM substantially outperforms the CCM on longer sentences. After length 15, CCM accuracy falls below the right branching base-line, whereas LLCCM remains significantly better than right-branching through length 40. Our log-linear variant of the CCM extends robustly to long sentences, enabling constituent grammar in-duction to be used in settings that typically include long sentences, such as machine translation reorder-ing (Chiang, 2005; DeNero and Uszkoreit, 2011; Dyer et al., 2011).
 We thank Taylor Berg-Kirkpatrick and Dan Klein for helpful discussions regarding the work on which this paper is based. This work was partially sup-ported by the National Science Foundation through a Graduate Research Fellowship to the first author.
