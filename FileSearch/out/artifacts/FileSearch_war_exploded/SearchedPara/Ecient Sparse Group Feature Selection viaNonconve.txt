 Shuo Xiang yz shuo.xiang@asu.edu Xiaotong Shen shenx002@umn.edu Jieping Ye yz jieping.ye@asu.edu During the past decade, sparse feature selection has been extensively investigated, on both optimization algorithms ( Bach et al. , 2010 ) and statistical proper-2009 ). When the data possesses certain group struc-ture, sparse modeling has been explored in ( Yuan &amp; for group feature selection. The group lasso ( Yuan &amp; Lin , 2006 ) proposes an L 2 -regularization method for each group, which ultimately yields a group-wisely sparse model. The utility of such a method has been demonstrated in detecting splice sites ( Yang et al. , 2010 )|an important step in gene nding and theoret-ically justi ed in ( Huang &amp; Zhang , 2010 ). The sparse group lasso ( Friedman et al. , 2010 ) enables to encour-age sparsity at the level of both features and groups simultaneously. In the literature, most approaches use convex methods to pursue the grouping e ect due to globality of the solution and tractable computation. However, this may lead to suboptimal results. Recent studies demonstrate that nonconvex methods ( Fan &amp; Huang et al. , 2009 ; 2012 ), particularly the truncated L -penalty ( Shen et al. , 2012 ; Mazumder et al. , 2011 ; Zhang , 2011 ), may deliver superior performance than the standard L 1 -formulation. In addition, ( Shen et al. , 2012 ) suggests that a constrained nonconvex formu-lation is slightly more preferable than its regulariza-tion counterpart due to theoretical merits. In this pa-per, we investigate the sparse group feature selection through a constrained nonconvex formulation. Ideally, we wish to optimize the following L 0 -model: where A is an n by p data matrix with its columns representing di erent features. x = ( x 1 ; ; x p ) is partitioned into j G j non-overlapping groups f x G i g and I ( ) is the indicator function. The advantage of the L -model ( 1 ) lies in its complete control on two levels of sparsity ( s 1 ; s 2 ), which are the numbers of features and groups respectively. However, problems like ( 1 ) are known to be NP-hard ( Natarajan , 1995 ) due to the discrete nature.
 This paper develops an ecient nonconvex method, which is a computational surrogate of the L 0 -method described above and has theoretically guaranteed per-formance. We contribute in two aspects: (i) com-putationally, we present an ecient optimization al-gorithm, of which the key step is a projection with two coupled constraints. (ii) statistically, the proposed method retains the merits of the L 0 approach ( 1 ) in the sense that the oracle estimator can be reconstructed, which leads to consistent feature selection and param-eter estimation.
 The rest of this paper is organized as follows. Section 2 presents our nonconvex formulation with its optimiza-tion algorithm explored in Section 3 . We analyze the theoretical properties of our formulation in Section 4 and discuss the signi cance of this work in Section 5 . Section 6 demonstrates the eciency of the proposed method as well as the performance on real-world ap-plications. Section 7 concludes the paper with a dis-cussion of future research. One major diculty of solving ( 1 ) comes from noncon-vex and discrete constraints, which require enumerat-ing all possible combinations of features and groups to achieve the optimal solution. Therefore we approxi-mate these constraints by their continuous computa-tional surrogates: minimize subject to approximating the L 0 -function ( Shen et al. , 2012 ; Zhang , 2010 ), and &gt; 0 is a tuning parameter such that J ( z ) approximates the indicator function I ( j z j X  0) as approaches zero.
 To solve the nonconvex problem ( 2 ), we develop a Dif-ference of Convex (DC) algorithm ( Tao &amp; An , 1997 ) based on a decomposition of each nonconvex constraint function into a di erence of two convex functions: where
S 1 ( x ) = are convex in x . Then each trailing convex function, say S 2 ( x ), is replaced by its ane minorant at the previous iteration
S which yields an upper approximation of the constraint function  X  Similarly, the second nonconvex constraint in ( 2 ) can be approximated by 1 Note that both ( 4 ) and ( 5 ) are convex constraints, which result in a convex subproblem as follows: minimize subject to where T 1 , T 2 and T 3 are the support sets 1 de ned as:
T 1 ( x ) = f i : j x i j g ; T 2 ( x ) = f i :  X  x G i
T 3 ( x ) = f i : x i 2 x G j ; j 2 T 2 ( x ) g ;  X  x
T 1  X  1 and  X  x T 3  X  G denote the corresponding value restricted on T 1 and T 3 respectively, and  X  x  X  G =  X  dated solution, denoted as ^ x ( m ) , which leads to a re-ned formulation of ( 6 ). Such procedure is iterated until the objective value stops decreasing. The DC al-gorithm is summarized in Algorithm 1 , from which we can see that ecient computation of ( 6 ) is critical to the overall DC routine. We defer detailed discussion of this part to Section 3 . Algorithm 1 DC programming for solving ( 2 ) Input: A , y , s 1 , s 2 Output: solution x to ( 2 ) 1: Initialize ^ x (0) . 2: for m = 1 ; 2 ; do 3: Compute ^ x ( m ) by optimizing ( 6 ). 4: Update T 1 , T 2 and T 3 . 5: if the objective stops decreasing then 6: return x = ^ x ( m ) 7: end if 8: end for As mentioned in Section 2 , ecient computation of the convex subproblem ( 6 ) is of critical importance for the proposed DC algorithm. Note that ( 6 ) has an identical form of the constrained sparse group lasso problem: except that x is restricted to the two support sets. As to be shown in Section 3.3 , an algorithm for solving ( 6 ) can be obtained through only a few modi cations on that of ( 7 ). Therefore, we rst focus on solving ( 7 ). Notice that if problem ( 7 ) has only one constraint, the solution is well-established ( Duchi et al. , 2008 ; Bach et al. , 2010 ). However, the two coupled constraints here make the optimization problem more challenging to solve. 3.1. Accelerated Gradient Method For large-scale problems, the dimensionality of data can be very high, therefore rst-order optimization is often preferred. We adapt the well-known accel-erated gradient method (AGM) ( Nesterov , 2007 ; Beck &amp; Teboulle , 2009 ), which is commonly used due to its fast convergence rate.
 To apply AGM to our formulation ( 7 ), the crucial step is to solve the following Sparse Group Lasso Projection (SGLP): which is an Euclidean projection onto a convex set and a special case of ( 7 ) when A is the identity. For conve-nience, let C 1 and C 2 denote the above two constraints in what follows.
 Since the AGM is a standard framework whose e-ciency mainly depends on that of the projection step, we leave the detailed description of AGM in the sup-plement and introduce the ecient algorithm for this projection step ( 8 ). 3.2. Ecient Projection We begin with some special cases of ( 8 ). If only C 1 exists, ( 8 ) becomes the well-known L 1 -ball projec-tion ( Duchi et al. , 2008 ), whose optimal solution is denoted as P s 1 1 ( v ), standing for the projection of v onto the L 1 -ball with radius s 1 . On the other hand, if only C 2 is involved, it becomes the group lasso projec-tion, denoted as P s 2 G . Moreover, we say a constraint is active , if and only if an equality holds at the optimal solution x ; otherwise, it is inactive .
 Preliminary results are summarized in Lemma 1 : Lemma 1. Denote a global minimizer of ( 8 ) as x . Then the following results hold: 1. If both C 1 and C 2 are inactive, then x = v . 2. If C 1 is the only active constraint, i.e.,  X  x  X  1 = 3. If C 2 is the only active constraint, i.e.,  X  x  X  1 &lt; 3.2.1. Computing x from the optimal dual Lemma 1 describes a global minimizer when either constraint is inactive. Next we consider the case in which both C 1 and C 2 are active. By the convex dual-ity theory ( Boyd &amp; Vandenberghe , 2004 ), there exist unique non-negative dual variables and such that x is also the global minimizer of the following regu-larized problem: whose solution is given by the following Theorem. Theorem 1 (( Friedman et al. , 2010 )) . The optimal solution x of ( 9 ) is given by x where v G 2002 ) v G i with threshold as follows: where SGN ( ) is the sign function and all the opera-tions are taken element-wisely.
 Theorem 1 gives an analytical solution of x in an ideal situation when the values of and are given. Un-fortunately, this is not the case and the values of and need to be computed directly from ( 8 ). Based on Theorem 1 , we have the following conclusion char-acterizing the relations between the dual variables: Corollary 1. The following equations hold:  X  x  X  1 = Suppose is given, then computing from ( 12 ) amounts to solving a median nding problem, which can be done in linear time ( Duchi et al. , 2008 ). Finally, we treat the case of unknown (thus un-known ). We propose an ecient bisection approach to compute it. 3.2.2. Computing : bisection Given an initial guess (estimator) of , says ^ , one may perform bisection to locate the optimal , pro-vided that there exists an oracle procedure indicating if the optimal value is greater than ^ 2 . This bisection method can estimate in logarithm time. Next, we shall design an oracle procedure.
 Let the triples be the optimal solution of ( 8 ) with both constraints active, i.e.,  X  x  X  1 = s 1 ,  X  x  X  G = s 2 , with ( ; ) be the optimal dual variables. Consider the following two sparse group lasso projections: The following key result holds.
 Theorem 2. If  X  and s 2 = s  X  2 , then s 1 s  X  1 . Theorem 2 gives the oracle procedure with its proof presented in the supplement. For a given estimator ^ , we compute its corresponding ^ from ( 12 ) and then ^ s 1 from ( 11 ), satisfying ( ^ x ; ^ ; ^ ) = SGLP( v ; ^ s 1 ^ s 1 is compared with s 1 . Clearly, by Theorem 2 , if ^ s 1 s 1 , the estimator ^ s &gt; s 1 means ^ &lt; . In addition, from ( 11 ) we know that ^ s 1 is a continuous function of ^ . Together with the monotonicity given in Theorem 2 , a bisection approach can be employed to calculate . Algorithm 2 gives a detailed description of this procedure.
 Algorithm 2 Sparse Group Lasso Projection Algo-rithm Input: v , s 1 , s 2 Output: an optimal solution x to the Sparse Group Projection Problem Function SGLP( v , s 1 , s 2 ) 1: if  X  x  X  1 s 1 and  X  x  X  G s 2 then 2: return v 3: end if 6: x C 12 = bisec( v , s 1 , s 2 ) 8: return x C 1 9: else if  X  x C 2  X  1 s 1 then 10: return x C 2 11: else 13: end if Function bisec( v , s 1 , s 2 ) 1: Initialize up , low and tol 2: while up low &gt; tol do 3: ^ = ( low + up ) = 2 4: if ( 12 ) has a solution ^ given v ^ then 5: calculate b s 1 using b and ^ . 6: if ^ s 1 s 1 then 7: up = ^ 8: else 9: low = ^ 10: end if 11: else 12: up = ^ 13: end if 14: end while 15: = up 16: Solve ( 12 ) to get 17: Calculate x from and via ( 10 ) 18: return x 3.3. Solving Restricted version of ( 7 ) Finally, we modify the above procedures to compute the optimal solution of the restricted problem ( 6 ). To apply the accelerated gradient method, we consider the following projection step: Our rst observation is: T 3 ( x ) T 1 ( x ), since if an element of x lies in a group whose L 2 -norm is less than , then the absolute value of this element must also be less than . Secondly, from the decomposable nature of the objective function, we conclude that: since there are no constraints on x j if it is outside T 1 and involves only the L 1 -norm constraint if j 2 T 1 n T Following routine calculations as in ( Duchi et al. , 2008 ), we obtain the following results similar to ( 11 ) and ( 12 ): s = Based on ( 14 ) and ( 15 ), we design a similar bisection approach to compute and thus ( x ) T 3 , as in Algo-rithm 2 . Details can be found in the supplement. Since the projection ( 13 ) does not possess an closed-form, it is instructive to discuss the convergence prop-erty of overall accelerated gradient method. Follow the discussion in ( Schmidt et al. , 2011 ), we can pro-vide sucient conditions for a guaranteed convergence rate. Moreover, we found in practice that a reasonable convergence property can be obtained as long as the precision level for the computation of the projection is small, as revealed in Section 6 .
 Remark Problem ( 7 ) can also be solved using the Alternating Direction Method of Multiplier (ADMM) ( Boyd et al. , 2011 ) instead of the acceler-ated gradient method (AGM). However, our evalua-tions show that AGM with our projection algorithm is more ecient than ADMM. This section investigates theoretical aspects of the pro-posed method. More speci cally, we demonstrate that the oracle estimator ^ x o , the least squares estimator based on the true model, can be reconstructed. As a result, consistent selection as well as optimal parame-ter estimation can be achieved.
 For better presentation, we introduce some notations that would be only utilized in this section. Let C = ( G i 1 ; ; G i k ) be the collection of groups that contain nonzero elements. Let A G j = A G j ( x ) and A = A ( x ) denote the indices of nonzero elements of x in group G j and in entire x respectively. De ne S where S is the feasible region of ( 2 ) and C 0 represents the true nonzero groups.
 The following assumptions are used to obtain consis-tent reconstruction of the oracle estimator: Assumption 1 (Separation condition) . De ne then for some constant c 1 &gt; 0 , where h ( x ; x 0 ) = is the Hellinger-distance for densities with respect to a dominating measure .
 Assumption 2 (Complexity of the parameter space) . For some constants c 0 &gt; 0 and any 0 &lt; t &lt; " 1 , H ( t; F j;i ) c 0 max((log( j G j + s 0 1 )) 2 ; 1) jB j;i where B j;i = S j;i \ f x 2 h ( x ; x 0 ) 2 " g is a local parameter space and F j;i = f g 1 = 2 ( x ; y ) : x 2 B j;i is a collection of square-root densities. H ( ; F ) is the bracketing Hellinger metric entropy of space F ( Kol-mogorov &amp; Tihomirov , 1961 ).
 Assumption 3. For some positive constants d 1 ; d 2 ; d 3 with d 1 &gt; 10 , log(1 h 2 ( x ; x 0 )) d 1 log(1 h 2 ( x ; x 0 )) d 3 d where x = ( x 1 I ( j x 1 j ) ; ; x p I ( j x p j )) . With these assumptions hold, we can conclude the fol-lowing non-asymptotic probability error bound regard-ing the reconstruction of the oracle estimator ^ x o . The proof is provided in the supplement.
 Theorem 3. Suppose that Assumptions 2 and 3 hold. For a global minimizer of ( 2 ) ^ x with ( s 1 ; s 2 ) = ( s and hold: P ( Moreover, with Assumption 1 hold, P and as n !1 , j G j!1 .
 Theorem 3 states that the oracle estimator ^ x o can be accurately reconstructed, which in turn yields fea-ture selection consistency as well as the recovery of the performance of the oracle estimator in parame-ter estimation. Moreover, as indicated in Assump-tion 1 , the result holds when s 0 1 j G j grows in the or-der of exp( c 1 1 nC min ) . This is in contrast to exist-ing results on consistent feature selection, where the number of candidate features should be no greater than exp( c n ) for some c ( Zhao &amp; Yu , 2006 ; Wang et al. , 2007 ). In this sense, the number of candidate features is allowed to be much larger when an ad-ditional group structure is incorporated, particularly when each group contains considerable redundant fea-tures. It is not clear whether such a result also holds for other bi-level 3 variable selection methods, such as the composite MCP ( Huang et al. , 2009 ) and group bridge ( Breheny &amp; Huang , 2009 ).
 To our knowledge, our theory for the grouped selec-tion is the rst of this kind. However, it has a root in feature selection. The large deviation approach used here is applicable to derive bounds for feature selec-tion consistency. In such a situation, the result agrees with the necessary condition for feature selection con-sistency for any method, except for the constants in-dependent of the sample size ( Shen et al. , 2012 ). In other words, the required conditions are weaker than those for L 1 -regularization commonly used in the lit-erature ( Van De Geer &amp; B X uhlmann , 2009 ). The use of the Hellinger-distance is to avoid specifying a sub-Gaussian tail of the random error. This means that the result continues to hold even when the error does not have a sub-Gaussian tail. Although we require ^ x to be a global minimizer of ( 2 ), a weaker version of the theory can be derived for a local minimizer obtained from the DC programming by following similar deriva-tions in ( Shen et al. , 2012 ). We leave such discussions in a longer version of the paper.
This section is devoted to a brief discussion of ad-vantages of our work statistically and computation-ally. Moreover, it explains why the proposed method is useful to perform ecient and interpretable feature selection given a natural group structure.
 Interpretability. The parameters in formulation ( 2 ) are highly interpretable in that s 1 and s 2 are upper bounds of the number of nonzero elements as well as that of groups. This is advantageous, especially in the presence of certain prior knowledge regarding the number of features and/or that of groups. However, such an interpretation vanishes with other (convex &amp; nonconvex) methods such as lasso, sparse group lasso, composite MCP or group bridge, in which incorporat-ing such prior knowledge often requires repeated trials of di erent parameters.
 Parameter tuning. Typically, tuning parameters for good generalization usually requires considerable amount work due to a large number of choices of pa-rameters. However, parameter tuning in model ( 1 ) may search through integer values in a bounded range, and can be further simpli ed when certain prior knowl-edge is available. This permits more ecient tuning than its regularization counterpart. Based on our lim-ited experience, we note that does not need to be tuned precisely as we may x at some small values. Performance and Computation. Although our model ( 2 ) is proposed as a computational surrogate of the ideal L 0 -method, its performance can also be theoretically guaranteed, i.e., consistent feature selec-tion can be achieved. Moreover, the computation of our model is much more ecient and applicable to large-scale applications. 6.1. Evaluation of Projection Algorithms Since DC programming and the accelerated gradient methods are both standard, the eciency of the pro-posed nonconvex formulation ( 2 ) depends on the pro-jection step in ( 8 ). Therefore, we focus on evaluat-ing the projection algorithms and comparing with two popular projection algorithms: Alternating Direction Method of Multiplier (ADMM) ( Boyd et al. , 2011 ) and Dykstra's projection algorithm ( Combettes &amp; Pesquet , 2010 ). We give a detailed derivation of adapting these two algorithms to our formulation in the supplement. To evaluate the eciency, we rst generate the vector v whose entries are uniformly distributed in [ 50 ; 50] and the dimension of v , denoted as p , is chosen from vector into 10 groups of equal size. Finally, s 2 is set to by For a fair comparison, we run our projection algorithm until converge and record the minimal objective value as f . Then we run ADMM and Dykstra's algorithm until their objective values become close to ours. More speci cally, we terminate their iterations as soon as f f ADMM and f Dykstra stand for the objective value of ADMM and Dykstra's algorithm respectively. Table 1 summarizes the average running time of all three al-gorithms over 100 replications.
 Next we demonstrate the accuracy of our projection algorithm. Toward this end, the general convex opti-mization toolbox CVX ( Grant &amp; Boyd , 2011 ) is chosen as the baseline. Following the same strategy of gener-ating data, we report the distance (computed from the Euclidean norm  X  X  X  2 ) between optimal solution of the three projection algorithms and that of the CVX as well as the running time. Note that the projection is strictly convex with a unique global optimal solution. For ADMM and Dykstra's algorithm, the termination criterion is that the relative di erence of the objec-tive values between consecutive iterations is less than a threshold value. Speci cally, we terminate the iter-projection algorithm, we set the tol in Algorithm 2 to be 10 7 . The results are summarized in Table 2 and Figure 1 . Powered by second-order optimization algo-rithms, CVX can provide fast and accurate solutions for medium-size problems but would su er from great computational burden for large-scale ones. Therefore we only report the results up to 5 ; 000 dimensions. From the above results we can observe that for projec-tions of a moderate size, all three algorithms perform well. However, for large-scale ones, the advantage of the proposed algorithm is evident as our method pro-vides more accurate solution with less time.
 6.2. Performance on Synthetic Data We generate a 60 100 matrix A , whose entries follow i.i.d standard normal distribution. The 100 features (columns) are partitioned into 10 groups of equal size. The ground truth vector x 0 possesses nonzero elements only in 4 of the 10 groups. In addition, only 4 elements in each nonzero group are nonzero. Finally y is gen-erated according to Ax 0 + z with z following distri-bution N (0 ; 0 : 5 2 ). The data are divided into training and testing set of equal size.
 We t our method to the training set and compare with both convex methods (lasso, group lasso and sparse group lasso) and methods based on nonconvex bi-level penalties (group bridge and composite MCP). Since the data are intentionally generated to be sparse in both group-level and feature-level, approaches that only perform group selection, such as group lasso, group SCAD and ordinary group MCP, are not in-cluded due to their suboptimal results.
 The tuning parameters of the convex methods are se-the number of nonzero groups is selected from the set f 2 ; 4 ; 6 ; 8 g and the number of features is chosen from f parameter tuning. Group bridge and composite MCP are carried out using their original R-package grpreg and the tuning parameters are set to the default values (100 parameters with 10-fold cross-validation). we list the number of selected groups and features by each method. In addition, the number of false posi-tive or false negative groups/features are also reported in Table 3 . We can observe that our model correctly identi es the underlying groups and features. More-over, our method e ectively excludes redundant fea-tures and groups compared to other methods, which is illustrated by our low false positive numbers and relatively high false negative numbers. Such a phe-nomenon also appears in the evaluations in ( Breheny &amp; Huang , 2009 ).
 6.3. Performance on Real-world Application Our method is further evaluated on the application of examining Electroencephalography (EEG) correlates of genetic predisposition to alcoholism ( Frank &amp; Asun-cion , 2010 ). EEG records the brain's spontaneous elec-trical activity by measuring the voltage uctuations over multiple electrodes placed on the scalp. This technology has been widely used in clinical diagnosis, such as coma, brain death and genetic predisposition to alcoholism. In fact, encoded in the EEG data is a certain group structure, since each electrode records the electrical activity of a certain region of the scalp. Identifying and utilizing such spatial information has the potential of increasing stability of a prediction. The training set contains 200 samples of 16384 di-mensions, sampled from 64 electrodes placed on sub-ject's scalps at 256 Hz (3.9-msec epoch) for 1 second. Therefore, the data can naturally be divided into 64 groups of size 256. We apply the lasso, group lasso, sparse group lasso, group SCAD, group MCP, group bridge, composite MCP and our proposed method on the training set and adapt the 5-fold cross-validation for selecting tuning parameters. More speci cally, for lasso and group lasso, the candidate tuning parameters are speci ed by 10 parameters 4 sampled using the log-arithmic scale from the parameter spaces, while for the sparse group lasso, the parameters form a 10 10 grid 5 , sampled from the parameter space in logarithmic scale. For our method, the number of groups is selected from the set: s 2 = f 30 ; 40 ; 50 g and s 1 , the number of fea-tures is chosen from the set f 50 s 2 ; 100 s 2 ; 150 s 2 fault settings in the R package grpreg (100 param-eters, 10-fold cross validation) are applied to other nonconvex methods. The accuracy of classi cation to-gether with the number of selected features and groups over a test set, which also contains 200 samples, are reported in Table 4 . Clearly our method achieves the best performance of classi cation. Note that, although lasso's performance is almost as good as ours with even less features, however, it fails to identify the underly-ing group structure in the data, as revealed by the fact all 64 groups are selected. Moreover, other nonconvex approaches such as the group SCAD, group MCP and group bridge seem to over-penalized the group penalty, which results in very few selected groups and subopti-mal performance.
 This paper expands a nonconvex paradigm into sparse group feature selection. In particular, an ecient op-timization scheme is developed based on the DC pro-gramming, accelerated gradient method and ecient projection. In addition, theoretical properties on the accuracy of selection and parameter estimation are an-alyzed. The eciency and ecacy of the proposed method are validated on both synthetic data and real-world applications. The proposed method will be fur-ther investigated on real-world applications involving the group structure. Moreover, extending our ap-proach to multi-modal multi-task learning ( Zhang &amp; Shen , 2011 ) is another promising direction.
