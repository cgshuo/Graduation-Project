 Weighted finite-state transducers (WFSTs), or lat-tices , are used in speech and language process-ing to compactly represent and manipulate a large number of strings. Applying a finite-state opera-tion (eg. PoS tagging) to a lattice via composition produces a WFST that maps input (eg. words) onto output strings (eg. PoS tags) and preserves the arc-level alignment between each input and output symbol (eg. each arc is labeled with a word-tag pair and has a weight). Typically, the result of such operation is a WFST that is am-biguous because it contains multiple paths with the same input string, and non-functional because it contains multiple output strings for a given input string (Mohri, 2009).

Disambiguating such WFSTs is the task of cre-ating a WFST that encodes only the best-scoring path of each input string, while still maintaining the arc-level mapping between input and output one algorithm has been described (Shafran et al., 2011); the main steps are: (a) Map the WFST into an equivalent weighted (b) Apply WFSA determinization under this (c) Expand the result back to an WFST that pre-
We present a new disambiguation algorithm that can efficiently accomplish this. In Section 2 we describe how the tropical sparse tuple vec-tor semiring can keep track of individual arcs in the original WFST as topological features during the mapping step ( a ). This allows us to describe in Section 3 an efficient expansion algorithm for step ( c ). We show in Section 4 empirical evidence that our algorithm is more efficient than Shafran et al. (2011) in their same PoS-tagging task. We also show how our method can be applied in rescor-ing translation lattices under a bilingual neural-network model (Devlin et al., 2014), obtaining BLEU score gains consistent with the literature. Section 5 reviews related work and concludes. A WFST T = ( X  ,  X  ,Q,I,F,E, X  ) over a semir-ing ( K ,  X  ,  X  , 0 , 1) has input and output alphabets  X  and  X  , a set of states Q , the initial state I  X  Q , a set of final states F  X  Q , a set of transitions (edges) E  X  ( Q  X   X   X   X   X  K  X  Q ) , and a final state function  X  : F  X  K . We focus on extensions to the tropical semiring ( R  X  X  , min , + ,  X  , 0) . in E . A path  X  = e 1 ...e n is a sequence of edges such that n [ e j ] = p [ e j +1 ] , 1  X  j &lt; n . w [  X  ] = N is accepting if p [  X  ] = I and n [  X  ]  X  F . The weight associated by T to a set of paths  X  is T ( X ) = L 2.1 Tropical Sparse Vector Semiring Let  X  f [ e i ] = f i  X  R N be the unweighted feature vector associated with edge e i , and let  X   X   X  R N be a global feature weight vector. The tropical weight is then found as w i = w [ e i ] =  X   X   X   X  f i =
Given a fixed  X   X  , we define the operators for the tropical vector semiring as:  X  f i  X   X   X  f j = min(  X   X   X   X  f ,  X   X   X   X  f j ) and  X  f i  X   X   X  f j = tropical weights are maintained correctly by the vector semiring as  X  f i  X   X   X  f j = w i  X  w j and  X  f  X  f j = w i  X  w j . Finally, we define the element-wise times operator as:  X  f i  X   X  f j =  X  f m , where f m,k f i,k + f j,k ,  X  k . It follows that w i  X  w j =  X   X  (
When dealing with high-dimensional feature vectors which have few non-zero elements, it is convenient in practice (for computational effi-ciency) to use a sparse representation for vectors:  X  f = [( i,f i ) ,i : f i 6 = 0] . That is,  X  f is comprised of a sparse set of tuples ( i,f i ) , where i is a feature index and f i is its value; e.g. [(2 ,f 2 )] is short for [0 ,f 2 , 0 , 0] if N = 4 .

The semiring that operates on sparse feature vectors, which we call tropical sparse tuple vec-tor semiring 2 , uses conceptually identical opera-tors as the non-sparse version defined above, so it also maintains the tropical weights w correctly. We now describe how we use the semiring de-scribed in Section 2 for steps ( a ) and ( b ), and de-scribe an expansion algorithm for step ( c ) that effi-ciently converts the output of determinization into an unambiguous WFST with arc-level alignments. WFSA with Sparse Topological Features Let T be a tropical-weight WFST with K edges. T is topologically sorted so that if an edge e k pre-ceeds an edge e k 0 on a path, then k &lt; k 0 . We now use tropical sparse tuple vector weights to create a WFSA A that maintains (in its weights) pointers to specific edges in T . These  X  X ointers X  are sparse topological features.
For each edge e k = ( p k ,i k ,o k ,w k ,n k ) of T , where  X  f k = [ w k , 0 ,..., 0 , 1 , 0 ,..., 0] ; the 1 is in the k th position. In other words, f k, 0 is the tropical weight of the k th edge in T and f k,k = 1 indicates that this tropical weight belongs to edge k in T . In sparse notation,  X  f k = [(0 ,w k ) , ( k, 1)] . For example, this non-deterministic transducer T : is mapped to acceptor A with topological features: Given  X  = [1 , 0 ,..., 0] , operations on A yield the same path weights as in the usual tropical semir-ing.
 WFSA determinization We now apply the standard determinization algo-rithm to A , which yields A D :
This now accepts only one best-scoring path for each input string, and the weights  X  X oint X  to the sequence of arcs traversed by the relevant path in T . In turn, this reveals the best output string for the given input string. For example, the path-level features associated with  X  ab  X  are [(0 , 3) , (1 , 1) , (3 , 1)] , indicating a path  X  = e 1 with tropical weight 3 through T (and hence out-put string  X  AB  X ).

The topology of A D is compact because multi-ple input strings may share arcs while still encod-ing different output strings in their weights. This is achieved by  X  X ancelling X  topological features in subsequent arcs and  X  X eplacing X  them by new ones as one traverses the path. For example, the string  X  ac  X  initially has feature (1 , 1) , but this gets can-celled later in the path by (1 ,  X  1) , and replaced by [(2 , 1) , (5 , 1)] , indicating a path  X  = e 2 e 5 through T with output string  X  ZD  X . Figure 1: Expansion and topological feature repo-sitioning algorithm for step ( c ).
 Expansion Algorithm We now describe an expansion algorithm to con-vert A D into an unambiguous WFST T 0 that main-tains the arc-level input-output alignments of the original transducer T . In our example, T 0 should be identical to T except for edge 4 , which is re-moved.

Due to the WFSA determinization algorithm, we observe empirically that the cancelling features in A D tend to appear in a path after the feature it-self. This allows us to define an algorithm that tra-verses A D in reverse (from its final states to its ini-tial state) and creates an equivalent acceptor with the topology of T 0 .

The algorithm is described in Figure 1. It per-forms a forward pass through A r (the reverse of ate a new arc where we  X  X op X  the highest topo-logical feature (as it will not be cancelled later) and its tropical weight. The new states encode the original state q and the residual features that have not been  X  X opped X  yet. For each edge E ( q ) , the auxiliary P OP TF EA (  X  f,e ) returns a ( w 0 ,t 0 ,  X  ple, where w 0 is the tropical weight obtained as  X  f  X   X   X  f [ e ] (which is equivalent to f 0 + f [ e ] 0 given our  X   X  ); t 0 is the index of the highest topological ual topological features after excluding f 0 and f t 0 , turns (5 , 6 , [(1 ,  X  1) , (2 , 1)]) ; if w has only one topological feature, the residual is 0 . The residual in all final states of B r will be 0 (no topological features still to be popped).

Graphically, in our running example A r is: and the output is B r :
Reversing B r yields an acceptor B (still in the sparse tuple vector semiring) which has the same topology as our goal T 0 and can be trivially mapped to T 0 in linear time: each arc takes the tropical weight via  X   X  and has only one topological feature which points to the arc in T containing the required output symbol.
 Two-pass Expansion As mentioned earlier, our algorithm relies on  X  X an-celling X  topological features appearing after the feature they cancel in a given path. In general, consider T a weighted transducer and A it X  X  equiv-alent automaton with sparse topological features, as described here. A p is the result of applying standard WFST operations, such as determiniza-tion, minimization or shortest path. Assume as a final step that the weights have been pushed to-wards the final states. It is worth noting the prop-erty that: two topological features in a path ac-cepted by A will never get reordered in A p , al-though they can appear together on the same edge, contains only one single path, all the topological features would appear on the final state.

Let us define a function d A ( e ) as the minimum number of edges on any path in A from the start state to n [ e ] through edge e .

Consider all edges e i in A and e p in A p , with f [ e i ] i = 1 and f [ e p ] i 6 = 0 , i.e. we are interested in the topological feature contribution on e p due to the edge e i in A . If d A ( e i )  X  d A p ( e p ) is al-ways satisfied, then E XPAND TF EA will yield the correct answer because the residual at each state, together with the the weight of the current edge, contains all the necessary information to pop the next correct topological feature.

However, many deterministic WFSAs will not exhibit this behaviour (eg. minimised WFSAs), even after pushing the weights towards the final states. For example see this acceptor A: HiFST lattices (right).
And A p is a minimised version of A :
As d A ( e 3 ) = d A ( e 4 ) = 2 and d A p ( e p the distance test fails for both topo-logical features (3 ,  X  1)(4 , 1) . Running feature (3 , 1) along the path  X  z x  X  and will pop (4 , 1) instead, storing the remaining none-0 residual in a final state of B r .

As mentioned before, two topological features along the same path in A will not reorder in A p . In this example, as (4 , 1) appears in edge e p ture (2 , 1) must also appear in this edge (or on an earlier edge, in a more complicated machine). In general, any remaining topological features along the path back to the start state of A p will all be popped after their correct edges in B r . All edges in B r pass the distance test compared to A r , the re-versed form of A : for all edges e i with f [ e i ] i = 1 in A r and e q in B r such that f [ e q ] i 6 = 0 , d A r d
B r ( e q ) . Edges in these machines are now reverse sorted, i.e. if an edge e k precedes an edge e k 0 on a path, then k 0 &lt; k .

We can perform a second pass with the same al-gorithm over B , with the only minor modification that t 0 is now the index of the lowest topological feature of  X  f  X   X  f [ e ] . This expands the acceptor cor-rectly. Because correct expansions yield 0 residu-als on the final states, the algorithm can be trivially modified to trigger the second pass automatically if the residual on any final state is not 0 . We evaluate our algorithm, henceforth called topo-logical , in two ways: we empirically contrast disambiguation times against previous work, and then apply it to rescore translation lattices with bilingual neural network models. 4.1 PoS Transducer Disambiguation We apply our algorithm to the 4,664 NIST English CTS RT Dev04 set PoS tagged lattices used by Sproat el al. (2014); these were generated with a speech recognizer similar to (Soltau et al., 2005) and tagged with a WFST-based HMM tagger. The average number of states is 493. We contrast with the lexicographic tropical categorial semiring im-plementation of Shafran et al. (2011), henceforth referred to as the categorial method.

Figure 2 (left) shows the number of disam-biguated WFSTs as processing time increases. The topological algorithm proves much faster (and we observe no memory footprint differences). In 50ms it disambiguates 3540 transducers, as op-posed to the 2771 completed by the categorial pro-cedure; the slowest WFST to disambiguate takes 230 seconds in the categorial procedure and 60 seconds in our method. Using sparse topological features with our semiring disambiguates all WF-STs faster in 99.8% of cases. 4.2 Neural Network Bilingual Rescoring We use the disambiguation algorithm to apply the bilingual neural network language model (BiLM) of Devlin et al. (2014) to the output lattices of the CUED OpenMT12 Arabic-English hierarchical Gispert et al., 2010). We use a development set mt0205tune (2075 sentences) and a validation set mt0205test (2040 sentences) from the NIST MT02 through MT05 evaluation sets.

The edges in these WFSTs are of the form t : i/w , where t is the target word, i is the source sentence position t aligns to, and w contains the translation and language model score. HiFST outputs these WFSTs by using a standard hi-ero grammar (Chiang, 2007) augmented with tar-get side heuristic alignments or affiliations to the source (Devlin et al., 2014).
 In a rule over source and target words the feature  X  2 , 1  X  indicates that the target word t 1 is aligned to source word s 2 and that t 2 aligns to s . As rules are applied in translation, this infor-mation can be used to link target words to absolute positions within the source sentence.

Allowing for admissible pruning, all possible affiliation sequences under the grammar for ev-ery translation are available in the WFSTs; dis-ambiguation keeps the best affiliation sequence for each translation hypothesis, which allows the rescoring of very large lattices with the BiLM model.

This disambiguation task involves much bigger lattices than the POS-tagging task: the average number of states of the HiFST lattices is 38,200. Figure 2 (right) shows the number of mt0205tune disambiguated WFSTs over time compared to the categorial method. As with the PoS disambigua-tion task, the topological method is always much faster than the categorial one. After 10 seconds, our method has disambiguated 1953 lattices out of 2075, whereas the categorial method has only fin-ished 1405. The slowest WFST to disambiguate takes 6700 seconds with the categorial procedure, which compares to 1000 seconds in our case.
 The BiLM model is trained with NPLM (Vaswani et al., 2013) with a context
Table 1: Translation scores in lower-case BLEU. of 3 source and 4 target words. Lattice rescoring with this model requires a special variation of the standard WFST composition which looks at both input and output labels on a transducer arc; we use KenLM (Heafield, 2011) to retrieve neural net-work scores for on-the-fly composition. We retune the parameters with Lattice MERT (Macherey et al., 2008) . Results are shown in Table 1. Acknowledging the task differences with respect to (Devlin et al., 2014), we find BLEU gains consistent with rescoring results reported in their Table 5. We have described a tagging disambiguation algo-rithm that supports non-functional WFSTs, which cannot be handled directly by neither WFST deter-minization (Mohri, 1997) nor WFST disambigua-tion (Mohri, 2012). We show it is faster than the implementation with a lexicographic-tropical-categorial semiring described by Shafran et al. (2011) and describe a use case in a practical rescoring task of an MT system with bilingual neural networks that yield 1.0 BLEU gain.

Povey et al. (2012) also use a special semir-ing that allows to map non-functional WFSTs into WFSAs by inserting the tag into a string weight. However, in contrast to our implementation and that of Shafran et al (2011), no expansion into an WFST with aligned input/output is described.

Lexicographic semirings, used for PoS tagging disambiguation (Shafran et al., 2011), have been also shown to be useful in other tasks (Sproat et al., 2014), such as optimized epsilon encoding for backoff language models (Roark et al., 2011), and hierarchical phrase-based decoding with Push-down Automata (Allauzen et al., 2014).

The tools for disambiguation and WFST com-position with bilingual models, along with a tu-torial to replicate Section 4.2, are all available at http://ucam-smt.github.io .
 We thank the authors of (Sproat et al., 2014) for generously sharing their PoS-tagging experiments.
