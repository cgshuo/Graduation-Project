 University of Maryland, College Park University of Maryland, College Park
In this article, we attempt to conduct a comprehensive and application-independent survey of appreciation for the importance and potential use of paraphrases in the field of NLP research. Recent work done in manual and automatic construction of paraphrase corpora is also examined. explore some future trends in paraphrase generation. 1. Introduction
Although everyone may be familiar with the notion of paraphrase in its most funda-mental sense, there is still room for elaboration on how paraphrases may be automat-ically generated or elicited for use in language processing applications. In this survey, we make an attempt at such an elaboration. An important outcome of this survey is of deployment. We also find that although many paraphrase methods are developed with a particular application in mind, all methods share the potential for more general applicability. Finally, we observe that the choice of the most appropriate method for an application depends on proper matching of the characteristics of the produced paraphrases with an appropriate method.
 promise but has not yet been tested for a long enough period (and in enough systems).
However, we believe this argument actually strengthens the motivation for a survey that can encourage the community to use paraphrases by providing an application-independent, cohesive, and condensed discussion of data-driven paraphrase generation techniques. We should also acknowledge related work that has been done on furthering the community X  X  understanding of paraphrases. Hirst (2003) presents a comprehensive survey of paraphrasing focused on a deep analysis of the nature of a paraphrase. The current survey focuses instead on delineating the salient characteristics of the various paraphrase generation methods with an emphasis on describing how they could be used in several different NLP applications. Both these treatments provide different but valuable perspectives on paraphrasing.
 the coverage of this survey X  X  discussion, and provides broader context and motivation by discussing applications in which paraphrase generation has proven useful, along with examples. Section 2 briefly describes the tasks of paraphrase recognition and textual entailment and their relationship to paraphrase generation and extraction. Sec-tion 3 forms the major contribution of this survey by examining various corpora-based techniques for paraphrase generation, organized by corpus type. Section 4 examines recent work done to construct various types of paraphrase corpora and to elicit human judgments for such corpora. Section 5 considers the task of evaluating the performance of paraphrase generation and extraction techniques. Finally, Section 6 provides a brief glimpse of the future trends in paraphrase generation and Section 7 concludes the survey with a summary. 1.1 What is a Paraphrase?
The concept of paraphrasing is most generally defined on the basis of the principle of semantic equivalence: A paraphrase is an alternative surface form in the same language expressing the same semantic content as the original form. Paraphrases may occur at several levels.
 paraphrases or, more commonly, synonyms , for example, hot , warm and eat , consume . However, lexical paraphrasing cannot be restricted strictly to the concept of synonymy.
There are several other forms such as hyperonymy , where one of the words in the example, reply , say and landlady , hostess .
 content. Although these fragments most commonly take the form of syntactic phrases ( work on , soften up and take over , assume control of ) they may also be patterns with linked variables, for example, Y was built by X , X is the creator of Y .
 paraphrases , for example, I finished my work , I completed my assignment .Althoughitis possible to generate very simple sentential paraphrases by simply substituting words significantly more difficult to generate more interesting ones such as He needed to make
Culicover (1968) describes some common forms of sentential paraphrases. 1.2 Scope of Discussion
The idea of paraphrasing has been explored in conjunction with, and employed in, a large number of natural language processing applications. Given the difficulty inherent 342 certain limits on the scope of our discussion. In this survey, we will be restricting our discussion to only automatic acquisition of phrasal paraphrases (including paraphrastic patterns) and on generation of sentential paraphrases. More specifically, this entails the exclusion of certain categories of paraphrasing work. However, as a compromise for the interested reader, we do include a relatively comprehensive list of references in this section for the work that is excluded from the survey.
 knowledge-based resources such as dictionaries (Wallis 1993; Fujita et al. 2004), hand-written rules (Fujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999;
Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from dis-cussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and
Dagan 2003; Shimohata and Sumita 2005). 1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for specific applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and
Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002). 1.3 Applications of Paraphrase Generation
Before describing the techniques used for paraphrasing, it is essential to examine the broader context of the application of paraphrases. For some of the applications we discuss subsequently, the use of paraphrases in the manner described may not yet be the norm. However, wherever applicable, we cite recent research that promises gains in performance by using paraphrases for these applications. Also note that we only discuss those paraphrasing techniques that can generate the types of paraphrases under examination in this survey: phrasal and sentential. 1.3.1 Query and Pattern Expansion. One of the most common applications of paraphrasing is the automatic generation of query variants for submission to information retrieval systems or of patterns for submission to information extraction systems. Culicover (1968) describes one of the earliest theoretical frameworks for query keyword expansion
Jones and Tait 1984) generates several simple variants for compound nouns in queries submitted to a technical information retrieval system. For example:
In fact, in recent years, the information retrieval community has extensively explored the task of query expansion by applying paraphrasing techniques to generate similar or related queries (Beeferman and Berger 2000; Jones et al. 2006; Sahami and Hellman 2006;
Metzler, Dumais, and Meek 2007; Shi and Yang 2007). The generation of paraphrases in these techniques is usually effected by utilizing the query log (a log containing the record of all queries submitted to the system) to determine semantic similarity. Jacquemin (1999) generates morphological, syntactic, and semantic variants for phrases in the agricultural domain. For example:
Ravichandran and Hovy (2002) use semi-supervised learning to induce several para-phrastic patterns for each question type and use them in an open-domain question answering system. For example, for the INVENTOR question type, they generate: (via a pivot-based sentential paraphrasing model employing bilingual parallel corpora, detailed in Section 3) and then using any new words introduced therein as additional query terms. For example, for the query how to live with cat allergies , they may generate the following two paraphrases. The novel words in the two paraphrases are highlighted in bold and are used to expand the original query: (Romano et al. 2006). Most recently, Bhagat and Ravichandran (2008) collect paraphras-tic patterns for relation extraction by applying semi-supervised paraphrase induction to a very large monolingual corpus. For example, for the relation of  X  X cquisition, X  they collect: 1.3.2 Expanding Sparse Human Reference Data for Evaluation. A large percentage of NLP applications are evaluated by having human annotators or subjects carry out the same 344 task for a given set of data and using the output so created as a reference against which to measure the performance of the system. The two applications where comparison against human-authored reference output has become the norm are machine translation and document summarization.
 chine translation system are evaluated against reference translations created by human translators by measuring the n -gram overlap between the two (Papineni et al. 2002). verbalizations that can convey the same semantic content. This may unfairly penalize translation hypotheses that have the same meaning but use n -grams that are not present against the reference R even though it conveys precisely the same semantic content: S : We must consider the entire community.
 R : We must bear in mind the community as a whole.

One solution is to use multiple reference translations, which is expensive. An alternative solution, tried in a number of recent approaches, is to address this issue by allowing the evaluation process to take into account paraphrases of phrases in the reference trans-lation so as to award credit to parts of the translation hypothesis that are semantically, even if not lexically, correct (Owczarzak et al. 2006; Zhou, Lin, and Hovy 2006). ( peers ) are also evaluated against reference summaries created by human authors ( models ). Zhou et al. (2006) propose a new metric called ParaEval that leverages an automatically extracted database of phrasal paraphrases to inform the computation of n -gram overlap between peer summaries and multiple model summaries. 1.3.3 Machine Translation. Besides being used in evaluation of machine translation sys-tems, paraphrasing has also been applied to directly improve the translation process.
Callison-Burch, Koehn, and Osborne (2006) use automatically induced paraphrases to improve a statistical phrase-based machine translation system. Such a system works by dividing the given sentence into phrases and translating each phrase individually by looking up its translation in a table. The coverage of the translation system is improved contains the phrase presidente de Brazil but the system does not have a translation for it, another Spanish phrase such as presidente brasile  X  no may be automatically detected as the paraphrase, the system can use the same translation for the given phrase. Therefore, paraphrasing allows the translation system to properly handle phrases that it does not otherwise know how to translate.
 reference sparsity . The fundamental problem that translation systems have to face is that there is no such thing as the correct translation for any sentence. In fact, any given source sentence can often be translated into the target language in many valid ways.
Because there can be many  X  X orrect answers, X  almost all models employed by SMT systems require, in addition to a large bitext, a held-out development set comprising multiple high-quality, human-authored reference translations in the target language in order to tune their parameters relative to a translation quality metric. However, given paraphrases and use them to expand the available reference translations for such sets so that the machine translation system can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and
Katz 2005). A more complex formulation of the task would be to detect or recognize 2005; Marsi and Krahmer 2005a; Wu 2005; Jo ` ao, Das, and Pavel 2007a, 2007b; Das and
Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monolingual parallel or comparable corpora (discussed in Section 3) can benefit immensely from this task. In general, paraphrase recognition can be very helpful for several NLP applications. Two examples of such applications are text-to-text gener-ation and information extraction.
 multi-document summarization system, detecting redundancy is a very important con-cern because two sentences from different documents may convey the same semantic this note, Barzilay and McKeown (2005) exploit the redundancy present in a given set systems (Marsi and Krahmer 2005b).
 structure X  X vents which are reported many times, about different individuals and in different forms X  X nd making them explicit so that they can be processed and used in other ways. Sekine (2006) shows how to use paraphrase recognition to cluster together extraction patterns to improve the cohesion of the extracted information.
 textual entailment : A piece of text T is said to entail a hypothesis H if humans reading
T will infer that H is most likely true. The observant reader will notice that, in this sense, the task of paraphrase recognition can simply be formulated as bidirectional entailment recognition. The task of recognizing entailment is an application-independent task and has important ramifications for almost all other language processing tasks that can derive benefit from some form of applied semantic inference. For this reason, the task has received noticeable attention in the research community and annual community-wide evaluations of entailment systems have been held in the form of the Recognizing
Textual Entailment (RTE) Challenges (Dagan, Glickman, and Magnini 2006; Bar-Haim et al. 2007; Sekine et al. 2007; Giampiccolo et al. 2008).
 provides a comprehensive framework for semantic inference and argues for building a concrete inference engine that not only recognizes entailment but also searches for all entailing texts given an entailment hypothesis H and, conversely, generates all entailed statements given a text T . Given such an engine, Dagan claims that paraphrase 346 generation is simply a matter of generating all entailed statements given any sentence.
Although this is a very attractive proposition that defines both paraphrase generation and recognition in terms of textual entailment, there are some important caveats. For example, textual entailment cannot guarantee that the entailed hypothesis H captures all of the same meaning as the given text T . Consider the following example: T : Yahoo X  X  buyout of Overture was finalized.

H 1 : Yahoo bought Overture.

H 2 : Overture is now owned by Yahoo.

Although both H 1 and H 2 are entailed by T , they are not strictly paraphrases of T because some of the semantic content has not been carried over. This must be an important consideration when building the proposed entailment engine. Of course, even these approximately semantically equivalent constructions may prove useful in an appropriate downstream application.
 than it might appear. Entailment recognition systems sometimes rely on the use of phrase recognition to improve their performance (Bosma and Callison-Burch 2007).
In fact, examination of some RTE data sets in an attempt to quantitatively determine also been observed that, in the entailment challenges, it is relatively easy for submitted systems to recognize constructions that partially overlap in meaning (approximately paraphrastic) from those that are actually bound by an entailment relation. On the flip side, work has also been done to extend entailment recognition techniques for the purpose of paraphrase recognition (Rus, McCarthy, and Lintean 2008).
 and diverse work that has been done in both these areas, we feel that any significant discussion beyond the treatment above merits a separate, detailed survey. 3. Paraphrasing with Corpora
In this section, we explore in detail the data-driven paraphrase generation approaches that have emerged and have become extremely popular in the last decade or so. These corpus-based methods have the potential of covering a much wider range of paraphras-ing phenomena and the advantage of widespread availability of corpora.
 a single monolingual corpus, monolingual comparable corpora, monolingual parallel corpora, and bilingual parallel corpora. This form of organization, in our opinion, is the most instructive because most of the algorithmic decisions made for paraphrase generation will depend heavily on the type of corpus used. For instance, it is reasonable to assume that a different set of considerations will be paramount when using a large single monolingual corpus than when using bilingual parallel corpora.
 it would be very useful to explain the motivation behind distributional similarity, an extremely popular technique that can be used for paraphrase generation with several different types of corpora. We devote the following section to such an explanation. 3.1 Distributional Similarity
The idea that a language possesses distributional structure was first discussed at length by Harris (1954). The term represents the notion that one can describe a language in terms of relationships between the occurrences of its elements (words, morphemes, phonemes) relative to the occurrence of other elements. The name for the phenomenon is derived from an element X  X  distribution  X  X ets of elements in particular positions that the element occurs with to produce an utterance or a sentence.
 hypothesis that such a structure exists naturally for language. Here, we closely quote these observations: utional similarity : words or phrases that share the same distribution X  X he same set of words in the same context in a corpus X  X end to have similar meanings.
 leverage distributional similarity. The input corpus is usually a single or set of mono-lingual corpora (parallel or non-parallel). After preprocessing X  X hich may include tagging the parts of speech, generating parse trees, and other transformations X  X he next step is to extract pairs of words or phrases (or patterns) that occur in the same context in the corpora and hence may be considered (approximately) semantically equivalent. This extraction may be accomplished by several means (e.g., by using a classifier employing possible to stop at this point and consider this list as the final output, the list usually such as collocations counts from another corpus (or the Web). Finally, some techniques into templates or rules which may then be applied to other sentences to generate their paraphrases. Note that generalization as a post-processing step may not be necessary if the induction process can extract distributionally similar patterns directly. that are distributionally similar may not necessarily end up being paraphrastic: Both 348 elements of the pairs boys, girls , cats, dogs , high, low can occur in similar contexts but are not semantically equivalent. 3.2 Paraphrasing Using a Single Monolingual Corpus
In this section, we concentrate on paraphrase generation methods that operate on a single monolingual corpus. Most, if not all, such methods usually perform paraphrase induction by employing the idea of distributional similarity as outlined in the previous section. Besides the obvious caveat discussed previously regarding distributional sim-ilarity, we find that the other most important factor affecting the performance of these methods is the choice of distributional ingredients X  X hat is, the features used to formu-late the distribution of the extracted units. We consider three commonly used techniques that generate phrasal paraphrases (or paraphrastic patterns) from a single monolingual corpus but use very different distributional features in terms of complexity. The first uses only surface-level features and the other two use features derived from additional semantic knowledge. Although the latter two methods are able to generate more so-phisticated paraphrases by virtue of more specific and more informative ingredients, we find that doing so usually has an adverse effect on their coverage.
 documents taken from the repository of documents crawled by Google. Although using
Web documents as input data does require a non-trivial pre-processing phase since such documents tend to be noisier, there are certainly advantages to the use of Web docu-ments as the input corpus: It does not require parallel (or even comparable) documents and can allow leveraging of even larger document collections. In addition, the extracted paraphrases are not tied to any specific domain and are suitable for general application. of a specific kind from each sentence: Each n -gram has L between M 1 to M 2 words in the middle, and another L c words at the end. Steps 7 X 13 can intuitively be interpreted as constructing a textual anchor A  X  X y concatenating a fixed number of words from the left and the right X  X or each candidate paraphrase C and storing the anchor, candidate tuple in H . These anchors are taken to constitute the distribution of the words and phrases under inspection. Finally, each occurrence of a pair of potential paraphrases, that is, a pair sharing one or more anchors, is counted. The final set of phrasal paraphrastic pairs is returned.
 considers all words and phrases that are distributionally similar X  X hose that occur with the same sets of anchors (or distributions) X  X o be paraphrases of each other. Addition-ally, the larger the set of shared anchors for two candidate phrases, the stronger the like-lihood that they are paraphrases of each other. After extracting the list of paraphrases, less likely phrasal paraphrases are filtered out by using an appropriate count threshold. attempting variants where they extract the n -grams only from sentences that include specific additional information to be added to the anchor. For example, in one variant, they only use sentences where the candidate phrase is surrounded by named entities
Algorithm 1 (Pa  X  sca and Dienes 2005) . Induce a set of phrasal paraphrase pairs H with associated counts from a corpus of pre-processed Web documents.
 Summary . Extract all n -grams from the corpus longer than a pre-stipulated length.
Compute a lexical anchor for each extracted n -gram. Pairs of n -grams that share lexical anchors are then construed to be paraphrases. 1: Let N represent a set of n -grams extracted from the corpus 2: N  X  X   X  } , H  X  X   X  } 3: for each sentence E in the corpus do 4: Extract the set of n -grams N E = {  X  e i s . t (2 L 6: end for 7: for each n -gram  X  e in N do 8: Extract the subsequence C ,suchthat L c  X | C | X  ( |  X  e 9: Extract the subsequence A L ,suchthat0  X | A L | X  ( L c 10: Extract the subsequence A R ,suchthat( |  X  e | X  L c ) 12: Add the pair ( A , C )to H 13: end for 14: for each subset of H with the same anchor A do 15: Exhaustively compare each pair of tuples ( A , C i )and( A , C 16: Update the count of the candidate paraphrase pair ( C 17: Update the count of the candidate paraphrase pair ( C 18: end for 19: Output H containing paraphrastic pairs and their respective counts 350 on both sides and they attach the nearest pair of entities to the anchor. As expected, the paraphrases do improve in quality as the anchors become more specific. However, they also report that as anchors are made more specific by attaching additional information, the likelihood of finding a candidate pair with the same anchor is reduced. tainly be more complex than simple phrases used by Pas  X ca and Dienes. Lin and Pantel (2001) discuss how to measure distributional similarity over dependency tree paths in order to induce generalized paraphrase templates such as: relationships, a sequence of links, or a path , can be understood to represent an indirect relationship. Here, a path is named by concatenating the dependency relationships and lexical items along the way but excluding the lexical items at the end. In this way, a path can actually be thought of as a pattern with variables at either end. Consider the first dependency tree in Figure 2. One dependency path that we could extract would be between the node John and the node problem .Westartat John and see that the first item in the tree is the dependency relation subject that connects a noun to a verb and so we append that information to the path. 3 The next item in the tree is the word found and we append its lemma ( find ) to the path. Next is the semantic relation object connecting a verb to a noun and we append that. The process continues until we reach the other slot (the word problem ) at which point we stop. 4 The extracted path is shown below the tree.
Similarly, we can extract a path for the second dependency tree. Let X  X  briefly mention the terminology associated with such paths:
Intuitively, one can imagine a path to be a complex representation of the pattern X finds answer to Y , where X and Y are variables. This representation for a path is a perfect fit for an extended version of the distributional similarity hypothesis: If similar sets of words fill the same variables for two different patterns, then the patterns may be considered to have similar meaning, which is indeed the case for the paths in Figure 2.
 dency parses for all the sentences in the corpus in the pre-processing step. Algorithm 2 compute their distributional properties, and Steps 3 X 14 extract pairs of paths which are similar, insofar as such properties are concerned. 5 At the end, we have sets of paths (or inference rules) that are considered to have similar meanings by the algorithm. root of the extracted path. For example, whereas verbs frequently tend to have several modifiers, nouns tend to have no more than one. However, if a word has any fewer than two modifiers, no path can go through it as the root. Therefore, the algorithm tends to perform better for paths with verbal roots. Another issue is that this algorithm, despite the use of more informative distributional features, can generate several incorrect or im-plausible paraphrase patterns (inference rules). Recent work has shown how to filter out incorrect inferences when using them in a downstream application (Pantel et al. 2007). as the one in which the paraphrases are desired. Wu and Zhou (2003) describe a 352 Algorithm 2 (Lin and Pantel 2001). Produce inference rules from a parsed corpus.
Summary . Adapt Harris X  X  (1954) hypothesis of distributional similarity for paths in dependency trees: If two tree paths have similar distributions such that they tend to link the same set of words, then they likely mean the same thing and together generate an inference rule. 1: Extract paths of the form described above from the parsed corpus 3: for each extracted path p do 4: Find all instances ( p , w 1 , w 2 )suchthat p connects the words w 5: for each such instance do 6: Update C ( p , SlotX , w 1 )and I ( p , SlotX , w 1 )in H 7: Update C ( p , SlotY , w 2 )and I ( p , SlotY , w 2 )in H 8: end for 9: end for 10: for each extracted path p do 11: Create a candidate set C of similar paths by extracting all paths from H that share 12: Prune candidates from C based on feature overlap with p 13: Compute the similarity between p and the remaining candidates in 14: Output all paths in C sorted by their similarity to p 15: end for bilingual approach to extract English relation-based paraphrastic patterns of the form w 1 , R , w 2 , where w 1 and w 2 are English words connected by a dependency link with the semantic relation R . Figure 3 shows a simple example based on their approach. First, instances of one type of pattern are extracted from a parsed monolingual corpus. In the figure, for example, a single instance of the pattern verb ,IN, pobj has been extracted.
Several new, potentially paraphrastic, English candidate patterns are then induced by replacing each of the English words with its synonyms in WordNet, one at a time. The figure shows the list of induced patterns for the given example. Next, each of the English words in each candidate pattern is translated to Chinese, via a bilingual dictionary. given English word, several Chinese patterns may be created for each English candidate pattern. Each Chinese pattern is assigned a probability value via a simple bag-of-words translation model (built from a small bilingual corpus) and a language model (trained on a Chinese collocation database); all translated patterns, along with their probability values, are then considered to be features of the particular English candidate pattern.
Any English pattern can subsequently be compared to another by computing cosine similarity over their shared features. English collocation pairs whose similarity value exceeds some threshold are construed to be paraphrastic.
 virtue of the increased informativeness of the distributional features X  X nd its coverage is seen in this work as well. When using translations from the bilingual dictionary, a knowledge-rich resource, the authors report significantly higher precision than compa-rable methods that rely only on monolingual information to compute the distributional similarity. Predictably, they also find that recall values obtained with their dictionary-based method are lower than those obtained by other methods.
 on some form of distributional similarity because there are no explicit clues available that indicate semantic equivalence. In the next section, we look at paraphrasing methods operating over data that does contain such explicit clues. 3.3 Paraphrasing Using Monolingual Parallel Corpora
It is also possible to generate paraphrastic phrase pairs from a parallel corpus where each component of the corpus is in the same language. Obviously, the biggest advantage of parallel corpora is that the sentence pairs are paraphrases almost by definition; they represent different renderings of the same meaning created by different translators 354 that are either semantically equivalent (sentential paraphrases) or have significant se-mantic overlap. Extraction of phrasal paraphrases can then be effected by extracting semantic content. We present four techniques in this section that generate paraphrases by finding such correspondences. The first two techniques attempt to do so by relying, distributional similarity algorithm and the other by simply adapting the previously described dependency path similarity algorithm to work with a parallel corpus. The next two techniques rely on more direct, non-distributional methods to compute the required correspondences.
 move beyond a single-pass distributional similarity method. They propose a bootstrap-ping algorithm that allows for the gradual refinement of the features used to determine similarity and yields improved paraphrase pairs. As their input corpus, they use mul-tiple human-written English translations of literary texts such as Madame Bovary and
Twenty Thousand Leagues Under the Sea that are expected to be rich in paraphrastic ex-pressions because different translators would use their own words while still preserving the meaning of the original text. The parallel components are obtained by performing sentences that are then lemmatized, part-of-speech tagged and chunked in order to identify all the verb and noun phrases. The bootstrapping algorithm is then employed to incrementally learn better and better contextual features that are then leveraged to generate semantically similar phrasal correspondences.
 paraphrase examples are extracted by using identical words from either side of the aligned sentence pair. For example, given the following sentence pair:
S 1 : Emma burst into tears and he tried to comfort her.

S 2 : Emma cried and he tried to console her.
 tried , tried , her , her may be extracted as positive examples and tried , Emma , tried , console may be extracted as negative examples. Once the seeding examples are ex-negative examples. These features take the form of aligned part-of-speech sequences of a given length from the left and the right of the example. For instance, we can extract the contextual feature [ L 1 : PRP 1 , R 1 : TO 1 , L 2 : PRP
POS tag sequence to the left of the word tried is a personal pronoun ( he )andthePOS tag sequence to the right of tired is the preposition to . The second tuple is identical for this case. Note that the tags of identical tokens are indicated as such by subscripts on the
POS tags. All such features are extracted for both the positive and the negative examples for all lengths less than or equal to some specified length. In addition, a strength value is calculated for each positive (negative) contextual feature f using maximum likelihood estimation as follows: value. The remaining contextual rules are then applied to the corpora to obtain addi-contextual rules, and so on. The process is repeated for a fixed number of iterations or until no new paraphrase examples are produced. The list of extracted paraphrases at the end of the final iteration represents the final output of the algorithm. In total, about 9, 000 phrasal (including lexical) paraphrases are extracted from 11 translations of five works of classic literature. Furthermore, the extracted paraphrase pairs are also gener-alized into about 25 patterns by extracting part-of-speech tag sequences corresponding to the tokens of the paraphrase pairs.
 nique that was originally developed for compiling translation lexicons from bilingual parallel corpora (Melamed 2001). This technique first compiles an initial lexicon using simple co-occurrence statistics and then uses a competitive linking algorithm (Melamed 1997) to improve the quality of the lexicon. The authors apply this technique to their monolingual parallel data and observe that the extracted paraphrase pairs are of much lower quality than the pairs extracted by their own method. We present similar obser-vations in Section 3.5 and highlight that although more recent translation techniques X  specifically ones that use phrases as units of translation X  X re better suited to the task of generating paraphrases than the competitive linking approach, they continue to suffer from the same problem of low precision. On the other hand, such techniques can take good advantage of large bilingual corpora and capture a much larger variety of paraphrastic phenomena.
 of the dependency path distributional similarity algorithm proposed by Lin and Pantel (2001) to the same monolingual parallel corpus (multiple translations of literary works) used by Barzilay and McKeown (2001). The authors claim that their technique is more tractable than Lin and Pantel X  X  approach since the sentence-aligned nature of the input parallel corpus obviates the need to compute similarity over tree paths drawn from sentences that have zero semantic overlap. Furthermore, they also claim that their technique exploits the parallel nature of a corpus more effectively than Barzilay and 356
McKeown X  X  approach simply because their technique uses tree paths and not just lexical information. Specifically, they propose the following modifications to Lin and Pantel X  X  algorithm: 1. Extracting tree paths with aligned anchors . Rather than using a single 2. Using a sliding frequency measure . The original dependency-based
Despite the authors X  claims, they offer no quantitative evaluation comparing their paraphrases with those from Lin and Pantel (2001) or from Barzilay and McKeown (2001).
 more direct approach instead of relying on distributional similarity. Pang, Knight, and
Marcu (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences. The alignment algorithm outputs a merged lattice from which lexical, phrasal, and sentential paraphrases can simply be read off. More specifically, they use the Multiple-Translation Chinese corpus that was originally developed for machine translation evaluation and contains 11 human-written
English translations for each sentence in a news document. Using several sentences for paraphrase induction.
 than 45 words is discarded. Next, each sentence in each of the groups is parsed. All rithm proceeds top-down and continues to recursively merge constituent nodes that are expanded identically. It stops upon reaching the leaves or upon encountering the same constituent node expanded using different grammar rules. Figure 5(a) shows is apparent that the leaves of the forest encode paraphrasing information. However, the merging only allows identical constituents to be considered as paraphrases. In addition, keyword-based heuristics need to be employed to prevent inaccurate merging of constituent nodes due to, say, alternations of active and passive voices among the path in the lattice for each merged node. Figure 5(b) shows the word lattice generated for the simple two-tree forest. The lattices also require some post-processing to remove redundant edges and nodes that may have arisen due to parsing errors or limitations in the merging algorithm. The final output of the paraphrasing algorithm is a set of word lattices, one for each sentence group.
 each other. For example, besides the obvious lexical paraphrases, the paraphrase pair ate at cafe, chowed down at bistro can also be extracted from the lattice in Figure 5(b).
In addition, each path between the START and the END nodes in the lattice represents a sentential paraphrase of the original 11 sentences used to create the lattice. away entirely with any need for measuring distributional similarity. In general, it has several advantages. It can capture a very large number of paraphrases: Each lattice has on the order of hundreds or thousands of paths depending on the average length of the sentence group that it was generated from. In addition, the paraphrases produced are of better quality than other approaches employing parallel corpora for paraphrase induction discussed so far. However, the approach does have a couple of drawbacks: 358 can be effected by building on word alignment techniques from the field of statistical machine translation (Brown et al. 1990). Current state-of-the-art SMT methods rely on unsupervised induction of word alignment between two bilingual parallel sentences to extract translation equivalences that can then be used to translate a given sentence in one language into another language. The same methods can be applied to monolingual use one such method to extract phrasal paraphrase pairs. Furthermore, they use these extracted phrasal pairs to construct sentential paraphrases for new sentences. generation may be expressed in terms of the typical channel model equation for statistical machine translation:
The equation denotes the search for the optimal paraphrase E We may use Bayes X  Theorem to rewrite this as: where P ( E p )isan n -gram language model providing a probabilistic estimate of the fluency of a hypothesis E p and P ( E | E p ) is the translation model, or more appropriately for paraphrasing, the replacement model , providing a probabilistic estimate of what is essentially the semantic adequacy of the hypothesis paraphrase. Therefore, the optimal sentential paraphrase may loosely be described as one that fluently captures most, if not all, of the meaning contained in the input sentence.
 unsupervised induction of word alignments typically requires a relatively large number of parallel sentence pairs. The monolingual parallel corpus (or more accurately, quasi-by scraping on-line news sites for clusters of articles on the same topic. Such clusters moving the mark-up, the authors discard any pair of sentences in a cluster where the difference in the lengths or the edit distance is larger than some stipulated value. This method yields a corpus containing approximately 140, 000 quasi-parallel sentence pairs { ( E proposed method can work well:
S : In only 14 days, U.S. researchers have created an artificial bacteria-eating virus
S : An artificial bacteria-eating virus has been made from synthetic genes in the
S : The largest gains were seen in prices, new orders, inventories, and exports.
S : Sub-indexes measuring prices, new orders, inventories, and exports increased.
For more details on the creation of this corpus, we refer the reader to Dolan, Quirk,
Algorithm 3 (Quirk, Dolan, and Brockett 2004) . Generate a set M of phrasal para-phrases with associated likelihood values from a monolingual parallel corpus C .
Summary . Estimate a simple English to English phrase translation model from C using word alignments. Use this model to create sentential paraphrases as explained later. 1: M  X  X   X  } 2: Compute lexical replacement probabilities P ( e 1 | e 2 3: Compute a set of word alignments { a } such that for each sentence pair ( E 4: for each word-aligned sentence pair ( E 1 , E 2 ) a in C do 5: Extract pairs of contiguous subsequences (  X  e 1 ,  X  e 6: Add all extracted pairs to M 7: end for 8: for each paraphrase pair (  X  e 1 ,  X  e 2 )in M do 9: Compute P (  X  e 1 |  X  e 2 ) = 10: end for 11: Output M containing paraphrastic pairs and associated probabilities 360 generate a set of phrasal paraphrase pairs and compute a probability value for each such pair. In Step 2, a simple parameter estimation technique (Brown et al. 1993) is used
Step 3 computes a word alignment (indicated by a ) between each pair of sentences. This alignment indicates for each word e i in one string that word e which it was most likely produced (denoted here by e i a  X  pair of sentences, pairs of short contiguous phrases that are aligned with each other according to this alignment. Note that each such extracted pair is essentially a phrasal paraphrase. Finally, a probability value is computed for each such pair by assuming that each word of the first phrase can be replaced with each word of the second phrase. This computation uses the lexical replacement probabilities computed in Step 2. generate paraphrases for any unseen sentence. Generation proceeds by creating a lattice for the given sentence. Given a sentence E , the lattice is populated as follows: 1. Create | E | + 1 vertices q 0 , q 1 ... q | E | . 2. Create N edges between each pair of vertices q j and q 3. Add the edges { ( q j  X  1 , q j ) } and label each edge with the token s
Figure 6 shows an example lattice. Once the lattice has been constructed, it is straight-forward to extract the 1-best paraphrase by using a dynamic programming algorithm such as Viterbi decoding and extracting the optimal path from the lattice as scored by the product of an n -gram language model and the replacement model. In addition, as with
SMT decoding, it is also possible to extract a list of n -best paraphrases from the lattice by using the appropriate algorithms (Soong and Huang 1990; Mohri and Riley 2002). literature so as to align phrasal equivalences as well as to utilize the aligned phrasal equivalences to rewrite new sentences. The biggest advantage of this method is its SMT inheritance: It is possible to produce multiple sentential paraphrases for any new sentence, and there is no limit on the number of sentences that can be paraphrased. However, there are certain limitations:
All of these limitations combined lead to paraphrases that, although grammatically sound, contain very little variety. Most sentential paraphrases that are generated involve little more than simple substitutions of words and short phrases. In Section 3.5, we will discuss other approaches that also find inspiration from statistical machine translation and attempt to circumvent the above limitations by using a bilingual parallel corpus instead of a monolingual parallel corpus. 3.4 Paraphrasing Using Monolingual Comparable Corpora
Whereas it is clearly to our advantage to have monolingual parallel corpora, such corpora are usually not very readily available. The corpora usually found in the real world are comparable instead of being truly parallel: Parallelism between sentences is replaced by just partial semantic and topical overlap at the level of documents. There-fore, for monolingual comparable corpora, the task of finding phrasal correspondences becomes harder because the two corpora may only be related by way of describing events under the same topic. In such a scenario, possible paraphrasing methods either distributional similarity workhorse or, (b) attempt to directly induce a form of coarse-grained alignment between the two corpora and leverage this alignment.
 comparable corpora. The first method falls under category (a): Here the elements whose butions themselves are the named entities with which the elements occur in various directly discover correspondences between two comparable corpora by leveraging a novel alignment algorithm combined with some similarity heuristics. The difference between the two latter methods lies only in the efficacy of the alignment algorithm. newspapers from the same day as their source of paraphrases. The comparable nature of the articles is ensured because both sets are from the same day. During pre-processing, 362 all named entities in each article are tagged and dependency parses are created for each sentence in each article. The distributional similarity driven algorithm then proceeds as follows: 1. For each article in the first set, find the most  X  X imilar X  article from the 2. From each sentence in each such pair of articles, extract all dependency 3. Find all sentences in the two newswire corpora that match these 4. Find all sentences that are most similar to each other (above some preset 5. For each pair of similar sentences, compare their respective attached
At the end, the output is a list of generalized paraphrase patterns with named entity types as variables. For example, the algorithm may generate the following two patterns as paraphrases: similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for specific concepts represented by keywords.
 interesting and has certainly been explored before (see the discussion regarding Pas  X ca and Dienes [2005] in Section 3.2). However, it has some obvious disadvantages. The authors manually evaluate the technique by generating paraphrases for two specific domains (arrest events and personnel hirings) and find that while the precision is reasonably good, the coverage is very low primarily due to restrictions on the patterns higher.
 spondences by leveraging the comparable nature of the corpora. Barzilay and Lee (2003) attempt to do so by generating compact sentence clusters in template form (stored as word lattices with slots) separately from each corpora and then pairing up templates from one corpus with those from the other. Once the templates are paired up, a new incoming sentence that matches one member of a template pair gets rendered as the other member, thereby generating a paraphrase. They use as input a pair of corpora: the first ( C 1 ) consisting of clusters of news articles published by Agence France Presse (AFP) and the second ( C 2 ) consisting of those published by Reuters. The two corpora may be considered comparable since the articles in each are related to the same topic and were published during the same time frame.
 how to cluster topically related sentences, construct a word lattice from such a cluster, and convert that into a slotted lattice  X  X asically a word lattice with certain nodes recast pertaining to the same topics and having similar structure. The word lattice is the prod-uct of an algorithm that computes a multiple-sequence alignment (MSA) for a cluster of sentences (Step 6). A very brief outline of such an algorithm, originally developed to compute an alignment for a set of three or more protein or DNA sequences, is as follows: 9 1. Find the most similar pair of sentences in the cluster according to a 2. Align this sentence pair and replace the pair with this single alignment. 3. Repeat until all sentences have been aligned together.

The word lattice so generated now needs to be converted into a slotted lattice to allow its use as a paraphrase template. Slotting is performed based on the following intuition:
Areas of high variability between backbone nodes, that is, several distinct parallel paths, may correspond to template arguments and can be collapsed into one slot that can be filled by these arguments. However, multiple parallel paths may also appear in the lattice because of simple synonymy and those paths must be retained for paraphrase generation to be useful. To differentiate between the two cases, a synonymy threshold s of 30% is used, as shown in Steps 8 X 14. The basic idea behind the threshold is that as the number of sentences increases, the number of different arguments will increase faster than the number of synonyms. Figure 7 shows how a very simple word lattice may be generalized into a slotted lattice.
 try to match the slotted lattices extracted from one corpus to those extracted from the other by referring back to the sentence clusters from which the original lattices were 364
Algorithm 4 (Barzilay and Lee 2003) . Generate set M of matching lattice pairs given a pair of comparable corpora C 1 and C 2 .

Summary . Gather topically related sentences from C 1 into clusters. Do the same for C
Convert each sentence cluster into a slotted lattice using a multiple-sequence alignment (MSA) algorithm. Compare all lattice pairs and output those likely to be paraphrastic. 1: Let W C 1 and W C 2 represent word lattices obtained from C 2: M  X  X   X  } , W C 1  X  X   X  } , W C 2  X  X   X  } 3: for each input corpus C i  X  X  C 1 , C 2 } do 4: Create a set of clusters G C i = { G C i , k } of sentences based on n -gram overlap such 5: for each cluster G C i , k do 6: Compute an MSA for all sentences in G C i , k by using a pre-stipulated scoring 7: Compute the set of backbone nodes B k for W C i , k 8: for each backbone node b  X  B k do 9: if no more than 30% of all the edges from b lead to the same node then 10: Replace all nodes adjacent to b with a single slot 11: else 12: Delete any node with &lt; 30% of the edges from b leading to it and preserve 13: end if 14: end for 15: Merge any consecutive slot nodes into a single slot 17: end for 18: end for 19: for each lattice pair ( W C 1 , j , W C 2 , k )  X  W C 21: if comparison score &gt; a pre-stipulated threshold  X  then 23: end if 24: end for 25: Output M containing paraphrastic lattice pairs with linked slots generated, comparing the sentences that were written on the same day and computing a comparison score based on overlap between the sets of arguments that fill the slots. If this computed score is greater than some fixed threshold value  X  , then the two lattices (or patterns) are considered to be paraphrases of each other.
 and actually use the patterns to generate paraphrases for new sentences. Given such a sentence S , the first step is to find an existing slotted lattice from either corpus that aligns best with S , in terms of the previously mentioned alignment scoring function.
If some lattice is found as a match, then all that remains is to take all corresponding lattices from the other corpus that are paired with this lattice and use them to create multiple rewritings (paraphrases) for S. Rewriting in this context is a simple matter of substitution: For each slot in the matching lattice, we know not only the argument from the sentence that fills it but also the slot in the corresponding rewriting lattice. performs almost all other sentential paraphrasing approaches surveyed in this article.
However, a paraphrase is produced only if the incoming sentence matches some existing the construction and generalization of lattices may become computationally expensive when dealing with much larger corpora.

Both take sentences grouped together in a cluster and align them into a lattice using a particular algorithm. Pang, Knight, and Marcu have a pre-defined size for all clusters since the input corpus is an 11-way parallel corpus. However, Barzilay and Lee have to construct the clusters from scratch because their input corpus has no pre-defined notion of parallelism at the sentence level. Both approaches use word lattices to represent and induce paraphrases since a lattice can efficiently and compactly encode n -gram similar-ities (sets of shared overlapping word sequences) between a large number of sentences.
However, the two approaches are also different in that Pang, Knight, and Marcu use the parse trees of all sentences in a cluster to compute the alignment (and build the lattice), whereas Barzilay and Lee use only surface level information. Furthermore, Barzilay and Lee can use their slotted lattice pairs to generate paraphrases for novel and unseen sentences, whereas Pang, Knight, and Marcu cannot paraphrase new sentences at all. 366 include syntactic constraints in the cluster alignment algorithm. In that way, it is doing something similar to what Pang, Knight, and Marcu do but with a comparable corpus simple alignment scoring function based on purely lexical features, Shen et al. try to bring syntactic features into the mix. The motivation is to constrain the relatively free nature of the alignment generated by the MSA algorithm X  X hich may lead to the gen-eration of grammatically incorrect sentences X  X y using informative syntactic features. In their approach, even if two words are a lexical match  X  X s defined by Barzilay and Lee (2003) X  X hey are further inspected in terms of certain pre-defined syntactic features.
Therefore, when computing the alignment similarity score, two lexically matched words across a sentence pair are not considered to fully match unless their score on syntactic features also exceeds a preset threshold.
 word:
With this information and a heuristic to compute the similarity between two words in terms of their POS and IOB tags, the alignment similarity score can be calculated as the sum of the heuristic similarity value for the given two words and the heuristic higher than some threshold and the two words have similar positions in their respective sentences, then the words are considered to be a match and can be aligned. Given this alignment algorithm, the word lattice representing the global alignment is constructed in an iterative manner similar to the MSA approach.
 pled from lattices constructed via the syntactically informed alignment method receive higher grammaticality scores as compared to sentences from the lattices constructed via the purely lexical method. However, they present no analysis of the actual paraphrasing capacity of their, presumably better aligned, lattices. Indeed, they explicitly mention that their primary goal is to measure the correlation between the syntax-augmented scoring function and the correctness of the sentences being generated from such lattices, even if the sentences do not bear a paraphrastic relationship to the input. Even if one were to assume that the syntax-based alignment method would result in better paraphrases, it still would not address the primary weakness of Barzilay and Lee X  X  method: Para-leading to lower coverage. 3.5 Paraphrasing Using Bilingual Parallel Corpora
In the last decade, there has been a resurgence in research on statistical machine transla-tion. There has also been an accompanying dramatic increase in the number of available bilingual parallel corpora due to the strong interest in SMT from both the public and private sectors. Recent research in paraphrase generation has attempted to leverage these very large bilingual corpora. In this section, we look at such approaches that rely on the preservation of meaning across languages and try to recover said meaning by using cues from the second language.
 sentences in the other language are exactly semantically equivalent to sentences in the intended paraphrasing language. Therefore, the most common way to generate paraphrases with such a corpus exploits both its parallel and bilingual natures: Align phrases across the two languages and consider all co-aligned phrases in the intended language to be paraphrases. The bilingual phrasal alignments can simply be generated by using the automatic techniques developed for the same task in the SMT literature.
Therefore, arguably the most important factor affecting the performance of these techniques is usually the quality of the automatic bilingual phrasal (or word) alignment techniques.
 proposed by Bannard and Callison-Burch (2005). This technique operates exactly as described above by attempting to infer semantic equivalence between phrases in the same language indirectly with the second language as a bridge. Their approach builds on one of the initial steps used to train a phrase-based statistical machine translation system (Koehn, Och, and Marcu 2003). Such systems rely on phrase tables  X  X  tabulation of correspondences between phrases in the source language and phrases in the target language. These tables are usually extracted by inducing word alignments between sentence pairs in a training corpus and then incrementally building longer phrasal correspondences from individual words and shorter phrases. Once such a tabulation of bilingual phrasal correspondences is available, correspondences between phrases in one language may be inferred simply by using the phrases in the other language as pivots. a bilingual corpus C by using word alignments. Steps 3 X 7 extract bilingual phrasal correspondences from each sentence pair in the corpus by using heuristically induced bidirectional word alignments. Figure 8 illustrates this extraction process for two exam-ple sentence pairs. For each pair, a matrix shows the alignment between the Chinese and the English words. Element ( i , j ) of the matrix is filled if there is an alignment link between the i th Chinese word and the j th English word e with the word alignment are then extracted. A consistent phrase pair can intuitively be thought of as a sub-matrix where all alignment points for its rows and columns are inside it and never outside. Next, Steps 8 X 11 take all English phrases that correspond to the same foreign phrase and infer them all to be paraphrases of each other. example, the English paraphrase pair effectively contained, under control is obtained from Figure 8 by pivoting on the Chinese phrase , shown underlined for both matrices. 368
Algorithm 5 (Bannard and Callison-Burch 2005) . Generate set M of monolingual para-phrase pairs given a bilingual parallel corpus C .
 Summary . Extract bilingual phrase pairs from C using word alignments and standard
SMT heuristics. Pivot all pairs of English phrases on any shared foreign phrases and consider them paraphrases. The alignment notation from Algorithm 3 is employed. 1: Let B represent the bilingual phrases extracted from C 2: B  X  X   X  } , M  X  X   X  } 3: Compute a word alignment a for each sentence pair ( E , F ) 4: for each aligned sentence pair ( E , F ) a do 5: Extract the set of bilingual phrasal correspondences { 6: B  X  B  X  X  (  X  e ,  X  f ) } 7: end for 8: for each member of the set { (  X  e j ,  X  f k ), (  X  e 9: M  X  M  X  X  (  X  e j ,  X  e l ) } 10: Compute p (  X  e j |  X  e l ) =  X  f p (  X  e j |  X  f ) p ( 11: end for 12: Output M containing paraphrastic pairs and associated probabilities probability value to any of the inferred paraphrase pairs as follows: as part of the bilingual phrasal extraction process:
Once the probability values are obtained, the most likely paraphrase can be chosen for any phrase.
 phrases from a bilingual parallel corpus. Such an approach is able to capture a large variety of paraphrastic phenomena in the inferred paraphrase pairs but is seriously limited by the bilingual word alignment technique. Even state-of-the-art alignment methods from SMT are known to be notoriously unreliable when used for aligning paraphrases obtained via manually constructed word alignments is significantly better than that of the paraphrases obtained from automatic alignments.
 are not ideal for use in translation and, furthermore, improving these techniques does not always lead to an improvement in translation performance. (Callison-Burch, Talbot, and Osborne 2004; Ayan and Dorr 2006; Lopez and Resnik 2006; Fraser and Marcu 2007). More details on the relationship between word alignment and SMT can be found in the comprehensive SMT survey recently published by Lopez (2008) (particularly
Section 4.2). Paraphrasing done via bilingual corpora relies on the word alignments in the same way as a translation system would and, therefore, would be equally susceptible to the shortcomings of the word alignment techniques. To determine how noisy automatic word alignments affect paraphrasing done via bilingual corpora, we inspected a sample of paraphrase pairs that were extracted when using Arabic X  X  language significantly different from English X  X s the pivot language. found that the paraphrase pairs in the sample set could be grouped into the following three broad categories: (a) Morphological variants . These pairs only differ in the morphological 370 (b) Approximate Phrasal Paraphrases . These are pairs that only share partial (c) Phrasal Paraphrases . Despite unreliable alignments, there were indeed a is incorrectly induced alignments between the English and Arabic words, and hence, phrases. Therefore, a good portion of subsequent work on paraphrasing using bilingual corpora, as discussed below focuses on using additional machinery or evidence to cope with the noisy alignment process. Before we continue, we believe it would be useful to draw a connection between Bannard and Callison-Burch X  X  (2005) work and that of primary language. However, Wu and Zhou rely on a pre-compiled bilingual dictionary to discover these cues whereas Bannard and Callison-Burch have an entirely data-driven discovery process.
 recently proposed an improvement that places an additional syntactic constraint on the phrasal paraphrases extracted via the pivot-based method from bilingual corpora and ity of the extracted paraphrases. 12 The syntactic constraint requires that the extracted paraphrase be of the same syntactic type as the original phrase. With this constraint, estimating the paraphrase probability now requires the incorporation of syntactic type into the equation: likelihood estimation is employed to compute the two component probabilities: have been extracted in the unconstrained approach. This leads to the familiar precision-recall tradeoff: It only extracts paraphrases that are of higher quality, but the approach has a significantly lower coverage of paraphrastic phenomena that are not necessarily syntactically motivated. To increase the coverage, complex syntactic types such as those used in Combinatory Categorial Grammars (Steedman 1996) are employed, which can help denote a syntactic constituent with children missing on the left and/or right hand sides. An example would be the complex type VP/(NP/NNS) which denotes a verb phrase missing a noun phrase to its right which, in turn, is missing a plural noun to its right. The primary benefit of using complex types is that less useful paraphrastic phrase pairs from different syntactic categories such as accurately , precise , that would have been allowed in the unconstrained pivot-based approach, are now disallowed. form of additional evidence in order to filter out phrase pairs from categories (a) and (b) as defined in the context of our manual inspection of pivot-based paraphrases above. Indeed, the authors conduct a manual evaluation to show that the syntactically constrained paraphrase pairs are better than those produced without such constraints.
However, there are two additional benefits of this technique: 1. The constrained approach might allow induction of some new phrasal 2. The effective partitioning of the probability space for a given paraphrase
We must also note that requiring syntactic constraints for pivot-based paraphrase ex-traction restricts the approach to those languages where a reasonably good parser is available.
 of pivoted English-to-English phrase pairs to generate sentential paraphrases for new sentences. Madnani et al. (2008a) combine the pivot-based approach to paraphrase acquisition with a well-defined English-to-English translation model that is then used in an (unmodified) SMT system, yielding sentential paraphrases by means of  X  X ranslating X  input English sentences. However, instead of fully lexicalized phrasal correspondences as in (Bannard and Callison-Burch 2005), the fundamental units of translation (and paraphrasing) are hierarchical phrase pairs. The latter can be extracted from the former by replacing aligned sub-phrases with non-terminal symbols. For example, given the initial phrase pair , growth rate has been effectively contained ,the hierarchical phrase pair X 1 X 2 , X 1 has been X 2 can be formed. maximum likelihood estimation during the extraction process. Such phrase pairs can formally be considered the rules of a bilingual synchronous context-free grammar (SCFG). Translation with SCFGs is equivalent to parsing the string in the source lan-guage using these rules to generate the highest-scoring tree and then reading off the 372 methods to extract such rules, to estimate their features, and to translate with them are now well established. For more details on building SCFG-based models and translating with them, we refer the reader to (Chiang 2006, 2007).
 features, the pivoting trick can be applied to infer monolingual hierarchical paraphrase are actually used as rules from a monolingual SCFG grammar in order to define an
English-to-English translation model. Features for each monolingual rule are estimated in terms of the features of the bilingual pairs that the rule was inferred from. A sentential paraphrase can then be generated for any given sentence by using this model along with an n -gram language model and a regular SMT decoder to paraphrase (or monolingually translate) any sentence just as one would translate bilingually.
 sentential paraphrases by leveraging the SMT machinery to address the noise issue.
However, although the decoder and the language model do serve to counter the noisy word alignment process, they do so only to a degree and not entirely.
 and Dolan (2004) (discussed in Section 3.3) because both treat paraphrasing as monolingual translation. However, as outlined in the discussion of that work, Quirk,
Brockett, and Dolan use a relatively simplistic translation model and decoder which leads to paraphrases with little or no lexical variety. In contrast, Madnani et al. use a more complex translation model and an unmodified state-of-the-art SMT decoder to produce paraphrases that are much more diverse. Of course, the reliance of the latter approach on automatic word alignments does inevitably lead to much noisier sentential paraphrases than those produced by Quirk, Brockett, and Dolan.
 with bilingual corpora. As with most approaches based on parallel corpora, they also start with phrase tables extracted from such corpora along with the corresponding phrasal translation probabilities. However, instead of performing the usual pivoting step with the bilingual phrases in the table, they take a graphical approach and represent graph are connected to each other if they are aligned to each other. In order to extract paraphrases, they sample random paths in the graph from any English node to another.
Note that the traditional pivot step is equivalent to a path of length two: one English phrase. By allowing paths of lengths longer than two, this graphical approach can find more paraphrases for any given English phrase.
 they take as input a number of phrase tables, each corresponding to a different pair of six languages. Similar to the single-table case, each phrase in each table is represented as a node in a graph that is no longer bipartite in nature. By allowing edges to exist between nodes of all the languages if they are aligned, the pivot can now even be a set of nodes rather than a single node in another language. For example, one could easily find the following path in such a graph:
In general, each edge is associated with a weight corresponding to the bilingual phrase translation probability. Random walks are then sampled from the graph in such a way that only paths of high probability end up contributing to the extracted paraphrases. have an adverse effect on this approach. In order to prevent this, the authors add special feature nodes to the graph in addition to regular nodes. These feature nodes represent domain-specific knowledge of what would make good paraphrases. For example, nodes phrases are added. This indicates that phrases that start and end with the same kind of words (interrogatives or articles) are likely to be paraphrases. Astute readers will make the following observations about the syntactic feature nodes used by the authors: show that they are able to generate a larger percentage of correct paraphrases compared to the syntactically constrained approach proposed by Callison-Burch (2008). They con-duct no formal evaluation of the coverage of their approach but show that, in a limited
However, they perform no comparisons of their coverage with the original pivot-based approach (Bannard and Callison-Burch 2005). 4. Building Paraphrase Corpora
Before we present some specific techniques from the literature that have been employed to evaluate paraphrase generation methods, it is important to examine some recent work that has been done on constructing paraphrase corpora. As part of this work, hu-man subjects are generally asked to judge whether two given sentences are paraphrases of each other. We believe that a detailed examination of this manual evaluation task provides an illuminating insight into the nature of a paraphrase in a practical, rather whether manual or automatic, that is used to evaluate the performance of a paraphrase generator.
 on a large scale. The Microsoft Research Paraphrase (MSRP) Corpus is a collection of 5, 801 sentence pairs, each manually labeled with a binary judgment as to whether it constitutes a paraphrase or not. As a first step, the corpus was created using a heuristic extraction method in conjunction with an SVM-based classifier that was trained to select likely sentential paraphrases from a large monolingual corpus containing news article clusters. However, the more interesting aspects of the task were the subsequent evaluation of these extracted sentence pairs by human annotators and the set of issues encountered when defining the evaluation guidelines for these annotators.
 sentence pairs that were strictly semantically equivalent or that exhibited bidirectional entailment as paraphrases, then the results were limited to uninteresting sentence pairs such as the following:
S : The euro rose above US$1.18, the highest price since its January 1999 launch.
S : The euro rose above $1.18, the highest level since its launch in January 1999. 374
S 1 : However, without a carefully controlled study, there was little clear proof that
S 2 : But without a carefully controlled study, there was little clear proof that the nations more interesting than simple lexical synonymy and local syntactic changes X  exhibited varying degrees of semantic divergence. For example:
S 1 : Charles O. Prince, 53, was named as Mr. Weill X  X  successor.

S 2 : Mr. Weill X  X  longtime confidant, Charles O. Prince, 53, was named as his successor.
S 1 : David Gest has sued his estranged wife Liza Minelli for beating him when she was
S 2 : Liza Minelli X  X  estranged husband is taking her to court after saying she threw a complex alternations, the instructions to the annotators had to be relaxed; the degree of mismatch accepted before a sentence pair was judged to be fully semantically divergent (or  X  X on-equivalent X ) was left to the human subjects. It is also reported that, given the idiosyncratic nature of each sentence pair, only a few formal guidelines were generaliz-able enough to take precedence over the subjective judgments of the human annotators.
Despite the somewhat loosely defined guidelines, the inter-annotator agreement for the task was 84%. However, a kappa score of 62 indicated that the task was overall a difficult one (Cohen 1960). At the end, 67% of the sentence pairs were judged to be paraphrases of each other and the rest were judged to be non-equivalent. able insight into what constitutes a paraphrase in the practical sense, it does have some that the two sentences in a pair must share at least three words. Using this constraint rules out any paraphrase pairs that are fully lexically divergent but still semantically equivalent. The small size of the corpus, when combined with this and other such constraints, precludes the use of the corpus as training data for a paraphrase generation or extraction system. However, it is fairly useful as a freely available test set to evaluate paraphrase recognition methods.
 proach to building a Japanese corpus containing sentence pairs with binary paraphrase judgments and attempt to focus on variety and on minimizing the human annotation cost. The corpus contains 2, 031 sentence pairs each with a human judgment indicating typology of paraphrastic phenomena (rewriting light-verb constructions, for example) and then manually create a set of morpho-syntactic paraphrasing rules and patterns describing each type of paraphrasing phenomenon. A paraphrase generation system news articles, and example paraphrases are generated for the sentences in the corpus.
These paraphrase pairs are then handed to two human annotators who create binary judgments for each pair indicating whether or not the paraphrase is correct. Using a class-oriented approach is claimed to have a two-fold advantage: 1. Exhaustive Collection of Paraphrases . Creating specific paraphrasing 2. Low Annotation Cost . Partitioning the annotation task into classes is
The biggest disadvantage of this approach is that only two types of paraphrastic phe-nomena are used: light-verb constructions and transitivity alternations (using intransi-tive verbs in place of transitive verbs). The corpus indeed captures almost all examples of both types of paraphrastic phenomena and any that are absent can be easily covered by adding one or two more patterns to the class. The claim of reduced annotation cost is not necessarily borne out by the observations. Despite partitioning the annotation task by types, it was still difficult to provide accurate annotation guidelines. This led to a significant difference in annotation time X  X ith some annotations taking almost twice as long as others. Given the small size of the corpus, it is unlikely that it may be used as training data for corpus-based paraphrase generation methods and, like the MSRP corpus, would be best suited to the evaluation of paraphrase recognition techniques. paraphrase annotations that can be used for both development and evaluation of para-phrase systems. These paraphrase annotations take the form of alignments between the words and sequences of words in each sentence pair; these alignments are analogous to the word-and phrasal-alignments induced in SMT systems that were illustrated in of different forms: one-word-to-one-word, one-word-to-many-words, as well as fully phrasal alignments. 15 corpora that we have already described elsewhere in this survey: (1) the sentence pairs judged equivalent from the MSRP Corpus: (2) the Multiple Translation Chinese (MTC) corpus of multiple human-written translations of Chinese news stories used by Pang, Knight, and Marcu (2003); and (3) two English translations of the French novel
Twenty Thousand Leagues Under the Sea , a subset of the monolingual parallel corpus used by Barzilay and McKeown (2001). The words in each sentence pair from this corpus are then aligned automatically to produce the initial paraphrase annotations that are then refined by two human annotators. The annotation guidelines required that the annotators judge which parts of a given sentence pair were in correspondence and to in-dicate this by creating an alignment between those parts (or correcting already existing 376 alignments, if present). Two parts were said to correspond if they could be substituted for each other within the specific context provided by the respective sentence pair. In addition, the annotators were instructed to classify the created alignments as either sure (the two parts are clearly substitutable) or possible (the two parts are slightly divergent either in terms of syntax or semantics). For example, given the following paraphrastic sentence pair:
S 1 : He stated the convention was of profound significance.

S 2 : He said that the meeting could have very long-term effects. the phrase pair the convention , the meeting will be aligned as a sure correspondence whereas the phrase pair was of profound significance , could have very long-term effects will be aligned as a possible correspondence. Other examples of possible correspondences could include the same stem expressed as different parts-of-speech (such as significance , significantly ) or two non-synonymous verbs (such as this is also , this also marks ). For more details on the alignment guidelines that were provided to the annotators, we refer the reader to (Callison-Burch, Cohn, and Lapata 2006).
 obtain good agreement values but they are still low enough to confirm that it is difficult for humans to recognize paraphrases even when the task is formulated differently.
Overall, such a paraphrase corpus with detailed paraphrase annotations is much more informative than a corpus containing binary judgments at the sentence level such as the MSRP corpus. As an example, because the corpus contains paraphrase annotations from these annotations and generate not only fully lexicalized phrasal paraphrases but also syntactically motivated paraphrastic patterns. To demonstrate the viability of the corpus for this purpose, a grammar induction algorithm (Cohn and Lapata 2007) is applied X  X riginally developed for sentence compression X  X o the parsed version of their paraphrase corpus and the authors show that they can learn paraphrastic patterns such as those shown in Figure 9.
 at the sub-sentential level, is extremely useful for the fostering of further research and development in the area of paraphrase generation. 5. Evaluation of Paraphrase Generation
Whereas other language processing tasks such as machine translation and docu-ment summarization usually have multiple annual community-wide evaluations using standard test sets and manual as well as automated metrics, the task of automated paraphrasing does not. An obvious reason for this disparity could be that paraphrasing is not an application in and of itself. However, the existence of similar evaluations for other tasks that are not applications, such as dependency parsing (Buchholz and Marsi 2006; Nivre et al. 2007) and word sense disambiguation (Senseval), suggests otherwise.
We believe that the primary reason is that, over the years, paraphrasing has been em-ployed in an extremely fragmented fashion. Paraphrase extraction and generation are used in different forms and with different names in the context of different applications (for example: synonymous collocation extraction, query expansion). This usage pattern does not allow researchers in one community to share the lessons learned with those from other communities. In fact, it may even lead to research being duplicated across communities.
 phrasal paraphrases (or patterns) does include direct evaluation of the paraphrasing itself: The original phrase and its paraphrase are presented to multiple human judges, along with the contexts in which the phrase occurs in the original sentence, who are asked to determine whether the relationship between the two phrases is indeed paraphrastic (Barzilay and McKeown 2001; Barzilay and Lee 2003; Ibrahim, Katz, and
Lin 2003; Pang, Knight, and Marcu 2003). A more direct approach is to substitute the paraphrase in place of the original phrase in its sentence and present both sentences the grammaticality of the new sentence (Bannard and Callison-Burch 2005; Callison-Burch 2008). Motivation for such substitution-based evaluation is discussed in Callison-
Burch (2007): the basic idea being that items deemed to be paraphrases may behave as such only in some contexts and not others. Szpektor, Shnarch, and Dagan (2007) posit a similar form of evaluation for textual entailment wherein the human judges are not only presented with the entailment rule but also with a sample of sentences that match its left-hand side (called instances ), and then asked to assess whether the rule holds under each specific instance.
 any surrounding context (Quirk, Brockett, and Dolan 2004). An intrinsic evaluation of this form must employ the usual methods for avoiding any bias and for maximizing inter-judge agreement. In addition, we believe that, given the difficulty of this task even for human annotators, adherence to strict semantic equivalence may not always be a suitable guideline and intrinsic evaluations must be very carefully designed. A number one, by utilizing the extracted or generated paraphrases to improve other applications such as machine translation (Callison-Burch, Koehn, and Osborne 2006) and others as described in Section 1.
 that of using automatic measures. The traditional automatic evaluation measures of pre-cision and recall are not particularly suited to this task because, in order to use them, a list of reference paraphrases has to be constructed against which these measures may be computed. Given that it is extremely unlikely that any such list will be exhaustive, any precision and recall measurements will not be accurate. Therefore, other alternatives semantic similarity or of paraphrase recognition, all of those metrics, including the ones discussed in Section 2, can be employed here.
 automatic measure that may be used to evaluate paraphrase extraction methods. This 378 work follows directly from the work done by the authors to create the paraphrase-paraphrastic sentence pairs with annotations in the form of alignments between their respective words and phrases. It is posited that to evaluate any paraphrase generation method, one could simply have it produce its own set of alignments for the sentence pairs in the corpus and precision and recall could then be computed over alignments instead of phrase pairs. These alignment-oriented precision (P measures are computed as follows: manual alignments for the pair s 1 , s 2 ,and N P ( s 1 , s the automatic alignments induced using the paraphrase method P that is to be evalu-ated. The phrase extraction heuristic used to compute N P alignments is the same as that employed by Bannard and Callison-Burch (2005) and illustrated in Figure 8.
 clever trick, it does require that the paraphrase generation method be capable of produc-ing alignments between sentence pairs. For example, the methods proposed by Pang,
Knight, and Marcu (2003) and Quirk, Brockett, and Dolan (2004) for generating sen-tential paraphrases from monolingual parallel corpora and described in Section 3.3 do produce alignments as part of their respective algorithms. Indeed, Callison-Burch et al. provide a comparison of their pivot-based approach X  X perating on bilingual parallel corpora X  X ith the two monolingual approaches just mentioned in terms of ParaMetric, since all three methods are capable of producing alignments.
 sentences and cannot produce any alignments, falling back on estimates of traditional formulations of precision and recall is suggested.
 violence in the Middle East that was used for evaluating the lattice-based paraphrase technique in (Barzilay and Lee 2003) has been made freely available. the original sentences for which the paraphrases were generated, the set also contains the paraphrases themselves and the judgments assigned by human judges to these paraphrases. The paraphrase-annotated corpus discussed in the previous section would also fall under this category of resources.
 evaluation (Belz 2009). As described herein, many paraphrase generation techniques serves as one form of extrinsic evaluation for the quality of the paraphrases generated by that technique. However, as yet there is no widely agreed-upon method of extrinsi-cally evaluating paraphrase generation. Addressing this deficiency should be a crucial consideration for any future community-wide evaluation effort.
 members of the community may share their ideas with their colleagues and receive valuable feedback. In recent years, a number of such fora have been made available to the automatic paraphrasing community (Inui and Hermjakob 2003; Tanaka et al. 2004;
Dras and Yamamoto 2005; Sekine et al. 2007), which represents an extremely important step toward countering the fragmented usage pattern described previously. 6. Future Trends
It is important for any survey to provide a look to the future of the surveyed task and general trends for the corresponding research methods. We identify several such trends in the area of paraphrase generation that are gathering momentum.
 sources of data for natural language processing applications, which should not be sur-prising given its phenomenal rate of growth. The (relatively) freely available Web data, massive in scale, has already had a definite influence over data-intensive techniques such as those employed for paraphrase generation (Pas  X ca and Dienes 2005). However, the availability of such massive amounts of Web data comes with serious concerns for efficiency and has led to the development of efficient methods that can cope with such large amounts of data. Bhagat and Ravichandran (2008) extract phrasal paraphrases by measuring distributional similarity over a 150GB monolingual corpus (25 billion words) via locality sensitive hashing , a randomized algorithm that involves the creation of fingerprints for vectors in space (Broder 1997). Because vectors that are more similar compared by comparing their fingerprints, leading to a more efficient distributional similarity algorithm (Charikar 2002; Ravichandran, Pantel, and Hovy 2005). We also believe that the influence of the Web will extend to other avenues of paraphrase genera-tion such as the aforementioned extrinsic evaluation or lack thereof. For example, Fujita and Sato (2008b) propose evaluating phrasal paraphrase pairs, automatically generated from a monolingual corpus, by querying the Web for snippets related to the pairs and using them as features to compute the pair X  X  paraphrasability .
 phrase generation is that of leveraging multiple sources of information to determine whether two units are paraphrastic. For example, Zhao et al. (2008) improve the sen-tential paraphrases that can be generated via the pivot method by leveraging five other sources in addition to the bilingual parallel corpus itself: (1) a corpus of Web queries similar to the phrase, (2) definitions from the Encarta dictionary, (3) a monolingual par-allel corpus, (4) a monolingual comparable corpus, and (5) an automatically constructed thesaurus. Phrasal paraphrase pairs are extracted separately from all six models and then combined in a log-linear paraphrasing-as-translation model proposed by Madnani et al. (2007). A manual inspection reveals that using multiple sources of information yields paraphrases with much higher accuracy. We believe that such exploitation of multiple types of resources and their combinations is an important development. Zhao et al. (2009) further increase the utility of this combination approach by incorporating application specific constraints on the pivoted paraphrases. For example, if the output paraphrases need to be simplified versions of the input sentences, then only those phrasal paraphrase pairs are used where the output is shorter than the input. 380 related to paraphrase generation since the former also relies on finding semantic equiv-alence, albeit in a second language. Hence, there have been numerous paraphrasing ap-proaches that have relied on different components of an SMT pipeline (word alignment, phrase extraction, decoding/search) as we saw in the preceding pages of this survey.
Despite the obvious convenience of using SMT components for the purpose of mono-lingual translation, we must consider that doing so usually requires additional work to deal with the added noise due to the nature of such components. We believe that SMT research will continue to influence research in paraphrasing; both by providing ready-to-use building blocks and by necessitating development of methods to effectively use such components for the unintended task of paraphrase generation.
 is well known that documents for health consumers are not very well-targeted to their purported audience. Recent research has shown how to generate a lexicon of semantically equivalent phrasal (and lexical) pairs of technical and lay medical terms from monolingual parallel corpora (Elhadad and Sutaria 2007) as well as monolingual comparable corpora (Del  X  eger and Zweigenbaum 2009). Examples include pairs such as myocardial infarction , heart attack and leucospermia , increased white cells in the sperm .
In another domain, Max (2008) proposes an adaptation of the pivot-based method to generate rephrasings of short text spans that could help a writer revise a text. Because the goal is to assist a writer in making revisions, the rephrasings do not always need pivot-based method. Several variants of such adaptations are developed that generate candidate rephrasings driven by fluency, semantic equivalence, and authoring value, respectively.
 come a trend since it is required to foster further research in, and use of, paraphrase extraction and generation. Although there have been recent workshops and tasks on paraphrasing and entailment as discussed in Section 5, this evaluation would be much recent NIST MT Evaluation Workshops (NIST 2009). 7. Summary
Over the last two decades, there has been much research on paraphrase extraction and generation within a number of research communities in natural language processing, in order to improve the specific application with which that community is concerned.
However, a large portion of this research can be easily adapted for more widespread use outside its particular host and can provide significant benefits to the whole field. Only recently have there been serious efforts to conduct research on the topic of paraphrasing by treating it as an important natural language processing task independent of a host application.
 extraction and generation motivated by the fact that paraphrases can help in a multi-tude of applications such as machine translation, text summarization, and information extraction. The aim was to provide an application-independent overview of paraphrase generation, while also conveying an appreciation for the importance and potential use of paraphrasing in the field of NLP research. We show that there are a large variety of paraphrase generation methods and each such method has a very different set of characteristics, in terms of both its performance and its ease of deployment. We also observe that whereas most of the methods in this survey can be used in multiple applications, the choice of the most appropriate method depends on how well the characteristics of the produced paraphrases match the requirements of the downstream application in which the paraphrases are being utilized.
 References 382 384 386
