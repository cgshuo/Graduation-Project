 We are concerned with the issue of detecting changes of clustering structures from multivariate time series. From the viewpoint of the minimum description length (MDL) prin-ciple, we propose an algorithm that tracks changes of clus-tering structures so that the sum of the code-length for data and that for clustering changes is minimum. Here we employ a Gaussian mixture model (GMM) as representation of clus-tering, and compute the code-length for data sequences us-ing the normalized maximum likelihood (NML) coding. The proposed algorithm enables us to deal with clustering dy-namics including merging, splitting, emergence, disappear-ance of clusters from a unifying view of the MDL principle. We empirically demonstrate using artificial data sets that our proposed method is able to detect cluster changes sig-nificantly more accurately than an existing statistical-test based method and AIC/BIC-based methods. We further use real customers X  transaction data sets to demonstrate the va-lidity of our algorithm in market analysis. We show that it is able to detect changes of customer groups, which correspond to changes of real market environments.
 H.2.8 [ Database Applications ]: Data mining Minimum Description Length principle, Normalized Maxi-mum Likelihood, Dynamic Model Selection, Clustering
We are concerned with the issue of clustering multi-variate data sequences. Suppose that the nature of data changes over time. We are then specifically interested in tracking changes of clustering structures, which we call clustering Figure 1.1: The Cus-tomers Change Their Purchase change detection (see also evolutionary clustering [2]). This is an important issue, for example, in marketing. Suppose that each customer may be specified by her/his purchase history for a number of products, i.e., each feature shows how large each product was bought by the customer in a day. One may conduct clustering of these data to obtain a number of clusters, each of which shows a group of customers having similar purchase behaviors. The customers X  data may change day by day. Tracking changes of such a time-varying clustering structure will lead to the understanding of how the organization of customer group changes over time. It is expected to give an important insight to marketing research. Figure 1.1 and 1.2 show an example of clustering change de-tection. Figure 1.1 shows that customers C and F change their patterns to create a new cluster. Figure 1.2 shows that new customers  X  and  X  emerge to create a new cluster.
The purpose of this paper is twofold. One is to propose a new algorithm that tracks changes of clustering structures in the sequential setting where time series data are sequentially given and the clustering must be conducted in a sequential fashion. Although changes of clustering structures do not always imply changes of the number of clusters in general, we specifically focus on clustering changes which accompny changes of the number of clusters. We employ a Gaussian mixture model (GMM) as a representation of clustering and design the algorithm on the basis of the minimum descrip-tion length (MDL) principle [10]. That is, it tracks changes of clustering structures so that the sum of the code-length for data and that for clustering changes is minimum. Here we calculate the code-length using the normalized maximum likelihood (NML) coding (see e.g.[7]). This is because the NML code-length has optimality in the sense that it achieves Shatarkov X  X  minimax criterion [13].

The other purpose is to empirically demonstrate the va-lidity of our proposed algorithm using both artificial and real data sets. As for the artificial data sets, we evaluate how well our algorithm works in comparison with cluster-ing change detection algorithms using other criteria and an existing statistical-test based algorithm [14]. As for the real data sets, we employ real customers X  transactions for a num-ber of kinds of beers to conduct clustering of customers, where each cluster is a group of customers having similar purchase patterns. We investigate how well our algorithm can detect changes of customers X  purchase patterns.
There exist a number of methods for tracking changes of clustering structures from non-stationary data sequences. Song and Wang [14] proposed a statistical-test based algo-rithm for dynamic clustering. It is an algorithm that esti-mates a GMM in an on-line manner and then conducts a statistical test to determine whether a new cluster is identi-cal to an old one or not. If it is, the new cluster is merged into the older one, otherwise it is recognized as a cluster which has newly emerged. Sato [11] proposed an algorithm for merging and splitting of clusters in a GMM based on the variational Bayes method. Note that changes of clusters are not necessarily classified into merging or splitting. Actually some members in a number of clusters may move to generate a new cluster. Sato X  X  algorithm cannot deal with the case.
Krempl et.al.[8] proposed a method of tracking clutering changes using the EM algorithm and Kalman filters. Our work is different from Krempl X  X  one in that the former is concerned with changes of the number of clusters while the latter is concerned with parameter trajectories keeping the number of clusters fixed.

Yamanishi and Maruyama [16, 17] developed a theory of dynamic model selection (DMS) for tracking changes of sta-tistical models from non-stationary data. Although this the-ory has been developed in a general setting for statistical model sequence selection, it can be applied to tracking of changes of clustering structures. The theory of DMS is based on the MDL principle and offers a strategy for selecting a model sequence so that the total sum of the code-length for the model sequence plus that for the data sequence relative to it is minimum. Sun et. al. [15] proposed a graph cluster-ing algorithm called GraphScope, in which a change point for clustering was detected when the sum of code-lengths before and after it was significantly smaller than the code-length calculated without assuming that change-point. It was also designed based on the MDL principle.
The novelty and significance of this paper are summarized as follows: 1) An extension of DMS into a sequential clustering set-ting: In [16, 17], the theory of DMS for estimating model sequences has been explored in the batch scenario where the whole data set is given at once, and the model sequence must be detected in a retrospective way. It has remained open how to extend the DMS algorithm into the sequential sce-nario where data are sequentially input and the model must be selected in a sequential fashion. In this paper we extend DMS to the sequential setting and newly propose a sequen-tial DMS algorithm . Every time data is input, it sequentially detects changes of clustering structures on the basis of the MDL principle so that the sum of the code-length for the data and that for the clustering change is minimum. This algorithm enables us to deal with the dynamics of cluster-ing structures, including X  X erging X , X  X plitting X , X  X mergence X ,  X  X isappearance X , etc. within a unified framework from the viewpoint of the MDL principle. 2) A new application of the RNML code-length to sequen-tial DMS: In the sequential DMS algorithm, the code-length for the data sequence relative to a GMM must be calcu-lated. It is crucial how to choose a method for coding. The best choice may be the NML coding, which turns out to be the optimal code-length in the sense of minimax crite-rion [10]. However, it is analytically and computationally difficult to compute the NML code-length for a GMM ex-actly. Hirai and Yamanishi gave a method for efficiently com-puting an approximate variant of the NML code-length for a GMM [5]. They have recently modified it using the renor-malizing technique to develop the renormalized maximum likelihood (RNML) coding [6]. We employ the RNML coding for the calculation of a code-length. Although it has turned out in [6] that RNML is more effective than NML in estimat-ing the number of clusters in the batch clustering scenario, this is the first work on the application of the RNML coding to the scenario of sequential clustering change detection. 3) Empirical demonstration of the superiority of the sequential DMS with the RNML code-length over the existing methods: There exist a number of methods for tracking changes of clustering structures, we employ both artificial and real data sets to empirically demonstrate that our method X  X he sequential DMS with the RNML code-length X  X utperforms other methods including Song and Wang X  X  method, AIC (Akaike X  X  information criteria) / BIC (Bayesian information criteria)-based tracking methods etc. 4) A novel application of clustering change detection into market research: This paper presents a new application sce-nario of clustering change detection into market research. We employ a real data set consisting of customers X  purchase records for a number of kinds of beers. Tracking changes of clusters of customers leads to the understanding of how customers X  purchase patterns change over time and how cus-tomers move from clusters to clusters. This analysis gives a new methodology to market research.

The rest of this paper is organized as follows: Sec.2 intro-duces the sequential DMS algorithm. Sec.3 gives experimen-tal results using artificial data sets. Sec.4 gives an application to market analysis. Sec.5 gives concluding remarks. We give a formal setting of clustering change detection. Suppose that we are sequentially given data. At each time t , we observe an n t tuple of d -dimensional data: X t = ( x 1 t , . . . , x n t t ) ( t = 1 , . . . , T ), where x it Let K t  X  N be the number of clusters at t , which we call a model . Let Z t = { 1 , 2 , . . . , K t } . Let Z t = ( z Z t ( t = 1 , . . . , T ), where z it  X  Z t ( i = 1 , . . . , n of z it as a cluster index which data x it comes from.
For example, we may consider the case where x it is data of the i -th customer at time t , specified by d -dimensional features (e.g., the j -th feature indicates how large the cus-tomer consumed the j -th brand beer). The cluster to which the i -th customer belongs is specified by an index z it .
Dynamic model selection (DMS) is a process of estimat-ing a model sequence K 1  X  X  X  K T on the basis of the MDL principle. We first give the original form of DMS that was designed for the batch scenario. Let X t  X  1 = X 1  X  X  X  X t  X  1 Z is supposed to be determined for X t once K t is given. Let is probabilistically determined by K t  X  1 . We write the code-length for K t given K t  X  1 as  X  ( K t | K t  X  1 ).
When a sequence X t ( t = 1 , . . . , T ) is given, we select an optimal sequence of models Z T = Z 1  X  X  X  Z T , K T = K 1  X  X  X  K T so that the following criterion is minimum: X where X 0 , Z 0 , and K 0 are initially given.

Eq.(1), which we call the DMS criterion, is the total code-length required for the encoding X T , Z T and K T . Hence the process for the minimizing Eq.(1) with respect to Z T and K T is considered as a strategy based on the MDL principle. Note that the minimization of Eq.(1) is a batch process in the sense that Z T and K T must be determined in a retro-spective way after the whole data X T is observed. We are rather concerned with sequential estimation of the number of clusters K t , i.e., every time X t is observed, we are to choose an optimal value of K t in a sequential fashion.
For the purpose of sequential estimation of Z t and K t , we approximate the minimum of Eq.(1) by locally minimizing it with respect to Z t and K t . That is, at each time t , letting  X  Z of cluster indices and the numbers of clusters obtained until t  X  1, respectively. We select an optimal model Z t , K t the following criterion is minimum: We call this criterion the sequential DMS criterion . The se-quential DMS algorithm is an algorithm that at each time t , takes X t as input and outputs K t and the resulting clus-tering result that minimize the criterion (2), and conducts this estimation sequentially with respect to t .
 of  X 
Z t and  X  K t so that Eq.(2) is minimum at each t . Then the sum of the minimum value of Eq.(2) is given by X This quantity is considered as an approximation of the min-imum of Eq.(1) with respect to Z T and K T .

We employ a Gaussian mixture model (GMM) as a rep-resentation of a clustering structure. Let f ( X t , Z t |  X  ) be a joint probability distribution of X t and Z t with parameter  X  . Then the likelihood for X t , Z t is given as follows: where we set  X  = (  X  j ,  X  j ,  X  j ) ( j = 1 , . . . , K t probability that z = j ,  X  j is the mean of cluster z = j , and  X  j is the variance-covariance matrix of cluster z = j . n tj the number of data in X t which fall into the cluster z = j with mean  X  j and variance-covariance matrix  X  j .
Let  X   X  ( X t , Z t ) be an estimator of  X  from X t and Z that we would like to employ the maximum likelihood esti-mator (MLE), but it is analytically intractable to compute it. Hence  X   X  may be obtained using the EM algorithm [3]. An initial value of  X  in the EM algorithm is determined by X mum likelihood (RNML) code-length [6] for X t and Z t for the calculation of  X  ( X t , Z t | X t  X  1 ,  X  Z t  X  1 : K fined as follows: where f NML is the NML distribution defined by where  X  is a hyper parameter,  X  is a parameter, and Y (  X  ) and Y  X  (  X  ) denote the ranges of X for which the integrals are taken. See Sec.2.2 for details of the RNML code-length. Below we give an algorithm for computing  X   X  ( X t , Z t ) and Z . For the sake of notational simplicity, we denote  X   X  ( X as  X   X  ( t ) . Note that the number of possible changes of cluster-ing is O( K n ). Hence we restrict possible changes of the num-ber of clusters into { X  1 , 0 , 1 } . In more details, we consider the following three cases: Case 1: K t = K t  X  1 ; the number of clusters does not Case 2: K t = K t  X  1  X  1; the number of clusters decreases. Case 3: K t = K t  X  1 + 1; the number of clusters increases. Next we consider the probability of transition from K t  X  1 K . Let  X  be a 1-dimensional parameter. Let K max be the upper bound on K t for any t . We set the model transition probability distribution as follows:
We set K max from a practical reason why an optimal K should be computed in finite memories from all possible K s. We do not simply input K max for the number of clusters because we are concerned with selecting the minimal num-ber of clusters from among those which explain the same clustering structures. Hence we require that any clusters to which no instance contributes do not exist.
 Here  X  should be estimated. We employ Krichevsky-Trofimov (KT) estimate [9] of  X  defined by where N t shows how many times the number of clusters has changed until time t  X  1.

For a Gaussian mixture, a single Gaussian distribution corresponds to a cluster. The increase of mixture size means that new clusters emerge and some members whose latent variables z correspond to other existing clusters move to the new ones. It includes as a special case  X  splitting, X  which means that a single Gaussian distribution is replaced with two ones. The decrease of mixture size means that an exist-ing cluster disappears and whose z correspond to that clus-ter move to others. It includes as a special case  X  merging, X  which means that two Gaussian distributions are no longer discriminated and are unified into a single one. Although the change of number of clusters is restricted to { X  1 , 0 , +1 } , it would easily be extended into the case where the number of
Once we have estimated the model transition probability, then we can compute the code-length for K t given  X  K t  X  1
Plugging Eq.(4) and Eq.(7) into Eq.(2) yields the following form of the sequential DMS criterion: =  X  log f NML ( X t , Z t ;  X   X  ( X t , Z t ) , K t ) P The sequential DMS algorithm outputs K t and Z t that min-imize (8) at each time t . It is given in Algorithm 1.
Next, we show how to compute the RNML code-length (4) for a GMM as in the sequential DMS criterion (8).
Let x n = ( x 1 , . . . , x n ) , x i = ( x i 1 , . . . , x be a given sequence where x i is distributed according to a Gaussian distribution with the mean  X   X  R m and the variance-covariance matrix  X   X  R m  X  m for a some positive integer m with density: f ( x ;  X ,  X ) = 1 The NML distribution for a Gaussian distribution is given by The reason why we employ this code-length is that it is op-timal in the sense that it attains Shtarkov X  X  minimax crite-rion [13]. Notice here that the normalization term in Eq.(9) diverges. According to [5], we restrict the range of data so that the MLE lies in a bounded range specified by a param-eter. Then the NML distribution is given as follows: where C ( R,  X  min ) = Z
Y ( R,  X  min ) def = { y n | ||  X   X  ( y n ) || 2  X  R,  X  ( j ) Algorithm 1 The sequential DMS algorithm STEP 1 . At time t = 1, compute  X  Z 1 and  X  K 1 from X 1 . STEP 2 . At time t = 2 , . . . , , compute  X  Z t and  X  K t for t  X  { 2 , 3 , . . . } do end for  X   X   X  min bounded, then the normalization term is also bounded.
Note here that the normalization term depends on the choice of the parameters R and  X  min . Next, we consider the optimization of the NML code-length with respect to R and  X  min . The terms including R and  X  min in the NML code-length are given by: Considering the range of parameters (10), the MLE of R and  X  min are given as follows:
We then introduce the hyper parameters  X  = (  X  1 ,  X  2 , R 1 , R 2 ) and define the renormalized maximum likelihood (RNML) distribution by where the normalization term is expanded as follows: C (  X  ) = Z Y (  X  ) = { y n | V ( of the m -dimensional ball with radius r .

Hirai and Yamanishi [6] proved that the RNML code-length for a GMM was further expanded as follows:
Theorem 2.1. [6] The RNML code-length of x n relative to a GMM is expanded as follows: where Here h k denotes the number of data belonging to the k -th cluster, and  X   X  p and  X   X  p denote the MLE of the mean and the variance-covariance matrix of the p -th cluster.
 Note that a straightforward computation of C 1 ( K, n ) and C ( K, n ) as in Eq.(12) and Eq.(13) requires O( n K ) time. Below we give methods for efficient computation of C 1 ( K, n ) and C 2 ( K, n ). As for the computation of C 1 ( K, n ), Kontka-nen and Myllym  X  aki [7] proved the following theorem:
Theorem 2.2. [7] C 1 ( K, n ) satisfies the following recur-sive formula: Hence C 1 ( K, n ) is computed in O( n + K ) time.
As for the computation of C 2 ( K, n ), Hirai and Yaman-ishi [6] gave the following result:
Theorem 2.3. [6] C 2 ( K, n ) satisfies the following recur-sive formula: C ( K +1 , n ) = X where J ( r 2 ) is as in Eq.(14). Hence C 2 ( K, n ) is computed in O( n 2 K ) time.

Combining all of the theorems as above, the RNML code-length of x n relative to a GMM is computed in O( n 2 K ) time. We employ Eq.(11) as the RNML code-length in Eq.(8). We give results on empirical comparison of the sequential DMS algorithm with other methods using artificial data sets.
First we consider variants of the sequential DMS algo-rithm in which the RNML code-length, Schwarz X  X  Bayesian information criterion (BIC) [12], and Akaike X  X  information criterion (AIC) [1] are employed as criteria for selecting the optimal number of clusters, which we call these methods RNML, BIC, and AIC, respectively.

For the RNML code-length, the sequential DMS criterion is given by Eq.(8). If the RNML code-length is replaced with BIC, the sequential DMS criterion is given as follows: where m is the dimension of each data. If the RNML code-length is replaced with AIC, the sequential DMS criterion is given as follows: where the code-length for a model change is not added to AIC because AIC has no interpretation of a code-length. All of RNML, AIC, and BIC are obtained by changing criteria in Algorithm 1. We compared these algorithms.

We introduce three criteria to evaluate the algorithms 1. AR (accuracy rate): It is defined as the average rate of 2. IR (identification rate): It is defined as the probability 3. FAR (false alarm rate): It is defined as the rate of the We prepared two artificial data sets to compare RNML, AIC, and BIC in terms of the three indices as above.
We generated Data Set 1 so that the number of clusters changed as follows: Algorithm 2 is the one which generated this data set. It created a new cluster at time 51.

For this data set, we conducted experiments 100 times by taking 100 different initial values. Figure 3.1 shows the average numbers of clusters and standard deviation per time for RNML, AIC, and BIC. Figure 3.2 shows AR s for RNML, AIC, and BIC. We observe that RNML was able to track changes of the number of clusters in a more stable way than AIC and BIC. Figure 3.3 shows AR s, IR s, and FAR s for RNML, AIC, and BIC. We observe that RNML was able to detect true change-points, and achieved significantly higher AR and IR and significantly less FAR than AIC and BIC. Figure 3.1: Data Set 1: Average
Number of Clusters Over Time Figure 3.4: Data Set 2: Average
Number of Clusters Over Time Algorithm 2 Data Generation Algorithm STEP 1 . At time t = 1, generate X 1 , Z 1 .
 STEP 2 . At time t = 2 , . . . , t 1  X  1, X t  X  X 1 +  X , Z Here  X   X  N (0 ,  X  2 I ) and I is an identity matrix. STEP 3 . At change-points t 1 , . . . , t c , generate data as fol-lows: for j = 2 to c do end for
We generated Data Set 2 so that the number of clusters changed as follows:
This data set was also generated by Algorithm 2. It cre-ated 2 new clusters at time 51 (the number of clusters in-creased from 3 to 5 at time t = 51 and an existing cluster disappeared at time 101).

Figures 3.4, 3.5, and 3.6 show the changes of the detected number of clusters and standard deviation, AR s per time, and AR s, IR s, and FAR s for RNML, AIC, and BIC.
We observe that RNML was able to identify the true numbers of clusters with higher probability and was able to detect change-points with significantly higher accuracy than AIC and BIC. Meanwhile, as the number of changes increased, the accuracy rates for BIC gradually decreased.
Note that RNML is designed assuming that the number of clusters changes by { X  1 , 0 , +1 } from time t to t + 1. We see from Figure 3.4 and 3.5 that even when the number of clusters changes by more than 1, say, 2 from time t to t + 1, RNML was able to detect the changes step by step within an for more than 66% data sets.
We evaluated change detection accuracies for RNML, AIC and BIC by varying the Kullback-Leibler divergence (KLD) between the distributions before and after a change point. In general, the larger the KLD is, the more accurately change-points can be detected. We investigated how the detection accuracies varied as the KL-divergence varied. We generated Data Set 3 so that the number of clusters changed as follows: This data set was also generated by Algorithm 2. It cre-ated a new cluster at time 51, for which we took various mean values  X  new . The KLD between the GMMs before and after a change point was not strictly calculated. Hence we approximated it by the following formula:
KLD ( f || g ) = X It is known from [4] that it gives a tight lower bound on the KLD between GMMs. Here f and g denote GMMs, f a and g b denote Gaussian components with indices a and b in f and g , and D ( f a || g b ) denotes the KLD between f a and g Figures 3.7 X 3.9 show AR s, IR s, and FAR s against the KLD, respectively. One plot corresponds to AR or IR or FAR for one mean value  X  new of a new cluster.

We observe that RNML was able to detect change-points with significantly higher accuracy than AIC/BIC in almost all plots. We further see that the larger the KLD between GMMs before and after the change-point was, the more ac-curately it was detected in terms of AR , IR , and FAR . This tendency turned out most clearly in RNML.
We employ Song and Wang X  X  algorithm [14], which we abbreviate as SW, for the comparison with our proposed method. SW is a hypothesis testing based algorithm, which is a different approach to clustering change detection from ours. Below we give its brief sketch. It first employs a GMM to conduct clustering of a newly input data, then makes sta-tistical tests to determine whether the new cluster is identi-cal to an old one or not. If it is, the new cluster is merged into the older one, otherwise it is recognized as a cluster which has newly emerged. SW is shown in Algorithm 3. Algorithm 3 Algorithm: SW [14] 1: INPUT g N ( x ): a GMM at the latest time, 2: OUTPUT g N + M ( x ): a GMM. 3: Generate a GMM a ( x ) from x N +1 , . . . , x N + M where the 4: for component k in a ( x ) do 5: for component j in g N ( x ) do 6: Make a statistical test to determine whether  X  k is 7: If  X  k =  X  j , make a statistical test to determine 8: end for 9: end for 10: Compare g N ( x ) with a ( x ) to check if both of the mean 11: If they are identical, then merge a new cluster into the 12: Any component that is not merged into any old compo-13: Output a new GMM g N + M ( x ).

See [14] for the details of statistical tests for fitness of the means and the variance-covariance matrices.

It is assumed in SW that a set of data is newly input every time. In order to meet this setting, we conducted ex-periments by setting M to be 128 , 256 , 512 , 1024 in order. In SW the number of clusters is determined on the basis of BIC [12]. Hence we also consider a variant of SW such that BIC is replaced with the RNML criterion for the number of clusters selection criterion.

In summary, we compare the following three algorithms: 1. Proposed : Our proposed algorithm. 2. SW-RNML : A statistical-test based algorithm using 3. SW-BIC : A statistical-test based algorithm using on-For comparison, we used the performance indices: AR , IR , and FAR . Tables 3.1 X 3.3 show results for the three al-gorithms. Red numerical values indicate the winners.
We observe that our proposed algorithm was the winner in terms of all the indices: AR , IR , and FAR and that it signif-icantly outperformed SW-BIC and SW-RNML. Comparing SW-BIC with SW-RNML, we see that the latter performed significantly better than the former in terms of AR and IR , while they were comparable in terms of FAR when the data size was small. In general, all of the indices for SW-BIC and SW-RNML got better as the data size increased. Meanwhile, our proposed algorithm performed well without depending on the data size so much.
We present an application of clustering change detection to market analysis. The experiment was conducted in corpo-ration with HAKUHODO, Inc. We employed a real data set provided by MACROMILL, Inc. It is specified as follows: 1. Period : November 1st 2010 to January 1st 2011. 2. Number of customers : 3185 customers. 3. Data specification : The data set consists of a number
We constructed a sequence of customers X  feature vectors as follows: A time unit is a day. At each time (day) t (=  X , . . . , T ), we denote the feature vector of the i -th customer as x it = ( x it, 1 , . . . , x it,d )  X  R d where d = 14. Each x the i -th customer X  X  consumption of the j -th brand beer from time t  X   X  + 1 to t . We denote the set of data at time t as X who purchased the products from time t  X   X  + 1 to t .
We evaluated the clustering change detection algorithms for this data set. Figures 4.1 X 4.3, and Tables 4.1 X 4.3 show the results at  X  = 14. Numerical values in Tables 4.1 and 4.2 show the average values of consumption where the average is Table 3.1: AR with Various Sample Size in Each Time
Figure 4.1: Changes of # of Clus-ters for RNML vs BIC taken over all customers within an individual cluster. Figure 4.1 shows the changes of the number of clusters per a day starting from Nov.14th, 2010.
 We first compared our proposed algorithm, the sequential DMS with the RNML criterion, with that using BIC. Fig-ure 4.1 shows the results of change detection from Nov. 14th on 2010 to Jan. 31st 2011. We see from Figure 4.1 that the sequential DMS with RNML was able to detect clustering changes that corresponded to the emergence and disappear-ance of the year X  X  ending demand. Meanwhile, the sequential DMS with BIC detected much more change-points. They in-cluded those which were not necessarily related to the year X  X  ending demand. It was so sensitive to irregularities of data that it produced too many change-points. Here is a list of other figures showing the results. The DMS with RNML detected four change points: Nov. 15th 2010, Jan. 1st 2011, Jan. 2nd 2011, Jan. 4th, and Jan. 22nd 2011. For example, as for the clustering change from Dec. 31st 2010 to Jan. 2nd 2011, Table 4.1 shows the average values of consumption for each beer within individual clusters before the change, while Table 4.2 shows those after the change. In Tables 4.1 and 4.2, c 1 , c 2 , . . . denote cluster 1, cluster 2, . . . , respectively.
Table 4.3 shows how customers moved from clusters to clusters from Dec. 31st 2010 to Jan. 2nd 2011. In Table 4.3, each row shows a cluster on Dec. 31st 2010 while each column shows a cluster on Jan. 2nd 2011. For example, it is shown that 139 customers moved from the old cluster 1 on Dec. 31st 2010 to the new cluster 5 on Jan. 1st 2011.
We see from Tables 4.1 X 4.3 that most of customers belong-ing to the old cluster 3, which had the largest consumption on Dec. 31st, moved into the new clusters 3 and 4.
The cluster 5 newly emerged on Jan. 1st. This cluster had large consumption for Beer A and Third Beer. Most of customers belonging to this cluster came from the old clusters 1 and 2, where the customers belonging to the old cluster 1 had relatively large consumption of Beer(A,B) and Premium-beer(A,B) while the customers belonging to the old cluster 2 had relatively large consumption of other types of beer (Third Beer, LM-Beer, Off-Beer). We see that many of customers belonging to the old clusters 1 and 2 changed their patterns to purchase Beer A and Third Beer at the years X  end. Through this analysis, it has turned out that the clustering change detection leads to the understanding of how customer groups changed and how customers moved from clusters to clusters. It was validated by domain experts.
We have considered the clustering change detection is-sue. In it we have proposed the sequential DMS algorithm that sequentially tracks changes of clustering structures. The key ideas of the algorithm are; 1) extending DMS (dynamic model selection) into a sequential clustering scenario, and 2) the use of the RNML (renormalized maximum likelihood) code-length in the DMS criterion. The proposed algorithm enables us to deal with merging, splitting, emergence, disap-pearance, etc. of clusters from a unifying view of the MDL principle. We have empirically demonstrated using artificial data sets that our algorithm is able to detect cluster changes much more accurately than AIC/BIC-based methods and the existing statistical-test based method. We have used real customers X  transaction data sets to demonstrate the validity of our algorithm in market analysis.
 Table 4.3: Movement of Customers from Dec. 31st to Jan.1st [1] H. Akaike. A new look at the statistical model [2] D.Chakrabrti, R.Kumar. Evolutionary clustering. [3] A. P. Dempster, N. M. Laird, and D. B. Rubin. [4] J. Hershey and P. Olsem. Approximating the kullback [5] S. Hirai and K. Yamanishi. Efficient computation of [6] S. Hirai and K. Yamanishi. Normalized maximum [7] P. Kontkanen and P. Myllym  X  aki. A linear time [8] Z. G. Krempl and M.Spiliopoulou. Online clustering of [9] R. E. Krichevsky and V. K. Trofimov. The [10] J. Rissanen. Stochastic Complexity in Statistical [11] M. Sato. Online model selection based on the [12] G. Schwarz. Estimating the dimension of a model. [13] Y. M. Shtarkov. Universal sequential coding of single [14] M. Song and H. Wang. Highly efficient incremental [15] J. Sun, S. Papadimitriou, P. S. Yu, and C. Faloutsos. [16] K. Yamanishi and Y. Maruyama. Dynamic syslog [17] K. Yamanishi and Y. Maruyama. Dynamic model
