 This paper describes a novel probabilistic method of mea-suring semantic similarity for real-world noisy short texts like microblog posts. Our method adds related Wikipedia entities to a short text as its semantic representation and uses the vector of entities for computing semantic similari-ty. Adding related entities to texts is generally a compound problem that involves the extraction of key terms, nding related entities for each key term, and the aggregation of related entities. Explicit Semantic Analysis (ESA), a popu-lar Wikipedia-based method, solves these problems by sum-ming the weighted vectors of related entities. However, this heuristic weighting highly depends on the rule of majority decision and is not suited to short texts that contain few key terms but many noisy terms. The proposed probabilis-tic method synthesizes these procedures by extending naive Bayes and achieves robust estimates of related Wikipedia entities for short texts. Experimental results on short text clustering using Twitter data indicated that our method out-performed ESA for short texts containing noisy terms. H.3.1 [ INFORMATION STORAGE AND RETRIEVAL ]: Content Analysis and Indexing| Linguistic processing, The-sauruses ; I.7.1 [ DOCUMENT AND TEXT PROCESS-ING ]: General Algorithms, Experimentation Semantic similarity; Semantic representation; Naive Bayes; Short text clustering
Recently, a great deal of attention has focused on analyz-ing short texts, such as microblogs, search queries, search results, ads, and news feeds. Semantic similarity measure-ments between short texts are especially substantial for vari-ous applications including text clustering [2] and text classi-cation [26]. To measure semantic similarity between short texts, enriching semantics of short texts is a vital technology because these types of text do not contain enough informa-tion for semantic similarity measurements. For example, two short texts Apple's new product and iPhone 5 was launched refer to similar topics, even though there are no term co-occurrence between them. Measuring their semantic simi-larity requires additional semantics (i.e., external knowledge sources).

Wikipedia 1 , a large-scale collaborative encyclopedia, is a promising knowledge source that can be used to add seman-tics of short texts. When compared to the Web, Wikipedia has more re ned link structures and less noise data, and it widely covers named entities, domain speci c entities, and new entities. Thanks to Wikimedia Foundation's contribu-tion to provide the dump data online 2 , it can easily be uti-lized. It has been used as a knowledge source to accomplish various tasks on handling short texts [2, 5, 19].
Explicit Semantic Analysis (ESA) [7], a method that is used to nd related Wikipedia entities as semantics of texts, is designed for various purposes rather than a speci c task. Additional semantics by Wikipedia entities represent the topic of the texts, enabling accurate semantic similarity mea-surements for short texts. Adding semantics of texts con-sists of several subproblems: extracting key terms, nding related entities for each key term, and aggregation of related entities. In order to solve these subproblems, ESA sums the weighted vectors of related entities for each keyword accord-ing to the majority rule.

ESA's weighting mechanism, however, is not suited to real-world noisy short texts. To nd related Wikipedia en-tities for a noisy short text, it is important to focus on key-words while ltering out noisy words. Since ESA relies on the majority rule, it does not have a function to lter out noisy words. ESA does not work well if a short text contain few keywords and many noisy words. h ttp://www.wikipedia.org/ http://dumps.wikimedia.org/
The purpose of this work is to use Wikipedia entities as the semantics of short texts to measure the semantic similar-ity. In order to achieve this objective, we adapt Wikipedia-based techniques to de ne the probabilistic scores for key term extraction and related entity nding, and introduce extended naive Bayes (ENB) to aggregate related entities. Our method is more robust for noisy short texts than E-SA because its weighting mechanism is based on the Bayes' theorem.
Short texts vary from traditional documents in their brevi-ty and sparsity, which makes statistical approaches to short texts less effective. Thus, enriching the semantics of short texts using external knowledge, such as Wikipedia, is essen-tial.

Most of the work leveraged Wikipedia for speci c tasks on short texts. Ferragina et al. [5] proposed a simple and fast method for entity disambiguation (entity linking) for short texts using Wikipedia. Meij et al. [14] also tackled en-tity disambiguation by using various features (e.g., anchor texts, links between articles) derived from Wikipedia for ma-chine learning. Phan et al. [19] utilized hidden topics ob-tained from Wikipedia for learning the LDA [3] classi er of short texts. Hu et al. [8] exploited features from Wikipedia for clustering of short texts. Their work demonstrated that Wikipedia was effective as an external knowledge source.
Contrast to the work described above, research on repre-senting semantics of short texts was used for multiple pur-poses [7, 15]. Especially, Explicit Semantic Analysis (ESA) [7] has been widely used because of its availability and ver-satility. ESA was developed for computing word similarity as well as text similarity written in natural languages. ESA builds a weighted inverted index that maps each word into a list of Wikipedia articles in which it appears, and computes the similarity between vectors generated from two words or texts.

Song et al. [24] illustrated the availability of ESA for short text clustering (as a comparative method), i.e., measuring semantic distance (semantic similarity) between short texts using ESA. Banerjee et al. [2] also employed a similar ap-proach to ESA for the purpose of clustering short texts. Sun et al. [26] utilized ESA to classify short texts with a sup-port vector machine (SVM), which is a supervised machine learning technique.

Thus, ESA has been demonstrated to be effective for mea-suring semantic similarity for short texts. However, ESA has a problem in its weighting system when it comes to analyz-ing real-world noisy short texts. We will describe this in Section 3.
Explicit Semantic Analysis (ESA) [7] is a method to rep-resent semantics of short texts for semantic similarity mea-surements. ESA builds a weighted inverted index that maps each word into a list of Wikipedia articles in which it ap-pears, and computes the similarity between vectors generat-ed from two texts or words. To make a short list of related Wikipedia entities for a text that contain multiple words, ESA sums the weighted vectors of related entities for each word. This simple weighting works well for long texts such as news articles and web pages because the scores of related entities belonging to the most dominant topic of the text naturally increase based on the majority rule.

However, we posit that ESA is not well-designed for nd-ing related Wikipedia entities for real-world noisy short texts. Noisy short texts may contain few key terms and many noisy terms, and the majority rule ESA has employed may not work well. It is important to focus on key terms as well as lter out noisy terms to correctly derive related entities from short texts.

Noisy terms cannot be ltered out statically because a noisy term in a text can be a key term in another text de-pending on the contexts. For example, general term tree may be a noisy term in many texts, but it can be a key term that indicates a data structure in the domain of computer sciences. A plant tree can also be a key term in the topic of botany. Named entities may be noisy terms in some cases. City name Liverpool may only explain John Lennon 's birth-place and the main topic of the text may be popular music. Uniformly giving low scores to such noisy terms does not lead to a resolution of the problem.

Another serious issue of ESA is that strongly related enti-ties to a single term tend to remain in the top of the output list. This situation is undesirable especially when the text contains ambiguous terms. For example, iPhone is strongly related to term Apple but is not related to short text Apple is a tree . In this case, summed scores of related entities that are related to both Apple and tree (e.g. Fruit and Golden Delicious ) should exceed the score of iPhone . However, with ESA's weighting mechanism, this hardly occurs if the differ-ential of the scores of Apple and tree is large. Related entities of the text are then almost derived from only Apple . Even if the scores of Apple and tree are similar, it is likely that few entities related to both Apple and tree are ranked higher than iPhone . That is, related entities belonging to different topics (one of them is actually wrong) coexist in the top of the output list, resulting in deteriorating the performance of semantic similarity measurements.
To achieve robust nding of related Wikipedia entities for short texts to measure semantic similarity, we propose a method that adapts Wikipedia-based techniques to de ne probabilistic scores and integrates the scores based on the Bayes' theorem. As described in Section 3, ESA is not suited to real-world noisy short texts because of its simple weight-ing mechanism of summing weighted vectors. Our method addresses the problem by extending the naive Bayes method, which enable us to emphasize key terms while lter out noisy terms.

Our method obtains probabilistic scores for key terms and related entities by analyzing Wikipedia. After that, our method synthesizes these probabilities and computes the output vector of related entities using extended naive Bayes (ENB). To measure the semantic similarity between two texts, the similarity of their related entity lists ranked by P ( c j T ), probability that entity c is related to a set of key terms T , is computed using cosine or other metrics.
Our method solves the compound problem of key term extraction, related entity nding, and the aggregation of re-lated entities in a probabilistic schema. In this section, we explain the probabilistic scores of key terms and related en-tities, as well as the prior probabilities of entities. In Section 4.2, we describe how to aggregate related entities for each key term using the probabilities introduced in this section.
P ( t 2 T ), which is the probability that term t in a text, T , is a key term, is computed using anchor texts in Wikipedia articles [16]. According to the editorial policy of Wikipedia called wikify 3 , a speci c term in Wikipedia articles that indi-cates another article (entity) should be linked to the article. Here, the more often a term is selected as an anchor text for a corresponding article, the more likely that the term is important. Based on this heuristics, we use the rate of arti-cles that contain a term as an anchor text. According to the literature [16], this method of extracting key terms outper-formed other common techniques, such as TFIDF [21] and the 2 independence test [13].

Given that CountArticlesHavingAnchortexts ( t ) is the number of articles that contain term t as an anchor text and CountArticlesHavingT erms ( t ) is the number of arti-cles that contain term t , the probability is computed as P ( t 2 T ) CountArticlesHavingAnchortexts ( t ) In order to avoid black or white probabilities (i.e., 0 or 1), Laplace Smoothing [11] is introduced.
P ( e j t ), which is the probability that term t is linked to entity e , is computed using anchor texts and their destina-tion article [18]. Using the policy of wikify , a speci c term that indicates an entity is linked to a corresponding article. This term then becomes an anchor text for the entity. The relationship between terms and entities can be extracted by analyzing anchor texts. Given that CountAnchortexts ( t; e ) is the number of times that the anchor text t is linked to entity e , the probability is as follows: E denotes a set of all Wikipedia entities.

P ( c j e ), which is the probability that entity c is related to entity e , is computed based on incoming and outgoing links of e . We introduce a link-based method that sim-ply uses the number of links between e and c . Given that CountLinks ( e; c ) is the number of links (regardless of in-coming or outgoing links) between two entities, e and c , the probability is computed as
By using Equations (2) and (3), P ( c j t ), the probability that entity c is related to term t , is computed as
P ( c ), which is the prior probability of entity c , means the generality of c . Because our method computes P ( c j using the number of links between articles, we also use it for h ttp://en.wikipedia.org/wiki/Wikipedia:WikiProject _ Wikify determining prior probability. Namely, prior probability is in proportion to the number of incoming and outgoing links. Given that CountLinks ( c ) is the number of incoming and outgoing links that an entity c has, the prior probability can be computed as
Based on the probabilistic scores extracted from Wikipe-dia, we attempt to integrate the information to nd related Wikipedia entities for texts. First, we start by assuming multiple key terms are input. In other words, we calculate P ( c j T  X  ) for a set of key terms T  X  = f t 1 ; :::; t naive Bayes [24]. Given that each term, t , is conditionally independent, the probability is speci cally computed by P ( c j T  X  = f t 1 ; :::; t K g ) =
Next, we tackle the case where members of T cannot be observed, i.e., it is not clear whether a term in a text is a key term or not. Because candidates of the key term in a text can be determined using anchor texts and titles of Wikipedia, this assumption is the same as what we have considered in this work. One of the possible approaches to this challenge may be the two-phase method that rst deter-mines key terms and then applies conventional naive Bayes to them. However, this approach gives rise to the problem of how key terms are determined. Threshold-based methods can be employed to select or discard terms, although this requires parameter adjustments. Adjusting thresholds is di-fficult because optimal thresholds may change along with texts.

Instead of using threshold-based methods, we propose ex-tended naive Bayes (ENB) 5 . ENB can be applied to a set whose members are probabilistically determined. Given a set of key terms T , P ( c j T  X  ) is computed for all possible states T  X  . Figure 1 outlines an example of ENB for a set of candidates of the key terms t 1 ; :::; t K . ENB is used to compute P ( T = T  X  ), which is the probability that a set of key terms, T , will become state T  X  . It then computes P ( c j T  X  ) for each state T  X  and sums up P ( c j T  X  ) weighted by P ( T = T  X  ).
 Given that each term, t , is conditionally independent, P ( T = T  X  ) is computed as P ( T = T  X  ) =
In this paper, we use T for a set of key terms whose mem-bers cannot be observed and T  X  (with an apostrophe) for a set of key terms whose members can be observed.
The basic idea of ENB was originally proposed in the report [23]. Figure 1: Extended naive Bayes (ENB) for set of key terms whose members cannot be observed.
 Therefore, related entities are estimated by using the ENB in Figure 1 utilizing Equations (6) and (7).
 Here, j T  X  j denotes the number of key terms contained in T . The computation of Equation (8) requires exponential time for the number of terms K because it separately applies conventional naive Bayes to each state T  X  . Equation (8) can be decomposed by dual cases t k 2 T  X  and t k = 2 T  X  as  X  The numerator of Equation (9) is then decomposed for each t to efficiently compute it.  X  = (  X  = = As a result, the following expression is derived. P ( c j T ) = Consequen tly, Equation (10) replaces each probability P ( c in the conventional naive Bayes (Equation (6)) with a lin-ear combination of P ( c j t k ) and its prior probability P ( c ). In the equation, P ( t k 2 T ) plays a role as the weight for smoothing. That is, ENB naturally includes the smoothing mechanism obtained by P ( t k 2 T ) to focus on the key terms while ltering out noisy terms.
 On the assumption that there is at least one key term, P ( t k 2 T ) can be normalized by dividing it by the maxi-mum probability. Also, P ( c j T ) may requires the normaliza-tion because P ( t k 2 T ), P ( c j t k ), and P ( c ) are approximate probabilities. The similarity of related entity lists ranked by P ( c j T ) obtained from two texts is computed using cosine or other metrics to measure semantic similarity.
We evaluated our method and ESA with a variety of pa-rameter combinations on benchmarks of short text seman-tic similarity. We particularly leveraged Pilot short text se-mantic similarity benchmark dataset [10], which contains 30 sentence pairs and their similarity scores rated by 32 partic-ipants. Additionally, we created three datasets using Con-ceptSim [22] and WordNet [4]. We followed the manner of the literature [10] to build short text similarity datasets, i.e., replaced a synset (a single meaning of a word) of WordNet with its de nition. As the result, we obtained three data-sets based on the gold standards of word similarity datasets: MC [17], RG [20], and WordSim (WS) [1, 6]. Spearman's rank correlation coefficient is used to measure the similarity scores with those by human judgments.
 We examined 16 combinations of parameter settings of ESA: keyphraseness [16] (KEY) or IDF [21] for key term ex-traction, count of anchor texts (A) or logarithmic count of anchor texts (logA) for linking a key term to entities, count of links (L) or logarithmic count of links (logL) for nding related entities from an entity, and cosine-normalized scores of related entities (COS) or unnormalized scores. More-over, we also implemented original ESA according to the Gabrilovich's work [7]. Since our method and ESA generat-ed a ranked list of entities as output, we used the top 100, 200, 500, 1,000, 2,000, and 5,000 entities to compute simi-larity scores 6 . We then marked the best score among them per method.

Table 1 shows the results of semantic similarity measure-ments on the benchmark datasets. Our method outper-formed ESA with KEY-A-L (the parameter settings are the same as our method's) for all the datasets. Compared to original ESA, the performance of our method was marginal-ly ne. However, by adjusting parameters, ESA was able to achieve higher scores than our method (e.g. IDF-A-logL-COS, IDF-logA-logL-COS). These datasets consist of formal texts and ESA is accurate enough to measure semantic simi-larity between the short texts. This means that our method has no signi cant advantage in these datasets because it is not needed to lter out noisy terms to correctly grasp the topic of the texts.
 In spite of these results, our method is more effective than ESA for real-world noisy short texts. In Section 5.2, we will demonstrate that our method can surpass ESA with the best parameter settings that are adjusted based on these datasets (i.e., IDF-A-logL-COS).
W e did not evaluate the methods when the number of relat-ed entities was less than 100, because the similarity scores became 0 for many unrelated sentence pairs. Spearman's rank correlation coefficient cannot be measured if not a few scores are the same. T able 1: Spearman's rank correlation coefficient for short text similarity datasets.
 ESA Original ESA [7] 0.797 0.833 0.698 0.562
Our method 0.857 0.840 0.717 0.573
We carried out an experiment on clustering of Twitter messages (tweets). In the same clustering algorithm, the performance of clustering depends on how the semantic dis-tance (semantic dissimilarity) is measured. Namely, the per-formance of semantic similarity measurements can be evalu-ated using clustering. We employed k-means clustering [12] as the clustering algorithm.

We utilized the hashtags , which are de ned by Twitter, to create large-scale datasets (i.e., ground truth) for clustering tasks. Hashtags are tags, such as #MacBook and #NFL , that Twitter users intentionally add to their tweets in order to clarify the topic of the tweet [9]. It has actually been demonstrated that hashtags are used to create datasets for short text clustering [24]. In our experiment, we carefully selected independent, unambiguous hashtags (topics) shown in Table 2 so that each cluster contained a maximum of ap-propriate tweets. Note that collected tweets still contained many ambiguous terms.

The procedure for constructing the dataset was as follows: 1) search tweets by using prede ned hashtags and store those written in English, 2) delete tweets that contain more than one prede ned hashtag, 3) delete retweets (tweets starting with RT ), 3) remove URLs in tweets, 4) remove hashtags at the end of tweets (to hide explicit clues for the topic) and the \#" of hashtags not at the end of tweets, and 5) delete tweets that are under four words. Table 3 represents the statistics of the dataset that was used in the evaluation.
We employed a bag-of-words model (BOW) as the base-line and ESA as the comparative method. For BOW, we used all words except stop words in short texts to compute the semantic similarity. For ESA, we employed two param-eter settings: the same parameter settings as our method's (ESA-same) and the best parameter settings for the bench-mark datasets in Section 5.1 (ESA-adjusted, i.e., IDF-A-Table 2: Prede ned hashtags to build the dataset (the number of collected tweets). #MacBo ok (1,251) #Silverlight (221) #VMWare (890) #MySQL (1,241) #Ubuntu (988) #Chrome (1,018) #NFL (1,044) #NHL (1,045) #NBA (1,085) #MLB (752) #MLS (981) #UFC (991) #NASCAR (878) logL-COS). We used the top 10, 20, 50, 100, 200, 500, 1,000, 2,000, and 5,000 related entities for measuring semantic sim-ilarity as for ESA and our method. We did not use combined methods of BOW and Wikipedia-based methods (ESA and our method) because the purpose of this experiment was to assess the performance of each method for semantic similar-ity measurements.

We employed normalized mutual information (NMI) [25] as the metric to evaluate the performance. NMI expresses scores based on information theory and is regarded as one of the most reliable metrics for clustering. NMI scores are between 0 and 1, and larger scores indicate better results. we conducted k-means clustering 20 times with random initial clusters and we recorded the average score for each method. Figure 2 shows the results of tweet clustering (maximum NMI scores are described in gures). In comparison with the bag-of-words (BOW) method, ESA-adjusted and our method achieved good performance because they were able to nely enhance the semantics of short texts to increase the co-occurrences of Wikipedia entities in tweets. From Table 3, the average number of words per tweet is less than 15. This indicates that there are few co-occurrences of terms in tweets and the BOW method often fails to measure semantic similarity between tweets. The same tendency can be ob-served in the literature [24], which has reported that BOW or statistical approaches, such as LDA [3], are ineffective for computing semantic distance in short text clustering. Gen-erated features by ESA-same were not superior to BOW because of inappropriate parameter settings.

Of the Wikipedia-based approaches, our method outper-formed ESA even if the parameters of ESA are well-adjusted. We applied t-test to compare the best performance between our method and ESA methods, and it was signi cantly dif-ferent with the p-value &lt; 0 : 01. From the results, our method is more suited to real-world noisy short texts than ESA.
A good example that illustrates the effectiveness of our method versus ESA is Kobe's 48 will be the highlight of the Lakers season lol (the topic is NBA ). Of all the terms in this sentence, Kobe (indicating NBA player Kobe Bryan-t) and Lakers (indicating NBA team Los Angeles Lakers) are key terms, and highlight and lol are likely to be noisy terms. Additionally, Kobe is highly ambiguous as it usually denotes a Japanese city Kobe . The output of ESA-adjusted or ESA-same contained many unrelated entities that were derived from the noisy or ambiguous terms. Only the pro-posed method accurately derived NBA-related entities such as Kobe Bryant and Los Angeles Lakers accomplishments and records by ltering out noisy terms and amplifying re-lated entities that were related to multiple key terms.
Using the Bayes' theorem, we proposed a novel probabilis-tic method to nd related Wikipedia entities for short texts to measure semantic similarity. Adding related entities to texts is a compound problem including key term extraction, related entity nding, and the aggregation of related enti-ties. To address the compound problem, our method derives the probabilistic scores for key term extraction and related entity nding using Wikipedia, and aggregates the weighted vector of related entities for each key term by using extend-ed naive Bayes (ENB). The performance of the proposed method on short text similarity datasets was inferior to that of Explicit Semantic Analysis (ESA) with well-adjusted pa-rameters, which solved the compound problem using the heuristic weighting mechanism of summing scores. Howev-er, from the experimental results using real-world Twitter data, we con rmed that our method was more robust in measuring semantic similarity for noisy short texts than the ESA.

We plan to develop our method in future work to deal with multi languages at once. Our method uses Wikipedi-a articles (entities) and does not use any NLP technique. Because Wikipedia articles of different languages are con-nected each other using inter-language links, our method can be multilingualized using them. We will explore how to incorporate probabilistic scores of language mapping into our probabilistic method.
This work was supported in part by CPS-IIP Project (In-tegrated Platforms for Cyber-Physical Systems to Acceler-ate Implementation of Efficient Social Systems (FY2012 -FY2016)) in the research promotion program for national level challenges \research and development for the realiza-tion of next-generation IT platforms" by the Ministry of Ed-ucation, Culture, Sports, Science and Technology (MEXT).
