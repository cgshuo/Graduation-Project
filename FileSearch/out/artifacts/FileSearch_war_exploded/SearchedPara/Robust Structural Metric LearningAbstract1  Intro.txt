 Daryl K. H. Lim dklim@ucsd.edu Brian McFee brm2132@columbia.edu Gert Lanckriet gert@ece.ucsd.edu Metric learning algorithms produce a (linear) trans-formation optimized to yield small distances between similar pairs of points, and large distances between dissimilar pairs of points (Xing et al., 2003; Weinberger et al., 2006; Davis et al., 2007). The transformation is usually optimized for a specific task, such as visu-alization or k -nearest-neighbor classification. More generally, structural metric learning algorithms opti-mize for the prediction of structured outputs induced by the learned transformation, such as nearest-neighbor rankings (McFee &amp; Lanckriet, 2010) or connectivity graphs (Shaw et al., 2011).
 In the usual setting, data is provided as a collection of vectors x i  X  R d , and the squared distance function ( x i  X  x j ) T W ( x i  X  x j ) is parameterized by a positive semidefinite matrix W , which is optimized to satisfy a set of pairwise similarity or structural constraints. It is often desirable that W be low-rank , as this limits model complexity, exploits correlations between fea-tures, and often improves predictive accuracy. When W is low-rank, distances are equivalently computed in a low-dimensional subspace of R d , effectively providing output sparsity ( i.e. , sparsity after transformation) and allowing for efficient storage and retrieval. Previous work on sparse metric learning focuses on this notion of output sparsity (Ying et al., 2009).
 However, input sparsity can be equally important to achieving good performance: in general, the input data may contain a significant amount of irrelevant fea-tures which should be detected and suppressed by the learning algorithm. In such cases, there will always exist a linear transformation which suppresses the non-informative features ( i.e. , by setting the corresponding entries of W to 0) and one would hope that a met-ric learning algorithm should find it. Unfortunately, existing algorithms frequently fail to find such a trans-formation, and their performance degrades rapidly as the number of noisy features increases.
 In this paper, we propose a robust extension to the metric learning to rank (MLR) algorithm (McFee &amp; Lanckriet, 2010). The proposed method imposes a group sparsity penalty on the learned metric to pro-mote input sparsity, and a trace penalty to promote output sparsity. We derive an efficient learning algo-rithm based upon the 1-slack structural SVM (Joachims et al., 2009) and the alternating direction method of multipliers (ADMM) (Boyd et al., 2011). Our experi-ments demonstrate that even a very simple noise model can dramatically reduce performance of existing meth-ods, while the proposed method correctly identifies and suppresses noisy features. 1.1. Related work Supervised metric learning is a well-studied problem, of which some representative methods are information-theoretic metric learning (ITML) (Davis et al., 2007), large margin nearest neighbor (LMNN) (Weinberger et al., 2006), and the method of Xing et al. (2003). However, many of these methods do not explicitly regularize for low rank ( i.e. , sparsity in the projected space) or sparsity of input features. For example, the log det regularizer of ITML constrains W to be strictly positive definite, which in practice often results in high-rank solutions which necessarily depend on all input features, including noisy ones. LMNN may output a low-rank metric, though it does not explicitly regularize for it.
 Ying et al. (2009) proposed a mixed-norm regularized metric learning algorithm to achieve dimensionality reduction ( i.e. , output sparsity). However, their for-mulation applies the regularization after a (dense) ro-tation of the input feature space, and therefore does not promote sparsity with respect to the input features. Low-rank regularization is used in many metric learn-ing algorithms (Shen et al., 2009; McFee &amp; Lanckriet, 2010; Huang et al., 2011), but these methods do not regularize for feature sparsity.
 Rosales &amp; Fung (2006) regularize for sparsity in the in-put space by minimizing P i,j | W ij | . Their formulation additionally restricts W to the set of diagonally domi-nant matrices, which allows for an efficient formulation as a linear programming problem, but tends to favor high-rank solutions.
 Other robust metric learning formulations have been proposed in which the similarity constraints or labels have been corrupted, rather than the features. Huang et al. (2010) developed an algorithm for the case where a fraction of the similarity constraints are corrupted. Similarly, Zha et al. (2009) leverage auxiliary data to learn a metric when the constraint set is sparse. 1.2. Preliminaries Let S d and S d + denote the sets of d  X  d , real-valued, symmetric and positive semidefinite matrices. Let  X 
S [ x ] denote the orthogonal projection of x onto a convex set S . For matrices A,B , denote the Frobe-nius inner product by  X  A,B  X  F  X   X  = P i,j A ij B ij , and norm by k A k F  X   X  = p  X  A,A  X  F . Finally, for x  X  R [ x ] +  X   X  = max(0 ,x ). In this paper, we will build upon the metric learning to rank (MLR) algorithm (McFee &amp; Lanckriet, 2010), a variant of the structural SVM (Tsochantaridis et al., 2005) which optimizes W  X  S d + to minimize a ranking loss function  X  : Y  X Y  X  R + ( e.g. , decrease in mean average precision) over permutations Y induced by distance.
 MLR can be expressed as the following convex opti-mization problem: Here, X  X  R d is the training set of n points; Y is the set of all permutations over X ; C &gt; 0 is a slack trade-off parameter;  X  : R d  X Y  X  S d is a feature encoding of an input-output pair ( q,y ); and  X ( y q ,y )  X  [0 , 1] is the desired margin, i.e. , loss incurred for predicting a ranking y rather than the true ranking y q . The feature map  X  (Joachims, 2005) is designed so that  X  W, X  ( q,y )  X  F is large when the ranking of X induced by distance from q agrees with y , and small otherwise. For a query q  X  R d with relevant set X + q  X  X and irrelevant set X  X  q  X  X  ,  X  is defined by The regularization term tr ( W ) in (1) is used as a convex surrogate for rank ( W ) to promote low-rank solutions. However, because it ignores the off-diagonal elements of W , it does not necessarily promote feature sparsity, and performance can degrade with the addition of non-informative features. 2.1. Robust MLR Ideally, we would like the learning algorithm to produce a metric W which relies only upon informative fea-tures. More precisely, if some input dimension i is non-informative, then the corresponding rows and columns of W should suppress the feature, i.e. , W i  X  = W  X  i = 0. In contrast, sparsity should not be enforced for rows corresponding to informative features, as this would limit the ability of the algorithm to exploit correlation between informative features, and reduce output spar-sity. This suggests a natural row (or column) grouping Algorithm 1 Robust metric learning to rank (R-MLR) 1: A X  X } 2: repeat 4:  X   X  max 5: b  X   X  0, b  X   X  0 6: for q = 1 , 2 ,...,n do 7: y 0  X  argmax 8: b  X   X  b  X  + 1 / n  X ( y q ,y 0 ) 9: b  X   X  b  X  + 1 / n (  X  ( q,y q )  X   X  ( q,y 0 )) 10: end for 11: A X  X  X  n ( b  X  , b  X ) o 12: until b  X   X  X  W, b  X   X  F  X   X  + of the entries of W when enforcing sparsity, so that rows corresponding to informative features may be dense, but sparsity is enforced over rows to avoid relying upon too many features.
 As in the group lasso, row-sparsity can be promoted by mixed-norm regularization (Yuan &amp; Lin, 2006): This leads to our Robust Metric Learning to Rank (R-MLR) formulation: R-MLR balances the trade-off between input and out-put sparsity through a hyper-parameter  X  &gt; 0, which may be tuned by cross-validation.
 As there are a super-exponential number of constraints in 2, we implement the 1-slack cutting-plane method (Joachims et al., 2009) to obtain Algorithm 1. Al-gorithm 1 approximates (2) by alternately solving a convex optimization problem (step 3) over a small set A of active constraints, and updating A with the con-straints most violated by the resulting W (steps 5 X 10). The process repeats until the most-violated constraint (and hence, all other constraints) is satisfied to within some specified &gt; 0 of the loss on the active set A . The original MLR implementation solved for W via projected sub-gradient descent (McFee &amp; Lanckriet, 2010). While this approach could be applied for R-MLR as well, projecting each iterate back onto S d + is computationally expensive: O ( d 3 ) for each spectral decomposition and thresholding operation. Instead, we will use the alternating direction method of multipli-ers (Boyd et al., 2011) to efficiently optimize (3). First, we transform the optimization problem (Algo-rithm 1, step 3) into an equivalent problem: min Introducing Lagrange multipliers  X  V ,  X  W  X  S d , we obtain the augmented Lagrangian:
L  X  ( W,V,Z,  X  W ,  X  V ) = f ( W ) + g ( V ) + h ( Z ) where  X  &gt; 0 is a scaling parameter. The ADMM algorithm can be written in scaled form as follows: where U W = 1  X   X  W , U V = 1  X   X  V . The optimization algorithm then cycles through each update listed above until convergence, or some maximum number of itera-tions is exceeded. 3.1. W -update: dual formulation The W -update (4) is a convex optimization problem similar to (3) with two modifications: 1) the constraint W  X  S d + has been removed, and 2) it is strongly convex, due to the quadratic term from L  X  . In principle, this could be solved directly. However, the active set A is often quite small: in practical problems, |A| rarely exceeds 100 X 200, while the number of parameters is O ( d 2 ) and can easily number in the tens of thousands. This suggests that a dual formulation may lead to a more efficient algorithm.
 To simplify the following derivation, let R t  X   X  = Z t  X  and m  X   X  = |A| . The W update (4) can be stated as the following linearly constrained quadratic program: Introducing Lagrange multipliers  X   X  R m + ,  X   X  R + , (6) has the following Lagrangian:
L ( W, X , X , X  ) = tr( W ) + C X  + Minimizing over W , we obtain the dual program: with the structure kernel H  X  S m + and cost vector b  X  R m defined as: H ij  X   X  =  X   X  i ,  X  j  X  F , b i  X   X  =  X   X R t  X  I,  X  i  X  (7) is a linearly constrained quadratic program in m variables, and can easily be solved by off-the-shelf tools. Note that the dual program is independent of both n and d , resulting in significant improvements in efficiency for large problems. After computing a dual optimum  X  , a primal optimum W t +1 can be recovered as follows: 3.2. V -update If there was no symmetry constraint in (5) , the V -update would take the form of a prox-operator prox ` computed via an element-wise thresholding operation where A i  X  is the i th row of A (Kowalski, 2009). However, this results in an asymmetric matrix, so we compute the S d -constrained V t +1 update via a separate ADMM algorithm, which alternates a prox ` projection  X  S d (  X  ) and an additive dual update, each of which can be computed in linear time.
 Algorithm 2 Robust MLR (step 3 of Algorithm 1) 1: W 0  X  0, V 0  X  0, Z 0  X  0, U 0 W  X  0, U 0 V  X  0 2: for t = 0 , 1 , 2 ,...,T (until convergence) do 3:  X  i : b i  X  X   X  ( Z t  X  U t W )  X  I,  X  i  X  F  X   X   X  i 4:  X   X  argmax  X  (7) 10: end for output W T 3.3. Z -update The Z -update simplifies to the orthogonal projection obtained by thresholding the negative eigenvalues of After consolidating the update steps, the resulting Ro-bust MLR algorithm is listed as Algorithm 2.
 To see the performance gains afforded by the ADMM-based method, we note that by setting  X  = 0 and skip-ping the V -update, we can obtain an ADMM-based algorithm to solve the MLR problem, which we call MLR-ADMM. This allows us to do a direct comparison between the ADMM-based MLR method and the orig-inal solver which used projected sub-gradient descent. Although the ADMM-based algorithm has the same worst-case complexity as projected sub-gradient descent  X  O (  X  2 ) for -sub-optimality  X  it has been observed to yield satisfactory solutions after a small number of steps. Coupled with early stopping (i.e. specifying a maximum number of iterations T for the ADMM algorithm), we were able to obtain significant speedups over projected gradient descent in our experiments. To evaluate the proposed method, we conducted three sets of experiments. In the first set of experiments, we augment standard UCI datasets with synthetic cor-related noise of varying dimensions to investigate the classification performance of various metric learning algorithms as noisy features are introduced. In the sec-ond set of experiments, we evaluate R-MLR on a music similarity task using data from CAL10K (Tingle et al., 2010) and the Million Song Dataset (MSD) (Bertin-Mahieux et al., 2011), and also evaluate the effects of early stopping on training time. In the third set of ex-periments, we evaluate performance and training time of the various algorithms on an image classification task using image data obtained from ImageNet (Deng et al., 2009).
 In all sets of experiments, we also provide comparisons to ` 1 -MLR, another variant of MLR which imposes a penalty on P i,j | W ij | instead of k W k 2 , 1 in (3) , as a similar regularizer has been shown to be effective in promoting feature sparsity by Rosales &amp; Fung (2006). ` -MLR can be trained via an ADMM algorithm in a similar fashion to R-MLR, and in fact allows for an ef-ficient element-wise thresholding for the corresponding V -update. To train MLR, we used the MLR-ADMM algorithm instead of the original projected sub-gradient descent implementation. 4.1. Classifying noise-augmented UCI data As a first experiment, we measure classifica-tion performance on four standard datasets from the UCI repository: Balance ( n = 625 ,d = 4 ), Ionosphere ( n = 351 ,d = 34 ), Iris ( n = 150 ,d = 4 ) and Wine ( n = 178 ,d = 13 ). We compare large-margin nearest neighbor (LMNN) (Weinberger et al., 2006), information-theoretic metric learning (ITML) (Davis et al., 2007), and MLR with both ` 1 -MLR and R-MLR. Each of the previously proposed algorithms are known to perform comparably well on these datasets, and in-troducing noise will allow us to carefully measure how performance degrades in higher dimensions relative to a known baseline (the noise-free case). 4.1.1. Experiment setup To study the effect of noisy features on each learning algorithm, we embedded each dataset into a higher-dimensional space by padding each example x i with D -dimensional correlated noise x  X  : For each dataset and D  X  X  2 5 , 2 6 , 2 7 , 2 8 } , we sample a covariance matrix  X   X  S D + from a unit-scale Wishart distribution. Each example was padded with noise according to (11) . Each padded dataset was then split into 25 random 40/30/30 training/validation/test splits, and each split was normalized by coordinate-wise z-scoring with the training set statistics.
 Performance was measured by k -nearest-neighbor ac-curacy using the training set as examples. For ITML, the slack parameter  X  was varied over { 1 , 10 ,..., 10 6 } For LMNN, the push-pull parameter  X  was varied over fixed to mean average precision (MAP), and C was varied over { 1 , 10 ,..., 10 6 } . For R-MLR and ` 1 -MLR number of nearest neighbors used for classification k was also varied in { 1 , 3 , 5 , 7 } . For each experiment, the hyper-parameters with the best classification accuracy on the validation set are selected. 4.1.2. Results Figure 1 displays example W s produced by ITML, LMNN, MLR, ` 1 -MLR and R-MLR on each UCI dataset, where the noise dimensionality D is set to 32. In each case, R-MLR correctly identifies and suppresses the noise dimensions by assigning small weights to the corresponding rows and columns of W . In contrast, MLR, ITML and LMNN assign significant weights to the noisy dimensions, degrading classification and re-trieval performance. ` 1 -MLR achieves input sparsity, but only at the expense of increased dimension. Figure 2 displays the error rates of the various learning algorithms across all datasets and values of D . We observe that R-MLR is able to achieve performance on par with other state-of-the-art algorithms even in the noiseless case. For all datasets and D  X  64, the R-MLR algorithm significantly outperforms MLR, ITML and LMNN under a Bonferroni-corrected Wilcoxon signed rank test (  X  = 0 . 05). R-MLR also significantly outperforms ` 1 -MLR for Ionosphere , D = { 128,256 } and Balance for D = 128.
 Figure 3 illustrates the effective dimensionality E  X  the number of dimensions necessary to capture 95% of the spectral mass of W  X  averaged across all splits of each UCI dataset. Effective dimensionality increases with input dimensionality for ITML and LMNN, but remains low for both MLR and R-MLR. ` 1 -MLR lies in between, producing metrics of higher rank than MLR or R-MLR.
 Across all UCI datasets, R-MLR training time was observed to be on the same order of magnitude as MLR-ADMM, and is consistently shorter than LMNN training time. It is still possible to accelerate our method further by using standard techniques, e.g. par-allelizing the constraint generation process, or sampling to approximate the most-violated constraint. 4.2. Music similarity: CAL10K In the music similarity task, we are provided with vector representations of song recordings, and the goal is to learn a distance (retrieval) function which successfully retrieves relevant songs in response to a query. We use a subset of the CAL10K dataset (Tingle et al., 2010), which is provided as ten 40/30/30 splits of a collection of 5419 songs (McFee et al., 2012). For each song x i , a relevant set X + i  X  X  train is defined as the subset of songs in the training set performed by the top 10 most similar artists to the performer of x i , where similarity between artists is measured by the number of shared users in a sample of collaborative filter data (McFee et al., 2012). This top-10 thresholding results in the relevant sets in this data being generally asymmetric and non-transitive, and therefore pair-wise methods such as ITML and classification-based methods like LMNN cannot be applied to the problem. Performance is measured by area under the ROC curve (AUC) of the rankings over the training set induced by distance from a test query. 4.2.1. Experiment setup We experiment with three song representations, derived from either audio features or lyrical content: Audio Each song is initially represented as a vector Lyrics-128, Lyrics-256 1396 of the songs above also MSD-33 Using the Million Song Dataset, we ex-Using the audio and lyrics representations, we com-pared the performance of MLR, ` 1 -MLR, and R-MLR, first on the audio and lyrics representations, and then after including the MSD-33 features. For this experi-ment, we vary C  X  X  10  X  1 ,  X  X  X  , 10 4 } and fix  X  to AUC (area under the ROC curve). For R-MLR and ` 1 -MLR, experiment, the hyper-parameters with the best AUC performance on the validation set are selected. To investigate the performance gains afforded by us-ing the ADMM-based algorithm, we compared MLR-ADMM to the original MLR implementation (MLR-Proj) on the Audio music similarity task, with the same experimental protocol as above. For MLR-ADMM, the maximum number of iterations T was additionally var-ied in { 1 , 5 , 10 , 25 , 50 , 100 } . We tracked performance as well as as the number of projection operations onto S + and calls to the constraint generator, which are the two key bottlenecks during training. 4.2.2. Results Figure 4 shows the average AUC of each algorithm across 10 folds. When MSD-33 features are included, R-MLR does significantly better than MLR under a Wilcoxon signed-rank test (  X  = 0 . 05). Moreover, even in the original audio or lyrics features where the mo-tivating assumption of noisy features may not hold, R-MLR does at least as well or better than MLR. In these experiments, we noted that the metrics produced by ` 1 -MLR tend to be strongly diagonal, with very few significant off-diagonal terms (due to the element-wise sparse regularizer). This may over-penalize weakly informative features, and limit the ability to exploit correlations between features. On the other hand, the group sparsity regularizer of R-MLR tends to produce solutions with denser rows, which better enables the algorithm to exploit feature correla-tions, leading to improved accuracy. In the experiments with MSD-33 features, we observed that the metrics learnt by MLR generally assign large weights to major-ity of the MSD-33 features, while the solutions learnt by R-MLR tended to suppress all but a few MSD-33 features.
 The results of the early stopping experiment are pre-sented in Figure 5. MLR-ADMM performs comparably to MLR-Proj across all values of T , and for small values of T , MLR-ADMM requires significantly fewer projection operations than MLR-Proj. For T = 1, the W returned at each step can be highly sub-optimal, and as a result, more cutting planes are required to converge. However, for intermediate values of T , the number of cutting planes does not significantly differ from MLR-Proj, and speedup is directly proportional to the decrease in projections. 4.3. Image Classification In the image classification task, we compare the classi-fication performance and training time of various algo-rithms. 100 images were chosen from each of 20 cate-gories (classes) from the ImageNet repository, and each image was represented using 1000-dimensional code-word histograms obtained from the ImageNet database. For this experiment, the data set was split into 5 folds of 60/20/20, and the same hyper-parameter values were used as in Section 4.1. The early-stopping parameter T was fixed to 10 for all ADMM-based algorithms. As in the early stopping experiment, we recorded the mean accuracy and training time only for the best hyper-parameter settings for each fold. 4.3.1. Results The results of the image classification task are shown in Table 1. The ADMM-based algorithms take signifi-cantly less time to run and achieve comparable accuracy to the existing methods, in particular MLR-Proj. We proposed a robust extension to the metric learning to rank algorithm, and derived an efficient learning algorithm. Our experiments demonstrate that by regu-larizing for both input and output sparsity, the R-MLR algorithm detects and suppresses noisy features, and outperforms previous methods in both low-and high-noise settings.
 Acknowledgements The authors acknowledge support from Qualcomm, Inc, Yahoo! Inc., Google, Inc., the Alfred P. Sloan Founda-tion, and NSF Grants CCF-0830535, IIS-1054960, and EIA-0303622. Daryl Lim was supported by a fellowship from the Agency for Science, Technology and Research (A*STAR), Singapore.
 Bertin-Mahieux, Thierry, Ellis, Daniel P.W., Whitman, Brian, and Lamere, Paul. The million song dataset. In International Conference on Music Information Retrieval , 2011.
 Blei, David M., Ng, Andrew Y., and Jordan, Michael I.
Latent dirichlet allocation. J. Mach. Learn. Res. , 3: 993 X 1022, March 2003. ISSN 1532-4435. URL http: //dl.acm.org/citation.cfm?id=944919.944937 .
 Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein,
J. Distributed optimization and statistical learning via the alternating direction method of multipliers.
Foundations and trends in machine learning , 3(1): 1 X 122, 2011.
 Davis, Jason V., Kulis, Brian, Jain, Prateek, Sra, Su-vrit, and Dhillon, Inderjit S. Information-theoretic metric learning. In ICML , 2007.
 Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-jia, Li,
Kai, and Li, Fei-fei. Imagenet: A large-scale hierar-chical image database. In In CVPR , 2009.
 Huang, Kaizhu, Jin, Rong, Xu, Zenglin, and Liu,
Cheng-Lin. Robust metric learning by smooth opti-mization. In UAI , pp. 244 X 251, 2010.
 Huang, Kaizhu, Ying, Yiming, and Campbell, Colin.
Generalized sparse metric learning with relative com-parisons. Knowl. Inf. Syst. , 28(1):25 X 45, 2011. Joachims, T. A support vector method for multivariate performance measures. In ICML , 2005.
 Joachims, Thorsten, Finley, Thomas, and Yu, Chun-Nam John. Cutting-plane training of structural svms. Mach. Learn. , 77(1):27 X 59, 2009.
 Kowalski, M. Sparse regression using mixed norms. Appl. Comput. Harmon. Anal. , 27(3):303  X  324, 2009. McFee, B., Barrington, L., and Lanckriet, G.R.G.
Learning content similarity for music recommenda-tion. IEEE Transactions on Audio, Speech, and Lan-guage Processing , 20(8):2207 X 2218, October 2012. McFee, Brian and Lanckriet, G.R.G. Metric learning to rank. In 27th annual International Conference on Machine Learning (ICML) , pp. 775 X 782, Haifa, Israel, June 2010.
 Rosales, R  X omer and Fung, Glenn. Learning sparse metrics via linear programming. In KDD , pp. 367 X  373, 2006.
 Shaw, Blake, Huang, Bert, and Jebara, Tony. Learning a distance metric from a network. In Advances in Neural Information Processing Systems 24 . 2011. Shen, Chunhua, Kim, Junae, Wang, Lei, and van den
Hengel, Anton. Positive semidefinite metric learning with boosting. In Advances in Neural Information Processing Systems 22 . 2009.
 Tingle, D., Kim, Y., and Turnbull, D. Exploring auto-matic music annotation with  X  X coustically-objective X  tags. In IEEE International Conference on Multime-dia Information Retrieval , 2010.
 Tsochantaridis, I., Joachims, T., Hofmann, T., and
Altun, Y. Large margin methods for structured and interdependent output variables. JMLR , 6:1453 X  1484, 2005.
 Weinberger, Kilian Q., Blitzer, John, and Saul,
Lawrence K. Distance metric learning for large mar-gin nearest neighbor classification. In NIPS , 2006. Xing, Eric P., Ng, Andrew Y., Jordan, Michael I., and Russell, Stuart. Distance metric learning, with application to clustering with side-information. In
Advances in Neural Information Processing Systems 15 , pp. 505 X 512, Cambridge, MA, 2003. MIT Press. Ying, Yiming, Huang, Kaizhu, and Campbell, Colin. Sparse metric learning via smooth optimization. In NIPS , pp. 2214 X 2222, 2009.
 Yuan, Ming and Lin, Yi. Model selection and estima-tion in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statisti-cal Methodology) , 68(1):49 X 67, February 2006. ISSN 1369-7412. doi: 10.1111/j.1467-9868.2005.00532.x.
URL http://dx.doi.org/10.1111/j.1467-9868. 2005.00532.x .
 Zha, Zheng-Jun, Mei, Tao, Wang, Meng, Wang, Zengfu, and Hua, Xian-Sheng. Robust distance metric learn-ing with auxiliary knowledge. In IJCAI , pp. 1327 X 
