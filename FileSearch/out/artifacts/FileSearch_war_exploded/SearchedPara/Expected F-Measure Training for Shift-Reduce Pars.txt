 Shift-reduce parsing is a popular parsing paradigm, one reason being the potential for fast parsers based on the linear number of parsing actions needed to an-alyze a sentence (Nivre and Scholz, 2004; Sagae and Lavie, 2006; Zhang and Clark, 2011; Goldberg et al., 2013; Zhu et al., 2013; Xu et al., 2014). Recent work has shown that by combining distributed rep-resentations and neural network models (Chen and Manning, 2014), accurate and efficient shift-reduce parsing models can be obtained with little feature engineering, largely alleviating the feature sparsity problem of linear models.

In practice, the most common objective for opti-mizing neural network shift-reduce parsing models is maximum likelihood. In the greedy search set-ting, the log-likelihood of each target action is max-imized during training, and the most likely action is committed to at each step of the parsing process dur-ing inference (Chen and Manning, 2014; Dyer et al., 2015). In the beam search setting, Zhou et al. (2015) show that sentence-level likelihood, together with contrastive learning (Hinton, 2002), can be used to derive a global model which incorporates beam search at both training and inference time (Zhang and Clark, 2008), giving significant accuracy gains over a fully greedy model. However, despite the ef-fectiveness of optimizing likelihood, it is often de-sirable to directly optimize for task-specific metrics, which often leads to higher accuracies for a variety of models and applications (Goodman, 1996; Och, 2003; Smith and Eisner, 2006; Rosti et al., 2010; Auli and Lopez, 2011; He and Deng, 2012; Auli et al., 2014; Auli and Gao, 2014; Gao et al., 2014).
In this paper, we present a global neural net-work parsing model, optimized for a task-specific loss based on expected F-measure. The model natu-rally incorporates beam search during training, and is globally optimized, to learn shift-reduce action se-quences that lead to parses with high expected F-scores. In contrast to Auli and Lopez (2011), who optimize a CCG parser for F-measure via softmax-margin (Gimpel and Smith, 2010), we directly op-timize an expected F-measure objective, derivable from only a set of shift-reduce action sequences and sentence-level F-scores. More generally, our method can be seen as an alternative approach for training a neural beam search parsing model (Watanabe and Sumita, 2015; Weiss et al., 2015; Zhou et al., 2015), combining the benefits of global learning and task-specific optimization.

We also introduce a simple recurrent neural net-work (RNN) model to shift-reduce parsing on which the greedy baseline and the global model is based. Compared with feed-forward networks, RNNs have the potential to capture and use an unbounded his-tory, and they have been used to learn explicit representations for parser states as well as actions performed on the stack and queue in shift-reduce parsers (Dyer et al., 2015; Watanabe and Sumita, 2015), following Miikkulainen (1996) and May-berry and Miikkulainen (1999). In comparison, our model is a natural extension of the feed-forward ar-chitecture in Chen and Manning (2014) using Elman RNNs (Elman, 1990).

We apply our models to CCG , and evaluate the re-sulting parsers on standard CCGBank data (Hock-enmaier and Steedman, 2007). More specifically, by combining the global RNN parsing model with a bidirectional RNN CCG supertagger that we have developed (  X  4)  X  building on the supertagger of Xu et al. (2015), we obtain accuracies higher than the shift-reduce CCG parsers of Zhang and Clark (2011) and Xu et al. (2014). Finally, although we choose to focus on shift-reduce parsing for CCG , we expect the methods to generalize to other shift-reduce parsers. In this section, we start by describing the baseline model, which is also taken as the pretrained model to train the global model (  X  2.4). We abstract away from the details of CCG and present the models in a canon-ical shift-reduce parsing framework (Aho and Ull-man, 1972), which is henceforth assumed: partially constructed derivations are maintained on a stack , and a queue stores remaining words from the input string; the initial parse item has an empty stack and no input has been consumed on the queue. Parsing proceeds by applying a sequence of shift-reduce ac-tions to transform the input until the queue has been exhausted and no more actions can be applied. 2.1 Model Our recurrent neural network model is a standard El-man network (Elman, 1990) which is factored into an input layer, a hidden layer with recurrent con-nections, and an output layer. Similar to Chen and Manning (2014), the input layer x t encodes stack and queue contexts of a parse item through con-catenation of feature embeddings. The output layer y represents a probability distribution over possible parser actions for the current item.

The current state of the hidden layer is determined by the current input and the previous hidden layer state. The weights between the layers are repre-sented by a number of matrices: matrix U contains weights between the input and hidden layers, V con-tains weights between the hidden and output layers, and W contains weights between the previous hid-den layer and the current hidden layer.

The hidden and output layers at time step t are computed via a series of vector-matrix products and non-linearities: where 2.2 Feature Embeddings Given a parse item, we first extract features using a set of predefined feature templates; each template belongs to a feature type f (such as word or POS tag), which has an associated look-up table, denoted as L f , to project a feature to its distributed represen-tation; and L f  X  R n f  X  d f , where n f is the vocabu-lary size of feature type f and d f is its embedding dimension. The embedding for a concrete feature is obtained by retrieving the corresponding row from L f . At time step t , the input layer x t is: x t = [ e f where  X ;  X  denotes concatenation, | f k | is the num-ber of feature templates for the k th feature type and x t  X  R a special embedding is used for unknown features. 2.3 Greedy Training To train a greedy model, we extract gold-standard actions from the training data and minimize cross-entropy loss with stochastic gradient descent (SGD) using backpropagation through time (BPTT; Rumel-hart et al., 1988). Similar to Chen and Manning (2014), we compute the softmax over only feasible actions at each step.

Unfortunately, although we use an RNN, which keeps a representation of previous parse items in its hidden state and has the potential to capture long-term dependencies, the resulting model is still fully greedy: a locally optimal action is taken at each step given the current input x t and the previous hidden state h t  X  1 . Therefore, once a sub-optimal action has been committed to by the parser at any step, it has no means to recover and has to continue from that mistake. Such mistakes accumulate until the goal is reached, and they are referred to as search errors .
In order to enlarge the search space of the greedy model thereby alleviating some search errors, we ex-periment with applying beam search decoding dur-ing inference; and we observe some accuracy im-provements by taking the highest scored action se-quence as the output (Table 3). However, since the greedy model itself is only optimized locally, as ex-pected, the improvements diminish after a certain beam size. Instead, we show below that by using the greedy model weights as a starting point, we can train a global model optimized for an expected F-measure loss, which gives further significant accu-racy improvements (  X  5). 2.4 Expected F1 Training The RNN we use to train the global model has the same Elman architecture as the greedy model. Given the greedy model, we summarize its weights as  X  = { U , V , W } and initialize the weights of the global model to  X  , and training proceeds as follows: 1. We use a beam-search decoder to parse a sen-2. Let y i be the shift-reduce action sequence of a 3. We compute the negative expected F1 objective
We note that the above process is different from parse reranking (Collins, 2000; Charniak and John-son, 2005), in which  X ( x n ) would stay the same for each x n in the training data across all epochs, and a reranker is trained on all fixed  X ( x n ) ; whereas the xF1 training procedure is on-line learning with pa-rameters updated after processing each sentence and each  X ( x n ) is generated with a new  X  .

More formally, we define the loss J (  X  ) , which in-corporates all action scores in each action sequence, and all action sequences in  X ( x n ) , for each x n as where F1 ( X  y parse derived by y i , with respect to the gold-standard malized probability score of the action sequence y i , computed as
To apply SGD, we derive the error gradients used for backpropagation. First, by applying the chain rule to J (  X  ) , we have Next, to compute  X  y propagated from the loss to the softmax layer, we rewrite the loss in (1) as J (  X  ) =  X  xF1 =  X  and by simplifying: since Finally, using (2) and (3) plus the above simplifica-tions, the error term  X  y quotient rule:  X  which has a simple closed form.

A naive implementation of the xF1 training pro-cedure would backpropagate the error gradients in-dividually for each y i in  X ( x n ) . To make it efficient, we observe that the unfolded network in the beam containing all y i becomes a DAG (with one hidden state leading to one or more resulting hidden states) and apply backpropagation through structure (Goller and Kuchler, 1996) to obtain the gradients. We explain the application of the RNN models to CCG by first describing the CCG mechanisms used in our parser, followed by details of the shift-reduce transition system. 3.1 Combinatory Categorial Grammar A lexicon , together with a set of CCG rules , for-mally constitute a CCG . The former defines a map-ping from words to sets of lexical categories repre-senting syntactic types, and the latter gives schemas which dictate whether two categories can be com-bined. Given the lexicon and the rules, the syntactic types of complete constituents can be obtained by recursive combination of categories using the rules.
More generally, both lexical and non-lexical CCG categories can be either atomic or complex : atomic categories are categories without any slashes, and complex categories are constructed recursively from atomic ones using forward ( / ) and backward slashes ( \ ) as two binary operators. As such, all categories can be represented as follows (Vijay-Shanker and Weir, 1993; Kuhlmann and Satta, 2014): where m  X  0 ,  X  is an atomic category, | 1 ,..., | m  X  {\ ,/ } and z i are meta-variables for categories.
CCG rules have the following two schematic forms, each a generalized version of functional com-position (Vijay-Shanker and Weir, 1993): The first schematic form above instantiates into a forward application rule ( &gt; ) for m = 0 , and for-ward composition rules ( &gt; B ) for m &gt; 0 . Similarly, the second schematic form, which is symmetric to the first, instantiates into backward application ( &lt; ) and composition ( &lt; B ) rules.

Fig.1 shows an example CCG derivation. All the rule instances in this derivation are instantiated from forward rules; for example, N / NN / N  X  N / N is an instance of forward composition and N / NN  X  N is an instance of forward application.

Given CCGBank (Hockenmaier and Steedman, 2007), there are two approaches to extract a gram-mar from this data. The first is to treat all CCG derivations as phrase-structure trees, and a binary, context-free  X  X over X  grammar, consisting of all CCG rule instances in the treebank, is extracted from local trees in all the derivations (Fowler and Penn, 2010; Zhang and Clark, 2011). In contrast, one can extract the lexicon from the treebank and define only the rule schemas, without explicitly enumerating any rule instances (Hockenmaier, 2003). This is the ap-proach taken in the C &amp; C parser (Clark and Curran, 2007) and the one we use here. Moreover, follow-ing Zhang and Clark (2011), our CCG parsing model is also a normal-form model, which models action sequences of normal-form derivations in CCGBank. 3.2 The Transition System The transition system we use in this work is based on the CCG transition system of Zhang and Clark (2011). We denote parse items as ( j, X , X ,  X ) 3 , where  X  is the stack (with top element  X  | s 0 ),  X  is the queue (with top element x w index of the word at the front of the queue, and  X  is the set of CCG dependencies realized for the input consumed so far (needed to calculate the expected F-score). We also assume a set of lexical categories has been assigned to each word using a supertag-ger (Bangalore and Joshi, 1999; Clark and Curran, 2004). The transition system is specified using three action types:  X  S HIFT ( sh ) removes one of the lexical cate-axiom: 0 : (0 ,, X , X  ) goal: 2 n  X  1 +  X  : ( n, X ,,  X )  X  + 1 : ( j + 1 , X  | x w  X  + 1 : ( j, X  | x, X ,  X   X  X  x  X  ))  X  + 1 : ( j, X  | x, X ,  X )  X  R EDUCE ( re ) combines the top two subtrees s 0  X  U NARY ( un ) applies either a type-raising or
The deduction system (Fig. 2) of our shift-reduce parse item is associated with a step indicator  X  , which denotes the number of actions used to build it. Given a sentence of length n , a full derivation requires 2 n  X  1 +  X  steps to terminate, where  X  is the total number of un actions applied. In Zhang and Clark (2011), a finish action is used to indicate termination, which we do not use in our parser: an item finishes when no further action can be taken. Another difference between the transition systems is that Zhang and Clark (2011) omit the  X  field in each parse item, due to their use of a context-free, phrase-structure cover, and dependencies are recovered at a post-processing step; in our system, we build depen-dencies as parsing proceeds. 3.3 RNN CCG Parsing We use the same set of CCG rules as in Clark and Curran (2007) and the total number of output units in our RNN model is equal to the number of lexical categories (i.e., all possible sh actions), plus 10 units for re 5 and 18 units for un actions.

All features in our model fall into three types: word, POS tag and CCG category. Table 1 shows the atomic feature templates and we have | f w | = 16 , | f p | = 16 and | f c | = 8 (all word-based features are generalized to POS features). Each template has two parts: the first part denotes parse item context and the second part denotes the feature type. s denotes stack contexts and q denotes queue contexts; e.g., s 0 is the top subtree on the stack, and s o . l is its left child. w represents head words of constituents and w 0 is the right-most word of the input string that has been shifted onto the stack. We extend the RNN supertagging model of Xu et al. (2015) by using a bidirectional RNN (BRNN). The BRNN processes an input in both directions with two separate hidden layers, which are then fed to one output layer to make predictions. At each time step t , we compute the forward hidden state h t for t = (0 , 1 ,...,n  X  1) ; the backward hidden state h 0 t is computed similarly but from the reverse direction for t = ( n  X  1 ,n  X  2 ,..., 0) as and the output layer, for t = (0 , 1 ,...,n  X  1) , is computed as The BRNN introduces two new parameter matrices matrix V with V 0 to take two hidden layers as in-put. We use the same three feature embedding types as Xu et al. (2015), namely word, suffix and capital-ization, and all features are extracted from a context window size of 7 surrounding the current word. Setup. All experiments were performed on CCG-Bank (Hockenmaier and Steedman, 2007) with the standard split. 6 We used the C &amp; C supertagger (Clark and Curran, 2007) and the RNN supertagger model of Xu et al. (2015) as two supertagger baselines. For the parsing experiments, the baselines were the shift-reduce CCG parsers of Zhang and Clark (2011) and Xu et al. (2014) and the C &amp; C parser of (Clark and Curran, 2007).

To train the RNN parser, we used 10-fold cross validation for both POS tagging and supertagging. For both development and test parsing experiments, we used the C &amp; C POS tagger and automatically as-signed POS tags. The BRNN supertagging model was used as the supertagger by all RNN parsing models for both training and testing. F-score over di-rected, labeled CCG predicate-argument dependen-cies was used as the parser evaluation metric, ob-tained using the script from C &amp; C .
 Hyperparameters. For the BRNN supertagging model, we used identical hyperparameter settings as in Xu et al. (2015). For all RNN parsing mod-els, the weights were uniformly initialized using the interval [  X  2 . 0 , 2 . 0] , and scaled by their fan-in (Bengio, 2012); the hidden layer size was 220 , and 50-dimensional embeddings were used for all feature types and scaled Turian embeddings were used (Turian et al., 2010) for word embeddings. We also pretrained CCG lexcial category and POS em-beddings by using the GENSIM word2vec implemen-ing a Wikipedia dump using the C &amp; C parser and concatenating the output with CCGBank Sections 02-21. Embeddings for unknown words and CCG categories outside of the lexical category set were uniformly initialized ( [  X  2 . 0 , 2 . 0] ) without scaling.
To train all the models, we used a fixed learning rate of 0 . 0025 and did not truncate the gradients for BPTT, except for training the greedy RNN parsing model where we used a BPTT step size of 9. We applied dropout at the input layer (Legrand and Col-lobert, 2015), with a dropout rate of 0 . 25 for the su-pertagger and 0 . 30 for the parser. 5.1 Supertagging Results Table 2 shows 1-best supertagging results. The MaxEnt C &amp; C supertagger uses POS tag features and a tag dictionary, neither of which are used by the RNN supertaggers. For all supertaggers, the same set of 425 lexical categories is used (Clark and Cur-ran, 2007). On the test set, our BRNN supertag-ger achieves a 1-best accuracy of 93 . 52% , an abso-lute improvement of 0 . 52% over the RNN model, demonstrating the usefulness of contextual informa-tion from both input directions.

Fig. 3a shows multi-tagging accuracy comparison for the three supertaggers by varying the variable-width beam probability cut-off value  X  for each su-pertagger. The  X  value determines the average num-ber of supertags (ambiguity) assigned to each word by pruning supertags whose probabilities are not within  X  times the probability of the 1-best supertag; for this experiment we used  X  values ranging from 0 . 09 to 2  X  10  X  4 and it can be seen that the BRNN supertagger consistently achieves better accuracies at similar ambiguity levels.

Finally, all shift-reduce CCG parsers mentioned in this paper take multi-tagging output obtained with a fixed  X  for training and testing; and in general, a smaller  X  value can be used by a shift-reduce CCG parser than by the C &amp; C parser. This is because a  X  value too small may explode the dynamic program of the C &amp; C parser, and it thus relies on an adap-tive supertagging strategy (Clark and Curran, 2007), by starting from a large  X  value and backing off to smaller values if no spanning analysis can found with the current  X  .
 5.2 Parsing Results To pretrain the greedy model, we trained 10 cross-validated BRNN supertagging models to supply su-pertags for the parsing model, and used a supertag-ger  X  value of 0 . 00025 which gave on average 5 . 02 supertags per word. We ran SGD training for 60 epochs, observing no accuracy gains after that, and epoch (Fig. 3b).

Furthermore, we found that using a relatively smaller supertagger  X  value (higher ambiguity) for training, and a larger  X  value (lower ambiguity) for testing, resulted in more accurate models; and we chose the final  X  value used for the greedy model to be 0 . 09 using the dev set (Table 3). This obser-vation was different from Zhang and Clark (2011) and Xu et al. (2014), which are two shift-reduce CCG parsers using the averaged perceptron and beam search (Collins, 2002; Collins and Roark, 2004; Zhang and Clark, 2008): they used the same  X  val-ues for training and testing, which resulted in lower accuracy for our greedy model.

Table 3 also shows the effect on dev F1 by us-ing different beam sizes at test time for the greedy model: with b = 6 , we obtained an accuracy of 85 . 02% , an improvement of 0 . 41% over b = 1 (with a  X  value of 0 . 09 ); we saw accuracy gains up to b = 8 (with very minimal gains with b = 16 for  X  values 0 . 06 and 0 . 07 ), after which the accuracy started to drop. F1 on dev with b = 6 across all training epochs are shown in Fig. 3b as well, and the
For the xF1 model, we used b = 8 and a supertag-ger  X  value of 0 . 09 for both training and testing. Fig.3c shows dev F1 versus the number of train-ing epochs. The best dev F1 was obtained after the 54 th epoch with an accuracy of 85 . 73% , 1 . 12% higher than that of the greedy model with b = 1 and 0 . 71% higher than the greedy model with b  X  X  6 , 8 } . This result improves over shift-reduce CCG models of Zhang and Clark (2011) and Xu et al. (2014) by 0 . 73% and 0 . 55% , respectively (Table 4). the xF1 trained beam-search model, is currently the most accurate shift-reduce CCG parser, achieving a final F-score of 86 . 42% , and gives an F-score im-provement of 1 . 47% over the greedy RNN base-line. We show the results for the model of Xu et al. (2014) for reference only, since it uses a more sophisticated dependency, rather than normal-form derivation, model.

At test time, we also used the precomputation trick of Devlin et al. (2014) to speed up the RNN models by caching the top 20K word embeddings RNN parser more than 3 times faster than the C &amp; C parser (all speed experiments were measured on a Optimizing for Task-specific Metrics. Our train-ing objective is largely inspired by task-specific opti-mization for parsing and MT. Goodman (1996) pro-posed algorithms for optimizing a parser for var-ious constituent matching criteria, and it was one of the earliest work that we are aware of on opti-mizing a parser for evaluation metrics. Smith and Eisner (2006) proposed a framework for minimiz-ing expected loss for log-linear models and applied it to dependency parsing by optimizing for labeled attachment scores, although they obtained little per-formance improvements. Auli and Lopez (2011) op-timized the C &amp; C parser for F-measure. However, they used the softmax-margin (Gimpel and Smith, 2010) objective, which required decomposing preci-sion and recall statistics over parse forests. Instead, we directly optimize for an F-measure loss. In MT, task-specific optimization has also received much attention (e.g., see Och (2003)). Closely related to our work, Gao and He (2013) proposed train-ing a Markov random field translation model as an additional component in a log-linear phrase-based translation system using a k -best list based expected BLEU objective; using the same objective, Auli et al. (2014) and Auli and Gao (2014) trained a large scale phrase-based reordering model and a RNN lan-guage model respectively, all as additional compo-nents within a log-linear translation model. In con-trast, our RNN parsing model is trained in an end-to-end fashion with an expected F-measure loss and all parameters of the model are optimized using back-propagation and SGD.
 Parsing with RNNs. A line of work is devoted to parsing with RNN models, including using RNNs (Miikkulainen, 1996; Mayberry and Miikkulainen, 1999; Legrand and Collobert, 2015; Watanabe and Sumita, 2015) and LSTM (Hochreiter and Schmid-huber, 1997) RNNs (Vinyals et al., 2015; Balles-teros et al., 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). Legrand and Collobert (2015) used RNNs to learn conditional distributions over syntac-tic rules; Vinyals et al. (2015) explored sequence-to-sequence learning (Sutskever et al., 2014) for parsing; Ballesteros et al. (2015) utilized character-level representations and Kiperwasser and Gold-berg (2016) built an easy-first dependency parser using tree-structured compositional LSTMs. How-ever, all these parsers use greedy search and are trained using the maximum likelihood criterion (ex-cept Kiperwasser and Goldberg (2016), who used a margin-based objective). For learning global mod-els, Watanabe and Sumita (2015) used a margin-based objective, which was not optimized for the evaluation metric; although not using RNNs, Weiss et al. (2015) proposed a method using the averaged perceptron with beam search (Collins, 2002; Collins and Roark, 2004; Zhang and Clark, 2008), which re-quired fixing the neural network representations, and thus their model parameters were not learned using end-to-end backpropagation.

Finally, a number of recent work (Bengio et al., 2015; Vaswani and Sagae, 2016) explored train-ing neural network models for parsing and other tasks such that the network learns from the oracle as well as its own predictions, and are hence more robust to search errors during inference. In princi-ple, these techniques are largely orthogonal to both global learning and task-based optimization, and we would expect further accuracy gains are possible by combining these techniques in a single model. Neural network shift-reduce parsers are often trained by maximizing likelihood, which does not optimize towards the final evaluation metric. In this paper, we addressed this problem by developing expected F-measure training for an RNN shift-reduce pars-ing model. We have demonstrated the effective-ness of our method on shift-reduce parsing for CCG , achieving higher accuracies than all shift-reduce CCG parsers to date and the de facto C &amp; C parser. 11 We expect the general framework will be applicable to models using other types of neural networks such as feed-forward or LSTM nets, and to shift-reduce parsers for constituent and dependency parsing. We thank the anonymous reviewers for their de-tailed comments. Xu acknowledges the Carnegie Trust for the Universities of Scotland and the Cam-bridge Trusts for funding. Clark is supported by ERC Starting Grant DisCoTex (306920) and EPSRC grant EP/I037512/1.

