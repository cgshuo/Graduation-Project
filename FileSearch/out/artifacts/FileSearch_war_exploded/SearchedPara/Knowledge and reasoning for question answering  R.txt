 1. Introduction
From the early beginnings of artificial intelligence, people dreamed of intelligent machines that would be able to answer their questions. Today, this dream is about to come true. The mass production of information has initiated a large demand for systems that intelligently search the information sources. Computational power and the development of sophisticated algo-rithms that make use of this power, have contributed to a rapid evolution of systems that aim at such a goal.
A question in natural language is the most natural way to express an information need. From the early days of informa-tion system development, research has been devoted to understanding natural language questions. Having a machine that can answer questions such as How much CO2 can my factory release in order to be compliant with the Kyoto norm? or Who are the board members of the company in Toulouse, which recently won the innovation award? would revolutionize our search technologies.

These ideas also inspired companies such as IBM and Wolfram j Alpha when developing question answering search sys-tems. The long term goal of the Wolfram j Alpha search engine is to build a huge knowledge base with objective and correct factual knowledge and so that this knowledge can be queried, taking into account the different backgrounds and educational level of users. 1 The IBM  X  X  DeepQA project aims to answer factual questions automatically like a superintelligent quiz candidate.
There are, however, a number of bottlenecks. First of all we have to automatically understand the question of the user and the general context of his or her information need. Second, how do we build the knowledge sources that are needed to answer the questions? How do we combine, fuse or reason with the knowledge once it has been extracted from the raw sources?
In addition, users do not only ask factual questions, which are often easy to answer, because the words of the question and of textual document sources overlap. Many interesting questions demand a more advanced understanding of the  X  meaning of the question and of the information sources, as well as techniques of answer fusion and generation. Very often the correct answer is distributed across several sources and the pieces have to be combined, or the right answer is found by means of inferencing. When the answer integrates different parts, a human-readable answer needs to be generated. We fur-ther refer to this type of information search as advanced question answering .

The aim of this article is to give the reader an overview of recent promising research avenues that make the above goals a realistic target, and of the possible consequences for future search technology. We believe that a refined understanding of a natural language question, the extraction of the necessary knowledge from content (e.g., the text sources), and the integra-tion of reasoning strategies will make the QA dream come true. We demonstrate in this paper that many ingredients of this approach are already in place.

The remainder of this article is organized as follows. The next section further explains the problems in question answer-ing and the aims of this article. Section 3 is devoted to the current achievements of knowledge acquisition in general and explains its value for question answering. Section 4 discusses the different facets of question answering and reveals the exis-tence of advanced types of questions. To answer these questions, the advanced information extraction technologies dis-cussed in Section 3 present the basis for reasoning mechanisms and open many new research tracks. 2. Problem definition and background
Whatever information system we use, information is always stored in some kind of representation that makes fast access possible. For instance, in a classic retrieval system the words of text documents and possibly positional and occurrence infor-mation are accessible via inverted files, using term or document distributed indexes. From the point of view of efficiency, it is impossible to search the texts of these documents in real time and check the occurrence of a certain word. The inverted files ignore relationships between content and interpretations that are necessary in a question answering context. In addition, they usually do not contain information on the role of words, phrases or sentences in a discourse. Neither do they provide the concepts expressed by certain time stamped image frames. This additional information is often necessary to give focused answers to the questions asked in natural language.

More complex representations of the content are needed in order to allow reasoning and the inference of the answer. An example of a well-studied and theoretically sound representation language is first-order predicate logic. With this method, it advanced logical formalisms including modal logics are preferred in certain situations. The acquisition of knowledge includ-ing the translation into a logical format is often performed by experts and knowledge engineers. The handcrafting of a knowledge base is a real bottleneck for the development of question answering systems.

Increasingly, this knowledge is automatically acquired from data sources (Web text pages, Web tabular data and multi-media data) as content extractors are being developed. Content extractors can be used for building indexed representations of documents (text and multimedia), for representing generic world knowledge (from text and multimedia) which is often useful in reasoning processes, and for understanding natural language questions. Assuming that we use generic predicates and arguments, i.e., generic semantic labels in open domain question answering and more specific labels for domain specific applications, how can we automatically label or extract the information from the data?
Users can ask a large variety of questions, for which answer extraction requires making inferences over the data. Besides making inferences through the coreference of arguments and predicates, temporal, spatial and evidential reasoning are important reasoning strategies in question answering systems. The question of how many inferences will be made a priori, stored and made accessible via indices and that of how many of the inferences will be made in real time during querying are very pertinent. However, we will not be further deal with them in this article, because we still need to build a consensus on what representations will be made from the texts and media sources. 3. Knowledge acquisition
The major question is how to translate human knowledge found in text and multimedia sources into a language with which a machine can reason and make computations, and easily map these to a user X  X  natural language question. We are most far advanced in extracting knowledge from text or textual tabular data than from any other media. However, multi-media extraction and especially cross-modal extraction are fields of growing research interest, offering many opportunities for future multimedia question answering systems. 3.1. Mining the World Wide Web
The World Wide Web gives us enormous possibilities both for the acquisition of long lasting world knowledge and of fugi-tive knowledge, i.e., knowledge that quickly changes over time, such as the price of a stock on a certain date. This is made possible through the current technological advances in the disciplines of machine learning, data mining and information retrieval, and their applications when processing natural language. Since the 1990s, methods have been developed for the extraction of knowledge from text based on machine learning from positive and negative examples. A classification algo-rithm is trained and learns the textual patterns of a certain type of knowledge. This is a form of supervised learning. Notwithstanding its good results, this approach has limitations as annotating texts demands a great amount of manual effort.
This also means that it is unrealistic to think that we can manually create training examples for all possible types of knowl-edge extractions. The current research question is how to learn accurate extraction patterns with a miminal amount of man-ual effort.

One factor that makes this task often easy, is that there is a huge quantity of sources available, which redundantly describe the same information. For instance, if someone looks up Mozart X  X  birth date, many sources on the World Wide
Web will give 1756 as an answer. 3.2. Fundamental problems
However, there are factors that make the task of knowledge extraction difficult. Natural language has many variant expressions that signal the same or similar content. This is the problem of synonymy and paraphrasing. In the example given above, Mozart X  X  birth date can be expressed through many different expressions, such as was born on or his date of birth is . So, if someone looks up the birth date of a less famous person, the phrasing of the question might not match the phrasing in any source document. On the other hand, one expression often has different meanings depending on the context. This is the problem of homonymy and polysemy. For instance, the word  X  X  X lant X  X  might refer to a factory or to a living organism. Certain extracted relations are ambiguous with regard to the scope. The expression There is a physician in each town can theoretically
Humans estimate the probability of one interpretation based on the world knowledge that they possess. Machines can also learn this world knowledge from large data sets ( Yates, 2009 ).

With regard to the problem of synonymy, researchers have focused on the resolution of the problem posed by coreferents, and more specifically noun phrase coreferents. The meaning of a word is to a large extent defined by the context in which it is used. A common way to identify the correct coreferents consists in a pairwise modeling of the context of potential refer-ents, and in deciding that two mentions refer to the same entity. For instance, Mozart and the composer of the Magic Flute refer to the same person, if both mentions share enough characteristics or context mentioned in the texts. This is not always easy as mentions of entities might be ambiguous themselves. In a given discourse the word composer might once refer to Mozart and further in the discourse to Bach . In the context of this discourse composer is a polysemous word. Once the similarity based on shared context between mentions is computed, we use cluster algorithms to group the mentions referring to one entity and restrict the clustering by language specific constraints. 3.3. Recognizing entity relations
Coreferring noun phrases illustrate an equivalence relation. Entities are usually connected through other semantic rela-tions. By relation we mean a connection between persons, companies or locations, or, in specific domains, between domain specific named entities. An example of an instance of the  X  X cquisition X  relation is Google acquires Youtube , or an instance of  X  X omain specific product X  relation Exoenzyme S is an extracellular product of Pseudomonas aeruginosa. . Commonly, a classifi-cation model is trained that predicts the relation between the entities. The task is often seen as a binary classification prob-lem, where the model allows us to recognize the presence or non-presence of a relation between any given input entities. In order to recognize a certain relation between the question and the sentences of a text, we need to automatically learn the textual patterns that indicate a relation between entities, so that they can be used to recognize the same relation in other sentences or text segments.

Different solutions have been designed for this problem. Most of these algorithms require, however, an annotated train-ing corpus of a substantial size. Building such a corpus is costly and would need to be done for each type of relation. A novel trend is the use of algoritms which require less supervision, i.e., less manual input ( Bunescu &amp; Mooney, 2007; De Belder, De Smet, Mochales Palau, &amp; Moens, 2009 ). Large collections of texts, for instance, found on the World Wide
Web, offer many possibilities. For example, by searching the Web for sentences that contain a pair of entities for which the relation holds, we obtain a bag of positive examples for each pair. These bags might also contain negative examples, i.e., pairs for which the relation does not hold. Performing the same exercise for pairs of entities for which the relation does not hold, yields bags of negative examples, which are expected to be noise free. Promising techniques of multiple instance learning allow the construction of robust relation recognizers. The above examples of  X  X cquisition X  and  X  X roduct of X  relations concern the extraction of binary relations. Generalizing the approaches to n-ary relationships seems the next challenge. This would allow us to recognize the attributes of a complex event, e.g., the actors, and the time and location of an action.

Relation recognition is often coupled to semantic role labeling. Semantic role labeling is mainly concerned with classify-ing sentence constituents and recognizing basic roles for its constituents, such as who does what to whom , where and when ( Moschitti, Morarescu, &amp; Harabagiu, 2003; Shen &amp; Lapata, 2007 ). We currently witness a similar interest for reducing the amount of training data to improve the portability of the technologies across languages and domains. Here, statistical lan-guage models, which are trained on large corpora in an unsupervised manner, easily learn the patterns of the sought rela-tions and could improve the performance of the extraction tasks and simultaneously reduce the number of training examples ( Deschacht &amp; Moens, 2009 ).
 3.4. The recognition of temporal, spatial and causal relations
As we will see later, there are specific relations to be recognized in texts that are very important for question answering, both for factual question answering and for advanced question answering. Among them are the recognition of temporal events and relations, of spatial events and relations, and of causal relations between events. The accurate recognition of tem-poral and spatial information in natural language questions is very valuable for time stamped video retrieval.
For English we use the TimeBank corpus, which contains the necessary annotations for learning the extractors of temporal expressionsandrelations.Researchershavestartedtousethiscorpusfortherecognitionoftemporalexpressions(e.g., lastTues-day 5 PM , Saturday , April 4 )( Boguraev&amp; Ando,2005 ). Thebestresultsareobtainedbyusingmachinelearningtechniques forthe tions in texts (e.g., recognizing that event A happened before event B ) sucessfully uses Markov Logic Networks, a technique whichallowsthetrainingofaprobabilisticrecognizerwithextraknowledgeofthelanguageorconstraintsintheformoflogical cumstances, goes back to a task defined in the Message Understanding Conferences (MUC), and is more difficult to recognize correctly ( Boguraev &amp; Ando, 2005; Sauri, Knippen, Verhagen, &amp; Pustejovsky, 2005; Bethard &amp; Martin, 2008 ).
Unfortunately, we do not have a corpus annotated for spatial expressions and relations. Their recognition is reported in isolated experiments in limited application domains mostly bound to understanding robot and route instructions and ques-tioning surveillance videos. With respect to the last task we refer to the work of Landau and Jackendoff (1993), Regier, Carl-son, and Corrigan (2005), Katz, Lin, Stauffer, and Grimson (2004) .

Recently, we also witnessed an interest in the recognition of causal relations in textual statements aiming at finding cau-sality between the events being described. A causal relation not only involves a relation of simple conjunction, but also one of cause-effect relations, to improve information retrieval effectiveness. The study investigates whether the information ob-tained by matching cause-effect relations expressed in documents with the cause-effect relations expressed in users X  queries can be used to improve document retrieval results. The recognition  X  as is the case when recognizing any other semantic 3.5. The recognition of other discourse relations
Temporal, spatial and causal relations are part of the more general class of discourse relations. This brings us to the domain of recognizing discourse relations and roles in text. Textual documents often have a hierarchical organization based on asymmetrical nucleus-satellite relationships. Pairs of elementary textual units combine into parent units, which are again recursively merged until at a certain point a unit spanning the entire text is reached. The constituents are linked together by a structuring relation, which typically holds between a more central unit, the nucleus and a more peripheral one, the satellite (although there is also a small set of multi-nuclear relations). A tree structure is common as discourse representation of a text, especially in case of expository text. However, the rhetorical relations that hold between text blocks might reveal other organizational structures ( Wolf &amp; Gibson, 2005 ). For most West-European languages, taxonomies have been built with rhe-torical relations such as evidence, elaboration, background, example, contrast and others. Rhetorical relations are linked to the functional role of some information (e.g., procedure to accomplish a goal). The recognition of rhetorical relations has a large potential for questioning informative Web pages ( Kwok, Etzioni, &amp; Weld, 2001; Conrad &amp; Schilder, 2007 ), which, for instance, inform their users about procedures to be followed, definitions to be taken into account, etc. Little research is devoted to extracting these roles from textual documents and to analysing natural language questions in terms of requests for information that plays a functional role in documents except for Marcu (2000) . 3.6. Knowledge acquisition from tabular data
The state of the art of automatic extraction of factual knowledge from text is moving forward quickly, both in terms of scalability and in terms of the refinement of the extracted knowledge. Information extraction systems started with the auto-matic extraction and classification of proper names; now they extract huge amounts of factual data from the Web, possibly covering billions of facts ( Yates, 2009 ).

In addition, this evolution is supported by emerging technologies for extracting information from tabular data found on the Web, for instance the extraction of semantically typed data from information that is in explicit HTML table format, or in other list formats in Web pages in Cafarella, Halevy, Wang, Wu, and Zhang (2008), Cafarella (2009) .In Elmeleegy, Madhavan, and Halevy (2009) it is estimated that 1 in 10 Web pages contains a table, and that 1 in 100 pages contains tables of typed data that can be processed to extract semantically typed information, such as Mayors of New York city , Low light cameras , etc.
The remaining challenge is how to mine and merge information in these tables. 3.7. Snowball effect
The automatic extraction of knowledge has a snowball effect. Once a certain amount of world knowledge has been auto-matically extracted, it can be used to extract additional knowledge from textual or other sources. For instance, in tasks that require natural language processing, we know that many subtasks such as noun phrase coreferent resolution, disambigua-tion and dependency parsing of a sentence into its constituents require world knowledge.

Researchers have started to use the knowledge in inference engines, which reason with knowledge stored in data and knowledge bases. These techniques have been existing for a long time, but now technology has reached a stage where we can automatically extract knowledge from natural language or other textual sources. This situation has important conse-quences for question answering. Knowledge extraction technology allows us to recognize entities and their relations both in the questions asked and in the documents, which allows us to improve the correctness of the given answers. 4. The facets of questions-answering Questions are most of the times categorized according to the type of the answer which is expected, e.g., ( Lehnert, 1978; tions that basically induce factoid answers, i.e., a short piece of information which can directly be extracted from a text (e.g., dates, costs, names) and questions, which for various reasons require more complex language and reasoning processes.
These fall into the paradigm of advanced question answering, not to be confused with complex question answering, such as, e.g., questions composed of several layers or sub-questions.

Advanced question answering is an area which deals with those numerous cases where (1) there is no direct answer to the question (i.e., some form of elaboration based on the question is needed) or there are too many answers (making the result somewhat confusing), or (2) the answer has a complex structure. Language processing and generation as well as ded-icated reasoning procedures, which are often based on domain or world knowledge, are required in order to correctly and adequately answer those questions. 4.1. Advanced question-answering Following are some of the main types of questions that fall into the area of what we call advanced question answering: Procedural questions, where the answer to a How-to question is a well-formed fragment of a procedure ( Delpech &amp; Saint-Dizier, 2008 ), i.e., a set of instructions (possibly sub-procedures), associated with pre-requisites, warnings, pictures, etc.
Causal questions, which answer questions in Why P ( Pechsiri, Sroison, &amp; Janviriyasopa, 2008 ) where the answer is in gen-eral a more or less organized set of events that caused P. The difficulty is obviously to identify, in various documents, chains of events leading to P or causing P.

Evaluative and comparative questions, where the challenge is to correctly understand the phrases of the content used in the comparison. For example, the question Is X an innovative researcher? needs subtle semantic elaborations around the semantics of the adjective innovative applied to researchers (for example in contrast with innovative applied to a company or to a product). So far, there are no foundational results in this area. The contextualised semantics of adjectives or adverbs in particular still has to be elaborated. However, some basic results from database querying and from generalized quantifiers theory ( Saint-Dizier, 1988 ) can be considered as a useful contribution to answering this type of complex questions.

Opinion questions, related to opinion mining and probably argumentation, require a very elaborated form of answer where opinions about a fact are contrasted, and probably supported by arguments for or against this fact.

Besides question types that clearly require language processing and reasoning, as those advocated above, more standard types of questions, including factoid questions, may require advanced treatments for various reasons. One of the first reasons is that there may be no direct answer or too many answers. For example, if you are looking for a hotel room in Toulouse for 9 people, you will not get any answer because such large hotel rooms are not available (see Maybury, 2004). A cooperative attitude (which is what most users expect) requires the system to be able to offer alternative solutions, as any travel agent would do. One strategy could be to first indicate that the resource does not exist, so that the user understands why an alter-native solution is necessary, and then to relax some constraints in the question, e.g., the number of people in the room. This requires well tuned reasoning strategies, since the ideal response would be to propose a group of, for example, four rooms in the same hotel, which would allow the accommodation of nine people.

The opposite case occurs when there are too many answers to a question. A solution is to abstract over those answers or to categorize them, if possible, and to propose a kind of intentional formulation. For example, if someone asks about the price of regular gas in Paris, instead of getting fares for each gas station, it would be much more interesting to elaborate a kind of interval encompassing most fares ( Moriceau, 2006 ). Once again, this requires well tuned, domain-and user dependent data fusion.
 Another situation where reasoning is required occurs when an answer is not complete, fully relevant or fully reliable.
Given that a question answering system must be cooperative and helpful, it is necessary to develop dedicated strategies to adequately answer the user. For example, if the answer comes from a blog or from any kind of personal Web site, the sys-tem should indicate that the answer may not be fully reliable. Completeness is another aspect, where, for example, the infor-mation retrieved by the system has a coarser grain than the one expected by the user. For example, when asking Did typhoon
A occur at night on June 20th over Manilla? and the retrieved answer is just It happened on June 20th , then the answer could clearly indicate that it is only partial.

Another complex situation occurs when the question is not sufficiently precise or contains terms which are not appropri-ate for a document search (e.g., metaphors, technical terms). In that case, the question often has too many answers, or leads to a failure. One strategy is to start a short and well-targeted dialogue aiming at clarifying terms (e.g., making propositions for more appropriate terms) or at obtaining missing information. In this latter case, a domain model is certainly necessary, unless the missing information can be induced from acquired knowledge.

As can be noted from these different situations, where some form of accuracy and cooperativity is needed to get appro-priate answers, the need for knowledge, dedicated inferences and accurate question term interpretation is central and very crucial. The necessity of undergoing clarification via dialogue also occurs quite frequently ( Kupiec, 1997 ). The landscape of question answering widely opens to a large number of ongoing and very challenging research topics in artificial intelligence, knowledge representation, dialogue and pragmatics. 4.2. The art of providing users with adequate answers
Another major challenge in question answering is the production of an answer. In the factoid question framework, the answer is in general limited to a portion of a text where the answer has been found. This may be sufficient for factoid ques-tions when the answer does not need any elaboration a priori, but this is not appropriate for the types of questions presented above. Answer generation is a complex and crucial task which has seldom been addressed in the question answering com-munity. As in other areas of language processing, most of the language generation aspects have been neglected; the result is that applications may have a rich question analysis facet but a poor output, possibly leading to misunderstandings. Let us briefly discuss below the issue of answer production for a number of types of advanced question answering.

For procedural questions, the answer, induced from document titles viewed as goals, is one or more well-formed text fragments that describe the actions (embodied by instructions or by a set of subgoals with their related instructions) in the procedure. Several closely related answers can be provided, associated with e.g., navigational tools, in order to let the user choose his/her prefered solution among a few (e.g., the different ways to make a Margarita pizza). In addition, the actions to be undertaken can be associated with a number of hints and warnings, extracted from other documents, in order are not straightforward. There is a need for language generation paired with navigation tools, possibly accompanied by images.

Causal or consequence questions call for answers formed out of sequences of events. These events may be interrelated at various degrees, and they may also be more or less strongly present in the causal or consequence chain. Besides the difficulty of matching the question with a cause or a consequence (e.g., Why is smoking dangerous? My rice has some yellow leaves, with some small holes at the extremities of the leaves, what should I do? ) the answer elaboration requires several forms of entailment before possibly reaching causes in texts. Next, the construction of the answer requires an elaborated and accurate planning of the chains of events that have been identified (most events are caused or entailed by several other events). These events do not have the same importance in the chain, some may be central (kernels) while others are satellites (elaborations, sub-events, examples, etc.). The answer should be able to outline the main events and their internal relations as well as their relations with the terms of the question, while making explicit the links with others. Furthermore, the answer may be con-structed via fusion from several documents.

Evaluative and comparative questions have not been the subject of many studies so far, although they cover quite a large set of questions. The problematics behind is very complex in terms of identification of modified properties. In some profes-sional domains, the response may even involve semantic elements which go beyond properties mentioned in the texts. The challenge is to be able to indicate in the answer the basis on which the comparison has been carried out, for instance, Com-pany A is innovative in drug production because it buys a lot of basic technologies and has a large set of customers that buy its know-how. Comparative and superlative questions thus have a lot in common with quantification, as in, for instance, the gen-eralized quantifiers approach ( Saint-Dizier, 1988 ).

The answer to opinion questions are even harder to elaborate. Given a statement, e.g., Should I get immunized against H1N1 flu? , the response is a set of articulated arguments for or against the statement, associated with e.g., elaborations and illus-trations. Arguments and elaborations can be more or less strongly marked in statements ( Palau &amp; Moens, 2009 ), by means of a number of linguistic marks such as adverbs. Arguments can attack or support each other, as in argumentation theory ( Caminada &amp; Amgoud, 2007 ). Even with this complex structure, no definitive answer is given, because the final decision de-pends on the system of values (or preferences) of the person asking the question: the answer depends on the individual weights given to each argument by the user. The answer can be created according to argumentation schemes ( Walton, Reed, &amp; Macagno, 2008 ) or, on a larger scale, following predefined explanation schemes.

Besides these very challenging and very open answer generation problems (the what to say of natural language genera-tion, e.g., Dale &amp; Reiter, 2000), answer production, as advocated above, may also need some forms of data or information fusion or conflation ( Moriceau, 2006 ). This requires quite an extensive domain knowledge base augmented with world knowledge, and accurate fusion strategies, possibly demanding uncertainty reasoning as the analysis from questions and documents might yield uncertain results. Automatic linking and fusion of information will become an important research topic. Very often the answer to a question, especially in an advanced question answering setting, fuses information from dif-ferent sentences, documents and media.
 In the same range of ideas, offering multiple solutions to a user requires the development of adequate navigation tools.
Finally, since one of the major challenges is to be cooperative with the user, answer personalization (conceptually and lin-guistically speaking) is crucial. Explanation production, when the answer is not direct, is also an important feature, however explanation production has not been the subject of much investigation so far in computational linguistic circles, but has been studied in didactics and psychology with a very different and much more applied perspective.

To answer complex questions, it may be appropriate not to leave the system answer by itself while leaving the user in a relatively passive attitude. One possibility is to view an advanced question answering system as a help for the user, a kind of collaborative tool where the user contributes in several ways, yet to be explored, to the production of the answer. One exam-ple is the case of evaluative questions where the system cannot always guess by itself the meaning of modifying expressions.
For example, the difference in meaning between a competitive company and a competitive researcher is characterized in con-ceptual semantics by the set of properties of a company or a researcher which are modified in some way by this adjective.
Obviously, these are not the same. If the system has no predefined definition and no means to automatically infer that list of properties, then it may be best to suggest to the user to identify properties (possibly via examples) among a set of properties, and then to memorize the result and its context. Some cases are almost impossible to answer without a dialogue, e.g. Which the question. 4.3. Matching questions and answers
We will not address the problem of question-document matching here since it is discussed in many papers in the case of factoid questions or questions that have relatively direct responses, via, for instance, some query reformulations as described evaluative and opinion questions, this is still an open issue. Previous research with regard to logic-based retrieval models interesting starting point for further research into question answering retrieval models. 4.4. Evaluation needs
The evaluation of advanced question answering systems is still a very open issue. It is certainly possible to evaluate some components in quite a standard way (e.g., the question analysis: its type, its focus, the relevance of retrieved documents), but for most components it turns out to be very difficult to elaborate relevant evaluation parameters.

The first difficulty is to be able to elaborate a realistic corpus of questions, asked by real users, and corresponding to real needs. This may be quite different from the sets of questions of e.g., TREC competitions which are basically aimed at com-paring systems. Therefore, those questions correspond, more or less, to the system capabilities, not necessarily to real user X  X  needs. Then, given that answers are complex and not necessarily unique, how is it possible to indicate if a question is cor-rectly answered? Finally, the limited number of evaluation criteria provided by the natural language generation community clearly reveals the challenges.

One possible, relatively objective measure remains user satisfaction, defined with a few simple parameters. Satisfaction may be measured e.g., in comparison with responses provided by humans in comparable situations. 5. Conclusions
This paper surveyed text mining and natural language processing methods that are ready to be integrated in innovative question answering approaches. The approaches have in common that they promote knowledge and reasoning as the basis for inferring answers from databases with textual information. The discussed technologies are very promising for accurately answering factual questions, and can form a starting point for building systems that answer questions that demand fusion and comparison of information. The paper has shown a roadmap to many open research directions.
 References
