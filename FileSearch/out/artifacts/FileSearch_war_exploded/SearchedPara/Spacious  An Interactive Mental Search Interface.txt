 We introduce in this work a novel approach for semantic indexing and mental image search. Given semantic con-cepts defined by few training examples, our formulation is transductive and learns a mapping from an initial ambient space, related to low level visual features, to an output space spanned by a well defined semantic basis where data can be easily explored. With this method, searching for a mental visual target reduces to scanning data according to their coordinates in the learned semantic space. We illustrate the proposed method through our graphical user interface  X  X pacious X , for the purpose of visualization and interactive navigation in generic image databases and satellite images. [ Users and Interactive IR ]: search interface mental search, subspace learning, data visualization Designing queries, in content based image retrieval, has been an open problem for years. Ideally, a query should not only be accurate but also easy to set. As the user quite often does not know exactly how the image of interest looks like, a search model with feedback based on visualization and interaction is necessary in order to have a global view of an image database and to make the search process effective and also tractable.

There have been efforts [2] in using visualization as a mean to query and collect feedback for image search, using hierar-chical trees or graphs. While the formers are hardly exten-sible for images with complex scenes or objects, the latters make the user getting easily lost due to the complexity of graph structures. Although spectral based dimensionality reduction techniques produce embeddings that preserve lo-cal similarity, their global shapes are often curved or clus-tered, and this makes them difficult to exploit. Recent ap-proaches [1] propose attribute based object models in which a visual object can be retrieved by querying its relative at-tributes, for example  X  X ind image C which is greener than image A but less open than image B. X  These attributes de-fine semantic spaces in which the location of a given image is determined by the scores of binary classifiers associated to these attributes. Even though promising, these methods require training and annotation steps, which are prohibitive especially when handling large scale datasets.

In our proposed method, we share the same perspective as [1] but we rather assume that semantics are described with few training images where each one includes only one semantic; these images are referred to as endmembers . A semantic space is then defined by assigning the endmembers to their corresponding semantic coordinates. By adopting an appropriate metric, we define the membership (and also the embedding) of any given image with respect to the axes of the learned semantic space. Thereby, searching for a mental visual target simply reduces to scanning and targeting data according to their coordinates in that semantic space. We apply this proposed method, using a novel software, referred to as  X  X pacious X . We now describe how Spacious performs interactive visualization and mental querying on real-world scenarios. The technical details are mentioned in Appendix. Nowadays, map services (Google Maps, GeoPortail, Bing, etc.) are becoming very popular; however, they are pow-erless to search for locations based on their visual content. Indeed, with these services, searching large satellite images is usually systematic and tedious (especially when metadata are scarce or not intuitive to the user). Spacious is an alter-native, based on a novel semantic subspace learning method (see appendix), that allows the user to visualize, explore and localize visual targets efficiently. Spacious is built on top of PartiView 1 , and it allows us to visualize and explore satel-lite images including more than ten thousands image regions. This software also provides different navigation modes, in-cluding region subset selection and filtering.

Given a satellite image, we split it into approximately 12,000 rectangular patches and we encode each patch using low level visual features. We also consider a vocabulary of semantics including building , road , vegetation and water ; for each semantic, we select 15 image patches and their mem-berships are set accordingly. Fig. 1(a), 1(d) shows a visu-alization of the learned embedding with Spacious. When http://www.haydenplanetarium.org/universe/partiview traversing dimensions separately, we observe a gradual vari-ation of the underlying semantics whereas in the span of these dimensions, patches mix several semantics. Similarly, we consider a vocabulary of semantics which is more diverse than in satellite images. This diversity results from the large number of concepts present into generic im-ages. We learn a semantic space using endmembers as dis-cussed earlier, and a user searches for a mental target by specifying and refining the abundance of each axis in the learned semantic space. In practice, we run experiments on a subset of the LabelMe database; we segment each image into non overlapping regions, and we describe each one us-ing visual features including SIFT, color histogram, texton histogram and GIST. Among 2,688 images, we select a sub-set of 200 pictures as endmembers in order to learn a twelve dimensional semantic space. Fig. 1(c), 1(d) shows a visual-ization of that space in 3D using  X  X pacious X . This work was supported in part by a grant from the Research Agency ANR (Agence Nationale de la Recherche) under the MLVIS project and a grant from CNES (Centre National d X  X tudes Spatiales) under the VENISE project. [1] A. K. et al. Whittlesearch: Image search with relative [2] D. Heesch. A survey of browsing models for content Let X  X  R d  X  n be a matrix of n input data points and Y  X  [0 , 1] K  X  n its underlying membership matrix where Y ki iff the k th semantic is present into X .i ; K is the number of semantics. Considering the first ` points as endmembers (i.e., each one contains only one semantic), the membership vector Y .i , of each endmember, is set to one of the canonical base vectors. Now, our problem learns an embedding Z  X  R K  X  n where each entry Z ki corresponds to a mapping of X .i into the k th semantic dimension min here tr stands for the matrix trace operator. The main term, in (1), is a regularizer that ensures similar embedding for neighboring data in X and L is a normalized graph Lapla-cian. The first ` constraints guarantee that the embedding { Z .i } ` i =1  X { Y .i } ` i =1 while the last n  X  ` constraints control the memberships of data points to different semantic axes.
