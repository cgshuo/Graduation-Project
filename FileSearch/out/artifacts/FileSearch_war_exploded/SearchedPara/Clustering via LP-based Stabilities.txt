 Clustering is considered as one of the most fundamental unsu pervised learning problems. It lies at the heart of many important tasks in machine learning, pat ter recognition, computer vision, data mining, biology, marketing, just to mention a few of its appl ication areas. Most of the clustering methods are center-based, thus trying to extract a set of clu ster centers that best  X  X escribe X  the input data. Typically, this translates into an optimization prob lem where one seeks to assign each input data point to a unique cluster center such that the total sum o f the corresponding distances is min-imized. These techniques are extremely popular and they are thus essential even to other types of clustering algorithms such as Spectral Clustering methods [1],[2].
 Currently, most center-based clustering methods rely on EM -like schemes for optimizing their clus-tering objective function [3]. K-means is the most characte ristic (and perhaps the most widely used) technique from this class. It keeps greedily refining a current set of cluster centers based on a simple gradient descent scheme. As a result, it can very eas ily get trapped to bad local minima and is extremely sensitive to initialization. It is thus lik ely to fail in problems with, e.g., a large number of clusters. A second very important drawback of many center-based clustering methods, which severely limits their applicability, is that they eit her require the input data to be of vectorial form and/or impose strong restrictions on the type of distan ce functions they can handle. Ideally, one would like to be able to cluster data based on arbitrary di stances. This is an important point because, by an appropriate choice of these distances, clust ering results with completely different characteristics can be achieved [4]. In addition to that, on e would prefer that the number of clusters is automatically estimated by the algorithm (e.g., as a bypr oduct of the optimization process) and not given as input. In contrast to that, however, many algori thms assume that this number is known a priori. To circumvent all the issues mentioned above, a novel center -based clustering algorithm is proposed in this paper. Similarly to other methods, it reduces cluste ring to a well-defined (but NP-hard) minimization problem, where, of course, the challenge now i s how to obtain solutions of minimum objective value. To this end, we rely on the fact that the abov e problem admits a linear integer programming formulation. By making heavy use of a dual LP rel axation to that program, we then manage to derive a dual based algorithm for clustering. As in all center-based clustering techniques, the most critical component in the resulting algorithm is de ciding what cluster centers to choose. To this end, we introduce, what we call, the stability of a dat a point as a cluster center (this is an LP-based quantity), which we consider as another contribut ion of this work. Intuitively, the stability of a data point as a cluster center tries to measure how much we need to penalize that point (by appropriately modifying the objective function) such that it can no longer be chosen as a center in an optimal solution of the modified problem. Obviously, one w ould like to choose as centers those points having high stability. For applying this idea in prac tice, however, a crucial issue that one needs to deal with is how to efficiently approximate these stabilit y measures. To this end, we introduce, what we call, the margins, another very important concept in our algorithm and a key contribution of our work. As we prove in this paper, margins can be considered as dual to stabilities. Furthermore, they allow us to approximate the latter on the fly, i.e., as our algorithm runs. The outcome is an efficient and very easily implementable optimization algor ithm, which works in the dual domain by iteratively updating a dual solution via two very simple o perations: DISTRIBUTE and PROJECT . It can cluster data based on an arbitrary set of distances, wh ich is the only input required by the algorithm (as a result, it can find use in a wide variety of appl ications, even in case where non-vectorial data need to be used). Furthermore, an important p oint is that, despite its generality, it does not get trapped to bad local minima. It is thus insensitive to initialization and can always compute clusterings of very low cost. Similarly to [5], the number of clusters does not need to be predefined, but is decided on the fly during the optimization process. How ever, unlike [5], convergence of the proposed method is always guaranteed and no parameters X  adj ustment needs to take place for this. Finally, an additional advantage of our method is that it can provide online optimality guarantees, which can be used for assessing the quality of the generated c lusterings. These guarantees come in the form of lower bounds on the cost of the optimal clustering and are computed (for free) by simply using the cost of the dual solutions generated during the cou rse of the algorithm. Given a set of objects V with distances d = { d pq } , clustering amounts to choosing a set of cluster centers from V (say { q i } k i =1 ) such that the sum of distances between each object and its cl osest center is minimized. To this end, we are going to use the follo wing objective function E ( ) (which will be referred to as the primal cost hereafter): Note that, in this case, we require that each cluster is chose n from the set V . Also note that, besides { q i } , here we optimize over the number of cluster centers k as well. Of course, to avoid the trivial solution of choosing all objects as centers, we regularize t he problem by assigning a penalty d qq to each chosen center q . Problem (1) has an equivalent formulation as a 0  X  1 linear integer program [6], whose relaxation leads to the following LP (denoted by P RIMAL hereafter): To get an equivalent problem to (1), we simply have to replace x pq  X  0 with x pq  X  { 0 , 1 } . In this case, each binary variable x pq with p 6 = q indicates whether object p has been assigned to cluster center q or not, while binary variable x qq indicates whether object q has been chosen as a cluster center or not. Constraints (3) simply express the fact that e ach object must be assigned to exactly one center, while constraints (4) require that if p has been assigned to q then object q must obviously be chosen as a center. Obviously at the core of any clustering problem of this type l ies the issue of deciding which objects will be chosen as centers. To deal with that, a key idea of our a pproach is to rely on, what we call, the stability of an object. This will be a well defined measure which, intuit ively, tries to quantitatively answer the following question:  X  X ow much do we need to penalize an object in order to ensure th at it is never selected as an optimal cluster center? X  For formalizing this concept, we will make use of the LP relaxation P RIMAL . We will thus define the stability S ( q ) of an object q as follows: An object q can be stable or unstable depending on whether it holds S ( q )  X  0 or S ( q ) &lt; 0 . To select a set of centers Q , we will then rely on the following observation: a stable object with high stability is also expected to be, with high probability, an o ptimal center in (1) . The reason is that the assumption of a high S ( q )  X  0 is essentially a very strong requirement (much stronger tha n simply requiring q to be active in the relaxed problem P RIMAL ): it further requires that q will be active for all problems P RIMAL ( d qq + s ) 1 as well (where s  X  S ( q ) ). Hence, our strategy for generating Q will be to sequentially select a set of stable objects, trying, at ea ch step, to select an object of approximately maximum stability (as already explained, there is high chan ce that this object will be an optimal center in (1)). Furthermore, each time we insert a stable obj ect q to Q , we reestimate stabilities for the remaining objects in order to take this fact into account (e.g., an object may become unstable if we know that it holds x qq = 1 for another object q ). To achieve that, we will need to impose extra constraints to P RIMAL (as we shall see, this will help us to obtain an accurate estim ation for the stabilities of the remaining objects given that objects in Q are already chosen as centers). Of course, this process repeats until no more stable objects can be foun d. 2.1 Margins and dual-based clustering For having a practical algorithm, the most critical issue is how to obtain a rough approximation to the stability of an object q in a computationally efficient manner. As we shall see, to ach ieve this we will need to to move to the dual domain and introduce a novel concept that lies at the core of our approach: the margin of dual solutions. But, first, we need to introduce the dual to problem P RIMAL , which is the linear program called D UAL in (7) 2 : Dual variables h pq can be thought of as representing pseudo-distances between objects, while each variable h p represents the minimum pseudo-distance from p (which is, in fact,  X  X hought X  by the dual as an estimation of the actual distance between p and its closest active center).
 Given a feasible dual solution h , we can now define its margin  X  q ( h ) (with respect to object q ) as follows: where (for any h )  X  h p hereafter denotes the next-to-minimum pseudo-distance fr om p .
 There is a very tight connection between margins of dual solu tions and stabilities of objects. The following lemma provides a first indication for this fact and shows that we can actually use margins to decide whether an object is stable or not and also to lower b ound or upper bound its stability accordingly (see [7] for proofs): Lemma 1 ([7]) . Let h be an optimal dual solution to D UAL . In fact, the following fundamental theorem goes even furthe r by proving that stabilities can be fully characterized solely in terms of margins. Hence, margins an d stabilities are two concepts that can be roughly considered as dual to each other: Theorem 2 ([7]) . The following equalities hold true: Furthermore, it can be shown that: What the above theorem essentially tells us is that one can co mpute S ( q ) exactly, simply by consid-ering the margins of optimal dual solutions. Based on this fa ct, it is therefore safe to assume that solutions h with high (but not necessarily maximum) dual objective D ( h ) will have margins that are good approximations to S ( q ) , i.e., it holds: This is exactly the idea that our clustering algorithm will r ely on in order to efficiently discover objects that are stable. It thus maintains a dual solution h and a set Q containing all stable objects chosen as centers up to the current point ( Q is empty initially). At each iteration, it increases the dual objective D ( h ) by updating solution h via an operation called DISTRIBUTE . This operation is repeatedly applied until a high enough objective value D ( h ) is obtained such that at least one stable object is revealed based on the estimated margins of h . At that point, the set Q is expanded and h is updated (via an operation called PROJECT ) to take account of this fact. The process is then repeated until no more stable objects can be found. A remarkable thing to note in this process is that, as we shall see, determining how to update h during the DISTRIBUTE operation (i.e., for increasing the dual objective) also relies critically on the use of margins .
 Another technical point that we need to solve comes from the f act that Q gets populated with objects as the algorithm proceeds, which is something that we certai nly need to take into account when estimating object stabilities. Fortunately, there is a ver y elegant solution to this problem: since all objects in Q are assumed to be cluster centers (i.e., it holds x qq = 1 ,  X  q  X  Q ), instead of working with problems P RIMAL and D UAL , it suffices that one works with the following primal-dual pa ir of LPs called P RIMAL Q and D UAL Q 3 : This means, e.g., that stability S ( q ) is now defined by using P RIMAL Q (instead of P RIMAL ) in (6). Likewise, lemma 1 and theorem 2 still continue to hold true pr ovided that D UAL is replaced with D
UAL Q in the statement of these theorems. In addition to that, the d efinition of margin  X  q ( h ) needs to be modified as follows :  X  q ( h ) = X The PROJECT operation: Given this modified definition of margins, we can now update Q at any iteration in the following manner: Based on the fact that margins are used as approximations to t he stabilities of objects, the above update simply says that the object  X  q with maximum stability should be chosen as the new center at the current iteration, provided of course that this object  X  q is stable. Furthermore, in this case, we also need to update the current dual solution h in order to take account of the fact that extra constraints have been added to D UAL Q (these are a result of the extra constraint x  X  q  X  q = 1 that has been added to P and, so, one has to apply the following operation, which simp ly projects the current dual solution into the feasible set of the updated linear program D UAL Q : Note that update h pp += h  X  qp  X  d  X  qp is needed for maintaining dual feasibility constraint (9). Essen-tially, PROJECT is a warm-start operation, that allows us to reuse existing i nformation for computing a solution h that has a high dual objective value D ( h ) and is also feasible to the updated D UAL Q . The DISTRIBUTE operation: In case it holds  X  q ( h ) &lt; 0 for all q /  X  Q , this means that we are unable to find an object with good stability at the current ite ration. To counter that, we will thus need to update solution h in order to increase its dual objective value (recall that, b y lemma 1, stable objects will necessarily be revealed at an optimal dual solu tion, i.e., at a dual solution of maximum objective). Intuitively, what happens is that as we increas e the dual objective D ( h ) , objects not in Q actually try to compete with each other for achieving a large margin. Interestingly enough, in order to increase D ( h ) , we will again have to rely on the margins of the current dual s olution. In update of h is guaranteed to increase the dual objective: In the above update, we denote by L Q the set of objects whose minimum pseudo-distance h p is attained at an object from Q , i.e., L Q = { p /  X  Q| h p = min q  X  X  h pq } , while |V q | denotes the cardinality of the set V q = { p /  X  X  X  X  Q | h p  X  d pq } X  X  q } . The following theorem then holds true: Theorem 3. If max q /  X  X   X  q ( h ) &lt; 0 , then the DISTRIBUTE operation maintains feasibility and, unless V = Q X  X  Q , it also strictly increases the dual objective.
 The pseudocode of the resulting algorithm is shown in Fig. 1. As already explained, it is an iterative algorithm, which keeps updating a dual solution h by using the DISTRIBUTE and PROJECT opera-tions (the latter applied only when needed) until the dual ob jective can no longer increase. Note also that, besides maintaining a dual solution h , the algorithm also maintains Q which provides a current clustering and also has a primal cost E ( Q ) . With respect to this cost, the following theorem can be shown to hold true: Theorem 4. If max q /  X  X   X  q ( h ) &gt; 0 , then the EXPAND operation strictly decreases the primal cost E ( Q ) .
 This implies that the sequence of primal costs E ( Q ) generated by the algorithm is decreasing (recall that we actually want to minimize E ( ) ). It is worth noting at this point that nowhere have we tried to enforce this property by explicitly considering th e primal cost when updating Q . This is achieved simply thanks to the requirement of always selec ting objects with high stability, thus showing how powerful this requirement actually is. We also n ote that the algorithm X  X  convergence is always guaranteed: the algorithm terminates when neithe r the primal cost E ( Q ) decreases nor the dual objective D ( h ) increases during the current iteration. Finally, we note th at exactly the same algorithm applies to the general case where the objects in V form a graph with edges E (distance d pq is then defined only for pq  X  X  ). In this case, it is easy to verify that the cost of each itera tion will be O ( |E| ) . Furthermore, the algorithm converges extremely fast in pr actice (i.e. in very few iterations). Before proceeding, let us briefly mention how our method rela tes to some state-of-the-art exemplar-based clustering techniques. Affinity propagation [5] is a r ecently proposed method for clustering, which relies on minimizing exactly the same objective funct ion (1). This is an iterative algorithm, which repeatedly updates (through messages) the so-called responsibilities and availabilities. These can be considered as counterparts to our pseudo-distances h pq . Affinity propagation also estimates the so-called self-availabilities for measuring the likel ihood of an object being a cluster center. On the contrary, we use for the same purpose the margins that app roximate the stability of an object. Furthermore, compared to affinity propagation, our method o ffers the following significant advan-tages: its convergence is always guaranteed, it is paramete r-free (no need for adjusting parameters such as damping factors in order to ensure convergence), it i s a descent method (objective func-tion (1) always decreases), and it can make use of the compute d dual solutions for deriving online optimality bounds for free (these can be used for assessing t hat the derived solutions are almost optimal). At the same time, our method performs equally well or better in practice. Very recently, another exemplar-based algorithm has been proposed as well , which relies on solving a convex for-mulation of clustering [8]. We note, however, that this meth od is used for solving a different and much easier problem, which is that of soft clustering. Furth ermore, it relies on a convex relaxation which is known to be much less tight than the LP relaxation P RIMAL we use here (essentially [8] replaces all constraints x pq  X  x qq ,  X  p  X  V with the much looser constraint P p x pq  X  |V| x qq ). As a result, generated solutions are expected to be of much lo wer quality. We also note that, unlike EM-like clustering algorithms such as K-means, our method i s totally insensitive to initialization conditions and does not get stuck at bad local minima (thus yi elding solutions of much better qual-ity). Also, it is much more efficient than methods like [6], th at require solving very large linear programs. To illustrate the robustness of our algorithm to noise and it s insensitivity to initialization, we start by showing clustering results on synthetic data. The synthe tic datasets were generated using the following procedure: 2D points were sampled from a mixture o f gaussian distributions, where the centers of the gaussians were arranged in an approximately g rid-like fashion over the plane. In addition to that, random outliers were generated uniformly all over the grid, with their number being equal to half the number of the points drawn from the gaussian distributions. One such dataset (consisting of 24 gaussians) is displayed in Fig. 2, where co lored crosses correspond to samples from gaussians, while the black dots correspond to outliers . The clustering result produced by our algorithm is shown in Fig. 2(a). As can be seen from that figure , despite the heavy percentage of noise, our method has been able to accurately detect all gaus sian centers and successfully cluster this 2D dataset. Note that the number of gaussians was not giv en as input to our algorithm. Instead, it was inferred based on a common penalty term d qq for all objects q , which was set roughly equal to the median distance between points. On the contrary, K-mean s was unable to produce a good result for this dataset despite the fact that it was restarted multi ple times (100 runs were used in this case). This is, of course, due to its well known sensitivity to initi alization conditions. We repeated multiple experiments by varying the number of gaussians. Contrary to our algorithm, behavior of K-means gets even worse as this number increases.
 We have also plotted in Fig. 2(c) the primal and dual costs tha t were generated by our algorithm when it was applied to the example of Fig. 2(a). These corresp ond to the solid red and dashed blue curves respectively. Note that the dual costs represent low er bounds to the optimum value of the objective function E ( ) , while the primal costs represent obviously upper bounds. T his fact allows us to obtain online optimality bounds with respect to how far our current primal solution Q is with respect to the unknown optimum of E ( ) . These bounds are, of course, refined continuously as the algorithm proceeds and can be useful for assessing its perfo rmance. For instance, in this particular example, we can be sure that the primal cost of our final soluti on is within 1% of the unknown optimum of function E ( ) , i.e., an approximately optimal solution has been obtained .
 Next we show some results from applying our algorithm to the c hallenging problem of multibody 3D segmentation, which has several applications in computer v ision. As we shall see, a non-Euclidean distance for clustering will have to be used in this case. Acc ording to the 3D segmentation problem, we are given a set of N pixel correspondences between two images. These correspon dences result from K objects undergoing K 3D rigid-body motions relative to a moving camera. The 3D-mo tion segmentation problem is the task of clustering these N pixel pairs according to the K moving ob-jects. We consider the more general and difficult scenario of a fully projective camera model. In this case, each pixel pair, say, p i = ( y i , z i ) that belongs to a moving object k should satisfy an epipolar constraint: where F k represents the fundamental matrix associated with the k-th 3D motion. Of course, the matrices F k corresponding to different motions are unknown to us. Hence , to solve the 3D segmen-tation problem, we need to estimate both the matrices F k as well as the association of each pixel pair p i = ( y i , z i ) to the correct fundamental matric F k . To this end, we sample a large set of fun-damental matrices by using a RANSAC-based scheme (we recall that a random set of, e.g., 8 pixel pairs p i is enough for generating a new fundamental matrix). The resu lting matrices, say, { F k } will then correspond to cluster centers, whereas all the input pi xel pairs { p i } will correspond to objects that need to be assigned to an active cluster center. A cluste ring objective function of the form (1) thus results and by minimizing it we can also obtain a solutio n to the 3D segmentation problem. Of center will not be Euclidean. Instead, based on (19), we can u se a distance of the following form: Due to being more robust, a normalized version of the above di stance is usually preferred in practice. Figure 3 displays 3D motion segmentation results that were o btained by applying our algorithm to two image pairs (points with different colors correspond to different motions). These examples were downloaded from a publicly available motion segmentat ion database [9] with ground-truth. The ground-truth motion segmentation is also shown for each example and, as can be seen, it is almost identical with the segmentation estimated by our alg orithm.
 We next compare our method to Affinity Propagation (AP). Some really impressive results on 4 very challenging datasets have been reported for that algor ithm in [5], indicating that it outperforms any other center-based clustering method. In particular, A P has been used for: clustering images of faces (using the squared error distance), detecting gene s in microarray data (using a distance based on exons X  transcriptions levels), identifying repre sentative sentences in manuscripts (using the relative entropy as distance), and identifying cities t hat can be easily accessed by airline travel. In Fig. 4(a), we compare our method to AP on these publicly ava ilable problems. Since both methods rely on optimizing the same objective function, we list the v alues obtained by the two methods for the corresponding problems. Exactly the same settings have been used for both algorithms, with AP using the parameters proposed in [5]. Note that in all case s our algorithm manages to obtain a solution of equal or lower value than AP. This is true even, e .g., in the Genes dataset, where a higher number of clusters is selected by our algorithm (and thus a higher penalty for activating them is paid). Furthermore, an additional advantage of our a lgorithm is that, unlike AP, it is always guaranteed to converge (e.g., see Figs 4(b), 4(c)). We note t hat, due to lack of space, a running time comparison with AP, as well as a comparison of our algorithm t o the method in [10], are included in [7]. In this paper we have introduced a very powerful and efficient center-based clustering algorithm, derived from LP duality theory. The resulting algorithm has guaranteed convergence and can handle data sets with arbitrary distance functions. Furthermore, despite its extreme generality, the proposed method is insensitive to initialization and computes clust erings of very low cost. As such, and considering the key role that clustering has in many problem s, we believe that our method can find use in a wide variety of tasks. As another very important (bot h practical and theoretical) contribution of this work we also consider the fact of introducing the noti ons of LP-based stabilities and margins, two quantities that, as we have proved, are dual to each other and can be used for deciding what objects should be chosen as cluster centers. We strongly bel ieve that these ideas can be of both practical and theoretical interest not just for designing c enter-based clustering algorithms, but also in many other contexts as well.

