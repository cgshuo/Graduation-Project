 Andrea Esuli  X  Tiziano Fagni  X  Fabrizio Sebastiani Abstract Hierarchical Text Categorization (HTC) is the task of generating (usually by means of supervised learning algorithms) text classifiers that operate on hierarchically structured classification schemes. Notwithstanding the fact that most large-sized classifica-tion schemes for text have a hierarchical structure, so far the attention of text classification researchers has mostly focused on algorithms for  X  X  X lat X  X  classification, i.e. algorithms that operate on non-hierarchical classification schemes. These algorithms, once applied to a hierarchical classification problem, are not capable of taking advantage of the information inherent in the class hierarchy, and may thus be suboptimal, in terms of efficiency and/or effectiveness. In this paper we propose T REE B OOST .MH, a multi-label HTC algorithm con-sisting of a hierarchical variant of A DA B OOST .MH, a very well-known member of the family of  X  X  X oosting X  X  learning algorithms. T REE B OOST .MH embodies several intuitions that had arisen before within HTC: e.g. the intuitions that both feature selection and the selection of negative training examples should be performed  X  X  X ocally X  X , i.e. by paying attention to the topology of the classification scheme. It also embodies the novel intuition that the weight distribution that boosting algorithms update at every boosting round should likewise be updated  X  X  X ocally X  X . All these intuitions are embodied within T REE B OOST .MH in an elegant and simple way, i.e. by defining T REE B OOST .MH as a recursive algorithm that uses A DA B OOST .MH as its base step, and that recurs over the tree structure. We present the results of experimenting T REE B OOST .MH on three HTC benchmarks, and discuss analytically its computational cost. Keywords Hierarchical text classification Boosting 1 Introduction Hierarchical text categorization (HTC) is the task of generating (usually by means of supervised learning algorithms) text classifiers that operate on classification schemes endowed with a hierarchical structure. Notwithstanding the fact that most large-sized classification schemes for text (e.g. the ACM Classification Scheme 1 the MESH thesaurus 2 the NASA thesaurus 3 ) indeed have a hierarchical structure, so far the attention of text classification (TC) researchers has mostly focused on algorithms for  X  X  X lat X  X  classification, i.e. algorithms that operate on non-hierarchical classification schemes. 4 These algorithms, once applied to a hierarchical classification problem, are not capable of taking advantage of the information inherent in the class hierarchy, and may thus be suboptimal, in terms of efficiency and/or effectiveness. On the contrary, many researchers have argued that by leveraging on the hierarchical structure of the classification scheme, heuristics of various kinds can be brought to bear that make the classifier more efficient and/or more effective.
An important intuition is that, by viewing classification as the identification of the paths that, starting from the root, funnel the document down to the subtrees where it belongs (in  X  X  X achinko machine X  X  style), entire other subtrees can be pruned from consideration. That is, when the classifier corresponding to an internal node outputs a negative response, the classifiers corresponding to its descendant nodes need not be invoked any more, thus reducing the computational cost of classifier invocation exponentially (Chakrabarti et al. 1998 ; Koller and Sahami 1997 ).

A second important intuition is that, by training a binary classifier for an internal node category on a well-selected subset of training examples of local interest only, the resulting classifier may be made more attuned to recognizing the subtle distinctions between doc-uments belonging to that node and those belonging to neighbouring nodes (Ng et al. 1997 ; Wiener et al. 1995 ). While this technique promises to bring about more effective classi-training, thereby making classifier learning speedier.

Many of these intuitions have been used in close association with a specific learning algorithm; the most popular choices in this respect have been na X   X  ve Bayesian methods (Chakrabarti et al. 1998 ; Gaussier et al. 2002 ; Koller and Sahami 1997 ; McCallum et al. 1998 ; Toutanova et al. 2001 ; Vinokourov and Girolami 2002 ), neural networks (Ruiz and Srinivasan 2002 ; Weigend et al. 1999 ; Wiener et al. 1995 ), support vector machines (Cai and Hofmann 2004 ; Dumais and Chen 2000 ; Liu et al. 2005 ; Yang et al. 2003 ), and example-based classifiers (Yang et al. 2003 ).

Within this literature, the absence of  X  X  X oosting X  X  methods is conspicuous: to the best of our knowledge, we do not know of any HTC method belonging to the boosting family. This is somehow surprising, (i) because of the high applicative interest of HTC, (ii) because boosting algorithms are well-known for their interesting theoretical properties and for their high accuracy, and (iii) because, given their relatively high computational cost, they would definitely benefit by the added efficiency that consideration of the hierarchical structure can bring about.

In this paper we try to fill this gap by proposing T REE B OOST .MH, a multi-label HTC algorithm that consists of a hierarchical variant of A DA B OOST .MH, the most important member of the family of boosting algorithms; here, multi-label (ML) means that a docu-ment can belong to zero, one, or several categories at the same time. T REE B OOST .MH embodies several intuitions that had arisen before within HTC: e.g. the intuitions that both feature selection and the selection of negative training examples should be performed  X  X  X ocally X  X , i.e. by paying attention to the topology of the classification scheme. T REE-B
OOST .MH also incorporates the novel intuition that the weight distribution that boosting algorithms update at every boosting round should likewise be updated  X  X  X ocally X  X . All these intuitions are embodied within T REE B OOST .MH in an elegant and simple way, i.e. by defining T REE B OOST .MH as a recursive algorithm that uses A DA B OOST .MH as its base step, and that recurs over the tree structure.

The paper is structured as follows. In Sect. 2 we give a concise description of boosting and the A DA B OOST .MH algorithm. Section 3 describes T REE B OOST .MH, our hierarchical version of A DA B OOST .MH. Section 4 goes the analytical way, comparing the computational costs of A DA B OOST .MH and T REE B OOST .MH, and showing that the latter obtains expo-nential savings over the former both at classifier-learning time and at classification time. In Sect. 5 we present experiments comparing A DA B OOST .MH and T REE B OOST .MH on three well-known HTC benchmarks, including a hierarchical version of the R EUTERS -21578 benchmark defined in Toutanova et al. ( 2001 ). Section 6 discusses related work, pointing out the differences between existing approaches and ours. Section 7 concludes. 2 An introduction to boosting and AdaBoost.MH A
DA B OOST .MH (Schapire and Singer 2000 ) (see Fig. 1 )isa boosting algorithm, i.e. an algorithm that generates a highly accurate classifier ^ U (also called final hypothesis )by combining a set of moderately accurate classifiers ^ U 1 ; ... ; ^ U S (also called weak hypothe-C i C is the set of categories to each of which d i belongs. For each c j [ C ,by Tr + ( c j )we denote the set of the positive training examples of c j . Furthermore, for each c j [ C we define the set Tr -( c j ) of its negative training examples simply as the set difference between Tr and Tr + ( c j ).

A DA B OOST .MH works by iteratively calling a weak learner to generate a sequence ^ U ; ... ; ^ U S of weak hypotheses; at the end of the iteration the final hypothesis ^ U is obtained as a sum ^ U  X  D C ! R such that sign  X  ^ U s  X  d i ; c j  X  X  can be interpreted as the prediction of ^ U s on whether d belongs to c j (i.e. ^ U s  X  d i ; c j  X  [ 0 means that d i is believed to belong to c j while ^ U (indicated by j ^ U s  X  d i ; c j  X j ) can be interpreted as the strength of this belief.
At each iteration s A DA B OOST .MH tests the effectiveness of the newly generated weak hypothesis ^ U s on the training set and uses the results to update a distribution D s of weights belongs to category c j or not. By passing (together with the training set Tr ) this distribution to the weak learner, A DA B OOST .MH asks this latter to generate a new weak hypothesis ^ U s  X  1 that concentrates on the pairs with the highest weight, i.e. those that had proven harder to classify for the previous weak hypotheses.

The initial distribution D 1 is uniform. At each iteration s all the weights D s ( d i , c j ) are updated to D s +1 ( d i , c j ) according to the rule where the target function U  X  d i ; c j  X  is defined to be 1 if d i [ c j and -1 otherwise, and is a normalization factor chosen so that D s +1 is in fact a distribution, i.e. so that P pair correctly classified by ^ U s is decreased. Weights are increased or decreased to a larger decisions taken with high confidence must have a higher impact in the process. 2.1 Choosing the weak hypotheses binary weights, where w ki = 1 (resp. w ki = 0) is normally interpreted to mean that term t k in at least one document in Tr . Of course, A DA B OOST .MH does not make any assumption on what constitutes a term; single words, stems of words, phrases, or character n -grams are all plausible choices.

In A DA B OOST .MH the weak hypotheses generated by the weak learner at iteration s are decision stumps of the form The choices for t k , a 0 j and a 1 j are in general different for each iteration s , and are made according to an error-minimization policy described in the rest of this section.

Schapire and Singer ( 1999 ) have proven that the Hamming loss of the final hypothesis ^ (in)effectiveness; therefore, a reasonable (although suboptimal) way to maximize the effectiveness of the final hypothesis ^ U is to  X  X  X reedily X  X  choose each weak hypothesis ^ U s (and thus its parameters t k , a 0 j and a 1 j ) in such a way as to minimize the normalization factor Z s .

Schapire and Singer ( 2000 ) define three different variants of A DA B OOST .MH, corre-sponding to three different methods for making these choices: 1. A DA B OOST .MH with real-valued predictions (here nicknamed AdaBoost.MH R ); 2. A DA B OOST .MH with real-valued predictions and abstaining ( AdaBoost.MH RA ); 3. A DA B OOST .MH with discrete-valued predictions ( AdaBoost.MH D ).

In this paper we concentrate on AdaBoost.MH R ; since it is the one that, in the experiments of Schapire and Singer ( 2000 ), has been experimented most thoroughly and has given the best results; however, everything we say in this paper about AdaBoost.MH R straightforwardly applies to AdaBoost.MH RA and AdaBoost.MH D :
At iteration s , AdaBoost.MH R (from now on simply called A DA B OOST .MH) chooses a weak hypothesis of the form described in Eq. 3 by the following algorithm.
 Algorithm 1 (The A DA B OOST .MH weak learner)
Step 1 is clearly the key step, since there are a non-enumerable set of weak hypotheses with t k as the pivot. Schapire and Singer ( 1999 ) have proven that, given term t k and category c j , where for b [ {1, -1}, x [ {0, 1}, j [ {1, ... ,m } and k [ {1, ... ,r }, and where [[ p ]] indicates the otherwise). For term t k and for these values of a xj we obtain value in the two following cases: 1. w ki = 1 (i.e. t k occurs in d i ) and the majority of the training documents in which t k
In all the other cases ^ U s outputs a negative real value. Here,  X  X  X ajority X  X  has to be understood in a weighted sense, i.e. by bringing to bear the weight D s ( d i , c j ) associated to ^ U  X  d i ; c j  X  is; this means that this absolute value represents a measure of the confidence that ^ U s has in its own prediction (Schapire and Singer 1999 ).
 may produce outputs with a very large or infinite absolute value when the denominator is very small or zero. 6
The output of the final hypothesis is the value obtained by summing the outputs of the weak hypotheses. 2.2 Implementing AdaBoost.MH Following Sebastiani et al. ( 2000 ), in our implementation of A DA B OOST .MH we have further optimized the final hypothesis ^ U  X  d i ; c j  X  X  f ^
U 1 ; ... ; ^ U S g contains a subset f ^ U same term t k and are of the form same as that of a  X  X  X ombined hypothesis X  X  In the implementation we have thus replaced D is the number of different terms that act as pivot for the weak hypotheses in f ^
U 1 ; ... ; ^ U S g :
This modification brings about a considerable efficiency gain in the application of the final hypothesis to a test example. For instance, the final hypothesis we obtained on R
EUTERS -21578 with A DA B OOST .MH when S = 1,000 consists of 1,000 weak hypotheses, but the number of different pivot terms is only 766. The reduction in the size of the final hypothesis which derives from this modification is usually larger when high reduction different terms that can be chosen as the pivot is smaller. A large reduction is also obtained when the total number of iterations S is high, since in this case the terms chosen as pivot in the last iterations tend to be ones that have been chosen already in previous iterations.

In this work we further implement two important optimizations for reducing classifi-cation time.

The first optimization consists in building the vectorial representations d i of the test documents after the final hypothesis ^ U  X  features that act as pivot for some weak hypothesis in ^ U are actually included in the vectorial representations d i of the test documents; the other terms can be discarded, since they play no role in the classification. Since it is usually the case that j D j r ; this brings about a substantial reduction in the space occupied by the d i  X  X . For instance, the size of the vectors that we have obtained by this method on RCV1-V 2 with A DA B OOST .MH for S = 1,000 is 950, while the length r of the original vectors (see Sect. 5.3 ) was equal to 55,051.

The second optimization consists in sorting the final hypothesis ^ U (here viewed for pivot appear in the same order as they appear in the vectorial representations of the documents. As a consequence, we can indeed view the final hypothesis as consisting of constants output by the compressed hypotheses of Eq. 9 . Classification thus amounts to computing
Since one of w ki and (1 -w ki ) is always 0, this amounts to a sum of D real numbers. This is extremely cheap, also due to the fact that D is typically small. Note for example that performing classification with any linear classifier (such as those generated by support vector machines) requires a dot product of length r , which is much more expensive than a sum of D r reals. This makes our system even more classification-time efficient than other leading-edge technologies.
 3 A hierarchical version of AdaBoost.MH for multi-label TC In this section we describe a version of A DA B OOST .MH, called T REE B OOST .MH, that is explicitly designed to work on tree-structured sets of categories, and is capable of lever-aging on the information inherent in this structure. 3.1 Notation, definitions, and the semantics of hierarchies notation and definitions. Let C be a tree-structured set of categories, and let r be its root category. For each category c j [ C , we will use the following abbreviations:
When discussing an HTC application it is always important to specify what the semantics of the hierarchy is, i.e., to specify the semantic constraints that a supposedly perfect classifier would enforce; which constraints are in place has important consequences on which algorithms we might want to apply to this task, and, more importantly, on how we should evaluate these algorithms.

For instance, one should specify whether a document can belong to zero, one, or several categories in C (which is indeed the case of this paper) or whether it always belongs to one and only one category in C .

No less importantly, one should specify whether it is the case that 1. a document d that is a positive example of a category c j is also a positive example of 2. a document d can in principle be a positive example of an internal node category c j and
Assumption 2 is indeed useful for tackling datasets, such as RCV1-V 2 (see Sect. 5.1 )in which documents with these characteristics do occur, while at the same time not preventing us to deal with datasets with the opposite characteristics. A consequence of these two assumptions is that i.e., the set Tr + ( c j ) of the positive training examples of a nonleaf category c j is a (possibly proper) superset of the union of the sets of positive training examples of all its descendant (leaf) categories. 3.2 The rationale T
REE B OOST .MH (which is fully illustrated in Fig. 2 ) embodies several intuitions that had arisen before within HTC.

The first, fairly obvious intuition (which lies at the basis of practically all HTC algo-rithms proposed in the literature) is that, in a hierarchical context, the classification of a document d i is to be seen as a descent through the hierarchy, from the root to the (internal or leaf node) categories where d i is deemed to belong. In ML classification this means that each nonroot category c j has an associated binary classifier ^ U j which acts as a  X  X  X ilter X  X  that prevents unsuitable documents to percolate to lower levels. All test documents that a classifier ^ U j deems to belong to c j are passed as input to all the binary classifiers corre-sponding to the categories in # X  c j  X  ; while the documents that ^ U j deems not to belong to c j are  X  X  X locked X  X  and analysed no further. Note that it may well be the case that a document d i sponding to the categories in # X  c j  X  ; this is indeed consistent with assumption (b) above. In the end, each document may thus reach zero, one, or several (leaf or internal node) categories, and is thus classified under them.

The second intuition is that the training of ^ U j should be performed  X  X  X ocally X  X , i.e. by paying attention to the topology of the classification scheme. To see this note that, during mostly) be presented with documents that belong to the subtree rooted in " X  c j  X  ; i.e. with documents that belong to c j and/or to some of the categories in $ X  c j  X  : As a result, the training of ^ U j should be performed by using, as negative training examples, the positive training examples of " X  c j  X  ; with the obvious exception of the documents that are also positive training examples of c j . In particular, training documents that only belong to categories other than those in + X " X  c j  X  X  need not be used. The rationale of this choice is that the negative training examples thus selected are  X  X  X uasi-positive X  X  examples of c j (Schapire et al. 1998 ), i.e. are the negative examples that are closest to the boundary between the positive and the negative region of c j (a notion akin to that of  X  X  X upport vectors X  X  in SVMs), and are thus the most informative negative examples that can be used in training. This is beneficial also from the standpoint of (both training and classification time) efficiency, since fewer training examples and fewer features are involved. In a similar form, this intuition (which we discuss at large in Fagni and Sebastiani 2007 ) had first been presented in Ng et al. ( 1997 ) and Wiener et al. ( 1995 ).

The third intuition is similar, i.e. that feature selection should also be performed  X  X  X ocally X  X , by paying attention to the topology of the classification scheme. As above, if the classifier for " X  c j  X  has performed reasonably well, ^ U j will only (or mostly) be presented with documents that belong to the subtree rooted in " X  c j  X  : As a consequence, for the classifiers corresponding to c j and its siblings, it is cost-effective to employ features that are useful in discriminating (only) among themselves and " X  c j  X  ; features that discriminate among categories lying outside the subtree rooted in " X  c j  X  are too general, and features that discriminate among the subcategories of c j , or among the subcategories of one of its siblings, are too specific. This intuition, albeit in the slightly different context of single-label classification, was first presented in Koller and Sahami ( 1997 ).

T REE B OOST .MH also embodies the novel intuition that the weight distribution that boosting algorithms update at every boosting round should likewise be updated  X  X  X ocally X  X . In fact, the two previously discussed intuitions indicate that hierarchical ML classification is best understood as consisting of several independent (flat) ML classification problems, one for each internal node of the hierarchy: for each such node c j we must generate a number of binary classifiers, one for each c q 2# X  c j  X  : In a boosting context, this means that several independent distributions, each one  X  X  X ocal X  X  to an internal node and its children, should be generated and updated by the process. In this way, the  X  X  X ifficulty X  X  of a category c q will only matter relative to the difficulty of its sibling categories. As discussed in Sect. 4 , this intuition is of key importance in allowing T REE B OOST .MH to obtain expo-nential savings in the cost of training over A DA B OOST .MH. 3.3 The algorithm T
REE B OOST .MH incorporates these four intuitions by factoring the hierarchical ML clas-sification problem into several  X  X  X lat X  X  ML classification problems, one for every internal node in the tree. T REE B OOST .MH learns in a recursive fashion, by identifying internal nodes c and calling A DA B OOST .MH to generate a ML (flat) classifier for the set of categories # X  c j  X  : Alternatively (and more conveniently), this process may be viewed as generating, for each nonroot category c j [ C , a binary classifier ^ U for c j , by means of which hierar-chical classification can be performed as described in Sect. 3.2.

Learning in T REE B OOST .MH proceeds by first identifying whether a leaf category has generated only at internal nodes.

If an internal node c j has been reached, a ML feature selection process may (optionally) be run (line 10) to generate a reduced feature set on which the ML classifier for # X  c j  X  will operate. This may be dubbed a  X  X  X local X  X  feature selection policy, since it takes an inter-mediate stand between the well-known  X  X  X lobal X  X  policy (in which the same set of features features is chosen for each different category in # X  c j  X  ). The glocal policy selects a different set of features for each (maximal) set of sibling categories in C , thus implementing a view of feature selection as described in Sect. 3.2 . 7 Any of the standard feature scoring functions (e.g. information gain, chi-square, odds ratio) can be used, as well as any of the standard feature score globalization methods (e.g. max, weighted average, Forman X  X  ( 2004 ) round robin). Note that all these functions require a precise notion of what the positive and negative training examples of a category are; here, consistently with the  X  X  X ocality X  X  prin-ciple discussed in Sect. 3.2 , the negative training examples of a category c are taken to be the set Tr  X   X "  X  c  X  X  Tr  X   X  c  X  : After the reduced feature set has been identified, T REE B OOST .MH calls upon A DA-B
OOST .MH (line 11) to solve a ML (flat) classification problem for the categories in # X  c j  X  ; again, in order to implement the  X  X  X uasi-positive X  X  policy discussed in Sect. 3.2 , the neg-ative training examples of a category c are taken to be the set Tr  X   X "  X  c  X  X  Tr  X   X  c  X  : Note that restricting the A DA B OOST .MH call to the categories in # X  c j  X  implements the view, discussed in Sect. 3.2 , of several independent,  X  X  X ocal X  X  distributions being generated and updated during the boosting process.

Finally, after the ML classifier for # X  c j  X  has been generated, for each category c q 2# X  c j  X  a recursive call to T REE B OOST .MH is issued (lines 12 X 18) that processes the subtree rooted in c in the same way. The final result is a hierarchical ML classifier in the form of a tree of binary classifiers, one for each nonroot node, each consisting of a committee of S decision stumps.

Note that the generated classifiers would allow us to implement another, alternative view of what the hierarchical ML classifier consists of: instead of a tree of committees, we might have a committee of trees, with each tree T s having a single decision stump (the one generated at iteration s ) at each nonroot node. In this paper we concentrate on the former view, leaving the latter for future investigation. 4 The computational cost of TreeBoost.MH We now analyse the computational costs of A DA B OOST .MH and T REE B OOST .MH, and show that the latter is computationally cheaper than the former, allowing exponential savings at both training and testing time with respect to the former.

Let us first discuss the cost of classifier training. The key steps of A DA B OOST .MH are (i) minimum, over all t k , of such Z s factors. By inspecting Eqs. 5 and 6 we can clearly see that, for each t k , Step (i) requires O ( gm ) operations for each t k , where g is the number of training documents and m is the number of categories; since there are r such terms, the entire step requires O ( gmr ) operations.

The cost of classifier training in T REE B OOST .MH heavily depends on the topology of the tree and on the distribution of positive training examples across the nodes of the tree; in particular, it depends from factors such as the ariety (i.e. branching factor) of each indi-positive training examples in each such node. We will thus limit our analysis to the best case and the worst case, since they are more easily identifiable; the cost of the other cases will be intermediate between these two. The worst possible case is that of a  X  X  X lat X  X , degenerate tree of height 1, i.e. a tree in which all leaf categories are children of the root B
OOST .MH calls A DA B OOST .MH exactly once, and on the entire category set, which means that the two algorithms coincide, and have thus the same cost. The best possible case is more interesting, and coincides with the  X  X  X ully grown X  X  case of a perfectly balanced tree of constant ariety a (in this case the height of the tree is h = log a m ) in which leaf categories have all the same frequency and each document belongs to exactly one leaf category. At B OOST .MH calls A DA B OOST .MH exactly a l -1 times. Since, as from the analysis above, A A
DA B OOST .MH requires O  X  g a l 1 ar  X  operations, given that (i) the training examples involved are not g but only g a l 1 (since we have made the hypothesis that leaf categories are evenly populated and each training example belongs to exactly one leaf category) and (ii) only a (instead of m ) categories are involved. This means that the number of operations required This means that, for each of the g training examples and for each of the r terms, T REE-B OOST .MH performs O ( ah ) operations and A DA B OOST .MH performs O ( m ) operations. Given that m = a h , this means that, at training time, T REE B OOST .MH is cheaper than A DA B OOST .MH by an exponential factor.

Let us then discuss the cost of testing (i.e. applying) the generated classifiers. Again, in the  X  X  X lat X  X  worst case discussed above the two algorithms are trivially the same. Let us then only analyse the  X  X  X ully grown X  X  best case, in the understanding that the cost of the other cases will be intermediate between these two. In A DA B OOST .MH, each test document must be given as input to O ( S ) weak hypotheses, each of which performs 1 test and m additions, one per category; the cost is thus O ( Sm ). 8 In T REE B OOST .MH, each test document is input to O ( h ) classifiers (corresponding to one or more X  X omplete or incomplete X  X aths down-wards from the root), each of them consisting of O ( S ) weak hypotheses each of which performs 1 test and a additions, one per category; the cost is thus O ( Sah ). Recalling that m = a h , we can see that T REE B OOST .MH is cheaper than A DA B OOST .MH by an exponential factor at testing time too. 5 Experimental results 5.1 Datasets The first benchmark we have used in our experiments is the  X  X  X  EUTERS -21578, Distribution 1.0 X  X  corpus, one of the most widely used benchmarks in TC research. 9 In origin, the R
EUTERS -21578 category set is not hierarchically structured, and is thus not suitable  X  X  X s is X  X  for HTC experiments; we have thus used a hierarchical version of it generated in Touta-nova et al. ( 2001 ) by the application of hierarchical agglomerative clustering on the 90 R
EUTERS -21578 categories that have at least one positive training example and one positive test example. The original R EUTERS -21578 categories are thus  X  X  X eaf X  X  categories in the resulting hierarchy, and are clustered into four  X  X  X acro-categories X  X  whose parent category is the root of the tree. Conforming to the experiments of Toutanova et al. ( 2001 ), we have used (according to the ModApte split) the 7,770 training examples and 3,299 test examples that are labelled by at least one of the selected categories; the average number of categories per document is 1.23, ranging from a minimum of 1 to a maximum of 15. The average number of positive examples per category is 106.50, ranging from a minimum of 1 to a maximum of 2,877 (Table 1 ).
 The second benchmark we have used is R EUTERS C ORPUS V OLUME 1 version 2 (RCV1-2), 10 a more recent text categorization benchmark made available by Reuters and con-sisting of 804,414 news stories produced by Reuters from 20 Aug 1996 to 19 Aug 1997; all news stories are in English, and have 109 distinct terms per document on average (Rose et al. 2002 ). In our experiments we have used the  X  X  X YRL2004 X  X  split defined in Lewis et al. ( 2004 ), in which the (chronologically) first 23,149 documents are used for training and the other 781,265 are used for testing. The documents are classified according to three different, orthogonal classification schemes:  X  X  X opics,  X  X  X ndustries X  X  and  X  X  X egions X  X ; consis- X  X  X opics X  X  classification scheme. Out of the 103  X  X  X opics X  X  categories, in our experiments we have restricted our attention to the 101 categories (21 internal node and 80 leaf node categories) with at least one positive training example. Of the three benchmarks we use in this work, RCV1-V 2 is the only one containing (both training and test) examples that are  X  X  X wn X  X  documents of internal node categories.; each of the 21 internal node categories has at least one own document. The RCV1-V 2 hierarchy is four levels deep (including the root, to which we conventionally assign level 0); there are four internal nodes at level 1, and the training examples on average.
 The third benchmark we have used is the ICCCFT from the 2007  X  X  X nternational Challenge on Classifying Clinical Free Text Using Natural Language Processing X  X , 11 organized by the Computational Medicine Center of the Cincinnati Children X  X  Hospital Medical Center and the University of Cincinnati Medical Center. The documents are short discharge reports classified according to the ICD-9-CM classification scheme, 12 the official system of assigning codes to diagnoses and procedures associated with hospital utilization in the United States. The experiments we present here use only the training set of the Challenge, since the labels of the test documents are not available to participants; unlike with the previous two benchmarks we thus compute effectiveness by leave-one-out cross-validation. There are only 978 documents in the training set, with an average length of 13.3 words. We restrict our experiments to the 79 categories that have at least one positive training document; of these 79 categories, 45 are leaf node categories and the other 34 are internal node categories, none of which has  X  X  X wn X  X  documents. The ICD-9-CM hierarchy (or, at least, that part of it that is used for labelling our training data) is again four levels deep (including the root, to which we conventionally assign level 0); there are seven internal nodes at level 1, and the leaves are all at the levels 2 and 3. The average number of positive examples per leaf category is 27.1, ranging from a minimum of 1 and a maximum of 266. One peculiar feature of this dataset is that some nodes are single children, i.e., have no siblings. This makes it impossible to adopt our standard policy of choosing, as negative training examples of a category, the positive training documents of the father category. In these cases we merge father and child categories into a single node, since they always have the same positive and negative examples. 5.2 Averaging effectiveness across categories As a measure of effectiveness that combines the contributions of precision ( p ) and recall ( q ) we have used the well-known F 1 function, defined as which corresponds to the harmonic mean of precision and recall, where TP stands for true positives, FP for false positives, and FN for false negatives. Note that F 1 is undefined when TP = FP = FN = 0; in this case, consistently with most other works in the literature, we take F 1 to equal 1.0, since the classifier has correctly classified all documents (as negative examples).

In text classification it is customary to average the category-specific F 1 scores by computing both microaveraged F 1 (denoted by F 1 l ) and macroaveraged F 1 ( F 1 M ). F 1 l is obtained by (i) computing the category-specific values TP i , (ii) obtaining TP as the sum of computing the F 1 values specific to the individual categories, and then averaging them across the c j  X  X .

However, in HTC one should specify exactly which categories the average is computed across. Should this average be computed across leaf categories only, or should it involve internal node categories too? It might seem reasonable that also the internal nodes that have  X  X  X wn X  X  positive examples (see Sect. 3.1 ) are considered, since these nodes are not to be viewed as merely routing documents to the subtrees below them. However, the presence of  X  X  X ubbled-up X  X  positive test examples within internal node categories means that, if averages involve these categories too, they will involve classification decisions that are not independent of each other. For instance, the fact that test document d i has been correctly classified under a leaf category c j entails that d i has also been correctly classified under all the categories in * X  d i  X  ; which means that this decision will count as n true positives (where n is the depth of c j ) instead of a single true positive.

The approach we take to averaging is intermediate between these two extremes, and involves distinguishing, for each internal node c j , the roles of  X  X  X wn X  X  and  X  X  X ubbled-up X  X  positive test examples. In turn, this corresponds to distinguishing the roles of internal nodes as  X  X  X outers X  X  towards its subordinate nodes or as  X  X  X epositories X  X  of documents in their own right (a distinction already addressed in Ruiz and Srinivasan 2002 ]).

The approach consists in 1. mapping the original hierarchy C into a modified hierarchy C 0 such that, for each 2. moving into c 0 j all of c j  X  X   X  X  X wn X  X  positive test examples.

This simple mapping, originally proposed in Cheng et al. ( 2001 ), produces a hierarchy C 0 semantically equivalent to C in which all documents are contained in at least one leaf category. 13 For evaluation purposes we then use the modified hierarchy C 0 instead of the original hierarchy C (even if learning and classification have indeed used C ), and average across leaf nodes only. The net effect is that we do take into consideration the ability of the system to correctly classify documents as  X  X  X wn X  X  documents of internal nodes (in the modified hierarchy, this is represented by the system X  X  effectiveness on  X  X  X ummy X  X  nodes), and at the same time we remove the dependence between classification decisions due to inherited examples. 5.3 The experiments In all the experiments discussed in this section, punctuation has been removed, all letters have been converted to lowercase, numbers have been removed, stop words have been removed using the stop list provided in Lewis ( 1992 , pp. 117 X 118), and stemming has been performed by means of Porter X  X  stemmer.

In a first experiment we have compared A DA B OOST .MH and T REE B OOST .MH using a full feature set.

We have then performed a number of experiments using feature selection; however, these have been run on R EUTERS -21578 and RCV1-V 2 only, due to the fact that the original feature set of ICCCFT has a very limited size already (1,294 terms only). Reduced feature sets were obtained according to a  X  X  X lobal X  X  feature selection policy in which (i) feature-category pairs have been scored by means of information gain , defined as and (ii) the final set of features has been chosen according to Forman X  X  round robin technique, which consists in picking, for each category c j , the v features with the highest IG ( t k , c j ) value, and pooling all of them together into a category-independent set (Forman 2004 ). This set thus contains a number of features q B vm , where m is the number of categories; it usually contains strictly fewer than vm , since it is usually the case that some features are among the best v features for more than one category. We have set v to 60 for R
EUTERS -21578 and to 43 for RCV1-V 2, which are the values that, for each corpora, best approximate a total number of features of 2,000; in fact, the reduced feature sets consist of 2,012 features for R EUTERS -21578 (11% of the 18,177 original ones) and 2,029 for RCV1-V 2 (3.7% of the 55,051 original ones).

We have also run an experiment in which we have used the  X  X  X local X  X  feature selection policy described in Sect. 3.3 , consisting in selecting a different subset of features (of the same cardinalities as in the global policy) for the set of children of each different internal node. Note that, for each corpus, the results obtained by means of this policy are A 5.4 Effectiveness The results of our experiments are reported in Table 2 .

We will now comment on the R EUTERS -21578 results; 14 the RCV1-V 2 and ICCCFT from A DA B OOST .MH to T REE B OOST .MH, effectiveness improves substantially. F 1 l improves from +2.3% to +17.2%, depending on the number S of boosting iterations. F 1 M improves even more substantially, from +22.0% to +197.4%; this means that T REE B OOST .MH is especially suited to categorization problems in which the distribution of training examples across the categories is highly skewed. For both F 1 l and F 1 M , the improvements tend to be more substantial for low values of S , showing that T REE B OOST .MH converges to optimum performance more rapidly than A DA B OOST .MH. Altogether, these effectiveness improve-ments are somehow surprising, since it is well-known that hierarchical TC can introduce a deterioration of effectiveness due to classification errors made high up in the hierarchy, which cannot be recovered at the lower levels (Koller and Sahami 1997 ; McCallum et al. 1998 ). The improvements thus show that the  X  X  X ilters X  X  placed at the internal nodes work positive X  X  examples of local interest as negative training examples.

Concerning the RCV1-V 2 dataset, note that the results obtained by both A DA B OOST .MH and T REE B OOST .MH are inferior to the ones reported in Lewis et al. ( 2004 ) and obtained, on the same dataset, by SVM-based classification systems. One of the reasons is certainly the fact that F 1 is (micro-and macro-)averaged across different sets of categories. In fact, while the authors of Lewis et al. ( 2004 ) choose to include internal node categories in the average, as mentioned in Sect. 5.2 we only include their associated dummy nodes. By doing so we avoid  X  X  X atering down X  X  the evaluation by considering nodes (the internal ones) that are both  X  X  X asy X  X  (since they typically have many training examples, of the  X  X  X ubbled-up X  X  type) and scarcely significant from an application point of view (since their most important role is as routers towards the leaves, rather than as categories in their own right).

Obviously, this means that the classification problem we tackle is more difficult that the one tackled in Lewis et al. ( 2004 ); in fact, easy nodes are now removed from consider-ation, while  X  X  X ard X  X  ones (i.e., the dummy ones, which have very few training examples) are introduced. This is best appreciated by looking at Tables 3 and 4 , in which the effectiveness of the classifiers is computed separately for original leaves (Table 3 ) and dummy nodes (Table 4 ). The fact that effectiveness is very, very low on dummy nodes shows that including them in the averages from which Table 2 was computed has con-siderably reduced the resulting values of effectiveness. 5.5 Significance testing In order to check whether these results are statistically significant we have subjected them to thorough statistical significance testing, by applying to the results reported in Table 2 (we use those for S = 1,000 boosting iterations) all the significance tests defined for text classification systems in Yang and Liu ( 1999 ), Sect. 4) i.e.: 1. the s-test : a sign test (Spiegel and Stephens 1999 , Chapter 17) which compares two 2. the S-test on F 1 : a sign test which compares two classifiers ^ U 1 and ^ U 2 by analysing the 3. the T-test on F 1 :a t -test (Spiegel and Stephens 1999 , Chapter 11) which compares two 4. the T 0 -test on F 1 :a X  X  t -test after rank transformation X  X  which compares two classifiers 5. the p-test on p l and q l :a t -test which compares two classifiers ^ U 1 and ^ U 2 by analysing Tests 1 and 5 are designed to evaluate the two systems at the ( X  X  X icro X  X ) level of individual classification decisions, while Tests 2, 3 and 4 are designed to evaluate them at the ( X  X  X acro X  X ) level of individual categories, i.e., by analysing the performance scores that the two systems have obtained on such categories. We refer the interested reader to (Yang and Liu 1999 , Sect. 4] for full mathematical definitions and for a discussion of the strengths and weaknesses of these five tests; we here only note, along with (Yang and Liu 1999 ,p. 47), that  X  X  X one of the tests is  X  X erfect X  for all the performance measures, or for performance analysis with respect to a skewed category distribution, so using them jointly instead of using one test alone would be a better choice X  X .

Table 5 clearly shows that the results reported in Table 2 are statistically significant. In T
REE B OOST .MH and A DA B OOST .MH are compared) T REE B OOST .MH turns out to be statis-tically significantly better than A DA B OOST .MH at a p -value B 0.01 A DA B OOST .MH (this corresponds to the cells marked ( X  X   X  X ), while in other 4 tests this holds only at a p -value B 0.05 (cells marked  X  X  \  X  X ); only in the remaining 5 tests no statistically significant dif-ference is found at p -values [ 0.05 (cells marked  X  X  *  X  X ).

Note that this result becomes even stronger if we restrict ourselves to the largest of the tested collection (namely, RCV1-V 2), on which T REE B OOST .MH turns out to be statistically significantly better than A DA B OOST .MH at a p -value B 0.01 in 17 out of 18 tests. The strength of these results is also witnessed by the fact that these tests have been run on the results obtained with S = 1,000 boosting iterations, i.e., the scenario in which (see Table 2 ) the difference between the two systems is smallest.

Finally, note (see 4th and 8th rows of Table 5 ) that no statistically significant difference is found between the global and  X  X  X local X  X  feature selection policies (however, note that global consistently scores better than glocal on micro-level tests on RCV1-V 2), thereby reinforcing the impression that no significant advantage is to be gained by using the latter instead of the former. 5.6 Efficiency In terms of efficiency, we can observe that training time is +50.4% smaller, irrespectively of the number of iterations, a reduction that confirms the theoretical findings discussed in Sect. 4 (and that might likely be even more substantial in classification problems char-acterized by a deeper, more articulated hierarchy). Classification time is also generally reduced; aside from an isolated case in which it increases by 16.8%, it is reduced from +5.5% to +67.4%, with higher reductions being obtained for high values of S ; this is likely due to the fact that, since high values of S bring about more effective classifiers, the classifiers placed at internal nodes are more effective at  X  X  X locking X  X  unsuitable documents from percolating down to leaves which would reject them anyway.

The experiments run after global feature selection qualitatively confirm the results above. Note that the effectiveness values are practically unchanged with respect to the full feature set experiment; this is especially noteworthy for the RCV1-V 2 experiments, in which more than 96% of the original features have been discarded with no loss in effec-tiveness. Effectiveness does not change also when using  X  X  X local X  X  feature selection. This is somehow surprising, since an effectiveness improvement might have been expected here, due to the generation of feature sets customized to each internal node. It is thus likely that the values of v chosen when applying the global policy were large enough to allow the inclusion, for each internal node, of enough features customized to it. We plan to carry out further experiments in order to check whether, at more aggressive levels of reduction (i.e. smaller values of v ), the glocal policy will indeed prove superior to the global one. 6 Related work neural networks and latent semantic indexing. The intuition that it could be useful to perform feature selection locally by exploiting the topology of the tree is originally due to Koller and Sahami ( 1997 ). However, this work dealt with 1-of-n text categorization, which means that feature selection was performed  X  X  X ollectively X  X , i.e., relative to the set of children of each internal node; given that we are in an m -of-n classification context, we instead do it  X  X  X ndividually X  X , i.e., relative to each child of any internal node. The intuition that the negative training examples for training the classifier for category c j could be limited to the positive training examples of categories topologically close to c j is due to Ng et al. ( 1997 ) and Wiener et al. ( 1995 ). The notion that, in an m -of-n classification context, classifiers at internal nodes act as  X  X  X outers X  X  informs much of the HTC literature, and is explicitly discussed at least in Ruiz and Srinivasan ( 2002 ), which proposes a HTC system based on neural networks.
Other works in hierarchical text categorization have focused on other specific aspects of the learning task. For instance, the  X  X  X hrinkage X  X  method presented in McCallum et al. ( 1998 ) is aimed at improving parameter estimation for data-sparse leaf categories in a 1-of-n HTC system based on a na X   X  ve Bayesian method; the underlying intuitions are specific to na X   X  ve Bayesian methods, and do not easily carry over to other contexts. Incidentally, the na X   X  ve Bayesian approach seems to have been the most popular among HTC researchers, since several other HTC models are hierarchical variations of na X   X  ve Bayesian learning algorithms (Chakrabarti et al. 1998 ; Gaussier et al. 2002 ; Toutanova et al. 2001 ; Vin-okourov and Girolami 2002 ); SVMs have also recently gained popularity in this respect (Cai and Hofmann 2004 ; Dumais and Chen 2000 ; Liu et al. 2005 ; Yang et al. 2003 ). Evaluation measures from flat classification are the most popular choices for evaluating HTC systems. Some other researchers (Ceci and Malerba 2007 ; Sun and Lim 2001 ) have proposed that evaluation measures specific to the hierarchical case should be used in HTC, so that credit is given to  X  X  X artially correct X  X  classification, i.e., to the misclassification of a document into a category topologically close to the correct one. We think that these measures are difficult to judge in the abstract, since whether a user would gain any more benefit from a  X  X  X artially correct X  X  classification than from a  X  X  X ompletely wrong X  X  classi-fication remains open to question, and fundamentally dependent on the particular application. We also believe that such proposals may be interesting, if at all, only in 1-of-m classification, in which there is such a notion as the correct category to which a document belongs. For the evaluation of our systems we have thus stuck to using  X  X  X raditional X  X  evaluation measures. 7 Conclusion We have presented T REE B OOST .MH, a recursive algorithm for hierarchical text categori-zation that uses A DA B OOST .MH as its base step and that recurs over the category tree structure. We have given complexity results in which we show that T REE B OOST .MH, by leveraging on the hierarchical structure of the category tree, is exponentially cheaper to train and to test than A DA B OOST .MH. These theoretical intuitions have been confirmed by thorough empirical testing on three standard benchmarks, on which T REE B OOST .MH has brought about substantial savings at both learning time and classification time. T REE-B
OOST .MH has also shown to bring about substantial improvements in effectiveness with respect to A DA B OOST .MH, especially in terms of macroaveraged effectiveness; this feature makes it extremely suitable to categorization problems characterized by a skewed distri-bution of the positive training examples across the categories.
 References
