 Categories and Subject Descriptors : H.3.4 [Information Storage and Retrieval]: Systems and Software X  X erformance evaluation Keywords: evaluation; negative results
Many, if not most, published research papers in Infor-mation Retrieval (IR) describe the following process: the authors identify an opportunity to improve on a particular IR task, implement an experimental system, and compare its performance against one or more baselines (or a con-trol condition, in the case of a user study). The quality of the research is judged based on the magnitude of the im-provement and whether the methodological choices suggest external validity and generalizability, for example, whether the experimental setup is  X  X ealistic X  or whether the baseline methods reflect the state of the art.

Unfortunately, research demonstrating the failure to re-produce or generalize previous results does not have a simi-lar publication venue. This sort of result X  X ften referred to as a  X  X egative result X  X  X erves to control the quality of pub-lished research in a scientific discipline and to better under-stand the limits of previously published methods. Publica-tion venues for such research exist in fields such as ecology, biomedicine, 2 pharmacy, 3 , and social science. 4
The SIGIR 2015 Workshop on Reproducibility, Inexplica-bility, and Generalizability of Results (RIGOR) aims to pro-vide a venue for publication and discussion of IR research that fails to reproduce a previously published result un-der the same or similar experimental conditions (e.g., same test collection and system configuration) and research that demonstrates the failure to generalize an existing approach to a new domain. To this end, we have developed a set of categories covering different ways in which a result may http://jnr-eeb.org/index.php/jnr http://www.jnrbm.com/ http://www.pnrjournal.com/ http://jspurc.org/intro2.htm fail to reproduce or generalize as well as a set of reviewing principles unique to this style of research. We believe the RIGOR Workshop provides a forum in which to present and discuss an under-represented aspect of IR research.
We see papers in this workshop focusing on different sce-narios in which previous results might fail to reproduce. Specifically, we invite submissions from the following four categories: repeatability of published experiments, repro-ducibility of published experiments on comparable data, gen-eralizability of published results to comparable tasks, and in-explicability of unpublished experiments. We provide more details about these categories below.
Although IR experiments vary in subtle ways that may influence the precise values of, for example, evaluation num-bers, we expect hypothesis tests to be robust to these subtle variations. A submission in this category demonstrates a failure to repeat a published result under approximately the same conditions in which the previously published exper-iments occurred. Examples include papers making claims such as: (a)  X  X ublished mean average precision (MAP) improve-Papers in this area serve to control the quality of results in IR research.
IR experiments are often conducted on specific corpora, sets of queries, and relevance judgments. In many cases, these experiments can be conducted on other comparable corpora, queries, or relevance judgments. A submission in this category fails to reproduce a published result on a com-parable dataset. Examples include papers making claims such as: (a)  X  X ublished MAP improvements on TREC8 for BM25 (b)  X  X ublished production interleaving improvements on Papers in this area serve to test the sensitivity of results in IR research to experimental conditions.
Many IR strategies have demonstrated effectiveness across different comparable task definitions (e.g.,  X  X M25 is an effec-tive term weighting scheme for different text ranking tasks X ). A submission in this category fails to reproduce a published result on a comparable task. Examples include papers mak-ing claims such as: (a)  X  X ublished MAP improvements on TREC8 for BM25 (b)  X  X ublished production interleaving improvements on Papers in this area serve to test the sensitivity of results in IR research to task definitions.
Finally, in some cases, IR research involves testing hy-potheses that we expect to be positive, based on prior work in IR or related disciplines. We would also like to test the the ability to generalize to tasks that are either vaguely com-parable to or completely different from previously-studied tasks. A submission in this category fails to obtain improve-ments using well-established principles/methods or well-mo-tivated approaches. Examples include papers making claims such as: (a)  X  X seudo-relevance does not improve performance on (b)  X  X ncorporating social signals does not improve produc-Papers in this area serve to test the sensitivity of results in IR research to task definitions and help understand the lim-its in applying straightforward techniques in novel domains.
