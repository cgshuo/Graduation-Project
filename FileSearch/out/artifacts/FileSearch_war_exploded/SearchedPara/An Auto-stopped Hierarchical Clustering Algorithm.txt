 ognition, image segmentation [1] and the organization of large databases [2]. value for parameter(s), which is also an important research direction in KDD [3] . For clusters k . However, it is very difficult to choose an appropriate k because of lacking the valuable prior knowledge. Even if such knowledge is available, it maybe inconsis-tent with data X  X  realistic distribution situation. 
Some researches try to make clustering algorithm automatically estimate k . [1] and parameter(s); difficult in choosing the appropriate f . A new clustering method based on dissimilarity increments is proposed in [8]. However, its basic hypothesis that the dissimilarity increment of all data exhibits exponential distribution is not always true for the real-life datasets. The algorithm is also short at handling outlier and detecting clusters with complex shape, such as the linearly inseparable datasets. outliers, like CURE, ROCK and DBScan [7] . The latter scheme actually treats outlier-plain the reason why the abnormality of data occurs. And these algorithms also waste the valuable domain knowledge in deleting the detected outliers as  X  X oise X . 
To overcome the above problems, the paper proposes a new strategy that combines outlier detection with the hierarchical clustering process. The strategy intends to stop clustering automatically according to the di ssimilarity degree implied by the detected outliers. It is based on the following observation: the distances among data or clusters not only show their similarity degree, but also demonstrate the dissimilarity. And with stop at the moment when C NN-A and C NN-B are so diverse from each other. The outlier-according to their  X  X reat difference X  from the others. 
To take the strategy into realization, the paper proposes a new outlier detection al-gorithm in section 2. Section 3 states the novel clustering algorithm ASHCA, which is constructed on the traditional hierarchical clustering process and the new outlier min-ing method. The algorithm ASHCA stops automatically according to outlier informa-tion and needs only one parameter, whose value can be decided in the outlier-conducts series experiments to compares ASHCA with several other clustering algo-rithms. And section 6 summarizes the paper. As a separate branch of KDD, outlier detection is to find the data that is considerably dissimilar with the others. The most widely accepted definition of outliers is given by tions as to arouse suspicion that it was generated by a different mechanism. of method ignores the local distribution feature of one data. For instance, in Figure 1, p is the first outlier candidate because it is the farthest one from the others. But data locating near q are more compressed than those near p , which makes q a more natural outlier. 
This section proposes a new distance-based algorithm, which determines D out ac-feature of data in the outlier mining process. data and its nearest neighbor are the same if data distribute evenly. To compute NN D , cided by a max and a min contains space S . Thus: 
To describe the diversity of the realistic distribution situation from the even distri-value of  X  : (1) when data cluster together, the average distance among data or clus-ters should be larger than NN D , which means NN a on a line, in that case NN
In order to evaluate the local dist ribution feature of a data, factor  X  is adopted. For  X  ( a )  X   X  . To avoid this, adjust the equation for  X  ( a ) as follows: Therefore, the outlier evaluation criterion is stated as follows: 
Data a is an outlier, if 
The following method is proposed to decide the value of parameter  X  : length , obviously ()() NN
Da a Db b  X  X   X  X  X  for any b ; (2) observe the increasing speed V viz. step Num. ; (3) if V reaches its first peak when 
This method is based on following observation: n out increases much faster with fur-ther decrease of D out after outliers are detected, since outliers are extremely far away from the others while the normal are relatively near to each other. Taking Figure 2 as detected as outliers with the increase of step num. l . Figure 2 shows the detail. 
Compared with other outlier detection algorithms [10, 11] , the proposed algorithm re-and states a semi-automated method to determine  X  . Obviously,  X  can be replaced by and  X  Step is a byproduct in computing the nearest neighbor of all data, these three are the input for the algorithm. The overview of the new algorithm is listed in Fig. 3. The overview of traditional hierarchical clustering process is as follows: clusters, if more than k clusters remain, go to (2), else stop. Many ways are available to compute D between two clusters. To be more general, the distance D is defined as the distance between two clusters X  center in the following parts. Compared with the traditional method, the Auto-Stopped Hierarchical Clustering Algorithm ASHCA shows its uniqueness in two aspects, outlier detection and auto-matic stop. 3.1 Outlier Detection detection in two phases: the second phase refines the clustering result.
 3.2 Automatic Stop Without user-specified k , it is necessary to extract the stop information from the proc-essed data. As stated in the former parts, it is a suitable opportunity to stop clustering if the clusters to be merged are too dissimilar from each other. tion pattern, which is also a useful reference for clustering since the existence of clus-ters shows the diversity of the realistic distribution situation from the even pattern. 
In practice, the distance between clusters equals the distance of their centers, which means the cluster is represented by its center. Therefore, equation (3) can be adopted that: Suppose C NN-A and C NN-B are the most similar clusters at present, stop clustering if 3.3 Complexity Analysis and Overview of ASHCA structed on traditional method, it is only necessary to analyze the complexity of each Thus, the complexity of ASHCA is O ( N 2 ). ity of outlier, thus the parameter k is canceled. Fig. 4 shows its overview. separate topic and little effort has been taken to combine them. 
Meanwhile, the proposed ASHCA algorithm makes clustering work together with utilized in the determination of k ; third, it discovers the common character of outliers computing of all data X  X  nearest neighbor is the fore-step for both outlier-detection and clustering. 
Table 2 is the theoretical comparison of As-Rock with other clustering algorithms, aspects: (1) number of parameters that the respective algorithm needs; (2) whether the ahead of clustering; (5) its ability to detect clusters with complex-shape; (6) the com-putational complexity. 1. Number of Pa-2. Automatic Re-trieval of Parameter Value 4. Mining Outlier 5. Ability to detect complex-shape clusters 6. The computation complexity O ( N attribute and dimensionality are adopted in the experiment to compare ASHCA with K-Means using random initial points, traditional hierarchical clustering method, DBScan and the algorithm of [8], which is nicknamed as the Frozen algorithm due to its major characteristic. The NBA dataset [9] is primarily used to evaluate the perform-listed in Table 3. Name N M Class Num. Attribute Type Missing Attribute Values NBA 330 3 Unknown real No Zoo 101 16 7 categorical No Vote 435 16 2 categorical Yes Iris 150 4 3 real No Wine 178 13 3 real No By applying the FastMap algorithm [14] , the high-dimensional datasets are mapped classes locate together and some sub-clusters belong to the same class distribute in the different part. are adopted: ideal result, Entropy =0.0 and Purity =1.0. 5.1 Performance of Outlier Detection rithm. Figure 6 shows the changes of V with the increasing of step num. l of the NBA and ZOO dataset. Compared with the outlier-detection algorithm in [11], the new Mckie. And the first detected outlier is Dennis Rodman. 5.2 Performance of Clustering The clustering results of ASHCA are listed in Table 5 along with the detected number Experiment also shows that the proposed value range [3, 5] of parameter  X  of Frozen DBScan and Frozen. To overcome the shortcomings of traditional approaches, the paper states a new strat-hierarchical clustering algorithm ASHCA, which is constructed on a new outlier-detection method and needs only one parameter. Experimental results show ASHCA X  X  good performance in both outlier detection and clustering. The future works will concentrate on the study of ASHCA to handle large dataset. ning process, especially when the sample set can not represent the actual distribution situation of the whole set. This work is sponsored by the Natural Science Foundation of China under grant num-ber 60373099 and the Natural Science Research Foundation of Harbin Engineering University under the grant number HEUFT05007. 
