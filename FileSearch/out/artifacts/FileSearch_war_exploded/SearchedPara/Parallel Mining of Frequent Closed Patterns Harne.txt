
Inspired by emerging multi-core computer architectures, in this paper we present MT C LOSED , a multi-threaded al-gorithm for frequent closed itemset mining (FCIM). To the best of our knowledge, this is the first FCIM parallel algo-rithm proposed so far.

We studied how different duplicate checking techniques, typical of FCIM algorithms, may affect this parallelization. We showed that only one of them allows to decompose the global FCIM problem into independent tasks that can be executed in any order, and thus in parallel.

Finally we show how MT C LOSED efficiently harness modern CPUs. We designed and tested several paralleliza-tion paradigms by investigating static/dynamic decompo-sition and scheduling of tasks, thus showing its scalabil-ity w.r.t. to the number of CPUs. We analyzed the cache friendliness of the algorithm. Finally, we provided addi-tional speed-up by introducing SIMD extensions.
We can recognize some important trends in the design of future computer architectures. The first one indicates that the number of processor cores integrated in a single chip will continue to increase. The second one is that memory hi-erarchies in chip multiprocessors (CMPs) will become even more complex, and their organization will try to maximize the number of requests that can be served on-chip, thus re-ducing the overall number of cache misses that involve the access to memory levels outside the chip. Finally, nowa-days modern processors have SIMD extensions, like Intel X  X  SSE, AMD X  X  3DNow!, and IBM X  X  AltiVec, and further in-structions and enhancements are expected to be added in future releases. Thus algorithm implementations can take large performance advantages from their use.

In order to maximize the utilization of the computing re-sources provided by CMPs, and increase application per-formance, it is mandatory to use multiple threads within ap-plications. Programs running on only one processor will not be able to take full advantage of such future processors, since they will only exploit a limited amount of implicit instruction-level parallelism (ILP). As a result, explicit par-allel programming has suddenly become relevant even for desktop and laptop systems. Also, coarse-grained paral-lelism is mandatory in order to avoid conflicts among the core X  X  private caches.

Unfortunately, as the gap between memory and proces-sor/CMPs speeds continues to widen, more and more cache efficiency becomes one of the most important components of performance. High performance algorithms must be con-scious of the need of exploiting locality in data layout and, consequently, in data access. Furthermore, it is still unex-plored the oppurtinity to exploit the SIMD co-processors that equip most modern CPUs. We instead think that the SIMD paradigm can be useful to speed-up several data min-ing sub-tasks operating on large data.

In this paper we discuss how a demanding algorithm to solve the general problem of Frequent Itemsets Mining (FIM) can be effectively implemented on modern computer architectures, by capturing all the technological trends men-tioned above.

More specifically, the algorithm that will be the subject of our analysis (DCI C LOSED [6]) aims at extracting all the frequent closed itemsets from a transactional database.
This algorithm is deeply analyzed in this paper with re-spect to its capability to harness modern architectures. First of all, the core computational kernel of DCI C LOSED is mainly based on bitwise operations performed on long lists. Such operations are data parallel, and can be efficiently pro-grammed using SIMD co-processors of modern CPUs.

Moreover, the data structures used by DCI C LOSED , and the way they are accessed, exhibit high locality, espe-cially spatial one. We will see that the largest data structure exploited by DCI C LOSED is read-only. The concurrent ac-cesses to such data by multiple processors thus do not need synchronizations and do not cause cache coherence invali-dations and misses.

These peculiarities of DCI C LOSED allowed us to de-sign of MT C LOSED , an efficient, shared memory parallel FCIM algorithm for SMPs or multi-core CPUs. This is the main contribution of our work, since MT C LOSED is to the best of our knowledge, the first FCIM parallel algorithm proposed so far
Note that a static scheduling/assignment of the mining sub-tasks to the various threads may involve load imbal-ance. In order to deal with this problem, we considered static scheduling as a sort of baseline, and investigated a dy-namic scheduling policy of such statically created tasks, and also dynamic decomposition of the search space to provide adaptive load balancing. This resembles a receiver-initiated work-stealing policy, where a local round-robin strategy is used to select the partner from which a thread could steal some work. In our experiments, MT C LOSED obtained very good speed-ups on SMPs/multi-core architectures with up to 8 CPUs.
Frequent Itemsets Mining (FIM) is a demanding task common to several important data mining applications that looks for interesting patterns within databases (e.g., as-sociation rules, correlations, sequences, episodes, classi-fiers, clusters). The problem can be stated as follows. Let I = { a 1 , ..., a M } be a finite set of items or singletons , and let D = { t 1 , ..., t N } be a dataset containing a finite set of transactions , where each transaction t is a subset of I . We call k -itemset a set of k items I = { i 1 , ..., i k | i Given a k -itemset I , let  X  ( I ) be its support , defined as the number of transactions in D that include I . Mining all the frequent itemsets from D requires to discover all the item-sets having a support greater or equal to a given minimum support threshold  X  . We denote with F the collection of fre-quent itemsets, which is indeed a subset of the huge search space given by the power set of I .

State-of-the-art FIM algorithms visit a lexicographical tree spanning over such search space, by alternating can-didate generation , and support counting steps. In the can-didate generation step, given a frequent itemset X of | X | elements, new candidate ( | X | +1 )-itemsets Y are generated as supersets of X that follow X in the lexicographical or-der. During the counting step, the support of such candidate itemsets is evaluated on the dataset, and if some of those are found to be frequent, they are used to re-iterate the al-gorithm recursively. This two-step process stops when no more frequent itemsets are discovered.

The collection of frequent itemsets F extracted from a dataset is usually very large. This makes the task of the an-alyst hard: since he has to extract useful knowledge from a huge amount of patterns, especially when very low mini-mum support thresholds are used.

Closed itemsets[2] are a solution to this problem. For each collection of frequent itemsets F , there exist a con-densed, i.e. both concise and lossless, representation of F given by the collection of closed itemset C , C  X  F . It is concise because |C| may be orders of magnitude smaller than |F| . This allows to extract additional potentially in-teresting itemsets by using lower minimum support thresh-olds, which would make the extraction of all the frequent itemsets intractable. Moreover, it is lossless, because from C it is possible to derive the identity and support of every frequent itemset in F . Finally, the extraction of associa-tion rules directly from closed itemsets has been shown to be more meaningful for analysts [10, 11], since C does not include many redundancies which are present in F .
The concept of closed itemset is based on the two fol-lowing functions f and g : where T and I , T  X  D and I  X  I , are subsets of all the transactions and items appearing in D , respectively. Func-tion f returns the set of items included in all the transactions belonging to T , while function g returns the set of transac-tions (called tid-list ) supporting a given itemset I . Definition 1. An itemset I is said to be closed iff where the composite function c = f  X  g is called Galois operator or closure operator .

The closure operator defines a set of equivalence classes over the lattice of frequent itemsets: two itemsets belong to the same equivalence class iff they have the same clo-sure, i.e. they are supported by the same set of transac-tions. Fig. 2(b) shows the lattice of frequent itemsets de-rived from the simple dataset reported in Fig. 2(a), mined with  X  = 1 . We can see that the itemsets with the same closure are grouped in the same equivalence class. Each equivalence class contains elements sharing the same sup-porting transactions, and closed itemsets are their maximal elements. Note that closed itemsets (six) are remarkably less than frequent itemsets (sixteen).
 Usually inspired by popular FIM algorithms, also Closed Itemset Mining (FCIM) algorithms exploit a visit of the search space layered on top of a lexicographic spanning tree. In addition to the usual candidate generation and support counting, they execute a closure computation step. Given a closed itemset X , new candidates Y  X  X are gen-erated. For every candidate Y that is found to be frequent, the closure c ( Y ) is computed in order to find a new closed Figure 1. (a) The input transactional dataset
D , represented in its horizontal form. (b) Lex-icographic spanning tree of all the frequent itemsets ( min supp = 1 ), with closed itemsets and their equivalence classes. itemset, which will be later used to re-iterate the algorithm. Note that since a closed itemset is the maximal pattern of its equivalence class, any superset Y surely belongs to another equivalence class, and therefore its closure c ( Y ) will lead to a new closed itemset.

Unfortunately, the closure operator may involve back-ward jumps with respect to the underlying lexicographic tree. For instance, in Fig. 2, given the closed item-set { C } and the candidate { C, D } , calculating the clo-sure c ( { C, D } ) would conduct ourselves to the itemset { A, C, D } , which is not a descendant of { C, D } . This may happen because, given any lexicographic order  X  , it is pos-sible that c ( Y )  X  Y .

The closure operator breaks the order given by the lexi-cographic tree spanning the search space, and thus we also loose the simple property of a visiting algorithm which guarantees that every node is visited only once. For exam-ple, the itemset { A, C, D } can be also reached by calculat-ing the closure of { A } .

In conclusion, FCIM algorithms may visit the same closed itemset multiple times, and they thus need to in-troduce some duplicate detection process. For efficiency reasons, this check is applied to candidate itemsets before calculating their closure. Therefore, rather than detecting a duplicate, FCIM algorithms try to avoid the generation of a duplicate.

Usually, such detection techniques exploit one of the fol-lowing two lemmas.
 Lemma 1 (Subsumption Lemma) . Given two itemsets X and Y , if X  X  Y and supp ( X ) = supp ( Y ) (i.e., | g ( X ) | = | g ( Y ) | ), then c ( X ) = c ( Y ) .
 Lets focus on Fig. 2. Suppose that the sub-trees rooted in A , B , C and D are mined (recursively) in this order. Once the algorithm generates the new candidate Y = { C, D } , it can apply the sub-sumption lemma to realize that Y is a subset of the closed itemset { A, C, D } , which has the same support, and which was already mined. Therefore, it can avoid calculating c ( Y ) since this closed itemset (and all its supersets) have been already extracted.

Note that this Lemma 1 requires a global knowledge of the collection of frequent closed itemsets mined so far. As soon as a new closed itemset is discovered, it should be stored in a suitable data structure.
 Lemma 2 (Extension Lemma) . Given an itemset X , if  X  i  X  I , i 6 X  X , such that g ( X )  X  g ( i ) , then X is not closed and i  X  c ( X ) .

This lemma provides a different method to perform du-plicate check. Back again to the candidate itemset Y = { C, D } in Figure 2, it is easy to see that g ( Y )  X  g ( A ) , and therefore the item A belongs to the closure of Y . On the other hand, by construction of the lexicographic tree, the item A does not belong to any descendant of Y , and there-fore neither c ( Y ) is descendant of Y . This means that c ( Y ) is a duplicate closed itemset, since it should be visited by a different branch of the lexicographic tree.

The Extension lemma uses the tid-lists of every single item, i.e. a global knowledge on the dataset . Differently from the Subsumption Lemma, it does not need any addi-tional dynamic data structure.

In the next sectin we will describe the implications of exploiting one of two lemma for a parallel FCIM algorithm.
To the best of our knowledge, no parallel algorithm has been proposed so far to mine frequent closed itemsets. On the other hand, many sequential FCIM algorithms were pro-posed in the past, and also some interesting parallel imple-mentation of FIM algorithms exist as well.

The lack of parallel FCIM solutions is probably due to the fact that mining closed itemsets is more challenging than mining other kind of patterns. As discussed before, during the exploration of the search space, a duplicate checking process is involved, and deciding whether a pattern leads to a duplicate or not, needs a global view either of the dataset or of the collection of closed patterns mined so far. This makes tougher the decomposition of the mining task into independent, thus parallelizable, sub-problems.
 In this section, we will take a guided tour of existing FCIM algorithms. We distinguish between algorithms ex-ploiting Lemma 1 or Lemma 2. More than this, we will consider pros and cons of the parallelization of these algo-rithms, taking into consideration the capability to take ad-vantage of modern CPUs. Within this category of algorithms we include C
HARM [12] and C LOSET [7]: these are the first depth-first algorithms providing really efficient performances.
The rationale behind them is to apply a divide-et-impera approach to the input dataset thus reducing the amount of data to be handled during each iteration. They both exploit recursive projections, where at each node of the spanning tree a small dataset is materialized, and used for the sub-sequent mining operations. This smaller dataset is pruned from all the non necessary information. When mining the supersets of some given closed itemsets X , two kinds of pruning can be applied: (1) remove transactions that do not contain X ; (2) remove items that cannot be used to gener-ate new candidates Y  X  X in accordance to the underlying lexicographical order.

This latter pruning, removes many items from each pro-jected dataset, and thus the Extension Lemma is not appli-cable. In fact, C HARM stores every newly discovered item-set in a historical collection of frequent closed itemsets ar-ranged in a two-level hash structure. Duplicates are detected by extensive sub-sumption checks on the basis of Lemma 1. named fp-trees to store both projected datasets and histori-cal closed itemsets. These fp-trees have become very pop-ular and they have been used many mining algorithms and applied for different kind of patterns.

The last algorithm in this category is FP-C LOSE [5]. The algorithm comes chronologically much later than the other two, but it still uses the same approach based on the Sub-sumption Lemma. FP-C LOSE is very simliar to C LOSET , but emphasized where the concept of recursive projection that is applied also to the historical collection of frequent closed itemsets. Not only a small dataset is associated to each node of the tree, but also a pruned subset of the closed itemsets mined so far is forged and used for duplicate detec-tion. Indeed, this technique is called progressive focusing and it was introduced by [4] for mining maximal frequent itemsets. Together with other optimizations, this truly pro-vides dramatic speed-ups, making FP-C LOSE order of mag-nitudes faster than C HARM and C LOSET , and also making it worth to be celebrated as the fastest algorithm at FIMI workshop[3].

These approaches have several drawbacks when we think to their parallelization. Assume that some strategy is used, such that subtrees of the global lexicographic spanning tree are spread among a pool of mining threads. Such an algo-rithm will incur in the following problems:
Spurious itemsets. Since different portions of the search space are mined in parallel, closed itemsets are dis-covered in an unpredictable order. Recall that this or-der is fundamental for the correctness of the Subsumption Lemma. Suppose that our favorite candidate Y = { C, D } is discovered before itemset { A, C, D } : the Subsumption Lemma will not detect Y as a duplicate. On the other hand, the algorithm cannot compute the correct closure c ( Y ) = { A, C, D } since no information about item A will be present in the projected database. The algorithm will thus produce a spurious itemset Y = { C, D } which is not closed.

Search space overhead. When a spurious itemsets is not detected as a duplicate, this is used to reiterate the mining. Therefore each thread will be likely to mine more itemsets than required, and therefore the cumulative cost of all the mining sub-tasks becomes larger than the serial cost of the algorithm.

Maintenance. In principle, the historical collection of closed itemsets should be shared among every task. Each of them will search for supersets and insert new itemsets. Since insertion requires exclusive access, this may become a limit the parallelism. Also consider that someone will be in charge of detecting and removing all the spurious item-sets generated. For what regards FP-C LOSE , this prob-lem is replicated for each projected collection of historical closed itemsets.
The FP-C LOSE algorithm has a large memory footprint: not only the dataset and the whole collection of frequent closed itemsets are stored into the main memory, but also as many projections as the number of levels of the spanning tree must be handled. It is not rare for FP-C LOSE to run out of memory, and start swapping projected fp-trees in and out from secondary memory.

In order to alleviate these large memory requirements, the authors of C LOSET +[8] introduced a new data projec-tion technique associated with a different duplicated detec-tion approach. Instead of materializing several projected datasets, the algorithm stores a set of pointers to the use-ful sub-trees of the original fp-tree. The algorithm al-ways works on the global fp-tree representing the original datasets. This allows to exploit the Extension Lemma, with a technique called upward checking by scanning the paths from the root to those useful sub-trees.

The upward checking nicely fits a parallel framework, overcoming the problems discussed above. Since the full dataset is always available, the Extension Lemma allows to detect duplicate itemsets correctly. Therefore spurious itemsets are never generated. Finally, the dataset is ac-cessed read-only and no maintenance of any other shared data structure is needed. As a matter of fact, this approach was used in P AR -CSP [1] for the parallel mining of closed sequential patterns.

However the C LOSET + algorithm is orders of magnitude slower than its younger cousin FP-C LOSE . Also, the up-ward checking technique was actually used by the author only with sparse datasets. In fact, handling the large num-ber of sub-tree arising in dense datasets is very inefficient compared to the original C LOSET algorithm.
 For this reason it is worth for us to focus on DCI C LOSED . This is yet another FCIM algorithm which sums the advantages of using a vertical bitmap to store the transactional dataset with the ones deriving for the exploita-tion of the Extension Lemma. Item tid-lists are stored in form of bit-vectors. Support counting and inclusions are performed with simple, and fast, bitwise operations. The result is an algorithm much more efficient then C LOSET +, almost as fast ad FP-C LOSE .

For the sake of completeness in Table 1 we report the running times and the memory footprint of FP-C LOSE and DCI C LOSED . As usual there is not a clear winner in terms of running times. They have similar performances on acci-dents and pumbs* . Symmetrically, DCI C LOSED is much faster on connect while FP-C LOSE is faster on USCen-sus1990 . Note that the original version of DCI C LOSED adopted in this experiment does not use SIMD extensions. Option that we will explore in Section 5.2.

However, the memory requirements of DCI C LOSED are always much smaller and almost independent from the minimum support threshold. FP-C LOSE is very demanding and it may happen that the algorithm is not even able to run to completion in dataset such as USCensus1990 where the algorithm is usually fast. The memory footprint of the al-gorithm becomes relevant in our contest where the memory requirements of the serial algorithm are almost multiplied for the number of threads of the corresponding parallel im-plementation.
 One of the reasons why DCI C LOSED can compete with FP-C LOSE , is given by its internal data structure. Consider that pattern mining algorithms are memory intensive: they perform a large number of load operations (about 60% ) due to the several traversals of dataset representation. Cache misses can thus considerably degrade the performance of these algorithms. In particular, fp-tree base algorithms are subject to the pointer-chasing phaenomenon: the next da-tum is only available through a pointer. The C LOSET + ap-proach obviously augments this problem. The authors of [9] improved prefix-tree based FIM algorithm introducing a tiling technique. The basic idea is to re-organize the fp-tree in tiles of consecutive memory locations small enough to fit into the cache, and to maximize their usage once they have been loaded. Unfortunately, in the case of closed itemsets , the tiling benefit would in fact be wasted by omnipresent du-plicate checks, that require a non localized access to a large data structures, whatever lemma of the two is exploited. On the other hand, the vertical bitmap representation pro-vides high spatial locality of access patterns, and this cache-friendliness brings a significant performance gain. In the experiments reported above, the best performance coincides with the connect and pumsb* datasets, where the vertical bitmap is less than 1MB large for each of the supports, and thus it can easily fit into the cache memory.

The approach of DCI C LOSED has three main advan-tages. First bit-wise tid-list allows high spatial locality and cache friendliness. Second, tid-list intersection and inclu-sion can easily exploit SIMD instructions. Finally, having item tid-lists immediately available makes it easy to use the Extension Lemma.
 In conclusion, DCI C LOSED is almost as fast as FP-C
LOSE , but, while the parallelization of the latter would incur in the sever additional overheads described above, DCI C LOSED can fruitfully be reshaped into a parallel al-gorithm.
This section describes the main aspects of MT C LOSED , our parallel version of DCI C LOSED .

As in all pattern mining algorithms, also in MT C LOSED the initialization of the internal data structures occurs during the first two scans of the dataset. In the first scan frequent single items, denoted with F 1 , are extracted. Then, infre-quent items are removed from the original dataset, and the new resulting set of transactions D is stored in a vertical bitmap. The resulting bitmap has size |F 1 |  X  |D| bits, and row i is a bit-vector representation of the tid-list of the i -th frequent item.

Once the vertical representation of D is materialized in the main memory, the real mining process performed ac-cording to Algorithm 1 starts. The input of the algorithm is a seed closed itemset X , a set of items I + X that can be added to X in order to generate new candidates or extending its closure, and another set of items I  X  X that cannot be used to calculate the closure, since they would lead to a duplicate closed itemset.

The above pseudo-code clarifies one important differ-ence of our algorithm from prefix-tree based ones. Once the entire the subtree rooted in Y = X  X  i has been mined, FP-C LOSE , C LOSET and C HARM , remove the singleton i from their recursive projections, thus loosing the chance to detect a jump to an already visited region of the search space by exploiting the Extension Lemma. Conversely, in MT C LOSED , the singleton i is not disregarded during the mining of subsequent closed itemsets. The item i is stored Algorithm 1 MT C LOSED Algorithm. in the set of items I  X  that cannot be used anymore to extend new candidate patterns.

As recursion progresses in the visit of the lattice, the algorithm grows a local data structure where per-iteration information is maintained. Such information of course in-cludes the vertical dataset D , the closed itemset X , the sets of items I + X , I  X  X , but also the tid-list g ( X ) and the item i used to generate a current candidate X  X  i (line 5). We will since it provides a exact snapshot of what the algorithm is doing and of what data it needs at a certain point in the execution. Moreover, due to the recursive structure of the algorithm, the job description is naturally associated with a mining sub-task corresponding to an invocation of the algo-rithm.

It is possible to partition the whole mining task into inde-pendent regions, i.e. sub-trees, of the search space, each of them described by a distinct job descriptor J . In principle, we could split the entire search space into a set of disjoint regions identified by J 1 , . . . , J m , and use some policy to assign these jobs to a pool of parallel threads. Moreover, since the computation of each J i does not require any co-operation with other jobs, and does not depend on any data produced by the other threads (e.g. the historical closed itemsets), each task can be executed in a completely inde-pendent manner. In the following we will see how to define and distribute J 1 , . . . , J m .

In Figure 2, we give a pictorial view of our parallel al-gorithm. We can imagine a public shared memory region, where the vertical dataset is stored: the bitmap is accessed read-only and can be thus shared without synchronizations among all the threads. At the same time, each thread holds a private data structure, where new job descriptors are ma-terialized for every new closed itemset encountered along the path of lattice visit.

We argument that this net separation between thread pri-vate and read-only shared data perfectly fits the multi-core architectures that inspired our work, and in particular their cache memory hierarchies. In the near future, not only it is expected that cache memories will increase their size, but we will also get used to quad-eight-core CPUs with compli-cated and deep cache memory hierarchies, where different levels of caches will be shared among different subsets of CPUs.
Parallelizing frequent pattern mining algorithms has been proved to be a non trivial problem. The main diffi-culty resides in the identification of a set of jobs to be later assigned to a pool of threads.

We have already seen that it is easy to find a decompo-sition into independent task, however this may be not suffi-cient. One easy strategy would be to partition the frequent single items and assign the corresponding jobs to the pool of threads. Unfortunately, it is very likely, that one among such jobs has a computational cost that is much higher than all the other. The difference may be such that its load is not comparable with the cumulative cost of all the other tasks.
Among the many approaches to solve this problem, an interesting one is [1]. The rationale behind the solution pro-posed by the authors, is that there are two ways to improve the na  X   X ve scheduling described above. One option is to es-timate the mining time of every single job, in order to as-sign the heaviest jobs first, and then the small ones to bal-ance the load during the final stages of the algorithm. The other is to produce a larger number of tasks thus providing a finer partitioning of the search space. Their solution merges these two objectives in a single strategy. First the cost of the jobs associated to the frequent singletons is estimated by running a mining algorithm on significant samples of the dataset. Then, the largest jobs are split on the bases of the 2-itemsets they contain.

Our choice is to avoid the expensive pre-processing step where small samples have to be mined to determine the as-signment to different threads.
 We will materialize jobs on the basis of the 2-itemsets in search space. The only drawback of this approach is that some of these seed itemsets may lead to discover a dupli-cated closed itemset. However, they will be promptly rec-ognized and discarded.

For what regards the load partitioning and scheduling policies, we investigated two different approaches and all their possible combinations. In particular, we considered Static and Dynamic solutions for both the load partition-ing and scheduling problems. We thus implemented three slightly different versions of MT C LOSED :
Static Partitioning / Static Assignment (SS) Every fre-quent 2-itemset originates a different valid job descriptor from which to start independent mining sub-tasks. The sets I
X and I fixed job descriptors are statically assigned to the pool of threads in a round-robin fashion. We will use this strategy as a sort of baseline.

Static Partitioning / Dynamic Assignment (SD) The job instances are the same as in the previous SS solution, but they are initially inserted in a virtual shared queue. Idle threads dynamically self-schedule the sub-tasks on demand, by accessing in mutual exclusion the queue to fetch the next job descriptor, thus exploiting a Work-Pool parallel pro-gramming paradigm. The SD strategy requires a limited number of synchronizations among the threads. Moreover, in most cases it should result in a quite good load balance. It is in fact important to notice that usually F 1  X  F 1 con-tains a quite large number of 2-itemsets with respect to the number of CPU/cores of modern architectures. Moreover, the last tasks to be taken from the shared queue are the least expensive since they are associated with sets I + X having a very low cardinality. These cheap sub-tasks thus help in balancing the load during the final stages of the algorithm. Dynamic Partitioning / Dynamic Assignment (DD) This totally dynamic policy is based on a work stealing prin-ciple. Once a thread has finished its sub-task, it polls the other running threads until it steals from a heavy loaded thread: a closed itemset X , half the set of items in I + not yet explored, and an adjusted I  X  X from which to initi-ate a new mining sub-task. Thread polling order is estab-lished locally on the basis of a simple round-robin policy so that the most recent victim of a theft is the less likely to be stolen again by the same thread. This work stealing ap-proach requires some data migration, since also the tid-list of X must be copied by the stealing thread in its own pri-vate data region. We designed this totally dynamic solution since experimental evidence showed that in some cases a very few sub-tasks results to be order of magnitudes more expensive than the others, and therefore they may harm the load balancing of the system. This DD solution has the great advantage that all the threads work until the whole mining process is finished, since there will be at least one other running thread from which to steal some workload. On the other hand, additional synchronization are required that may slightly slow down the algorithm.

Finally, it is worth noting that, as most FCIM algorithms, also our algorithm exploit projections of the dataset in order to improve its scalability with respect to dataset size. In our context, projecting the dataset introduces another challenge. In principle different threads may work on different pro-jections, thus resulting in increased memory requirements and possible cache pollution. On the other hand, forcing every thread to work on the same projection would intro-duce many implicit barriers that remarkably reduce the par-allelism degree. In the case of the DD we take advantage of the available degrees of freedom in the generation of new jobs. Our solution is to first force a thread to steal work from some other thread, and therefore to work on the same dataset projection. If this is not possible, i.e. there is noth-ing to steal, than the thread may build a new projection. During the construction of the new projection, the thread cannot be requested to split its workload until it does not start to mining it. However, we permit to more than one thread to create new projections at the same time, thus im-plicitly parallelizing also the dataset projecting process.
Unfortunately, large multicore cpus are still not avail-able. In order to asses the scalability of our algorithm we used a large SMP cluster. The experiments were conducted on an  X  X BM SP Cluster 1600 X  machine which was kindly provided us by Cineca ( http://www.cineca.it ). The cluster contains 60 SMP nodes, each equipped with 8 processors and 16 GB of RAM. Each processor is an IBM PowerPC 5, having only one core enabled out of two.
We tested our three scheduling strategies on different datasets, whose characteristics are reported in Table 2. These strategies were plugged into two different versions of the same algorithm: one of them does not exploit pro-jections. This is because we wanted to test the additional overheads introduced by the projection of the dataset. We just reported mining times , since the time to read the dataset is not parallelized.

As expected, the entirely static strategy SS was the one resulting in the worst performance in every experiment con-ducted. In some tests, it hardly reaches a sufficient speedup: in the tests conducted with the USCensus1990 dataset it never reaches a speedup of 3 . It is well known that split-ting the visit of the lattice of frequent patterns result in very unbalanced sub-tasks: a few tasks may be much more ex-pensive than all the others. Therefore, our SS strategy fails in well balancing the load among the various threads be-cause it considers every sub-task having the same cost.
On the other hand, our two dynamic strategies provide a considerable improvement. According to the SD strategy, the algorithm partitions the search space in a quite large number of independent sub-tasks, that are self-scheduled by the pool of concurrent threads. Threads receiving low cost sub-tasks will ask for additional workloads, while a thread receiving a demanding sub-task will not be forced to fulfill other requests.

The SD strategy may fail in well balancing the load only when a very few sub-tasks have a huge cost. In this case it may happen that it is not possible to compensate these few demanding tasks with a large set of small-sized ones. As proved by our experiments, our lastly proposed strategy DD helps to alleviate this problem. Once one thread has completed his task, he eagerly steals work of some other running thread. Therefore, even if an initial assignment of work is very unbalanced, large workloads may be dynami-cally split into smaller ones executed on other threads. The improvement of DD becomes significant when projections are performed, i.e. when a more dynamic and adaptive sup-port is required from the pool of threads.

We also report the results achieved on the connect dataset, where none of the strategies proposed resulted in a performance less close to the ideal case. This is probably due to the additional synchronizations required by the DD strategy that do not allow to profit of its great adaptiveness. Further analysis are however needed to understand in depth this behavior.

Tough the results reported refer to a single minimum supports, in our experiments we witnessed an identical be-havior for different support thresholds.
The three functions is frequent (Alg. 1.6), is duplicate (Alg. 1.9) and is in closure (Alg. 1.13) implement the three basic steps of a typical FCIM algorithm discussed in Section 2. Their cost determines the effectiveness of the al-gorithm. Our MT C LOSED algorithm behaves as follows: is frequent ( X ) The support of an itemset X is equal to the number of bits set in the bit-vector representing its tid-list. The function returns true if and only if  X  ( X )  X   X  . There are a number of tricks for counting the number of bits set in a 32-bit memory location, the one we used exploits additions and logical operators for a total of 16 operations. is duplicate ( Y, I  X  Y ) According to the Extension Lemma, if there exist an item i  X  I  X  Y such that g ( Y )  X  g ( i ) , then the function returns true. The op-eration a  X  b is translated into a couple of bit-wise operations: ( a &amp; b ) == a . Note that it is usually not necessary to scan the whole tid-list to discover that the inclusion does not hold. is in closure ( j, Y ) This is actually another application of the Extension Lemma that requires a tid-list inclusion test as for the duplicate detection.

In [6] it is shown how to optimize these operations by reusing some information collected along the path of the lattice visit. However, they share the nice property of be-ing very suitable for a SIMD approach. All these three ba-sic operations are characterized by high spatial locality, and can effectively exploit the streaming instructions perform-ing 128-bit operations that equip modern processor archi-tectures, eg. AMD 3DNow! and Intel SSE2 instruction sets. The Itanium 2 processor also includes the POPCNT instruc-tion, that counts the number of bits set in a 64-bit long string in one clock cycle, and this instruction will be included in newly released processors supporting SSE4.
 We implemented both of the above three functions using Intel SSE2 extensions that allow not only to load and store quad-words with a single instruction, but also to perform the simple operations we need in parallel on each word. We can thus theoretically quadruplicate the throughput of each one of the above functions. For instance, we count the num-ber of bits set in a 128 bit quad-word, still with only 16 operations. In general we do not expect to have a fourfold improvement in the performance of the algorithm, however the improvement in the mining time resulted to be signif-icant on several datasets, as reported in Figure 5.2 which plots the relative speedups obtained with the exploitation of SSE2 instruction sets on a Intel Xeon processor.

Given this clear trend of integration of SIMD co-processors in modern CPU architectures, we think it is im-portant to report on the exploitation of this interesting as-pect.
In this paper we presented MT C LOSED , the first multi-threaded parallel algorithm for extracting closed frequent itemsets from transactional databases. MT C LOSED de-
Figure 4. Speedup given by the exploitation of Intel X  X  SSE2 SIMD instructions. sign was strongly inspired by nowadays trends in the field of processor architectures that indicate that the number of processor cores integrated in a single chip will continue to increase.

The choice of DCI C LOSED was motivated by two other important features. First, its peculiar duplicate checking method that allows an easy decomposition of the min-ing tasks into independent sub-tasks. Second, the vertical bitmap representation of the dataset allowed to exploit the streaming SIMD instructions that equip modern processor architectures. Finally, we took into account also the cache efficiency of the algorithm and the possibility to take advan-tage of memory hierarchies in a multi-threaded algorithm.
As a result of its accurate design, MT C LOSED perfor-mances are impressive. In many experiment we measured quasi-linear speedups with a pool of 8 processors. These promising result allow us to predict that the proposed solu-tions would exploit efficiently also hardware architectures with a larger number of processors/cores.

