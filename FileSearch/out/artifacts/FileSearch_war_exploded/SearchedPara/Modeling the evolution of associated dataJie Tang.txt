 1. Introduction document collections can bene fi t many data mining applications. For example, likely to be the experts on a given subject.
 working on would re fl ect the shifts of his interest.
 information simultaneously in a uni fi ed model. This is the problem addressed in this paper. a mixture of word distributions over authors. The Author  X  interests. However, all of the aforementioned models ignore important factor research trend analysis on scienti fi c papers, and hot topic over time.
 investigated.

In this paper, we present two generative models, i.e., Author responsible for generating both observed words and timestamps. The other model, i.e., Hidden Markov Author models.
 dependencies between topics and (c) the empirical veri fi experimental results. Section 6 concludes this work. 2. Related work topic evolution modeling, author interest modeling, and topic dependency modeling. 2.1. Modeling topic evolution time slices or consider continuous time without discretization.
 alignment among topics across time slices is captured by a Kalman changes in expression among highly complex organisms.

Continuous Time Bayesian Networks (CTBN) to model continuous time previous work has simultaneously modeled trends of change in different types of information. 2.2. Modeling author interest with topics distribution over document and a mixture of the distributions associated with authors. modeled with a mixture of topics [31] [32] . To generate a word of a document, an author is the authors, then a topic is selected from a topic distribution speci topics, ideas, and  X  memes  X  across the Web.
 propose using a topic modeling approach to model the social annotation data. 2.3. Modeling dependencies between topics account for semi-supervised semantic role labeling.
 natural language parser.
 how to capture the interactions of subsets of feature values for instance classi 3. The proposed topic models Table 1 summarizes the notations used throughout this paper. Given a collection of documents where w d denotes the sequence of N d words in document d , timestamp of document d . Each w di 2 w d is chosen from a vocabulary of size V and each author a authors of size A . In addition, let x di indicate an author, chosen from author is associated with a distribution over topics  X  , chosen from a symmetric Dirichlet( topics is denoted by T .
 words  X  (BOW) assumption. However, such an approach cannot take advantage the types of objects and cannot capture the relationships between words. contents and authors' interests, called Author  X  Time  X  Topic (ATT) model and Hidden Markov Author changes in the associated data. For example, the word  X  learner human learners. If the previous word is  X  decision  X  ,  X  transductive to be assigned to be a machine learning topic, whereas if the previous word is to be assigned with a human learning topic. 3.1. Author  X  Time  X  Topic model the word and the timestamp. The corresponding generative process in the ATT model can be described as  X 
For each topic z , draw  X  z from a Dirichlet prior  X  z ;  X 
For each word w di in document d  X  Draw an author x di from a d uniformly;  X  Draw a topic z di from a multinomial distribution  X  x  X  Draw a word w di from a multinomial distribution  X  z  X  Draw a timestamp t di from a Beta distribution Beta  X  z the timestamp of the document (e.g., the publication year of a paper).
In the ATT model, the joint probability of words w , timestamps is de fi ned as where m xz is the number of times that topic z has been associated with the chosen author x and n word w v has been generated by topic z . Beta(.) is de fi multinomial distribution as for words, which results in a model similar to the Author found that the multinomial and other distributions cannot produce satisfactory results.
By placing a Dirichlet prior  X  over  X  and another prior  X  over  X  and  X  , we obtain
There is a set of unknown parameters in the ATT model: (1) the distribution topic-words; (2) the distribution  X  of T  X  M topic-time and the corresponding topic z [23] . We choose Gibbs sampling for its ease of implementation. Speci directly estimating the model parameters, as instead it fi to infer  X  ,  X  , and  X  .

Given D documents, a set of topics z , and hyperparameters given a topic) and  X  (the probability of a given topic given an author) can be estimated via
The random variables  X  z can be updated after Gibbs sampling by 3.2. Hidden Markov Author  X  Time  X  Topic model model.

There are several ways of describing the dependencies. We propose a Hidden Markov Author which de fi nes a conditional distribution P ( z k | z j x
 X  , with P( z k |z j x k ) =  X  ( zk|zjxk ) Each row is a distribution over topics for a particular context z
Based on this consideration, the generative process for a corpus can be de  X 
For each topic z , draw  X  z from a Dirichlet prior  X  z ;  X 
For each word w di in document d  X  Draw an author x di from a d uniformly;  X  Draw a topic z di from a multinomial  X  x  X  Draw a word w di from a multinomial distribution  X  z  X  Draw a timestamp t di from a Beta distribution Beta  X  z author but also by the previous topic. Hence, the joint probability of words where z j is the sampled topic for the previous word w j and z By integrating out the variables  X  and  X  , we have For the random variables, we estimate  X  using Eq. (4) and
We also use Eqs. (6  X  7) to estimate  X  z . 4. Parameter estimation Gibbs sampling for its ease of implementation.

As for the hyperparameters  X  and  X  , previous sampling-based treatments usually take a  X  Gibbs EM algorithm [2] .

The algorithm for the parameter estimation in the ATT model can be summarized as follows 1. Initialize z ,  X  , and  X  . 2. Do until termination (a) E-step: for each word token w di in each document d , draw an author x (b) M-step: update  X  using Eqs. (6  X  7) and fi nd the optimal hyperparameters by maximizing or (HMATT) where the superscript  X  t denotes a quantity, excluding the current instance (the di -th word token).
In the M-step, given the samples z , the optimal  X  can be computed using used to estimate  X  [23] . In both the ATT and HMATT models we assume that there are T hyperparameters for also consider other prior settings, for example only one  X  hyperparameters for  X  ).
 for each word in the new document by (ATT as the example) where n z di w di is the number learned from the training data and n new document d  X  , excluding the current instance. 4.1. Parallelization multiple days (even weeks) to learn the topic model on a large scale of the scienti training of the topic model. The basic idea is to conduct the inference in a given P processors, we distribute the document collection D over the P processors, with D be updated independently. Essentially, for performing a fi the perplexity of the parallel ATT model with the single-machine version, and respectively update each element of two duplicated (topic by word, topic by conference) matrices by where the number n (old) with the superscript (old) denotes the count before distribution and the number n superscript (new) denotes the count after merging. The number n training algorithm has been implemented using Hadoop. 1 4.2. Computational complexity
We analyze the complexity of the proposed topic models. The ATT model has a complexity of O ( MDN
O ( MDAN  X  d T ). In the parallel ATT model, the time complexity is O ( M (( D / P ) DN
Similarly, the complexity of the parallel HMATT is O ( M (( D / P ) DAN signi fi cant reduction of the time complexity. 5. Experimental results 5.1. Experiment setting
We conducted experiments on two real-world data sets: NIPS conference papers Yahoo groups).

NEWSGROUP; each post's timestamp is determined by the posted time (with format (AT) model. Then we analyze the results obtained by our models on the two data sets. 5.2. Perplexity and time prediction for estimating the performance of a probabilistic model. The perplexity of an unseen test document ( and the HMATT models, it takes only 20 min (ATT) and 30 min (HMATT).
Fig. 3 plots the average perplexity of the four models with different numbers of topics. First we
Speci fi cally, we train the model on the training data set (
LDA and AT on Yahoo and 0.0089 and 0.0074 on NIPS, which indicates that at a signi models over the baseline models are statistically signi fi timestamp probability of all word tokens, e.g., in ATT and HMATT, we have argmax 5.3. Analysis number of topics ( T =50).
 the KL divergence of the topics. Figs. 4 and 5 plot the occurrence probability of NIPS at different times, using HMATT.
 sensitive to the time. These results have a clear explanation. Topic #4 talks about in 2004 and obtained great achievements in that season.
 there is already much research analyzing the NIPS data, e.g., [31] and [40] .
Fig. 6 plots the trends of changes in a few representative terms in the topic quickly with the end of the preseason (Nov.). The words  X 
Fig. 7 plots the changes in interests of fi ve representative authors on the topic and quickly become silent or switch to another topic of discussion.
In addition, we plot changes in the topic interest of the author authors who seem to have stable interests, for example, from Apr. 2004 to Jan. 2005, the author messages on Topic #10 and seldom posted messages on other topics. 6. Conclusion have proposed two generative models, i.e., Author  X  Time  X  clearly outperform the state-of-the-art topic models, the LDA and the Author prediction. Analysis on the topic fi nding results also unveils some interesting patterns in the topics. example modeling the evolution of image and associated tags.
 Acknowledgements Research Fund (No. 20070003093).

References
