 Many applications on blog search and mining often meet the challenge of handling huge volume of blog data, in which one single blog could contain hundreds or even thousands of entries. We investigate novel techniques for profiling blogs by selecting a subset of representative entries for each blog. We propose two principles for guiding the entry selection task: representativeness and diversity .Further,weformu-late the entry selection task into a combinatorial optimiza-tion problem and propose a greedy yet effective algorithm for finding a good approximate solution by exploiting the the-ory of submodular functions. We suggest blog classification for judging the performance of the proposed entry selection techniques and evaluate their performance on a real blog dataset, in which encouraging results were obtained. H.3.1 [ Content Analysis and Indexing ]: Abstracting methods Algorithm, Experimentation Blog profiling, entry selection, blog classification
In general, a blog contains a number of elements, includ-ing blog entries (or posts), tags, comments, links, and others. Among various elements, entries are the most important ele-ment for content analysis of a blog. To simplify the problem, in our study, each blog is treated as a set of entries or a se-quence of entries. Given this entry set representation, some conventional web search and mining techniques can be ap-plied for the blog search and ming applications by treating each entry as a web page. However, they often encounter the efficiency challenge since one single blog can contain hun-dreds or even thousands of entries. In addition, blogs often contain noisy entries that may degrade the performance of blog search and mining tasks. For example, some blog posts may be only a few sentimental words or just a single URL, which are irrelevant to the main theme of the blog. These noisy entries and define the representativeness measure on the major clusters. Specifically, we first cluster the entries in the blog into k clusters and then treat the  X  X mall X  clusters as outliers and drop them before the entry selection phase. In the next phase, for an entry B ij  X  B i ,wemeasureits representativeness below: where l ( B ij ) is the number of distinct words in the entry B
The diversity of a given subset of selected entries S i is measured by: Similar measures are often adopted in other applications [1, 3]. Based on the above measures of representativeness and diversity, for a candidate subset of entries S i , we define the quality evaluation function below: where the function r ( S i ; B i )measuresthe representativeness measures the diversity among the entries in S i ,and  X  is a parameter to balance the tradeoff between them.

The task of blog entry selection is then reduced to the issue of finding the optimal subset S i for the following opti-mization: where k is the number of entries to be selected. The above problem is a combinatorial optimization problem, which is often NP-hard to find the global optima.

To solve the optimization problem above, we explore the theory of submodular functions for seeking the sub-optimal solution. In particular, we show the following theorem.
Theorem 1. The objective function in (5) is a submod-ular function.

Given a submodular function f ( S ) and the related combi-natorial optimization problem above, a straightforward so-lution is to employ a greedy approach (denoted by GES): we start an empty set for S ; in each iteration, we expand the set S with the element e that maximizes the difference f (
S X  e )  X  f ( S ). We keep on expanding S till the number of elements in S is k . The submodular theorem in [2] provides the performance guarantee for the greedy algorithm above for finding the sub-optimal solution.
In our experiment, we use blog classification to exam-ine the performance of the proposed entry selection tech-niques. In our experiments, we crawled a dataset from the BlogFlux 1 , which consists of 5,000 blogs with 840,150 entries in English. These blogs come from 10 popular categories and each blog belongs to one or more categories. For experimen-tal evaluations, we partition the dataset into two parts: one half for training and the other half for test. Table 1 shows the statistics of the dataset. http://dir.blogflux.com
