 The term OLAP (On-Line Analytical Processing) is now widely used to refer to multidimensional databases and to data warehouse systems. However, originally, it was meant to denote a specific class of queries characterized by high interac-tivity and flexibility, small formulation effort, read-only access, and data ag-gregation, run by decision makers to analyze their business trend and effectively explore key figures and indicators. While OLTP (On-Line Transactional Process-ing) queries are normally grouped into transactions that support the everyday operational processes in a company, OL AP queries are typically sequenced into sessions . Users create sessions by applying a s equence of OLAP operations (such as drill-down and slice-and-dice) that transform one multidimensional query into another, starting from an initial query that is usually predefined [15]. During an OLAP session the user analyzes the results of a query and, depending on the spe-cific data she sees, applies one operatio n to determine a new query that will give her a better understanding of information. The resulting sequences of queries are strongly related to the issuing user, to the analyzed phenomenon, and to the current data.

Differently from OLTP workloads, that are 90% frozen within operational applications, OLAP workloads are hardly predictable due to their inherently extemporary nature. Besides, obtaining real OLAP workloads by monitoring the queries actually issued in companies and organizations is quite hard because (i) OLAP queries are at the core of the decision-making process, hence they are jealously guarded by managers and administrators, and (ii) reconstructing OLAP sessions by interpreting the query log of a multidimensional engine operating in a multi-user context is very complex.

On the other hand, hardware and software benchmarking in the industrial world, as well as comparative evaluation of novel approaches in the research community, both need reference databases and workloads. To this end, some ef-forts have been done over the years to provide standard benchmarks. Specifically, in the OLAP context, the TPC-DS benchm ark [14] has been recently developed; it is based on a fixed set of star schemata including 7 fact tables and 17 dimen-sion tables, and it provides a workload featuring queries that address complex business problems and use a variety of access patterns.
 The TPC-DS benchmark is carefully designed and offers a solid reference. However, especially in research papers, there is often a need for using bench-marks based on schemata with varying characteristic and on multiple alterna-tive workloads with different features. For instance, it coul d be interesting to understand how the performance of a proposed approach varies with the num-ber of dimensions in a cube, with the average branching factor of hierarchies, with the maximum length of sessions, or with the average selectivity of queries. In particular, generating parametric OLAP workloads is crucial to the experi-ments made in the context of OLAP prediction and recommendation, where the features of sessions and queries may have a strong impact on the approach effec-tiveness and efficiency. So, the papers in this context often rely on synthetically generated OLAP workloads, where queries and session are built in a completely random way based on a set of structural and statistical parameters [1 X 4]. Un-fortunately, while these synthetic workloads serve well for efficiency tests, they cannot provide significant results for eff ectiveness tests because they do not lean on a realistic user model.
 To fill this gap, in this paper we present CubeLoad , a parametric generator of OLAP workloads. The main features of CubeLoad are:  X  No predefined multidimensional schema is used. The benchmarker 1 can cre- X  The workload is generated in the form of sessions, each including a variable  X  Sessions are generated according to a se t of four templates, that model re- X  If an instance of the multidimensional schema is available (in particular,  X  The generated workload is exported in XML to ensure maximum usability. CubeLoad is written in Java and can be downloaded at http://big.csr.unibo.it/ downloads/CubeLoad.zip . It can be freely used by researchers, practitioners, and vendors whenever they need to create parametric bulk OLAP workloads for benchmarking and testing.

The paper outline is as follows. After discussing some related literature in Sec-tion 2, in Section 3 we describe the overall functional architecture of CubeLoad. Then we present our workload model and the session templates we defined so far in Sections 4 and 5, respectively. Finally, in Section 6 we discuss the results of some tests we made to profile the generated workloads and in Section 7 we draw the conclusions. A milestone in OLAP benchmarking is the TPC-DS [14], that models the deci-sion support functions of a retail product supplier relying on multiple snowflake schemata with shared dimensions. The TPC-DS provides four classes of queries; in particular, the class of iterative OLAP queries is distinguished by the ten-dency of one query to be related to the previous query so as to create sequence of queries  X  X ssentially, OLAP sessions. Queries are randomly g enerated starting from four templates; however, there is no way of parameterizing the generation of sessions.

In [7] the authors introduce the concept of workload profile as a way for sum-marizing the features of an OLAP workload to support designers during logical and physical design. However, the profile used there has a merely statistical na-ture, and has no relationship with classes of users. Besides, only stand-alone queries are generated.

A workload for evolutionary analytics is proposed in [10] together with several test metrics and with a methodology for running the workload. The emphasis there is not on standard OLAP sessions but rather on queries that evolve over time (which may imply much more drastic changes than those obtained through OLAP operations) and are formulated over changing data and schemata.
A Data Warehouse Engineering Benchmark (DWEB) that allows to generate various ad-hoc synthetic data warehouses and workloads is presented in [6]. Though the DWEB workload is parameterized to fulfill data warehouse design needs, it does not create queries in sessions and is ruled by statistical parameters rather than by realistic assumptions.

The author of [13] starts from the query generator of the TCP-DS to define a set of rules that transform a SQL query into another SQL query similar to the original. However, this transformation works at a merely syntactical level (e.g., a new query can be created by changing the comparison operator in the selection predicate) and does not consider OLAP operations such as slicing and drilling.
In [18] the authors introduce a query generator to evaluate the quality of a query optimizer. Similarly to ours, the gen erator presented is schema-independent and is able to produce valid queries on any database. However, only OLTP queries are generated and, therefore, there is no mention of query sessions.
Finally, a benchmark on star schemata that extends the TPC-H is presented in [12]; the emphasis here is more on data schemata than on queries, so only 4 non-parameterized OLAP sessions (called query flights here) are provided. A functional overview of the CubeLoad architecture is sketched in Figure 1. The main input is the multidimensional schema on which the workload is to be generated. To provide this input we adopt the XML specification used by Mondrian for its metadata [9].
 Example 1. IPUMS is a public database storing census microdata for social and economic research [11]. An excerpt of the XML specification for its CENSUS multidimensional schema is given below. Here, a CITY hierarchy is declared that features three aggregation levels, Region , State ,and City besides the AllCity level. Besides, two measures SumCostGas and SumIncTot are declared.
 To maximize interoperability, the workloads generated by CubeLoad are coded using XML; an example is shown below: (an explanation of the parameters and of the other workload elements mentioned in this XML will be given in Section 4).

To generate realistic selection predica tes and enable report sizes to be esti-mated, dimension data are needed. These data can be fed into CubeLoad using the CSV (comma-separated values) format, which can be easily obtained by benchmarkers by exporting dimension tables.

Internally, CubeLoad includes five components: 1. The user interface , that allows benchmarkers to select the XML multidi-2. The file interface , in charge of reading and parsing XML and CSV input 3. The multidimensional schema manager , that builds an internal repre-4. The session generator , that runs the basic procedures for creating sessions 5. The template manager , that gives the session generator additional rules The output of CubeLoad is an OLAP workload, defined as a set of sessions .A session is a sequence of queries. In the current implementation, we support a basic form of multidimensional query consisting of (i) a group-by (i.e., a set of hierarchy levels on which measure values are grouped); (ii) one or more measures whose values are returned (the aggregation operator used for each measure is defined by the multidimensional schema); and (iii) zero or more selection predicates ,each operating on a hierarchy level. We call report the result of a query; its size is the number of facts returned. Roughly, the size of a report can be estimated as the product of the domain cardinalities for all levels in the query group-by, reduced by considering the selectivity f actors of the selection predicates; more accurate estimates can be computed if the sparsity of the cube is known [5]. Two consecutive queries within a session are normally separated by the application of one OLAP operation, that changes either the group-by, or the selection predicate, or the set of measures returned, as shown in the following example.
 Example 2. An example of a session starting from seed query q 1 is s = q 1 ,q 2 ,q 3 ,q 4 ; the group-by X  X , selection predicates, and returned measures for the queries involved are shown in Table 1. Query q 2 is obtained from q 1 by drilling-down the cube along the CITY dimension; q 3 is obtained from q 2 by slicing-and-dicing the cube; q 4 is obtained from q 3 by changing the measure returned.
 In company settings, users of OLAP front-ends are normally grouped into pro-files with different skills (e.g., CEO, marketing analyst, department manager) and involved in business analyses with diff erent features (e.g., more or less repet-itive, more or less complex). Importantly, different profiles generally have quite different permissions for accessing data; often, a profile has one or more segre-gation predicates , i.e., it can only view a specific slice of the cube data (e.g., a department manager can only acces s the sales for her department).

When a user logs to the OLAP front-end, she is typically shown a page where some predefined queries (which we call seed queries ) are linked. Sometimes seed queries include a prompt , meaning that the front-end asks the user to select one value out of the domain of a level (often, the year). After choosing and executing one of these queries, the user starts applying a sequence of OLAP operations that progressively transform a query into another so as to build an analysis session. Features such as the number of seed queries available, the maximum size and complexity of reports returned by seed qu eries, and the average length of sessions may significantly depend on the typical ICT skills and business understanding for the users of each profile  X  X esides on the quality of the OLAP fron-end.
To simulate the above setting, CubeLoad uses a set of parameters that rule workload generation and are distinguished into global parameters and profile parameters . The global parameters rule:  X  the number of distinct user profiles to be simulated. Each profile simu- X  the maximum number of measures that can be returned by a single  X  the minimum and maximum size of seed query reports . The size (i.e.,  X  the number of surprising queries , whose meaning will be explained in
Each profile is then associated to a fur ther set of parameters, that rule:  X  the number of seed queries . Specialists X  profiles have a large number of  X  the minimum and maximum length of sessions . The values for these  X  the number of sessions to be created. The more intensive the use of the  X  the fraction of seed queries that include a year prompt . This fraction  X  the presence of a segregation predicate . A segregation predicate is typ-
The workload model is summarized in Figure 2 in the form of a UML class diagram.
 Each session generated by CubeLoad for a given profile starts from one of the seed queries for that profile and evolves, consistently with global and profile parameters, according to a template . In its current implementation, CubeLoad uses four different templates for generating sessions: 1. Slice-and-Drill. In several OLAP front-ends, the default behavior when a 2. Slice-All. Users are sometimes interested in n avigating a cube by slices, i.e., 3. Explorative. Some queries may return reports that are particularly inter-4. Goal-Oriented. Sessions of this type are run by users who have a specific
Figure 3 shows an intuition of sessions based on the four templates in a qual-itative group-by/select ion predicate space.
 To verify that the CubeLoad parameters and templates actually allow a wide spectrum of workloads to be generated, and to help benchmarkers better under-stand the relationships between those parameters/templates and the workload features, we use a similarity function that was specifically proposed in [2] for comparing OLAP queries and sessions. The query similarity function,  X  que ,is a combination of three components: one related to group-by X  X , one to selection predicates, and one to measure sets.
 Definition 6.1 (Similarity of OLAP queries). Let q and q be two queries on the same n -dimensional schema. The similarity between q and q is where:  X  The similarity between the group-by X  X  of q and q , { l 1 ,...,l n } and { l 1 ,...,l n }  X  The similarity between the selection predicates of q and q , { p 1 ,...,p n } and  X  The similarity between the measure sets returned by q and q , M and M
The session similarity function,  X  ali ( s, s )  X  [0 .. 1], is based on the best align-ment between the queries belonging to sessions s and s . The best alignment is computed by means of the Smith-Waterman algorithm, which efficiently matches subsequences of two given sequences by ig noring the non-matching parts [17]. It is a dynamic programming algorithm based on a matrix whose value in position ( i, j ) expresses the score for aligning subsequences of s and s that end in queries s and s j , respectively. This score is computed starting from the similarity be-tween the queries included in the aligned subsequences [2].
To explore the range of possibilities of CubeLoad we generated three sample workloads with the following  X  X xtreme X  features: 1. Workload W 1 is a sparse one, i.e., the sessions generated are quite different 2. Workload W 2 is a clustered one, i.e., the sessions generated are similar 3. Workload W 3 is a dense one, i.e., the sessions generated are all quite similar For a fair comparison, all three workloads include the same numbers of sessions (200); the values for the other parameters are summarized in Table 2. A qualitative analysis of these three workloads can be made by observing Figure 4, that shows for each of them the session-to-session similarity. Each row and column corresponds to one of the 200 sessions of the workload, so each cell shows the similarity between two different sessions of the same workload: white means  X  ali =0,black  X  ali = 1, gray shades mean 0 &lt; X  ali &lt; 1. As expected, in Figure 4.a we find a very low average similarity between sessions, while in Figure 4.c the average similarity is much higher. In Figure 4.b we can easily find the five cluster as areas with higher-than-average similarity. A quantitative confirmation of this fact can be found in Figure 5, that shows for each workload the average session-to-session similarity and its standard deviation: they are both lower for the sparse workload W 1 (where all sessions are different), while they increasingly grow high er for the clustered workload W 2 (where sessions in the same cluster are very similar to eac h other and very different from those in the other clusters) and the dense workload W 3 (in the latter case, the standard deviation is high because the four templates adopted inevitably introduce a scattering in the sessions generated).

Figure 5 also shows the propensity of each workload to being clustered. The indicator we adopted to this end is the Hopkins statistics [8]. Given a workload W , i.e., a set of N sessions, we first generate a set S of m fake sessions ( m N ) that are randomly and uniformly distributed in the space of possible sessions. For each fake session s i  X  S ,let u i be its distance from the nearest-neighbor session in W (where Distance ( s, s )=1  X   X  ali ( s, s )). Then, m sessions are randomly chosen from W ;let w i be the distance of the i -th of these sessions from its nearest-neighbor in W . The Hopkins statistics is then defined as For workload W 1, H is near to 0.5; this means that the distance of each session in W 1 from its nearest-neighbor is very similar to the distance of each fake session, i.e., that W 1 has a random distribution. For W 2 is quite small; this is because the w i  X  X  are small, which means that sessions are well clustered. For W 3 H is even smaller, because all sessions a re part of a single, dense cluster.
Finally, Figure 6 gives a quantitative explanation of the differences between our four templates by showing the similarity  X  que between the first query and the subsequent queries for sessions bas ed on each template. In the slice-and-drill template, the saw-tooth trend arises because when a sequence of slice-and-drill clicks along hierarchy h leads to a query grouped by the finest level of h , the simulated user behavior is to go back to the seed query and start a new slice-and-drill sequence along a different hierarchy (three such sequences are clearly visible in the figure). In the sli ce-all template, only the specific member appearing in the query selection predicate is changed during the session, so the query similarity is mostly constant and quite high. In the explorative template, the session rapidly converges towards the surprising query (the sixth query in the session in this case), then it moves r andomly in the query space (in this case, it tends to reapproach the seed query). Finally, in the goal-oriented template the session randomly moves towards its goal query. In this paper we have described the fea tures of CubeLoad, a generator of OLAP sessions aimed at simulating realistic workloads. The sessions generated are cur-rently based on four templates and ruled by a set of parameters. The template features and the impact of parameters on the resulting workload have been dis-cussed with the support of some tests using a similarity function specifically devised for OLAP sessions.
 Some comparison between CubeLoad and TPC-DS is useful at this point. Overall, the focus in the TPC-DS is more on the complexity of single queries rather than on query sessi ons. Indeed, while the query model is more expressive than in CubeLoad because nesting is supported, three of the four classes of queries provided in the TPC-DS (namely, ad hoc queries , reporting queries ,and data mining queries ) only include stand-alone queries; as such, they could be generated with CubeLoad by setting the maximum length of sessions to 1 and properly tuning the maximum size of s eed query reports (differently from the first two classes, data mining queries are characterized by high cardinality of the results). Conversely, the class of iterative OLAP queries comprises four base sessions each including exactly 2 querie s; more sessions can be generated from each base session by randomly changing a selection predicate. In two of the base sessions, the subsequent queries are not related by the application of a single OLAP operator like in CubeLoad, so they can be quite  X  X istant X  from each other, but still they are finalized to the same analysis goal. In the other two base sessions, the two subsequent queries diff er from their selection predicate. Thus, an effective way to generate sessions like these ones with CubeLoad is to use the goal-oriented and the slice-all templates and fix the number of seed queries to 4, with a session length equal to 2.

Our future work on this topic will be mainly aimed at enhancing the capabili-ties of CubeLoad in three directions: (i) by allowing benchmarkers to distinguish skilled and non-skilled profiles, so as to enable a finer tuning of the workload fea-tures; (ii) by defining other templates, so as to make CubeLoad more flexible and usable for a wider array of benchmarks; and (iii) by adopting a more complex query model, so as to make the generated workloads more realistic still. From the engineering point of view, we plan to r efactor the CubeLoad code according to an open architecture where each benc hmarker can write her own templates in the form of a plugin.
 Acknowledgements. We would like to thank Luca Spadazzi for his support in implementing and testing CubeLoad, and Patrick Marcel (Universit  X  eFran  X  cois Rabelais, Tours, France) for the fruitful discussions about the features of a real-istic OLAP workload.
