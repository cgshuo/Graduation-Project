
Kansas State University dcaragea@ksu.edu
Many real-world problems, e.g. protein function or protein subcellular localization prediction, can be cast as sequence classification problems (1). Markov models (MMs), which capture dependencies between neighboring sequence ele-ments, are among the most widely used generative models of sequence data (2), (3). In a k th order MM, the sequence elements satisfy the Markov property : each element is in-dependent of the rest given k preceding elements (called parents ). MMs have been successfully applied in many applications including natural language processing (3) and molecular sequence classification (2). One of the main disadvantages of MMs in practice is that the number of MM parameters increases exponentially with the range k of direct dependencies, thereby increasing the risk of overfitting when the data set is limited in size.

Against this background, we present abstraction aug-mented Markov models (AAMMs) aimed at addressing these difficulties. AAMM X  X  advantages are as follows:  X  AAMMs effectively reduce the number of numeric  X  Abstraction acts as a regularizer that helps minimize
We evaluate AAMMs on three protein subcellular lo-calization prediction tasks. The results of our experiments show that AAMMs are able to use significantly smaller number of features (by one to three orders of magnitude) as compared to MMs. AAMMs often yield significantly more accurate classifiers than MMs. Moreover, the results show that AAMMs often perform significantly better than variable order Markov models (VMMs) (4), such as decomposed context tree weighting, prediction by partial match, and probabilistic suffix trees.

The rest of the paper is organized as follows: Section 2 introduces AAMMs. Section 3 presents experimental design and results and Section 4 concludes with a summary and discussion.

Before introducing abstraction augmented Markov mod-els , we briefly review Markov models.
 A. Markov Models Let x = x 0  X  X  X  x n  X  1 be a sequence over a finite alphabet X , x  X  X  X  , and let X i , for i = 0 ,  X  X  X  ,n  X  1 , denote the random variables corresponding to the sequence elements x
X order abstraction augmented Markov model In a k th order Markov model (MM), the sequence elements satisfy the Markov property : That is, X i is conditionally independent of X 0 ,  X  X  X  ,X given X i  X  k ,  X  X  X  ,X i  X  1 for i = k,  X  X  X  ,n  X  1 . X i  X  k are called parents of X i . Hence, under a k th order MM, the joint probability of X = { X 0 ,  X  X  X  ,X n  X  1 } can be factorized as follows: p ( X ) = p ( X 0 ,  X  X  X  ,X k  X  1 ) An MM can be represented as a directed graph where the nodes represent the random variables X i , and the edges represent direct dependencies between neighboring elements of x . Figure 1a shows the directed graph for a 2 nd order MM on a subset of nodes of x : { X i  X  3 ,  X  X  X  ,X i +1 } . k th order MM. The values of S i  X  1 represent instantiations of X i  X  k  X  X  X  X i  X  1 , which are substrings of length k (i.e., k -grams) over the alphabet X . Furthermore, let S denote the set of k -grams over X , s denote a k -gram in S , and  X  a symbol in X . The set of parameters  X  that define an MM is:  X  = {  X   X  | s :  X   X  X ,s  X  S ;  X  s : s  X  S} , where  X   X  | s = p (  X  | s ;  X  ) ,  X  s = p ( s |  X  ) .
 The cardinality of S (i.e., |S| ) is |X| k and is denoted by N . Hence, the number of parameters of a k th order MM is proportional to N , which grows exponentially with the length k of direct dependencies.
 B. Abstraction Augmented Markov Models
Abstraction augmented Markov models (AAMMs) ef-fectively reduce the number of numeric parameters of a k th order MM by grouping k -grams into an abstraction hierarchy.

Definition 1 (Abstraction Hierarchy) An abstraction hierarchy T over a set of k -grams S is a rooted tree such that: (1) the root of T denotes S ; (2) the leaves of T correspond to singleton sets containing individual k -grams in S ; (3) the children of each node (say a ) correspond to a partition of the set of k -grams denoted by a . Thus, a denotes an abstraction or grouping of  X  X imilar X  k -grams.

Note that each internal node (or abstraction a) contains the k -grams at the leaves of the subtree rooted at a.
Definition 2 ( m -Cut ) An m -cut  X  m through an abstrac-tion hierarchy T is a subset of m nodes of T such that for any leaf s i  X  S , either s i  X   X  m or s i is a descendant of some node in  X  m . The set of abstractions A at any given m -cut  X  m forms a partition of S .

Specifically, an m -cut  X  m partitions the set S of k -grams into m ( m  X  N ) non-overlapping subsets A = { a 1 : S ,  X  X  X  ,a m : S m } , where a i denotes the i -th abstraction and S i denotes the subset of k -grams that are grouped together into the i -th abstraction based on some similarity measure. Note that S 1  X  X  X  X  X  X  m = S and  X  1  X  i,j  X  m, S i  X  X  j =  X  .
AAMMs extend the graphical structure of MMs by in-troducing new variables A i that represent abstractions over the values of S i  X  1 , for i = k,  X  X  X  ,n  X  1 . In AAMMs, each node X i directly depends on A i as opposed to S i  X  1 in MMs). Figure 1b shows the directed graph for a 2 order AAMM on a subset of nodes: { X i  X  3 ,  X  X  X  ,X { A i  X  1 ,  X  X  X  ,A i +1 } . Each variable A i takes values in the set of abstractions A = { a 1 ,  X  X  X  ,a m } corresponding to an m -cut,  X  m , which specifies an AAMM. We model the fact that A i is an abstraction of S i  X  1 by defining p ( A i = a i | S s i  X  1 ) = 1 if s i  X  1  X  a i , and 0 otherwise, where s i  X  1 a  X  X  represent instantiations of S i  X  1 and A i , respectively.
Under a k th order AAMM, the joint probability of the entire set of variables X  X  A can be factorized as follows: p ( X , A ) = p ( S k  X  1 )  X  The set of parameters  X  of an AAMM is:  X  = {  X   X  | a X ,a  X  A ;  X  a | s : a  X  A ,s  X  S ;  X  s : s  X  S} , where  X  p (  X  | a ;  X  ) ,  X  a | s = p ( a | s ;  X  ) , and  X  s = p ( s |  X  ) . 1) Learning AAMMs: In what follows we show how to learn AAMMs from data. This involves: learning an abstraction hierarchy; and learning model parameters using the resulting abstraction hierarchy.

Learning an Abstraction Hierarchy: The procedure for learning an abstraction hierarchy (AH) over the set S of k -Algorithm 1 Abstraction Hierarchy Learning
Input: A set of k -grams S = { s 1 ,  X  X  X  ,s N } ; a set of Output: An abstraction hierarchy T over S Initialize A = { a 1 : { s 1 } ,  X  X  X  , a N : { s N }} , and for w = N + 1 to 2 N  X  1 do end for grams is shown in Algorithm 1. The input consists of the set S of k -grams and a set D of sequences over the alphabet X , D = { x l } l =1 ,  X  X  X  , |D| . The output is an AH T over S . The algorithm starts by initializing the set of abstractions A such that each abstraction a i  X  A corresponds to a k -gram s i  X  S , i = 1 ,  X  X  X  ,N (the leaves of T are initialized with elements of S ). The algorithm recursively merges pairs of abstractions that are most  X  X imilar X  to each other and terminates with an AH T after N  X  1 steps. We store T in a last-in-first-out (LIFO) stack. For a given choice of the size m of an m -cut through T , we can extract the set of abstractions that specifies an AAMM, by discarding the top m  X  1 elements from the stack.

Next we introduce a measure of similarity between a pair of abstractions. We consider two abstractions to be  X  X imilar X  if they occur within similar contexts .

Context of an abstraction. We define the context of a k -gram s  X  S to be the conditional probability distribution p ( X i | s ) of the sequence element X i that  X  X ollows X  the k -from the data set D of sequences as follows:  X  p ( X i | s ) = where #[ s X , x l ] represents the number of times the symbol  X   X  X ollows X  the k -gram s in the sequence x l .

The context of an abstraction a (i.e., a set of k -grams a = { s 1 ,  X  X  X  ,s | a | } ) is obtained using a weighted aggregation of the contexts of its constituent k -grams. The weights are chosen to ensure that such aggregation yields a proper probability distribution. That is, where # s t = 1 + P |D| l =1 #[ s t , x l ] .

From the preceding definitions it follows that p ( X i =  X  | a ) corresponds to the conditional probability that the symbol  X  ,  X   X  X  ,  X  X ollows X  some k -gram s t  X  a .

Distance between abstractions. We proceed to define a distance between a pair of abstractions a u and a v , denoted by d D ( a u ,a v ) . As we shall see below, the definition of d ensures that, at each step, Algorithm 1 selects a pair of abstractions to merge such that the loss of information resulting from the merger is minimized.

The reduction, due to a single step of Algorithm 1, in mutual information between a node X i and its parent A i in an AAMM (see Figure 1b) can be calculated as follows: Let  X  m be an m -cut through the AH T and  X  m  X  1 be the ( m  X  1) -cut through T that results after the merger of a u and a v a , i.e., { a u ,a v } X  a w . Let A m and A m  X  1 denote the sets of abstractions corresponding to  X  m and  X  m  X  1 , respectively. Furthermore, let  X  u and  X  v denote the prior probabilities of a u and a v in the merger a w , i.e.,  X  u =  X 
Proposition 1: The reduction in the mutual information between each variable X i and its parent A i , due to the merger of a u and a v into a w is given by  X I ( { a u ,a v ( p ( a u ) + p ( a v ))  X  JS  X  u , X  v ( p ( X i | a u ) ,p ( X Jensen-Shannon divergence (5) between two probability dis-tributions p ( X i | a u ) and p ( X i | a v ) with weights  X  respectively.

We define the distance between two abstractions a u a v in D as follows: d
D ( a u ,a v ) =  X I ( { a u ,a v } ,a w ) where a w = { a The effect of one merge of Algorithm 1 on the log likelihood of the data is given by the following proposition.

Proposition 2: The reduction in the log likelihood of the data D given an AAMM based on the merger { a u ,a v } X  a w is given by  X LL ( { a u ,a v } ,a w ) = M  X  ( p ( a u ) + p ( a nality of the multiset of ( k +1) -grams in D . (See Appendix B for the proof sketch of Propositions 1 and 2 ).

Algorithm Analysis: Recall that S = { s 1 ,  X  X  X  ,s N } is the set of unique k -grams in D , N = |S| , and that A = { a 1 ,  X  X  X  ,a m } is the set of constructed abstractions, m = |A| . At each step, the algorithm searches for a pair of abstractions that are most  X  X imilar X  to each other. The computation of d D ( a u ,a v ) takes O ( |X| ) time. Furthermore, at each step, for each A w = { a 1 : S 1 ,  X  X  X  ,a w : S 3 -cut  X  3 through T ; (b) The computation of p ( x = x 0 ) given the abstraction hierarchy T and the cut  X  3 . w = N,  X  X  X  ,m + 1 , there are w ( w  X  1) 2 possible pairs of abstractions to consider. However, the computational time can be reduced by a factor of N by precomputing the distances d D ( a u ,a v ) between each pair of (trivial) a a v in A N , and then, at each step, updating only the distances between pairs containing a u min and a v min . Thus, the time complexity of Algorithm 1 is O ( N 2 |X| ) .

Next we show how to learn AAMM parameters from data using the resulting abstraction hierarchy T .

Learning AAMM Parameters: AAMMs are completely observable graphical models (i.e. there are no hidden vari-abstractions A corresponding to an m -cut,  X  m , through the resulting AH T , learning an AAMM reduces to estimating the set of parameters  X  from D , denoted by  X   X  , using maximum likelihood estimation (6). This can be done as follows: use Equation (4) to obtain the estimates h  X   X   X  | s of  X   X  | s  X   X  X  for any k -gram s  X  X  (note that these estimates correspond to the estimates h  X   X   X  | a i i.e., the leaf level in the AH T ). The estimates of  X   X  | a  X   X  X  , when a = { s 1 ,  X  X  X  ,s | a | } , are a weighted aggregation of the estimates of a  X  X  constituent k -grams, i.e., where # s t are defined as before. The estimate  X   X  s obtained from D as follows: where #[ s, x l ] is the number of times s occurs in x used Laplace correction to obtain smoothed estimates of probabilities.

Given an AH and a choice of the size m of an m -cut, an array of indices of size N (corresponding to the number of unique k -grams extracted from D 2 ) is used to specify the membership of k -grams in the abstractions on the m -cut. Hence, the space complexity for storing this array is N . However, the number of parameters of the corresponding AAMM is m |X| , as opposed to N |X| in the case of MMs ( m N ).

Figure 2a shows an example of an AH T learned from a training set, which consists of a single se-quence s = abracadabra over the set of 2 -grams S = { ra,ca,da,ab,br,ac,ad } extracted from s , where the al-phabet X is { a,b,c,d,r } . In the figure, the subset of nodes { a 10 ,a 3 ,a 8 } represents a 3 -cut  X  3 through T . The nodes of  X  3 are annotated with the AAMM parameters learned from the same training set of a single sequence abracadabra . Thus, the probabilities that the letters a , b , c , d , and r  X  X ollow X  the abstraction a 10 : { ra,ca,da } , i.e., h  X   X  (Note that, in practice, T is learned from a training set consisting of a large number of sequences). 2) Using AAMMs for Classification: Given a new se-quence x = x 0 ,  X  X  X  ,x n  X  1 and an AAMM (corresponding the abstraction a j  X   X  m it belongs to and retrieve the parameters associated with a j . Successively multiply  X   X  for i = k,  X  X  X  ,n  X  1 to obtain p ( x |  X   X  ) .

Figure 2b shows how to compute p ( x ) given the resulting abstraction hierarchy over the set of 2 -grams and the cut  X  . For example, p ( abracadabra |  X   X  ) , where  X   X  represents the AAMM corresponding to the cut { a 10 ,a 3 ,a 8 } in Figure 2a, is obtained as follows: p ( abracadabra ) = p ( ab ) p ( r | a 3 ) p ( a | a 8 ) p ( c | a )
AAMMs can be used for classification by learning a model for each class and selecting the model with the highest posterior probability when classifying new data. Specifically, classification of a sequence x requires computation of con-ditional probability p ( c j | x ;  X   X  ) , for each class c C is the set of possible classes. By applying Bayes rule, we obtain: The class with the highest posterior probability, arg max j p ( c j | x ;  X   X  ) is assigned to x .
 A. Experimental Design
Our experiments are designed to explore the following questions: (i) How does the performance of AAMMs com-pare with that of MMs and Na  X   X ve Bayes (NB) classifiers, given that AAMMs effectively reduce the number of nu-meric parameters of MMs through abstraction ? (ii) What is the effect of the algorithms for learning AHs on the quality of the predictions made by AAMMs? (iii) How does the performance of AAMMs compare with that of variable order Markov models (VMMs) that use more compact representations of the abstraction hierarchies compared to AAMMs?
To answer the first question, we trained AAMMs for values of m that range from 1 to N , where m is the cardinality of the set of abstractions A m used as  X  X eatures X  in the classification model, and N is the number of unique k -grams, and compared the performance of AAMM with that of MM and NB over the entire range from 1 to N .
To answer the second question, we compared our AAMM clustering algorithm with agglomerative information bot-tleneck (AIB) introduced by Slonim and Tishby (7). The primary difference between our AAMM clustering algorithm and AIB is in the criterion used to cluster the k -grams, i.e., in AAMM, the k -grams are clustered based on the similarity between the conditional distributions of X i given the k -grams, where X i takes values in X ; in AIB, the k -grams are clustered based on the similarity between the conditional distributions of the class variable C given the k -grams, where C takes values in C .

We learned AHs from training sequences as follows: (i) a class-specific AH for each class using our AAMM clus-tering algorithm (from sequences belonging to that class); (ii) a class-independent AH using our AAMM clustering algorithm (from all training sequences, independent of the class variable); and (iii) an AH using the AIB clustering algorithm (from all sequences). In each case, we learned AAMM parameters for each class (from sequences in that class). We compared the performance of AAMMs (using different clustering algorithms) over the entire range from 1 to N .

To answer the third question, we trained AAMMs for values of m ranging from 1 to N and compared their performance with that of VMM-type learning algorithms (4), including Lempel-Ziv 78 (LZ78); an improved ver-sion of LZ78, namely LZ-MS; decomposed context tree weighting (DE-CTW); prediction by partial match (PPM-C); and probabilistic suffix tree (PST). In our experiments, the AAMM order is k = 3 . We set the parameters of the VMM-type algorithms as follows: input shifting S = 2 , back-shift parsing M = 1 for LZ-MS; the upper bound k on the Markov order for DE-CTW, PPM-C, and PST is set to 3 . In addition, for PST, the other parameters are set as in (4), namely p min = 0 . 001 ,  X  = 0 ,  X  = 0 . 0001 , and r = 1 . 05 . (see Appendix A for a brief description of these five learning algorithms and an explanation of parameters). For the VMM-type learning algorithms, we have used the VMM implementation of Begleiter et al., 2004 (4).

We present results of experiments on three protein sub-cellular localization data sets: psortNeg 3 introduced in (8), plant , and non-plant 4 introduced in (9). The psortNeg data set is extracted from PSORTdb v.2.0 Gram-negative sequences, which contains experimentally verified localiza-tion sites. Our data set consists of all proteins that belong to exactly one of the following five classes: cytoplasm (278), cytoplasmic membrane (309), periplasm (276), outer membrane (391) and extracellular (190). The total number of examples (proteins) in this data set is 1444. The plant data set contains 940 examples belonging to one of the following four classes: chloroplast (141), mitochondrial (368), secre-tory pathway/signal peptide (269) and other (consisting of 54 examples with label nuclear and 108 examples with label cytosolic). The non-plant data set contains 2738 examples, each in one of the following three classes: mitochondrial (361), secretory pathway/signal peptide (715) and other (consisting of 1224 examples labeled nuclear and 438 ex-amples labeled cytosolic).

For all of the experiments, we report the average classifi-cation accuracy obtained in a 5 -fold cross-validation experi-ment. We define the relative reduction in classification error between two classifiers to be the difference in error divided significance of our results, we used the 5 -fold cross-validated paired t test for the difference in two classification accuracies as  X  X eatures X  in the classification model, on a logarithmic scale. (10). The null hypothesis (i.e., two learning algorithms M and M 2 have the same accuracy on the same test set) can be | t ( M 1 , M 2 ) | by | t | in what follows.
 B. Results
We trained AAMMs and MMs using 3 -grams extracted from the data. For psortNeg , plant , and non-plant data sets, the numbers of 3 -grams are 7970 , 7965 , and 7999 respectively.

Comparison of AAMMs with MMs and NB. Figure 3 shows results of the comparison of AAMMs with MMs on all three data sets considered in this study. As can be seen in the figure, AAMM matches the performance of MM with substantially smaller number of abstractions. Specifically, the performance of MM trained using approximately 8000 3 -grams is matched by that of AAMM trained using only 79 , 19 and 855 abstractions on the psortNeg , plant , and non-plant data sets, respectively. On the psortNeg and non-plant data sets, AAMM has performance similar to that of MM over a broad range of choices of m . On the plant data set, AAMM significantly outperforms MM for many choices of m . For example, with only 168 abstractions, AAMM achieves its highest accuracy of 71 . 59% as compared to MM which achieves an accuracy of 68 . 19% with N = 7965 ( | t | = 3 . 03 ). This represents 13% reduction in classification error. Not surprisingly, when m = N , the performance of AAMMs is the same as that of MMs (AAMM trained using N abstractions and MM are exactly the same models).

We conclude that AAMMs can match and, in some cases, exceed the performance of MMs using significantly smaller number of abstractions (by one to three orders of magnitude). AAMMs could provide more robust estimates of model parameters than MMs, and hence, help minimize overfitting .

Figure 3 also shows the comparison of AAMM with NB trained using a  X  X ag of letters X  feature representation. As can be seen, except for a few values of m ( m &lt; 18 , m &lt; 5 , and m &lt; 2 on psortNeg , plant , and non-plant , respectively), AAMM significantly outperforms NB (for any other choices of m ). MM is superior in performance to NB on all data sets.
Comparison of AAMM clustering algorithm with Ag-glomerative Information Bottleneck. Figure 4 shows, on all three data sets, results of the comparison of AAMMs trained based on (i) class-specific AHs, with one AH for each class, (ii) a single class-independent AH, and (iii) an AH produced using AIB (7). As can be seen in the figure, AAMMs trained based on class-specific AHs generated by the clustering algorithm proposed here significantly outper-form AAMMs trained based on an AH generated by AIB, over a broad range of values of m (from 1 to 1000 ). For example, on the plant data set, with m = 100 , the accuracy of AAMM based on our clustering algorithm is 69 . 57% , whereas that of AIB-clustering based AAMM is 48 . 29% ( | t | = 11 . 31 ). This represents 42% reduction in classification error. As expected, AAMMs trained using class-specific AHs significantly outperform AAMMs trained using a class-independent AH on all three data sets.

We conclude that organizing k -grams in an AH based on the conditional distribution of the next element in the sequence rather than the conditional distribution of the class given the k -grams produces AHs that are better suited for AAMMs, and hence, result in better performing AAMMs.
Comparison of AAMMs with VMM-type learning algorithms. Table I summarizes, on all three data sets, the results of the comparison of AAMMs with five VMM-type learning algorithms: Lempel-Ziv 78 (LZ78); an improved version of LZ78, namely LZ-MS; decomposed context tree weighting (DE-CTW); prediction by partial match (PPM-C); and probabilistic suffix tree (PST). For AAMMs, we show data sets (SEM = standard error of the means). the best classification accuracy over the entire range of val-ues of m , on each data set. The values of m where AAMM reaches the best classification accuracy are: 438 , 168 , 7070 on psortNeg , plant , non-plant data sets, respectively.
As can be seen in the table, AAMM significantly out-performs LZ78, LZ-MS, and DE-CTW on all three data sets ( p &lt; 0 . 05 ). AAMM significantly outperforms PPM-C on psortNeg ( | t | = 4 . 973 ), and non-plant ( | t | = 3 . 099 ), and has the same performance as PPM-C on plant . Fur-thermore, AAMM significantly outperforms PST on plant ( | t | = 4 . 163 ), and is comparable in performance with PST on psortNeg (the null hypothesis is not rejected). On non-plant , PST significantly outperforms AAMM ( | t | = 4 . 433 ).
We conclude that AAMMs are competitive with, and often significantly outperform, VMM-type learning algorithms on the protein subcellular localization prediction task. A. Summary
We have presented abstraction augmented Markov models that simplify the data representation used by the standard Markov models. The results of our experiments on three protein subcellular localization data sets ( psortNeg , plant , and non-plant ) have shown that:  X  Organizing the set of k -grams in a hierarchy using  X  While abstraction helps reduce the number of MM  X  AAMMs are competitive with, and in many cases, These conclusions are supported by results of additional experiments on three other data sets compiled from the Structural Classification of Proteins (SCOP) database: E , F , and G SCOP classes that have been used by other authors (4) (data not shown due to space limitation).
 B. Related Work
Several authors have used abstraction hierarchies over classes to improve the performance of classifiers (e.g., (11; 12). Others have explored the use of abstraction hier-archies to learn compact predictive models (13; 14). Slonim and Tishby (7), Baker and McCallum (15), and Silvescu et al. (16) have generated abstraction hierarchies over fea-tures or k -grams based on the similarity of the probability distributions of the classes conditioned on the features or k -grams (respectively) and used the resulting abstract features to train classifiers (15; 16). In contrast, the focus of this paper is on abstraction hierarchies that group k -grams (or more generally their abstractions) based on the similarity of the probability distributions of each letter of the alphabet conditioned on the abstractions, and the use of the resulting abstraction hierarchies over k -grams to construct generative models from sequence data.

Begleiter et al. (4) (and papers cited therein) have ex-amined and compared several methods for prediction using variable order MMs (VMMs), including probabilistic suffix trees (PSTs) (17). PSTs can be viewed as a variant of AAMMs wherein the abstractions are constrained to share suffixes. Hence, the clustering of k -grams in PSTs can be represented more compactly compared to that of AAMMs, which require storing an array of indices of size N to specify the membership of k -grams in the abstractions (or clusters). The results of our experiments show that AAMMs are competitive with VMM-type learning algorithms. Inter-polated MMs (18), which recursively combine several fixed-order MMs, capture important sequence patterns that would otherwise be ignored by a single fixed-order MM.
 C. Discussion
Abstraction helps reduce the model input size and, at the same time, could potentially improve the statistical estimates of complex models by reducing the number of parameters that need to be estimated from data (hence reducing the risk of overfitting ). However, one limitation of our abstraction-based approach is that, while it provides simpler models, the simplicity is achieved at the risk of some information loss due to abstraction . To trade off the complexity of the model against its predictive accuracy, it would be useful to augment the algorithm so that it can choose an optimal cut in an AH. This can be achieved by designing a scoring function (based on a conditional MDL score), similar to Zhang et al. (13) in the case of Na  X   X ve Bayes, to guide a top-down search for an optimal cut.

It is worth noting that the AHs can be learned using any top-down or bottom-up clustering procedure. However, in this study, we have used a bottom-up approach because it is simple, fast, and allows iterating through all cardinalities, from 1 to N .

Connection between AAMMs and HMMs. An AAMM can be simulated by an appropriately constructed HMM where the number of hidden states is equal to the number of abstractions in the AAMM. However, as a state corresponds to an abstraction over the observable variables, the state is not really  X  X idden X . It can be derived in a feed-forward man-ner, thus not requiring a Backward Reasoning/Expectation step. This allows a  X  X ne pass through the data X  learning procedure based on MAP learning (19) once the set of abstractions is chosen. Unlike the learning procedure of HMMs (which involves the Expectation Maximization (EM) algorithm), the AAMM learning procedure is not prone to local maxima, has lower variance as no uncertainty is inherited from the inference performed in the E step, and requires less time.

The overall time complexity of AAMM is O ( N 2  X |X| + N  X  |D| ) , where N is the number of unique k -grams, |X| is the alphabet size, and |D| is the data set size, i.e., the number of (non-unique) k -grams in D . The time complexity of HMM EM learning procedure for a fixed number of hidden states is O ( T  X |D| X  ( | H | 2 + | H | X |X| )) , where | H | is the number of hidden states and T is the number of EM iterations. Running this procedure for all numbers of hidden states (from 1 to N ) requires an overall time of O ( N 2 ( N + |X| )  X  T  X |D| ) . Because N |D| , our algorithm requires at least a factor of N  X  T less time than HMM. While algorithms that attempt to automatically determine the number of hidden states in HMMs can be used (e.g., based on the Chinese Restaurant Process (20)), they incur additional costs relative to standard HMM, and are prone to the difficulties encountered by hidden variable models. Hence, although less expressive than HMMs, AAMMs are easier to learn.
 D. Future directions
Some directions for further research include: (i) design of AA Interpolated MMs (AAIMMs) that extend Interpo-lated MMs in the same way AAMMs extend MMs; (ii) applications of AAMMs to settings where data have a much richer structure (e.g., images and text); (iii) exploration of alternative clustering algorithms for generating abstraction hierarchies for use with AAMMs; (iv) incorporation of MDL-like criteria for finding an optimal level of abstraction.
This research was funded in part by an NSF grant IIS 0711356 to Vasant Honavar and Doina Caragea.

In what follows, we briefly describe five algorithms for learning variable order Markov models (VMMs) from a set of training sequences D over an alphabet X . See (4) for more details and citations therein.

Lempel-Ziv 78 (LZ78): The LZ78 learning algorithm (21), (22) extracts a set  X  S of non-overlapping adjacent phrases from sequences in D as follows:  X  S initially contains only the empty phrase ; at each step, a new phrase, which extends an existing phrase from  X  S by a symbol in X , is added to
The algorithm then constructs a phrase tree T over X such that the degree of each internal node is exactly |X| . Initially, T contains the root and |X| leaves (i.e., one leaf for each symbol in X ). For any phrase s  X   X  S , start at the root and traverse T according to s . When a leaf is reached, it is made an internal node by expanding it into |X| leaf nodes. Each node stores a counter such that the counter of a leaf node is one, and that of an internal node is the sum of the counters stored at the child nodes.

Improved Lempel-Ziv 78 Algorithm (LZ-MS): Two major disadvantages of the LZ78 learning algorithm are: (i) unre-liable estimation of model parameters when subsequences s of a sequence x are not parsed, and hence, they are not part of T ; (ii) unreliable computation of  X  p (  X  | s ) when the algo-rithm ends in a leaf node while traversing T along the path corresponding to s starting from the root. LZ-MS learning algorithm (23) aims at addressing these disadvantages by introducing two parameters: input shifting , denoted by S , and back-shift parsing , denoted by M . The S parameter ensures that more phrases are extracted during learning, whereas the M parameter ensures the existence of a suffix of s when computing  X  p (  X  | s ) .
 Decomposed Context Tree Weighting (DE-CTW): The CTW learning algorithm (24) combines exponentially many VMMs of k -bounded order in an efficient way. The CTW algorithm over binary alphabets X constructs a perfect binary tree T of height k from sequences in D . Each node is labeled with the string s corresponding to the path from this node to the root, and stores two counters, which represent the number of times each symbol in X (0 or 1) occurs after s in D .

CTW algorithm can be extended to work with multi-alphabets. One approach, called decomposed CTW (DE-CTW), uses a tree-based hierarchical decomposition of the multi-valued prediction problem into binary problems.
Prediction by Partial Match (PPM-C): The PPM learning algorithm (25) requires an upper bound k on the Markov order of the VMM it learns. It constructs a tree T of maximal depth k +1 from sequences in D as follows: it starts with the root node which corresponds to the empty string and parses sequences in D , one element at a time; each element x i in a sequence x and its k preceding elements x i  X  k ,  X  X  X  ,x form a path of length k +1 in T . Each node in T is labeled by a symbol  X  and stores a counter. The counter of a node  X  on a path s X  from the root represents the frequency counts of s X  in D , denoted by # s X  .

To obtain smoothed estimates of probabilities for any string s , | s |  X  k , PPM introduces a new variable, called escape , for all symbols in the alphabet that do not ap-pear after s in D and allocates a probability mass for all these symbols, i.e., p ( escape | s ) . 1  X  p ( escape | s ) is distributed among all other symbols that occur after s in D . A successful PPM variant, namely PPM-C, per-forms mass probability allocation for escape and mass probability distribution over the other symbols as follows:  X  p (  X  | s ) = # s X  in X that occur after s in D .

Probabilistic Suffix Tree (PST): The PST learning algo-rithm (26) constructs a non empty tree T over an alphabet X such that the degree of each node varies between 0 (for the leaves) and |X| (for the internal nodes). Each edge e is labeled by a single symbol in X , and each node v is labeled by a sub-sequence s that is obtained by the concatenation of edge labels on the path from v up to the root of T .
In the first stage, the PST learning algorithm identifies a set  X  S of candidate suffixes of length  X  k from D ( k is the maximal length of a suffix), such that the empirical probability of each suffix s  X   X  S ,  X  p ( s ) , is above some threshold p min . In the second stage, a candidate suffix s and all its suffixes are added to T if s satisfies two conditions: s is  X  X eaningful X  for some symbol  X  (i.e.,  X  p (  X  | s ) is above some user threshold (1+  X  )  X  min ), and s provides additional information relative to its parent s 0 , where s 0 is the string obtained from s by deleting the leftmost letter (i.e.,  X  p (  X  | s ) is greater than a user threshold r or smaller than 1 /r ). In the last stage, the probability distribution associated with each node, p (  X  | s ) over X for each s , are smoothed.

Lemma 1: Let X and Z be two random variables such that Z can take on k possible values. Let p ( z i ) be the prior distribution of z i and p ( x | z i ) be the conditional distribution of X given z i for i = 1 ,  X  X  X  ,k . Then: = H where H (  X  ) is Shannon X  X  entropy (5).

Proof of Proposition 1: Without loss of generality, let us assume that the merger is { a 1 ,a 2 }  X  a . Let  X I ( { a 1 ,a 2 } ,a ) = I ( A ( A m ) ,X i )  X  I ( A ( A the reduction in the mutual information I ( A ; X i ) , where A ( A m ) represents the variable A that takes values in the set A m = { a 1 ,  X  X  X  ,a m } . We use the above lemma. Hence,  X I ( { a 1 ,a 2 } ,a ) = H = p ( a ) H ( p ( x i | a ))  X  = p ( a ) H = p ( a ) = ( p ( a 1 ) + p ( a 2 ))  X  JS  X  1 , X  2 ( p ( X i | a 1
Proof of Proposition 2: As in Proposition 1 , let us assume, without loss of generality, that the merge is { a 1 ,a 2 }  X  a . Hence, # a = # a 1 + # a 2 . Furthermore, let  X   X LL ( { a 1 ,a 2 } ,a ) = LL ( A ( A m ) ,X i )  X  LL ( A ( A = X = X = X ) =  X  Mp ( a ) X  X   X  = Mp ( a ) = M  X  (( p ( a 1 ) + p ( a 2 ))  X  JS  X  1 , X  2 ( p ( X i | a where M is the cardinality of the multiset of ( k + 1) -grams. We have used that: p ( x i | a 1  X  a 2 ) =  X  1  X  p ( x i | a 2 ) , when a 1  X  a 2 =  X  . and #[ a j ,x i ] = p ( a M = p ( x i | a j )  X  p ( a j )  X  M = p ( x i | a j )  X   X  [1] P. Baldi and S. Brunak, Bioinformatics: the Machine [2] R. Durbin, S. R. Eddy, A. Krogh, and G. Mitchison, [3] E. Charniak, Statistical Language Learning, Cam-[4] R. Begleiter, R. El-Yaniv, and G. Yona,  X  X n predic-[5] J. Lin,  X  X ivergence measures based on the Shannon [6] G. Casella and R. L. Berger, Statistical Inference. [7] N. Slonim and N. Tishby,  X  X gglomerative information [8] J. Gardy,  X  X sort-b: improving protein subcellular local-[9] O. Emanuelsson, H. Nielsen, S. Brunak, and G. von [10] T. G. Dietterich,  X  X pproximate statistical tests for com-[11] E. Segal, D. Koller, and D. Ormoneit,  X  X robabilistic [12] A. K. McCallum, R. Rosenfeld, T. M. Mitchell, and [13] J. Zhang, D.-K. Kang, A. Silvescu, and V. Honavar, [14] M. desJardins, L. Getoor, and D. Koller,  X  X sing feature [15] L. D. Baker and A. K. McCallum,  X  X istributional [16] A. Silvescu, C. Caragea, and V. Honavar,  X  X ombining [17] G. Bejerano and G. Yona,  X  X odeling protein families [18] F. Jelinek and R. L. Mercer,  X  X nterpolated estimation of [19] T. M. Mitchell, Machine Learning . New York: [20] D. M. Blei, T. Griffiths, M. I. Jordan, and J. Tenen-[21] G. G. Langdon,  X  X  note on the zivlempel model for [22] J. Rissanen,  X  X  universal data compression system, X  [23] M. Nisenson, I. Yariv, R. El-Yaniv, and R. Meir, [24] F. M. J. Willems, Y. M. Shtarkov, and T. J. Tjalkens, [25] J. G. Cleary and I. H. Witten,  X  X ata compression using [26] D. Ron, Y. Singer, and N. Tishby,  X  X he power of
