 Recognizing textual entailment (Geffet and Da-gan, 2005; Androutsopoulos and Malakasiotis, 2009; Zanzotto et al., 2009; Berant et al., 2011) is an important task for many NLP applications, such as relation extraction (Romano et al., 2006) or question-answering (Harabagiu and Hickl, 2006). Text L entails text R if the information written in the latter can be deduced from the information written in the former. As building blocks to rec-ognize entailment relations between texts, numer-ous works have focused on recognizing entailment relations between patterns, such as  X  grew up in X  X   X   X  lived in X  X  or  X  X grew up in Y  X   X   X  X lived in Y  X  (Lin and Pantel, 2001; Weeds and Weir, 2003a; Hashimoto et al., 2009; Berant et al., 2011; Kloet-zer et al., 2013b).

We propose in this paper a method for ac-quiring on a very large-scale, entailment pairs of Table 1: Average precision for baseline method with various amounts of training data such class-dependent binary patterns as  X  under-went X exam on Y date  X   X   X  X exam carried out on Y date  X . Our starting point is a supervised baseline trained with 83,800 manually labeled pattern pairs detailed in Kloetzer et al. (2013b). Its top 205 mil-lion output pairs have an estimated 80% precision, but this baseline X  X  performance is saturated. Ta-ble 1 shows the baseline X  X  average precision when varying its amount of hand-labeled training data. Since the average precision only improves slightly with additional training data, the investment in hand-labeling additional training data is difficult to justify.

To improve our baseline further, we exploit the transitivity property of entailment to automatically generate new features for it. The entailment is transitive; if we detect that L entails C and C en-tails R, we can infer an entailment relation be-tween L and R even if no such relation was de-tected beforehand. Based on this idea, we pro-pose a self-training scheme that works in the fol-lowing way. For pattern pair  X  P , Q  X  , we use the baselines output to find all the chains of patterns from P to Q that are linked by entailment rela-tions, which we call transitivity paths , and encode the information related to them as new features to judge the validity of pair  X  P , Q  X  . Our expectation is that even if our supervised baseline fails to judge  X  P , Q  X  as an entailment pair, the existence of paths from P to Q that are comprised of pairs judged as entailments by our baseline might strongly suggest that P entails Q; hence, adding our new features to the baseline should help it make better decisions based on the information encoded in the features. This self-training approach is the first that encodes the information contained in transitivity paths as features for a classifier, and as such it differs from previous state-of-the-art methods that exploit tran-sitivity to extract new pairs using Integer Linear Programming (Berant et al., 2011) or that auto-generate training data (Kloetzer et al., 2013a).
From a corpus of 600 million web pages, we show that our proposed method extracted 217.8 million entailment pairs in Japanese with 80% pre-million pairs output by our baseline with identi-cal precision. It also extracted 138.1 million en-tailment pairs with 70% precision with non-trivial lexical substitution (generally deemed difficult to extract), which is a 50.4 million pair increase (57.5% size improvement) over the 87.7 million pairs output by our baseline with the same preci-sion. These include such pairs as  X  use X to dis-tribute Y  X   X   X  Y is available on X  X ,  X  underwent X on Y  X   X   X  X carried out on Y  X ,  X  start X at Y  X   X   X  Y X  X  X  X  or  X  attach X to Y  X   X   X  put X on Y  X . Even though we only present results for the Japanese language, we believe that our method should be applicable to other languages as well. This is because none of the few language dependent features of our classi-fier are strictly needed by the baseline or our pro-posed method, and its performance boost is unre-lated to these features. The task of recognizing entailment between texts has been proposed by Dagan et al. (2006) and in-tensively researched (Malakasiotis and Androut-sopoulos, 2007; Szpektor et al., 2004; Androut-sopoulos and Malakasiotis, 2009; Dagan et al., 2009; Hashimoto et al., 2009; Berant et al., 2011) using a various range of techniques, includ-ing Integer Linear Programming (Berant et al., 2011), machine learning with SVMs (Malakasio-tis and Androutsopoulos, 2007), and probabilis-tic models (Wang and Manning, 2010; Shnarch et al., 2011). Entailment recognizer or entail-ment data sets have been used in such fields as relation extraction (Romano et al., 2006) and question-answering (Harabagiu and Hickl, 2006; Tanaka et al., 2013). In this work, we are inter-ested into recognizing entailment between syntac-tic patterns, which can then be used as building blocks in a complete entailment recognition sys-tem (Shnarch et al., 2011). Recognizing entail-ment between patterns has generally been stud-ied using unsupervised techniques (Szpektor et al., 2004; Hashimoto et al., 2009; Weeds and Weir, 2003b), although we showed that supervised techniques naturally obtain stronger performance (Kloetzer et al., 2013b).

The two works that are most closely related to our work are Berant et al. (2011) and Kloetzer et al. (2013a), both of which exploit transitivity to improve the result of a baseline classifier. Berant et al. (2011) proposed an entailment recognition method for binary patterns that exploits Integer Linear Programming techniques (ILP) to expand the results of an SVM classifier. This method en-codes into an ILP problem an entailment graph, which is a valued graph where nodes and edges re-spectively represent patterns and their entailment relations, and the values equal the SVM classi-fiers score output. The problems variables E P Q  X  { 0 , 1 } indicate whether pattern pairs (here,  X  P , Q  X  ) have an entailment relation, and the goal is to max-imize the sum of the scores of the pairs selected as entailment relations { X  P, Q  X  X  E P Q = 1 } . In (Kloetzer et al., 2013a), we proposed a contradic-tion acquisition method that uses a training data expansion scheme; it automatically generates new contradictions by exploiting transitivity and adds the highest scoring contradictions based on a novel score (CDP) to the training data of the original classifier. The score is based on the assumption that if pair  X  P , Q  X  , when chained by transitivity to other pairs  X  Q , R i  X  , generally leads to correct en-tailment pairs  X  P , R i  X  , then all pairs  X  P , R i  X  should be correct entailment pairs. Although this work was designed for contradiction recognition, it is easily adapted to entailment. Target Pattern Pairs We extracted our binary patterns from the TSUBAKI corpus (Shinzato et al., 2008) of 600 million Japanese web pages. Bi-nary patterns are defined as sequences of words on the path of dependency relations connecting two nouns in a sentence and have two variables.  X  use Y to distribute X  X  and  X  X is available on Y  X  are such binary patterns. Like previous works (De Saeger et al., 2009; Berant et al., 2011; Kloetzer et al., 2013a), we pose restrictions on the noun-pairs that co-occur with each pattern using word classes to disambiguate their various potential meanings:  X  X We used the EM-based noun clustering algorithm presented inKazama and Torisawa (2008) to clas-sify one million nouns into 500 semantic classes. Our target set, to which we apply all of our classi-fiers, is set  X  of around 11 billion class-dependent pattern pairs for which both patterns share at least three co-occurring noun-pairs.
 Baseline Classifier Our baseline classifier ( BASE ) is an SVM classifier trained with about 83,800 binary pattern pairs that were hand-labeled as entailment (25,436 pairs, 30 . 4% of the total) or non-entailment (58,361 pairs). We trained polynomial kernel of degree 2.

Following previous work (Kloetzer et al., 2013b), we used three types of features in BASE : surface features indicate clues like the presence of n-grams or measure the string overlap between two patterns; database features exploit existing language resources; and distributional similar-ity scores measure the patterns X  semantic similar-ity based on the nouns that co-occur with them. See Kloetzer et al. (2013b) for more details about BASE s features. Our method consists of the following three steps: Step 1 Chain together the entailment pairs pro-Step 2 Train new classifiers with features that en-Step 3 Combine the output of these classifiers
Figure 1 shows an overview of our method, and we describe its details in the following sections. 4.1 Step 1: Transitivity Paths We generate chains of entailment pairs (or tran-sitivity paths) in the following way. First, we extract from the output of the baseline classifier BASE set E (  X  ) of the pattern pairs for which BASE returns a score over given threshold  X  : E (  X  ) = { X  P, Q  X   X   X  | S BASE ( P, Q )  X   X  } , where S
BASE ( P, Q ) is the score returned by BASE for pattern pair  X  P , Q  X  . The higher  X  is, the greater the precision of the pairs in E (  X  ) should be. Then by chaining the entailment pairs from E (  X  ) together, we build sets of transitivity paths composed of two entailment pairs T r (  X , 1) for  X   X  X  0 ,  X  X  X } and of three entailment pairs T r (  X , 2) for  X  = 0 . Since additional chaining is computationally expensive, we stopped at paths that consist of three pairs. 4.2 Step 2: Training New Classifiers In this step, we train new classifiers by adding new features to BASE . The training data, the classifier software, and the settings are the same as for BASE . For given pattern pair  X  P , R  X  , P ath ( P, R,  X , N ) is the set of all the transitivity paths in T r (  X , N ) that lead to pair  X  P , R  X  . We en-code the information contained in these paths in three new feature sets.

Before explaining these three new feature sets, we define three scoring functions for the transi-tivity paths to assess their quality; the MinScore of a path is the minimum of scores returned by BASE for each pair in the path, and ArScore and GeoScore are the arithmetic and geometric aver-ages of the scores returned by BASE for each pair in the path. Each of the three feature sets is com-puted for each of the three scoring functions, but we just mention MinScore in our explanations due to space limitations.
 Feature set 1: scores of top-ranked paths Here we select the top ten paths of P ath ( P, R,  X , N ) ranked by MinScore and use as features a new vec-tor that consists of the following values: (1) the MinScore of each path and (2) the scores returned by BASE for each of the pairs in the ten paths. When there are fewer than ten paths, the missing features are set to 0.
 Feature set 2: BASE features of the pairs in the highest ranking transitivity paths Here we se-lect the transitivity path of P ath ( P, R,  X , N ) with the highest MinScore and use the BASE feature values for each pair in the transitivity path as a new feature vector for pair  X  P , R  X  .
 Feature set 3: score distribution Given thresh-old  X  , we count the number of paths whose Min-Score exceeds  X  . By varying  X  from lower bound low to upper bound up , we derive vec-tor [ |{ p  X  P ath ( P, R,  X , N ) | M inScore ( p )  X  feature vector for pair  X  P , R  X  . We set  X  = 0 . 1 and low and up such that the score values returned by BASE are bounded by low and up . 4.3 Step 3: Optimization and Weighted Sum The final output of our method combines the out-puts of BASE and two new classifiers: (1) a classi-fier with new features computed with 1-step tran-sitivity paths ( N = 1 ), and (2) another with new features computed with up to 2-step transitivity paths ( N  X  { 1 , 2 } ). We then use a weighted sum and compute score S P ROP OSED ( P, Q ) = P sents the scores of the respective classifiers, and we set n 0 + n 1 + n 2 = 100 ( n i are all natural numbers).

For each potential combination of three weights n , we computed the average precision returned by our method on DEV , our development set, and selected for our final output the weight combina-tion that gave the best average precision on DEV . The final classifiers weights obtained in our ex-periments were 62 for BASE , 30 for the classifier with 1-step transitivity features, and 8 for the one with the 1-and 2-step transitivity features.
Using the same method, we also performed ab-lation tests to remove the features that harmed the classifiers and ensured that every proposed new feature set and every scoring function were useful.
Finally, we confirmed that using a weighted sum for our proposed method returned higher av-erage precision than Stacking (Wolpert, 1992), Figure 2: Precision curves for PROPOSED and baseline methods for non-similar pairs which is a more standard combination method, or than using the output of any of the new classifiers alone. In this section, we evaluate our proposed method in a large-scale setting and compare it to BASE and to state-of-the-art methods based on ILP (Be-rant et al., 2011) and automatic training data ex-pansion (Kloetzer et al., 2013a). We also indicate that our method shows the best performance gain for pattern pairs with non-trivial lexical substitu-tions, which are more difficult to acquire. Evaluation Method For our evaluation, we pre-pared test set TEST of 15,000 pattern pairs ran-domly sampled from  X  (our target set of 11 bil-lion pairs). The pairs were annotated by three hu-mans (not the authors) who voted to settle labeling discrepancies. We also prepared development set DEV of 5 , 000 pattern pairs from  X  for tuning our method. The Kappa score was 0.55 for the anno-tation of these two sets.

We measured the performance of each method by computing its average precision (Manning and Sch  X  utze, 1999) on the TEST set. We used the av-erage precision instead of the traditional F-value because the latters value greatly varies depend-ing on where the classification boundary is drawn, even for similar rankings. We also drew precision curves for each method using the same TEST set. Proposed Methods Performance We first show the performance of PROPOSED (our proposed method) and BASE (the baseline classifier). As another baseline, we consider BASE + DEV where the 5,000 samples of the DEV set were added to the BASE training data. We show the average pre-cision for each of these three classifiers and the Table 2: Average precision and entailment pairs obtained (in millions) for proposed method, base-line classifiers, and state-of-the-art methods PROPOSED 78.73% 39.53% BASE + DEV 77.72% 37.24% Table 3: Average precision for similar and non-similar pairs number of pairs obtained at 80% precision in Ta-ble 2. We also show the performance of these classifiers over similar pattern pairs (both patterns share a content word) and non-similar pairs (they do not share a content word) in Table 3.

As mentioned in the introduction, BASE + DEV shows that the addition of 5,000 hand-labeled sam-ples to the training data of BASE (a 6% increase) only improves the average precision performance by 0.17%. Our proposed method, on the other hand, exploits the same 5,000 new annotated sam-ples for tuning its parameters and obtains a 1 . 85% gain of average precision. Using PROPOSED , we acquired 217.8 million pattern pairs with 80% pre-cision, an improvement of 6 . 0% over BASE .
As shown in Table 3, BASE s performance is much lower for non-similar pairs like  X  use Y to distribute X  X   X   X  X is available on Y  X , which have non-trivial lexical substitutions and are more dif-ficult to acquire than similar pairs. This is also where PROPOSED obtains the biggest gain in performance: an average precision of 39 . 53 com-pared to 36 . 98 for BASE . We show the preci-sion curves we obtained when ranking the non-similar pairs with BASE and PROPOSED in Fig. 2. PROPOSED acquired 138.1 million non-similar pairs at 70% precision, which is an in-crease of 50.4 million pairs (a 57 . 5% size im-provement) compared to BASE with the same pre-cision. We believe that the strong performance of BASE for similar entailment pairs helped it dis-cover, through transitivity, the variations of non-similar entailment pairs it could already detect. Comparison to State-of-the-art Methods We also compared PROPOSED with two state-of-the-art methods that exploit transitivity: the ILP-based method of Berant et al. (2011) ( ILP ) and the training data expansion method we proposed in Kloetzer et al. (2013a) ( CDP ). The latter, which was initially designed for acquiring contradiction pairs, was adapted to acquire entailment for com-parison purposes. The results of this compari-son are summarized in Table 2, and the precision curves for these two methods as well over non-similar pairs are shown in Fig. 2. Our proposed method is the only one that provides stable im-provement in our large-scale setting; at best, the other two just slightly outperform BASE . We be-lieve that our feature encoding provides more in-formation to the classifier than the raw scores in the transitivity paths that are exploited by the other state-of-the-art methods, and as such strengthens the performance.

As for explaining the poor performance of the state-of-the-art methods, ILP is unfortunately not tractable for big problems; our ILP solver failed to solve 82% of the independent problems we fed it due to insufficient memory even on 64-Gb mem-ory machines, making ILP just slightly better than BASE . Our pattern graph is also sparser than that in Berant et al. (2011), and as such ILP might not be completely efficient. But we assume that even if we had used the graphs whole closure, the ILP problem instances would have become even less tractable, resulting in performance that only slightly exceeds BASE . As for CDP , since our baseline classifier already has more than 80,000 hand-annotated samples as training data, the addi-tion of automatically generated training samples is actually harmful. In this work, we proposed a method that exploits the transitivity relation of entailment in a self-training scheme and combines classifiers with a weighted sum. In our large-scale setting, our method outperforms state-of-the-art methods that are also based on a transitivity approach, includ-ing an ILP-based method. Using our proposed method, we acquired 217.8 million Japanese en-tailment pairs with 80% precision and 138.1 mil-lion non-trivial pairs with 70% precision. We are considering an extrinsic evaluation for these data such as the RTE test in future research.
