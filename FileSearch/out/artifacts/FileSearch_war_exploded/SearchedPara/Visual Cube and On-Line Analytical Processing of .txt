 On-Line Analytical Processing (OLAP) has shown great suc-cess in many industry applications, including sales, market-ing, management, financial data analysis, etc. In this paper, we propose Visual Cube and multi-dimensional OLAP of image collections, such as web images indexed in search en-gines (e.g., Google and Bing), product images (e.g. Amazon) and photos shared on social networks (e.g., Facebook and Flickr). It provides online responses to user requests with summarized statistics of image information and handles rich semantics related to image visual features. A clustering struc-ture measure is proposed to help users freely navigate and explore images. E ffi cient algorithms are developed to con-struct Visual Cube. In addition, we introduce the new issue of Cell Overlapping in data cube and present e ffi cient solutions for Visual Cube computation and OLAP operations. Exten-sive experiments are conducted and the results show good performance of our algorithms.
 H.2.8 [ Information Systems ]: Data mining; Image databases; H.3.5 [ Information Storage and Retrieval ]: On-line Informa-tion Services Algorithms, Experimentation OLAP, data cube, image management
With the construction of numerous large data warehouses, the industry witnesses a surge of demands of On-Line Ana-lytical Processing (OLAP) in multi-dimensional way [6]. By o ff ering users the ability to access data collections of any dimension subsets, an OLAP system provides the ease and flexibility for navigating data and summarizing statistics at di ff erent granularity levels and from di ff erent angles. OLAP systems have shown great success in many applications, in-cluding sales, marketing, management, and financial data analysis.

The importance of OLAP for image analysis has been rec-ognized in applications such as remote sensing image anal-ysis [13] and raster image analysis [1]. We believe image OLAP can also be used in image search engines, social net-works and product e-commercial websites to support e ffi cient multi-dimensional online analysis of images.

Data cube [5] is the workhorse for OLAP. Because of the dis-tinguished nature of visual information, the data cube model for images should be di ff erent from the existing data cube models, such as [11] [7] [10] [2]. To design a good image data cube model and support e ffi cient OLAP of images, there are four fundamental issues that need to be addressed: (1) design useful cube dimensions to support multi-dimensional organization of the images; (2) design useful measures to sup-port user analysis; (3) e ffi ciently construct data cube; and (4) design e ffi cient OLAP operations.

We provide comprehensive and e ffi cient solutions for im-age data cube regarding all the above four fundamental is-sues. We propose a general model of Visual Cube for OLAP of images. Visual Cube not only summarizes statistics infor-mation, but also helps users navigate and analyze the images e ffi ciently.

Related studies [19] [13] partially deal with the image di-mension issue in a traditional data cube way. However, there are open problems if considering the special properties of images. For example, how to design dimensions on im-age tags or major colors? The challenge is that an image can have more than one tag or major color, this is di ff er-ent from traditional data cube where a record only has one unique value for a dimension. We introduce two types of schemes, MDS (Multi-Dimension Scheme) and SDS (Single-Dimension Scheme), and will show that SDS is the practical scheme to build dimensions on such information for large image datasets. However, SDS scheme introduces the Cell Overlapping phenomenon. We will analyze the impact of overlapping and propose e ffi cient solutions in Visual Cube computation. We also introduce new OLAP operations con-sidering the overlapping and develop e ffi cient algorithm to handle it.

The dimensions, such as Time, Location and Tags, in Visual Cube provide a good way of image organization. However, they are not enough for navigation and analysis of large image data, because many images could share the same dimension value. For a query like Q : ("Year = 2006"), there could be over thousands of images taken in that year. Simply return all those large number of images without a good organization or summarization would be too overwhelming.

To solve this problem, we introduce clustering structure as a measure for Visual Cube. The idea is to perform content-based image clustering (based on image features such as color / texture / shape) to group visually similar images together, and then choose central images from the clusters as the rep-resentatives shown to the user. When interested in any rep-resentative image, the user can click on it and explore into related similar images. As addressed in [3] [17], clustering technique is a natural fit for exploring and describing large image repositories. We also propose other types of measures for Visual Cube and provide a general categorization.
Based on the Visual Cube model, we study how to compute the cube e ffi ciently. For the clustering structure measure, a straightforward algorithm is to aggregate image ids at the cells and perform clustering for a cell based on the content features of all the images in the cell. In this approach, the clustering of a cell is performed independently of other cells.
We also propose more e ffi cient methods. The basic idea is to utilize the already computed clustering structures of the lower level cells to help the clustering of images in a higher level cell. For example: Given two lower level cells c 1 (Tag = "tree", Year = "2006") and c 2 (Tag = "tree", Year = "2007"), and the higher level cell c 3 (Tag = "tree", Year = * ), where * means cell c 3 is aggregated on the Year dimension. (Refer to Fig. 2 for a real similar example in our system.) Suppose clustering are performed in cells c 1 and c 2 , we want to use the clustering structures to help e ffi ciently perform clustering in the higher level cell c 3 .

The motivation is that since the images in the higher level cell is a combination of the images in the lower level cells, if images are similar and grouped together by clustering in a lower level cell, they will have high possibility to be also clus-tered together in the higher level cell. We assume the images in di ff erent cells are clustered based on the same type of image feature. So we can treat any set of clustered images as a meta-point, and directly aggregate them to a higher level without doing clustering on the individual images again. In this way, the clustering on higher level cells will be performed based on meta points, thus will be much faster than performed on the individual images.

The above method is a naive approach to achieve better e ffi ciency, but may degrade the clustering quality, we will provide deeper analysis and present clustering aggregation algorithms to achieve good balance on speed and clustering quality. Consideration and solution on the Overlapping case will be also presented. In addition, dynamic aggregation selection will be proposed in this paper to select the best lower level cells for aggregation and improve the performance.
To support multiple features, such as color, texture and shape, in Visual Cube, we can save di ff erent clustering struc-tures at a cell, each for color, texture and shape, respectively. When aggregating from lower level cells to a higher level cell, we perform three clustering aggregations, each based on the clustering structures of the same image feature type. In this way, the system provides user with the flexibility of choos-ing his / her interested features to navigate and analyze the images.

Based on Visual Cube, we can support e ffi cient OLAP on images, as shown in Figure 1. The dimensions include Time (Year, Month, Day), GPS locations (two levels of latitude and longitude), Tag and Color. The measures are Count, and the representative images with one from each cluster. By clicking on any image, we can further explore into related similar images.
 Figure 2 shows two example OLAP operations, Roll-up and Drill-down, on the Lattitude (Lat) dimension. The images in the cells are arranged in a clustered manner by clustering (here we show all images instead of representative images). Figure 2: OLAP operations on the Lattitude (Lat) Dimen-sion.
 Figure 3 shows the system framework for Visual Cube and OLAP for images.

This paper is organized as follows: Section 2 outlines the design of Visual Cube. Section 3 presents Visual Cube con-struction algorithms. Section 4 discusses Visual OLAP op-erations. Section 5 presents experimental results. Section 6 concludes this paper.
Given a set of images, we extract their meta information (such as title, date, GPS location, tags and url) and visual features (such as color / texture / shape), from which the raw image database will be built. Table 1 shows an example image database with extracted color histogram H i  X  R d .
Id Date GPS Tags ColorHist 1 9 / 10 / 2007 (31.1, 78.5) sunset H 1 2 2 / 5 / 2007 (45 . 2, 28.9) girl H 2 3 4 / 8 / 2007 (35.7, 128.9) girl, sunset H 3 4 6 / 15 / 2008 (  X  78 . 5, 135.4) sunset H 4 5 10 / 1 / 2008 (  X  68 . 5, 35.1) sunset H 5
Based on the raw image database, given a multidimen-sional data model with indicated dimensions A i , we can build Visual Cube. A cell c is represented as ( a 1 , a 2 ,..., a n is the number of dimensions and M is the measure. When a  X  X  X  X  , the cell is aggregated on dimension i .

Without losing generality, take a 4-D cube as an example, with dimensions A, B, C, D. Suppose we aggregate cuboid AB from ABC, then we call AB as the target cuboid , and ABC the supporting cuboid . A cell in the target cuboid is a target cell . A cell in the supporting cuboid is a supporting cell . A target cuboid could have more than one candidate supporting cuboid . For example, AB can be aggregated from either ABC or ABD. Two types of dimensions are built for Visual Cube: (1) Meta Information Dimensions, such as date, title, file name, owner, URL, tag, description, interestingness, license and GPS lo-cation; and (2) Visual Dimensions (based on image visual features), such as image size, major colors, face dimension (indicating the existence of faces), color / texture histogram.
The tag or major colors dimension is especially challenging for Visual Cube, because an image may have multiple tags or major colors. We introduce two schemes to build such type of dimension: Multiple Dimension Scheme (MDS) and Single Dimension Scheme (SDS).

Table 2 shows cuboids built from the example image database in Table 1 using the MDS scheme (a) and the SDS scheme (b), respectively. In the MDS scheme, the cuboid has three di-mensions: Year, Sunset and Girl; while in the SDS scheme, the cuboid has only two dimensions: Year and Tag. The two cuboids share the same information, but the latter is more compact. Table 2 (c) shows the 1-D cuboid aggregated to the Year dimension.
 (b) SDS scheme, a 2-D cuboid
The MDS scheme treats each tag as one dimension, with value 0 or 1, indicating whether the tag is annotated to the images in the cell. There is only one level available for each dimension. In this scheme, no overlapping exists between cells, i.e., no image belongs to more than one cell.
The disadvantage of the MDS scheme is that since the over-all number of unique tags in the image collection is usually large (over tens of thousands), there would be too many di-mensions. We know the number of cuboids | B | is at least expo-where | P | is the number of non-tag dimensions, and | T | is the Visual Cube using this scheme when there are many tags.
To solve the problem of MDS, the SDS scheme builds a single dimension for all the tags. The tag hierarchy could be manually or automatically built in this scheme. The ad-vantage of SDS is that there is only one tag dimension, so than the MDS scheme.

Overlapping OLAP . We have shown that the SDS scheme is significantly more compact than MDS and is the practical choice for building Visual Cube with many tags. However, it introduces a problem which we call Cell Overlapping . If an image has multiple tags, i.e., multiple values in the tag dimension, it will appear in multiple cells. For example, in Table 2 (b), image 3 belongs to both the first and second cells. We call the dimension which causes cell overlapping as Overlapping Dimension . Examples include tag, color (an im-age may have several major colors), topic (in text / topic cube, a document may belong to several topics), etc. We call an OLAP system containing overlapping dimensions Overlap-ping OLAP .

In Overlapping OLAP, even the simplest measure Count can not be directly aggregated from any lower level cuboid on the overlapping dimension. For example, if we directly add the Count of the first and second cell in cuboid from Table 2 (b), the Count for cell "Year = 2007" of the aggregated 1-D cuboid in Table 2 (c) will be 4 which is not correct. We introduce four categories of measures for Visual Cube. The first category is similar to the measures used in traditional data cube, however, the other three categories are unique for Visual Cube. (1) Summarized information: count (number of images in a cell), max / mean / min / std rating, etc; (2) Summa-rized image feature: mean image, average color histogram, major colors, etc; (3) Subset of the images: top-k representa-tive images (by clustering images based the image features (such as color histogram) and choose the central images as representatives), etc; and (4) All the images in the cell, but in some kind of organization, such as a ranked list (like in image search engines) and clustering structure, which is computed from the image features and provides user better overview of the images.

We define the clustering structure G c of cell c to consist of the cluster centers and a set of member image ids for each cluster. An image can be represented as a feature point p  X  R d . By employing the clustering structure as a measure, Visual Cube enables users to not only navigate the images freely, but also find interesting phenomena from image groups.

We define the clustering quality of cell c as the average distance between points to its cluster center, i.e., Q ( c ) = P the cell. The overall clustering quality of cube B is Q ( B ) = P all its cells. To avoid large size cells dominating the cube increasing. Given a certain number of clusters, smaller value of Q means the images in the same cluster are more similar to each other, which indicates better clustering quality.
Semi-Holistic Measure: In traditional data cube, there are three standard types of measures: distributive , algebraic and holistic . A holistic measure is hard to aggregate because it needs to access all the corresponding records in the raw database to compute the value. We introduce a new type of measure for Visual Cube: semi-holistic , which only needs to access part of the corresponding records in the raw database to compute the value. We will show in this section how semi-holistic works and why it is more e ffi cient than holistic . Aggregate Measures in Overlapping OLAP. We use Rid-Set (the set of record ids of the cell. RidS for short) to help the aggregation of non-distributive measures. RidSet is distributive . To aggregate it, the basic operation involved is Union of the sets. We discuss two examples to demonstrate its usage in computing other types of measures. (1) Count . In traditional OLAP, this measure is distributive and can be aggregated directly by adding up the counts from supporting cells. However, in overlapping OLAP, this mea-sure becomes Algebraic . We need to first aggregate RidSet, and then use its size as count. (2) Mean : There are two ways to aggregate this kind of measure: holistic and semi-holistic . The holistic method works as follows: (1) compute the union of the RidSet X  X  of sup-porting cells sequentially U = of supporting cells; (2) access values in U ; and (3) compute mean as Mean agg =
The semi-holistic method works as follows: (1) aggregate count; (2) compute the intersection of the RidSet X  X  of the sup-porting cells sequentially I = an id repeats, so ( repeat i  X  1) is duplicate number; (3) access original values of records in I ; and (4) compute aggregated mean using the following formula:
Table 3 summarizes each measure X  X  aggregation property in traditional (non-overlapping) data cube and overlapping data cube.

Time and Space E ffi ciency . Instead of literally saving the set of ids for RidSet, we can save it in a compressed form by techniques such as  X  -code. For time e ffi ciency considera-tion, we can save the RidSet as bitmap , thus the set compu-tation can be e ffi ciently performed by bitmap operations. To save space, bitmap can be compressed by techniques such as Word-Aligned Hybrid (WAH) [18].
 Table 3: Aggregation type of measures in traditional OLAP (T-OLAP) and Overlapping OLAP (O-OLAP).

We focus on full materialization for Visual Cube construc-tion since it enables the fastest OLAP operations and sets the algorithmic foundation for partial materialization. Partial materialization helps reducing storage especially when the cube is in high dimension, and we leave this for future work.
We propose a general framework for Visual Cube construc-tion as follows: (1) based on the raw image database, build the base cuboid. For multi-value attributes, such as tags and major colors , adopt the SDS scheme and expand all combi-nations of the values to make sure each cell is unique; (2) calculate measures for each base cell; and (3) perform aggre-gation selection and aggregate measures for cells in higher level cuboids. We choose bottom-up approach (i.e., from base cells up) and leave the top-down approach for future work.

Algorithm 1 shows the framework for the Visual Cube con-struction with clustering structure measure (other measures can also be computed in this framework). The popular k -Means [16] algorithm is used for the basic clustering. Many other fast clustering algorithms, such as GAD [8], are also ap-plicable. If the user is interested in soft partitioning, we can use EM algorithm and aggregate high possibility assignments as hints to help clustering on higher level cells.
We propose the idea of dynamic aggregation selection in this section. Aggregation selection answers this question of Algorithm 1 Visual Cube Construction Framework Input: IDB, the image database Output: A Visual Cube with clustering measure 1. build base cuboid from IDB 2. for each cell c in the base cuboid do 3. if Count c &gt; k do clustering( c ) 4. end for 5. for each cuboid b at higher level do 6. for each cell c in b do 7. b 0 = AggregationSelection ( c ) 8. S : = the supporting cells from b 0 for c 9. G c = AggregationClustering( S ) 10. end for 11. end for choosing the best candidate supporting cuboid for aggrega-tion. For example, if the cells in cuboid AB can be aggregated from both ABC and ABD , which one to choose to get better performance?
Traditional data cube computation algorithms, such as Mul-tiWay [20], adopt static aggregation selection scheme which follows a global order based on decreasing dimension car-dinality. More specifically, the candidate supporting cuboid whose aggregation dimension has minimum cardinality is selected for a target cuboid. This is widely used in traditional data cube computation. However, in Visual Cube, especially for the computation of the clustering measure, the aggrega-tion selection problem becomes complex. Because di ff erent choices may result in di ff erent computation time and quality.
In the static method, cells in the same cuboid always select the same cuboid to aggregate from. However, this is not the optimal scheme, because di ff erent cells in the same cuboid may have the best set of supporting cells from di ff erent can-didate cuboids.

Based on the above observation, we go deep into the cell level, and dynamically select the best supporting cuboid for each individual cell. We calculate a selection score SS ( b , c ) of a supporting cuboid b w.r.t the target cell c , and choose the one with the best score. There are several factors: C (the cardinal-ity of the aggregation dimension), S (number of supporting cells in b for c ), Q (the overall clustering structure quality of the supporting cells) and O (overlapping property of the sup-porting cells). We use two schemes DynCN ( SS ( b , c ) =  X  S ) and DynCQ ( SS ( b , c ) =  X  Q ) to demonstrate the advantage of dynamic aggregation selection for Visual Cube.

For the dynamic schemes, the cells in the same targeting cuboid may choose di ff erent supporting cuboids for aggrega-tion. The advantage is that it X  X  able to select the best support-ing cuboid for each individual target cell. Fig. 4 shows the di ff erence between static and dynamic aggregation selection.
In order to compute clustering structure for the cells, a naive aggregation approach is to perform clustering indepen-dently at each cell, which we call Independent Aggregation ( IA ). It works as follows: For each target cell, aggregate Rid-Set from the supporting cells, access the corresponding image features based on the RidSet and perform clustering based on the image features.

In the IA algorithm, the clustering structures of the sup-porting cells are ignored when computing the target cell. The disadvantage is that it is time consuming to access and com-pute the high dimensional image feature of all the images for all the cells. For OLAP query, if the target cell is not materi-alized, this approach cannot provide real-time response.
In the following sections, we present more e ffi cient cube ag-gregation algorithms with di ff erent trade-o ff s between speed and clustering quality. The general idea is to utilize the ex-isting clustering structures of the supporting cells to improve the e ffi ciency for data cube computation at a target cell. This is related to distributive computing [12].
A meta-point (or meta-cluster) is defined as a center or cen-troid point that represents a set of similar individual points. We introduce the Meta-Point aggregation ( MP ) algorithm. The basic idea is to directly use clusters from supporting cells as meta-points to be clustered in the target cell.

MP works as follows: Beginning with the base cuboid, perform clustering for each base cell. Each cluster in the base cells forms a meta-point. To calculate the clustering structure of any higher level cell, we first aggregate the meta-points from the supporting cells of the selected supporting cuboid using union operation, and then perform clustering on the meta-points. Points in the same meta-cluster are assigned to the same center at the higher level cell. Overlapping removal is finally performed. Algorithm 2 describes the aggregation part of the MP algorithm. Figure 5 shows an example. Figure 5: Meta-Point Aggregation. Aggregated from three supporting cells (red, green and yellow, respectively). k = 2 for the target and supporting cells. Blue circles are the aggregated clusters for the target cell.

Overlapping Removal . Fig. 6 shows an overlapping situ-ation. Point P 1 appears in both meta-point M 1 and M 2 , which are assigned to C 1 and C 2 , respectively. In order to remove duplicates, we need to assign the point only to the best match cluster. Algorithm 2 Meta-Point Aggregation (MP) Input: S , the set of supporting cells Output: tc , the target cell 1. M =  X  // initialize meta points M 2. for each supporting cell s in S do 3. if s has clustering structure then 4. for each cluster g of s do M 5. else 6. for each member point p of s do M 7. end for 8. G M = clustering( M ) 9. for each cluster g i of G M do 10. G tc ( i ) . members =  X  11. for each member point m of cluster g i do 12. if m is a meta-point then 13. G tc ( i ) . members 14. else G tc ( i ) . members 15. end for 16. end for 17. tc = removeOverlapping( tc )
To remove overlapping e ffi ciently, instead of checking each pair of clusters to find overlappings, we perform fast linear complexity checking by sequentially scanning the clusters only once and accumulating the cluster ids for each point of the cell. Then, if a point belongs to more than one cluster, decide the best match cluster and assign to it; otherwise, stick to its original cluster. Algorithm 3 describes the overlapping removal procedure.
 Algorithm 3 Remove Overlapping Input: c , the target cell Output: c 0 , the cell with overlapping removed 1. initialize the candidate clusters of each point p in c as 2. for each cluster g in c do 3. for each point p in g do p . clusters 4. end for 5. for each point p in c do 6. if | p . clusters | &gt; 1 then 7. i : = the best match cluster among p . clusters 8. assign p to cluster i 9. else 10. keep p  X  X  original cluster assignment 11. end for
There are two methods to decide the best match cluster for an overlapping point. One method accesses the point X  X  original feature and calculates its distances to all the related clusters to find the nearest one. The other method directly uses the distance of the meta-point to its cluster center, and chooses the minimum distance center. The latter approach is more e ffi cient because it does not access the original fea-ture and no new distance calculation is involved, but it may degrade the clustering quality because it cannot guarantee always finding the correct the nearest center.

Time e ffi ciency of MP . The MP algorithm is the most time e ffi cient, because it solely depends on the already computed clustering structure from the supporting cells and avoids ac-cessing the original image features.

Clustering quality of MP . MP performs clustering on meta-points instead of individual points. It assumes that all the member individual points of a meta-point are closest to the center of the cluster which the meta-point belongs to. How-ever, the assumption does not hold in some real applications and results in low clustering quality. Fig. 7 shows an exam-ple. C 1 and C 2 are the centers of two clusters, 1 and 2, respec-tively. M 1 is the center of a meta-point, P 1 is a member point of the meta-cluster. Let D () denotes the distance function, assigned to cluster 2. However, when we decluster the meta-point and reveal point P 1 , we get D ( P 1 , C 1 ) &lt; D ( P point should be assigned to cluster 1 instead of cluster 2. Figure 7: Situation when cluster assignment conflicts for meta-point and its member individual point.
To improve the quality of MP, we introduce Partial Declus-tering aggregation ( PD ). The idea is to find boundary meta-points to decluster and release the individual points associ-ated with them. For each released individual point, calculate its distance to each cluster center, find the closed center and assign to it. In this way, we can find the true nearest cen-ter of such boundary points and thus improve the clustering quality.

A boundary meta-point locates at boundary areas and has high chance to contain many individual points which should be re-assigned to another cluster. We define a criteria to find boundary meta-points for declustering.
 Take 2-D feature space as an example, as shown in Fig. 8. Assume that a meta point M 1 lies between its nearest cluster center C 1 and its second nearest cluster center C 2 . Build a Cartesian coordinate system in the following way: set C 1 the origin, set the line which goes through the origin to C as the X -axis. Given the radius r of the meta-point, distance d = D ( M 1 , C 1 ) and distance d 2 = D ( M 1 , C 2 ), we calculate m as the middle of C 1 and C 2 , E as to what extent the meta-point goes beyond the middle: Criteria for de-clustering:
Parameter  X  is an approximate estimation for how many percent of member individual points go beyond the middle line, in which case the de-clustering is needed.

Algorithm 4 presents the partial-declustering aggregation procedure.
 Algorithm 4 Partial-Declustering Aggregation (PD) Input: S , set of supporting cells Output: tc , target cell 1. Same as step 1 to 13 of Algorithm 2 2. for each cluster g i of G M do 3. G tc ( i ) . members =  X  4. for each member point m of cluster g i do 5. if m is a meta-point then 6. calculate r , d 1 , d 2 7. if m satisfies the declustering criteria then 8. for each individual point p of m do 9. j = NearestCenter( p , G M ) 10. G tc ( j ) . members 11. else G tc ( i ) . members 12. else G tc ( i ) . members 13. end for 14. end for 15. tc = removeOverlapping( tc )
PD gets higher clustering quality than MD by decluster-ing some hard clusters, with the expense of spending more computation on those released individual points.
Instead of selecting some meta-clusters to decluster, we can perform Full-Declustering aggregation ( FD ) which de-clusters all meta-points to guarantee every individual point will be correctly re-assigned to its true nearest cluster, thus achieving higher clustering quality than PD, with the ex-pense of possibly more computation time because more meta-clusters are declustered to be analyzed.

To achieve even better quality, we can allow the points to be reassigned according to updated centers and reiterate the pro-cedure. We called this extension as FD with Re-assignment ( FDR ). Parameter r controls the number of reassignment it-erations.
Based on the Visual Cube, OLAP operations, such as Drill-down, Roll-up and Slice, can be e ffi ciently supported. We also introduce a new type of operation regarding the overlapping phenomenon in Visual OLAP and give an e ffi cient solution.
Take Slice as an example. It performs a selection on one di-mension of the cube. If the selection is on multiple values, (for example, the user is interested in both 2009 and 2010 on the Year dimension), for non-overlapping OLAP, only OR opera-tion can be performed, while AND operation is not discussed in the literature because it always generate NULL since no record exists in multiple cells. However, for overlapping OLAP where a record can exists in multiple cells, the AND operation is meaningful, an example query is: (Tag = "sunset" AND Tag = "girl"). OR operation is the Union aggregation of the selected cells, while AND operation is the Intersection ag-gregation of the selected cells. For the clustering structure measure, OR operation can be supported by any of the clus-tering aggregation algorithms proposed in this paper.
We call intersection aggregation of cells within the same cuboid on the same subset of dimensions as IntraSect oper-ation. A basic algorithm for such operation is as follows: (1) detect related records by intersection of the RidSet X  X  of the selected cells; (2) perform Drill-through to the raw database to access the original features of those records; and (3) do image clustering based on the content features.

The above algorithm is not e ffi cient to support online query when the related images are in large number: firstly, it re-quires accessing the raw database which takes time; secondly, it performs clustering on original returned images and the features, the number of images could be large and the fea-tures are usually in high dimension. So the procedure cannot be finished in short time.

To deal with the above problem, we can design method to directly utilize the existing clustering structures of the cells and perform clustering on reduced size without considering all the original features accessed from the raw database.
This section presents extensive experimental evaluation for the Visual Cube computation algorithms. Experiments were conducted on a PC with a Intel Pentium(R) D 3.4GHz CPU and 4GB RAM, running Windows XP.
Our dataset consists of over 114,000 Flickr [4] images down-loaded via using Flickr API. We extract information such as the time, tags, GPS locations, and image features to build up the raw database, and compute the Visual Cube. We extract 75-dimension LAB features [9] and use Principle Component Analysis (PCA) [14] to reduce the dimension size to 30. Note that our framework is flexible to other image features.
For time performance evaluation, we use the Speedup of algorithm A over baseline B : Speedup ( A , B ) = T B / T T A is the execution time of using A to compute the Visual Cube and T B is the execution time of using B . For clustering structure quality evaluation, we use Quality Ratio (QR): QR ( A , B ) = Q B / Q A , where Q B is the quality of the cube computed by the baseline algorithm B . Since the value of Q is the smaller the better, if QR &gt; 1, algorithm A gets better clustering quality cube than the baseline algorithm B .
We compare the performances of Visual Cube construc-tion algorithms, including the aggregation selection schemes (Static, DynCN and DynCQ) and clustering measure ag-gregation algorithms: the baseline Independent Aggregation (IA), Meta-Point aggregation (MP), Partial-Declustering ag-gregation (PD), Full-Declustring aggregation (FD) and Full-Declustering with Re-arrangement (FDR).
Fig. 9 shows the performance of aggregation selection schemes: Static, DynCN and DynCQ. The Speedup and QR are calculated using Static as the baseline. We set n = 20000 , d = 9 , k = 30, and the results are generally similar for other set-tings. We use MP as the clustering aggregation algorithm. Figure 9: Performance of Aggregation Selection Schemes. Y -axis denotes the measure.

The result shows that compared to the traditional static method, the dynamic aggregation approaches DynCN and DynCQ get better performance in time and quality. DynCN achieves a 2.62 times Speedup over Static. The reason is that for any target cell, DynCN is able to always find the mini-mum number of supporting cells for aggregation. Usually, a smaller number of supporting cells result in lower time com-plexity. DynCQ achieves better quality than Static, because it is able to always choose the best quality supporting cells for computing the target cell.

We use DynCN as the default aggregation selection scheme for all other experiments to make sure they are in the same setting for comparison of other aspects.
Fig. 10 shows the performances of the clustering structure aggregation algorithms: MP, PD, FD and FDR. The parame-ter r of FDR varies from 1 to 5. The evaluation metrics are computed using IA as the baseline. To prove the usefulness of doing clustering, we also compare with Random Partition (RP).

The result shows that the clustering aggregation based algorithms get much better time performance than IA. MP achieves the highest speedup but lower quality because its computation is soley based on meta-points. The de-clustering idea makes the quality better and still get very high speedup. RP shows very poor quality compared with baseline, and Figure 10: Performances of the aggregation algorithms. Y -axis denotes the performance measure. X -axis denoted the algorithms. ( n = 10000 , d = 6 , k = 40 ) proves that there does exist hidden clustering structure with similar images in Visual Cube, and performing clustering (neither directly or aggregated) can find such similar images. PD gets better quality(improve to be over 96% approxima-tion) than MP by de-clustering some boundary meta-points to make the cluster assignment for the individual points more accurate, while FD further improve the quality (to be over 98%) by de-clustering all meta-points. Finally, by performing several re-assignments, FDR achieves the highest quality.
For the FDR algorithm, with the increase of r , the computa-tion time increases, but the clustering quality also increases. When r = 5, it can achieve almost the same quality as the base-line. We set r = 5 as the default value for the FDR algorithm for other experiments.
Fig. 11 shows the performances of aggregation algorithms w.r.t n (the number of records) which varies from 1000 to 110,000. The baseline algorithm is IA. The four aggregation algorithms are much faster than IA. The best speedup is 94 times faster by MP at n = 80 , 000 . With the increase of n , the speedup also increases, the reason is that the algorithms are initially based on the meta-points instead of individual points, with a given number of clusters, the meta-points will tend to represent more individual points and thus reduce the computation complexity. For the same reason, the quality tends to become lower but the decrease is very slightly since in most cases a meta-point is a good representative for its individual member points.
Fig. 12 shows the performances of the aggregation algo-rithms w.r.t d (the number of dimensions) which varies from Figure 11: Performances of aggregation algorithms w.r.t n , the number of records ( X -axis). Y -axis denotes the perfor-mance metric. ( d = 6 , k = 20 ) 3 to 9. The baseline algorithm is IA. The four aggregation algorithms always beat IA in time performance. The best performance of 57 times speedup is achieved by MP when d = 7. When the d is smaller, the number of cells which per-form aggregation based on meta-points becomes also smaller among all cells, so the speedup is not very high (but still over 5 times); however, correspondingly, the overall quality becomes very high since more computations are performed directly on the individual points.

For 8 and 9 dimensions, the time performances of the al-gorithms drop (but still several times faster than baseline). There are two reasons: firstly, since we fix the number of records, with large dimensions, many cells are small and thus the advantage of using meta-point instead of individ-ual point becomes relatively less significant; secondly, the last two dimensions are overlapping dimensions, some extra computations are needed for removing overlapping.
Fig. 13 shows the performances of the aggregation algo-rithms w.r.t k (the number of clusters) which varies from 10 to 100. The baseline algorithm is IA. No matter what number of clusters to perform, the four algorithms always beat the baseline in speed. With the increase of k , all the four algo-rithms (MP, PD, FD and FDR) tend to increase the quality (the FDR algorithm can even outperform the baseline). The rea-son is that larger k means more meta-points, which results in finer summarization of the individual points, thus achieves better quality. Correspondingly, the speedup decreases be-cause on average each meta-point will represent less number of individual points.
Throughout all the above experiments, our clustering ag-Figure 12: Performances of algorithms w.r.t d , the number of dimensions ( X -axis). Y -axis denotes the performance metric. ( n = 10000 , k = 20 ) Figure 13: Performances of the algorithms w.r.t k , the num-ber of clusters ( X -axis). Y -axis denotes the evaluation met-ric. ( n = 10000 , d = 6 ) gregation algorithms show much better performance than the non-aggregation baseline. MP gets as high as 94 times faster and about 93% approximate quality. PD gets as high as 67 times faster with 96% quality. MP gets as high as 54 times faster with 98% quality. FDR achieves as high as 20 times faster with almost the same quality (99.7%, sometimes even over 100%, which means even better quality) as the baseline.
The cube aggregation algorithms work by aggregating from a single selected supporting cuboid and ignore other cuboids. An ensemble based aggregation which utilizes all candidate cuboids could be adopted to improve quality. When cluster-ing quality is more important than cube construction speed, we can first perform temporary individual aggregations from each candidate supporting cuboid and then ensemble the in-dividual aggregations to get a good final aggregation by con-sensus decision [15].
In this paper, we presented a framework for OLAP of im-ages and proposed the Visual Cube to support e ffi cient OLAP analysis. Our contribution is summarized as follows. (1) We designed Visual Cube and proposed algorithms for Visual Cube construction. For the clustering structure mea-sure, in addition to the intuitive algorithm of independent ag-gregation, we also proposed more e ffi cient algorithms which are much faster while being able to achieve similar quality (in some cases even better quality). (2) We proposed the idea of dynamic aggregation selection which can improve the performance of data cube computa-tion. (3) We introduced the cell overlapping issue in Visual Cube and OLAP. Overlapping has a great impact on the aggre-gation type of data cube measures, computation and OLAP operations. We proposed e ffi cient solution and a new type of OLAP operation to support overlapping.

Some future work include: (1) OLAP based Image Re-trieval; (2) Cell Level Top-k Query, given query cell, find top-k similar cells measured by the similarity of their images; (3) Visual frequent patterns mining in Visual Cube; and (4) In-cremental Visual Cube, given new images, e ffi ciently update the cube.
 Research was sponsored in part by Kodak Inc., NSF grants IIS-09-05215, AFOSR MURI award FA9550-08-1-0265, and by the Army Research Laboratory under Cooperative Agree-ment Number W911NF-09-2-0053 (NS-CTA). The views and conclusions contained in this document are those of the au-thors and should not be interpreted as representing the o ffi cial policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Govern-ment purposes notwithstanding any copyright notation here on. [1] G. G. Angelica. Applying OLAP Pre-Aggregation [2] C. K. Chui. The design and implementation of an olap [3] R. Datta, D. Joshi, J. Li, and J. Z. Wang. Image retrieval: [4] Flickr. http://www.flickr.com . [5] J. Gray, S. Chaudhuri, A. Bosworth, A. Layman, [6] J. Han and M. Kamber. Data Mining: Concepts and [7] F. M.-f. Jiang, J. Pei, and A. W.-c. Fu. Ix-cubes: iceberg [8] X. Jin, S. Kim, J. Han, L. Kao, and Z. Yin. Gad: General [9] H. Labs. Hunter lab color scale. Insight on Color , 8 [10] J. Li, H. Zhou, and W. Wang. Gradual cube: Customize [11] C. X. Lin, B. Ding, J. Han, F. Zhu, and B. Zhao. Text [12] S. Merugu and J. Ghosh. A distributed learning [13] X. Mingjie. Experiments on Remote Sensing Image [14] K. Pearson. On lines and planes of closest fit to systems [15] W. Punch. Clustering ensembles: Models of consensus [16] L. SP. Least squares quantization in pcm. Technical [17] S. Wang, F. Jing, J. He, Q. Du, and L. Zhang. Igroup: [18] K. Wu, E. Otoo, and A. Shoshani. On the performance [19] O. R. Za X ane, J. Han, Z.-N. Li, S. H. Chee, and J. Y. [20] Y. Zhao, P. M. Deshpande, and J. F. Naughton. An
