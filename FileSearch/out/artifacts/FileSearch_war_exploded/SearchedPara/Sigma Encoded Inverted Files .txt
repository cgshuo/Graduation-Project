 Compression of term frequency lis ts and very long document-id lists within an inverted file s earch engine are examined. Several compression schemes are compared including Elias  X  and  X  codes, Golomb Encoding, Variable Byte Encoding, and a class of word-based encoding schemes includi ng Simple-9, Relative-10 and Carryover-12. It is shown that these compression methods are not well suited to compressing these ki nds of lists of numbers. Of those tested, Carryover-12 is preferred because it is both effective at compression and fast at decompression. A novel technique, Sigma Encoding prior to compression, is proposed and tested. Sigma Encoding utilizes a parameterized dictionary to reduce the number of bits necessary to store an integer. This method shows an about 0.3 bit per integer improvement over Carryover-12 while costing only about 3 extra clock cycles per integer to decompress. H.3.1 [Information Storage and Re trieval]: Content Analysis and Indexing -Indexing methods Algorithms, Performance Inverted Files, Compression Inverted file search engines are thought to be I/O bound. The amount of time it takes to proce ss the index and produce the list of results is more a function of the time it takes to read the postings from disk than the time to process the postings. For this reason, Zobel &amp; Moffat [7] and Williams &amp; Zobel [6] recommend adding compression in order to increase throughput. Scholer et al . [4] and Trotman [5] examine two fundamentally different forms of index compre ssion: bit-based and byte-based. Trotman [5] compares the effectiveness and decompression speed of Elias- X  Elias- X  , Golomb, Binary Interpolative Coding, and Variable Byte Encoding and show s that Variable Byte Encoding requires more storage, but can be decompressed more efficiently than others. Scholer et al . [4] showed that a search engine using Variable Byte Encoding is more efficient that one based on bit-wise compression schemes. Variable Byte Encoding is preferred because the rate of decompression easily compensates for the loss in compression effectiveness. Decompression can be performed in as little as 10% of the time taken for bit-based schemes such as Golomb. The loss in storage space, however, can be as much as 300%. Anh and Moffat [2] propose several word-based compression schemes. These schemes pack many integers into a fixed-sized word by carving that word into a number of fixed bit-length pieces. Doing this allows many integers to be packed into a word, whereas Variable Byte Encoding packs at most one integer into a byte. Several schemes have been proposed including Simple-9. Simple-9 compression uses a 32-bit word divided into two parts, a 4-bit selector and a 28-bit body. This 28-bit body can be divided into 9 The 4-bit selector is used to desc ribe which division is being used. Decompression is fast and comp ression effectiveness is high. Relative-10 compression shrinks the selector to just 2 bits (using 30 bits for storage). Instead of di rectly representing the division, it represents the division relative to the previous word. That is, the next word might be divided the same way, one worse, or one better then the current word (or it might be reset to one integer only). When the division of 30 bits into 7-bits each (or 4-bits each) is performed, there are 2 wasted bits in a word. These might be used as the selector bits for the next word (consequently 32 bits are available in the latter). Exactly this is the case in Carryover-12 compression. The algorithms of Anh and Moffat [2] result in compression almost as effective as bit-wise schemes while being almost as efficient as Variable Byte Encoding at decompression. Common to these prior studies is the desire to increase throughput and decrease the storage space necessary to store the document-ids in an inverted file search engine. We, instead, examine compression of the term frequencies. We show that the nature of the term frequencies is different from that of document-ids and propose a dictionary-based encoding (Sigma Encoding) followed by integer compression. When used with Carryover-12, a space saving of about 0.3 bits per integer for an extra about 3 cycles per integer to decompress is shown as compared to Carryover-12 without Sigma Encoding. The postings for a single term in an inverted file search engine are often represented as: &lt;df t : &lt;d t1 ,tf t1 &gt;, &lt;d where df t is the number of docum ents containing term t (the document frequency) and tf tn is the number of times the term occurs in document d tn (the term frequency). The df t often stored in the vocabulary and the &lt; d tn ,tf tn &gt; (posting) pairs in the postings file. document-ids and the frequencies should be encoded separately and stored sequentially [3]. This allows the loading of only document-ids for Boolean searching, or additionally the frequencies for ranking. The document-ids are a monotonic sequence and delta coding is often applied. The sequence &lt; d 1 , d 2 , d 3 ,..., d 10, 16, ..., 123&gt; is stored as &lt; d 1 -0, d 2 -d 1 , d necessity, smaller than the ids and hence compress more effectively with adaptive compression. As the frequency of a term increases, the average delta must decrease in size. The deltas for a term occurring in every document form the sequence &lt;1, 1, 1, documents, the mean delta will be 2. Term frequencies, the tf tn component of the postings, unlike document-ids, do not form a m onotonic sequence. Further, as document frequency ( df t ) increases, we expect the average term frequency ( tf tn ) to increase as well, and so we expect the sequence to compress less effectively. The grammatical conjunctions (but, when, etc.), for example, are expected to occur in a large number of documents and to occur many times in thos e documents. The consequence of this observation is that as document frequency increases, the document-ids will compress more effectively, but the term frequencies will compress less effectively. Impact ordered inverted files [1] are represented &lt; i 1 of the given set of documents. The impact factor is, essentially, a coarse-grained bucketing of term frequency. With respect to compression, documents with the same impact factor form a monotonic sequence. Using impact factors does not f undamentally change the nature of inverted file compression; however it does (essentially) remove the necessity of compre ssing term frequencies. Section 4 demonstrates that Sigma Enc oding is effective for very long lists of document-ids. These con tinue to be seen even with impact ordering; however, we leave for further work the demonstration of the effectivene ss of Sigma Encoding in impact ordered inverted files. Each term frequency list is compressed separately and there are no dependencies between the lists. Gi ven a single list, a dictionary seen in the list. The dictionary is then sorted by decreasing frequency of occurrence of value in the list. For example, for &lt;5, then 12 and finally 4, so the dictionary is &lt;5, 3, 12, 4&gt;. Integers in the original list are then renumbe red with the ordinal value from the dictionary (the sigma ). In the example, this results in the sequence &lt;0, 1, 2, 0, 1, 0, 0, 1, 2, 3&gt;, when counting from 0. This is standard dictionary-based compression. To encode each term frequency list, it is necessary to encode the dictionary length, the dictionary, and the sigmas. In the example, 3&gt;&gt;. This new sequence is longer th an the original list. But if an adaptive compression scheme such as Carryover-12 is used to further compress the whole sequence, gains can be seen because the most frequent terms are represented by the smallest sigmas, which in turn compress well. consequently, possible to swap two entries in the dictionary with no effect on the number of bits used to compress a sequence containing the two, as long as their sigmas are of the same magnitude. That is, if a dictionary entry is in position 14 and the two does not matter when considering how many bits are necessary to store the sigma (because both sigmas are 4 bits in length). Reordering the dictionary is effective if, consequently, the dictionary can be stored more efficiently. Within the dictionary, all values whose sigmas are of the same base-2 magnitude (take the same number of bits to represent) are sorted into increasing order. To compress the dictionary, delta coding is applied to each base-2 range, and then compression using another compression sche me (such as Carryover-12). dictionary is created, which is then ordered and delta-coded. Figure 1 illustrates the compression process for a single term frequency list. First, the freque ncy ordered dictionary is constructed, then those entries that take 1-bit to store (0-1), and then 2-bits to store (2-3) are sort ed into increasing order. To each of these ranges delta encoding is applied. The tf renumbered with the sigmas. Fi nally, the entire sequence is composed of the dictionary length (| D |), the delta-encoded dictionary, and sigma-encoded te rm frequency values. Of course, lengths, deltas and sigmas count from 0 (sigma=0 is the first dictionary entry). unique. In this case the dictionary doubles the length of the sequence. This can result in Sigma Encoding adding an overhead. Such a problem is likely to occur wh en the lists are short, which is frequently in an inverted file index. To alleviate this, we parameterize the scheme. Only terms occurring more than threshold T times are added to the dictionary, the others are stored as | D | + tf tn where | D | is the dictionary length. Throughout this investigation, we set T =1, which means that a value must occur two or more times to enter the dictionary. When indexing very large documen t collections, the postings for a single term could become very large and contain many distinct values. If this is the case, the dictionary could also become very large. The size of the dictionary might be controlled either by only taking terms that occur more than some T number of times, or else by imposing a strict limit on the length. Experiments were conducted on an Intel Celeron processor at 1.06GHz. The document collection was the TREC Wall Street Journal (WSJ) Collection. Furthe r experiments used the TREC Wt10g web collection and a Pentium 4 2.8GHz processor. Results for Sigma Encoding include the cost of storing and decompressing the dictionary (one for each list). 
Bits Per Inte g er
Figure 2: The effectiveness of Carryover-12 compression on term frequency lists (in bits per integer) decreases as the We tested the hypothesis that the compression effectiveness of term frequency lists decreases as document frequency increases. The postings lists were generated for each unique term in the Wall Street Journal collection. They were then compressed using Carryover-12. In Figure 2 the mean number of bits per integer (BPI) needed to store the term frequency lists is plotted against increasing document frequency (t he mean is shown dotted). Although unstable at the beginning, a general upward trend is seen suggesting that, indeed, co mpression effectiveness of term frequency lists decreases as the number of documents in which the term is seen increases. 
Table 1: Compress effectiveness and decompress efficiency of bit-wise, byte-wise, word-wis e compression schemes and for efficiency of Carryover-12 a nd Sigma Encoded Carryover-12. Compression effectiveness and decompression efficiency for both term frequencies and delta-encode d document-ids were compared to several other schemes. Ta ble 1 shows (left to right) Golomb, Variable Byte Encoding, Simple-9, Relative-10, Carryover-12, and Sigma Enc oded Carryover-12. For term frequency lists Sigma Encodi ng shows an improvement on Carryover-12 of 0.3 bits per integer. The additional cost of decompression in clock-cycles per integer (CPI) is about 3. A comparison of Carryover-12 with Sigma Encoded Carryover-12 in the larger TREC Wt10g collection is presented in Table 2. There an improvement of 0.2 bits per integer is shown for an additional cost of 4 clock cycles per integer. Size (Kb) (dashed) to Carryover-12 compression on term frequency lists Using the Wall Street Journal collection, the performance of Sigma Encoded Carryover-12 was compared to Carryover-12 as document frequency increases. In Figure 3 the solid line shows the size of the compressed list when Carryover-12 is used, the dashed line shows the same for Sigma Encoded Carryover-12. It can be seen that the improveme nts are small but consistent. The same comparison was performed for document-id lists where encoding proved ineffective when averaged over all lists (see Table 1 and Table 2). The results are presented in Figure 4 where it can be seen that Sigma Encoding is effective when document frequency is large, but not so wh en small. For long lists, a more ordered distribution around the mean is expected (due to the standard error of the mean) and hence, the dictionary is more effective. Size (Kb) (dashed) to Carryover-12 compressi on on document-id lists as Compression effectiveness for te rm frequencies was examined and (at least in the Wall Street Journal collection) the effectiveness in bits per integer was shown to decrease as the document frequency increases. This is likely to be because terms that occur in many documents are terms that are likely to be frequent in a single document (such as the grammatical conjunctions and articles). The larger term frequency values compress less well than the smaller term frequency values seen for uncommon terms. A dictionary-based coding sche me called Sigma Encoding is introduced as a preprocessing step before compression. This coding scheme, when used in conjunction with Carryover-12 compression, is shown to be effective in compressing term frequency lists. When tested on document-id lis ts, Sigma Encoding followed by Carryover-12 compression is shown to be ineffective in the general case, but effective when document frequencies are large (the lists are long). In future work we plan to examine Sigma Encoding with impact ordered indexes. We expect it to c ontinue to be effective; perhaps more so than on non-impact ordere d lists. For the simple case of two impacts, the monotonic documen t-id list is represented by two shorter monotonic lists, one fo r each impact. The mean delta in each list will necessarily be larg er than the single list; therefore, dictionary compression can be expected to be effective in reducing the number of bits necessary to store each delta. With more than two lists, the gaps will become larger and effectiveness is expected to be increase. We also plan to examine Sigma Encoding with phrase searching. In this case, the gaps between term occurrences are expected to be large, but may not form regular patterns. The effectiveness of Sigma En coding is dependant on how well the dictionary can be stored. We are examining techniques to store the dictionary more efficiently, especially when long sequences of consecutive numbers are present. We are also examining alternative threshold methods. Sigma Encoding followed by Carryover-12 compression is an effective method of storing term frequency lists and long document-id lists in an invert ed file search engine. An improvement of about 0.3 bits per integer is seen at the cost of about 3 clock cycles to decomp ress. Further improvements are expected with an improved threshold method and better compression of the dictionary. [1] Anh, V. N., &amp; Moffat, A. (2002). Improved retrieval [2] Anh, V. N., &amp; Moffat, A. (2005). Inverted index compression [3] Anh, V. N., &amp; Moffat, A. (2006). Structured index [4] Scholer, F., Williams, H. E., Yiannis, J., &amp; Zobel, J. (2002). [5] Trotman, A. (2003). Co mpressing inverted files. Information [6] Williams, H. E., &amp; Zobel, J. (1999). Compressing integers for [7] Zobel, J., &amp; Moffat, A. (1995). Adding compression to a full-
