 Chronic obstructive pulmonary disease (COPD) is a lung disease characterized by airflow limitation usually associated with an in-flammatory response to noxious particles, such as cigarette smoke. COPD is currently the third leading cause of death in the United States and is the only leading cause of death that is increasing in prevalence [15]. It also represents an enormous financial bur-den to society, costing tens of billions of dollars annually in the U.S. It is widely accepted by the medical community that COPD is a heterogeneous disease, with substantial evidence indicating that genetic variation contributes to varying levels of disease suscepti-bility. This heterogeneity makes it difficult to predict health de-cline and develop targeted treatments for better patient care. Al-though researchers have made several attempts to discover disease subtypes, results have been inconclusive, in part because standard clustering methods have not properly dealt with disease manifesta-tions that may worsen with increased exposure. In this paper we introduce a transformative way of looking at the COPD subtyping task. Specifically, we model the relationship between risk factors (such as age and smoke exposure) and manifestations of disease severity using Gaussian Processes, which allow us to represent so-called  X  X isease trajectories X . We also posit that individuals can be associated with multiple disease types (latent clusters), which we assume are influenced by genetics. Furthermore, we predict that only subsets of the numerous disease-related quantitative features are useful for describing each latent subtype. We model these as-sociations using two separate beta process priors, and we describe a variational inference approach to discover the most probable la-tent cluster assignments. Results are validated with associations to genetic markers.
 G.3 [ Mathematics of Computing ]: Probability and Statistics X  Correlation and regression analysis, stochastic processes, nonpara-metric statistics Gaussian processes; beta processes; Bayesian nonparametrics; la-tent clusters; disease trajectories
COPD is a major cause of chronic morbidity and mortality through-out the world, being the fourth leading cause of death worldwide [7]. By 2030 it is estimated that approximately 9 million people will die annually from the disease in the U.S. [15, 14]. COPD also imposes an enormous financial burden on society, with an estimated cost of $2.1 trillion in 2010 globally.

COPD is characterized by airflow limitation resulting from chronic inflammatory responses in the airways and lungs to noxious parti-cles or gases. There are two basic components of COPD: emphy-sema (destruction and loss of lung tissue) and small airways dis-ease (inflammation and thickening of airway walls). Both of these processes result in breathing difficulty. Patients often undergo high resolution computed tomography (CT) scanning, which enables the direct evaluation of the lungs and airways (Figure 1). Addition-ally, COPD severity is assessed in the clinic using spirometry, a technique in which patients blow into a device that measures lung function.
 Tobacco smoke is the most common environmental risk factor of COPD, but it is known to be a heterogeneous disease, with genetic factors predisposing individuals to varying levels of disease sever-ity as a function of exposure. An improved understanding of the interplay between genetics and exposure should lead to better strat-ification of patients for prognosis and personalization of therapies. There have been several large clinical projects to better understand this complicated disease, one of the largest being COPDGene [17] (http://www.copdgene.org). A number of authors have also applied established machine learning approachs in an effort to identify dis-tinct disease subtypes. [2] applied principal component analysis (PCA) followed by cluster analysis using the VARCLUS procedure on eight measures of disease severity. The authors in [6] used fac-tor analysis to select a subset of features and followed this with K-means clustering to identify population subgroups which they then investigated with genetic analysis. Recently our group performed a similar analysis in the COPDGene study cohort to identify four subgroups of subjects that associated with known genetic variants [4].

Although COPD clustering efforts to date have shed light on the disease, they have been limited by the application of approaches that do not specifically model the interplay of the available fea-tures. In particular, they pool together groups of people with differ-ent levels of disease severity, though these group differences may be partly explained by different levels of smoke exposure and age differences. Here we claim that individuals should be grouped ac-cording to biological and/or genetic similarity regardless of their level of disease severity; therefore, we seek associations of indi-viduals to disease trajectories (i.e., grouping individuals based on their similarity in response to environmental and/or disease causing variables). We introduced this concept in a previous paper in which we described a clustering with constraints method using a Dirich-let process mixture of Gaussian processes in a variational Bayesian nonparametric framework [18]. This model assumes that each in-dividual is a member of a single cluster (disease subtype), and it assumes that each feature is important for describing each of these clusters.

Here we introduce a more general framework that 1) permits both instances and features to belong to more than one cluster, 2) allows for overlapping clusters, and 3) identifies subsets of features associated with each cluster. We are again principally motivated by the concept of disease trajectories , but we allow for the possi-bility that a given individual may be suffering from multiple dis-ease subtypes concurrently. Furthermore, it is likely the case that only a subset of features are needed to describe a particular sub-type. Because the number of subtypes is unknown, we again turn to Bayesian nonparametric methods: we will describe a model that continues to use Gaussian Processes to represent trajectories and also builds on dual beta processes for instance and feature assign-ment to the latent subtype variables.

The rest of the paper is layed out as follows. In section 2 we provide an overview of the theory behind our model, focusing on Gaussin processes in section 2.1 and Beta processes in section 2.2. In section 3 we describe our probabilistic model; we define both the structure and the constituent probability distributions. The update equations used for variational inference are given in section 4.We demonstrate algorithm performance on both synthetic and clinical datasets in section 5 and conclude in section 6.
The model we propose is an instance of nonparametric overlap-ping subspace clustering. There are a number of related works in this context. [11] introduced the Infinite Overlapping Mixture Model (IOMM), a nonparametric clustering method that allows an unbounded number of potentially overlapping clusters. Assign-ments of points to (multiple) clusters is modeled using an Indian Buffet Process (IBP) and realized with a multiplicative mixture model likelihood function. This mixture model can be seen as a nonparametric generalization of the products-of-experts model in-droduced by [12]. While they assume an unknown number of clus-ters and that all features are relevant for each cluster, [8] assume that the number of clusters is known but that features are local to a particular cluster. Their Bayesian Overlapping Subspace Cluster-ing (BOSC) model is a hierarchical generative model for matrices with potentially overlapping sub-block structures.

While [8] used a pair of Beta-Bernoulli distributions with an as-sumed known number of clusters as priors for the latent row and column membership vectors, we adopt nonparametric priors based on dual beta processes: one prior for the association of data in-stances to latent clusters, and another for the association of ob-served features to latent clusters. This allows our model to discover the number of latent clusters best supported by our data, the associ-ations of instances to those clusters, and the observed features that best describe them. We continue to use a Gaussian process likeli-hood function as described in [18], but we extend it using a multi-plicative mixture model as [11], which permits both instances and features to be assigned to multiple clusters / subtypes. Importantly, we perform inference using a variational approach which further distinguishes our method from both [8] and [11]. In the remain-der of this section we describe two elements central to our model: Gaussian and beta processes.
Gaussian Processes (GPs) have been used extensively for Bayesian nonlinear regression. We cover the key concepts here as they per-tain to our framework and refer the reader to [16] for details.
Gaussian Processes can be interpreted as a nonparametric prior over functions. They have the property that given a finite sampling of the domain, the corresponding vector of function values, distributed according to a multivariate Gaussian with mean ically set to 0 in standard practice) and covariance matrix The elements of K are determined by the kernel function, k k ( x n , x n 0 ) . The choice of kernel function and selection of its pa-Figure 2: Ten random draws from Gaussian processes with a squared exponential kernel (left) and a linear kernel (right). rameter values controls the behavior of the GP. One popular kernel function is the squared exponential (SE) given by and another, more restrictive kernel function is the linear kernel: where D is the vector dimension. A collection of random draws from each of these two kernels is illustrated in Figure 2.
In order to perform GP regression, we assume an observed dataset of inputs and corresponding (noisy) targets, D  X  { x n , y where we model the targets as p ( y | f )= N y | f , 2 I N the predicted mean and variance of target value y  X  at some new input x  X  are given by where k  X  X  X  = k ( x  X  , x  X  ) and [ k  X  ]
The beta process can be used as a Bayesian nonparametric prior for sparse latent feature models [9], i.e. as a prior over infinite bi-nary matrices that indicate associations between instances (rows) to a potentially infinite number of latent features (columns). Beta processes are closely related to the Indian Buffet Process (IBP) in-troduced by [10], which has seen wide applicability to a number of problems. One limitation of the IBP, however, is that the distribu-tion on the number of features per object and on the total number of latent features is coupled through a single parameter. This limits the expressivity of the prior. [9] later introduced a two-parameter version of the IBP which enables the amount of sharing between instances and latent features to be controlled. This is an attractive feature, but the authors did not describe how to realize this con-struction in a variational inference framework (which we note has computational benefits over sampling based inference methods as used in [9]).

More recently, [3] described a two-parameter, stick-breaking con-struction for beta process priors specifically for variational infer-ence approaches. Their prior can be represented in the following manner where Z is the infinite binary matrix and V , T , and d are latent features in their model. and  X  are the model parameters that control the amount of latent feature sharing between instances. We refer the reader to [3] for a detailed description of this construction.
The desiderata of the model we propose in this section are a) the ability to identify the most probable number of latent disease subtypes, b) the flexibility to assign multiple subtypes to a given data instance (patient), and c) the capability to identify the subsets of features that best describe a given subtype.

In the remainder of this section we formalize the elements of our framework. Let X =[ x 1  X  X  X  x Q ] be the N  X  Q matrix of observed inputs where N is the number of instances and Q is the dimen-sion of the inputs. Let Y =[ y of corresponding target values, where D represents the dimension of the target variables. We designate the set of latent functions as n d ( x ) k in the matrix F ( k ) = plete set of latent functions as binary indicator matrices, Z r and Z c (where the superscripts refer to the original N  X  D data matrix: r referring to the rows of that matrix, and c the columns). Z r is an N row by infinite column matrix where each row represents a data instance and each column represents a latent cluster (subtype). Similarly, Z c is a infinite column matrix corresponding to the target features. We place beta process priors on each of these binary matrices using the stick-breaking construction introduced in [3].

The probabilistic graphical model describing our formulation can be seen in Figure 3, and the corresponding joint distribution is given by where we use two beta process priors  X  one for representing the association of instances to the latent clusters (indicated with the superscript), and one for representing the association of observed features to latent clusters (indicated with the c superscript). (See section 2.2 above and [3] for details about the beta process prior).
The prior placed over the GPs representing each disease trajec-tory are given by Note that we specify distinct kernel matrices and mean vectors for each target feature dimension d .

The likelihood term is given by the element-wise (Hadamard) product between vectors Z r n ,  X  Z d ,  X  , and c ( z ) is the normalization factor. If every element of 0 , then Y n , d is assumed to be generated from the noise component,
Figure 3: Probabilistic graphical model for our formulation.
In this section we give the variational inference update equations that are specific to our model. Variational inference is a method of approximate inference that makes assumptions (typically a factor-ization) over the distribution of interest, and it turns an inference problem into an optimization problem [13]. Additionally, whereas approximate inference methods based on sampling (such as Monte Carlo Markov Chain) can be slow to converge, variational inference enjoys a greater computational advantage in this regard.
For our application, we are interested in the distribution over the latent variables in our model given our observations: Direct evaluation of this posterior is intractable, so we approximate the posterior with a factorized distribution: As described in [3] we have and similarly for p  X  ( d r ) , p  X  ( V r ) , p  X  ( T r ) rameter updates for the variational distributions over d r T mainder of this section we provide updates for p  X  and p  X  ( Z c ) .

To derive update expressions for the factors in the variational dis-tribution, we compute the expectation of the log joint distribution with respect to the other factors in the variational distribution. In the case of p  X  where p (  X  ) abbreviates the joint distribution given in Equation 7 and the expectation is with respect to every variable in the the vari-ational distribution other than { F ( k ) } . This leads to the variational distribution over { F ( k ) } where and where  X  n , d is the probability that instance n is associated with any latent feature that observed feature d can describe and is given by
Unfortunately, there is no closed form update expression for the tween the factorized approximation and the true posterior is min-imized when a quantity known as the variational lower bound is maximized. Therefore, we must turn to the variational lower bound and directly optimize those terms that depend on these parameters. We proceed by describing updates for Z r ; updates for Z c are ob-tained analogously. First we note that there are four terms in the lower bound that directly depend on Z r : Variables ' k and r appear in the stick-breaking construction of the beta process described in [3].

Expansion of the first term in Equation 22 gives where (  X  ) is the digamma function. The second term in Equation 22 expands to n , k ' k ( r &gt; 1) ( a k ) ( a k + b k ) where k ( m ) is given by and M is set to 1 , 000 as in [3]. Expansion of the third term in 22 gives Finally, the last term in 22 expands as where we use the following definitions and we note that the expression in Equation 30 can be evaluated in a straightforward manner by expanding the terms.

Together Equations 23, 24, 26, and 27 constitute an objective function for n , k that we optimize using Brent X  X  method on the interval [0 , 1] . Brent X  X  method is a derivative-free optimization al-gorithm that uses inverse parabolic interpolation when possible to speed up convergence of the golden section method [1]. An anal-ogous procedure is applied to update each d , k using a similar ob-jective function, except the outer sum in Equation 27 is from to N .

There are two terms in the updates for n , k and d , k that deserve special attention. The expectation in Equation 30 is an approxima-tion the term This term emerges as a consequence of using the multiplicative mixture model, and the sum in the denominator makes this expec-tation intractable. However, we note that the expression we use in 30 is an upper bound of the original term given that the denomina-tor in 31 will always be greater than or equal to 1 (recall that the likelihood given in Equation 9 is only active when z 6 =0 is the negative of Equation 31 that shows up in the lower bound, our approximation is in fact a lower bound of the original term. Hence we are gauranteed to be optimizing a lower bound of the original lower bound.

The second term of interest also comes from expanding the last term in Equation 22: which we again note is intractrable. However, noting that the sum will always be between 1 and K and that the natural log is a con-cave function, we can substitute Equation 33 with which is easy to evaluate.
In this section we describe experimental results. We begin by illustrating our approach on a synthetic dataset and then describe the application of our algorithm to data taken from the COPDGene cohort [17].
Our algorithm is designed to identify not only the number of latent clusters, but also which data instances and which observed features associate with them. We demonstrate this with a synthetic example consisting of 120 samples and four observed features. The first two observed features are designed to be redundant: they both describe the same two latent clusters, represented by two lines; half of the samples belong to one line in both features, and the other half belong to the other. The third observed feature is a noise feature. Here all samples are drawn from the same normal distribution. The fourth observed feature contains three line segments that are dis-tinct from the groupings present in the first two observed features (each of the three line segments consists of a third of the samples from each of the two lines described by observed features one and two).

We ran our algorithm with K =20 , 2 = X  2 =0 . 2 ,  X  r =3 . 5 and r =  X  c = c =1 . 5 . We used a linear kernel for each of the Gaussian processes with  X  0 =2 . 0 and  X  1 =1 . 5 . Except for the variances, no special attention was given to the parameter settings; these were the first values selected and they produced the reported results. The variances provided to the algorithm were the same values used to generate the samples.

Figure 5.1 illustrates the results. Each latent feature is color-coded, so it can be seen that the first two observed features both describe the same two latent clusters. Observed feature three was correctly identified as a noise feature, i.e. not capable of describing any of the five latent clusters present in the data set. The algorithm also found the three latent clusters described by observed feature four.
Here we report results from an experiment performed on clinical data from the COPDGene study, a large epidemiologic and genetic study of over 10 , 000 current and former smokers with and with-out COPD [17]. All subjects had blood collected for genetic anal-ysis, and they completed spirometry and chest computed tomog-raphy (CT) scans, resulting in a large collection of features. The set of observed features we consider consists of seven lung func-tion measures: functional residual capacity (FRC), pre-and post-bronchodilator forced expiratory volume in one second (FEV1), pre-and post-bronchodilator forced vital capacity (FVC), and pre-and post-bronchodilator FEV1/FVC. We also consider four CT-based measures of emphysema: total percent emphysema, the fif-teenth percentile level of the intensity histogram in the lungs (Perc 15), and percent emphysema in the upper and lower lung lobes. We also include three airway disease measures: the wall area percent (WA%), percent gas trapping, and the predicted WA% of an airway with a 10 mm perimeter (Pi10).

The predictors of our model are age, height, and pack years, which is a measure of life-long smoke exposure defined as the num-ber of packs per day times the number of years of cigarette smok-ing. These constitute the inputs to the GP covariance matrices, and are meant to capture the factors that are causative of COPD sever-ity. The use of height as a predictor of disease may seem odd, but it is known to influence lung function given that it affects lung me-chanics, so we opted to include it in our model. We again choose to use the linear kernel for our experiments. This amounts to a form of nonparametric polynomial regression, and there are less computationally heavy ways to do this than with a Gaussian pro-cess framework. However, we emphasize that using GPs provide a much more flexible representation of possible disease trajectories. But in the absence of constraints between instances or observed features, we choose a more restrictive class of kernel function so as Table 1: Inputs and linear kernel parameters used for each GP in the clinical experiment % Emph. Upper Lobes Age, Pack Yrs 2 . 03 . 5 % Emph. Lower Lobes Age, Pack Yrs 50 . 03 . 5 to mitigate the effect of local minima during variational inference. Note that in our previous work we were able to apply more general kernel functions in the presence of data constraints [18]. We hope to extend the current approach once such constraints are available.
The inputs and kernel parameters for each of the observed fea-tures are given in Table 1. In order to guide the selection of inputs and kernel parameters, we performed standard multi-variate linear regression on each of the observed features using each of the three inputs as predictors. For a given observed feature, we only used as kernel inputs those predictors that had a significant association in the linear regression model. The slope intercept generated by the model informed the selection of the  X  0 parameter, which controls the intercept range over which a draw from the GP is likely to come from. For all fourteen observed features, we noticed only modest slope values for each of the regression coefficients. Therefore, we selected a  X  1 value of 3 . 5 for all kernel functions; we observed em-pirically from repeated draws of GPs using this value that slopes tend to be modest.

We chose 2 d for each of the observed features by taking one tenth of the variance of the residuals from the multi-variate regres-sion stage. This was an ad hoc selection, but the measurement vari-ances for each of the observed features is as yet unknown. The se-lected values were chosen to be significantly lower than the resid-ual variances in order to explore subgroups within the data. By comparison, simply using the residual variances would likely have caused the algorithm to simply recapitulate the multi-variate re-gression result, although we did not attempt this experiment. The % Emph. Upper Lobes 1.00 1.00 0.49 0.54 % Emph. Lower Lobes 1.00 1.00 0.98 0.65 Table 2: Inputs and probability of association to each of four latent features, labeled by color for easy comparison to other figures. noise means and variances,  X   X  d and  X  2 the mean and variance of each observed feature.

For the beta process priors, we set K =20 ,  X  r =3 . 5 , and r =  X  c = c =1 . 5 as in the synthetic experiments. We con-sidered the first 1 , 000 subjects to have enrolled in the COPDGene study, keeping only those that have complete data for all inputs and observed features, resulting in a collection of 851 subjects. We deployed our algorithm on our institution X  X  computer cluster, and ran 200 jobs in parallel, executing 15 iterations for each job. Us-ing this dataset with these parameters, each job took approximately 10 hours to complete. For each job we recorded the final varia-tional lower bound value, and report results for the job that gave the largest lower bound.

Our algorithm identified four latent clusters in this data set, and assigned approximately half of the samples to the noise model. For each of the observed features, we give the probability of each be-ing associated with each of the latent clusters in Table 2. We see that both Pi10 and WA% poorly describe the latent clusters; that is to say, those observed features are better described by the noise model. This is not unexpected, given that they both rely on di-rect measurements of airway wall thickness on CT images. Given that airways are small structures and difficult to measure accurately, these measurements are known to be noisy. On the other hand, the lung function measurements tend to do a much better job at de-scribing the latent clusters.

Figure 5 provides scatter plots for several of the observed fea-tures, where we have again color-coded the latent clusters. We include a plot of Pi10, one of the variables that poorly describes the latent clusters, to illustrate the lack of structure in the data. It is also interesting to note that Perc 15 is only useful for describing two of these latent clusters. Summarizing these data we can iden-tify two groups of individuals who have preserved lung function and lung tissue despite increasing age and smoke exposure (red and blue groups). The magenta and green groups appear to rep-resent those individuals that are more susceptible to COPD: they show increased levels of emphysema and gas trapping as well as lower levels of lung function given increasing age and smoke ex-posure, with the magenta group being more susceptible than the green group.

Finally, we evaluate our clustering results by performing genetic association analysis using several single nucleotide polymorphisms (SNPs) known to associate with COPD [5]. As a comparison, we apply the same methodology we used in [4]: K-means clustering (with K =4 ) on a feature set determined by factor analysis. We note however that our previous analysis was conducted on the en-tire COPDGene cohort, and here we only consider the 851 subjects analyzed by our algorithm. Table 3 shows the resulting odds ra-tios and associated confidence intervals. The odds ratio represents the effect size of the genetic variant; it is the odds of having the variant in one subtype divided by the odds of having the variant in another subtype  X  values further from 1 . 0 indicate greater effect of the genetic variant on membership in that subtype. Odds ratios are computed with respect to the healthiest group in each cluster-ing result (the  X  X ed X  group in the case of our algorithm). In the genome-wide analyses of COPD, effect sizes for the previously de-scribed genetic variants were approximately less than or equal to 1 . 4 , suggesting that our approach can identify a more genetically susceptible subgroup.
We have introduced a nonparametric overlapping subspace clus-tering algorithm that relies on dual beta process priors in order to identify latent cluster structure. Our model finds associations be-tween data instances and latent clusters, allowing for a given in-stance to belong to multiple clusters. Additionally, our algorithm identifies the observed features that best describe the latent clus-ters and thus serves as a form of feature selection. The likelihood term in our model is specifically chosen to address the challenges of disease subtype discovery in COPD: by using Gaussian processes to represent the dependence between observed features that mea-sure levels of disease severity and inputs that are causative agents of disease progression, we can flexibly represent so-called  X  X isease trajectories X . We believe our contribution represents a step forward towards a better understanding of a complicated disease that will hopefully lead to better patient care.
 We have made an implementation of our algorithm available on GitHub here, and the COPDGene data can be obtained from dbGaP here. This work was supported by NSF grant IIS-0915910, and by the US National Heart, Lung, and Blood Institute grants R01HL089856, R01HL089897 (COPDGene), K08HL102265, K08HL097029 and R01HL113264. The COPDGene study is also supported by the COPD Foundation through contributions made to an Industry Advi-sory Board comprised of AstraZeneca, Boehringer Ingelheim, No-vartis, Pfizer, Siemens, and Sunovion.
 [1] R. P. Brent. Algorithms for minimization without derivatives . [2] P. R. Burgel, J. Paillasseur, D. Caillaud, I. Tillie-Leblond, [3] L. Carin, D. M. Blei, and J. W. Paisley. Variational inference [4] P. J. Castaldi, J. Dy, J. Ross, Y. Chang, G. R. Washko, [5] M. H. Cho, P. J. Castaldi, E. S. Wan, M. Siedlinski, C. P. [6] M. H. Cho, G. R. Washko, T. J. Hoffmann, G. J. Criner, E. A. [7] M. Decramer, W. Janssens, and M. Miravitlles. Chronic [8] Q. Fu and A. Banerjee. Bayesian overlapping subspace [9] Z. Ghahramani, T. L. Griffiths, and P. Sollich. Bayesian [10] T. Griffiths and Z. Ghahramani. Infinite latent feature models [11] K. A. Heller and Z. Ghahramani. A nonparametric bayesian [12] G. E. Hinton. Training products of experts by minimizing [13] M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. An [14] C. D. Mathers and D. Loncar. Projections of global mortality [15] S. L. Murphy, J. Xu, and K. D. Kochanek. Deaths: final data decline in lung function with increasing smoke exposure. [16] C. Rasmussen and C. Williams. Gaussian processes for [17] E. A. Regan, J. E. Hokanson, J. R. Murphy, B. Make, D. A. design. COPD: Journal of Chronic Obstructive Pulmonary
Disease , 7(1):32 X 43, 2011. [18] J. Ross and J. Dy. Nonparametric mixture of gaussian processes with constraints. In Proceedings of the 30th
International Conference on Machine Learning (ICML-13) , pages 1346 X 1354, 2013.
