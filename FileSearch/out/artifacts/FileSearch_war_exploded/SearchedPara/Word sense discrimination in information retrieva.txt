 1. Introduction
According to Lin (1997) ,  X  X  X iven a word, its context and its possible meanings, the problem of word sense disambiguation (WSD) is to determine the meaning of the word in that context X  X . issue for computers. The problem becomes even more difficult to solve when an ambiguous word occurs in short chunks of texts, such as a query in an information retrieval (IR) system.

Applying WSD to improve IR results is a well studied problem, but with controversial results as evidenced in the litera-ture. Several authors have concluded that WSD in IR does not lead to significant retrieval performance improvement ( Guyot,
Voorhees, 1993 ) have argued that the main problem in improving retrieval performance when using WSD is the inefficiency of the existing disambiguation algorithms, a problem which increases in the case of short queries.

In more recent years the issue remained  X  X  X s to whether less than 90% accurate automated WSD can lead to improvements  X  identifies the meaning of the ambiguous word in context. This type of WSD is generally based on external sources, such as therefore knowledge-based.

Attempts to use knowledge-based WSD in IR have been numerous. In ( Gonzalo, Verdejo, Chugur, &amp; Cigarran, 1998 ) as well synsets. However, they were all conducted on small data sets. As commented in ( Ng, 2011 ), the evaluation is scaled up to a reported in ( Kim, Seo, &amp; Rim, 2004 ), although the quantum of improvements is small.

Zhong and Ng (2012) are among the few authors who more recently have expressed a growing belief in the benefits brought by WSD to IR  X  when using a supervised WSD technique. They constructed their supervised WSD system directly from parallel corpora. Experimental results on standard TREC collections show that, using the word senses tagged by this supervised WSD system, significant improvements over a state-of-the-art IR system can be obtained ( Zhong &amp; Ng, 2012 ).
However, it is well known that supervised WSD cannot be used on a large scale in practice due to the absence of the nec-essary annotated/parallel corpora.

In contrast to all these authors, we are suggesting and investigating the usage of an unsupervised WSD technique. In this paper, we present an approach that aims at identifying clusters from similar contexts, where each cluster shows a polyse-ful. Our approach is therefore not concerned with performing a straightforward WSD, but rather with differentiating among the meanings of an ambiguous word. Considering word sense discrimination rather than straightforward WSD avoids the use of external sources such as dictionaries or WN type synsets which are commonly used ( Carpineto &amp; Romano, 2012 ).
In this paper, we propose a new word sense discrimination method for IR based on spectral clustering. This state of the art clustering technique is now a hot topic; for example, Takacs and Demiris (2009) studied the use of spectral clustering in multi-agent systems while Borjigin and Guo (2012) recently discussed the cluster number determination in spectral cluster-ing. Spectral clustering has been used in WSD for the first time by Popescu and Hristea (2011) who point out the importance of the clustering method used in unsupervised WSD.

We hereby show that WS discrimination based on spectral clustering outperforms the baseline when no WS discrimina-tion is applied and also when using another unsupervised method (Na X ve Bayes).

The present paper is organized as follows: in Section 2 we present the related works on WSD in IR; the focus is on unsu-pervised methods. Section 3 presents word sense discrimination based on spectral clustering. Section 4 presents the two step
IR process using the proposed WS discrimination model. The evaluation is presented in Section 5 . A more thorough analysis of the obtained results is performed in Section 6 . Section 7 lays out the impact of automatically generated context on our proposed method. Section 8 concludes this paper. 2. Related work
Word sense ambiguity is a central concern in natural language processing (NLP). SENSEVAL defined the first evaluation framework for word sense disambiguation (WSD) in NLP ( Kilgarriff, 1997 ). According to Kilgarriff and Rosenzweig (2000) ,
SENSEVAL participants defined systems that can be classified into two categories: supervised systems, which use training instances of sense-tagged words and non-supervised systems. According to ( Navigli, 2009 ), supervised systems are typically employed when a restricted number of words have to be disambiguated, while this type of system encounters more diffi-culties when all open-class words from a text have to be disambiguated. In addition to general WSD, many recent papers
Wieloch, &amp; Sydow, 2009 ) and disambiguation of place names ( Leidner, 2007 ). Indeed, WSD has many applications, such as text processing, machine translation and information retrieval (IR), for which this type of disambiguation  X  proper names  X  can be useful (although not sufficient).
 Krovetz and Croft (1992) were among the first to conduct a thorough analysis of ambiguity in IR. They used the CACM and
TIME test collections and compared query word sense with word senses in retrieved documents. They found that sense mis-match occurs more often when the document is non-relevant to the query and when there are few common words bridging the query and the retrieved document. Another large scale study of word sense disambiguation in IR was conducted by
Voorhees (1993) . The automatic indexing process she developed used the  X  X  X s-a X  X  relations from WN and constructed vectors of senses to represent documents and queries. This approach was compared to a stem-based approach for 5 small collections (CACM, CISI, CRAN, MED, TIME). The results showed that the stem-based approach was superior overall, although the sense-based approach improved the results for some queries ( Voorhees, 1993 ). Sanderson (1994) used the Reuters collection in his experiments and showed that disambiguation accuracy should be of at least 90% in order for it to be of practical use. He used pseudo-words in his experiments.

Sch X tze introduced word sense discrimination in IR ( Sch X tze &amp; Pedersen, 1995; Sch X tze, 1998 ). Moreover, Sch X tze con-siders that, in some cases, WSD can be defined as a two-stage process: first sense discrimination, then sense labeling. Sense discrimination aims at classifying the occurrences of a word into categories that share the same word sense. This type of senses. Sch X tze and Pedersen (1995), Sch X tze (1998) created a lexical co-occurrence based thesaurus. They associated each ambiguous term with a word vector where coordinates correspond to co-occurring term frequencies. Words with the same meaning were assumed to have similar vectors. Word vectors were clustered together to determine the word uses. Similarity was based on the cosine measure. The application in IR consisted of modifying the standard word-based vector-space model.
The words from the  X  X  X ag of words X  X  text representation were replaced by word senses. Evaluation of TREC 1 showed that average precision is improved when using sense-based retrieval rather than word-based retrieval. Combining word and sense-based retrieval improves precision as well. They were the first to demonstrate that disambiguation, even if imperfect, can indeed improve text retrieval performance ( Sch X tze &amp; Pedersen, 1995 ).
 Sch X tze (1998) context group discrimination uses a form of average link clustering known as McQuitty X  X  Similarity
Analysis. Sch X tze adapts LSI/LSA so that it represents entire contexts rather than single word types using second-order co-occurrences of lexical features. The created clusters are made up of contexts that represent a similar or related sense.
In Sch X tze (1998) it is again shown that unlabeled clusters of occurrences of a word representing the same sense result in improved IR.

Unlike that described in Sch X tze and Pedersen (1995), Sch X tze (1998) , the method we propose in the present paper is based on re-ranking and not on modifying document representation.

Much more recently, Chifu and Ionescu (2012) also show that the combination of word-based ranking and sense-based used a classical clustering technique based on the Na X ve Bayes model (for which a WN-based feature selection is performed).
However, in the case of their best obtained result, the highest difference between this result and the baseline under consid-eration was 0.1091.

The present paper will investigate a similar type of technique for IR that uses spectral clustering. Our aim is not only to restate the benefits of unsupervised WSD in IR, but also to point out the importance of the clustering technique involved in this task. While Chifu and Ionescu (2012) , in spite of performing WN-based feature selection, were not able to move beyond the baseline when considering all queries, and therefore only targeted the lowest precision ones, we hereby show that, when using spectral clustering (that performs its own feature weighting) the same baseline is, in most cases, surpassed.
Analysis of the results provided by the newly proposed method will be carried out (see Section 5.6 ) against these two be shown as promising in sustaining the concept of sense discrimination being beneficial for IR applications, especially when used from a re-ranking perspective. 3. Spectral clustering-based word sense discrimination
Word sense discrimination can be considered as a clustering problem since a way to solve it is to group the contexts of an ambiguous word into a number of groups and to discriminate between these groups without labeling them. As is well tering algorithms have been proposed in order to deal with situations where the data is not linearly separable and the clus-ters are non-convex. In particular, two related families of methods, kernel and spectral methods, have proven to be very effective in solving different tasks.
 In computational linguistics, spectral clustering has been used for machine translation ( Gangadharaiah, Brown, &amp; 2005 ), and in unsupervised WSD ( Popescu &amp; Hristea, 2011 ).

Spectral clustering has been used in WSD for the first time by Popescu and Hristea (2011) who point out the importance enough to make up for the lack of external knowledge of all types, solving many problems on its own, including that of fea-ture selection for WSD. Disambiguation results, after using an unsupervised algorithm based on spectral clustering (that uses its own feature weighting) were superior to those obtained using a classical unsupervised algorithm (with an underlying Na X ve Bayes model, for which feature selection was performed) for all parts of speech ( Popescu &amp; Hristea, 2011 ).
The disambiguation accuracy obtained when using spectral clustering in unsupervised WSD, relative to all parts of speech, encouraged us to adopt this clustering technique for sense discrimination in the context of IR. 3.1. Spectral clustering method
The method of spectral clustering is briefly presented here. For more details and justification of the method the reader is referred to von Luxburg (2007) and Hastie, Tibshirani, and Friedman (2008) .

Given a set of observations x 1 ; ... ; x n and some notion of similarity s intuitive goal of clustering is to divide the observations into several groups such that observations in the same group are similar and observations in different groups are dissimilar to each other. One possible way to represent the pairwise simi-vations (the vertex v i represents the observation x i ). Two vertices are connected if the similarity s corresponding observations x i and x j is positive (or exceeds some threshold). The edges are weighted by the s
The problem of clustering can then be reformulated as a graph-partition problem, where we identify connected components with clusters. Our intention is to find a partition of the graph such that the edges between different groups have very low weights (which means that observations in different clusters are dissimilar to each other) and the edges within a group have high weights (which means that observations within the same cluster are similar to each other).

An important element in spectral clustering is to construct similarity graphs that reflect the local neighborhood relation-
Luxburg, 2007 ) to define a similarity graph that reflects local behavior: e -neighborhood graph, k -nearest neighbor graphs, fully connected graph. One of the most popular graphs and the one that we will use for unsupervised WSD, is the mutual k -nearest-neighbor graph. The vertex v i is connected to the vertex x is among the k -nearest neighbors of the observation x j or the observation x vation x i . The weight of the edge v i v j will be w ij  X  s In order to formally present the method of spectral clustering we introduce the following notations.

Let G  X  X  V ; E  X  be an undirected graph with vertex set V  X  weighted, that each edge between two vertices v i and v j
As G is undirected, we require that w ij  X  w ji . The degree of a vertex will be the diagonal matrix with the degrees d 1 ; ... ; d joint sets A ; B # V we define
We can now formulate the graph-partition problem in relation to spectral clustering. For a given number k of subsets (clusters) there is a partition of VA 1 ; ... ; A k which minimizes
Unfortunately, the above optimization problem is NP hard ( von Luxburg, 2007 ). Spectral clustering solves a relaxed ver-sion of this problem. Relaxing  X  X  X atioCut X  X  leads to unnormalized spectral clustering.
 The unnormalized graph Laplacian matrix of a similarity graph G is defined as: stant eigenvector corresponding to the eigenvalue 0). Using a standard method like K-means, the rows of U are clustered, giving a clustering of the original observations.
 The unnormalized spectral clustering algorithm is summarized in Algorithm 1.
 Algorithm 1. Unnormalized spectral clustering algorithm Input: Similarity matrix S 2 R n n , number k of clusters to construct.

Output: Clusters A 1 ; ... ; A k with A i  X f j j y j 2 C i 3.2. Using spectral clustering for unsupervised WSD
There are a number of issues that must be dealt with when applying spectral clustering in practice. One must choose how the mutual k -nearest-neighbor graph, the parameter k , representing the number of nearest neighbors, must be set. In the light of all these issues we follow the approach adopted by Popescu and Hristea (2011) where spectral clustering was used in WSD for the first time.
In unsupervised WSD, the observations are represented by contexts of the ambiguous word. The contextual features are get, in a window of fixed length, centered or not centered on the target. A window of size n denotes the consideration of n content words to the left and n content words to the right of the target, whenever possible. The total number of words con-get word occurs represents the context window. The classical window size, generally used in WSD, and also in the present sponding word in the given context window. Thus, a context is represented as a feature vector and the similarity between two contexts is given by the value of the dot product of the corresponding feature vectors. The dot product was chosen as the measure of similarity between feature vectors because of the success of the linear kernel in supervised WSD ( M X rquez, Escudero, Mart X nez, &amp; Rigau, 2006 ).

As a method for building the similarity graph from the similarity matrix we use the mutual k -nearest-neighbor graph method. This involves the choice of the parameter k , the number of neighbors. As in Popescu and Hristea (2011) , we use a value of 30 for the number of neighbors. 3 4. Word sense discrimination in IR
The main contribution of this paper is the proposal of a new spectral clustering-based method for performing word sense discrimination for IR. This method aims to increase the top level precision for queries which contain ambiguous words. Our suggested approach reorders an initially retrieved document list by pushing to the fore documents that are semantically sim-ilar to the target query.

To start with, for each query we retrieve a set of documents by means of a state of the art search engine; documents are ordered according to their scores. Our objective is then to pinpoint the documents which are more relevant to the informa-tion need because they share the term sense with the query; and to enable these documents to improve their position at the top of the document list.

The first phase in the method is based on clustering the retrieved document set with respect to the senses of each ambig-uous word and to decide which documents share the query term sense. Spectral clustering is used in this phase. In the sec-ond phase, we reorder the initially retrieved document list by boosting documents belonging to the selected cluster. 4.1. Query WSD
Before we can apply the WSD technique described in Section 3.2 to the queries, we need to process the data within a pre-processing step. The preprocessing step identifies the polysemous words of a query (as a result of their occurrence in multi-ple WN synsets).

Term discrimination uses a sub-set of documents. More precisely, for each query, we consider the first n documents retrieved by the IR system. For each document we thus know the score which indicates how similar that document is to document is then calculated by creating an incidence matrix with rows representing the documents and columns represent-ing the features. Each element of this matrix is either 1 or 0, depending whether or not the feature indicated by the column index is present in the document indicated by the row index. The number of WN senses and the incidence matrix obtained after data preprocessing is used as input for the WSD algorithm. Thus, the WSD process is performed on n  X  1 documents ( n initially retrieved documents and the query itself).
 Let us now describe in more details the entire WSD process in relation to a single polysemous target word. ing words are stemmed using the Porter stemmer algorithm ( Porter, 1980 ). The stem corresponding to the target word is not retained, while the remaining stems, alphabetically ordered, represent the final set of features that are used in the WSD process.

The second step is to build the incidence matrix that indicates what features occur in each document. We determine the position of the target word within each of the documents. In our experiments we used a context window of size 25, as sug-gested in Hristea, Popescu, and Dumitrescu (2008) in order to obtain the best possible disambiguation accuracy. The features that occur in the context window are stored in the row of the matrix that corresponds to the analyzed document. If a certain document contains the target several times, we only consider its first occurrence. This is done in accordance with the  X  X  X ne-of a word to preserve its meaning across all its occurrences in a given discourse.

For each query and for each ambiguous term occurring in that query, we cluster the retrieved documents into a number of clusters equal to the number of senses the ambiguous term has, according to a lexical database (WN), which will be used as sense inventory. The final task of the WSD process for a given polysemous word is to determine the document clusters relative to that word. Each obtained cluster corresponds to a specific sense of the polysemous target word, with one of the clusters containing the query itself. Note that two documents are similar (and thus belong to the same cluster), from the
WSD point of view, if the polysemous word has the same sense in both documents. Therefore, disambiguating a polysemous term results in retaining only those documents occurring in the same cluster as the query.

A query can contain several ambiguous terms. In this case, as many clusters of documents as the number of ambiguous words in the query are retained. In order to form a unique list of documents, we fuse these sets of documents; we consider fusion function CombMNZ ( Shaw &amp; Fox, 1995 ).
 The CombMNZ function computes the final document scores as follows: where S i f represents the final score for each document d document d i does not exist in one particular cluster j , then S document i ( c i  X  k if the document d i occurs in k clusters).

We should point out that our unsupervised WSD method performs word sense discrimination and therefore does not give the actual word sense (since we do not know which cluster refers to a specific sense). However, it is not necessary to pair clusters with senses, as document clusters are sufficient for explicit automatic disambiguation in IR. 4.2. Document Re-ranking
Our approach aims to improve the top retrieved document list. The first phase in the discussed method leads to a set of documents extracted by the search engine, corresponding to each query, and in the clusters of documents obtained as described in Section 4.1 . Our main purpose for using a WSD technique in IR is to find the most probable relevant documents and to assign them a higher rank in the initial document list. The second phase of the method thus corresponds to a re-rank-ing method ( Meister, Kurland, &amp; Kalmanovich, 2011 ).

In our approach, the way to reach this goal is to modify the order of the retrieved documents by pushing those documents uments with those obtained as a result of clustering. According to the method discussed above, the documents obtained by We therefore use a parameter to assign a weight to the fusion function.
 This function has the following structure: where S i f represents the final score of a document d i , score  X  d the initially retrieved document set, Clust is the document cluster containing the query itself and a 2 X  0 ; 1 represents the weight of the clustering method for the final results.

As reported in the evaluation section (Section 5 ), we started with a  X  0 and then increased this parameter by 0.01 at each trial.

Finally, in Section 6 , the method is used on subgroups of queries with the purpose of analyzing its behavior with regard to search engine. 5. Evaluation framework 5.1. Data collection features To evaluate the described method we have used data collections from the TREC competition. TREC (Text REtrieval
Conference) is an annual workshop hosted by the US government X  X  National Institute of Standards and Technology which provides the necessary infrastructure for the large-scale evaluation of text retrieval methods.
We opted for three collections for use in the ad hoc task: TREC7, TREC8 and WT10G. For TREC7 and TREC8, the competition provided approximately 2 gigabytes worth of documents and a set of 50 natural language topic statements (per collection). The documents were articles from newspapers like the Financial Times, the Federal Register, the Foreign Broadcast Information Service and the LA Times. The WT10G collection provided approximately 10 gigabytes worth of Web/Blog page documents.
TREC distinguishes between a statement of information need (the topic ) and the text that is actually processed by a retrie-val system (the query ). The TREC test collections provide topics. What is now considered the  X  X  X tandard X  X  format of a TREC the key words a user could have used to send a query to a search engine. The description contains one or two sentences that describe the topic area. The narrative part gives a concise description of what makes a document relevant ( Voorhees &amp;
Harman, 1998 ). Both the descriptive and the narrative parts can offer clues about the word senses used in the title part. 5.2. Ambiguous queries and ambiguous terms
In our approach, the ambiguity of a query was evaluated with reference to the title part of the topic. The ambiguous terms were detected using the WN knowledge database. If the term occurred in multiple WN synsets, then it was considered as an ambiguous term. A query is defined as ambiguous if it contains at least one ambiguous term. The queries from the three col-lections contained from zero to four ambiguous words, as presented in Table 1 , with most of them being nouns. 5.3. Evaluation measures
In TREC, ad hoc tasks are evaluated using the trec _ eval including some single valued summary measures that are derived from the two basic measures in IR: recall and precision. uments relevant to the query that are successfully retrieved. The average precision is defined as:
The MAP (Mean Average Precision) stands for the mean of the average precision scores for each query. The a cut-off level of ten defines the retrieved set as the top ten documents in the ranked list (P@10).

This study uses three cut-off levels: P@5, P@10 and P@30, which are high precision measures. 5.4. Baselines
The present study is based on runs constructed by Terrier. Terrier is an open source search engine that implements state-of-the-art indexing and retrieval functionalities. Terrier was developed at the School of Computing Science, University of Glasgow. 5 The sets of documents retrieved by Terrier are the first 1000 ranked documents returned by the search engine.
We tried several configurations of the Terrier parameters. For each collection, we chose as our baselines the settings with
Runs (associated with the baselines), which determine the set of documents to be used by our WSD method, were constructed as additional baselines.

The best configuration for TREC7 was the following: a two step indexation (both direct and inverted index), with active indexation by block and the use of the BB2 (parameter c  X  1) as a weighting model. As a query expansion model our choice was the parameter-free KL model (KLbfree). The configuration required 3 documents to be used for the query expansion. A term has to occur in two documents in order to be considered relevant. Finally, the number of terms to be added to the query
However, for TREC8 and WT10G, the parameter configuration with the best results in terms of MAP stays in place, except for two differences: the weighting model which is changed with the DFRee model (no parameters) and the narrative part of the topic which is not taken into account when the query is constructed. The values of the MAP corresponding to the best initial number of topics and the number of documents.

It is worth mentioning that, since we re-ordered an initial retrieved document list, we were unable to retrieve documents that would not have initially been retrieved (no recall improvement). We target high precision improvements. 5.5. WSD settings
For each query, the set of the top 1,000 documents retrieved by the best settings for Terrier was considered. These 1,000 documents are the documents considered as the most similar to the information need, sorted by their obtained score, in descending order. The method we promote aims to filter and reorder those documents before retrieving them for the user.
The target terms for the described WSD process are taken from the title part of the TREC topic only. However, our from the resulting text were removed (Terrier X  X  stopwords list is used).
 5.6. Results
The results as compared to the baselines are presented in Fig. 1 . The graphs illustrate the manner in which the top levels of precision evolve with respect to the alpha parameter. Alpha is the parameter which gives a greater or smaller level of row, the results for P@5 are shown, in comparison with the three collections. The next two rows present the results for P@10 and P@30 respectively. For each cut-off level, the vertical axis is recalibrated in order to obtain a clearer view.
Fig. 1 shows that the best results were obtained when the value of the alpha parameter was between 0.02 and 0.20. This observation holds for all the top levels of precision (P@5, P@10 and P@30) and for all the three collections involved in the study. p -values smaller than 10 6 of T -Tests have confirmed the statistical significance of our results. T -Tests have used the two following populations: the baseline value (fixed across the alpha parameter) and our results per alpha, respectively. cluster documents to participate with not more than 1 = 5 in the final document score. The alpha parameter is similar to lambda in the case of RM3, since the optimal lambda values for the short queries vary between 0 : 01 and 0 : 4( Zhai &amp;
While for P@5 and P@10 the difference between the results and the baselines is clear, for P@30 the curve representing the results remains closer to the baseline. The best improvement occurred for the WT10G collection in the case of P@10. A pre-cision value of 0.2937 was obtained (the baseline was 0.2688), which represents an improvement of 8.48%, and is statistically significant with a p-value &lt; 10 6 ( T -Test).

Analysis of the results has been carried out against the two major approaches existing in the literature. We compare our results with those obtained when implementing the disambiguation method proposed in Sch X tze and Pedersen (1995) ,as well as with those reported in Chifu and Ionescu (2012) . The latter authors also test over the TREC7 benchmark, while employing a Na X ve Bayes clustering technique. The baselines are therefore identical and represent the performance of the best runs (see Section 5.4 ), for all the 35 ambiguous queries considered in our study.

In the case of the terms co-occurrence-based method ( Sch X tze &amp; Pedersen, 1995 ) we have reimplemented this method and have organized the same testing setup as the one originally used by Sch X tze and Pedersen (1995) . The stop words have been removed (Terrier stop-words list) and the target term-centered context window was set to size 40 (20 terms before the target term, 20 terms after the target term). We have identified 1,466,983 unique terms that induced the 1,466,983 1,466,983 sparse term co-occurrence matrix. We mention that, due to the high number of vocabulary terms, the co-occurrence matrix is difficult to handle from a computational point of view. The SVD for the co-occurrence matrix was set to 100 dimensions and the reduced matrix was computed using the classify the context vectors we used the Buckshot algorithm implemented using sampling for the initial hierarchical clustering step. The query terms that occurred less than 100 times in the corpus were occurrence, as well as reindexing after replacing words with their senses (for each set of queries), also represent time and resource consuming operations.

In Table 3 we present the results of our comparison in terms of high precision. Best Run represents the best run obtained based method. Na  X  ve Bayes represents the method from Sch X tze and Pedersen (1995) . Spectral Clustering is the method proposed in this article. Sense-based represents the terms co-occurrence-based method of Sch X tze and Pedersen (1995) and CombRank represents the modified co-occurrence method, also presented in Sch X tze and Pedersen (1995) , which con-siders the sum of ranks from the word-based retrieval and from the sense-based retrieval as the final rank for a retrieved document. It was reported ( Sch X tze &amp; Pedersen, 1995 ) as better than using the sense-based method alone. CMNZ-WB-SB represents the combined document list resulting from the word-based and sense-based retrievals, using the CombMNZ func-tion ( Shaw &amp; Fox, 1995 ). CMNZ-WB-SB-alpha represents the CMNZ-WB-SB results with the sense-based retrieved list weighted by an alpha parameter. We tested various alpha values. The best turned out to be 0 : 1.

The various combinations help to improve the initial performance of Sense-based results, although the baseline results ( Best Run ) are not surpassed. The number of the improved queries with respect to Best Run was also computed for the
CMNZ-0 : 1 (which is the best alpha for co-occurrence-based results) and for the Spectral Clustering results, with the same alpha value of 0 : 1. The results are presented in Table 4 . Only very few queries are improved by the terms co-occurrence-based method.

As opposed to the Sense-based model, the peak results of Spectral Clustering outperform the Best Run baseline, for all the levels of high precision (from 1.01% to 3.73%). The average results do not overcome the baseline due to the performance decrease after a certain value of alpha (see Fig. 1 ).
 tion, our method outperforms the Na X ve Bayes method both on average and on peak results. The average is computed across all alpha parameter values, considering all the ambiguous queries. This again suggests the importance of the clustering tech-nique used in unsupervised WSD for IR. We hereby conclude that spectral clustering is an appropriate clustering method for the purpose of sense discrimination in IR.

The present method was also tested for 5000 document runs, but the results were not improved. We think the reason for ranking method cannot reach efficiency at the top level of precision (P@5, P@10 and P@30). We also considered a two cluster model in which documents could either be clustered in the query cluster if similar enough to the query, or in the non-query cluster. Results were better when as many clusters as WN senses were considered. 6. Further analysis of the results
This section aims to deepen the analysis of the results obtained when considering two types of query clusters: those based on the query performance and those taking into account the number of ambiguous terms per query. 6.1. Improvements in baseline precision intervals
Following previous research showing that results can differ according to query difficulty ( Bigot, Chrisment, Dkaki, Hubert, &amp; Mothe, 2011 ) and with the purpose of observing where the proposed method behaves most accurately, (independently for sponding to the 104 ambiguous queries were divided into 5 groups, according to the baseline precision (0.0 X 0.2, 0.2 X 0.4, ... , 0.8 X 1.0) and for each of the top levels of precision being investigated (P@5, P@10 and P@30, respectively). documents retrieved by the search engine puts more relevant documents at the top of the list. On the other hand, for the queries with a good or very good performance (0.6 X 0.8 and 0.8 X 1.0), our re-ranking method could not bring more relevant documents to the fore because the search engine X  X  results were either already as good as possible (0.8 for the interval 0.6 X 0.8, or 1.0 for the interval 0.8 X 1.0), or very close to this. The results can hence overcome the baseline only by chance.
All the comparisons are statistically significant, with the p -values &lt; 10 and Baseline vs. Average Result , respectively). The conclusions for P@5 are also consistent with P@10. The good results for P@10 can be explained by the fact that there is a higher chance of obtaining new relevant documents in a list of 10 documents than in a list of 5. However, for P@30, the improvements were not as significant as for the other top levels of precision. In order to improve the performance in a list of 30 retrieved documents it would be necessary to bring more than 1 or 2 new relevant documents from the re-ranking process. (For P@5, 1 new relevant document represents a 20% improvement, while for P@30, 1 new relevant document represents only a 3.33% improvement).

In Table 5 we present the peak (the best results) and average improvements for each baseline precision interval in the case of P@5 and P@10 respectively. 6.2. Detailed results after taking into account the number of ambiguous terms
The queries from the data set utilized in this study contain from 0 to 4 ambiguous terms (see Table 1 ). A high number of ambiguous terms also suggests an increased level of query difficulty caused by multiple possible combinations of senses between terms. Keeping this aspect in mind, we investigated the behavior of our method over clusters of queries classified by the number of ambiguous terms. We proceeded as in Section 6.1 (independently for each collection) by grouping all three data sets into a single one. All of the 104 ambiguous queries were divided into three classes: queries that contain 1 ambig-uous term, 2 ambiguous terms and 3 ambiguous terms respectively. Our method was not applied to the queries that con-tained no ambiguous terms (see Section 5.2 ). The population for the cluster corresponding to queries with 4 ambiguous terms was very weak (only one query) and therefore was also not taken into account.
 The peak results and the percentages of improvements for each cluster, by the top levels of precision, are presented in Table 6 .

The highest values were obtained for the clusters of queries containing 3 ambiguous terms, which suggests that our method best improves the most ambiguous queries. For P@30 the improvement was almost 8 % . The results are statistically significant with p-values &lt; 10 6 ( T -Test for columns Baseline vs. Peak Res. ). It is also worth mentioning that constant improvements were also obtained for the other two clusters being investigated. 7. The spectral clustering method using automatically generated context
The method we propose in this paper uses all the three parts of the TREC topics, title, description and narrative (TDN), as disambiguation context. TDN implies the assumption that a context exists for the query, which is not the case in real world applications. For this reason, in this section we automatically build a context in order to validate our approach. This auto-matic context is not optimal (weaker performance than for TDN) and it is not optimized since our point was only to validate that our method still works with automatic context. We present the automatic contextualization method and we discuss the obtained results. 7.1. Automatic contextualization using pseudo relevance feedback
We chose a straightforward pseudo relevance feedback (PRF) approach in order to obtain the context ( Attar &amp; Fraenkel, that the first retrieved documents have high chances to be relevant and thus they presumably contain the target words with the correct sense.

First of all, we run retrieval on the initial query (title part of the TREC topic) over the TREC document collections and we query expansion models ( He &amp; Ounis, 2009 ) based on the assumption that, when taking into account more than five docu-ments, the probability of treating irrelevant documents increases. Having these top documents, we concatenate the texts, we remove the stopwords and we search for the presence of at least two query terms in a moving context window of 50 words. If this presence occurs, we keep the text in the context window and add it to our context. The search for at least two query terms together is motivated by the assumption that two ambiguous words tend to disambiguate each other when found together, for example  X  X  X ava X  X  and  X  X  X sland X  X  ( Andrews, Pane, &amp; Zaihrayeu, 2011 ).
 External sources such as Wikipedia were avoided when building the context due to differences in terms of actuality.
Moreover, the relevance judgments were constructed considering the information in the description and narrative parts of the topic, suggesting some kind of a closed circuit. For instance, supposing that we have obtained a context with senses for target words different than the senses suggested for pooling, this would lead the evaluation of the disambiguation pro-cess to complete failure.

The usage of our PRF-based context and insights regarding the performance are presented in the following subsection. 7.2. Experiments and results
In order to prove the effectiveness of our method in the case of automatically generated context we created four TREC runs, as follows: Title : retrieved documents when the query represents only the title part of the topic;
Title + Context : retrieved documents when the query represents the title part of the topic, together with the automati-cally built context;
Spectral-Title : the re-ranked documents after applying the spectral clustering method, when the query is represented only by the title part of the topic;
Spectral-Title + Context : the re-ranked documents after using the automatic context as WSD context for the spectral clustering method.

For few queries in each collection, our method was not able to provide any context either due to a title part of the TREC topic formed only by one term, or due to the complete nonexistence of co-occurrences of at least two terms in the context window, in the retained text. Hence, we considered only the queries containing ambiguous terms and for which the auto-matic method was able to provide a context, as follows: 29 out of 35 ambiguous queries in TREC7, 35 out of 35 ambiguous queries in TREC8 and 28 out of 32 ambiguous queries in WT10G, respectively.

Tables 7 X 9 provide precision values at 5, 10 and 30 retrieved documents after evaluating the above mentioned runs, for each collection. For comparison we recall the results obtained using the reformulated TD(N) runs (from Section 5 ). We men-tion that the queries without automatic context were also removed from the TD(N) evaluations, in order to maintain the the basic Title run and Spectral-Title + Context ) is also marked with asterisks in the tables ( p -value &lt; 10 In terms of top level precision, the Spectral-Title + context run is better than the Title run, which is better than the
Title + Context run. This suggests that the generated context is harmful for the retrieval process itself but beneficial for the spectral clustering method ( Spectral-Title + Context run is generally better than Spectral-Title ). We believe that this is due to the amount of  X  X  X oisy X  X  terms in the context. Unlike the feature selection process using a Na X ve Bayes technique less of a problem than for the retrieval process.

We notice that in 89 % of cases the Spectral-Title + Context run has the greatest performance. Even if the relative improvement (0.6 X 6.5%) is not very high, this improvement allows us to state that our method remains effective even with automatic contextualization. In addition, the results we obtained in Section 5 show that using a better context would improve the results even more.
 lowest performance among all three considered collections, therefore the context quality decreases, since the P@3 is rela-tively low (TREC7: 0.5977, TREC8: 0.5619, WT10G: 0.3810). Having a poor context implies a less performing WSD process. 8. Conclusions
This paper presents a re-ranking method for IR. It shows a remarkable improvement in high rank precision for ambiguous ure in the case of this particular type of queries ( Stokoe et al., 2003; Mothe &amp; Tanguy, 2007 ).

Several previous studies ( Sanderson, 1994; Guyot et al., 2008 ) have failed to prove the usefulness of WSD in IR. On the contrary, we show that unsupervised WSD, namely WS discrimination, can improve IR results. We are of the opinion that
WS discrimination is sufficient in IR and that WS disambiguation is not compulsory, as opposed to text translation, for exam-when using a Na X ve Bayes-based clustering technique, Chifu and Ionescu (2012) also demonstrated that WS discrimination can improve IR performance. However, in their work they only recorded very small improvement and only on some sub-cases, hence the importance of the clustering technique used for WS discrimination in IR, another point which we have made here.

Our method exploits TREC topic definitions which have a level of detail in their description, a level which is not normally ever, using the complete statement of the topic could lead to valid criticism of experiments such as ours because we exploit eficial way to improve retrieval effectiveness, and we have achieved this goal. However, even if our goal was not to develop mechanisms which can capture in an optimal way the needed level of detail, we do propose a method to capture the context of the query and show that our own method for WS discrimination in IR remains useful. Such a method of contextualization, namely the usage of PRF ( Buckley et al., 1994 ), has been employed by us in Section 7 for validating our conclusions in the presence of automatically generated context. Indeed, contextualizing short texts, such as tweet contextualization ( SanJuan, we think it will be worth considering new contextualization techniques in our WS discrimination method as a future goal. Our future work will also concentrate on query difficulty prediction, which is already an active research area ( Carmel &amp; improves poor performing queries (Section 6.1 ), especially those with multiple ambiguous terms (Section 6.2 ), should drive in-depth research along this path.
 Acknowledgements The authors would like express their gratitude to Taoufiq Dkaki from the University of Toulouse and Radu Ionescu from
University of Bucharest for their useful comments and discussions, as well as to the ANR agency who partially funded this work.
 References
