 Statistical NLP models usually rely on hand-designed features, customized for each task. These features typically combine lexical and contextual in-formation with the label to be scored. In relation extraction, for example, there is a parameter for the presence of a specific relation occurring with a fea-ture conjoining a word type (lexical) with depen-dency path information (contextual). In measur-ing phrase semantic similarity, a word type is con-joined with its position in the phrase to signal its role. Figure 1b shows an example in dependency parsing, where multiple types (words) are conjoined with POS tags or distance information.
To avoid model over-fitting that often results from features with lexical components, several smoothed lexical representations have been proposed and shown to improve performance on various NLP tasks; for instance, word embeddings (Bengio et al., 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014).

However, using only word embeddings is not suf-ficient to represent complex lexical features (e.g.  X  in Figure 1c). In these features, the same word em-bedding conjoined with different non-lexical prop-erties may result in features indicating different la-bels; the corresponding lexical feature representa-tions should take the above interactions into consid-eration. Such important interactions also increase the risk of over-fitting as feature space grows ex-ponentially, yet how to capture these interactions in representation learning remains an open question. general and unified approach to reduce the feature space by constructing low-dimensional feature rep-resentations, which provides a new way of combin-ing word embeddings, traditional non-lexical prop-erties, and label information. Our model exploits the inner structure of features by breaking the fea-ture into multiple parts: lexical, non-lexical and (op-tional) label. We demonstrate that the full feature is an outer product among these parts. Thus, a param-eter tensor scores each feature to produce a predic-tion. Our model then reduces the number of param-the same sizes; and define element-wise (Hadamard) multiplication a  X  b between vectors with the same sizes.
 Tucker Decomposition: Tucker Decomposition represents a d 1  X  d 2  X  ...  X  d K tensor T as: where each  X  i is the tensor i -mode product and each W i is a r i  X  d i matrix. Tensor g with size r 1  X  r 2  X  ...  X  r K is called the core tensor . We say that T has a Tucker rank ( r (1) ,r (2) ,...,r ( K ) ) , where r ( i ) = rank ( T ( i ) ) is the rank of mode-i un-folding. To simplify learning, we define the Tucker rank as r ( i ) = rank( g ( i ) ), which can be bounded sim-ply by the dimensions of g , i.e. r ( i )  X  r i ; this allows us to enforce a rank constraint on T simply by re-stricting the dimensions r i of g , as described in  X  6. CP Decomposition: CP decomposition represents a d 1  X  d 2  X  ...  X  d K tensor T as a sum of rank-one tensors (i.e. a sum of outer products of K vectors):
T = where each W i is an r  X  d i matrix and W i [ j, :] is the vector of its j -th row. For CP decomposition, the rank r of a tensor T is defined to be the number of rank-one tensors in the decomposition. CP decom-position can be viewed as a special case of Tucker decomposition in which r 1 = r 2 = ... = r K = r and g is a superdiagonal tensor. Suppose we have feature  X  that includes information from a label y, multiple lexical items w 1 ,..., w and non-lexical property u. This feature can be fac-torized as a conjunction of each part:  X  = y  X  u  X  w 1  X  ...  X  w n . The feature fires when all ( n +2) parts fire in the instance (reflected by the  X  symbol in  X  ). The one-hot representation of  X  can then be viewed as a tensor e  X  = y  X  u  X  w 1  X  X  X  X  X  X  w n , where each Figure 1d illustrates this case with two lexical parts.
Given an input instance x and its associated la-bel y, we can extract a set of features S ( x , y ) . In a traditional log-linear model, we view the instance x as a bag-of-features, i.e. a feature vector F ( x , y ) . Each dimension corresponds to a feature  X  , and has value 1 if  X   X  S ( x , y ) . Then the log-linear model scores the instance as s ( x , y ; w ) = w T F ( x , y ) = P tor. We can re-write s ( x , y ; w ) based on the factor-ization of the features using tensor multiplication; in which w becomes a parameter tensor T : Here each  X  has the form ( y , u , w 1 ,..., w n ) , and
Note that one-hot vectors w i of words themselves are large ( | w i | &gt; 500k), thus the above formulation with parameter tensor T can be very large, making parameter estimation difficult. Instead of estimating only the values of the dimensions which appear in training data as in traditional methods, we will re-duce the size of tensor T via a low-rank approxima-tion. With different approximation methods, (4) will have different equivalent forms, e.g. (6), (7) in  X  4.1. Optimization objective: The loss function ` for training the log-linear model uses (3) for scores, e.g., Learning can be formulated as the following opti-mization problem: where the constraints on rank( T ) depend on the cho-sen tensor approximation method (  X  2).

The above framework has some advantages: First, as discussed in  X  1 and here, we hope the represen-tations capture rich interactions between different parts of the lexical features; the low-rank tensor ap-proximation methods keep the most important inter-action information of the original tensor, while sig-nificantly reducing its size. Second, the low-rank structure will encourage weight-sharing among lex-ical features with similar decomposed parts, leading to better model generalization. Note that there are examples where features have different numbers of multiple lexical parts, such as both unigram and bi-gram features in PP-attachment. We will use two different methods to handle these features (  X  5). Remarks (advantages of our factorization) Compared to prior work, e.g. (Lei et al., 2014; Lei et al., 2015), the proposed factorization has the following advantages: 1. Parameter explosion when mapping a view 2. No weight-sharing among conjunctions with The above advantages are also key to overcome the problems of prior work mentioned at the end of  X  1. Using one-hot encodings for each of the parts of fea-ture  X  results in a very large tensor. This section shows how to compute the score in (4) without con-structing the full feature tensor using two tensor ap-proximation methods (  X  4.1 and  X  4.2).

We begin with some intuition. To score the orig-inal (full rank) tensor representation of  X  , we need a parameter tensor T of size d 1  X  d 2  X  ...  X  d n +2 , where d 3 =  X  X  X  = d n +2 = | V | is the vocabulary size, n is the number of lexical parts in the feature and d 1 = | L | and d 2 = | F | are the number of different labels and non-lexical properties, respec-tively. (  X  5 will handle n varying across features.) Our methods reduce the tensor size by embedding each part of  X  into a lower dimensional space, where we represent each label, non-lexical property and words with an r 1 ,r 2 , r 3 ,...,r n +2 dimensional vec-tor respectively ( r i d i ,  X  i ). These embedded features can then be scored by much smaller ten-sors. We denote the above transformations as ma-for i = 1 ,...,n , and write corresponding low-dimensional hidden representations as h ( l ) y = W l y, h u = W f u and h
In our methods, the above transformations of em-beddings are parts of low-rank tensors as in (5), so the embeddings of non-lexical properties and la-bels can be trained simultaneously with the low-rank tensors. Note that for one-hot input encodings the transformation matrices are essentially lookup ta-bles, making the computation of these transforma-tions sufficiently fast. 4.1 Tucker Form For our first approximation, we assume that tensor T has a low-rank Tucker decomposition: T = g  X  l W express the scoring function (4) for a feature  X  = ( y , u , w 1 ,... w n ) with n -lexical parts, as: s ( y , u , w 1 ,  X  X  X  , w n ; g, W l , W f , { W i } n i =1 which amounts to first projecting u , y, and w i (for all i ) to lower dimensional vectors h ( f ) u , h ( l ) and then weighting these hidden representations us-ing the flattened core tensor g . The low-dimensional representations and the corresponding weights are learned jointly using a discriminative (supervised) criterion. We call the model based on this repre-sentation the Low-Rank Feature Representation with Tucker form , or LRFR n -TUCKER . 4.2 CP Form For the Tucker approximation the number of param-eters in (6) scale exponentially with the number of lexical parts. For instance, suppose each h ( i ) w mensionality r , then | g |  X  r n . To address scalabil-ity and further control the complexity of our tensor based model, we approximate the parameter tensor using CP decomposition as in (2), resulting in the following scoring function: s ( y , u , w 1 ,  X  X  X  , w n ; W l , W f , { W i } n i =1 ) = We call this model Low-Rank Feature Representa-tion with CP form ( LRFR n -CP ). 4.3 Pre-trained Word Embeddings One of the computational and statistical bottlenecks in learning these LRFR n models is the vocabulary size; the number of parameters to learn in each ma-trix W i scales linearly with | V | and would require very large sets of labeled training data. To alle-viate this problem, we use pre-trained continuous word embeddings (Mikolov et al., 2013) as input embeddings rather than the one-hot word encodings. We denote the m -dimensional word embeddings by e ; so the transformation matrices W i for the lexical parts are of size r i  X  m where m | V | .

We note that when sufficiently large labeled data is available, our model allows for fine-tuning the pre-trained word embeddings to improve the expres-sive strength of the model, as is common with deep network models.
 Remarks Our LRFR s introduce embeddings for non-lexical properties and labels, making them bet-ter suit the common setting in NLP: rich linguistic properties; and large label sets such as open-domain tasks (Hoffmann et al., 2010). The LRFR -CP better suits n -gram features, since when n increases 1, the only new parameters are the corresponding W i . It is also very efficient during prediction ( O ( nr ) ), since the cost of transformations can be ignored with the help of look-up tables and pre-computing. For features with n lexical parts, we can train an LRFR n model to obtain their representations. How-ever, we often have features of varying n (e.g. both unigrams ( n =1) and bigrams ( n =2) as in Figure 1). We require representations for features with arbi-trary different n simultaneously.

We propose two solutions. The first is a straight-forward solution based on our framework, which handles each n with a ( n +2) -way tensor. This strat-egy is commonly used in NLP, e.g. Taub-Tabib et al. (2015) have different kernel functions for differ-ent order of dependency features. The second is an approximation method which aims to use a single tensor to handle all n s.
 Multiple Low-Rank Tensors Suppose that we can divide the feature set S ( x , y ) into subsets S ( x , y ) ,S 2 ( x , y ) ,...,S n ( x , y ) which correspond to features with one lexical part (unigram features), two lexical parts (bigram features) ,... and n lexi-cal parts ( n -gram features), respectively. To handle these types of features, we modify the training ob-jective as follows: where the score of a training instance ( x , y ) is de-fined as s ( x , y ; T ) = use the Tucker form low-rank tensor for T 1 , and the CP form for T i (  X  i &gt; 1) . We refer to this method as Word Clusters Alternatively, to handle different numbers of lexical parts, we replace some lexical parts with discrete word clusters. Let c ( w ) denote the word cluster (e.g. from Brown clustering) for word w. For bigram features we have: s ( y , u , w 1 , w 2 ; T ) = s ( y , u  X  c ( w 1 ) , w 2 ; T ) + s ( y , u  X  c ( w 2 = T  X  l y  X  f ( u  X  c ( w 1 ))  X  w e w where for each word we have introduced an addi-tional set of non-lexical properties that are conjunc-tions of word clusters and the original non-lexical properties. This allows us to reduce an n -gram feature representation to a unigram representation. The advantage of this method is that it uses a sin-gle low-rank tensor to score features with different numbers of lexical parts. This is particularly helpful when we have very limited labeled data. We denote this method as LRFR 1 -B ROWN , since we use Brown clusters in practice. In the experiments we use the Tucker form for LRFR 1 -B ROWN . The goal of learning is to find a tensor T that solves problem (5). Note that this is a non-convex objec-tive, so compared to the convex objective in a tradi-tional log-linear model, we are trading better fea-ture representations with the cost of a harder op-timization problem. While stochastic gradient de-scent (SGD) is a natural choice for learning rep-resentations in large data settings, problem (5) in-volves rank constraints, which require an expensive proximal operation to enforce the constraints at each iteration of SGD. We seek a more efficient learning algorithm. Note that we fixed the size of each trans-formation matrix W i  X  R r i  X  d i so that the smaller dimension ( r i &lt; d i ) matches the upper bound on the rank. Therefore, the rank constants are always sat-isfied through a run of SGD and we in essence have an unconstrained optimization problem. Note that in this way we do not guarantee orthogonality and full-rank of the learned transformation matrices. These properties are assumed in general, but are not neces-sary according to (Kolda and Bader, 2009).
 The gradients are computed via the chain-rule. We use AdaGrad (Duchi et al., 2011) and apply L2 regularization on all W i s and g , except for the case of r i = d i , where we will start with W i = I and reg-ularize with k W i -I k 2 . We use early-stopping on a development set. We evaluate LRFR on three tasks: relation extraction, PP attachment and preposition disambiguation (see Table 1 for a task summary). We include detailed feature templates in Table 2.

PP-attachment and relation extraction are two fundamental NLP tasks, and we test our models on the largest English data sets. The preposition disam-biguation task was designed for compositional se-mantics, which is an important application of deep learning and distributed representations. On all these tasks, we compare to the state-of-the-art.
We use the same word embeddings in Belinkov et al. (2014) on PP-attachment for a fair comparison. For the other experiments, we use the same 200-d word embeddings in Yu et al. (2015).
 Relation Extraction We use the English portion of the ACE 2005 relation extraction dataset (Walker et al., 2006). Following Yu et al. (2015), we use both gold entity spans and types, train the model on the news domain and test on the broadcast conversation domain. To highlight the impact of training data size we evaluate with all 43,518 relations (entity mention pairs) and a reduced training set of the first 10,000 relations. We report precision, recall, and F1.
We compare to two baseline methods: 1) a log-linear model with a rich binary feature set from Sun et al. (2011) and Zhou et al. (2005) as described in Yu et al. (2015) (B ASELINE ); 2) the embedding model ( FCM ) of Gormley et al. (2015), which uses rich linguistic features for relation extraction. We use the same feature templates and evaluate on fine-grained relations (sub-types, 32 labels) (Yu et al., 2015). This will evaluate how LRFR can utilize non-lexical linguistic features.
 PP-attachment We consider the prepositional phrase (PP) attachment task of Belinkov et al. or nouns) must be selected from content words be-fore the PP (within a 10-word window). We formu-late the task as a ranking problem, where we opti-mize the score of the correct head from a list of can-didates with varying sizes.

PP-attachment suffers from data sparsity because of bi-lexical features, which we will model with methods in  X  5. Belikov et al. show that rich fea-tures  X  POS, WordNet and VerbNet  X  help this task. The combination of these features give a large num-ber of non-lexical properties, for which embeddings of non-lexical properties in LRFR should be useful.
We extract a dev set from section 22 of the PTB following the description in Belinkov et al. (2014). Preposition Disambiguation We consider the preposition disambiguation task proposed by Ritter et al. (2014). The task is to determine the spatial re-lationship a preposition indicates based on the two objects connected by the preposition. For example,  X  X he apple on the refrigerator X  indicates the  X  X upport by Horizontal Surface X  relation, while  X  X he apple on the branch X  indicates the  X  X upport from Above X  re-lation. Since the meaning of a preposition depends on the combination of both its head and child word, we expect conjunctions between these word embed-dings to help, i.e. features with two lexical parts.
We include three baselines: point-wise addition (SUM) (Mitchell and Lapata, 2010), concatena-tion (Ritter et al., 2014), and an SVM based on hand-crafted features in Table 2. Ritter et al. show that the first two methods beat other compositional models. Hyperparameters are all tuned on the dev set. The chosen values are learning rate  X  = 0 . 05 and the weight of L2 regularizer  X  = 0 . 005 for LRFR , except for the third LRFR in Table 3 which has  X  = 0 . 05 . We select the rank of LRFR -TUCKER with a grid search from the following values: r 1 = { 10 , 20 ,d 1 } , r 2 = { 20 , 50 ,d 2 } and r 3 = { 50 , 100 , 200 } . For LRFR -CP , we select r = { 50 , 100 , 200 } . For the PP-attachement task there is no r 1 since it uses a ranking model. For the Preposition Disambiguation we do not choose r 1 since the number of labels is small. Relation Extraction All LRFR -TUCKER models improve over B ASELINE and FCM (Table 3), making these the best reported numbers for this task. How-ever, LRFR -CP does not work as well on the features with only one lexical part. The Tucker-form does a better job of capturing interactions between differ-ent views. In the limited training setting, we find that LRFR -CP does best.

Additionally, the primary advantage of the CP approximation is its reduction in the number of model parameters and running time. We report each model X  X  running time for a single pass on the de-velopment set. The LRFR -CP is by far the fastest. The first three LRFR -TUCKER models are slightly slower than FCM , because they work on dense non-lexical property embeddings while FCM benefits from sparse vectors.
 PP-attachment Table 4 shows that LRFR (89.6 and 90.3) improves over the previous best stan-dalone system HPCD (88.7) by a large margin, with exactly the same resources. Belinkov et al. (2014) also reported results of parsers and parser re-rankers, which can access to additional resources (complete parses for training and complete sentences as in-puts) so it is unfair to compare them with the stan-dalone systems like HPCD and our LRFR . Nonethe-performs the state-of-the-art parser RBG (88.4), re-ranker Charniak-RS (88.6), and the combination of the state-of-the-art parser and compositional model RBG + HPCD (90.1). Thus, even with fewer re-sources, LRFR becomes the new best system.
 Not shown in the table: we also tried LRFR 1 -TUCKER &amp; LRFR 2 -CP with postag features only (89.7), and with grand-head-modifier conjunctions removed (89.3) . Note that compared to LRFR , RBG benefits from binary features, which also ex-ploit grand-head-modifier structures. Yet the above reduced models still work better than RBG (88.4) results of LRFR can still be potentially improved by combining with binary features. The above results show the advantage of our factorization method, which allows for utilizing pre-trained word embed-dings, and thus can benefit from semi-supervised learning.
 Preposition Disambiguation LRFR improves (Ta-ble 5) over the best methods (SUM and Concate-nation) in Ritter et al. (2014) as well as the SVM
Table 5: Accuracy for spatial classification of PPs. based on the original lexical features (85.1). In this task LRFR 1 -B ROWN better represents the unigram and bigram lexical features, compared to the usage of two low-rank tensors ( LRFR 1 -TUCKER &amp; LRFR 2 -). This may be because LRFR 1 -B ROWN has fewer parameters, which is better for smaller training sets. We also include a control setting ( LRFR 1 -B ROWN -Control), which has a full rank parameter ten-sor with the same inputs on each view as LRFR 1 -B
ROWN , but represented as one hot vectors without transforming to the hidden representations h s. This is equivalent to an SVM with the compound cluster features as in Koo et al. (2008). It performs much worse than LRFR 1 -B ROWN , showing the advantage of using word embeddings and low-rank tensors. Summary For unigram lexical features, LRFR n -TUCKER achieves better results than LRFR n -CP . However, in settings with fewer training examples, features with more lexical parts ( n -grams), or when faster predictions are advantageous, LRFR n -CP does best as it has fewer parameters to estimate. For n -grams of variable length, LRFR 1 -TUCKER &amp; LRFR 2 -CP does best. In settings with fewer training exam-ples, LRFR 1 -B ROWN does best as it has only one parameter tensor to estimate. Dimensionality Reduction for Complex Features is a standard technique to address high-dimensional features, including PCA, alternating structural op-timization (Ando and Zhang, 2005), denoising au-toencoders (Vincent et al., 2008), and feature em-beddings (Yang and Eisenstein, 2015). These meth-ods treat features as atomic elements and ignore the inner structure of features, so they learn separate em-bedding for each feature without shared parameters. As a result, they still suffer from large parameter
Another line of research studies the inner struc-tures of lexical features: e.g. Koo et al. (2008), Turian et al. (2010), Sun et al. (2011), Nguyen and Grishman (2014), Roth and Woodsend (2014), and Hermann et al. (2014) used pre-trained word embed-dings to replace the lexical parts of features ; Sriku-mar and Manning (2014), Gormley et al. (2015) and Yu et al. (2015) propose splitting lexical fea-tures into different parts and employing tensors to perform classification. The above can therefore be seen as special cases of our model that only embed a certain part (view) of the complex features. This restriction also makes their model parameters form a full rank tensor, resulting in data sparsity and high computational costs when the tensors are large. Composition Models (Deep Learning) build rep-resentations for structures based on their component word embeddings (Collobert et al., 2011; Bordes et al., 2012; Socher et al., 2012; Socher et al., 2013b). When using only word embeddings, these models achieved successes on several NLP tasks, but some-times fail to learn useful syntactic or semantic pat-terns beyond the strength of combinations of word embeddings, such as the dependency relation in Fig-ure 1(a). To tackle this problem, some work de-signed their model structures according to a specific kind of linguistic patterns, e.g. dependency paths (Ma et al., 2015; Liu et al., 2015), while a recent trend enhances compositional models with linguis-tic features. For example, Belinkov et al. (2014) concatenate embeddings with linguistic features be-fore feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) en-hanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.g. phrase types). These models are similar to ours in the sense of learning representations based on lin-guistic features and embeddings.
 Low-rank Tensor Models for NLP aim to handle the conjunction among different views of features (Cao and Khudanpur, 2014; Lei et al., 2014; Chen and Manning, 2014). Yu and Dredze (2015) pro-posed a model to compose phrase embeddings from words, which has an equivalent form of our CP-based method under certain restrictions. Our work applies a similar idea to exploiting the inner struc-ture of complex features, and can handle n -gram features with different n s. Our factorization (  X  3) is general and easy to adapt to new tasks. More impor-tantly, it makes the model benefit from pre-trained word embeddings as shown by the PP-attachment results. We have presented LRFR , a feature representation model that exploits the inner structure of complex lexical features and applies a low-rank tensor to effi-ciently score features with this representation. LRFR attains the state-of-the-art on several tasks, includ-ing relation extraction, PP-attachment, and preposi-tion disambiguation. We make our implementation Acknowledgements A major portion of this work was done when MY was visiting MD and RA at JHU. This research was supported in part by NSF grant IIS-1546482.
