 The main g oal of the TREC Session track search task is to improve the rankin g of a c u rrent q u ery g iven information abo u t the previo u s interactions of a u ser in the same session. To that end, TREC Session track participants are provided with a set of q u ery sessions, each of which contains a c u rrent q u ery, one or more previo u s q u eries and a rankin g list of doc u ments (doc u ment Id, title, URL, snippets, clicked URLs and dwell time) for the previo u s q u eries [13,14]. P articipants are asked to exploit that informa-tion to better rank Cl u eWeb09 res u lts for the last q u ery in the session. 
We address the problem u sin g CombMNZ and two other rank a gg re g ation me-thods. Rank a gg re g ation is concerned with combinin g the res u lts of vario u s rankin g s. Here the vario u s rankin g s that we combine are the rankin g s for q u eries that we con-information as related q u eries, and combinin g the rankin g s of the related q u eries u s-in g either one of the rank a gg re g ation methods that we u sed, we are able to g et si g nif-icant improvements ( u p to 46% increase in some cases) over the baseline, especially when we exploit u sers  X  search history (i.e. session data). Improvements of the res u lts are si g nificant for both traditional meas u res and meas u res of diversity. Session search is an important problem as it has been shown by Shoko u hi et al, after analysis of Bin g and Yandex search lo g s, that between 40% and 60% of sessions have two or more q u eries [19]. Q u ery reform u lations may be d u e to the diffic u lty, the am-bi gu ity, the complexity or the broadness of a topic. Zhan g et al. approached the problem by proposin g a relevance feedback model relevance score between a c u rrent q u ery and a doc u ment, terms  X  wei g hts are increased or decreased dependin g on whether the term was added, retained or removed from model sessions as Markov Decision P rocesses wherein the u ser a g ent  X  s actions cor-respond to q u ery chan g es, and the search a g ent  X  s actions correspond to increasin g or decreasin g or maintainin g term wei g hts. O u r work is similar to these two in the sense that we exploit the different form u lations of the q u ery, and in some instances we u se relevance feedback. B u t o u r work differs in many points. First, in addition to exploit-from previo u s rankin g lists in a session. Secondly, we u se those pieces of information in a different way, namely by considerin g them as related q u eries and a gg re g atin g over their res u lts. 
In another st u dy closely related to o u rs, Raman et al. u se related q u eries to diversify res u lts while maintainin g cohesion with the c u rrent q u ery in order to satisfy the c u rrent where the u ser  X  s interaction in the first level is u sed to infer her intent in order to pro-vide a better rankin g that incl u des second level rankin g s. The commonality between covera g e of vario u s aspects), we do the same, albeit we do so implicitly by relyin g on the a gg re g ation al g orithm to prefer doc u ments that appeared in most previo u s rankin g lists  X  and th u s likely cover the most aspects. However, while we u se previo u s session interactions to aid the performance for the c u rrent q u ery, Raman et al. u se a q u ery to rankin g u sin g an objective f u nction that optimizes the selection of doc u ments and q u e-q u eries and a gg re g ate over their res u lts to select the best doc u ments. 
In yet another closely related work, G u an identifies text n ugg ets deemed interest-Notable approaches u sed by TREC participants incl u de the work of [12] in which the a u thors combined Seq u ential Dependence Model feat u res in both c u rrent q u eries and previo u s q u eries in the session for one system, and combined that method with pse u -do-relevance feedback for other systems. Another notable approach is the u se of anc-hor texts for q u ery expansion proposed by [15] and adopted by others. tional ad-hoc retrieval tasks [2]. B u t, to the best of o u r knowled g e, there hasn  X  t been any previo u s attempt like o u rs to adapt s u ch techniq u es to the context of session search. The int u ition behind u sin g u sers  X  session data is that u sers may be reform u latin g their q u eries u sin g pieces of information that were displayed to them d u rin g their previo u s more likely to hold added terms that are u sed in q u ery reform u lations, we inspect some so u rces of terms  X  snippets, titles and key-phrases  X  from doc u ments in the rankin g lists for previo u s q u eries. Key-phrases were obtained u sin g AlchemyA P I [1] and Xtrak4Me [18]. The res u lts shown in Table 1 s ugg est that in most cases, the new terms added by u sers in their reform u lations come from snippets and key-phrases of doc u ments from the rankin g lists of previo u s q u eries. For instance, in the 2012 data-set, 41% of q u eries u se new terms from previo u s interactions  X  snippets. This s ugg ests formation need, if u sed as q u eries. 
Armed with that findin g , we conject u re that we can u se terms from those u sef u l so u rces in order to improve relevance for the session. Given the exploratory nat u re of many session q u eries, we f u rther conject u re that we can achieve g ood res u lts u sin g a method that promotes doc u ments that cover a myriad of relevant aspects. Th u s, we terms and that prefer doc u ments that appeared in most previo u s rankin g lists. There are two components to o u r approach. First, we g enerate and select related q u e-those q u eries. 4.1 Collecting Related Queries Baselines: Language Model (LM) and Pseudo-Relevance Feedback The first baseline we are comparin g o u r res u lts to is obtained u sin g Indri  X  s lan gu a g e model (LM) on each of the c u rrent q u ery [20]. We filter baseline res u lts and all other res u lts u sin g Waterloo  X  s f u sion spam classifier with a threshold of 0.75 [7]. 
For the second baseline, we u se an adaptation of Lavrenko  X  s pse u do-rele van ce n u mber of doc u ments u sed in the feedback: 10, 50, 100, 200, 300, 400 and 500. For each of the two datasets, we select the r u n with the maxim u m nDCG@10 as o u r second baseline. For 2011, the best feedback size is 50 doc u ments with 0.3339 for nDCG@10 (and 0.2785 for ERR@10). For 2012, the best size is also 50 with 0.2317 for nDCG@10 (and 0.1626 for ERR@10). Using Session-Dependent Data The int u ition behind u sin g u sers  X  session data is that u sers may be reform u latin g their q u eries u sin g pieces of information that were displayed to them d u rin g their previo u s abo u t the u ser  X  s act u al intent, context or topic. We u se pieces of information s u ch as a u ser  X  s own previo u s q u eries in the same session, doc u ment titles, doc u ment snippets (and important key-phrases) that appeared d u rin g a u ser  X  s previo u s interactions in the same session. Specifically, 1. The previo u s q u eries in the u ser's session; 2. The titles of doc u ments ranked for previo u s q u eries in the u ser's session; 3. Entire snippets (free of stop words) for the top 10 doc u ments ranked for previo u s q u eries in the u ser's session; 4. Key phrases extracted from top-10 ranked doc u ments for previo u s q u eries in the u ser's session. In this case we concatenate the top 5 key-phrases from each doc u -ment into a sin g le q u ery; 
Key-phrases were extracted u sin g a model that exploits lin gu istic and statistical me-thods. The method u ses statistical lexical analysis to determine the most si g nificant sin g le-word terms, and extracts those terms as well as their immediate context to form complex terms  X  u sin g no u n-ch u nks where applicable or n-g rams. Then it proceeds by cl u sterin g similar complex terms  X  u sin g Mon g e-Elkan distance as the strin g -similarity meas u re  X  and selectin g a representative for each cl u ster to be a candidate key-phrase. For selectin g a representative, a similarity maximization al g orithm is u sed that prefers the key-phrase that resembles the remainin g key-phrases most closely. Finally, all the candidates are analyzed in order to determine confidence scores for each in the context of the doc u ment in q u estion. The confidence scores are obtained by combinin g the si g nificance of c u e tokens in the representin g candidate, the scope, as determined by the distrib u tion of the candidate cl u ster over the doc u ment, and n u mber of words contained in the candidate. 
Xtrak4Me [17] is an open-so u rce library that performs this. We also u se Alche-traction al g orithms. And g iven the similarly g ood performances achieved by both the prod u ction-ready method and the academic open-so u rce method, it is fair to concl u de that decent key-phrase extraction al g orithms can provide u s with g ood eno ug h key-phrases to be u sed as candidate related q u eries for the p u rpose of o u r experiment. 
We s u bmit the previo u s q u eries, titles, snippets, or keyphrases as q u eries to an In-dri index of Cl u eWeb09 and a gg re g ate res u lts u sin g the methods described below. Examples of related q u eries obtained from some doc u ments in interaction 1 of session 7 in the 2011 dataset can be seen in Table 2 for Xtrak4Me method. The u ser  X  s q u ery for that interaction was  X  X osmetic laser treatment X . Using Bing X  X  Related Queries In this method, we u se Bin g to obtain related q u eries for each c u rrent q u ery. We pro-vide q u eries u sin g Bin g A P I [3] and obtain related q u eries in response by the service. We s u bmit them as q u eries and a gg re g ate the res u lts like in the previo u s case. Using a combination of Bing related queries and session-dependent data Here o u r aim is to observe what happens when we add Bin g related q u eries to the set of related q u eries obtained from each so u rce of session-dependent data. 4.2 Aggregation Algorithms P revio u s research has shown that combinin g the evidence of m u ltiple q u ery reform u -lations helps improve res u lts in information retrieval [2]. Two s u ch methods are CombSUM and CombMNZ. For each q u ery, CombSUM reranks doc u ments based on their c u m u lative scores over all related q u eries. CombMNZ, on the other hand, u ses the s u m of similarity val u es times the n u mber of non-zero similarity val u es [16]. Ad-ditionally, we propose another method, CombCAT with the p u rpose of experimentin g what happens when we g ive precedence to doc u ments that appear the most in o u r rankin g s. In CombCAT, for each q u ery, we first g ro u p doc u ments into different cate-ments that have the lar g est s u m take priority over others. Both CombMNZ and CombCAT explicitly reward doc u ments that appear in the lar g est n u mber of rankin g s  X  tho ug h in different ways. B u t since CombMNZ has been deeply analyzed in the a gg re g ation techniq u es. Most CombCAT and CombSUM res u lts are left o u t d u e to space limitation. However, Fi g . 1 and Fi g . 2 show nDCG@10 and ERR@10 res u lts for each of the three a gg re g ation methods on the 2012 dataset. 5.1 Data For o u r experiments, we u se the TREC 2011 and 2012 Session track datasets [13,14]. In each, there are several sessions containin g one or many interactions. The 2011 data-set contains 76 sessions while the 2012 dataset contains 98 sessions. A session is a seq u ence of q u ery reform u lations and interactions made by a u ser in order to satisfy an that led to the c u rrent q u ery. In o u r datasets, an interaction more specifically consists of snippets (prod u ced by Yahoo! BOSS) for each doc u ment. Also incl u ded are clicked doc u ments and the time spent by the u ser readin g a clicked doc u ment. Note t ha t o n l y doc um e n ts t ha t exist i n Cl u eWeb09 w ere ret a i n ed.
 session u sin g their systems. The corp u s on which the retrieval is performed for the 2011 and 2012 dataset is the cate g ory B s u bset of Cl u eWeb09. This was the same (s u b)set u sed by the top TREC Session track systems. 
The 2011 dataset incl u des s u btopic j u d g ments, makin g it possible to comp u te di-versity meas u res for the 2011 dataset. 5.2 Evaluation Measures For traditional meas u res, we opted for u sin g two of the meas u res adopted by the TREC Session track or g anizers, namely the primary meas u re nDCG@10 and the second meas-u re ERR@10 [5], [11]. Given that a u ser  X  s information thro ug ho u t a session mi g ht span several aspects of a topic, we deemed it necessary to u se diversity meas u res to eval u ate o u r methods. We u sed  X  -nDCG@10 and ERR-IA@10 [4], [6] as diversity meas u res. 5.3 Results In the followin g tables, B is short for Bin g , Q for Q u ery, Alch for AlchemyA P I, Xtrak for Xtrak4Me and Snip for Snippets. For brevity, we left o u t ERR@10 and ERR-IA@10 res u lts of some of o u r experiments. The best nDCG@10 is 0.432 for 2011 and 0.314 for 2012 which is on par with TREC Session track systems for 2011 and 2012. r u n of the top g ro u p and the best r u n of the second-best team (which had nDCG@10 of 0.3221 and 0.3033 respectively). Similarly, in 2011, o u r best r u n wo u ld be sq u eezed between the best r u n of the second-best g ro u p and the best r u n of the third-best team (which had nDCG@10 of 0.4409 and 0.4307 respectively, and were domi-nated by the top team which reached 0.4540). Note: In our tables, + and ++ mean statistically significant at p&lt;0.05 over the LM and pseudo-relevance feedbac k baselines respectively. baseline (19.06% nDCG@10 increase) for the 2012 dataset as shown in Table 5. The impact on the 2011 data u sin g Bin g only is more moderate (4% nDCG@10 increase). We g et better improvements when we u se session-dependent data. In partic u lar, for 2012, we g et a peak of 27.88% increase when we u se snippets as related q u eries, and for 2011, we g et a peak of 23% increase when we u se Xtrak4Me key-phrases. Combinin g Bin g X  s related q u eries to session-dependent data g ives even better res u lts. For the 2012 data, u sin g nDCG@10, we observe a peak of 42.5% increase when we u se B+Alch and a low of 26.16% increase when we u se B+Q. In fact for both 2011 and 2012 datasets, combin g Bin g and any session-dependent data prod u ces a better res u lt than u sin g that session-dependent data alone. This may be beca u se the combi-nations cover more aspects than the ran g e of aspects covered by each so u rce  X  s q u eries alone. ERR@10 res u lts follow the same trend as nDCG@10. in g that, we were able to comp u te  X  -nDCG@10 and ERR-IA@10 diversity meas u res for the 2011 dataset. The trend is similar to that of traditional meas u res, with a peak of 44.72% increase for ERR-IA@10 (and 36.98%  X  -nDCG@10 increase) when com-binin g Bin g and XtraK4Me. These res u lts were nothin g b u t expected since we are exploitin g intrinsic diversity in o u r methods. In this section we show res u lts for only some select cases simply for brevity. In g eneral, increasin g the n u mber of top doc u ments exploited from 5 to 10 ca u ses an ca u se most of the u sef u l data is in the top 5 doc u ments, and only little in the next 5. F u rther investi g ation wo u ld help determine the exact reasons. Effects of Clicked Data: A gg re g atin g u sin g data from clicked doc u ments alone h u rts the performance si g nifi-cantly. However when we incl u de q u eries taken from clicked doc u ments twice (in-improves performance. Determinin g by how m u ch the votin g ri g hts m u st be increased is left for f u t u re work. 
We f u rthered o u r analyses to show that we are not merely rearran g in g , promotin g and redisplayin g doc u ments that were shown in previo u s interactions of a g iven ses-sion. For that, we looked into doc u ment overlaps: for each session, there is only little all 76 q u eries of the 2011 session dataset, 41 q u eries ret u rned 10 top doc u ments that previo u s interactions. Also, 21 o u t of the 41 q u eries witnessed an increase of ERR@10 over the baseline while only 12 witnessed a decrease and 7 remained with the same res u lt. 25 ret u rned 1 overlappin g doc u ment o u t of 10. Only 1 q u ery ret u rned the maxim u m of 4 doc u ments o u t of 10 that overlap with previo u s interaction doc u -ments. This s ugg ests that we are not merely redisplayin g the doc u ments displayed to u sers in previo u s interactions when the session data was bein g collected. 
For the 2012 session dataset, the topics are cate g orized u nder different types de-pendin g on their search g oal (specific or amorpho u s g oal) and their prod u ct type (fac-o u r methods achieve their bi gg est improvements on the intellect u al session searches with a peak of 67.5% improvement over the baseline  X  s nDCG@10 when u sin g the combination of XtraK4Me keyphrases and Bin g X  s related q u eries (B+Xtrk). Even when u sin g session-dependent data only, we achieve a si g nificant peak of 40.72% increase of the nDCG@10. Intellect u al searches are more diffic u lt than fact u al While the dirichlet-smoothed Lan gu a g e model g ets 0.2215 for fact u al tasks and only intellect u al tasks comparatively. This may be beca u se o u r system u ncovers hidden interestin g keywords that were not obvio u s from intellect u al task topic description. However, it is noteworthy that o u r methods also g et si g nificant improvements on the other task types. 
In a related effort, we strive to determine how many tokens introd u ced by o u r me-thods are not part of the main q u ery, b u t are part of the topic description. Stop words are excl u ded from the lists of tokens. As can be seen in Fi g . 4, Snip, Xtrak and Alch methods introd u ce a sizeable n u mber of tokens that were not part of the c u rre n t-q u er y and wo u ld have hence been overlooked even tho ug h they are potentially important (since they are part of the topic description). In fact, Fi g . 3 shows that o u r method is s u ccessf u l at promotin g doc u ments that cover two or more aspects of a topic. O u t of the total 76 sessions, 35 witness Xtrak4Me doin g better than the baseline in promot-in g doc u ments that cover two or more aspects, 20 witness the inverse, and 21 witness no difference. In this paper, we showed that u sin g simple rankin g a gg re g ation methods over a g ood set of related q u eries helps improve res u lts and we show which so u rces are u sef u l for collectin g g ood related q u eries. Bin g X  s related q u eries are a g ood choice, and session-dependent data are even better. B u t we achieve even better res u lts by combinin g Bin g related q u eries to session-dependent q u eries. One possible f u t u re work idea wo u ld be to come u p with a scheme to select only the more pertinent related q u eries pertainin g to the act u al topic to perform o u r a gg re g ations in order to achieve even better perfor-mance. More efficient implementations are left for f u t u re work as well. Acknowledgments. This work was s u pported in part by the National Science Fo u ndation (NSF) u nder g rant n u mber IIS-1350799. Any opinions, findin g s and concl u sions or rec-those of the sponsor. 
