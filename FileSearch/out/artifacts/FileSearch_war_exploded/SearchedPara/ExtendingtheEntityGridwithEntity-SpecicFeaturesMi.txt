 A well-written document is coherent (Halliday and Hasan, 1976) X  it structures information so that each new piece of information is interpretable given the preceding context. Models that distinguish coherent from incoherent documents are widely used in gen-eration, summarization and text evaluation.
Among the most popular models of coherence is the entity grid (Barzilay and Lapata, 2008), a sta-tistical model based on Centering Theory (Grosz et al., 1995). The grid models the way texts focus on important entities, assigning them repeatedly to prominent syntactic roles. While the grid has been successful in a variety of applications, it is still a surprisingly unsophisticated model, and there have been few direct improvements to its simple feature set. We present an extension to the entity grid which distinguishes between different types of entity, re-sulting in signicant gains in performance 1 .
At its core, the grid model works by predicting whether an entity will appear in the next sentence (and what syntactic role it will have) given its his-tory of occurrences in the previous sentences. For instance, it estimates the probability that  X Clinton X  will be the subject of sentence 2, given that it was the subject of sentence 1. The standard grid model uses no information about the entity itself X  the prob-ability is the same whether the entity under discus-sion is  X Hillary Clinton X  or  X wheat X . Plainly, this assumption is too strong. Distinguishing important from unimportant entity types is important in coref-erence (Haghighi and Klein, 2010) and summariza-tion (Nenkova et al., 2005); our model applies the same insight to the entity grid, by adding informa-tion from syntax, a named-entity tagger and statis-tics from an external coreference corpus. Since its initial appearance (Lapata and Barzilay, 2005; Barzilay and Lapata, 2005), the entity grid has been used to perform wide variety of tasks. In addition to its rst proposed application, sentence ordering for multidocument summarization, it has proven useful for story generation (McIntyre and Lapata, 2010), readability prediction (Pitler et al., 2010; Barzilay and Lapata, 2008) and essay scor-ing (Burstein et al., 2010). It also remains a criti-cal component in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Char-niak, 2008), which typically combine it with other independently-trained models.

There have been few attempts to improve the en-tity grid directly by altering its feature representa-tion. Filippova and Strube (2007) incorporate se-mantic relatedness, but nd no signicant improve-ment over the original model. Cheung and Penn (2010) adapt the grid to German, where focused con-stituents are indicated by sentence position rather than syntactic role. The best entity grid for English text, however, is still the original. The entity grid represents a document as a matrix (Figure 1) with a row for each sentence and a column for each entity. The entry for (sentence i , entity j ), which we write r i;j , represents the syntactic role that entity takes on in that sentence: subject ( S ), object ( O ), or some other role ( X ) 2 . In addition, there is a special marker ( -) for entities which do not appear at all in a given sentence.

To construct a grid, we must rst decide which textual units are to be considered  X entities X , and how the different mentions of an entity are to be linked. We follow the -COREFERENCE setting from Barzi-lay and Lapata (2005) and perform heuristic coref-erence resolution by linking mentions which share a head noun. Although some versions of the grid use an automatic coreference resolver, this often fails to improve results; in Barzilay and Lapata (2005), coreference improves results in only one of their tar-get domains, and actually hurts for readability pre-diction. Their results, moreover, rely on running coreference on the document in its original order ; in a summarization task, the correct order is not known, which will cause even more resolver errors.
To build a model based on the grid, we treat the columns (entities) as independent, and look at lo-cal transitions between sentences. We model the transitions using the generative approach given in Lapata and Barzilay (2005) 3 , in which the model estimates the probability of an entity's role in the next sentence, r i;j , given its history in the previ-ous two sentences, r i 1 ;j ; r i 2 ;j . It also uses a sin-gle entity-specic feature, salience, determined by counting the total number of times the entity is men-tioned in the document. We denote this feature vec-tor F i;j . For example, the vector for  X ight X  after the last sentence of the example would be F 3 ;f l ig ht = h X ; S; sal =2 i . Using two sentences of context and capping salience at 4, there are only 64 possi-ble vectors, so we can learn an independent multino-mial distribution for each F . However, the number of vectors grows exponentially as we add features. We test our model on two experimental tasks, both testing its ability to distinguish between correct and incorrect orderings for WSJ articles. In doc-ument discrimination (Barzilay and Lapata, 2005), we compare a document to a random permutation of its sentences, scoring the system correct if it prefers the original ordering 4 .

We also evaluate on the more difcult task of sen-tence insertion (Chen et al., 2007; Elsner and Char-niak, 2008). In this task, we remove each sentence from the article and test whether the model prefers to re-insert it at its original location. We report the av-erage proportion of correct insertions per document.
As in Elsner and Charniak (2008), we test on sec-tions 14-24 of the Penn Treebank, for 1004 test doc-uments. We test signicance using the Wilcoxon Sign-rank test, which detects signicant differences in the medians of two distributions 5 . Our main contribution is to extend the entity grid by adding a large number of entity-specic features. Before doing so, however, we add non-head nouns to the grid. Doing so gives our feature-based model Random 50.0 50.0 12.6 Grid: NPs 74.4 76.2 21.3 Grid: all nouns y 77.8 79.7 23.5 more information to work with, but is benecial even to the standard entity grid.

We alter our mention detector to add all nouns in the document to the grid 6 , even those which do not head NPs. This enables the model to pick up premodiers in phrases like  X a Bush spokesman X , which do not head NPs in the Penn Treebank. Find-ing these is also necessary to maximize coreference recall (Elsner and Charniak, 2010). We give non-head mentions the role X . The results of this change are shown in Table 1; discrimination performance increases about 4%, from 76% to 80%. As we mentioned earlier, the standard grid model does not distinguish between different types of en-tity. Given the same history and salience, the same probabilities are assigned to occurrences of  X Hillary Clinton X ,  X the airlines X , or  X May 25th X , even though we know a priori that a document is more likely to be about Hillary Clinton than it is to be about May 25th. This problem is exacerbated by our same-head coreference heuristic, which sometimes creates spu-rious entities by lumping together mentions headed by nouns like  X miles X  or  X dollars X . In this section, we add features that separate important entities from less important or spurious ones.
 Proper Does the entity have a proper mention? Named entity The majority OPENNLP Morton et Modiers The total number of modiers in all men-Singular Does the entity have a singular mention?
News articles are likely to be about people and organizations, so we expect these named entity tags, and proper NPs in general, to be more important to the discourse. Entities with many modiers through-out the document are also likely to be important, since this implies that the writer wishes to point out more information about them. Finally, singular nouns are less likely to be generic.

We also add some features to pick out entities that are likely to be spurious or unimportant. These features depend on in-domain coreference data, but they do not require us to run a coreference resolver on the target document itself. This avoids the prob-lem that coreference resolvers do not work well for disordered or automatically produced text such as multidocument summary sentences, and also avoids the computational cost associated with coreference resolution.
 Linkable Was the head word of the entity ever Unlinkable Did the head word of the entity occur 5 Has pronouns Were there 5 or more pronouns No pronouns Did the head word of the entity occur
To learn probabilities based on these features, we model the conditional probability p ( r i;j j F ) us-ing multilabel logistic regression. Our model has a parameter for each combination of syntactic role r , entity-specic feature h and feature vector F : r h F . This allows the old and new features to in-teract while keeping the parameter space tractable 7 .
In Table 2, we examine the changes in our esti-mated probability in one particular context: an entity with salience 3 which appeared in a non-emphatic role in the previous sentence. The standard entity grid estimates that such an entity will be the sub-ject of the next sentence with a probability of about Context P(next role is subj) Standard egrid .045
Head coref in M U C 6 .013 ...and proper noun .025 ...and NE type person .037 ...and 5 modiers overall .133
Never coref in M U C 6 .006 ...and NE type date .001 .04. For most classes of entity, we can see that this is an overestimate; for an entity described by a com-mon noun (such as  X the airline X ), the probability as-signed by the extended grid model is .01. If we suspect (based on M U C 6 evidence) that the noun is not coreferent, the probability drops to .006 ( X an increase X ) X  if it is a date, it falls even further, to .001. However, given that the entity refers to a person, and some of its mentions are modied, suggesting the ar-ticle gives a title or description ( X Obama's Secretary of State, Hillary Clinton X ), the chance that it will be the subject of the next sentence more than triples. Table 3 gives results for the extended grid model on the test set. This model is signicantly better than the standard grid on discrimination (84% ver-sus 80%) and has a higher mean score on insertion (24% versus 21%) 8 .
 The best WSJ results in previous work are those of Elsner and Charniak (2008), who combine the entity grid with models based on pronoun coreference and discourse-new NP detection. We report their scores in the table. This comparison is unfair, however, because the improvements from adding non-head nouns improve our baseline grid sufciently to equal their discrimination result. State-of-the-art results on a different corpus and task were achieved by Sori-cut and Marcu (2006) using a log-linear mixture of an entity grid, IBM translation models, and a word-correspondence model based on Lapata (2003). Random 50.00 50.00 12.6 Elsner+Charniak 79.6 81.0 23.0 Grid 79.5 80.9 21.4 Extended Grid 84.0 y 84.5 24.2 Grid+combo 82.6 84.0 24.3
To perform a fair comparison of our extended grid with these model-combining approaches, we train our own combined model incorporating an en-tity grid, pronouns, discourse-newness and the IBM model. We combine models using a log-linear mix-ture as in Soricut and Marcu (2006), training the weights to maximize discrimination accuracy.
The second section of Table 3 shows these model combination results. Notably, our extended entity grid on its own is essentially just as good as the com-bined model, which represents our implementation of the previous state of the art. When we incorpo-rate it into a combination, the performance increase remains, and is signicant for both tasks (disc. 86% versus 83%, ins. 27% versus 24%). Though the im-provement is not perfectly additive, a good deal of it is retained, demonstrating that our additions to the entity grid are mostly orthogonal to previously de-scribed models. These results are the best reported for sentence ordering of English news articles. We improve a widely used model of local discourse coherence. Our extensions to the feature set involve distinguishing simple properties of entities, such as their named entity type, which are also useful in coreference and summarization tasks. Although our method uses coreference information, it does not re-quire coreference resolution to be run on the target documents. Given the popularity of entity grid mod-els for practical applications, we hope our model's improvements will transfer to summarization, gen-eration and readability prediction. We are most grateful to Regina Barzilay, Mark John-son and three anonymous reviewers. This work was funded by a Google Fellowship for Natural Lan-guage Processing.

