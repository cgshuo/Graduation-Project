 Andrew Y. Ng ang@cs.st anf ord.edu We consider sup ervised learning in settings where there are man y input features, but where there is a small subset of the features that is sucien t to ap-pro ximate the target concept well.
 In sup ervised learning settings with man y input fea-tures, over tting is usually a poten tial problem unless there is ample training data. For example, it is well-kno wn that for unregularized discriminativ e mo dels t via training-error minimization, sample complexit y (i.e., the num ber of training examples needed to learn \well") gro ws linearly with the VC dimension. Fur-ther, the VC dimension for most mo dels gro ws about linearly in the num ber of parameters (Vapnik, 1982), whic h typically gro ws at least linearly in the num ber of input features. Thus, unless the training set size is large relativ e to the dimension of the input, some special mec hanism|suc h as regularization, whic h en-courages the tted parameters to be small|is usually needed to prev ent over tting.
 In this pap er, we focus on logistic regression, and study the beha vior of two standard regularization metho ds when they are applied to problems with man y irrel-evant features. The rst, L 1 regularization, uses a penalt y term whic h encourages the sum of the abso-lute values of the parameters to be small. The second, L 2 regularization, encourages the sum of the squares of the parameters to be small. It has frequen tly been observ ed that L 1 regularization in man y mo dels causes man y parameters to equal zero, so that the parameter vector is sparse. This mak es it a natural candidate in feature selection settings, where we believ e that man y features should be ignored. For example, linear least-squares regression with L 1 regularization is called the Lasso algorithm (Tibshirani, 1996), whic h is kno wn to generally give sparse feature vectors. Another example of learning using L 1 regularization is found in (Zheng et al., 2004).
 In this pap er, we pro ve that for logistic regression with L 1 regularization, sample complexit y gro ws only log-arithmic ally in the num ber of irrelev ant features (and at most polynomially in all other quan tities of inter-est). Logistic regression with L 1 regularization is an app ealing algorithm since it requires solving only a con vex optimization problem. Further, the logarith-mic dep endence on the input dimension matc hes the best kno wn bounds pro ved in various feature selection con texts (e.g., Ng, 1998; Ng &amp; Jordan, 2001; Little-stone, 1988; Helm bold et al., 1996; Kivinen &amp; War-muth, 1994).
 We also consider logistic regression with L 2 regular-ization. (E.g., Nigam et al., 1999). We sho w that this gives a rotationally invarian t algorithm, and that any rotationally invarian t algorithm|whic h also includes SVMs, neural net works, and man y other algorithms| has a worst case sample complexit y that gro ws at least line arly in the num ber of irrelev ant features, even if only a single feature is relev ant. This suggests that these algorithms may not be e ectiv e in settings where only a few features are relev ant, and the num ber of training examples is signi can tly smaller than the in-put dimension. We consider a sup ervised learning problem where we amples dra wn i.i.d. from some distribution D . Here, x ( i ) 2 [ 1 ; 1] n are the n -dimensional inputs, and y ( i ) 2f 0 ; 1 g are the lab els. For notational con venience, we assume that the last coordinate of the input vec-tors x ( i ) n = 1 alw ays, so that the intercept term needs not be treated separately . We will focus on logistic regression, so our mo del will be where 2 R n are the parameters of our mo del. One way to describ e regularized logistic regression is as the nding the parameters that solv e follo wing optimization problem: where R ( ) is a regularization term that is used to pe-nalize large weigh ts/parameters. If R ( ) 0, then this mo del is the standard, unregularized, logistic regres-sion mo del with its parameters t using the maxim um likeliho od criteria. If R ( ) = jj jj 1 = P n i =1 j i j , then this is L 1 regularized logistic regression. If R ( ) = sion.
 In the optimization problem in Equation (2), the pa-rameter 0 con trols a tradeo between tting the data well, and having well-regularized/small parame-ters. In this pap er, it will sometimes be useful to con-sider an alternativ e way of parameterizing this trade-o . Speci cally , we will also consider the constrained optimization problem: For every solution to Equation (2) found using some particular value of , there is some corresp onding value of B in the optimization problem (3-4) that will give the same . Thus, these are two equiv alen t reparame-terizations of the same problem. Readers familiar with con vex analysis (Ro ckafellar, 1970) may also verify the equiv alence between the two problems by noting that the Lagrangian for the constrained optimization (3-4) is exactly the objectiv e in the optimization (2) (plus a constan t that does not dep end on ), where here is the Lagrange multiplier. Thus, (3-4) may be solv ed by solving (2) for an appropriate .
 Because our logistic regression mo del is t via (regu-larized) maxim um likeliho od, one natural metric for a tted mo del's error is its negativ e loglik eliho od (also called the \logloss") on test data: Here, the subscript \( x; y ) D " indicates that the exp ectation is with resp ect to a test example ( x; y ) dra wn from D . Our main theoretical results regarding L 1 regularization will use this error metric. Giv en a dataset S , we also de ne the empirical logloss on S to be Sometimes, we will also be interested in the 0/1 mis-classi cation error of our algorithm. We de ne where t is a threshold function ( t ( z ) = 1 if z 0 : 5, t ( z ) = 0 otherwise). The empirical 0/1 misclassi ca-tion error ^ " m ( ) = ^ " m S ( ) is also de ned analogously to be the fraction of examples in S that a mo del using parameter misclassi es.
 It is straigh tforw ard to verify that, for the logistic re-gression mo del, we have " l ( ) (log 2) " m ( ). Thus, an upp er-b ound on logloss also implies an upp er-bound on misclassi cation error, and a lower-b ound on misclassi cation error (suc h as given in Section 4) also implies a lower-b ound on logloss. We are interested in sup ervised learning problems where there is a very large num ber n of input features, but where there may be a small subset|sa y r n of them|that is sucien t to learn the target concept well. We will consider the follo wing implemen tation of L 1 regularized logistic regression: 1. Split the data S into a training set S 1 consisting of 2. For B = 0 ; 1 ; 2 ; 4 ; : : : ; C , 3. Among the B 's from Step 2, select and output Thus, this algorithm uses uses hold-out cross valida-tion to select the regularization parameter B used in (3-4). In Step 3 of the algorithm, we did not exactly specify the error metric ^ " S our exp ected logloss on the test data, it would mak e sense to use ^ " S mum logloss setting to whic h the theoretical results in this section apply . However, if the goal is to minimize 0/1 misclassi cation error, then it would also mak e sense to pick the i with the smallest misclassi cation error on the hold-out test set, and use ^ " S We want to sho w that if there is some hypothesis that attains low generalization error using only a small num ber r of features, then L 1 regularized logistic re-gression will attain performance that is (nearly) as good as that of this hypothesis, even if the training set is small.
 Theorem 3.1 : Let any &gt; 0 ; &gt; 0 ; C &gt; 0 ; K 1 be given, and let 0 &lt; &lt; 1 be a xe d constant. Supp ose ther e exist r indic es 1 i 1 ; i 2 ; : : : ; i r and a parameter vector 2 R n such that only the r corresponding comp onents of are non-zer o, and j rK . Then, in order to guar ante e that, with probability at least 1 , the parameters ^ output by our learning algorithm does nearly as wel l as , i.e., that it suc es that The main tools used to sho w this result are certain cov-ering num ber bounds sho wn by (Bartlett, 1998; Zhang, 2002). The pro of is given in App endix A. This result sho ws that the sample complexit y of our algorithm| that is, the num ber of training examples needed to learn \well"|gro ws only logarithmic ally in the num-ber of irrelev ant features. Thus, logistic regression with L 1 regularization is capable of learning in prob-lems even where the num ber of irrelev ant features may be far larger than the training set size.
 Space constrain ts preclude a full discussion, but we also note that C can be chosen automatically (as a function of m ) so that the same bound as stated above holds, but with the dep endence on C remo ved. Fur-ther, by mo difying the de nition of p ( y j x ; ), it is straigh tforw ard to generalize this result to L 1 regu-larized versions of other mo dels from the generalized linear mo del family (McCullagh &amp; Nelder, 1989), suc h as linear least squares regression. Let M = f M 2 R n n j M M T = M T M = I; j M j = 1 g be the class of rotational matrices. 1 Thus, if x 2 R n and M 2M , then M x is x rotated through some angle around the origin. 2 Giv en a training set S = f ( x ( i ) ; y ( i ) ) g m i =1 all the inputs rotated according to M . Giv en a learn-ing algorithm L , we let L [ S ]( x ) denote the predicted lab el resulting from using the learning algorithm to train on a dataset S , and using the resulting hypoth-esis/classi er to mak e a prediction on x .
 De nition 4.1: Giv en a (deterministic) learning al-gorithm L , we say that it is rotationally invarian t if, for any training set S , rotational matrix M 2 M , and test example x , we have that L [ S ]( x ) = L [ S 0 ]( x where S 0 = M S; x 0 = M x . More generally , if L is a stochastic learning algorithm so that its predictions are random, we say that it is rotationally invarian t if, for any S; M; x , the predictions L [ S ]( x ) and L [ S 0 have the same distribution.
 Some readers familiar with logistic regression may al-ready recognize that its L 2 regularized version is ro-tationally invarian t. But for the sak e of completeness, we will state and formally pro ve this here.
 Prop osition 4.2: L 2 regularize d logistic regression (Equation 2, with &gt; 0 ) is rotational ly invariant. Pro of. Let any S; M; x be given, and let S 0 = M S , x Further, R ( ) = T = ( M ) T ( M ) = R ( M ). De ne J ( ) = P m i =1 log p ( y ( i ) j x ( i ) ; ) R ( ), and J ( ) = P m i =1 log p ( y ( i ) j M x ( i ) ; ) R ( ). Let arg max J ( ) be the parameters resulting from t-ting L 2 regularized logistic regression to S . (Be-cause &gt; 0, the Hessian of J can be sho wn to be negativ e de nite, and thus J has a unique maxi-mum.) Similarly , let ^ 0 = arg max J 0 ( ) be the pa-rameters resulting from tting to S 0 . By our pre-vious argumen t, clearly J ( ) = J 0 ( M ) for all . Thus, ^ = arg max J ( ) = M 1 arg max J 0 ( ) = M 1 ^ 0 , whic h implies ^ 0 = M ^ . Hence, L [ S ]( x ) = 1 = (1 + exp( ^ T x )) = 1 = (1 + exp( ( M ^ ) T ( M x ))) = 1 = (1 + exp( ( ^ 0 ) T x 0 )) = L [ S 0 ]( x 0 ). We also give, without pro of, additional examples of rotationally invarian t algorithms: Examples of non-rotationally invarian t algorithms in-clude logistic regression with L 1 regularization, naiv e Bayes, decision trees that mak e only axis-aligned splits, Winno w (Littlestone, 1988), EG (Kivinen &amp; Warm uth, 1994), and most feature selection algo-rithms (Blum &amp; Langley , 1997; Koha vi &amp; John, 1997; Ng &amp; Jordan, 2001; Ng, 1998).
 We now give a lower-b ound on the worst-case sample complexit y of feature selection using any rotationally invarian t algorithm.
 Theorem 4.3: Let L be any rotational ly invariant learning algorithm, and let any 0 &lt; &lt; 1 = 8 , 0 &lt; &lt; 1 = 100 be xe d. Then ther e exists a learning problem D so that: (i) The labels are deterministic ally relate d to the inputs according to y = 1 if x 1 t , y = 0 otherwise for some t , and (ii) In order for L to attain or lower 0/1 misclassi c ation error with probability at least 1 , it is necessary that the training set size be at least Thus, for any rotationally invarian t algorithm L , there exists at least one problem that should have been \easy" in the sense that there is only one relev ant fea-ture ( x 1 ) and the lab els are simply obtained by thresh-olding x 1 , but L requires a large num ber of training examples to learn it. Note that a good feature se-lection algorithm should be able to learn any target concept of this form using only O (log n ) training ex-amples. (E.g., Ng, 1998, Littlestone, 1988.) However, L requires a num ber of training examples that's at least line ar in the dimension of the input.
 This suggests that rotationally invarian t algorithms are unlik ely to be e ectiv e feature selection algorithms, particularly in settings where only a small subset of the features are relev ant, and the dimension of the input n is signi can tly larger than the training set size m . The pro of of this result is given in App endix B, and uses ideas from the lower-b ounds originally pro ved by (Ehrenfeuc ht et al., 1989; Vapnik, 1982). A related result was also sho wn by (Kivinen et al., 1995) for the perceptron learning algorithm. They point out that the perceptron is rotationally invarian t, and that an adv ersary choosing the sequence of training examples can force it (or, more generally , any \additiv e linear online prediction algorithm") to mak e ( n ) mistak es. Remark. Supp ort Vectors Mac hines have been pro ved to work well in extremely high dimensional in-put spaces, even in nite-dimensional ones, as long as the data is separated with a large margin . (Vapnik, 1998) Thus, it may seem surprising that we can sho w that SVMs perform poorly in the presence of high di-mensional inputs (with man y irrelev ant features). To reconcile this, we note that while the margin does not shrink as extra irrelev ant features are added, the di-ameter of the data (e.g., maxim um distance between any two points measured in the L 2 -norm) gro ws with the num ber of irrelev ant features, and it is actually the margin divide d by the diameter that governs general-ization performance. (Vapnik, 1998) We now presen t some empirical results comparing lo-gistic regression using L 1 and L 2 regularization. All results rep orted here are averages over at least 100 in-dep enden t trials, and in eac h exp erimen t, 30% of the data was used as hold-out data for selecting the reg-ularization parameter. (Very similar results are ob-tained if the regularization parameters are tuned on test data.) In the rst exp erimen t, we let the total num ber of fea-tures vary and let just a single feature be relev ant. 8 Figure 1a sho ws the misclassi cation error of the two metho ds, when trained using 100 training examples. As we see, the results are dramatically di eren t. Us-ing L 1 regularization, logistic regression is extremely insensitiv e to the presence of irrelev ant features. Note the scale on the horizon tal axis: Even learning with just 100 examples in a 1000-dimensional input space, it is able to attain very low generalization error. In con trast, the error of logistic regression with L 2 regu-larization rapidly approac hes 0.5.
 Figure 1b sho ws the same exp erimen t rep eated with three relev ant features. Figure 1c sho ws results from a third exp erimen t where all the features con tain some information about the output, but where the degree to whic h feature i is relev ant decreases exp onen tially with i . Only the rst few features have a signi can t e ect on the output lab el, and to mo del the data well, it is sucien t to use only a very small num ber of fea-tures. Again, L 1 regularization is clearly sup erior as n becomes large.
 Figures 1d-f rep eat the same exp erimen ts, but here the logloss is plotted instead of misclassi cation error. Figures 1g-i sho w the same exp erimen ts rep eated using 200 training examples. In all cases, logistic regression with L 1 regularization, as predicted by the theoretical results, exhibits a signi can tly higher tolerance to the presence of man y irrelev ant features.
 I give warm thanks to Pieter Abb eel, Chris Manning, Rajat Raina, Yoram Singer and Kristina Toutano va for helpful con versations. This work was supp orted by the Departmen t of the Interior/D ARP A under con-tract num ber NBCHD030010.
 Our pro of of Theorem 3.1 is based on bounding the covering num bers of certain function classes. Due to space constrain ts, our pro of is necessarily brief, but for highly readable introductions to covering num bers, see, e.g., (An thon y &amp; Bartlett, 1999; Haussler, 1992). Let there be a class of functions F with some do-main U and range [ M; M ] R . Giv en some set jj t jj p = ( P m i =1 j t i j p ) 1 =p . De ne N p ( F ; ; [ z to be the size of the smallest set that -covers F sup z Let there be some distribution D over U , and de ne " ( f ) = E z D [ f ( z )] : If z (1) ; : : : ; z ( m ) iid lard, 1984) sho wed that P " Further, (Zhang, 2002) sho ws that if G = f g : g ( x ) = x; x 2 R n ; jj jj q a g is a class of linear functions parameterized by weigh ts with q -norm bounded by a , and if the inputs x 2 R n are also norm-b ounded so that jj x jj p b , and further 1 =p + 1 =q = 1 (so the p -and q -norms and dual) with 2 p 1 , then (A special case of this is also found in Bartlett, 1998.) Some other well-kno wn prop erties of covering num bers (e.g., Anthon y and Bartlett, 1999; Zhang, 2002; Haus-sler, 1992) include that and that given a class of functions G with domain R , if F is a class of functions R Y 7! R de ned according where ` ( ; y ) (for any xed y and view ed a function of the rst parameter only) is Lipsc hitz con tinuous with Lipsc hitz constan t L , then We now give the main part of the pro of. First, notice that the algorithm uses hold-out cross validation to se-lect amongst the values B = 0 ; 1 ; 2 ; 4 ; : : : : Let ^ equal to rK . Notice therefore that rK ^ B 2 rK . We will begin by considering the step in the algorithm where logistic regression was t using the regulariza-tion parameter ^ B . Speci cally , let ^ denote the pa-rameter vector resulting from solving the optimization problem given by Equations (3-4) with B = ^ B . Let G = f g : [ 1 ; 1] n 7! R : g ( x ) = T x; jj jj 1 ^ B g be a class of linear functions parameterized by with L -norm bounded by ^ B . Using Equations (12,11), we have that (Recall our assumption in Section 2 that x 2 [ 1 ; 1] n , whic h implies jj x jj 1 1.) From Holder's inequalit y, we also have Now, let F be a class of functions f : R Y 7! R de ned according to F = f f ( x; y ) = ` ( g ( x ) ; y ) : g 2 G ; y 2 f 0 ; 1 gg , where ` ( g ( x ) ; 1) = log 1 = (1 + exp( g ( x ))), and ` ( g ( x ) ; 0) = log(1 1 = (1 + exp( g ( x )))). Thus, ` ( g ( x ) ; y ) is the logloss su ered by the logistic regression mo del on an example where it predicts p ( y = 1 j x ) = 1 = (1 + exp( g ( x )), and the cor-rect lab el was y . It is straigh tforw ard to sho w that j dt ` ( t; y ) j 1 for any y 2 f 0 ; 1 g . Thus, ` ( ; y ) is Lipsc hitz con tinuous with Lipsc hitz constan t L = 1. Hence, com bining Equations (13,14), we get It is also straigh tforw ard to sho w that j ` ( t; 1) j = j log 1 = (1+exp ( t )) jj t j +1 (and similarly for ` ( t; 0)). Together with Equation (15), this implies that j f ( x; y ) j = j ` ( g ( x ) ; y ) j = j ` ( T x; y ) j Let m 1 = (1 ) m be the num ber of examples the parameters ^ were trained on in the inner-lo op of the algorithm. (The remaining m 2 = m examples were used for hold-out cross validation.) Recalling tion, and putting together Equations (16,10,17) with M = ^ B + 1, we nd that We would like for this probabilit y to be small. By setting the righ t hand side to and solving for m 1 , we nd that in order for the probabilit y above to be upp er-b ounded by , it suces that m 1 = ((log n ) poly( ^ B; 1 ; log 1 )) = ((log n ) poly( r; K; 1 ; log 1 where to obtain the second equalit y we used the fact (sho wn earlier) that rK ^ B 2 rK . Since m 1 = (1 ) m , if we treat (1 ) as a constan t that can be absorb ed into the big-notation, then to ensure the above holds, it suces that To summarize, we have sho wn that if m satis es Equa-tion (19), then with probabilit y 1 , it will hold true that for all f 2F , we have that By referring to the de nitions of F and G , we see this would imply that for all : jj jj 1 ^ B , we have 1 and therefore that for all : jj jj 1 ^ B , In summary , we have sho wn that with a training set whose size has to be at most logarithmic in n and poly-nomial in all quan tities of interest, with probabilit y 1 , we will have that ^ " l S mate for " l ( ). Now, recall that the parameter vector ^ was found by solving ^ = arg min : jj jj ing Equation (22), a standard uniform con vergence re-sult 9 (e.g., Vapnik, 1982; Anthon y and Bartlett, 1999) sho ws that minimizing ^ " l is nearly as good as minimiz-ing " l , and that in particular, Equation (22) implies where the second step used the fact that jj jj 1 ^ B . Hence, we have sho wn that in Step 2 of the algorithm, we will nd at least one parameter vector ^ whose per-formance (generalization error as measured according to " l ) is nearly as good as that of . In Step 3 of the algorithm, we use hold-out cross validation to select from the set of B 's found in Step 2. Using another entirely standard argumen t, it is straigh tforw ard to sho w that, with only a \small" (at most polynomially large, and indep enden t of n ) num ber of examples used in the hold-out set, we can ensure that with proba-bilit y 1 , the selected parameter vector will have performance at most worse that that of the best pa-rameter vector in the set. The details of this step are omitted due to space, but is entirely standard and may be found in, e.g., (Vapnik, 1982; Anthon y &amp; Bartlett, 1999). Putting this together with (23), we have sho wn that, with probabilit y 1 2 , the output satis es Finally , replacing with = 2 and with = 3 everywhere in the pro of sho ws the theorem.
 Let L and ; be as given in the statemen t of the theo-rem. Consider the concept class of all linear separators in n dimensions, C = f h : h ( x ) = 1 f T x g ; 6 = 0 g . (Here, we do not use the con vention adopted previ-ously that necessarily x n = 1. Also, 1 fg is the indica-tor function, so that 1 f True g = 1, and 1 f False g = 0.) It is well-kno wn that VC( C ) = n + 1. (Vapnik, 1982) From a standard PAC lower bound, there must there-fore exist a distribution D X over the inputs, and a target concept h 2 C , so that if D X is the input dis-tribution, and the lab els are given by y = h ( x ), then for L to attain or lower 0/1 misclassi cation error with probabilit y at least 1 , it is necessary that the training set size be at least m = ( n= ). (Results of this type have been pro ved by Vapnik, 1982 and Ehrenfeuc ht et al., 1989. The particular result stated here is also given in, e.g., Theorem 5.3 of Anthon y and Bartlett, 1999).
 Since h 2C is a linear target concept, it can be writ-ten h ( x ) = 1 f T x g for some 2 R n and any positiv e constan t c does not change anything, we may assume without loss of generalit y that jj jj 2 = 1. Let M be any orthogonal matrix whose rst row is T . Suc h a matrix must exist. (Strang, 1988) Thus, M = [1 ; 0 ; : : : ; 0] T = e 1 (the rst basis vector). Fur-ther, by ipping the signs of any single row (other than the rst row) of M if necessary , we may en-sure that j M j = 1, and hence M 2 M . Now, con-sider a learning problem where the input distribution is induced by sampling x D X , and then comput-ing x 0 = M x . Further, let the lab els be given by y 0 = 1 f x 0 g = 1 f ( M ) T ( M x ) g = 1 f ( ) T x g = y , we therefore see that a learning problem with exam-ples dra wn from the ( x 0 ; y 0 ) distribution is simply a rotated version of the problem with examples ( x; y ). But since L is rotationally invarian t, its predictions on test sets under the original and rotated problems will be iden tical, and thus its generalization error, and sample complexit y, must also be the same under either
