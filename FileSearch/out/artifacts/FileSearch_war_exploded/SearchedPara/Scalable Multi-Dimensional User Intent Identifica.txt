 The problem of identifying user intent has received consid-erable attention in recent years, particularly in the context of improving the search experience via query contextualiza-tion. Intent can be characterized by multiple dimensions, which are often not observed from query words alone. Ac-curate identification of Intent from query words remains a challenging problem primarily because it is extremely dif-ficult to discover these dimensions. The problem is often significantly compounded due to lack of representative train-ing sample. We present a generic, extensible framework for learning the multi-dimensional representation of user intent from the query words. The approach models the latent re-lationships between facets using tree structured distribution which leads to an efficient and convergent algorithm, FastQ, for identifying the multi-faceted intent of users based on just the query words. We also incorporated WordNet to extend the system capabilities to queries which contain words that do not appear in the training data. Empirical results show that FastQ yields accurate identification of intent when com-pared to a gold standard.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -Search process ; G.3 [ Probability and Statistics ]: Probabilistic algorithms Algorithms, Performance, Theory web search, query intent, facets, Chow-Liu, WordNet, FastQ
Identifying user intent 1 behind search queries plays a cru-cial role in providing a better search experience [16, 29, 28]. The traditional classification of user intent as navigational , transactional or informational has focused on intent as a single dimension [3] or hierarchical extensions [26, 2, 14].
There is a need for a systematic framework to analyze user intent in terms of multiple dimensions (or facets ) of inter-est. In fact, if a facet can be predicted well, it is possible to contextualize the answer for a query, triggering a differ-ent ranking algorithm or even a completely different user experience. For example, an informational query will rank better information coming from trusted web sites or a time sensitive query may give better ranking to news. Similarly, a local query may trigger a map or a genre specific query may show a particular screen layout. The multiple facets of interest might be correlated, which poses a challenge com-pared to past approaches [19, 2, 15]. Further, one should be able to predict the user intent for new queries in a rea-sonably fast manner which can be incorporated into existing search engine technology [8].

This paper tries to model the probabilistic dependencies between the various dimensions underlying the user intent of a search query, and then proposes to use the model to identify the intent behind the query. Our hope is that by modeling the dependencies well, it might yield a more accu-rate identification of the underlying intent. We use the set of facets from [4] to illustrate our method. As our model is more complex, we explore the potential of our technique using the minimal set of features available, that is, just the query words. Using more features, such as clicks, would just improve our results.

Our approach immediately raises several challenging ques-tions, namely (a) What kind of models should one use in modeling dependencies? (b) How does one model the joint distribution of query words and the dimensions? and (c) Is it possible to augment the model to cater for unseen query words?
This paper poses the problem of identifying the user X  X  in-tent behind a query as an inference problem over a tree struc-tured graphical model. We argue that the tree structured
In th is paper we use intent in a broad sense, including generic and specific intents. model serves as a natural choice for discovering latent de-pend encies between several dimensions of a query as it is the simplest distribution which goes beyond the independence assumption. The associated learning problem is solved by using the Chow-Liu algorithm [6, 21, 27]. In addition, the resultant algorithm should be scalable and preferably online for real world use.

Most of the works available in the literature are based on the analysis of query logs and click-through data. How-ever, the additional information might not be available, or restricted due to privacy concerns. Our method infers the user intent from the query words only, without using click-through documents, web session information, and other meta-data. This shows the potential of the algorithm, as with more features, the performance should improve.

We model the query as a factorial distribution over the observed query words , which is equivalent to making the as-sumption that the impact of a word in the query is indepen-dent of the other words. This results in a simplified model which allows an online algorithm for user X  X  intent identifica-tion. We also incorporate WordNet [12], a lexical database, to improve the classification accuracy of the user intent of search queries for unseen query words.

In order to perform the identification of the underlying user X  X  needs in the described setting, we introduce FastQ , an online distributed algorithm for classifying user intent for search queries. The algorithm dynamically constructs a tree structured factor graph [18] with the appropriate dis-tribution. The intent is inferred by using standard belief propagation (BP) algorithms, and the tree structure of the dynamically constructed factor graph guarantees the con-vergence of the BP algorithm. Further, the small size of the resulting factor graph corresponding to a search query allows fast (approximate) inference and allows online classification of the user X  X  intent. Experimental timing results show that the method can be incorporated into existing search engine technology [8, 5].
 Our Contribution. The main contributions of this work are: 1. We cast the problem of identifying user intent for web 2. The resulting inference is difficult; and we present a 3. We incorporate WordNet [12], a lexical database, to 4. Our framework is generic and naturally lends itself to
The rest of this paper is organized as follows: Section 2 outlines previous work related to ours. Section 3 describes our query intent model for the web query classification; and the algorithm for the online query classification; and Sec-tion 4 presents our experimental setup and the results ob-tained. Finally, Section 5 presents our conclusions.
User X  X  intent. According to Broder X  X  taxonomy [3], the user intents can be informational, navigational and trans-actional . Albeit this taxonomy was further extended by Rose &amp; Levinson [26], the original categories are the most widely used, and have been the basis for an important num-ber of studies in the IR area. In the same context, Baeza-Yates et al. [2] have proposed that the user intents can be classified as informational, not-informational and ambigu-ous . Hu et al. [14] have interpreted the user X  X  query intent through the concepts included in Wikipedia; the authors ex-emplify their proposal throughout the identification of the following intents: travel, personal name, and job finding. More recently, Calder  X on-Benavides et al. [4] have studied the user X  X  intent as a multi-dimensional composition of facets. Sources to identify the user X  X  intent. In their work, Kang &amp; Kim [17] used information from document content, links and URLs to classify the user X  X  queries into topic rel-evance task (informational intent), homepage finding task (navigational intent), and service finding task (transactional intent). Lee et al. [19] took into account the past user X  click behaviour and the anchor link distribution to deter-mine the navigational and informational intents of queries. The content of the documents selected by the users in an-swer to a query is used in [2]. Liu et al. [20] exploited click-through data in order to find navigational and infor-mational/transactional type queries.

Nguyen and Kan [23] defined functional facets to describe the user X  X  intent. In their work, Ashkan et al. [1] developed a methodology to determine commercial/non-commercial and navigational/informational intent of queries by using ad click-through features, query features, and the content of search engine result pages. In [14], the authors have mapped the user X  X  query intent into the concepts included in Wikipedia, and have represented them through the articles and cate-gories related to each concept.

Recent work on query personalization has focused on mod-eling the query intent [9, 29, 24, 28] and user behaviour [10, 31]. In particular, Teevan et al. [29] investigated the vari-ability in user intent and the relevance search results ob-tained in terms of implicit relevance measures based on click-through data. They use interdependent sets of features defined using information available in (i) the queries (e.g. query strings , special syntax , etc.); (ii) history of similar queries (e.g. # { queries issued } , etc.); and (iii) query re-sults (e.g. click entropy , etc.). Our work can be viewed as a probabilistic framework for semi-supervised inference of multi-dimensional user intent using query strings and the history information available in the queries.

Methods to determine the user X  X  intent. Although early research was based on the manual classification of small sets of queries [3, 26], the use of some of the existing machin-ery from IR, machine learning, as well as natural language processing techniques have allowed an important progress towards determine the user query intents in bigger datasets. In this context, [17] calculated the probability that the task of a user query is topic relevance (informational queries), or homepage finding (navigational queries), based on the occur-rence patterns of the query terms in web pages. Lee et al. [19] made an automatic identification of goals (navigational and informational) through the application of heuristics over clicks made by the users on the results offered by the search engine. In [2] supervised and unsupervised learning meth-ods were applied in order to determine the user goals in con text of ODP 2 categories for a set of six thousand queries from a vertical search engine.Jansen et al. [15] presented a rule-based automated classification system for queries into hierarchical level of informational, transactional, and navi-gational intent. However, a clear principle for deriving new rules is not presented and the extension of the rules for a new system of classification or another language is not clear. In [14] they use the Wikipedia link graph to represent the re-lations among the concept articles and the relations among articles and categories. Such a graph is traversed randomly to obtain a vector with the probabilities that each concept belong to a intent.

Existing methods use multiple features (click-through, web-logs, etc.) to identify the intent either as a simple hierarchy within the framework of navigational , transactional and in-formational goals. To the best of our knowledge, ours is the first work that identifies user X  X  intent in terms of multiple meaningful dimensions using automated server-side classifi-cation. The multi-dimensional based description encapsu-lates additional information about the user intent in a num-ber of related dimensions, instead of a hierarchical scheme. As a difference to other proposals, in this work the identifica-tion of the user X  X  intent does not include click-through data or the document content, but is done by using solely the query words. Additionally, the task of identifying the user intent is performed in an online setting, creating a technol-ogy that can be incorporated into existing search engines environments with reasonably fast response time.
We chose the multi-dimensional description of user intent presented in [4]. A description of the dimensions, as well as the values that each one of them can take, is presented in Ta-ble 1. The multi-dimensional based description encapsulates additional information about the user intent in a number of related dimensions, instead of a hierarchical scheme.
Let  X  q = [ q 1 , . . . , q m ] denote a query of length |  X  q | = m consisting of distinct 3 query words { q 1 , . . . , q m } . We assume that the user intent can be encapsulated in terms of K facets e.g. Genre , Scope , etc., where F k denote the state space of the k th facet e.g. { Yes, No } for the facet Time Sensitivity .
Given the training data D = { (  X  q ( n ) , ~ f ( n ) ) } ing on N queries  X  q ( n ) and the corresponding user intent (in terms of facets) ~ f ( n ) , we cast the problem of finding the user intent  X  f  X   X  X  for a new query  X  q  X  as an inference problem where F = F 1  X F 2  X  . . .  X F K denotes the state space of facet classifications corresponding to a query;  X  =  X ( D ) and  X  =  X ( D ) are parameters based on the training data. http://dmoz.org
Dup licate instances of a word in a query are removed during pre-processing.
 Table 1: The dimensions or facets whic h character-ize user intent as in [4]. .

The following section discusses the choice of  X  and  X  based on the training data; and and then we discuss FastQ , a scalable, convergent algorithm for solving the overall in-ference problem.
Now we present our generative model for search query  X  q and corresponding user intent ~ f assuming queries and cor-responding user intents are drawn from an unknown joint distribution P (  X  Q =  X  q, ~ F = ~ f ).

A full realization of the joint distribution P (  X  q, ~ f ) is com-putationally prohibitive. For example, if the query words are drawn from the vocabulary V with maximum query length M , there are P M m =1 C ( |V| , m ) possible queries where sible states; the full probabilistic modeling would require O (2 |V| Q K i =1 |F i | ) states. Hence, we use a parametric repre-sentation to reduce the dimensionality of the problem.
We assume the dataset D consists of i.i.d. samples drawn from a joint distribution where  X  and  X  are parameters which model the dependen-cies between the inter-related facets ( X ), and the impact of words of the query on its facet classification ( X ) respectively.
We model the dependencies between the facets using the class of second order product distributions defined on the
Figure 1: Example spanning tree ( T ) on fac ets. facet variables. Let T be the set of possible spanning trees defined on the facets. Then, the parameter  X  =  X  T defines a marginal polytope [30] on the spanning tree T  X  T . In other words  X  T = {  X  ij : ( i, j )  X  T } X  X   X  i : i  X  X } is the set of probability distributions  X  ij defined on the edges of the spanning tree ( i, j )  X  T ; and the probability distributions  X  i defined on the facet variables i  X  X  such that which satisfy the marginal consistency Then, the probability distribution P ( ~ F |  X  T ) is given by where d i denotes the degree of the i th facet in the spanning tree T . Figure 1 shows a possible tree structure T defined on the facet variables. Now we describe the probability distribution of queries P (  X 
Q =  X  q | ~ F = ~ f ) conditioned on the facet variables where  X  q = { q 1 , . . . , q m } denotes a query of length |  X  q | = m and { f 1 , . . . , f K } denotes the facet states. The model assumes the following, 1. Each query has at most M distinct words. This is a 2. The length of the query |  X  q | is distributed as P ( |  X  q | = 3. The query words are chosen independently conditioned 4. The probability of a word being present in a query The resulting probability distribution P ( ~ Q =  X  q | ~ parametrized by  X  is given as where Z ( ~ f ) is the normalization function given as
The generative model for a search query and the corre-sponding user intent is presented in Algorithm 1. Algorithm 1 The generative model for query (  X  q, ~ f ) Require: V { Vo cabulary } Require:  X  T { Marginal polytope on the facets } Require:  X  { Conditional word distribution } Choose ~ f  X  P ( ~ f |  X  T ) as in (7) { Facet for query }
Choose m with P  X   X  ( ) { Query length } for i = 1 to m do end for return (  X  q, ~ f ) { Query and facet classification }
This section presents the procedure for computing the maximum likelihood estimation (MLE) for the model pa-rameters {  X  ,  X  T , T } based on the dataset. The maximum likelihood estimates are obtained as a result of the optimiza-tion in subject to the constraints in (4), (5), (6), (10); where the and (11). The resulting optimization can be decoupled into sepa rate optimization problems for ( X   X  , T  X  ) as s.t.
 and an optimization problem for  X   X  as s.t. where Z ( ~ f ( n ) ) is given as (12). Now we solve the optimiza-tion problems in (14) and (15).
 Notation. We introduce the function #( C ) where C is a valid Boolean expression as the number of queries in dataset D which satisfy the condition C , i.e. #( C ) = P (  X  q, ~ where 1 ( C ) is the indicator function, i.e., 1 ( C ) = 1 if C is T RU E and 0 otherwise. For example, #( w  X   X  q ) denotes the number of queries in the dataset D having query word w ; #( f k = f ) denotes the number of queries for which the k facet has state f ; and so on.
The optimization in (14) depends on the facet classifica-tions ~ f ( n ) of the queries in the dataset D , and not on the fit a second order product distribution to the multivariate data (facets) ~ f ( n )  X  X  .

Chow and Liu [6] first investigated the problem of fitting a second order product distribution to multivariate data. They showed that the problem is equivalent to minimizing the Kullback-Liebler divergence [7] between the target dis-tribution on D and tree based distribution. Their algorithm shows that for the dataset D and a spanning tree T ; the optimal marginal polytope  X   X  T is given by for edges ( i, j )  X  T . The mutual information I D ij between facets i and j is given as The optimal tree T  X  is such that the maximum possible mutual information is encapsulated by its edges.

The algorithm proceeds in two steps, namely, computing the mutual information I D ij for each pair of facets i and j as in (18); and selecting the top ( K  X  1) edges with the high-est mutual information by computing the maximum weight spanning tree for a weighted graph between facets, where each edge between facet i and j has weight I D ij .
A direct computation of the log Z ( ~ f ( n ) ) is computation-ally prohibitive. However, using a variational approach and Jensen X  X  inequality, we can compute an approximate esti-mate  X   X   X  characterized by the conditional distribution of query words conditioned on the facets specification
This section presents a fast online procedure for the user intent classification of a test query as: A naive solution requires computation of O ( Q i  X  X  F i ) prod-uct terms, each having O ( L |K| ) multiplications. Further, the computation of Z ( ~ f ) for each ~ f requires summation over V m terms, which is computationally infeasible.
 Instead, we perform approximate inference using standard Belief Propagation (BP) on a dynamically constructed factor graph based on the model parameters learnt in Section 3.4. The resulting factor graph is tree structured having K vari-able nodes and O ( M K ) factor nodes, and the inference pro-cedure converges in at most K iterations.
The factor graph for finding the optimal facet assignment ~ f  X  for a test query  X  q  X  consists of two classes of factors, namely,
Figure 2 shows the factor graph corresponding to the facet classification. The variable nodes, F 1 , . . . , F K , denote the facet variables which can take possible states in F 1 , . . . , F respectively. The factors T i , T ij model the latent relation-ship between facets, while the factors W i  X  model the impact of the  X  th word of the test query  X  q  X  affecting the i variable.

The factors, T i , i  X  K and T ij , ( i, j )  X  T  X  correspond to the latent relationships between the facets in (7). Their beliefs correspond to the parameters ( X   X  , T  X  ) based on the training data, given as
The factors W i  X  model the conditional distribution  X  P ( F f | w  X   X  q ) under the assumption that all facets are indepen-dent of each other.The factors are specified by the beliefs, Figure 2: Factor graph for query facet classification. The v ariable nodes, F 1 , . . . , F K , denote the facets; W denotes the factor corresponding to the  X  th query word affecting the i th facet variable; and T i and T ij are the factors corresponding to the Chow-Liu prob-ability distribution on facet variables which capture the facet interdependencies. where  X  k w ; f is an approximation for  X  P ( F k = f | w  X   X  q ) ob-tained as distribution of the k th facet under simplifying assumption that the each facet is independent of other facets.
The parameter  X   X  = {  X  k w ; f : f  X  F k , k  X  K , w  X  V} models the impact of the words in a query on the facets. Specifically,  X  k w ; f measures the evidence that the k th has value f when the query contains the word w .

We note that the evidence  X  k w ; f in (22) is not defined if the word w is not part of the training vocabulary, i.e., the test query  X  q  X  contains a word w that is not present in the training data D . The following section presents a method for incorporating the influence of new words, i.e. words in the test query which are not present in the vocabulary using the semantic relationships between the words and the existing vocabulary. Let w denote a new word not present in the vocabulary V
D . Then, the corresponding  X  i w ; f are not defined and the previous algorithm cannot be used. However, a semantically related word w  X  might be present in the vocabulary V D ; for which the corresponding  X  i w  X  ; f are defined. It is safe to as-sume that a strong semantic relationship between the words w and w  X  will be reflected in the resulting facet classifica-to be able to relate new words in the query to existing words in vocabulary.

We present a two-step method for integrating the capabil-ities of WordNet [12], a lexical database, into the facet clas-sification for queries containing new words. We first query WordNet for the related word candidates W  X  with similarity scores S  X  = { s w  X  : w  X   X  W  X  } for a new query word w using Breadth First Search (BFS) till a maximum search depth L . The second step uses the candidates which are present in the vocabulary W  X  = W  X   X  X  D to compute the parameter  X   X  w ; f . Algorithm 2 presents the heuristic procedure for incor-porating related words obtained from WordNet to augment results for a new word.
 We note that if the word has no neighbours either in Word-Net or in the existing vocabulary, the conditional distribu-tion P ( F i = | w  X   X  q  X  ) is a Dirichlet distribution with param-test word w has no semantically related neighbours w  X  with known statistics  X  k w  X  ; f , then the conditional distribution is randomly distributed such that, on average, the count statis-tics for a facet would be same as that found in the training data. However, if there exist neighbours of the word W  X  , that information is incorporated into the conditional distri-bution for the new word.
 Algorithm 2 Wor dNet integration  X   X  w = F WN ( w ) Require: L { Maxim um depth for BFS } Require:  X   X  { Parameter based on D} Ensure: w /  X  X  D { New word not present in vocabulary } ( W  X  , S  X  )  X  BF S ( w, L ) { Related words in WordNet }
W  X   X  W  X   X  X  D { Related words in vocabulary } for i = 0 to K do end for return  X   X  w = {  X   X  i w ; f : i  X  X  , f  X  X  i } Algorithm 3 Fas tQ  X  f  X  = F (  X  q  X  |D ) Require:  X  q  X  { Test query } Require:  X   X  ,  X   X  , T  X  , V { Parameter from training data D}
Initialize G ( K , []) { factor graph on facet variables } for all ( i, j ) in T  X  do end for for all i in K do end for for l = 1 to L do end for  X  f  X  = MAX-PROD( G ) { Standard BP } return  X  f  X  = { f  X  1 , . . . , f  X  K }
The overall procedure for facet classification when a new query ,  X  q  X  = { q 1 , . . . , q L } is given in Algorithm 3. The al-gorithm constructs the dynamic factor graph by adding the factors T i , T ij corresponding to the probability distribution on facets constructed using the Chow-Liu algorithm. Then, if a query word q  X  is in the vocabulary V , it adds words to the corresponding word factors, and constructs the word factor W i  X  based on parameter  X   X  , as in (22). Otherwise, if query word q  X  is not in vocabulary V , it constructs the fac-tors W i  X  based on semantically related words in vocabulary V found using WordNet as in Algorithm 2. The facet as-signment  X  f  X  for the test query is computed using a standard implementation of the Max-Product Belief Propagation [18].
Claim 1. The FastQ algorithm has the following proper-ties (a) The dynamic factor graph is a tree consisting of K vari-(b) The Max-Product Belief Propagation (BP) converges in (c) The average time for query classification is O ( K +  X  c
Proof. (a) By construction. (b) BP converges on trees iterating in at most the number (c) The average time spent in WordNet search is O (  X  c L
The small size of the constructed factor graph allows an implemen tation with limited resources. The maximum search depth L in WordNet is a user-controlled parameter which can be reduced to speedup the classification.
This section discusses the procedure for generalizing our model in order to explicitly study some other aspect of user intent. For example, the facet Medical Sensitive might be facet of interest relevant to medical search engines such as PubMed 4 .

Our query intent model and inference procedure is generic and can be used with other multi-dimensional descriptions of user intent. The requirement is the use of training data which has been classified according to the facet description of interest. Thus, in order to add the facet Medical Sensitive , one has to provide the labels for the new facet corresponding to the queries in the training data.

Sometimes, one can avoid the reclassification due to the nature of the data collection procedure. For example, a query having the word gene may not be Medical Sensitive if the queries are drawn from a generic search engine. How-ever, PubMed queries having the query word gene can be assumed to be Medical Sensitive (Similar assumptions have been made in [31]). Thus, one can obtain a rough estimate for the normalized influence of words in the query by simply assuming that the queries in PubMed are Medical Sensitive , but not in a generic search engine. http://www.pubmedcentral.nih.go v Table 2: Inter-annotation Agreement and Kappa Coe fficient for the dimensions of user X  X  intent.
Alternatively, some of the facets may be known and the remaining might be unknown. For example, if a test query is generated from PubMed, it is safe to assume that the facet Topic is Health (Similar assumptions have been made in [31]). It is easy to condition on some facets if they are known. The inference for the remaining facets is conditioned [11] over the facet Topic having state Health .
The evaluation data consists of 5 , 249 queries from a ver-tical search engine query log in a Spanish speaking country. The sampling of queries took place randomly after calculat-ing the popularity of each query (i.e. the number of times a query was posed), to capture the Zipf X  X  law distribution (in [2] only popular queries were considered). Hence, 15% were taken from to the set of the most popular queries, 15% from the (long) tail set, and the remaining 70% was sampled from the middle set (i.e., queries with average popularity).
The complete collection of queries was manually classi-fied, assigning a value for each of the dimensions described in Section 3.1. Additionally, 10% of the queries (randomly selected) were classified by two judges. In order to measure the consistency of the judges two well known metrics were applied: the Overall Agreement [13] and Kappa Coefficient [25]. The former metric express a score of how much homo-geneity, or consensus, there is in the ratings given by judges; however it does not take into account the agreement that would have been expected due solely to chance. In order to overcome this problem, we used the second metric (i.e., the Kappa Coefficient), which is the proportion of agreement be-tween judges after chance agreement is removed from con-sideration. Table 2 contains the overall agreement of the judges as well as the Kappa coefficient values.

Results from the overall agreement indicate that the con-sistency of the manual classification is highly satisfactory. In average, the overall agreement is 80%. Eight out of the nine dimensions have reached an overall agreement higher than 65%, which is quite high if we consider the number of dimensions that were assessed, the number of possible val-ues that each dimension can take, as well as the different criteria and the subjectivity of the judges.

In terms of the Kappa coefficient the values obtained re-flect a consistency and reliability of the manual classifica-tion. The assessment from the judges for all the dimensions is beyond chance, and more over, for three dimensions the agreement was almost perfect.

Training and testing sets . The amount of labelled % Tr. data 1% 5% 10% 50%
Model BASE WN FASTQ BASE WN FASTQ BASE WN FASTQ BASE WN FASTQ data available in most IR scenarios is much smaller than the corresponding test sets. In order to test the models in the low data regime, we have used 1, 5, 10, 50 % of the available queries as training data. We have repeated the experiment for N = 10 independent trials where the training samples have been randomly chosen. We report the resulting mean and variance across the trials.
The two key components of FastQ are the modeling of the latent relationships between facets and the incorporation of WordNet lexical database. In order to highlight the impact of the above two factors in the overall results and establish a baseline, we construct two variants of the FastQ algorithm described below:
Table 3 presents the classification accuracy of the individ-ual facets. We note that the latent modeling of the facets in FastQ improves the accuracy compared to BASE and WN for the facets: Time Sensitivity, Scope, Spatial Sensitivity, Authority Sensitivity, Genre.

We use the Hamming error to measure the overall accu-racy of the prediction, i.e., for a query  X  q  X  with true facet clas-sification ~ f  X  and the algorithm prediction  X  f ; the Hamming error d H ( ~ f  X  ,  X  f ) is d H ( ~ f  X  ,  X  f ) = P K k =1 1 if the k th facets are not same; else 0. A low (ideally zero ) Table 4: This table shows the classification ( FastQ ) re sults for the facet Task as Informational ( Inf ), Not Informational ( NotInf ) and Both ( Both ). The table shows precision ( P ), recall ( R ) and the F1-score ( F 1 ) when 50% of the data is used for training and the rest as test queries.
 Hamming distance means few (ideally none ) of the K facets have been incorrectly predicted.
 Figure 3(a)-3(c) shows the cumulative distribution of the Hamming errors (number of individually incorrect facets) in the prediction of K facets for 1%, 10% and 50% training data. For example, when 10% of the data is used for train-ing, FastQ classifies over 60% of the test queries with at most two mis-classified facets; compared to WN (45%) and BASE (40%). The modeling of the facet inter-relationships ( FastQ ) improves the overall quality of classification com-pared to BASE and WN , especially at low amounts of train-ing data.

We note that FastQ performance compared to BASE is slightly inferior on the facet  X  X opic X , which has 23 possible states. Therefore, accurately modelling the probability dis-tribution requires more data. We note that at 90% training data 5 , FastQ does perform better on  X  X opic X  (and all other facets) compared to BASE. We note that in a real world setting, the amount of available data might be greater than a few thousand queries (for example, when using ODP cat-egories). Under such scenario, FastQ will perform better modeling compared to the results shown here.

We attempt to learn a multi-faceted classification of user intent, which is inherently more sophisticated compared to learning the classical notion of user intent as informational , navigational or transactional . However, the closest analogue in our case is the Task dimensions whose values are infor-mational , not information or both .

Table 4 shows the classification results for the Task di-mension when 50% of the queries was used as training data, and the remaining for testing. An average accuracy of 76% is achieved by FastQ in the classification of Task . not shown here due to space restriction from N = 10 independent trials.
 Table 5: Comparison of existing approaches for in-ten t classification with size of the dataset and the classification accuracy. (  X  Results of [17] are based on TREC test collection.)
Table 5 shows FastQ compares favorably to existing meth-ods. We re-iterate the multi-dimensional nature of our work, i.e. Task is one of the nine facets which characterize user intent in our case, compared to past approaches.
A comparison of the results in Table 3 and Figure 3(a)-3(c) show the incorporation of WordNet for computing the influ-ence of new words in test query based on semantic neigh-bours in the existing vocabulary improves the quality of re-sults in WN compared to BASE , especially at low amounts of training data (1  X  10%).

Figure 3(e) shows that around 33% of the new words are semantically related to known words in the training data. This might be surprising, but this is explained from the characteristics of the training set that only includes 15% of queries from the head of the query distribution. Thus, incorporating WordNet may allow a better modeling for a significant portion of unseen words in a practical web search scenario.

Figure 3(d) shows the histogram of the number of related words in WordNet, experimentally observed, for a query word that is not present in training data, and its average  X  c wn  X  8. This affects the time spent in search for related candidates of a word not present in training data.
The experiments reported used a breadth first search till maximum depth 3 using the words falling in the synsets category. We observed that the similarity scores for the neighbours often is either very close to one, or slightly above zero.
The average time for query classification at WordNet max-imum search depth 3 was observed to be 2  X  3 ms on an AMD Turion64 machine with 4 GB RAM, which is considerably less than the average latency for a search query which is 200 ms [8]. This allows the incorporation of FastQ as input to the ranking algorithm of a search engine; using the user intent information to improve the user experience.
In addition, our results show that with a small training set (a few thousand queries), we can get good predictive performance. Hence, with training sets available for large sear ch engines the results might be quite better.
We have presented an efficient and convergent algorithm for inferring the intent of the web search query, described in terms of multiple facets. Experimental results show that modelling the latent relationships between facets improves the accuracy of prediction. Further, the incorporation of WordNet improves intent classification using the semantic relationships in the language vocabulary.

One possible area of improvement is the usage of multi-ple semantic relationships, e.g. synonyms, antonyms, hy-ponyms , available in WordNet, which might improve the ef-ficacy of WordNet in intent classification.

Our framework model can be used for fast contextualized ranking of results of a user query with or without click-through data. For example, it can be used for a much more refined query classification and clustering. Similarly, the in-tent classification can be used as input to existing search engine ranking technology [5]. Alternatively, we can use the facet classifications for sorting and displaying the search en-gine results in a different way creating a new user experience.
In addition, the generality of the framework allows in-tegration of alternative aspects of user intent making it a powerful tool for contextualizing and personalizing search. [1] A. Ashkan, C. L. Clarke, E. Agichtein, and Q. Guo. [2] R. Baeza-Yates, L. Calder  X on-Benavides, and [3] A. Z. Broder. A taxonomy of web search. SIGIR [4] L. Calder  X on-Benavides, C. Gonz  X alez-Caro, and [5] O. Chapelle and S. S. Keerthi. Efficient algorithms for [6] C. I. Chow, S. Member, and C. N. Liu. Approximating [7] T. M. Cover and J. A. Thomas. Elements of [8] J. Dean. Challenges in building large-scale information [9] D. Downey, S. Dumais, and E. Horvitz. Heads and [10] D. Downey, S. Dumais, D. Liebling and E. Horvitz. [11] F. Eaton and Z. Ghahramani. Choosing a variable to [12] P.-V. Eds. EuroWordNet. A multilingual database with [13] G. Hripcsak and A. S. Rothschild. Agreement, the [14] J. Hu, G. Wang, F. Lochovsky, J. T. Sun, and [15] B. J. Jansen, D. L. Booth, and A. Spink. Determining [16] B. J. Jansen, A. Spink, and I. Taksa. Handbook of [17] I.-H. Kang and G. Kim. Query type classification for [18] F. R. Kschischang, B. J. Frey, and H.-A. Loeliger. [19] U. Lee, Z. Liu, and J. Cho. Automatic identification of [20] Y. Liu, M. Zhang, L. Ru, and S. Ma. Automatic query [21] M. Meila. An accelerated chow and liu algorithm: [22] M. Mendoza and R. Baeza-Yates. A web search [23] V. B. Nguyen and M.-Y. Kan. Functional faceted web [24] E. Pitler and K. Church. Using word-sense [25] J. J. Randolph. Free-marginal multirater kappa: An [26] D. E. Rose and D. Levinson. Understanding user goals [27] V. Y. F. Tan, A. Anandkumar, L. Tong, and A. S. [28] J. Teevan, S. Dumais, and E. Horvitz. Potential for [29] J. Teevan, S. Dumais, and D. Liebling. To personalize [30] M. J. Wainwright and M. I. Jordan. Graphical Models, [31] R. White, S. Dumais, and J. Teevan. Characterizing [32] Web query classification. http://en.wikipedia.org/
