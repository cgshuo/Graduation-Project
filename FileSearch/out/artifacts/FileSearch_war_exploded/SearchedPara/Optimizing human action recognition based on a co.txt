 1. Introduction
Nowadays, human behavior analysis (HBA) is gaining more and more interest in the fi eld of arti fi cial intelligence and machine learning. Motivated by the wide possible application areas as gaming, natural user interfaces or assistive technologies, just to name a few, great advances have been made in the learning and recognition of human behavior, especially by means of computer vision techniques ( Moeslund et al., 2006 ). For instance, in the case of video surveillance, initially the main goal was to handle person detection, identi fi cation (and re-identi fi cation) and tracking, whereas activity recognition and scene analysis currently spark the greatest interest, not only of the researchers but also of the industry ( Wang, 2013 ). Speci fi cally, human action recognition (HAR) deals with the lowest level of semantic interpretations of basic human behaviors. For example, motion-based actions as walking, jumping or falling fi t into this category. This essential part allows to process further recognition stages. In combination with scene analysis and event detection techniques, complex human activities and long-term behaviors or routines can be recognized ( Jiang et al., 2013 ), for instance, in smart home environments ( Rho et al., 2012 ; Silva et al., 2012 ; Chaaraoui et al., 2012b ).

In this paper, a state-of-the-art vision-based human action recognition method is used addressing multiple optimization targets. First, we seek the best possible set of instances. Due to different kinds of recording errors, noise and instance-or subject-related peculiarities (as clothes, body build, etc.), not all the available instances of a training set are equally useful. Whereas having more samples is usually valuable to learn the intra-class variance of an action class, this is not the case for random noise appearances and outlier values, which tend to spoil and over the learning model ( Cano et al., 2005 ). Furthermore, fi the redundant instances and optimizing the set of instances to the smallest one which maintains or improves the initial recognition rate lead to a signi fi cant spatial and temporal improvement.
Second, feature subset selection is applied in order to obtain the optimal selection of elements out of a feature vector. Also in this case, redundant or noisy feature elements can be discarded, which bene fi ts the work of the classi fi er ( Cant X -Paz, 2004 ). In our case, a human silhouette-based feature is employed whose spatial organization follows a radial fashion. This leads to the natural relation between feature elements and body parts. Certainly, depending on the action to recognize, some body parts may be more relevant than others, and some can be discarded completely.
Finally, the optimal values of the algorithm's parameters are determined in order to achieve the best empirical con fi guration, i.e. the one that leads to the highest recognition rate. The use of evolutionary algorithms for parameter selection is the most common, since it has been applied for decades ( De Jong, 1975 ). of time, a cooperative coevolutionary algorithm is proposed.
Unsupervised selection of instances, features and parameter values is performed simultaneously by using a coevolutionary algorithm with a cooperative behavior. This choice is motivated by the fact that coevolutionary algorithms make it possible to split the domain of the problem relying on the divide and conquer strategy, and tackle each part of the optimization problem with respect to their different solution spaces and data types. Further-more, the cooperative coevolution allows us to consider the intrinsic dependencies which may exist between optimization goals by using a global fi tness function and evaluating the coopera-tion between populations ( Derrac et al., 2012 ). As Section 5 shows, a signi fi cant increase in recognition accuracy and a considerable decrease in spatial and temporal complexity are achieved with this proposal, leading to outstanding results on publicly available datasets.
 summarizes recent and related work in human action recognition and data reduction techniques. A brief de fi nition of a cooperative coevolutionary algorithm is also included. In Section 3 , the human action recognition method which is our object of optimization is outlined. Section 4 details the coevolutionary algorithm that is proposed for simultaneous instance, feature and parameter selec-tion. Experimental results and a comparison with the state of the art are speci fi ed in Section 5 . Finally, we present conclusions and discussion in Section 6 . 2. Related work recognition and data reduction techniques are summarized. The necessary background on coevolutionary algorithms is also included. 2.1. Human action recognition can be categorized by the visual features they employ in order to classify a speci fi c image or a sequence of frames. These are either local (also known as sparse ) descriptors which describe character-istics of multiple relevant points or areas in the image, or global (also known es dense ,or holistic ) representations which encode the image information as a whole. Whereas the former mostly rely on color-and gradient-based information in order to detect and describe the points of interest, global features can rely on shape, motion and/or temporal data ( Poppe, 2010 ).
 human silhouettes (e.g. Bobick and Davis, 2001 ; Blank et al., 2005 ;
Tran and Sorokin, 2008 ; Weinland et al., 2006 ; Thurau and Hlav X  , 2007 and  X  kizler and Duygulu, 2007 ).Humansilhouettescanbe obtained based on image processing techniques as background subtraction, human body detection, or using infra-red, laser or depth cameras. Commonly background subtraction is applied to remove the static background from an image and extract the foreground. Then, a blob detector can identify the part of the foreground that corresponds to the human silhouette. This reduc es the problem to a single shape-based region of interest. Bobick and Davis (2001) proposed motion history and motion energy images (MHI, MEI), which respectively encode the age and the location of the motion at pixel-level over a sequence of frames. Weinland et al. (2006) extended this technique to a multi-view and viewpoint-independent motion history volume (MHV) by means of invariant motion descriptors in Fourier space.
Classi fi cation has been performed combining principal component analysis (PCA) and linear discriminant analysis (LDA) for dimension-ality reduction, and Mahalanobis distance for feature matching. Radial histograms of the human silhouette and the optical fl ow of the X -and
Y -axis are employed in Tran and Sorokin (2008) .Thisvisualdescriptor has successfully been used by other authors as, for example, in Li and
Zickler (2012) for cross-view action recognition.  X  kizler and Duygulu (2007) describe the human silhouette based on histograms of oriented rectangular patches extracted over the whole body. Then, different ways of considering the temporal domain are tested. Although best results have been achieved using dynamic time warping, frame-by-frame voting and global histogra mming achieved similar results, suggesting that dynamics are not indispensable.

Looking at related optimizations of HAR methods, we fi nd that feature subset selection has been applied previously with success.
In Jhuang et al. (2007) , feature subset selection by means of a support vector machine (SVM) is applied to position-invariant spatio-temporal features, resulting in a reduction of 24 times of the number of features. Spatio-temporal interest points are also used by Bregonzio et al. (2010) , where the global distribution information of interest points is exploited. Since the feature space dimension is very high, redundant features are eliminated. Feature selection is applied based on the relevance of each feature, i.e. the proportion of inter-class variation with respect to the intra-class variation. Kovashka and Grauman (2010) target to learn the most discriminative shapes of space  X  time feature neighborhoods.
Multiple kernel learning is employed in order to determine the appropriate distance metrics between interest points. Entropy is used as a measure of importance in Ikizler et al. (2008) ,soasto choose the region of the human body where most of the motion occurs. In this way, the feature size of a histogram of orientations of border lines could be reduced by a factor of three. 2.2. Data reduction based on evolutionary algorithms
As has been brie fl yintroducedin Section 1 , two of our optimiza-tion targets address data reduction. These are to fi nd the best performing selection of instances and feature subset. As stated in
Liu and Motoda (2002) , this can be seen as selecting rows (training instances) and columns (features) out of the training data. In this sense, a two-fold objective is pursued. First, the recognition rate can be improved by fi ltering noisy and outlier data (which could lead to over fi tting, Wilson and Martinez, 2000 ), obtaining a more consistent learning model. Second, execution time can be reduced without compromising the success rate if the redundant training data is ignored. Note at this point that obt aining suboptimal selections in acceptable execution times is pursued, since to assure condition of optimality would require exhaustive search algorithms.
Whereas a solid state of the art exist s regarding instance selection ( Wilson and Martinez, 2000 ; Jankowski and Grochowski, 2004 ;
Grochowski and Jankowski, 2004 ), evolutionary algorithms (EA) for this purpose are still sparingly being used. Cano et al. elaborated a comparison between evolutionary and non-evolutionary instance and feature selection methods, and concluded that the former consistently performedbetterinbothtermsofrecognitionaccuracyandspatial and temporal performance. A generational genetic algorithm (GA), a steady-state GA, a heterogeneous recombination and cataclysmic mutation (CHC) adaptive search algorithm, and a population-based incremental leaning speci fi cEAhavebeenincludedinthecomparison ( Cano et al., 2003 ). In Garc X a et al. (2008) , a memetic algorithm is proposed for instance selection, tackling the problem of selection in large scale databases. A cooperative coevolutionary algorithm is used for instance selection in Garc X a-Pedrajas et al. (2010) ,wherethe obtained results compared favorably with standard and also recently published state-of-the-art algorithms (see Garcia et al., 2012 ; Olvera-L X pez et al., 2010 for more details).

Regarding feature subset selection, the usage of evolutionary algorithms goes back a long way ( Siedlecki and Sklansky, 1989 ). Commonly, the approach relies on a binary selection. For example, in the case of a genetic algorithm, an individual is built up by an array of genes which indicate whether or not a speci fi c feature is selected. Then, the feature subset is evaluated either based on intrinsic properties as distance, dependence or consistency (so-called model), or using the actual learning algorithm, where the feature subset is going to be used, as a fi tness function (known as the wrapper model) ( Cant X -Paz, 2004 ; Casado Yusta, 2009 ). This last one comes with the disadvantage that each feature subset selection needs to be tested going through a complete classi fi cation process. Nonetheless, it embed the feature selection in the construction of the classi during training the classi fi er selects the appropriate features to improve the results ( Guyon and Elisseeff, 2003 ). Besides, as stated by Espejo et al. (2010) , the application of genetic programming (GP) for inducing classi fi ers usually implies a feature selection process which is inherent to the evolution of classi fi ers.

Finally, some other works apply both instance and feature subset selections simultaneously. Kuncheva and Jain (1999) used a GA for this purpose overcoming the disadvantages of a consecutive approach. A similar method is presented in Ros et al. (2008) , where additional heuristics are considered to promote diversity and elitism in the population. A cooperative coevolutionary algo-rithm is successfully employed in Derrac et al. (2010) on datasets of different data nature. McIntyre and Heywood (2011) and Doucette et al. (2012) combined competitive and symbiotic (coop-erative) coevolution multi-objective optimization and GP classi-fi ers. Competition provides a mechanism for scaling to potentially large unbalanced datasets while cooperation allows the decom-position of the training set to improve the results. Feature subset selection is embedded in the GP classi fi ers. 2.3. Cooperative coevolutionary algorithms
A coevolutionary algorithm (CEA) can be de fi ned as one or more EA, in which the fi tness value of an individual of one of the populations depends on its relationships to the individuals from the other populations ( Wiegand, 2004 ). In other words, the search problem is divided into sub-problems, where each population handles one of them separately. Nonetheless, the pro fi ciency of the individuals is evaluated in a correlative way ( Derrac et al., 2010 ). Coevolutionary algorithms can be categorized by means of the type of relation between individuals. Whereas cooperative CEA reward individuals that work well together, competitive CEA follow a predator  X  prey relationship rewarding those individuals whose opponents perform poorly against them ( Wiegand, 2004 ).
Coevolutionary algorithms are being used successfully in dif-ferent domains as process planning and scheduling ( Kim et al., 2003 ), multiobjective optimization ( Tan et al., 2006 ) and cluster-ing ( Potter and Couldrey, 2010 ), among others. 3. Human action recognition method
In this section, the human action recognition method is detailed, whose performance is our optimization objective. This multi-view human action recognition method relies on multi-view silhouette-based pose representations, and performs recognition of sequences of key poses. The present proposal builds upon previous contribu-tions ( Chaaraoui et al., 2012a , 2013 ). In Chaaraoui et al. (2012a) , an action learning method based on a bag of key poses is presen-ted, and different types of multi-view fusion are considered.
In Chaaraoui et al. (2013) , the recognition of actions based on sequences of key poses is introduced. In the present work, these proposals are further enhanced with a novel multi-view pose representation and a weighted feature fusion scheme in which the quality of each viewpoint is learned. The complete outline of the resulting method is detailed in the following. 3.1. Multi-view pose representation
Our method relies on the input of multiple cameras covering the same fi eld of view. Therefore, a multi-view pose representa-tion is computed based on the single-view pose representations. 3.1.1. Single-view pose representation
Initially, we assume that human silhouettes are obtained out of the RGB video frames. These are usually processed by means of background subtraction or human body detection techniques. But it is also possible to obtain them using infra-red, laser or depth cameras. We process these binary segmentations in order to extract the contour points p 1 ; p 2 ; ... ; p n of the human silhouette.
Using these points, the pose representation is obtained as follows: 1. The centroid C of the silhouette is computed as
C  X  X  x c ; y c  X  ;  X  1  X  x  X  2. The Euclidean distance between each contour point and the centroid is obtained: d  X  J C p i J ; 8 i A  X  1 ... n :  X  3  X  3. Then, a radial scheme is applied in order to spatially align the parts of the human silhouette with independence from the speci fi c shape and the resulting contour length. The radial bin s of each contour point is determined as (for the sake of simplicity  X  i  X  0 is considered as  X  i  X  360  X  s  X  S  X  where S stands for the number of radial bins. 4. Finally, the contour points of each radial bin are summarized into a single value based on the range of change between their distances to the centroid: v  X  max  X  d k ; d k  X  1 ; ... ; d l  X  min  X  d k ; d k  X  1 ; ... ; s k ... s l  X  j 4 k ; l A  X  1 ... n ; 8 j A  X  1 ... S :  X  6  X  5. These summary values are normalized and concatenated into the resulting feature vector:
V  X  v 1 J v 2 J ... J v S ;  X  7  X  v  X  v j  X  S
The shape-based feature vector V has the advantage of pre-senting a very low dimensionality and a reduced computation time. Fig. 1 shows a graphical explanation of the feature extraction process.
 3.1.2. Weighted feature fusion scheme fusion based approach. In other words, the single-view features are joined together to a multi-view pose representation. Although other strategies have been considered ( Chaaraoui et al., 2012a ), this method proved to have the required robustness with an increased number of views.
 feature concatenation. Additionally, a weighted feature fusion scheme is proposed in order to consider the quality of each viewpoint. Depending on the action class, the orientation and the location of the subject, some viewpoints may be more useful than others to recognize what a person is doing. For this reason, camera weights need to be class-speci fi c, and should be obtained for each camera setup.

The camera weights are obtained as follows: w m ; a  X  Test  X  m  X  a ; 8 m A  X  1 ... M 4 8 a A  X  1 ... A ; where M stands for the number of available camera views and
A action classes are being learned. Test ( m ) returns the per-class recognition results of a single-view test using view m . In this way, the camera weights for each action class are determined based on the success rates of that view. Then, these weights are normalized to unit-sum. 3.2. Bag of key poses
Once the multi-view pose representations are extracted, the most representative and common poses are learned. This way, the characteristic instances of each action class can be retained, and redundant examples can be ignored. These multi-view key poses are obtained using K -means clustering and taking the resulting cluster centers as representatives. The key poses of each action class are joined together in the same bag of key poses. As it can be seen in Fig. 2 , we take the per class pose representations obtained as detailed in Section 3.1 , and apply the clustering algorithm in these are the parameters which are selected by means of the evolution of the parameter population. Further insight about the bag-of-key-poses model can be looked up in previous work ( Chaaraoui et al., 2012a ). 3.3. Sequences of key poses
So as to model the temporal relation between key poses, i.e. the typical order and transitions between them, sequences of key poses are built. In this manner, for each of the available training sequences, each pose representation is substituted with its nearest neighbor key pose out of the bag of key poses. The successive key poses make up a sequence of key poses Seq  X f kp 1 ; kp 2 ; ... ; This step achieves to fi lter noise and outlier values, due to the transition to the common domain of the bag of key poses.
In order to classify a new action performance, the same steps are taken till an equivalent sequence of key poses is obtained. Then, sequence matching is employed in order to perform recog-nition. For this purpose, dynamic time warping (DTW) has been chosen, because it inherently supports the alignment of sequences with non-uniform speeds, which is especially needed at action performances. The best match is determined based on the lowest DTW distance d DTW  X  Seq ; Seq  X   X  : d
DTW  X  Seq ; Seq  X   X  X  dtw  X  t ; u  X  ;  X  10  X  dtw  X  i ; j  X  X  min where the distance between key poses d  X  kp i ; kp  X  j  X  takes into account the previously obtained camera weights. For this purpose, the action class a of the known sequence Seq is considered so as to apply the appropriate weights. In other words, we suppose that the current comparison is a correct match, and use the weights that should be used in that case, since these indicate to which degree each view should be considered. d  X  kp i ; kp  X  j  X  X   X  M where kp i  X  V 1 J V 2 J ... J V M and kp  X  j  X  V  X  1 J V
In this way, the label of the best matching training sequence determines the result of the multi-view recognition. 4. Cooperative coevolutionary algorithm for instance, feature and parameter selection
In this section, the proposed cooperative coevolutionary algo-rithm for the simultaneous selection of instances, features and parameter values is detailed. For this purpose, we will present how the populations and the individuals are de fi ned, and which steps are executed in the coevolution. 4.1. Population structure
Since our problem of fi nding an optimal classi fi cation con uration is divided in the search of a selection of instances, a feature subset and parameter values, we use these sub-problems to de three populations. Individuals out of each population are com-bined to build a possible solution that can be evaluated using the fi tness function, which in our case relies on the success rate of the human action recognition algorithm using the con fi guration determined by the individuals (see Fig. 3 ). 4.2. Individuals ' representation
Instance and feature subset selections are regarded as binary selections. This means that a training instance is either used or not during learning (it does not make sense to apply this in the recognition stage), and that a speci fi c feature element of the feature vector is excluded during the whole classi fi cation. There-fore, the individuals of these two populations are encoded as
Boolean arrays, in which each gene indicates whether or not this element is selected. The instance population's individuals have
I elements, one for each training instance. And the feature population's individuals have one element for each of the F elements of the feature vector.

Parameter values are more closely related to the used HAR method. In this case, as has been seen in Section 3 , the method learns a parametrized number of key poses for each action class.
Therefore, for A action classes, the same amount of parameters need to be indicated, leading to an individual of A integer values.
Speci fi cally, the parameters K 1 ; K 2 ; ... ; K A which indicate the per class number of key poses are learned in order to setup and optimize the method. 4.3. Coevolutionary algorithm Algorithm 1. Cooperative coevolutionary algorithm.

Initialise randomly three populations for instances, features and parameters with N I , N F and N A individuals respectively
Order each population by descending fi tness repeat proposed coevolutionary algorithm. Following details should be considered: 1. The individuals that are used for recombination and mutation 2. A one-point crossover recombination operator is employed. 3. In the case of the instance and feature populations, the 4. The fi tness value is obtained by combining one individual from each population ( i 1 , i 2 and i 3 ) and evaluating the success rate of the human action recognition method with the con fi guration encoded in the individuals (see Section 3 ). As Algorithm 1 shows, whereas i 1 constitutes the new individual, i 2 and i selected out of their populations based on ranking. As stated in Coello et al. (2006) , to choose the best individuals would cause undersampling and excessive greediness in dependant populations. 5. The obtained fi tness value is adopted by the new individual i and by i 2 and i 3 if it improves their current fi tness value. 6. When populations are ordered by descending fi tness value, we also apply an optimization of spatial and temporal constraints.
If two individuals present the same fi tness value, the most ef fi cient one is given priority. This means that in the case of the instance and feature populations, the individual with less selected values is favored. In the parameter population, the individual with less accumulated sum is preferred, since a higher value indicates a greater amount of key poses and this results in a more costly classi fi cation.

Fig. 4 shows how the proposed coevolutionary algorithm employs the wrapper model that has been seen in Section 2.2 in order to optimize the recognition of human actions. 5. Experimentation
In this section, the performed experimentation on two publicly available datasets is detailed. These two datasets present notorious differences. Whereas the Weizmann dataset ( Blank et al., 2005 ) constitutes one of the basic and most popular human action recognition datasets and provides automatically obtained binary segmentations, the MuHAVi dataset ( Singh et al., 2010 ) includes multi-view data for up to 14 different action classes and comes along with a subset of manually obtained human silhouettes.
In order to test the accuracy of the recognition of the proposed method, a leave-one-out cross validation has been employed.
In this type of test, the dataset is divided into several subsets, so as to iterate over these parts using one of them for testing, and all the others for training. This procedure is repeated for all available subsets and the accuracy scores are averaged. In this way, leave-one-sequence-out cross validation (LOSO) tests the robustness of the method toward instance-variance, whereas leave-one-actor-out cross validation (LOAO) does so for actor-variance, regarding the pro fi ciency of the method in recognizing unseen actors. As in these cross validations training and testing sets change in each fold, an over fi tted result of the optimization can be avoided.
The speci fi c con fi guration details about the performed tests are indicated as follows: 1. The camera weights used at the weighted feature fusion scheme are obtained based on the success rates of the same test that is being performed, but with single-view data. 2. The size of the instance and parameter individuals is given by the speci fi c dataset, i.e. I equals the number of training instances, and A equals the number of action classes to recognize. 3. The number of elements of the feature vector F  X  S M , because in the multi-view recognition S feature vector ele-ments are employed for each of the M views. 4. The indicated results have been obtained with populations of ten individuals  X  N I  X  N F  X  N A  X  10  X  and a single offspring. 5. Regarding the mutation operator, random probability values mut I ; mut F and mut P are employed. The two possible mutations of the parameter individuals detailed in Section 4.3 are chosen with a 50  X  50 chance. A range between 5 and 130 key poses is considered for the parameters K 1 ; K 2 ; ... ; K A . 6. We set gen max  X  250, i.e. the evolution is considered to be stable after 250 generations without changes in its best performing individual. 7. The number of radial bins S is detailed for each test. Its value has been obtained based on a statistical analysis of the classi fi cation results of the human action recognition method from Section 3 . Since this parameter is needed in order to apply the feature selection, tests have been executed before applying any optimization. Besides S , the method also relies on the parameters K 1 ; K 2 ; ... ; K A , which determine the number of key poses per action class. Although these parameters will be optimized later on, they are needed in order to analyze the sensitivity of the algorithm with respect to its parameters.
Because of the non-deterministic behavior of the K -means algorithm, we run ten tests with each con fi guration. The median success rates obtained on the Weizmann dataset are shown in Fig. 5 , where the same K value has been established for all the classes. As it can be observed, the parameter K does not present a great in fl uence on the recognition rate. Different action classes involve distinct kinds of motion. For some actions, more or less key poses may be required in order to capture the relevant areas of the feature space. Therefore, choosing the same value for all the action classes will favor some and hinder others, leading for this reason to similar results on average. However, it can be seen that the parameter S has a direct impact on the recognition rate. In order to obtain reproducible results, we chose the value of S based on the highest median success rate, which has been obtained with
S  X  14 in the case of the Weizmann dataset (94.6%). If multiple con fi gurations return the same result, the lowest value will be chosen, as this reduces the feature size and the computational cost of the classi fi cation. 5.1. Benchmarks 5.1.1. Weizmann dataset
The Weizmann dataset ( Blank et al., 2005 ) includes 93 sequences of ten different action classes which have been performed by nine actors. Even if it is intended for sing le-view human action recognition only, it is commonly used as baseline benchmark. Table 1 shows a comparison of our results, before and after applying the proposed CEA optimization, and the ones that can be found among the state of the art. Several works excluded the skip action because it tends to decrease the overall recognition rate, due to its inter-class similarity.
Therefore, we indicated the number of actions used in each test. All of them report results for the LOAO cross validation.

It can be observed that the presented method of Section 3 achieves a high recognition rate, and that by means of the applied
CEA optimization of both learning and classi fi cation stages, perfect recognition is reached. 5.1.2. MuHAVi dataset
The MuHAVi dataset ( Singh et al., 2010 ) is a more extensive multi-view benchmark for human action recognition. In MuHAVi-
MAS, a manually annotated subset of two views and either 14 (MuHAVi-14) or 8 (MuHAVi-8) action classes is provided. In the 136 available sequences, two actors performed these actions up to four times. In difference to MuHAVi-14, in MuHAVi-8 the direction in which an action is performed is ignored. This means that although fewer action classes need to be recognized, these are more dif fi cult to learn since they present more differences.
Tables 2 and 3 show the obtained results. As is usual for this dataset, we applied both the LOSO and LOAO cross validations.
In this sense, whereas LOSO is used to provide a measure for the overall recognition performance, LOAO is used to show the robustness of the method to speci fi c actor-related differences, as clothes, body build or ways to perform an action. Regarding the state-of-the-art recognition rates on this dataset, we can observe that lower results are reported in the case of the LOAO cross validation. Our method not only outperforms all the available recognition rates on both versions of the dataset, but it also reaches perfect recognition on all four tests with the proposed optimization based on a coevolutionary algorithm. To the best of our knowledge, these are the highest results reported so far on this dataset. 5.2. Optimization results
For each of the applied tests, the values of the best performing individuals from the three populations are detailed. The instances column shows the number of training instances that have been selected out of the available ones, and with which the highest recognition results have been obtained. In the case of the features column, the feature subset selection of the multi-view pose representation is detailed. Finally, the number of key poses used to represent the action classes is indicated in the parameters column.

On average, the coevolutionary algorithm reduced the required training instances to 66%. The feature subset used during learning and classi fi cation has been reduced by 36%. Best results have been obtained representing each action class with 26 key poses on average.

With the purpose of providing further insight about how the obtained results correspond to the data, we analyzed the Weiz-mann LOAO cross validation test. In Table 5 , the resulting selection of instances is given in terms of both actors and action classes. It can be observed that the sequences of actors as Moshe and
Shahar got selected less, which could be related to the way they performed some of the actions. For instance, the run sequence from Shahar is performed from left to right, whereas most of the others are performed from right to left. Regarding the action-wise selection, we can observe that a fairly similar distribution of actions has been selected, since samples from all actions are necessary in order to recognize them. In the case of wave1 , less samples are necessary because all of them are performed with the right hand and mostly only the noise of the background segmen-tation differs between the samples.
 Applying a similar analysis to the obtained feature subset, in
Fig. 6 , it can be seen which feature elements of the radial scheme have been discarded here due to redundant or noisy components.
Finally, regarding the selection of parameter values, note that the data given in Table 4 follows the alphabetical order of the action classes. In this case, signi fi cant differences can be observed regarding the different number of key poses that were necessary to model the representative poses of action classes as run ( K  X  19) and wave2 ( K  X  106) and obtain the best result.

To illustrate the performance gain achieved with this optimiza-tion, we performed a temporal evaluation on the Weizmann dataset. Our proposal has been implemented using the NET
Framework and the OpenCV library ( Bradski, 2000 ). Tests have been performed on a standard PC with an Intel Core 2 Duo CPU at 3 GHz and 4 GB of RAM running Windows 7 64, without apply-ing any hardware-related optimizations. The learning stage of the algorithm is executed in 0.72 s, whereas the testing stage requires 3.29 s for the whole dataset. Applying the obtained con fi an improvement of 41% and 33% can be observed. The fi nal recognition rate considering the binary silhouette images as input is 210 frames per second (FPS). 6. Discussion and future work
In this paper, a human action recognition optimization based on a cooperative coevolutionary algorithm is presented. By means of evolutionary search, the redundant or noisy training instances which confuse or unnecessarily extend the learning process are fi ltered. Similarly, the feature subset which includes the relevant parts of the human body in order to recognize human actions is selected considering multiple views. Last but not least, the appro-priate number of key poses is sought in order to represent the different poses involved in each action class. This con fi employed in a human action recognition algorithm, which relies on a multi-view silhouette-based pose representation and a weighted feature fusion scheme, and performs action recognition by means of matching sequences of key poses.

Results obtained during the experimentation have shown that our proposal achieves to improve the initial recognition rates, reaching perfect recognition for all the applied benchmarks, and considerably outperforming the available rates of the MuHAVi dataset. Furthermore, the execution time and memory needs of the learning algorithm are reduced, since the evolutionary algo-rithm implicitly prioritizes the more ef fi cient con fi gurations.
In conclusion, the presented work achieves to successfully optimize both the recognition, and the temporal and spatial performance of human action recognition. Since it only has to be applied as a pre-classi fi cation stage, it is also suitable for online human action recognition, where the obtained con fi guration can be applied.

In future works, further experimentation on larger and more complex datasets is going to be performed in order to analyze which margins of optimization can be reached. So as to fi optimal con fi guration of a HAR method intended to perform online in a real world application, a compound of tests could be used assigning the appropriate weights to consider the speci fi the system. Furthermore, if so required by the application, not only the recognition rate, but also the execution time could be used as optimization function.
 Acknowledgments This work has been partially supported by the European
Commission under project  X  caring4U  X  A study on people activity in private spaces: towards a multisensor network that meets privacy requirements  X  (PIEF-GA-2010-274649) and by the Spanish
Ministry of Science and Innovation under project  X  Sistema de visi X n para la monitorizaci X n de la actividad de la vida diaria en el hogar  X  (TIN2010-20510-C04-02). Alexandros Andre Chaaraoui acknowledges fi nancial support by the Conselleria d'Educaci X , Formaci X  i Ocupaci X  of the Generalitat Valenciana (fellowship
ACIF/2011/160). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. We sincerely thank the reviewers for their construc-tive and insightful suggestions that have helped to improve the quality of this paper.
 References
