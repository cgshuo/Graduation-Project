 Recognizing sarcasm is important for understand-ing people X  X  actual sentiments and beliefs. For example, failing to recognize the following mes-sage as being sarcastic  X  X  love that I have to go back to the emergency room X , will lead a senti-ment and opinion analysis system to infer that the author X  X  sentiment is positive towards the event of  X  X oing to the emergency room X . Current ap-proaches have framed the sarcasm detection task as predicting whether a full utterance is sarcastic or not (Davidov et al., 2010; Gonz  X  alez-Ib  X  a  X  nez et al., 2011; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014).

We propose a re-framing of sarcasm detection as a type of word sense disambiguation problem: given an utterance and a target word, identify whether the sense of the target word is literal or sarcastic . We call this the Literal/Sarcastic Sense Disambiguation (LSSD) task. In the above utter-ance, the word  X  X ove X  is used in a sarcastic, non-literal sense (the author X  X  intended meaning be-ing most likely the opposite of the original literal meaning -a negative sentiment, such as  X  X ate X ). Two key challenges need to be addressed: 1) how to collect a set of target words that can have a lit-eral or a sarcastic sense, depending on context; and 2) given an utterance containing a target word, how can we determine whether the target word is used in its literal sense (e.g.,  X  X  love to take a nice stroll in the park every morning X ), or in a sarcastic sense (e.g.,  X  X  love going to the dentist. X ).
To address the first challenge, we need to iden-tify a set of words from sarcastic utterances, which have a figurative/sarcastic sense (e.g.,  X  X ove X  in the utterance  X  X  love going to the dentist X ). We pro-pose a crowdsourcing task where Turkers in Ama-zon Mechanical Turk (MTurk) platform are given sarcastic utterances (tweets labeled with #sarcasm or #sarcastic hashtags) and are asked to re-phrase those messages so that they convey the author X  X  in-tended meaning ( X  X  love going to the dentist X  can be rephrased as  X  X  hate going to the dentist X  or  X  X  don X  X  like going to the dentist X ). 1 Given this parallel dataset, we use unsupervised alignment techniques to identify semantically opposite words (e.g.,  X  X ove X   X   X  X ate X ,  X  X rilliant X   X   X  X tupid X ,  X  X ever X   X   X  X lways X ). The words from these pairs that appear in the original sarcastic utterances are then considered as our collection of target words (e.g.,  X  X ove X ,  X  X rilliant X ,  X  X ever X ) that can have both a sarcastic and a literal sense depending on the context (Section 2).

To address the second challenge, we compare several distributional semantics methods generally used in word sense disambiguation tasks (Sec-
Table 1: Examples of Targets and their Senses tion 3). We show that using word embeddings in a modified SVM kernel achieves the best results (Section 4). To collect training and test datasets for each of the target words, we use Twitter mes-sages that contain those words. For the sarcas-tic sense ( S ), we use tweets that contain the target word and are labeled with the #sarcasm or #sar-castic hashtags. For the literal sense ( L ), we col-lect tweets that contain the target word and are not labeled with the #sarcastic or #sarcasm hash-tags. Table 1 shows examples of two targets words ( X  X reat X  and  X  X roud X ) and their sarcastic sense ( S ) and literal sense ( L ). In addition, for the literal sense , we also consider a special case, where the tweets are labeled with either positive or nega-tive hashtags (e.g., #happy, #sad) as proposed by Gonzalez et al. (2011). We denote these senti-ment tweets as L sent (Table 1). Gonzalez et al. (2011) argue that it is harder to distinguish sar-castic from non-sarcastic messages where the non-sarcastic messages contain sentiment. Our results support this argument (97% F1 measure for the best result for S vs. L , compared to 84% F1 for the best result for S vs. L sent ; Section 4). 2 To collect a set of target words that can have either literal or sarcastic meaning depending on context, we propose a two step approach: 1) a crowdsourc-ing task to collect a parallel dataset of sarcastic utterances and their re-phrasings that convey the authors X  intended meaning; and 2) unsupervised alignment techniques to detect semantically oppo-site words/phrases.
 Crowdsourcing Task. Given a sarcastic mes-sage (SM), Turkers were asked to re-phrase the message so that the new message is likely to ex-press the author X  X  intended meaning (IM). Exam-ples of an original sarcastic message (1) and three messages generated by the Turkers (2) is given be-low:
From the above examples, we can see that align-ing the sarcastic message (SM) to the re-phrasings containing the author X  X  intended meaning gener-ated by the Turkers ( IM 1 , IM 2 , IM 3 ) will al-low us to detect that  X  X appy X  can be aligned to  X  X on X  X  like X ,  X  X pset X , and  X  X nhappy X . Based on this alignment,  X  X appy X  will be considered as a target word for the LSSD task.

We used 1,000 sarcastic messages collected from Twitter using the #sarcasm and #sarcastic hashtags. The Turkers were provided with de-tailed instructions of the task including a defini-tion of sarcasm , the task description, and multi-ple examples. In addition, for messages that con-tain one or more sentences and where sarcasm is related to only a part of the message, the Turk-ers were instructed to consider the entire message in their rephrasing. This emphasis was added to avoid high asymmetry in the length between the original sarcastic message and the rephrasing of the intended meaning. For each original sarcas-tic message (SM), we asked five Turkers to do the rephrasing task. Each HIT contains 1 sarcastic message, and Turkers were paid 5 cents for each HIT. To ensure a high quality level, only quali-fied workers were allowed to perform the task (i.e., more than 90% approval rate and at least 500 ap-proved HITs). In this way, we obtained a dataset of 5,000 SM-IM pairs.
 Unsupervised Techniques to Detect Semanti-cally Opposite Words/Phrases. We use two methods for unsupervised alignment. First, we use the co-training algorithm for paraphrase detec-tion developed by Barzilay and McKeown (2001). This algorithm is used for two specific reasons. First, our dataset is similar in nature to the parallel monolingual dataset used in Barzilay and McK-eown (2001), and thus lexical and contextual in-formation from tweets can be used to extract the candidate targets words for LSSD. For instance, we can align the [SM] and [ IM 3 ] (from the above examples), where except for the words happy and unhappy , the majority of the words in the two messages are anchor words and thus happy and unhappy can be extracted as paraphrases via co-training. To model contextual information, such as part of speech tagging for the co-training algo-rithm, we used Tweet NLP (Gimpel et al., 2011). Second, Bannard and Callison-Burch (2005) no-ticed that the co-training method proposed by-Barzilay and McKeown (2001) requires identical bounding substrings and has bias towards single words while extracting paraphrases. This apparent limitation, however, is advantageous to us because we are specifically interested in extracting target words. Co-training resulted in 367 extracted pairs of paraphrases.

We also considered a statistical machine transla-tion (SMT) alignment method -IBM Model 4 with HMM alignment implemented in Giza++ (Och and Ney, 2000). We used Moses software(Koehn et al., 2007) to extract lexical translations by align-ing the dataset of 5,000 SM-IM pairs. From the set of 367 extracted paraphrases using Barzi-lay and McKeown (2001) X  X  approach, we selected only those paraphrases where the lexical transla-tion scores  X  (resulted after running Moses) are  X  0.8. After filtering via translation scores and manual inspection, we obtained a set of 80 seman-tically opposite paraphrases. Given this set of se-mantically opposite words, the words that appear in the sarcastic messages were consider our target words for LSSD (70 target words after lemmatiza-tion). They range from verbs, such as  X  X ove X  and  X  X ike X , adjectives, such as  X  X rilliant X ,  X  X enius X , and adverbs, such as  X  X eally X . Our Literal/Sarcastic Sense Disambiguation (LSSD) task is formulated as follows: given a candidate utterance (i.e., a tweet) that contains a target word t , identify whether the sense of t is sarcastic ( S ) or literal ( L ). In order to be able to solve this problem we need training and test data for each target word that consists of utterances where the target word is used either in the literal sense or the sarcastic sense. Table 2: Target words and # of training instances per sense 3.1 Data Collection To collect training and test datasets for each of the target words, we use Twitter messages that contain those words. For the sarcastic sense ( S ), we use tweets that contain the target word and are labeled with the #sarcasm or #sarcastic hashtag. For the literal sense ( L ), we collect tweets that contain the target word and are not labeled with the #sarcastic or #sarcasm hashtags. In addition, for the literal sense we also consider a special case, where the tweets are labeled with either positive or negative sentiment hashtags (e.g., #happy, #sad). Thus, we consider two LSSD tasks: S vs. L and S vs. L sent , and aim to collect a balanced dataset for each tar-get word.

For the 70 target words (see Section 2), we col-lected a total of 2,542,249 tweets via Twitter API . We considered a setup where 80% of data is used for training, 10% for development, and 10% for test. We empirically set the number of minimum training instances for each sense of the target word to 400 without any upper restriction. This resulted in 37 target words to be used in the LSSD exper-iments. Table 2 shows all the target words and their corresponding number of training instances for each sense ( S and L / L sent ). The size of train-ing data ranges from 26,802 for the target word  X  X ove X  to 427 for the word  X  X ature X . As we will see in the results sections, however, the size of the training data is not always the key factor in the LSSD task, especially for the methods that use word embeddings. 3.2 Learning Approaches We consider two classical approaches used in word sense disambiguation tasks: 1) distributional approaches where each sense of a target word is represented as a context vector derived from the training data; and 2) classification approaches ( S vs. L ; S vs. L sent ) for each target word. 3.2.1 Distributional Approaches The Distributional Hypothesis in linguistics is de-rived from the semantic theory of language usage, i.e., words that are used and occur in the same contexts tend to purport similar meanings (Harris, 1954). Distributional semantic models (DSMs) use vectors that represent the contexts (e.g., co-occurring words) in which target words appear in a corpus, as proxies for meaning representations. Geometric techniques such as cosine similarity are then applied to these vectors to measure the simi-larity in meaning of corresponding words.
 The DSMs are a natural approach to model our LSSD task. For each target word t we build two context-vectors that will represent the two senses of the target word t using the training data: one for the sarcastic sense S using the sarcastic training data for t ( ~v s ) and one for the literal sense L using the literal sense training data for t ( ~v l ). 3 Given a test message u containing a target word t , we first represent the target word as a vector ~v u using all the context words inside u . To predict whether t is used in a literal or sarcastic sense in the test message u we simply apply geometric techniques (e.g., cosine similarity) between ~v u and the two sense vectors ~v s and ~v l , choosing the one with the maximum score.

To create the two sense vectors ~v s and ~v l for each of the target words t , we use the posi-tive pointwise mutual information model (PPMI) (Church and Hanks, 1990). Based on t  X  X  con-text words c k in a window of 10 words, we sep-arately computed PPMI for sarcastic and literal senses using t  X  X  training data. The size of the con-text widow used in DSMs is generally between 5 and 10, and in our experiments we used a win-dow of 10 words since tweets often include mean-ingful words/tokens at the end of the tweets (e.g., interjections, such as  X  X ay X ,  X  X hh X ; upper-case words, such as,  X  X REAT X ; novel hashtags, such as  X #notreally X ,  X #lolol X ; emoticons, such as  X :( X ). We sorted the context words based on the PPMI scores and for each target word t we selected a maximum of 1,000 context words per sense to ap-proximate the two senses of the target word (i.e., the vectors ~v s and ~v l for each target word t consist of a maximum of 1,000 words). Table 3 shows some target words and their corresponding con-
Table 3: Target words and their context words text words that were selected based on high PPMI scores.

To predict whether t is used in a literal or sar-castic sense in the test message u we simply apply the cosine similarity to the ~v u (vector representa-tion of the target word t in the test message u ) and the two sense vectors ~v s and ~v l of t , choosing the one with the maximum score. All vector elements are given by the tf-idf values of the corresponding words. This approach, denoted as the  X  X PMI base-line X , is the baseline for our DSM experiments. Context Vectors with Word Embedding: The above method considers that the context vectors ~v s and ~v l of each target word t contain the co-occurring words selected by their PPMI values. We enhance the representation of context vectors to represent each word in the context vector by its word embedding. We experiment with three different methods of obtaining word embeddings: Weighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012b); word2vec that imple-ments the skip-gram and continuous bag-of-words models (CBOW) of Mikolov et al. (2013a), and GloVe (Pennington et al., 2014), a log-bilinear re-gression model based upon global word-word co-occurrence count in the training corpora.

After removing the tweets that are used as test sets, we build the three word embedding mod-els in an unsupervised fashion with the remaining 2,482,763 tweets from our original data collection (Section 3.1). In each of the three models, each word w is represented by its d -dimensional vec-tor ~w of real numbers, where d =100 for all of the embedding algorithms in our experiments. For the size of the embedding vectors, it is common to use 100 or 300 dimensions, with larger dimensions for larger datasets. Our current dataset is smaller than the ones used in other applications of word embed-dings (e.g., Pennington et al. (2014) have used bil-lion tweets to create word embedding) so we opted for 100 dimensional vectors. Below are the short descriptions of the three word embedding models:  X  Weighted Textual Matrix Factorization  X  word2vec Representation: We use both the  X  GloVe Representation: GloVe (Pennington et
Here, the LSSD task is similar to the baseline: to predict whether the target word t in the test mes-sage u is used in a literal or sarcastic sense, we simply use a similarity measure between the ~v u (vector representation of the target word t in the test message u ) and the two sense vectors ~v s and ~v of t , choosing the one with the maximum score. The difference from the baseline is twofold: First, all vectors elements are word embeddings (i.e., 100-d vectors). Second, we use the maximum-valued matrix-element (MVME) algorithm intro-duced by Islam and Inkpen (2008), which has been shown to be particularly useful for computing the similarity of short texts. We modify this algorithm to use word embeddings ( MV ME we ). The idea behind the MVME algorithm is that it finds a one-to-one  X  X ord alignment X  between two utterances (i.e., sentences) based on the pairwise word sim-ilarity. Only the aligned words contribute to the overall similarity score.
 Algorithm 1 MV ME we 1: procedure MV ME we ( v s , v u ) 4: M [ v s 5: for k  X  0 ,v s 7: ~c k  X  getEmbedding ( c k ) 8: for j  X  0 ,v u 10: ~w j  X  getEmbedding ( w j ) 11: M [ k ][ j ]  X  cosine ( ~c k , ~w j ) 12: end for 13: end for 14: while True do 15: repeat 16: max  X  getMax ( M ) 17: Sim  X  Sim + max 18: r m ,c m  X  getRowCol ( M,max ) 19: . Remove r m row and c m column from M 20: remove ( M,r m ,c m ) 21: until max &gt; 0 Or M.size () &gt; 0 22: end while 23: Return Sim 24: end procedure 27: Return we model [ word ] 28: end procedure 29: procedure GET R OW C OL (M,max) 30: row,col  X  M.indexOf ( max ) 31: Return row,col 32: end procedure
Algorithm 1 presents the pseudocode of our modified algorithm for word embeddings, MV ME we . Let the total similarity between ~v s and ~v u be Sim . For each context word c k from ~v s and each word w j from ~v u , we compute a ma-trix where the value of the matrix element M jk denotes the cosine similarity between the embed-ded vectors ~c k and ~w j [lines 5 -13]. Next, we first select the matrix cell that has the highest similarity value in M ( max ) and add this to the Sim score [lines 16-17]. Let the r m and c m be the row and the column of the cell containing max (maximum-valued matrix element), respectively. Next, we re-move all the matrix elements of the r m -th row and the c m -th column from M [line 20]. We repeat this procedure until we have traversed through all the rows and columns of M or max = 0 [line 21]. 3.2.2 Classification Approaches The second approach for our LSSD task is to treat it as a binary classification task to identify the sar-castic or literal sense of a target word t . We have two classification tasks: S vs. L and S vs. L sent for each of the 37 target words. We use the lib-SVM toolkit (Chang and Lin, 2011). Development data is used for tuning parameters.
 SVM Baseline: The SVM baseline for LSSD tasks uses n-grams and lexicon-based binary-valued features that are commonly used in exist-ing state-of-the-art sarcasm detection approaches 2014). They are derived from i) bag-of-words (BoW) representations of words, ii) LIWC dic-tionary (Pennebaker et al., 2001), and iii) a list of interjections (e.g.,  X  X h X ,  X  X h X ,  X  X eah X ), punc-tuations (e.g.,  X ! X ,  X ? X ), and emoticons collected from Wikipedia. CMU Tweet Tokenizer is em-unchanged when all the characters are upper-case (e.g.,  X  X EVER X  in  X  X  shooting in Oakland? That NEVER happens! #sarcasm X  ) but otherwise words are converted to lower case. We also change all numbers to a generic number token  X 22 X . To avoid any bias during experiments, we removed the target words from the tweets as well as any hashtag used to determine the sense of the tweet (e.g., #sarcasm, #sarcastic, #happy, #sad).
 SVM with MV ME we Kernel: We propose a new kernel kernel we to compute the semantic similarity between two tweets u r and u s using the MV ME we method introduced for the DSM ap-proach, and the three types of word embeddings (WTMF, word2vec, and GloVe). The similarity measure in the kernel is similar to the algorithm MV ME we described in Algorithm 1, but instead of measuring the similarity between the sense vec-tors of t ( ~v s , ~v l ) and the vector representation of t in test message ( ~v u ), now we measure the similar-ity between two tweets u r and u s . For each k -th index word w k in u r and l -th index word w l in u s we compute the cosine similarity between the embedded vectors of the words and fill up a sim-ilarity matrix M . We select the matrix cell that has the highest similarity, add this similarity score to the total similarity Sim , remove the row and column from M that has highest similarity score, and repeat the procedure (similar to Algorithm 1). We noticed that MV ME we algorithm carefully chooses the best candidate word w l in u s for the w k word in u r since w l is the most similar word to w k . The algorithm continues the same procedure for all the remaining words in u r and u s . The fi-nal Sim is used as the kernel similarity between u r and u s . We augment this kernel kernel we into libSVM and during evaluation we run supervised LSSD classification for each target word t sepa-rately. Tables 4 and 5 show the results for the LSSD experiments using distributional approaches and classification-based approaches, respectively. For brevity, we only report the average Precision (P), Recall (R), and F1 scores with their standard deviation (SD) (given by  X   X   X ), and the targets with maximum/minimum F1 scores. w 2 v sg and w 2 v cbow represent the skip-gram and CBOW mod-els implemented in word2vec, respectively.

Table 4 presents the results of distributional approaches (Section 3.2.1). We observe that the word embedding methods have better perfor-mance than the PPMI baseline for both S vs. L and S vs. L sent disambiguation tasks. Also, the average P/R/F1 scores for S vs. L are much higher than for S vs. L sent . Since all tweets with L sent sense were collected using sentiment hash-tags (Gonz  X  alez-Ib  X  a  X  nez et al., 2011), they might be lexically more similar to the S tweets than the L tweets are and thus identifying the sense of a tar-get word t between S vs. L sent is a harder task. In Table 4 we also observe that the average F1 scores between WTMF, w 2 v sg , w 2 v cbow , and GloVe are comparable and between 84%-86%, with w 2 v sg and w 2 v cbow achieving slightly higher F1.
Table 5 outlines the LSSD experiments us-ing the classification approaches (Section 3.2.2): SVM baseline ( SV M bl ) and SVM using the kernel we with word embeddings ( kernel W T MF , The classification approaches give better perfor-mance compared to the distributional approaches. The SV M bl is around 7-8 % higher than the PPMI bl and comparable with the word embed-dings used in distributional approaches (Table 4). In addition, our new SVM kernel method using word embeddings shows significantly better re-sults when compared to the SV M bl (and distri-butional approaches). For instance, for the S vs. L task, the average F1 is 96-97%, which is more than 10% higher than SV M bl . Similarly, for S vs. L sent task, F1 scores reported by the kernel using word2vec embeddings are in the range of 83%-84% compared to 77% given by the SV M bl , showing an absolute increase of 7%. As stated ear-lier, MVME algorithm aligns similar word pairs found in its inputs and this performs well for short texts (i.e., tweets). Thus, the MVME algorithm combined with word embedding in kernel we re-sults in very high F1. Among the word embedding models, word2vec models give marginally better results compared to GloVe and WTMF, and GloVe outperforms marginally WTMF. Similar to Table 4, here, the average F1 scores for S vs. L task are higher than the S vs. L sent results.

In terms of the best and worst performing tar-gets, SV M bl prefers targets with more training data (e.g.,  X  X eah X ,  X  X ove X  vs.  X  X weet X ,  X  X ttractive X ; see Table 2). In contrast, word embedding mod-els for  X  X oy X  and  X  X ature X , two targets with com-paratively low number of training instances have achieved very high F1 using both distributional and classification approaches (Table 4 and 5). This can be explained by the fact that for words, such as  X  X oy X ,  X  X ature X ,  X  X ute X , and  X  X rilliant X , the con-texts of their literal and sarcastic sense are quite different, and DSMs and word embeddings are able to capture the difference. For example, ob-serve in the Table 3, negative sentiment words, i.e.,  X  X ick X ,  X  X orking X ,  X  X now X  are the context words for targets  X  X oy X  and  X  X ove X , where as posi-tive sentiment words, such as,  X  X lessed X ,  X  X amily X ,  X  X hristmas X , and  X  X eace X  are the context words for L or L sent senses. Overall, out of 37 targets, only 5 targets ( X  X ature X ,  X  X oy X ,  X  X ute X ,  X  X ove X , and  X  X eah X ) achieved  X  X aximum X  F1 scores in vari-ous experimental settings (Tables 4 and 5) whereas targets such as  X  X nterested X ,  X  X enius X , and  X  X ttrac-tive X  achieved low F1 scores.

In terms of variance in results, SVM results show low SD (0-4%). For distributional ap-proaches, SD is slightly higher (5-8%) for several cases. Two lines of research are directly relevant to our work: sarcasm detection in Twitter and applica-tion of distributional semantics, such as word em-bedding techniques to various NLP tasks. In con-trast to current research on sarcasm and irony de-tection (Davidov et al., 2010; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014), we have introduced a reframing of this task as a type of word sense disambiguation problem, where the sense of a word is sarcastic or literal. Our SVM baseline uses the lexical features pro-posed in previous research on sarcasm detection (e.g., LIWC lexicon, interjections, pragmatic fea-tures) (Liebrecht et al., 2013; Gonz  X  alez-Ib  X  a  X  nez et al., 2011; Reyes et al., 2013). Our analysis of tar-get words where the sarcastic sense is the opposite of the literal sense is related to the idea of  X  X os-itive sentiment toward a negative situation X  pro-posed by Riloff et al. (2013) and recently studied by Joshi et al. (2015). In our approach, we chose distributional semantic approaches that learn con-textual information of targets effectively from a large corpus containing both literal and sarcastic uses of words and show that word embedding are highly accurate in predicting the sarcastic or lit-eral sense of a word (Tables 4 and 5). This ap-proach has the potential to capture more nuanced cases of sarcasm, beyond  X  X ositive sentiment to-wards a negative situation X  (e.g., one of our target words was  X  X hocked X  which is negative). How-ever, our current framing is still inherently limited to cases where sarcasm is characterized as a figure of speech where the author means the opposite of what she says, due to our approach of selecting the target words.
 Low-dimensional text representation, such as WTMF, have been successful in WSD disam-biguation research and in computing similarity be-tween short texts (Guo and Diab, 2012a; Guo and Diab, 2012b). word2vec and GloVe representa-tions have provided state-of-the-art results on var-ious word similarity and analogy detection task (Mikolov et al., 2013c; Mikolov et al., 2013b; Pennington et al., 2014). Word embedding based models are also used for other NLP tasks such as dependency parsing, semantic role labeling, POS tagging, NER, question-answering (Bansal et al., 2014; Collobert et al., 2011; Weston et al., 2015) and our work on LSSD is a novel application of word embeddings. We proposed a reframing of the sarcasm detec-tion task as a type of word sense disambiguation problem, where the sense of a word is its sarcas-tic or literal sense. Using a crowdsourcing exper-iment and unsupervised methods for detecting se-mantically opposite phrases, we collected a set of target words to be used in the LSSD task. We compared several distributional semantics meth-ods, and showed that using word embeddings in a modified SVM kernel achieves the best results (an increase of 10% F1 and 8% F1 for S vs. L and S vs. L sent disambiguation task, respectively, against a SVM baseline). While the SVM base-line preferred larger amounts of training data (best performance achieved on the targets words with higher number of training examples), the methods using word embeddings seem to perform well on target words where there might be an inherent dif-ference in the contextual sarcastic and literal use of a target word, even if the training data was smaller.
We want to investigate further the nature and size of training data useful for the LSSD task. For example, to test the effect of larger training dataset, we utilized pre-trained word vectors from GloVe (trained with 2 Billion tweets, using 100 di-mensions). 6 For S vs. L disambiguation, the av-erage F1 was 88.9%, which is 7% lower than the result using GloVe on our training set of tweets (much smaller) designed for the LSSD task. This shows the training data utilized to create word em-bedding models in GloVe probably do not contain enough sarcastic tweets.

Regarding the size of the training data, recall that the unsupervised alignment approach had ex-tracted 70 target words (Section 2), although we have used 37 target words as we did not have enough training data for the remaining targets. Thus, we plan to collect more training data for these targets as well as more target words (espe-cially for the S vs. L sent task). In addition, we plan to improve our unsupervised methods for de-tecting semantically opposite meaning (e.g., us-ing the IM-IM dataset in addition to the SM-IM dataset).

One common criticism of research based on use of hashtags as gold labels is that the training ut-terances could be noisy. In other words, tweets might be sarcastic but not have #sarcasm or #sar-castic hashtags. We did a small manual validation on a dataset of 180 tweets from the L sent class us-ing 3 annotators (we asked them to say whether the tweet is sarcastic or not). For cases where all 3 coders agree none of them were considered sar-castic, while when only 2 coders agree 1 tweet out of 180 was considered sarcastic. In future, we plan to perform additional experiments to study the is-sue of noisy data. We hope that the release of our datasets will stimulate other studies related to the sarcasm detection problem, including addressing the issue of noisy data.

We also plan to study the effect of hyper-parameters in designing the DSMs. Recently, Levy et al. (2015) have argued that parameter set-tings have a large impact on the success of word embedding models. We want to follow their ex-periments to study whether parameter tuning in PMI based disambiguation can improve its perfor-mance. This paper is based on work supported by the DARPA-DEFT program. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. The authors thank the anonymous reviewers for helpful comments.

