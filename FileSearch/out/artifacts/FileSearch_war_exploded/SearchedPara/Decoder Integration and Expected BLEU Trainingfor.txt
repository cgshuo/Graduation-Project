 Neural network-based language and translation models have achieved impressive accuracy im-provements on statistical machine translation tasks (Allauzen et al., 2011; Le et al., 2012b; Schwenk et al., 2012; Vaswani et al., 2013; Gao et al., 2014). In this paper we focus on recurrent neural network architectures which have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011; Sundermeyer et al., 2013) with several subsequent applications in ma-chine translation (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Hu et al., 2014). Recurrent models have the potential to capture long-span de-pendencies since their predictions are based on an unbounded history of previous words (  X  2).

In practice, neural network models for machine translation are usually trained by maximizing the likelihood of the training data, either via a cross-entropy objective (Mikolov et al., 2010; Schwenk et al., 2012) or more recently, noise-contrastive es-timation (Vaswani et al., 2013). However, it is widely appreciated that directly optimizing for a task-specific metric often leads to better perfor-mance (Goodman, 1996; Och, 2003; Auli and Lopez, 2011). The expected BLEU objective pro-vides an efficient way of achieving this for ma-chine translation (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014) instead of solely relying on tra-ditional optimizers such as Minimum Error Rate Training (MERT) that only adjust the weighting of entire component models within the log-linear framework of machine translation (  X  3).

Most previous work on neural networks for ma-chine translation is based on a rescoring setup (Arisoy et al., 2012; Mikolov, 2012; Le et al., 2012a; Auli et al., 2013), thereby side stepping the algorithmic and engineering challenges of di-rect decoder-integration. One recent exception is Vaswani et al. (2013) who demonstrated that feed-forward network-based language models are more accurate in first-pass decoding than in rescoring. Decoder integration has the advantage for the neu-ral network to directly influence search, unlike rescoring which is restricted to an n-best list or lat-tice. Decoding with feed-forward architectures is straightforward, since predictions are based on a fixed size input, similar to n-gram language mod-els. However, for recurrent networks we have to deal with the unbounded history , which breaks the usual dynamic programming assumptions for effi-cient search. We show how a simple but effective approximation can side step this issue and we em-pirically demonstrate its effectiveness (  X  4).
We test the expected BLEU objective by train-ing a recurrent neural network language model and obtain substantial improvements. We also find that our efficient approximation for decoder inte-gration is very accurate, clearly outperforming a rescoring setup (  X  5). Figure 1: Structure of the recurrent neural network language model. Our model has a similar structure to the recurrent neural network language model of Mikolov et al. (2010) which is factored into an input layer, a hid-den layer with recurrent connections, and an out-put layer (Figure 1). The input layer encodes the word at position t as a 1-of-N vector w t . The out-put layer y t represents scores over possible next words; both the input and output layers are of size | V | , the size of the vocabulary. The hidden layer state h t encodes the history of all words observed in the sequence up to time step t . The state of the hidden layer is determined by the input layer and the hidden layer configuration of the previous time step h t  X  1 . The weights of the connections between the layers are summarized in a number of matrices: U represents weights from the in-put layer to the hidden layer, and W represents connections from the previous hidden layer to the current hidden layer. Matrix V contains weights between the current hidden layer and the output layer. The activations of the hidden and output layers are computed by:
Different to previous work (Mikolov et al., 2010), we do not use the softmax activation function to output a probability over the next word, but in-stead just compute a single unnormalized score . This is computationally more efficient than sum-ming over all possible outputs such as required for the cross-entropy error function (Bengio et al., 2003; Mikolov et al., 2010; Schwenk et al., 2012). Training is based on the back propagation through time algorithm, which unrolls the network and then computes error gradients over multiple time steps (Rumelhart et al., 1986); we use the expected BLEU loss (  X  3) to obtain the error with respect to the output activations. After training, the output layer represents scores s ( w t +1 | w 1 ...w t , h t ) for the next word given the previous t input words and the current hidden layer configuration h t . We integrate the recurrent neural network lan-guage model as an additional feature into the stan-dard log-linear framework of translation (Och, 2003). Formally, our phrase-based model is pa-rameterized by M parameters  X  where each  X  m  X   X  , m = 1 ...M is the weight of an associated feature h m ( f,e ) . Function h ( f,e ) maps foreign sentences f and English sentences e to the vector h ( f,e ) ... ( f,e ) , and the model chooses transla-tions according to the following decision rule: We summarize the weights of the recurrent neural network language model as  X  = { U , W , V } and add the model as an additional feature to the log-linear translation model using the simplified nota-tion s  X  ( w t ) = s ( w t | w 1 ...w t  X  1 , h t  X  1 ) : which computes a sentence-level language model score as the sum of individual word scores. The translation model is parameterized by  X  and  X  which are learned as follows (Gao et al., 2014): 1. We generate an n-best list for each foreign 2. Next, we fix  X  , set  X  M +1 = 1 and opti-3. We fix  X  and re-optimize  X  in the presence 3.1 Expected BLEU Objective Formally, we define our loss function l (  X  ) as the negative expected BLEU score, denoted as xBLEU (  X  ) for a given foreign sentence f : level BLEU score with respect to the reference translation e ( i ) , and E ( f ) is the generation set BLEU approximation similar to He and Deng (2012). 3 The normalized probability p  X  , X  ( e | f ) of a particular translation e given f is defined as: p  X  , X  ( e | f ) = where  X  T h ( f,e ) includes the recurrent neural net-work h M +1 ( e ) , and  X   X  [0 , inf) is a scaling factor that flattens the distribution for  X  &lt; 1 and sharp-ens it for  X  &gt; 1 (Tromble et al., 2008). 4 Next, we define the gradient of the expected BLEU loss function l (  X  ) using the observation that the loss does not explicitly depend on  X  : where  X  w The error term indicates how the loss changes with 3.2 Derivation of the Error Term  X  w We rewrite the loss function (2) using (3) and sep-arate it into two terms G (  X  ) and Z (  X  ) as follows: l (  X  ) =  X  xBLEU (  X  ) =  X  =  X  Next, we apply the quotient rule of differentiation: Using the observation that  X  is only relevant to the recurrent neural network h M +1 ( e ) (1) we have  X  X   X  T h ( f,e ) which together with the chain rule, (3) and (4) al-lows us to rewrite  X  w  X  where U (  X ,e ) = sBLEU ( e,e i )  X  xBLEU (  X  ) . Directly integrating our recurrent neural network language model into first-pass decoding enables us to search a much larger space than would be pos-sible in rescoring.

Typically, phrase-based decoders maintain a set of states representing partial and complete transla-tion hypothesis that are scored by a set of features. Most features are local, meaning that all required information for them to assign a score is available within the state. One exception is the n-gram lan-guage model which requires the preceding n  X  1 words as well. In order to accommodate this fea-ture, each state usually keeps these words as con-text . Unfortunately, a recurrent neural network makes even weaker independence assumptions so that it depends on the entire left prefix of a sen-tence. Furthermore, the weaker independence as-sumptions also dramatically reduce the effective-ness of dynamic programming by allowing much
To solve this problem, we follow previous work on lattice rescoring with recurrent networks that maintained the usual n-gram context but kept a beam of hidden layer configurations at each state (Auli et al., 2013). In fact, to make decoding as efficient as possible, we only keep the single best scoring hidden layer configuration. This approx-imation has been effective for lattice rescoring, since the translations represented by each state are in fact very similar: They share both the same source words as well as the same n-gram context which is likely to result in similar recurrent his-tories that can be safely pruned. As future cost estimate we score each phrase in isolation, reset-ting the hidden layer at the beginning of a phrase. While simple, we found our estimate to be more accurate than no future cost at all. Baseline. We use a phrase-based system simi-lar to Moses (Koehn et al., 2007) based on a set of common features including maximum likeli-hood estimates p ML ( e | f ) and p ML ( f | e ) , lexically weighted estimates p LW ( e | f ) and p LW ( f | e ) , word and phrase-penalties, a hierarchical reorder-ing model (Galley and Manning, 2008), a linear distortion feature, and a modified Kneser-Ney lan-guage model trained on the target-side of the paral-lel data. Log-linear weights are tuned with MERT. Evaluation. We use training and test data from the WMT 2012 campaign and report results on French-English and German-English. Transla-tion models are estimated on 102M words of par-allel data for French-English, and 99M words for German-English; about 6.5M words for each language pair are newswire, the remainder are parliamentary proceedings. We evaluate on six newswire domain test sets from 2008 to 2013 con-taining between 2034 to 3003 sentences. Log-linear weights are estimated on the 2009 data set comprising 2525 sentences. We evaluate accuracy in terms of BLEU with a single reference.
 Rescoring Setup. For rescoring we use ei-ther lattices or the unique 100-best output of the phrase-based decoder and re-estimate the log-linear weights by running a further iteration of MERT on the n-best list of the development set, augmented by scores corresponding to the neural network models. At test time we rescore n-best lists with the new weights.
 Neural Network Training. All neural network models are trained on the news portion of the parallel data, corresponding to 136K sentences, which we found to be most useful in initial exper-iments. As training data we use unique 100-best lists generated by the baseline system. We use the same data both for training the phrase-based sys-tem as well as the language model but find that the resulting bias did not hurt end-to-end accu-racy (Yu et al., 2013). The vocabulary consists of words that occur in at least two different sentences, which is 31K words for both language pairs. We tuned the learning rate  X  of our mini-batch SGD trainer as well as the probability scaling parameter  X  (3) on a held-out set and found simple settings of  X  = 0 . 1 and  X  = 1 to be good choices. To prevent over-fitting, we experimented with L2 regulariza-tion, but found no accuracy improvements, prob-ably because SGD regularizes enough. We evalu-ate performance on a held-out set during training and stop whenever the objective changes less than 0.0003. The hidden layer uses 100 neurons unless otherwise stated. 5.1 Decoder Integration We compare the effect of direct decoder integra-tion to rescoring with both lattices and n-best lists when the model is trained with a cross-entropy ob-jective (Mikolov et al., 2010). The results (Ta-ble 1 and Table 2) show that direct integration im-proves accuracy across all six test sets on both lan-guage pairs. For French-English we improve over n-best rescoring by up to 1.1 BLEU and by up to 0.5 BLEU for German-English. We improve over lattice rescoring by up to 0.4 BLEU on French-English and by up to 0.3 BLEU on German-English. Compared to the baseline, we achieve improvements of up to 2.0 BLEU for French-English and up to 1.3 BLEU for German-English. The average improvement across all test sets is 1.5 BLEU for French-English and 1.0 BLEU for German-English compared to the baseline. 5.2 Expected BLEU Training Training with the expected BLEU loss is compu-tationally more expensive than with cross-entropy since each training example is an n-best list in-stead of a single sentence. This increases the num-ber of words to be processed from 3.5M to 340M. To keep training times manageable, we reduce the hidden layer size to 30 neurons, thereby greatly increasing speed. Despite slower training, the ac-tual scoring at test time of expected BLEU mod-els is about 5 times faster than for cross-entropy models since we do not need to normalize the out-put layer anymore. The results (Table 3) show improvements of up to 0.6 BLEU when combin-ing a cross-entropy model with an expected BLEU variant. Average gains across all test sets are 0.4 BLEU, demonstrating that the gains from the ex-pected BLEU loss are additive. We introduce an empirically effective approxima-tion to integrate a recurrent neural network model into first pass decoding, thereby extending pre-vious work on decoding with feed-forward neu-ral networks (Vaswani et al., 2013). Our best re-sult improves the output of a phrase-based decoder by up to 2.0 BLEU on French-English translation, outperforming n-best rescoring by up to 1.1 BLEU and lattice rescoring by up to 0.4 BLEU. Directly optimizing a recurrent neural network language model towards an expected BLEU loss proves ef-fective, improving a cross-entropy trained variant by up 0.6 BLEU. Despite higher training complex-ity, our expected BLEU trained model has five times faster runtime than a cross-entropy model since it does not require normalization.

In future work, we would like to scale up to larger data sets and more complex models through parallelization. We would also like to experiment with more elaborate future cost estimates, such as the average score assigned to all occurrences of a phrase in a large corpus. We thank Michel Galley, Arul Menezes, Chris Quirk and Geoffrey Zweig for helpful discussions related to this work as well as the four anonymous reviewers for their comments.
