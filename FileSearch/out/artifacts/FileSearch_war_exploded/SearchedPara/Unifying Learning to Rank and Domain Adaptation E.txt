 For document scoring, although learning to rank and domain adap-tation are treated as two different problems in previous works, we discover that they actually share the same challenge of adapting keyword contribution across different queries or domains. In this paper, we propose to study the cross-task document scoring prob-lem, where a task refers to a query to rank or a domain to adapt to, as the first attempt to unify these two problems. Existing solu-tions for learning to rank and domain adaptation either leave the heavy burden of adapting keyword contribution to feature designers, or are difficult to be generalized. To resolve such limitations, we abstract the keyword scoring principle , pointing out that the contri-bution of a keyword essentially depends on, first, its importance to a task and, second, its importance to the document. For determin-ing these two aspects of keyword importance, we further propose the concept of feature decoupling , suggesting using two types of easy-to-design features: meta-features and intra-features . Towards learning a scorer based on the decoupled features, we require that our framework fulfill inferred sparsity to eliminate the interference of noisy keywords, and employ distant supervision to tackle the lack of keyword labels. We propose the Tree-structured Boltzman-n Machine (T-RBM), a novel two-stage Markov Network, as our solution. Experiments on three different applications confirm the effectiveness of T-RBM, which achieves significant improvement compared with four state-of-the-art baseline methods.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models; I.2.7 [ Natural Language Processing ]: Text analysis Learning to Rank; Domain Adaptation; Feature Decoupling; Tree-Structured Restricted Boltzmann Machine  X 
With much information in the world represented by unstructured text, many applications study how to develop document scorers , which can analyze the content of a document and determine its relevance for some information need, e.g. , text categorization [8], document retrieval [5]. Since manually crafting scorers is difficult, people are interested in how to automatically learn such scorers from labeled documents.

Scorer learning techniques have been actively studied for years  X  unlike early applications which usually deal with only one topic ( e.g. , judging if a document is about  X  X inance X  [8]), with the prevalence of the Web serving the whole world of users and connecting numerous sources of data, most applications nowadays must handle various user needs, i.e. , queries , and diverse sources, i.e. , domains .
First, in terms of queries , many applications aim at ranking doc-uments for queries that represent users X  information needs, i.e. , realizing learning to rank . Unlike traditional IR which deals with short queries, learning to rank applications nowadays have to handle more sophisticated queries: Application 1 : Verbose-Query-based Retrieval [11, 2], which ad-dresses verbose queries that consist of one or more long sentences. It is more challenging than traditional IR, because long queries usually contain extraneous terms which might hinder retrieval of relevant documents.
 Application 2 : Entity-Centric Document Filtering [22], which stud-ies, given an entity ( e.g. , a person) characterized by an identification page ( e.g. , her Wikipedia page) as a query, how to identify docu-ments relevant to the entity. Since an identification page is usually long and noisy, the solution has to identify keywords that represent the characteristics of the entity ( e.g. ,  X  X icrosoft X  for entity  X  X ill Gates X ) for better retrieval accuracy.

Second, in terms of domains , traditional scorer learning tech-niques work well only when the training and testing documents use similar distributions of keywords, i.e. , documents are from the same domain. Due to the expensive cost of labeling, we expect to learn a document scorer that could be adapted across different domains, i.e. , realizing domain adaptation , for example: Application 3 : Cross-Domain Sentiment Analysis [4], which studies how to adapt a sentiment scorer across different domains of reviews. It is challenging because people tend to use different sentiment keywords in different domains, e.g. ,  X  X oring X  in book reviews, and  X  X eaking X  in kitchen appliance reviews instead.

We observe that, although learning to rank and domain adaptation seem distinct from each other, both of them have to handle the varying importance of each keyword in different queries/domains (in this paper, we do not consider other properties which are orthogonal to the keyword content, e.g. , pagerank, although they can be easily incorporated as well). For example, in entity-centric document filtering, keyword  X  X icrosoft X  is important for query  X  X ill Gates, X  but not for  X  X ichael Jordan; X  in sentiment analysis, as mentioned earlier, different domains would use different keywords to represent the same sentiment. Therefore, as the common challenge for these two types of applications, both of them have to consider how to bridge keywords across different queries or domains.
 Problem  X  Cross-Task Document Scoring . As the first contribu-tion of this paper, observing such a common challenge, we propose to study a general cross-task document scoring problem, which, as far as we know, is the first attempt to unify learning to rank and domain adaptation. Formally, cross-task document scoring aims at learning a scorer F ( d,t ) to predict the relevance of document d for task t , where the notion of  X  task  X  represents the scoring of documents for one query in one domain. In the training phase, we are given some documents labeled for some tasks t ( e.g. , some queries) to learn F ( d,t ) ; in the testing phase, as highlighted by  X  X ross-task, X  F ( d,t ) should be capable of handling new tasks t ( e .g. , new queries) which do not appear in the labeled data. Challenge  X  Learning to Adapt Keyword Contribution. As our second contribution , we identify the core challenge of cross-task document scoring as learning to adapt keyword contribution across tasks. Formally, if we use Contrib ( w,d,t ) to denote the contri-bution of keyword w for document d with respect to task t , the relevance of document d is essentially the accumulation of its key-words X  contribution Contrib ( w,d,t ) . Therefore, the challenge of learning F ( d,t ) lies in how to determine keyword contribution Contrib ( w,d,t ) for tasks that do not appear in the training data, i.e. , learning to adapt keyword contribution .

Learning to adapt keyword contribution is critical for enabling cross-task scoring; however, traditional learning to rank frameworks ( e.g. , RankSVM [10]) simply circumvent the problem. To learn a scorer, such frameworks adopt an ensembling idea to combine a set of manually crafted sub-scorers f k ( d ,t ) as features, e.g. , in document retrieval [5, 14], f k ( d ,t ) could be BM25, language mod-el. These learning to rank frameworks are feasible for traditional document retrieval, as there exist many readily studied scorers that could be used as features; however, for newly proposed applications, such scorers are seldom available, and we need laborious feature engineering in order to adopt these frameworks.
 Insight  X  Keyword Scoring Principle . As our third contribution , towards learning to adapt keyword contribution, we propose our key insight: Keyword Scoring Principle  X  the importance of keyword w for doc-ument d with respect to task t , i.e., Contrib ( w,d,t ) , depends on: 1. the importance of keyword w for task t ; 2. the importance of keyword w for document d .

Although such a principle is not abstracted before, it is intuitive and implicitly followed in the previous design of manually crafted scorers. Take BM25 as example: in terms of the first aspect, BM25 assumes that keywords with high inverse document frequency ( i.e. , high IDF ) are more important for the query; in terms of the sec-ond aspect, BM25 assumes that keywords mentioned a lot in the document ( i.e. , high TF ) should have higher contribution. Abstract  X  Feature Decoupling . As our fourth contribution , to ful-fill our goal of learning Contrib ( w,d,t ) based on the principle, we propose the idea of feature decoupling , which suggests  X  X ecoupling X  the original scorer-as-feature design in traditional learning to rank frameworks into two types of more elementary features:
To determine  X  X he importance of keyword w for task t , X  we de-keyword properties. In learning to rank, some recent works [1, 11, 3, 22] adopt such an idea, e.g. , using meta-features such as keyword position to identify important keywords from queries. In domain adaptation, Blitzer et al. [4] propose a model to use keyword cor-relation to bridge keywords from different domains. We can use meta-features to realize the same insight, which will be discussed in details in Section 3.4.

To determine  X  X he importance of keyword w for document d , X  we propose the concept of intra-features , denoted by f ( I ) characterize how keyword w occurs in document d . The motivation is that, besides simply counting keywords as most applications do, we can characterize the keyword occurrence more generally and systematically for higher prediction accuracy. For example, keywords that appear in the title, anchor text or URL usually have larger contribution to the relevance.

Given such a decoupled feature design, we have to appropriately and F ( d,t ) finally takes the following abstraction: To the best of our knowledge, such a learning framework, which aims at learning a scorer upon two types of elementary features, has not been studied before (previous works [1, 11, 3, 22] which have the concept of meta-features do not model intra-features).
Towards learning keyword contribution based on decoupled fea-tures, our framework has to fulfill two requirements: Requirement 1 : Inferred Sparsity . Unlike traditional learning to rank frameworks [5, 14] which do not model the concept of key-words, our scorer should be aware of the potential interference from noisy keywords. For example, both verbose-query-based retrieval and entity-centric document filtering focus on long queries, in which many keywords are unrelated to user intent or target entities; in sen-timent analysis, a review usually contains many keywords that are irrelevant to sentiment. Even if we assign such keywords with s-mall contribution, their values, once accumulated, will still severely affect the document score.

Therefore, in order to filter noisy keywords, we require that the keyword contribution function Contrib ( w,d,t ) be sparse , which only outputs non-zero values for important keywords. Different from traditional sparse learning [20] which aims at learning sparse feature weightings to enforce feature sparsity, our goal is to sparsify the inferred value of a function , i.e. , achieving inferred sparsity (their difference will be further discussed in Section 4.1). Requirement 2 : Distant Supervision . Toward realizing learning to score keywords, another challenge arises from the lack of the keyword labels. In most applications, we are only provided with document labels, and it is impractical to request manual keyword labels for learning Contrib ( w,d,t ) . Therefore, we require that the scorer should fulfill distant supervision , by only using document labels to  X  X istantly X  guide the adaptation of keyword contribution. Solution  X  Tree-Structured Restricted Boltzmann Machine . As our fifth contribution , to fulfill these two requirements, we propose a novel Tree-structured Restricted Boltzmann Machine (T-RBM) model for the cross-task document scoring problem.

First, to achieve inferred sparsity , the model needs a scoring scheme which can eliminate the contribution of noisy keywords. We develop a two-stage procedure: in the first stage, we learn a classifier to discretize the importance of keywords into different levels based on their meta-features, and regularize the classifier to enforce the elimination of noisy keywords; in the second stage, we determine the contribution of important keywords by their intra-features and set the contribution of unimportant ones to be zero.

Second, to achieve distant supervision , we propose to join the two stages in one model. Specifically, we take advantage of Markov Network to connect keyword importance and document relevance, such that we can use document labels to directly supervise the learning of the keyword classifier.
 Based on these ideas, we design T-RBM, which, as a variant of Restricted Boltzmann Machine [19] (one type of bipartite Markov Network), models each document as a tree graph with the root node representing a document and the leaf nodes representing its keywords. Free of loop structures, T-RBM could be efficiently trained by exact belief propagation [17].

In the experiments, we performed our evaluation on verbose-query-based retrieval, entity-centric document filtering and cross-domain sentiment analysis in three different datasets. By comparing T-RBM with four state-of-the-art baselines, we observed that our framework not only provides a conceptually unified modeling but also significantly improves the results on different applications.
In terms of abstraction , learning to rank and domain adaptation are separately abstracted and studied in previous works, e.g. , doc-ument retrieval [5, 1, 11, 3], entity-centric document filtering [22], cross-domain sentiment analysis [4]. Inspired by their works, we identify their common challenge and propose a novel framework to unify these two problems for achieving a more general solution.
In terms of challenge , cross-task document scoring boils down to learning to adapt keyword contribution across different tasks. 1. For learning to rank , most previous works [5, 14] simply cir-cumvent the challenge, leaving the burden of determining keyword contribution to feature designers. Some of the recent works [1, 11, 3, 22] start to confront the challenge by modeling keyword-level features, i.e. , meta-features, to learn keyword contribution; howev-er, none of them explicitly model intra-features, failing to capture different kinds of keyword occurrences in the document. 2. For domain adaptation , Blitzer et al. [4] propose structural correspondence learning (SCL) to bridge keywords from different domains. The intuition is that, given two domain-specific keywords ( e.g. ,  X  X oring X  from book reviews and  X  X eaking X  from kitchen appli-ance reviews), if both of them co-occur a lot with some pivot key-words ( i.e. , keywords like  X  X ad, X   X  X orst X  which are commonly used in all domains), these two keywords should share similar sentiment ( e.g. , both  X  X oring X  and  X  X eaking X  represent negative sentiment). To realize such an insight, SCL learns a set of pivot predictors, each of which predicts the occurrence of one pivot keyword based on other domains-specific keywords, to relate different domains. Li et al. [13] and Pan et al. [16] follow the same intuition but use different approaches such as feature alignment [16] and matrix decomposition [13]. Different from these approaches which  X  X ardcode X  the logic of keyword adaptation in the model design, our feature decoupling idea allows designers to conveniently incorporate different ways of keyword adaptation by meta-features.
 In terms of technique , we propose T-RBM, a novel two-stage Markov Network taking the decoupled features as input and fulfilling the requirements of inferred sparsity and distant supervision. 1. With respect to inferred sparsity , unlike sparse learning [20] which learns sparse parameters as feature weightings, we aim at sparsifying the inferred value of the keyword contribution function. Existing meta-feature-based solutions have different limitations in fulfilling this requirement. The solutions proposed by Lease et al. [11], Bendersky et al. [3] and Zhou et al. [22] do not fulfill the requirement, making the prediction result vulnerable to noisy keywords. Zhou et al. [22] propose another BoostMapping model, which achieves inferred sparsity by clustering keywords based on their meta-features and eliminating noisy clusters; as the limitations, BoostMapping is difficult to solve when we have to model multiple intra-features, and might overfit the training data. We will compare T-RBM with these models in details in Section 4.1. 2. With respect to the requirement of distant supervision , similar to T-RBM, Bendersky et al. [1] propose to learn a keyword classifier to discover important concepts for retrieval; however, their solution relies on the existence of keyword labels, which is impractical for most applications. Different from their work, taking the advantage of Markov Network, T-RBM manages to train the keyword classifier based on only document labels. 3. With respect to the model structure , the most related work to ours is the Markov Random Field model proposed by Lease et al. [11]. As the key difference, their solution is a generative model characterizing P ( q,d )  X  the joint probability of observing query q and document d , while our T-RBM model directly models the conditional probability P ( d | q ) . It has been repeatedly confirmed that a discriminative model usually yields better generalization performance compared with a generative model [21]; furthermore, as mentioned earlier, their solution does not fulfill the inferred sparsity requirement. 4. Restricted Boltzmann Machine (RBM), as one type of bipartite Markov Network, is adopted in many applications such as topic modeling [18], deep learning [6], etc. With distinct settings and objectives, our proposed T-RBM is different from traditional RBM models in two aspects: first, T-RBM takes a tree structure which can be trained efficiently; second, T-RBM adopts a novel hidden variable regularization technique, which allows the model to control the inferred sparsity of keyword contribution.
In this section, we formally define the cross-task document scor-ing problem. To tackle the challenge of learning to adapt keyword contribution , we propose the insight of keyword scoring principle and the concept of feature decoupling .
In document scoring, we aim at predicting the relevance of a document d for a particular task t . Formally, task t could be rep-resented as a function t : d  X  R , which takes a document d as input, and outputs a score denoting the relevance of d . We define that two tasks t 1 and t 2 are different, if there exists one document d satisfying t 1 ( d ) 6 = t 2 ( d ) . Therefore, identifying relevant docu-ments for different queries belongs to two different tasks, as one document usually has different relevance for two queries; similarly, the sentiment judgement of a book review is also different from that of a kitchen appliance review.

In contrast with single-task document scoring which tackles only one task ( e.g. , text categorization [8] learns a scorer to predict if a document is about a fixed topic such as  X  X inance X ), in a cross-task document scoring problem, we are interested in a set of different but related tasks T . For example, in verbose-query-based retrieval and entity-centric document filtering, each t  X  T represents a task of predicting document relevance for one verbose query or one entity; in cross-domain sentiment analysis, each task t is to judge the sentiment of reviews form one particular domain.

Formally, in cross-task document scoring , our goal is to auto-matically learn a document scorer F ( d,t ) , which could output the relevance of document d for task t  X  T .

In the training phrase , we are given a set of training docu-ments d = { d 1 ,d 2 ,...,d N } and a list of document labels  X y = {  X  y 1 ,...,  X  y N } , where each d i is labeled by  X  y i denoting the relevance of d i for task t i  X  T ( t i and t j could refer to the same task, indicat-ing d i and d j are labeled for the same query or in the same domain). Here,  X  y i could be either binary ( e.g. , 0 X  X egative, 1 X  X ositive) or ordinal ( e.g. , 0 X  X rrelevant, 1 X  X elevant or 2 X  X erfectly relevant).
Based on the training documents, we aim at learning F , which, in the testing phase , could predict the relevance of a new document d for task t  X   X  T . Specifically, as highlighted by  X  X ross-task, X  we require that t  X  should differ from all the training tasks, i.e. , t for all i . That is because, in most applications, the target task set T could not be covered by finite training examples ( e.g. , there are infinite possible queries in document retrieval) and thus, F should be adaptable to unseen queries or domains, i.e. , new tasks.
To score a document d for a task t , the scorer has to assess the content of document d . Formally, we use V ( t ) to represent the vocabulary of keywords that are considered in task t , where each keyword w  X  V ( t ) could be 1-gram, 2-gram, noun phrases, etc. Fol-lowing previous works [3, 22, 4], in verbose-query-based retrieval and entity-centric document filtering, V ( t ) covers keywords that are mentioned in the query or the entity identification page, while in cross-domain sentiment analysis, V ( t ) includes all the keywords. We then define W ( d,t )  X  V ( t ) as the content of document d that is related with task t .

Based on the document content W ( d,t ) , the relevance of doc-ument d could be viewed as the accumulation of its containing keywords X  contribution. Formally, if we use Conrib ( w,d,t ) to de-note the contribution of keyword w to document d with respect to task t , F ( d,t ) takes the form of
As Eq. 2 shows, the challenge of cross-task document scoring becomes how to learn Conrib ( w,d,t ) to determine the keyword contribution for new tasks t  X  which do not appear in the training data, i.e. , learning to adapt keyword contribution .
Due to the requirement of  X  X ross-task, X  the learning of Conrib ( w,d,t ) is non-trivial. If our target is a single-task document scoring prob-lem ( i.e. , tackling the same task in both training and testing phas-es), as a common solution [8], we can define Conrib ( w,d,t ) =  X  w  X  TF w ( d ) , where each feature function TF w ( d ) represents how many times document d contains a specific keyword w , and  X  as its weighting, characterizes the importance of keyword w ( e.g. , in text categorization , if the target topic is about finance, keyword  X  X tock X  should have value of  X  w ). The scorer is then defined by where  X  w could be learned by standard learners. However, such a design could not be applied in cross-task document scoring, because, as we mentioned in Section 1, one keyword usually has very different importance for different tasks, and thus,  X  w learned from training data could not be adapted to new tasks.

Traditional learning to rank frameworks ( e.g. , RankSVM [10]) manage to tackle different tasks ( i.e. , queries); however, they cir-cumvent the problem of learning Conrib ( w,d,t ) , which define the scorer by and  X  k is learned to represent the confidence of f k ( d ,t ) , e.g. , in document retrieval [5], f k ( d ,t ) could be vector space model, BM25 and language model. As we discussed in Section 1, such frameworks require designers to manually determine the keyword contribution in the feature design, laying heavy burden on designers.
In order to automatically learn the contribution of keywords, as our key insight, we propose the keyword scoring principle: Definition 1 (Keyword Scoring Principle). If we use R t ( w ,t ) to denote the importance of keyword w for task t , and R d ( w ,d ) to denote the importance of keyword w for document d , the keyword contribution Contrib ( w,d,t ) should take the following form:
As we introduced in Section 1, towards automatically learning the keyword contribution Contrib ( w,d,t ) based on the principle, we propose the concept of feature decoupling , which suggest  X  X e-coupling X  the original scorer-as-feature design ( i.e. , f Eq. 4) into two types of more elementary features: Definition 2 (Feature Decoupling). To learn Contrib ( w,d,t ) , we propose to design, first, meta-features f ( M ) k ( w ,t ) , which charac-terize task-t -related properties of keyword w , for determining the importance of keyword w for task t , i.e. , defining R t ( w ,t ) by and, second, intra-features f ( I ) k ( w ,d ) , which describe how doc-ument d contains keyword w , for determining the importance of keyword w for document d , i.e. , defining R d ( w ,d ) by
Figure 1 lists all the features we use for verbose-query-based retrieval, entity-centric document filtering, and cross-domain senti-ment analysis.

First, for meta-features , the designs vary a lot across applications: a) In verbose-query-based retrieval, meta-features are designed to identify important keywords in the query. For example, Query-Pos is used, as keywords mentioned earlier in the query tend to be more important. Following previous works [3], we model not only unigram, but also adjacent bigrams in the query. To discrim-inate their importance, we design separate features for them, e.g. , QueryPos [unigram] and QueryPos [bigram]. b) In entity-centric document filtering, we only model unigrams, because the query is already very long, and we find that considering bigrams does not help improve the performance. In additional to the features used in verbose-query-based retrieval, we also design meta-features like TFInTitle , TFInInfoBox , to leverage the structure of Wikipedia pages for identifying importance keywords. c) In cross-domain sentiment analysis, the design of meta-features is very different from the other applications. In order to realize the insight proposed by Blitzer et al. [4] (introduced in Section 2), given a domain-specific keyword w , we propose to calculate a list of meta-features Corr [ w p ] , each of which measures the Pearson correlation between w and one particular pivot keyword w p  X  the correlation is positive if two keywords are positively correlated, negative if negatively correlated and zero if independent.

Second, intra-features characterize how a document contains a keyword. As discussed in Section 1, our designed intra-features describe how a keyword appears in different positions of a document ( e.g. , title, anchor text) by different representations of term frequency ( e.g. , term frequency normalized by document length). In verbose-query-based retrieval, as suggested by Bendersky et al. [3], we also design WindowTF specifically for bigrams, which counts the number of times that two keywords appear within a fixed size of window. Training Phase : Testing Phase :
Finally, we design doc-features f ( D ) k ( d ) , which characterize document features that have the same weightings across different tasks. For example, in cross-domain sentiment analysis, we de-sign PivotTF [ w p ] for each pivot keyword w p ( e.g. ,  X  X ood, X   X  X ad X ), which represents the same sentiment in all domains. The design of doc-features is not the main focus of this paper, as they are the same with the features used in traditional learning models.

Given such a decoupled feature design, the cross-task document scoring problem is summarized in Figure 2.
Towards realizing learning to adapt keyword contribution, we re-quire that our framework fulfill the requirements of inferred sparsity and distant supervision . As our solution, we propose Tree-Structured Restricted Boltzmann Machine (T-RBM) to learn a two-stage docu-ment scorer based on the decoupled features.
As we motivated in Section 1, in many applications, the keyword vocabulary V ( t ) we model is very noisy. If such noisy keywords are not appropriately filtered, their inferred values of Contrib ( w,d,t ) , once accumulated, will severely affect the prediction accuracy of F ( d,t ) . Therefore, we require that our framework achieve inferred sparsity : Requirement 1 (Inferred Sparsity for Keyword Contribution). Contrib ( w,d,t ) should equal to zero for unimportant keywords w .
This concept of inferred sparsity is closely related with feature sparsity, which traditional sparse learning works [20] aim to achieve. The goal of feature sparsity is to learn a sparse feature weighting vector, in which only a few features have non-zero weightings, for the purpose of reducing model complexity and increasing prediction accuracy. One common technique for achieving feature sparsity is l regularization. For example, in single-task document scoring (which defines F ( d,t ) by Eq. 3), we can add an l 1 regularization term |  X  to the objective function to learn sparse feature weightings  X 
In contrast with feature sparsity which aims at learning sparse feature weightings, in inferred sparsity, we would like to  X  X parsify X  the inferred value of a function, i.e. , Contrib ( w,d,t ) . Traditional feature sparsity techniques simply do not work, because the value of Contrib ( w,d,t ) is jointly determined by a set of decoupled features  X  setting some of their weightings to be zero can not make the whole function become sparse.

Besides inferred sparsity, due to the lack of keyword labels, the learning framework should also achieve distant supervision : Requirement 2 (Distant Supervision by Document Labels). The learning of keyword contribution Contrib ( w,d,t ) should be distant-ly guided by only document labels  X y .

As we introduced in Section 2, some existing works [3, 11, 22] on verbose-query-based retrieval and entity-centric document filtering also adopt the concept of meta-features. Although we can extend their works to support multiple intra-features, their models still have different limitations in fulfilling our requirements.

First, the concept weighting model proposed by Bendersky et al. [3], the Markov random field model proposed by Lease et al. [11] and the linear weighting model proposed by Zhou et al. [22] all belong to same the category, which define Contrib ( w,d,t ) = features. Since DocTF ( w,d ) is just one type of intra-feature, we can easily extend it to support multiple intra-features, by defining Contrib ( w,d,t ) = P i,j  X  ij  X  f ( M ) i ( w ,t )  X  f scorer becomes a generated feature, and  X  ij could be learned by standard learners.
Such a linear design of Contrib ( w,d,t ) fails to fulfill the require-ment of inferred sparsity. Actually, except the trivial solution which sets all  X  ij to be 0 , for other assignments of  X  ij , we can only get a small number of keywords (no more than the number of  X  ij have zero contribution. The drawback of failing to achieve inferred sparsity could be observed by checking the semantics of the generat-ed features. Take meta-feature QueryPos and intra-feature DocTF as example. The generated feature denotes the QueryPos summation of keywords from a document, which fails to capture the intuition that keywords with low QueryPos are more important, and documents containing more low-QueryPos keywords are more relevant.
Second, realizing the limitations of linear models, Zhou et al. [22] propose BoostMapping, which first adopts a boosting framework to generate a set of clusters c 1 ,c 2 ,... , with each cluster c ing keywords sharing similar meta-features, e.g. ,  X  IDF &gt; 10 and QueryPos &lt; 5 , X  and then assumes that keywords from the same cluster c i share the same contribution Contrib i ( w ,d,t ) =  X  DocTF ( w ,d ) . Similar to the linear weighting model, we can extend it to support multiple intra-features by defining Contrib P
BoostMapping manages to achieve inferred sparsity, as the learn-er will assign  X  ij = 0 for unimportant clusters. However, it is difficult to extend the learning algorithm of BoostMapping (specifi-cally, for generating clusters c i ) to support multiple intra-features. Furthermore, as reported in the paper [22] and confirmed in our experiment, BoostMapping might overfit to training tasks in some specific settings, which would lead to poor generalization capability.
To fulfill these two requirements, we develop a two-stage scoring procedure. Specifically, to achieve inferred sparsity, we discretize the importance of a keyword into different levels, and explicitly define Contrib ( w,d,t ) = 0 for unimportant keywords; to realize distant supervision, we unify the two stages in one single model, and learn the feature weightings by only document labels. The model is described in the following: based on meta-features f ( M ) k ( w ,t ) to discretize the importance of keywords, where C ( w,t ) = 0 indicates that w is a noisy keyword for task t , and C ( w,t )  X  { 1 ,...,L } represents keywords of different importance levels.  X  In the second stage, we determine contribution of each keyword w based on its importance level C ( w,t ) . For important keywords with C ( w,t ) = l 6 = 0 , the value of Contrib ( w,d,t ) should de-C ( w,t ) = 0 ), we set Contrib ( w,d,t ) = 0 to fulfill the requirement of inferred sparsity.
 Formally, such a two-stage model defines Contrib ( w,d,t ) as,
Contrib ( w,d,t ) = U l ( w ,d ) if C ( w,t ) = l 6 = 0; where U l ( w ,d ) is a function defined over intra-features f
In order to realize the design of Contrib ( w,d,t ) in Eq. 10, the model should, first, characterize how keyword classifier C ( w,t ) de-pends on meta-features f ( M ) ( w ,t ) , second, determine how the con-tribution of important keywords, i.e. , U ( w,d ) , is defined based on in the absence of keyword labels. To achieve these three goals, we propose a novel Tree-structured Restricted Boltzmann Machine (T-RBM), in the framework of Markov Network, as our solution. Restricted Boltzmann Machine (RBM) [19] refers to one type of bipartite Markov network, which is widely used in many applica-tions ( e.g. , deep learning [6], topic modeling [18]). As a simplified Figure 3: Tree-structured Restricted Boltzmann Machine.
 RBM, our proposed T-RBM takes a tree structure to characterize the dependency between documents and keywords.

We highlight three important features of our proposed T-RBM model. First, to tackle the lack of keyword labels, T-RBM models the importance of keywords as hidden variables , and only uses document labels to guide the learning. Second, T-RBM adopts a novel hidden variable regularization idea, which allows the model to control the inferred sparsity of keyword contribution. Third, taking a tree structure, T-RBM could be efficiently learned by standard optimization techniques.
Figure 3 shows a T-RBM model designed for entity-centric docu-ment filtering. Generally, T-RBM is composed of N tree-structured Markov Networks, each of which corresponds to one document. There are no edges between trees, indicating that the documents are independent with each other. Each tree contains two types of nodes y i and h i = { h i 1 ,...,h i | h i | } , with y ing the relevance of document d i ( y i = 1 if d i is relevant, and y i = 0 otherwise) and h ij  X  { 0 , 1 , ...,L } is a hidden variable denoting the importance of w ij for task t i (for notational conve-nience, we use w i = { w i 1 ,w i 2 ,... } to represent document con-tent W ( d i ,t i ) , where w ij denotes the j -th keyword in document d ). More specifically, h ij = 0 indicates that w ij is unimportant, and h ij  X  { 1 , ...,L } corresponds to different importance levels of keywords, e.g. , in sentiment analysis, both  X  X ood X  and  X  X ad X  are important keywords, but have different contribution to the document sentiment.

Three types of factors  X  ij ( h ij ) ,  X  ij ( y i ,h ij ) and  X  fined in T-RBM. Specifically,  X  ij ( h ij ) characterizes whether key-word w ij is important for task t i ,  X  ij ( y i ,h ij ) models how keyword w ij contributes its importance to document d i , and  X  i ( y the effect of other document features f ( D ) k ( d i ) . Based on the fac-tors, the conditional probability P ( y i , h i | d i , w represent the probability of a specific assignment of y i in the following,
We are interested in document scorer F ( d i ,t i ) , which, in the lan-guage of probability, is formally described by P ( y i = 1 | d Based on Eq. 11, we can represent P ( y i = 1 | d i , w i of graph factors, by marginalizing P ( y i = 1 , h i | d i possible assignments of h i ,
To enforce inferred sparsity, we also want to control the percent-age of noisy keywords in the model learning. In the language of probability, the inferred sparsity of keyword contribution could be represented by P ( h i = 0 | d i , w i ,t i ) , defined as follows,
P ( h i = 0 | d i , w i ,t i ) =
In T-RBM, we design factors  X  ij ( h ij ) ,  X  ij ( y i ,h to characterize the dependency required by Eq. 10.

First,  X  ij ( h ij ) characterizes how keyword classifier C ( w judges if w ij is an important keyword for task t i , which should depend on meta-features f ( M ) k ( w ij ,t i ) : Eq. 14 characterizes different levels of important keywords by defin-ing L sets of feature weightings  X  ( M ) k ,l . Note that we set  X  since only relative value between  X  i ( l ) and  X  i (0) matters in the Markov Network.

Second,  X  ij ( y i ,h ij ) models how noisy keywords are filtered, and how the contribution of important keywords U l ( w ,d ) depends on intra-features f ( I ) k ( w ij ,d i ) , We set  X  ij (0 , 0 ) =  X  ij (1 , 0 ) = 1 to represent that if w unimportant keyword, w ij would not contribute any score to doc-ument d i . When w ij is important, i.e. , h ij = l &gt; 0 , the value of  X  ij ( y i ,h ij ) depends on how w ij appears in document d sume only one keyword importance level with L = 1 , such a factor design can still discriminate keywords of different importance, be-cause the contribution of keyword w ij is the marginalization result of P ( y i ,h ij = 1 | d i , w i ,t i ) and P ( y i ,h ij
Finally, we define  X  i ( d i ) to incorporate document features f that are generalizable across tasks: We use  X  = {  X  ( M ) the set of parameters that need to be determined in T-RBM, where K ( M ) , K ( I ) and K ( D ) denote the number of designed meta-features, intra-features and doc-features respectively, and the total number of
Following the maximal likelihood principle, our objective is to learn  X  to maximize the likelihood L (  X  ;  X y ) of observing document labels  X y , regularized by G (  X  ) = P N i = 1 log P ( h i to control the inferred sparsity of all the keywords. Formally the likelihood is defined by argmax = argmax = argmax +  X  l og  X 
Y where  X  controls the inferred sparsity of keyword contribution. Specifically, when  X  &gt; 0 , the model will favor more sparse keyword contribution, and vice versa. Here we first study document scoring as a classification problem by assuming that document labels are all binary ( i.e. ,  X  y i  X  { 0 , 1 } ), and will later extend our solution to document ranking problems which accept ordinal labels.
 To optimize the objective function, we use the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm, which is a popular gradient-based method for solving unconstrained nonlinear problems  X  specifically, we adopt the LibLBFGS library [15], a c-implementation of L-BFGS. As the optimization routine, in each step, we compute the current gradient value of the objective func-tion at  X  , and LibLBFGS will update  X  according to the gradient value. The optimization routine stops until the objective function converges.
 The gradient value of the objective function at  X  , as required by LibLBFGS, is computed as follows,  X  L (  X  ;  X y )  X  L (  X  ;  X  y ) In Eq. 18, Eq. 19 and Eq. 20, all the probabilities could be computed by belief propagation [17]. As T-RBM is a tree-structured graph, belief propagation could efficiently compute the exact result.
In general, the time complexity of T-RBM is O [ T  X  ( N d N e  X  K ( I ) + N w  X  K ( M ) )] , where T denotes the number of to-tal iterations, N d , N e and N w represent the number of documents, keyword-document pairs and keywords respectively. Such time com-plexity stems from the computation of factors  X  ij ( h ij and  X  i ( y i ) given current model parameters  X  . After computing the factors, the time complexity of belief propagation is O ( N updating parameter  X  takes O ( |  X  | ) , which are much faster than the factor computation and could be ignored.
This section discusses how to extend T-RBM to ranking prob-lems, where training labels are ordinal instead of binary, e.g. ,  X  y { 0 , 1 , 2 } , denoting { X  X rrelevant X ,  X  X elevant X ,  X  X erfectly relevant X  X .
As the solution, we apply the cumulative logits approach [12] to convert the ranking problem back to a binary classification problem. Assume that each  X  y i  X  { 0 , 1 , ...,V } , we will construct V different binary document classifier separately. For the v -th document classi-fier, we partition the data into two groups: { y i &lt; v } and { y and learn a document classifier P ( y i  X  v | d i , w i ,t learning algorithm in Section 4.3.3. Given the results of the classi-fiers, we can compute the expected relevance as the ranking score of each document, given as follows, F ( d i ,t i ) =
In this section, we compare the overall performance of T-RBM with four state-of-the-art baselines, to demonstrate the effectiveness of applying T-RBM for cross-task document scoring problems.
To demonstrate the capacity of T-RBM on general cross-task document scoring problems, we study three different applications  X  verbose-query-based retrieval, entity-centric document filtering and cross-domain sentiment analysis  X  on different datasets with specification shown in Figure 4. 1. Robust (Verbose-query-based Retrieval) . In order to construct a large dataset, we combined two newswire collections released by TREC 2004 and 2005 Robust tracks. Unlike most datasets that con-tain only short keyword queries, these two collections are the largest publicly available datasets that have detailed query descriptions, which could be used as verbose queries. Following previous verbose query works [3], we only used the h desc i portions of TREC queries, and ignored the h title i portions. Given one query, each document is labeled as 2 X  X erfectly relevant, 1 X  X elevant and 0 X  X rrelevant. 2. TREC-KBA (Entity-centric Document Filtering) . This dataset includes 29 Wikipedia entities covering living persons from differ-ent domains and a few organizations. For each entity, 100  X  3000 candidate documents are collected and labeled as garbage, neutral, relevant or central. Following the same procedure in the previous work [22], we got binary labels by viewing central and relevant documents as positive, and others negative. 3. Review (Cross-domain Sentiment Analysis) . This dataset was constructed by Blitzer et al. [4] through selecting Amazon product reviews from four different domains: books, DVDs, electronics and kitchen appliances. Each domain contains 1000 positive, 1000 negative and 3000  X  5000 unlabeled reviews. We selected 60 pivot keywords by mutual information as suggested by Blitzer et al. [4], and learned pivot predictors (for SCL) and Pearson Correlation (for T-RBM) based on unlabeled reviews.

Among the three applications, verbose-query-based retrieval was studied as a ranking problem in previous works, while entity-centric document filtering and cross-domain sentiment analysis were treated as classification problems. Following such conventions, we used ranking-oriented metrics like NDCG@k and MAP (Mean Aver-age Precision) for verbose-query-based retrieval, and classification-oriented metrics including precision, recall, F1-measure and accura-cy for the other two tasks.

To confirm the confidence of the comparison, for both verbose-query-based retrieval and entity-centric document filtering, we di-vided queries into 5 sets, and adopted 5-fold cross validation with standard t-test. For cross-domain sentiment analysis, we applied the evaluation method in [4], which trained one model for each domain, adapted the model to the other three domains and reported the average ranking performance over 12 sets of results.
To deal with unbalanced data, we reweighed the training instances to balance positive and negative data. All features were first stan-dardized to reduce the impact of different feature scales. In those baselines which require classifier or ranker learning, we used SVM-Light [9] and RankSVM [10] to learn the feature weightings with default parameters. In T-RBM, we empirically set the number of keyword importance level L to be 2 and regularization factor  X  to be 0.005, and will later investigate their impact.
We designed different baselines to incrementally validate the performance of our proposed T-RBM model. First, we experimented the StandardLearner baseline to demonstrate the necessity of realizing learning to score keywords. Second, we compared T-RBM with LinearMapping [22, 3, 11] and BoostMapping [22] to verify the effectiveness of T-RBM on learning to adapt keyword contribution. In cross-domain sentiment analysis, we also compared T-RBM with SCL [4] to demonstrate that T-RBM well realizes the domain adaptation insight proposed by Blizter et al. [4]. 1. Standard Learning Model ( StandardLearner ). This baseline solves the problems by standard learning to rank/classify techniques, which defines the scorer by Eq. 4. In the feature design, for verbose-query-based retrieval, we followed the LETOR benchmark [14] to extract ranking features such as TFIDF, BM25 and Language model for each query-document pair; for entity-centric document filtering, as the input query is an entity document, we designed features to calculate the document similarity using different metrics introduced in [7], e.g. , Euclidean distance, cosine similarity; in cross-domain sentiment analysis, we used all keywords as features and disregarded domain differences. 2. Linear Weighting Model ( LinearWeight ). As discussed in Sec-tion 4.1, the models proposed by Bendersky et al. [3], Lease et al. [11], and Zhou et al. [22] all belong to the same category. This base-line extends such models to support multiple intra-features, which defines the scorer by Eq. 8. 3. Boosting Mapping Model ( BoostMapping ). This baseline adopts the boosting framework proposed by Zhou et al. [22], which defines the document scorer by Eq. 9. Since learning keyword clusters based on multiple intra-features is difficult, we first learned the keyword clusters c i based on only DocTF ( w,d ) , and then applied the SVM or RankSVM again to re-learn the feature weightings  X  ij . 4. Structural Correspondence Learning ( SCL ) [4]. This baseline implements the SCL algorithm (introduced in Section 2) proposed by Blitzer et al. We used SVMLight to train pivot predictors and the final classifiers, and adopted the same parameter setting used in [4].
Figure 5 demonstrates T-RBM consistently outperforms other baselines in three applications. Specifically, we observe that T-RBM achieves encouraging improvement against runner-up Boost-Mapping in verbose-query-based retrieval (NDCG@5 +10 . 2% , NDCG@10 +7 . 3% , NDCG@20 +8 . 5% and MAP +9 . 4% ) and entity-centric document filtering (F-Measure +3 . 5% and accuracy +5 . 4% ) with p-value &lt; 0 . 05 , while in cross-domain sentiment anal-ysis, T-RBM outperforms runner-up SCL (F-Measure +4 . 4% and accuracy +2 . 7% ). We analyze the performance differences between T-RBM and other baselines in the following.

First, for the StandardLearner baseline, its performance largely depends on the quality of the adopted features. The results in Figure 5 show that, intuitive feature designs fail to achieve satisfactory results for general cross-document scoring problems. Specifically, in verbose-query-based retrieval, using traditional document scorers as features does not perform well because of the existence of noisy keywords. In entity-centric document filtering, since there exist many relevant documents that are not similar to the identification page ( e.g. , they might discuss only one or two aspects of the entity), StandardLearner based on document similarity fails as well. In cross-domain sentiment analysis, the result confirms the necessity of adapting keyword importance for different domains.

Second, LinearWeight does not perform well. As we discussed in Section 4.1, the linear definition of keyword contribution function makes it vulnerable to noisy keywords. From the result, we can observe that LinearWeight achieves relatively better performance in verbose-query-based retrieval, which is a task that involves less noisy keywords compared with the other two. The results confirm the importance of fulfilling the inferred sparsity requirement.
Third, BoostMapping fulfills the inferred sparsity requirement, by eliminating the contribution of keywords from noisy clusters; however, it might easily overfit the dataset in some specific settings. In the experiment, we can observe that BoostMapping outperforms LinearWeight over Robust and KBA datasets, but achieves very poor performance on the Review dataset. That is because, unlike the first two applications where the training dataset contains many different tasks ( i.e. , queries or entities), in cross-domain sentiment analysis, the training data only contains one domain at a time. As the result, BoostMapping severely over-fits the training domain and fails to be generalized to the other domains.

Fourth, SCL outperforms other baselines, and the result is con-sistent with the performance reported in [4]. However, SCL relies on the idea of pivot predictors to realize domain adaptation, which could not be generalized to other cross-task document scoring prob-lems that do not have the concept of pivot keywords. Moreover, in the experiment, we discover that the performance of SCL is sensi-tive to the model used for training the pivot predictors  X  different pivot predictors ( e.g. , trained by SVM and logistic regression) tend to result in very different prediction performance.

Finally, T-RBM outperforms all the four baselines on three differ-ent applications. Essentially, the insight of T-RBM is very similar to BoostMapping , which classifies keywords into groups based on their meta-features, and learns the importance of different groups. However, different from BoostMapping which adopts a greedy algorithm to generate clusters, T-RBM takes advantage of Markov Network to jointly learn the weightings for meta-features and intra-features, which greatly reduces the risk of over-fitting. The results demonstrate that T-RBM achieves satisfactory performance on both learning to rank and domain adaptation.
In T-RBM, we have to manually determine two parameters: the number of keyword importance levels L and regularization param-eter  X  . We experimented different values of L ranging from 1 to 5, and discovered that the performance of T-RBM is not sensitive to L . That is because, as we discussed in Section 4.3.2, even if we set L = 1 , T-RBM can still model different levels of keyword importance by the marginalization of hidden variables.

In this section, we only investigate the impact of  X  . Figure 6 shows how different values of regularization parameter  X  affect the performances of T-RBM (in Figure 6 (a)) and the number of keywords that are classified as noise (in Figure 6 (b)). In Figure 6 (a), we can discover that, when  X  is negative, the performance of T-RBM decreases significantly for all three datasets. That is because, as Figure 6 (b) illustrates, a negative value of  X  will tempt the model to judge more keywords as important, making the scorer vulnerable to noisy keywords. The result again confirms the importance of ful-filling the requirement of inferred sparsity. T-RBM usually achieves the best performance when  X  is around 0 . 005 and 0 . 05 . Continually increasing the value of  X  will over-regularize the model and lower down the performance.
In this section, we are going to investigate the training efficiency of T-RBM. Figure 7(a) shows that the likelihood value converges very quickly within around 100 iterations in all three applications, which, as Figure 7(b) displays, takes 358 seconds for verbose-query-based retrieval, 2672 seconds for entity-centric document filtering, and 211 seconds for cross-sentiment analysis. In general, the train-ing speed is very fast to get a stable solution.
To give an intuitive illustration of how T-RBM identifies impor-tant/unimportant keywords for cross-task document scoring applica-tions, in Figure 8 (a), we perform case studies by showing 2 example tasks per dataset, and listing some of their unimportant ( L = 0 ) and important keywords ( L = 1 ) based the value of P ( h ij ) derived by T-RBM. To analyze the effect of our designed features, we list the meta-features and intra-features which have the maximal absolute weightings in Figure 8 (b).

As Figure 8 displays, in terms of meta-features, in verbose-query-based retrieval, it meets our expectation that IDF , for both unigrams and bigrams, has the highest weightings; the result also includes QueryTF[Bigram] , implying that bigrams ( e.g. ,  X  X rug purpose, X   X  X revent excess X ) usually have higher importance than unigrams. In entity-centric document filtering, QueryPos is the most impor-tant meta-feature, demonstrating the position of a keyword is a very useful signal for identifying important keywords from a long Wikipedia page; TFInfoBox also has high weighting, which verifies our intuition of using Wikipedia page structure to identify important keywords. In cross-domain sentiment analysis, the result shows that the correlation with keywords such as  X  X _wonderful X  and  X  X asy_to X  is helpful for identifying the sentiment of a keyword, demonstrating that our framework successfully realizes the insight proposed by Blizter et al. [4]. In terms of intra-features, we can observe DocTF always has the highest weighting, whereas, different applications favor different term frequency representations. For example, in entity-centric document filtering, DocTF_Normalized has higher weighting because the TREC-KBA dataset contains a large number of short documents ( e.g. , tweets), and without normalization, the scorer would severely bias towards long documents.
In this paper, we proposed to study the cross-task document scoring problem  X  given some labeled documents for some user queries or data domains, how to learn a scorer which could predict the relevance of a new document for a new query or domain. As the key insight, towards facilitating the feature design, we proposed the idea of feature decoupling. To design a document scorer based on the decoupled features, we proposed a novel T-RBM model as our solution. Experiments on three applications confirmed the effectiveness of T-RBM. [1] M. Bendersky and W. B. Croft. Discovering key concepts in [2] M. Bendersky, D. Metzler, and W. B. Croft. Learning concept [3] M. Bendersky, D. Metzler, and W. B. Croft. Parameterized [4] J. Blitzer, M. Dredze, and F. Pereira. Biographies, bollywood, [5] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. Hon. [6] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning [7] A. Huang. Similarity measures for text document clustering. [8] T. Joachims. Text categorization with support vector [9] T. Joachims. Making large scale svm learning practical. 1999. [10] T. Joachims. Optimizing search engines using clickthrough [11] M. Lease. An improved markov random field model for [12] P. Li, Q. Wu, and C. J. Burges. Mcrank: Learning to rank [13] T. Li, V. Sindhwani, C. Ding, and Y. Zhang. Knowledge [14] T.-Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: [15] N. Okazaki. Liblbfgs: a library of limited-memory [16] S. J. Pan, X. Ni, J.-T. Sun, Q. Yang, and Z. Chen.
 [17] J. Pearl. Probabilistic reasoning in intelligent systems: [18] R. Salakhutdinov and G. E. Hinton. Replicated softmax: an [19] P. Smolensky. Information processing in dynamical systems: [20] R. Tibshirani. Regression shrinkage and selection via the [21] V. Vapnik. Statistical learning theory, 1998. [22] M. Zhou and K. C.-C. Chang. Entity-centric document
