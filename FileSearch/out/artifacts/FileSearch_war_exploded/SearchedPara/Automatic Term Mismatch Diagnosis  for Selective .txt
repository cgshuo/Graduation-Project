 People are seldom aware that their search queries frequently mismatch a majority of the releva nt documents. This may not be a big problem for topics with a large and diverse set of relevant documents, but would largely increase the chance of search failure for less popular search needs. We aim to address the mismatch problem by developing accurate a nd simple queries that require minimal effort to construct. This is achieved by targeting retrieval interventions at the query terms that are likely to mismatch relevant documents. For a given topic, the proportion of relevant documents that do not contain a te rm measures th e probability for the term to mismatch relevant documents, or the term mismatch probability. Recent research dem onstrates that this probability can be estimated reliably prior to retr ieval. Typically, it is used in probabilistic retrieval models to provide query dependent term weights. This paper develops a new use: Automatic diagnosis of term mismatch. A search engine can use the diagnosis to suggest manual query reformulation, guide interactive query expansion, guide automatic query expansion, or motivate other responses. The research described here uses the diagnosis to guide interactive query expansion, and create Boolean conjunctive normal form (CNF) structured queries that selectively expand  X  X roblem X  query terms while leaving the rest of th e query untouched. Experiments with TREC Ad-hoc and Legal Track datasets demonstrate that with high quality manual expansion, this diagnostic approach can reduce user effort by 33%, and produce simple and effective structured queries that surpass their bag of word counterparts. H.3.3 [ Information Search and Retrieval ]: Query formulation, Retrieval Models Query term diagnosis, term misma tch, term expansion, Boolean conjunctive normal form queries, simulated user interactions Vocabulary mismatch between queries and documents is known to be important for full-text search. Recent research formally defined the term mismatch proba bility, and showed that on average a query term mismatches (fails to appear in) 40% to 50% of the documents relevant to the query [32]. With multi-word queries, the percentage of rele vant documents that match the whole query can degrade very quickly. Even when search engines do not require all query terms to appear in result documents, including a query term that is likely to mismatch relevant documents can still cause the mismatch problem: The retrieval model will penalize the relevant documents that do not contain the term, and at the same time favor documents (false positives) that happen to contain the term but are irrelevant. Since the number of false positives is typically much larger than the number of relevant documents for a topic [8], these false positives can appear throughout the rank list, burying the true relevant results. 
This work is concerned with the term mismatch problem, a long standing problem in retrieval. What X  X  new here is the term level diagnosis and intervention. We us e automatic predictions of the term mismatch probability [32] to proactively diagnose each query term, and to guide further in terventions to directly address the problem terms. Compared to prior approaches, which typically handle the query as a whole, the targeted intervention in this work generates simple yet effective queries. 
Query expansion is one of the most common methods to solve mismatch. We use the automati c term mismatch diagnosis to guide query expansion. Other fo rms of intervention, e.g. term removal or substitution, can also solve certain cases of mismatch, but they are not the focus of this work. We show that proper diagnosis can save expansion effort by 33%, while achieving near optimal performance. 
We generate structured expansion queries of Boolean conjunctive normal form (CNF) --a conjunction of disjunctions where each disjunction typically contains a query term and its synonyms. Carefully created CNF queries are highly effective. They can limit the effects of the expansion terms to their corresponding query term, so that while fixing the mismatched terms, the expansion query is still faithful to the semantics of the original query. We show that CN F expansion leads to more stable retrieval across different levels of expansion, minimizing problems such as topic drift even with skewed expansion of part of the query. It outperforms bag of word expansion given the same set of high quality expansion terms. This section discusses how this work relates to the other research that tries to solve the mismatch problem. In particular, research on predicting term mismatch a nd on conjunctive normal form (CNF) structured queries forms the basis of this work. Furnas et al. [7] were probably the first to study vocabulary mismatch quantitatively, by measuring how people name the same concept/activity differently. They showed that on average 80-90% of the times, two people will name the same item differently. The best term only covers about 15-35% of all the occurrences of the item, and the 3 best terms together only cover 37-67% of the cases. Even with 15 aliases, only 60-80% coverage is achieved. The authors suggested one solution to be  X  X nlimited aliasing X , which led to the Latent Semantic Analysis (LSA) [6] line of research. 
Zhao and Callan [32] formally defined the term mismatch appear in a document d , given that d is relevant to the topic ( d  X  R ), or equivalently, the proportion of relevant documents that do not contain term t . Furnas et al. [7] X  X  definition of vocabulary mismatch is query independent, and can be reduced to an average case of Zhao and Callan [32] X  X  query dependent definition. The complement of term mismatch is the term recall probability: documents relevant to the topic. This query dependent probability P( t | R ) is not new in retrieval research. It is known to be part of the Binary Independence Model (BIM ) [23], as part of the optimal term weight. Accurate estimation of P( t | R ) requires knowledge of R --the relevant set of a topic, which defeats the purpose of retrieval, and P( t | R ) was thought to be di fficult to estimate. 
Recent research showed that P( t | R ) can be reliably predicted without using relevance information of the test topics [8,20,32]. Zhao and Callan [32] achieved the best predictions from being the first to design and use query dependent features for prediction, features such as term centrality, replaceability and abstractness. 
Previously, P( t | R ) predictions were used to adjust query term weights of inverse document fre quency (idf)-based retrieval models such as Okapi BM25 and statistical language models. Term weighting is not a new technique in retrieval research, neither is predicting term weights. 
Our work is a significant depart ure from the prior research that predicted P( t | R ). We apply the P( t | R ) predictions in a completely new way, to automa tically diagnose term mismatch problems and inform further interventions. Query expansion is one of the most common ways to solve mismatch. In recent years, the research community has focused on expansion of the whole query, for example using pseudo relevance feedback [17]. This form of expansion is simple to manage and effective. It also allows introduction of expansion terms that are related to the query as a whole, even if their relationship to any specific original query term is tenuous. 
When people search for information, they typically develop queries in Boolean conjunctive norm al form (CNF). CNF queries are used by librarians [16,12], lawyers [2,26] and other expert searchers [4,13,21]. Each conjunct represents a high-level concept, and each disjunct represents alternate forms of the concept. Query expansion is accomplished by adding disjuncts that cover as many ways of e xpressing the concept as possible. 
For example, the query below from TREC 2006 Legal Track [2]  X  sales of tobacco to children  X  is expanded manually into ( sales OR sell OR sold ) AND ( tobacco OR cigar OR cigarettes ) AND ( children OR child OR teen OR juvenile OR kid OR adolescent ) 
CNF queries ensure precision by specifying a set of concepts that must appear (AND), and improve recall by expanding alternative forms of each concept. Compared to LSA or bag of word expansion, CNF que ries offer control over what query terms to expand ( the query term dimension ) and what expansion terms to use for a query term ( the expansion dimension ). 
However, these two dimensions of flexibility also make automatic formulation of CN F queries computationally challenging, and makes manual crea tion of CNF queries tedious. The few experiments demonstrating effective CNF expansion either used manually created queries or only worked for a special task. Hearst [13] and Mitra et al. [21] used ranked Boolean retrieval on manual CNF queries. Zhao and Callan [31] automatically created CNF queries for the question answering task, based on the semantic structure of the question. 
Along the two directions of term diagnosis and expansion, prior research has focused on identifying synonyms of query terms, i.e. the expansion dimension. Google has patents [15] using query logs to identify possible synonyms for query terms in the context of the query. Jones and colleagues [14] also extracted synonyms of query terms from query logs. They called it query substitutions. Wang and Zhai [28] mined effective query reformulations from query logs. Dang and Croft [5] did the same with TREC Web collections. Xue, Croft and Smith [30] weighted and combined automatic whole-query reformulations, similar to the way alternative structured queries were combined in [31]. If more extensive expansions were used, the more compact CNF expansion would be a reasonable next step. Because of reasons such as suboptimal quality of e xpansions or insufficient number of topics for evaluation, prior research on ad hoc retrieval has not seen automatic CNF expansion to outperform keyword retrieval. Perhaps the only exception is the related pro blem of context sensitive stemming [27,22,3], where expansion terms are just morphological variants of the qu ery terms, which are easier to identify and more accurate (less likely to introduce false positives). 
Such prior work tried to expand any query term, and did not exploit the term diagnosis dime nsion, thus th ey essentially expanded the query terms whose synonyms are easy to find. 
This work focuses on selectively expanding the query terms that really need expansion, a less well studied dimension. Exploiting this diagnosis dimension can guide further retrieval interventions such as automatic query reformulation or user interaction to the areas of the query that need help, leading to potentially more effective retrieval interventions. It also reduces the complexity of formulating CNF queries, manually or automatically. The prior research on synonym extraction is orthogonal to this work, and can be applied with term diagnosis in a real-world search system. Our diagnostic intervention framework is general and can be applied to both automatic and manual expansions. However, our experiments are still constrained by the availability of effective intervention methods. That is wh y we use manual CNF expansion, which is highly effective. To avoid using the expensive and less controllable online user studies, we use existing user-created CNF queries to simulate online diagnostic expansion interactions. 
Simulations of user interactions are not new in interactive retrieval experiments. Harman [9] simulated relevance feedback experiments by using relevance j udgements of the top 10 results, and evaluated feedback retrieval on the rest of the results. White et al. [29] also used relevanc e judgements and assumed several user browsing strategies to simula te users X  interactions with the search interface. The interaction simulations were used as user feedback to select expansion term s. Similarly, Lin and Smucker [18] assumed a set of strategies that define how the user browses the search results, to simulate and evaluate a result browsing interface using known relevance judgements. 
Compared to the prior work, our simulations never explicitly use any relevance judgements, a nd only make a few relatively weak assumptions about the user. We simulate based on existing user created Boolean CNF queries, which can be seen as recorded summaries of real user interactions . These fully expanded queries are used to simulate selective expansion interactions. This section discusses in more detail the diagnostic intervention framework, and shows how term diagnosis can be applied and evaluated in end-to-end retrieval experiments in an ideal setting. 
We hope to answer the following questions. Suppose the user is willing to invest some extra time for each query, how much effort is needed to improve the initial query (in expansion effort, how many query terms need to be expanded, and how many expansion terms per query term are needed)? When is the best performance achieved? Can we direct the user to a subset of the query terms so that less effort is needed to ac hieve a near optimal performance? What X  X  an effective crit erion for term diagnosis? The diagnostic intervention frame work is designed as follows. The user issues an initial keyw ord query. Given the query, the system selects a subset of the quer y terms and asks the user to fix (e.g. expand) them. The performance after user intervention is used to compare the different diagnosis strategies. 
This evaluation framework need s to control two dimensions, the diagnosis dimension (selecting the set of problem query terms) and the intervention dimension (determining the amount of intervention for each selected te rm). Diagnosis of terms with mismatch problems can be achieved using criteria such as low predicted P( t | R ) or high idf. The intervention dimension when implemented as query expansion ca n be controlled by asking the user to provide a certain number of expansion terms. We consider two term diagno sis methods, idf based term diagnosis and predicted P( t | R ) based diagnosis. 
The idf based diagnosis selects the query terms that have the highest idf first. Idf is known to have a correlation with P( t | R ) [8] and has been used as a feature for predicting P( t | R ) [8,20,32]. A rare term (high idf) usually mean s a high likelihood of mismatch, while a frequent word (e.g. stopword) would have a high P( t | R ). 
Diagnosis based on predicted P( t | R ) selects the query terms dependent features for prediction. It used top ranked documents from an initial retrieval to automatically extract query dependent synonyms. These synonyms were used to create some of the effective query dependent features, e.g. how often synonyms of a query term appear in top ranked documents from the initial retrieval, and how often such synonyms appear in place of the original query term in collection documents. Section 4 describes implementation details. To exactly follow this ideal fram ework, for each query, many user interaction experiments are needed  X  one experiment for each possible diagnostic intervention setup, preferably, one user per setup. Many factors need to be controlled, such as users X  prior knowledge of the topic, the quality of the manual interventions, users X  interaction time and interaction method (whether retrieval results are examined), so that the final retrieval performance will reflect the effect of the diagnostic component instead of random variation in the experiment setup. These factors are difficult to eliminate even with hundreds of experiments per topic. We show how simulations may help in the section below. In this section, we design the retrieval experiments to evaluate diagnostic interventions. We explain how user simulations may be an appropriate substitute for costly online experiments with human subjects. We explain how to design the diagnostic intervention experiment so that it measures the effects of term diagnosis, minimizing effects fr om confounding factors such as the quality of the post-diagnosis interventions. We examine the datasets used for this simulation, in particular how well the manual CNF queries fit the simulation assumptions. We also describe the whole evaluation procedure, including the implementation of the mismatch diagnosis methods which (together with the user intervention) produce the post-intervention queries, the retrieval model behind query execution, and the evaluation metrics used to measure retrieval effectiveness. 
We focus on interactive expansion as the intervention method, using existing CNF queries to simulate interactive expansion. This is due to both the effectiveness of query expansion to solve mismatch and the lack of user data for other types of interventions. As explained in Section 3.3, a large number of experiments and users are needed to evaluate diagnostic expansion using online user studies. To avoid this, we use offline automatic simulations, sketched in Figure 1. There are three players in the simulation, the user , the diagnostic expansion retrieval system and the simulation based evaluation system. Before evaluation, the user creates fully expanded CNF queries. These manually created CNF queries are used by the evaluation system to simulate selective expansions, and are accessible to the diagnostic retrieval system only through the simulation system. During evaluation, the simulation system first extracts a basic no-expansion keyword query from the CNF query, feeding the keyword query to the diagnosis system. The diagnostic expansion system automatically diagnoses which keywords are more problematic. Then, the simulation system takes the top several problem terms, and extracts a certain number of ex pansion terms for each selected query term from its corresponding conjunct in the manual CNF query. This step simulates a user expanding the selected query terms. The number of problem query terms to expand and the number of expansion terms to include are controlled by the simulation system, which will evalua te retrieval at five different selection levels and five different expansion levels. Finally, given these expansion terms for each se lected query term, the diagnostic expansion system forms an expa nsion query and does retrieval. 
For example, based on the CNF query in Section 2.2, the diagnosis method is given the keyword query sales tobacco children . It may see children as more problematic than the other terms, then a full expansion of this problem term would produce the query sales AND tobacco AND ( children OR child OR teen OR juvenile OR kid OR adolescent ), whose retrie val performance is evaluated as the end result of diagnostic expansion. If the evaluation system selects two query terms sales and children for expansion, with a maximum of one expansion term each, the final query would be ( sales OR sell ) AND tobacco AND ( children OR child ). These diagnostic expansion queries are partial expansions simulated using the fully expanded queries created by real users. 
Our simulation allows us to answer the same set of questions about the diagnostic expansion sy stem which we hope to answer through online user interactions, and requires simpler experiments. In our simulations, the same set of expansion terms is always used for a given query term, those from its corresponding CNF conjunct. Doing so minimizes the variation from the expansion terms as we measure the effects of the diagnosis component. The order in which expansion terms are added for a query term is also fixed, in the same order as they appear in the CNF conjunct. This way, we can tweak the level of expansion by gradually including more expansion terms from the li sts of expansion terms, and answer how much expansion is needed for optimal performance. 
Our simulation makes three a ssumptions about the user expansion process. We examine them below. Expansion term independence assumption : Expansion terms from fully expanded queries are held back from the query to simulate the selective and partial expansion of query terms. This simulation is based on the assumption that the user (a random process that generates expansion terms) will produce the same set of expansion terms for a query term whenever asked to expand any subset of the query terms. Equivalently, given the topic, the expansion process for one query term does not depend on the expansion of other query terms. In reality, a human user focusing on a subset of the query terms can typically achieve higher quality expansion. Thus, selective expa nsion may actually do better than the reported performance from the simulations. Expansion term sequence assumption : Controlling to include only the first few expansion terms of a query term simulates and measures a user X  X  expansion effort for that query term. It is assumed that the user would come up with the same sequence of expansion terms for each query te rm, no matter whether the user is asked to expand a subset or al l of the query terms. A downside of this simulation is that we do not know exactly how much time and effort the user has spen t on each expansion term. CNF keyword-query induction assumption : Instead of actually asking users to expand their initial queries, preexisting fully expanded CNF style queries are used to infer the original keyword query and to simulate the expansi on process. For example, given the CNF query in Section 2.2, the original keyword query is assumed to be ( sales tobacco children ). This simulation assumes that the original keyword query can be reconstructed from the manual CNF query, which could be missing some original query terms ( of and to in the example) or introduce new terms into the original keyword query. Howeve r, as long as we use highly effective CNF queries, it is safe to use the CNF induced keyword queries as the no-expansion baseline. 
We also made an effort to ensure that our  X  X everse-engineered X  keyword query is faithful to the vocabulary of the original query. Given the TREC official topic descri ption of a topic, we try to use the first term from each conjunct that appears in this description to reconstruct the keyword query. For conjuncts that do not have description terms, th e first term in the conjunct is used. 
For example, the topic described as sales of tobacco to children , with CNF query ( sales OR sell OR sold ) AND ( tobacco OR cigar OR cigarettes ) AND ( children OR child OR teen OR juvenile OR kid OR adolescent ), would have ( sales tobacco children ) as the unexpanded keyword query. If the description were sell tobacco to children , the keyword query would be instead ( sell tobacco children ), even when sales appears first in its conjunct. Using user simulations instead of real users can eliminate confounding factors such as the user X  X  prior knowledge of the topic and other details of the user interaction process. 
This work tests the hypothes is that term diagnosis can effectively guide query expansion. However, two factors directly determine the end performance of diagnostic expansion, 1) the effectiveness of term diagnosis, a nd 2) the benefit from expansion. 
Since our focus is on diagnosis, not query expansion, one of the most important confounding factors is the quality of the expansion terms, which we leave out of the evaluation by using a fixed set of high quality expansion terms from manual CNF queries to simulate an expert user doing manual expansion. 
Automatic query expansion is more desirable in a deployed system, but the uncertain quality of the expansion terms can confuse the evaluation. Thus, it is not considered in this paper. Datasets with high quality manual CNF queries are selected to simulate and evaluate diagnostic expansion. Four different datasets have been used, those from TREC 2006 and 2007 Legal tracks, and those from TREC 3 and 4 Ad hoc tracks. They are R ) prediction model. Here, the TREC 2006 (39 topics) and TREC 3 (50 topics) datasets are used for training the baseline model parameters and the P( t | R ) prediction models, while TREC 2007 (43 topics) and TREC 4 (50 t opics) are used for testing. The TREC Legal tracks contain Boolean CNF queries created through expert user interaction. They are fairly specific, averaging 3 conjuncts per topic, i. e., 3 concepts conjoined to form a query. The information needs of the Legal track topics are fictional, but mimic the real cases. 
The lawyers who created the TREC Legal queries know what the collection is, and have expert knowledge of what terminology the corpus documents might use to refer to a concept being requested. The lawyers would give very high priority to the recall of the queries they create. They tried to fully expand every query term, so as not to miss any pote ntially relevant document. An effort to avoid over-generalizi ng the topic was also made. However, the lawyers never looked at the retrieval results when creating these CNF style queries. We call this a case of blind user interaction , because no corpus information is accessed during user interaction. We use the Boolean queries from [33], which achieved near best performance in TREC 2007 Legal track. 
The 2006 and 2007 TREC Legal tracks share the same collection of documents. These are about 6.7 million tobacco company documents made public th rough litigation. They are on average 1362 words long. Many of them are OCR text, and contain spelling and spacing errors. 
For relevance judgments, because of the relatively large size of the collection, a sampled pooling strategy was adopted in Legal 2007, with 555.7 judgments per topic and 101 judged relevant documents per topic. 
More details about the dataset, the information needs, query formulation procedure, and relevance judgments can be found in the overview papers [2,26]. For the TREC 3 and 4 Ad hoc track datasets, high quality manual CNF queries were created by the University of Waterloo group [4]. An example query is ( responsibility OR standard OR train OR monitoring OR quality ) AND ( children OR child OR baby OR infant ) AND  X  au pair  X , where the  X  au pair  X  is a phrase. The information needs for the TREC 3 and 4 Ad hoc tracks are simpler (or broader), averaging 2 conjuncts per topic. 
The Waterloo queries were created for the MultiText system by an Iterative Searching and Judging (ISJ) process. These queries were manually formulated with access to the results returned by the retrieval system, thesaurus and other external resources of knowledge. This constitutes a case of user and corpus interaction . Quality of the manual Boolean queries is ensured by examining the retrieval results, thus, should be better than those created from blind user interaction of the Legal tracks. Since the interaction with search results, expansion processes of the query terms may not be independent of each other. For example, in order to discover the expansion term of a query term, one may need to expand another query term first, to bring up a result document that contains the expansion term. Thus, the expansion independence assumption (of Section 4.1) is more likely to be violated by the ISJ queries than by the Legal ones. 
The TREC 3 and 4 Ad hoc tracks used different collections, but they both consisted of newswire texts published before 1995. Each collection has about 0.56 million documents. The texts are non-OCR, thus cleaner than the Legal documents. 
The relevance judgments of th e Ad hoc tracks are deeper, because the collections are much smaller. The TREC 4 Ad hoc track made 1741 judgments per topic with 130 relevant. More details can be found in [10,11]. 
For all documents and queries, the Krovetz stemmer was used (more conservative than Porter), and no stopwords were removed. We explain the implementation of the diagnosis methods, idf and predicted P( t | R ), in more detail. 
Idf is calculated as  X  X f X /df X  X  X og X  X  X  , where N is the total number of documents in the colle ction and df is the document frequency of the term. This follows the RSJ formulation [23]. Automatic features used for prediction include idf, and the three features derived from applying late nt semantic analysis (LSA) [6] over the top ranked documents of an initial keyword retrieval.  X 2 X  X  X | X /1 X  , where  X  is the number of relevant documents containing t and | X | the total number of relevant documents for the query, with Laplace sm oothing used. Support Vector Regression with RBF kernel is used to learn the prediction model. 
There are 3 parameters: The number of top ranked documents for LSA, which is set at 180 for the Ad hoc datasets and 200 for the Legal track datasets, ba sed on a monotonic relationship between this parameter and the total number of collection Recently a more effici ent method of predicting P( t | R ) was developed that eliminates the need for an initial retrieval [Zhao, personal communication]. documents observed [32]. The num ber of latent dimensions to keep is fixed at 150, and the ga mma parameter which controls the width of the RBF kernel is fixed at 1.5 (as in [32]). [32] also used a feature that indicated whether a word in the query appears as a leaf node in the dependency parse of the query. Here, the feature is assumed to be 0 for all query terms, because the unexpanded query is usually not a natural language sentence or phrase, hence parsing may be inappropriate. 
A small number of the original terms in these CNF queries are phrases, windowed occurrences or other complex structures. They are assumed to have a P( t | R ) value of 0.5. The LSA component of the Lemur Toolkit is not designed to handle these complex terms, preventing the use of [32] X  X  model. This is a small handicap to our P( t | R ) prediction implementation, but not to the idf method, which is based on accurate df values calculated by the Indri search engine. To achieve a state-of-the-art performance, the retrieval model needs to rank collection docum ents using the Boolean CNF queries. Before the mid 1990 X  X , unranked Boolean was popular. Later research found ranked keyword to be more effective. However, to be fair, a ranked Boolean (e.g. soft or probabilistic) model should be used to compar e with other ranking approaches. 
This work adopts the language model framework, using probabilistic Boolean query execution (with Lemur/Indri version 4.10) [19]. The Boolean OR operator is still the hard OR, treating all the synonyms as if they are the same term for counting term-and document-frequencies (i.e. #syn operator in Indri query language). The Boolean AND is im plemented as the probabilistic AND (the Indri #combi ne operator) to produ ce a ranking score. 
Equations (1, 2) show how the retrieval model scores document times term a appears in document d .  X  is the parameter for Dirichlet smoothing, which is set at 900 for the Ad hoc datasets and 1000 for the Legal data sets based on training. = P( ( a OR b ) AND ( c OR e ) | d ) = P( ( a OR b ) | d ) * P( ( c OR e ) | d ) 
P( ( a OR b ) | d ) (2) = P( a | d ) + P( b | d ) (under Dirichlet smoothing) 
This language model based ranke d Boolean model is not the only possibility. Other ranked B oolean models include using the Boolean query as a two-tiered filter for the keyword rank list [13] [31], using the Okapi BM25 model for the conjunction [25], using probabilistic OR for the expansion terms (in Indri query language, #or instead of #syn), or using the p-norm Boolean ranking model [24]. We have tried some basic variations of the language model ranked Boolean model. Our pilot st udy shows that for our datasets, tiered filtering is sometimes worse than probabilistic Boolean, mostly because of the inferior ranking of the keyword queries. Probabilistic OR (#or) is numerically similar to treating all expansion terms the same as the original term (#syn), and the two methods perform similarly in retrie val. We did not try Okapi or p-norm, because the focus of this paper is P( t | R ) based diagnostic expansion, not to find the best ranked Boolean model. What is needed is one ranked Bool ean model that works. We use standard TREC evaluation measures for the datasets. Traditionally, pooled judgments and precision at certain cutoffs have been used in TREC. Mean Average Precision (MAP) at top 1000 is a summary statistic that cares about both top precision and precision at high recall levels, a nd has been used as the standard measure in TREC Ad hoc and Legal tracks. 
The statAP measure [1] is the standa rd measure for TREC Legal 2007. StatAP is an unbiased stat istical estimate of MAP designed to work with sampled pooling. It is unbiased in the sense that if all pooled documents were judged, the MAP value would have been the same as the mean of the estimated statAP. In traditional TREC pooling, the top 50 to top 100 documents from each submitted rank list are pooled, and all pooled documents are judged. In sampled pooling, only a sampled subset of the pool is judged. The idea is to use im portance sampling to judge fewer documents while maintaining a reliable estimate of MAP. Highly ranked documents from multiple pooled submissions are more likely to be relevant, and they are sampled more by importance sampling. StatAP takes into acc ount these sampling probabilities of the judged relevant documents, so that during evaluation, a judged relevant document with sampling probability p would be counted as a total of 1/ p relevant documents. This is because on average 1/ p  X  1 relevant documents are missed during the sampling procedure, and they are being represented by that one sampled relevant document. 
For topics where some relevant documents have low sampling probabilities, statAP estimates can deviate from the true AP a lot, but according to [26], when averaged over more than 20 topics, statAP provides a reliable estimate. These experiments test two main hypotheses. H1 : Mismatch diagnosis can direct expansion to the query terms that need expansion. H2 : Boolean CNF expansion is more effective than bag of word expansion with th e same set of high quality expansion terms. To test H1, the first experiment verifies the accuracy of idf and P( t | R )-prediction based term diagnosis against the true P( t | R ). The second experiment shows the effects of diagnosis by evaluating overal l retrieval performance along the query term dimension (5 diagno stic selection levels) and the expansion dimension (5 e xpansion levels). The third experiment compares predicted P( t | R ) diagnosis with idf based diagnosis. H2 is tested by the fourth experiment comparing CNF and bag-of-word expansion at various leve ls of diagnostic expansion. Listed below is the retrieval performance of the no expansion keyword retrieval baseline on the two test sets. terms, to rank them in a priority order for the user to fix (expand). This section is a unit test of the diagnosis component, in which accuracy is measured by how well the diagnosis method identifies the most problematic query terms (those most likely to mismatch). We measure how well the priority order (e.g. ascending predicted P( t | R )) ranks the query term with the true lowest P( t | R ), thus use Mean Reciprocal Rank (MRR) as the measure. Rank correlation measures do not distinguish ranki ng differences at the top vs. bottom of the rank lists, thus are less appr opriate here. 
On the Legal 2007 dataset, predicted P( t | R ) achieves an MRR of 0.6850, significantly higher than the MRR of 0.5523 of the idf method is still much better than random chance which has an MRR of 0.383, given the average 3 conjuncts per topic. 
This result that P( t | R ) prediction using [32] X  X  method is better than idf, and idf is better than random is consistent with prior research that predicted P( t | R ) [8,32]. Tables 1 2 and 3 report the expansion retrieval performance of predicted-P( t | R ) based and idf based diagnostic expansion, following the evaluation procedure detailed in Section 4.1. The results are arranged along two dimensions of user effort, the number of query terms selected for expansion, and the maximum number of expansion terms to incl ude for a selected query term. 
For example, results reported in column 2 row 2 selects 1 original query term of the highest idf for expansion, and a maximum of 1 expansion term is included for the selected query term. When the manual CNF query doesn X  X  expand the selected query term, no expansion term will be included in the final query. 
We are most concerned with the performance changes along each row of the tables, which are caused by the diagnosis methods. In Figure 2, we compare the relative performance gains of the different diagnosis methods as more query terms are being selected for expansion. Results ba sed on the last row of Tables 2 and 3 are presented in Figure 2. No expansion is 0%, and full expansion of all query terms gets 100%. With only 2 query terms selected for expansion, predicted P( t | R ) diagnosis is achieving 95% or 90% of the total gains of CNF expansion. Idf diagnosis is only achieving 64% or 83% of the total gains with 2 query terms, and need to fully expand 3 query terms to reach a performance close to the best (full expansion of all query terms). Thus, predicted P(t | R) based diagnosis saves 1/3 of users X  expansion effort while still achieving near optimal retrieval performance . 
Dataset Legal 2007 (MAP/statA P) TREC 4 (MAP) no expansion 0.0663/0.0160 0.1973 
Figure 2. Relative retrieval performance gains of diagnostic expansion as the number of quer y terms selected for expansion increases. Calculated based on the last row of Tables 2 and 3. 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
Gain in retrieval (MAP)
Tables 1, 2 and 3 show that th e more query terms selected for expansion, the better the performance. This is not surprising, as we are using carefully expanded manual queries. Similarly, including more expansion terms (along each column) almost always improves retrieval, except for the idf method in Table 1 with only one query term selected for expansion. 
The improvement over the no expansion baseline becomes significant after expanding two que ry terms for the idf method, and after only expanding one query term for predicted P( t | R ). 
Overall, P( t | R ) diagnostic expansion is more stable than the idf method. This shows up in seve ral areas. 1) Including more expansion terms always improves performance, even when only one original query term is selected for expansion. 2) Performance improvement over the no expansion baseline is significant even when only including one expansion term for one query term. These are not true for idf diagnosis. 3) Only two query terms need to be selected for expansion to achieve a performance close to the best, 33% less user effort than that of idf based diagnosis. 
The statAP measure from Table 2 correlates with the MAP measure, however, the sudden in creases in statAP from the 2 investigation shows that 1 to 2 topics benefited a lot from the extra (more than 4) expansion term s. The benefit is because of the successful matching of some relevant documents with low sampling probability, which increases statAP a lot, but not MAP. On the Legal 2007 dataset, the t opic that benefited most from the more than 4 expansion terms is a topic about James Bond movies. Certainly, there are more than 4 popular James Bond movies. 
Overall, predicted P( t | R ) can effectively guide user expansion to improve retrieval. Expandin g the first few query terms can result in significant gains in retrieval. The gain diminishes as more query terms are expanded, ev entually leading to the best performance of expanding every query term. The subsection above shows that diagnosis can help reduce expansion effort, and that P( t | R ) diagnosis results in more stable retrieval than idf. This section directly compares two retrieval experiments using predicted P( t | R ) vs. idf guided expansion. 
The two experiments both select 1 query term for full expansion (Table 1 last row, 2 nd vs. 3 rd column from the left). The MAP difference between 0.0754 of idf and 0.0798 of P( t | R ) is not baseline unexpanded queries produced an MAP of 0.0663. 0.0892 ** 0.0896 *** 0.0893 ** 0.0904 0.0916 *** 0.0938 # 0.0947 *** 0.0961 0.0938 *** 0.0965 # 0.0969 *** 0.0988 0.0968 # 0.0993 # 0.0996 # 0.1015 0.0986 # 0.1008 # 0.1016 # 0.1031 are inappropriate for the samplin g based statAP measure [26].) \ # terms selected # expansions per term\ 0.2350 ** 0.2358 ** 0.2356 ** 0.2358 0.2552 *** 0.2567 *** 0.2578 *** 0.2581 3 0.2187 * 0.2435 0.2589 *** 0.2608 # 0.2619 # 0.2622 4 0.2242 * 0.2489 0.2706 *** 0.2731 # 0.2753 # 0.2756
All 0.2319 ** 0.2526 0.2875 *** 0.2916 # 0.2935 # 0.2938 statistically significant. We investigate why below. According to the diagnostic expansion framework, two causes are possible, 1) to expand, or 2) the quality of the expansion terms that idf selected happen to be higher, causing the idf method to have better MAP sometimes, thus decreasing statistical significance. To separate the effects of diagno sis and expansion, we plot the Legal 2007 topics along two dimensions in the scatter-plot Figure 3. The x axis represents the diagnosis dimension: the difference between the true P( t | R ) of the two query terms selected by lowest predicted P( t | R ) and highest idf. The y axis represents the expansion performance dimension: the difference between the Average Precision (AP) values of the two methods on a topic. When idf and predicted P( t | R ) happen to select the same term to expand for a given topic, that to pic would be plotted on the origin ( x =0, y =0)  X  no difference in both diagnosis and expansion. From Figure 3, firstly, most points have x &lt; 0, meaning the P( t | R ) predictions are better at finding the low P( t | R ) terms than the idf based diagnosis. Secondly, mo st points are in the top left and bottom right quadrants, supporting the theory that expanding the the bottom right quadrant, occasionally idf method picks the right terms with lower P( t | R ) to expand, and does better in retrieval. 
However, there are three outliers in the bottom left quadrant, which are not fully explained by our theory. At the bottom left quadrant, predicted P( t | R ) does identify the right term with a lower P( t | R ), but the retrieval performance is actually worse than that of idf guided expansion. 
By looking into these three to pics, we found that the manual queries for topics 76 and 86 do not have any expansion terms for do have effective expansion terms. All such topics where a query term without expansion terms is selected are annotated with diamond shaped borders in the plot. Topic 55 is because of poor expansion term quality. The P( t | R ) method selects the chemical name apatite for expansion, which represent a class of chemicals. The manual expansion terms seem very reasonable, and are just names or chemical formulas of the chemicals belonging to the apatite family. However, the query is really about apatite rocks as they appear in nature, not any specific chemical in the apatite family. Thus, even expansion te rms proposed by experts can still sometimes introduce false positives into the rank list, and this problem cannot be easily identified without corpus interaction, e.g. examining the result rank list of documents. 
If these 3 topics were removed from evaluation, predicted P( t | R ) guided expansion would be signif icantly better than idf guided expansion, at p &lt; 0.05 by the two tailed sign test. 
Of the 50 TREC 4 topics, similarly, 4 topics are outliers. In two cases, the P( t | R ) selected query terms do not have manual the right term, but MAP is higher than idf diagnosis, because the idf method selected a query term with poor expansion terms. In one topic, the retrieval performance does not differ at top ranks, and the idf method only excels after 30 documents. 
Table 5. Retrieval performance of P( t | R ) guided bag of word expansion on TREC 4 Ad hoc track, as measured by MAP . 
The baseline unexpanded queries produced an MAP of 0.1973. \#qt #exp\ 1 0.2101 ** 0.2102 * 0.2117 ** 0.2113 2 0.2146 *** 0.2161 *** 0.2200 ** 0.2201 3 0.2160 *** 0.2154 *** 0.2222 *** 0.2226 4 0.2204 # 0.2272 *** 0.2288 *** 0.2309
All 0.2215 # 0.2290 ** 0.2329 ** 0.2343 Figure 3. Difference in prediction accuracy vs. difference in MAP for the two selective query expansion methods on 43 TREC 2007 Legal Track topics. The X axis shows the difference in true P( t | R ) between the first query terms selected by each method. The Y axis shows the difference in MAP between queries expanded by each method. The differences are calculated as that from predicted P( t | R ) based diagnosis minus that from idf based diagnosis. Points surrounded by a diamond represent topics in which one method selected a term that had no expansions. -1 -0.5 0 x -P( t | R ) Difference (correct prediction, but lower MAP)
The baseline unexpanded queries produced an MAP/statAP of 0.0663/0.0160. 
Overall, most topics confirm our hypothesis that expanding the query term likely to mismatch relevant documents leads to better retrieval, and better diagnosis al so leads to better retrieval. 
This analysis also shows the in herent difficulty of evaluating term diagnosis in end-to-end retr ieval experiments. Even with high quality manual expansion terms, there is still some variation in the quality of the expansion interventions, which can still interfere with the assessment of the diagnosis component. We compare CNF style expansion with two advanced bag-of-word expansion methods. 
For a fair comparison with manual CNF expansion, our first bag of word expansion baseline also uses the set of manual expansion terms selected by predicted P( t | R ). Expansion terms are then grouped and combined with the original query for retrieval. 
To make this baseline strong, both individual expansion terms and the expansion term set can be weighted. The individual expansion terms are weighted with the Relevance Model weights [17] from an initial keyword retrieval, with the parameter (the number of feedback documents) t uned on the training set. Manual expansion terms that do not appear in the feedback documents are still included in the final query, but a minimum weight is used to conform to the relevance model we ights. Uniform weighting of the expansion terms was also tried. It is more effective than relevance model weights when expa nsion is more balanced, i.e. more than 3 query terms are selected for expansion. When combining the expansion terms with the original query, the combination weights are 2-fold cross-validated on the test set. 
Table 4 shows the best case of both relevance-model-weight and uniform-weight bag of word expansion. Bag of word expansion performs worse than CN F expansion in almost all the different setups. The best perf ormance is achieved with full expansion of 4 query terms, with a MAP of 0.1038, slightly lower than that of CNF (0.1039 in Tabl e 1), however, the statAP value of 0.0211 is much worse than that of CNF (0.0508, Table 2). 
Table 5 shows the best case bag of word expansion results on the TREC 4 Ad hoc dataset. Cons istent with the statAP measure on TREC Legal 2007, CNF queries are much better than bag of word expansion. For example, with full expansion of all query terms, CNF expansion (Table 3) gets a MAP of 0.2938, 23% better than 0.2384 of the bag of word expansion with the same expansion terms, significant at p &lt; 0.0025 by the randomization test and weakly significant at p &lt; 0.0676 by the sign test. 
Some results of bag of word retrie val at low selection levels, i.e. selecting one query term to expand, perform better than idf guided CNF expansion. But since the ba g of word expansion here uses better expansion terms selected by predicted P( t | R ), this does not mean that bag of word is some times better than CNF expansion. The second bag of word expansion baseline is the standard Lavrenko Relevance Model itself [17], which uses automatic Parameters trained on Legal 2006 dataset when applied to Legal 2007 lead to an MAP of 0.0606, statAP of 0.0168, worse than the no expansion baseline. On TREC 4, it gets a MAP of 0.2488, slightly better than 0.2384, the best manual bag of word expansion, but still much worse than CNF (0.2938). In sum, given the same set of high quality expansion terms, CNF expansion works much better than bag of word expansion. We set out with the hypothesi s that term mismatch based diagnosis can successfully guide further retrieval intervention to fix  X  X roblem X  terms and improve re trieval. In this work, we applied the term mismatch diagnos is to guide interactive query expansion. Simulated interactiv e query expansion experiments on TREC Ad hoc and Legal track data sets not only confirmed this hypothesis, but also showed that automatically predicted P( t | R ) probabilities (the complement of term mismatch) can accurately guide expansion to the terms that need expansion most, and lead to better retrieval than when expa nding rare terms first. From the user X  X  point of view, it usually isn't necessary to expand every query term. Guided by predicted P( t | R ), expanding two terms is enough for most topics to achieve close-to-top performance, while guided by idf (rareness), three terms need to be expanded. P( t | R ) guidance can save user effort by 33%. 
In addition to confirming the main hypothesis, experiments also showed that Boolean conjunctiv e normal form (CNF) expansion outperforms carefully weighted bag of word expansion, given the same set of high quality expansion terms. The unstructured bag of word expansion typically needs ba lanced expansion of most query terms to achieve a reliable performance. 
Although the effect from adding more expansion terms to a query term diminishes, for the query terms that do need expansion, the effects of the expansion terms are typically additive, the more the expansion the better the performa nce. This is consistent with prior observations on vocabulary mismatch, that even after including more than 15 aliases, the effects of mismatch can still be observed, and further expans ion may still help [7]. 
For bag of word expansion, including more manual expansion terms also helps, but requires a balanced expansion of most query terms, and is not as effective and stable as CNF expansion. 
This work is mostly concerned with automatic diagnosis of problem terms in the query, and presents them to the user for manual expansion. It is still a question whether the diagnosis can help automatic formulation of eff ective CNF queries. We hope to let the system suggest or select expansion terms, automatically or semi-automatically with minimal user effort. Automatic identification of high quality e xpansion terms would be useful when the candidate expansion term s may not be of high quality, e.g. expansion terms from result documents, thesaurus or non-expert users. Poor expansion te rms in CNF queries are especially harmful, when they over-general ize the query and introduce false positives throughout the rank list. Tools such as performance prediction methods (e.g. query clarity), may help in such scenarios to detect the adverse effects of the poor expansion terms. 
In the future, we also hope to diagnose precision related problems as well as mismatch problems. We can then use the diagnosis to guide disambiguation, phrasing or fielded retrieval, as well as term substitution, removal or expansion. This work is supported by NSF grant IIS-1018317. Opinions in this work are solely the authors X . We thank Chengtao Wen, Grace Hui Yang, Jin Young Kim, Charlie Clarke, Gordon Cormack and NIST for helpful discussions, feedback and access to data. [1] J. A. Aslam and V. Pavlu. A practical sampling strategy for [2] J. R. Baron, D. D. Lewis and D. W. Oard. TREC 2006 Legal [3] G. Cao, S. Robertson and J. Nie. Selecting Query Term [4] C. L. A. Clarke, G. V. Corm ack and F. J. Burkowski. [5] V. Dang and W. B. Croft. Query reformulation using anchor [6] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer [7] G. W. Furnas, T. K. Landauer, L. M. Gomez, and S. T. [8] W. Greiff. A theory of term weighting based on exploratory [9] D. Harman. Towards interactive query expansion. In [10] D. Harman. Overview of the Third Text REtrieval [11] D. Harman. Overview of the Third Text REtrieval [12] S. Harter. Online Information Retrieval: Concepts, [13] M. Hearst. Improving full-tex t precision on short queries [14] R. Jones, B. Rey, O. Madani and W. Greiner. Generating [15] J. Lamping and S. Baker. De termining query term synonyms [16] W . Lancaster. Information Retrieval Systems: [17] V. Lavrenko and W. B. Croft. Relevance-based language [18] J. Lin and M. D. Smucker. How do users find things with [19] D. Metzler and W.B. Croft. Combining the language model [20] D. Metzler. Generalized inve rse document frequency. In [21] M. Mitra, A. Singhal and C. Buckley. Improving automatic [22] F. Peng, N. Ahmed, X. Li and Y. Lu. Context sensitive [23] S. E. Robertson and K. Sp X rck Jones. Rele vance weighting [24] G. Salton, E. A. Fox and H. Wu. Extended Boolean [25] S. Tomlinson. Experiments with the Negotiated Boolean [26] S. Tomlinson, D. W. Oard, J. R. Baron and P. Thompson. [27] E. Tudhope. Query based stemming. PhD Thesis. [28] X. Wang and C. Zhai. Mining term association patterns from [29] R. W. White, I. Ruthven, J. M. Jose, and C. J. Van [30] X. Xue, W. B. Croft and D. A. Smith. Modeling [31] L. Zhao and J. Callan. Effective and efficient structured [32] L. Zhao and J. Callan. Term necessity prediction. In [33] Y. Zhu, L. Zhao, J. Callan and J. Carbonell. Structured 
