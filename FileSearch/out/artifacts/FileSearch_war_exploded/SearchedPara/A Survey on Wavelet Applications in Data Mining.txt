 zsh@cs.rochester.edu Recently there has been significant development in the use of wavelet methods in various data mining processes. However, there has been written no comprehensive survey available on the topic. The goal of this is paper to fill the void. First, the paper presents a high-level data-mining framework that reduces the overall process into smaller components. Then applications of wavelets for each component are reviewd. The paper concludes by discussing the impact of wavelets on data mining research and outlining potential future research directions and applications. The wavelet transform is a synthesis of ideas that emerged over many years from different fields, such as mathematics and signal processing. Generally speaking, the wavelet transform is a tool that divides up data, functions, or operators into different frequency components and then studies each component with a resolution matched to its scale [52]. Therefore, the wavelet transform is antic-ipated to provide economical and informative mathematical repre-sentation of many objects of interest [1]. Nowadays many computer software packages contain fast and efficient algorithms to perform wavelet transforms. Due to such easy accessibility wavelets have quickly gained popularity among scientists and engineers, both in theoretical research and in applications. Above all, wavelets have been widely applied in such computer science research areas as im-age processing, computer vision, network management, and data mining.
 Over the past decade data mining, or knowledge discovery in databases (KDD), has become a significant area both in academia and in industry. Data mining is a process of automatic extraction of novel, useful and understandable patterns from a large collection of data. Wavelet theory could naturally play an important role in data mining since it is well founded and of very practical use. Wavelets have many favorable properties, such as vanishing moments, hier-archical and multiresolution decomposition structure, linear time and space complexity of the transformations, decorrelated coeffi-cients, and a wide variety of basis functions. These properties could provide considerably more efficient and effective solutions to many data mining problems. First, wavelets could provide presentations of data that make the mining process more efficient and accurate. Second, wavelets could be incorporated into the kernel of many data mining algorithms. Although standard wavelet applications are mainly on data which have temporal/spatial localities (e.g. time series, stream data, and image data) wavelets have also been suc-cessfully applied to diverse domains in data mining. In practice, a wide variety of wavelet-related methods have been applied to a wide range of data mining problems.
 Although wavelets have attracted much attention in the data mining community, there has been no comprehensive review of wavelet ap-plications in data mining. In this paper we attempt to fill the void by presenting the necessary mathematical foundations for understand-ing and using wavelets as well as a summary of research in wavelets applications. To appeal to a broader audience in the data mining community, this paper also providea brief overview of the practical research areas in data mining where wavelet could be used. The reader should be cautioned, however, that the wavelet is so a large research area that truly comprehensive surverys are almost impos-sible, and thus, that our overview may be a little eclectic. An inter-ested reader is encouraged to consult with other papers for further reading, in particular, surveys of wavelet applicaations in statis-tics [1; 10; 12; 121; 127; 163], time series analysis [124; 44; 129; 121; 122], biological data [9], signal processing [110; 158], image processing [133; 115; 85] and others [117; 174]. Also, [93] pro-vides a good overview on wavelet applications in database projects. The reader should be cautioned also that in our presentation mathe-matical descriptions are modified so that they adapt to data mining problems. A reader wishing to learn more mathematical details of wavelets is referred to [150; 52; 46; 116; 169; 165; 151]. This paper is organized as follows: To discuss a wide spectrum of wavelet applications in data mining in a systematic manner it seems crucial that data mining processes are divided into smaller components. Section 2 presents a high-level data mining frame-work, which reduces data mining process into four components. Section 3 introduces some necessary mathematical background re-lated to wavelets. Then wavelet applications in each of the four components will be reviewed in Sections 4, 5, and 6. Section 7 discusses some other wavelet applications which are related to data mining. Finally, Section 8 discusses future research directions. In this section, we give a high-level framework for data mining process and try to divide the data mining process into components. The purpose of the framework is to make our following reviews on wavelet applications in a more systematic way and hence it is colored to suit our discussion. More detailed treatment of the data mining process could be found in [79; 77].
 Data mining or knowledge discovery is the nontrivial extraction of implicit, previously unknown, and potentially useful informa-tion from large collection of data. It can be viewed as a multi-disciplinary activity because it exploits several research disciplines of artificial intelligence such as machine learning, pattern recog-SIGKDD Explorations. Volume 4, Issue 2 -page 49 nition, expert systems, knowledge acquisition, as well as mathe-matical disciplines such as statistics, information theory and uncer-tain inference. In our understanding, knowledge discovery refers to the overall process of extracting high-level knowledge from low-level data in the context of large databases. In the proposed frame-work, we view that knowledge discovery process usually consists of an iterative sequence of the following steps: data manage-ment , data preprocessing , data mining tasks algorithms and post-processing . These four steps are the four components of our framework.
 First, data management concerns the specific mechanism and structures for how the data are accessed, stored and managed. The data management is greatly related to the implementation of data mining systems. Though many research papers do not elaborate explicit data management, it should be note that data management can be extremely important in practical implementations.
 Next, data preprocessing is an important step to ensure the data quality and to improve the efficiency and ease of the mining pro-cess. Real-world data tend to be incomplete, noisy, inconsistent, high dimensional and multi-sensory etc. and hence are not di-rectly suitable for mining. Data preprocessing usually includes data cleaning to remove noisy data and outliers, data integration to integrate data from multiple information sources, data reduction to reduce the dimensionality and complexity of the data and data transformation to convert the data into suitable forms for mining etc.
 Third, we refer data mining tasks and algorithms as an essen-tial step of knowledge discovery where various algorithms are ap-plied to perform the data mining tasks. There are many different data mining tasks such as visualization, classification, clustering, regression and content retrieval etc. Various algorithms have been used to carry out these tasks and many algorithms such as Neu-ral Network and Principal Component Analysis could be applied to several different kinds of tasks.
 Finally, we need post-processing [28] stage to refine and evaluate the knowledge derived from our mining procedure. For example, one may need to simplify the extracted knowledge. Also, we may want to evaluate the extracted knowledge, visualize it, or merely document it for the end user. We may interpret the knowledge and incorporate it into an existing system, and check for potential con-flicts with previously induced knowledge.
 The four-component framework above provides us with a simple systematic language for understanding the steps that make up the data mining process. Since post-processing mainly concerns the non-technical work such as documentation and evaluation, we then focus our attentions on the first three components and will review wavelet applications in these components.
 It should be pointed out that categorizing a specific wavelet tech-nique/paper into a component of the framework is not strict or unique. Many techniques could be categorized as performing on different components. In this survey, we try to discuss the wavelet techniques with respect to the most relevant component based on our knowledge. When there is an overlap, i.e., a wavelet technique might be related to different components, we usually briefly exam-ine the relationships and differences. In this section, we will present the basic foundations that are neces-sary to understand and use wavelets. A wavelet can own many at-tractable properties, including the essential properties such as com-pact support, vanishing moments and dilating relation and other preferred properties such as smoothness and being a generator of an orthonormal basis of function spaces L 2 ( R n ) etc. Briefly speak-ing, compact support guarantees the localization of wavelets (In other words, processing a region of data with wavelets does not af-fect the the data out of this region); vanishing moment guarantees wavelet processing can distinguish the essential information from non-essential information; and dilating relation leads fast wavelet algorithms. It is the requirements of localization, hierarchical rep-resentation and manipulation, feature selection, and efficiency in many tasks in data mining that make wavelets be a very power-ful tool. The other properties such as smoothness and generators of orthonormal basis are preferred rather than essential. For ex-ample, Haar wavelet is the simplest wavelet which is discontinu-ous, while all other Daubechies wavelets are continuous. Further-more all Daubechies wavelets are generators of orthogonal basis for L ( R n ) , while spline wavelets generate unconditional basis rather than orthonormal basis [47], and some wavelets could only gener-ate redundant frames rather than a basis [138; 53]. The question that in what kinds of applications we should use orthonormal ba-sis, or other (say unconditional basis, or frame) is yet to be solved. In this section, to give readers a relatively comprehensive view of wavelets, we will use Daubechies wavelets as our concrete exam-ples. That is, in this survey, a wavelet we use is always assumed to be a generator of orthogonal basis.
 In signal processing fields, people usually thought wavelets to be convolution filters which has some specially properties such as quadrature mirror filters (QMF) and high pass etc. We agree that it is convenient to apply wavelets to practical applications if we thought wavelets to be convolution filters. However, according to our experience, thinking of wavelets as functions which own some special properties such as compact support, vanishing moments and multiscaling etc., and making use of some simple concepts of func-tion spaces L 2 ( R n ) (such as orthonormal basis, subspace and inner product etc.) may bring readers a clear understanding why these ba-sic properties of wavelets can be successfully applied in data min-ing and how these properties of wavelets may be applied to other problems in data mining. Thus in most uses of this survey, we treat wavelets as functions. In real algorithm designs and implementa-tions, usually a function is straightforwardly discretized and treated as a vector. The interested readers could refer to [109] for more details on treating wavelets as filters.
 The rest of the section is organized to help readers answer the fun-damental questions about wavelets such as: what is a wavelet, why we need wavelets, how to find wavelets, how to compute wavelet transforms and what are the properties of wavelets etc. We hope readers could get a basic understanding about wavelet after reading this section. So, first, what is a wavelet? Simply speaking, a mother wavelet is a function  X  ( x ) such that {  X  (2 j x  X  k ) ,i,k  X  Z } thonormal basis of L 2 ( R ) . The basis functions are usually referred ness refers to the condition that we desire that the function is of finite length or compactly supported. The wave refers to the con-dition that the function is oscillatory. The term mother implies that the functions with different regions of support that are used in the transformation process are derived by dilation and translation of the mother wavelet. Note that this orthogonality is not an essential property of wavelets. We include it in the definition because we discuss wavelet in the context of Daubechies wavelet and orthogonality is a good property in many applications.
 SIGKDD Explorations. Volume 4, Issue 2 -page 50 At first glance, wavelet transforms are pretty much the same as Fourier transforms except they have different bases. So why bother to have wavelets? What are the real differences between them ? The simple answer is that wavelet transform is capable of provid-ing time and frequency localizations simultaneously while Fourier transforms could only provide frequency representations. Fourier transforms are designed for stationary signals because they are ex-panded as sine and cosine waves which extend in time forever, if the representation has a certain frequency content at one time, it will have the same content for all time. Hence Fourier transform is not suitable for non-stationary signal where the signal has time varying frequency [130]. Since FT doesn X  X  work for non-stationary signal, researchers have developed a revised version of Fourier transform, The Short Time Fourier Transform(STFT). In STFT, the signal is divided into small segments where the signal on each of these seg-ments could be assumed as stationary. Although STFT could pro-vide a time-frequency representation of the signal, Heisenberg X  X  Uncertainty Principle makes the choice of the segment length a big problem for STFT. The principle states that one cannot know the exact time-frequency representation of a signal and one can only know the time intervals in which certain bands of frequencies exist. So for STFT, longer length of the segments gives better frequency resolution and poorer time resolution while shorter segments lead to better time resolution but poorer frequency resolution. Another serious problem with STFT is that there is no inverse, i.e., the orig-inal signal can not be reconstructed from the time-frequency map or the spectrogram.
 Wavelet is designed to give good time resolution and poor fre-quency resolution at high frequencies and good frequency reso-lution and poor time resolution at low frequencies [130]. This is useful for many practical signals since they usually have high frequency components for a short durations (bursts) and low frequency components for long durations (trends). The time-frequency cell structures for STFT and WT are shown in Figure 1 and Figure 2 respectively. Figure 1: Time-Frequency structure of STFT. The graph shows that time and frequency localizations are independent.
 The cells are always square. In data mining practice, the key concept in use of wavelets is the discrete wavelet transform(DWT). So our following discussion on wavelet is focused on discrete wavelet transform. How to find the wavelets? The key idea is self-similarity. Start with a function  X  ( x ) that is made up of smaller version of itself. This is the refinement (or 2-scale,dilation) equation a s are called filter coefficients or masks. The function called the scaling function (or father wavelet). Under certain con-ditions,  X  ( x ) = What are the conditions? First, the scaling function is chosen to preserve its area under each iteration, so R  X  grating the refinement equation then Z Hence P a k = 2 . So the stability of the iteration forces a con-dition on the coefficient a k . Second, the convergence of wavelet m = 0 , 1 , 2 ,..., N 2  X  1 (if a finite sum of wavelets is to represent the signal as accurately as possible). Third, requiring the orthogo-nality of wavelets forces the condition P N  X  1 m = 0 , 1 , 2 ,..., N 2  X  1 . Finally if the scaling function is required to be orthogonal P N  X  1  X   X   X   X   X   X   X  This class of wavelet function is constrained, by definition, to be zero outside of a small interval. This makes the property of com-pact support. Most wavelet functions, when plotted, appear to be extremely irregular. This is due to the fact that the refinement equa-tion assures that a wavelet  X  ( x ) function is non-differentiable ev-erywhere. The functions which are normally used for performing transforms consist of a few sets of well-chosen coefficients result-ing in a function which has a discernible shape.
 wavelets. They are named for pioneers in wavelet theory [75; 51]. First, consider the above constraints on the a k for N = 2 stability condition enforces a 0 + a 1 = 2 , the accuracy condition implies a 0  X  a 1 = 0 and the orthogonality gives a 2 The unique solution is a 0 = a 1 = 1 . if a 0 = a 1 = 1  X  ( x ) =  X  (2 x ) +  X  (2 x  X  1) . The refinement function is satisfied by a box function Once the box function is chosen as the scaling function, we then get the simplest wavelet: Haar wavelet, as shown in Figure 3.  X  a means the conjugate of a . When a is a real number,  X  a = a with support at [0 , 1] , called db 1 .
 SIGKDD Explorations. Volume 4, Issue 2 -page 51 Second, if N = 4 , The equations for the masks are: The solutions are a 0 = 1+  X  3 that is supported on intervals [0 , 3] , as shown in Figure 4. This construction is known as Daubechies wavelet construction [51]. In general, db n represents the family of Daubechies Wavelets and is the order. The family includes Haar wavelet since Haar wavelet represents the same wavelet as db 1 . Generally it can be shown that Finally let X  X  look at some examples where the orthogonal property does not hold. If a  X  1 = 1 The solution to this is the Hat function So we would get  X  ( x ) =  X  1 Note that the wavelets generated by Hat function are not orthogo-nal. Similarly, if a  X  2 = 1 we get cubic B-spline and the wavelets it generated are also not orthogonal. How to compute wavelet transforms? To answer the question of efficiently computing wavelet transform, we need to touch on some material of MRA. Multiresolution analysis was first intro-duced in [102; 109] and there is a fast family of algorithms based on it [109]. The motivation of MRA is to use a sequence of em-bedded subspaces to approximate L 2 ( R ) so that people can choose a proper subspace for a specific application task to get a balance between accuracy and efficiency (Say, bigger subspaces can con-tribute better accuracy but waste computing resources). Mathemat-ically, MRA studies the property of a sequence of closed subspaces V ,j  X  Z which approximate L 2 ( R ) and satisfy S all
V j ) and T j  X  Z V j =  X  (the intersection of all V j is empty). So what does multiresolution mean? The multiresolution is reflected by the additional requirement f  X  V j  X  X  X  f (2 x )  X  V j +1 (This is equivalent to f ( x )  X  V 0  X  X  X  f (2 j x )  X  V j ),i.e., all the spaces are scaled versions of the central(reference) space So how does this related to wavelets ? Because the scaling func-tion  X  easily generates a sequence of subspaces which can pro-vide a simple multiresolution analysis. First, the translations of  X  ( x ) , i.e.,  X  ( x  X  k ) ,k  X  Z , span a subspace, say  X  ( x  X  k ) ,k  X  Z constitutes an orthonormal basis of the subspace V ). Similarly 2  X  1 / 2  X  (2 x  X  k ) ,k  X  Z span another subspace, say V . The dilation equation 3.1 tells us that  X  can be represented by a basis of V 1 . It implies that  X  falls into subspace V translations  X  ( x  X  k ) ,k  X  Z also fall into subspace V is embedded into V 1 . With different dyadic, it is straightforward to obtain a sequence of embedded subspaces of L 2 ( R ) from only one function. It can be shown that the closure of the union of these sub-spaces is exactly L 2 ( R ) and their intersections are empty sets [52]. So here, we see that j controls the observation resolution while controls the observation location.
 Given two consecutive subspaces, say V 0 and V 1 , it is natural for people to ask what information is contained in the complement space V 1 V 0 , which is usually denoted as W 0 . From equation 3.2, it is straightforward to see that  X  falls also into V 1 (and so its trans-lations  X  ( x  X  k ) ,k  X  Z ). Notice that  X  is orthogonal to is easy to claim that an arbitrary translation of the father wavelet  X  is orthogonal to an arbitrary translation of the mother wavelet  X  . Thus, the translations of the wavelet  X  span the complement subspace W 0 . Similarly, for an arbitrary j ,  X  k,j an orthonormal basis of W j which is the orthogonal complement space of V j in V j +1 . Therefore, L 2 ( R ) space is decomposed into an infinite sequence of wavelet spaces, i.e., L 2 ( R ) = L More formal proof of wavelets X  spanning complement spaces can be found in [52].
 A direct application of multiresolution analysis is the fast discrete wavelet transform algorithm, called pyramid algorithm [109]. The core idea is to progressively smooth the data using an iterative pro-cedure and keep the detail along the way, i.e., analyze projections of f to W j . We use Haar wavelets to illustrate the idea through the following example. In Figure 5, the raw data is in resolution 3 (also called layer 3). After the first decomposition, the data are divided SIGKDD Explorations. Volume 4, Issue 2 -page 52 into two parts: one is of average information (projection in the scal-ing space V 2 and the other is of detail information (projection in the wavelet space W 2 ). We then repeat the similar decomposition on the data in V 2 , and get the projection data in V 1 and W also give a more formal treatment in Appendix B.
 Layer 2
Layer 3 12 16 20 The fact that L 2 ( R ) is decomposed into an infinite wavelet sub-space is equivalent to the statement that  X  j,k ,j,k  X  Z orthonormal basis of L 2 ( R ) . An arbitrary function f  X  L then can be expressed as follows: where d j,k =  X  f, X  j,k  X  is called wavelet coefficients . Note that j controls the observation resolution and k controls the observa-tion location. If data in some location are relatively smooth (it can be represented by low-degree polynomials), then its corresponding wavelet coefficients will be fairly small by the vanishing moment property of wavelets. In this section, we give two detailed examples of Haar wavelet transform. Haar transform can be viewed as a series of averaging and differ-encing operations on a discrete function. We compute the aver-ages and differences between every two adjacent values of The procedure to find the Haar transform of a discrete function f ( x ) = [7 5 1 9] is shown in Table 1: Resolution 4 is the full res-Table 1: An Example of One-dimensional Haar Wavelet Transform obtained by taking the average of (7 5) and (1 9) at resolution 4 respectively. (-1 4) are the differences of (7 5) and (1 9) divided by 2 respectively. This process is repeated until a resolution reached. The Haar transform H ( f ( x )) = (5.5 -0.5 -1 4) is obtained by combining the last average value 5 and the coefficients found on the right most column, -0.5, -1 and 4. In other words, the wavelet transform of original sequence is the single coefficient representing the overall average of the original average of the original numbers, followed by the detail coefficients in order of increasing resolu-tions. Different resolutions can be obtained by adding difference values back or subtracting differences from averages. For instance, (6 5) = (5.5 + 0.5,5.5  X  0.5) where 5 . 5 and  X  0 . 5 are the first and the second coefficient respectively. This process can be done recur-sively until the full resolution is reached. Note that no information has been gained or lost by this transform: the original sequence had 4 numbers and so does the transform.
 Haar wavelets are the most commonly used wavelets in database/computer science literature because they are easy to com-prehend and fast to compute. The error tree structure is often used by researchers in the field as a helpful tool for exploring and un-derstanding the key properties of the Haar wavelets decomposi-tion [113; 70]. Basically speaking, the error tree is a hierarchical structure built based on the wavelet decomposition process. The error tree of our example is shown in Figure 6. The leaves of the tree represents the original signal value and the internal nodes cor-respond to the wavelet coefficients. the wavelet coefficient associ-ated with an internal node in the error tree contributes to the signal values at the leaves in its subtree. In particular, the root corresponds the overall average of the original data array. The depth of the tree represents the resolution level of the decomposition. Multi-dimensional wavelets are usually defined via the tensor prod-tion we will illustrate the two-dimensional Haar wavelet transform through the following example.
 Let X  X  compute the Haar wavelet transform of the following two-dimensional data The computation is based on 2  X  2 matrices. Consider the upper left matrix 6 For a given component function f 1 ,  X  X  X  f d , define Q sional basis functions based on mutual transformations of the di-mensions and interested readers may refer to [149] for more details. SIGKDD Explorations. Volume 4, Issue 2 -page 53 We first compute the overall average: (3 + 5 + 9 + 8) / 4 = 6 . 25 then the average of the difference of the summations of the rows: 1 / 2[(9 + 8) / 2  X  (3 + 5) / 2] = 2 . 25 , followed by the average of the difference of the summations of the columns: 1 / 2[(5 + 8) / 2  X  (3 + 9) / 2] = 0 . 25 and finally the average of the difference of the summations of the diagonal: 1 / 2[(3+8) / 2  X  (9+5) / 2] =  X  0 . 75 So we get the following matrix For bigger data matrices, we usually put the overall average ele-ment of all transformed 2  X  2 matrix into the first block, the average of the difference of the summations of the columns into the second block and so on. So the transformed matrix of the original data is In this section, we summarize and highlight the properties of wavelets which make they are useful tools for data mining and many other applications. A wavelet transformation converts data from an original domain to a wavelet domain by expanding the raw data in an orthonormal basis generated by dilation and translation of a father and mother wavelet. For example, in image process-ing, the original domain is spatial domain, and the wavelet domain is frequency domain. An inverse wavelet transformation converts data back from the wavelet domain to the original domain. Without considering the truncation error of computers, the wavelet transfor-mation and inverse wavelet transformation are lossless transforma-tions. So the representations in the original domain and the wavelet domain are completely equivalent. In the other words, wavelet transformation preserves the structure of data. The properties of wavelets are described as follows: 1. Computation Complexity: First, the computation of wavelet 2. Vanishing Moments: Another important property of wavelets 3. Compact Support: Each wavelet basis function is supported 4. Decorrelated Coefficients: Another important aspect of 5. Parseval X  X  Theorem: Assume that e  X  L 2 and  X  i be the or-In addition, the multiresolution property of scaling and wavelet functions, as we discussed in Section 3.3, leads to hierarchical rep-resentations and manipulations of the objects and has widespread applications. There are also some other favorable properties of wavelets such as the symmetry of scaling and wavelet functions, smoothness and the availability of many different wavelet basis functions etc. In summary, the large number of favorable wavelet properties make wavelets powerful tools for many practical prob-lems. One of the features that distinguish data mining from other types of data analytic tasks is the huge amount of data. So data man-agement becomes very important for data mining. The purpose of data management is to find methods for storing data to facilitate fast and efficient access. Data management also plays an important role in the iterative and interactive nature of the overall data min-ing process. The wavelet transformation provides a natural hierar-chy structure and multidimensional data representation and hence could be applied to data management.
 Shahabi et al. [144; 143] introduced novel wavelet based tree struc-tures: TSA-tree and 2D TSA-tree, to improve the efficiency of mul-tilevel trends and surprise queries on time sequence data. Frequent queries on time series data are to identify rising and falling trends and abrupt changes at multiple level of abstractions. For example, we may be interested in the trends/surprises of the stock of Xe-rox Corporation within the last week, last month, last year or last decades. To support such multi-level queries, a large amount of raw data usually needs to be retrieved and processed. TSA (Trend and Surprise Abstraction) tree are designed to expedite the query SIGKDD Explorations. Volume 4, Issue 2 -page 54 Figure 7: 1D TSA Tree Structure: X is the input sequence. and DX i are the trend and surprise sequence at level i . process. TSA tree is constructed based on the procedure of discrete wavelet transform. The root is the original time series data. Each level of the tree corresponds to a step in wavelet decomposition. At the first decomposition level, the original data is decomposed into a low frequency part (trend) and a high frequency part (surprise). The left child of the root records the trend and the right child records the surprise. At the second decomposition level, the low frequency part obtained in the first level is further divided into a trend part and a surprise part. So the left child of the left child of the root records the new trend and the right child of the left child of the root records the new surprise. This process is repeated until the last level of the decomposition. The structure of the TSA tree is described in Fig-ure 7. Hence as we traverse down the tree, we increase the level of abstraction on trends and surprises and the size of the node is de-creased by a half. The nodes of the TSA tree thus record the trends and surprises at multiple abstraction levels. At first glance, TSA tree needs to store all the nodes. However, since TSA tree encodes the procedure of discrete wavelet transform and the transform is lossless, so we need only to store the all wavelet coefficients (i.e., all the leaf nodes). The internal nodes and the root can be easily ob-tained through the leaf nodes. So the space requirement is identical to the size of original data set. In [144], the authors also propose the techniques of dropping selective leaf nodes or coefficients with the heuristics of energy and precision to reduce the space requirement. 2D TSA tree is just the two dimensional extensions of the TSA tree using two dimensional discrete wavelet transform. In other words, the 1D wavelet transform is applied on the 2D data set in differ-ent dimensions/direction to obtain the trends and the surprises. The surprises at a given level correspond to three nodes which account for the changes in three different directions: horizontal, vertical and diagonal. The structure of a 2D TSA-tree is shown in Fig 8. Venkatesan et al. [160] proposed a novel image indexing tech-nique based on wavelets. With the popularization of digital images, managing image databases and indexing individual images become more and more difficult since extensive searching and image com-parisons are expensive. The authors introduce an image hash func-tion to manage the image database. First a wavelet decomposition of the image is computed and each subband is randomly tiled into small rectangles. Each rectangle X  X  statistics (e.g., averages or vari-ances) are calculated and quantized and then input into the decod-ing stage and a suitably chosen error-correcting code to generate the final hash value. Experiments have shown that the image hashing is robust against common image processing and malicious attacks. Santini and Gupta [141] defined wavelet transforms as a data type for image databases and also presents an algebra to manipulate the wavelet data type. It also mentions that wavelets can be stored us-Figure 8: 2D TSA Tree Structure: X is the input sequence. AX i ,D 1 X i ,D 2 X i ,D 3 X i are the trend and horizontal, vertical and diagonal sequence at level i respectively. ing a quadtree structure for every band and hence the operations can be implemented efficiently. Subramanya and Youssef [155] ap-plied wavelets to index the Audio data. More wavelet applications for data management can be found in [140]. We will discuss more about image indexing and search in Section 6.5. Real world data sets are usually not directly suitable for performing data mining algorithms [134]. They contain noise, missing values and may be inconsistent. In addition, real world data sets tend to be too large, high-dimensional and so on. Therefore, we need data cleaning to remove noise, data reduction to reduce the dimension-ality and complexity of the data and data transformation to con-vert the data into suitable form for mining etc. Wavelets provide a way to estimate the underlying function from the data. With the vanishing moment property of wavelets, we know that only some wavelet coefficients are significant in most cases. By retaining se-lective wavelet coefficients, wavelets transform could then be ap-plied to denoising and dimensionality reduction. Moreover, since wavelet coefficients are generally decorrelated, we could transform the original data into wavelet domain and then carry out data min-ing tasks. There are also some other wavelet applications in data preprocessing. In this section, we will elaborate various applica-tions of wavelets in data preprocessing. Noise is a random error or variance of a measured variable [78]. There are many possible reasons for noisy data, such as measure-ment/instrumental errors during the data acquisition, human and computer errors occurring at data entry, technology limitations and natural phenomena such as atmospheric disturbances, etc. Remov-ing noise from data can be considered as a process of identifying outliers or constructing optimal estimates of unknown data from available noisy data. Various smoothing techniques, such as bin-ning methods, clustering and outlier detection, have been used in data mining literature to remove noise. Binning methods smooth a sorted data value by consulting the values around it. Many data mining algorithms find outliers as a by-product of clustering algo-rithms [5; 72; 176] by defining outliers as points which do not lie in clusters. Some other techniques [87; 14; 135; 94; 25] directly find points which behave very differently from the normal ones. Aggarwal and Yu [6] presented new techniques for outlier detec-tion by studying the behavior of projections from datasets. Data can also be smoothed by using regression methods to fit them with a function. In addition, the post-pruning techniques used in deci-SIGKDD Explorations. Volume 4, Issue 2 -page 55 sion trees are able to avoid the overfitting problem caused by noisy data [119]. Most of these methods, however, are not specially de-signed to deal with noise and noise reduction and smoothing are only side-products of learning algorithms for other tasks. The in-formation loss caused by these methods is also a problem. Wavelet techniques provide an effective way to denoise and have been successfully applied in various areas especially in image re-search [39; 152; 63]. Formally, Suppose observation data ( y 1 ,...,y n ) is a noisy realization of the signal x = ( x where i is noise. It is commonly assumed that i are independent from the signal and are independent and identically distributed ( iid ) Gaussian random variables. A usual way to denoise is to find such that it minimizes the mean square error (MSE), The main idea of wavelet denoising is to transform the data into a different basis, the wavelet basis, where the large coefficients are mainly the useful information and the smaller ones represent noise. By suitably modifying the coefficients in the new basis, noise can be directly removed from the data.
 Donoho and Johnstone [60] developed a methodology called waveShrink for estimating x . It has been widely applied in many applications and implemented in commercial software, e.g., wavelet toolbox of Matlab [69].
 WaveShrink includes three steps: 1. Transform data y to the wavelet domain. 2. Shrink the empirical wavelet coefficients towards zero. 3. Transform the shrunk coefficients back to the data domain. There are three commonly used shrinkage functions: the hard, soft and the non-negative garrote shrinkage functions: where  X   X  [0 ,  X  ) is the threshold.
 Wavelet denoising generally is different from traditional filtering approaches and it is nonlinear, due to a thresholding step. Deter-mining threshold  X  is the key issue in waveShrink denoising. Min-imax threshold is one of commonly used thresholds. The expression where R  X  (  X  ) = E (  X   X  ( x )  X   X  ) 2 ,x  X  N (  X , 1) . Interested readers can refer to [69] for other methods and we will also discuss more about the choice of threshold in Section 6.3. Li et al. [104] inves-tigated the use of wavelet preprocessing to alleviate the effect of noisy data for biological data classification and showed that, if the localities of data the attributes are strong enough, wavelet denois-ing is able to improve the performance. A wide class of operations can be performed directly in the wavelet domain by operating on coefficients of the wavelet transforms of original data sets. Operating in the wavelet domain enables to per-form these operations progressively in a coarse-to-fine fashion, to operate on different resolutions, manipulate features at different scales, and to localize the operation in both spatial and frequency domains. Performing such operations in the wavelet domain and then reconstructing the result is more efficient than performing the same operation in the standard direct fashion and reduces the mem-ory footprint. In addition, wavelet transformations have the ability to reduce temporal correlation so that the correlation of wavelet co-efficients are much smaller than the correlation of corresponding temporal process. Hence simple models which are insufficient in the original domain may be quite accurate in the wavelet domain. These motivates the wavelet applications for data transformation. In other words, instead of working on the original domain, we could working on the wavelet domain.
 Feng et al. [65] proposed a new approach of applying Principal Component Analysis (PCA) on the wavelet subband. Wavelet transform is used to decompose an image into different frequency subbands and a mid-range frequency subband is used for PCA rep-resentation. The method reduces the computational load signif-icantly while achieving good recognition accuracy. Buccigrossi and Simoncelli [29] developed a probability model for natural images, based on empirical observation of their statistics in the wavelet transform domain. They noted that pairs of wavelet co-efficients, corresponding to basis functions at adjacent spatial loca-tions, orientations, and scales, generally to be non-Gaussian in both their marginal and joint statistical properties and specifically, their marginals are heavy-tailed, and although they are typically decor-related, their magnitudes are highly correlated. Hornby et al. [82] presented the analysis of potential field data in the wavelet domain. In fact, many other wavelet techniques that we will review for other components could also be regarded as data transformation. using some smaller set of data with or without a loss of information. Wavelet transformation represents the data as a sum of prototype functions and it has been shown that under certain conditions the transformation only related to selective coefficients. Hence simi-lar to denoising, by retaining selective coefficients, wavelets can achieve dimensionality reduction. Dimensionality reduction can be thought as an extension of the data transformation presented in Section 5.2: while data transformation just transforms original data into wavelet domain without discarding any coefficients, di-mensionality reduction only keeps a collection of selective wavelet coefficients.
 More formally, the dimensionality reduction problem is to project the n -dimensional tuples that represent the data in a k -dimensional space so that k &lt;&lt; n and the distances are preserved as well as possible. Based on the different choices of wavelet coefficients, there are two different ways for dimensionality reduction using wavelet, SIGKDD Explorations. Volume 4, Issue 2 -page 56 Keeping the largest k coefficients achieve more accurate represen-tation while keeping the first k coefficients is useful for index-ing [74]. Keeping the first k coefficients implicitly assumes a priori the significance of all wavelet coefficients in the first els and that all wavelet coefficients at a higher resolution levels are negligible. Such a strong prior assumption heavily depends on a suitable choice of k and essentially denies the possibility of local singularities in the underlying function [1].
 It has been shown that [148; 149], if the basis is orthonormal, in terms of L 2 loss, maintaining the largest k wavelet coefficients pro-vides the optimal k -term Haar approximation to the original signal. Suppose the original signal is given by f ( x ) = P M  X  1 where  X  i ( x ) is an orthonormal basis. In discrete form, the data can then be expressed by the coefficients c 0 ,  X  X  X  ,c be a permutation of 0 ,...,M  X  1 and f 0 ( x ) be a function that f that the decreasing ordering of magnitude gives the best permuta-tion as measured in L 2 norm. The square of L 2 error of the approx-imation is Hence to minimize the error for a given M 0 , the best choice for is the permutation that sorts the coefficients in decreasing order of magnitude; i.e., | c Using the largest k wavelet coefficients, given a predefined preci-sion , the general step for dimension reduction can be summarized in the following steps: || c i || is the norm of c i . In general, the norm can be chosen as L 2 norm where || c i || = ( c i ) 2 or L 1 norm where || c other norms. In practice, wavelets have been successfully applied in image compression [45; 37; 148] and it was suggested that norm is best suited for the task of image compression [55]. Chan and Fu [131] used the first k coefficients of Haar wavelet transform of the original time series for dimensionality reduction and they also show that no false dismissal (no qualified results will be rejected) for range query and nearest neighbor query by keeping the first few coefficients. Data mining tasks and algorithms refer to the essential procedure where intelligent methods are applied to extract useful information patterns. There are many data mining tasks such as clustering, clas-sification, regression, content retrieval and visualization etc. Each task can be thought as a particular kind of problem to be solved by a data mining algorithm. Generally there are many different al-gorithms could serve the purpose of the same task. Meanwhile, some algorithms can be applied to different tasks. In this section, we review the wavelet applications in data mining tasks and al-gorithms. We basically organize the review according to different tasks. The tasks we discussed are clustering, classification, regres-sion, distributed data mining, similarity search, query processing and visualization. Moreover, we also discuss the wavelet applica-tions for two important algorithms: Neural Network and Princi-pal/Independent Component Analysis since they could be applied to various mining tasks. The problem of clustering data arises in many disciplines and has a wide range of applications. Intuitively, the clustering problem can be described as follows: Let W be a set of n data points in a multi-dimensional space. Find a partition of W into classes such that the points within each class are similar to each other. The clustering problem has been studied extensively in machine learning [41; 66; 147; 177], databases [5; 72; 7; 73; 68], and statistics [22; 26] from various perspectives and with various approaches and focuses. The multi-resolution property of wavelet transforms inspires the researchers to consider algorithms that could identify clusters at different scales. WaveCluster [145] is a multi-resolution clustering approach for very large spatial databases. Spatial data objects can be represented in an n-dimensional feature space and the numerical attributes of a spatial object can be represented by a feature vector where each element of the vector corresponds to one numerical at-tribute (feature). Partitioning the data space by a grid reduces the number of data objects while inducing only small errors. From a signal processing perspective, if the collection of objects in the fea-ture space is viewed as an n-dimensional signal, the high frequency parts of the signal correspond to the regions of the feature space where there is a rapid change in the distribution of objects (i.e., the boundaries of clusters) and the low frequency parts of the dimensional signal which have high amplitude correspond to the ar-eas of the feature space where the objects are concentrated (i.e., the clusters). Applying wavelet transform on a signal decomposes it into different frequency sub-bands. Hence to identify the clusters is then converted to find the connected components in the transformed feature space. Moreover, application of wavelet transformation to feature spaces provides multiresolution data representation and hence finding the connected components could be carried out at different resolution levels. In other words, the multi-resolution property of wavelet transforms enable the WaveCluster algorithm could effectively identify arbitrary shape clusters at different scales with different degrees of accuracy. Experiments have shown that WaveCluster outperforms Birch [176] and CLARANS [126] by a large margin and it is a stable and efficient clustering method. Classification problems aim to identify the characteristics that in-dicate the group to which each instance belongs. Classification can be used both to understand the existing data and to predict how new instances will behave. Wavelets can be very useful for classi-fication tasks. First, classification methods can be applied on the wavelet domain of the original data as discussed in Section 5.2 or selective dimensions of the wavelet domain as we will discussed in this section. Second, the multi-resolution property of wavelets can be incorporated into classification procedures to facilitate the process.
 Castelli et al. [33; 34; 35] described a wavelet-based classification SIGKDD Explorations. Volume 4, Issue 2 -page 57 algorithm on large two-dimensional data sets typically large dig-ital images. The image is viewed as a real-valued configuration on a rectangular subset of the integer lattice Z 2 and each point on the lattice (i.e. pixel) is associated with a vector denoting as pixel-values and a label denoting its class. The classification prob-lem here consists of observing an image with known pixel-values but unknown labels and assigning a label to each point and it was motivated primarily by the need to classify quickly and efficiently large images in digital libraries. The typical approach [50] is the traditional pixel-by-pixel analysis which besides being fairly com-putationally expensive, also does not take into account the corre-lation between the labels of adjacent pixels. The wavelet-based classification method is based on the progressive classification [35] framework and the core idea is as follows: It uses generic (paramet-ric or non-parametric) classifiers on a low-resolution representation of the data obtained using discrete wavelet transform. The wavelet transformation produce a multiresolution pyramid representation of the data. In this representation, at each level each coefficient corre-sponds to a k  X  k pixel block in the original image. At each step of the classification, the algorithm decides whether each coefficient corresponds to a homogeneous block of pixels and assigns the same class label to the whole block or to re-examine the data at a higher resolution level. And the same process is repeated iteratively. The wavelet-based classification method achieves a significant speedup over traditional pixel-wise classification methods. For images with pixel values that are highly correlated, the method will give more accurate results than the corresponding non-progressive classifier because DWT produces a weight average of the values for a block and the algorithm tend to assume more uniformity in the im-age than may appear when we look at individual pixels. Castelli et al. [35] presented the experimental results illustrating the per-formance of the method on large satellite images and Castelli et al. [33] also presented theoretical analysis on the method. Blume and Ballard [23] described a method for classifying image pixels based on learning vector quantization and localized Haar wavelet transform features. A Haar wavelet transform is utilized to generate a feature vector per image pixel and this provides in-formation about the local brightness and color as well as about the texture of the surrounding area. Hand-labeled images are used to generated the a codebook using the optimal learning rate learning vector quantization algorithm. Experiments show that for small number of classes, the pixel classification is as high as Scheunders et al. [142] elaborated texture analysis based on wavelet transformation. The multiresolution and orthogonal de-scriptions could play an important role in texture classification and image segmentation. Useful gray-level and color texture features can be extracted from the discrete wavelet transform and useful rotation-invariant features were found in continuous transforms. Sheikholeslami [146] presented a content-based retrieval approach that utilizes the texture features of geographical images. Vari-ous texture features are extracted using wavelet transforms. Us-ing wavelet-based multi-resolution decomposition, two different sets of features are formulated for clustering. For each feature set, different distance measurement techniques are designed and experimented for clustering images in database. Experimental re-sults demonstrate that the retrieval efficiency and effectiveness im-prove when the clustering approach is used. Mojsilovic et al. [120] also proposed a wavelet-based approach for classification of texture samples with small dimensions. The idea is first to decompose the given image with a filter bank derived from an orthonormal wavelet basis and to form an image approximation with nigher resolution. Texture energy measures calculated at each output of the filter bank as well as energies if synthesized images are used as texture fea-tures for a classification procedure based on modified statistical t-test.The new algorithm has advantages in classification of small and noisy samples and it represents a step toward structural analysis of weak textures. More usage on texture classification using wavelets can be found in [100; 40]. Tzanetakis et al. [157] used wavelet to extract a feature set for representing music surface and rhythm information to build automatic genre classification algorithms. Regression uses existing values to forecast what other values will be and it is one of the fundamental tasks of data mining. Consider the standard univariate nonparametric regression setting: g ( t i ) + i ,i = 1 ,...,n where i are independent N (0 , X  2 ) dom variables. The goal is to recover the underlying function from the noisy data y i , without assuming any particular parametric structure for g . The basic approach of using wavelets for nonpara-metric regression is to consider the unknown function g expanded as a generalized wavelet series and then to estimate the wavelet co-efficients from the data. Hence the original nonparametric problem is thus transformed to a parametric one [1]. Note that the denoise problem we discussed in Section 5.1 can be regarded as a subtask of the regression problem since the estimation of the underlying function involves the noise removal from the observed data. For linear regression, we can express where c 0 = &lt; g, X  &gt;,w jk = &lt; g, X  jk &gt; . If we assume to a class of functions with certain regularity, then the correspond-ing norm of the sequence of w jk is finite and w jk  X  X  decay to zero. So for some M and a corresponding truncated wavelet estimator is [1] Thus the original nonparametric problem reduces to linear regres-sion and the sample estimates of the coefficients are given by: The performance of the truncated wavelet estimator clearly de-pends on an appropriate choice of M . Various methods such as Akaike X  X  Information Criterion [8] and cross-validation can be used for choosing M . Antoniadis [11] suggested linear shrunk wavelet estimators where the  X  w jk are linearly shrunk by appropriately cho-sen level-dependent factors instead of truncation. We should point out that: the linear regression approach here is similar to the di-mensionality reduction by keeping the first several wavelet coeffi-cients discussed in section 5.3. There is an implicit strong assump-tion underlying the approach. That is, all wavelet coefficients in the first M coarsest levels are significant while all wavelet coef-ficients at a higher resolution levels are negligible. Such a strong assumption clearly would not hold for many functions. Donoho and Johnstone [60] showed that no linear estimator will be optimal SIGKDD Explorations. Volume 4, Issue 2 -page 58 in minimax sense for estimating inhomogeneous functions with lo-cal singularities. More discussion on linear regression can be found in [10]. Donoho et al. [58; 61; 60; 59] proposed a nonlinear wavelet esti-mator of g based on reconstruction from a more judicious selec-tion of the empirical wavelet coefficients. The vanishing moments property of wavelets makes it reasonable to assume that essentially only a few  X  X arge X   X  w jk contain information about the underlying function g , while  X  X mall X   X  w jk can be attributed to noise. If we can decide which are the  X  X ignificant X  large wavelet coefficients, then we can retain them and set all the others equal to zero, so obtaining an approximate wavelet representation of underlying function The key concept here is thresholding. Thresholding allows the data itself to decide which wavelet coefficients are significant. Clearly an appropriate choice of the threshold value  X  is fundamental to the effectiveness of the estimation procedure. Too large threshold might  X  X ut off X  important parts of the true function underlying the data while too small a threshold retains noise in the selective recon-struction. As described in Section 5.1, there are three commonly used thresholding functions. It has been shown that hard thresh-olding results in larger variance in the function estimate while soft thresholding has large bias. To comprise the trade-off between bias and variance, Bruce and Gao [27] suggested a firm thresholding that combines the hard and soft thresholding.
 In the rest of the section, we discuss more literatures on the choice of thresholding for nonlinear regression. Donoho and John-stone [58] proposed the universal threshold  X  un =  X   X  2 log n/ where  X  is the noise level and can be estimated from the data. They also showed that for both hard and soft thresholding the re-sulting nonlinear wavelet estimator is asymptotically near-minimax inhomogeneous functions. They [59] also proposed an adaptive SureShrink thresholding rule based on minimizing Stein X  X  unbiased risk estimate. Papers [123; 86] investigated using cross-validation approaches for the choice of threshold. Some researchers [2; 128] developed the approaches of thresholding by hypothesis testing the coefficients for a significant deviation from zero. Donoho et al. [61] proposed level-dependent thresholding where different thresholds are used on different levels. Some researchers [30; 76] proposed block thresholding where coefficients are thresholded in blocks rather than individually. Both modifications imply better asymptotic properties of the resulting wavelet estimators. Various Bayesian approaches for thresholding and nonlinear shrinkage has also been proposed [161; 4; 3; 159]. In the Bayesian approach, a prior distribution is imposed on wavelet coefficient and then the function is estimated by applying a suitable Bayesian rule to the resulting posterior distribution of the wavelet coefficients. Garo-falakis and Gibbons [70] introduced a probabilistic thresholding scheme that deterministically retains the most important coeffi-cients while randomly rounding the other coefficients either up to a larger value or down to zero. The randomized rounding enables unbiased and error-guaranteed Reconstruction of individual data values. Interested readers may refer to [162] for comprehensive reviews of Bayesian approaches for thresholding. More discussion on nonlinear regression can be found in [10]. Over the years, data set sizes have grown rapidly with the advances in technology, the ever-increasing computing power and computer storage capacity, the permeation of Internet into daily life and the increasingly automated business, manufacturing and scientific pro-cesses. Moreover, many of these data sets are, in nature, geograph-ically distributed across multiple sites. To mine such large and dis-tributed data sets, it is important to investigate efficient distributed algorithms to reduce the communication overhead, central storage requirements, and computation times. With the high scalability of the distributed systems and the easy partition and distribution of a centralized dataset, distribute clustering algorithms can also bring the resources of multiple machines to bear on a given problem as the data size scale-up. In a distributed environment, data sites may be homogeneous , i.e., different sites containing data for exactly the same set of features, or heterogeneous , i.e., different sites storing data for different set of features, possibly with some common fea-tures among sites. The orthogonal property of wavelet basis could play an important role in distributed data mining since the orthogo-nality guarantees correct and independent local analysis that can be used as a building-block for a global model. In addition, the com-pact support property of wavelets could be used to design parallel algorithms since the compact support guarantees the localization of wavelet and processing a region of data with wavelet does not affect the the data out of this region.
 Kargupta et al.[92; 81] introduced the idea of performing distributed data analysis using wavelet-based Collective Data Mining (CDM) from heterogeneous sites. The main steps for the approach can be summarized as follows: The foundation of CDM is based on the fact that any function can be represented in a distributed fashion using an appropriate basis. If we use wavelet basis, The orthogonality guarantees correct and independent local analysis that can be used as a building-block for a global model. Hershberger et al. [81] presented applications of wavelet-based CDM methodology to multivariate regression and linear discriminant analysis. Experiments have shown that the re-sults produced by CDM are comparable to those obtained with cen-tralized methods and the communication cost was shown to be di-rectly proportional to the number of terms in the function and inde-pendent of the sample size. The problem of similarity search in data mining is: given a pattern of interest, try to find similar patterns in the data set based on some similarity measures. This task is most com-monly used for time series, image and text data sets. For time series, for example, given the Xerox stock prices over last 7 days and wish to find the stocks that have similar behaviors. For image, given a sample image and wish to find similar im-ages in a collection of image database. For text, given some keywords, wish to find relevant documents. More formally, A dataset is a set denoted DB = { X 1 ,X 2 ,...,X i ,...,X N X i = [ x i 0 ,x i 1 ,...,x i n ] and a given pattern is a sequence of data points Q = [ q 0 ,q 1 ,...,q n ] . Given a pattern Q , the result set SIGKDD Explorations. Volume 4, Issue 2 -page 59 from the data set is R = { X i { i ,i 2 ,  X  X  X  ,i m }  X  { 1 ,  X  X  X  ,N } , such that D ( X i j ,Q ) &lt; d use Euclidean distance between X and Y as the distance function D ( X,Y ) , then, which is the aggregation of the point to point distance of two pat-terns. Wavelets could be applied into similarity search in several different ways. First, wavelets could transform the original data into the wavelet domain as described in Section 5.2 and we may also only keep selective wavelet coefficients to achieve dimension-ality reduction as in Section 5.3. The similarity search are then conducted in the transformed domain and could be more efficient. Although the idea here is similar to that reviewed in Section 5.2 and Section 5.3: both involves transforming the original data into wavelet domain and may also selecting some wavelet coefficients. However, it should be noted that here for the data set: to project the n -dimensional space into a k -dimensional space using wavelets, the same k -wavelet coefficients should be stored for objects in the data set. Obviously, this is not optimal for all objects. To find the optimal coefficients for the data set, we need to compute the aver-age energy for each coefficient. Second, wavelet transforms could be used to extract compact feature vectors and define new similarity measure to facilitate search. Third, wavelet transforms are able to support similarity search at different scales. The similarity measure could then be defined in an adaptive and interactive way. Wavelets have been extensively used in similarity search in time series [83; 172; 131; 132]. Excellent overview of wavelet meth-ods in time series analysis can be found in [44; 121; 122]. Chan and Fu [131] proposed efficient time series matching strategy by wavelets. Haar transform wavelet transform is first applied and the first few coefficients of the transformed sequences are indexed in an R-Tree for similarity search. The method provides efficient for range and nearest neighborhood queries. Huhtala et al. [83] also used wavelets to extract features for mining similarities in aligned time series. Wu et al.[172] presented a comprehensive compari-son between DFT and DWT in time series matching. The exper-imental results show that although DWT does not reduce relative matching error and does not increase query precision in similar-ity search, DWT based techniques have several advantage such as DWT has multi-resolution property and DWT has complexity of O ( N ) while DFT has complexity of O ( N log N ) . Wavelet trans-form gives time-frequency localization of the signal and hence most of the energy of the signal can be represented by only a few DWT coefficients. Struzik and Siebes [153; 154] presented new similar-ity measures based on the special presentations derived from Haar wavelet transform. Instead of keeping selective wavelet coeffi-cients, the special representations keep only the sign of the wavelet coefficients (sign representation) or keep the difference of the log-arithms (DOL) of the values of the wavelet coefficient at highest scale and the working scale (DOL representation). The special rep-resentations are able to give step-wise comparisons of correlations and it was shown that the similarity measure based on such repre-sentations closely corresponds to the subjective feeling of similarity between time series.
 Wavelets also have widespread applications in content-based sim-ilarity search in image/audio databases. Jacobs et al.[85] pre-sented a method of using image querying metric for fast and ef-ficient content-based image querying. The image querying metric is computed on the wavelet signatures which are obtained by trun-cated and quantized wavelet decomposition. In essential, the image querying metric compares how many wavelet significant wavelet coefficients the query has in common with the potential targets. Natsev et al. [125] proposed WALRUS (WAveLet-based Retrieval of User-specified Scenes) algorithm for similarity retrieval in im-age diastases. WALRUS first uses dynamic programming to com-pute wavelet signatures for sliding windows of varying size, then clusters the signatures in wavelet space and finally the similarity measure between a pair of images is calculated to be the fraction of the area the two images covered by matching signatures. Ardizzoni et al. [13] described Windsurf (Wavelet-Based Indexing of Images Using Region Fragmentation), a new approach for image retrieval. Windsurf uses Haar wavelet transform to extract color and texture features and applies clustering techniques to partition the image into regions. Similarity is then computed as the Bhattcharyya met-ric [31] between matching regions. Brambilla [24] defined an ef-fective strategy which exploits multi-resolution wavelet transform to effectively describe image content and is capable of interac-tive learning of the similarity measure. Wang et al. [167; 84] described WBIIS (Wavelet-Based Image Indexing and Searching), a new image indexing and retrieval algorithm with partial sketch image searching capability for large image databases. WBIIS ap-plies Daubechies-8 wavelets for each color component and low fre-quency wavelet coefficients and their variance are stored as fea-ture vectors. Wang, Wiederhold and Firschein [166] described WIPETM (Wavelet Image Pornography Elimination) for image re-trieval. WIPETM uses Daubechies-3 wavelets, normalized central moments and color histograms to provide feature vector for similar-ity matching. Subramanya and Youssef [155] presented a scalable content-based image indexing and retrieval system based on vector coefficients of color images where highly decorrelated wavelet co-efficient planes are used to acquire a search efficient feature space. Mandal et al. [112] proposed fast wavelet histogram techniques for image indexing. There are also lots of applications of wavelets in audio/music information processing such as [103; 56; 101; 156]. In fact, IEEE Transactions on Signal Processing has two special issues on wavelets, in Dec. 1993 and Jan. 1998 respectively. Interested readers could refer to these issues for more details on wavelets for indexing and retrieval in signal processing. Query processing is a general task in data mining and similarity search discussed in Section 6.5 is one of the specific form of query processing. In this section, we will describe wavelet applications in approximate query processing which is another area within query processing. Approximate query processing has recently emerged as a viable solution for large-scale decision support. Due to the exploratory nature of many decision support applications, there are a number of scenarios where an exact answer may not be required and a user may in fact prefer a fast approximate answer. Wavelet-based techniques can be applied as a data reduction mechanism to obtain wavelet synopses of the data on which the approximate query could then operate. The wavelet synopses are compact sets of wavelet coefficients obtained by the wavelet decomposition. Note that some of wavelet methods described here might overlap with those described in Section 5.3. The wavelet synopses reduce large amount of data to compact sets and hence could provide fast and reasonably approximate answers to queries.
 Matias, Vitter and Wang [113; 114] presented a wavelet-based technique to build histograms on the underlying data distribu-tions for selectivity estimation and Vitter et al. [164; 88] also pro-posed wavelet-based techniques for the approximation of range-sum queries over OLAP data cubes. Generally, the central idea is to apply multidimensional wavelet decomposition on the input data SIGKDD Explorations. Volume 4, Issue 2 -page 60 collection (attribute columns or OLAP cube) to obtain a compact data synopsis by keeping a selective small collection of wavelet coefficients. Experiments in [113] showed that wavelet-based his-tograms improve the accuracy substantially over random sampling and results from [164] clearly demonstrated that wavelets can be very effective in handling aggregates over high-dimensional OLAP cubes while avoiding the high construction costs and storage over-heads. Chakrabarti et al. [36] extended previous work on wavelet techniques in approximate query answering by demonstrating that wavelets could be used as a generic and effective tool for decision support applications. The generic approach consists of three steps: the wavelet-coefficient synopses are first computed and then using novel query processing algorithms SQL operators such as select, project and join can be executed entirely in the wavelet-coefficient domain. Finally the results is mapped from the wavelet domain to relational tuples( Rendering ). Experimental results verify the effec-tiveness and efficiency. Gilbert et al. [71] presented techniques for computing small space representations of massive data streams by keeping a small number of wavelet coefficients and using the repre-sentations for approximate aggregate queries. Garofalakis and Gib-bons [70] introduced probabilistic wavelet synopses that provably enabled unbiased data reconstruction with guarantees on the ac-curacy of individual approximate answers. The probabilistic tech-nique is based on probabilistic thresholding scheme to assign each coefficient a probability of being retained instead of deterministic thresholding. Visualization is one of the description tasks (exploratory data anal-ysis) of data mining and it allows the user to gain an understanding of the data. Visualization works because it exploits the broader in-formation bandwidth of graphics as opposed to text or numbers. However, for large dataset it is often not possible to even perform simple visualization task. The multiscale wavelet transform facil-itates progressive access to data with the viewing of the most im-portant features first.
 Miller et al. [118] presented a novel approach to visualize and ex-plore unstructured text based on wavelet. The underlying tech-nology applies wavelet transforms to a custom digital signal con-structed from words within a document. The resultant multireso-lution wavelet energy is used to analyze the characteristics of the narrative flow in the frequency domain. Wong and Bergeron [170] discussed with the authenticity issues of the data decomposition, particularly for data visualization. A total of six datasets are used to clarify the approximation characteristics of compactly supported orthogonal wavelets. It also presents an error tracking mechanism, which uses the available wavelet resources to measure the quality of the wavelet approximations. Roerdink and Westenberg [137] considered multiresolution visualization of large volume data sets based on wavelets. Starting from a wavelet decomposition of the data, a low resolution image is computed; this approximation can be successively refined. Du and Moorhead [62] presented a tech-nique which used a wavelet transform and MPI(Message Passing Interface) to realize a distributed visualization system. The wavelet transform has proved to be a useful tool in data decomposition and progressive transmission. Neural networks are of particular interest because they offer a means of efficiently modeling large and complex problems and they can be applied to many data mining tasks such as classification, clustering and regression. Roughly speaking, a neural network is a set of connected input/hidden/output units where each connection has an associated weight and each unit has an associated activated function. Usually neural network methods contain a learning phase and a working phase. A learning phase is to adjust the weights and the structures of the network based on the training samples while the working phase is to execute various tasks on new instances. For more details on neural network, please refer to [80; 64; 90]. The idea of combining neural networks with multiscale wavelet de-composition has been proposed by a number of authors [42; 98; 43; 54; 97; 49; 95; 96; 171]. These approaches either use wavelets as the neuron X  X  activation functions [98; 38](usually call these as wavelet neural network), or in a pre-processing phasing by the ex-traction of features from time series data [42; 54; 171]. The proper-ties of wavelet transforms emerging from a multi-scale decomposi-tion of signals allow the study of both stationary and non-stationary signals. On the other hand the neural network performs a non-linear analysis as well linear dependencies due to different possi-ble structures and activation functions. Hence combining wavelets and neural network would give us more power on data analysis. A wavelet neural network, using the wavelets as activation func-tions and combining the mathematically rigorous, multi-resolution character of wavelets with the adaptive learning of artificial neu-ral networks, has the capability of approximating any continuous nonlinear mapping to any high resolution. Learning with wavelet neural network is efficient, and is explicitly based on the local or global error of approximation. A simple wavelet neural network displays a much higher level of generalization and shorter com-puting time as compared to three-layer feed forward neural net-work [173]. Roverso [139] proposed an approach for multivariate temporal classification by combining wavelet and recurrent neural network. Kreinovich et al. [99] showed that wavelet neural net-works are asymptotically optimal approximators for functions of one variable in the sense that it require to store the smallest possi-ble number of bits that is necessary to reconstruct a function with a given precision. Bakshi et al. [18] described the advantages of wavelet neural network learning over other artificial neural learning techniques and discussed the relationship between wavelet neural network and other rule-extraction techniques such as decision trees. It also shows that wavelets may provide a unifying framework for various supervised learning techniques.
 WSOM is a feedforward neural network that estimates optimized wavelet bases for the discrete wavelet transform on the basis of the distribution of the input data [32]. Sheng and Chou [105] reported the application of using wavelet transform and self-organizing map to mine air pollutant data. A widely used technique for data mining is based on diagonalizing the correlation tensor of the data-set, keeping a small number of coherent structures (eigenvectors) based on principal components analysis (PCA) [19]. This approach tends to be global in char-acter. Principal component analysis (PCA) has been adopted for many different tasks. Wavelet analysis and PCA can be combined to obtain proper accounting of global contributions to signal energy without loss of information on key local features. In addition, the multi-resolution property of wavelets could help to find the princi-pal component at multiple scales.
 Bakshi [16] used multiscale PCA(MSPCA) for process monitor-ing. Multiscale PCA combines the ability of PAC to decorrelate the variables by extracting a linear relationship with that of wavelet analysis to extract deterministic features and approximately decor-relate autocorrelated measurements. MSPCA computes the PCA of the wavelet coefficients at each scale, followed by combining SIGKDD Explorations. Volume 4, Issue 2 -page 61 the results at relevant scales. Due to its multiscale nature, MSPCA is approximate for modeling of data containing contributions from events whose behavior changes over time and frequency. Pro-cess monitoring by MSPCA involves combining only those scales where significant events are detected, and is equivalent to adap-tively filtering the scores and residuals and adjusting limits for easiest detection of deterministic changes in the measurements. Bakshi [17] presented an overview of multiscale data analysis and empirical modeling methods based on wavelet analysis. Feng et al. [65] proposed an approach of applying Principal Component Analysis (PCA) on the wavelet subband as described in Section 5.2. Wavelet analysis could also be combined with Independent Com-ponent Analysis (ICA). The goal of ICA is to recover independent sources given only sensor observations that are unknown linear mixtures of the unobserved independent source signals. Briefly, ICA attempts to estimate the coefficients of an unknown mixture of n signal sources under the hypotheses that the sources are sta-tistically independent, the medium of transmission is deterministic, and crucially, the mixture coefficients are constant with respect to time. One then solves for the sources from the observations by inverting the mixture matrix. In contrast to correlation-based trans-formations such as Principal Component Analysis (PCA), ICA not only decorrelates the signals (2nd-order statistics) but also reduces higher-order statistical dependencies, attempting to make the sig-nals as independent as possible. In other words, ICA is a way of finding a linear non-orthogonal co-ordinate system in any multi-variate data. The directions of the axes of this co-ordinate system are determined by both the second and higher order statistics of the original data. The goal is to perform a linear transform which makes the resulting variables as statistically independent from each other as possible. More details about the ICA algorithms can be found in [21; 48; 20; 89]. A fundamental weakness of existing ICA algorithms, namely that the mixture matrix is assumed to be essentially constant. This is unsatisfactory when moving sources are involved. Wavelet transforms can be utilized to this problem by using time-frequency characteristics of the mixture matrix in the source identification. Moreover, ICA algorithms could also make use of the multiscale representation of wavelet transforms. There are some other wavelet applications that are related to data mining.
 Web Log Mining : Wavelets offer powerful techniques for mathe-matically representing web requests at multiple time scales and a compact and concise representation of the requests using wavelet coefficients. Zhai et al. [175] proposed to use wavelet-based tech-niques to analyze the workload collected from busy web servers. It aims at finding the temporal characteristics of the web server web-log which contains workload information and predicting the trend it evolves.
 Traffic Monitoring : The wavelet transform significantly reduces the temporal dependence and simple models which are insuffi-cient in the time domain may be quite accurate in the wavelet do-main. Hence wavelets provide an efficient way to modeling net-work traffic. Riedi et al. [136] developed a new multiscale mod-eling framework for characterizing positive-valued data with long-range-dependent correlations. Using the Haar wavelet transform and a special multiplicative structure on the wavelet and scaling coefficients to ensure positive results. Ma and Ji [106; 107; 108] presented the work on modeling on modeling temporal correlation (the second-order statistics) of heterogeneous traffic, and modeling non-Gaussian (high-order statistics) and periodic traffic in wavelet domain.
 Change Detection : The good time-frequency localization of wavelets provides a natural motivation for their use in change point detection problems. The main goal of change detection is estima-tion of the number, locations and sizes of function X  X  abrupt changes such as sharp spikes or jumps. Change-point models are used in a wide set of practical problems in quality control, medicine, eco-nomics and physical sciences [1]. The general idea of using wavelet for detecting abrupt changes is based on the connection between the function X  X  local regularity properties at a certain point and the rate of decay of the wavelet coefficients located near this point across increasing resolution level [111]. Local regularities are identified by unusual behavior in the wavelet coefficients at high-resolution levels at the corresponding location [168]. Bailey et al. [15] used wavelet to detect signal in underwater sound. Donoho et al. [57] discussed the application of wavelets for density estimation. This paper provides an application-oriented overview of the math-ematical foundations of wavelet theory and gives a comprehensive survey of wavelet applications in data mining The object of this paper is to increase familiarity with basic wavelet applications in data mining and to provide reference sources and examples where the wavelets may be usefully applied to researchers working in data analysis. Wavelet techniques have a lot of advantages and there al-ready exists numerous successful applications in data mining. It goes without saying that wavelet approaches will be of growing importance in data mining.
 It should also be mentioned that most of current works on wavelet applications in data mining are based orthonormal wavelet basis. However, we argue that orthonormal basis may not be the best rep-resentation for noisy data even though the vanishing moments can help them achieve denoising and dimensionality reduction purpose. Intuitively, orthogonality is the most economical representation. In other words, in each direction, it contains equally important infor-mation. Therefore, it is usually likely that thresholding wavelet coefficients remove useful information when they try to remove the noise or redundant information (noise can also be regarded as one kind of redundant information). To represent redundant infor-mation, it might be good to use redundant wavelet representation  X  wavelet frames. Except orthogonality, wavelet frames preserve all other properties that an orthonormal wavelet basis owns, such as vanishing moment, compact support, multiresolution. The re-dundancy of a wavelet frame means that the frame functions are not independent anymore. For example, vectors [0 , 1] is an orthonormal basis of a plane R 2 , while vectors [1 / 2 , 1 / 2] [  X  1 / 2 , 1 / 2] , and [0 ,  X  1] are not independent, and consist a frame specific directions to record the noise. Our work will be the estab-lishment of criteria to recognize the direction of noise or redundant information.
 Wavelets could also potentially enable many other new researches and applications such as conventional database compression, mul-tiresolution data analysis and fast approximate data mining etc. Fi-nally we eagerly await many future developments and applications of wavelet approaches in data mining. [1] F. Abramovich, T. Bailey, and T. Sapatinas. Wavelet analysis [2] F. Abramovich and Y. Benjamini. Thresholding of wavelet SIGKDD Explorations. Volume 4, Issue 2 -page 62 [3] F. Abramovich and T. Sapatinas. Bayesian approach to [4] F. Abramovich, T. Sapatinas, and B. Silverman. Wavelet [5] C. C. Aggarwal, J. L. Wolf, P. S. Yu, C. Procopiuc, and J. S. [6] C. C. Aggarwal and P. S. Yu. Outlier detection for high di-[7] R. Agrawal, J. Gehrke, D. Gunopulos, and P. Raghavan. Au-[8] H. Akaike. Information theory and an extension of the max-[9] A. Aldroubi and M. Unser, editors. Wavelets in Medicine and [10] A. Antoniadis. Wavelets in statistics: a review. J. It. Statist. [11] A. Antoniadis, G. Gr  X  egoire, and I. W. McKeague. Wavelet [12] A. Antoniadis and G. Oppenhiem, editors. Wavelets and [13] S. Ardizzoni, I. Bartolini, and M. Patella. Windsurf: Region-[14] A. Arning, R. Agrawal, and P. Raghavan. A linear method [15] T. C. Bailey, T. Sapatinas, K. J. Powell, and W. J. [16] B. Bakshi. Multiscale pca with application to multivariate [17] B. Bakshi. Multiscale analysis and modeling using wavelets. [18] B. R. Bakshi, A. Koulouris, and G. Stephanopoulos. Learn-[19] D. Ballard. An introduction to natural computation . MIT [20] A. Bell and T. Sejnowski. Fast blind separation based on in-[21] A. J. Bell and T. J. Sejnowski. An information-maximization [22] M. Berger and I. Rigoutsos. An algorithm for point cluster-[23] M. Blume and D. Ballard. Image annotation based on learn-[24] C. Brambilla, A. D. Ventura, I. Gagliardi, and R. Schet-[25] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander. LOF: [26] M. Brito, E. Chavez, A. Quiroz, and J. Yukich. Connectivity [27] A. Bruce and H.-Y. Gao. Waveshrink with firm shrinkage. [28] I. Bruha and A. F. Famili. Postprocessing in machine learn-[29] R. W. Buccigrossi and E. P. Simoncelli. Image compression [30] T. Cai. Adaptive wavelet estimation: a block thresholding [31] J. P. Campbell. Speaker recognition: A tutorial. In Proceed-[32] G. Carpenter. Wsom: building adaptive wavelets with self-[33] V. Castelli and I. Kontoyiannis. Wavelet-based classification: [34] V. Castelli and I. Kontoyiannis. An efficient recursive parti-[35] V. Castelli, C. Li, J. Turek, and I. Kontoyiannis. Progressive SIGKDD Explorations. Volume 4, Issue 2 -page 63 [36] K. Chakrabarti, M. Garofalakis, R. Rastogi, and K. Shim. [37] A. Chambolle, R. DeVore, N. Lee, and B. Lucier. Nonlinear [38] P.-R. Chang and B.-F. Yeh. Nonlinear communication chan-[39] S. Chang, B. Yu, and M. Vetterli. Spatially adaptive wavelet [40] T. Chang and C. Kuo. Texture analysis and classification [41] P. Cheeseman, J. Kelly, and M. Self. AutoClass: A bayesian [42] B. Chen, X.Z.Wang, S. Yang, and C. McGreavy. Applica-[43] B. Chen, X.Z.Wang, S. Yang, and C. McGreavy. Applica-[44] C. Chiann and P. A. Morettin. A wavelet analysis for time se-[45] C. Chrysafis and A. Ortega. Line based, reduced memory, [46] C. K. Chui. An Introduction to Wavelets . Academic Press, [47] C. K. Chui and J. Lian. A study of orthonormal multi-[48] P. Comon. Independent component analysis -a new concept? [49] P. Cristea, R. Tuduce, and A. Cristea. Time series prediction [50] R. F. Cromp and W. J. Campbell. Data mining of multidi-[51] I. Daubechies. Orthonormal bases of compactly support [52] I. Daubechies. Ten Lectures on Wavelets . Capital City Press, [53] I. Daubechies, B. Han, A. Ron, and Z. Shen. Framelets: [54] C. J. Deschenes and J. P. Noonan. A fuzzy kohonen network [55] R. A. DeVore, B. Jawerth, and B. J. Lucier. image compres-[56] P. Q. Dinh, C. Dorai, and S. Venkatesh. Video genre cate-[57] D. Donoho, I. Johnstone, G. Kerkyacharian, and D. Picard. [58] D. L. Donoho and I. M. Johnstone. Ideal spatial adaptation [59] D. L. Donoho and I. M. Johnstone. Adapting to unknown [60] D. L. Donoho and I. M. Johnstone. Minimax estimation [61] D. L. Donoho, I. M. Johnstone, G. Kerkyacharian, and D. Pi-[62] X. S. Du and R. J. Moorhead. Multiresolutional visualization [63] G. Fan and X. Xia. Wavelet-based statistical image process-[64] L. Fausett. Fundamentals of Neural Networks . Prentice Hall, [65] G. C. Feng, P. C. Yuen, and D. Q. Dai. Human face recogni-[66] D. H. Fisher. Iterative optimization and simplification of hi-[67] P. Flandrin. Wavelet analysis and synthesis of fractional [68] V. Ganti, J. Gehrke, and R. Ramakrishnan. CACTUS -clus-[69] H.-Y. Gao. Threshold selection in WaveShrink, 1997. theory [70] M. Garofalakis and P. B. Gibbons. Wavelet synopses with SIGKDD Explorations. Volume 4, Issue 2 -page 64 [71] A. C. Gilbert, Y. Kotidis, S. Muthukrishnan, and M. Strauss. [72] S. Guha, R. Rastogi, and K. Shim. CURE: an efficient clus-[73] S. Guha, R. Rastogi, and K. Shim. ROCK: A robust cluster-[74] D. Gunopulos. Tutorial slides: Dimensionality reduction [75] A. Haar. Zur theorie der orthogonalen funktionensysteme. [76] P. Hall, G. Kerkyacharian, and D. Picard. Block threshold [77] J. Han and M. Kamber. Data Mining: Concepts and Tech-[78] J. Han and M. Kamber. Data Mining: Concepts and Tech-[79] D. Hand, H. Mannila, and P. Smyth. Principles of Data Min-[80] S. Haykin. Neural Networks . Prentice Hall, 1999. [81] D. E. Hershberger and H. Kargupta. Distributed multivariate [82] P. Hornby, F. Boschetti, and F. Horowitz. Analysis of poten-[83] Y. Huhtala, J. Karkkainen, and H. Toivonen. Mining for sim-[84] O. F. J. Z. Wang, G. Wiederhold and S. X. Wei. Wavelet-[85] C. E. Jacobs, A. Finkelstein, and D. H. Salesin. Fast mul-[86] M. Jansen, M. Malfait, and A. Bultheel. Generalized cross [87] W. Jin, A. K. H. Tung, and J. Han. Mining top-n local out-[88] J.S.Vitter, M. Wang, and B. Iyer. Data cube approximation [89] C. Jutten, J. Herault, P. Comon, and E. Sorouchiary. Blind [90] G. K. An Introduction to Neural Networks . UCL Press, 1997. [91] L. Kaplan and C. Kuo. Fractal estimation from noisy [92] H. Kargupta and B. Park. The collective data mining: A tech-[93] D. Keim and M. Heczko. Wavelets and their applications in [94] E. M. Knorr and R. T. Ng. Finding intensional knowledge of [95] K. Kobayashi and T. Torioka. A wavelet neural network for [96] K. Kobayashi and T. Torioka. Designing wavelet networks [97] P. Kostka, E. Tkacz, Z. Nawrat, and Z. Malota. An applica-[98] A. Koulouris, B. R. Bakshi, and G. Stephanopoulos. Empir-[99] V. Kreinovich, O. Sirisaengtaksin, and S. Cabrera. Wavelet [100] A. Laine and J. Fan. texture classification by wavelet packet [101] T. Lambrou, P. Kudumakis, R. Speller, M. Sandler, and [102] P. C. Lemari  X  e and Y. Meyer. Ondelettes et bases hilberti-[103] G. Li and A. A. Khokhar. Content-based indexing and re-[104] Q. Li, T. Li, and S. Zhu. Improving medical/biological SIGKDD Explorations. Volume 4, Issue 2 -page 65 [105] S.-T. Li and S.-W. Chou. Multi-resolution spatio-temporal [106] S. Ma and C. Ji. Modeling heterogeneous network traffic in [107] S. Ma and C. Ji. Modeling heterogeneous network traffic in [108] S. Ma and C. Ji. Modeling heterogeneous network traffic [109] S. Mallat. A theory for multiresolution signal decompo-[110] S. Mallat. A Wavelet Tour of Signal Processing . Academic [111] S. G. Mallat and W. L. Hwang. Singularity detection and [112] M. K. Mandal, T. Aboulnasr, and S. Panchanathan. Fast [113] Y. Matias, J. S. Vitter, and M. Wang. Wavelet-based his-[114] Y. Matias, J. S. Vitter, and M. Wang. Dynamic maintenance [115] P. Meerwald and A. Uhl. A survey of wavelet-domain wa-[116] Y. Meyer. Wavelets and Operatiors . Cambridge University [117] Y. Meyer. Wavlets X  X lgorithms and Application . SIAM, [118] N. E. Miller, P. C. Wong, M. Brewster, and H. Foote. TOPIC [119] T. M. Mitchell. Machine Learning . The McGraw-Hill Com-[120] A. Mojsilovic and M. v. Popovic. Wavelet image extension [121] P. Morettin. Wavelets in statistics. (3):211 X 272, 1997. [122] P. A. Morettin. From fourier to wavelet analysis of time se-[123] G. P. Nason. Wavelet shrinkage by cross-validation. Journal [124] G. P. Nason and R. von Sachs. Wavelets in time series analy-[125] A. Natsev, R. Rastogi, and K. Shim. Walrus:a similarity [126] R. T. Ng and J. Han. Efficient and effective clustering meth-[127] R. Ogden. Essential Wavelets for Statistical Application and [128] R. T. Ogden and E. Parzen. Data dependent wavelet thresh-[129] D. Percival and A. T. Walden. Wavelet Methods for Time Se-[130] R. Polikar. The wavelet tutorial. Internet Re-[131] K. pong Chan and A. W.-C. Fu. Efficient time series match-[132] I. Popivanov and R. J. Miller. Similarity search over time [133] L. Prasad, S. S. Iyengar, and S. S. Ayengar. Wavelet Analysis [134] D. Pyle. Data Preparation for Data Mining . Morgan Kauf-[135] S. Ramaswamy, R. Rastogi, and K. Shim. Efficient algo-[136] R. H. Riedi, M. S. Crouse, V. J. Ribeiro, and R. G. Bara-[137] J. B. T. M. Roerdink and M. A. Westenberg. Wavelet-[138] A. Ron. Frames and stable bases for shift invariant subspaces SIGKDD Explorations. Volume 4, Issue 2 -page 66 [139] D. Roverso. Multivariate temporal classification by win-[140] S. Santini and A. Gupta. A data model for querying wavelet [141] S. Santini and A. Gupta. Wavelet data model for image [142] P. Scheunders, S. Livens, G. V. de Wouwer, P. Vautrot, [143] C. Shahabi, S. Chung, M. Safar, and G. Hajj. 2d TSA-[144] C. Shahabi, X. Tian, and W. Zhao. TSA-tree: A wavelet-[145] G. Sheikholeslami, S. Chatterjee, and A. Zhang. WaveClus-[146] G. Sheikholeslami, A. Zhang, and L. Bian. A multi-[147] P. Smyth. Probabilistic model-based clustering of multivari-[148] E. J. Stollnitz, T. D. DeRose, and D. H. Salesin. Wavelets [149] E. J. Stonllnitz, T. D. DeRose, and D. H. Salesin. Wavelets [150] G. Strang. Wavelets and dilation equations: A brief intro-[151] G. Strang. Wavelet transforms versus fourier transforms. [152] V. Strela. Denoising via block wiener filtering in wavelet do-[153] Z. R. Struzik and A. Siebes. The haar wavelet transform [154] Z. R. Struzik and A. Siebes. Measuring time series X  simi-[155] S. R. Subramanya and A. Youssef. Wavelet-based index-[156] G. Tzanetakis and P. Cook. Musical genre classification of [157] G. Tzanetakis, G. Essl, and P. Cook. Automatic musical [158] P. Vaidyanathan. Multirate digital filters, filter banks, [159] M. Vannucci and F. Corradi. Covariance structure of wavelet [160] R. Venkatesan, S. Koon, M. Jakubowski, and P. Moulin. Ro-[161] B. Vidakovic. Nonlinear wavelet shrinkage with Bayes rules [162] B. Vidakovic. Wavelet-based nonparametric bayes methods. [163] B. Vidakovic. Statistical Modeling by Wavelets . John Wiley [164] J. S. Vitter and M. Wang. Approximate computation of [165] J. S. Walker. A Primer on Wavelets for Their Scientific Ap-[166] J. Z. Wang, G. Wiederhold, and O. Firschein. System for [167] J. Z. Wang, G. Wiederhold, O. Firschein, and S. X. [168] Y. Wang. Jump and sharp cusp detection by wavelets. [169] P. Wojtaszczyj. A Mathematical Introduction to Wavelets . [170] P. C. Wong and R. D. Bergeron. Authenticity analysis of [171] B. J. Woodford and N. K. Kasabov. A wavelet-based neu-[172] Y.-L. Wu, D. Agrawal, and A. E. Abbadi. A comparison SIGKDD Explorations. Volume 4, Issue 2 -page 67 [173] T. Yamakawa, E. Uchino, and T. Samatsu. Wavelet neu-[174] R. Young. Wavelet Theory and its Application . Kluwer Aca-[175] A. Zhai, P. Huang, and T. J. yu Pan. A study on web-log [176] T. Zhang, R. Ramakrishnan, and M. Livny. BIRCH: an ef-[177] S. Zhu, T. Li, and M. Ogihara. CoFD: An algorithm for A function  X  ( x )  X  L 2 ( R ) is called a wavelet if it satisfies the following properties:  X  R  X  ( x ) dx = 1 ;  X  There is a finite interval [ a,b ] , such that  X  ( x ) = 0  X  There exists a function  X  ( x ) such that  X   X , X   X  = 0 (i.e.,  X  There exist a finite sequence of real numbers g 0 ,...,a  X  The dyadic dilation and translation of  X  , For a given function f  X  L 2 ( R ) one can find a N f N  X  V N approximates f up to predefined precision (in terms of L P in W i as a means of representing the parts of a function in that can not be represented in V i . So the decomposition process is as follows: given a f N in V N , we first decompose f parts where one part is in V N  X  1 and the other part is in At next step, we continue to decompose the part in V N  X  1 from previous step into two parts where one in V N  X  2 and the other in
W N  X  2 . This procedure is then repeated. This is exactly the wavelet decomposition.
 Recall that we have  X  ( x ) = P  X  P is related to the space V 0 and  X  ( x ) , the mother wavelet function, is related to W 0 . Define b k = (  X  1) k a 1  X  k and usually the sequences { a k } , { b k } are called Quadrature Mirror filters(QMF) in the ter-minology of signal processing. a k is a low-band or low-pass filter and b k is a hi-band or hi-pass filter. For a sequence f = { f represents the discrete signal to be decomposed and the operators H and G are defined by the following coordinativewise relations: ( Hf ) k = X (This can be represented as convolution: Hf = f ( k )  X  a ( n  X  k ) ,Gf = f ( k )  X  b ( n  X  k ) ). They represent filtering a signal through digital filters a ( k ) ,b ( k ) that corresponds to the mathematical oper-ation of convolution with the impulse response of the filters. The factor 2 k represents downsampling. The operators H and G respond to one-step in the wavelet decomposition. Thus the DWT transformation can be summarized as a single line: The authors would like to thank the anonymous reviewers for their invaluabale comments. This work was supported in part by NSF Grants EIA-0080124, DUE-9980943, and EIA-0205061 and by NIG Grants 5-P41-RR09283, RO1-AG18231, and P30-AG18254.
 Tao Li received his BS degree in Computer Science from Fuzhou University, China and MS degree in Computer Science from Chi-nese Academy of Science. He also got a MS degree in mathematics from Oklahoma State University. He is currently a doctoral candi-date in the computer science department at University of Rochester. His primary research interests are: data mining, machine learning and music information retrieval.
 Qi Li received his BS degree from Department of Mathematics, Zhongshan University, China in 1993, a Master degree from De-partment of Computer Science, University of Rochester in 2002. He is currently a PHD student in Department of Computer and In-formation Science, University of Delaware. His current interests are visual data mining and object recognition.
 Shenghuo Zhu obtained a bachelor degree in Computer Science at Zhejiang University in 1994, and a master degree in Computer Science at Tsinghua University in 1997. He has been pursuing his Ph.D degree in the Computer Science Department at University of Rochester since 1997. His primary research interests are machine learning, data mining and information retrieval.
 Mitsunori Ogihara received a PhD in Information Sciences at Tokyo Institute of Technology in 1993. He is currently Professor and Chair of the Department of Computer Science at the Univer-sity of Rochester. His primary research interests are data mining, computational complexity, and molecular computation.

