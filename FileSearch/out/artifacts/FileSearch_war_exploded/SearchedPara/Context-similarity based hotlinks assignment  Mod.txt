 1. Introduction The world wide web has been witnessed through the last few years as a platform for a plethora of aggregated information.
This information extends way beyond its original hypertextual form including, most commonly, streaming audio, video, dy-namic web content, etc. Notably, web is also manifesting its presence in ubiquitous computing devices, extending thereby its user community.

The diverse form of media and the continuously broadening user community of the web, are posing a handful of intrigu-ing challenges to web developers and website administrators, as far as the visit volume and the content presentation is con-cerned. The main target in the design and presentation task is to be able to analyze user visit patterns and accordingly modify website content in order to promote popular content. Recently, new algorithms are presented that utilize webpage categories to personalize search results [17,18] , intelligent information agents are developed in order to cope with the dif-ficulties associated with the information overload of the user [13], recommender systems are applicable to an even broader range of applications [1] and information and knowledge can be retrieved on favor of the user using data mining and the construction of automated lessons [16,20] .

In this paper we reexamine hotlink assignment , a popular site modification technique. The concept of hotlinks, first intro-duced by Perkowitz and Etzioni [21], presents the idea of non-destructive modification of the link structure of the web in order to minimize the total path length to popular pages in the site. Subsequent papers, see, e.g. [3,7] and the list of works modeled as directed acyclic graphs.
The problem can be defined as follows. Consider a directed rooted tree T  X  X  V ; E  X  representing a collection V of webpages weight representative of the frequency with which it is visited. By adding hotlinks, shortcuts from a node to one of its descendants, the goal is to minimize the expected number of steps needed to visit the leaf pages from the home page. Let T be the tree resulting from an assignment A of hotlinks. The expected number of steps from the root to find a webpage on a leaf is given by the formula where d A  X  i  X  is the distance of the leaf i from the root in T leaves of the original tree T that governs the distribution of frequency weights. We are interested in finding an assignment A which minimizes E  X  T A ; p .

This paper examines the concept of hotlink assignment from a new point of view. An algorithm is presented to ameliorate the accessibility of highly popular pages, by adding extra links to them from non-popular pages, which however are highly conceptually relevant. Moreover in the proposed algorithm the target is not the minimization of a metric characterizing the each webpage separately.

The novelty of our proposal is twofold and it appears: (i) in the integration of the similarity between two webpages into the hotlink assignment model, and (ii) in the employment of a more realistic graph model than the classical tree model uti-exploit the similarity of the webpages with each other and with the access points in the site. There are three ways that sim-ilarity is typically measured: content-based, link-based and usage-based. The one that interests us is the content-based, which is calculated based on words and word sequences appearing in pages. Hence, a node will be connected via a hotlink with another node depending not only on the frequency with which it is visited but on its similarity with the other node as well.

In order to facilitate our discussion we present the notation and conventions used in the remainder of this paper. We think of a website as a directed acyclic graph (DAG), G  X  V ; E  X  , where j V j X  n , i.e. the site has n pages. Let p probability of page i . All the p i 0 s form a distribution p . The entropy of p is defined as H  X  p  X  X  we let the outgoing links of each page i , form a distribution, i.e. similarity weights are further normalized at each node. The rest of the paper is organized as follows. In Section 2, we discuss previous and related work in more detail. Next in
Section 3, we provide the architectural overview of the algorithm that serves as a roadmap for the rest of the paper. In Sec-tion 4, we introduce a novel model and new metrical methods that can be used to measure the efficiency and evaluate the performance of the algorithm, based on four metrics that are discussed. In Section 5, we describe in detail the core steps of the proposed algorithm. In Section 6, we describe the experimental environment and in Section 7 we discuss the results of the execution of the algorithm based on the performance measurement methods described previously. Finally, in Section 8 we conclude this paper and discuss future work. 2. Related work
The concept of hotlinks was introduced by Perkowitz and Etzioni [21] where among a set of other useful site transforma-work, there was a lot of work published in the literature [3,6,8,9,11,12,14,19,22,23] ; this list of works can be separated according to the underlying user model. There are mainly two models that are followed: the clairvoyant user model [3,6,14] , and the greedy user model [8,9,11,12,14,19,22,23] . In the former model the user has knowledge of all the hotlinks that have been added, and hence of the structure of the whole site, and it is possible for him to follow a shortest path from the homepage to the desired information. In the second model at each node in the new web site structure, the user has knowledge of only the hotlinks at the present node and then (since he has no global knowledge) he takes a local greedy deci-sion following the link that guides him closed to the desired information.

Bose et al. [3] were the first to systematically study the problem in the clairvoyant model. They proved that solving the problem to a DAG is NP-hard (the same holds also for the greedy user model [11]) and they also provided algorithms for assigning hotlinks, as well as upper and lower bounds on the expected number of steps to reach the leaves from a homepage rithm that assigns a hotlink per node such that the expected path length after the assignment, approximates entropy within a constant. This algorithm has already been shown to be outperformed by greedy-like strategies (see the heuristics formu-lated in [6]).

Concerning the greedy model, an optimal algorithm whose runtime and space requirements are exponential in the depth of the tree (and thus polynomial for trees of logarithmic depth) has been independently discovered by Gerstel et al. [9] and
Pessoa et al. [22]. An implementation of this algorithm has been confirmed in [23] to be able to find optimal solutions for trees having ten thousands of nodes and a depth up to 14. In [12] it has been proven that the natural greedy strategy achieves at least half of the gain of an optimal solution in the greedy user model. Another 2-approximation in terms of the gain has been presented by Matichin and Peleg in [19]. Their algorithm assigns only hotlinks that bypass exactly one node and holds for both models.

Dou X   X  eb and Langerman [7] were able to present a linear-time algorithm to the above problem and were also able to effi-ciently maintain the hotlink assignment in the presence of dynamic operations (node insertion, deletion, weight reassign-ment); their algorithm employs techniques from [25]. The same authors [8] were able to provide an improvement to the ning time.

In general, a considerable number of approximation algorithms have been proposed and worst case bounds for the quality of the computed solutions have been given. However, only little is known about the practical behavior of most of these algo-rithms. In [11] there has been made an effort to close this gap by evaluating all recent strategies experimentally. These experiments are based on trees extracted from real websites as well as on synthetic instances.

A common feature of all these models is the assumption that the underlying tree model (bearing the original content of the pages) does not change and is a tree; in contrast our model treats the website X  X  structure as a DAG and takes into account the content of the pages, moreover in our model we do not aim at minimizing a target metric reflecting the accessibility by the use of an appropriate metric. 3. Architectural overview
In the sequel we briefly describe the overall architecture of the scenario of an application of the proposed hotlinks assign-ment algorithm on a website. First of all, given a website, users may visit a set of webpages belonging to it. Each visit to a webpage increases its popularity and the visit is recorded in the log file of the website. Depending on its size, we may set a threshold of visits, above which we may suggest that the application of the algorithm will improve the organization and the functionality of the website, concerning the speed and efficiency of a search for the information desired by a user. the website X  X  log file (step 1). This step aims at getting information concerning the topology/map and of course the webpag-es X  popularity so far. Secondly, the contents of all webpages must be stored in a database (step 2) in order to be processed later (step 3).

In order to be able to feed the proposed algorithm with input data to process (step 5), firstly offline collection of the re-quired data and preprocessing (step 4) is needed as depicted in Fig. 2 . In particular, we compute the content similarity be-tween every two pages and store it in a 2-dimensional matrix, n n , where n is the number of webpages. In order to compute this content similarity several algorithms from the Information Retrieval arena [2,24] can be employed. After completing the page similarity 2-dimensional matrix, we sort the webpages at hand according to their popularity based on the user visits found in the respective web server log file of the site.

We import the collected and processed data (step 5) and we execute the hotlinks assignment algorithm (step 6). The out-come of the algorithm will be the same set of webpages as before, but organized in a different topology, since new links will have been placed between web pages.
 4. Measuring the performance of a hotlinks assignment algorithm 4.1. The model
Before presenting our algorithm, we need to define the model and metrics to measure the performance of a hotlinks assignment algorithm. Therefore, we explain the factors that we take under consideration, having as our aim to propose a model that will take into account the content of the various involved pages.

We will describe firstly how the previous approach followed in the hotlink literature could possibly be modified in order to embed similarity between pages and then we will present our metric.
 The initial step is to assign to each edge a weight corresponding to the similarity of the two nodes connected by this edge. real distance a user has to traverse in order to approach the path.

Hence, if we denote with S i the sum of the similarities for each pair of nodes connected in the path from the node to the root, then formula (1) can be extended as follows:
A different approach could be to employ products instead of sums. In this case, if we denote with P ilarities for each pair of nodes connected in the path from the node to the root, then formula (1) can be extended as follows:
Both directions are maybe worth the effort to be explored, however we felt that they could lead to counterintuitive complex algorithms. Our approach deviates from the just described procedure, by the fact that instead of minimizing a metric for the whole site, we propose a metric that is attached to every webpage of the processed site, and designates the accessibility of the specific page; this metric extends the Pagerank metric introduced in the Google web search engine [28]. The original Pagerank formula for a webpage A is:
The Pagerank is a metric for evaluating the human interest and attention to a webpage algorithmically and objectively. The original Pagerank formula for a webpage deals with the whole web, while here we deal with the webpages constituting just modified Pagerank , we also take into account the similarity between pages, we call it Simetric (SImilarity METRIC). In particular, we calculate Simetric as follows: for each webpage of the website, we attach an intermediate value called
Aux that initially has value zero. Moreover the initial Simetric value of each page is equal to 1 computed recursively using the scores of the pages that point to it, that is pages distribute their Simetric across all their the sum of the previous Aux value of the page plus the product of the Simetric value of all the webpages in In  X  w  X  with the similarity of the pages with w , divided by the number of their outgoing links. Aux  X  w  X  is initially 0. Hence:
Then, the Simetric of w is calculated as: Simetric  X  w  X  X  factor parameters whose values are defined from the implementer. In particular, factor where q is the damping factor. A high damping factor means that results are less dampened and since there is little damping,
SimRank received from external pages will be passed around in the system. The above calculation of Simetric is repeated, for all the pages of the website, a constant number of times in order to have a suitable transfer of weights from one page to the real Simetric values (in the experiments that we performed a suitable number was approximately a hundred). A clear advan-tage of our proposed model, in comparison to the previous approaches, is that it does not depend in the assumption of the existence of a homepage, that is the source of all accesses to the site, but computes a score for each page that reflects its accessibility according to the connections the page has in the specific website, and assuming that each page can become the source of access in the site (this assumption models more accurately the reality, in comparison with the assumption that the homepage is the only entry point in the content of the site). Moreover our model does not assume that the leafs are the only information source, but permits to every node to play the role of an information source.

Based on the above metric we can define our algorithmic target as follows: given the set of webpages in a website, or-dered according to their popularity, add hotlinks (one per page) in order to make the ranking of the pages (according to their
The intuition is to connect webpages to content-based similar ones in an analogous manner to their popularity. To achieve this two parameters are taken into consideration (a) the popularity of the page and (b) the similarity between the page at hand and the rest of webpages in the site. Popularity of the page is utilized to bias and define the number of new links in an analogous manner, but simultaneously is  X  X  X ontrolled X  using the similarity between the current page and the page to connect to. As a result, in this way achieving a ranking based on Simetric close to the ranking based on the popularity means that the more popular the pages of the resulted web graph, to more similar pages they are connected. 4.2. The evaluation methods
In order to measure the performance of our algorithm, we propose four metrics that may efficiently indicate the way the rics that can prove that the new topology of the website indeed serves the purposes for which the proposed algorithm was designed. 4.2.1. Distance metric
The first metric compares the ranking of the pages according to their Simetric in relevance with their ranking according to their popularity. One way to do this is to compare the two sequences in terms of pairs. More specifically, we compare every of the pages according to their popularity and denote with r Simetric . We measure the similarity between the two rankings by the formula:
The closer to zero the outcome of the calculation, the more efficient the algorithm can be considered to be. 4.2.2. Weight metric tively. Let, max  X  i  X  X  max f r p  X  i  X  ; r s  X  i  X g and min  X  i  X  X  min f r ings by the formula:
The closer to zero the outcome of the calculation, the more efficient the algorithm can be considered to be. The weight metric follows the same logic with the distance metric, and hence in our experiments we employed only the distance metric. 4.2.3. Entropy based metric
Another, way to measure the performance of the algorithm on a given website is to measure the entropy of the site before and after the execution of the algorithm (this metric approach is in the spirit of the previous works, and hence we are not going to employ it in our experiments). We represent each page by a state and we associate probabilities with the links, resulting in a Markov chain model. These probabilities are in fact the similarities between the webpages. In the model de-scribed in [15], each user navigation session can be viewed as a trail through an ergodic Markov chain such that as soon as one session is completed a new one can be started at any state with nonzero probability according to an empirical initial distribution. An ergodic Markov chain is a finite Markov chain which is aperiodic and irreducible. In particular, G  X  X  N ; E  X  called the size of G . In addition, the probabilities P ij since the Markov chain is ergodic, for some m &gt; 0 we have that for all i ; j ; P where p denotes the attained stationary distribution.

We should note before closing that in [15] a novel method for online incremental computation of the entropy is pre-sented, by simulating a user X  X  random walk through the website. 4.2.4. Path length based metric
Our last choice would be to count path lengths and check whether they are reduced. If the average path length from the homepage to a webpage is reduced, this would mean that in general, a user would be able to access information faster and in the spirit of previous approach since it assumes that the homepage is the main source of website accesses, however it could be interesting to see how our algorithm can be measured using it.

After having settled a generalized model and a number of metrics for the evaluation a hotlink assignment algorithm, we present in the next section our novel algorithmic approach. 5. The algorithm
We present a randomized algorithm, which takes under consideration the popularity of the webpages and the similarity between them, and suggests the placement of hotlinks, one at most per page. The algorithm consists of four basic steps: Step 1. We divide the initial set P of all the webpages into two subsets, named POP and NonPOP (popular and non popular).
Step 2. We choose one node from each set. The node from the set POP is selected according to the probability distribution Step 3. We connect via a new link, node i with the node that it is most similar with and we delete the second one from set
Step 4. We repeat until NonPOP is empty. Then we create two new sets POP and NonPOP out of the old set POP and execute
Below we present the algorithm X  X  pseudo-code: 3. Initialize : 4. Sort nodes according to popularity. 5. Assign nodes to sets POP and NonPOP . 6. While ( NonPOP not empty) { 7. Choose node i from set POP . 8. In order to choose nodes from set NonPOP . 9. Locate the most popular path to i from the homepage (a randomly selected page). 10. Choose nodes j such that their nearest common ancestor with i belongs to the most popular to i path, and the 11. Sort the candidate nodes according to their similarity with i . 12. Choose the most similar node with i , y . 13. Connect via hotlink  X  y ; i  X  nodes i and y . 14. Delete node y from NonPOP . 15. Assign weight to the new link. 16. Normalize weights of outgoing links of y . 17. }//end of While 18. HotlinkAssign ( P ) 19. } end ;
Our proposed algorithm needs three basic algorithmic components: 1. A sorting component. The best sorting algorithm, for an input of n items, takes O  X  nloglogn  X  in the worst case and linear space [10]. The assignment of the nodes to the sets POP and NonPOP , can be performed in O  X  1  X  time for each node. 2. An all pairs shortest path component. For the purposes of our experiments, we used the Bellman X  X ord algorithm [3]. The time complexity of Bellman X  X ord algorithm is O  X  nm  X  for a graph with n nodes and m edges. 3. An NCA (nearest common ancestor) component for DAGs. The best to our knowledge result concerning an algorithm for
NCA in DAGs is the one proposed in [5], where the first method proposed is quite simple and solves the NCA problem for the input DAG on n vertices and m edges in constant time after an O  X  nm  X  preprocessing. 6. Experiments
In order to demonstrate the performance of the proposed algorithm we have performed experiments using synthetic and real life data. We have developed a website simulator. It was written in C++, using LEDA, the library of efficient data types and algorithms [26].

Our goal is to represent a website using a graph and by performing the algorithm on the graph, evaluate its results. To conduct an experiment we follow the following procedure: first, we build a connected directed graph. We choose the num-ber of nodes representing the webpages and the number of edges that will be placed on the graph, denoting the links be-tween webpages. Finally, we delete all the cycles found in the graph, by deleting the corresponding edges. After the construction of the graph, we randomly select the homepage of the graph. In addition, we assign arbitrary weights to all nodes and to all edges, which represent the nodes X  popularity and the similarity between them, respectively.
In order to compute the most popular path from the homepage to each node/webpage, we use the Bellman X  X ord algo-rithm [4], which computes single-source shortest paths in a weighted digraph (where some of the edge weights may be neg-ative). The advantages of using the simulator are that by using the same number of nodes and edges at each experiment, we perform the algorithm on different topologies each time and observe the variations of its performance, and by selecting dif-ferent numbers of nodes and edges, we can observe how the results for a large website differ from those of a small website.
On the other hand, it would be very enlightening and helpful to test the algorithm on a real website. In fact, we tested the algorithm on the site of the Department of Computer Engineering and Informatics of the University of Patras, http:// of its pages receive adequately high average hits per day, in order to ensure that the popularity rankings obtained are trust-works especially better for deep site graphs. In particular, the site at hand has an average maximum depth of three pages and the great majority of the popular pages are in depth of two. As a consequence, a few hotlinks are assigned, that provide improvements at the website navigation and most important serve as a proof of concept that the proposed rankings are effi-in the case of popular pages at depth three, hotlinks directly from the homepage were assigned to them, reducing their depth by one. In order to test the algorithm on the site, we mapped the site into a graph and parsed it with the website simulator.
As long as the content similarity between webpages computation is concerned, we used an online tool, Similar Page Checker [27]. This tool allows one to determine the percentage of similarity between two pages. What we noticed, is that pages like a present the summary of the log file of the site.
 7. Evaluation
In the experiments described below, we used connected graphs of 500 and 100 nodes. We have conducted many exper-iments using the same number of nodes, but with different topology each time. The results presented in the figures are representative of the average results after the execution of the algorithm a hundred times on a hundred different topologies.

One of the main goals of the algorithm is to modify the website X  X  topology, in order to promote popular content. In Fig. 4 , we present the percentage of nodes, whose Simetric is improved and they represent the 10%, 20% and 50% most popular nodes respectively, in a set of 500 nodes, for a random different topology each time.

In Fig. 5 , we present the percentage of nodes, whose Simetric is improved and they represent the 20% most popular nodes, in a set of 100 nodes, for a random different topology each time. In the case of 100 nodes, we notice that the percentage of case of 500 nodes, is that in the first case, the amount of popular nodes improved is smaller than the one calculated in the second. Hence, we can conclude that the algorithm performs better on large websites than smaller ones. This fact is not only quite normal and expected but desired as well. More specifically, in websites that consist of many pages, the popular pages participate in many hotlinks assignments with high probability and the larger the number of pages is, the more assignments are performed and popular pages are favored since they are chosen to receive an incoming link with probability distribution that depends on their popularity. On the other hand, in small websites, there is not such great necessity to improve access to popular webpages in distance terms, due to their nature. In other words, in a small website we expect the distance from the homepage to popular pages to be relatively short and therefore access to them is easy either way.

Fig. 6 displays path length improvements. We find the shortest paths from the homepage to every node, before and after the execution of the algorithm. Depending on the topology of the site, the number of nodes, the paths to which are reduced reaches the 10% of the number of total nodes. Fig. 6 shows that, as long as the distance from the homepage is concerned, the percentage of nodes improved varies from 1% to 10%. Taking under consideration the fact that the graphs constructed are strongly connected, these results are very positive. To be more exact, the graphs constructed are very dense, and hence a user can already follow multiple paths to visit a node. Therefore, achieving a distance decrease for almost 10% of the nodes means that in the case of real websites, whose topology may be lacking connectivity, the results will be much better.
As we described earlier, we wish to compare the ranking of the pages according to their Simetric in relevance with their ranking according to their popularity, before and after the execution of the algorithm. Fig. 7 presents the percentage of improvement on the first proposed metric, the distance metric.

As we see in Fig. 7 , the improvement of the distance metric reaches the amount of 34%. This means that, after the exe-cution of the algorithm, the above sum was reduced by 34%. As we have already mentioned, the optimal case is when we manage to make this sum equal to zero. Hence, in this case, 34% of the nodes after the execution of the algorithm have the same ranking according to their Simetric , with the one according to their popularity. This means that more popular nodes now have large Simetric value and therefore access to them will be promoted in the future. The results of the application of the algorithm on the second metric, the weight metric, were almost identical with the ones just mentioned, due to the fact that both metrics X  values depend on the rankings of each page according to its popularity or Simetric .

In Fig. 8 , we present the average decrease of the distance of the webpages in a website from the homepage, measured in links. In the case of topologies consisting of 500 nodes, the minimum average decrease is almost 10 links, while the maxi-mum average decrease of the distance reaches the number of 160 links. This means that a user will be able in the future to access each webpage of the website following 160 less links in average. The transformation of the website in this case would visit 160 webpages more, that is 32% of the total number of the webpages of the topology, in order to reach a certain webpage.
In Fig. 9 , we present the way the application of the algorithm on the graph of a site affects its entropy. In all cases, the entropy is increased by an amount that in some cases exceeds 10% of the entropy before the application of the algorithm.
These results were expected and encouraging as well, since the insertion of links between nodes of the graph increases the entropy according to formula (7) on the one hand and on the other hand, this means that the connectivity of the graph we concentrate again on the entropy of the site, but the algorithm is applied on graphs that are 40% less dense than the ones used for the purposes of the experiments of the Fig. 9 . In this case, the improvement of the entropy reaches the amount of connectivity and hence there are more nodes that are not connected at all or they are connected via long paths, in contra-diction with the dense graphs where the connectivity means the existence of more paths. This is the reason that in the first case, applying hotlinks is more meaningful and shows better results in practice (see Table 2 ).

As long as experiments to compare the algorithm proposed in the paper with other algorithms that assign hotlinks to im-prove website accessibility are concerned, [11] contributes by evaluating all recent strategies experimentally. The experi-ments are based on trees extracted from real websites as well as on synthetic instances. The latter are generated by a new method that imulates the growth of a website over time. Based on the path length a number of additional values are between different instances the relative gain as g ( A )/ p (0) has also been computed. The main focus of the study was on the approximation ratios that occur in practice. Therefore, for all instances where an optimal solution OPT could be computed, is better than two techniques but it is much worse than the ones achieved by the latest algorithms. This fact is natural and not disencouraging, since the nature of the algorithm and its goal differs from those of the other algorithms, which do not take similarity under consideration and therefore the assignment of hotlinks is easier. In other words, our purpose is to con-nect similar and popular nodes at the same time, taking under consideration the circumstances and needs in the case of a real site. 8. Conclusions and future work
This article has presented a novel approach to hotlinks assignment. We presented a randomized algorithm, which takes under consideration the popularity of the webpages and the content similarity between them, in order to suggest the place-ment of hotlinks, with the constraint that each page can have at most one hotlink. We have observed that a website which uses our algorithm benefits in terms of speeding up browsing in the site as expected. Equally important to us, was the pur-pose of using the algorithm especially in favor of the popular pages. Indeed, experiments have indicated that popular terms are promoted in terms of Simetric and distance from the homepage.

Future work includes addressing theoretical and technological issues. It would be interesting to see the efficiency of our model on additional websites from different domains to further verify the effects of content sensitivity upon hotlink assign-confirm that the transformation of the topology and the theoretical promotion of the popular nodes, appears in practice as well. In particular, what we would like to check is whether users use the new paths created or keep on using paths consisting of non similar pages. In addition, the average number of links followed in the site is a very important metric, which can show us, how much the topology of the site is improved and surfing in the site is simplified.

Furthermore on the theoretical side, since we refer to a randomized algorithm, we aim at investigating more heuristics and techniques to improve access to popular nodes in terms of distance and connection with similar nodes, but without overloading the time complexity of the algorithm.

References
