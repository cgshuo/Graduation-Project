 High findability of documents within a certain cut-off rank is considered an important factor in recall-oriented appli-cation domains such as patent or legal document retrieval. Findability is hindered by two aspects, namely the inher-ent bias favoring some types of documents over others in-troduced by the retrieval model, and the failure to cor-rectly capture and interpret the context of conventionally rather short queries. In this paper, we analyze the bias impact of different retrieval models and query expansion strategies. We furthermore propose a novel query expan-sion strategy based on document clustering to identify dom-inant relevant documents. This helps to overcome limita-tions of conventional query expansion strategies that suffer strongly from the noise introduced by imperfect initial query results for pseudo-relevance feedback documents selection. Experiments with different collections of patent documents suggest that clustering based document selection for pseudo-relevance feedback is an effective approach for increasing the findability of i ndividual documents and decreasing the bias of a retrieval system.
 H.3.3 [ Information Search and Retrieval ]: Retrieval Models Measurement, Algorithms, Experimentation Conventionally, retrieval systems are evaluated based on Average Precision , Q-measure , Normalized Discounted Cu-mulative Gain , and other metrics [12]. However, these met-rics do not evaluate, what we can find and cannot find glob-ally throughout the whole collection. In spite of low retriev-ability, systems can still achieve good precision performance [1], which is a perfectly just target in precision-oriented ap-plication domains, where the focus is on providing a small set of perfectly matching results. In this paper, we argue that in recall oriented domains like legal or patent retrieval settings, it is very important to considered retrievability of each and every document, that is, whether each document can potentially be found for a query that it is relevant for.
Document retrievability [1] is a measurement used in In-formation Retrieval (IR) for determining, in how far all doc-uments in a collection can actually be found given a specific retrieval system. In other words, retrievability is used to analyze the bias of retrieval systems. To measure retriev-ability, a large set of potential queries (e.g. all combinations of all important keywords up to a pre-specified query length) are passed to a retrieval system, and the number of docu-ments that can and that cannot be retrieved is evaluated. The resulting figure provides an estimate on the amount of bias introduced by a certain retrieval system, indicating its suitability for recall-oriented applications.

In this paper, we use Query Expansion (QE) as an ap-proach for increasing the retrievability of individual docu-ments in recall oriented settings. The main motivation be-hind our work is to analyze the strength of different QE approaches, such as to what extent they can make individ-ual documents more findable. However, first experiments revealed, that state of art QE approaches showed worse re-trievability performance. This is because in QE it is as-sumed, that the set of top-ranked documents used for Pseudo-Relevance Feedback (PRF) is relevant, and learning from these Pseudo-Relevance documents can improve the rank-ing [10]. However, due to system bias, top-ranked docu-ments may contain noise [10], which can ultimately result in the query representation drifting away from the original query.

Targeting this limitation and for improving the retriev-ability of individual documents, in this paper, we propose an improved clustering based resampling method for PRF selec-tion. The main idea is to use document clustering based on local frequent terms to find dominant documents for PRF. ment d  X  D can be retrieved within the top c ranked results for all queries in Q . More formally, retrievability r ( d )of d  X  D can be defined as follows.
Here, f ( k dq ,c ) is a generalized utility/cost function, where k dq is the rank of d in the result set of query q , c denotes the maximum rank that a user is willing to proceed down the ranked list. The function f ( k dq ,c ) returns a value of 1 if k dq  X  c ,and0otherwise.
 Retrievability inequality can be further analyzed using the Lorenz Curve . Documents are sorted according to their re-trievability score in ascending order, plotting a cumulative score distribution. If the retrievability of documents is dis-tributed equally, then the Lorenz Curve will be linear. The more skewed the curve, the greater the amount of inequality or bias within the retrieval system. The Gini coefficient G is used to summarize the amount of bias in the Lorenz Curve, and is computed as follows.
 where n = | D | is the number of documents in the collec-tion. If G = 0, then no bias is present because all documents are equally retrievable. If G = 1, then only one document is retrievable and all other documents have r ( d )=0. Bycom-paring the Gini coefficients of different retrieval methods, we can analyze the retrievabilit y bias imposed by the underly-ing retrieval system on the given document collection. Existing Approach: Lee et al. [10] select documents for PRF based on cluster analysis. Under their assumption, a document is considered relevant for PRF, if it can cluster lot of high similarity documents. A document which has either no nearest neighbor or some neighbors with low similarity is considered irrelevant for PRF. The main limitation of their approach is that, it is not useful for those application do-mains, in where large variation exists between documents length and large diversity exists in a domain X  X  vocabulary for expressing information, like patent documents. For ex-ample, if longer length documents contain many vague or general terms [3], then they can cluster a large number of irrelevant documents with good similarity to the centroid, but with poor intra-cluster similarity. On the other hand, if the documents are short and contain many technical or synonym terms, then the clusters may be small, and more importantly it can become very difficult to cluster related documents into a single cluster.

For overcoming the limitations with [10] approach, in this paper, we describe an improved approach for clustering, where clusters are rejected or accepted for PRF on the ba-sis of their intra-cluster similarity, rather than only on the basis of their size. Our hypothesis is that a cluster should be better in terms of both factors; its size and intra-cluster similarity. Low intra-cluster similarity of large size clusters indicates that they clustered other documents due to their noisy terms. The main features of our clustering process are that, it checks intra-cluster similarity of clusters on the basis of local frequent terms , which is more efficient. Smaller size related clusters are merged efficiently using local fre-quent terms of clusters and merged cluster feature vectors. After ranking clusters for PRF, top-ranked documents in high rank clusters for relevance feedback are selected based on their similarity with the centroid local frequent terms of the cluster. (a) Constructing Clusters: After constructing initial clus-ters using k -nearest neighbors with cosine similarity, we use intra-cluster similarity scores of clusters for selecting rele-vant clusters for PRF. For fast computation, we check intra-cluster similarity of individual clusters on the basis of their number of local frequent terms . If a cluster contains at least (min_local_terms) number of local frequent terms with minimum support (min_term_supp) , then it is consid-ered relevant for PRF. For subsequent analysis we use these min_local_terms of clusters as centroids. (b) Merging Similar Clusters: As each document ba-sically creates its own cluster of similar documents, most documents will end up in numerous clusters. This overlap is used to merge clusters into larger consistent groups. Two clusters which contain a high number of overlapping docu-ments are merged [5]. Intuitively, a cluster C a is good to merge with another cluster C b , if there are many terms in C a that are also locally frequent in C b . For calculating final inter-similarity of C a to C b , we use the scoring function of Equation 3. The score of Equation 3 is normalized using Equation 4 to remove the effect of varying document size. Score ( C i ,doc ( C j )) = [
Where C i and C j are two clusters; doc ( C j ) stands for combining all the documents of cluster C j into a single doc-ument. x  X  X represents a term in doc ( C j )thatislocal frequent in C i ; x  X  X represents a local frequent term of doc ( C j ) that is not locally frequent in C i ; n ( x )and n ( x )are the weighted frequencies of x and x in the feature vector of doc ( C j ). s ( x ) represents term x support in cluster C s ( x ) represents term x in cluster C j .

The similarity function in Equation 4 only calculates the similarity from one side Sim ( C a ,C b ). However, for remov-ing the effect of noisy terms, it is necessary that both val-ues Sim ( C a ,C b )and Sim ( C b ,C a ) should be high Equation 5. We thus use the geometric mean of the two normalized scores. After merging two similar clusters the local fre-quent terms sets of both clusters are also merged.
Inter Sim ( C a  X  C b )=[ Sim ( C a ,C b )  X  Sim ( C b ,C (c) Ranking Clusters: In [10] a cluster-based query like-hood language model is used for ranking clusters for rele-vance feedback. After ranking clusters a set of top docu-ments from high rank clusters are used as an input for rel-evance feedback [9]. We use the same method for ranking clusters. However, for selecting the set of top documents from high rank clusters for relevance feedback, we further rank individual documents in the selected clusters based on their similarity with the centroid local frequent terms of the cluster.

After calculating individual similarity scores of all docu-ments with their centroid frequent terms, documents from high-rank clusters are order by decreasing order of their scores, and placed into single set S . Finally, the top K doc-uments from set S are selected for relevance feedback, and terms for expansion are selected using Language Modeling approach [9].
Where K is the set of documents that are relevant to the query q . We assume that P ( d ) is uniform over the set. After this estimation, the most e terms (words) from P ( w | K )are chosen as for expanding queries. The values P ( w | d )and P ( q | d ) can be calculated following Equations 7 and 8. where  X  i is the i th query term, m is the number of terms in a query q ,and d is a document model.

Dirichlet smoothing is used to estimate non-zero values for terms in the query which are not in a document. It is applied to the query likelihood language model as follows. where P ML ( w | d ) is the maximum likehood estimate of a term w in document d , D is the entire collection, and  X  is the smoothing parameter. | d | and | D | are the lengths of a document d and collection D , respectively, freq ( w, d )and freq ( w, D ) denote the frequency of a term w in d and D , respectively.
For our experiments, we use a collection of freely available patents from the US patent and trademark office, down-loaded from (http://www.uspto.gov/) . We collected all patents that are listed under United State Patent Clas-sification (USPC) classes 422 and 423 .Thereareato-tal of 54 , 353 documents in our collection, with an average document size of 3 , 317 . 41 words (without stop words re-moving). Seven state-of-the art IR models and QE methods along with our proposed clustering technique are used for evaluating the retrievability inequality. These are T FIDF, OKAPI retrieval function ( BM25), E xact match model, Lan-guage Modeling with term smoothing ( LM) [13], Kullback-Leibler divergence ( KLD) [2], Term Selection value ( QE-TS) [11], PRF document selection using clustering ( Lee et al.) [10], and our clustering approach using local frequent terms (
Cluster-LFT). For all QE models, we select the top 35 doc-uments for PRF and 50 terms for query expansion. In our clustering approach, for accepting or rejecting clusters un-der intra-cluster similarity hypothesis, we set the values of parameters min_local_terms and min_term_supp as 5 and 60% , respectively.
 Controlled Query Generation: For generating repro-ducible and theoretically consistent queries for retrievability measurement, we use the method of controlled query gen-eration (CQG) [7]. We use two different variations, based on how patent experts generate queries for searching their relevant information in patent corpus.
 Query Generation combining Frequent Terms (QG-FT) : In this CQG approach, we try to reflect the way how Table 1: Queries set sizes and average retrievability scores (ARS) patent examiners generate queries sets in patent invalidity search problems [8]. In this search process, the examiners extract relevant query terms from a new patent applica-tion, particularly from the Claim sections for creating query sets [6]. QG-FT first extract al l those frequent terms that are present in the Claim sections of each patent document and have a support greater than a certain threshold. Then, QG-FT combines the single frequent terms of each individ-ual patent document into two , three and four terms com-binations for constructing longer length queries.
In QG-FT, we consider all the frequent single terms, which have a minimum support  X  3inthe Claim section . For gen-erating larger length queries for every document, we expand the single frequent terms into two , three and four term combinations. For documents which contain large number of single frequent terms, the different co-occurring term com-binations of size two , three and four can become very large. Therefore, for generating similar number of queries for every document, we put an upper bound of 90 queries generated for every patent document.
 Query Generation with Document Relatedness (QG-DR) : Patent applications may contain many vague or tech-nical terms in order to hide the relation to other documents from patent examiners [3]. In such situations patent examin-ers extract relevant terms fro m other patent documents that are similar to the new patent application using the concept of document relatedness [4]. With QG-DR, we adopt this strategy. We first define a set of related documents for each document in the corpus based on k -nearest neighbor with co-sine similarity. QG-DR then generates a set of queries based on each of these sets of related documents. Using the rel-ative entropy of individual terms in language modeling [7], the most discriminating terms are identified for constructing two , three and four terms combinations queries.

In QG-DR mechanism, we construct the related docu-ment set for every document in the collection considering 35 neighbors. After applying language modeling on related documents sets, we select the top 70 terms that contribute most to the relative entropy with the language model. Two , three and four term queries are constructed with the same approach and maximum number of queries per document boundary as above. Table 1 shows the distribution of differ-ent queries sets.
 Discussion of Results: We show the retrievability in-equality of different retrieval systems using Lorenz Curves with a rank cut-off factor of 30 (Figures 1). Tables 2 shows the retrievability inequality with other rank cut-off factors using Gini coefficient. The exact match method, whichiswidelyusedinprofessionalpatentretrievalsystem, consistently shows the worst performance. The clustering Figure 1: Lorenz Curves for retrievability scores with rank cut-off c=30. Equality refers to a optimal system which has no bias. approach of Lee et al. and term selection also do not perform too well with respect to providing potential access to all documents. The standard TFIDF approach performs surprisingly well, only surpassed by our clustering approach based on frequent terms with the enhanced cluster selection strategy. This indicates that our approach makes individ-ual documents more findable than other systems, due to its improved selecting procedure for pseudo relevance feedback.
Table 2 depicts the Gini coefficient for various rank cut-off (c) parameter configurations. We can see that, as c in-creases, the Gini coefficient tends to slowly decrease on all different queries sets with both collections. This indicates that the retrievability inequality within the collection is mit-igated by the willingness of the user to search deeper down into the ranking, as expected. If users are willing to examine only the top documents, then they will face a greater degree of retrieval bias.
In this paper, we evaluated the effectiveness of different retrieval systems using r etrievability measurement with a focus on recall oriented application domains. Our results yield that state of the art QE methods provide a large in-equality in the retrievability of documents, as compared to those systems which do not expand queries. This is due to their ineffective assumption that top rank documents for PRF are always relevant and learning from these relevance feedback documents for expanding queries can increase the effectiveness of retrieval systems. For overcoming this limi-tation, in this paper we utilize clustering approach for better document selection for PRF. On our extensive retrievability Table 2: Gini coefficient values with different re-trieval models with different rank cut-off factors (c) experiments with using large collection of queries, our clus-tering approach for PRF, provides less bias than all other systems on different rank factors. [1] L. Azzopardi, V. Vinay. Retrievability: an evaluation [2] W. B. Croft. Advances in information retrieval. [3] C. J. Fall, A. Torcsvari, K. Benzineb, G. Karetka. [4] A. Fujii, M. Iwayama, N, Kando. Introduction to the [5] B.C.M.Fung,K.Wang,M.Ester.Hierarchical [6] H. Itoh, H. Mano, Y, Ogawa. Term distillation in [7] C. Jordan, C. Wattters, Q. Gao. Using controlled [8] K. Konishi. Query term s extraction from patent [9] V. Lavrenko, W. B. Croft. Relevance based language [10] K. S. Lee, W. B. Croft, J. Allan. A cluster-based [11] S. E. Robertson, S. Walker. Okapi/Keenbow at [12] T. Sakai. Comparing metrics across TREC and [13] C. Zhai, J. Lafferty. A study of smoothing methods for
