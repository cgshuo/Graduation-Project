 Motivation : In the field of bioinformatics there is an emerging need to integrate all knowledge discovery steps into a standardized modular framewo rk. Indeed, component-based development can significantly enhance reusability and productivity for short timeline pr ojects with a small team. We present Interactive Knowledge Discovery and Data mining ( iKDD ), an application framework written in Java that was specifically designed for these purposes. Results : iKDD consists of a component -based architecture and a web-based tool for pre-clinical research and prototype development. The platform provides an intuitive and consistent interface to create and maintain components, e.g., data structures, algorithms and utilities to load, save and visualize data and pipelines. The rich-featured tool supplies database connectivity, workflow processing and rapid prototype building. The architecture was carefully designed using an object-oriented approach that respects crucial goals: usability, openness, robustness and functionality, especially in the abstraction and description of the components, which distinguishes it from other packages. iKDD is well-suited to serve as a public repository of components, to run scientific e xperiments with a high-level of reproducibility, and also to rapidly build prototypes. This paper describes the general architect ure, and demonstrates through examples the ease by which a complex scenario implementation can be facilitated with iKDD . D.2.11 [ Software engineering ]: Software architectures  X  data abstraction, patterns. H.2.8 [ Database Management ]: Database applications  X  data mining. Management, Design, Experi mentation, Human Factors Data mining, bioinformatics, workflow Industrial projects in bioinformatic s often have to be developed under tight time constraints while at the same time have to achieve a high standard of quality. Our previous strategy to address this challenge was to develop from scratch for each project a tailored solution, and this concept led generally to very good results; the algorithms and soft ware tools were embedded in slim and tightly organized architectures and allowed rapid optimization and adjustments. Howe ver, the heterogeneity of the programs and the rapidly increasi ng number of specific solutions became difficult to manage considering limited resources. Each solution was residing in di fferent project locations organizationally and technically, each of the solutions had different architectures depending on the choice of the principal scientist, data structures were incompatible and the exchange of algorithms, parsers and visualiza tion tools required a substantial amount of engineering time. At the same time the diversity and complexity of the projects increas ed until we realized that this approach was too costly to continue. Based on that experience and focusing on a significant increase of development related cost-efficiency, we collected the organizational and functional requi rements which we identified in our previous projects; we combined these to address all practical and technical challenges within one single framework, which is flexible to build up new applicati ons, allows exchange of existing technologies and defines guidelines for compatibility standards. In addition, it has to be able to address different projects with limited resources in parallel, a nd the exploitation of synergy effects needs to be enabled and even encouraged by the high quality of existing parts. After collecting the requirements we specified the architecture for which we applied state of the art design patterns, and realized the framework technically using Java , the most widely programming language in the bioinformatics community. The framework has already been applied successfully in industrial projects within Siemens, and has achieved the desi red effect of cost-reduction by decreasing development cycle times and by increasing the quality of the solution. We present belo w the collected requirements and the existing framework analysis. The requirements engineering in cluded review of previous projects, interviews with our cu stomers, literature studies and a prognosis of upcoming challenge s in customer related and scientific projects, and compared the identified requirements with features provided by existing systems. The system should have the ability to retrieve any kind of biomedical data from either databa ses or flat files and any other common data stream, e.g., web-ba sed data repositories. In our projects we are usually working with data types like gene expression (micro-array data), pr otein expression (mass spectra data), gene sequence data, pr otein/peptide sequences, flow cytometry data, clinical data and medical image features. Then, our main concern is to handle both research and industrial projects. The system should provi de a development and testing environment for pre-clinical research, e.g. drug target identification. It should also offer a rapid prototype building ability and comprehensive visualization for industrial projects. This unified framework should se rve as repository of components such as data models, visuali zation entities, pre-processing and data-mining algorithms, among ot hers. Finally, this unified framework should integrate a quality control process and enhance productivity within the team, since the motivation is cost efficient development. Code-reusability, ge nerally speaking, increases quality and reduces costs; however, it also requires a constant amount of software engineering to support technology and algorithm oriented projects. As su ch it is today a key selling point [3]. In addition the environmen t should allow incremental and parallel development and be mana geable in a straightforward way. After a thorough analysis of our past experiences and existing technologies, we recognized that a component-based concept would fit most our needs. Indeed, this approach provides many benefits [1] [2]: modular ity, extensibility, reusability, maintainability, quality and allows parallel development [3] within the team. It requires employing an object-oriented technology for a better abstraction, and potential reuse [2]. The analysis led to the formulation of five main design principles we wanted to apply to the framework: usability , openness , portability , robustness and functionality .  X  Usability is the first priority in the design of the framework. The 
API should be intuitive and consis tent for the developers. The system should provide a comprehensive and friendly user interface for the researchers.  X  Openness is the key of the com ponent-based approach. The architecture should be extensible with minor changes of code. 
The framework should allow an easy integration of proprietary source code and other external libraries. It should also afford a language mapping to avoid a code specific deployment environment [3].  X  Portability is of crucial importan ce in the industrial area. 
Applications should run consiste ntly on any computer using a common operating system (MS Windows, Linux, UNIX, Mac, 
Solaris...), and provide a web browsing capability. Customers should not deal with installation issues. Additionally, serialization and de-serializati on of components have to be compatible across platforms.  X  Robustness is an essential goal of the KDD process. The system should cope with faulty input data, e.g., missing values and invalid formats; it needs to identify exceptions caused by improper processing of com ponents, and has to report abnormalities to the user.  X  High functionality enhances code re-use. The framework We collected all requirements and compared them with features from systems accessible via the Internet, and analyzed to what degree these systems would be applicable for our industrially oriented projects as a host frame work. The candidate framework should match the following criteria: a preferably component-based system capable of quickly producing prototypes for either pre-clinical research or industria l purposes. It should also provide a rich-featured user-interface handling workflow processing for scientific experimentations. Th e API should be easy and powerful to develop freely new components and integrate other tools into it. Taverna [11] is a popular tool used in the bioinformatics community. It supports biologist s and bioinformaticians to construct a highly complex analysis over public and private data and computational resources. Ho wever, it does not support the building of prototypes. The da ta mining frameworks XELOPES [12], Yale [13], and Weka [14] were analyzed, but none of them matched all of our requirements; for example XELOPES and Yale do not provide the concept of dynamic workflow processing [8]. Finally, Weka and its extension for bioinformatics BioWeka [15], interoperability with other tools [7] and have performance issues on large datasets [6]. This comparison led us to the decision not to use an existing open-source, but rather to implem ent a new system from scratch. We describe in this paper the component-based framework which provides a slim and flexible architecture, an easy-to-use and efficient API for data modeli ng, analysis, manipulation and visualization, and a rich-feature user interface allowing dynamic workflow processing and rapid prot otype building. In this paper we discuss the system structure (Chapter 2) and applications (Chapter 3). iKDD is written in the Java 2 Platform, Standard Edition (J2SE) and makes use of the Standard Development Toolkit (SDK). It can be compiled and run on any machine having a valid Java Virtual Machine installed. An overview of the structure of iKDD is given in Figure 1. The database layer provides different drivers which connect the users to intern al databases e.g. PostgreSQL [16]. The core package from iKDD models the component abstraction, implements a gra ph-based processing module, and covers basic components e.g. XM L import / export. In the third layer, we find the different st eps required in the KDD approach i.e. data selection , preprocessing , transformation , data-mining and statistics evaluation . This is achieved by addressing a package for each step of the process. Furthermore, all new components will be integrated at this level. Finally, the top layer represents the framework applications such as iKDD Wizard that was specially designed to run scien tific experiments in an easy-to-use and friendly user interface. We present in the next chapter the different layers of the system, its architecture and graphical support. This package contains approximately 150 classes and interfaces, and numerous important packages . We present two important concepts visible to the users: process and plug-in . iKDD is a component-based frame work, and hence defines the component abstraction in its kernel. The core package makes use of a class hierarchy to represent the generalization mechanism. Indeed, all components have to be sub-classes of the abstract class AComponent . Additionally, we are using a process concept . The architecture allows designing any kind of process into a component. This is achieved by using an internal tree-like structure that represents inputs , outputs , parameters , properties and fields , which is illustrated in the Figure 2. For each input an infinite number of ports can be defined. A parameter is a single-port input that defines in which way the input objects will be processed. It is also possible to define a finite number of object ports for each output , which represents the objects produced during the process. A property is a single-port output that gives important information about the process. Finally, a field combines both parameters and properties behavi or; it can be initially assigned and be modified during the process. To each of these nodes has to be assigned a Java class object whic h represents the data type that the component will accept. It can be either a component class or any registered structures e.g. ArrayList from Java. To ensure a safe usage of external data structures, we use a class that plays the role of structure registry. Finally, this tree-node design supports the visitor pattern [9] and facilitates an external process to navigate through the nodes e.g. XML configuration export. The AComponent class provides an intuitive and complete set of methods to program a new com ponent. In one hand, the design step must be done into the bla nk constructor by invoking methods to add new nodes and to refine a data type from an existing one. Each node of the same category, e.g., input, output... must have a unique id. A description can also be assigned to each node. In order to prevent inconsistent design, it is as well possible to lock a category of nodes, so that all children cannot allocate new nodes of this type. In the other hand, the execution function of the process has to be implemented in all components. The core package integrates the specialization mechanism by providing pre-defined categories of components. Indeed, each of these categories is represented as an abstract class deriving from AComponent . Hence, components are grouped into data containers , loaders , savers , visualizers , algorithms and pipelines . Each sub-class establishes common interfaces, for instance all algorithms have a data input and a data output, all file loaders have a data output and a field representing the source file. Additionally, the architecture de fines an open design and makes extensions straightforward. This class hierarchy provides guidelines to the developers a nd enables them to define sub-categories for a better ease of de velopment. Although this is an engineering task, experience ga ined from algorithm development showed that a clear abstraction and organization of the developed modules led to low maintenance and better quality. At last, leaves of this tree hierarchy, the compone nts, become concrete classes. Their implementation is facilitated by inheriting and overriding parents X  properties. Methods for da ta retrieval, e.g., input objects and data storage, are provided, as well as an enforced checked exception and process logging. We present in the next section, a complex component specialization that makes use of a plug-in concept. The concrete class IKDDGraph was specially designed to handle workflow processing. Indeed, it incorporates a plug-in concept that is represented with dir ected links between components. Internally, we make use of a receiver  X  sender pattern, relatively close to the visitor pattern, to check the validity of the connections. Each link consists of both source and destination connectors. The component nodes, i.e., input, parameter, output, field, property, are potential connectors and this pattern helps define rules with a minimum cha nge of code. For instance, any output can send data to any input, but a property cannot receive data at all. Through an intuitive and well-defined API, users can add, remove components and links , and are able to execute the whole workflow or a given component. Two execution approaches are addressed: top-down and bottom-up. To ensure a safe synchronization and handle pa rallel processing, we make use of the Java built-in multi-threading. Each involved component has a state, e.g. unprocessed, wa iting, processing, invalid, and processed. Finally, this gra ph-based processing instrument distinguishes itself from other p ackages in its flexibility and functionality. By designing this graph module as a framework component, we could develop additional advanced functionalities. First, we could address the challenging grouping/ungrouping concept with a minimum of code. Indeed a graph instance contains a set of components and links. As IKDDGraph is a component specialization, it can integrate into itself another graph. This concept directed us also towards loop execution. This is a very important feature in workflow processing, and particularly in data-mining when using an iterativ e cross-validation with a given number of folds. The implementation of IKDDGraph led us to the development of sub-classes repres enting each a loop e.g. for, for-each, while. Each of these graph specializations pre-defines parameters and overrides the execution function according to their mechanism. Figure 3 shows the sub-class IKDDForLoop that defines two parameters of type integer: from , and to and a field current that is accessible for all grouped components. In this example, the component IKDDForLoop controls the iteration of the cross-validation with the given parameters from and to and the modifiable field current . For each iteration, both training and testing dataset creation component s acquire two different values, the number of folds via to and the current fold index via current . This leads to the training of the cla ssifier model, and its transfer to the testing of the classifier. Finally, the output of this latter component is directed as an output of the grouping component represented by IKDDForLoop , which means the quantity of produced classification results is equal to the number of iterations. Furthermore, these results can be later used for statistical evaluation and visualization, e.g., confusion matrix, receiver operating characteristic (ROC) curve. Last, these different advances can help to rapidly build prototypes. The graph-based class can be easily embedded in any application. The different desi gn patterns, as well as the multi-threading, support safe and reliabl e application development. It has been especially proved during the development of iKDD Wizard, a workflow tool that takes full advantage of the graph-based class features. the top-down execution, and the visualization of the results. iKDD Wizard is mainly based on the class IKDDGraph . It provides a friendly user interface to run scientific experiments. It makes use of many Java built-in technologies, such as reflection to allow an online access to the repository of components, serialization to store memory object s in a file, and also logging to record messages thrown by the system during a protocol. We make use here of the visitor pattern to be able to visualize any component. Figure 4 shows an exam ple of the workflow used for reasoning on flow cytometry data. The tool also allows users to load and save through XML the current configuration of the graph. During a process it is also possible to pause, resume and stop components that are represented internally by a thread. Finally, researchers can run the same experiment on their machine with a high-level of reproducibility given the same data and share the same configuration. We present in the next chapter, two case studies which illustrate the bene fits of iKDD for both industrial and research purposes. To test the usability of the iKDD framework we integrated a prototype for a reasoning system utilizing mass spectra data [17]. A potential scenario of application can be found in the pre-clinical area for marker identification and in the clinical area for decision support applications [19]. Mass spectrometry data have an i nherent challenge due to their size and underlying biological complexity. Typically, a mass spectra generated by state of the art machines contains 300  X  400,000 data points, whic h requires the efficient handling of the data in a relational database. The three main components necessary for handling this type of data are: We developed a relational database as a component that can be addressed within the iKDD framework; the PostgreSQL model contains the spectra data with their Mz-values, together with information about the instrument and the protocol of the experiment, among others. The preprocessing algorithms are re presented as components, and we implemented algorithms for a lignment, normalization, base-line correction and peak-detection [18]. As a machine learning algorithm we used our Bayesian Networks algorithm, which has an excellent performance in the automated analysis of non-linear causal dependencies in high-dimensional data [10]. When we implemented such an application in iKDD , we realized the solution in a short time as planned and the outcome proved to be stable and reliable. The test of the environment therefore had a positive outcome which encouraged us then to apply it to a customer related project, discussed in the next section. Micro-array technologies allow the investigation of the impact of compounds on the expression of each gene of a certain cell in parallel. This unprecedented amount of information comes with the challenge that the target of the drug, which is typically a protein, is not directly identifiable by the expression change of its related messenger-RNA, but merely by the collective changes of the downstream affected genes and their messenger-RNA concentrations [20] [21]. We addressed this challenge by developing a novel set of algorithms specifically looking for drug targets and their related metabolic pathways; during the development it was very important to quickly exchange, modify and combine in a new way existing as well as newly developed algorithms. The size and complexity of the data and customer specific technical requirements were additional boundary conditions, which came along with tight time-lines to keep the costs on track. By using iKDD and implementing algorith ms as interchangeable components, we could address th e technical and organizational challenges appropriately. The ease of use, the well-defined interfaces and strategies for the parallel development allowed a small team to quickly ramp up all required technologies in a relatively small amount of time. In the team we are aware that we could have developed also a stab le and reliable solution for this particular challenge without using iKDD , but it would certainly have required a significantly longer development period and therefore higher costs, which would have prohibited the overall project success. In this paper we outline the need for a novel approach to projects in industrial bioinformatics. The increase of the data and the complexity of the biological ques tions asked as well as the need to combine data from different modalities requires a rethinking of existing approaches and a careful analysis of new and emerging requirements. We proposed the modular and component based development of bioinformatics technologies, and outlined the analytical processes that led us to this conclusion. A collection of requirements gathered in previous projects, analysis of customer requirements, literature research and discussion of related work motivated us to define the abstr action of processes to a general level. Our approach differentia tes from existing ones by allowing the implementation of not only all types of data structures and algorithms into a single framework, but also the component based approach to describe any type of algorithm workflow which may arise in data mining in gene ral. Additional challenges for bioinformatics projects are addre ssed with a server-client based architecture, highly open interfacing to incorporate open-source libraries, a robust and at the same time versatile data handling, and interoperability by allowing all components to be exported/imported via XML. Each session configured in the system can be stored, saved and retrieved as well as exchanged with other users, which allows complete reproducibility of the data analysis process. We outlin e the details of the architecture and show by practical examples that this approach leverages industrial projects performed in a short amount of time. We are now aiming to bring this development to the next step, and briefly summarize the upcomi ng challenges which we believe are of high importance for future industrial projects. Grid based computing : the volume of the data and required computational time leads to the need of cost-efficient grid-computing infrastructures [4] [5], which provide resources for large scale data storage and processing. Our intention is to enable iKDD to run on a grid, where it allows applying a knowledge discovery process to available da ta repositories. Applications for that can be grid-based decision support systems applied in the clinical domain. Bridging of bioinformatics wi th medical and small animal imaging: Perhaps the most challenging and promising development in the area of KDD in bioinformatics is the combination of methods for analyzing molecular data with image data, stemming from humans as well as from animals. Applications of such technologies are required for supporting the development of contrast agents [22] but also in the area of pharmaceutical research to measure the impact of a drug [23]. In both areas, bioinformatics and imagi ng, the processing of the data already constitutes extremely difficult tasks; it is yet difficult to imagine how complex the combin ation of these two areas will actually be. However, if it becomes possible to achieve that goal in small steps, we may see unprecedented advances in biomedical informatics. We see participation in this area as a long term goal and are investigating potential a pproaches for pilot studies. [1] G. Larsen, Component-based enterprises frameworks, [2] F. Berzal, I. Blanco, J.C. Cubero, N. Marin, Component-[3] M. Sparling, Lessons learned through six years of [4] M. Serazi, A. Perera, Q. Di ng, V. Malakhov, W. Perrizo, [5] M. Cannataro, D. Talia, The knowledge grid, [6] J.C. Hernandez, P. Isasi, J. M. Sierra, C. Mex-Perera, B. [7] P.A. Grigoriev, S.A. Yevtus henko, Elements of an agile [8] O.Povel, C.Giraud-Carrier, SwissAnalyst, Data Mining [9] E. Gamma, R. Helm, R. Johnson, J. Vlissides, 1995, Design [10] T. Fuchs, B. Wachmann, C. Neubauer, J. Tyan, R. Krieg, Ch. [11] Taverna Project, http://taverna.sourceforge.net/ [12] Xelopes Data Mining Framework, [13] Yale Data Mining Framework, http://www-ai.cs.uni-[14] Weka Data Mining Framework, [15] BioWeka Data Mining Framework, [16] PostgreSQL OS Database, http://www.postgresql.org/ [17] Jason W.H. Wong, Gerard Ca gney and Hugh M. Cartwright, [18] Sauve A.C., Speed T.P., Normalization, Baseline Correction [19] Petricoin E.F. and Liotta , Mass Spectrometry-based [20] Di Bernardo D. et al, Chemogenomic Profiling on a [21] Marton et al., M.J., Drug Target Validation and [22] R. Weissleder and V. Ntziach ristos, "Shedding light onto live [23] Mike Ward, "Seeing is believing", BioCentury , The 
