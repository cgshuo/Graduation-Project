 HAIFENG WANG and HUA WU, Baidu The term  X  X ollocation X  was originated from Latin locare (to locate) and cum (together). In linguistics, it is used to describe a common and essential phenomenon of words col-locating in natural languages such as strong tea or make up in English. Without incor-porating collocations in speech, a speaker cannot speak fluently, as illustrated in the example of strong tea versus powerful tea [Halliday 1966]. It is a convention in English to say strong tea , though English speakers can also understand the meaning of such unconventional expression as powerful tea . In natural language processing, colloca-tions are widely used in a variety of tasks. For example, based on the observation that the words in a collocation generally have only one sense, collocations were applied to word sense disambiguation successfully [Yarowsky 1995]. In the work of Wehrli et al. [2010], collocations were used to reduce ambiguities and to find the attachments in a parse tree. In addition, collocations are adopted to improve the performance of in-formation retrieval [Hull and Grefenstette 1996], text classification [Williams 2002], and topic segmentation [Ferret 2002]. Collocations are also used in other applications, such as the disambiguation of optical character recognition [Church and Hanks 1990] and context-sensitive dictionary look-up [Michiels 2000].

In previous theoretical and applied studies on collocations, they were defined from different points of view [Seretan 2008]. In the linguistic and lexicographic view, col-locations cover a wide range of expressions from free word combinations to idiomatic expressions. In the computational linguistic view, a collocation is generally defined as a group of words that occur together more often than by chance [McKeown and Radev 2000]. In our study, a collocation is composed of two words habitually occurring as ei-ther a consecutive word sequence or an interrupted word sequence in sentences, such as by accident or take ... advice . Collocations in this article include phrasal verbs ( put on ), proper nouns ( New York ), idioms ( dry run ), compound nouns ( ice cream ), correl-ative conjunctions ( either ... or ), and commonly used combinations of patterns such as verb+noun, adjective+noun, adverb+verb, adverb+adjective, and adjective+preposition ( break rules , strong tea , softly whisper , fully aware ,and fond of ).

It is well known that, in bilingual word alignment method [Brown et al. 1993], the source word and its translation counterparts can be successfully aligned, because bilingual word pairs frequently co-occur in the bilingual sentence pairs. One of the main properties of collocation is that the words in a collocation frequently co-occur in similar contexts, as the bilingual word pairs in the word alignment. Therefore, we propose a novel Monolingual Word Alignment (MWA) method to extract two-word col-locations only from monolingu al corpus, without any addition al resources or linguistic processing. The MWA method works on monolingual parallel corpus, rather than bilin-gual corpus used by the bilingual word alignment method. The monolingual corpus is replicated to generate a parallel corpus, where each sentence pair consists of two iden-tical sentences of the same language, instead of a sentence in one language and its translation in the other. In the MWA method, we adapt the bilingual word alignment algorithm into the monolingual scenario to align potentially collocated word pairs in the monolingual sentences, with such a constraint that a word is not allowed to be aligned to itself in one sentence. In addition, we propose a ranking method to finally extract collocations from the aligned wor d pairs. This method assigns scores to the aligned word pairs using alignment probabilities multiplied by a factor derived from the exponential function on the frequencies of the aligned word pairs. The pairs with higher scores are selected as collocations.

The main contribution of this article is that the well-studied statistical bilingual word alignment method is successfully adapted for the collocation extraction from a monolingual corpus, which improves the collocation extraction performance without using any linguistic information. To evaluate the proposed method, we carried out a series of experiments on corpora of different languages.
 First, we investigate the effectiveness of the MWA method on a Chinese corpus. Experimental results show that the precisio ns of the extracted collocations are effec-tively improved by explicitly modeling the distribution of the word positions of colloca-tions and accurately estimating the number of words collocated with the same word in a sentence. These results indicate that the method is indeed appropriate for collocation extraction.

Then we compare the proposed method with previous approaches that use asso-ciation measures to extract collocations f rom the co-occurrence word pairs within a given window. Based on the same experimental data, we could conclude that the MWA method significantly outperforms these methods. Especially in our method, since the potentially collocated word pairs are used as the collocation candidates, the noise of the collocation candidates is reduced and th e precision of extracted collocations is im-proved. Moreover, our method explicitly models the word positions of collocations in a sentence, which performs better than the relative position model used in previous studies.

Finally, we manually evaluate the performance of the proposed method and the baseline methods. The results show that our method achieves precisions of 62% on a Chinese corpus and 64% on an English corpus (absolute improvements of 29% and 28%). In addition, we find from the results that 83% of long-span collocation candi-dates 1 are true, indicating that our method can extract reliable collocations of long spans. Since our method reduces the noise of collocation candidates, the collocation candidates generated are much less than those by the baseline methods. Moreover, our method also achieves higher recall than the baseline methods do.

The rest of this article is organized as follows. Section 2 describes the related work. Section 3 briefly introduces the statistical bilingual word alignment method. Section 4 describes the MWA model for collocation extraction. Experiments are shown in Sections 5, 6, and 7, followed by a discussion in Section 8. Finally, the conclusions are presented in Section 9. Many studies on collocation extraction we re carried out based on co-occurrence fre-quencies of word pairs in texts. Choueka et al. [1983] employed frequency as a mea-sure to identify collocations. In their approa ch, the collocation is restricted to adjacent sequences of two to six words which co-occur more than a given frequency thresh-old. Church and Hanks [1990] used a correlation-based metric, mutual information, to extract both adjacent and distant two-word sequences that tend to co-occur within a fixed-size window. Smadja [1993] used a collocation compiler, Xtract , to extract both adjacent and distant word sequences withi n a given window. Besides word frequency information, Xtract also took information on the relative positions of words into ac-count to ensure that the co-occurrence word p air with a narrower spread over relative positions would have higher association strength.

To effectively discover co llocations from co-occurrence words, many measures have been taken to evaluate their association strength. All of the measures can be grouped into three categories generally: frequency-based measures, hypothesis test-based measures, and information theoretic-based measures. However, the precisions of the extracted collocations with these measures do not get higher as expected. The frequency-based measures use the co-occu rrence frequency of words to find colloca-tions in a text corpus. In practice, only sele cting the most frequently occurring words as collocation does not achieve good performance, because the top of the extracted list includes many high-frequency words for example,  X  and  X ,  X  the  X , and  X  that  X , which can occur frequently with any other words. In order to improve the performance, these measures are generally used together with some heuristics derived from linguistic information such as syntactic patterns. In the information theoretic-based measures, mutual information is widely used for collocation extraction by comparing the observed number of co-occurrences with the number pre dicted by a default model which invari-ably makes independence assumptions. However, mutual information is actually a measure of independence rather than of dependence [Manning and Sch  X  utze 1999]. In the hypothesis test-based measures such as log-likelihood ratio [Dunning 1993] and t-score [Church et al. 2009], collocation extraction is expressed as to determine whether the word co-occurrence is lexically corre lated or the words co-occurs by chance ( null hypothesis). The null hypothesis can be rejected with certain confidence when the used statistic surpasses a certain threshold. This kind of measure is not always ef-fective on collocation extraction. For example, the t-test has been criticized since it assumes that probabilities are approximately normally distributed, which is not true in general [Church and Mercer 1993].

Using the aforesaid association measures, collocations can be discovered from the word pairs in a given window. To avoid combinatorial explosion, the window size is generally set to a small number. As a result, collocations with long spans can hardly be extracted. In addition, since the number of collocations is limited in a sentence, all word pairs in the given window are regarded as potential collocations, leading to a considerable number of false collocations. For example, according to the methods, more than 90% of the candidates in the Chinese corpus are false (refer to Section 7.2.2). The problems described in Section 2.1 could be partially solved by introducing more resources into collocation extraction. The part-of-speech of a word is widely used to filter collocation candidates, based on simple syntactic patterns [Breidt 1993; Justeson and Katz 1995; Smadja 1993]. However, a strong filter involving part-of-speech in-formation has a side effect on the extraction recall. Shallow parsing (e.g., chunking) information is also useful for collocation extraction, since chunking information de-scribes the structures of contiguous constituents. For example, Wermter and Hahn [2004] constructed the collocation candidates based on the PP-verb combination col-lected from the chunked text. Nevertheless, this method may miss word combinations that are in distinct constituents. Basili et al. [1994] solved this problem by employ-ing skip rules to detect long-distance dependencies between co-occurrence word pairs. There are also some methods extracting co llocations from the corpus preprocessed by a full parser [Blaheta and Johnson 2001; Lin 1998; Walde 2003; Seretan and Wehrli 2006]. For example, in Lin X  X  [1998] method, collocation candidates come from the de-pendency triples of the parsed corpus, and mutual information is used as a measure to filter the dependency triples occurring me rely by coincidence. Seretan and Wehrli [2006] employed a syntactic parser to detect collocation information from multilingual corpora, in which a pair of lexical items is selected as a candidate if there is a syntactic relation between the two.

In addition, the semantic resource has been used in the collocation extraction meth-ods. Pearce [2001] presented a collocation extraction technique based on WordNet 2 . In this method, first they used WordNet to obtain the mapping from a given word to its synonyms for each of its senses. Then the word collocating with a particular word is selected from these synonyms according to their association strength. To extract so-called synonymous collocations, Wu and Zhou [2003] also used WordNet to obtain the candidates of synonymous collocation pairs, and selected the appropriate pairs out of the candidates based on their translations in the other language.

Recently, some hybrid approaches were proposed. For example, different types of information are combined using a machine learning algorithm to produce more robust and precise multi-word expressions [Boukobza and Rappoport 2009; Ramisch et al. 2010]. Several association measures are integrated to score the strength of collocation [Pecina and Schlesinger 2006].

The linguistic resources used in these methods help to improve the performance of collocation extraction. However, not all the languages have these resources, and heavy dependence on the preprocessed information may degrade the extraction recall. In recent years, the bilingual word alignment method has been used in the extraction of multi-word or idiomatic expressions, based on the assumption that many source words aligned to one same target word tend to be a multi-word or idiomatic expres-sion [Villada and Tiedmann 2009; Caseli et al. 2010]. In these methods, the words in the bilingual sentences are first aligned by bilingual word alignment models, and the multi-word or idiomatic expressions are identified by referring to the word alignment information.

These methods employed the original bilingual word alignment models [Brown et al. 1993] to obtain the aligned words of bilingual sentences. The difference between these and our method is that we align the potentially collocated words in the monolingual corpus and extract collocations directly. There are many studies that investigated the properties of Chinese collocations [Sun 1998; Sun et al. 1997] and the extraction of Chinese collocations [Li et al. 2005; Lu et al. 2003; Piao et al. 2006; Xu et al. 2009]. For example, Sun et al. [1997] analyzed the properties of Chinese collocations on a News corpus. Lu et al. [2003] proposed a system of Chinese collocation extraction, in which the same measures as defined in the Xtract system are employed, together with linguistic information, to improve ex-traction performance. Li et al. [2005] proposed a method to extract bigram collocations using lexical statistics model with synonym information. The major difference between the method proposed in this article and the methods described before is that we model the features of collocation with the statistical meth-ods similar to the statistical bilingual word alignment. In this section, the statistical bilingual word alignment is briefly introduced.

Given a bilingual sentence pair, a source language word can be aligned to its cor-responding word in the target language. Figure 1 shows an example of Chinese-to-English word alignment.

In Figure 1, a word in one language is aligned to its counterpart in the other lan-guage. For example, the Chinese word  X   X  X  /tuan-dui team  X  is aligned to its English translation  X  X eam X , while the Chinese word  X   X  X  X  X  /fu-ze-ren leader  X  is aligned to its English translation  X  X eader. X 
The statistical bilingual word alignment method can automatically extract this kind of aligned bilingual word pairs [Brown et al. 1993]. Given a bilingual sentence pair E = e l 1 and F = f m 1 , the optimal alignments A between E and F are obtained according to the following method where A = a m 1 means that f j is aligned to e a ment method, the word alignment probability of a sentence pair is calculated according to the following three statistical models.  X  Word translation model describes the probability of translating e i to f j ;  X  position distortion model describes the probability of some position in E being aligned to some position in F ;  X  word fertility model describes the probability of the number of words that e i can align to.
 The preceding statistical models measure the possibility of alignment in differ-ent aspects. The word translation model co nsiders the co-occurrence frequencies of aligned words, the position distortion model considers the positions of aligned words, and the word fertility model prevents unrelated words from being aligned to the same word.

In practice, the word alignment method is implemented in a series of five models (Model 1  X  5). Model 1 only employs the word translation model to calculate the prob-abilities of alignments. In Model 2, both the word translation model and position dis-tribution model are used. Models 3, 4, and 5 consider the word fertility model besides the word translation model and position distribution model. These three models are similar except for the position distribution model. In Model 3, the position distortion probability depends on the aligned positions and the lengths of the source and target sentences. By contrast, in Models 4 and 5, the probability depends additionally on the aligned source and target words, as well as on the positions of any other target words aligned to the same source word. Model 5 resembles Model 4 very much, except that it is not theoretically deficient.

The models can be automatically trained from the bilingual corpus. Brown et al. [1993] suggested that these models can be estimated step by step. For example, the parameters of Model 1 are first estimated, and then the parameters of Model 2 are estimated using the parameters of Model 1 as the initial parameters. For each model, the Expectation-Maximization (EM) algorithm is employed to optimize its parameters iteratively. In the Expectation-step, the probabilities of all possible alignments are calculated using the existing parameters. And in the Maximization-step, the counts weighted by probabilities are collected and the parameters are reestimated based on the counts.

Given a bilingual sentence pair, each of the five trained models can generate an op-timal alignment sequence individually. Och et al. [2000] implemented a tool, GIZA++ 3 , for statistical bilingual word alignment, which has been successfully used in statistical machine translation. In Chinese sentence in Figure 1, there are some Chinese collocations such as (  X  X  /tuan-dui team ,  X  X  X  X  /fu-ze-ren leader ). There are also some English collocations in English sentence such as (team, leader). We separately illustrate the collocations in
Chinese sentence and English sentence in Figure 2, where the collocated words are aligned with each other.

Comparing the alignments in Figures 1 and 2, we can see that the task of monolin-gual collocation construction is similar to that of bilingual word alignment. In a bilin-gual sentence pair, a source word is aligne d to its corresponding target word, while in a monolingual sentence, a word is aligned to its collocations. The bilingual word alignment method is based on the statistical models trained with the co-occurrence frequencies of source words and target word s. And one of the main properties of collo-cation is that the collocated words co-occur frequently in similar contexts. Therefore, it is reasonable to regard collocation construction as a task of aligning the collocated words in monolingual sentences. Thus, in this article, we adapt the bilingual word alignment algorithm into the monolingual scenario to align the collocated words in a monolingual corpus.

Given a sentence with l words S = { w 1 , ..., w l } , the word alignment set can be ob-tained through maximizing the word alignment probability of the sentence, according to Eq. (2).

In a monolingual sentence, a word never collocates with itself. Thus the alignment to the word w a
In this article, we investigate the performance of three models, MWA Model 1  X  3 (corresponding to Model 1  X  3 of the bilingual word alignment method), for collocation extraction 4 . The probabilities of the alignment sequences are calculated according to Eqs. (3) X (5), respectively. Here  X  i denotes the number of words that are aligned with w i . In the MWA method, three kinds of statistical mod els are described as follows.  X  t ( w j | w a with another word in position j (or a j );  X  n (  X  i | w i )denotes the probability of the number of words that the word w i can collocate with.
 Figure 3 shows an example of word alignment on an English sentence in Figure 2(b) with the MWA method. In the sentence, the seventh word  X  X ole X  collocates with both the fourth word  X  X lay X  and the sixth word  X  X ey X . Thus, t ( w 4 | w 7 )and t ( w 6 | w 7 ) describe the probabilities that the word  X  X ole X  collocates with  X  X lay X  and  X  X ey X , respectively. with the words in positions 4 and 6 in a sentence of 12 words. For the word  X  X ole X ,  X  7 is 2, which indicates that the word  X  X ole X  collocates with two words in the sentence.
The parameters of the MWA models can be trained using the EM algorithm that is similar to that used for bilingual word alignment. In monolingual word alignment, a word should not be aligned to itself. Thus, in MWA Models 1 and 2, before estimating the probability of one word collocating with another word, their positions are checked. If two words were in the same position, they should be ignored. In MWA Model 3, if the processed words in the operation of swap or move are located at the same position, the operation is skipped. To effectively estimate the parameters, the MWA method is run from MWA Model 1 to MWA Model 3 step by step, and each model is run for several iterations 5 .
Based on the estimated parameters, for a given sentence, an optimal alignment sequence can be obtained by one of the MWA models. Specifically, MWA Model 1 or 2 can directly generate an alignment sequence, and MWA Model 3 can generate a better alignment sequence based on the alignment sequence obtained by MWA Model 2 with the hill-climbing algorithm [Al-Onaizan et al. 1999]. Given a monolingual corpus, we use the trained MWA model to align the collocated words in each sentence. As a result, we can generate a set of aligned word pairs on the corpus. According to the alignment resul ts, we calculate the frequency for the two words aligned in the corpus, denoted as freq ( w i ,w j ). Furthermore, we filtered aligned word pairs whose frequencies are lower than five. Based on the alignment frequency, we estimate the alignment probabilities for each aligned word pair as shown in Eqs. (6) and (7).
With alignment probabilities, we assign scores to the aligned word pairs and those with higher scores are selected as collocations, which are estimated as shown in Eq. (8).
The score of Eq. (8) indicates the strength of a collocation. The word pairs with high scores tend to be collocations. However, it overestimates the word pairs co-occurring infrequently, leading to the outcome that the low-frequency word pairs may be given high scores. Figure 4 shows the relationship between the alignment probabilities and frequencies of the aligned word pairs on Chinese data (refer to Section 5.1 for more details).

The results in Figure 4 imply that if Eq. (8) is directly adopted to rank the aligned word pairs, low-frequency pairs will be ranked near the top of the list. As a result, the selected collocations with higher scores may include many false collocations, while the collocations composed of habitua lly co-occurring words are missed.

To solve the preceding problem, we penalize the aligned word pairs with lower fre-quencies using a penalization factor derived from a function on the frequencies of the aligned word pairs. This penalization function y = f ( x ) should satisfy the following two conditions, where x represents the log function of frequencies. (1) The function is monotonic. When x is set to a smaller number, y is also small. This (2) When x  X  X  X  , y is set to 1. This means that we don X  X  penalize the aligned word
As described earlier, we propose to use the exponential function in Eq. (9).
This function is illustrated in Figure 5. The constant b in the function is used to adjust the shape of the line. The line is sharp with b set to a small number, while the line is flat with b set to a larger number. In this case, if b is set to a larger number, we assign a larger penalization weight to the aligned word pairs with lower frequencies.
According to the previous discussion, we can use the following measure to assign scores to the aligned words pairs generated by the MWA method. We have ties as shown in Eqs. (6) and (7). log( freq ( w i ,w j )) is the log function of the frequencies of the aligned word pairs ( w i ,w j ). In this section, the three MWA models are employed to align the potentially collocated words on a monolingual Chinese corpus. We investigated the effectiveness of the MWA method for collocation extraction through a series of experiments on a Chinese corpus. The Chinese corpus is the Xinhua News corpus of one-year (2004) from LDC 6 ,con-taining about 28 million Chinese words. To tune the parameters, we also use the Xinhua News corpus of 2003 as a development set. For example, when the constant b in Eq. (10) was tuned to 25, the algorithm of Eq. (10) achieved the best performance on the development set.

First the Chinese corpora are processed with a Chinese word segmentation system [Li et al. 2006]. Then the texts are split into sentences based on punctuations to ensure that the words in a collocation cannot occur i n different sentences, and to reduce the computational cost of the MWA models. Finally, punctuations were removed from the corpora, since they are rarely us ed to construct collocations. In order to automatically calculate collocation precision, we built a gold set by col-lecting Chinese collocations from a handcrafted collocation dictionary [Lin 2004] that contains about 89,000 common collocations. The gold set contains 46,542 two-word collocations where at least o ne word in each collocation o ccurs in the training data. Based on the gold set, the precision of extracted collocations on the Chinese corpus can be calculated according t o the following algorithm where C Top  X  N and C gold denote the top collocations in the N-best list and the colloca-tions in the gold set, respectively.

To further test the evaluation results, we use the UCS toolkit 7 to calculate con-fidence intervals for the results and significance tests for the differences of results [Evert 2004]. Figure 6 shows the precision of the top N collocations as N steadily increases with an increment of 1K, which are extracted from the results of MWA Models 1  X  3, respectively.

From the results in Figure 6, it can be seen that the absolute precision of colloca-tions are not high. For instance, among the top 200K collocation candidates, about 4% of the collocations are correct. This is because our gold set contains only about 47K collocations. In this case, even if all gold co llocations are successfully extracted, the precision of the top 200K is only 23%. Thus, it is more useful to compare the precision curves of different methods on the same test data (refer to Section 8.2 for more details). In addition, since this gold set only includes a small number of collocations, the preci-sion curves of different methods are getting closer as N increases. For example, when N is set to 200K, the MWA Models 1, 2, and 3 achieved precisions of 3.09%, 3.80%, and 4.09%, respectively. And when N is set to 400K, they achieved 1.87%, 2.32%, and 2.78%, respectively. For the convenience of comparison, we set N up to 200K in the experiments.

We calculate the confidence intervals of the results in Figure 6 when N is set to 1K, 50K, 100K, and 200K. It can be seen that the results shown in Table I are statistically significant with the increase of N.
Table II shows the statistical significance of the paired results. For all but one comparison (MWA Model 2 versus 3, N = 1K), the differences between different models are statistically significant with p &lt; 0.05.

Therefore, the results in Figure 6 show that MWA Model 3 achieves the highest precision among the methods and the precision of MWA Model 2 is higher than that of MWA Model 1. It indicates that the position distortion model and word fertility model are effective in improving the preci sion of the extracted collocations. As compared to MWA Model 1 where the positions of the aligned words are disregarded completely, MWA Model 2 considers the possibility of the word position alignment using the position distortion model. In this subsection, we investigate the effectiveness of the position distortion model by comparing the word position information of the word pairs aligned with MWA Models 1 and 2.
 We first compare the distribution of the relative positions of the aligned word pairs. To avoid any effects caused by the absolute frequency, standard deviation of the nor-malized frequency is adopted as follows. ring in the word-aligned corpus when the relative position is d ;  X  c ( w 1 ,w 2 )istheaverage value of  X  c N 1 ( w 1 ,w 2 ).

The top 200K collocation candidates are broken into 200 sets,  X  200 1 ,fromthetopto the bottom of the list. Each set includes 1000 collocations candidates. We calculate an average standard deviation for each set according to Eq. (13).
Figure 7 shows the standard deviations o f the collocation candidate sets from the alignment results of MWA Models 1 and 2. As compared to MWA Model 1, the colloca-tion candidates of MWA Model 2 have lower standard deviations. It indicates that the aligned words of MWA Model 2 tend to have a narrow spread over relative positions due to the position distortion model, while the aligned words of MWA Model 1 are apt to occur in any position of the sentence since the word position is ignored.
Next, we further analyze the effect of the position distortion model using a common collocation  X   X  X  /ju-you have  X  X  /neng-li ability  X  as a case study 8 . The occurrence numbers of the collocation in two alignment results are almost the same (71 and 70, respectively). However, their distributions are different. In the result of MWA Model 1, the relative positions are mainly in the interval of [  X  2, 7]. And in the result of MWA Model 2, the relative positions are mainly in the interval of [2, 5], similar to the observation on the parallel corpus (Xinhua News corpus of 1990 and 1991) [Sun et al. 1997]. We manually checked whether the aligned words are a collocation according to the contexts and found that the precision of MWA Model 2 is 93%, higher than that of MWA Model 1 (83%). The results indicate that the position distortion model improves the performance of MWA method. Besides the statistical models used in MWA Model 2, the word fertility model is in-cluded in MWA Model 3 to calculate the probability of the number of words that a word can collocate with. We investigated the effectiveness of the word fertility model by comparing the results of MWA Models 2 and 3.

First, we compare the performance of MWA Models 2 and 3. Here, the max fertility (denoted as  X  max )inMWAModel3issetto  X  , which means that the number of words a word collocating with in a sentence is theoretically unlimited. Figure 8(a) shows that the precision of both methods is almost the same. We further compare the distributions of the fertilities in the results to find that they are very close as shown in Figure 8(b), and there are some words collocating with more than ten words in one sentence, which is unusual in natural languages.

In general, a word only collocates wit h a few other words in a sentence. We ex-amined the true collocations in a manually labeled set (refer to Section 7.2.2) to find that 78% of words collocate with only one word, and 17% of words collocate with two words. In sum, 95% of words in the corpus ca n only collocate with two words at most. According to the preceding observation, we set  X  max to 2.

In order to further examine the effect of  X  max on collocation extraction, we used sev-eral different  X  max in our experiments. The comparison results are shown in Figure 9. The highest precision is achieved when  X  max is set to 2. The differences between the curve of  X  max = 2 and others are significant with p &lt; 0.05. This result verifies our observation on the corpus.

From the previous analysis, it can be seen that, by selecting a suitable fertility for a word, unrelated words can be effectively prevented from collocating with each other, and the performance of collocation extraction is further improved. Thus, in all of the experiments involving MWA Model 3, the maximum fertility has been set to 2. In the MWA models, the optimal alignment sequence of a given sentence is generated based on all potentially collocated words. Thus, the spans of the aligned word pairs are not limited and the MWA method can extract long-span collocations. In this sub-section, we investigate the distribution of the span of the aligned word pairs. For those that occur more than once, we calculated their average span as shown in Eq. (14). Here, Span ( w i ,w j ; s ) is the span of the aligned words w i and w j in the sentence s ; A v eSpan ( w
The distribution of the spans is shown in Figure 10. It can be seen that the number of aligned word pairs decreases exponentially as the average spans increased. About 17% of the aligned word pairs have span is longer than 6. Further evaluation results show that the precision of the long-span collocation candidates is higher than that of the short-span collocation candidates (refer to Section 7.2.1 for more details). There-fore, our method can extract high-qua lity collocations with long spans. In this section, we compare the performance of baseline methods and our methods on the same Chinese data described in Section 5.1. And the evaluation metric in Section 5.2 is used. Since our method does not include any linguistic knowledge, we compared it with the baseline methods without linguistic knowledge. These baseline methods take all co-occurrence word pairs within a given window as collocation can-didates, and then use association measures to rank the candidates. Candidates with higher association scores are extracted as collocations.

We investigated several baseline methods using different association measures 9 on the Chinese data: co-occurrence frequency, log -likelihood ratio, chi-square test, point-wise mutual information, and t-test. In the baseline methods, we filtered the word pairs whose frequencies are lower than five. To obtain the optimal performance of baseline methods, the window size is tuned from [  X  1, +1] to [  X  10, +10] based on the development set. As the window size increases, the precision of the extracted collocations decreases, and the recall increases. Here, we use f-measure to evalu-ate the performance of baseline methods. Finally, the log-likelihood ratio measure achieves the highest f-measure score when the window size is tuned to [  X  6, +6]. Thus, in the following experiments, we only show the performance of the baseline method using the log-likelihood ratio measure with the window size set to [  X  6, +6]. In this subsection, we compare the performance of MWA Model 1 and the baseline method, since both methods are only based on the co-occurrence information of word pairs. In MWA Model 1, these co-occurrence frequencies are used to estimate the collocation probabilities, while in the baseline method they are used to calculate the log-likelihood ratio.

Figure 11 shows the precision of our method and the baseline method. The precision of our method is higher than that of the baseline method (with p &lt; 0.05), when N &lt; 100K. This is because: (1) our method obtains the collocation candidates based on the whole sentence, while the baseline method constructs the collocation candidates within a given window; (2) the co-occurrence word pairs that are used as collocation candidates in the baseline method include more noise, and the baseline method cannot effectively distinguish collocations from noise. For example, if the windows size is set to m in a sentence of n words ( n &gt; m ), then the baseline method can obtain ( n  X  m +1)  X  ( m  X  1) + ( 2 m  X  1 ) collocation candidates 10 . The number of these collocation candidates is much more than that of the true collocations in the sentence. Besides the preceding baseline methods that only adopt association measures to score word pairs, there are some methods using wo rd position information for collocation extraction [Lu et al. 2003; Smadja 1993]. In these methods, the relative positions of co-occurrence words are taken into account to filter collocation candidates whose words occur equally anywhere in the given window. In this article, we refine the baseline method using the relative word position model, as follows. Here, s ( w 1 ,w 2 ) denotes the association strength of collocations calculated using the refined baseline method. s AM ( w 1 ,w 2 ) is the score of an association measure.  X  ( w 1 ,w 2 ) is the standard deviation on the relative positions of the word pair, which is calculated according to Eq. (12).  X  and  X  denote the weights. When they were tuned to 1.6 and 0.4, the best performance is achieved on the development set.

Figure 12 shows the precision of the baseline methods, the refined baseline method and MWA Model 2. The refined baseline method achieves higher precision than the baseline method does (with p &lt; 0.05), while the precision of MWA Model 2 is higher than that of the refined baseline method (with p &lt; 0.05).

The relative position model used in the refined baseline method is effective to im-prove the performance of the baseline method. Is this model as effective as the position distortion model when used in our method? To answer this question, we integrate the relative position model into our MWA Model 1. The refined MWA Model 1 measures the strength of collocati on according to Eq. (16). Here,  X  p r ( w 1 ,w 2 ) is the alignment score on the results of MWA Model 1, according to Eq. (10);  X  ( w 1 ,w 2 ) is the standard deviation of the relative positions. Here, based on the development set,  X  and  X  were manually tuned to 1.0.
 Figure 13 shows the precision of MWA Models 1 and 2, and the refined MWA Model 1. From the results, it can be seen that the precision of the refined Model 1 is slightly higher than that of MWA Model 1, but is lower than that of MWA Model 2(with p &lt; 0.05). This indicates that the position distortion model used in the MWA method is more effective than the relative position model.
 In this subsection, we investigate the effect of the corpus size on the precision of the extracted collocations. First, the whole corpus (newspaper of one year) was split into 12 parts according to the publication dates. Then we calculated the precisions as the training corpus increases part by part. T he top 20K collocations were selected for evaluation.

Figure 14 shows the experimental results. The precision of collocations extracted by our method is obviously higher than that of collocations extracted by the baseline method. The larger the size of the training corpus becomes the bigger the difference between our method and the baseline method. When the training corpus contains more than 9 parts of corpora, the precision of collocations extracted by the baseline method ceased to increase. On the contrary, the precision of collocations extracted by our method kept on increasing. This indicates the MWA method can extract more true collocations of higher quality when it is trained with data of larger size. In the automatic evaluations, the gold set only contains collocations in the existing dictionary. Some collocations closely attached to the corpus are not included in the set. Therefore, we manually evaluated the top 1K collocation candidates extracted by MWA Model 3 and the baseline method. True collocations are strictly limited to phra sal verbs, proper nouns, idioms, compound nouns, correlative conjunctions, and commonly used word combinations of patterns such as verb+noun, adjective+noun, adverb+verb, adverb+adjective, verb+prepositions and adjective+preposition. The true collocations are denoted as  X  X rue X . The false collocations were further classified into the following classes. (A) Candidates consisting of two words that are semantically related, such as (  X  X  /yi-(B) Candidates being a part of the multi-word (  X  3) collocation. For example, (  X  X  X  /zi-(C) Candidates consisting of the adjacent words that frequently occur together, such (D) Candidates in which certain two words have no relationship between one another,
The selected collocation candidates were labeled by two human judges. We use K-score [Fleiss 1981] and raw agreement to estimate the inter-annotator agreement of the labeled results. The K-score is calculated as where P 0 is the observed agreement among the judges; P c is the expected agree-ment that describes the probability that the judges agree by chance. The K-score is constrained to the interval [  X  1, 1]. K-score = 1 means perfect agreement, k-score = 0 means that agreement is equal to chance, and K-score =  X  1 means  X  X erfect X  disagreement. 7.2.1. Precision. The inter-annotator agreements are shown in Table III. From the results, K-scores on the two labeled data respectively are 0.73 and 0.78, which means that the results are  X  X ubstantial agreement X  [Landis and Koch 1977]. In our experi-ments, the data with the same labels are selected as the reference set. Thus, we got 837 and 847 labeled collocation candidates for our method and the baseline method, respectively.

Table IV shows the evaluation results 11 and the confidence intervals of the preci-sion (with p = 0.05). The precision of our method and the baseline method are 61.6% and 32.6%, respectively. Among the 516 true collocations extracted by our method, there are 25 long-span collocations such as (  X  X  /chu-yu in ,  X  X  X  /zhuang-tai state ) and (  X  X  /you-yu because ,  X  X  /yin-ci so ). Such long-span collocations were not cov-ered by the baseline method because its window size is set to 6. Since totally 30 long-span collocations (25 true collo cations included) are extracted from the 1K collocation candidates, our method achieves a higher precision of 83% on long-span collocation candidates. The results indicate that, in addition to extracting short-span collocations, our method can extract collocations with longer spans.

Classes C and D count for the most part of th e false collocations. Although the words in these two classes co-occur frequently, they cannot be regarded as collocations. And we also found that the errors of class D produced by the baseline method are much more numerous than those produced by our method. This indicates that our MWA method can remove much more noise from the frequently occurring word pairs. In Class A, the two words are semantically re lated and occur together in the corpus. These kinds of collocations cannot be distinguished from the true collocations by our method without additional resources.

As compared to the two-word collocations, there is a very small faction of multi-word (  X  3 words) collocations in the candidates. In our method, these multi-word collocations were split into two-word colloca tions, resulting in the errors of Class B. 7.2.2. Recall. Recall was evaluated on a manually labeled subset of the training cor-pus. The subset cont ains 100 sentences that were randomly selected from the whole corpus. The sentence average length is 24. All true collocations were labeled manually by two human judges. The raw agreement of two labeled results is 80%. In these ex-periments, the results with the same labels are selected as the reference (totally 586). The recall was calculated according to Eq. (18). Here, C Top  X  N denotes the top collocations in the N-best list and C subset denotes the true collocations in the subset.

Figure 15 shows the recall of collocations extracted by MWA Model 3 and the base-line method on the labeled subset. According to the significance test as described in Section 5.2, their difference is significant (with p &lt; 0.05). The results show that our method can extract more true colloca tions than the baseline method does.

In our experiments, the baseline method extracts about 20 million collocation candi-dates, while our method only extracts about 3 million collocation candidates. Although the collocation candidates extracted by our method are much less than those by the baseline, the recall of our method is higher. This again proves that our method has stronger ability to distinguish true collocations from false ones. We also manually evaluate the MWA method on an English corpus, which is a subset randomly extracted from the British National Corpus. 12 The extracted English cor-pus contains about 20 million words. The English corpus was preprocessed using the same manner as done on the Chinese corpus. First the texts are split into sentences based on punctuations, and then the sentences are tokenized and lowercased. Finally, punctuations were remo ved from the corpora. 7.3.1. Precision. We estimated the precision of the top 1K collocations taking the same steps as done on Chinese data. Table V shows the interannotator agreement of two human judges, which indicates substantial agreement. Table VI shows the evaluation results 13 and the confidence intervals of the precision ( p = 0.05). The results show that our methods outperformed the baseline method using the log-likelihood ratio measure. And the distribution of the false collocations is similar to that on the Chinese corpus. 7.3.2. Recall. We used the method described in Section 7.2.2 to calculate the recall. 100 English sentences were labeled manually and the raw agreement is 82%. Finally, 182 collocations were obtained. Figure 16 sh ows the recall of the collocations in the N-best lists. Our method significantly outperforms the baseline method (with p &lt; 0.05). From the figure, it can be seen that the trend on the English corpus is similar to that on the Chinese corpus. Like the association measures, the MWA method only uses word information and dis-cards any language-dependent resources or linguistic processing. But it essentially differs from the association mea sures in the following aspects:
Firstly, the association measures estimat e the strength of collocation only based on co-occurrence frequency of word pairs or fre quency of words. Our method extracts the potentially collocated words in a sentence by using the MWA models. This method explicitly models the collocation probability of two words in a sentence, based on not only the co-occurrence information of lexical tokens, but also the position of words and the fertility of words in a sentence. The collocation probability of a word pair in our method depends on its contexts, besides its co-occurrence frequency. Although position information can be also combined with the association measures, they are indepen-dently used. In our method, the position information is integrated into a statistical model and interacts with other information for collocation extraction. Moreover, the fertility of words is introduced in collocation extraction for the first time. This feature effectively prevents one word from being c ollocated with other irrelevant words. In addition, we integrate word collocation probability, position distortion probability, and word fertility probability in one single model to search for the potentially collocated word pairs in a sentence, rather than use them in a cascade model. The experimental results indicated that the integrated model (MWA Model 3) significantly outperforms both the baseline methods and MWA Model 1 that only employ co-occurrence informa-tion of lexical tokens.

Secondly, from the view of algorithm, asso ciation measures estimate collocation strength based on information theory or hypothesis test, which only consider two sub-ject words in a collocation. However, our method employs the EM algorithm to globally optimize the parameters of the models. And based on the optimized parameters, the MWA models can accurately estimate the collocation probabilities of the word pairs in a sentence. For example, although MWA Model 1 uses the same statistical information as the baseline methods to model the strength of collocation, MWA Model 1 achieved better performance.

Finally, for a given sentence, our method can generate an optimal set from all poten-tial word pairs in a whole sentence, while the association measures generally choose co-occurrence word pairs that appear in a spe cified window as collocation candidates. Thus, our method effectively reduces the noise of collocation candidates and improves the precision of the extracted collocations. For example, although the number of candi-dates extracted by our method is only 15% of that extracted by the baseline methods, our method achieved higher precision and recall. Moreover, our method extracts high-quality collocations with long spans. From the results in Figure 6, the precision are about 4  X  25% among the top 200K collocation candidates. It seems that the precision are so low that the proposed method can hardly be adopted in practice. This is mainly because the gold set contains only about 47K collocations. In this case, even if all collocations in the gold set are included in the 200K-best list, the precision could reach only 23%. In fact, the gold set includes such common collocations that only cover a small number of collocations extracted from the training data, as confirmed by human evaluation in Section 7.2.1. In the human evaluation, the precision in the top 1K collocation candidates reaches 61.6%, which is much higher than that in the gold set.

Although the precision against the gold set are not high, the significance tests indi-cate that the differences between methods are statistically significant. Therefore, the evaluations based on the gold set are meaningful.

In the previous work, the precisions of the collocations extracted by the baseline methods were similar to ours. For example, Smadja [1993] reported that only 20% in a subset including 4000 candidates are good collocations 14 . In the experiments de-scribed in Sun et al. [1997], the precision on the extracted collocations is only 33%. Based on the baseline methods, the precision can be further improved by introducing linguistic information to filter noise, or by adopting manually labeled experimental data. Similarly, we can also improve the precision by doing so. We will address it in future work. We have presented a monolingual word alignm ent method to extract collocations from monolingual corpus. We first replicated the m onolingual corpus to generate a parallel corpus in which each sentence pair consists of two identical sentences of the same lan-guage. Then we adapted the bilingual word alignment algorithm into the monolingual scenario to align the potentially collocated word pairs in the monolingual sentences. In addition, a ranking method was proposed to finally extract the collocations from the aligned word pairs by scoring collocation candidates with alignment probabilities multiplied by a factor derived from the exponential function on the frequencies. Can-didates with higher scores are selected as collocations. Both Chinese and English collocation extraction experiments indicate that our method outperforms the baseline approaches in terms of both precision and recall. According to the human evaluations on the Chinese corpus, our method achieved a precision of 61.6% that is much higher than that of the baseline method (32.6%).

In addition, the investigations on the position distortion model show that this model accurately describes the distribution of th e word positions of collocations. The preci-sion of the monolingual alignments is improved by the position distortion model. And the performance of the position distortion model is better than the relative position model used in previous work.

The word fertility model is introduced into collocation extraction methods for the first time. This feature prevents many irre levant words from collocating with one word. The experiments show that this model is effective to further improve the quality of the extracted collocations when suitable maximum fertility is set.

Moreover, our proposed method can extract collocations with longer span. Human evaluation on the Chinese collocation extraction shows that 83% of the long-span col-location candidates are correct.

