 The performance of OLAP is always the most critical issue due to the large data volume which grows from TB level to PB level. Our target is to leverage the big problem of OLAP performance by optimizations with the support of advanced hardware. OLAP performance includes performance for query processing on large data volume, concurrent user query workloads and complex queries. Performance can be improved by three levels: hardware support, optimized storage model and high efficiency query processing algorithms. 
Hardware support is once considered as free lunch for databases. Two important factors stopped the free lunch, one is memory wall the other is multi-core architecture. Memory wall is generated by the unbalanced latency of CPU, main memory and hard disk. The parallelization of traditional database is limited and multi-core resource can not be automatically utilized by database software without changing inner design. 
OLAP is data-intensive application. Storage model decides the physical data layout of database which influences the performance of data retrieval and query processing. Researches of data model can be classified into five types. (1) Row-wise model. It is optimized for update and dense access(width of all accessed attributes/tuple width&gt;  X  , e.g.  X  &gt;75%). On the contrary, for sparse access, only very small proportion of whole tuple is to be accessed, cache utilization rate is low. (2) Column-wise model. Column-wise model stores data densely and independently, independent columns generate table, too many joins upon large columns affect performance remarkably. Typical analytical DBMSs, e.g., C-Store[1], Sybase IQ[2] and MonetDB[3] employ column-wise model. (3) PAX[4] model. It is tradeoff for I/O and memory bandwidth performance. In each page, data is organized as column-wise model, so PAX acts as row-wise model when updating with only one I/O cost and it acts as column-wise model when accessed in memory. (4) Clustered model[5]. Correlative columns are clustered stored to gain high cache hits and reduce joins between independent columns. This model can be seen as light-weight row-wise model for dense tables. (5) Row-wise simulating column-wise model. [6] proposed that by simulating column-wise model in a row-wise database, the features of compression and column accessing can be achieved without cost of data migration. Most researches focused on data access performance and didn X  X  pay much attention to the OLAP schema and query feature, how to combine OLTP and OLAP system as one is still un-answered. 
For the multi-core era, the keys of improving performance are to promote data locality and to simplify query processing, the too complex SQL engine limits the parallelization of OLAP. 
So it is time to re-think whether the database should be re-designed according to features of data-intensive workload and characters of advanced hardware. In this paper, we propose a reduced OLAP engine MOSS-DB(Memory Optimizing Star Schema Database) for star schema OLAP applications. Storage model, data retrieval, query processing and concurrent query workload are all optimized based on star schema features and hardware features. The large but not large enough main-memory is utilized for frequently accessed column-wise dimensional attributes. Large fact table still employs traditional row-wise(N-Array) storage engine, the optimized query processing algorithm named DDTA-JOIN is the key for simplifying complexity of OLAP query processing and optimizing for multi-core scenario. 
Our contributions are as follows: 1. We present a hybrid-storage model for optimizing key performance factor and 2. We simplify query processing with the support of memory optimizing 3. We use light-weight compression and pr edicate vector to improve predicate 4. We develop DDTA-JOIN algorithm for multi-core algorithm, an OLAP engine 
The OLAP model analysis and related work are presented in Sec.2. Algorithms are discussed in Sec.3. Sec.4 shows the results of experiments. Finally, Sec.5 summarizes the paper and discusses future work. First of all, we analyze the features of data warehouse and OLAP workloads and then discuss the ideal OLAP model with support of state-of-the-art hardware. 
In Data Warehouse, star schema and snow-flake schema are typical schemas, snow-flake schema can be considered as extended star schema with more hierarchical dimension tables and more join cost for OLAP query processing. The schema is Dimensional attributes in fact table are foreign keys referenced to multiple dimension tables. The workload upon fact table and dime nsion tables is skewed too. So the ideal storage device and dimension tables should employ small but high performance random access storage device. 
Blink[7] used de-normalized schema with machine-word sized banks to simplify query processing and improve performance of predicate processing. [8] explored good parallelization on multi-core CPU for its de-normalized pre-join table scan. In Star schema, dimensional attributes in fact table are all foreign keys referenced to primary de-normalized table. Figure 1 illustrates the difference of de-normalized schema and star schema, de-normalized schema is simple for query processing and easily to be Star schema is optimized for storage efficiency but involved more join cost. So the ideal schema should have both good storage performance as normalized star schema and simple but efficient query processing performance as de-normalized schema. 
The ideal storage model in OLAP should be hybrid model in which large sequential correlative attributes in fact tabl e are row-wise stored and the small size but large amount dimensional attributes are co lumn-wise stored. For storage hierarchy, hard disk provides large capacity sequential storage, SSD and RAM can support moderate capacity for random accessing with higher performance, the hierarchical cache levels(L1,L2,L3 caches) provide even higher performance with small capacity. possible bottlenecks may be shared cache and memory bandwidth which multi-cores may contend same resource simultaneously. 
So the ideal strategy is to store small but most frequently accessed data in main memory and make data set even smaller for resident possibility in cache, sequential scanned data should be allocated within small address space and the frequently accessed data set should be allocated within more address space to promote cache hits. Query processing should be simplified as uniform linear processing for maximum parallelization and the large data accessing should be shared for multiple users to reduce conflicts for bandwidth. Most of all, the storage model should keep compatible with traditional commercial DBMS and simplify SQL engine for OLAP processing. MOSS-DB(Memory Optimizing Star Schema Database) system employs two-level storage model, the large fact table uses traditional row-wise storage engine to support increasing large data volumes, dimension tables are stored in row-wise tables too but during query processing, dimension tables are loaded into main memory to form compressed column-wise memory dimension tables. 
In MOLAP(Multi-dimensional OLAP) model, dimension tables are stored as ROLAP(Relational OLAP), measure data of each fact tuple is stored in a multi-dimensional array item, the multi-dimensional array indexes act as dimensional three dimension tables, m , n and p denote boundaries of each dimension table, dimension tuples D1 [ i ], D2 [ j ], D3 [ k ]. In MOSS-DB model, we inherit this feature for our column-wise memory dimension table, each dimension table is divided into multiple arrays with same boundary. In SSB dataset, primary keys of 3 dimension tables are all incremental sequences as 0,1,2..., so primary key can be replaced by memory array index. Primary key of dimension table DWDATE is incremental sequence as 19920101, 19920102, 19920103..., we can follow two rules:(1) employ a mapping function D_Map ( x )= date ( x )-date (19920101). Each primary key can be on-the-fly mapped to array index of dimension table DWDATE by D_Map ;(2) reserve current primary key and use array index as new primary key, replacing dimensional attributes in fact table with array index. The update strategy of column-wise memory dimension table also inherits dimension table in MOLAP model. Fact table is stored with ROLAP model to compress space cost and make MOSS-DB scalable for increasing data volume. 
Compared with DRDB such as PostgreSQL, MOSS-DB fully utilizes main memory with more efficient MMDB enabled query processing algorithms. When compared with MMDB such as MonetDB, because of skewed data volume for dimension tables, main memory capacity is no longer limitation for in-memory query processing. For general purpose OLAP workloads, for example SSB [10] (improved TPC-H benchmark with single fact table) and FoodMart benchmark, fact table is dense with narrow fact table width and more correlative attributes. Our hybrid-model can optimize dimensional attribute accessing than row-wise model and can also optimize fact table accessing by eliminating join cost of large fact attributes than column-wise model. 
Figure 2 shows the storage architecture of MOSS-DB, for upper level, data size should be even small for better locality. 3.1 Na X ve DDTA-JOIN Algorithm With array modeled column-wise memory dimension table, join operation of hash probe or index probe is simplified to address access with mapped array index. So the costly hash table or index is no longer needed. The whole query processing is divided into three phases as figure 3. In first phase, we define operator DDTA-JOIN(Directly Dimensional Tuple Access JOIN) as following procedure: (1) sequence scan fact table for each fact tuple; (2) for each predicate expression in traversa l path of predicate tree (optimized (3) for each fact tuple with TRUE result of predicate expressions, fetch the project (4) produce output tuple for the following operations. 
In DDTA-JOIN procedure, star schema tables are used as virtual pre-join table, the order of predicate expressions decides how many dimensional attributes will be accessed. We optimize predicate expression order by predicate tree structure based on cost model with selectivity, data type, data width, predicate computing cost, data padding, etc. The sophisticated cost model will be in our further research. 
In second phase, each output tuple is pipelined to hash group-by function for grouped aggregating. For most cases, the group-by result set is very small due to cardinality of group-by attributes. 
In third phase, when all the result tuples are aggregated, the order-by function is invoked upon group-by result set to re-order final result set. 3.2 Light-Weight Compression Optimization for DDTA-JOIN store_id=1  X  store_state [ 1 ]=  X  X uerrero X  ), and then the attribute value is read for predicate computing. The dimensional attr ibute values are read repeatedly during sequence scan on fact table. Many dimensional attributes of higher hierarchy are long VARCHAR data type, so compression on column dimensional attribute can save much space for main memory. Figure 4 illustrates light-weight dictionary compression for in-memory column, it is suitable for low cardinality columns. When each short data for predicate processing. Although compressed column and dictionary table is performance. [11] discussed the order-preserving string compression for directly same order, so the equal and range condition can be re-written as condition on compressed data but LIKE and many complex condition can X  X  be re-written. We employ order-preserving string compression in MOSS-DB, and compare the performance in experiment section. [12] proposed Invisible Join to reduce join cost for fact table columns. In MOSS-the main cost is dimensional attribute accessing and predicate processing. We use bitmap as predicate vector to indentify whet her certain value satisfies the predicate. In figure 4, store_state= X  X A X  is performed on column store_state to generate predicate vector 001100 ..., then predicate is re-written as pred_vector=1 . Without predicate vector, predicate computing is performed over and over, now we perform predicate computing only once and DDTA-JOIN performs simple vector read instead of original complex predicate computing. Predi cate vector can be used in any predicate processing, whatever attribute cardinality is hi gh or low. If we store predicate vector consumed for bit operation. Details are discussed in experiment section. 3.3 Parallel DDTA-JOIN for Multi-core System Parallelization is limited by hardware features and software design. Parallel accessing on sequential storage such as hard disk produces frequently track seek actions which makes performance even poor. Random access storage such as main memory is suitable for parallel accessing, but memory bus is shared for parallel processes. 
Now we discuss the parallelization of DDTA-JOIN enabled query processing with three phases, DDTA-JOIN, group-by and order-by. (1) DDTA-JOIN. DDTA-JOIN is sequence scan based join algorithm, there are two kinds of locality as [13] classified as strong and weak locality. Scan on fact table is weak-locality without tuple re-read, dimensional attribute data is frequent dataset with strong-locality. For complex OLAP queries, DDTA-JOIN has to access multiple dimensional attributes simultaneously, so multiple dimensional columns and fact table contend for shared cache. The ideal strategy is to employ cache partition technique to reduce cache pollution by table scan, which requires support from the OS domain. (2) Group-by. Group-by merges large amount result tuples to small aggregate tuple set. [14] summarized four methods of enabling parallelism in the writing of tuples to output partitions. For hash group-by operation, independent output buffer can reduce conflicts between threads, it needs additional global group-by result merge procedure buffer for concurrent group-by processing, the performance suffers from write conflicts from each thread. In SSB, the group-by result sets vary from 1 to 800, so we select independent output buffer mode to guarantee maximum parallelization. (3) Order-by. Since the target of order-by is small aggregate result set, we perform order-by operation on hash group-by result set without parallel processing. 
In MOSS-DB, large fact table is resident on hard disk as pages as figure 5, when data are loaded into main memory, fact table is transformed into sequential buffered data, the whole data can be divided into partitions based on available cores. Dimensional columns and predicate vectors are shared for different threads. Each thread performs DDTA-JOIN algorithm and generates local group-by result set. Then the group-by result sets are merged together for final aggregate results. Finally, query result set is ordered based on order-by clause. According to Amdahl's law, maximum parallelization must be achieved by minimizing sequential fraction of the process. For DDTA-JOIN, the costly join operations are all sequentially processed, the parallelization can be maximized. 
For parallel query processing, another critical performance issue is memory bandwidth, in experiments we find that because of simplified query processing, OLAP query is no longer CPU bound but memory bandwidth bound, when more parallel processes are performed, memory bandwidth becomes bottleneck of system. 
For concurrent OLAP workloads, traditional DBMS perform inter-parallel processing, [14] optimized co-runner queries by locality strength of queries for minimum cache conflicts. For intra-paralle l processing, cores are dispatched to parallel query processing threads, so we can maintain a queue for concurrent query workloads and submit queries one by one for parallel processing. Table scan of OLAP is a costly procedure due to hard disk latency, memory bandwidth limitation makes CPU latency even larger, the cores have much idle processing resources. To balance memory bandwidth and CPU processing capacity, concurrent queries should be combined into intra-parallel query group. [8] grouped concurrent queries for sharing scan, each tuple serves for multiple queries. [15] serialized query processing to pipelined hash filters, during heavy OLAP workloads, fact table is looped scanned, queries can join processing group dynamically, each fact tuple goes through filters to perform predicate clauses and results are distributed to aggregate processors. Bit-vector is used for identifying queries. In MOSS-DB model, query processing is uniform linear tuple reading and address accessing, concurrent queries can be pushed down to parallel DDTA-JOIN operator with multiple predicate expressions. Figure 6 illustrates the concurrent parallel DDTA-JOIN processing. 
Intra-parallel reduces query processing latency, inter-parallel reduces user response time and improves CPU utilization rate. Static concurrent query task group can be easily pushed down to intra-parallel query processing, but dynamic concurrent query group has to maintain meta data for each processing threads, it is hard to synchronize multiple meta data replicas. MOSS-DB supports both MMDB model and DRDB model. With multi-core support, the processing capacity becomes more powerful. To balance the processing power, main memory capacity must be even larger to overlap hard disk latency. So MOSS-DB considers tow typical scenarios, one is MMDB model, the other is DRDB model with large main memory support. We also design experiments for these two models. 4.1 Main Memory MOSS-DB We perform our experiments on a HP ML350 server with two Intel Xeon Quad-core CPUs of 1.60 GHz, running Ubuntu 4.3.3-5 with 6GB RAM and 80GB hard disk. In each CPU, every two cores share 4MB L2 cache. We use SSB data generator to create 4GB dataset with 24 million fact tuples. There are 17 attributes in fact table and only 7 attributes are utilized in all 13 queries, only 47.26% width of fact tuple is effective for cache reading. So we use two fact tables for experiments one is original fact table, the other is clustered fact table with only 7 accessed attributes in testing queries. A. Na X ve DDTA-JOIN algorithm We compare our DDTA-JOIN algorithm with open-source column-wise main memory database MonetDB(version:May2009-SuperBall-SP2) which stores each attribute as independent BAT with OID and VALUE pairs, relational algebra is translated into BAT algebra with uniform BAT functions. It outperforms traditional row-wise database for OLAP scenario with sparse attribute accessing, but join cost will increase rapidly with more large fact a ttributes or higher selectivity of predicate. Figure 7(a) compares four groups of queries, MonetDB-join represents re-written SSB queries without group-by and order-by clauses which focuses on the most critical performance of join. MonetDB-full represents standard SSB queries, we can see that these two kinds of queries have very tiny difference due to small carnality of group-by result set, so we perform the rest experiments with SSB-J benchmark(join performance test). DDTA-JOIN denotes na X ve algorithm with directly dimensional tuple accessing, and clusDDTA-JOIN denotes qu ery process on clustered fact table. 
We can see that narrow fact table improves join performance because of better cache performance. In 13 SSB queries, DDTA-JOIN outperforms MonetDB for 10 queries, MonetDB works better in Q3.2~Q3.4 because filer factor for select predicates larger string comparison cost than MonetDB. B. Compression for DDTA-JOIN algorithm Figure 7(b) shows the speedup of compression. Light-weight dictionary compression can save space cost for VARCHAR type column stored dimensional attributes but additional CPU cost for de-compressing deteriorates the whole performance. We perform process directly on dictionary compressed data for equal and range predicates, in which keys are retrieved on dictionary table first and then re-write query with matched compressed values. The results show that the compressed and frequently accessed dimensional attribute can improve performance very much. 
Figure 7(c) tests the predicate vector pe rformance with Q3.4. Predicate vector acts as mediate BAT in MonetDB, the difference is that MonetDB join mediate predicate BAT with other BAT based on equal OIDs while DDTA-JOIN uses predicate vector as directly memory addressed short flags without lookup cost. We design two kinds of additional CPU cost for parsing certain bit from the whole bit vector. The other uses short data type to store predicate vector, for example char [] with values of  X  y  X  or  X  n  X , although storage cost is larger, the CPU parsing cost is saved, performance can be further improved. We can see that for SSB workload, predicate vector outperforms other algorithms with compressed columns, clustered fact table and MonetDB.
 C. Parallel DDTA-JOIN algorithm DDTA-JOIN is easily parallelized for in-memory sequential fact table, when dividing fact table into required parallel partitions by available cores, each thread performs DDTA-JOIN individually. Figure 7(d) illustrates the parallel performance of Q1.1 for different conditions. Quad-core CPU shares L2 cache between two cores, if parallel tasks are less than total amount of L2 caches, each thread is located for individual core tasks increase, different threads have to contend the shared L2 cache. We extend Q1.1 to four queries, Simple(reduce predicate expressions), morePred(add more predicate expressions), largePred(maximize predicate complexity), moreComprPred(maximize compressed predicate complexity). Performance curve of AR-largePred is most close to linear acceleration rate, when amount of threads exceeds four, the curve declines because of conflicts in shared L2 caches. The rest four types slowly increase as thread amount increases. We can conclude that, DDTA-JOIN with less complex predicate processing is memory bandwidth bound while with complex predicate processing is CPU bound. CPU bound workload can achieve better parallelization. 
Figure 7(e) measures parallel performance of 6 queries, we observe that queries with more costly predicate processing e.g. Q2.3 achieve better parallel performance. Figure 7(f) illustrates the parallel performance of grouped concurrent queries, we create groups with 2,3,4,5 queries to simulate different concurrent workloads. We can see that parallel performance increases as workload grows. For SSB workload, DDTA-JOIN simplifies complexity of query processing, the memory bandwidth is more critical for parallel performance, grouped concurrent query workload increases CPU cost and balances CPU and memory bandwidth consuming. 4.2 Disk Resident MOSS-DB For large dataset, we employ tradition row-wise storage engine to store large fact table and dimension tables. Dimension tables are loaded into main memory to form memory column dimension tables and resident in memory during system running. We extend PostgreSQL by embedding our DDTA-JOIN algorithm in table scan module, when tuples are fetched from disk, DDTA-JOIN carries out the rest query processing work. By this, we transform the ordinary SQL engine into memory optimizing OLAP engine by embedding DDTA-JOIN operator for special star schema OLAP workload. 
We use 24GB dataset for performance test in which there are 143999787 fact tuples, the amounts of four dimension tables are 1000000, 720000, 240000 and 2556. Figure 8 compares performance of PostgreSQL and disk resident DDTA-JOIN, the average response time improves 19%, we get maximum acceleration of 76.84% on Q4.1. Predicate vector normalizes different predicate processing co st into equal cost, the performance of DDTA-JOIN is stable even when selectivity is high while PostgtreSQL consumes large space and time cost for hash tables and proves poor performance with high selectivity. I/O latency overlaps time cost of hash join for PostgreSQL, with 4GB dataset, average execution time is also shorten by 13.5% in 10 queries with low selectivity. DDTA-JOIN is stable algorithm for various OLAP workloads even for more complex queries with higher selectivity. There are many researches on OLAP performance, main memory and multi-core optimizations, most of these works sound well but hard to realize because of the complexity and efficiency, we often ask ourselves  X  X hy so complex? Can it be simpler? X  By analyzing the features of star schema, workload and hardware, we build virtual pre-joined single table for OLAP with DDTA-JOIN support, so the complex query plan tree is transformed into simp le tuple reading, addressed accessing and predicate processing with complexity of O ( n ). Compression and predicate vector further optimize cache performance with less data size by reducing cache conflicts. Furthermore, DDTA-JOIN is easily deployed on multi-core platform by evenly dividing the whole fact tuples into several parallel partitions, only small proportion of global group-by result sets merging and orde r-by operations are serially processed. We also discuss and measure the concurrent query processing strategy for balanced OLAP engine. We owe our major contribution to the simplicity of star schema OLAP processing which can be easily embedded into SQL engine and make SQL engine performance between hard disk sequential read, memory access and CPU processing. This work is supported by 863 Project of China under Grant No.2009AA01Z149; the joint research of Large Scale Data Management for HP Lab China and Renmin University; the project of Education committee of Beijing(in-memory Online Analytical system); the Graduate Science Foundation of Renmin University of China No.08XNG040 and No. 10XNH096; the Funding of Renmin University of China No. 10XNB053. 
