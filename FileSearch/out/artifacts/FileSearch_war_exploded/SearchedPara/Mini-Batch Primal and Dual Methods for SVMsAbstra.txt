 Martin Tak  X a X c martin.taki@gmail.com Avleen Bijral abijral@ttic.edu Peter Richt  X arik peter.richtarik@ed.ac.uk Nathan Srebro nati@uchicago.edu Stochastic optimization approaches have been shown to have significant theoretical and empirical advan-tages in training linear Support Vector Machines (SVMs), as well as in many other learning applica-tions, and are often the methods of choice in prac-tice. Such methods use a single, randomly cho-sen, training example at each iteration. In the con-text of SVMs, approaches of this form include pri-mal stochastic gradient descent (SGD) (e.g., Pega-sos, Shalev-Shwartz et al. 2011 , NORMA, Zhang 2004 , and dual stochastic coordinate ascent (e.g., SDCA, Hsieh et al. 2008 , RCDM, Richt  X arik &amp; Tak  X a X c 2013 ). However, the inherent sequential nature of such ap-proaches becomes a problematic limitation for parallel and distributed computations as the predictor must be updated after each training point is processed, pro-viding very little opportunity for parallelization. A popular remedy is to use mini-batches . That is, to use several training points at each iteration, instead of just one, calculating the update based on each point sep-arately and aggregating the updates. The question is then whether basing each iteration on several points can indeed reduce the number of required iterations, and thus yield parallelization speedups.
 In this paper, we consider using mini-batches with Pegasos (SGD on the primal objective) and with Stochastic Dual Coordinate Ascent (SDCA). We show that for both methods , the quantity that controls the speedup obtained using mini-batching/parallelization is the spectral norm of the data .
 In Section 3 we provide the first analysis of mini-batched Pegasos (with the original, non-smooth, SVM objective) that provably leads to parallelization speedups (Theorem 1 ). The idea of using mini-batches with Pegasos is not new, and is discussed already by Shalev-Shwartz et al. ( 2011 ), albeit without a theoret-ical justification. The original analysis does not ben-efit from using mini-batches X  X he same number of it-erations is required even when large mini-batches are used, there is no speedup, and the serial runtime (over-all number of operations, in this case data accesses) increases linearly with the mini-batch size. In fact, no parallelization speedup can be guaranteed based only on a bound on the radius of the data, as in the orig-inal Pegasos analysis. Instead, we provide a refined analysis based on the spectral norm of the data. We then move on to SDCA (Section 4 ). We show the situation is more involved, and a modification to the method is necessary. SDCA has been consistently shown to outperform Pegasos in practice ( Hsieh et al. , 2008 ; Shalev-Shwartz et al. , 2011 ), and is also popu-lar as it does not rely on setting a step-size as in Pegasos. It is thus interesting and useful to obtain mini-batch variants of SDCA as well. We first show that a naive mini-batching approach for SDCA can fail, in particular when the mini-batch size is large relative to the spectral norm (Section 4.1 ). We then present a  X  X afe X  variant, and an analysis that estab-lishes the same spectral-norm-dependent paralleliza-tion speedups as for Pegasos (Section 4.2 ). Similar to a recent analysis of non-mini-batched SDCA by Shalev-Shwartz &amp; Zhang ( 2012 ), we establish a guar-antee on the duality gap, and thus also on the sub-optimality of the primal SVM objective, when us-ing mini-batched SDCA (Theorem 2 ). We then go on to describe a more aggressive, adaptive, method for mini-batched SDCA, which is based on the analysis of the  X  X afe X  approach, and which we show often outper-forms it in practice (Section 4.3 , with experiments in Section 5 ).
 For simplicity of presentation we focus on the hinge loss, as in the SVM objective. However, all our results for both Pegasos and SDCA are valid for any Lipschitz continuous loss function.
 Related Work. Several recent papers consider the use of mini-batches in stochastic gradient descent, as well as stochastic dual averaging and stochas-tic mirror descent, when minimizing a smooth loss function ( Dekel et al. , 2012 ; Agarwal &amp; Duchi , 2011 ; Cotter et al. , 2011 ). These papers establish paral-lelization speedups for smooth loss minimization with mini-batches, possibly with the aid of some  X  X cceler-ation X  techniques, and without relying on, or consid-ering, the spectral norm of the data. However, these results do not apply to SVM training, where the ob-jective to be minimized is the non-smooth hinge loss. In fact, the only data assumption in these papers is an assumption on the radius of the data, which is not enough for obtaining parallelization guarantees when the loss is non-smooth.
 Our contribution is thus orthogonal to prior work, showing that it is possible to obtain parallelization speedups even for non-smooth objectives, but only with a dependence on the spectral norm. We also an-alyze SDCA, which is a substantially different method from the methods analyzed in these papers. It is inter-esting to note that a bound of the spectral norm could perhaps indicate that it is easier to  X  X mooth X  the ob-jective, and thus allow obtaining results similar to ours (i.e. on the suboptimality of the original non-smooth objective) by smoothing the objective and relying on mini-batched smooth SGD, where the spectral norm might control how well the smoothed loss captures the original loss. But we are not aware of any analysis of this nature, nor whether such an analysis is possible. There has been some recent work on mini-batched coordinate descent methods for  X  1 -regularized prob-lems (and, more generally, regularizes by a sepa-rable convex function), similar to the SVM dual. Bradley et al. ( 2011 ) presented and analyzed SHOT-GUN, a parallel coordinate descent method for  X  1 -regularized problems, showing linear speedups for mini-batch sizes bounded in terms of the spectral norm of the data. The analysis does not directly ap-ply to the SVM dual because of the box constraints, but is similar in spirit. Furthermore, Bradley et al. ( 2011 ) do not discuss a  X  X afe X  variant which is ap-plicable for any mini-batch size, and only study the analogue of what we refer to as  X  X aive X  mini-batching (Section 4.1 ). More directly related is recent work of Richt  X arik &amp; Tak  X a X c ( 2013 ; 2012 ); Tappenden et al. ( 2013 ); Richt  X arik &amp; Tak  X a X c which provided a theoreti-cal framework and analysis for a more general setting than SHOTGUN, that includes also the SVM dual as a special case. However, guarantees in this framework, as well as those of Bradley et al. ( 2011 ), are only on the dual suboptimality (in our terminology), and not on the more relevant primal suboptimality, i.e., the suboptimality of the original SVM problem we are in-terested in. Our theoretical analysis builds on that of Richt  X arik &amp; Tak  X a X c ( 2012 ), combined with recent ideas of Shalev-Shwartz &amp; Zhang ( 2012 ) for  X  X tandard X  (se-rial) SDCA, to obtain bounds on the duality gap and primal suboptimality. We consider the optimization problem of training a linear 1 Support Vector Machine (SVM) based on n la-beled training examples { ( x i , y i ) } n i =1 , where x and y i  X   X  1. We use X = [ x 1 , . . . , x n ]  X  R d  X  n note the matrix of training examples. We assume the data is normalized such that max i k x i k  X  1, and thus suppress the dependence on max i k x i k in all results. Training a SVM corresponds to finding a linear predic-tor w  X  R d with low  X  2 -norm k w k and small (empir-ical) average hinge loss  X  L ( w ) := 1 n P n i =1  X  ( y i where  X  ( z ) := [1  X  z ] + = max { 0 , 1  X  z } . This bi-objective problem can be serialized as where  X  &gt; 0 is a regularization trade-off parameter. It is also useful to consider the dual of ( 1 ): Q is the Gram matrix of the (labeled) data. The (pri-mal) optimum of ( 1 ) is given by w  X  = 1  X n P n i =1  X   X  where  X   X  is the (dual) optimum of ( 2 ). It is thus nat-ural to associate with each dual solution  X  a primal solution (i.e., a linear predictor) We will be discussing  X  X ini-batches X  of size b , repre-sented by random subsets A  X  h n i := { 1 , 2 , . . . , n } of examples, drawn uniformly at random from all sub-sets of h n i of cardinality b . Whenever we draw such a subset, we for simplicity write A  X  Rand( b ). By Q
A  X  R b  X  b we denote the submatrix of Q correspond-ing to rows and columns indexed by A , v A  X  R b to denote a similar restriction of a vector v  X  R n , and v [ A ]  X  R n for the  X  X ensored X  vector where entries in-side A are as in v and entries outside A are zero. The average hinge loss on examples in A is denoted by Algorithm 1 Pegasos with Mini-Batches Input: { ( x i , y i ) } n i =1 ,  X  &gt; 0, b  X  h n i , T  X  1
Initialize: set w (1) = 0  X  R d for t = 1 to T do end for Pegasos is an SGD approach to solving ( 1 ), where at each iteration the iterate w ( t ) is updated based on an unbiased estimator of a sub-gradient of the objective P ( w ). Whereas in a  X  X ure X  stochastic setting, the sub-gradient is estimated based on only a single training example, in our mini-batched variation (Algorithm 1 ) at each iteration we consider the partial objective: where A t  X  Rand( b ). We then calculate the subgradi-ent of the partial objective P t at w ( t ) : where and  X  i ( w ) := 1 if y i h w , x i i &lt; 1 and 0 otherwise (in-dicator for not classifying example i correctly with a margin). The next iterate is obtained by setting w Analysis of mini-batched Pegasos rests on bounding the norm of the subgradient estimates  X  ( t ) . The stan-dard Pegasos analysis uses the bound k X   X  L A ( w ) k  X  get k  X  ( t ) k  X   X  k w ( t ) k + 1; the standard Pegasos anal-ysis follows. This bound relies only on the assumption max i k x i k  X  1, and is the tightest bound without fur-ther assumptions on the data.
 The core novel observation here is that the expected (square) norm of  X   X  L A can be bounded in terms of (an upper bound on) the spectral norm of the data: where kk denotes the spectral norm (largest singular value) of a matrix. In order to bound  X   X  L A , we first perform the following calculation, introducing the key quantity  X  b , useful also in the analysis of SDCA. Lemma 1. For any v  X  R n ,  X  Q  X  R n  X  n , A  X  Rand( b ) , Moreover, if  X  Q ii  X  1 for all i and 1 n k  X  Q k  X   X  2 Proof.
 where in (  X  ) the expectations are over i, j chosen uni-formly at random without replacement. Now using  X  Q ii  X  1 and k We can now apply Lemma 1 to  X   X  L A : Lemma 2. For any w  X  R d and A  X  Rand( b ) we have E [ k X   X  L A ( w ) k 2 ]  X   X  b b , where  X  b is as in Lemma 1 . Proof. If  X   X  R n is the vector with entries  X  i ( w ), then Using the by-now standard analysis of SGD for strongly convex functions, we obtain the main result of this section: Theorem 1. After T iterations of Pegasos with mini-batches (Algorithm 1), we have that for the averaged Proof. Unrolling ( 9 ) with  X  t = 1 / (  X t ) yields The performance guarantee is now given by the analysis of SGD with tail averaging (Theorem 5 of Rakhlin et al. 2012 , with  X  = 1 2 and G 2 = 4  X  b b ). Parallelization speedup. When b = 1 we have  X  b = 1 (see ( 11 )) and Theorem 1 agrees with the stan-dard (serial) Pegasos analysis 2 ( Shalev-Shwartz et al. , 2011 ). For larger mini-batches, the guarantee depends on the quantity  X  b , which in turn depends on the spec-tral norm  X  2 . Since 1 n  X   X  2  X  1, we have 1  X   X  b  X  b . The worst-case situation is at a degenerate extreme, when all data points lie on a single line, and so  X  2 = 1 and  X  b = b . In this case Lemma 2 degenerates to the worst-case bound of E [ k X   X  L A ( w ) k 2 ]  X  1, and in Theorem 1 we have  X  b b = 1, indicating that using larger mini-batches does not help at all, and the same number of parallel iterations is required.
 However, when  X  2 &lt; 1, and so  X  b &lt; 1, we see a benefit in using mini-batches in Theorem 1 , corresponding to a parallelization speedup of b  X  when  X  2 = 1 n , and so  X  b = 1, which happens when all training points are orthogonal. In this case there is never any interaction between points in the mini-batch, and using a mini-batch of size b is just as effec-tive as making b single-example steps. When  X  b = 1 we indeed see that the speedup speedup is equal to the number of mini-batches, and that the behavior in terms of the number of data accesses (equivalently, se-rial runtime) bT , does not depend on b ; that is, even with larger mini-batches, we require no more data ac-cesses, and we gain linearly from being able to perform the accesses in parallel. The case  X  2 = 1 n is rather ex-treme, but even for intermediate values 1 n &lt;  X  2 &lt; 1 we get speedup. In particular, as long as b  X  1  X  2 , we have  X  b  X  2, and an essentially linear speedup. Roughly speaking, 1  X  2 captures the number of examples in the mini-batch beyond which we start getting significant interactions between points. An alternative stochastic method to Pegasos is Stochastic Dual Coordinate Ascent (SDCA, Hsieh et al. 2008 ), aimed to solve the dual problem ( 2 ). At each iteration we choose a single training example ( x i , y i ), uniformly at random, corresponding to a single dual variable (coordinate)  X  i = e  X  i  X  . Subsequently,  X  i is updated so as to maximize the (dual) objective, keeping all other coordinates of  X  unchanged and maintaining the box constraints. At iteration t , the update  X  ( t ) i to  X  ( t ) i is computed via  X  i := arg max where clip I is projection onto the interval I . Vari-ables  X  ( t ) j for j 6 = i are unchanged. Hence, a single ilar to a Pegasos update, at each iteration a single, random, training point is considered, the  X  X esponse X  y i w (  X  ( t ) ) , x i is calculated (this operation dominates the computational effort), and based on the response, a multiple of x i is added to the weight vector w (cor-responding to changing  X  i ). The two methods thus involve fairly similar operations at each iteration, with essentially identical computational costs. They differ in that in Pegasos,  X  i is changed according to some pre-determined step-size, while SDCA changes it opti-mally so as to maximize the dual objective (and main-tain dual feasibility); there is no step-size parameter. SDCA was suggested and studied empirically by Hsieh et al. ( 2008 ), where empirical advantages over Pegasos were often observed. In terms of a theo-retical analysis, by considering the dual problem ( 2 ) as an  X  1 -regularized, box-constrained quadratic prob-lem, it is possible to obtain guarantees on the dual suboptimality, D (  X   X  )  X  D (  X  ( t ) ), after a finite num-ber of SDCA iterations ( Shalev-Shwartz &amp; Tewari , ever, such guarantees do not directly imply guaran-tees on the primal suboptimality of w (  X  ( t ) ). Recently, Shalev-Shwartz &amp; Zhang ( 2012 ) bridged this gap, and provided guarantees on P ( w (  X  ( t ) ))  X  P ( w  X  ) after a finite number of SDCA iterations. These guarantees serve as the starting point for our theoretical study. 4.1. Naive Mini-Batching A naive approach to parallelizing SDCA using mini-batches is to compute  X  ( t ) i in parallel, according to ( 14 ), for all i  X  A t , all based on the current iterate  X  and keep  X  ( t +1) j =  X  ( t ) j for j 6 X  A t . However, not only might this approach not reduce the number of required iterations, it might actually increase the number of required iterations. This is because the dual objective need not improve monotonically (as it does for  X  X ure X  SDCA), and even not converge.
 To see this, consider an extreme situation with only two identical training examples: Q = [ 1 1 1 1 ],  X  = 1 n and mini-batch size b = 2 (i.e., in each iteration we use both examples). If we start with  X  (0) = 0 with D (  X  (0) ) = 0 then  X  (0) 1 =  X  (0) 2 = 1 and follow-ing the naive approach we have  X  (1) = (1 , 1) T with objective value D (  X  (1) ) = 0. In the next iteration  X  1 =  X  So the algorithm will alternate between those two so-lutions with objective value D (  X  ) = 0, while at the optimum D (  X   X  ) = D ((0 . 5 , 0 . 5)  X  ) = 0 . 25. This is of course a simplistic toy example, but the same phenomenon will occur when a large number of train-ing examples are identical or highly correlated. This can also be observed empirically in some of our exper-iments discussed later, e.g., in Figure 2 .
 The problem here is that since we update each  X  i in-dependently to its optimal value as if all other coordi-nates were fixed , we are ignoring interactions between the updates. As we see in the extreme example above, two different i, j  X  A t , might suggest essentially the same change to w (  X  ( t ) ), but we would then perform this update twice , overshooting and yielding a new it-erate which is actually worse then the previous one. 4.2. Safe Mini-Batching Properly accounting for the interactions between co-ordinates in the mini-batch would require jointly op-timizing over all  X  i , i  X  A t . This would be a very powerful update and no-doubt reduce the number of required iterations, but would require solving a box-constrained quadratic program, with a quadratic term of the form  X   X  A Q A  X  A ,  X  A  X  R b , at each iteration. This quadratic program cannot be distributed to different machines, each handling only a single data point. Instead, we propose a  X  X afe X  variant, where the term  X 
A Q A  X  A is approximately bounded by the separable surrogate  X  k  X  A k 2 , for some  X  &gt; 0 which we will dis-cuss later. That is, the update is given by: for j 6 X  A t . In essence, 1  X  serves as a step-size, where we are now careful not to take steps so big that they will accumulate together and overshoot the objective. If handling only a single point at each iteration, such a short-step approach is not necessary, we do not need a step-size, and we can take a  X  X ull step X , setting  X  i optimally (  X  = 1). But with the potential for interac-tion between coordinates updated in parallel, we must use a smaller step.
 We will first rely on the bound ( 10 ), and establish that the choice  X  =  X  b as in ( 11 ) provides for a safe step size. To do so, we consider the dual objective at  X  +  X  , and the following separable approximation to it: in which  X  b k  X  k 2 replaces  X   X  Q  X  . Our update ( 15 ) with  X  =  X  b can be written as  X  = arg max (we then use the coordinates  X  i for i  X  A and ig-nore the rest). We are essentially performing paral-lel coordinate ascent on H (  X  ,  X  ) instead of on D (  X  +  X  ). To understand this approximation, we note that H ( 0 ,  X  ) = D (  X  ), and show that H (  X  ,  X  ) provides an expected lower bound on D (  X  +  X  ): Lemma 3. For any  X  ,  X   X  R n and A  X  Rand( b ) , Proof. Examining ( 16 ) and ( 17 ), the terms that do not depend on  X  are equal on both sides. For the linear term in  X  , we have that E [  X  [ A ] ] = b n  X  , and again we have equality on both sides. For the quadratic term we use Lemma 1 which yields E [  X   X  [ A ] Q  X  [ A ] ]  X  b n  X  and after negation establishes the desired bound. Inequalities of this general type are also studied in ( Richt  X arik &amp; Tak  X a X c , 2012 ) (see Sections 3 and 4). Based on the above lemma, we can modify the anal-ysis of Shalev-Shwartz &amp; Zhang ( 2012 ) to obtain (see complete proof in the supplementary material): Theorem 2. Consider the SDCA updates given by ( 15 ) , with A t  X  Rand( b ) , starting from  X  (0) = 0 and with  X  =  X  b (given in eq. ( 11 ) ). For any  X  &gt; 0 and we have By Theorem 2 , the # of iterations of mini-batched SDCA, sufficient to reach primal suboptimality  X  , is We observe the same speedup as in the case of mini-batched Pegasos: factor of b  X  ear speedup when b  X  1  X  2 . It is interesting to note that the quantity  X  b only affects the second,  X  -dependent, term in ( 22 ). The  X  X ixed cost X  term, which essentially requires a full pass over the data, is not affected by  X  b and is always scaled down by b . 4.3. Aggressive Mini-Batching Using  X  =  X   X  is safe, but might be too conservative. In particular, we used the spectral norm to bound  X   X  Q  X   X  k Q k k  X  k 2 in Lemma 3 (through Lemma 1 ), but this is a worst case bound over all possible vectors, and might be loose for the relevant vectors  X  . Rely-ing on a worst-case bound might mean we are taking Algorithm 2 SDCA with Mini-Batches (aggressive)
Initialize: set  X  (0) = 0 , w (0) = 0 ,  X  (0) =  X  b for t = 0 to T do end for much smaller steps then we could be. Furthermore, the approach we presented thus far relies on knowing the spectral norm of the data, or at least a bound on the spectral norm (recall ( 10 )), in order to set the step-size. Although it is possible to estimate this quantity by sampling, this can certainly be inconvenient. Instead, we suggest a more aggressive variant of mini-batched SDCA which gradually adapts  X  based on the actual values of k  X  ( t ) [ A one can observe the advantages of this aggressive strat-egy. In this variant, at each iteration we calculate the towards it by updating it to a weighted geometric av-erage of the previous step size and  X  X ptimal X  step size based on the step  X  considered. One complication is that due to the box constraints, not only the magni-tude but also the direction of the step  X  depends on the step-size  X  , leading to a circular situation. The ap-proach we take is as follows: we maintain a  X  X urrent step size X   X  . At each iteration, we first calculate a tentative step  X   X  A , according to ( 15 ), with the current  X  . We then calculate  X  according to this step direc-tion, and update  X  to  X   X   X  1  X   X  for some pre-determined parameter 0 &lt;  X  &lt; 1 that controls how quickly the step-size adapts. But, instead of using  X   X  calculated with the previous  X  , we actually re-compute  X  A us-ing the step-size  X  . We note that this means the ratio  X  does not correspond to the step  X  A actually taken, but rather to the tentative step  X   X  A . We could poten-tially continue iteratively updating  X  according to  X  A and  X  A according to  X  , but we found that this does not improve performance significantly and is not worth the extra computational effort. This aggressive strategy is summarized in Algorithm 2 . Note that we initialize  X  =  X  b , and also constrain  X  to remain in the range [1 ,  X  b ], but we can use a very crude upper bound  X  2 for calculating  X  b . Also, in our aggressive strategy, we refuse steps that do not actually increase the dual ob-jective, corresponding to overly aggressive step sizes. Carrying out the aggressive strategy requires comput-parallel. The main observation here is that: and so the main operation to be performed is an ag-quired in mini-batched Pegasos. As for the dual objec-tive, it can be written as D (  X  ) =  X  k w (  X  ) k 2  X  1 n and can thus be readily calculated if we maintain w (  X  ), its norm, and k  X  k 1 . Figure 1 shows the required number of iterations (cor-responding to the parallel runtime) required for achiev-ing a primal suboptimality of 0 . 001 using Pegasos, naive SDCA, safe SDCA and aggressive SDCA, on four benchmark datasets detailed in Table 1 , using different mini-batch sizes. Also shown (on an inde-pendent scale; right axis) is the leading term  X  b b in our complexity results. The results confirm the advantage of SDCA over Pegasos, at least for b = 1, and that both Pegasos and SDCA enjoy nearly-linear speedups, at least for small batch sizes. Once the mini-batch size is such that  X  b b starts flattening out (correspond-ing to b  X  1  X  2 , and so significant correlations inside each mini-batch), the safe variant of SDCA follows a similar behavior and does not allow for much paral-lelization speedup beyond this point, but at least does not deteriorate like the naive variant. Pegasos and the aggressive variant do continue showing speedups be-yond b  X  1  X  2 . The experiments clearly demonstrate the aggressive modification allows SDCA to continue enjoying roughly the same empirical speedups as Pega-sos, even for large mini-batch sizes, maintaining an ad-vantage throughout. It is interesting to note that the aggressive variant continues improving even past the point of failure of the naive variant, thus establishing that it is empirically important to adjust the step-size to achieve a balance between safety and progress. In Figure 2 we depict the evolution of solutions using the various methods for two specific data sets. Here we can again see the relative behavior of the methods, as well as clearly see the failure of the naive approach, which past some point causes the objective to deteri-orate and does not converge to the optimal solution. Contribution. Our contribution in this paper is twofold: (i) we identify the spectral norm of the data, and through it  X  b , as the important quantity control-ling guarantees for mini-batched/parallelized Pegasos (primal method) and SDCA (dual method). We pro-vide the first analysis of mini-batched Pagasos, with the non-smooth hinge-loss, that shows speedups, and we analyze for the first time mini-batched SDCA with guarantees expressed in terms of the primal prob-lem (hence, our mini-batched SDCA is a primal-dual method); (ii) based on our analysis, we present novel variants of mini-batched SDCA which are necessary for achieving speedups similar to those of Pegasos, and thus open the door to effective mini-batching using the often-empirically-better SDCA.
 Related work. Our safe SDCA mini-batching approach is similar to the parallel coordinate descent methods of Bradley et al. ( 2011 ) and Richt  X arik &amp; Tak  X a X c ( 2012 ), but we provide an analysis in terms of the primal SVM objective, which is the more relevant object of interest. Furthermore, Bradley et al.  X  X  analysis does not use a step-size and is thus limited only to small enough mini-batches X  if the spectral norm is unknown and too large a mini-batch is used, their method might not converge. Richt  X arik &amp; Tak  X a X c  X  X  method does incorporate a fixed step-size, similar to our safe variant, but as we discuss this step-size might be too conservative for achieving the true potential of mini-batching. In the context of SVMs with mini-batches, the analysis of Duchi et al. ( 2012a ; b ) is valid in small dimensions d , implying nearly linear speedups up to a batch size that gets worse as d increases. On the other hand, our analysis is dimension independent, allows for infinite dims (as in the kernelized case) or extremely high d (e.g., when the feature vectors are very sparse).
 Generality. We chose to focus on Pegasos and SDCA with regularized hinge-loss minimization, but all our results remain unchanged for any Lipschitz loss func-tions. Furthermore, Lemma 2 can also be used to es-tablish identical speedups for mini-batched SGD op-timization of min k w k X  B  X  L ( w ), as well as for direct stochastic approximation of the population objective (generalization error) min L ( w ). In considering the population objective, the sample size is essentially in-finite, we sample with replacements (from the popula-tion),  X  2 is a bound on the second moment of the data distribution, and  X  b = 1 + ( b  X  1)  X  2 .
 Experiments. Our experiments confirm the empiri-cal advantages of SDCA over Pegasos, previously ob-served without mini-batching. However, we also point out that in order to perform mini-batched SDCA ef-fectively, a step-size is needed, detracting from one of the main advantages of SDCA over Pegasos. Further-more, in the safe variant, this stepsize needs to be set according to the spectral norm (or bound on the spec-tral norm), with too small a setting for  X  (i.e., too large steps) possibly leading to non-convergence, and too large a setting for  X  yielding reduced speedups. In contrast, the Pegasos stepsize is independent of the spectral norm, and in a sense Pegasos adapts implic-itly (see, e.g., its behavior compared to aggressive SDCA in the experiments). We do provide a more aggressive variant of SDCA, which does match Pega-sos X  X  speedups empirically, but this requires an explicit heuristic adaptation of the stepsize.
 Parallel Implementation. In this paper we ana-lyzed the iteration complexity, and behavior of the iterates, of mini-batched Pegasos and SDCA. Un-like  X  X ure X  ( b =1) Pegasos and SDCA, which are not amenable to parallelization, using mini-batches does provide opportunities for it. Of course, actu-ally achieving good parallelization speedups on a spe-cific architecture in practice requires an efficient par-allel, possibly distributed, implementation of the it-erations. In this regard, we point out that the core computation required for both Pegasos and SDCA is scalar function. Parallelizing such computations effi-ciently in a distributed environment has been studied by e.g., Dekel et al. ( 2012 ); Hsu et al. ( 2011 ); their methods can be used here too. Alternatively, one could also consider asynchronous or delayed updates ( Agarwal &amp; Duchi , 2011 ; Niu et al. , 2011 ). The work of MT and PR was supported by EP-SRC grants EP/J020567/1 (Algorithms for Data Sim-plicity), EP/I017127/1 (Mathematics for Vast Digital Resources) and by NAIS (funded by EPSRC grant EP/G036136/1 and the Scottish Funding Council). Collaboration on this work was also supported in part by a Google research award.
 Agarwal, A. and Duchi, J. Distributed delayed stochastic optimization. In NIPS , 2011.
 Bradley, J.K., Kyrola, A., Bickson, D., and Guestrin,
C. Parallel coordinate descent for l1-regularized loss minimization. In ICML , 2011.
 Cotter, A., Shamir, O., Srebro, N., and Sridharan, K.
Better mini-batch algorithms via accelerated gradi-ent methods. In NIPS , 2011.
 Dekel, O., Gilad-Bachrach, R., Shamir, O., and Xiao,
L. Optimal distributed online prediction using mini-batches. Journal of Machine Learning Research , 13: 165 X 202, 2012.
 Duchi, John, Bartlett, Peter L., and Wainwright, Mar-tin J. Randomized smoothing for (parallel) stochas-tic optimization. In ICML , 2012a.
 Duchi, John, Bartlett, Peter L., and Wainwright, Mar-tin J. Randomized smoothing for stochastic opti-mization. SIAM Journal on Optimization , 22(2): 674 X 701, June 2012b.
 Hsieh, C-J., Chang, K-W., Lin, C-J., Keerthi, S.S., and Sundarajan, S. A dual coordinate descent method for large-scale linear svm. In ICML , 2008. Hsu, D., Karampatziakis, N., Langford, J., and Smola, A. Parallel online learning. arXiv:1103.4204 , 2011. Libsvm. Datasets . http://www.csie.ntu.edu.tw/  X  cjlin/ libsvmtools/datasets/binary.html.
 Nesterov, Yu. Efficiency of coordinate descent meth-ods on huge-scale optimization problems. SIAM J. Optimization , 22:341 X 362, 2012.
 Niu, F., Recht, B., Re, C., and Wright, S. Hogwild:
A lock-free approach to parallelizing stochastic gra-dient descent. In Shawe-Taylor, J., Zemel, R.S.,
Bartlett, P., Pereira, F.C.N., and Weinberger, K.Q. (eds.), NIPS 24 , pp. 693 X 701. 2011.
 Rakhlin, A., Shamir, O., and Sridharan, K. Mak-ing gradient descent optimal for strongly convex stochastic optimization. ArXiv:1109.5647 , 2012. Richt  X arik, P. and Tak  X a X c, M. Distributed coordinate descent methods for big data optimization. Techni-cal report.
 Richt  X arik, P. and Tak  X a X c, M. Parallel coordi-nate descent methods for big data optimization. ArXiv:1212.0873 , 2012.
 Richt  X arik, P. and Tak  X a X c, M. Iteration complex-ity of randomized block-coordinate descent meth-ods for minimizing a composite function. Math-ematical Programming , 2013. doi: 10.1007/ s10107-012-0614-z.
 Shalev-Shwartz, S. and Tewari, A. Stochastic Meth-ods for l1-regularized Loss Minimization. JMLR , 12: 1865 X 1892, 2011.
 Shalev-Shwartz, S. and Zhang, T. Stochastic dual co-ordinate ascent methods for regularized loss mini-mization. ArXiv:1209.1873 , 2012.
 Shalev-Shwartz, S.S., Singer, Y., Srebro, N., and Cot-ter, A. Pegasos: Primal estimated sub-gradient solver for svm. Mathematical Programming: Series
A and B-Special Issue on Optimization and Ma-chine Learning , pp. 3 X 30, 2011.
 Tappenden, R., Richt  X arik, P., and Gondzio, J. Inex-act coordinate descent: complexity and precondi-tioning. ArXiv:1304.5530 , 2013.
 Zhang, T. Solving large scale linear prediction using stochastic gradient descent algorithms. In ICML ,
