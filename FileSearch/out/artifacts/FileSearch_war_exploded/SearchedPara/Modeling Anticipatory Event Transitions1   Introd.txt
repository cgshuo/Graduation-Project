 The value of providing accurate and timely information, especially news, has exponentially increased with the preval ence of the Internet. For example, push-based news alert systems like Google News Alerts notify subscribers when user pre-specified events (called Antic ipatory Event or AE) take place.
As alerts are pushed into handheld devi ces 24/7, it is extremel y important[1] for the alerts to be delivered accurately and precisely . Unfortunately, current alert systems are not smart enough to figure out if a news document contain-ing the user defined words satisfy the AE. In fact, to ensure uncompromisable accuracy, some portals like Yahoo rely on a human operator to approve system triggered news alerts, whereas others like Google use a completely automated approach, resulting in many false alarms[2].

AED systems based on classifying sen tences/documents into pre/post AE states have been previously proposed[2,3]. The idea is to train a classifier using the pre/post documents of historical events with similar characteristics to the AE. For example, to create an AED sys tem to detect an AE such as  X  X S invades Iran X , it can be trained using available documents from the pre/post states of the historical events  X  X S invades Afghanistan X  and  X  X S invades Iraq X . The crux of the AED system is thus the 2-state Event Transition Graph (ETG), where documents are assigned to either the pre or post states of the AE.

However, the trained model is a black box that does not open up for in-terpretation. Further, the element of t ime is not considered if we use standard Information Retrieval (IR) techniques to represent the documents from each of the two states. We therefore propose a novel approach to select representative features from the pre and post states based on the burstiness of words before and after the AE transition in the ETG, respectively. He et al.[3,2] originally proposed the AED concept as a new area under Topic Detection and Tracking (TDT). They also defined the general AED framework which includes an Event Transition Graph (ETG). Our work focus on deriving better features to represent the two states of an ETG, thereby improving the model.

Tax et al.[4] explained the application of support vector classifiers in two-class classification problems. The paper discussed the strengths as well as some of the critical limitations of the support vector classifier. Amongst these were the problems of small sample size as well as peaking effect. It proved that using as many features as possible does not necessarily improve the overall classification accuracy. Instead, there exists an op timal number of features beyond which performance of the classifier degrades. Our work, hopes to approximate this approach by limiting the features selected for classification in hope of improving the classification accuracy.

Kleinberg[5] formulated a finite state automaton to identify and define bursty words in a document stream. Bursty words are useful for extracting the structure of the document stream and thus attach a formal meaning to the stream. This is possible because bursty words add an additional time dimension to the set of documents. Such a model can be applied in many ways, one of which is to identify scientific research topics and trends as illustrated in [6]. Noting its usefulness, we applied this algorithm in our feature selection approach. 3.1 The Anticipatory Event Detection Model An overview of the AED framework is given in Figure 1. The ETG involves the formation of the individual events and the AE transition from one event to the next. Our primary goal is thus to accurately model each event in the ETG. An AE could be any event with a predecessor in the ETG. Once the ETG is well defined, say with a list of keywords, it could then be used to classify and trigger an AE.
 3.2 Kleinberg X  X  Burst Model Kleinberg X  X  burst model [5] finds bursty words from a document stream. A word is considered bursty if its document freq uency (DF) exceeds a pre-defined thresh-old over a given time period. The idea thus is to extract a set of bursty words related to the two states of an AE. The bursty words would provide additional disciminatory information between the two states. We adopt the batch process-ing formulation of Kleinberg X  X  2-state automaton model as defined by Ketan Mane[6]. Figure 2 shows an overview of the model with two states q 0 and q 1 , where the transition cost from the lower to the higher state is defined along with the cost for remaining in each state. Note that there is no cost for going from state q 1 back to q 0 .

The objective of Kleinberg X  X  model is to find for each word a state sequence with minimum cost. A word is considered bursty if its state sequence includes at least one bursty state q 1 . Each burst has an associated weight, which is simply the reduction in cost between states q 1 and q 0 over its bursty interval. Thus, if the aggregated bursty weight value of a word is large, it implies that there are many periods in time when the word DF is extremely high. 3.3 Our Feature Selection Approach We applied Kleinberg X  X  algorithm to word DF of the set of pre and post transition documents separately as shown in Figure 3. In particular, 1. Select a set of documents corresponding to an AE. 2. Manually label each document as negative (pre) or positive (post). 3. For every word in a given document set, its DF is plotted and ranked. 4. The set of top DF words from each set are then fed into Kleinberg X  X  algo-5. If a word X  X  ouput state sequence has at least one transition to state q 1 ,it 6. Top bursty words from each of the pre and post document streams are then
We tried different ways of selecting the bursty words for classification: 1. Union ( P  X  N ): Union of all bursty words found in both sets. 2. Union+ ( P  X  N + P  X  N ): Same as Union, except t hat weights of common 3. Discriminatory ( P  X  N  X  P  X  N ): Union of unique words from both sets. 4. Baseline : Set of all (bursty and non-bursty) words.

Our approach aims to combine the best of static and temporal information from the document stream. The word docum ent frequency ranking returns a list of highest document frequency terms w ithin documents in an AE state. On the other hand, Kleinberg X  X  model aims to select the words that encounter a sudden and extended burst in document frequency. The former selects the most frequent words and the latter extract words amo ng which that becomes more relevant in some time periods. 3.4 SVM Classifier Representing the documents from the pre/post event states using each of the 4 feature spaces, we train 4 SVM classi fiers[7]. The SVM attempts to create a hyperplane that separates the pre and post states of the AE. 4.1 Dataset Six topics from the TDT3[8] corpus of news articles were selected and manually filtered to find a suitable event for modelling a state transition, i.e., from a  X  X egative X  event state to a  X  X ositive X  event state. In simple terms, this means that we manually split each topic dataset into a two-state event-transition model.
Listed in Figure 4 are the six manually selected datasets. Negative and posi-tive events were defined for each dataset, and the corresponding documents were identified and appropriately tagged. These datasets consist of only relevant doc-uments with no off-topic items. Figure 5 shows the well balanced characteristics of each dataset.

Document vectors were created for all the documents within each of the test datasets. This was achieved by running the dataset documents through the Lucene [9] software that enabled indexing, removal of common stopwords, and representation of the dataset items i nto a document vector format. These were then utilized to obtain the top 500 high frequency terms and high document-word frequency terms for each dataset.

These high frequency words were then f ed into Kleinberg X  X  two state model, which will then output the burstiest words amongst the set of 500 high frequency words. It should be noted that Kleinberg X  X  model does not take into account the term frequency of the word but instead considers the word document frequency to calculate the transition costs from state to state.

The SVM classifier is then used to train and test the datasets using the doc-ument vectors created using each of t he 4 feature space representation. 4.2 Overlap of Bursty Features In our experiments, bursty words are i ndependently selected over the collec-tions of negative and positive AE documents. The intention is simply to identify possible terms within the event states that would best represent each AE.
Figure 6 illustrates the overlap of the top 15 bursty words between the two sets. We see that the words selected are quite representative of the fired AE, e.g.  X  X eal X  and  X  X oney X  were some of the discriminating words for the passed budget, while  X  X elief X ,  X  X id X ,  X  X illion X ,  X  X ood X  confirms the arrival of Hurricane Mitch. 4.3 AED Results For each topic, five-fold cross validation was done to obtain the average SVM test results, which include the overall accuracy, and for each class of data the precision, recall, and F-measure. Resul ts are summarized in Figure 7. Clearly, the 3 bursty feature selections consistently meet or better the full feature space, with the Union+ strategy leading the pack in every topic. Results of the Discrim-inatory selection seems more varied, probably due to SVM needing the removed common words to determine an optimal hyperplane. Note that some of the 100% baseline precision and recall values are due to SVM classifying the majority or all of the data as positive, which skews the F-Measure for the positive target class. This can be seen by the corresponding lower negative class metric values. Moreo ver, accuracy should be considered since the classes are well-balanced. Our feature selection approach effectivel y enables the accurate classification of documents to their appropriate event states for AED. Equal or better classifi-cation accuracies were obtained by usin g less than 4% bursty features from the whole corpus, a great savings in time/complexity. This opens up further research in AED. Specifically, the formation of a more complex ETG including the tran-sitions from one event to the next will be our focus research area in the near future. Lastly, it is noted that our simple yet effective feature selection approach can be applied to a myriad of applications involving text streams, such as chat messages.

