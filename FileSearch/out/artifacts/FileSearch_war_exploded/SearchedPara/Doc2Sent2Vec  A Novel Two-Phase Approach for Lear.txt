 Doc2Sent2Vec is an unsupervised approach to learn low-dimensional feature vector (or embedding) for a document. This embedding captures the semantics of the document and can be fed as input to machine learning algorithms to solve a myriad number of applications in the field of data min-ing and information retrieval. Some of these applications include document classification, retrieval, and ranking.
The proposed approach is two-phased. In the first phase, the model learns a vector for each sentence in the document using a standard word-level language model. In the next phase, it learns the document representation from the sen-tence sequence using a novel sentence-level language model. Intuitively, the first phase captures the word-level coherence to learn sentence embeddings, while the second phase cap-tures the sentence-level coherence to learn document em-beddings. Compared to the state-of-the-art models that learn document vectors directly from the word sequences, we hypothesize that the proposed decoupled strategy of learn-ing sentence embeddings followed by document embeddings helps the model learn accurate and rich document represen-tations.

We evaluate the learned document embeddings by consid-ering two classification tasks: scientific article classification and Wikipedia page classification. Our model outperforms the current state-of-the-art models in the scientific article classification task by  X  12.07% and the Wikipedia page clas-sification task by  X  6.93%, both in terms of F 1 score. These results highlight the superior quality of document embed-dings learned by the Doc2Sent2Vec approach.
Document representations plays a vital role in the perfor-mance of several downstream IR applications such as doc-ument classification (or tagging), retrieval, ranking and so on. The most commonly used document representation is bag-of-words (BOW) or bag-of-n-grams [1]. Despite its sim-plicity and efficiency, it fails to capture the semantics of the documents as it suffers from data sparsity and curse of high dimensionality. Latent Dirichlet Allocation (LDA) [2] is an-other widely adopted distributed document representation.
In an attempt to harness the power of neural networks for document representations, Le et al. [3] propose a sim-ple approach to learn document embedding from the word sequence using a standard word-level language model. The representations learned capture the ordering of words (un-like BOW) and also the semantics of the words in an ef-ficient way (unlike LDA). In their follow-up work [4], the authors proposed an incremental model by jointly learning the word embeddings along with its document embedding. This change leads to learning rich and accurate representa-tion compared to the previous model, which freezes the word vectors while learning the document vectors.

Inspired by the superior results obtained by the neural lan-guage models, we present a two-phase approach, Doc2Sent2Vec, to learn document embedding. In the first phase, we learn the sentence embedding using the word sequence generated from the sentence. Intuitively, the sentence representation is computed by modeling word-level coherence. In the next phase, we propose a novel model that learns the document representation from the sentence sequence generated from the document. Intuitively, the document embedding is com-puted by modeling sentence-level coherence. We argue in this work that the proposed decoupled strategy allows our model to compute accurate and rich document representa-tions.

We validate the learned document embeddings using two classification tasks. In the first task, we aim at classifying a research article (or paper) among one of the eight different fields of computer science domain. In the second task, we predict the tag of a Wikipedia page. Doc2Sent2Vec outper-forms the existing state-of-the-art model in scientific article classification task by  X  12.07% and Wikipedia page classifi-cation task by  X  6.93%, both in terms of F 1 score.
Our main contributions are summarized below. between the two tiers.
Our hierarchical framework as shown in Figure 1 consists of two tiers; one learning the sentence representation from the word context, another learning the document represen-tation from the sentence context.
 Problem Formulation : Formally, let us denote a set D of M documents, D = { d 1 , d 2 ,  X  X  X  , d M } , where each document d m is a sequence of T m sentences, d m = { s ( m, 1) , s ( m, 2) s ( m,T m ) } . Each sentence is a sequence of T n words, s { w sentence index m when it is obvious in the context. The goal of the Doc2Sent2Vec approach is to jointly learn low-dimensional representations of words, sentences and docu-ments as a continuous feature vector of dimensionality D w D s and D d respectively. We will realize this goal in two phases, as discussed in the following.
 Modeling word-level coherence : In the first phase, the model aims to learn sentence representation from the word sequence within the sentence. We add the sentence vector to the standard language model that predicts the next word given its context word. This sentence vector must capture the topics of the sentence in a compact form. Each word is mapped to a unique vector, denoted by a column in the matrix V word , whose size is given by D w  X | V w | (where | V the vocabulary size). Similarly, each sentence in a document is mapped to a unique vector denoted by a column in the matrix V sent with size D s  X | V s | , where | V s | is the number of unique sentences. The model uses the concatenation of word vectors of context words along with the sentence vector as features to predict the given word in a sentence.
 w ( n,t + c w ) as the context words for the target word w appearing in the sentence s ( m,n ) . The objective of the word-level language model is to maximize the log likelihood prob-ability.

Here 2  X  c w denotes the length of the context for the word sequence. The probability of observing the central word w ( n,t ) given the context words and the sentence is defined using the following softmax function.
 where v 0 w is the concatenation of the input embeddings (ignoring the central term w ( n,t ) ), with dimensionality 2  X  c w  X  D
Similarly, we define the probability of observing the sen-tence s ( m,n ) , given the words present in it as follows. where v 0 s is the concatenation of the input embedding of all the words present in the sentence s m,n , with dimensionality T n  X  D Modeling sentence-level coherence : In the next phase, we propose a novel language model which constructs the doc-ument representations from the sentence sequence present in the document. The novel task is to predict the current sentence using the embeddings of the surrounding sentences and the document embedding as features. We add the doc-ument vector in the input layer of this model, that captures the topics of the entire document in a compact form. Each document is mapped to a unique vector denoted by a col-umn in the matrix V doc , whose size is given by D (where | V d | is the number of unique documents).
 s ( m,t + c s ) as the context sentences for the target sentence s ( m,t ) , appearing in the document d m . The objective of our novel sentence-level language model is to maximize the following log likelihood probability.
Here 2  X  c s denotes the length of the context for the sen-tence sequence. The probability of observing the central sentence s ( m,t ) given the context sentences and the docu-ment is defined using the softmax function as given below. where v 0 s is the concatenation of the input embeddings (ignoring the central term s ( m,t ) ), with dimensionality 2  X  c s  X  D
Similarly, we define the probability of observing the doc-ument d m given the sentences present in it as follows. where v 0 d m is the output representation of d m and  X  v the concatenation of the input embedding of all the sen-tences present in the document d m , with dimensionality T Training details : The overall objective function of Doc2Sent2Vec is to maximize the log likelihood probability as follows.
We employ stochastic gradient descent to learn the pa-rameters, where the gradients are obtained via backprop-agation [12], with fixed learning rate of 0.1. However, it takes O ( V w ), O ( V s ), O ( V s ) and O ( V d ) to compute  X  log from Equations 2, 4, 7 and 9, which is undesirable in prac-tice. Hence, we use hierarchical softmax [6], to facilitate faster training. The testing phase was excluded as the em-beddings for all the documents in the dataset are estimated during the training phase.
In this section, we present the experimental results to show the effectiveness of the learned document embeddings by considering two classification tasks.
 Dataset Description : The dataset details are displayed in Table 1. Citation Network Dataset (CND) [5] consists of a collection of research papers (along with abstracts) from different fields of computer science domain. Inspired by the recent work [13], we use only a sample of the original dataset to speed up the training process. We construct a dataset of 8000 papers by randomly sampling 1000 research papers from 8 different fields, as mentioned in Table 1. In the sec-ond task where we perform Wikipedia page classification, we make use of Wiki10+ dataset [14], which contains one or more social tags (along with the number of users who have annotated this tag) for each Wikipedia page retrieved from delicious.com. We find the most frequent 25 social tags and only keep those documents that contain any of these tags. It results in a collection of 19740 documents as shown in Ta-ble 1, with each document associated with the most voted social tag. For simplicity, we consider only the first para-graph of the Wikipedia article for learning the embeddings. Experimental Setup : In all our experiments, we consider the following four models. To ensure fair comparison, we empirically set C w , C s , D D , D d to 5, 1, 100, 100, 100 respectively, for all the models. We lowercase all the words, remove those which occur less than 10 (15) times in the CND (Wiki10+) corpus. We use pre-trained Glove [11] word vectors trained successively on Wikipedia 2014 1 and Gigaword 5 2 corpus, to initialize the word embeddings ( V w ). It is important to notice that we use a linear classifier (one-vs-rest logistic regression 3 prediction. It can be argued that the model performance can be improved using non-linear models, but this falls out of scope of our goal. We use 5-fold cross-validation to report the model performance for both the tasks. http://dumps.wikimedia.org/enwiki/20140102/ https://catalog.ldc.upenn.edu/LDC2011T07 http://scikit-learn.org/stable/modules/generated/sklearn. linear model.LogisticRegression.html Analysis on CND : Scientific article classification results are shown in Table 2. We observe that it is beneficial to learn word vectors too while training instead of merely using pre-initialised word vectors. On incorporating the learning of word vectors, the improvement of  X  5.88% and  X  17.47% in F 1 score for Paragraph2Vec and Doc2Sent2Vec respec-tively, justifies this claim. Moreover, we see that our model outperforms the state-of-the-art model by a significant mar-gin of  X  12.07%. This is mainly because our model is able to exploit both word-level and sentence-level coherence to enrich the embeddings.
 Analysis on Wiki10+ : We present the Wikipedia page classification results in Table 3. It is interesting to see that learning the word vectors has a negative impact for Para-graph2Vec algorithm. This is shown by a decline in the F 1 score by  X  6.5%. We believe that the word vectors be-fore training are semantically accurate as they are learned from the complete Wikipedia corpus. While training them again, the vectors tend to get distorted leading to poor re-sults. It can be argued that the same trend should follow for our model when we learn the word vectors. However, the Doc2Sent2Vec results indicate that the F 1 improves by  X  26.93% on learning word vectors. This illustrates that the proposed strategy of jointly exploiting sentence level and word level coherence is insensitive to the distortions gener-ated by word vectors, resulting in robust embeddings of the Wikipedia pages. The performance improvement of  X  6.93% over the best baselines in terms of F 1 score, highlights the superiority of the Doc2Sent2Vec approach.
Our work is inspired by the outburst of representation learning works using neural networks [6, 8, 9] to solve many natural language processing tasks. In this work, we focus on the important problem of capturing the semantics of the document in a machine-understandable format (or represen-tation).

A recent work Paragraph2Vec [3] provides a simple so-lution to this problem by extending the popular algorithm  X  X ord2Vec X  [6]. Their strategy of adding a memory vector to the input layer of the neural network exhibits significantly better results than commonly used representations such as LDA, BOW and so on. In the original work the authors freeze the word vectors, while in the extension [4] they let the gradients update the word vectors too, along with doc-ument vectors. This trick further improves the results in understanding longer documents such as research articles and Wikipedia pages. Our work is inspired by the superior results of these neural language models.

Similar to the spirit of Doc2Sent2Vec, Djuric et al. [7] propose a Hierarchical Document Vector (HDV) model to learn representations from a document stream. In the first phase, the model learns document representation from the word sequence (similar to [3]). In the next phase, the model enriches the representations further by exploiting the docu-ment sequence in the stream. The Doc2Sent2Vec approach is different from HDV because our model does not assume the existence of a document stream and HDV does not model sentences.
We proposed a novel two-phase approach Doc2Sent2Vec to learn document representations in an unsupervised fash-ion. To this end, we introduced a novel sentence-level lan-guage model which exploits the sentence sequence present in the document. We validated the document embeddings by considering two classification tasks. Our classification results indicate the superiority of the proposed approach, thereby constituting a step towards learning accurate and rich document representations.

In the future, we plan to extend the current approach to a general multi-phase approach where every phase corre-sponds to a logical sub-division of a document like words, sentences, paragraphs, subsections, sections and documents. Also, it will be interesting to investigate how the document embeddings that are learned through the Doc2Sent2Vec ap-proach can be enhanced by considering the document se-quence in a stream such as news click-through streams [7]. This work is supported by SIGIR Donald B. Crouch Travel Grant. The authors would like to thank NVIDIA for donat-ing one Tesla K40 GPU card.
