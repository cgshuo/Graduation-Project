 1. Introduction
Construction is a risky business and oftentimes has suffered poor performance as a consequence. According to Zeng et al. 2007 , the nature of construction industry (marked by constant changes in the environment, pressures to maintain schedules and reduce costs, and increasingly complex construction techniques) makes handling risks effectively a significant challenge. Thus, to operate profitably, construction companies must frequently monitor project cost performance to detect deviations quickly and make appropriate responses. However, construction firms typically focus on budget planning during the initial project stage, which practically ignores the impact of engineering cost changes and information updates during construction ( Cheng et al., 2010 ).
This fact prevents effective project cost control and the detection of potential problems. As project conditions do not remain stable following the commencement of a construction project, regular budget revisions are critical to project cost management.
Within the project manager community, Earned Value Man-agement (EVM) is the managerial and monitoring technique best able to facilitate project control in all three critical elements of project management, i.e., scope, time, and cost ( Vandevoorde and Vanhoucke, 2005 ). Moreover, project managers often rely on EVM to compute estimation at completion (EAC), a quick and auto-matic formula for evaluating costs of completing pre-scheduled activities ( Bolles and Fahrenkrog, 2004 ). Based on estimated EAC, a manager can calculate the deviation between planned and actual costs and figure out the underlying problem.

In practice, to compute the EAC, managers first collects cost management data, then employ formulas to perform calculation ( Christensen, 1993 ). One drawback of using formulas is that there is a wealth of methods potentially available for the EAC calcula-tion. At least eight methods have been noted in previous research works ( Christensen et al., 1995 ). Therefore, project managers must employ their own judgments to select the most suitable calculation methods. Needless to say, factors influencing project cost are myriad, and each project is a distinct entity with its own characteristics. That explains why a certain formula may be acceptable for a particular case, but it may be unqualified for others ( Cheng et al., 2010 ).

Since construction project is context-dependent and highly uncertain, developing a deterministic model to forecast EAC is expensive and even impractical. Approximate inference, which is fast and cost effective, represents a viable alternative ( Cheng and Wu, 2009 ). The inference model is used to derive new facts from historical data. The inference process changes adaptively in response to alteration in historical data. Naturally, the capability to infer new facts from previously acquired information is an advantage of the human brain. Hence, artificial intelligence (AI) can be used to develop models that simulate the human inference capability. AI refers to computer systems able to handle complex or ill-structure problems using techniques such as Artificial
Neural Network (ANN), Support Vector Machine (SVM), fuzzy logic, and so on. As the AI model equips a computer system to cope with tasks for which humans excel naturally, utilizing AI inference model represents a promising solution for EAC problems.

In the AI field, various ANN based models have been estab-lished for regression analysis and time series prediction ( Mencar et al., 2005 ; Cheng et al., 2008 ; Nejad and Jaksa, 2010 ). Notably, Chen (2008) employed an ANN based model, namely Evolutionary Fuzzy Neural Inference Model (EFNIM), to predict EAC. However,
ANN also encounters disadvantages such as selection of network structure and high computational expense for training process.
Additionally, it is not guaranteed for an ANN model to converge to optimal solution ( Sapankevych and Sankar, 2009 ). The reason is that the ANNs suffer from the existence of multiple local solutions ( Suykens et al., 2002 ). Moreover, a wealth of research works has demonstrated the superiority of SVM over ANN ( Samanta et al., 2003 ; Huang et al., 2004 ; Hur and Lim, 2005 ). Thus, replacing
ANN with SVM can bring about the improvement in EAC prediction.

In an attempt to improve the effectiveness of construction cost control, Evolutionary Support Vector Machine Inference Model (ESIM) ( Cheng and Wu, 2009 ) has been utilized in EAC forecasting ( Cheng et al., 2010 ). ESIM fuses two AI techniques, namely SVM and fast messy Genetic Algorithm (fmGA). In this previous work, it can be shown that the ESIM outperforms all traditional formulas ( Christensen et al., 1995 ) and the EFNIM ( Chen, 2008 ) in EAC prediction. However, one shortcoming of SVM is that it considers all training data points equally in order to establish the regression function. In practice, real-world time series is often unbalanced. That is, some data sets should be considered more relevant than the others. In this case, original SVM will likely not yield desirable results. Accordingly, Lin and Wang (2002) proposed a modification of SVM, named fuzzy SVM (fSVM). In fSVM, the authors utilized different kinds of time functions to weight training data points, which is the reason why fSVM is also known as  X  X  X eighted SVM X  X  (wSVM). Previous researches ( Bao et al., 2005 ; Cheng and Roy, in press ) have demonstrated wSVM to be an effective tool for time series data analysis.

Additionally, in the AI field, fuzzy logic can be fused with different techniques to boost the approximate reasoning capabil-ity of inference models ( Jang et al., 1997 ). Developed in 1960s, various scholars employ fuzzy logic to solve practical problems in different fields ( Khemchandani et al., 2009 ; Ko, 2002 ; Kumar and
Reddy, 2005 ). Previous research studies have proven the ability of fuzzy logic to absorb presumptions, maintain subjectivity, handle vague information, and enhance approximate reasoning. With the objective of constructing an AI model to solve the
EAC problem, it is noticed that EAC forecasting itself is character-ized by numerous uncertain conditions. One of the reasons lies in the large number of variable data involved in construction costs.
Besides, myriad factors influencing project cost involve physical site-productivity, weather conditions, and socioeconomic condi-tions. Predicting such factors is challenging and mostly imprac-tical to evaluate one by one. As a consequence, the forecasting model must possess the ability to cope well with large amount of influencing factors and variable data to achieve desirable EAC predictions.

Therefore, the main purpose of this research was to integrate fuzzy logic, wSVM and fmGA into a fuzzy inference system designed specifically for EAC forecasting problems. The contribu-tion of each aforementioned technique is critical to the construction of the model. The wSVM is employed as a supervised learning technique that can focus on the features of time series data ( Lin and Wang, 2002 ; Khemchandani et al., 2009 ). Besides, the fuzzy logic is aimed to enhance the model capability of inference and to deal with uncertainty ( Jang et al., 1997 ; Cheng and Roy, in press ). Finally, the fmGA is applied to optimize model X  X  tuning parameters. Feasibility and aptitude of the newly proposed model were demonstrated by comparing results with existing methods, including EFNIM ( Chen, 2008 ) and ESIM ( Cheng and Wu, 2009 ). 2. Literature review 2.1. Estimate at Completion (EVM) methodology is commonly defined as a management technique that relates resource planning and usage to schedules and to technical performance requirement ( Abba, 1997 ). Metho-dology of EVM consists of three essential components that support project control: Plan Value (PV) or Budgeted Cost of
Work Schedule (BCWS), Earned Value (EV) or Budgeted Cost of Work Performed (BCWP), and Actual Cost (AC) or Actual Cost of Work Performed (ACWP). In construction industry, EVM attracts project managers as it provides a tool for tracking project status and measuring project performance.
 value widely known as Estimate at Completion (EAC).The essenti-ality of EAC is emphasized due to the fact that EAC enables project manager to appraise the total cost with the assumption that past occurrence will affect the future project X  X  consequence. In prac-tice, the EAC is often computed by formulas using cost manage-ment data provided by the contractor to the owner in progress report, usually monthly report. Additionally, for the contractors, in order to form the periodic report to the owner, their site-engineers must collect sufficient data summarized in the daily man-hours summary, daily material summary and daily equip-ment summary. Finally, all EAC formulas are based on the combination of several data elements presented in the report: BCWS, BCWP, and ACWP ( Christensen et al., 1995 ). To forecast the
EAC, it is noted that various index-based formulas have been utilized. Those formulas are divided into three categories: non-performance method, performance method, and composite method (see Table 1 )( Chen, 2008 ; Cheng et al., 2010 ; Christensen, 1993 ). An additional method to obtain EAC is simply the amount of executed budget. Due to the review done by Christensen et al. (1995) , the accuracy of index-based formulas depends signifi-cantly on the type of system, and the stage and phase of project. This interprets why performance of a particular formula might be quiet acceptable in a certain case, while it could be much worse in other cases. Needless to say, this drawback of traditional methods points out the need for more advanced prediction method. 2.2. Fuzzy logic
Fuzzy logic is an effective tool for forecasting, decision making and process control in environmen ts characterized by uncertainty, vagueness and subjectivity ( Bojadziev and Bojadziev, 2007 ). Fuzzy logic consists of four major compon ents, including fuzzification, rule base, inference engine and defuzzi fication. Fuzzification is a process that uses membership functions (MFs) to convert crisp input variable into linguistic variables. The result, which is used by the inference engine, simulates the human decision-making process based on fuzzy implications and available rules. In the final step, the fuzzy set, as the output of the inference process, is converted into crisp output using a certain method of defuzzification.
Despite the advantages of fuzzy logic, the approach features a number of challenges. The reason is that the ability of fuzzy logic to deal with vagueness and uncertainty depends heavily on the appropriate distribution of membership functions, number of rules, and selection of proper fuzzy set operations. This process is subjective in nature and reflects the context in which a problem is viewed. As a consequence, the more complex a problem is, the more difficult MFs construction and rules become ( Cheng and
Roy, in press ). Some researchers view this difficulty as an optimization problem, because determining MFs configurations and fuzzy rules is a complicated and problem-oriented task. To overcome the aforementioned difficulties, some researchers have tried to fuse fuzzy logic with AI optimization techniques such as
GA, fmGA ( Chen 2008 ; Cheng and Wu, 2009 ; Ishigami et al., 1995 ). These optimization methods have demonstrated their ability to minimize time-consuming operations and human inter-vention that are necessary to optimize MFs and fuzzy rules. 2.3. Fast messy Genetic Algorithm (fmGA)
Within the field of machine learning and optimization, the fmGA ( Goldberg et al., 1993 ) is a state of the art tool that utilizes a population-based approach for large-scale permutation problems. It is an improvement on messy genetic algorithms (mGAs) ( Deb and Goldberg, 1991 ), which were initially developed to overcome linkage problems in simple genetic algorithms (sGAs) resulting from a parameter-coding problem that could generate suboptimal solutions. Unlike sGA, which represents possible solutions as fixed length strings, fmGA applies messy chromosomes to form strings of various lengths, which can efficiently find optimal solutions for large-scale permutation problems.

The fmGA algorithm consists of two loop types (inner and a competitive template (e.g., a randomly generated or a problem-specific fixed-bit string) is generated. Subsequently, the inner loop is invoked. In the inner loop, fmGA contains three phases, namely initialization phase, building block filtering (BBF) or primordial phase, and juxtapositional phase. The ability of the fmGA to manipulate building blocks (BBs) explicitly in the primordial phase to obtain  X  X  X ood X  X  solutions and, potentially, the global optimum distinguishes the fmGA from other genetic approaches ( Cheng and Roy, in press ). In the initialization phase, an adequately large population contains all possible BBs of order k. fmGA performs the probabilistically complete initialization (PCI) process at this stage, which randomly generates n chromo-somes and calculates their fitness values. The primordial phase contains two operations namely building-block filtering and threshold selection. In the primordial phase,  X  X ad X  genes that do not belong to BBs are filtered out so that, in the end, the resultant population encloses a high proportion of  X  X ood X  genes belonging to BBs. In the juxtaposition phase, operations are more similar to those of simple GA. The selection procedure for good genes is used together with a cut-and-splice operator to form a high kk+1 quality generation, which may contain the optimal solution. The next outer loop can commence once the inner loop is finished.
The competitive template is replaced by the best solution found so far, which becomes the new competitive template for the next outer loop. The whole process is repeated until the largest number of eras ( k max ) is achieved. In addition, the fmGA can also operate over epochs ( e ). An epoch carries out a complete process that begins from the first era to the maximum number of eras.
The number of epochs ( e max ) must be specified by user and it can be as many as desired. The algorithm finally terminates when the maximum number of epochs is reached. 2.4. Weighted Support Vector Machines (wSVM)
The weighted Support Vector Machine (wSVM) is the former name of fuzzy support vector machine (fSVM 3 )( Bao et al., 2005 ).
In wSVM, a weighting value is a fuzzy membership addressed at each training data point. The original SVM treats all training points of a given class uniformly. However, in time series fore-casting, it is common that data sets are unequally important.
Therefore, wSVM was proposed to enhance SVM in reducing the effect of outliers, noise and irrelevant data ( Lin and Wang, 2002 ).
In wSVM, a weighting value is applied to each input point in SVM; this allows different input points to contribute unevenly to the learning of the regression function. With regard to time series prediction problems, older training data points are associated with lower weights so that their influences are reduced. The following section describes the mathematical formulation of wSVM ( Bao et al., 2005 ; Cheng and Roy, in press ).

Given a set S of training data points accompanied with weights  X  y , x 1 , s 1  X  , ... ,  X  y m , x m , s m  X  X  1  X  where x i A R n is the input vector, y i A R is the desired output, and s non-negative s represents the lower bound of weights. The wSVM for regression analysis requires solving the following optimization problem: Minimize 1 2 ww  X  C
Subjected to where C is the penalty parameter and j  X  x  X  denotes the high dimensional feature space, which is mapped from input space x . x and x n i are upper and lower training errors. For wSVM, a smaller weighting value s i lessens the effect of the parameter x
Thus, the corresponding data point f  X  x i  X  is considered as less relevant.

The aforementioned optimization problem can be transformed into following dual form: Maximize 1 2 Subjected to and the Karush X  X uhn X  X ucker condition for optimality is given as a i  X  e  X  x i y i  X  wx i  X  b  X  X  0 , i  X  1 , ... , l  X  4  X  a n i  X  e  X  x n i  X  y i wx i b  X  X  0 , i  X  1 , ... , l  X  5  X   X  s i C a i  X  x i  X  0 , i  X  1 , ... , l  X  6  X   X  s i C a n i  X  x n i  X  0 , i  X  1 , ... , l  X  7  X  where a  X  n  X  i denotes support value of training data point iK  X  x kernel function which governs the mapping of data from input space to feature space.
 problems) in which a point from the recent past is weighted more heavily than points further in the past, the time function f  X  t be selected for determination of the wSVM X  X  s i . Lin and Wang (2002) proposed three time functions (linear, quadratic, and exponential), as shown in Eqs. (8), (9) and (10). Those three time functions were applied to financial time series forecasting pro-blems by Khemchandani et al. (2009) who demonstrated the ability of this approach to achieve superior results to SVM s i  X  f l  X  t i  X  X  1 s t s i  X  f q  X  t i  X  X  X  1 s  X  t i t 1 t s i  X  f e  X  t i  X  X  1 1  X  exp  X  s 2 s  X  t determined the trade-off between margin maximization and violation error minimization, was an issue that required appro-priate handling. Another point of concern was kernel parameter (e.g., gamma, g ) which must also be tuned properly to improve predictive accuracy. In addition, using wSVM requires users to set another parameter, namely the weighting data parameter, s .In summary, three different parameters must be optimized, includ-ing the penalty parameter ( C ), kernel parameter ( g ), and s .To overcome this challenge, an optimization technique (e.g., fmGA) may be used to identify the optimum values of parameters simultaneously ( Cheng and Wu, 2009 ). 3. Estimate at Completion using time-depended evolutionary fuzzy support vector machine inference model (EAC-EFSIM T accomplished by Chen (2008) and Cheng et al. (2010) . Previous research works have identified ten factors (see Table 2 ) which have significant influence on EAC prediction problems. The structure of EAC-EFSIM T is depicted in Fig. 2 . The rest of this section dedicates to describe the model in detail. 3.1. Historical data reinforced concrete building projects (labeled projects A through
M) executed between 2000 and 2007 by one construction com-pany headquartered in Taipei City. All projects were carried out in northern Taiwan with heights ranging from 9 to 17 stories (including underground floors). Contract values ran from NT$80 million to NT$1.1 billion. Total floor areas for the projects ranged from 2094 m 2 to 31,797 m 2 . Besides, construction durations varied between 15 to 63 months. Historical data sets were divided into training (from A to K) and testing sets (L and M). The training and testing data sets consist of 269 and 37 data cases, respec-tively. Table 3 shows the 10 input variables from project C, which had 20 completion periods. 3.2. Data weighting
For time series prediction problems, some data points are more important for the training process than the others due to time relevance and degree of noise corru ption. To prioritize data points, the model applies a weighting value to each input point drawn from three types of time functions, as shown in Eqs. (8), (9), and (10). By doing so, different input points contribute dissimilarly to the learning of the regression function, and can improve the SVM by diminishing the effect of outliers and noisy data. This weighting process treats the last data point x m as the most important (assigned an weighting value s of 1) and the first data point x 1 as the least important (assigned a weighting value equal to s , a relatively small number). In this step, the value of s was generated randomly and encoded by fmGA. 3.3. Fuzzification
In this process, each normalized input attribute is converted into corresponding membership grades. This mapping of crisp to fuzzy quantity is carried out by a set of membership function (MF) generated and encoded by fmGA. This study used trapezoi-dal MFs and triangular MF shapes (see Fig. 3 ) that, in general, may be developed by referencing summit points and widths. The summit and width representation method (SWRM) ( Ko, 2002 ) was used in this study to encode complete MF sets (see Fig. 3 (c)). Fig. 4 illustrates the fuzzification process. For more details of SWRM and fuzzification process, readers are guided to previous works of Ko (2002) and Cheng and Roy (in press) . 3.4. Weighted Support Vector Machine (wSVM)
In this step, wSVM are deployed to handle fuzzy input X  X utput mapping. Fuzzification process output, in the form of membership grades, acts as fuzzy input for wSVM, which trains this data set to using time function 7. EAC calculation (EAC = ETC + AC) 8. Fitness evaluation obtain the prediction model. wSVM uses penalty ( C ) and kernel parameters ( g ), which are generated randomly and optimized using fmGA. This study used the RBF kernel as a reasonable first choice ( Cheng and Wu, 2009 ). Moreover, it is noticed that this research utilized the source code of SVM provided in LIBSVM ( Chang and Lin, 2001 ). 3.5. Defuzzification
Once wSVM has finished the training process, output numbers are expressed in terms of the fuzzy output, and must be converted into crisp numbers. Employing fmGA, the model generates a random dfp substring and encodes it to convert wSVM output.
This evolutionary approach is simple and straightforward, as it uses dfp as a common denominator for wSVM output. 3.6. Fast messy GA searching
In the model, fmGA is utilized to search simultaneously for the fittest shapes of MFs, dfp , penalty parameter C , RBF kernel parameter g and the weighting data parameter s . In fmGA, the chromosome that represents a possible solution for searched parameters consists of five parts, namely the MF substring, dfp substring, penalty parameter substring, kernel parameter substring, and lower bound of weighting data substring ( Cheng and Roy, in press ). The chromosome is encoded into a binary string. Fig. 5 illustrates the chromosome structure. Additionally, in chromosome design, the required length of each binary sub-string must be defined beforehand ( Cheng and Wu, 2009 ). Table 4 summarizes parameter settings and number bits required for chromosome design. 3.7. EAC calculation
This step occurs between defuzzification and fitness evalua-tion steps. Defuzzification results are in the form of an  X  X  X stimate to completion percentage X  X  (ETC). Actual Cost (AC) percentage of completed jobs is then added to the ETC in order to obtain an EAC value, which is the final output of the model, as defined in the following equation:
EAC  X  AC p  X  ETC p  X  11  X  where ETC is a value used to determine anticipated expenditures necessary to complete remaining project work. AC percentage is defined as the ratio of actual AC value to the budget at completion (BAC), where BAC represents the cost of the project at the point of completion of all contracted works. EAC replaces BAC for the predicted total cost of the project at a specific period of time during construction. 3.8. Fitness evaluation.

In fmGA, the fitness function was designed to measure model accuracy and the fitness of generalization properties ( Cheng et al., 2009 ). This function describes the fittest shape of MFs, optimized defuzzification and wSVM parameters. The fitness function inte-grates model accuracy and model complexity, as expressed in the following equation: f  X  c aw s er  X  c cw mc  X  12  X  where c aw represents the accuracy weighting coefficient, s represents the prediction error between actual output and desired output, c cw represents the complexity weighting coefficient, and mc represents model complexity, which can be quantified by counting the number of support vectors.

The optimization process aims to minimize the prediction error ( s er ) in the training data set. However, to avoid over-fitting, it is needed to reduce the model complexity ( mc ) by limiting the number of support vectors. In other words, the fitness function represents a trade-off between model accuracy and model generalization. 3.9. Termination criterion
The process is t erminated when termination criterion is satis-fied. Prior to this point, the model will proceed to the next generation. As the model uses fmGA, the termination criterion used here is either era number ( k ) or epoch number ( e ). The loop process will end when specified criterion is met. 3.10. Optimal prediction model.

The loop stops once the termination criterion is fulfilled. This means that the prediction model has identified the input/output mapping relationship with optimal tuning parameters. After-wards, the model has finished the training process and it is ready for predicting new facts. 4. Simulation results and comparison
Prior to perform prediction, the cost database of projects was established. Database record contains planned values and actual values for each month and calculates the difference between the two. The mapping relationship between ten influencing factors and cost value (EAC) is discovered by case learning and EAC-EFSIM T training. After the training process, the complete cost of a new project is forecasted by inputting the monthly cost information into the optimal prediction model.

This section validates the performance of the proposed prediction model. To show the EAC-EFSIM T as more accurate and reliable than traditional methods used in construction indus-try ( Christensen et al., 1995 ) as well as recently proposed EFNIM ( Chen, 2008 ) and ESIM ( Cheng et al., 2010 ), average error, deviation, and RMSE are employed to evaluate the proposed model and performance achieved by each method (see Table 1 ).
Table 5 shows the EAC average error from each method for both training and testing data sets. It is observable that AI approaches obtained results superior to traditional methods. Furthermore, in comparing between the three AI approaches, the proposed method, EAC-EFSIM T (either using linear, quadratic or exponential time functions), outperformed EFNIM and ESIM.
This research used a 5% error as its qualification criterion. Based on this criterion, the qualified percentage obtained from the pro-posed model, with any time function , for training data sets are higher than 82%. Moreover, it is observable that average errors of EAC-EFSIM T for all testing sets are below 5%. Meanwhile, the qualified percentage for EAC1 as the best traditional method reached only 36.36% for training data sets and 50% for testing sets. The qualified percentage of EFNIM achieved 55% for training data sets and 50% for testing data sets. Additionally, the qualified percentage of ESIM reached 45% for training data sets and 50% for testing data sets.
Two Tables 6 and 7 exhibit details of project L and M deviation from the testing sets. In both tables, comparisons are given only for EFNIM, ESIM, and EAC-EFSIM T due to the poor results obtained when employing the eight traditional methods. Project L and M consist of 20 and 17 completion periods, respectively. In testing data set L, deviations of EAC prediction obtained from both EFNIM and ESIM are, to some degree, unstable in completion periods from 11 to 15 (errors surpass 10%). Observably, ESIM yields undesirable forecasting error (9.84%) in the last completion period of data set M. In contrary, EAC-EFSIM T (using linear, quadratic or exponential time functions) provides much more stable forecasts throughout completion periods especially when exponential time function is utilized (maximum prediction errors of data set L and M are 5.28% and 2.96%, respectively). Such results show the better ability of the proposed model to cope with time-dependent information inherent in EAC data.
 Moreover, comparing EFNIM and ESIM to EAC-EFSIM T , the
RMSE obtained by using the newly proposed model performs mostly better than the others, as shown in Table 8 . Such results again prove that the new model yields better forecasting perfor-mance than the previous models. For training data sets, the RMSE obtained for linear function, quadratic function, and exponential function were 0.0408, 0.0386, and 0.0288, respectively, while for testing data sets, the RMSE (linear function, quadratic function, and exponential function) obtained for project L were 0.0258, 0.0298, and 0.0253, and 0.017, 0.0102, and 0.0097, respectively, for project M.

As the model using the exponential function possessed better performance in terms of both training and testing data sets, this function is deemed best suited for EAC forecasting problems.
Obviously, this weighting strategy is understandable in the context of EAC prediction, as in the initial stage of a project, fewer tasks were commenced and thus only a small proportion of work was carried out. Potential problems and working informa-tion were subsequently less likely to appear. In addition, owners imposed few change orders on the contractor during the early stage. Thus, the weighting value for data points collected in this period should be low. Meanwhile, as the project moved forward, more information is revealed. Additionally, financial, technical, and environmental problems began to emerge, which made owners more likely to require changes or adjustments. These factors contribute to the adjustments of costs and schedules for certain activities, leading to alterations in the project EAC. There-fore, data points collected at later periods of time are considered more valuable and given greater weights. Accordingly, the expo-nential weighting function is found to best describe this strategy as it delivers the best forecasting results. 5. Conclusions
This paper presented a new prediction model, EAC-EFSIM T ,to facilitate construction managers in cost control through improve-ment of EAC forecasting. The EAC-EFSIM T model was developed by a fusion of fuzzy logic, wSVM, and fmGA. Fuzzy logic was utilized to enhance the ability of approximate reasoning. Mean-while, wSVM was utilized to handle fuzzy input X  X utput mapping, and focused on time series data features. Finally, fmGA was applied as the optimizer to search for fuzzy logic and weighted
SVM optimum parameters. The EAC-EFSIM T integrated the super-ior characteristics of several current AI techniques to achieve comparatively better prediction performance and virtually elim-inate the disadvantages of various traditional techniques.
Furthermore, the new developed model has the capability to diminish considerably human intervention in determining shapes of membership functions from questionnaire surveys and expert interviews. As a consequence, the model may be used by profes-sionals without domain or AI knowledge.
 Result comparisons demonstrate the prediction accuracy of EAC-EFSIM T superior to all previous EAC calculation methods.
EAC-EFSIM T successfully reduced maximum error margins in each forecasting period, decreased average prediction error, and yielded considerably stable results through all completion peri-ods. These facts demonstrate the strong potential of EAC-EFSIM as a predictive tool for project managers to help control construc-tion cost effectively.
 References
