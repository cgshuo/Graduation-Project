 Vincent Conitzer CONITZER @ CS . CMU . EDU When an agent is inserted into an unfamiliar environment with some objective, two goals present themselves. The first is to learn the relevant aspects of the environment, so that eventually, its behavior is optimal or near optimal with regard to the given objective. The second is to minimize the cost of learning to behave well. This can be done by minimizing the time necessary to learn enough to perform well, but also by ensuring that its behavior in the learning process, while not yet optimal or near optimal, is at least reasonably good with regard to the objective. There is of-ten an exploration/exploitation tradeoff here: attempting to learn fast often requires disastrous short term results, while slow learning may accumulate large losses even if the loss per unit time is small.
 Learning in games (for a review, see (Fudenberg &amp; Levine, 1998)) is made additionally difficult because the learner is confronted with another player (or multiple other players). If the other player plays in a predictable, repetitive manner, this is no different from learning in an impersonal, disin-terested environment. Usually, however, the other player changes its strategy over time. One reason for this may be that the other player is also learning. A less benign reason, however, may be that the opponent is aware of the learner X  X  predicament and is trying to exploit its superior knowledge. This is the case that we study.
 In the case where an opponent is trying to exploit the learner X  X  lack of knowledge about the game, it becomes especially important to focus on the cumulative cost of learning rather than the time the learning takes. It is likely that the opponent will allow the learner to learn the game very quickly, if the opponent can take tremendous advan-on the learner X  X  part that allows this should not be consid-ered good. On the other hand, a learning strategy that may learn the relevant structure of the game only very late or even never at all, but allows the opponent to take only min-imal advantage, should be considered good. This analysis is consistent with numerous learning results in the game theory and machine learning literatures which guarantee convergence to a strategy OR that the payoffs approach those of the equilibrium (e.g. (Jehiel &amp; Samet, 2001; Singh et al., 2000)). It suggests a Win-or-Learn-Fast, or WoLF, approach (a term coined by Bowling and Veloso (Bowling &amp; Veloso, 2002), though they actually just pursued conver-gence results). Various previous work has considered the case where learning players are concerned with their long-term losses, for instance when players have beliefs about the opponents X  strategies (Kalai &amp; Lehrer, 1993). Much of the prior work on learning in games in the machine learning literature did not consider such a metric of the per-formance of a learning strategy (Littman, 1994; Hu &amp; Well-man, 1998). In contrast, our work is especially closely re-lated to recent work by Brafman and Tennenholtz on learn-ing in stochastic games (Brafman &amp; Tennenholtz, 2000), where the opponent can make it difficult to learn parts of the game, leading to a complex exploration vs. exploita-tion tradeoff (building upon closely related work (Kearns &amp; Singh, 1998; Monderer &amp; Tennenholtz, 1997)); and on learning equilibrium (Brafman &amp; Tennenholtz, 2002), where the agents X  learning algorithms over a class of games are considered as strategies themselves.
 In another strand of research, Auer et al. also study the problem of learning a game with the goal of minimizing the cumulative loss due to the learning process, with an ad-versarial opponent (Auer et al., 1995). (This problem is studied towards the end of that paper.) They study the case where the learner knows nothing at all about the game (ex-cept the learner X  X  own actions and bounds on the payoffs), and they derive an algorithm for this general case, which improves over previous algorithms by Ba  X  nos (Banos, 1968) and Megiddo (Megiddo, 1980). (Some closely related re-search makes the additional assumption that the learner, at the end of each round, gets to see the expected payoff for all the actions the learner might have chosen, given the opponent X  X  mixed strategy (Freund &amp; Schapire, 1999; Fu-denberg &amp; Levine, 1995; Foster &amp; Vohra, 1993; Hannan, 1957). We will not make this assumption here.) The main difference between that line of work and the framework presented here is that our framework allows the learner to take advantage of partial knowledge about the game (that is, knowledge that the game belongs to a certain family of games). This allows the learner to potentially perform In this paper, we introduce the BL-WoLF framework, where a learner X  X  strategy is evaluated by the loss it can expect to accrue as a result of its lack of knowledge. (We consider the worst-case loss across all possible opponents as well as all possible games within the class considered. BL stands for  X  X ounded loss X .) We present a guaranteed version of learnability where the learner is guaranteed to lose no more than a given amount, and a nonguaranteed version where the agent loses no more than a given amount in expecta-tion . We also allow for approximate learning in both cases, where we only require that the agent comes close to act-ing optimally. The framework is applicable to any class of (repeated) games, and allows us to measure the inherent disadvantage in that class to a player that initially cannot distinguish which game is being played. It does not assume a probability distribution over the games in the class. We do not consider difficulties of computation in games; rather we assume the players can deduce all that can be deduced from the knowledge available to them. While some of the most fundamental strategic computations in game theory have unknown (Papadimitriou, 2001) or high (Conitzer &amp; Sandholm, 2003) complexity in general, zero-sum (Luce &amp; Raiffa, 1957) and repeated (Littman &amp; Stone, 2003) games tend to suffer fewer such problems, game families in this paper, computation will be simple. The rest of this paper is organized as follows. In Section 2, we give some basic definitions and known results. We present guaranteed BL-WoLF learnability in Section 3, and its approximate version in Section 4. We present nonguar-anteed BL-WoLF learnability in Section 5, and its approx-imate version in Section 6. Throughout the paper, there will be two players: the learner (player 1 ) and the opponent (player 2 ). Because we try to assess the worst-case scenario for the learner, restrict-ing ourselves to only one opponent is without loss of generality X  X f there were multiple opponents, the worst-case scenario for the agent would be when the opponents all colluded and acted as a single opponent.
 zero-sum game over and over. Player 2 knows the game; player 1 (at least initially) only knows that it is in a larger family of games. In this section, we will first define the stage game, and discuss what it means to play it well on its own. We then define the uncertainty that player 1 has about the game. Finally, we define what strategies the players can have in the repeated game. Definitions on what it means for the learner to play the repeated game well are presented in later sections. 2.1. Zero-sum game theory for the stage game Definition 1 A (stage) game consists of sets of actions A ;A 2 for players 1 and 2 respectively, together with (in the case of deterministic payoffs) a function u : A U player i (usually simply IR ); or (in the case of stochastic payoffs) a function p P ( U 1 U 2 ) is the set of probability distributions over util-ity pairs. We say the game is zero-sum if the utilities of agent 1 and 2 always sum to a constant.
 We often say that the random selection of an outcome in a game with stochastic payoffs is done by Nature . For the following strategic aspects, it is irrelevant whether Nature plays a part or not.
 Definition 2 A (stage-game) strategy for player i is a prob-ability distribution over A on one action, it is a pure strategy , otherwise it is a mixed strategy .) A pair of strategies in Nash equilibrium if neither player can obtain higher ex-pected utility by switching to a different strategy, given the other player X  X  strategy. A strategy if The following theorem shows the relationship between maximin strategies and Nash equilibria in zero-sum games. Informally, it shows why, against a knowledgeable oppo-nent, a player is playing well if and only if that player is playing a maximin strategy.
 Theorem 1 (Known) In zero-sum games, a pair of strate-gies are both maximin strategies. The expected utility that each player gets in an equilibrium is the same for every equilib-rium; this expected utility (for player 1 ) is called the value V of the game.
 Thus, player 1 is guaranteed to get an expected utility of at least V by playing a maximin strategy (and player 2 can make sure player 1 gets at most V by playing a maximin strategy). We call a strategy strategy if it guarantees an expected utility of V  X  . The stage-game loss of player 1 in playing the stage game once is
V minus the utility player 1 received. 2.2. What player 1 does not know Player 1 (at least initially) does not know which of a family of zero-sum stage games is being played. Such a family is defined as follows: Definition 3 A parameterized family of stage games with deterministic (stochastic) payoffs is defined by action sets A G ( A 1 ;A 2 ) ( g : K !G s ( A 1 ;A 2 ) ), where G d ( A 1 ( G deterministic (stochastic) payoffs with action sets A Here, player 1 does not know the parameter k 2 K cor-this paper, the elements of K will take many forms, such as integers, permutations, and subsets. Player 1 can eliminate values of K on the basis of outcomes of games played. We note that there is no probability distribution on the fam-ily of games. Rather, we assume the game is adversarially chosen relative to the learner X  X  learning strategy. 2.3. Strategies in the repeated game A strategy in the repeated game (in the case of player 1 ,a learning strategy) prescribes a stage-game strategy given any history of what happened in previous stage games. Thus, the stage-game strategy can be conditional on the players own past actions, the other player X  X  past actions, and past payoffs. In our paper, it will usually be sufficient for it to just be conditional on player 1  X  X  knowledge about the game. To evaluate how well player 1 is doing, we de-fine player 1  X  X  (cumulative) loss as the sum of all stage-game losses. Thus, if player 1 knew the game, playing the maximin strategy forever would give an expected loss of at most 0 against any opponent. (We do not use a discounting rate; rather, when we aggregate utilities, we consider the In the simplest form of learning in our framework, there is a learning strategy for player 1 such that, having accumulated a given amount of loss, player 1 is guaranteed to know enough about the game to play it well. In this section, we give the formal definition of this type of learnability, and demonstrate that some example game families (including games with stochastic payoffs) are learnable in this sense. Definition 4 A parameterized family of games is guaran-teed BL-WoLF-learnable with loss l if there exists a learn-ing strategy for player 1 such that, for any game in the family, against any opponent, the loss incurred by player 1 before learning enough about the game to construct a maximin strategy is never more than l .
 Game family description 1 6 For a given n , the game family get-close-to-the-target is defined as follows. Play-ers 1 and 2 both have action space A = f 1 ; 2 ;:::;n The outcome function is defined by a parameter k 2 f 1 ; 2 ;:::;n g , that the players try to get close to. Given the actions by the players, the outcome of the game is as follows (winning gives utility 1 , losing utility  X  1 ):
If j a
If a wins if a&gt;k ;
Otherwise ( a Player 1 initially does not know: the parameter k . Theorem 2 The game family get-close-to-the-target is guaranteed BL-WoLF-learnable with loss d log( n ) e . Proof : We first observe that if we ever have a draw, player 1 can immediately infer k  X  X t is the average of the players X  actions. Also, after any number of rounds, the set of possi-ble values for k that are consistent with the outcomes so far is always an interval f k min ;k min +1 ;:::;k max g . (The set of possible values for k that are consistent with a single outcome is always an interval, and the intersection of two intervals is always an interval.) Now consider the follow-ing learning strategy for player 1 : always play the action in the middle of the remaining interval, a If player 1 loses, it can be concluded that k is on the side of a 1 where player 2 played. ( a 2 a 1 ) k&lt;a 1 and a 2 &gt;a 1 half (sometimes the remainder is less than half, because the action player 1 played is also eliminated; it is never more). So, after d log( n ) e losses, player 1 knows k , and the max-imin strategy (which is simply to play k ).
 The parameter to be learned need not always be an integer. In the next example, it is a permutation of a finite set. Game family description 2 For given m&gt; 2 and n , the game family generalized-rock-paper-scissors-with-duds is defined as follows. Players 1 and 2 both have action space A = f 1 ; 2 ;:::;m + n g . The outcome function is defined by a permutation f : f 1 ; 2 ;:::;m + n g!f 1 ; 2 ;:::;m + n The set of duds is given by f i : m +1 f ( i ) m + n g . Given the actions by the players, the outcome of the game is as follows (winning gives utility 1 , losing utility  X  If only one player plays a dud, that player loses;
If neither player plays a dud and f ( a 1( modm ) , player i wins (effectively, the nonduds are ar-ranged in a circle, and playing the action right after your opponent X  X  in the circle gives you the win); Otherwise, we have a draw.
 Player 1 initially does not know: the permutation f .(We observe that for m =3 and n =0 , we have the classic rock-paper-scissors game.) Theorem 3 The game family generalized-rock-paper-scissors-with-duds is guaranteed BL-WoLF-learnable with loss m  X  1 if m is even, or with loss m if m is odd. If n =0 it is guaranteed BL-WoLF-learnable with loss 0 .
 Proof : Consider the following learning strategy for player 1 . Keep playing action 1 first; then, whenever player wins a round, switch to the action that he just won with, and keep playing that until player 2 wins again. Because it is impossible to win when playing with a dud, the first action that player 2 wins a round with must be a nondud. After this, player 2 can win only by playing the next ac-tion in the circle of nonduds. Thus, every loss reveals the next element in the circle. Thus, after m losses, the whole circle of nonduds is revealed and player 1 can choose a maximin strategy. (For instance, randomizing uniformly over the nonduds.) In the case where m is even, only m  X  1 losses are needed, as this reveals the whole circle but one X  and when m is even, it is a maximin strategy to randomize uniformly over all the nonduds i such that f ( i ) is even (or all the nonduds i such that f ( i ) is odd), and we can deter-mine one of these two sets even with a  X  X ap X  in the circle. Finally, if n =0 , we need not learn anything about f at all: simply randomize uniformly over all the actions.
 Game families with stochastic payoffs can also be guaran-teed BL-WoLF-learnable. The following modification of the previous game illustrates this.
 Game family description 3 The game family random-orientation-generalized-rock-paper-scissors-with-duds is defined exactly as generalized-rock-paper-scissors-with-duds , except each round, Nature flips a coin over the orientation of the circle of nonduds. That dud and f ( a wins; otherwise, if neither player plays a dud and f ( a i )= f ( a  X  i )  X  1( modm ) , player i wins. The other cases are as before: nonduds still (always) beat duds, and we have a draw in any other case.
 Player 1 initially does not know: the permutation f . Theorem 4 The game family random-orientation-generalized-rock-paper-scissors-with-duds is guaranteed BL-WoLF-learnable with loss 1 (or loss 0 if n =0 ). Proof : We simply observe that playing any nondud action is a maximin strategy in this case. (Any nondud action is as likely to lose against it as to win.) Player 1 will know such an action upon being beaten once (or, if there are no duds, player 1 will know such an action immediately). We now introduce approximate BL-WoLF-learnability. Definition 5 A parameterized family of games is guaran-teed approximately BL-WoLF-learnable with loss l and precision if there exists a learning strategy for player 1 such that, for any game in the family, against any opponent, the loss incurred by player 1 before learning enough about the game to construct an -approximate maximin strategy is never more than l .
 To save space, we only present one straightforward approx-imate learning result on a game family we have studied al-ready, to illustrate the technique. A similar result can be shown for generalized-rock-paper-scissors-with-duds . Theorem 5 The game family get-close-to-the-target is guaranteed approximately BL-WoLF-learnable with loss r and precision 1  X  2 r Proof : We consider the same learning strategy as before, where we always play the middle of the remaining interval. Randomizing over the remaining interval will give at least Guaranteed learning (even approximate) is not always pos-sible. In many games, no matter what learning strategy player 1 follows, it is possible that an unlucky sequence of events leads to a tremendous loss for player 1 without teaching player 1 anything about the game. Such unlucky sequences of events can easily occur in games with stochas-tic payoffs, but also in games with deterministic payoffs where player 1  X  X  only hope of learning against an adver-sarial opponent is by using a mixed strategy. (We will see examples of both these cases later in this section.) Never-these games that in all likelihood will allow player learn about the game without incurring too much of a loss. In this section, we present a more probabilistic definition of learnability; we show that it is strictly weaker than guaran-teed BL-WoLF-learnability; we present a useful lemma for showing this type of BL-WoLF-learnability; and we apply this lemma to show BL-WoLF-learnability for some games that are not guaranteed BL-WoLF-learnable. 5.1. Definition Definition 6 A parameterized family of games is BL-WoLF-learnable with loss l if there exists a learning strat-egy for player 1 such that, for any game in the family, against any opponent, and for any integer N , player 1  X  X  expected loss over the first N rounds is at most l . We now show that BL-WoLF-learnability is indeed a weaker notion than guaranteed BL-WoLF-learnability. Theorem 6 If a parameterized family of games is guaran-teed BL-WoLF-learnable with loss l , it is also BL-WoLF-learnable with loss l .
 Proof : Given the learning strategy that will allow player 1 to learn enough about the game to construct a maximin egy 0 which plays until the maximin strategy has been learned, and plays the maximin strategy forever after that. Then, after N rounds, if we are given that no maximin strat-egy has been learned yet, the loss must be less than l .Given that a maximin strategy was learned after i N rounds, the loss up to and including the i th round must have been less than l , and the expected loss after round i is at most (because a maximin strategy was played in every round af-ter this). It follows that the expected loss is at most l 5.2. A central lemma The next lemma will help us prove the BL-WoLF-learnability of games that are not guaranteed BL-WoLF-learnable.
 Lemma 1 Consider a learning strategy for player 1 that plays the same stage-game strategy every round until some learning event. (Call a sequence of rounds between learn-ing events throughout which the same stage-game strategy is played an epoch .) Suppose that the following two facts hold for any game in the parameterized family:
For any epoch i  X  X  stage-game strategy i any stage-game strategy nonzero probability cause the learning event that changes the epoch to i +1 , or will not give player 2 any advantage (i.e. player 1  X  X  expected loss from the round when player plays
For any of those strategies bility cause the learning event that changes the epoch to (Here ( i 1 , and p i ( i the learning event that changes the epoch to i +1 .) Then with this learning strategy, the family of games is BL-WoLF-learnable with loss Proof : Given the number N of rounds, divide up player 1  X  X  total loss l over the epochs. That is, for epoch i ,wehave l = l = mize the expectation of a given l gives this opponent any advantage in this epoch (player 1 is already playing a maximin strategy), the expected value of l i cannot exceed 0 c i . If there is an action that gives the opponent some advantage, by the first fact, it causes the end of the epoch with some nonzero probability. In this case, playing an action that does not cause the end of the epoch with some nonzero probability is a bad idea for the opponent, because doing so gives the opponent no advan-of rounds N . So we can presume that the opponent only plays actions that cause the end of the epoch with some nonzero probability. Now suppose that there is no limit to the number of rounds, but the opponent is still restricted to playing actions that cause the end of the epoch with some nonzero probability. (This is still a preferable scenario to the opponent.) In this scenario, we have max max follows that max follows that the expectation of any l any opponent. Thus (by linearity of expectation) the total expected loss is bounded by 5.3. Specific game families We first give an example of a game family with stochastic payoffs where guaranteed BL-WoLF learning is impossible because Nature might be noncooperative.
 Game family description 4 For given n;p the game family get-close-to-one-of-two-targets is defined exactly as get-close-to-the-target , except now there are two k ;k 2 2f 1 ; 2 ;:::;n g , with k 1 6 = k 2 . Each round, Nature randomly chooses which of the two is  X  X ctive X  ( k with probability p have won get-close-to-the-target with that k of winning is dependent on j : the winner receives r r 6 = r 2 ; the loser gets 0 ).
 Player 1 initially does not know: the parameters k Get-close-to-one-of-two-targets is not guaranteed BL-WoLF-learnable, for the following reason. Consider the scenario where k right of the middle, and player 2 is consistently playing ex-actly in the middle. Now, regardless of which action player 1 plays, for one of the k tive; and player 1 will be able to infer nothing more than which side of the middle that k happens to keep picking k cumulate a huge loss without learning anything more than which sides of the middle the k that, if one of the k the other, this can leave us arbitrarily far away from know-ing a maximin strategy. Nevertheless, with the probabilis-tic definition, get-close-to-one-of-two-targets is BL-WoLF-learnable for a large class of values of the parameters p p , r k is much more likely and valuable than the other), as the next theorem shows.
 Theorem 7 If p ily get-close-to-one-of-two-targets is BL-WoLF-learnable with loss d log( n ) e r Proof : First we observe that if p k to show is that both players playing k When the other player is playing k gives at most p rewards given in a round, player 1 can tell which of the k lowing learning strategy for player 1 : ignore the rounds in which k as we did for get-close-to-the-target in the proof of The-orem 2, as if k play the action in the middle of the remaining interval for k , setting a we do not update our stage-game strategy until we lose or draw a round where k apply Lemma 1: such a change in strategy will be the end of an epoch. By similar reasoning as in Theorem 2, we will know the value of k (after which there is one more epoch where we play the maximin strategy k We now show that the required preconditions of Lemma 1 are satisfied. First, if a stage-game strategy for player has no chance of changing the epoch, that means that with that stage-game strategy, player 2 has no chance of win-ning or drawing if k get at most p thus has no advantage. Second, if a stage-game strategy for player 2 causes the change with probability p , the expected utility of that stage-game strategy for player 2 can be at most pr in the round to player 1 is at most pr the c can set to 0 , because in the corresponding epoch we will be playing the maximin strategy), and we can conclude by Lemma 1 that the game family is BL-WoLF-learnable with loss d log( n ) e r We now give an example of a game family with determinis-tic payoffs where guaranteed BL-WoLF learning is impos-sible because the opponent might be lucky enough to keep winning without revealing any of the structure of the game. Game family description 5 For given m&gt; 0 and n , the game family generalized-matching-pennies-with-duds is defined as follows. Players 1 and 2 both have action space A = f 1 ; 2 ;:::;m + n g . The outcome function is defined by a subset D A , with j D j = n , of duds. Given the actions by the players, the outcome of the game is as follows (the winner gets 1 , the loser 0 ): if one player plays a dud and the other does not, the latter wins. Otherwise, if both players play the same action, player 2 wins; and if they play different actions, player 1 wins. Player 1 initially does not know: the subset D . (We observe that for m =2 and n =0 , we have the classic matching-pennies game.) Generalized-matching-pennies-with-duds is not guaranteed BL-WoLF-learnable, because for any learning strategy for player 1 , it is possible that player 2 will happen to keep picking the same action as player 1 in every round. In this case, player 1 accumulates a huge loss with-out learning anything at all about the subset B .Nev-ertheless, generalized-matching-pennies-with-duds is BL-WoLF-learnable, as the next theorem shows.
 Theorem 8 The game family generalized-matching-pennies-with-duds is BL-WoLF-learnable with loss n . Proof : We first observe that player 1 is guaranteed to win at nonduds; this is in fact the maximin strategy. Now consider the following learning strategy for player 1 : in every round, randomize uniformly over all the actions besides the ones player 1 knows to be duds. We will again use Lemma 1. An epoch here ends when player 1 can classify another action as a dud; thus, there can be at most n +1 epochs, and in the last epoch player 1 is playing the maximin strategy and player 2 can have no advantage. We now show that the re-quired preconditions of Lemma 1 are satisfied. First, in any epoch but the last, player 1 plays duds with some nonzero probability; and if player 2 plays a nondud when player 1 plays a dud, player 1 will realize that it was a dud and the epoch will end. Thus, if player 2 plays a nondud with nonzero probability, the epoch will end with some proba-bility. On the other hand, if player 2 always plays duds, player 2 will win only if player 1 happens to play the same is the number of actions player 1 is randomizing over. Be-cause q&gt;m , this means player 2 wins with probability first precondition is satisfied. Second, if in a given epoch where player 1 is randomizing over q actions (the m non-duds plus q  X  m duds), player 2 plays a stage-game strat-egy that plays a nondud with probability p , this will end the epoch with probability at least p q  X  m that player 2 wins is at most p q  X  m so that the expected loss in the round to player 1 is at n +1 th one which we can set to 0 , because in the corre-sponding epoch we will be playing the maximin strategy), and we can conclude by Lemma 1 that the game family is BL-WoLF-learnable with loss n . Definition 7 A parameterized family of games is approx-imately BL-WoLF-learnable with loss l and precision if there exists a learning strategy for player 1 such that, for any game in the family, against any opponent, and for any integer N , player 1  X  X  expected loss over the first N rounds is at most l + N .
 We now show that approximate BL-WoLF-learnability is indeed a weaker notion than guaranteed approximate BL-WoLF-learnability.
 Theorem 9 If a parameterized family of games is guar-anteed approximately BL-WoLF-learnable with loss l and precision , it is also approximately BL-WoLF-learnable with loss l and precision .
 Proof : Given the learning strategy that will allow player 1 to learn enough about the game to construct an -approximate maximin strategy with loss at most l , con-sider the learning strategy 0 which plays until the -approximate maximin strategy has been learned, and plays the -approximate maximin strategy forever after that. Then, after N rounds, if we are given that no -approximate maximin strategy has been learned yet, the loss must be less than l . Given that an -approximate maximin strategy was learned after i N rounds, the loss up to and including the i th round must have been less than l , and the expected loss after round i is at most ( N  X  i ) (because an -approximate maximin strategy was played in every round after this). It follows that the expected loss is at most l + N . A version of Lemma 1 for approximate learning that takes advantage of the fact that we are allowed to lose per round is straightforward to prove. We will not give it or any ex-amples of its application here, because of space constraint. We presented a general framework for characterizing the cost of learning to play an unknown repeated zero-sum game. In our model, the game falls within some family that the learner knows, and subject to that, the game is ad-versarially chosen. In playing the game, the learner faces an opponent who knows the game and the learner X  X  learn-ing strategy. The opponent tries to give the learner high losses while revealing little about the game. Conversely, the learner tries to either not accrue losses, or to quickly learn about the game so as to be able to avoid future losses (this is consistent with the Win or Learn Fast (WoLF) prin-ciple). Our framework allows for both probabilistic and approximate learning.
 In short, our framework allows one to measure the worst-case cost of lack of knowledge in repeated zero-sum games. This cost can then be used to compare the learnability of different families of zero-sum games.
 We first introduced the notion of guaranteed BL-WoLF-learnability , where a smart learner is guaranteed to have learned enough to play a maximin strategy after losing a given amount (against any opponent). We also intro-duced the notion of guaranteed approximate BL-WoLF-learnability , where a smart learner is guaranteed to have learned enough to play an -approximate maximin strategy after losing a given amount (against any opponent). We then introduced the notion of BL-WoLF-learnability where a smart learner will, in expectation , lose at most a given amount that does not depend on the number of rounds (against any opponent). We also introduced the no-tion of approximate BL-WoLF-learnability , where a smart learner will, in expectation , lose at most a given amount that does not depend on the number of rounds, plus times the number of rounds (against any opponent). We showed, as one would expect, that if a game family is guaranteed (approximately) BL-WoLF-learnable, then it is also (ap-proximately) BL-WoLF-learnable in the weaker sense. We presented guaranteed BL-WoLF-learnability results for families of games with deterministic payoffs (namely, the families get-close-to-the-target and generalized-rock-paper-scissors-with-duds ). We also showed that even families of games with stochastic payoffs can be guaranteed BL-WoLF-learnabile (for example, the random-orientation-generalized-rock-paper-scissors-with-duds game family). We also demonstrated that these families are guaranteed approximate BL-WoLF-learnable with lower cost.
 We then demonstrated families of games that are not guar-anteed BL-WoLF-learnable X  X ome of which have stochas-tic payoffs (for example, the get-close to-one-of-two-targets family) and some of which have deterministic pay-offs (for example, the generalized-matching-pennies-with-duds family). We showed that those families, nevertheless, are BL-WoLF-learnable. To prove these results, we used a key lemma which we derived.
 Future research includes giving general characterizations of families of zero-sum games that are BL-WoLF learn-able with some given cost (for each of our four definitions of BL-WoLF learnability) X  X s well as characterizations of families that are not. Future work also includes applying these techniques to real-world zero-sum games.

