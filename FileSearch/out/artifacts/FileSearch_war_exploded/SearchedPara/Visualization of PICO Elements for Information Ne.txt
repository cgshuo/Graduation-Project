 swers returned by a question-answering (QA) system depends on the quality of ques-tions posed by users. Doctors have difficulty in generating high quality questions that unambiguously and comprehensively defined their information needs [1]. The use of PICO (an acronym for P roblem/P opulation, I ntervention, C omparison and O utcome) framework has been widely accepted for the formulation of answerable clinical ques-PICO elements present. Two examples of questions maintained by the National Li-(Question 1) and  X  X  have a lady with graves X  disease (33 years old). She was trying to get pregnant when she was diagnosed with graves. So the question is, what is the best treatment for graves in someone who is trying to get pregnant, and if we use radioac-tive iodine, how long does she need to wait? X  (Question 2). Both of the questions are categorized under  X  X reatment and Preventio n X . Question 1 represents a definitional the expected treatment outcome, which denote the C and O elements respectively, are not stated in both Questions 1 and 2. As reported in [4], questions with the I/C and O elements are unlikely to go unanswered. Therefore, the visualization of PICO ele-ments in documents relevant to a user X  X  input query has the potential to assist the user in refining his/her information needs. finement of questions relating to treatments and drugs. The proposed user interface is tered by the user contains only the P element ( X  X reast cancer X ). To assist the user in particular subject domain by browsing through the interventions which have been pre-clustered into a hierarchical structure. Simultaneously, the user can identify the inter-ventions encompassed in each cluster and discover the relationships between the the P and O elements identified from the titles or abstracts. It is expected that through this process, a user can understand his/her information needs and obtain a more com-prehensive knowledge about the domain of interest. An ambiguous query can also be refined by selecting the PICO query that best described the information needs. 2.1 Collection of MEDLINE Documents medical concepts:  X  X reast cancer X  and  X  X reast neoplasms X . The concepts were used as the main search terms and the following filters were activated to retrieve relevant doc-uments from the MEDLINE database: randomized controlled trial, abstract available, publication date from 2002/10/01 to 2012/10/04, humans and English. The documents documents were collected for the extraction of PICO elements. 2.2 Generation of PICO Sentences determining the PICO elements that the sentence carries [5,6]. Two types of abstracts METHODS and RESULTS, and unstructured abstracts written in paragraph format segments respectively based on the headings and the position of the sentences in the abstracts (Table 1). The extracted sentences were called in the remainder of this paper the  X  X ICO sentences X . 2.3 Generation of PICO Elements NL questions, titles and PICO sentences were processed by the MetaMap Transfer (MMTx) program [7] to semantically iden tify medical concepts as PICO elements. list of best matching concept candidates together with their associated semantic types from the Unified Medical Language System (UMLS) Metathesaurus. Each of the candidates was labeled with a concept unique identifier (CUI) number. 
The concept candidates were post-processed using Rapidminer 5.2 [8] to identify recognized as PICO elements whereas those with other semantic types were deleted. Duplicate terms, synonyms and stopwords were removed by identifying their CUI numbers. For instance,  X  X lood sugar X  and  X  X lood glucose X  are synonyms with the same CUI number (i.e. C0005802). Examples of stopwords are  X  X ind X ,  X  X elease X , were identified at the same location in a document, candidates with the highest num-ber of words were selected. For example, the processing of the phrase  X  X anagement titles and the abstracts as PICO elements. 2.4 Preprocessing of PICO Elements ( X  X ile + Abstract X ), (2) the interventions were tokenized using  X  X oose X  (LO) or  X  X trict X  (ST) method, and (3) the interventions were weighted using normalized term frequency (TF), binary term occurrence (BI), term occurrence (TO) or term frequen-cy-inverse document frequency (TF-IDF). The tokenization methods and weighting schemes are detailed as follow:  X 
LO: The interventions were tokenized into single term. For instance, the phrase  X  X scorbic acid X  is tokenized into  X  X scorbic X  and  X  X cid X ; ST: The interventions were not tokenized. For example, the phrase  X  X reast radiotherapy X  remains unchanged.  X 
TF: The ratio of the frequency of a term to the maximum term frequency of any term in a document, producing a numerical value between 0 and 1; BI: The occur-rence of a term in a document with a binary value of 0 or 1; TO: A nominal value obtained by calculating the number of times a term occurs in a document; TF-IDF: 
A numerical value calculated by multiplying the frequency of a term in a document to the inverse of the number of documents in a collection that contains the term. 
The three steps described above were achieved using Rapidminer 5.2 [8]. 24 sets of baseline data were generated based on the derivation of, and the tokenization methods and weighting schemes applied to the interventions. 2.5 Inter-document Similarity Tests The baseline data were assembled into pairs of interventions. The similarity between each pair of interventions was computed using the  X  X ist X  and  X  X imil X  functions avail-able in the R package  X  X roxy X  [9]. A total of 42 similarity/distance measures The similarity values were normalized to a scale of 0 to 1. The normalized similarity value of each pair of interventions S i was calculated using (2).  X   X  X  X  X  is the minimum ventions. 
A retrospective analysi s pairs of interventions. Inter interventions whereas thos e tions. Histograms and box p larity/distance measures i n tions. A one-way ANOVA between paired and unpair e to compare the performan c (3).  X   X  X  X  X  X  X  X  X   X   X   X   X   X   X   X   X   X  is the mean of the mean of similarity valu e 2.6 Cluster Structure A Similar interventions were tering methods: average-l i three types of clusterings w R package  X  X tats X  [9]. 
A sample of clustering w tions (e.g.  X  X amoxifen Be v levels and the number of d relevant documents for a t need to explore two levels tion Tamoxifen X . 
The precision (P), recall the ratio of relevant docu m ber of relevant and irrelev a of relevant documents retri e vant documents retrieved mean of P and R (6). 
Random-baseline clusterings were constructed to evaluate the information retrieval performance of similarity-based clusterings. A random-baseline clustering was number of clusters and the same number of documents in each cluster of a similarity-based clustering. An example is given in Fig. 2. The P, R and mean average precision (MAP) over 25 topics were computed using the TREC_EVAL program [10]. 3.1 The Inter-document Similarity Tests between pairs of interventions. A value close to 1 indicates strong similarity whereas a value close to 0 means low similarity. The MD was calculated to indicate the differ-ence between the mean of paired and the mean of unpaired similarities. The larger the paired and unpaired similarities. Table 4 summarizes the measures that produced the highest MD over the 24 sets of baseline data. The table revealed that: (1) BI is better than other weighting schemes, (2) the tokenization method, LO, is superior to ST, (3) interventions derived from title are better than those derived from abstract or  X  X itle + abstract X , and (4) Yule gives the highest MD compared to other measures. One of the most popular distance measures between two document vectors is the (average MD = 0.86  X  0.04) is evidently higher than the MD of Cosine (average MD = 0.50  X  0.02 and 0.40  X  0.02 respectively when tokenized by TF and TF-IDF). For both measures, a significant difference in similarity values between paired and un-paired interventions was found (p &lt; 0.005). 
Histograms and boxplots (Fig. 3b) were plotted to investigate the frequency distri-bution of similarity values of 450 paired and 450 unpaired interventions. As shown in between the two distributions. The paired histogram for BI-Yule combination skewed significantly to the right, showing that most of the paired interventions has a similari-combinations are relatively flat with similarity values range between 0 and 1 (as indi-three combinations looks apparently the same. In terms of classifiability, Yule gave a plots than Cosine. 
In summary, the similarity measure, Yule, performed better than other measures at identifying paired interventions or at differentiating between paired and unpaired interventions derived from titles and which had been weighted and tokenized respec-tively using BI and the LO method. 3.2 The Clustering Tests documents were calculated using the BI-LO-Title-Yule combination. Hierarchical clusterings were computed using AL, CL and WL algorithms. As shown in Fig. 4a (1 st column), the structure of AL and CL clusterings are wide with many branches at the top of the hierarchies, whereas for WL clustering, branches are located mainly at the bottom of the hierarchy. The CL algorithm produced a structure with lower number of comparison of the structures obtained by calculating the similarities using the TF-LO-Title-Cosine combination (Fig. 4a, 2 nd column) revealed a higher number of levels in the clusterings (the highest number of levels = 7 for CL, compared to 13 for WL and user needs to explore to reach a topic of interest. 
The clusters that best represent 33 topics that covered in the 114 documents were identified by computing the P, R and F of each cluster in a hierarchy. Table 5 shows the level of the clustering hierarchy (Lev), the number of documents (Doc), the num-ber of relevant documents (Rel) and the P, R and F values of a cluster in a Yule-based CL clustering. A good cluster is supposed to contain as many relevant documents as possible with high P and high R. The F-measure quantifies the balance between P and R = 1.0) or two clusters (e.g. Level 1 of Topic 2, R = 0.5 and 0.5 respectively), (2) the best clusters appear at the top of the structure with high P, R and F for Topics 1, 2 and Topic 3, (4) Topic 4 can be identified with out exploring the structure (Level = 0), and documents (e.g. Level 1 of Topic 3, P = 0.6 and 0.3 respectively). The results indicate that the best clusters located at different levels of the structure. 
Table 6 shows the average number of levels that a user needs to explore to discover the best clusters for the 33 topics. The Yule-based CL clustering provides the respectively). The findings suggest that the best clusters appear on average at the top two levels of a CL clustering. This was evaluated by identifying the clusters with the average over the 33 topics, the clusters from the top two levels contain approximately 81% and 79% of the relevant documents respectively for Yule-based and Cosine-based CL clusterings. This suggests that only a small number of clusters that will need to be further explored to obtain all the relevant documents from the clusterings. 
The effectiveness of similarity-based clus terings in grouping similar interventions to the same or small number of clusters were evaluated by comparing with random-clusterings outperform random-baseline clus terings in terms of mean average preci-sion (MAP = 0.43 vs. 0.24 and 0.48 vs. 0.25 respectively for Yule-based and Cosine-based CL clusterings). This is further indicated in the P-R curves shown in Fig. 4b. clustering provide the most appropriate hier archical clustering for the exploration and visualization of interventions. What types of clinical information do doctors need? Where do they search for informa-[13,14] further supported that one of the doctors X  greatest information needs is for infor-mation about treatments and drugs. The primary electronic resource used by doctors for evidence-based clinical decision making is MEDLINE [15,16]. Junior doctors accessed MEDLINE (44%), UpToDate (42%), internet search engines (5%), MDCONSULT (3%) and the Cochrane Library (2%) for clinical information [17]. The findings support the use of MEDLINE in this study as the preferred information source for PICO elements. The Use of PICO Framewor k for Query Refinement. One of the obstacles that pre-vents physicians from answering patient-care question is the tendency to formulate unanswerable question [1]. To formulate an answerable question, physicians are rec-ommended to change their search strategies by rephrasing their questions [17]. Other studies recommended the use of question frameworks such as PICO, PICOT, PICOS and PESICO for the formulation of clinical question [18,19,20,21]. A study evaluat-found that questions that contain a proposed intervention and a relevant outcome were unlikely to go unanswered. It is recommended by [22] that at least 3 of the PICO ele-ments are needed to formulate an answerable question. In summary, the completeness of PICO elements determines whether a clinical question is likely to be answered. Visualization of Interventions Using Similarity-Based Clustering. Current medical QA questions. A recent review by [23] concluded that current MedQA systems have limi-tations in terms of the types and formats of questions that they can process. The Info-format but not in NL. An example of PICO query is  X  Atrial Fibrillation AND Warfa-the ability of users to apply Boolean operators (e.g. AND and OR) and by the lack of vocabulary due to limited knowledge of a particular domain. The AskHermes system [26,27], on the other hand, accepts both well-structured and ill-formed NL questions. For example,  X  What is the best treatment for a needle stick injury after a human im-munodeficiency virus exposure?  X  A poorly formulated question cannot be refined. This can in turn lead to the discovery of irrelevant documents. Current MedQA exploring a problem domain and clarifying their information needs. 
The present study adopted the concept of system-mediated information access, intro-duced by [28], to assist users in refining an ill-defined question. It is expected that users at contributing to the identification of similar interventions. The Cosine similarity, which measures the cosine of the angle between two vectors, has been applied to both document larity, [31] reported an improvement in clustering performance using the Yule measure. likely to be grouped into the same cluster. The top two levels of a CL clustering provide information that can be accessed quickly with minimal effort [11]. Therefore, it is argued that CL provides a quicker and more appropriate clustering than AL for the visualization and exploration of interventions. should be undertaken with higher number of questions addressed with different combina-collected from the abstracts. The case study however shows that the title-based approach effectiveness of the methodologies used for PICO extraction and the effects of different numbers of interventions between two documents on the measurement of similarity. Despite of the limitations, the experimental results show that the similarity-based cluster-ing approach has the potential to aid the visualization and exploration of interventions for the applications of clinical information needs clarification and query refinement. 
