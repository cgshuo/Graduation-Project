 1. Introduction
An ad-hoc Wireless Sensor Network (WSN) consists of a number of sensors spread across a geographical area. Each sensor has wireless communication capability and some level of intelli-gence for signal processing and networking of the data. The development of WSNs was originally motivated by military applications such as battlefield surveillance. However, they are currently being employed in many industrial and civilian applica-tion areas including industrial process monitoring and control, machine health monitoring, environment and habitat monitoring, healthcare applications, home automation, and traffic control ( Callaway, 2003 ; Zhao and Guibas, 2004 ; Bulusu and Jha, 2005 ). A few excellent surveys on the present state-of-the-art research on sensor networks can be traced in Al-Karaki and Kamal (2004) , Bojkovic and Bakmaz (2008) and Yick et al. (2008) .

An important problem of any sensor node design is the deployment of the sensor nodes in the area to be monitored. The number of sensor nodes that can be deployed in an area is a limitation to the designers. Again, there are always some limita-tions to the payload of the sensor nodes that are generally carried and deployed by an aircraft apart from the cost limitation ( Liu and
Mohaparta, 2007 ). Sensor nodes usually have limited energy storage and low processing and communication capabilities ( Park et al., 2001 ). So the energy consumed in them must be small enough so that all the deployed nodes can function till a certain time interval. The impossibility of recharging or replacing the node batteries, especially in networks installed in regions of difficult access, imposes a serious constraint for the designers: each node in the network has a limited lifetime, which cannot be extended. The main motive behind the sensor node deployment is the monitoring of the area concerned. Monitoring of the area may be based on uniform event detection or differentiated event detection, where the probability of the appearance of the event in the area concerned varies both geographically and with time.
And last but not the least maintaining connectivity among the nodes so that the data collected by any individual sensor node can flow through other nodes to the sink node. Deployment of the sensor nodes satisfying all such objectives together is a challen-ging problem.

In the past few years researchers attempted to tackle several of the above mentioned objectives through various optimization processes both by mathematical/numerical programming and through evolutionary computing techniques. The objectives were sometimes modeled as a constrained single objective functions, in special cases aggregating several objectives through well chosen weights. But multi-objective approaches has always been a favorable one for tackling all the aspects of sensor node deploy-ment as will be discussed in the later section of this paper. In literature several works are presented to suggest deployment strategies to tackle those problems. Younis and Akkaya (2007) presented a good overview of various strategies for coverage problems. Meguerdichian et al. (2001) have pointed out that the coverage objective is a measure of the Quality of Service (QoS) that is provided by a particular network design. Several research-ers ( Chakrabarty et al., 2002 ; Wu et al., 2007 ) have proven the NP-hardness of various deployment problems. The main focus is often to determine an optimal sensor placement to cover a grid area (sometimes under uncertainty ( Wu et al., 2007 )) and mini-mize the cost or prolong the network lifetime ( Cardei and Wu, 2005 ). Another major step in WSN design is to assign energy efficient transmit power levels to sensors to maximize the net-work lifetime under certain energy constraints ( Xue and Ganz, 2006 ). Some researchers have also worked on probabilistic or distributed event detection ( Aitsaadi et al., 2009a , 2009b ).
Another way of dealing with coverage, connectivity and lifetime maximization is to deploy a densely distributed sensor network randomly in area concerned and to use active and inactive nodes.
After certain period of time active and inactive nodes are rear-ranged to prolong the network lifetime maintaining connectivity and coverage as described in Martins et al. (2007 , 2011) .
In this work we have considered the deployment of sensor nodes in a given area taking into account all the above mentioned objectives i.e.: (i) minimizing the number of sensor nodes to reduce cost and payload of deployment; (ii) minimizing the net energy consumed by all the nodes in the deployed sensor node arrangement; (iii) maximizing the area covered by the nodes so that any event occurring in the region of interest is easily detected and the data can be sent to the sink node; (iv) maximizing the lifetime of the network. All the nodes deployed have certain limited initial energy. At each time cycle as the nodes transmit the received data to the sink node a certain amount of energy is consumed and the energy remaining in the sensor nodes decreases until it breaks down. N etwork lifetime can be thought of asthetimeinwhichjustonenodebreaksdownthushamperingthe whole network setup. But still th e network can work at a reduced level until all the nodes break down. (v) Maintain connectivity in the deployed node configuration so every node can communicate with the sink node. The communication in the network is often structured as a tree graph, in which there is only a single path between each sensor node and the sink node. The choice of the tree structure is justified by the energy consumption imposed by this topology, which is lower than the consumption in a redun-dant network. Considering all the objectives in a single problem is surely challenging and demands efficient algorithms. As it is evident, multi-objective approach is always best in those cases dealing with multiple and conflicting objectives. Maximizing coverage means the nodes must be placed far apart from the sink node, which is considered at the center of the area concerned.
Again minimizing energy consumption or maximizing network lifetime demands sensor nodes to be placed near the sink node.
Thus it is evident that both the objectives are conflicting in nature and thus multi-objective technique is the best way to deal with those problems. Again with increase in node number net energy consumption in the network increases though the energy con-sumed by each node decreases thus increasing the network lifetime. Connectivity problem is modeled as a constraint by using tree structure of communication.

Motivated by the inherent multi-objective nature of the WSN deployment problems and the overwhelming growth in the field of multi-objective evolutionary algorithms (MOEAs), we started to look for the most recently developed MOEAs that could solve the WSN deployment problem more efficiently as compared to the conventional single-objective approaches. Since differential evolution (DE) ( Price et al., 2005 ; Das and Suganthan, 2011 ) has emerged as one of the most powerful stochastic real-parameter optimizers of current interest and unlike PSOs and GAs, has not been used extensively in WSN context, we were also looking for the state-of-the-art MO variants of DE, when our search con-verged to a decomposition-based MOEA, called MOEA/D-DE ( Li and Zhang, 2009 ; Zhang et al., 2009 ), that ranked first among 13 state-of-the-art MOEAs in the unconstrained MOEA competition held under the IEEE Congress on Evolutionary Computation (CEC) 2009 ( Zhang et al., 2008 ). MOEA/D-DE uses DE as its main search strategy and decomposes an MO problem into a number of scalar optimization subproblems to optimize them simultaneously. Each sub-problem is optimized by only using information from its several neighboring subproblems and this feature considerably reduces the computational complexity of the algorithm.
In this work, we have used a fuzzy dominance concept to synthesis a algorithm based on the decomposition strategy of
MOEA/D and call the same MOEA/decomposition with fuzzy dominance (DFD) ( Nasir et al., 2011 ). Net energy consumed by the sensor nodes and the non-coverage of the demand points are modeled as the two objectives with connectivity as the constraint.
Our goal is to minimize both the objectives with different number of nodes to study the variation of lifetime and energy with the number of nodes. From the obtained Pareto front we employ a decision-making technique to select a particular node configura-tion with minimum energy above a defined non-coverage level.
With that node configuration we can find the lifetime of each sensor node and thus that of whole WSN setup. For multi-objective approach we have compared the result of our algorithm with that of MOEA/D and NSGA-II. For single objective approaches we have simulated with PSO ( Kennedy and Eberhart, 1995 ),
CLPSO ( Liang et al., 2006 ), classical DE (DE/rand/1/bin) ( Price et al., 2005 ) and JADE ( Zhang and Sanderson, 2009 ) with the non-coverage objective modeled as a constraint. We have seen that the MO approach is far better than single-objective ones in minimizing the energy consumed and maximizing network life-time. Also MOEA/DFD surpasses NSGA-II and MOEA/D in almost all the cases when compared on the basis of the spacing and coverage metrics.

The rest of the paper is organized as follows. Section 2 formulates the sensor node deployment problem in a given area and also describes the necessary objective functions and con-straints. Section 3 provides an overview of the MO problem, outlines MOEA/D and then the proposed MOEA/DFD algorithm. In
Section 4 , we describe the experimental set-up and parameters and present our results and figures with explanation. Section 4 finally concludes the paper and unfolds a few important future avenues of research. 2. Problem formulation
In order to model the coverage of WSN, it is usual to discretize the monitoring area into a set of points, called demand points which require sensing. Usually the whole area is broken up into number of small square tiles. The center of each such square tile is considered as a demand point. Each node have a definite radius of sensing, R_sense and any demand points within that circle of radius R_sense is said to be covered by that sensor node. The coverage problem in WSN requires that each demand point must be covered by at least one sensor node. On the other hand connectivity means at least one path must be there between each sensor node and the sink node. Two sensor nodes are said to be connected if the distance between them are less than R_comm (radius or range of communication the sensor nodes). The energy consumed in the sensor node can be primarily composed into three parts: (i) maintenance energy: required for maintaining the nodes in active state (ii) reception energy: depends on the number of nodes from which it receives data for transmitting to sink node (iii) transmission energy: depends on the path in which the energy flows from the sensor node to the sink node. Each node has a specific initial energy. Energy consumed by each node gives the measure of the working time of the node and thus the network lifetime. The mathematical modeling of the WSN is shown below. 2.1. Mathematical modeling
The objective of our optimization process is to deploy the nodes, i.e. to find the co-ordinates of the sensor nodes in the two-dimensional plane which satisfy our required goals. Here the decision variable is an array containing positions of the nodes in the plane as shown below:
Before the mathematical modeling is done some parameters are defined:
X N number of sensor nodes; S set of sensor nodes; D set of demand points;
A d set of arcs connecting sensor nodes to demand points;
A s set of arcs which connect the sensor nodes between
A m set of arcs which connect the sensor nodes to the sink
I ( A ) set of arcs ( i , j ) A A s [ A d incoming on the sensor node j
O ( A ) set of arcs ( i , j ) A A s [ A m outgoing the sensor node i ME node maintenance energy; TE node transmission energy; RE node reception energy;
NC non-coverage penalty, cost of no coverage of a demand x ij variable that has value 1 if node i covers demand point j , z lij variable that has value 1 if arc ( i , j ) is in the path between h j variable to indicate if demand point j is not covered,
Now as we have argued earlier consumed by each node is calculated as the summation of energy spent in maintenance, transmission and reception operations in each node. The follow-ing equation gives the measure of energy consumed in each node: e  X  ME i  X 
The net energy consumed is considered as the first objective function given as f  X 
If the initial energy capacity of a node is E , and the energy consumed by each node is e i , then the time up to which the node can work is given as t i  X  E / e i . All such array of t i the information about network lifetime. Considering the network stops working when the first node breaks down, the lifetime of the network is given as T  X  min f t i g , 8 i A S :  X  3  X  But we consider even when the first node have broke down the WSN can continue its operation at a reduced level with reduced coverage till all the node breaks down. The time instant when the whole network stops working is given as T  X  max f t i g , 8 i A S :  X  4  X 
The second objective is to maximize the area of coverage. But since we need to minimize energy, coverage must also be modeled as a minimization problem. So coverage is expressed as a non-coverage penalty function which assumes the value 0 in case of full coverage. The second objective function is thus given as f  X  where NC j is a non-coverage penalty parameter that is removed in Eq. (15) by normalizing the variable.

Connectivity of the sensor network is important objective for deployment of sensor nodes. The connectivity problem is gen-erally modeled as a constraint as follows:
X
Constraints described in Eqs. (6) and (7) ensure that there is at least one path between each sensor node and the sink. Thus the whole multi-objective problem can be formulated as min Y  X  Fx !  X  X  f 1 , f 2  X  ,  X  8  X  with Eqs. (6) and (7) as the constraints. 2.2. Multi-objective formulation
Since we are not using any mathematical optimizers, this linear formulation of the problem is not necessary. Hence we formulate the problem in a different and much easier way by taking help from the basic graph theory. The energy consumed by each node e i can now be formulated as e  X  ME i  X  TE i P im  X  RE i a i ,  X  9  X  where ME , TE , RE holds the same meaning as described above.
P im means the cost of the minimum path from a sensor node i to the sink node m. a i refers to the number of sensor nodes from which the node i receives data and transfer it to the sink node in multi-hop communication. Thus the net energy consumed E and the objective function f 1 are defined similarly as in Eq. (2).
Similarly the two lifetime T 1 and T 2 for the WSN breakdown are defined as in Eqs. (3) and (4) respectively. Now the non-coverage penalty function is modified and the non-coverage penalty parameter NC is thus removed by normalizing the function f 2 . f  X   X 
P
The constraints given in Eqs. (6) and (7) to maintain connec-tivity can be modified with the help of graph theory considering communication in tree structure. Also the minimum path P im from node i to the sink node m and the value of a i can be expressed by use of graph theory. Thus the net multi-objective approach can be summed up as min Y  X  Fx !  X  X  f 1 , f 2  X  with NCP i  X  0  X  11  X 
NCP is defined as a Non-Connectivity Penalty parameter which is returned by the graphical algorithms used when the deployed
WSN solutions are not connected with the sink. It counts the number of active nodes which are not connected to the sink node.
The nodes closer to the sink node can have higher penalty for non-connectivity by considering that all the information should be directed to the sink node. But in our scheme we select only those solutions with full connectivity. So there is no need to bring in any additional parameter as connectivity is almost like a dead penalty. So in this context let us discuss few aspects of tree structure and related algorithms in a nut shell. 2.3. Some basic definitions of graph theory
The solutions generated and thus the sensor nodes deployed in the area should be such that they are well connected to the sink node for data transmission. Our use of graph theory is to satisfy or fulfill three objectives: (i) Ensure connectivity in the WSN by assuming tree structure.
In this context we may use Prim X  X  Minimum spanning Tree (MST) algorithm ( Cherition and Tarjan, 1976 ) or Dijkstra X  X  algorithm ( Dijkstra, 1959 ) to check whether the sensor nodes are connected to the sink or not. (ii) To find a minimum path for transmission of data from the sink node i to the sink node m . Dijkstra X  X  algorithm gives the minimum path for each node to the sink node. (iii) To find the number of nodes a i from which the node i receives data while considering the minimum path for each node as established before.

A tree is type of graph theory where there is a topmost node called root and other nodes are spanned about the root. Any node at depth d 1 is a parent to a node at depth d if both are connected. In our WSN problem, the sink node is the root. Other sensor nodes are connected to the sink as shown in Fig. 1 .
Here we have used Dijkstra X  X  algorithm to satisfy the above mentioned requirement. From the sensor node arrangement a graph G is created containing all the sensor nodes. An adjacency matrix is created which contains the information which nodes in the graph are connected. An edge ( u , v ) can belong to G only if the distance between the nodes u and v A G is shorter than the maximum communicating range R_Comm. So Dijkstra X  X  shortest path algorithm is applied from each of the sensor node to the sink node. The output returned by the algorithm is the cost of the path, which is utilized in calculating P im and a the sequence of nodes in the path, which can help us to determine a i and NCP i . Thus all our requirement is fulfilled. Hence using graph theory helps us to reduce the huge computational effort in formulating the objective functions. 2.4. Single objective approach
To establish that the deployment problem works better when modeled as a multi-objective problem than a single objective one, we need to simulate with some single objective evolutionary algorithms. For this the above described model needs to be reformulated in a single objective approach. The multi-objective approach mainly gives the Pareto front between energy consump-tion and the non-coverage penalty function. To select the best suited result from the obtained Pareto set we need to have a decision maker. Here as the problem demands a certain level of coverage the decision maker is made to choose from the multi-objective Pareto front the solution with least energy having non-coverage value less than a certain threshold called Cov_Threshold.
Using this concept we can model the second objective f 2 as a constraint defined as  X 
X h  X  = 9 h j 9 r Cov _ Threshold :  X  12  X 
Thus the net single objective approach is defined as min f 1 , subjected to  X  where the objective function f 1 is defined as in Eq. (2) 3. Evolutionary Multi-objective Optimization and MOEA/DFD algorithm
Due to the multiple criteria nature of most real-world pro-blems, multi-objective optimization (MO) problems are ubiqui-tous, particularly throughout engineering applications. As the name indicates, multi-objective optimization problems involve multiple objectives, which should be optimized simultaneously and that often are in conflict with each other. This results in a group of alternative solutions which must be considered equiva-lent in the absence of information concerning the relevance of the others. The concepts of dominance and Pareto-optimality may be presented more formally in the following way ( Deb, 2001 ). 3.1. General MO problems
Definition 1. Consider without loss of generality the following multi-objective optimization problem with D decision variables x (parameters) and n objectives y :
Minimize: Y !  X  f  X  X !  X  X  X  f 1  X  x 1 , :::: , x D  X  , :::: , f to various equality and inequality constraints: and h j  X  x !  X  X  0 , j  X  p  X  1 , ::: , q where X !  X  X  x 1 , ::::: , x D T A P and Y !  X  X  y 1 , :::: , y decision (parameter) vector, P is the parameter space, Y ! objective vector, and O is the objective space. A decision vector
A !
A P is said to dominate another decision vector B written as A ! ! B ! for minimization) if and only if: 8 i
A f 1 , :::: , n g : f i  X  A Based on this convention, we can define non-dominated, Pareto-optimal solutions as follows:
Definition 2. Let A ! A P be an arbitrary decision vector. (a) The decision vector A ! is said to be non-dominated regarding (b) The decision (parameter) vector A ! is called Pareto-optimal if 3.2. The MOEA/D-DE algorithm
Multi-objective evolutionary algorithm based on decomposi-tion was first introduced by Zhang and Li (2007) and extended with DE-based reproduction operators in Li and Zhang (2009) and
Zhang et al. (2009) . Instead of using non-domination sorting for different objectives, the MOEA/D algorithm decomposes a multi-objective optimization problem into a number of single objective optimization subproblems by using weights vectors l and opti-mizes them simultaneously. Each sub-problem is optimized by sharing information between its neighboring subproblems with similar weight values. MOEA/D uses Tchebycheff decomposition approach ( Miettinen, 1999 ) to convert the problem of approx-imating the PF into a number of scalar optimization problems. Let l ! 1 , ::: , l ! N be a set of evenly spread weight vectors and Y problem, y n i  X  min f f i  X  X !  X  9 X ! A O g for each i  X  1, 2 problem of approximation of the PF can be decomposed into N scalar optimization subproblems by Tchebycheff approach and the objective function of the j th subproblem is g  X  X ! 9 l objective functions simultaneously in a single run. Neighborhood relations among these single objective subproblems are defined based on the distances among their weight vectors. Each subpro-blem is then optimized by using information mainly from its neighboring subproblems. In MOEA/D, the concept of neighbor-hood, based on similarity between weight vectors with respect to Euclidean distances, is used to update the solution. The neighbor-hood of the i th subproblem consists of all the subproblems with the weight vectors from the neighborhood of l ! i . At each genera-tion, the MOEA/D maintains following variables: 1. A population ( X ! 1 , ::: , X ! N ) with size N , where X solution to the i th subproblem. 2. The fitness values of each population corresponding to a specific subproblem. 3. The reference point Y ! n  X  X  y n 1 , y n 2 , ::: , y n value found so far for objective i. 4. An external population (EP), which is used to store non-dominated solutions found during the search.

The MOEA/D-DE algorithm is schematically presented in Table 1 . 3.3. Concept of fuzzy dominance and MOEA/DFD algorithm
The basic definition of dominance and Pareto optimality played very important roles in the development of effective MOEAs. However, they do not provide a complete framework for easily implementing new methods. The reason is two-fold: Firstly, the basic definition of dominance does not make a difference between two solutions when neither is dominating, and secondly, it measures the extent by which one solution dominates the other. In order to circumnavigate these problems, some numerical measures for dominance have been proposed recently ( Farina and Amato, 2004 ; Koduru et al., 2004 ). In this article we shall use the concept of fuzzy dominance. This measure can more directly correspond to the crisp definition of dominance and favors quick convergence as shown in Adra et al. (2009) .
Suppose a minimization problem as discussed above involves m objective functions f i , i  X  1, y , m . The solution space will be denoted by O C R n .
 Definition 3. ( fuzzy i-dominance by a solution ): The mapping m dom i  X  f i ( O ) tonically non-decreasing membership function. A solution u is said to i -dominate solution v ! A O ,if f i ( u ! ) o f equal to: m dom i ( f i ( u ! ) f i ( v ! )) m dom i ( u ! be regarded as a fuzzy relationship u ! ! F i v ! between u
Definition 4. ( Fuzzy dominance by a solution ): Solution u said to fuzzy dominate v ! A O if and only if 8 i A {1, y
This relationship is represented by u ! ! F i v ! .If u ! intersection of the fuzzy relationships u ! ! F i v ! for each i . The fuzzy intersection operation, denoted with  X  \  X  is performed using a family of functions called t-norms ( Mendel, 2003 ) and is computed as: m dom ( u ! ! F i v ! )  X \ m dom ( u ! ! F i
We here at first outline the modified MOEA/D algorithm with fuzzy dominance concept and dynamic resource allocation. Here in MOEA/DFD (Decomposition with Fuzzy Dominance), we use Tchebycheff approach for decomposition during non-domination. During the search process this approach maintains:
A population of N points x ! 1 , x ! 2 , y , x ! N A O , where x current solution to the i th subproblem.

FV 1 , y , FV N , where FV i is the F -value of x ! i , i.e. FV each i  X  1, y , N .

CV 1 , CV 2 , y , CV L where CV i is the overall constraint violation of x ! i , i.e. CV i  X  C  X  x ! i  X  for each i  X  {1, ..., L } and C  X  x overall constraint violation function. z !  X  X  z for objective f i . p 1 , y , p N : where p i utility of subproblem i. gen : the current generation number. max_gen  X  maximum number of generations.

A  X  fuzzy function decay constant.  X  domination threshold.  X  fuzzy i -dominance level. ^ m  X  fuzzy dominance level.
 A complete pseudo-code of the algorithm is provided below: Begin Step 1: Initialization any two weight vectors and then find the T closest weight vectors to each weight vector. For each i  X  1, y , N ,set uniformly randomly sampling from the search space.
 { f ( x ! 1 ), f Step 2: Selection of subproblems for search: individual objectives f i are selected to form initial I. By using 10-tournament selection based on p i , select other [ N /5]-m indexes and add them to I .
 Step 3: For each i A I , do: randomly generate a number rand from (0,1). Then set: indexes r 2 and r 3 from P, and then generate a solution y mutation operator on y ! with probability p m to produce a new solution y ! .
 of O , it X  X  value is reset to be a randomly selected value inside the boundary.
 then set z j  X  f j ( y ! ).
 For each j A P ,
For each i A {1, y , m } Step 3.6: Update of solutions: then set x ! j  X  y ! , FV j  X  F !  X  y !  X  ; end end
Step 4 Stopping criteria If the stopping criteria is satisfied then stop and output { x ! 1 , y , x ! N } and { F ( x ! 1 ), y Step 5 gen  X  gen  X  1.
 relative decrease of the objective for each subproblem during the last 50 generations, update Endif Go to Step 2.
 End
In 10-tournament selection in Step 2 , the indices with the highest p i value from 10 uniformly randomly selected indexes are chosen to enter I. We should do this selection [ N /5]-m times. In step 5 , the relative decrease is defined as (old function value-new function value)/old function value If D i is smaller than 0.001, the value of p i will be reduced.
In DE operator used in Step 3.2 each element y k for k A {1, in y !  X  X  y 1 , ::: , y n  X  T is generated as follows: y  X  x k  X  F  X  x  X  x r 1 k with probability 1 CR , where CR is the crossover rate and F is the scale factor.
The mutation operator in Step 3.2 generates y !  X  X  y 1 , ::: , y from y as follows: y  X  y k  X  s  X  b k a k  X  with probability p m ,  X  y k with probability 1 p m , with  X  X  2 rand  X  1 =  X  Z  X  1  X  1 , if rand r 0 : 5  X  1  X  2 2 rand  X  1 =  X  Z  X  1  X  , otherwise , where rand is uniformly random number from {0,1}. The dis-tribution index Z and the mutation rate p m are two control parameters. a k and b k are the lower and upper bounds of the k th decision variable, respectively.

Though decomposition is a very effective process for maintaining convergence and diversity at the same time for an MO problem, still there are few drawbacks in this method. These are mainly reflected in the convergence towards the Pareto front. Each of the weight a new offspring whose position in the objective function hyper space is farther away from the parent individual and hence in this process diversity is maintained. But in course of this process, the weight vector having affinity towards the offspring at a further position in the function space with respect to the parent individual, will neglect those offspring which may be poten tially better but lying closer in comparison to that parent individual. 3.4. Constraint handling technique
As the deployment of sensor nodes problem is formulated as a constrained multi-objective one, we employ a constraint handling technique where feasibility is given priority over the objectives, as the other solution j if any of the following conditions are satisfied: (1) Solution i is feasible and solution j is not. (2) Both solutions are infeasible, but solution i has a smaller (3) Both solutions are feasible but solution i has better objective 4. Simulation experiments and results 4.1. Experimental setup
To validate our mathematical formulation of the deployment problem and to find the most optimized position we need to simulate them with MOEA/DFD and compare its results with different kinds of algorithms. As i t has been already discussed that deployment problem can be studied both as a single-objective one and also as a multi-objective one, we need to simulate both kinds of algorithms. For multi-objective algorithms we compare our results with two contemporary best state-of-the-arts MOEA/D ( Zhang and Li, 2007 )andNSGA-II( Miettinen, 1999 ). For single objective optimiza-tion differential evolution and PSO is two strong algorithms. We simulate DE/rand/1 ( Price et al., 2005 ) and a strong variant of DE called JADE ( Zhang and Sanderson, 2009 ) and the basic gbest PSO ( Kennedy and Eberhart, 1995 ) and CLPSO ( Liang et al., 2006 ).
We consider a square area of 60 m length in each side where the sensors are to be deployed for monitoring purpose. The area is divided into number of squares. The center of each such square is taken as the demand point. So more the number of demand points tougher the problem of monitoring it becomes. We haven con-sider 12 12  X  144 demand points in the whole area. Other parameters of the sensor networks are described as follows: R_sense (radius of sensing)  X  15 m; R_comm (range of communication)  X  15 m; Initial energy  X  1 Ah; ME (node maintenance energy)  X  13 mA; TE (node transmission energy)  X  20 mA/m; RE (node reception energy)  X  2 mA;
Cov_Threshold (the minimum limit of the full coverage that must be maintained while deploying the nodes or the max-imum limit of the non-coverage value that is allowed)  X  0.05 or 95% of full coverage.

NCP (Non-Connectivity Penalty parameter, the value returned when the nodes are not connected to the sink)  X  1 for each node being not connected with the sink node.
 We have simulated each run for different node configuration.
For the above mentioned experimental parameters the cover-age threshold is satisfied for a minimum of 10 nodes. So we have simulated it for 10, 12, 14, 18 nodes to study the variation of energy and the maximum lifetime of the network with the number of nodes.

In general specifications of the charge capability of batteries are made in A-h and they resemble the energy capability of the battery. Similarly A which is the unit of current is the unit which can be used to resemble power consumption or rate of energy consumption. So the units are kept in A or A-h rather than Watt or J to meet the standard battery specification. 4.2. Algorithm parameters
For simulating all the evolutionary algorithms we need to specify a certain set of parameters:
Maximum number of function evaluations is kept the stopping criteria for all the algorithms: 1,00,000 F.E.s.

For single objective algorithms number of population ( N )is kept 100.

For multi-objective algorithms population ( N ) is kept 500 to have proper Pareto front representation.

Now the parameters for our algorithm MOEA/DFD are described here
T  X  0.1 N (size of the neighborhood) q  X  0.9
In DE and mutation operator CR  X  0.9, F  X  0.5, Z  X  20 and p
A  X  50 (fuzzy function parameter) t is varied linearly from 1 to 0.9 with increase in FEs.
Parameters of other algorithms are kept as recommended in their respective articles. 4.3. Multi-objective representation and performance evaluation
One of the purposes of simulating the results are to show that deployment problem works better when formulated as a multi-objective one. In this section we present the Pareto front represen-tation between two objectives: energy cost function and the non-coverage penalty function. The use of multi-objective representation is useful to the decision maker. We can see from the Pareto diagrams that as argued before with increase in non-coverage that is decrease in coverage, energy also decreases and thus lifetime increases. So the decision maker can choose any specified coverage threshold as required by the application and can get a much lower energy and longer lifetime than that would have been possible in full coverage situation. In single objective optimization procedure such freedom of decision making is not possible.

To justify the performance of the multi-objective algorithms we need to have certain performance metric to evaluate them. In multi-objective optimization the performance of the algorithm is determined both in terms of the convergence of the obtained Pareto front towards the ideal one and the diversity of the obtained front so that it can approximate the ideal Pareto front to the best. A straightforward comparison metric between two sets of non-dominated solutions is the C-metric ( Deb et al., 2002 ). metric, evaluate the ration of the non-dominated solutions in an algorithm A X  s Pareto front dominated by the non-dominated solutions in an algorithm B X  s Pareto front, divided by the total number of non-dominated solutions obtained by algorithm A .
C  X  A , B  X  X  u A B
C( A , B )  X  1 means that all the solutions in B are dominated by some in A while C ( A , B )  X  0 implies that no solution in B is dominated by a solution in A .

The second measure is spacing ( Schott, 1995 )  X  a metric used to measure the distribution of solutions throughout the non-dominant solutions found so far. As the  X  X eginning X  and  X  X nd X  of the current Pareto front found are known, this metric judges the distribution quality of solutions in such front and is defined as
S  X  v u u t ,  X  18  X  m th objective value of the i th (or, k th) solution in the final non-the approximated Pareto front for all the node setups.
Table 2 shows the comparisons of C-metric for different node instances. Table 3 gives the spacing metric measures of the three algorithms on the above test setup.

Thus we can see both from C-metric Table 1 and from Pareto front figures that MOEA/DFD outperforms NSGA-II in all cases and is better of MOEA/D in almost all the cases. The spacing measure shown in Table 3 also suggests that the distribution of the approximated Pareto front is best for MOEA/DFD. 4.4. Comparison with single-objective formulation
For deployment of sensor networks the decision maker need to know the exact location of the sensor nodes to be placed such that the energy consumed is minimum within certain coverage threshold and also the lifetime is maximized. For lifetime max-imization we can treat it as the time when the first node breaks down or we can consider that even after one node has broken down the WSN will continue working at reduced level. So the time when all the nodes break down can also be taken as the lifetime of the network.

From the Pareto front obtained in the multi-objective repre-sentation we need to employ a decision maker to select the best compromise solution which will give us the location of the deployed nodes. In general decision makers are made with the problem specific knowledge to select the best compromise solu-tion. In the sensor node deployment problem with the non-coverage and energy consumption as two objectives, the decision maker is made to select the solution with minimum energy below a certain non-coverage threshold. This is done to provide a solution with fixed coverage level as needed by the user for the specific problem. In our case we have chosen a coverage threshold of 95% i.e. non-coverage penalty function value of 0.05. Then the solution with minimum energy having non-coverage penalty function value below 0.05 is chosen as the best compromise solution. In Table 4 , we present the minimum energy within the above mentioned 95% coverage threshold for both multi-objective and single objective approaches. For single objective approach the modeling has already been described.

We see that MOEA/DFD is best in all instances and produces minimum energy. Multi-objective algorithms perform much better than single objective ones. Particle swarm optimizers are very poor in comparison to others. The explanation for such behavior can be given from the fact that in single objective modeling apart from the connectivity constraint we also have a coverage constraint. And the coverage constraint is a very hard one and allows only very small populations which do not have any constraint violation. Thus the evolution of the population becomes difficult and that causes the failure. On other hand multi-objective problems having coverage as an objective helps in evolution of the populations without any hindrance and decision maker can be applied later to get the required minimum energy. So multi-objective algorithms perform better than single objective ones. Fig. 3 presents the variation of energy with the number of nodes for different algorithms.
One may question at this point that if with 10 nodes we are getting the minimum energy with then what is the point in considering the variation of number of sensor nodes. In
Tables 5 and 6 we can see the variation of lifetime with number of nodes for different algorithms. We see that as the number of nodes increases the lifetime also increases. For 10 nodes where we have minimum net energy consumption we are having minimum lifetime. This fact can be attributed that with lesser number of nodes as one node breaks down the functioning is hampered to a large extent. Having satisfied the coverage threshold with 10 nodes the optimizer will try to place the extra nodes closer to the sink node. Due to addition of extra node net energy consumption may be increased but energy consumed by individual node gets decreased. So in situations where lifetime is important criteria than the number of nodes the decision maker can choose any number of nodes to meet his demand. Here we can also see that MOEA/DFD performs best for all node setup. We next present the graphical figures to show the variation of percentage of active nodes with the time of failure in Fig. 4 . Here as the network is established it will start working and will continue to do so till any one node breaks down. Then after a node breaks down it will continue working at reduced rate until all the nodes are broken. In the figure step diagram is used to represent this concept. The change in steps occurs only when a node breaks down resulting in decrease of number of active nodes. The main objective is to have maximum lifetime. The algorithm which achieves it performs best as is the case with MOEA/DFD. Different markers have been used for different algo-rithms to mark the time when one node breaks down. On advantage of this figure is that, according to the choice of the designers one may consider different cut-offs of number of active nodes to define working lifetime of the sensor network. Based on that from the figure it can easily be deduced which algorithm gives higher work-ing lifetime for that choice of cut-off.
 In Fig. 5 , few sample node architectures are shown for MOEA/ DFD. This figure is a result of simulation and the best solutions for
MOEA/D are plotted in Fig. 5 a for 10 nodes and in Fig. 5 b for 12 nodes. The figures clearly reveal our assumption of tree graph of communication to be accurate enough. Also it shows that with increasing number of nodes more nodes are placed closer to the sink node to maintain lower energy. For clarity the co-ordinates of the positions of the sensors to be deployed as found by simulating with MOEA/DFD is also stated below. For 10 node set-up, the solution vector is: X 9.772 22.721 47.976 31.222 16.084 44.274 26.743 48.194 19.764 35.109 For 12 node set-up, the solution vector is: For 14 node set-up, the solution vector is: For 16 node set-up, the solution vector is:
The co-ordinates have been rounded off up to 3 places after decimal to keep it concise for representation. 5. Conclusion
This paper proposes a new fuzzy dominance concept along with the decomposition type of multi-objective evolutionary algorithm. This fuzzy dominance falls under a category of newly defined relaxation dominance concept. MOEA/DFD happens to tackle the multi-objective sensor node deployment problem better than other contemporary state-of-the-art. There are lots of challenges in developing new algorithm based on alternative definition of dominance like relaxation dominance which plays an essential role in the evolution of the algorithm. The problem of deployment of sensor nodes to optimize the coverage, energy consumption, lifetime and number of nodes maintaining connec-tivity can be modified and applied for different fields of applica-tion. The modeling of the objective function is to be changed as per the requirement of the problem. This problem can also be applied in areas where probabilistic event detection is necessary rather than distributed event detection.

In recent times density control of a largely deployed sensor node through sleep scheduling is an important area of research. Tracking of a single particle or a dynamic probabilistic distribu-tion is also a challenging problem. Sensor network is a large research area having lots of challenging problems to tackle other than deployment. Localization, tracking, Routing and Scheduling, Data fusion, Security etc. are the promising research areas where multi-objective algorithms and evolutionary computations can be applied. There are scopes of developing dynamic multi-objective optimization to solve the deployment problem more efficiently. The evolutionary multi-objective optimization approach can also be extended to those areas for better flexibility and higher accuracy.
 References
