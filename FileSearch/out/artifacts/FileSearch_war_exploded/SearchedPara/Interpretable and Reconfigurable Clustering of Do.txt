 Clusters of text documents output by clustering algorithms are of-ten hard to interpret. We describe motivating real-world scenarios that necessitate reconfigurability and high interpretability of clus-ters and outline the problem of generating clusterings with inter-pretable and reconfigurable cluster models. We develop a cluster-ing algorithm toward the outlined goal of building interpretable and reconfigurable cluster models; it works by generating rules with disjunctions and conditions on the frequencies of words, to decide on the membership of a document to a cluster. Each cluster is com-prised of precisely the set of documents that satisfy the correspond-ing rule. We show that our approach outperforms the unsupervised decision tree approach by huge margins. We show that the purity and f-measure losses to achieve interpretability are as little as 5% and 3% respectively using our approach.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Clustering ; I.2.7 [ Artificial Intelligence ]: Natural Language Processing X  Text Analysis ; I.5.4 [ Pattern Recognition ]: Applications X  Text Processing Algorithms, Experimentation
Text Clustering is the process of grouping text documents into clusters so that the documents within a cluster are more similar than documents across clusters (in the absence of label informa-tion). Similarities between text documents are often assessed us-ing the cosine similarities between TF-IDF 1 vectors. Popular tech-niques for text clustering include partitional clustering algorithms such as K -Means [11] and hierarchical clustering algorithms such as Single-Linkage Clustering among others [17]. It is often nec-essary to interpret the clusters generated, for knowledge discovery http://en.wikipedia.org/wiki/Tf-idf ports along with a few reports about celebrities who attended a specific sports event, none of the above may generate a word in the description that is indicative of the latter. Thus, an analysis of the WBR corresponding to that cluster could lead one to conclude that the cluster has only sports reports. Further, all documents in the cluster are not guaranteed to contain any one of the words in-cluded in the representation. WBRs are mostly read-only models; i.e., editing of such models do not lead to an intuitive reconfig-uration of cluster memberships. A simplistic model of deleting documents containing the word when the word is deleted from the cluster model would not work since documents that do not contain any words in the description are also part of the cluster. WBRs are hence not self-contained nor do they allow easy manual reconfigu-ration.
A standard approach to summarize multi-dimensional points is to represent them by a set of hyper-rectangles [1] e.g., (3 . 80  X  GP A  X  4 . 33  X  0  X  minutes _ in _ gym _ per _ week  X  30) may describe a set of nerds . Sum of Rectangles (SOR) , the canonical for-mat for cluster descriptions in databases has been used for numeri-cal data [6]. Text data is unique in being highly multi-dimensional and extremely sparse. SOR representation works by discovery of bounds and convex structures; the inherent high dimensionality and sparsity of document datasets makes upper bound discovery im-practical. Decision trees have been adapted for clustering [2] to generate rules based on attribute frequencies.
Interpretability in machine learning models has been studied in various contexts [13, 6, 2] and its need cannot be overemphasized. Now, we describe a scenario where reconfiguration of cluster mem-berships is highly desirable, and elaborate on other real-world sce-narios that demand or are benefitted by interpretability and recon-figurability of cluster models.

Service Delivery Organizations (SDO): SDOs mostly operate by providing support to solve customer issues, and are bounded by SLAs 3 . Resolution of each issue is guided by manually authored documented procedures (e.g., call flow charts). Each such issue is recorded in the form of a problem/change ticket whose contents are mostly textual. In current practice, managers use text clus-tering tools to cluster tickets that resulted in SLA violations (the most important quality indicator), and analyze such clusters using word-based representations such as tag clouds to identify distinct categories of problems that led to SLA violations. Seemingly prob-lematic clusters are then given to Quality Analysts who analyse the clusters by reading and assimilating the tickets in those and pro-vide insights to enable faster resolutions of such problems. Here, we would want to minimize the number of irrelevant documents in such clusters since they would contribute only marginally (or not at all) to derive insights from the cluster. Rule based interpretable models, being self contained, could boost the chances of being able to filter out such cases by just glancing at the rules. In such a set-ting, usage of word-based representations is counter-intuitive since we want to remove statistically insignificant concepts, that are pre-cisely the ones least likely to be represented in WBRs . Other Applications: The problem outlined above is a manifes-tation of a more general class of scenarios where users inspect large document datasets and select a few clusters for closer man-ual inspection. This poses the challenge of being able to refine the clusterings at any cost, since the laborious process of inspecting documents manually is the target of optimization. Other contexts http://en.wikipedia.org/wiki/Service_level_agreement Alg. 1 RGC-N 1: C  X   X  , R  X   X  , W  X  = top-t words acc. to CR 2: for w  X  W  X  do 3: C =[ C, { w } ] ,R =[ R, { f w  X  1 } ] 4: end for 5: while | C | &lt;k do 6: Remove the most similar pair of clusters c i ,c j from C 7: Avoid_Overlap( c i ,c j ) 8: Merge them and add them to C 9: end while 10: Reduce_Cluster_Rules(C,R) 11: while W has words yet to be considered do 12: pick w  X  W where | D w  X  C | is maximum 13: if D w overlaps with the cluster with which it has maximum 14: Merge D w with that cluster 15: end if 16: end while
The Algorithm (Algorithm 1) identifying W  X  , the top-t words according to CR that covers at least  X  % of the dataset. The clus-ter generation phase (lines 2-4) generates one cluster out of each word in W  X  , the cluster comprising of all words containing at least one occurence of the word. It may be noted that these clusters need not be disjoint and may overlap. The Merging phase (lines 5-10) starts off with multiple overlapping clusters, and merges them to k clusters, eliminating overlaps when necessary (as described in Example 1 ). The Merging phase may introduce some redun-dancy in the rules due to merging to a cluster to which overlap was avoided earlier; here, the negated condition added earlier could be eliminated due to the merger (as illustrated in Example 1). The ReduceClusterRules ( . ) function eliminates such redundancies in an easy and straightforward way. The coverage enhancement phase (lines 11-15) considers remaining words, picking those words that occur in most unclustered documents first. Each such word that has a single overlapping cluster would have itself added to the clus-ter if it bears maximal similarity with that cluster. Each such merger leads to a merger of the corresponding rules using a disjunction.
Example 1: Consider two clusters c 1 and c 2 chosen for merger, represented by rules f w 1  X  1 and f w 2  X  1 respectively. Let it be the case that c 1 overlaps with another cluster c having a rule f w  X  1 . Since we want to eventually have non-overlapping clusters in the output, we resort to avoiding the overlap between the merged cluster and c . In RGC-N, we accomplish this merger by forming a new cluster merging c 1 and c 2 but excluding those documents that are in c . This leads to the following rule: The condition ( f w 1  X  1  X  !( f w  X  1)) represents c 1 \ c ,whichis then merged (using disjunction) with c 2 . This negated condition may be removed later if the merged cluster and c become part of a single cluster by mergers later on.
UDT [2] is an approach for interpretable clustering that generates text clusters that could be represented by rules on word frequen-cies, and hence, is fully reconfigurable. This decision tree based technique works by starting with the entire corpus as the dataset associated with the root node, and progressively splits it into child RGC outperforms UDT by almost 4 times; the total rule lengths of RGC and UDT stand at 494 and 1946 respectively. Although UDT seemingly gives high purity, such high purities are achieved with as many as 60-90 clusters and are hence not very useful. Evaluations based on other measures such as Entropy and F-Measure also were found to assert that RGC and RGC-N are much better than UDT. In summary, RGC is seen to empirically perform much better than UDT for text clustering.
Now, we analyze RGC and its performance against the K-Means algorithm. High values of net purity and f-measure are desirable whereas lower values of entropy indicate that the clustering corre-sponds better to the labels. The Purity of the clusterings generated are illustrated in Figure 2. K -Means performs consistently better than RGC leading to an average purity of 0 . 93 wheras the RGC clusterings have a purity of 0 . 88 on the average. The F-Measure plot (Figure 3) also reveals a similar behavior with RGC faring 0 . 03 lesser than K -Means clusterings on an average.
Document clustering techniques are well evolved and give very high accuracies, but often produce models that are hard to inter-pret. However, in certain real world scenarios where clusters of documents are to be selected for manual review, it becomes nec-essary to produce an interpretable and reconfigurable model of the clustering. A variety of such scenarios exist. Most document clus-tering algorithms in literature score poorly on the combined goal of interpretability and reconfigurability of cluster models. Our ap-proach for interpretable document clustering, RGC, associates each cluster with a rule of conditions on word frequencies; the rule is satisfied by only those documents that belong to the cluster. How-ever, such an approach could leave out some documents as un-clustered. An empirical evaluation against UDT illustrates the ef-fectiveness of our approaches. Our analysis of the well studied accuracy-interpretability trade-off in the context of RGC shows that the RGC clusterings are only at most 5% less pure than those from classical clustering algorithms for a wide variety of text datasets.
