 Bayesian methods are widely used throughout engineering for estimating quantities from corrupted measurements. Those that minimize the mean squared error (known as Bayes least squares , or BLS) are particularly widespread. These estimators are usually derived assuming explicit knowledge of the observation process (expressed as the conditional density of the observation given the quantity to be estimated), and the prior density over that quantity. Despite its appeal, this approach is often criticized for the reliance on knowledge of the prior distribution, since the true prior is usually not known, and in many cases one does not have data drawn from this distribution with which to approximate it. In this case, it must be learned from the same observed measurements that are available in the estimation problem. In general, learning the prior distribution from the observed commonly used  X  X mpirical Bayesian X  approach [1], one assumes a parametric family of densities, whose parameters are obtained by fi tting the data. This prior is then used to derive an estimator that may be applied to the data. If the true prior is not a member of the assumed parametric family, however, such estimators can perform quite poorly.
 An estimator may also be obtained in a supervised setting, in which one is provided with many pairs containing a corrupted observation along with the true value of the quantity to be estimated. In this case, selecting an estimator is a classic regression problem: fi nd a function that best maps the observations to the correct values, in a least squares sense. Given a large enough number of training samples, this function will approach the BLS estimate, and should perform well on new samples drawn from the same distribution as the training samples. In many real-world situations, however, one does not have access to such training data.
 In this paper, we examine the BLS estimation problem in a setting that lies between the two cases described above. Speci fi cally, we assume the observation process (but not the prior) is known, and we assume unsupervised training data, consisting only of corrupted observations (without the correct values). We show that for many observation processes, the BLS estimator may be written directly in terms of the observation density. We also show a dual formulation, in which the BLS estimator may be obtained by minimizing an expression for the mean squared error that is written only in terms of the observation density. A few special cases of the fi rst formulation appear in the empirical Bayes with improvement of estimators [3, 4, 5]. Our work serves to unify these prior-free methods within a linear algebraic framework, and to generalize them to a wider range of cases. We develop practical nonparametric approximations of estimators for several different observation processes, demonstrat-ing empirically that they converge to the BLS estimator as the amount of observed data increases. We also develop a parametric family of estimators for use in the additive Gaussian case, and examine their empirical convergence properties. We expect such BLS estimators, constructed from corrupted observations without explicit knowledge of, assumptions about, or samples from the prior, to prove useful in a variety real-world estimation problems faced by machine or biological systems that must learn from examples. Suppose we make an observation, Y , that depends on a hidden variable X , where X and Y may be scalars or vectors. Given this observation, the BLS estimate of X is simply the conditional likelihood function is P Y | X then this can be written using Bayes X  rule as where the denominator is the distribution of the observed data: If we know P X and P Y | X , we can calculate this explicitly.
 Alternatively, if we do not know P X or P Y | X , but are given independent identically distributed estimator f ( y )= E { X | Y = y } nonparametrically, or we could choose a parametric family of estimators { f  X  } , and choose  X  to minimize the empirical squared error: However, in many situations, one does not have access to P X , or to samples drawn from P X . 2.1 Prior-free reformulation of the BLS estimator In many cases, the BLS estimate may be written without explicit reference to the prior distribution. We begin by noting that in Eq. (1), the prior appears only in the numerator This equation may be viewed as a composition of linear transformations of the function P X ( x ) where and the operator A computes an inner product with the likelihood function Similarly, Eq. (2) may be viewed as the linear transformation A applied to P X ( x ) . If the linear transformation A is 1-1, and we restrict P Y to lie in the range of A , then we can then write the numerator as a linear transformation on P Y alone, without explicit reference to P X : diagonal matrix containing values of x , and  X  is matrix multiplication.
 This allows us to write the BLS estimator as Note that if we wished to calculate E { X n | Y } , then Eq. (3) would be replaced by ( A  X  X n  X  we may extend this to any polynomial function (and thus to any function that can be approximated with a polynomial): from P Y . In many situations, this operation will not be well-behaved. For example, in the case of additive Gaussian noise, A  X  1 is a deconvolution operation which is inherently unstable for high frequencies. The usefulness of Eq. (4) comes from the fact that in many cases, the composite oper-ation L may be written explicitly, even when the inverse operation is poorly de fi ned or unstable. In section 3, we develop examples of operators L for a variety of observation processes. 2.2 Prior-free reformulation of the mean squared error In some cases, developing a stable nonparametric approximation of the ratio in Eq. (4) may be dif fi -cult. However, the linear operator formulation of the BLS estimator also leads to a dual expression parameterized by  X  , the mean squared error may be decomposed into two orthogonal terms: The second term is the minimum possible MSE, obtained when using the optimal estimator. Since it does not depend on f  X  , it is irrelevant for optimizing  X  . The fi rst term may be expanded as Again, the second expectation does not depend on f  X  . Using the prior-free formulation of the previ-ous section, the second component of the fi rst expectation may be written as all of the above, we have: where the expectation on the right is over the observation variable, Y . In practice, we can solve for an optimal  X  by minimizing the sample mean of this quantity: where { Y n } is a set of observed data. Again this does not require any knowledge of, or samples drawn from, the prior P X . by noting that the de fi nition implies that or, equivalently an eigenfunction (eigenvector, for discrete variables) of eoperator L , with associated eigenvalue x . Consider a standard example, in which the variable of interest is corrupted by independent additive noise: Y = X + W . The conditional density is We wish to fi nd an operator which when applied to this conditional density (viewed as a function of y ) will give for all x . Subtracting y P W ( y  X  x ) from both sides gives where is a linear shift-invariant operator (acting in y ).
 Taking Fourier transforms and using the convolution and differentiation properties gives: so that This gives us the linear operator where F  X  1 denotes the inverse Fourier transform. Note that throughout this discussion X and W played symmetric roles. Thus, in cases with known prior density and unknown additive noise density, one can formulate the estimator entirely in terms of the prior.
 Our prior-free estimator methodology is quite general, and can often be applied to more complicated observation processes. In order to give some sense of the diversity of forms that can arise, Table 1 provides additional examples. References for the speci fi c cases that we have found in the statistics literature are provided in table.
 Obs. process Obs. density: P Y | X ( y | x ) Numerator: N ( y )= L { P Y } ( y ) Discrete A ( A  X  X  X  A  X  1 ) P Y ( y ) Gen. add. P W ( y  X  x ) y P Y  X  X   X  1 i  X   X  ln P W (  X  ) P Y (  X  )
Add. Gaussian [6]/[4]* Add. Poisson  X  k e  X   X  k !  X  ( y  X  x  X  ks ) yP Y ( y )  X   X sP Y ( y  X  s )
Add. uniform
Add. random # of components Disc. exp. [2]/[5]* h ( x ) g ( n ) x n g ( n ) g ( n +1) P Y ( n +1) Disc. inv. exp. [5]* h ( x ) g ( n ) x  X  n g ( n ) g ( n  X  1) P Y ( n  X  1) Cnt. inv. exp. [3]* h ( x ) g ( y ) e T ( y ) /x g ( y ) y  X  X  X  T (  X  y ) g (  X  y ) P Y (  X  y ) d  X  y Poisson [7]/[5]* x n e  X  x n ! ( n +1) P Y ( n +1)
Gauss. scale mixture 1  X  2 Lapl. scale mixture 1 x e  X  y x ; x, y &gt; 0 P Y { Y&gt;y } Table 1: Prior-free estimation formulas. Functions written with hats or in terms of  X  represent multiplication in the Fourier Domain. n replaces y for discrete distributions. Bracketed numbers are references for operators L , with * denoting references for the parametric (dual) operator, L  X  . 4.1 Non-parametric examples Since each of the prior-free estimators discussed above relies on approximating values from the observed data, the behavior of such estimators should approach the BLS estimator as the number of data samples grows. In Fig. 1, we examine the behavior of three non-parametric prior-free estimators based on Eq. (4). The fi rst case corresponds to data drawn independently from a binary source, which are observed through a process in which bits are switched with probability 1 4 . The estimator does not know the binary distribution of the source (which was a  X  X air coin X  for our simulation), but does know the bit-switching probability. For this estimator we approximate P Y using a simple histogram, and then use the matrix version of the linear operator in Eq. (3). We characterize the behavior of this estimator as a function of the number of data points, N , by running many Monte Carlo simulations for each N and indicating the mean improvement in MSE (compared with the ML estimator, which is the identity function), the mean improvement using the conventional BLS estimation function, E { X | Y = y } assuming the prior density is known, and the standard deviations of the improvements taken over our simulations.
 Figure 1 b shows similar results for additive Gaussian noise, with SNR replacing MSE. Signal den-sity is a generalized Gaussian with exponent 0 . 5 . In this case, we compute Eq. (4) using a more Fig. 1: Empirical convergence of prior-free estimator to optimal BLS solution, as a function number of observed samples of Y . For each number of observations, each estimator is simulated many times. Black dashed lines show the improvement of the prior-free estimator, averaged over simulations, relative to the ML estimator. White line shows the mean improvement using the conventional BLS solution, E { X | Y = y } , assuming the prior density is known. Gray regions denote  X  one standard deviation. ( a ) Binary noise (10,000 simulations for each number of observations); ( b ) additive Gaussian noise (1,000 simulations); ( c ) Poisson noise (1,000 simulations). sophisticated approximation method, as described in [8]. We fi t a local exponential model similar to that used in [9] to the data in bins, with binwidth adaptively selected so that the product of the number of points in the bin and the squared binwidth is constant. This binwidth selection proce-dure, analogous to adaptive binning procedures developed in the density estimation literature [10], provides a reasonable tradeoff between bias and variance, and converges to the correct answer for any well-behaved density [8]. Note that in this case, convergence is substantially slower than for the binary case, as might be expected given that we are dealing with a continuous density rather than a single scalar probability. But the variance of the estimates is very low.
 Figure 1 c shows the case of estimating a randomly varying rate parameter that governs an inho-mogeneous Poisson process. The prior on the rate (unknown to the estimator) is exponential. The observed values Y are the (integer) values drawn from the Poisson process. In this case the his-togram of observed data was used to obtain a naive approximation of P Y ( n ) . It should be noted that improved performance for this estimator is expected if we were to use a more sophisticated approximation of the ratio of densities. 4.2 Parametric examples In this section we discuss the empirical behavior of the parametric approach applied to the additive Gaussian case. From the derivation in section 3, and restricting to the scalar case, we have In this particular case,, it is easier to represent the estimator as Substituting into Eq. (5) gives and are given data { Y n } we can try and minimize This expression, known as Stein X  X  unbiased risk estimator (SURE) [4], favors estimators g  X  that have small amplitude, and highly negative derivatives at the data values. This is intuitively sensible: the resulting estimators will  X  X hrink X  the data toward regions of high probability.
 Recently, an expression similar to Eq. (12) was used as a criterion for density estimation in cases where the normalizing constant, or partition function, is dif fi cult to obtain [11]. The prior-free Fig. 2: Example bump functions, used for linear parameterization of estimators in Figs. 3(a) and 3(b). Fig. 3: Empirical convergence of parametric prior-free method to optimal BLS solution, as a function number of data observations, for three different parameterized estimators. ( a )3 bump; ( b )15 bumps; Gaussian noise. approach we are discussing provides an interpretation for this procedure: the optimal density is the one which, when converted into an estimator using the formula in Table 1 for the additive Gaussian case, gives the best MSE. This may be extended to any of the linear operators in Table 1. As an example, we parametrize g as a linear combination of nonlinear  X  X ump X  functions where the functions g k are of the form as illustrated in Fig. 2. Recently, linear parameterizations have been used in conjunction with Eq. (12) for image denoising in the wavelet domain [12].
 is a quadratic function of the coef fi cients. For our simulations, we used a generalized Gaussian prior, with exponent 0 . 5 . Figure 3 shows the empirical behavior of these  X  X URE-bump X  estimators off inherent in the fi xed parameterization. Three bumps behaves fairly well, though the asymptotic behavior for large amounts of data is biased and thus falls short of ideal. Fifteen bumps asymp-totes correctly but has very large variance for small amounts of data (over fi tting). For comparison purposes, we have included the behavior of SURE thresholding [13], in which Eq. (4.2) is used to choose an optimal threshold,  X  , for the function As can be seen, SURE thresholding shows signi fi cant asymptotic bias although the variance behavior is nearly ideal. We have reformulated the Bayes least squares estimation problem for a setting in which one knows the observation process, and has access to many observations. We do not assume the prior density is known, nor do we assume access to samples from the prior. Our formulation thus acts as a bridge between a conventional Bayesian setting in which one derives the optimal estimator from known prior and likelihood functions, and a data-oriented regression setting in which one learns the optimal estimator from samples of the prior paired with corrupted observations of those samples. In many cases, the prior-free estimator can be written explicitly, and we have shown a number of examples simple cases, we developed implementations and demonstrated that these converge to optimal BLS estimators as the amount of data grows. We also have derived a prior-free formulation of the MSE, which allows selection of an estimator from a parametric family. We have shown simulations for a linear family of estimators in the additive Gaussian case.
 These simulations serve to demonstrate the potential of this approach, which holds particular ap-peal for real-world systems (machine or biological) that must learn the priors from environmental observations. Both methods can be enhanced by using data-adaptive parameterizations or fi tting procedures in order to properly trade off bias and variance (see, for example [8]). It is of partic-ular interest to develop incremental implementations, which would update the estimator based on incoming observations. This would further enhance the applicability of this approach for systems that must learn to do optimal estimation from corrupted observations.
 Acknowledgments This work was partially funded by the Howard Hughes Medical Institute, and by New York Univer-sity through a McCracken Fellowship to MR.
 [1] G. Casella,  X  X n introduction to empirical Bayes data analysis, X  Amer. Statist. , vol. 39, pp. 83 X  [2] J. S. Maritz and T. Lwin, Empirical Bayes Methods . Chapman &amp; Hall, 2nd ed., 1989. [3] J. Berger,  X  X mproving on inadmissible estimators in continuous exponential families with ap-[5] J. T. Hwang,  X  X mproving upon standard estimators in discrete exponential families with appli-[6] K. Miyasawa,  X  X n empirical bayes estimator of the mean of a normal population, X  Bull. Inst. [7] H. Robbins,  X  X n empirical bayes approach to statistics, X  Proc. Third Berkley Symposium on [8] M. Raphan and E. P. Simoncelli,  X  X mpirical Bayes least squares estimation without an explicit [9] C. R. Loader,  X  X ocal likelihood density estimation, X  Annals of Statistics , vol. 24, no. 4, [10] D. W. Scott, Multivariate Density Estimation: Theory, Practice, and Visualization . John Wiley, [11] A. Hyvarinen,  X  X stimation of non-normalized statistical models by score matching, X  Journal [12] F. Luisier, T. Blu, and M. Unser,  X  X URE-based wavelet thresholding integrating inter-scale [13] D. Donoho and I. Johnstone,  X  X dapting to unknown smoothness via wavelet shrinkage, X  J
