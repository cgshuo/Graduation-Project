 Understanding the content of user X  X  image posts is a partic-ularly interesting problem in social networks and web set-tings. Current machine learning techniques focus mostly on curated training sets of image-label pairs, and perform im-age classification given the pixels within the image. In this work we instead leverage the wealth of information available from users: firstly, we employ user hashtags to capture the description of image content; and secondly, we make use of valuable contextual information about the user. We show how user metadata (age, gender, etc.) combined with image features derived from a convolutional neural network can be used to perform hashtag prediction. We explore two ways of combining these heterogeneous features into a learning framework: (i) simple concatenation; and (ii) a 3-way mul-tiplicative gating, where the image model is conditioned on the user metadata. We apply these models to a large dataset of de-identified Facebook posts and demonstrate that mod-eling the user can significantly improve the tag prediction quality over current state-of-the-art methods.
 I.2.6 [ Learning ]: Connectionism and neural nets; I.5.1 [ Models ]: Neural nets; I.5.4 [ Applications ]: Computer vision Social media, user modeling, deep learning, hashtagging, large scale image annotation
Hashtags (single words, abbreviations or word concate-nations, prefixed by the # symbol) commonly accompany online image content, most notably on social media plat-forms. Far richer than conventional semantic labels, they  X  Work done while at Facebook AI Research c  X  impressive performance [7]. Training such models has typ-ically depended on large sets of manually annotated data (e.g. Imagenet [1]), which is time-consuming and arduous to obtain. Further, such data ignores several aspects of image understanding that are of particular interest to web users: (i) their focus is on precise physical description so aspects such as sentiment are not addressed; (ii) the data distribu-tion differs from online data and it is also unlikely to adapt quickly to changing user interests; and (iii) labels are inde-pendent of the users who originally authored the images or image posts (e.g. they are labeled by a Mechanical Turk worker).

In this work, we consider the vast amount of image con-tent on the web where users have provided hashtags as a powerful, alternative training data source. In addition to generating very large amounts of labeled data compared to a manually curated set, we can also directly train on the actual data we wish to capture, rather than one whose dis-tribution differs from user X  X  interests. We thus define our training task as follows: we wish to predict the hashtags for a given image uploaded by a given user . The learning techniques we employ thus contain feature representations modeling both the image via pixels and the user in the form of metadata. Our hypothesis is that the combination of these sources provides useful information.

We consider several possible architectures based on em-bedding models. Embeddings are vector representations of images, metadata or text. Embedding models have been successfully used in a variety of contexts such as large scale image annotation [17], zero-shot learning of image categories [2] and hashtag prediction for text posts [18]. An embedding model maps inputs and hashtags into a common embedding space. For a given input, hashtags are ranked according to the dot product between hashtag embedding vectors and the image embedding vector. For the image features, a pre-trained convolutional neural network acts as a feature ex-tractor, converting a raw input image into a concise image descriptor. We present three different methods of embed-ding inputs. The simplest method learns a linear mapping from image descriptors to embedding space. We then intro-duce two different methods of incorporating user metadata into the embedding process: (i) simple concatenation; and (ii) a 3-way multiplicative gating, where the image model is conditioned on the user metadata. In our experiments we in-corporate the user X  X  gender, age and city into the prediction. Our proposed methods are general, robust and scalable. As such, they can be combined with a variety of machine learn-ing approaches and also deployed in large-scale real-world situations.
 Image hashtag prediction has a variety of applications. For example, social media sites could use such a system to recommend hashtags to users as they upload image con-tent. They can also be used for image search or for recom-mendation and ranking images based on content. Hashtags can also fulfill other roles such as disambiguating synonyms (e.g. jaguar #car vs jaguar #bigcat ), or identifying enti-ties ( #nyc ). In our experiments we apply our models to a large dataset of de-identified Facebook posts and demon-strate that modeling the image and user can significantly improve the tag prediction quality over current state-of-the-art methods. Figure 2: How the user metadata is combined with the image features in the user-biased model . where V  X  R Y  X  d is the hashtag embedding matrix and V i indexes the i th row of V . Following [17], we constrain the hashtag embedding matrix such that This regularization technique helps prevent the model from overfitting.

The image embedding function takes an image descriptor as input and produces a d -dimensional embedding vector. We use a pre-trained convolutional neural network [8] as a feature extractor to obtain the image descriptor from the raw image post. The convolutional network was trained sep-arately on  X  1 million Facebook images. The network was trained to classify 1000 different concepts which span mul-tiple categories such as Object, Scenes, Actions, Food, Ani-mals, etc. The image descriptors of dimensionality n = 4096 are extracted from the final fully connected layer of the net-work, which has an architecture similar to that of [7].
We propose three different methods of embedding image descriptors. The simplest model does not use any user in-formation and embeds an image descriptor via a linear map-ping. We also propose two different methods of incorporat-ing user metadata into the embedding process. The user-biased model introduces a user-dependent additive bias into the image embedding function and the user-multiplicative model has a user descriptor gate, via a multiplication opera-tion, the image embedding function. We now describe each of these models in detail.
The bilinear embedding model [17] does not incorporate any user information into the image embedding function. The image embedding function is a linear map of the form where P  X  R d  X  n is the image embedding matrix. As with the hashtag embedding matrix, we constrain the norm of P : to help prevent overfitting. The bilinear embedding model is illustrated in Figure 1.
Assume now that we have information associated with the users responsible for each image post. Let u  X  R m denote a vector representation of a user. The user-biased model gives a simple method for leveraging user information by adding a user dependent bias term to the image embedding as  X   X  R m  X  K ,  X   X  R d  X  K and  X   X  R n  X  K . In this constrained form, Z has K ( m + d + n ) free parameters. Section A of the appendix explains the 3-way multiplicative model in further detail, including the gradient computations performed dur-ing training.

As it is unclear how to choose an appropriate vector repre-sentation for user information, we learn one instead. This is done using a neural network which converts the user meta-data to an appropriate vector prior to the  X  matrix in the 3-way connection. The full picture of this model is shown in Figure 3. In our experiments we use a single layer neural network with 24 hidden units (i.e. the user descriptor acted upon by the 3-way connection is 24-dimensional) with Rec-tified Linear Units [11]. The weights of this network, along with the 3-way factors, are learned jointly. Adding this ex-tra layer makes the model more powerful and also provides a way of extracting concise user vectors from the hidden layer activations, which could potentially be transferred to other tasks, beyond the one considered in this paper.
We train our models by minimizing the weighted approximate-rank pairwise (WARP) loss as described in [17]. The WARP loss approximately optimizes precision at k using a negative sampling technique. This type of loss is ideal for our task since it easily scales to large hashtag vocabularies.
The algorithm proceeds as follows: At every iteration, we sample a positive example, ( x,y + ). Up to 1000 negative hashtags, y  X  , are then sampled in an attempt to find one such that where m specifies the margin. We then take a gradient step to minimize Following [18], we set m = 0 . 1 in all our experiments.
We minimize the loss with parallel stochastic gradient de-scent with the hogwild algorithm [12]. Weights were initial-ized from a zero-mean Gaussian distribution with standard deviation 0.001. The learning rate was initialized to 0.01 and was manually reduced by a factor of 10 when perfor-mance stopped improving on a validation set. We used a momentum constant of 0.9. We found training of the 3-way connection was significantly improved by normalizing the image descriptors to have L2 norm of 1 and so this step was performed for all models.
We evaluate our image embedding models on a large Face-book dataset, consisting of 20 million public images up-loaded by  X  10.4 million de-identified users. The images were collected at random from uploads over a period of several days in October 2014. The images were selected to have at least one hashtag per image, but often had multiple hash-tags. In total, the dataset contained  X  4.6 million distinct hashtags. The mean number of hashtags per image was 2.7, Method d K P@1 R@10 A@10 Freq. baseline --3.04% 5.63% 9.45% Bilinear 64 -7.37% 11.71% 18.69% Bilinear 128 -7.37% 11.69% 18.44% Bilinear 256 -6.75% 10.84% 17.25% Bilinear 512 -6.50% 10.83% 17.17% User-biased 64 -9.02% 13.63% 21.88% User-biased 128 -9.00% 13.67% 21.83% User-biased 256 -8.48% 13.03% 20.96%
User-biased 512 -7.98% 12.51% 20.05% 3-way mult. 64 50 8.95% 13.66% 21.82% 3-way mult. 64 100 9.03% 13.81% 22.04% 3-way mult. 64 200 8.96% 13.81% 22.05% 3-way mult. 64 300 9.00% 13.74% 21.96% 3-way mult. 64 400 8.96% 13.65% 21.82% Table 3: Prediction results for models trained on data with a natural hashtag distribution.
 Table 2. We restricted ourselves to images posted by users from five populous English speaking countries to ensure that the hashtags had a common underlying language. Imprecise location information for each de-identified user took the form of the GPS location of their home town/city 1 . The dataset was split into disjoint training and test sets, with each user only appearing in one of them (i.e. no overlap). The test set was 100k images in size, with the remainder used for training.

We construct a 10-dimensional user descriptor based on the 4 pieces of metadata. One dimension represents age as a continuous value, scaled to have value in [0, 1]. Another dimension represents gender, recorded as -1 or 1 for females and males respectively. An unknown gender is represented by 0. The country of origin is encoded as a 1-hot 5d vector for the five different countries. Finally, GPS coordinates are converted to 3-dimensional Cartesian coordinates and scaled to [0,1] range.
We evaluate the embedding models described in Section 3 on both the natural and balanced versions of our dataset. For all three models, we vary d , the dimensionality of the embedding space to understand its effect on performance. For the 3-way multiplicative model, we also vary the rank K of the tensor factorization, which controls the number of parameters in the embedding function  X  I  X  U . We include a simple baseline that ignores the test image and ranks hash-tags according to their natural distribution in the training set.

The models are evaluated with three different metrics: precision, recall and accuracy. These are defined as follows: let Rank( x,u,k ) denote the set of top k ranked hashtags by the model for example ( x,u ) and let GroundTruth( x,u ) de-note the set of hashtags actually tagged by the user u for the image x , then:
I.e. the GPS coordinates of the town/city center associated with the user X  X  profile, not the home address of the users, or where the photo was taken.
 the bilinear model and the multiplicative model is greatest. prediction task harder as evidenced by the significant re-duction in frequency baseline performance. In these experi-ments, the user-biased approach gives slight gains over bilin-ear. But the user-multiplicative model significantly outper-forms both, showing it is able to make effective use of the user metadata. Examples of the predicted hashtags from this model are shown in Figure 5. The predictions can be seen to be far more varied and interesting than those from the same model trained on the natural version of the dataset. For example, very common hashtags such as #tbt , #wcw , #mcm are ranked in the top 10 for almost every photo by the model trained on the natural hashtag distribution. The model trained on the balanced distribution produces less frequent hashtags that are still relevant to the image.
Tables 3 and 4 show precision and recall values for im-ages, averaged over all examples in the test set. Since some hashtags are predicted more accurately than others, we also show precision and recall values for individual hashtags. The average precision and recall at k for a particular hashtag, y ,
Age Females Males 13-17 43-47 Toronto users, and vice-versa.

Figure 5b plots precision-recall curves of a subset of hash-tags for models trained on the balanced distribution. In this case we see the user-multiplicative model performs much better than the bilinear and user-biased models on most hashtags.

Figure 6a and Figure 6b show Recall@10 for several hash-tags from the three models trained on the natural and bal-anced distributions respectively. In the leftmost plot we see the recall values for the 20 most frequent hashtags. For these very frequent hashtags, the models that incorporate user information perform comparably to the bilinear model. By comparing the leftmost plots in Figure 6a and Figure 6b we also see that the most frequent hashtags have much higher recall values for models trained on the natural hash-tag distribution than for the models trained on the balanced distribution. The middle plot shows the top 20 hashtags for which the bilinear model has the highest recall, i.e. the hashtags that are predicted well without the aid of addi-tional user information. Many of the hashtags with highest recall ( #belascothursdays , #litsunday , #acehollywood ) correspond to large event posters. Many different users post the event image and hashtag the image with the event name. Since the images and hashtags are nearly identical, the model is able to achieve nearly perfect recall. The right-hand plot shows the top 20 hashtags for which the difference in performance between the bilinear model and the multi-plicative model is greatest. In other words, these figures Table 5: Top ranked hashtags from user-multiplicative model for sample test images. Left column shows hashtags predicted by model trained on the natural hashtag distribu-tion. Right column shows hashtags from model trained on the balanced hashtag distribution. Photos used with owner permission. Figure 7: t-SNE visualization of user embeddings for 10K different users. Each plot is color coded by a different di-mension of the user meta data. t-SNE algorithm [16]. Four different color codings are used to show different user attributes. Clear structure is apparent in many cases, for example, the radius from the origin seems to correspond to increasing age.
We introduced a set of embedding models that predict highly diverse and relevant hashtags for real-world Facebook images. The simplest of these shows how image features de-rived from a convolutional neural network can be used to perform image hashtag prediction. We then showed how user metadata could be combined with image features to improve image hashtag prediction. The addition of user in-formation gave a significant performance boost, particularly when incorporated in a multiplicative fashion.

Particular care has to be taken when working on real world datasets rather than curated ones. We addressed the highly skewed hashtag distribution observed in our dataset by downsampling the more frequent hashtags. We show that this technique produces more varied hashtag predictions.
Our models produce hashtags that capture many subtle social and sentiment of the images, and are far richer than the precise semantic descriptions output by many current recognition models. The resulting system is highly scalable and could be used in a number of applications such as au-tomated hashtag suggestion, image search or for recommen-dation and ranking images based on content. [1] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and [18] J. Weston, S. Chopra, and K. Adams. #tagspace: [19] L. Xiong, X. Chen, T. Huang, J. Schneider, and [20] M. Zeiler, G. Taylor, L. Sigal, I. Matthews, and [21] V. Zheng, B. Cao, Y. Zheng, X. Xie, and Q. Yang.
The user-multiplicative model gates the image embedding matrix by a user descriptor, u . The user conditioned image embedding matrix is where Z (1) ,...,Z ( m ) are d  X  n matrices and u i indexes the i th component of the user descriptor. Combining each of the Z ( i )  X  X  into a tensor gives the image embedding tensor Z  X  R m  X  d  X  n . To facilitate learning, Z is constrained to be of the form where  X  k  X  R m ,  X  k  X  R d ,  X  k  X  R n and the user factors, embedding factors and image factors respectively. The pa-rameter K specifies the number of factors.

Letting  X   X  R m  X  K ,  X   X  R d  X  K and  X   X  R n  X  K denote the factors in matrix form, the image embedding matrix can be written as:
The user-multiplicative image embedding function can be re-written as:
