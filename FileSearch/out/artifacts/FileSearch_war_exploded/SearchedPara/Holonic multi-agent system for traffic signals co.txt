 1. Introduction development of large-scale distributed systems. The domain of traffic and transportation systems is well suited for an agent-based approach because transportation systems are usually geo-graphically distributed in dynamic changing environments ( Chen and Cheng, 2010 ). The reason for the growing success of agent technology in these areas is that the inherent distribution allows for a natural decomposition of the system into multiple agents that interact with each other to achieve a desired global goal ( Chen and Cheng, 2010 ). The agent technology can significantly enhance the design and analysis of problem domains under the following three conditions ( Adler and Blue, 2002 ): 1. The problem domain is geographically distributed. 2. The subsystems exist in a dynamic environment. 3. The subsystems need to interact with each other more flexibly.
Agent-based transportation systems allow distributed subsys-tems collaborating with each other to perform traffic control and management based on real-time traffic conditions. Traffic signals control is a principle part of transportation systems management. Applying intelligent techniques to improve the traffic flow of the system is necessary due to increase in the number of vehicles in urban networks and limitation of road infrastructure ( Bazzan et al., 2008 ). It has been shown that traffic signal timing has important effects on traffic delay reduction and consequently reduction in travel time and energy consumption.
Techniques and methods for traffic signals control are usually applied in a single or a small number of intersections ( Gre  X  goire et al., 2007 ; Weiring, 2000 ). For large networks with multiple intersections, the interdependency between them makes it diffi-cult to set the signals appropriately. An approach to deal with this complicated traffic signals control is using agent-based distrib-uted control techniques which involve multiple intelligent agents, multi-agent systems (MASs).

In this paper, we present a method based on multi-agent systems to control the traffic signals in a large network. Multi-agent systems (MASs) are generally composed of autonomous, reactive, proactive and interacting entities called agents usually engaged in the realisation of a common goal. Although, there are many actors in a traffic network which can be considered as an autonomous agent, in this paper each agent represents an intersection or traffic signal.
Hence, a large traffic network conta ins many agents interacting with each other which made a large-scale MAS.

A common approach for tackling the interactions among a large population of agents is assigning an organisation to agents for cooperating and working towards a common goal. In the agent organisational paradigm, interactions among the agents are optimised and reduced effectively and this makes it a very popular technique to manage the complexity of large systems. In MAS, an organisation is defined as the collection of roles, relationships and authority structures that govern the behaviour of the multi-agent system.

We present an organisation-based MAS to model large traffic networks which is called holonic multi-agent system . A holonic organisation is a hybrid, recursive and hierarchical structure which is able to generate dynamic linkages to control the structure. The problem of traffic signals control for a large network can be divided into sub-problems each assigned to a holon. This paper also introduce a control strategy based on reinforcement learning that is especially developed for holonic multi-agent systems. In summary, the following main contribu-tions are made in this paper:
Modelling a large traffic network using holonic multi-agent system which enables us to decompose the system into smaller sub-systems in order to improve the efficiency.
Development of a hierarchical control strategy based on reinforcement learning which increases reliability and perfor-mance of the system.

The rest of the paper is organised as follows: Section 2 reviews the related works and preliminaries about learning model and holonic concepts. The proposed learning method is presented in Section 3 . This section also includes basic concepts used in our holonic model. Section 4 gives traffic signals control method based on holonic Q-learning in details. Experimental results are presented in Section 5 and finally the paper is concluded in Section 6 . 2. Related works 2.1. Learning models
Classically, Q-learning is modelled using a Markov decision process (MDP) formalism. It means that the agent is in state s , performs action a and gets scalar reward r from the environment. The environment changes to state s 0 according to a given prob-ability transition matrix T . The goal of the agent is to choose actions such that maximise discounted cumulative rewards over the time. Discounted means that short term rewards are weighted more heavily than distant future rewards. The discount factor,  X  X  0 :: 1 , awards greater weight to future reinforcements if set closer to 1. In Q-learning, Q  X  s , a  X  is a state-action value represent-ing the expected total discounted return resulting from taking action a in state s and continuing with the optimal policy there-after. During the learning process Q -values are updated by Q  X  s , a  X   X  b  X  r  X  g V  X  s 0  X  X  Q  X  s , a  X  X  1  X  V  X  s 0  X   X  max
In Eq. (1) , b is the learning rate parameter and V  X  s 0  X  is given by Eq. (2) . The discount factor, g , determines the importance of future rewards. Q  X  s , a  X  measures the quality value of the state-action pair and V  X  s 0  X  gives the best Q -value for the actions in next state, s 0 . Q-learning helps to decide upon the next action based on the expected reward of the action taken for a particular state. Successive Q -values are built increasing the confidence of the agent as more rewards are acquired for an action. As the agent explores the state space, its estimation of Q -value improves gradually. Watkins and Dayan (1992) have shown that the Q-learning algorithm converges to an optimal decision policy for a finite Markov decision process.

Assume that a i shows an agent in a MAS. a i is a tuple  X  S , P , A i , j i  X  of the set of its possible states, S i , the set of its percepts, P i , and actions, A i , and its agent function, j
P -
S i A i . A classic Q-learning technique for a i , has three main steps:
The agent estimates the state using its percept, P i , and determines which state the agent is in at time t , s i ( t ).
The agent selects an action, a i ( t ) according to the learned policy and a mechanism for exploration of the environment.
The agent updates its policy using the reward value received from environment.

The state of the environment changes and a new learning iteration starts. 2.2. Holonic models
The term  X  X  X olonic X  X  is derived from the word  X  X  X olon X  X , which introduced by Koestler (1967) . The word holon is a combination of the Greek holos  X  whole, and the suffix -on which means a particle or part. Koestler proposed this word because of two observations. The first observation is from Simon (1969) who concludes that complex systems will evolve from simple systems much more rapidly if there are stable intermediate forms than if there are not. Simon X  X  analysis reveals why every complex adaptive system is hierarchic. Koestler analysed hierarchies and stable intermediate forms in both living organisms and social organisations and found that  X  X  X holes X  X  and  X  X  X arts X  X  do not exist in an absolute sense. This fact makes his second observation. For example, a human is a composition of organs on one hand, and is a part of a human society on the other hand. Koestler proposed the word holon to describe the hybrid nature of subwholes/parts in real-life systems. Holons simultaneously are self-contained wholes to their subordinated parts, and dependent parts when considered from the inverse direction.

The organisational structure of a holonic system, or holarchy, has some advantages: They are robust against external and internal damages, they are efficient in their use of resources and they can adapt to environment changes. Koestler X  X  ideas have been applied to manufacturing systems for the first time by Suda (1990) . This led to the formation of Holonic Manufacturing
Systems (HMS) later. Holons in an HMS are characterised by their holonic attributes, autonomy and cooperativeness which have roughly the same semantics as for multi-agent systems. Auton-omy is the ability to create and control the execution of plans and strategies and cooperativeness is joint planning and coordination for a joint plan execution.

Koestler also points out that holons are autonomous self-reliant units, which have a degree of independence and handle contingencies without asking higher authorities for instructions.
Simultaneously, holons are subject to control from (multiple) higher authorities. The first property ensures that holons are stable forms, which survive disturbances. The latter property signifies that they are intermediate forms, which provide the proper functionality for the bigger whole.

Finally, Koestler defines a holarchy as a hierarchy of self-regulating holons which function (a) as autonomous wholes in super-ordination to their parts, (b) as dependent parts in sub-ordination to controls on higher levels, and (c) in co-ordination with their local environment.
 defining holonic systems, which he calls Open Hierarchical Systems (OHS). we can keep the following principles: mentioned principles. A holon in an HMAS can be seen as an agent in a multi-agent system. To understand the concept of a holon, Gerber et al. (1999) listed the properties of an agent and extend them to holons:
A holonic multi-agent system must therefore have a flexible organisation in a hierarchical structure. The holons unite to form a whole and each one can be broken down into holonic agents, this is what called the recursive breakdown of the problem.
In a holonic system, a holarchy can be modelled using whole-part relationships and it is managed in a distributed manner by system elements or holons. In HMASs the structure can be seen as a set of hierarchical levels, where the agents can interact only with other agents at the same level or at the level immediately below or above. In these systems, individual holons define their activities based on their local knowledge and decide about their behaviour by means of the permitted interactions.

The concept of holon is central to our discussion and therefore a definition of a holon can be helpful. In MAS, the vision of holons is much closer to the one that MAS researchers have of recursive or composed agents. Recursiveness means that holons can contain other inferior levels of holons, that can also be contained in another superior level of holons, being in a recursive architecture.

A holon constitutes a way to gather local and global, individual and collective points of view. A holon is thus a self-similar structure composed of holons as sub-structures and the hierarch-ical structure composed of holons is called a holarchy. Fig. 1 shows a holonic system arranged in four holarchical levels.
The strength of the holonic paradigm is its recursive definition of holons. Thus it is well adapted for large complex systems where different granularities are required. Holonic systems offer the possibility to model a system from a high-level coarse-grained perspective to a low-level fine-grained one. On the other hand, using holons as modelling approach for a small close system may not always be recommended. If the system does not require multiple levels of granularity, holons will probably introduce an unnecessary overhead. Before applying any approach to model a system, the designer should analyse the adequacy of the under-lying concepts. Hence, holonic systems are well suited for analysing and modelling of large systems where multiple levels of abstraction exist. Another type of systems are those that can be decomposed recursively into smaller sub-components ( Rodriguez, 2005 ).

It has been proved that HMAS is an effective solution for several problems associated with hierarchical and self-organising structures ( Rodriguez et al., 2005 ). It has been successfully applied in a wide range of complex systems. For instance, we can mention the works done in transportation ( Burckert et al., 1998a ), distributed sensor management ( Benaskeur and Irandoust, 2008 ), adaptive mesh problem ( Rodriguez et al., 2003 ), supply chain management ( Marcellino and Sichman, 2010 ), health orga-nisations ( Ulieru and Geras, 2002 ), biological network simulation ( Shafaei and Aghaee, 2008 ) and complex software systems ( Moise, 2008 ).

In this paper, we develop a holonic multi-agent system to model a large traffic network and use a reinforcement learning method for learning the signals timing in the holarchy. Since a large traffic network is definitely a complex system ( Bazzan, 2007 ), holonic multi-agent system gives a promising tool to model the intersections and manage the traffic jam.

Two overlapping aspects have to be distinguished in holons: the first is directly related to the holonic nature of the entity (a holon, called super-holon, is composed of other holons, called sub-holons or members) and deals with the role of a super-holon. This aspect is common to every holon and thus called the holonic aspect. The second aspect is related to the problem and depends on the application. Hence, we first describe the holonic Q-learning generally and then in the context of traffic signals control. 2.3. Learning and holonic models for traffic simulation Intersections are bottlenecks in urban transportation systems. Multi-agent systems have been used for intersection control in many works. It is beyond the scope of this paper to review the literatures on agent-based technologies in traffic and transporta-tion systems, the readers are referred to Chen et al. (2009) for a survey in this area. We mention some related works which use agent-based hierarchical control of traffic signals control or apply reinforcement learning in order to find the traffic control policy. However, for the survey of existing methods on learning and multi-agent systems in traffic control the readers are also referred to Bazzan (2009) .

Roozemond (2001) proposed an agent-based urban intersec-tion control system that reacts to changes in the traffic environ-ment and adapts itself to changing environments based on internal rules. The system consists of several intersection traffic signalling agents (ITSAs), road segment agents (RSAs), and some authority agents. The ITSAs manage the intersection controls helped by RSAs. The authority agents control and coordinate ITSAs to achieve a globally optimal system performance. Although the interesting issues tackled in this research are cooperation between multiple agents, and estimation of prediction model and adaptation to dynamic environments, the drawback is necessity for some knowledge in the form of rules. Hence, the performance highly depends on the rules.

In Cai and Yang (2007) a system composed of four types of agents, segment agent, crossing agent, section agent and central decision agent are presented. The system can achieve the global optimisation of the region, alleviate traffic jam, reduce the emission of vehicles and then increase the efficiency of traffic management by information sharing in a region. The central decision agent which stands on the top of the hierarchy, analyses the decisions from section agents. Since central decision agent controls all management processes, their system has the disad-vantages regarding centralised control such as design complexity and robustness.

Hierarchical decentralised agent-based control methods decompose the system into smaller sub-systems that have inter-actions with each other. They generally are used in large networks that decomposition causes a reduction in dimensionality.
Choy et al. (2003) and Srinivasan et al. (2006) presented a hierarchical multi-agent system that consists of three layers of agents with the lowest layer of intersection controller agents (ICAs), the middle layer of zone controller agents (ZCAs), and the highest layer of regional controller agents (RCAs). A ZCA controls several preassigned ICAs, and one RCA controls all of the ZCAs. The implementation of agents is based on feed-forward neural network and fuzzy logic theories. The neural network consists of five layers such that different parts of fu zzy reasoning mechanism stand between the layers. The system also provides online learning facilities, including reinforcement learning, learning rate and weight adjustment, and dynamic update of fuzzy relations using evolution-ary algorithm, to allow agents dynamically adapting to changing environment. However, the hybrid m ethod controls the traffic signals effectively in a dynamically chan ging environment, the design and implementation of diffe rent parts of the system is complicated and computationally expensive.

Learning techniques which were developed for multi-agent systems, can potentially give decisive contributions to control and management of traffic systems, as they meet the demands of dynamic, changing systems of many actors with different goals, distinct cognitive capabilities and learning pace. Reinforcement learning (RL) offers some potentially significant advantages in dealing with dynamic environments. Hence, RL has become a good solution to control single intersection and also network of intersections specially when there is no previous knowledge about the environment.

Some researchers have tried to use a model-based reinforce-ment learning method for traffic signals control. Gre  X  goire et al. (2007) designed a single adaptive traffic signals controller using learning agents. The agents learn a control policy trying to optimise the traffic flow by interacting directly in the system.
However, they showed that the results are better than optimal pre-timed controllers, they focused on a single intersection. There are some problems which arise from working a large number of controller together specially when the intersections are controlled by learning.

Weiring (2000) describes the use of reinforcement learning by traffic signal agents in order to minimise the overall waiting time of vehicles in a small grid. Those agents learn a value function that estimates expected waiting times of vehicles given different settings of traffic signals. One interesting issue tackled in this research is that a kind of co-learning is considered: value func-tions are learned not only by traffic signals, but also by the vehicles that can thus compute policies to select optimal routes to their destinations. One drawback of his method is that the kind of communication and knowledge may have a high cost specially when we extend the method to a larger network.

A similar model-based RL method for controlling traffic signals is presented in Steingr  X  over et al. (2005) to minimise the total travel time of all vehicles in the network. Thus, the control perspective is a global one, although actions are local to the agents. Agents here are the traffic signals but the learning task is formulated in a way that the state representation is vehicle-based (waiting times for individual vehicles), aggregated over all vehi-cles around the intersection. The network used in their simulation includes fifteen intersections. An issue is that the more informa-tion about the individual vehicle, the bigger the state space, specially when larger network is used. Hierarchical methods that use a divide-and-conquer strategy can be used to deal with large problems.

Since model-based RL can work in stationary environments, when dealing with non-stationary environments, they cannot adapt to the changing dynamics of the environment. In Silva et al. (2006) described a method for complementing RL algo-rithms so that they perform well in a specific class of non-stationary environments. They have presented a reinforcement learning with context detection that creates a partial model of the environment on demand. They validated their proposed method on traffic signals control containing nine intersections. The results showed that the performance of their method is better than both model-based and model-free RL assuming some properties that cause the method cannot scale up to larger network.
 et al. (2011) to control traffic signals in the non-stationary environment of a large network. They used a parametric repre-sentation of action space which can be used in different forms.
Although the results show improvements on delay time for a large network comparing with fixed timing method, the agents work independent of each other without any interaction. a model free traffic environment in Wen et al. (2008) and Dai et al. (2010) . Adaptive control of traffic signals has also been studied in Prashanth and Bhatnagar (2010) , which uses a function approximation method as a mapping between the states and signal timing. However, the mentioned model free RL methods have some advantages regarding non-stationary environments, they tested on isolated intersection ( Wen et al., 2008 ) or regular networks either grid ( Dai et al., 2010 ; Prashanth and Bhatnagar, 2010 ) or corridor ( Prashanth and Bhatnagar, 2010 ) networks.
These algorithms could not be implemented on larger road net-works due to the exponential increase in the computational complexity.
 traffic signals control in a large network. For example a colla-borative reinforcement learning introduced in Salkham et al. (2008) which tried to exploit local knowledge of neighboring agents in order to learn signal timings. Balaji et al. (2010) have also presented a method in which agents try to compute a new phase length based on both local and communicated information that received from neighbours.
 ods decompose the system into smaller sub-systems. The idea of using reinforcement learning method to hierarchical control of the traffic signals in a large network explored in Bazzan et al. (2010) . They organised agents in groups of limited size so that the number of joint actions is reduced. These groups are then coordinated by another agent, a tutor or supervisor that recom-mend a joint action. The paper investigates and compares the task of multi-agent reinforcement learning for control of traffic signals under the following two situations: agents act individually and agents can be tutored by another agent with a broader view of the system.
 complex systems, there are some important issues that made them inadequate: hierarchical control usually creates a rigid communication hierarchy and an authoritarian top-down flow of commands; the systems will be more vulnerable upon failures in one or more of the master controllers. Holonic organisation tries to overcome these deficits with the help of autonomy, cooperation, and self-organisation (see more description in the next section).
 by Burckert et al. (1998a) for the first time in 1998. They introduced a system called TELETRUCK for dynamic planning and optimisation of transport orders based on holonic agents ( Burckert et al., 1998a , b ). Holonic models have also been used for railway traffic control system ( Ciufudean and Filote, 2009 ) and for communication of knowledge on traffic network in Adam et al. (2012) . Adam et al. (2012) presented a multi-level architecture, inspired from holonic principals which propagate knowledge between mobile agents. They have used flexibility of holonic organisation for knowledge transfer on traffic network. optimisation using negotiation in Koz  X  lak and Pisarski (2011) .
They introduced theoretic representation of the abstract holon and a model for building and reorganising holons which was used for transportation management and a delivery strategy develop-ment in a company.

In this paper, we develop a holonic multi-agent system to model a large traffic network and use a reinforcement learning method for learning the signals timing.

Although holonic multi-agent systems have already used in traffic networks, the main contributions of this paper are applica-tion of HMAS to model traffic signals and using reinforcement learning in holonic organisation to control the signals. Since holonic organisation is composed of self-similar holons which stand in a hierarchy, reinforcement learning method is used for all holons in different levels in the same way. Therefore, all holons in the hierarchy try to learn the control policy simultaneously. 3. Holonic multi-agent Q-learning
Reinforcement learning is best suited learning method for domains in which we have limited or no previous knowledge.
RL offers a promising way to model learning in problem solving tasks because it defines problems in a compatible way using states, actions and goals. Q-learning is a particular reinforcement learning algorithm which is model-free. In this section, first we give a brief definition of Q-learning and then introduce the holonic Q-learning method which is used for traffic signals control. 3.1. Basic concepts
According to the model presented by Hilaire et al. (2000) ,there are three interrelated concepts in organisational models: role, interactions and organisation. Th is concepts are defined as follows:
Role: A role is defined as an abstraction of behaviour in a given context.

Interaction: An interaction determines the connection between two roles such that an action in each of them produces a reaction in the other role.

Organisation: A set of roles and their interactions pattern define an organisation in a specific domain. The concept of organisation combines roles and their interactions.

Holonic organisations present a powerful abstraction tool to model large-scale systems. Different roles and interaction model should be defined when holonic organisation is used for modelling. 3.1.1. Roles
Holons are composed of other holons which referred as super-holon and sub-holon, respectively. A super-holon can be seen as a set of interacting sub-holons. The interactions are required to achieve the goal of the super-holon. Three roles are defined in holonic organisation ( Rodriguez, 2005 ): head , part and multi-part .
The head is representative of the holon which composed of a set of holons and represents the shared intentions of the holon.
It negotiates these intention to other holons either its members or other holons. For the members two different roles are defined ( Rodriguez, 2005 ): The part role represents the members which belong to one super-holon; the multi-part role represents the member which belongs to more than one super-holons. Multi-part roles lead to overlapping holons which is beyond the scope of this paper. Hence, we consider two roles, head and part which are equivalent to super-holon and sub-holon, respectively.
The members of a super-holon interact and cooperate to achieve the goal of super-holons. As mentioned before, the behaviour of holons in higher levels is more complex and reactive behaviour is done at the bottom of the hierarchy. A super-holon is created to fulfil a task for specific goal. To achieve the goal, the members should interact with each other and coordinate their actions according to the policy of the super-holon. Super-holons make abstraction of interactions to concentrate on the behaviour of the members. Super-holons on the higher levels make more abstraction comparison with lower level super-holons. The abstraction level of a holon is generally defined by level  X  h  X  X 
According to Eq. (3) the abstract level of the first level agents is 0. A holon with recursion level n such that n 4 1 is made up of holons with abstract level k such that k o n . Depending on the complexity of the system, designers can use different abstract levels with more or less abstraction. 3.1.2. Interaction model
Holonic organisation includes a set of holons arranged in different levels. Distributed holons try to achieve a common goal through cooperation and coordination. Two types of interactions can be seen in a holonic organisation as shown in Fig. 2 ( Rodriguez, 2005 ):
Intra-level interaction: Agents interact with each other agents at the same level. This type of interaction sometimes referred to as horizontal interaction.

Inter-level interaction: Agents of two different levels in the hierarchy interact with each other. It sometimes referred to as vertical interaction.

Intra-level interaction has been investigated in multi-agent system research. A holonic or any other intelligent agent-based framework which is entirely based on intra-level negotiation or interaction is likely to be inadequate. In practice, in addition to the intra-level interaction among the agents, there are vital interactions that take place between agents in different levels. For example, in any multi-level hierarchical multi-agent system, inter-level interactions play an important role.

Although a number of mechanisms are introduced in order to handle inter-level interaction, there is no general mechanism to transfer information between different levels of abstraction ( Rodriguez, 2005 ). The approaches in this area usually propose their own mechanism for information transfer.

In this context, interactions between agents are implemented through asynchronous message exchange in two opposite direc-tions: upward to transfer data towards higher level holons and downward in the opposite direction. A message is composed of four components / from , to , type , value S : from: holonID (sender identification), to: holonID (receiver identification), type: i , i  X  0 , 1 , 2 (0: abstract state, 1: abstract action, 2: reward), content: value (state index, action index, reward value). Three types of messages are used in the proposed system.
Depending on the direction, the messages contain abstract state, abstract action or reward. 3.2. The proposed learning method
Developing a distributed cooperation and coordination mechanisms for agents with a partial view of the whole system is challenging for complex systems. Agents need to estimate the unobserved states of the system and adapt their actions to the dynamics of the environment. These problems have caused that using classical learning methods is hard or even impossible for large scale domains. In order to handle these problems, some heuristics have been used in the literatures. Some of them tried to reduce the policy search space ( Makar et al., 2001 ) while the others used some heuristics to guide the policy search ( Zhang et al., 2009 ).

Holonic organisation provides a mechanism for abstract data transfer in multi-agent systems that makes it possible to use learning methods in complex systems. Abstract data usually contains useful information that the entities need them for their operation in the environment.

Classic Q-learning usually has three main steps as mentioned before: state estimation, action selection and reward calculation.
However, the holonic Q-learning contains the same steps, the transferred abstract data contribute to different steps. We define three abstract data types that transfer between holons in different levels.

According to characteristics of HMAS defined by Gerber et al. (1999) , the agents of level n are composed of agents of level n 1.
The holons in lower level are reactive holons while the higher level holons are proactive according to the rules defined by
Koestler (as mentioned before). The complex activities and behaviour are related to the top of the holarchy and the lower level holons act directly in the environment. According to the directions shown in Fig. 1 , the data stream for the main steps of the learning is as follows: Transferred messages are graphically shown by numbered arrow.
The numbers show the order of messages considering one holon in each level. Let a 1 , b 1 , g 1 and o 1 are the marked holons such that: level  X  a 1  X  X  0, level  X  b 1  X  X  1, level  X  g 1  X  X  2 and level  X 
Arrows 1 X 3 shows the abstract state stream which starts from a up to o 1 . Abstract action stream is shown by arrows 4 X 6 and the reward stream corresponds to arrows 7 X 9 start from o 1 down to a 1 .
 four components / from , to , type , value S which are given in Table 1 .
Assume that we have a holarchy that contains k holons, h i 1 , ... , k in d holarchical level, according to Eq. (3) we have: min  X  level  X  h i  X  X  X  0 ; max  X  level  X  h i  X  X  X  d 1  X  4  X 
The holons keep the sub-holon role, super-holon role, or both of them at the same time, depending on their abstract level. In other words, the holon keep only the sub-holon role if level  X  h i  X  X  0; keep only the super-holon role if level  X  h and keep the both roles if 0 o level  X  h i  X  o d 1.

The learning steps for sub-holon and super-holon are described separately in Table 2 . If a holon keeps both roles in a holarchy, the learning iteration includes both sequence together.
If a holon keeps both roles in a holarchy, i.e. sub-holon and super-holon, the learning iteration includes both sequence together.

The learning iteration starts from atomic agents (the holons in level 0). They perceive the environment, estimate their state and send abstract state information to their super-holons. The super-holons, the holons in level 1, estimates the state using the information received from sub-holons and their percepts. They also send abstract state information to their super-holons, the holons in level 2. This process continues from the lowest level to highest level. These steps form the abstract state stream for state estimation. The action selection process starts from the holons in the highest level. The highest level holons select an action and send the selected action to their members. The members select an action using their policy considering the abstract action received from their super-holon.

They also send their selected action as an abstract action to their members. This process continues from the highest level to lowest level and form the abstract action stream for action selection.

Since the higher level holons have a broader view of the system, they get the reward to update the policy and also determine the efficiency of the members. They assign a reward value for their members and send it to them. This process continue down to the lowest level and form the reward stream .
Each holon updates its own policy using the two reward values, the value received directly from the environment and the value received from the super-holon. 4. Traffic signals control
The traffic network is a complex system ( Bazzan, 2007 ) composed of many different entities interacting with each other, whose actions are highly coupled. The traffic network is consid-ered as a system composed of intelligent agents that each agent controls an intersection. In particular, traffic signals controllers located at intersections can be seen as autonomous agents. Since the number of agents is high in real urban network, holonic organisation is useful for modelling the traffic signals controllers.
The network used for simulation is shown in Fig. 4 . It contains 50 four-way intersections and more than 100 links. We used four-way intersections with three lanes in each link.

Organisation of a HMAS is formation of different holons and makes different levels in the holarchy. Ideally, this process carried out dynamically during the time. Dynamic organisation in HMAS includes creation and integration of new members, and self-organisation ( Rodriguez et al., 2006 ). Since dynamic holons lead to challenges regarding the curse of instability which depends on the application, a fixed formation and grouping of the sub-holons is assumed according to the problem in most applications. In this paper, we use a holon formation method that is described in Abdoos et al. (2012) . Since the formation of holons in the holarchy is out of the scope of this study, readers are referred to Abdoos et al. (2012) for more details. 4.1. Network configuration
A holonic organisation composed of two layers is used to model the network. In this model, each holon controls an inter-section in the first level while a holon in the second level controls a region of network which contains a number of intersections. The proposed method can be easily extended to three levels if we have a policy to control a number of regions. Developing a control strategy is necessary to extend the number of levels. The model is extendible even for more levels, if we can develop a proper control strategy for each level. Since interactions between neigh-bouring intersections are only considered in this model, our model consists of two levels. Fifty agents are used for the first level (one for each intersection) which are arranged in 13 holons as it can be seen in Fig. 4 . The holons are determined according to a graph-based algorithm that is described in Abdoos et al. (2012) .
The intersections and links are considered as nodes and edges of the graph, respectively. Hence, the network is modelled using a weighted graph which the weights show the dependency between every two intersections. The dependency is computed according to the distance between them and the traffic rate flowed through the links that connect them. The algorithm finds the holons greedily by maximising a quality measure which is defined for a traffic network. The quality measure computes the sum of average dependencies over the number of intersections inside the holons to guarantee that more dependent agents become members of one holon ( Abdoos et al., 2012 ).

We discuss the learning process of holons in two levels separately. The holons in the first level and the second level are called sub-holons and super-holons, respectively. 4.2. First level
The traffic network is considered as a system composed of 50 agents that each agent controls an intersection in the first level.
For these agents we use the parametric Q-learning method presented in Abdoos et al. (2011) . In each learning iteration, the agents perceive the environment and receive statistical data in a fixed cycle. Since all the intersections are 4-way crossroad, the system contains homogeneous agents. The signals groups are also shown in Fig. 5 . A signals group is assigned to each approaching link. Therefore, each agent controls a signalised intersection which has four phases. Each agent controls a signalised intersec-tion which has four phases, each assigned to an approaching link.
A simple way to determine the states is to consider the average queue length in the approaching links. The state space is dis-cretised by ranking the approaches according to the statistic gathered in the last cycle of time. The agents use the average queue length of approaching links as statistical data in a fixed cycle for state estimation. Let l i is the i th approaching link for an intersection, then l 1 Z l 2 Z l 4 Z l 3 shows a state in which the average queue length of approaching link l 1 is the longest and l is the shortest. If two approaching links have the same average queue length, they are ranked according to their order in the signal plan. The number of states is 24 which is the number of the permutations of the four lanes, l 1 , l 2 , l 3 and l 4 (as shown in Table 3 ).

The action space determines a plan for agents in such a way that each approaching link has an individual portion of green time in a fixed order. In the experiments, 2 s considered for yellow time after the green time assigned to four signal groups. The cycle adjusted to 100 s, and four 2-s have been assigned to yellow time.
The total green time splits to four time interval, each assigned for a signal group as shown in Table 4 . For example, action 12 is graphically shown in Fig. 6 . Readers are referred to Abdoos et al. (2011) for more details.

Moreover, we use E -greedy as a mechanism to select an action in Q-learning. This means that the best action is selected for a proportion 1 E of the trials, and a random action is selected (with uniform probability) for a proportion E .

In this configuration, the number of signal groups is four as it can be seen in Fig. 5 . The actions are defined by assigning different time split to the available signal groups in a fixed order. 4.3. Second level in Abdoos et al. (2012) . A number of neighbouring intersections forms a region which is controlled by a holon in the second level.
Since all holon in the second level contains a set of first level holons, let us call them as super-holon. Since super-holon has a broader view of the environment in comparison to its members, it can estimate the state of the corresponding holon using more global information.

 X  and each member has four approaching links: l 1 i , j , l shown in Fig. 5 . vehicles in all links inside the holon. Consider Z is the maximum density value of approaching links. The state of a i is estimated using Z , as it shown in Table 5 .

Z  X  max
A super-holon plays its role in the corresponding holon by restricting the action space of the members. The super-holon tries to decrease the traffic jam in the approaching link which has the maximum density of vehicles. To this end, it affects members that are directly connected to that link.

Assume that l x i , j is the link which has the maximum density among all links inside the holon a i . Furthermore, assume that a another member of a i which is directly connected to a i , j (see Fig. 7 ). The action space of super-holon is defined such that it increases the output rate of l x i , j on one hand and decreases the input vehicles to enter to l x i , j by assigning specific time splits to signal groups. There are three possible di rections for each signal groups: l which connects a i , j to a i , p more than the other approaching links. Fig. 7 shows the two members for x  X  1and y  X  3.

According to this description, the action space for a super-holon, a i , is defined as follows: Action 1: No command.

Action 2: Change priority of a i , j such that the green time assigned to l x i , j is more or equal than the other links.
Action 3: Preemption of green time for l x i , j such that the shortest green time is assigned to the other phases and the remaining green time is assigned to l x i , j .
 priority to l x i , j and a i , p assigns a high priority to green time for l x i , j and a i , p changes the priority of priority.

For example, for x  X  1 and y  X  3, Table 6 shows the relation between the action selected by a i and the two involved members, they satisfy the conditions for green time assignment. Actions 4 and 5 are joint actions that involve two members. Obviously, the subset of actions are determined according to x and y and hence it definitely depends on x and y .
 4.4. Algorithm
The proposed Q-learning process contains the following steps: 1. Sub-holons in the first level receive information from the 2. The sub-holons send the density of vehicles in all approaching 3. The super-holons (the holons in the second level) estimate 4. Each super-holon selects an action and sends it to its 5. The sub-holons receive the abstract action and find the 6. The sub-holons select an action from the sub-set of actions 7. Each super-holon perceives the environment and calculate 8. The super-holons send the assigned reward value to their 9. The sub-holons calculate the overall reward by local reward 10. All holons in both levels update their own policy. The holonic Q-learning process for two levels is described in Algorithm 1 . The algorithm includes three main parts (that is determined in the body of the algorithm): state estimation, action selection and reward calculation.
 Algorithm 1. The proposed holonic Q-learning algorithm. for all h i A H do for all a i , j A h i do // state estimation end calculate Z  X  according to Eq :  X  5  X  X  estimate the state , s i  X  t  X  , using Z select an action , a i  X  t  X  for all a i , j A h i do // action selection end calculate the rewards , r i , j  X  t 1  X  and r i , j  X  t 1  X  for all a i , j A h i do // reward calculation end update Q-table , Q i end 4.5. Cost of interactions
According to the framework proposed by Gerber et al. (1999) , the right to interaction with other agents is an exclusive resource of the holon (not of its sub-holons). Hence, there is a data channel between two equally ranked holons which is managed only by the super-holon.

The constraints in holonic models lead to increase in the cost of interaction, especially between sub-holons of different holons.
It is not possible to compute the precise order of cost of interaction generally, because it highly depends on the structure characteristics such as the number of levels and holons, the number of members in each holon and allowed interactions between different entities in the holonic organisation. Let focus on a two-level holonic organisation. Assume that a and b are two sub-holons that belong to super-holons h 1 and h 2 , respectively. A message passing between a and b includes: (i) a sends a message to h ; (ii) h 1 sends the message to h 2 (if it is allowed); and (iii) h sends the message to b . Hence, the cost will be three times higher than a direct message passing.

For a fully connected network composed of n agents the order of cost of interaction increase from O  X  n 2  X  to O  X  n 3  X  in a two-level holarchy. Since we only consider the interactions between sub-holons and their super-holons, the cost order of interactions is
O ( n ) in the proposed model. 4.6. An example
Consider a part of the network managed by holon a 1 which
The average queue length and density value of the links in a fixed cycle are reported in Table 7 .

The state of the members is estimated as follows:
Each a 1 , j , j  X  1 , ... , 4 sends a message containing density value to its super-holon, a 1 , which is used to estimate state of the holon.
Since the maximum density is 0.226, the state of the holon is estimated as State 2 .
 Assume that action 4 is selected by a 1 . Action 4 is mapped to , j  X  1 , ... , 4 for the members as follows:  X f 2 , 4 , 6 , 13 , 14 , 15 g  X f 1 , 2 , 3 , 7 , 8 , 9 g  X  A 1 , 3  X f 1 , 2 , ... , 19 g  X  A 1 , 4  X f 1 , 2 , ... , 19 g whole set, i.e. A 1 , 3 and A 1 , 4 . 5. Experimental results contains 50 intersections and 112 two-way links as depicted in
Fig. 4 . Aimsun traffic simulator 1 has been used to create the topology of the network. The number of lanes per link is three for all links and the maximum speed allowed to vehicles is 50 km/h.
Furthermore, the vehicles are inserted to the network through 20 sources which lie on the border of the network. The system configuration parameters are shown in Table 8 . We used normal (Gaussian) distribution for vehicles arrival.
 parameters value, most of them have a common effect on the performance for different methods. For example, an increase in the maximum allowed speed decreases the delay value in simulation when different methods are used. Among different simulation parameters, the most effective one on delay time is traffic demand value. On the other hand, traffic demand usually varies in different hours of a day. A practical approach for traffic signals control, should be enabled to work in different traffic demands. Therefore, we only consider the impact of traffic demand values on average delay time during simulations.
Some experiments were performed in order to compare the proposed method in two modes: the individual Q-learning in the first level and the holonic Q-learning designed for two holarchical levels. Learning parameters values which are used in the simula-tion are given in Table 9 . A series of experiments was performed to evaluate delay time and flow of the network for the proposed approach. The experiments determine the simulated average delay time for different traffic demands ( Table 9 ).
Among different traffic demands we choose three values as low, medium and high traffic congestion (traffic demands 1 X 3 in
Table 8 ). In these experiments, the average delay time and flow are used for performance evaluation.

In the first experiments, there is no holonic control and first level agents run the classic Q-learning algorithm in an individual way. In the second experiments, the proposed holonic Q-learning is used to control the traffic signals. The average delay time and flow of the individual Q-learning and the holonic Q-learning for traffic demand 1 are shown in Figs. 9 and 10 . The reported delay values are the average computed over 10 runs.

Table 10 shows the average delay time and the average standard deviation of delay time computed over 10 simulations during 10 h. Furthermore, the average harmonic speed and standard deviation of the speed are also given.

As it can be seen from the figures, the performance is almost the same for the holonic Q-learning and individual Q-learning. According to the data reported in Table 10 , the average standard deviation of delay time of the holonic Q-learning method is less than the individual Q-learning method. A high standard deviation shows that the data is widely spread (less reliable) and a low standard deviation shows that the data are clustered closely around the mean (more reliable).

Figs. 11 and 12 depict the average delay time and flow of the network for traffic demand 2. The performance is good at the beginning because of the time necessary for the network to saturate. However, this changes during the time, the figures show a significant increase in performance of the holonic Q-learning. The network begins to saturate after some hours when individual Q-learning is used for signals control while the holonic Q-learning keeps the average delay time value around a fixed value. The figures also show that the holonic Q-learning controls the signals more reliable than the individual one. The average delay time values reported in Table 9 indicate that the holonic control of the network leads to less delay time and more flow rate while the standard deviation indicates the reliability of the holonic Q-learning.

The average delay time and flow of the network for traffic demand 3 are shown in Figs. 13 and 14 . Although the network begin to saturate after a few hours using the both methods, but the performance of the methods are different. When the indivi-dual Q-learning is used, the network starts to saturate before four hours and when the holonic Q-learning is used the network starts to saturate after six hours. Since the statistical data are not valid after saturation, it is not possible to calculate the average value and standard deviation for this traffic demand. It is worth noting that there are some situations where preventing from saturation is unavoidable. The holonic Q-learning is highly advantageous due to postponing the network saturation. 6. Conclusion and future works
Multi-agent system is usually used to model distributed complex real-world problems. An approach to cope with large-scale complex system is to organise agents and decompose the problem in some smaller sub-problems. Holonic multi-agent system provides such an organisation in which self-similar holons arranged in a multi-level holarchy. This paper uses a holonic multi-agent system to model a large number of signals controller agents. A holonic Q-learning method is applied to learn signals timing in two holarchical levels. A network containing fifty intersections decomposes to thirteen holons each assigned to a holon which stands on the second level.
 while the holons in the second level use a smaller space. The holons in both levels try to learn the optimal policy simulta-neously. Lower level holons send message to higher-level holons which is used to estimate the state of a part of the network.
Higher-level holons learns how restrict the action space for the members and lower-level holons learns which actin leads to the best future value among the set/subset of actions at each state.
During all the learning process each holon updates its own policy using the data that transfer between the holons in two levels. The results show that the holonic Q-learning stands out the individual Q-learning.
 proposed method and the analysis of the holonic Q-learning that will be published in future. How to organise the agents to form the holons in different levels is another issue that we are working on. We also wants to implement a learning method that deal with dynamic organisation of holons in different levels.
 ical levels in order to contribute more abstracted data that contain broader view of the system; to apply the proposed holonic Q-learning method in other real-world applications.
Furthermore, analysis of the impact of higher level data to speed up the convergence of the Q-learning is another open issue in this area.
 Acknowledgement
Information and Communication Technology  X  ITRC (Tehran, Iran) for their supports.
 References
