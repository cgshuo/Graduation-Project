 ORIGINAL PAPER David W. Embley  X  Matthew Hurst  X  Daniel Lopresti  X  George Nagy Abstract Tables are a ubiquitous form of communication. While everyone seems to know what a table is, a pre-cise, analytical definition of  X  X abularity X  remains elusive be-cause some bureaucratic forms, multicolumn text layouts, and schematic drawings share many characteristics of tables. There are significant differences between typeset tables, electronic files designed for display of tables, and tables in symbolic form intended for information retrieval. Most past research has addressed the extraction of low-level geomet-ric information from raster images of tables scanned from printed documents, although there is growing interest in the processing of tables in electronic form as well. Recent re-search on table composition and table analysis has improved our understanding of the distinction between the logical and physical structures of tables, and has led to improved for-malisms for modeling tables. This review, which is struc-tured in terms of generalized paradigms for table process-ing, indicates that progress on half-a-dozen specific research issues would open the door to using existing paper and elec-tronic tables for database update, tabular browsing, struc-tured information retrieval through graphical and audio in-terfaces, multimedia table editing, and platform-independent display.
 Keywords Document analysis  X  Table recognition  X  Ta b l e understanding 1 Introduction 1.1 Why tables? Tables are the prevalent means of representing and commu-nicating structured data. They may contain words, numbers, formulae, and even graphics. Developed originally in the days of printed or handwritten documents (indeed, tables may pre-date sentential text [ 47 ]), they have been adapted to word processors and page composition languages, and form the underlying paradigm for spreadsheets and rela-tional database systems.
 form of tables are calendars, rail and flight schedules, fi-nancial reports, experimental results, and grade reports. It is worth noting that the need to reformat and analyze the 1890 U.S. Census forms launched the punched-card  X  X abulator X  industry. Electronic computers were commissioned during WWII for computing ballistic tables. The major commercial applications envisioned for computers in the 1950s centered on database manipulation, which remains the mainstay of business data processing.
 a list. If we consider ordered lists analogous to vectors, then we can think of tables as analogous to matrices. Unlike vec-tors and matrices, lists and tables may contain non-numeric data items. Graphs are required for relationships more com-plex than can be represented by tables and are used primarily for inter -document structure. Trees are often used to repre-sent intra -document structure.
 ing only common sense: consider, for instance, the Periodic Table of the Elements, which requires substantial domain knowledge to understand (see Table 1 ). It is exceedingly easy to come up with other examples that are challenging even from a human perspective. Rather than belabor this point, for the purposes of this survey, we choose instead to focus on the kinds of tables that researchers have addressed with some degree of success.
 document image analysis [ 67 ]. A common objective of find-ing and delimiting tables, equations and illustrations, is to clear the path for optical character recognition (OCR) or, if the document is already in electronic form, for text analy-sis. Tables are between text and graphics with regard to the relative proportion of alphanumeric symbols, linear compo-nents and white space. If the text is sufficient for the purpose at hand, then all other document components can simply be either eliminated or preserved only in some image format. That is indeed adequate for many keyword-based document retrieval applications.
 sides in tables, then the tables themselves must be processed. There is little published research on OCR for tables. Current commercial OCR systems are hampered by the non-uniform spacing, multiple typefaces and sizes, rulings, and lack of language context found in tables. Without special provisions for tables, the OCR format preprocessor may simply attempt to decolumnize them. Some OCR products offer a zoning feature that marks and avoids table regions. More sophisti-cated systems attempt to transform tables, without further analysis, into the table format of the target representation (like Microsoft Word), but the results tend to be unreliable. None of these alternatives are satisfactory for table-rich doc-uments: hence this special issue. 1.2 Rationale for this review It appears likely that the automated or semi-automated in-terconversion of tables from one medium to another (e.g., from paper or electronic text to a spreadsheet, database, or query X  X nswer system), or from one format to another in the same medium (e.g., for different display sizes) will prove desirable in a variety of computing environments. In some applications, it may be advantageous to query and refer-ence tabular data without regard to the underlying medium or form.
 on table processing have appeared over the past several years [ 37 , 47 , 62 , 63 , 97 ]. The review by Lopresti and Nagy [ 62 ] aims at exploring the diversity and extent of the table world, and the many areas where further progress is needed to make the transition between traditional tables and digital presentation of structured information. A large col-lection of examples is included to illustrate the difficulty of both human and machine understanding of many tables. As an experiment, the entire survey was converted into tabular form for the version of the GREC proceedings later pub-lished as a separate book [ 63 ].
 tions on table recognition and forms recognition with accu-rate and detailed descriptions of many previously published algorithms.
 ences to much new material, organized according to a view of table recognition as the interaction of  X  X odels, observa-tions, transformations, and inferences. X  Work in the area is partitioned according to the methods used for classification (e.g., decision trees and neural networks) and segmentation (e.g., clustering and grammars). This paper also has a useful section on performance evaluation.
 bles are included in Hurst X  X  Ph.D. thesis [ 47 ] and a recent paper by Embley et al. [ 28 ]. At the time of writing the paper, another extensive bibliography, compiled by Price, could be found on-line. 1 people have actually done with tables but also what they would like to do with them, what they cannot do, and what and how they think about them. Our object is to collect in-formation about the composition, use, interpretation, and un-derstanding of tables that may prove useful in the develop-ment of tools for manipulating tables presented in a variety of media. 1.3 Guide to the remainder of this paper Our organizing principle is to attempt to orthogonalize the various issues, so as to be able to make independent deci-sions regarding algorithm development.
  X  X hat is a table? X  Rather than get hung up on the complexi-ties of what is certainly a deep and debatable issue, we take a pragmatic approach to our definition of tabularity, informed largely by what researchers have already been able to ac-complish in the area of table understanding.
 consider, we then proceed to describe half-a-dozen appli-cations that would result from new developments in table processing. This is followed by a brief overview of existing commercial approaches to the problems of table and forms processing.
 tronic X  and  X  X aper. X  The former can be further subdivided into plain ASCII and page-descriptor representations. Elec-tronic tables such as those found in word processing docu-ments, e-mail, Portable Document Format (PDF) files, and the Web, already have the content of the leaf cells in sym-bolic form, so OCR is not necessary, but the structure is sel-dom available in a convenient form. Tables on paper must be optically scanned for any type of automated processing. for table processing . Here we outline the major steps as a series of generic tasks to be performed. Most, but not all, past work can be cast in this framework, which provides a foundation for describing and discussing research results to this point in time.
 rections. 2Tables 2.1 What is a table? Although many consider the idea of a table to be simple, careful study (e.g., [ 63 ]) reveals that the question  X  X hat constitutes a table? X  is indeed difficult to answer. Several researchers have provided definitions. Peterman et al. [ 71 ], for example, state that  X  X ables have a regular repetitive structure along one axis so that the data type is determined either by the horizontal or vertical indices. X  These defini-tions, however intuitive, do not provide a theoretical basis from which to work.
 [ 21 ] do provide a theoretical basis for tables. Axiomatically, relations in a relational database can be considered to be ta-bles in a canonical form. Using a standard, formal definition of a relational table [ 66 , 84 ] shows how to define a canonical table as follows. A schema for a canonical table is a finite set L ={ L ply called labels . Corresponding to each label L i ,1  X  i is a set D i , called the domain of L i .Let D = D 1  X  X  X  X  X  A canonical table T is a set of functions T ={ t 1 ,..., t from L to D with the restriction that for each function t t ( L i )  X  D i ,1  X  i  X  n .
 tables in two dimensions. When we display a table two di-mensionally, we fix the order of the labels in the schema for each function and factor these labels to the top as column headers. Each row in the table constitutes the domain values for the corresponding labels in the column headers. Thus, for example, we can display the canonical table: {{(LAST NAME, Smith), (INITIAL, J), {(LAST NAME, Barr), (INITIAL, K), as Table 2 shows. Displayed in this form, a canonical table is simply called a table . Whether any format in which this same information may be displayed (e.g., as the set of sets illustrated earlier) should be called a  X  X able X  may be debat-able. To avoid the argument, whenever there may be doubt, we can refer to the information as table-equivalent data [ 84 ]. Displayed in its usual way as depicted in Table 2 , this infor-mation would certainly be called a table.
 mally investigate the boundary conditions constituting de-generate table-equivalent data. When there is only one col-umn, the table is more commonly called a list . When there are no domain-value rows, we may think of the empty ta-ble as a form with slots to be filled in. When there is only one row, we may think of the table as a filled-in form .Ifa label is missing (e.g., if either LAST NAME or INITIAL is missing), we may think of the label as being implicit. Com-mon sense (e.g., the names look like names and the initials look like initials) and context (e.g., BIRTH DATE usually implies people with names) allow us to reconstruct missing labels, or at least synonymously equivalent missing labels. If all labels are missing, self-identifying data may allow us to reconstruct all implicit labels. If all labels are missing and all domain values are numbers, we think of the table as a matrix . Further, if a matrix has only one row or one column, we think of it as a vector , as noted previously.
 2.2 What is table understanding? Another consequence of the formal definition for tables is that it leads directly to a formal definition of table under-standing. A table is understood if we can recover the set of labels L ={ L 1 ,..., L n } , the set of domains D ={ D 1 D maps L to D . Often, we use a less-inclusive definition that does not require us to identify D and the individual domains, D 1 ,..., D n , that constitute D . In this case, recovering the label X  X alue pairs for each function t i  X  T is sufficient. Thus, for example, Table 2 is understood if we can recover the set of functions: {{(LAST NAME, Smith), (INITIAL, J), {(LAST NAME, Barr), (INITIAL, K), not always agree on the sets of label X  X alue pairs for a ta-ble [ 41 ]. Thus, we should not be surprised that automating table understanding is difficult.
 tion does tell us exactly what we have to do to automate table understanding: we must recover the label X  X alue pairs from the representation of a given table. To formalize this pro-cess, we can adopt the ideas from [ 36 ], which proposes the use of an ontology for automated table understanding. Since  X  X n ontology is a formal, explicit specification of a shared conceptualization X  [ 34 ], a table understanding ontology for-mally and explicitly specifies a shared conceptualization of table understanding. Basically, the idea is to ontologically capture all the relevant representational knowledge about a table (the input ontology) and transform it algorithmically to sets of label X  X alue pairs (the output ontology).
 tology for canonical tables. Later in this paper, when we describe table-processing paradigms, we will show how to represent input tables ontologically and explain how the paradigms can all be thought of as transforming an input table captured ontologically into an output ontology such as the one in Fig. 1 . In the diagram, we use boxes to repre-sent object sets, solid boxes for abstract items represented by object identifiers, and dotted boxes for concrete items represented by value strings. Thus, for example, Table in Fig. 1 is a set of table identifiers representing the tables of interest, and Label is a set of labels such as LAST NAME or BIRTH DATE . Hyperedges connecting object sets represent relationship sets. N -ary edges have a diamond; binary edges do not have a diamond. Edges may be functional, denoted by an arrowhead on their range side. Thus, the relationship be-tween tuples and tables is functional: each tuple (identified by a tuple identifier) belongs to one and only one table. The absence of an arrowhead allows an object to participate with many other objects. Thus, in the n -ary relationship set, a tu-ple may have many label X  X alue pairs. Additional constraints may further restrict object-or relationship sets. Thus, we can force the conceptualization to correspond to the formal def-inition of a table, which requires a distinct and equal set of labels for every tuple belonging to a particular table. 2.3 Generalizing tables and table understanding A further consequence of the formal definition for tables is generalizations of tables and table understanding. Indeed, some researchers have offered generalizations [ 47 , 84 ]. One way we can formally extend the definition is by defining nested labels [ 47 ]. We can alter Table 2 , for example, by nesting LAST NAME and INITIAL under EMPLOYEE and BIRTH DATE under PENSION STATUS . For nested tables, we can use a nesting structure to describe the more complex attribute value pairs.
 tation of Table 2 using the notation of [ 47 ]. Categories are made up of minimal sequences of dependent cells. Thus, in the extended version of Table 2 ,asEMPLOYEEhasno discriminative power other than defining LAST NAME and INITIAL, it forms part of the values in that category. The reading set describes the subset of the cartesian product of the categories that the syntax of the table allows. A reading path, then, is a subset of the reading set with the specific data category removed.
 Categories : {EMPLOYEE.LAST NAME, EMPLOYEE.INITIAL}, Reading Set : {{EMPLOYEE.LAST NAME, Smith, dimensions are used to index a third data category. two dimensions are Position { First , Second , Third } and nucleotide { U , C , A , G } . Understanding the full label (or reading path) Second Position C as a complex object, not a simple string, is required if we are to interpret the table against a model of the domain. This table is described as follows: Categories : {First Position, Second Position, {Nucleotide.U, Nucleotide.C, {Phe, Ser, Tyr, Cys, Leu, Stop, Trp, Reading Set : {{First Position, Nucleotide.U, by defining collections of tables, in which case we have the equivalent of a relational database [ 66 ]. Further, we can re-verse engineer (e.g., [ 20 ]) a collection of tables into a con-ceptual model (e.g., the entity X  X elationship model [ 18 ]). In a similar vein, we can consider reverse engineering a single table or a group of related tables into an ontology and pop-ulate them as described in [ 84 ]. Using this technique, Fig. 2 shows the ontological representation for Table 2 that has LAST NAME and INITIAL nested under EMPLOYEE and BIRTH DATE nested under PENSION STATUS , and Fig. 3 shows the ontological representation for the nested genetic code in Table 3 .
 elements connected to its base into the aggregate connected to its apex, while the open triangles denote ISA relationships between specialization elements connected to their bases and generalization elements connected to their apexes. The symbol  X   X  in an open triangle denotes a partition among the specialization elements with respect to the generalization el-ement. The large black dots represent individual elements, thought of as singleton sets in the partition.
 two different ways in this discussion: (1) ontologies to rep-resent understood tables (Figs. 2 and 3 ) and (2) ontologies to represent input and output descriptions of table knowl-edge (Fig. 1 ). Although much too complex to depict and describe in this survey, [ 27 ] gives an ontology that for-mally describes ontologies that represent understood tables. In this sense, ontologies that represent output descriptions are meta-ontologies. Indeed, the output ontology in Fig. 1 is a meta-description of a canonical relational table. 2.4 Models of tables Not all table-processing research aims at table understand-ing. For example, some may just want to convert a scanned table into an editable Microsoft Excel or Word table that has no meaning except to a human. A substantial amount of ta-ble processing to date has not attempted to interpret tables; rather, recovering the grid and cell contents are considered the target. The researchers in question did formulate ade-quate models based on their intended goals and their views of what constitutes a table even though these models are largely insufficient for the task of table understanding. This variation is generally in line with the particular tasks addressed by the systems described or the particular philos-ophy of document encoding. Low-level models use line-art [ 31 ], white space [ 78 ], and character distributions [ 51 ]as key features to drive analysis. Grid-based models include [ 60 ]and[ 80 ].
 specific table. This is not only a syntactic constraint but also a semantic constraint X  X he model fixes the labels of the po-sitions in the data area of the table, thus obviating the need to interpret the labels either syntactically or semantically. The application context is one in which a known table (or small set of tables) is input many times and requires interpreta-tion [ 80 ].
 can be customized to specific families of recursively or it-eratively generated structured documents, including tables. Enhanced Position Formalism (EPF) is a 2-D grammar able to describe document layouts for sheet music, mathematical equations, and tables [ 24 , 25 ]. Although the examples pre-sented in this paper are forms rather than tables, it should be possible to compile EPF grammars for specific families of tables. The authors claim that the grammars can be readily combined, for example to recognize tables of mathematical equations. The method was applied to several thousand de-graded 19th century military documents with similar layout. authored tables. Green uses this analogy to refer to the printed table as printed manifestations of relational infor-mation [ 30 ]. He then continues by describing the complexi-ties of the relational model in terms of joining and merging multiple relations. This effectively appeals to the implicit se-mantics behind the join as an analogy to the complex cate-gorical structure present in all but the most trivial uniform grid table.
 framework is proposed for combining information from a domain ontology, and standard-unit ontology, and table metadata (possibly derived from surrounding context) into relational tables for populating a database [ 4 ]. It is not clear from the paper whether the SRD has been implemented. its the complexity of the tables, requiring a model of a suit-ably limited scope. Pyreddy and Croft [ 73 ] characterize ta-bles in a typed-line-manner due to the limitations of an ASCII representation.
 stract models, we can see the influence of table editing sys-tems. Wang X  X  is perhaps the most well-known model which captures both logical and physical aspects [ 88 ]. She de-scribes the Improv system [ 23 ] as being perhaps the first sys-tem which provided a clear separation of logical and physi-cal aspects.
 Hurst [ 47 ] develops a characterization of tables as document objects in context, recognizing the potential for surrounding text to impact the understanding of the table. If the related text informs the reader that  X  X he values in the second column are the median value, X  we read the data quite differently to the case where we have been told  X  X he values in the second column are the maxima. X  out mentioning research in the field of psycholinguistics. Wright [ 95 ] describes the understanding of the organiza-tional principles in tables, and Guthrie et al. [ 35 ] consider the nature of categories. 3 Applications of table processing In this section, we separate applications into as many dis-crete categories as possible. It may or may not be advan-tageous to develop a table-processing framework that can handle several of these applications in a unified way. 3.1 Large-volume, homogeneous table conversion An example of an application in this area is the work done at AT&amp;T/Lucent on the conversion of telephone billing state-ments to a usable form [ 80 ]. Although the tables may vary in format and content, all contain similar types of data that are compatible with an existing database. The database itself can be used to facilitate and validate data extraction from the tables [ 29 ]. This application is very similar to forms pro-cessing and could probably make use of advanced existing commercial software developed for this purpose.
 the importance of a well-designed graphical user interface (GUI) to allow customization of the table-processing tools for specific formats. The use of table templates eliminates the need for elaborate structure hypotheses, and the success of the approach depends mainly on thorough preprocessing and accurate OCR. 3.2 Large-volume, mixed table conversion This is a preliminary step for data mining from sources that are available only as paper or electronic tables. This applica-tion may require table spotting and table-similarity detection in addition to content and structure extraction.
 could be used to facilitate what is regarded as traditional in-formation retrieval. The answers to certain kinds of queries seem most naturally expressed in tabular form. Consider, for example, the following ad hoc topic (#219) from the TREC 4 evaluation [ 85 ]: a table comparing auto imports/exports over time or by country. 3.3 Individual database creation This is a filing application for data that arrives in e-mail, by post, or is discovered on the Web [ 94 ]. The individual sets up some goal-oriented digital filing system and populates it with items that arrive at unpredictable times. The tables are processed either as they arrive, or batched for more con-venient interactive processing. An important consideration here is minimization of the original set-up time and level of skill required. 3.4 Tabular browsing Interactively extracting specific information from a large ta-ble is somewhat similar to addressing queries to a database with a language like SQL. Wang gives examples where the results of a query consist of highlighting specific cells in a table. She also mentions the possibility of creating subtables in response to a query, which is similar to view generation in a database [ 88 ]. 3.5 Audio access to tables In the EMU project [ 81 ], it may be desirable to detect and access newly received tables in e-mail by telephone. Access may take the form of an abbreviated reading or summariza-tion of the table, a query X  X nswer interface directly to the table, or conversion of the table to a database and access through an existing audio-database interface (if one were to exist). A protocol for direct access to tables was devised for  X  X alking books X  for the blind [ 76 ]. It requires repeating the appropriate table heading before each content cell is voiced, which can be a slow and painful process. 3.6 Table manipulation Existing tables often need to be reformatted, combined, or modified for specific target audiences. Such manipulation may take place at the level of format, using a word pro-cessor, page-composition language, or spreadsheet, or at the deeper level of the underlying database. The latter can use independently-developed facilities for view generation and database output formatting. This application is mentioned in [ 48 , 88 ]. 3.7 Table modification for display Retargeting Web page displays for small-screen devices like personal digital assistants (PDAs) and cell phones has as-sumed increased urgency and importance with the deploy-ment of fast wireless connectivity. A recent review [ 2 ] lists four alternative techniques: hand recoding, trans-coding (au-tomatic replacement of HTML tags by device-and target-specific tags), re-authoring based on automatic layout anal-ysis, and re-authoring based on natural language processing (NLP). Re-authored pages can be presented hierarchically, with a root node consisting of a table-of-contents with links to detailed content. Although no table-specific techniques are given, some of the methods we describe are referenced. structed with the HTML &lt;table&gt; construct (just as fig-ures in Microsoft Word are often laid out using its table fa-cility). The real problem with layout analysis on Web pages is that everything floats. The geometry is not fixed until the page is displayed by a particular browser, with specific set-tings and window size. Nevertheless, HTML preserves some relative ordering. This is exploited in [ 3 ] to re-author an HTML list. Further suggestions for generalizing the notions of precedence, proximity, prominence, and preference for in-terpreting content flow in HTML documents are presented in [ 2 ]. As seen in the following discussion, PDF documents share this problem of lack of association between content and form, therefore some of the same techniques may be useful for retargeting them to different formats.
 such as a PDA, one may wish to modify a page-width ta-ble to single-column width. Additional headers must be in-serted to divide long tables to fit pages. A research issue here that may draw on database concepts is the division of one or more tables into a set of equivalent tables ( cf.  X  X arge Ta-bles X  in [ 88 ]).
 uments is evanescent compared to the conversion of paper documents because XML-based schemes are conceived with the goal of assuring machine interpretability [ 75 ]. 3.8 Information extraction from tables Information extraction from tables is perhaps analogous to the task of the same name applied to sentential text. The nar-row definition requires a target schema and requires that ar-bitrary input (generally of some standard encoding) be trans-formed into instances of the schema. Examples of systems that fit this definition include [ 22 , 28 , 31 , 60 , 80 ]. Each of these systems works with varying definitions of tables and varying data formats. 3 ogous to message understanding and represents perhaps the ultimate goal of table understanding.
 the state of the art for this task. Part of the challenge is to provide a standard data set against which systems may be tested and results compared.
 lutions uses the schema to drive the segmentation of the doc-ument and the recognition and interpretation of the tables. Knowing that one is looking for a financial table of a certain sort, and the form of likely labels and values, is invaluable knowledge even at the OCR and text blocking stage. 3.9 Ontology learning from tables Ontology learning (e.g., [ 65 ]) has recently received consid-erable attention because of the emergence of the Seman-tic Web. The Semantic Web requires an abundance of on-tologies, and creating them by hand is seen as a barrier preventing widespread use of the Semantic Web. In an at-tempt to break through this barrier, researchers have begun to build systems to  X  X earn X  ontologies from existing docu-ments. Learning ontologies from sentential text, however, has proven to be difficult. Learning ontologies from tables may be more fruitful.
 the consolidation of information from multiple tables (usu-ally downloaded from the Web) to generate domain-specific ontologies. The TANGO project [ 84 ] is an initial effort to use table analysis for generating ontologies. At least par-tially automating the preparation of such bodies of factual information may help pave the way towards a realization of the Semantic Web. 4 The commercial landscape The majority of current table applications, as described in this paper, can be found in academic and other research con-texts. However, like any advanced technology, a number of commercial systems are now available that either offer direct table-processing capabilities, or which rely to some extent on table understanding technology.
 Page [ 79 ], provide table location and segmentation fea-tures. These are generally targeted at explicitly gridded ta-bles (with some packages permitting user-guided analysis of non-gridded tables). Although the location of tables in such systems is generally adequate, market forces are such that the appearance of high-quality table segmentation features for arbitrary document input is unlikely.
 services, such as XML Cities [ 96 ], recognize the importance of capturing table data X  X s well as the need to index this data appropriately. The work-flow around these services permits the creation of new matching rules, as well as the validation and correction of conversion by a human operator, thus pro-viding the required quality level demanded by the customer. derstanding can be customized by domain to include con-straints that enable high-quality results with almost complete automation, the medical insurance domain is perhaps one of the most successful. Insiders Information Management GmbH [ 49 ] and TCG Informatik AG [ 83 ], for example, both adopt this approach.
 not generally available. In the low-end OCR market, the input is so varied that claims X  X f available X  X ould be hard to interpret. Where the work-flow involves a human, the quality is generally controlled according to individ-ual customer needs through customization and/or validation processes.
 beled boxes used for information collection: the items spec-ified by the labels are written or typed into the boxes, then the form is returned to the originator and the relevant in-formation is extracted. Common examples of forms are tax returns and catalog order forms. The advent of graphic print-ers allowed printing forms on demand: forms intended for the same purpose became diversified. The rulings and boxes lost their importance. In document analysis, the distinction between forms, invoices, and business letters is fading. cations, such as medical claims processing, state income tax, insurance, and retirement systems require conversion of sev-eral hundred thousand forms per day. In many such appli-cations, most forms are filled out by hand. The similarities between table and form processing are emphasized in [ 87 ] and [ 13 ]. Other notable work on forms includes [ 5 ]and[ 69 ]. Continuing efforts to pass processing costs down to the end users will cause many of these mass form-processing ap-plications to be migrated to the Web. Electronic forms are based on HTML, JAVA, Active-X, or XML.
 vironments are described in the research literature. An ex-ception is smartFIX, which evolved from 10 years of re-search at the German Artificial Intelligence Research Cen-ter (DFKI), and is now used by a dozen medical insurance companies to process tens of thousands of bills daily [54 X  56]. The system is able to classify about 60 types of docu-ments (hospital bills, prescription drug bills, dentist X  X  bills), and extracts over 100 different types of information from them (about 20 items per document on average). It relies on large databases of customers, products, and price sched-ules, and has elaborate models of the each customer X  X  infor-mation flow, accuracy requirements, audit practices, train-ing schedules, and distributed computational resources. Al-though constraint satisfaction methods are incorporated, ev-ery extracted field is subject to human verification. About 75% of the fields are labeled  X  X afe, X  with less than 1 error per 1000. The major source of inadequately processed fields is OCR error. It is reported that the system saves 65 X 75% time over conventional manual data entry.
 but some service bureaus do offer conversion of printed ta-bles to electronic form. 5 Input media and formats We consider tables that are presented in two different media: electronic and paper. We further subdivide the former based on encoding schemes. The net result is three (broad) classes of input tables: 1. ASCII file with only  X  X ure X  linguistic content and 2. Page-descriptor file (Word, L A T E X, HTML, PostScript, 3. Bitmap file of an image of a table with white space 5.1 Tables presented in electronic format Tables in plain text format may appear in e-mail or on certain kinds of Web pages. The structure of the table is represented only by ASCII symbols for space (blanks), tab characters, and carriage returns. Occasionally, printable ASCII symbols are used to show horizontal and vertical rules.
 smaller and simpler than paper tables. The amount of de-tail that can be displayed on a typical monitor is less than one-tenth of what can be seen on a typeset page.
 special conventions for tables, but there is no assurance that table tags are not abused or misused. Page composition lan-guages have elaborate facilities for formatting tables, like TROFF Tbl [ 61 ]andtheL A T E X table and array environments [ 59 ]. Many other table composition systems are surveyed in [ 88 ].
 provides interconversion between tables in plain-text, Word-table, Rich Text Format (RTF), and Excel spreadsheets. FrameMaker offers PDF for posting tables on the Web in non-editable form, and XML for applications where the structure needs to be accessible. VXML is a proposed general-purpose format for audio access to Web documents. mat, such as TIF or GIF, or rendered directly in PostScript [ 74 ]. Although directly-generated tables in image format may look superficially like scanned paper tables, they are not affected by noise or skew.
 widely used formats for document representation. PDF files can be readily transformed to and from PostScript, and are relatively small due to embedded compression. PDF can be used for both computer-generated documents (conversion options are built into many word processing systems) and for scanned pixel maps in black-and-white, gray scale, or color. It also has facilities for searching, indexing, annotation and limited editing, but does not encode document structure be-low the page level: the file is simply a list of low-level ob-jects like groups of characters, curves, and blobs, with asso-ciated style attributes like font, color, and shape. While there are several on-going research projects on recovering logical structure from PDF documents, we have found no research specifically on PDF table recognition. rendering instructions: (1) control instructions produce no output; (2) text instructions render glyphs of symbols; (3) graphics instructions render line art; (4) image instructions map bitmapped images [ 6 ]. It is therefore possible to apply directly the methods developed for hard-copy table recog-nition, but this requires error-prone image processing and OCR, the results of which are already explicitly and unam-biguously provided in the PDF representation of computer-generated documents.
 als into an indexed database of training material. The manu-als are annotated according to a domain ontology. An impor-tant step is the extraction of logical structure from PDF files. This is accomplished by assigning logical functions (section header, text paragraph) to each layout object and refining the assignment as more evidence (bullets, boldface) becomes available. A shallow grammar is implemented for recogniz-ing each function: tables are recognized as a proximate set of  X  X loating X  text [ 6 ]. The approach is based on the notion that layout objects do not explicitly represent logical struc-ture but contain cues about their role in the structure [ 82 ]. to reuse the layouts of existing PDF documents as templates for creating new pages. This necessitates the identification of logical components and the extraction of the content of each component. The procedure first separates into text, image, and vector graphics layers. Compound objects are reduced to simple objects. Each component block is represented as a polygonal outline, a set of style attributes, and content. Text word, line, and segment (paragraph block) analysis is per-formed on the text layer, taking into account style attributes such as type size and italics. The contents are transformed to XML format. Bitmap analysis of the graphics layer was, perhaps surprisingly, found easier than performing segmen-tation following drawing paths. The segmented graphic ob-jects are eventually converted to SVG format. Based on the analysis of the 18 page-segmentation errors that arose in pro-cessing 200 test pages, the development of specific table and map recognition modules is suggested to reduce errors fur-ther. It is clear that the combination of the current text layer and vector graphics layer analysis provides the necessary foundations [ 16 , 17 ]. Among references that address elec-tronic tables are [ 26 , 48 , 71 , 73 ]. 5.2 Tables presented on paper Paper tables are usually typeset, typewritten, or computer-generated. In principle, they can also be hand-printed or drafted (like telephone-company drawings [7 X 11, 15, 19], and the header-blocks of old engineering drawings), but we deem such hand-drawn tables as more akin to forms and ex-clude them from consideration here.
 scanning. Printed tables are typically scanned at sampling rates of 200 X 600 dpi, but for some applications facsimile scans (100  X  200 dpi) may be important. High-speed du-plex scanners have a throughput of 100 pages per minute at 300 dpi and 24-bit color depth. Bilevel scanners, which are suitable for most tables, are even faster.
 Both of these are more effectively corrected on a gray-level representation of the page. Image-reparation software is available from many vendors, including Lead Technolo-gies, Mitek, Visual Image, Cardiff, and Captiva. The major-ity of the published work on table processing deals with the extraction of structure from scanned paper tables [ 1 , 10 , 15 , 32 , 39 , 50 , 60 , 87 , 93 , 98 ]. 5.3 Table detection Conceptually, table processing can be broken into two logi-cal steps: table detection and table recognition. Much exist-ing work on tables described in the literature addresses the latter step and assumes that the table has already been iden-tified and segmented out from the input (or that identifying the table is trivial X  X .g., the whole document is the table). While this is, in fact, the focus of our survey, we digress briefly to consider the table detection problem.
 concentrated on detecting tables in scanned images, and the vast majority depends on the presence of at least some ruling lines (e.g., [ 60 ]). Hirayama [ 39 ] uses ruling lines as initial evidence of a table or figure and then further refines this de-cision to distinguish tables from figures by a measure based on such features as the presence of characters. There is, of course, no guarantee that such lines will be present in printed tables. Notable exceptions to this assumption include work by Rahgozar and Cooperman [ 74 ], where a system based on graph-rewriting is described and work done by Shamalian et al. [ 80 ] in which a system based on predefined layout struc-tures in given.
 bles, though they are becoming increasingly important. As noted earlier, these may originate either in ASCII form (e.g., as part of an e-mail message), or as the result of saving a  X  X icher X  document (e.g., an HTML page) in  X  X ext-only X  for-mat. They may also be encoded in a page-descriptor lan-guage such as PDF or PostScript, or in an electronic file format such as the one used by Microsoft Word. More of-ten than not, ASCII tables contain no ruling lines whatso-ever, depending only on the 2-D layout of the cell contents to convey the table X  X  structure. Little of the past research on printed tables is applicable in such cases.
 that does not rely on ruling lines and has the desirable prop-erty that an identical high-level approach can be applied to tables expressed as ASCII text (or any other symbolic for-mat) and those in image form. This general framework is based on computing an optimal partitioning of a page col-umn into some number of tables. A dynamic programming algorithm is presented to solve the resulting optimization problem.
 to detect tables in incoming documents are discussed by Klein et al. [ 57 ]. The first, based on searching OCR results for predefined table headers, was found to be too susceptible to a variety of real-world complications and hence unaccept-able from a user standpoint. More sophisticated techniques based on detecting column structure and inter-textline simi-larities proved to be more robust.
 approach for locating and extracting tables based on condi-tional random fields. Applied to plain-text government sta-tistical reports, they report a detection accuracy of 92%. 5.4 Simplifying assumptions We note that in order to focus on a core set of issues, we have been forced to omit numerous important prob-lems relating to the processing of tables, including plausi-ble sources of tables, table similarity detection, and human X  machine interfaces (graphical and spoken) to tabular data. For these, we refer the reader to the previously mentioned surveys [ 62 , 63 , 97 ].
 consideration the following concerns. 1. Information external to the table proper, including ti-2. Tables outside our restrictive definition, including folded 3. Expandable (clickable) Web tables and Web tables em-4. Multidimensional data arrays ( D &gt; 2). 5. Tables that may or may not be tables, including matrices; 6 Table-processing paradigms Tables may be encoded in many different input formats. However, in this section we take the view that a table is a table if and only if it appears as such when presented in its intended visual form to the end user. Hence, the con-cept of a 2-D rendering is central to our discussion of table-processing paradigms.
 text are so self-evident that it is easy to forget that they are still based on a set of underlying assumptions (e.g., what is connoted by a carriage return and, in most cases, that the ren-dering will use a monospaced font). While other encoding schemes such as PostScript and HTML have the potential to be much more complex, the simple fact is that such docu-ments are rendered all the time, and developing systems to perform this function is not considered a particularly daunt-ing task. The former are known as PostScript interpreters (e.g., Ghostscript), while the latter are referred to as Web browsers (e.g., Mozilla).
 generality, from a pragmatic standpoint, the vast majority of table-processing research to date has focused on two spe-cific classes of inputs. Tables encoded in ASCII format are a canonical instance of rendering on a coarse (i.e., character-level) grid, while scanned bitmap tables are a canonical in-stance of rendering on a fine (i.e., pixel-level) grid. Hence, these are the concrete examples we turn to most frequently in the following exposition.
 row and column numbers. Logically, their output is a list: (i,j) cell-content , etc. Top X  X own methods recover the underlying grid structure, then find the content of each cell. Bottom X  X p methods first delimit cell contents, then construct the grid.
 list. These paradigms associate cell contents with row and/or column headers. If row and column headers are absent, vir-tual headers are assigned. This requires some renumbering. The most complex algorithms target nested headers.
 information from the output of the earlier paradigms, i.e., row and column numbered headers and cell contents. Its out-put is suitable for downstream applications like SQL, PRO-LOG, XML or other logic-based schemata. While this is of increasing interest, especially arising out of the Semantic Web, there has been more work on the earlier aspects. 6.1 Simple tables In the simplest case, it is possible to determine the cell struc-ture of the table using purely geometric cues from the 2-D rendering. If it is known that the maximum intra-cell hori-zontal spacing is strictly less than the minimum inter-column horizontal spacing, and that the maximum intra-cell vertical spacing is strictly less than the minimum inter-row vertical spacing, the table can be parsed into columns and rows by using these parameters to determine whether a given  X  X ap X  represents a continuation of the current cell or the start of a new cell.
 dently of the input format of the table because it is defined in terms of the intended 2-D rendering of the tabular infor-mation. All we need is an understanding of the way the file is to be rendered, a way to identify the basic  X  X nit X  in the input under study (i.e., character strings in the case of ASCII and connected components in the case of bitmaps), and a way to measure distances between these basic units.
 many tables, but the notion of thresholds that allow merging intra-cell constituents without merging the contents of sepa-rate cells is subsumed in many of the paradigms mentioned later.
 strings delimited by column separators (e.g., consecutive spaces) and row separators (e.g., carriage returns). For example, we might have the following input, where spaces are indicated by a dash symbol, -, and carriage returns are indicated by a new paragraph symbol,  X  : LASTNAME----INITIAL----BIRTHDATE X Smith-------J----------12/3/1988 X Barr--------K----------25/5/1975 X  which would be rendered (on a coarse grid) as: LASTNAME INITIAL BIRTHDATE Smith J 12/3/1988 Barr K 25/5/1975 represented, thus: /Courier-New findfont 8 scalefont setfont 0 100 moveto (LASTNAME) show 60 100 moveto (INITIAL) show 120 100 moveto (BIRTHDATE) show 0 90 moveto (Smith) show 60 90 moveto (J) show 120 90 moveto(12/3/1988) show 0 80 moveto (Barr) show 60 80 moveto (K) show 120 80 moveto (25/5/1975) show showpage with a rendering (on a fine grid) like this: LASTNAME INITIAL BIRTHDATE Smith J 12/3/1988 Barr K 25/5/1975 &lt;html&gt;&lt;body&gt;&lt;table cellpadding="5"&gt; &lt;tr&gt;&lt;td&gt;LASTNAME&lt;/td&gt; &lt;td&gt;INITIAL&lt;/td&gt; &lt;td&gt;BIRTHDATE&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Smith&lt;/td&gt; &lt;td&gt;J&lt;/td&gt; &lt;td&gt;12/3/1988&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;Barr&lt;/td&gt; &lt;td&gt;K&lt;/td&gt; &lt;td&gt;25/5/1975&lt;/td&gt;&lt;/tr&gt; &lt;/table&gt;&lt;/body&gt;&lt;/html&gt; the rendering (on a fine grid) might appear as in Fig. 4 . can capture the table in an ontology that formally describes the observable input. Figure 5 shows an ontology describing the observable input for the table described by the ASCII character sequence above. In Fig. 5 the ASCII Character ob-ject set consists of all ASCII characters. The Character In-stance object set consists of object identifiers, one for each instance of an ASCII character in a table at a particular posi-tion. Thus, we can capture each ASCII character (including each space and carriage-return character) and its position in the sequence of ASCII characters representing the table. 5 observe that it is possible to model ontologically all ob-servable input features. Different features would be captured for alternative input media. For PostScript, for example, the input ontology would capture bounding boxes for strings along with string content, and for HTML, the input ontol-ogy would capture the table-row and table-data structure as provided by the tr and td tags. For tables whose input media is an image, we can model the input down to the pixel level if we wish. Our ASCII example here only indicates the possibilities, and we will not attempt in this paper to produce a full ontology of all features of interest. Our desire here is only to indicate that it is possible to create such ontological models as suggested in both [ 36 ]and[ 4 ].
 (whether it is a simple string of ASCII characters, a PostScript file, an HTML file, or an ontologically described sequence of ASCII character instances), the goal is to obtain the following kind of output: &lt;cell row="1" col="1"&gt;LASTNAME&lt;/cell&gt; &lt;cell row="1" col="2"&gt;INITIAL&lt;/cell&gt; &lt;cell row="1" col="3"&gt;BIRTHDATE&lt;/cell&gt; &lt;cell row="2" col="1"&gt;Smith&lt;/cell&gt; &lt;cell row="2" col="2"&gt;J&lt;/cell&gt; &lt;cell row="2" col="3"&gt;12/3/1988&lt;/cell&gt; &lt;cell row="3" col="1"&gt;Barr&lt;/cell&gt; &lt;cell row="3" col="2"&gt;K&lt;/cell&gt; &lt;cell row="3" col="3"&gt;25/5/1975&lt;/cell&gt; which is a logical representation of the cell structure of the table, with row and column indices assigned to each cell. of view, we can see this paradigm as generating object and relationship sets in an intermediate, derived ontology that includes a derived object set Cell with associated Row and Column object sets. Figure 6 shows an ontology with these derived object sets with their derived relationships among each other and their relationship to Table .Aspartofthe derivation, the paradigm would have recognized character-instance blocks of words, spaces, and carriage returns. An intermediate ontology can represent these derived object sets as they are created as Fig. 6 shows.
 sponds to transforming a list to an array, is as follows: 1. Render table on logical 2-D grid. 2. Parse 2-D representation. If necessary (i.e., if table is 3. Advance column count for each column separator. 4. Advance row count for each row separator and reset col-lar isothetic tessellation of a rectangular region into virtual cells, and a superimposed partitions of the virtual cells with which cell content is associated. The authors propose to link together cells in the same row or column with a text-block-adjacency graph reminiscent of DocStrum [ 70 ]. analysis by seeking regions with horizontally and vertically aligned word bounding boxes is advanced in [ 89 ]. No exper-imental results are presented because the test data had not yet been collected when the paper was written. 6.2 Compound tables with blank lines In this case, the input is character strings delimited by column-separators and row-separators. Each cell may have multiple components on the same or different logical text lines. For example, in ASCII this might be: LAST----INITIAL----BIRTH X NAME---------------DATE X  X Smith---J----------12/3-1988 X  X  Barr----K----------25/5-1975 X  which would be rendered (on a coarse grid) as: LAST INITIAL BIRTH NAME DATE Smith J 12/3 1988 Barr K 25/5 1975 lows: 1. Render table on logical 2-D grid. 2. Parse 2-D representation. If necessary (i.e., if table is 3. Make horizontal cuts at the end of groups of blank text 4. Then consider any text within a cell delimited by hori-5. Assign row and column numbers.
 tion were Laurentini and Vida [ 60 ]. Their objective was to transform tables found on scanned pages into electronic form, rather than extract the table structure for further analy-sis. They find rulings by run-length analysis, and then check if tight groups of character-sized connected components fall within the resulting cells. Groups with large gaps in their projection profiles are subdivided by invisible virtual rul-ings.
 jection profiles and aligned spaces between word bound-ing boxes, was applied to convert Japanese tables into HTML [ 86 ]. This paper has a very concise review of pre-vious work, but only examples and no statistical results. the lines in partially ruled tables as successions of adjacent black runs. Missing demarcations are found by an analy-sis of white streams. After a set of horizontal and vertical demarcations are obtained, individual blocks are labeled as heading, subheading, or entry. A block is labeled as a head-ing if it has more than one child, and as a subheading if it is the first block with a single child, followed by a similar single-child pattern. Only column headings are considered. tual lines X  that separate aligned components of text. He con-verts the resulting  X  X able skeleton X  first into an X X  X  tree [ 68 ] and to Microsoft Excel spreadsheets using Excel macros [ 1 ]. He did not OCR the text itself.
 documents to find tables consisting of clusters of horizon-tal and vertical cuts [ 14 ]. The algorithm has five thresh-olds, which are optimized to obtain the maximum value of a  X  X able Location Index X  on training documents. The method locates correctly over 58 of 75 tables in almost noise-free IEEE-PAMI pages, and 22 of 58 tables in U. Washington test images. They report that this performance is far superior to that of two leading commercial OCR systems that also find tables.
 boxes to separate the cells and construct cell separators for the table frame. His method handles large, complex, fully-lined, semi-lined, and line-less cell tables with multiple lines of symbols per cell by iteratively identifying all cell sepa-rators and cells. Although spanning header cells are found, their relationship to the leaf cells is not determined [ 38 ]. menting a partially-ruled table into a lattice composed of a grid of rectangles [ 39 ]. Lines are grouped when they inter-sect, are close and nearly parallel, or if their endpoints are close. Rulings are extended by virtual lines to the outermost ruling. Eventually, rectangles separated only by virtual lines are joined. The resulting polygons form cells only if they are rectangular, contain only character strings, or are empty. Alignment is performed left-to-right with a string-correction algorithm (the DP X  X ynamic programming X  X hat appears in the title of the paper) where the weights for substitution are the differences between the baselines of two text strings. This method can find the cell structure even when the cell contents are of unequal size or when there are many empty cells. 6.3 Compound tables without blank lines Here we have no guarantee that there is vertical separation between logically distinct rows in the table. For example, input in the ASCII case might consist of character strings delimited by column separators and row separators. Each cell may have multiple word blocks on the same or differ-ent text lines. The contents of cells overlap both horizontally and vertically.
 LAST--INITIAL-BIRTH X NAME----------DATE X S mith--J-------12/3-1988 X Barr--K-------25 /5-1975 X  which would be rendered (on a coarse grid) as: LAST INITIAL BIRTH NAME DATE Smith J 12/3 1988 Barr K 25/5 1975 not find the horizontal separators directly. The paradigm here combines top X  X own and bottom X  X p processing: 1. Render table on logical 2-D grid. 2. Parse 2-D representation. If necessary (i.e., if table is 3. Group word blocks that have high association (or cohe-4. Find and use bounding box of phrase blocks (cell over-5. Finally, assign row and column numbers to phrase ceptual cell commonalities, such as  X  X mount fields X  and  X  X ates, X  was developed by Bayer [ 12 ]. While the work does not specifically address tables, it offers a toolbox of syntac-tic, lexical, and geometrical properties in a manner suitable for a table ontology.
 ble extraction and retrieval experiment involving 6,509 ta-bles from a corpus consisting of 6 years of text from the Wall Street Journal [ 73 ]. This data, professionally written and from a single source, is likely to be unrealistically uni-form, however. Pyreddy and Croft have elaborate heuristics to separate leaf cells from other table content but do not dif-ferentiate between table captions and headings because they are used in a similar way in their information retrieval sys-tem.
 of cells: data , vertical indices , horizontal indices , title ,and footnotes . They present a detailed analysis of  X  X able topol-ogy, X  i.e., the conventions governing the layout of cells, and of the placement of data within the cells. The contents of each cell are analyzed by string matching to discover cells with similar letter syntax. The resulting rules for determin-ing the  X  X eading order X  of the table are embodied in a PERL script. They present experimental results on a heterogeneous corpus of 100 electronic tables that they suggest mimic the results of processing typeset paper tables with 99% accurate OCR. It is clear that even aside from possible OCR and im-age processing errors, manual editing would be required for most applications [ 71 ].
 manian [ 77 ] offer an interactive method of building models consisting of modular interactive agents for information access and capture in distributed databases. They give examples of structure detectors and segmentation mod-ules for both paper and electronic tables. These modules subdivide documents according to prevalent white spaces and match table rows by syntactic string matching. In an interesting digression, they predict the probability of incidental white streams from word length statistics. medium-independent approach to table detection and struc-ture recognition based on a dynamic programming algorithm that computes the optimal partitioning of the input into some number of tables, uses hierarchical clustering to determine the column structure, and then applies heuristics to deter-mine table headers and row segmentation. They also present evaluation measures for quantifying the performance of such algorithms [ 45 ]. One targeted application is automatically reformulating tables found in email for user access over the telephone [ 43 ]. 6.4 Tables with rules Previously, we considered the table cell contents to be de-limited by white space. Now we turn to the scenario where cells are delimited by ruling lines. Such situations are more likely to arise in the case of scanned tables, so our examples will now refer to that mode of input.
 bitmap of a ruled table: 1. Process image to find and assemble line segments to de-2. If necessary (i.e., if table is in bitmap format), OCR cell 3. Use frame for row and column numbering.
 Order of line finding and OCR, if it is necessary, may be interchanged.
 segments include the Hough Transform [ 87 ], thinning, vec-torization [ 1 ] and projection profiles [ 50 ]. Turolla et al. suc-ceed in detecting 95% of 11,513 lines in 114 tables. They located cell entries of fully boxed tables by finding the mini-mal cycle of the graph corresponding to the frame. The lines are found using the Hough Transform. The system was de-veloped primarily for French tax forms.
 lines. He expands the text bounding boxes until they meet either rulings or other text. Then he aligns cell boundaries in partially ruled tables with projection profiles. The method is applied to tables scanned at 400 dpi. The method attempts to extract spanning vertical and horizontal header cells, but sometimes fails on multiline header cells because of inaccu-rate local text-block extraction.
 content-separator overlaps. Instead of seeking the intersec-tion of horizontal and vertical lines, inner (white) and outer (black) bounding boxes constitute the lowest-level structure analyzed. The proposed underlying model is described only as follows:  X  X  table-form document is a type of form com-posed of strings and cells made from vertical and horizon-tal lines. X  The system was tried only on 10 fairly complex forms, and only the timing results are given in detail. cell structure from fully lined but highly degraded tables with broken rulings and overlapping cell contents [ 40 ]. The main contributions are the use of fine and coarse scans, and a separate set of bounding boxes in both for white spaces and for foreground connected components. The boxes in the coarse and fine images are reconciled according to the ex-pected grid layout, and converted into a cell structure that corresponds to an idealized version of the scanned table. gram for the structural analysis of ASCII tables based on bottom X  X p clustering of words, is described in [ 53 ]. The method works on both electronic and paper tables, starting with word bounding boxes. It can handle very narrow gaps, misaligned cells, and cells that span more than one printed line. It ignores ruling lines completely because it was de-signed for blocked text structures not only regular tables. A more flexible approach, T-Recs ++ , that can detect and analyze less regular tables as well as business letters, was subsequently reported [ 52 ]. 6.5 Tables with simple headers The input to this stage of processing is the output from the previous paradigms, i.e., phrase blocks with row and column numbers. For the example we have been using thus far, the output from the previous stage might be: &lt;cell row="1" col="1"&gt;LAST NAME&lt;/cell&gt; &lt;cell row="1" col="2"&gt;INITIAL&lt;/cell&gt; &lt;cell row="1" col="3"&gt;BIRTH DATE&lt;/cell&gt; &lt;cell row="2" col="1"&gt;Smith&lt;/cell&gt; &lt;cell row="2" col="2"&gt;J&lt;/cell&gt; &lt;cell row="2" col="3"&gt;12/3 1988&lt;/cell&gt; &lt;cell row="3" col="1"&gt;Barr&lt;/cell&gt; &lt;cell row="3" col="2"&gt;K&lt;/cell&gt; &lt;cell row="3" col="3"&gt;25/5 1975&lt;/cell&gt; &lt;colhead col="1"&gt;LAST NAME&lt;/colhead&gt; &lt;colhead col="2"&gt;INITIAL&lt;/colhead&gt; &lt;colhead col="3"&gt;BIRTH DATE&lt;/colhead&gt; &lt;cell row="1" col="1"&gt;Smith&lt;/cell&gt; &lt;cell row="1" col="2"&gt;J&lt;/cell&gt; &lt;cell row="1" col="3"&gt;12/3 1988&lt;/cell&gt; &lt;cell row="2" col="1"&gt;Barr&lt;/cell&gt; &lt;cell row="2" col="2"&gt;K&lt;/cell&gt; &lt;cell row="2" col="3"&gt;25/5 1975&lt;/cell&gt; lows: 1. Determine whether rows or columns or both are homo-2. If a row or column is homogeneous, it may have a header. paradigm as the derived ontology in Fig. 6 . Using algorithms to derive headers, produces the ontology in Fig. 7 , where the only change to the diagram in Fig. 6 is the object set Column Header , which marks certain cells as header cells. From this information it should be clear that we can derive the label X  value pairs needed for the output ontology in Fig. 1 . parse a table. The sequence of productions reproduces the table structure of rows and columns [ 74 ]. 6.6 Tables with nested headers The input in this case is the intermediate output of Paradigm 6.3 : phrase blocks and phrase block bounding boxes that do not constitute a uniform grid. For example: LAST NAME INITIAL BIRTH DATE Smith J 12/3 1988 Barr K 25/5 1975 &lt;colhead col="1"&gt;EMPLOYEE LAST NAME &lt;colhead col="2"&gt;EMPLOYEE INITIAL &lt;colhead col="3"&gt;PENSION STATUS BIRTH &lt;cell row="1" col="1"&gt;Smith&lt;/cell&gt; &lt;cell row="1" col="2"&gt;J&lt;/cell&gt; &lt;cell row="1" col="3"&gt;12/3 1988&lt;/cell&gt; &lt;cell row="2" col="1"&gt;Barr&lt;/cell&gt; &lt;cell row="2" col="2"&gt;K&lt;/cell&gt; &lt;cell row="2" col="3"&gt;25/5 1975&lt;/cell&gt;} 1. Determine top/leftmost spanning cells (using language 2. Create virtual cells by subdividing spanning cells. 3. Determine whether elements of next row or column dis-4. If yes, distribute (inverse of  X  X actoring X ) contents of 5. If no, create virtual row/column headers with unique This paradigm may become immensely complex with multi-ply nested row and column headers. The paradigm may also include analysis of stub (top-left cell), which is often the header for the row headers.
 are the Table Syntax [ 32 , 58 ], the Structure Description Tree [ 93 ], and the Cohesion Domain Template [ 48 ]. All three model only local horizontal and vertical adjacency re-lationships between cells. They aim at finding an appropriate tiling of the table. The foundations for a more sophisticated scheme are laid in [ 46 ].
 between cells can be exploited for validating an interpreta-tion. Some examples are given in [ 93 ].
 namoorthy apply a compiler design approach to parsing scanned ruled tables. The analysis consists of lexical, syn-tactic, and semantic steps starting at the pixel level and end-ing up with an EXCEL-like cell enumeration scheme suit-able for multiple levels of spanning headers. Although the method is quite general, a model must be defined for every new family of tables.
 aim at a complete description of the various types of infor-mation necessary to interpret a ruled scanned table. A train-ing set of diverse tables is used to populate a classification tree, and each node of the classification tree contains infor-mation, in the form of a Structure Description Tree (SDT), to interpret a specific family of tables. In the operational phase, unrecognized documents are added to the classification tree, and a new STD is created for them.
 horizontally and vertically repeated structures. It is both a logical layout representation and a syntactic description. Single and multiple horizontal and vertical location de-pendence relations are defined. These relations allow the analysis of rectangular substructures (called  X  X tructure frag-ments X ) of cells with spanning cells (usually headers) to the left or above related to content cells below and to the right. The semantic properties of individual table entries (city, zip-code) are expressed as item frames. Item fields may be name fields or data fields. The authors view the SDT as 2-D infor-mation, item sequence rules as 1-D, and a pattern dictionary as 0-D.
 include extraction of horizontal and vertical line segments and corners. Image processing errors may be recovered in the course of subsequent analysis. The final output, aside from the meta-information used to process the tables, is the grid outlay and a set of interpreted name and data fields. Recognition of the table type assumes that the relationship between these fields is already known, hence high-level in-terpretation is moot.
 a grid structure by finding horizontal and vertical  X  X plit points X  using connected components, projection profiles, and gap thresholds. The method was developed for a mul-tilingual FineReader OCR product. He also suggests using a high-level declarative definition of possible table layouts in the form of a grammar, with the extracted table grid and its cells as the terminal symbols. Heuristics are provided for common layouts of simple and compound (header) cells, where allowable layouts are specified by a  X  X tyle X  variable. The examples in the paper show successful segmentation of quite complex and dense tables. 6.7 Nested tables with row and column headers Ta b l e 4 shows a table with nested headers for both rows and columns. The spanning header for the rows, which is Char. in Table 4 , is in the stub of the table. 6 In general, a table with row headers may have none, one, or several spanning headers. When there are several, they typically appear to the left of the row labels in spanning boxes.
 ample is the output of Paradigm 6.4 . Since there is a row spanning header, we should distribute it over the rows in the same way we distribute column spanning headers over columns as explained in Paradigm 6.6 . The output in this case should be: &lt;rowhead row="1"&gt;Char. A&lt;/rowhead&gt; &lt;rowhead row="2"&gt;Char. B&lt;/rowhead&gt; ... &lt;colhead col="1"&gt;Binary Zone&lt;/colhead&gt; &lt;colhead col="2"&gt;Binary Numeric&lt;/colhead&gt; &lt;colhead col="3"&gt;Hex&lt;/colhead&gt; &lt;cell row="1" col="1"&gt;1010&lt;/cell&gt; &lt;cell row="1" col="2"&gt;0001&lt;/cell&gt; &lt;cell row="1" col="3"&gt;A1&lt;/cell&gt; &lt;cell row="2" col="1"&gt;&lt;/cell&gt; &lt;cell row="2" col="2"&gt;0010&lt;/cell&gt; &lt;cell row="2" col="3"&gt;A2&lt;/cell&gt; ...
 The paradigm is a combination of Paradigm 6.6 along with a similar paradigm that produces row headers. 6.8 N -dimensional tables Ta b l e 4 is a 2-D table; Table 3 is a 3-D table. In principle, we can have any number of dimensions, although higher dimen-sions are not typical because their layout becomes increas-ingly complex. Higher dimension tables, for example, may be recursively nested, broken into labeled groups of tables, or successively linked through hypertext in cells of HTML tables.
 tables generalize Paradigm 6.7 , which in turn generalizes Paradigm 6.6 . To generalize the output, we can use dimen-sions rather than rows and columns. The output for Table 3 , for example, would be: a future challenge for the community. 7 Conclusions We have identified a number of potential applications for table processing and the corresponding research problems for which little work has been reported thus far. We have also expressed our opinions of the relative difficulties of the tasks involved. To recapitulate, the applications are as follows: 1. Large-volume, homogeneous table conversion. 2. Large-volume, mixed table conversion. 3. Individual database creation. 4. Tabular browsing. 5. Audio access to tables. 6. Table manipulation. 7. Table modification for display.
 An obvious next step would be to analyze these applications further to determine their commonalities and differences. 1. Query mechanisms for freeform electronic tables. 2. Audio navigation and access to a gridded table. 3. Subdividing a table into a set of equivalent tables. 4. Spotting tables in electronic mail. 5. Clustering tables into similarity groups. 6. Converting a paper or electronic table into an abstract 7. Effects of  X  X oise X  in tables and correction of errors in-8. Performance evaluation of both table conversion and ta-The ways in which the applications and problems interrelate are depicted in Table 5 . Unless we make headway on perfor-mance evaluation, including acquisition of statistically ade-quate test material, it will be difficult to evaluate progress on any of the other tasks.
 cessing the graphic elements of the table. Very little has been reported on combining such image processing with the re-sults of character recognition of the cell contents. Although the logical interpretation of paper and electronic tables is similar, the overhead of image processing and OCR makes the former a much more difficult task. Current OCR sys-tems often de-columnize tables because superficially they look like multicolumn text. No test on a large, heterogeneous corpus has been reported, and few researchers have consid-ered the need to provide a mechanism for the correction of residual errors from automated processing.
 easier problem of electronic table conversion. Several com-mercial organizations advertise their capability of converting electronic tables to various forms, including spreadsheets. Some advertise conversion of tables presented in raster im-age form.
 SGML, HTML, XML, L A T E X, Tbl, or other, can probably be converted with moderate effort to an abstract form with over 90% accuracy. Spotting large tables in electronic doc-uments is relatively easy, but delineating them precisely is more difficult. A limit on achievable accuracy is imposed by the ambiguity inherent in these tasks.
 ing in terms of relational tables and table ontologies. The derivation of information from a table could be accom-plished by converting the table to a relational database or equivalent and formulating queries in SQL. Alternatively, queries can be answered by direct interactive access to a pre-processed table. Such preprocessing need not be much more elaborate than division into rows and columns.
 formation for conversion into a database, although they can be converted into an abstract table or spreadsheet. To add the necessary semantics, a model of the table is required. The model can be derived from an existing database cor-responding to similar tables, or it can be provided by the user/operator. The user can either provide the model explic-itly, or implicitly by correcting errors. Except for large vol-umes of similar tables, it appears sensible to take advantage of the user X  X  understanding of the context of the table; en-dowing a table-understanding system with such context is difficult.
 point that has often been ignored. Clearly, an investment in table processing must bring with it benefits that exceed the expenses involved. If it is always easier to recover the de-sired information through some other means (by browsing, say, or via a simple keyword query), then table processing serves no purpose. The formulation of such a model would be invaluable, and may very well provide insight into where we should apply our efforts to obtain the greatest possible return.
 centrated either on the problems associated with low-level analysis of printed tables, or on guidelines for table presen-tation, with comparatively little work on the topic of making tabular information useful (other than for highly specialized applications). What has changed to make this an interest-ing question to consider? The unprecedented explosion in the amount of information people are confronted with each day. Whereas large-scale databases were once the province of a select few, nowadays anyone with Internet access and an e-mail account is inundated with vast quantities of un-structured (or at best semi-structured) data. Automated table processing presents one promising way of recovering useful, familiar structure making it possible to realize more of the benefits of universal data access.
 also play a role in moving the state of the art forward. Such solutions have the potential to give data processing shops an advantage in throughput. These businesses service, for ex-ample, medical insurance companies (where tabulated claim data is verified) and telephone companies (where competi-tive analysis is performed on customer billing statements). Business intelligence and financial services companies have an interest in enhancing reaction speeds to dense informa-tion such as SEC filings, news wire items (which often con-tain tabular data) and other tabulated financial data. In cer-tain domains, archival tasks are wholly enabled by table-processing systems, as are data conversion and storage so-lutions (e.g., conversion to XML).
 transition from pixel and cell level analysis to table interpre-tation in a multi-document context.
 References
