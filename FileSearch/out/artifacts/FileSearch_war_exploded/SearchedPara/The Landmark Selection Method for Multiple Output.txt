 Krishnakumar Balasubramanian krishnakumar3@gatech.edu Guy Lebanon lebanon@cc.gatech.edu Conditional modeling x 7 X  y is a central problem in machine learning. Specific cases include classification, where y is a discrete random variable, and regression, where y is a continuous random variable. Much of the attention in recent years has focused on the case where x is a high dimensional vector. In this case, traditional statistical methods are inefficient due to overfitting. Proposed alternatives for high dimensional x include feature selection and regularized models.
 We consider, instead, the case of a high dimensional y , where x is either low dimensional or high dimensional. The baseline approach in this case is to independently construct models x 7 X  y i  X  R for i = 1 , . . . , k (assum-ing y is a k -dimensional real vector). This approach has the advantage of drawing from a wide variety of available single output models, including linear and non-linear regression, logistic regression, and support vector machines. The main disadvantage is that the independent models do not take advantage of a likely correlation between the dimensions of y . Incorporat-ing this correlation becomes especially important when the dimensionality of y is higher or of similar order to the dimensionality of x .
 Our approach is based on selecting a small subset L  X  { 1 , . . . , k } of the dimensions of y , and constructing two models: where we use the standard notation y L = { y i : i  X  L } . We thus have three problems: selecting the subset L , estimating (1), and estimating (2).
 Specifically, we estimate model (2) in conjunction with selecting L via least-squares regression with group Lasso based hierarchical regularization. The precise model (1) varies, based on whether y is discrete or con-tinuous. It may be any low-dimensional multiple out-put model, such as multilabel logistic regression and SVM, or multiple linear regression. If the dimension-ality of x is high, regularization for model (1) is also necessary.
 The underlying assumption of our model is that there exists a subset L of the dimensions of y , called land-mark variables, such that the remaining dimensions of y may be expressed as a noisy linear combination y = Ay L +  X  , with sparse coefficients. Several practi-cal data sets exhibit such a kind of relationship. One example is the prediction of future stock prices y from current stock prices x . The relationship y = Ay L +  X  is motivated by the identical trends of stock prices of multiple companies with a similar business model, or of multiple investment banks with similar holding portfo-lio. This phenomenon has been well documented in fi-nance under the term cointegration. Another example is the classification of images ( x ) depending on what objects appear or do not appear in them ( y ). Obvi-ously, some objects tend to appear or to not appear simultaneously, such as sky and tree, or car and road. The cardinality s of the subset L is typically orders of magnitude lesser than the actual dimension of the output space making the method scale well to ultra-high dimensional outputs. For example, the naive one vs. all method requires O ( k ) independent models that need to be learnt from the data, whereas the number of subproblems selected in the proposed approach scales at the rate of O ( s ). Assuming s  X  d we see that there is a huge advantage in terms of number of subproblems selected.
 We report in this paper experimental results for clas-sification and regression on multiple datasets. Based on our experimental study, we conclude that our model outperforms the one vs. all approach as well as several sophisticated multiple output prediction methods. Several methods have been proposed for multi-output prediction both in regression and classification setting. In the regression setting, most approaches have fo-cused on penalization of the regression matrix or in-put space sharing. For example (Izenman, 1975) in-troduced low-rank penalization of the regression ma-trix, which was analyzed in (Reinsel &amp; Velu, 1998) in the low-dimensional setting. Recent work focused on analyzing penalized regression in high dimensions (Rohde &amp; Tsybakov, 2011). An alternative approach that is directly applicable to multi-output prediction is group lasso (Yuan &amp; Lin, 2006). Though these methods are popular and widely applicable, they do not directly model correlations in the output dimen-sions, which can be used to reduce the complexity of the problem. A notable exception is the curds and whey method (Breiman &amp; Friedman, 1997) which uses shrinkage techniques in output space to reduce predic-tion error.
 In the classification setting, the popular approach of one-vs-all was proposed by several researchers (see (Rifkin &amp; Klautau, 2004) for a discussion). This method ignores the dependencies between the differ-ent dimensions of y , and is inefficient when y is high dimensional. A summary of improvements over the one-versus-all method is available in (Tsoumakas et al., 2010). Alternative approaches assume a class hierar-chy on the output space (Cesa-Bianchi et al., 2006), graph structure on the output space (Vens et al., 2008) and joint feature extraction from output and input spaces in large margin setting (Tsochantaridis et al., 2006).
 A paper related to our proposed method is (Hsu et al., 2009), which consider multi-label prediction in a sparse high-dimensional output space. Their pro-posed method for multi-label classification is to ran-domly project y and construct a regression model on the reduced subspace. There are two significant dif-ferences between this paper and our approach: (i) our approach uses data-dependent transformation, rather than a random projection, and (ii) our approach se-lects a subset of the dimensions of y that contributes to computational efficiency, statistical analysis, and is in line with some practical scenarios (see previous sec-tion). Furthermore, the approach by (Hsu et al., 2009) might not be applicable in the regression setting, as output sparsity assumption does not hold for regres-sion in practice.
 Recently proposed variations on (Hsu et al., 2009) in-clude (Tai &amp; Lin, 2012) that propose to reduce the dimensionality of the output space by PCA, and (Bi &amp; Kwok, 2011) that propose to reduce the label space by preserving a graph structure hierarchy on y . While these methods are sub-linear, they still project on to a low-dimensional real subspace, and hence they do not guarantee that the problem in the reduced subspace is easier than the original problem.
 Our approach also has a close connection to sparse PCA (Zou et al., 2006). Two significant differences are: (i) sparse PCA is generally applied to the covari-ates x rather than y , and (ii) our focus is on identifying the landmarks L and the relationship y L 7 X  y rather than estimating the principal components themselves. We consider a common scenario where x  X  R d and y  X  Y , where Y is either R k (regression) or { 0 , 1 } k (classification), and k, d are high dimensional. We de-note the data matrices, containing n labeled samples, Step 1: Selecting the landmark set L and modeling (2) A convenient way to select the set of landmark di-mensions L , and to model (2) simultaneously is the following regularized least squares regression model A = arg min where The first term in (3) is the least squares empirical risk that is standard in linear regression models. Obvi-ously, the identity A = I minimizing that term con-stitutes a trivial solution that is ineffective when y is high dimensional. The second and third terms in (3) promote a  X  X mall X  A and thus prevent the estimated model to be the trivial minimizer I of the first term. Much like group lasso, the second term in (3) enforces joint group sparsity across the rows of A . To see this note that k A k 1 , 2 is the L 1 norm of the L 2 norms of the individual rows. Due to the sparsity promoting nature of the L 1 minimizer,  X  A will have only a few rows that are not identically zero. The resulting ef-fect is the selection of landmark dimensions y L where L corresponds to the non-zero rows. We thus have that the first two terms in (3) simultaneously select the landmark dimensions L , and model y L 7 X  y . The third term k A k 1 promotes sparsity within the coeffi-cients of the model y L 7 X  y . This additional sparsity assumption reduces the prediction risk when y is high dimensional.
 The regularization parameter  X  1 controls the num-ber of landmark output dimensions. The regulariza-tion parameter  X  2 controls the sparsity of the model y L 7 X  y . Both  X  1 and  X  2 should increase with k . When the landmark assumption holds and there ex-ists a landmark set L  X  such that y is a noisy sparse linear combination of y L  X  , the row sparsity pattern of  X 
A should coincide with L  X  (assuming an appropri-ate selection of  X  1 ,  X  2 ). As  X  1 / X  2 increases, the group sparsity constraints become dominant implying that each dimension of y depends on all of the dimensions of y L . As  X  1 / X  2 decreases,  X  A tends to be more sparse within groups, implying that the dimensions of y are sparse linear combinations of the y L .
 From a practical point of view, with a proper selection of the regularization parameters  X  1 ,  X  2 (for example using cross-validation), the model (3) is quite flexible. It allows handling situations involving large landmark sets L and small landmark sets L , and high or low sparsity for the model y L 7 X  y . Empirically, the de-pendence on the precise value of  X  1 ,  X  2 is robust, as small variations in  X  1 ,  X  2 do not substantially change the predicted values.
 Handling non-linear output relationship: In or-der to select and learn non-linear relationships between the outputs and the landmarks, one could use func-tional joint sparsity models with L 1 /L  X  constraints as proposed by (Liu et al., 2008) or with L 1 + L 1 /L 2 con-straints (appropriately defined on a function space). With this change in step 1, the proposed approach could be used to handle non-linear relationships be-tween the outputs, making the proposed method more flexible. Developing concrete algorithms and analysis for this setting is left as future work.
 Step 2: Estimating (1) Once the landmark outputs L are identified, we can proceed with fitting model (1). In the case of contin-uous y (regression), model (1) can be estimated using a using multivariate regression model. In the case of a discrete y (classification), a one vs. all classifier may multiple output classifier may be used for x 7 X  y L Examples include support vector machines and log-linear models. From a statistical perspective, when y is high dimensional the reduction in the number of es-timated parameters from kd to sd (in the regression setting) where s  X  k , contributes to lower prediction risk. If the dimensionality of x is also high, the models x 7 X  y L or x 7 X  y i should use careful feature selection or regularization to avoid overfitting.
 Step 3: Prediction In many cases, a statistical model for (1) provides not only point estimates, but also a full probabilistic model p ( y L | x ). Similarly, a statistical model for (2) provides a full probabilistic model for p ( y | y L ). The implied model suggests the following procedure for predicting y from x An alternative to (5) is to use the following approxi-mation arg max In other words, given a new test sample x , we predict y
L using the model from step 2, and then estimate y
L c using the model from step 1, operating on the pre-dicted y L . In the case of classification, we follow stan-dard practice and set the components of y to 1 if the corresponding prediction of model (2) is greater than 0.5 and to 0 if it lesser than 0.5. Finally the outputs are concatenated and they represent the prediction for the given sample x . Algorithm 1 summarizes this pro-cedure. In this section, we give a brief theoretical analysis of the proposed approach in the regression setting high-lighting the advantage of the proposed approach. We and provide conditions under which it could be recov-ered consistently. Specifically, following the analysis Algorithm 1 Landmark selection method
Step 1: Simultaneously find the landmark set L and solve the optimization problem in step 1 to obtain the model y L  X  y and estimate  X  A .

Step 2: Estimate the model x  X  y L using inde-pendent models for each component of y L or us-ing multiple-output classification or regression algo-rithms.

Step 3: Given a new test point x , estimate y by (4)-(5). developed in (Obozinski et al., 2011) for random de-sign linear regression with group Lasso regularization, we can get a lower bound on the number of samples needed for recovering the support of the subset L  X  of the landmark labels. For simplicity, we consider the regression setting with the assumption that  X  2 = 0. We assume that Y consists of i.i.d. rows sampled from N (0 ,  X ). This distribution could in fact be any sub-Gaussian distribution (which includes any bounded random variable for example the Bernoulli random variable) for which a similar analysis could be carried out. We make the following assumption on the the covariance matrix  X : (1) there exists  X  min &gt; 0 and  X  max &lt;  X  such that all the eigenvalues of the s  X  s covariance matrix  X  s of the the landmark output y L  X  R mutual incoherence: there exist a incoherence parame-ter  X   X  (0 , 1] such that k  X  S c S c ( X  SS )  X  1 k  X   X  1  X   X  and (3) self-incoherence: there exists D max &lt;  X  such that k ( X  SS )  X  1 k  X   X  D max . Note that these are standard conditions assumed for support recovery results in the modern sparse recovery analysis. Condition (1) is needed to prevent over-dependency between the land-mark outputs. Conditions (2) and (3) are necessary conditions for model selection consistency of sparse re-covery problems. For example, several classes of ma-trices, for example Toeplitz matrices, tree-structured matrices and bounded off-diagonal matrices are shown to satisfy the above conditions (Zhao &amp; Yu, 2006). In the absence of these conditions, landmark recovery might fail even with arbitrarily large training set. We also make the following assumption on the regres-sion matrix. Let a min def = min i  X  L k A i k 2 where A note the i th non-zero row of the matrix A . We denote matrix, and This quantity characterizes the amount of overlap that could be captured given the output samples. Note that the support overlap function  X  ( A ) satisfies for any Y that satisfies assumption (1).
 Proposition 1. Consider the label matrix Y with rows i.i.d. drawn from N (0 ,  X ) satisfying assumptions (1)-(3), suppose that a 2 min decays no more slowly than f ( k ) /s  X  0 and f ( k )  X   X  . Then, as long as n &gt; C  X   X  max  X  ( A  X  ) log( k  X  s ) , we have with probability greater than 1  X  c 1 exp ( c 2 log s ) : (1) the optimization problem in 3 (with  X  2 = 0 ) has a unique solution when  X  the unique solution of the optimization problem 3 is equal to the row support of the true model.
 Proof. The proof follows from the corresponding proof in (Obozinski et al., 2011).
 The main consequence of the above proposition is that if there exist a set of landmark variable L  X  in the out-put space, the sample complexity is of logarithmic or-der in the original dimension of the output space k . Using sub-Gaussian assumptions on the label matrix, analogous conditions for classification are possible. Following (Reinsel &amp; Velu, 1998) we note that for a matrix regression problem y =  X  x +  X  with  X   X  R m 1  X  m 2 , the Frobenious norm error rate (with n sam-ples, unit noise variance and no assumption on the regression matrix) Since in our case the estimated matrix (1) (assuming linear regression model) is of the dimension s  X  d , the error is of the order of O ( sd n ) samples (Reinsel &amp; Velu, 1998), much smaller than the classical setting without the landmark selection method of O ( kd n ). In particu-lar, when s  X  k , there is a significant gain in efficiency. We conclude that the landmark method makes a struc-tural assumption on the output space in order to facil-itate regression in high dimensional setting ( n  X  kd ). Other methods, making a different set of structural assumptions (e.g., low-rank regression) try to achieve the same goal, but work under a different set of as-sumptions. Empirically, the landmark method works better than low-rank regression and group Lasso based multivariate regression on a variety of datasets (see Section 6).
 Here, we provide the optimization procedure required to solve the optimization problem described in step 1. The spaRSA method, proposed recently in (Wright et al., 2009), is a solver for optimization problems of the form where f is a convex loss function and  X  is a convex regularizer. The main advantage of spaRSA is that when the regularizer is group separable, the problem decomposes over the group.
 Using vectorization and block-diagonalization, it can be shown that (3) falls under this framework. Upon initial investigation, it appears that the block-diagonalization operation complicates the solver as it increases the size of the data matrix. However, we describe below a variation on spaRSA that works di-rectly with the Y and A . A similar approach was used in (Sprechmann et al., 2011) for the problem of collab-orative dictionary learning with hierarchical penalty. The main advantage of the spaRSA procedure (that the problem decouples across groups) is still preserved and further in our case, each subproblem could be solved via thresholding.
 In order to solve the optimization problem, the spaRSA procedure generates a sequence of updates that converges to the solution. We refer the reader to (Wright et al., 2009) for a complete description of the general procedure. In our case, we let f ( A i ) de-note the reconstruction error (the squared loss in our case) for A i (here and below we denote the i -column of a matrix A as A i ) and define the matrix U ( t )  X  R k  X  k whose i -column is given by The sequence of spaRSA updates that converge to the true solution is A which is group separable into k independent problems as below: A i = arg min The solutions for each of these sub-problems are avail-able in closed form (similar to (Sprechmann et al., 2011)) as follows: A olding require operations that are linear in the dimen-sionality of the matrix Y . The above procedure is repeated until convergence to obtain the final solu-tion that features row sparsity, and potential sparsity within rows as well. In this section, we compare our landmark selection method, which we refer to below as moplms, to alter-native baselines on classification and regression prob-lems. In our experiments we used code from (Wright et al., 2009) for performing the mixed norm penalty (group lasso and lasso) landmark selection. The regu-larization parameters were set by cross-validation. 6.1. Synthetic experiments We conducted an experiment on synthetic regression data with k = 500 (dimensionality of y ), d = 500 (di-mensionality of x ). The number of landmark outputs s was varied in the set { 50 , 100 , 200 } . The data was simulated from the above model, including the speci-fied landmark outputs. Figure 1 (left) shows the plot of the test MSE prediction error as a function of the sample size for various values of the parameter s/k . From section 4, we have that if the landmark output selection method is not used, with a linear regression model for x  X  y ,the Frobenious norm error between the true and estimated matrix scales as O kd n . Where as with the landmark output assumption the error for model 1 scales as O sd n . This benefit in the estimation error of the regression matrix is reflected in the MSE prediction error. Specifically, as s decreases, the sam-ple complexity decreases. This phenomenon is espe-cially important in high-dimensional cases, when there are fewer samples than the number of parameters to be estimated. We also compared the proposed approach to group-Lasso and low-rank multivariate regression. Figure 1 (middle) shows the mse prediction error rate of the moplms method decays faster compared to the other methods.
 We also experimented with synthetic classification data where y is a 500 dimensional binary vector and the input x  X  R 500 . Similar to the regression setting, the landmark outputs were first generated with s  X  { 50 , 100 , 200 } and the dependent outputs where gen-erated as sparse linear combination of the landmark outputs. Figure 1 (right) shows the Hamming loss as a function of the sample size. The x 7 X  y L model was collection of multiple one-vs-all SVMs. Similar to the regression case, the prediction error decays with the number of landmark outputs s/k . We further compare the proposed approach on synthetic data set against the following methods: 1. One vs. all: This is a standard base-2. Multilabel compressive sensing (mlcs): This 3. Multi-label classification via canonical cor-From Figure 2, we note that the proposed approach has a better rate of decay of hamming loss compared to the other approaches. This phenomenon is further observed in the real world data sets as described in the next section.
 We conducted an additional experiment to study the number of sub-problems selected. Specifically, we var-ied the number of sub-problems and the tuning param-eters of mlcs, and noted the values achieving the lowest prediction error. We then trained moplms, gradually reducing the regularization parameter until the pre-diction error matched that of mlcs. The two methods achieved identical prediction error with the following (mean) values of s/k : 0.45 (mlcs) and 0.30 (moplms), indicating moplms selected fewer sub-problems while achieving identical performance. Note, however, that mlcs always uses base regressors and moplms uses base classifiers. 6.2. Real-world data sets 6.2.1. Classification We experimented with the following two multiple out-put classification datasets. 1. del.icio.us This dataset consists of data from 2. Image data set. This dataset contains 68000 Note that we use thresholding to convert the real out-put to the binary form of the data. The regulariza-ml-cca 0.0164 0.3822 0.0041 0.3183 one.vs.all 0.0144 0.4512 0.0034 0.3923 moplms 0.0142 0.4522 0.0032 0.4031 tion parameters  X  1 and  X  2 were estimated using cross-validation. The number of selected landmarks s was 231 for the del.ic.ious data and 278 for the image data set. This was less than the number of sub-problems in both the mlcs and ml-cca approaches, which were also tuned for optimal prediction error.
 Table 1 displays the F1-score and hamming loss that are two standard evaluation metrics for multi-label classification.
 The landmark selection method performed better in terms of both evaluation metrics. The one-versus-all method was the second best in terms of prediction accuracy, but takes a significantly greater amount of train and test time, compared to the alternative meth-ods.
 Figure 3 (left and middle) shows the decay of the Ham-ming loss as a function of the sample size for mlcs, mo-plms and ml-cca method. We omitted the one-vs-all method as it took significantly more amount of time compared to the other approaches, and thus is not computationally attractive. The proposed landmark selection approach has lower prediction error than mlcs and ml-cca. 6.2.2. Regression In the regression setting, we consider predicting the stock prices of several companies based on previous values via the landmark selection approach on the SP 500 data set. More specifically, the data consists of closing stock prices of the 500 companies in the S&amp;P index in the period from August 21, 2009 to August 20, 2010 (a total of 245 entries). We assume the following autoregressive 1 or AR(1) model where y t = log S t S the stock price at time t ) for day t and E is the noise matrix. The problem is motivated by the observation in finance that multiple companies have stock prices that share identical stochastic trends (cointegration). We compare our landmark selection approach to low-rank multivariate regression (using trace norm regu-larization) and group lasso based multivariate regres-sion. These two baselines are popular multivariate re-gression methods. In our case (moplms), we used a multivariate ridge regression for estimating model (1), which is Equation 6 in the current setting. As in the classification setting, the regularization parameter was tuned by cross validation, and resulted in s = 98 land-mark outputs.
 Table 2 shows that moplms outperformed the two baselines (group lasso and low-rank multivariate re-gression). Figure 3 displays the prediction error rate as a function of the sample size. It confirms this con-clusion as the prediction error of moplms decays faster than the baselines. In this paper we propose a framework for multi-output prediction based on parsimonious modeling on the out-put space. By selecting a subset of the output dimen-sions (landmarks) and focusing on modeling the de-pendency of that subset of y on x , we reduce the sam-ple complexity considerably. This is most noticeable when the output dimensionality is high and the differ-ent component feature high correlation. Our experi-ments indicate that the proposed method outperforms standard multi-output methods in both the classifica-tion and regression scenarios.
 The results in this paper raise several interesting ques-tions and follow up directions. First, a detailed analy-sis is required to characterize the improvements of the proposed methods over competing methods. Second it is interesting to consider cases in which the label vector has a pattern of missingness.
 Acknowledgments: The authors would like to thank Kai Yu and Parikshit Ram for discussions.
 Bi, W. and Kwok, J. Multi-label classification on tree and dag structured hierarchies. ICML , 2011.
 Breiman, L. and Friedman, J. H. Predicting multivari-ate responses in multiple linear regression. Journal of the Royal Statistical Society:B , 59, 1997. Cesa-Bianchi, N., Gentile, C., and Zaniboni, L. In-cremental algorithms for hierarchical classification. Journal of Machine Learning Research , 2006.
 Hsu, D., Kakade, S. M., Langford, J., and Zhang, T. Multi-label prediction via compressed sensing. NIPS , 2009.
 Izenman, A.J. Reduced-rank regression for the multi-variate linear model. Journal of multivariate analy-sis , 5, 1975.
 Liu, H., Lafferty, J., and Wasserman, L. Nonparamet-ric regression and classification with joint sparsity constraints. NIPS , 2008.
 Obozinski, G., Wainwright, M. J., and Jordan, M. I. support union recovery in high-dimensional multi-variate regression. Annals of statistics , 39, 2011. Reinsel, G. C. and Velu, R. P. Multivariate reduced-rank regression: theory and applications . Springer New York, 1998.
 Rifkin, R. and Klautau, A. In defense of one-vs-all classification. The Journal of Machine Learning Re-search , 5:101 X 141, 2004.
 Rohde, A. and Tsybakov, A. B. Estimation of high-dimensional low-rank matrices. The Annals of Statistics , 39, 2011.
 Sprechmann, P., Ram  X  X rez, I., Sapiro, G., and El-dar, Y. C. C-hilasso: A collaborative hierarchi-cal sparse modeling framework. Signal Processing, IEEE Transactions on , 59, 2011.
 Tai, F. and Lin, H. T. Multi-label classification with principle label space transformation. Neural Com-putation , 2012.
 Tsochantaridis, I., Joachims, T., Hofmann, T., and Al-tun, Y. Large margin methods for structured and in-terdependent output variables. Journal of Machine Learning Research , 2006.
 Tsoumakas, G., I., I. Katakis, and Vlahavas. Mining multi-label data. Data mining and knowledge dis-covery handbook , pp. 667 X 685, 2010.
 Vens, C., Struyf, J., Schietgat, L., D X zeroski, S., and
Blockeel, H. Decision trees for hierarchical multi-label classification. Machine Learning , 2008. Wright, S. J., Nowak, R. D., and Figueiredo, M. A. T. Sparse reconstruction by separable approximation. Signal Processing, IEEE Transactions on , 57, 2009. Yang, J., Yu, K., Gong, Y., and Huang, T. Linear spatial pyramid matching using sparse coding for image classification. Computer Vision and Pattern Recognition , 2009.
 Yuan, M. and Lin, Y. Model selection and estima-tion in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology) , 68, 2006.
 Zhao, P. and Yu, B. On model selection consistency of lasso. Journal of Machine Learning Research , 2006. Zou, H., Hastie, T., and Tibshirani, R. Sparse prin-cipal component analysis. Journal of computational
