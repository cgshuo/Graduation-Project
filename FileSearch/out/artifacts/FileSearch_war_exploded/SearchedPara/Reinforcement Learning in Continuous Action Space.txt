 Most of the research on Reinforcement Learning (RL) [13] has studied solutions to finite Markov Decision Processes (MDPs). On the other hand, learning in re al-world environments requires to deal with continuous state and action spaces. While several s tudies focused on problems with con-tinuous states, little attention has been deserved to tasks involving continuous actions. Although stance using the tile coding approach [11, 12]), a different approach is required for problems in which high-precision control is needed and actions slightl y different from the optimal one lead to very low utility values. In fact, since RL algorithms need to experience each available action several times to estimate its utility, using very fine discretizatio ns may be too expensive for the learning process. Some approaches, although using a finite set of targ et actions, deal with this problem by selecting real-valued actions obtained by interpolation o f the available discrete actions on the basis of their utility values [9, 14]. Despite of this capability, the learning performance of these algorithms relies on strong assumptions about the shape of the value fun ction that are not always satisfied in highly non-linear control problems. The wire fitting algorithm [2] (later adopted also in [4]) tries to solve this problem by implementing an adaptive interpolati on scheme in which a finite set of pairs h action, value i is modified in order to better approximate the action value fu nction. Besides having the capability of selecting any real-valued action, RL algorithms for continuous ac-tion problems should be able to efficiently find the greedy act ion, i.e., the action associated to the highest estimated value. Differently from the finite MDP cas e, a full search in a continuous action space to find the optimal action is often unfeasible. To overc ome this problem, several approaches limit their search over a finite number of points. In order to k eep low this number, many algorithms (e.g., tile coding and interpolation-based) need to make (o ften implicit) assumptions about the shape of the value function. To overcome these difficulties, sever al approaches have adopted the actor-critic architecture [7, 10]. The key idea of actor-critic me thods is to explicitly represent the policy (stored by the actor) with a memory structure independent of the one used for the value function (stored by the critic). In a given state, the policy followed by the agent is a probability distribution over the action space, usually represented by parametric fu nctions (e.g., Gaussians [6], neural net-works [14], fuzzy systems [5]). The role of the critic is, on t he basis of the estimated value function, to criticize the actions taken by the actor, which consequen tly modifies its policy through a stochas-tic gradient on its parameter space. In this way, starting fr om a fully exploratory policy, the actor progressively changes its policy so that actions that yield higher utility values are more frequently selected, until the learning process converges to the optim al policy. By explicitly representing the policy, actor-critic approaches can efficiently implement the action selection step even in problems with continuous action spaces.
 In this paper, we propose to use a Sequential Monte Carlo (SMC ) method [8] to approximate the sequence of probability distributions implemented by the a ctor, thus obtaining a novel actor-critic algorithm called SMC-learning. Instead of a parametric fun ction, the actor represents its stochastic policy by means of a finite set of random samples (i.e., action s) that, using simple resampling and moving mechanisms, is evolved over time according to the val ues stored by the critic. Actions are initially drawn from a prior distribution, and then they are resampled according to an importance sampling estimate which depends on the utility values learned by the c ritic. By means of the resam-pling and moving steps, the set of available actions gets mor e and more thick around actions with larger utilities, thus encouraging a detailed exploration of the most promising action-space regions, and allowing SMC-learning to find real continuous actions. I t is worth pointing out that the main goal here is not an accurate approximation of the action-val ue function on the whole action space, but to provide an efficient way to converge to the continuous optimal policy. The main characteris-tics of the proposed approach are: the agent may learn to exec ute any continuous action, the action selection phase and the search for the action with the best es timated value are computationally effi-cient, no assumption on the shape of the value function is req uired, the algorithm is model-free, and it may learn to follow also stochastic policies (needed in mu lti-agent problems).
 In the next section, we introduce basic RL notation and briefl y discuss issues about learning with continuous actions. Section 3 details the proposed learnin g approach (SMC-Learning), explaining discussed in Section 4, and Section 5 draws conclusions and c ontains directions for future research. In reinforcement learning problems, an agent interacts wit h an unknown environment. At each time step, the agent observes the state , takes an action , and receives a reward . The goal of the return. An RL problem can be modeled as a Markov Decision Proc ess (MDP) defined by a quadru-T : S X A X S  X  [0 , 1] is a transition distribution that specifies the probability of observing a certain state after taking a given action in a given state, R : S X A X  X  X  is a reward function that specifies the instantaneous reward when taking a given action in a given st ate, and  X   X  [0 , 1) is a discount factor. after is formalized by the action-value function Q  X  ( s, a ) = E P  X  where r tion in each state. The optimal action-value function can be computed by solving the Bellman equa-tion: Q  X  ( s, a ) = R ( s, a )+  X  P the greedy action in each state:  X   X  ( a | s ) is equal to 1 / | arg max and 0 otherwise.
 Temporal Difference (TD) algorithms [13] allows the comput ation of Q  X  ( s, a ) by direct interaction with the environment. Given the tuple h s agent), at each step, action values may be estimated by onlin e algorithms, such as SARSA, whose update rule is: where  X   X  [0 , 1] is a learning rate and u ( r Although value-function approaches have theoretical guar antees about convergence to the optimal policy and have been proved to be effective in many applicati ons, they have several limitations: algorithms that maximize the value function cannot solve pr oblems whose solutions are stochas-tic policies (e.g., multi-agent learning problems); small errors in the estimated value of an action may lead to discontinuous changes in the policy [3], thus lea ding to convergence problems when function approximators are considered. These problems may be overcome by adopting actor-critic methods [7] in which the action-value function and the polic y are stored into two distinct repre-sentations. The actor typically represents the distributi on density over the action space through a function  X  ( a | s,  X  ) , whose parameters  X  are updated in the direction of performance improvement, as established by the critic on the basis of its approximatio n of the value function, which is usually computed through an on-policy TD algorithm. SMC-learning is based on an actor-critic architecture, in w hich the actor stores and updates, for the beginning of the learning process, without any prior inf ormation about the problem, the actor usually considers a uniform distribution over the action sp ace, thus implementing a fully exploratory policy. As the learning process progresses, the critic coll ects data for the estimation of the value function (in this paper, the critic estimates the action-va lue function), and provides the actor with information about which actions are the most promising. On t he other hand, the actor changes its policy to improve its performance and to progressively redu ce the exploration in order to converge actor represents its evolving stochastic policy by means of Monte Carlo sampling. The idea is the following: for each state s , the set of available actions A ( s ) is initialized with N samples drawn from a proposal distribution  X  0 ( a | s ) : Each sampled action a to 1 /N , so that the prior density can be approximated as where a density function. This means that the actor can approximate ly follow the policy specified by the are the selection probabilities. Given the continuous acti on-value function estimated by the critic and chosen a suitable exploration strategy (e.g., the Boltz mann exploration), it is possible to define the desired probability distribution over the continuous a ction space, usually referred to as the tar-get distribution . As long as the learning process goes on, the action values es timated by the critic become more and more reliable, and the policy followed by the agent should change in order to choose more frequently actions with higher utilities. This means that, in each state, the target dis-tribution changes according to the information collected d uring the learning process, and the actor must consequently adapt its approximation.
 In general, when no information is available about the shape of the target distribution, SMC meth-ods can be effectively employed to approximate sequences of probability distributions by means of random samples, which are evolved over time exploiting importance sampling and resampling tech-niques. The idea behind importance sampling is to modify the weights of the samples to account for the differences between the target distribution p ( x ) and the proposal distribution  X  ( x ) used to generate the samples. By setting each weight w weighted distribution P N the importance sampling step is performed by the actor, whic h modifies the weights of the actions according to their utility values estimated by the critic. Wh en some samples have very small or very large normalized weights, it follows that the target densit y significantly differs from the proposal density used to draw the samples. From a learning perspectiv e, this means that the set of available Algorithm 1 SMC-learning algorithm actions contains a number of samples whose estimated utilit y is very low. To avoid this, the ac-tor has to modify the set of available actions by resampling n ew actions from the current weighted approximation of the target distribution.
 In SMC-learning, SMC methods are included into a learning al gorithm that iterates through three main steps (see Algorithm 1): the action selection performe d by the actor, the update of the action-value function managed by the critic, and finally the update o f the policy of the actor. 3.1 Action Selection One of the main issues of learning in continuous action space s is to determine which is the best action in the current state, given the (approximated) action-valu e function. Actor-critic methods effectively solve this problem by explicitly storing the current policy . As previously described, in SMC-learning the actor performs the action selection step by taking one ac tion at random among those available in the current state. The probability of extraction of each a ction is equal to its normalized weight in the number of actions samples. 3.2 Critic Update While the actor determines the policy, the critic, on the basi s of the collected rewards, computes an approximation of the action-value function. Although se veral function approximation schemes could be adopted for this task (e.g., neural networks, regre ssion tress, support-vector machines), we use a simple solution: the critic stores an action value, Q ( s, a (like in tabular approaches) and modifies it according to TD u pdate rules (see Equation 1). Using on-policy algorithms, such as SARSA, the time complexity of the critic update is constant (i.e., does not depend on the number of available actions). 3.3 Actor Update The core of SMC-learning is represented by the update of the p olicy distribution performed by the actor. Using the importance sampling principle, the actor m odifies the weights w a policy improvement step based on the action values compute d by the critic. In this way, actions with higher estimates get more weight. Several RL schemes co uld be adopted to update the weights. In this paper, we focus on the Boltzmann exploration strateg y [13].
 The Boltzmann exploration strategy privileges the execution of actions with higher es timated utility values. The probabilities computed by the Boltzmann explor ation can be used as weights for the available actions. At time instant t , the weight of action a where  X  Q t +1 ( s, a ature) specifies the exploration degree: the higher  X  , the higher the exploration.
 possible to optimally solve continuous action MDPs by explo ring only a finite set of actions sampled from a prior distribution, since the optimal action may not b e available. Since the prior distribution after a few iterations, several actions will have negligibl e weights: this problem is known as the weight degeneracy phenomenon [1]. Since the number of samples should be kept lo w for efficiency reasons, having actions associated with very small weights means to waste learning parameters for approximating both the policy and the value function in regi ons of the action space that are not and update utility values of actions that are not likely to be optimal. Therefore, following the SMC approach, after the importance sampling phase, a resamplin g step may be needed in order to improve the distribution of the samples on the action domain. The deg eneracy phenomenon can be measured through the effective sample size [8], which, for each state s , can be estimated by where w in
A ( s ) , and low values of b N eff ( s ) reveal high degeneracy. In order to avoid high degeneracy, the actions are resampled whenever the ratio between the eff ective sample size b N number of samples N falls below some given threshold  X  . The goal of resampling methods is to replace samples with small weights, with new samples close t o samples with large weights, so that the discrepancy between the resampled weights is reduced. T he new set of samples is generated by resampling (with replacement) N times from the following discrete distribution proaches that have been proposed, here we consider the syste matic resampling scheme, since it can be easily implemented, takes O( N ) time, and minimizes the Monte Carlo variance (refer to [1] f or more details). The new samples inherit the same action value s of their parents, and the sample weights are initialized using the Boltzmann distribution.
 Although the resampling step reduces the degeneracy, it int roduces another problem known as sam-resampling steps a significant number of samples could be ide ntical. Furthermore, we need to learn over a continuous space, and this cannot be carried out using a discrete set of fixed samples; in fact, the learning agent would not be able to achieve the opti mal policy whenever the initial set of may be overcome by means of a smoothing step, that consists of moving the samples according to a continuous approximation  X   X  ( a | s, w by using a weighted mean of kernel densities: where h &gt; 0 is the kernel bandwidth. Typical choices for the kernel dens ities are Gaussian kernels and Epanechnikov kernels. However, these kernels produce o ver-dispersed posterior distributions, and this negatively affects the convergence speed of the lea rning process, especially when a few samples are used. Here, we propose to use uniform kernels: As far as boundary samples are concerned (i.e., a to K overlapped) kernel densities, each sample is moved locally within an interval which is determined by its distances from the adjacent samples, thus achieving f ast convergence. Besides reducing the dispersion of the samples, this resamp ling scheme implements, from the critic perspective, a variable resolution generalization approa ch. Since the resampled actions inherit the action value associated to their parent, the learned values are generalized over a region whose width depends on the distance between samples. As a result, at the b eginning of the learning process, when the actions are approximately uniformly distributed, SMC-learning performs broad generalization, thus boosting the performance. On the other hand, when the le arning is near convergence the avail-able actions tend to group around the optimal action, thus au tomatically reducing generalization which may prevent the learning of the optimal policy (see [12 ]). In this section, we show experimental results with the aim of analyzing the properties of SMC-learning and to compare its performance with other RL approa ches. Additional experiments on a mini-golf task and on the swing-up pendulum problem are repo rted in Appendix. 4.1 The Boat Problem To illustrate how the SMC-learning algorithm works and to as sess its effectiveness with respect to approaches based on discretization, we used a variant of the boat problem introduced in [5]. The problem is to learn a controller to drive a boat from the left b ank to the right bank quay of a river, with a strong non-linear current (see Figure 1). The boat X  X  b ow coordinates, x and y , are defined dynamics of the boat X  X  bow coordinates is described by the fo llowing equations: where the effect of the current is defined by E ( x ) = f current, and the boat angle  X  where I is the system inertia, s goal,  X  is the rudder angle, and p is a proportional coefficient used to compute the rudder angl e in order to reach the desired direction U The reward function is defined on three bank zones. The succes s zone Z the viability zone Z Therefore, the reward function is defined as: Figure 2: Performance comparison between SMC-learning and SARSA (left) , SMC-learning and tile coding and Continuous Q-learning (right) where D is a function that gives a reward decreasing linearly from 10 to -10 relative to the dis-tance from the success zone. In the experiment, each state va riable is discretized in 10 intervals and the parameters of the dynamics are those listed in Table 1 . At each trial, the boat is positioned at random along the left bank in one of the points shown in Figu re 1. In the following, we com-pare the results obtained with four different algorithms: S ARSA with Boltzmann exploration with different discretizations of the action space, SARSA with t ile coding (or CMAC) [12], Continuous Figure 2-left compares the learning performance (in terms of total reward per episode) for SARSA 5 and 10 samples. As it can be noticed, the more the number of ac tions available the better the performance of SARSA is. With only 5 actions (one action each 36  X  ), the paths that the controller result, the controller learned by SARSA achieves a very poor performance. On the other hand, a finer discretization allows the boat to reach more frequentl y the quay, even if it takes about three times the number of episodes to converge with respect to the c ase with 5 actions. As it can be noticed, SMC-learning with 5 samples outperforms SARSA wit h 5 and 10 actions both in terms of performance and in convergence time. In fact, after few tria ls, SMC-learning succeeds to remove the less-valued samples and to add new samples in regions of t he action space where higher rewards can be obtained. As a result, not only it can achieve better pe rformance than SARSA, but it does not spend time exploring useless actions, thus improving al so the convergence time. Nonetheless, with only 5 samples the actor stores a very roughly approxima ted policy, which, as a consequence of resampling, may converge to actions that do not obtain a pe rformance as good as that of SARSA with 20 and 40 actions. By increasing the number of samples fr om 5 to 10, SMC-learning succeeds in realizing a better coverage of the action space, and obtai ns equivalent performance as SARSA with 40 actions. At the same time, while the more actions avai lable, the more SARSA takes to converge, the convergence time of SMC-learning, as in the ca se with 5 samples, benefits from the initial resampling, thus taking less than one sixth of the tr ials needed by SARSA to converge. Figure 2-right shows the comparison of the performance of SMC-learning, SA RSA with tile coding using two tilings and a resolution of 2 . 25  X  (equivalent to 80 actions), and Continuous Q-learning with 40 actions. We omit the results with fewer actions becau se both tile coding and Continuous Q-learning obtain poor performance. As it can be noticed, SM C-learning outperforms both the compared algorithms. In particular, the generalization ov er the action space performed by tile cod-ing negatively affects the learning performance because of the non-linearity of the dynamics of the system. In fact, when only few actions are available, two adj acent actions may have completely dif-ferent effects on the dynamics and, thus, receive different rewards. Generalizing over these actions prevents the agent from learning which is the best action amo ng those available. On the other hand, as long as the samples get closer, SMC-learning dynamically reduces its generalization over the ac-tion space, so that their utility can be more accurately esti mated. Similarly, Continuous Q-learning is strictly related to the actions provided by the designer a nd to the implicit assumption of linearity of the action-value function. As a result, although it could learn any real-valued action, it does not succeed in obtaining the same performance as SMC-learning e ven with the quadruple of actions. In fact, the capability of SMC-learning to move samples toward s more rewarding regions of the action space allows the agent to learn more effective policies even with a very limited number of samples. In this paper, we have described a novel actor-critic algori thm to solve continuous action problems. The algorithm is based on a Sequential Monte Carlo approach t hat allows the actor to represent the current policy through a finite set of available actions a ssociated to weights, which are updated using the utility values computed by the critic. Experiment al results show that SMC-learning is able to identify the highest valued actions through a proces s of importance sampling and resam-pling. This allows SMC-learning to obtain better performan ce with respect to static solutions such as Continuous Q-learning and tile coding even with a very lim ited number of samples, thus improv-ing also the convergence time. Future research activity wil l follow two main directions: extending SMC-learning to problems in which no good discretization of the state space is a priori known, and experimenting in continuous action multi-agent problems.
 [1] M. Sanjeev Arulampalam, Simon Maskell, Neil Gordon, and Tim Clapp. A tutorial on particle [2] Leemon C. Baird and A. Harry Klopf. Reinforcement learni ng with high-dimensional, con-[3] D.P. Bertsekas and J.N. Tsitsiklis. Neural Dynamic Programming . Athena Scientific, Belmont, [4] Chris Gaskett, David Wettergreen, and Alexander Zelins ky. Q-learning in continuous state and [5] L. Jouffe. Fuzzy inference system learning by reinforce ment methods. IEEE Trans. on Systems, [6] H. Kimura and S. Kobayashi. Reinforcement learning for c ontinuous action using stochastic [7] V. R. Konda and J. N. Tsitsiklis. Actor-critic algorithm s. SIAM Journal on Control and Opti-[8] J. S. Liu and E. Chen. Sequential monte carlo methods for d ynamical systems. Journal of [9] Jose Del R. Millan, Daniele Posenato, and Eric Dedieu. Co ntinuous-action q-learning. Ma-[10] Jan Peters and Stefen Schaal. Policy gradient methods f or robotics. In Proceedings of the IEEE [12] Alexander A. Sherstov and Peter Stone. Function approx imation via tile coding: Automating [13] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction . MIT [14] Hado van Hasselt and Marco Wiering. Reinforcement lear ning in continuous action spaces. In
