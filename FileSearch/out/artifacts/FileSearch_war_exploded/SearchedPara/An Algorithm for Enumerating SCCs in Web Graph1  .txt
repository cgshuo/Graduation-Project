 The web pages and the hyperlinks between them form a huge directed graph. The nodes of the graph represent web pages while the directed edges represent the hyper-some classic algorithms in web search, topic classification and cyber-community enumeration. 
Connectivity analysis is an important part of the research on web graph. Enumerat-ing SCCs is the most common way when studying the connectivity of web graph. Some algorithms are available in O(n+e) ti me when enumerating SCCs in a directed graph into the main memory. Thus, we cannot use these algorithms directly. 
In this paper, we first review some traditional algorithms for enumerating SCCs in SCCs in this graph. In this algorithm, to take advantage of main memory, we split the original graph into parts which are smaller enough, decompose them one by one and finally merge them together (Section 3). Finally, we discuss our detailed implementa-tion of this algorithm on the web graph in China. The algorithm ends in a week while years (Section 4). The web graph contains billions of nodes and grows exponentially with time. Interest-ing properties are found in this graph such as the power law distribution and the bow-tie structure. 
In the prior observations from Broder and Kumar [1], the web is depicted as a bow-tie in Figure 1. 
The web graph is divided into several parts according to Figure 1. The core of the figure, CORE, represents the maximum strongly connected component of the graph. The left side of the bowtie, named IN, represents the pages from which at least a path exists to some nodes in CORE. The right side of the bowtie, named OUT, represents the pages which can be reached from some nodes in CORE. The TENDRILS contains but not yet been discovered by CORE, while OUT can be viewed as some well known pages whose links point to internal pages only. As for TENDRILS, The web has not yet discovered these pages, and these pages do not link to better-known regions. The deeper analysis of Figure 1 reveals the connectivity of the web graph. If pages u and v are randomly chosen, the probability that there exists a path from u to v is only 1/4. 
In order to compute the structure of the web graph, enumerating SCCs in web graph is necessary. 
Tarjan presented an algorithm to decom pose a directed graph into strongly con-notes the number of edges. The algorithm was composed of two interleaved depth-first searches (DFS). Firstly, a DFS traverses all the nodes and constructs a depth-first the descendents of the root including the root itself are in the same SCC. 
However, the most famous algorithm to decompose a directed graph into strongly SCCs in a directed graph in O(n+e) time. In the algorithm, two DFSs are performed. original graph G. Then, DFS again on G T in the order of decreasing f(u). Each tree in the forest output by the second DFS denotes a strongly connected component. 
Lisa.K.Fleischer brought up a parallel algorithm in year 2000 [5]. The main idea of the algorithm is as follows: 
DCSC (G): 
The algorithm divides the graph into three sub-graphs after outputting each SCC and decomposes the three sub-graphs recursively. As for finding the predecessors and rithm works efficiently in multiprocessor system. The algorithms of SCC enumeration described in section 2 require traversing the graph, which is sometimes not applicable to the web graph. Generally speaking, web graph consists of several hundreds of millions of nodes and several billions of edges. Although machines with 8GB main memory are popular in many organizations in-volved in web graph research and powerful algorithms of web graph compression [6] are available, sometimes it X  X  still impossible to load the entire graph into main mem-ory. Therefore, link information will be loaded from hard disk to main memory back infeasible to enumerate SCCs in the web graph in a straightforward way. 3.1 The Basic Idea The principal difficulty of the problem is the large scale of web graph. Were the graph the scale of each part will be much smaller. Thus, we get a rough idea of split-merge as follows: The split-merge algorithm: 1) Classify the nodes of graph G into n groups. Build a sub-graph with each 2) Decompose each sub-graph into SCCs. If the sub-graph is small enough, use 3) Assume each SCC in a sub-graph is a node, and eliminate the duplicated links 4) Decompose the contracted graph G X  into SCCs. If the G X  is small enough, use 5) Merge the SCCs from sub-graphs with the help of the decomposition of G X . For instance, look at the directed graph in Figure 2. 
The directed graph G consists of 10 nodes and 15 edges (Figure 2(a)). We split the graph into three sub-graphs (Figure 2(b)). Thus, the largest sub-graph only contains 4 nodes and 5 edges. The sub-graph in box 1 can be decomposed as (A, B, C) and (D). The sub-graph in box 2 can be decomposed as (E, F). And the sub-graph in box 3 can contract the graph as G X  (Figure 2(C)). G X  only contains 5 nodes and 6 edges and can from Figure 2(c) and Figure 2(b), we can enumerate all the SCCs in the original graph both sub-graphs and the contracted graph G X  are much smaller than that of the origi-nal graph G. If we split the web graph into sub-graphs, it X  X  possible to load one entire sub-graph into main memory when decomposing. Thus, the extra cost of split and merge seems to be affordable compared with swapping edges between hard disk and main memory back and forth. 
However, the basic split-merge algorithm does not always work on a general di-rected graph. Have a look at Figure 3. 
Figure 3 illustrate another way to split the original graph G. But this time, the basic split-merge algorithm doesn X  X  work because of the awful split. The scale of G X  is only a bit smaller. Thus, we should split the graph G X  again. The only result of this round of split and contraction is that E and F are in the same SCC. In contrast with the cost graph in this way, the cost of split and contraction will make the algorithm endless. 
Now, what remains is to find a way to split the web graph appropriately. However, it seems to be difficult to do the job well if only the link information is concerned. So pages and sites in the web graph. 3.2 Pages and Sites The web graph contains not only link information, but also URL information. Taking advantage of URL information will make the problem easy. On the web, each html page belongs to its owner site. For example, the page with same site. In our observation on the web graph in China, more than two thirds of the links are pointed to a local page within the same site, while only about one third links are remote which is across different sites. Further, for most sites, a homepage is pro-vided to guide the user to the pages they want. The homepage points to the most im-portant pages and those pages also link back to the homepage. In conclusion, pages in their owner sites, the split will not be too bad. 3.3 Clustering the Sites graph will be small enough to fit into main memory. But when we decompose all the law: the number of sites which contain x pages is proportional to 1/x k with some k &gt; 1. In our observation on the web graph in China, the exponent k is about 1.74. Under this distribution, the richest 10% sites possess more than 90% pages while 90% sites contain only less than 10% pages. Figure 4 exhibits the power law. graph, the small scale of sub-graph will make the effect of split unsatisfactory because we can only get a little information in many small sub-graphs. If we cluster the sites and regard pages in each cluster of sites as a sub-graph, the effect will be better. 
Algorithms of graph partitioning [7] are available, but most of them demand extra algorithms requires at least O(nm), which is still too large to applied to site graph. 
Here we propose three easier clustering methods which demand affordable time re-spectively. Random Assignment The easiest way to cluster the sites is random assignment. In this way, sites are clus-composed of pages in several random-chosen sites. The advantage of random assign-ment is we can control the size of each cluster in order to be fit into memory. 
The disadvantage of random assignment is obvious. Sometimes the sites in a clus-ter are irrelevant, and the connectivity among these sites is poor. The effect is as the cluster the sites. Site Graph SCC If we want high quality of the clusters, na X ve random assignment is not a good idea. Here, we introduce a method following the idea of hierarchical clustering. graph as the site graph. In this graph, n odes represent sites and edges represent links. Both the nodes and the edges of this direct ed graph are weighted. The weight of the denotes the number of real hyperlinks across the pages of two sites. 
If we regard each SCC of site graph as a cluster of site, the internal connectivity of each cluster could be high. Sites in the same component can reach each other by di-rected hyperlinks between their pages. 
In our recommendation, most well-connected sites can be clustered into the same sub-graph. We ignore the edges with small weight in the site graph and decompose it into SCCs, and regard each component as a well connected cluster of sites. 
While we can get well-connected sub-graphs by using site graph SCC, we can not control the size of each sub-graph precisely. So how to compromise between the qual-ity of the sub-graphs and the size of them is a problem. In section 4 we will discuss our detailed implementation of this work. Hierarchical Site SCC Each of the two methods we just mentioned considers only one of the two factors, the which takes both these factors into account. Then we fill the current cluster with those detached components. Once the size of the current cluster is estimated to reach the memory limit, we begin to construct another cluster. This procedure is stopped when the remained graph is estimated to be smaller than the memory limit. 
This method will somehow decrease the equality of the blocks because it may clus-memory. 3.4 Efficiency properly. The cost of each step of the algorithm is as follow. Split: Splitting the graph requires one extra copy of the full graph each time. It may several hours.) Decompose Sub-graphs/G X : Decomposing sub-graphs (or G X ) can be finished in number of nodes and e denotes the number of edges in the graph. As a fact, each edge tracted graph G X . It may cost a few days in total depending on the size of the whole graph. Contract the Graph: Graph contraction requires at most one full-graph copy. In fact, only a small portion of edges will be copied if the split is appropriate. Merge the SCCs: The cost of merging SCCs is O(nlog(n)). When we got the SCC the information in M1. Then we can output each component in M2 using binary-hours to merge SCCs and we can ignore the cost in contrast with those in other steps. 
To sum up  X  the extra cost of split-merge algorithm only depends on how many times we need to split the graph. Such costs only come from the IO operation during the process of split and graph contraction. If we just split the graph a few times, such assure us that there will be no redundant computing on SCC information. zero out-degree (or in-degree) is a good advice if the final G X  is a bit larger. contains around 140 millions of nodes and 4. 3 billions of edges. Our machine is Xeon 2GHz*4 /4GB SDRAM /150GB*7 HDD RAID5 /Windows2003, and our compiler is gcc version 2.95.3 for windows. Some of the main memory is assigned to system processes and the other processes. In our real implementation, about 2.1G main mem-ory is available to us. 4.1 Traditional Algorithm ited the edges in the graph by cache. Here, hit rate of the cache is crucial for the per-formance. Unfortunately, the hit rate is very low. Only around a hundred of edges are hit rate a lot because of the large scale of the graph. 4.2 The Split-Merge Algorithm Then we tried the split-merge algorithm. First we split the graph into 100 sub-graphs. In order to group nodes of the giant web graph, we built a site graph. The site graph con-tains about 470 kilos of nodes and 18 millions of edges. In this graph, we find that the of total weight of the graph. It assured us that the connectivity of each site is good. nected components with at least three sites. Each component was viewed as a cluster cluster and hyperlinks between them constructed a sub-graph. And also, we recorded the first round, the threshold was set to 1000. Thus, 1249 sites were classified into 99 famous ones and have more pages than the unclassified sites, and most of the unclas-sified sites were very small sites. Some of the clusters were just parts of a big famous http://business.china.com, http://news.china.com, http://sports.china.com, http:// fi-nance.china.com, etc. In fact, they all belong to http://china.com. 
We then built a sub-graph for each cluster, and also for the unclassified site. Unfor-owned around 25% and 70% of total pages respectively. We thus recursively applied these two sub-graphs. 
After all the SCCs in each sub-graph had been found, we get the final G X . As a fact, the G X  contained only less than 46 millions of edges. At last, we decomposed the G X  and merge the SCCs of each sub-graphs with the help of the decomposition of G X . 4.3 Efficiency quired one scan of the full graph, one scan of URL table and I/O operation of copying the edges of the site graph. In our implementation, we split the graph twice, so the I/O As a fact, a copy of the full graph took less than one day. And thus the cost of I/O was less than five days. The decomposition of all sub-graph and the G X  requires two scans of full graph in main memory and one loading of the full graph in total, which took a bit more than one day and a half. Merging SCCs was very fast. It only took less than six hours in total. The other cost in the algorithm like decomposing of the site graph was not listed here, but they can be ignored in contrast with those we have listed. To sum up, the total time cost of this run took only less than a full week. 4.4 Result Here we just exhibit our result briefly. Figure 5 is the bowtie structure of our data set of web graph in China. 
The graph shows that around 80% of web pages are in the maximum SCC in the web graph. And, if pages u and v are randomly chosen, the probability that there ex-distribution. components. Then we analyzed our detailed implementation. The algorithm is applied on the web graph in China, and the task is accomplished in an affordable time. viewed as a folded version of web graph. In the future work, we will make further ob-servation on this graph, find more potential relationship between sites and pages, and try to predict some properties on the web graph with the analysis on the site graph. 
