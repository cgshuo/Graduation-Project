 Vijay Gandhi  X  James M. Kang  X  Shashi Shekhar  X  Junchang Ju  X  Eric D. Kolaczyk  X  Sucharita Gopal Abstract Many statistical queries such as maximum likelihood estimation involve finding the best candidate model given a set of candidate models and a quality estimation function. This problem is common in important applications like land-use classification at multiple spatial resolutions from remote sensing raster data. Such a problem is computationally chal-lenging due to the significant computation cost to evaluate the quality estimation function for each candidate model. For example, a recently proposed method of multi-scale, multi-granular classification has high computational overhead of function evaluation for various candidate models independently before comparison. In contrast, we propose an upper bound based context-inclusive approach that reduces computational overhead based on the con-text, i.e. the value of the quality estimation function for the best candidate model so far. We also prove that an upper bound exists for each candidate model and the proposed algorithm is correct. Experimental results using land-use classification at multiple spatial resolutions from satellite imagery show that the proposed approach reduces the computational cost significantly.
 1 Introduction We are interested in a probabilistic statistical query to find the preeminent candidate model from a set of candidate models using a quality estimation function. We refer to such a problem as the best candidate model problem . Formally, it can be stated as follows: Given a set of candidate models and a function to evaluate the quality of each candidate model, the goal is to find the best candidate model probabilistically. The evaluation of this measure is generally very expensive and thus minimizing the computation time is a key objective.

One important example of the best candidate model problem is in classification of a spec-tral image, obtained from a satellite, with domain-specific labels to produce a thematic map . Thematic maps are widely used in applications including agricultural monitoring, land cover change analysis, and environmental assessment. Examples in land cover change can be seen in Fig. 1 , where 217 square miles of Louisiana X  X  coastal lands were transformed to water after hurricanes Katrina and Rita (Fig. 1 a); the deforestation in Brazil causing the loss of 150,000 sq. km. of forest between May 2000 and August 2006 (Fig. 1 b); and urban sprawl in Atlanta, GA between 1976 and 1992 (Fig. 1 c).

Image classification at multiple spatial resolutions is an important application of spatial data mining (e.g. [ 1 , 3 , 20 ]). For example, NASA X  X  Earth observation systems obtain a spectral image of land-use, which is then classified at multiple resolutions. The best candidate model problem to find the best classification label can be considered as a parameter estimation problem. Since estimating parameters at a spatial region is an important function in spatial data mining, the best candidate model problem may be a sub-class of spatial data mining.
Numerous studies in remote sensing have been done for multi-resolution land-use classi-resolution classification. A statistical method to classify an image at varying spatial and categorical resolutions was proposed in [ 12 ]. This approach is context-exclusive, based on using a query tree to identify each candidate model independently. The maximum likelihood is used as a set operator among quality measures for each candidate model, thus making it necessary to analyze all candidate models together. The result is very high computation costs to identify the preeminent candidate model.

Figures 2 and 3 gives an example of a multi-scale, multi-granular image classification based on land usage. Figure 2 gives the input that includes a set of domain-specific labels (also called classes) logically grouped as a hierarchy (Fig. 2 a), and the values of each specific class: coni f er , hard w ood , br ush ,and g rass (Fig. 2 c X  X ). The values of a specific class are derived from a synthetic remotely sensed satellite image [ 12 ](Fig. 2 b), that has been provided in this paper for completeness. The goal is to classify each pixel in the satellite image to one of the labels based on a quality measure called likelihood .The likelihood measure is calculated for non-specific classes using expectation maximization (EM) [ 2 ] which is computationally expensive because of the large number of iterations required until convergence. Also, multiple the size of the image). An example output in shown in Fig. 3 having a set of classified images at scales 1  X  1 , 2  X  2 , 4  X  4 , and 64  X  64.

Calculating the likelihood of each candidate model makes the problem computationally expensive. For instance, the approach proposed by [ 12 ], takes about 7 hours of computation time to classify an image of size 512  X  512 pixels with 12 labels at varying spatial resolu-tions . About 80% of the total computation time is consumed to find the quality measure for each candidate model. Thus, as the image size grows the computation time increases, which makes this problem challenging.

The focus of our work is to develop a computational efficient EM based multi-scale multi-granular (MSMG) method (see [ 12 ] for details on MSMG). In our previous paper [ 4 ], we proposed two heuristics to reduce the computation time for the evaluation of candidate mod-els. The first heuristic, called limiting factor (LF), is based on the precision of convergence for each candidate model. As LF decreases, the computation time is reduced due to faster convergence of each candidate model. The second heuristic, called context-inclusive heu-ristic (CIH), simultaneously considers information from all models during evaluation rather than independently as in the context-exclusive (CE) approach. Rather than executing each model till convergence and then ranks the models as in CE, CIH evaluates all models together as it executes and exits as soon as a single model is dominant over all the others.
However, CIH may not obtain the same exact results as in CE due to the fact that each model may not fully converge. Thus, we propose an upper bound based context-inclusive approach (UBCI) that utilizes a filter and refine technique for each model. We provide an upper bound for each model and use this information to obtain the same exact results as CE while still obtaining a significant savings in computation time than the CE approach.
Initial results were reported in [ 4 ], this paper makes additional new contributions: 1. We propose a novel Upper Bound based Context-Inclusive approach that finds correct 2. We prove than an upper bound exists for each model is our best candidate model problem 3. A proof of correctness is provided for our proposed approach (Sect. 4 ). 4. Experimental evaluation of synthetic and real datasets is given in terms of the number
In addition, we have made several revisions to improve its readability. For example, we provide a new section discussing Expectation Maximization (EM), its application to our problem, and an example execution trace in Sect. 2 .
 Scope. The following topics are out of scope in this paper: (i) the choice of using an optimal EM and statistical models for Multi-Scale Multi-Granular (MSMG) classification, (ii) the actual accuracy of the context exclusive approach, and (iii) a comparison between MSMG and traditional approaches. Interested readers are encouraged to see [ 9 , 10 ] for more details. Outline. The rest of the paper is organized as follows: A background on the EM algorithm and its application to our problem is described in Sect. 2 . Section 3 gives a detailed overview of our approach along with the major differences with previous work. As part of our proposed approaches, we prove an upper bound exists in each candidate model (Sect. 4 ). Experimental results to compare the previous and the proposed approach are given in Sect. 5 . Finally, Sect. 6 concludes this paper with a discussion and future work. 2 Background This section presents a general overview of Multi-Scale Multi-Granular Image classification and its application to the best candidate problem. Also, an execution trace example is shown. 2.1 EM-based multi-scale multi-granular image classification Satellite remote sensing provides timely and continuous coverage of the earth X  X  surface and has proven to be extremely important in making thematic maps such as land cover maps. Such remote sensing data is available now at a variety of spatial scales as part of the Earth Observing System (EOS) enterprise undertaken by NASA. This explosion of satellite data is leading to a paradigm shift in the way classification is done. Land cover classification in remote sensing is traditionally carried out at a single spatial scale and a single categorical scale and increasingly, the need to integrate data at multiple spatial scales, is leading to a new emphasis in classification that is both multiscale, in the sense of being able to incorporate information across multiple spatial scales in constructing a map and also multigranular in the sense that class labels of different granularity, or categorical scale, similarly co-exist in thesamemap.AsdiscussedinJuetal.[ 10 ], multiscale and multigranular framework may improve the accuracy of pixel-wise classification as well as impact the validation of coarse-scale land cover classification, provide an adaptive choice of scale in correspondence with local complexities in the image as well address a problem relating to cartographic general-ization, where the objective is to best represent selected classes of features at different map scales. Our framework provides a statistically grounded procedure for producting landcov-er maps that are potentially more intuitive and visually appealing than those created using monogranular methods.
 In this section, we present a mathematical representation of our previous method [ 12 ]. Please use Table 1 as a reference for the list of notations used in this section. Suppose we have digitized a spatial region into an image of n 2 pixels i.e., { I i 1 , i 2 } n i c , i 2 , and is assumed to be a member of some set of specific classes C ={ 1 ,..., C } .
A typical mono-granular probability model for a measurement x ,atapixel I , will specify truth c . But the multiscale/multigranular probability model [ 12 ] allows the user to assign a less-specific class to an entire region R containing I and certain surrounding pixels. The choice of possible regions R is restricted to those allowed under a quad-tree decomposition of the image region. Less-specific classes are constrained to belong to a subset S ,ofthe specific classes in C . Such subsets are pre-defined, based on prior knowledge of the image domain, and restricted in number.

The likelihood for the multiscale/multigranular probability model is specified using a mixture model i.e., The class-specific densities g ( x | c ) are assumed known i.e., they are input for the overall algorithm. So the unknown quantities in the model are (i) the choice of regions R , (ii) the choice of subsets S , for each region R , and (iii) the choice of relevant weights  X  c , for each class c  X  S . These unknown quantities must be inferred from data.
 A maximum likelihood approach is natural here, using the spectral measurements { x i 1 , i 2 } . Note that since the regions R can contain more than one pixel, by design, selection of the model parameters for any one pixel involves an optimization incorporating all pixels. And, since this is a very rich model class, some sort of penalty is advisable to discourage over-fitting. A complexity-penalized maximum likelihood method for model inference was proposed in [ 12 ].

Specifically, let M = ( R , S ,) be the set of regions R , subsets S = S ( R ) of class labels c , and weights  X  c =  X  c ( S ) under a given MSMG model for the image region. The optimal model  X  M is chosen as where ( x | M ) is the log-likelihood of the data under model M and pen ( M ) is a penalty function. Here x ={ x i 1 , i 2 } represents the full set of observations.

The penalty in ( 2 ) is essentially the code-length for describing the model M , a precise definition of which can be found in [ 12 ], the likelihood is of the form and the log-likelihood is of the form f ( x | S ) in ( 2 ) were known, then optimization would reduce to a search over all partitions of the image into regions R , and assignment of subsets S of categories to each region R .Give the use of a quad-tree structure in defining the regions R , this would result in an algorithm of O ( n 2 | S | ) complexity, where S is the collection of all possible subsets S allowed by the user.
However, the mixture weights in the densities f ( x | S ) are unknown and must be fit for each candidate region R and each candidate subset S . Usually estimation of mixture weights through maximum likelihood is done using the EM algorithm. Thus, the optimization in ( 2 ) nominally requires O ( n 2 | S | ) EM algorithms to be run. This requirement represents the major bottleneck in optimizing computational performance for the MSMG methodology and is this aspect that this paper seeks to improve upon. 2.2 Execution trace There are many implementations of the Expectation Maximization algorithm [ 2 ].
 Algorithm 1 presents the Expectation Maximization algorithm that is used in our prob-lem domain as described in [ 12 ]. First, we present the algorithm to discuss the computation aspect but not the correctness. Second, we give an example of this Algorithm using a two class problem. The input to Algorithm 1 is the Non-Specific class (e.g. forest ) to determine the maximum likelihood of its specific class (e.g. coni f er and hard w ood ) proportions  X  1 and  X  2 , respectively, the likelihoods of each of the specific classes, and the spatial region (e.g.aquadofsize2  X  2pixels).

In general, there are five major steps in a single iteration of Algorithm 1 . First, the pro-portions are set to their current values. If this is the first iteration, then the proportions are initialized such that each has an equal weight totaling to one (e.g., for two specific classes, the proportions are  X  1 = 0 . 5and  X  2 = 0 . 5. Otherwise, the proportions are obtained in the previous iteration. Second, the product of the likelihood at each pixel for a specific class and calculated: The third step is to take the sum of all likelihood values from all the specific classes. For two classes, the following would be used: the values obtained in step 3, g ,andissetto j ( x i 1 , i 2 | C k ) . For example, in two classes: proportion  X  k . For example, in two classes the proportions are found by: This process is repeated until the desired accuracy of the proportions (i.e., difference between the current and previous proportions) is acquired. Accuracy is determined by the amount of change in the likelihood value based on the proportion size between iterations.
 Algorithm 1 EM-based MSMG Image Classification
To give an example, consider the use of the Expectation Maximization algorithm for a non-specific class forest , calculated for a quad of size 2  X  2 pixels. The given likelihood values of the specific classes, Conifer ( C 1 ) and Hardwood ( C 2 ), corresponding to forest are represented by vectors in Table 2 .

Based on Algorithm 1 , there are five major steps in a single iteration. First, the propor-proportion values (Table 3 ), h ( x i 1 , i 2 | C k ) is obtained in Table 3 .

The third step uses ( 6 ) to find the total value between all specific classes. In our running example, Table 4 gives the results for g by summing the values obtained in the second step.
The fourth step uses the values from steps 2 and 3 within ( 7 ) and the values from are 2 class example is given in Table 5 .

Finally, the new proportions for each specific class can be created by taking the average process will continue to iterate until the 16th iteration where the final converging proportions are  X  1 = 0 . 6042 and  X  2 = 0 . 3958. The log-likelihood for forest can be calculated using ( 4 ) which is 1.17162. The likelihoods for the specific classes coni f er and hard w ood can be obtained by simply summing the values in Table 2 which is 2.2 and 1.8, respectively. Thus, the class having the maximum likelihood from forest , coni f er ,and hard w ood is con f i er with a log-likelihood value of 2.2 and is assigned to the given 2  X  2 region. 3 Approaches In this section, we present two approaches to address the best candidate model problem .The first approach is based on a previous approach [ 12 ] which we call context exclusive since each candidate model is evaluated independently (Sect. 3.1 ). The second approach is our proposed upper bound based context-inclusive method that uses a filter and refine technique to prune models based on their upper bound while evaluating each simultaneously (Sect. 3.2 ). 3.1 Context-exclusive approach Algorithm 2 presents the pseudo code of the context-exclusive approach from [ 12 ]. The input to Algorithm 2 is the set of non-specific class candidates and the output is the arg max can-didate in the candidate set and its corresponding likelihood value (i.e. maximum value in the Set of Candidate and Likelihood values SCL ). The main objective in Algorithm 2 is to obtain the candidate classification for a spatial region. Each candidate model in the candidate set is analyzed independently to obtain its likelihood for a spatial region by executing EM from Algorithm 1 (Line 4 in Algorithm 2 ). The candidate model containing the maximum or largest likelihood of all candidates in Cand is declared the best candidate model ( arg max ) for a spatial region (Line 6 in Algorithm 2 ). Algorithm 2 Context-Exclusive Approach To illustrate the context exclusive approach using an example, consider the hierarchy in Fig. 2 aandtheEMvaluesinFig. 4 . Initially, the specific class with the highest log-likelihood is chosen. Then, each non-specific class is analyzed in EM until convergence. The number of EM iterations for Ve g etati on , Forest ,and Non -Forest is 46, 34, and 3, respectively. Then, the class with the highest log-likelihood value among all of the classes (specific and non-specific) is chosen. In this example and as shown in Fig. 4 , Non -Forest has the highest value and is assigned to the given region. It is important to note and explained further in the next section that using this example, the CE approach took 83 EM iterations to find the answer. 3.2 Upper bound based context-inclusive approach Our proposed approach evaluates each model together to obtain a correct candidate model, a relationship which we refer to as context-inclusive using an upper bound. Essentially, the upper bound allows for pruning of non-specific class models (e.g. Forest) which allows for a significant reduction in computation time while achieving correct results. Using remote sensing terminology, each candidate model represents a classification consisting of either a specific (i.e., coni f er or hard w ood )ora general (i.e., forest )class(seeFig. 2 a). The qual-ity measure for a specific class is a single value whereas a general class consists of several proportions of specific classes. For example, a general class of type forest may have several proportions of coni f er and hard w ood trees. A computationally very expensive function is used (i.e., EM) to identify the likelihood value for each candidate model. The main objective is to find the maximum likelihood value or best candidate model to represent a land-use classification.

Algorithm 3 presents the upper bound based context inclusive approach. An upper bound is calculated for each non-specific candidate to ensure that the correct candidates are pruned from the candidate set (Theorem 1 ). By determining the upper bound on the likelihood value of a non-specific class (Line 2 of Algorithm 3 ), it is possible to prune candidates whose upper Algorithm 3 Upper Bound based Context-Inclusive Approach bound is lower than the current best likelihood value. This ensures that those candidates that have an upper bound less than the current best candidate can not be the best candidate and are then pruned (Lines 7 X 9 of Algorithm 4 ). For all other candidates, their respective maxi-mum likelihoods are found through the EM algorithm and compared against the current best candidate (Lines 11 X 15 of Algorithm 4 ). Finally, the best candidate having the largest or maximum likelihood of all other candidates is returned (Line 18 of Algorithm 4 ).
To illustrate with an example, consider the hierarchy defined in Fig. 2 a and the actual likelihood and upper bound values in Fig. 5 . Initially the likelihood values are calculated for the specific classes ( coni f er , hard w ood , br ush , g rass ) and the upper bounds for the non-specific classes ( forest , non -forest , v e g etati on ). Then, the class (both specific and non-specific) with the highest value is chosen. Figure 5 shows that the upper bound for non -forest is the largest. EM is executed for the non -forest class until convergence (3 iterations) which obtains a log-likelihood value of approximately  X  50 . 2. Any non-specific class having an upper bound less than  X  50 . 2 can be pruned out because its EM converg-ing value cannot be higher. Thus, both forest and v e g etati on can be pruned out, leaving non -forest as the dominant class and is assigned to the given region. Notice that using this example, the Upper Bound based Context-Inclusive approach can obtain the same answer as in the Context Exclusive (CE) approach in only three iterations rather than 83 in CE. 4 Analytical evaluation This section presents an analysis on obtaining and using an upper bound for the multi-scale multi-granular image classification. First, we prove an upper bound (Lemma 1 ) for the can-didate models in our problem. Second, we present the proof of correctness for the Upper Bound based Context Inclusive approach (Theorem 1 ). 4.1 Upper bound Lemma 1 The pixel-wise sum of the lo g ( max likelihood function (( x | M ) . Formally, where b x , S = ma x c  X  S g ( x | c ) .
 Proof Based on ( 1 ), note that Since c  X  S  X  c g ( x | c ) is a convex combination [ 18 ] of densities g ( x | c ) . Let us revisit the log-likelihood ( 4 ) and rewrite the right hand side using ( 1 ) Combining ( 11 ) and the inequality in ( 10 ), the conclusion follows.
 The density defined in ( 1 ) is a convex combination of the densities in its sum. Therefore, This bound is a function of x and of S . The bound for the log-likelihood = lo g L as since the logarithm function in monotonic increasing which is a model-specific bound. A bound over all models is
For example, suppose we want to determine the upper bound for Forest in Table 2 for the given likelihood values. First, we would find the maximum value between the two specific classes ( coni f er and hard w ood ) for each pixel (Table 6 ). Then, we would take the sum of each value in Table 6 which will give the upper bound for Forest . Thus, the upper bound for Forest will have a likelihood of 3.4 or a log-likelihood of 1.2238. Recall that in Sect. 2.2 ,the likelihood for Forest after executing EM is 1.17162 or the log-likelihood of 0.1584 which shows that the upper bound found in this example is correct. 4.2 Proof of correctness of upper bound based context inclusive approach Theorem 1 The Upper Bound based Context Inclusive approach (UBCI) is correct such that each pixel in the image is classified with the maximum likelihood or best candidate class from the user-defined concept hierarchy.
 Proof Based on Algorithm 3 , an upper bound is assigned to each non-specific class candi-date. The candidate with the highest upper bound is chosen first and the converging value based on EM is assigned as the initial best candidate (Lines 2 X 4 of Algorithm 3 ). If the upper bound for every other candidate is less than the current best candidate X  X  likelihood, then the candidate will not ever have a higher likelihood value and can then be pruned from the list (Lines 5 X 8 of Algorithm 3 ). All other candidates having a higher upper bound than the current best candidate may have a higher actual likelihood value (from EM), which is then assigned as the best candidate (Lines 10 X 14 of Algorithm 3 ). Thus, the best correct candidate will be retrieved from the candidate list. Also, the output of the UBCI and the Context Exclusive approaches are identical. 5 Experimental evaluation The goal of our experiments was to compare the Upper Bound based Context-Inclusive approach with the Context-Exclusive approach in terms of computation efficiency. Compu-tation efficiency is measured at a physical (computation time) and at a logical (EM Iterations) levels. Figure 6 provides a schematic representation of our experimental design. All experi-ments were performed on two different datasets: (1) A synthetic input image having the size of, 7 total class, and 3 general classes (Fig. 2 ); and (2) A real dataset of Plymouth County, Massachusetts having 12 total classes, and 4 general classes. Both images have the size of 64  X  64 where multiple image sizes were analyzed (i.e., 16, 64, 256, 1,024, 4,096 pixels). Outputs were obtained for varying the number of pixels that was analyzed by each proposed approach. All experiments were performed on an UltraSparc III 1.1 GHz processor with 1 GB of RAM.
 5.1 EM iterations As discussed in Sect. 1 , evaluation of the quality measure takes up most of the time. Hence the number of iterations in EM may be used to evaluate the computation of the Upper bound based context-inclusive and context-exclusive algorithms. Experimental results of both approaches at multiple spatial scales on the synthetic and real datasets are shown in Figs. 7 and 9 , respectively. As the number of pixels increase, the number of iterations increases as well because of the larger size of regions. Note that the real dataset has more general classes than the synthetic, and thus is more computationally expensive. Experi-mental results show that the number of iterations is less for the Upper Bound based con-text-inclusive approach because several non-specific classes may be pruned due to its upper bound being less than the current best class. Compared to the context-exclusive approach, at the largest number of pixels, the number of iterations with the upper bound context-inclusive approach reduces by 57.2% for the synthetic and by 40.4% for the real dataset (Figs. 8 , 9 ). 5.2 Computation time Figures 8 and 10 provides the execution time taken for the synthetic and real datasets, respec-tively. The computation time was measured using MATLAB 7 and using the functions  X  X lock X  and  X  X timeof X  to capture the elapsed time for both the Context-Exclusive and the Upper Bound based Context-Inclusive approaches. Since the number of iterations taken by the upper bound based context-inclusive approach is less than that for the context-exclusive approach, the execution time for the context-inclusive approach will also be less. Since the number of models that are evaluated by the upper bound based approach will likely be less than the context-exclusive approach. As the number of pixels in an image increases the number of regions that need classification also increases. This results in the need of more non-specific classes that need to be analyzed resulting in the significant increase in the context-exclusive approach, whereas in the upper bound based context-inclusive approach, several models are pruned resulting in a much lower execution time. At the largest image size, the execution time for the upper bound based context-inclusive approach, as compared to the context-exclusive approach, is reduced by 40 and 38.3% for synthetic and real datasets, respectively. 6 Discussion and future work We presented an upper bound based context-inclusive approach (UBCI) that utilizes an upper bound for each candidate model to allow for pruning to obtain efficient and correct results. Also, we proved that an upper bound exists for each candidate model and experimentally showed our dominance over the context exclusive approach. Our approach is based on set context where candidates are evaluated together to obtain the optimal candidate model. Other types of context may be explored such as spatial context i.e., the correlation of a variable with space [ 16 ], may be used. Classification of spatial data based on an extended regression model called the Spatial Auto-regression model (SAR) is provided in [ 17 ].
 As discussed, most of the execution time is spent in calculating the quality measure using EM. EM is used to find the best candidate model; more specifically, EM is used to find the best Gaussian mixture model in the case of land-use classification. We plan to explore faster implementations of EM such as [ 6 , 7 , 14 ].Largerdatasetsmaybeusedtoextendthe experimental evaluation along with alternative measures such as I/O costs. Also, further opti-mizations of the UBCI may be explored such as ordering the non-specific classes based on their upper bounds.
 References Author Biographies
