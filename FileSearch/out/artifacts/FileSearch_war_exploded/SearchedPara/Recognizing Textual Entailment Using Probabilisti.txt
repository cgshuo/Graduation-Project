 For the natural language, a common phenomenon is that there exist a lot of ways to express the same or similar meaning. To discover such dif-ferent expressions, the Recognising Textual En-tailment (RTE) task is proposed to judge whether the meaning of one text (denoted as H ) can be in-ferred (entailed) from the other one ( T )(Dagan et al., 2006). For many natural language processing applications like question answering, information retrieval which involve the diversity of natural lan-guage, recognising textual entailments is a critical step.

PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006) have witnessed a variety of excellent systems which intend to recognize the textual entailment in-stances. These systems mainly employ  X  X hallow X  techniques, including heuristics, term overlap, syntactic dependencies(Vanderwende et al., 2006; Jijkoun and de Rijke, 2005; Malakasiotis and Androutsopoulos, 2007; Haghighi et al., 2005). As Hickl (2008) stated, the shallow approaches do not work well for long sentences for the missing of underlying information which needs to be mined from the surface level expression.

Recently, some deep techniques are developed to mine the facts latent in the text. Hickl (2008) proposed the concept of discourse commitments which can be seen as the set of propositions in-ferred from the text, and used a series of syntax-level and semantic-level rules to extract the com-mitments from the T -H pairs. Then the RTE task is reduced to the identification of the commitments from T which are most likely to support the infer-ence of the commitments from H . From the work of Hickl (2008), we can see that a deep under-standing of text is critical to the RTE performance and discourse commitments can serve a good me-dia to understanding text. However, the limitation of Hickl (2008) X  X  work is, the extracted discourse commitments are still from the original text and do not explore the implicit meaning latent behind the text.

Another kind of deep methods involves first transferring natural language to logic represen-tation and then conducting strict logic inference based on the logic representations (de Salvo Braz et al., 2006; Tatu and Moldovan, 2006; Wotzlaw and Coote, 2013). Through logic inference, some implicit knowledge behind the text can be mined. However, it is not easy to translate the natural lan-guage text into formal logic expressions and the translation process inevitably suffer from great in-formation loss.

Through analysis above, in our work, we pro-pose to use the predicate-argument structure to represent the extracted discourse commitments. Inspired by the work of (Mintz et al., 2009), we make use of the external knowledge YAGO and borrow the distant supervision technique to mine implicit facts for the extracted predicates. For ex-ample,
Ayrton Senna was married to a doctor who lives
We translate this example into the predicate-argument structures such as bemarried(Senna, doctor), livein(doctor, Austin), captial(Austin, Texas). Then through distant supervision, we can get some new facts livein(Senna, Austin), livein(Senna, Texas).

To judge the confidence of the new facts, we construct a probabilistic network with all the facts and adopt the Markov Logic Network (MLN) to calculate the probability of each new fact, which can be further used to recognize text entailments. To make full use of the underlying information in sentences and lessen the effect brought by natu-ral language X  X  vagueness, we design a RTE sys-tem which is composed of three stages, as shown in Figure 1.

First, we decompose the sentences in T -H pair to a series of discourse commitments as Hickl (2008) did. Since the syntax of these commit-ments are very simple, we can directly transform them to predicates (or 3-arg tuples). Then we use YAGO, a large semantic database including sev-eral thousands relations, to provide distant super-vision. The predicates in T are matched to YAGO facts due to some metrics. At last, we use the Markov Logic Network(MLN) (Richardson and Domingos, 2006) to infer the correctness of the predicates in H . The MLN is constructed us-ing the inference rules ( X  X oft X  rules) generated by AMIE system (Gal  X  arraga et al., 2013) on YAGO. Each rule has a weight which should be trained by real world facts. Using this framework, we can ap-ply the  X  X oft X  logic to the textual entailment recog-nition task. 2.1 Extracting Predicates from Sentences Discourse commitment is our baseline system pro-posed by Hickl (2008) which can decompose one sentence to a series of shorter and simpler sen-tences which completely contain the origin sen-tence X  X  information. One of the advantages of discourse commitments is that it can use a lot of syntax-level and semantic level rules to extract the underlying information of one sentence. For ex-ample, the following T -H pair can be decomposed as Figure 2. The discourse commitments of T con-tains the information: Ayrton Senna married in 1998, which is not easy to be captured by  X  X hal-low X  methods.
 T : Ayrton Senna was married to a doctor who H : Ayrton Senna lives in Texas.

Since we need to infer new facts using the ex-tracted commitments, we transfer all the commit-ments to predicates. For example, the commit-ments in Figure 2 can be transformed to the pred-icates (or triples) shown in Figure 3. We use R E -VERB (Fader et al., 2011) to extract the triples (predicate + 2 Arguments). To make the infer-ence process in the next section more convenient, we order that all of the arguments should be NPs. Therefore, we check if the arguments in the triples contain or have overlap with any of the NPs, re-place it with that NP, and the predicates are suc-cessfully extracted. 2.2 Distant supervision with YAGO The goal of distant supervision is to use the knowl-edge of YAGO (Mahdisoltani et al., 2014) to help the textual entailment recognition task. The facts in YAGO have various type of connections with each other. We think this connection is very useful for RTE. Therefore, the predicates in the T -H pair should be matched to the YAGO facts for making advantage of the connection information.

Since YAGO is very large, the common predi-cates can easily matched to YAGO facts in most cases. However, YAGO cannot contain every predicate in T . We run DIRT (Lin and Pan-tel, 2001) system on 1GB text random sampled from Gigaword corpus and for each predicate we choose the top-10 similar predicates as its synony-mous predicate. If the origin predicate cannot be found in YAGO, we instead check for the top-10 similar predicates. If we still cannot find a match, that means this predicate has very little connection with other predicates and cannot be supervised by YAGO. 2.3 Probabilistic Inference The goal of MLN (Richardson and Domingos, 2006) is to implement the probabilistic inference, or  X  X oft X  logic inference. MLN is constructed by an inference rule base. Each rule has a weight which needs to be well trained by real word facts. We use AMIE to mine inference rules from YAGO.

AMIE 1 (Gal  X  arraga et al., 2013) is a state-of-the-art inference rule mining system. The motiva-tion of AMIE is that KBs themselves often already contain enough information to derive and add new facts. If, for example, a KB contains the fact that a child has a mother, then the mother X  X  husband is most likely the father. AMIE can mine such inference rules from large KBs. The inference rules can be directly used for constructing Markov logic network in the next sec-tion. In addition, the process of mining inference rules is quite efficient so that it is very helpful for our RTE task.

We use AMIE only to extract the inference rules. After the inference rules are prepared, we can construct a MLN. We give the related facts in YAGO to the MLN, then the weights of each infer-ence rule can tune to a best fit for these facts. After the weights of each inference rule are well trained, the MLN is well prepared to use. Given the pred-icates in T , we first select all the related rules to construct a simple MLN, and then give the MLN some facts. After that, the MLN will calculate the probabilities of the unknown new facts. The ar-guments of the new facts are the permutations of all the ground atoms (or entities). For example, if we give the facts  X  X asChild (Cliton, Chelsea) X  and  X  X sMarriedto (Cliton, Hillary) X , the MLN will out-put the probability of  X  X asChild (Cliton, Hillary) X ,  X  X asChild (Hillary, Chelsea) X ,  X  X sMarriedto (Cli-ton, Chelsea) X , etc. Obviously, the probability of  X  X asChild (Hillary, Chelsea) X  may be the highest, so that it is most likely to be true. The MLN constructing and inferring can be implemented by Table 1: Performance of Inference based RTE for statistical relational learning and probabilistic logic inference, based on the Markov logic repre-sentation.

After the new facts are inferred, we check if these facts can cover the predicates in H . If so, we decide T entails H . We evaluate the performance of our framework datasets, which has 1600 examples. We use the YAGO2 for aligning predicates and mining infer-ence rules. YAGO2 contains more than 940K facts and about 470K entities. We run the AMIE sys-tem on YAGO2 for only one time to get all infer-ence rules (about more than 1.8K in total). For each T -H pair, we only choose a portion of re-lated inference rules to construct MLN. The cho-sen rules must contain at least one predicate which occurred in the predicates of T -H pair. We only use the MLN to infer when the discourse commit-ment paraphrasing cannot identify a T -H pair as  X  X ntailment X , which is a back-off method.

We compare our result with 5 baseline systems: (1) Zanzotto and Moschitti (2006) X  X  simple term-overlap measure, (2) MacCartney et al. (2006) X  X  semantic graph-mapping approach, (3) Hickl et al. (2006) X  X  classification-based term alignment ap-proach. (4) Hickl (2008) X  X  discourse commitment based Alignment, (5) Tatu and Moldovan (2006) X  X  strict logic based method. The comparison of the 5 baselines and our framework is shown in Table 1. Since we only need to judge  X  X es X  or  X  X o X  for the 1600 examples, the precision is equal to the recall, so that we only report the precision.

According to the Table 1, the performance of our framework is higher than Hickl (2008) X  X  base-line, which is significant (Wilcoxon signed-rank test, p &lt; 0 . 05 ). The reason is that we have added the inference portion to Hickl (2008) X  X  method. Therefore, some T -H pairs which had to be judged by semantic reasoning can be corrected by our framework. For instance, T is  X  X ughes loved his wife, Gracia, and was absolutely ob-sessed with his little daughter Elicia. X  and H is  X  X racia X  X  daughter is Elicia. X  It is not easy for the former baselines to recognize this entailment, but our framework can easily recognize it to be  X  X rue X . In this way, our framework has achieved a higher result. Textual Entailment Recognizing (RTE) task has been widely studied by many previous works. Firstly, the method based on similarity and over-lap (Malakasiotis and Androutsopoulos, 2007; Ji-jkoun and de Rijke, 2005; Wan et al., 2006). This kind of methods can help solve the paraphrase recognition problem, which is a subset of RTE. Another important similarity-based method is tree kernel (Zanzotto and Moschitti, 2006), which rely on the cross-pair similarity between two pairs ( T 0 , H 0 ) and ( T 00 , H 00 ) .

Secondly, some approaches extract the knowl-edge in T -H pair and check if the knowledge in T contains the knowledge in H . Hickl (2008) trans-formed the T -H pair into discourse commitments, reducing the RTE task to the identification of the commitments from a T which support the infer-ence of the H . Other works map the text to logical meaning representations, and then strict logic en-tailment methods, possibly by invoking theorem provers.

Thirdly, some works make use of statistical classifiers which leverages a wide variety of fea-tures. The language expression of each T -H pair are represented by a feature vector  X  f 1 , f 2  X  X  X  f m  X  . The feature vector contains the scores of different similarity measures applied to the pair, and possi-bly other features.

There are also other works based on predicate-argument representations with Markov Logic for RTE, such as Rios et al. (2014) and Beltagy et al. (2013). However, they did not use discourse com-mitments to extract predicate-argument triples, which may lead to severe information loss. This paper introduced a new framework to solve the Textual Entailment Recognizing task. This framework makes full use of Markov logic net-work for probabilistic inference. We hold that probabilistic inference is better than strict logic method since transforming from language form to strict logic form could lose a lot of informa-tion. Therefore it is extremely hard for the theo-rem provers to perform well.

In addition, we use YAGO database for distant supervision. The predicates extracted from T -H pair are first aligned with YAGO. If it succeeds, the inference procedure of MLN will become much more accurate. In addition, the inference rules for constructing MLN are also extracted from YAGO database using AIME system.

This framework can correctly recognize the en-tailment T -H pairs which must be judged using inference. This is our improvement over the pre-vious work.
 This research is supported by National Key Basic Research Program of China (No.2014CB340504) and National Natural Science Foundation of China (No.61375074,61273318). The contact authors of this paper are Sujian Li and Baobao Chang.

