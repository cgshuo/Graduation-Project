 In this paper, we use query-level regression as the loss function. The regression loss function has been used in pointwise methods, however pointwise methods ignore the query boundaries and treat the data equally across queries, and thus the effectiveness is limited. We show that regression is an effective loss function for learning to rank when used in query-level. We propose a method, namely ListReg, to use neural network to model the ranking function and gradient descent for optimization. Experimental results show that ListReg significantly outperforms pointwise Regression and the state-of-the-art listwise method in most cases. H.3.3 [ Information Search and Retrieval ]: Retrieval Models Algorithms, Experimentation, Performance. Learning to Rank, Loss Function, Query-Level, Regression. Learning to rank [3] methods can be categorized into the following three groups: pointwise, pairwise and listwise approaches according to their different input spaces. The listwise approaches have been paid more and more attention for that all IR evaluation measures are defined at the query level. Listwise approaches can be divided into two types: using query-level loss function and directly optimizing evaluation measures. In this paper, we propose a novel listwise approach based on the first type. In prior work, Cao et al [1] proposed to use Top k Probability based on Plackett-Luce model [5] to represent the listwise instances. Assuming there are documents for a given query , the number of permutation elements of the model is of order of of distribution and gradient descent as optimization algorithm proposing ListNet. Similarly, Xia et al [3] used likelihood loss based on Plackett-Luce model proposing ListMLE. Qin et al [4] proposed RankCosine using boosting algorithm to optimize cosine loss. To train a good ranking model, a robust loss function is needed. In this paper, we show how to use a robust and effective loss function of order to improve the retrieval performance. Lan et al. [2] proposed a query-level ranking framework, within which the query-level loss is defined. In the listwise approach, each query is represented by a group of documents and thei r ground truth labels, and queries are assumed to be i.i.d. random variables. Let be the input space and b e the output space, then space according to an unknown probability distribution. The query-level (also named listwise) loss is defined as follows: Here is the objective ranking function, is the current query, and is the permutation of given according to the ir ground truth relevance . In our approach, we use the query-level regression (specifically square loss) as listwise loss function. Regression has been used in pointwise approach, in which the loss on a single sample is given by , and the training goal is to minimize the total losses of all samples in which the queries boundaries are obviously ignored. The query-level regression loss for query is given as follows: Here we assume that there are documents for query , and the ground truth label for document is . We denote the output for document given by ranking function is . The objective ranking function can be obtained by optimizing the query-level regression: here we use neural network as model and choose linear ranking i.e. the features number of document . We use gradient descent for optimization and refer our method as ListReg. The gradient of the loss function with respect to parameter can be calculated as follows: where if we denote the t -th weight of and . Eq. 4 is then used in Gradient Descent for updating weight vector . Algorithm 1 shows the learning algorithm of ListReg.
 The query-level loss of query is a sum of objects X  regression losses, the time complexity is of order . Also note that the calculation of in ListReg is much simpler than that of ListNet, for more detail, [1] can be referred. We use three datasets published in LETOR3.0 [4] package: OHSUMED, TD2003 and TD2004 and use Regression and ListNet as baselines. We perform 5-fold cross validation experiments, where in each fold 3/5 is used for training, 1/5 for testing, and the left 1/5 for validation. Models are trained using the training set, tuned on the validation set and tested on the testing set. For evaluation purpose, we use three widely used IR evaluation measures: precision (P@k) and normalized d iscounted cumulative gain (NDCG@k) and mean-average precision (MAP). The initial learning rate of ListReg is set to be 1E-4 on OHSUMED and 1E-5 on TD2003 and TD2004 respectively, and the dropping rate is set to be 0.5 on all datasets in the experiment. It can be observed from Table 1 that our method ListReg significantly outperforms Regression on all metrics on all three datasets except in terms of P@5 on TD2004 (which is comparative nevertheless). The reason is that there are great diversities between different queries, but Regression is of pointwise style, whose loss function is based on document, treating different queries equally, however, ListReg is a query-level method (of listwise style), and it can capture the characteristics of different queries. Accordingly ListNet also outperforms Regression in most cases. It can also be observed that ListReg outperforms ListNet in most cases, especially on the top P@k and NDCG @k categories. For example, ListReg outperforms ListNet with 8.5% in terms of P@1 on OHSUMED and with 33.3% in terms of NDCG@1 on TD2004. This is quite meaningful in the scenario of real search engine where users usually care more about the top-most retrieved documents. Furthermore, the query-level regression loss function is much simpler than cross entropy, and the training time consumption for ListReg is less than ListNet. In this paper, we have proposed a complexity listwise approach using query-level regression as the loss function and gradient descent as the optimization algorithm. Experimental results show that by considering regression at query-level, ListReg significantly outperforms both the pointwise approach Regression and the listwise approach ListNet. In the future, we will investigate other loss functions in query-level style; we hope to find more appropriate loss functions both in time complexity and performance for information retrieval. 
