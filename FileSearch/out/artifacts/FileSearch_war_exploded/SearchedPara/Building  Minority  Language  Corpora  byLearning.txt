
Electronic text corpora are used for modeling language in many language technology applications including speech recognition ( Jelinek 1999), optical character recogni-tion, handwriting recognition, machine translation (Brown et al. 1993), and spelling correction (Golding and Roth 1999). They are also useful for linguistic and sociolin-guistic studies, as they are r eadily searchable an d statistics can easily be computed. cant human effort and are very time consuming. The Linguistic Data Consortium (LDC) has corpora for 20 languages (Liberman and Cieri 1998), while Web search engines currently perform language iden tification on about a dozen of the languages they index, allowing language-specific searches in those languages. Documents in many other languages are also indexed, though no explicit labeling of the language they are written in is available.

Resnik (1999) explored the Web as a source of parallel text and automatically con-structed a parallel corpus of English and French. In this paper we describe techniques that only require the user to give a handful of keywords or documents for automati-cally collecting language-specific resources from the Web and present a system that automatically generates Web search queries to construct corpora for minority lan-guages. Our proposed approach requires no human intervention once the system is provided with the initial documents and is a very cheap and fast way to collect corpora for minority languages from the Web.
 that find a wide range of documents in the target language and that iteratively filter out a large proportion of closely related languages. Our hypothesis is that by se-lecting appropriate inclusion and exclusio n terms from documents already collected, and by using the results of classification by a high-precision language filter, we can automatically construct very high-precision queries. This approach should work well without specialized knowledge about which languages are related.
 show that query generation can bring in a much higher proportion of documents in the target language than random crawling or use of a search engine X  X   X  X imilar
Documents X  option. We describe various methods for selecting query words and use online learning to modify the queries based on feedback by the language filter. We show that our system, initialized with a single document from the target concept, can learn to generate queries that can ef ficiently acquire a reasonable number of documents in Slovenian from the Web and that our approach also generalizes to other minority languages on the Web.
While search engines are an invaluable m eans of accessing the Web for users, auto-mated systems for learning from the Web have primarily been installed in crawlers, or spiders. A new generation of algorithms is seeking to augment the set of search capabilities by combining other kinds of topic-or target-directed searches. for specific documents with terms designed to find document genres such as home-pages and calls for papers. Rennie and McCallum (1999) use reinforcement learning to help a crawler discover the right kinds of hyperlinks to follow to find postscript re-search papers. Diligenti et al. (2000) make use of hyperlink structure to learn naive
Bayes models of documents a few backlinks away from target documents to aid a crawler. WebSail (Chen et al. 2000) uses reinforcement learning based on rele-vance feedback from the user. Our approach differs from WebSail in that we derive our learning signal automatically from a language filter and do not require any user input. Boley et al. (1999) proposed using the most frequent words for query gen-eration for their WebACE system, generating these from clusters and seeking to maximize term frequency and document frequency of the terms selected. They used stemmed versions of words as query terms. They showed by example that automati-cally generated queries with a combination of conjunctive and disjunctive terms can be used to find more related documents. They used queries that used a combina-tion of conjunctive and disjunctive terms. However, they did not evaluate a system employing automatic query generation.

In earlier work (Ghani and Jones 2000), we described an algorithm for building a language-specific corpus from the World Wide Web. However, our experiments were limited to a small closed corpus of less than 20,000 documents, vastly limit-ing the generalization power of the results to the Web. We showed that single-word queries were sufficient for finding documents in Tagalog and that selecting the query words according to their probabilities in t he current documents performed the best.
It is important to note that the experiments were run on a small corpus of Taga-log and other distractor documents collected from the Web and stored on disk. compare our earlier best-performing methods against the query-generation methods and lengths presented in the current paper to find Tagalog and Slovenian documents on the Web and find that applying single-word term frequency and probabilistic term frequency queries to the Web for Slovenian results in relatively low precision and our odds ratio query-generation method described in Sect. 3.3 outperforms the proba-bilistic term frequency approach with single include-word and exclude-word queries.
Furthermore, using more words in the query (three for inclusion and three for ex-clusion) performs better than the single-word queries previously used.
In this section we describe the CorpusBuild er architecture, the query-generation methods, and the language filter used. The retrieved documents are labeled by an automatic language classifier as relevant or irrelevant, and this feedback is used to generate new queries.
CorpusBuilder iteratively creates new queries in order to build a collection of docu-ments in a single minority language. The target language is defined by one or more initial documents provided by the user and the language filter.

At a high level, CorpusBuilder is initialized by the user with two sets of docu-ments, relevant and nonrelevant. Given these documents, it uses one of several term-selection methods to select words from the relevant and nonrelevant documents to be used as, respectively, inclusion and exclusion terms for the query. This query is sent to a search engine, and the highest ranking document is retrieved, passed through the language filter, and added to the set of relevant or nonrelevant documents according to the classification by the filter. The process is then iterated, updating the set of documents that the words are selected fro m at each step. When querying the search engine with a new query, only the first hit is used but the remaining hits are stored to a file for efficiency to avoid requerying the search engine for the same query. we have used the same query before, w e take the next unseen hit from our cached results. If all hits have been seen, no hit is returned.
 1. Initialize frequencies and scores based on relevant and nonrelevant documents. 2. Generate query terms from relevant and nonrelevant documents. 3. Retrieve next most relevant document for the query. 4. Language filter, assign document to relevant or nonrelevant set. 5. Update frequencies and scores based on relevant and nonrelevant documents. 6. Return to step 2.
 tunities to improve performance using the language filter at every step. Interesting future work would involve investigating how the number of documents retrieved before updating models could be optimized, possibly by examining the number of positive documents returned so far by the current query.
We only describe initialization for the relevant class, which is used for selecting inclusion terms for the query. The operation for the nonrelevant class and exclusion terms is performed identically.
 for the positive class. We also experimented with using a single initial document as well as with using only ten keywords instead of an entire document.
Given a collection of documents classified into relevant and nonrelevant classes, the task of a query-generation method can be described as follows: examine current rel-evant and nonrelevant documents to generate a query that is likely to find documents that are similar to (but not the same as) the relevant documents (i.e., also relevant) and not similar to the nonrelevant documents.

If we think of the Web as a collection of documents in various languages, we can view this process as sampling without replacement from this collection. The query-generation techniques and a search e ngine are used to sample most effectively and efficiently. Ghani and Jones (2000) e xperimented with sampling with and with-out replacement on a subset of the Web and found that both strategies performed similarly for their dataset.

We construct queries using conjunction and negation of terms (literals). A query is defined as consisting of a set of terms required to appear in the documents re-trieved (inclusion terms) and a set of terms forbidden from appearing in the docu-ments retrieved (exclusion terms). Cons equently, each query can be described by four parameters: the number of inclusion te rms, the number of exclusion terms, the inclusion-term-selection method, and the exclusion-term-selection method. This con-trasts with full Boolean queries, which give greater expressive power by also em-ploying disjunction and negation with a greater variety of scope. We chose to use only conjunction to simplify the experiments.

The term-selection method selects k inclusion and k exclusion terms using the words that occur in relevant and nonrelevant documents. Since our task does not have a fixed goal in terms of a single best query, we need query-generation methods that adapt to the current situation where we have already acquired a set of documents from the target concept and do not want to explore the same space again.
The query-generation methods we use are as follows: uniform , term frequency , probabilistic term frequency , rtfidf , odds ratio ,and probabilistic odds ratio . Each is described below for inclusion terms with k being the number of terms to be generated. The operation for exclusion terms is analogous, swapping relevant and nonrelevant documents where appropriate.  X 
Uniform (UN) selects k terms from the relevant documents, with equal probability of each term being selected. We use this mehod as a baseline.  X 
Term frequency (TF) selects the k most frequent terms from the relevant docu-ments. Scoring terms according to their frequency (TF) has been known to give good results for feature scoring in document categorization (Yang and Peder-sen 1997; Mladenic and Grobelnik 1999).  X 
Probabilistic term frequency (PTF) selects k words from the relevant documents according to their unsmoothed maximum -likelihood probabilities, that is, with probability proportional to their frequenc y. More frequent words are more likely to be selected. This technique was shown to perform better than simple frequency on a similar problem in earlier work (Ghani and Jones 2000).  X  rtfidf (RTFIDF) selects the the top k words ranked according t o their rtfidf scores.
The rtf score of a term is the total frequency of that term calculated over all rel-evant documents as classified by the language filter. The idf score of a term is calculated over the entire collection of documents retrieved and is given by rtf and idf. Haines and Croft (1993) show that RTFIDF is a good scoring mech-anism for information retrieval.
  X  Odds ratio (OR) selects the k terms with highest OR scores. The OR score for  X  Probabilistic odds ratio (POR) selects words with probability proportional to their guages can be seen in Table 7.
 issued previously either b ecause the method is probabilis tically selecting terms or because the addition of new doc uments did not change word distributions in a way that influences the term selection.
In the case of a deterministic term-selection method, such as TF, RTFIDF, and OR, query terms can only change when a new document is retrieved and the underlying document statistics changed. When a query adds no new documents, it will keep on issuing the same query, which will result in no new documents. In order for the system to recover from such a situation ( an empty query), we need a method of al-tering the query. We took the approach of successively incrementing a counter i ,first through inclusion terms and then through the exclusion terms, taking the i through i + k -th highest-scoring terms till a query was found that returned a URL.
After each query is generated, it is passed t o a search engine and the next matching document is retrieved. We pass each docum ent retrieved by a query through a lan-guage filter based on van Noord X  X  TextCat implementation (van Noord 1997) of Cavnar and Trenkle X  X  character n-gram-based algorithm (Cavnar and Trenkle 1994).
Cavnar and Trenkle show accuracy of ove r 90% on a variety of languages and docu-ment lengths. We considered a document to belong to the target language if that lan-guage was top-ranked by the language filter. To test the performance of the filter on Slovenian Web pages, we asked a native speaker to evaluate 100 randomly selected
Web pages from a list of several thousand classified as Slovenian by the language filter. Ninety-nine of these were in Slovenian, giving a precision of nearly 100%. An analogous evaluation for Web pages judged to be negative shows that 90% X 95% of the pages classified as non-Slovenian were actually non-Slovenian. All our results are reported in terms of this automatic langua ge classification. No additional manual evaluation was carried out.
We conducted exhaustive experiments compa ring the performance of all the term-selection methods (described in Sect. 3.3) while varying the length of the queries to gain insight into their relative performance. The minority concept we use for our experiments is that of Slovenian on the Web. These experiments used three different initial documents, and we found that the var iance in the results was small. The eval-uation measures we used were (a) percentage of documents retrieved in the target class ( PosDocs ) and (b) the number of documents in the target class per unique
Web query ( PosQueries ). The higher the value of these two metrics, the better we judge our methods to be. We compared t he term-selection methods according to these two performance measures for each length independently.

In the experiments reported below, the system was initialized with a single docu-ment from the target language and some documents from other languages. For ex-periments with Slovenian we supplied four negative example documents, one each in English, Czech, Croatian, and Serbian. I n all experiments, the language model for the language filter was also supplied. A summary of results for different query-generation methods is given in Table 1 and
Table 2, while detailed graphs comparing query lengths, query methods, documents retrieved, and queries issued are shown in Fig. 2. Odds ratio (OR) is consistently the best with respect to both evaluation measures. Observing the number of queries issued, (OR) finds the greatest number of target documents. In terms of the number of documents examined, OR is again the best (between lengths 1 X 3) except when all methods have about the same performance (length perform well in terms of the number of relevant documents returned per query. This is because longer queries are much more likely to return no documents at all. Even if all the words contained in the query are from the target concept, they are unlikely to co-occur in the same document. OR performed best for all query lengths except 5, where term frequency (TF) found many documents with few queries, contrasting with query length 1, where it found the least.

The comparison of different term-selection methods is given in Table 2. We re-port results after retrieving 3000 documents and for 1000 issued queries, unless the result is marked by  X  , where the values are given at a much lower number of docu-ments, for instance, for PO and UN on length 5, less than 100 documents and for length 10 less than 5 documents. This shows that the two methods are mostly is-suing queries that are very p recise (high percentage of documents returned in the target class) but most of the time do not return any documents (especially for longer queries).
 We found that each method performs best within a small range of query lengths.
For TF, the best performance is achieved with length 4 (when four terms are included and four terms are excluded). For PTF and OR length 3 gives the best results, while for POR using more than one include and one exclude term gives a very small number of the target language documents while using a very large number of queries. Figures 3 and 4 show comparison of different lengths for the TF, PTF, OR, POR,
RTFIDF, and UN term-selection methods.
As described in Sect. 4.1, different que ry-selection methods excel with different query lengths. For term frequency (TF), for example, the best performance is achieved with length 4 (when four terms are included and four terms are excluded). While the previous experiments permit us to see how different methods and query lengths perform in isolation, it is still possible that the best overall querying strategy would use one method and length initially and then change the ideal method and length as more and more documents in the target language are retrieved. As corpus construc-tion proceeds, our system may explore differe nt parts of Web/feature space and per-form better using different querying mechan isms. This observatio n motivates a fam-ily of algorithms that have access to the same term-selection methods as before and learn the ideal method and length at different points in time. We present a fam-ily of online learning algorithms in the next section and also report experimental results.
The query-generation module of CorpusBuilder selects which method to use for se-lecting query words and how many words to select. Our queries can then be described by the following four parameters: inclusion term selection method, exclusion term selection method, inclusion length, and exclusion length. Learning techniques that are aimed at optimizing this process should learn the ideal parameters for a given target concept and adapt to changing environments. Since the target concept is shift-ing (we always want previously unseen documents) and a query method that works well in the beginning in one part of the feature space may not work well later during the process (when the documents it finds have been exhausted), we incorporate some randomness in our learning methods. Instead of directly estimating or learning the four parameters for a query by maximizing some objective function, we focus on learning the success rate for each term-selection method  X  term frequency (TF), prob-abilistic term frequency (PTF), odds ratio (OR), and probabilistic odds ratio (POR)  X  and length (0 X 10). We impose a multinomial distribution over all methods and lengths (their probabilities being proporti onal to their success rates) so we can prob-abilistically (according to the multinomial distribution) se lect the parameter values to use in every iteration.
 possible. For this reason there is a tradeoff between exploration and exploitation.
A method that quickly finds a reasonable querying mechanism can then exploit that mechanism. However, an algorithm that s pends more time searching for a very strong querying mechanism may do better over the long term. Depending on whether our goal is to acquire as many documents as possible, a fixed number, or documents with sufficient vocabulary coverage, the ideal measurement statistic can vary.
The learning techniques we designed for our query-generation task vary the time horizon used in learning the query parameters: from all available history, to a time-decaying view of the past, to a learner firm ly rooted in the present. Since our target concept at every step is previously unseen documents in the minority class, the set of target positive documents is reduced at every step. Thus more recent queries may be more relevant and useful in generating the next query. At the same time, the aggregated knowledge from past queries may prove invaluable for learning about the task as a whole.

The implementation of each method fo r online learning (Blum 1996) of query-generation methods is described below.

This was designed to permit a successful querying method to continue as long as it kept finding positive documents. When the previously successful method fails, our learning algorithm selects a different method from the available ones (with uniform probability). The algorithm is as follows: upon success, otherwise it has a zero probability of being selected, with a uniform probability distribution over the remaining methods.
This method estimates each method X  X  future probability of success based equally on all past performance. We used two kinds of updating rules: additive update (LTA), outlined below, and multiplicative ( LTM), Winnow-like update using the Winnow-like update, we use various update mechanisms for the successful and unsuccessful methods. For the successful query-generation method m (either length or term-selection method) we use Score ( m )  X  = 1  X  , and for an unsuccessful length or scoring method we use Score ( m )  X  =  X  .
This algorithm uses historical information but gradually reduces the impact of learn-ing experiences further in the past. The algorithm is as follows:
For all experiments we set  X  = 0 . 9. Note that in step 2 we increase the scores of methods not used in that iteration in the event of failure.
The experiments conducted with the learning algorithms described in the previous section can be broken down into two categories. In the first set of experiments, we fix the length of the queries and allow the algorithm to vary the query-generation method. This allows us to observe the effects of various query lengths on the different query-generation methods. In the second set of experiments, we allow the learning algorithms to vary both the length of the query and the method used to select the query words.
In this experiment, we fixed the query length for both inclusion and exclusion terms to reduce the number of query parameters be ing estimated. This is shown in graph titles by  X  X F X  (assume methods are I ndependent and use F ixed query length). We ran experiments for each length separatel y and found that the best performance in terms of the number of target-language documents found was achieved by lengths 3 and higher. Looking at the results in terms of the number of queries used, lengths 1 to 3 were the best, closely followed by length 5, while higher lengths used many more queries for the same number of target-language documents. This is consistent with our previous experiments where we ev aluated the query-generation methods in isolation and found that longer queries were precise but returned very few docu-ments. From this we can conclude that the b est performance is achieved when using lengths 3 X 5 (Fig. 5). Shorter queries are more successful in getting documents but less accurate than the longer queries.
Our hypothesis here is that the online learning techniques presented in the previous section will converge toward values of the parameters that were shown to be supe-rior in our earlier experiments (Table 2). To test this, we recorded probabilities of various parameter values at each iteration f or different learning methods. The results in Table 3 show that odds ratio (OR) was preferred by all the learning methods for selecting inclusion terms and that the number of inclusion terms was typically lower than the number of exclusion terms (1 X 7 vs. 4 X 10). For selecting exclusion terms, probabilistic odds ratio (POR) was preferred in most cases.
 iterations of the long-term memory method LTA . Early in the process of learning,
LTA converges to an inclusion length of 3 and odds ratio for choosing the inclusion terms. For the other learning methods there is no such clear convergence. We compared different learning methods: memoryless ML , long-term memory LTA ,
LTM , and fading-memory FM described in Sect. 5.2. For methods involving rewards based on success ( LTA and FM ), we also tried a variant penalizing more for returning no document than for returning a negati ve document (negative documents give us some information to update our language model, while no document means wasted query). However, we did not observe substantial differences in their performance. formance since they use different time horizons in the online learning process. The performance is measured the same way as in experiments with the fixed-query param-eters described in Sect. 4, as the number of the target concept documents depending on the number of retrieved documents and on the number of queries issued as a way of capturing accuracy and efficiency.

When comparing different learning methods, LT and FM learning perform better than ML (Fig. 7). LT learning dominates both FM and ML in terms of the number of documents retrieved as well as queries issued. The difference is more evident when retrieving 1000 X 2000 documents and issuing 1000 X 4000 queries. However, all the learning methods underperform the best-performing combination of parameters (OR) using lengths 3 X 5) that we found by manually searching the parameter space exhaustively.
To examine how our experiments generalize to other conditions, we examined the ef-fects of varying the initialization conditi ons to a variety of documents and to a hand-ful of keywords. We also varied the target language from Slovenian to Croatian, Czech, and Tagalog. The results of th ese experiments are given below.
Since all the query-generation methods described in this paper derive the query terms from documents already found, it is important to consider the effect of varying the initial document used. We used three different initial positive documents in Slovenian. The properties of these initial documents are shown in Table 4.
 3 under different initial conditions are g iven in Fig. 8a. The other methods and lengths show similar behavior. It can be seen that the method performs compara-bly for all initial conditions, getting about the same proportion of target documents. At the time the results diverged, over 1000 positive documents had been found.
It is interesting to note that  X  X ormal X  and  X  X ews X  initial documents needed fewer queries to retrieve the first 200 documents, while the  X  X nformal X  initial document retrieved over 1000 target documents only after issuing about 700 queries. Experi-ments comparing learning starting with different initial documents and starting with the user-provided keywords show that all sets of words led to similar results. Fig-ure 8b shows the results for the LT approach; the other approaches exhibit similar behavior.
It may not always be easy to find entire documents in a language to initialize our system. As an alternative to starting with a positive document in the target language, we asked three native speakers of Slovenian to supply us with words. We asked three questions of each native speaker to elicit initial words of varying types. We asked for ten common words to obtain words a Slovenian may be first likely to think of.
We then asked for ten uniquely Slovenian words. Croatian and Czech, for example, share many strings and words with Slovenian. A native speaker of Slovenian may know words that are shared with other languages. They may, however, not know that some are shared with other languages. Finally, we asked for ten words useful for finding texts in Slovenian on the Web.

The words we obtained from native Slovenian speakers shown in Table 5 were used as mock initial documents and ten English stop words as the initial negative document. Figure 9 shows experimental results using fixed query parameters set to odds ratio with length 3 and learning query parameters using long-term memory with multiplicative update rule. All sets of wo rds led to similar results; moreover, the results are similar to those obtained when starting with the initial documents shown Fig. 8.

Our previous successful experiments with Slovenian do not provide any evidence that our techniques will work well with other languages or concepts. To test the hy-pothesis that our approach will indeed pe rform well on many languages, we repeat some of the experiments for Tagalog, Croatian, and Czech, which can all be con-sidered minority languages on the Web. Moreover, Slovenian, Croatian, and Czech are all Slavic languages and share many words, which makes our task more diffi-cult and provides evidence that our algorithms can indeed work with closely related languages.

TF and PTF for the same number of total documents examined, for all of Slove-nian, Croatian, Czech, and Tagalog. This s uggests that the superiority of this query-generation mechanism generalizes across languages. Table 7 and Fig. 10, however, show that the number of queries is dependent on the target language and method. This may reflect the number of Web pages that are confusable between Slovenian,
Czech, and Croatian when queries are made with frequent words, whereas Tagalog X  X  frequent words are more unique on the Web.
To test the generalization power of our learning methods for different target lan-guages, we performed experiments on two other natural languages, Croatian and
Tagalog, also representatives of minority languages on the Web. The results confirm that after 700 X 1000 documents and about 1000 queries issued, the methods start to differ on all three languages. The best performance is achieved by the long-term memory methods. On Tagalog, as on Slovenian, the Winnow-like update rule gives the best performance, while on Croatian its performance is the worst performing and the additive update rule is the best performing. In Fig. 11 we show the number of documents retrieved for experiments using Croatian and Tagalog.
Our approach performs well at collecting documents in a minority language start-ing with a few words or documents, but it does require a language filter for that minority language. There are filters available for quite a few languages, but this is a potential limitation of our approach. In earlier work (Ghani and Jones 2000), we experimented with constructing a filter on the fly, starting with the initial document and bootstrapping, and our expe riments with Tagalog yield ed encouraging results.
We plan to experiment with this idea further and use different methods of building language filters with small amounts of labeled data.
 corpus and verifying the decisions made by the language filter. Although it gives us information about the precision of our classifier and thus the corpus, it does not give any indication of the level of coverage obtained by our system. Since the number of total Web pages in the minority languages we have worked on so far is not available, we can use approximation techniques for evaluation such as using the intersection of Web pages found by using each of the three documents as seeds for our experiments.
We can also measure the rate at which new Slovenian documents are found as our experiments progress and a decreasing rate would impose a bound on the number of documents that can be found using our methods. Callan et al. (1999) and Ghani and Jones (2000) use various measures like percent vocabulary coverage and ctf to evaluate the coverage of their language m odels. Although our task is not to construct a language model for Slovenian and we do not have the  X  X rue X  model to compare against, we can still use these measures and calculate the rate at which we add new vocabulary words and the distribution of the words we have in our vocabulary.
Since we are sampling in the space of Slovenian words, convergence of the sampled distribution would indicate a reasonable coverage.
 queries that are highly accurate and cove r different parts of the language space we are dealing with. There are several other ways that we could collect corpora from the Web, and we discuss the merits and drawbacks of some of the other approaches below. words to query and then iterating a few times to eliminate function words that are shared with other languages is one approach that would work well, but it would require considerable domain knowledge in the form of knowledge about function words for the particular language. Our approach learns from examples in that it only requires a handful of documents and requires very little domain knowledge. countries where that language is spoken. For example, in the case of Slovenian, we could spider all Web pages in the .si domain. This would not result in good coverage or precision since not all pages under the .si domain are in Slovenian and there are
Web pages outside the .si domain that are in Slovenian. In our experiments, we found approximately 31,000 Web pages under the .si domain, 24% of which were classified as not in Slovenian by our classifier. Similarly, out of the 30,000 Slovenian documents we found, 22% of them were not under the .si domain.
 starting from the Regional  X  Countries  X  Slovenia category in Yahoo! (or the category of the country where the language is used). This again would not be accurate because Yahoo! does not have an exhaustive list of Slovenian Web sites in general and of Web pages in Slovenian in particular.
 functionality of looking up  X  X imilar X  Web pag es that some search engines provide.
The details of the comparison are as follows.
Search engines provide functionality to allow users to find similar pages based on both content and hyperlink information (Dean and Henzinger 1999). We hypothesize that our system can outperform the  X  X elated pages X  feature, and to test our hypothesis we ran an experiment using AltaVista X  X   X  X elated Pages X  function. We start with the same initial documents as in the systematic Slovenian experiments described earlier and use CorpusBuilder query generation, setting include and exclude lengths to 5 and the term-selection method to odds ratio (OR). Then, for each AltaVista hit classified as relevant by our language filter, we query AltaVista using like:URL to find documents  X  X ike X  the one we retrieved, then continue to repeat this process with all of the relevant ones, finding more  X  X ike X  them each time.

The algorithm is as follows:
We u s e d generate_query(odds ratio, 5, odds ratio, 5) as this method had been performing well in other experiments. We find that OR outperforms More-
LikeThis, as can be seen in Fig. 12. MoreLikeThis finds a document in the target lan-guage around 33% of the time, while using a query based on the OR term-selection method with five terms each for inclusion and exclusion finds a document in the target language around 90% of the time.
Ghani and Jones (2000) showed that single-word queries were sufficient for finding documents in Tagalog and that selectin g the query words according to their prob-abilities in the current documents performed the best. It is important to note that their experiments were run on a small corpus of Tagalog documents and other dis-tractor documents collected from the Web and stored on disk. best-performing methods against other query-generation methods and lengths on the tasks of finding both Tagalog and Slovenian documents on the Web.
 to the Web for Slovenian results in relatively low precision, as shown in Fig. 13.
Using the odds ratio query-generation method described in Sect. 3.3 outperforms the probabilistic term frequency approach with single include-and exclude-word queries.
Furthermore, using more words in the query (three for inclusion and three for ex-clusion) performs better than the single-word queries previously used.
 guage, we performed the same evaluation on Tagalog. Although fewer documents were found overall, the trend remained the same, with OR-3 finding over 600 docu-ments in Tagalog after examining 1000 documents, while OR-1 had found just over 300, PTF-1 had found 122, and TF-1 had found only 17 documents in Tagalog, as showninFig.13.
Up till now we have been estimating parameters separately for each length and method by assuming that they are inde pendent of each other. We can formalize these assumptions by proposing four different ways of parameterizing query lengths, which differ according to two kinds of i ndependence assumpti ons and combinations of these: is independent of the query-generation method used. This assumption means that fewer parameters need to be estimated. In our experiments we use this assumption of independence; experimentation with the other models is part of future work. of one another. This is the multinomial que ry length model and uses more parameters (one per candidate length). This setting is denoted in our experiments by II . Another model requiring fewer parameters matches our intuition that similar length queries are likely to perform similarly. To this end we parameterized query lengths as a Gamma distribution, denoted here by ID .

We can summarize the use of the indepe ndence and dependence models as fol-lows:
The abbreviations will be used when describing these combinations. In the ex-periments in this paper, we only used II and ID. Experimentation with the other models is part of future work. The previous experiments reported results using the
II model for the parameters. We ran the same experiments again using the ID model for the learning methods using both variants of long-term memory (LT) and fading memory (FM). Using the ID model actually hurts the performance of FM method and does not result in any improvement for the LT method when compared to the II model. Actually, after about 5500 queries , the LT method using the additive update rule LTAID catches up with LTMII.
We presented an approach for automatically collecting Web pages in a minority lan-guage and showed that it performs well on several natural languages (Slovenian,
Croatian, Czech, and Tagalog) and only r equires the user to supply a handful of documents (or keywords).

We described a variety of term-selection measures and found that our odds ratio query construction method outperforms others and also performs better than using the existing  X  X elated Pages X  function in AltaVista. Odds ratio picks inclusion query terms that are highly unique to the target language while excluding terms that are unique to nonrelevant languages. Since this is the only method that uses both relevant and nonrelevant documents simultaneously to select query terms, we believe that this property is the key to its success. Term frequency picks terms that are frequent in the target language but not necessarily unique and hence results in queries that are not as precise as those generated by odds ratio .

We also presented a series of online learning algorithms that are able to choose from different query-generation methods and vary the number of words in a query.
We found that memoryless learning performs worse than all the other learning methods over different natural languages. This was expected since memoryless learn-ing is a naive algorithm that persists with a successful mechanis m until it fails and then switches to another one randomly, thus ignoring past knowledge of success rates.
The best performance was achieved using long-term memory learning with either the multiplicative or the additive update rule. I t accumulates all the information from earlier queries by counting the successes and failures of individual mechanisms and updates their scores accordingly.

In experiments investigating length parameterization, we found that using a Gamma distribution hurts the performance when long-term memory is used for learning. Fur-ther experimental work is needed to draw solid conclusions about the consequences of the length independence assumption used in this work. We plan to consider using more than one document at each iteration so tha t the learning process can be faster, although the increased speed could come at the expense of precision.
 as collecting documents about some topic or getting documents that match a user profile. We conducted some preliminary experiments for collecting course Web pages starting from a handful of examples, and we plan to pursue collecting topic-specific corpora in future work. Using these techniques to augment existing techniques for developing a domain-specific search engine is also an interesting future direction. we are able to build a very accurate, but expensive, classifier. We can then use the search engine as a simpler filtering mechanism, bringing in only those documents likely to satisfy our classifier, thus saving us the expense of running the classifier on all documents retrieved by the crawler. The kinds of high-precision classifiers that would benefit from this approach include systems with parsers, systems that fetch more Web pages to decide on the classifi cation of a given Web page, and systems that involve user input, thus making our approach applicable in various useful and important areas.

