 We have developed a system that offers comprehensive anal-ysis functionality for information retrieval experiments com-bined with a storage facility for persisting experiment dat a in a uniform fashion to facilitate repeatability and compar a-bility of experiments. Our Service-Oriented IR Evaluation Framework  X  SOIRE offers a number of technological in-terfaces based on open networking standards and modeling languages to connect to other systems while regarding ease-of-use for researchers as the feature of utmost importance. H.3.3 [ Information Storage and Retrieval ]: Miscella-neous; H.3.4 [ Information Storage and Retrieval ]: Sys-tems and Software X  Performance evaluation (efficiency and effectiveness) Algorithms, Experimentation, Performance information retrieval evaluation, experimentation, serv ice-oriented architectures
Open networking standards like HTTP and XML-based specifications such as SOAP (Simple Object Access Proto-col) and WSDL (Web Service Description Language) have facilitated the development of distributed software archi tec-tures consisting of independent components called Web Ser-vices. Interestingly, especially researchers in various a reas of natural sciences have spawned projects using these tech-nologies within scientific experimentation platforms such as the Software Environment for the Advancement of Schol-arly Research (SEASR) [5], Taverna, a free software tool for designing and executing workflows [3], in combination with the social Web site myExperiment.org, or the open source scientific workflow system Kepler [6]. These projects are intended to enable sharing and combining scientific results across laboratory boundaries and are prepared for making use of the computing power of current and emerging grid and cloud computing facilities.

Even though IR researchers collaborate and compete in evaluation campaigns such as TREC, CLEF, FIRE, INEX or MIREX, tools or even platforms using such technologies supporting collaboration are rare. Quite recently however , the system EvaluatIR has been put online [1]. The Xtrieval system [4] is a platform that has initially been developed for participating in the CLEF campaing but is currently being extended for a wider use. The project Networked En-vironment for Music Analysis (NEMA) 1 aims at creating a collaboration infrastructure for experimentation in the a rea of music analysis and retrieval.

We have developed SOIRE, a Service-Oriented IR Evalu-ation Architecture, as an attempt to leverage Web Service technologies in the field of information retrieval to foster the comparability of IR experiment results and collaboration o n and exchange of data, methods and results. Because of the immanent importance of evaluation, the implementation of services offering a comprehensive of IR measures, statisti-cal tests for comparison of results, and a facility for stori ng experiments and test collections is the starting point for a wider range of services for IR experimentation as a whole, especially when combined with scientific workflow engines as mentioned above. We will use SOIRE for evaluating the 2009 CLEF IP track 2 as a first testbed.
The data model of the SOIRE system, internally repre-sented by XML schemas, follows the traditional structure of the Cranfield experiments [2] and comprises the test col-lection, the experiment and the analysis specification. Cur -rently, the test collection schema covers the topics (i.e. t he questions) and the relevance assessments, but does not con-tain the documents themselves due to their inhomogeneity in structure across collections. The experiment schema con -tains a link to the particular test collection used and the ranked lists of search results for the topics. An experiment consists of one or more runs, by which we mean results of different retrieval engines, the same retrieval engine with dif-ferent parameter settings, different document preprocessi ng and indexing techniques, and the like. The analysis specifi-cation defines the measures to be used for analysis, and in the case of multiple runs, the statistical tests to be used fo r judging the significance of differences in run results. http://nema.lis.uiuc.edu http://www.ir-facility.org/the_irf/ clef-ip09-track The typical workflow for evaluating an IR experiment with SOIRE starts by submitting the results of an experiment along with the configuration of the measures to be calculated by the system. The experiment is automatically stored and given a unique identifier. The analysis component fetches the experiment from storage and performs the measure cal-culations. In case of multiple runs, the appropriate statis ti-cal tests will be selected and performed pairwise for the run s. The experiment is updated by adding the analysis results to the experiment representation in storage. The experiment i s transformed into the desired format by the reporting com-ponent and is delivered to the user via e-mail.

The SOIRE functionality is exposed via many different interfaces as shown in Figure 1. SOIRE offers Web Services using two technological paradigms. First, SOAP-based ser-vices allow for remote procedure call like programming in-terfaces that can be used from scripting and programming languages supporting XML and HTTP. Second, so-called RESTful services, in which everything is abstracted into re -sources, are accessible via distinct URLs. For convenience , we provide a set of command line utilities to use the system.
Additionally, a basic Web frontend exists which can also be used to upload an experiment to the system and to start the evaluation and report generation workflow. This fron-tend is implemented using the Rails framework based on the Ruby programming language, which facilitates the rapid de-velopment of more sophisticated user interface functional ity.
Besides the delivery component, which currently sends the report via e-mail to the user or publishes the report onto a Wiki, the three central components are storage , analy-sis and reporting . The storage component is based on an XML database that stores both test collections and experi-ments. This allows for easy and efficient retrievability of ei -ther complete experiments or parts thereof by using XPath or XQuery expressions. The basic idea for the mode of op-eration is that deleting or modifying an experiment is not permitted. Everytime an experiment is submitted to the system, it is stored as a new experiment in order not to lose any information that might later be of value.

The heart of SOIRE is the analysis component implement-ing IR measures and test statistics. We have chosen R, a free software environment for statistical computing and graphi cs, over implementing a wrapper for trec eval 3 , because of its potential regarding extension, generalization and powerf ul data plotting capabilities.

Currently, the list of implemented IR measures comprises all of those calculated by trec eval . Tests for statistically judging the significance of different run results are Student  X  X  t-test, Wilcoxon signed rank test and  X  2 -Test with more to be added. Since the applicability of these statistical test s depends on the experiment setup and the actual data dis-tribution, we have implemented a decision tree for selectin g appropriate tests for a particular experiment. In particu-lar, the decision tree considers two important constraints in terms of statistical testing, namely, the dependency of the sample data, i.e. unpaired / paired data, as well as the feasi-bility of using parametric or non-parametric statistics ba sed on the estimated distributions of the data samples.
As the experiments are internally represented by XML documents, report formats that are easier to read for humans are generated by the reporting component. Currently, it is possible to generate comprehensive PDF reports including plots generated by the analysis component. The Wiki report format is especially interesting for organizations using t hese collaborative Web platforms. We have chosen Confluence by Atlassian, a wide-spread commercial Wiki, because it is a) used inside our company and b) offers a SOAP-based Web Service interface for manipulating content, e.g. adding pa ges or attaching files to pages.

Further report formats being especially beneficial for re-searcher will include L A T E X code and an MS Word compati-ble format so that results can directly be included in resear ch papers or technical reports.
We will present SOIRE, an IR evaluation framework of-fering comprehensive functionality for measuring the perf or-mance of IR systems and for assessing the statistical signif -icance of differences when comparing IR evaluation results. [1] T. Armstrong, A. Moffat, W. Webber, and J. Zobel. [2] C. Cleverdon. The cranfield tests on index language [3] D. Hull, K. Wolstencroft, R. Stevens, C. Goble, [4] J. K  X  ursten, T. Wilhelm, and M. Eibl. Extensible [5] X. Llor`a, B.  X  Acs, L. Auvil, B. Capitanu, M. Welge, and [6] B. Lud  X  ascher, N. Podhorszki, I. Altintas, S. Bowers, http://trec.nist.gov/trec_eval
