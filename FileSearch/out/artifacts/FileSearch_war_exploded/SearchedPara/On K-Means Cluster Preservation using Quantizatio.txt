
This work examines under what conditions compression methodologies can retain the outcome of clustering oper-ations. We focus on the popular k-Means clustering al-gorithm and we demonstrate how a properly constructed compression scheme based on post-clustering quantization is capable of maintaining the global cluster structure. Our analytical derivation indicate that a 1-bit moment preserv -ing quantizer per cluster is sufficient to retain the orig-inal data clusters. Merits of the proposed compression technique include: a) reduced storage requirements with clustering guarantees, b) data privacy on the original val-ues, and c) shape preservation for data visualization pur-poses. We evaluate the quantization scheme on various high-dimensional datasets, including 1-dimensional and 2 -dimensional time-series (shape datasets) and demonstrate the cluster preservation property. We also compare with previously proposed simplification techniques in the time-series area and show significant improvements both on the clustering and shape preservation of the compressed datasets.
The exponential increase in data sizes is currently driv-ing mining techniques into combining approximation tech-niques with popular knowledge extraction algorithms, in order to allow even more tractable execution times. In this study we explore how clustering algorithms could be combined with compression techniques, with the concur-rent goal of providing quality guarantees on the cluster-ing outcome of the approximated data. In particular, we examine under what circumstances the outcome of the K -Means clustering results can be preserved by compression or data simplification methods. To this end, we present a bit-quantization technique that satisfies the problem desidera ta. Therefore, clustering on the simplified dataset will lead to similar results as on the uncompressed dataset. To the best of our knowledge, this is the first work that provides such guarantees for the K -Means algorithm.

Our choice for selecting K -Means as our focus of study is due to its widespread use and popularity among the data-mining and AI community. Even though many other clus-tering techniques with superior clustering properties hav e appeared (such as spectral methods [2]), K -Means is still a prevalent approach due to its many desirable proper-ties; simplicity of implementation, amenity to paralleliz a-tion and speed of execution. For applications where speed is of essence or even for doing an initial pre-clustering for data analysis, K -Means is still very much the algorithm of choice. Variations of the K -Means process are widely used as sub-processes in many analytic components.
 Our approach may be intuitively described as follows. We begin by examining the objective function optimized for K -Means clustering, and then determine circumstances under which quantization or data simplification does not affect it. Since the objective function is defined in terms of the intra-cluster variance, we show that by designing a quantizer which preserves the first two moments of a time-series cluster, the objective function is preserved. We the n show that this also means that the clustering outcome will be preserved. Finally, we show that using Moment Preserv-ing Quantization (MPQ) a 1-bit quantizer per sample and class is sufficient to retain perfectly the clustering struc ture.
The results that we present here apply for the optimal partition of the K -Means clustering. In practice though, because of the gradient-descent based algorithm for Means execution (Lloyd X  X  algorithm), and/or potentially malformed clusters on the original data, final results may deviate slightly. Our empirical results, on multiple data s ets, show that clustering results are well preserved by this quan -tization. Importantly, we identify the cases and condition s which lead to potential discrepancies in final results.
Applications of the preservation quantization based com-pression scheme with K -Means cluster preservation, can find applications in the following areas: a) Reduced Storage . Storing the quantized dataset requires much less space than storing the original set. This also translates to reduced transmission cost if the data set need s to be distributed. Importantly, the level of compression is tunable based on the number of bits allocated to the scalar quantizers. Other approaches that investigate scaling-up the K -Means algorithm include [8, 4]. They examine the prob-lem either from a dimensionality reduction or sampling per-spective, while this work views the problem from a quan-tizer design angle. Additionally, these techniques make no assertions with regards to the cluster preservation. b) Quantization with Shape Preservation . When the high-dimensional objects in a dataset represent time-seri es, (i.e., the T samples of the 1-D time series are collected into a
T dimensional vector), the proposed quantization retains very closely the shape of the original sequences. This makes the compressed data amenable for a variety of mining and visualization purposes, besides the intended clustering a p-plication. In this area, our work has overlap with various time-series simplification techniques. Bagnall et al. [3] p ro-pose a binary clipping method for time-series data, where the data are converted into 0 and 1 if they lie above or below the mean value baseline. This representation has interesti ng theoretical underpinnings and has been applied for speed-ing up the execution of the K -Means algorithm. We com-pare against this work in the experimental section. Mega-looikonomou et al. [9] present a piecewise vector quan-tized approximation for time-series data, which preserves with high accuracy the shape of the original sequences. Fi-nally, approaches such as wavelet or Fourier approxima-tions have been used extensively for time-series simplifica -tion, but none of these approaches are inherently designed for providing guarantees on the clustering outcome, which is one of the significant contributions of this work. c) Privacy Preserving Clustering . By disseminating the quantized dataset, an added benefit is that the original val-ues are not distributed -only sufficient approximations are revealed. Therefore our approach can also be utilized for privacy enabled mining. Vaidya et al. [12] and Jagan-nathan et al. [6] present privacy preserving variations for K -Means, considering the scenario when the data are seg-regated either vertically or horizontally. In our case, the data are not separated, but are distributed as a whole. Sim-ilar in spirit to our work are also the following efforts: Parmeswaran and Blough [11] present clustering preserva-tion techniques through Nearest Neighbor data substitutio n and Oliveira and Zaane [10] present rotation based transfor -mations (RBT) that retain the clustering outcome, by chang-ing the object values but maintaining the pairwise object distances and hence the clustering results.

The remaining of the paper is organized as follows; we review the K -Means objective in Section 2 and describe our problem of interest in Section 3. In Section 4 we introduce a 1-bit Moment Preserving Quantization scheme (MPQ) and discuss its properties. We detail our algorithm in Section 5 and present results on real data sets in Section 6. We con-clude in Section 7 with directions for future research. In the course of the paper we will describe the basic notions of our technique utilizing time-series as data objects. This i s to capture in a more visual way the fundamental constructs, and also to illustrate more effectively the shape preservat ion property of the proposed scheme. However, the following discussion applies to any high-dimensional object.
Consider a set S consisting of N sample vectors x ( 1  X  j  X  N ), each containing T dimensions x ji ( 1  X  i  X  T Means clustering involves grouping the N sample vectors into K non-overlapping clusters, i.e., subsets S with  X  minimized. We may define the sum of intra-class variances as: where  X  ter. We can define the number of vectors in cluster N k = |S k | algorithm attempts to minimize by selecting subsets S The objective can be expanded as follows: in terms of the individual dimensions x Furthermore by swapping summations we get: or or or finally V = where we use the fact that  X  From the above derivation we observe two things:  X  The objective function depends on the first ( P x ji ) and  X  The result of the objective function depends on the ob-
In the upcoming sections we will explicate a quantization scheme that (under certain assumptions) closely obeys the above two points.
Our goal is to design a quantization scheme that retains the clustering structure as required by the K -Means algo-rithm. An illustration of this is shown in Figure 1. Consider the original data that consists of six time-series belongin g to three different clusters. These clusters may be identifie d using the K -Means algorithm on the original data set. We wish to design a quantization scheme such that the cluster-ing structure is retained after quantization. Equivalentl y, if the K -Means algorithm is used to cluster the quantized time series, it should result in exactly the same clustering as ob -tained on the original time series.

In order to achieve the defined objective we take the ap-proach shown in Figure 2. As shown, we design T scalar 1-bit quantizers per cluster, with one per dimension (or tim e instance, when dealing with time-series), i.e., one quanti zer for the set of N located in time) for cluster k . Each quantizer is described in terms of a codebook, consisting of a single threshold, as well as two corresponding reconstruction levels. After quantization, the resulting signals may be viewed as binary 0-1 sequences. In the rest of the paper we show that we can build such 1-bit (single threshold) moment preserving quantizers that ensure that the simplified (quantized) data set will result in identical clusters as the original (un-quant ized) dataset, when using the K -Means algorithm.
Here we briefly review Moment Preserving Quantization (MPQ), which typically being used in the image processing literature [5] for retaining the texture properties of an im age while providing good compression. We will present a quan-tizer that preserves the first two moments (i.e, mean and variance) for a set of samples, without making any assump-tions on the distribution of the samples. We then derive two important observations for the proposed quantization:  X  The quantization will preserve the mean and variance  X  The quantization will lead to  X  X hrinking X  of the origi-
We utilize these key observations in showing that the (optimal) K -Means clustering on the original and the quan- X   X   X  tized data will not change.
 MPQ: The key idea behind MPQ may be described as fol-lows. Consider a set of N 1-D samples x sample mean  X  and sample variance  X  2 . The samples are transformed using the following 1-bit quantizer: where N greater than or equal to  X  . The resulting quantizer guaran-tees the preservation of the first two moments [5] of this set of samples i.e.
We underscore that this is a 1-bit quantizer since each sample can be replaced by a 0 or 1 indicating that it is above or below the mean value, since the mean value is preserved and hence can be reconstructed from the quantized values themselves.
 More importantly for our discussion, we can show that this quantization leads to clusters that are tighter than before quantization. Formally, let the extent of the cluster be defined by d d min = min j ( x j  X   X  ) min j ( X  x j  X   X  ) and  X  d max = max j ( X  x j  X   X  ) . We first prove that we cannot have the cluster extent increase in both di-rections simultaneously, i.e., we cannot have  X  d and  X  d contradiction. The moment preservation of the quantizatio n implies
X We use the fact that all points above the mean are quantized to the same value with distance  X  d all points below the mean are quantized to the same value with distance  X  d  X  d
X since d tribution. This is clearly a contradiction, since the LHS is equal to N  X  2 .

Additionally, we show that the extent of the cluster in each direction does not increase due to quantization, ex-cept under some special conditions. If label the direc-tion in which samples are greater than the mean as the right direction, and the direction in which samples are less than the mean as the left direction. Consider first the ex-tent of the cluster in the right direction. For the clus-ter extent to increase after quantization, we need to have  X  d of the quantizer. The above means that the extent in the right direction can increase only when N is a necessary, but not sufficient condition). Furthermore, as
N g decreases for a fixed N , it is more likely that quan-tization will lead to a greater increase in the right extent. The scenario when this does actually happen is when a clus-ter contains a small number of points that are far from the mean in the right direction, combined with a large number of points to the left of the mean. Intuitively, such a cluster is  X  X ll-behaved X  in that the data within it, actually belong to multiple sub-clusters that minimally overlap with each other. Ideally, this indicates that we can get better cluste ring performance by partitioning this into multiple sub-cluste rs. This idea is illustrated in Figure 3, where we show two 1-D cluster examples. On the left side is a  X  X ell-behaved X  clus-ter that has a reasonably uniform spread, whereas on the right side is an ill-behaved cluster, where data to the right of the mean consists of very few but distant samples. In the figure we show the cluster samples, the mean, as well as the reconstruction levels in both directions. As shown, for the ill-behaved clusters it is possible that the extent of th e cluster increases in one direction after quantization.
In general, for well-behaved data clusters, we have  X  d  X  d
We now show how the MPQ scheme may be used to quantize data samples while retaining the clustering objec -tive function of the K -Means algorithm. Given N time-series of length T each and the clustering result, i.e., subsets S ( 1 . . . K ), as determined by the outcome of the K -Means algorithm on the unquantized data, we build T 1-bit scalar quantizers per class. The scalar quantizer for cluster S erates on N sion (or time for time-series data), and appropriately maps them into two bins. Equivalently we design one 1-bit quan-tizer per dimension of the N k . A value  X 0 X  represents that the time sample is below the sample is above the quantizer threshold. As a result of this quantization each time-series is then converted into a bina ry sequence of T 1 X  X  and 0 X  X . Note that a 0 (or 1) may actu-ally correspond to different reconstruction levels at diff erent time instances. 5.1 Preservation of the K -Means Cluster-Recall equation 6 which is the expanded derivation of the K -Means objective function. For the new quantized values  X  x tization, it is guaranteed that  X  V = What this means is that, given the cluster labels associated with each point, we can use 1-bit scalar moment preserv-ing quantization across each dimension (time sample) of the vector (time series) within each cluster to guarantee that t he resulting clustering metric is preserved.

Additionally, consider that we know the optimal set of clusters S opt the gradient descent Lloyd Algorithm for K -Means. This means that, for any other partitioning of the data into clus-ters S  X  If we now use the scalar moment preserving quantization, designed based on the optimal clustering labels S show that. The moment preserving property of the quantization en-the optimal clustering scheme. Additionally, we know that for  X  X ell-behaved X  clusters, quantization actually resul ts in shrinking the cluster extent towards the mean, making them tighter. As the mean squared error (MSE) optimal K -Means clustering uses nearest-neighbor assignment of data sam-ples to cluster centroids, the tighter clusters are guarant eed to retain the optimality of the original clustering scheme.
This is an important result as it implies that if we quan-tize the original time series using a set of 1-bit moment pre-serving quantizers per cluster, designed based on the opti-mal clustering labels, the new set of quantized samples re-tains the same optimal clustering structure. Alternativel y, if we take this new set of quantized time series, and cluster them using the K -Means algorithm, they will result in the
Here we analyze the compression efficiency of the pro-posed quantization scheme. For the set of N object with T dimensions clustered into K clusters, let each unquan-tized sample be represented by B bits. Then the total stor-age requirement is BT N bits for the unquantized data, and N log log 2 ( K ) bits per object.
 Instead, if we use a 1-bit quantizer we need to store only T N bits for all objects, along with 2 BT K bits to store the two reconstruction levels per dimension per cluster. Note, that the threshold does not need to be explicitly stored as it can be deduced from the reconstructed samples, since the quantization does not distort the mean. Finally, N log bits are also required to indicate the clustering labels. He nce the compression efficiency  X  achieved by our moment pre-serving quantization scheme is: Since typically we have N  X  1 &gt; 2 K the compression effi-ciency  X  &lt; 1 .

A better compression ratio can be achieved by noting that since the quantization preserves the underlying clusterin g structure, one does not explicitly need to store the cluster labels. Then, while we require BT N bits for the original data set, for the quantized data sets only log bits are sufficient. This is because there are only 2 possible values that each of the T samples can take per cluster, and K clusters, i.e., a total of at most 2 T K values. Hence we need at most log ing no label compression efficiency  X   X  may be defined as:
In the experimental section 6.4 we evaluate on real datasets the compression efficiency of the quantization scheme.

We also note that, while other quantization schemes such as the the clipping approach of [3] require just B bits to rep-resent the temporal mean of each of the N object, and then 2 T bits per time sample, i.e., a total of N ( B + T ) bits, this however comes at the expense of post quantization cluster-ing accuracy. In the experiments show that our technique achieves superior clustering performance on the quantized data compared to such approaches.
While it is easy to see that a trivial cluster-preserving compression can be achieved by retaining only cluster cen-troids and the label for each vector and distributing just those, it is nonetheless apparent that this compression is very lossy and destroys distinctions among objects within the same cluster. Therefore such an approach would have limited the use of the compressed dataset in other opera-tions such as visualization or Nearest-Neighbor search. In -stead, in our approach we can control the granularity of compression, and show that even with 1-bit quantization the compressed data samples retain  X  X hape X  as well as neigh-borhood relationships. We depict that neighborhood is ob-jects is well preserved in the experimental section. Finall y, we note that pre-clustering cluster preservation techniqu es, such as [10, 11], can achieve cluster preservation and data obfuscation, but they are not designed for data compression or shape preservation. Such approaches completely change the  X  X hape X  of the original data by transforming them into a different domain.

Our K -Means cluster preservation technique is designed for Euclidean based separable distance functions between objects. However, in many high-dimensional datasets (e.g. , for time-series) warping distance functions and other non-separable functions are also typically used. While our technique does not carry over to such distance measures, in Section 6.3 we show sample results of our technique on phase (rotation) invariant distance measures, the out-come of which closely resembles that of warped distance functions. Extensions to support other types of distance measures are under consideration.
In this section we validate the performance of our quan-tization on real data sets. We examine various character-istics of the quantization scheme including its effect on the  X  X hape X  of the data, quality of cluster preservation and we also delineate preliminary results for for neighborhood (k-NN) classification. We utilize stock market time-series data, as well as 2-dimensional shape contour data. The 2D contours come from three datasets, representing skulls , fish and leaves shapes. We convert the 2D shapes into 1D sequences by finding the center of mass and extracting the distance to all perimeter points. Such 1D sequence fea-tures are commonly utilized in shape indexing and search experiments [7].
Here we demonstrate that the proposed 1-bit quantiza-tion does in fact retain the with high accuracy the object  X  X hape X . We present results using the 1D sequences ex-tracted from the skulls dataset. Figure 4 depicts 6 skulls with extracted 1D sequences, the quantized representation and the  X  X lipped X  representation as presented in the work of [3]. In our quantized representation, we use the reconstruc -tion levels of each quantizer instead of the  X 0 X  or  X 1 X  values . We observe that the proposed moment-preserving quantiza-tion retains very closely the form of the original sequences . On the other hand the clipped representation captures the data fluctuations, but due to the purely binary representa-tion cannot discriminate the difference between the relati ve amplitudes at different positions of the sequence.
Now we examine how well the clusters are retained on the quantized dataset. We utilize time series from stock market data corresponding to 1800 stock symbols from companies listed on Nasdaq, reporting the stock values for a period of approximately 3 years.

Our theoretical results about cluster preservation are ap-plicable for the optimal K -Means algorithm. Since the ex-act solution is NP-hard to compute, in practice a gradient descent (Lloyd X  X  algorithm) variation is used for computa-tional simplicity. Therefore, here we empirically evaluat e the discrepancy of the clustering results, when the non-exa ct algorithm is utilized.

Since, we do not have the cluster labels for aforemen-tioned dataset, we cluster these time-series into 8 cluster s using the Lloyd Algorithm for K -Means 3 . As we desire a near-optimal set of clusters, we repeat the K -Means algo-rithm with multiple starting points and select the set that achieves the smallest K -Means metric. Note that we can also use algorithms such as K -Means++ [1] to achieve a very good initial estimate of the cluster centroids and maximize the probability of finding the global optimum. We then use this clustering structure as ground truth, and quantize the time series from each cluster using a separate moment-preserving 1-bit quantizer. The resulting cluster centroids, sample time series from each cluster, and the up-per and lower reconstruction levels for each 1-bit quantize r are shown in Figure 5.

Cluster centroids are shown in black, while 3 exam-ple series from each cluster are shown in gray. The 1-bit quantizer levels for each sample for each cluster are shown in red. As is clear, the quantizer does tend to follow the  X  X ean X  shape of the cluster quite nicely. We then rerun the clustering using Lloyd X  X  algorithm on the quantized time series, using the same initial conditions (the same sam-ple points before and after quantization were chosen as the initial cluster centroids) and compare the resulting train ed cluster centroids before and after quantization. These cen -troids are shown in Figure 6. As may be seen from the fig-ure, a majority of the cluster centroids are almost identica l before and after quantization. This is however not true for clusters 3 and 6. There are two reasons for this, clusters 3 and 6 are the least well-behaved of the clusters, and both violate the  X  d Lloyd algorithm is gradient descent based, while the opti-mal clustering structure remains before and after quantiza -tion, it is possible that an intermediate sub-optimal clust er-ing, as demonstrated here, may be different. However, even for this data set, quantization results in the mis-labeling of only 57 of the 1800 time-series (  X  3%), a majority of which (49) belong to these two clusters. The resulting confusion matrix, comparing unquantized and quantized labels is: where the column index corresponds to the label before quantization and the row index represents the label after quantization. The confusion is primarily restricted to dat a from clusters 3 and 6, and this is also reflected in Figure 6.
For the same dataset we use the clipped data representa-tion [3] to quantize the time series into 1 bit per sample and then redo the clustering. Clipping is performed per time se-ries and replaces a sample with  X 1 X  if it lies above the mean (computed across the time samples of that series) and  X 0 X  if it lies below. Clipping thus does not require any prior knowledge of the data clusters. However, this results in a significant impact on the label preservation performance. For the stock data set, the clustering after clipping result s in only 203 time-series retaining their label -less than 20% of the data . Hence, clipping not only retain as well the ob-ject shape, but also performs significantly worse the cluste r operations of the quantized data.
 Sensitivity to initial centers : Here we evaluate the sen-sitive to the selection of the right starting point (the seed centroids). In order to quantify the impact of mismatched starting points on clustering before and after quantizatio n, we use a set of disjoint starting points for the two cases. The resulting mismatch is quantified in terms of the number of signals mislabeled. We find that for a set of 10 different starting points, the mean mismatch was 140 (7.8%) signals, with a maximum of 230 (12.7%) signals. Additionally, in most cases cluster centroids are very close before and after quantization. This low level of mismatch indicates that the clustering structure is well retained after quantization. Neighborhood preservation : While the quantization scheme is not designed for k-nearest neighbor (k-NN) clas-sification, the fact that it tends to retain the clustering st ruc-ture indicates that it may also preserve results for k-NN -if the class labels are assumed to be determined by the clus-tering structure. In order to verify this, we assume that the unquantized clusters represent ground truth with 8 labels f or the data. We use k-NN with varying numbers of neighbors and use majority voting among neighbors to determine the predicted class label. We present classification results on the training set before and after quantization in Table 1.
The limited separation between the clusters ensures that increasing the number of neighbors does not always result tantly, 1-bit quantization degrades the k-NN performance only by 5 X 6% in this scenario. While the scheme does not provide any guarantees on nearest neighbor classifica-tion, this is a useful illustrative example to show the perfo r-mance on real data. Attempting to bound performance of this scheme for the k-NN classification scheme is a direc-tion for future work.
In this section we repeat the clustering experiments for the fish and the leaves data sets, both of which rep-resent the 2D contours of the corresponding images. The fish data set consists of 247 contours, while the leaves data set contains 1125 contours. These contours are con-verted into series of samples by extracting the distance of the perimeter points from the center of mass. Additionally, when one wishes to support rotation invariance, their pe-riodogram can be extracted and used as the sequence fea-ture, similar to the method used in [14, 13]. The new rotation-invariant sequences can now be used to provide more flexible clustering results. Utilizing the fish dataset we demonstrate some of those clustering results in Figure 7. Observe that rotated versions of the same shape are now clustered together. Obviously, various erroneous ob-ject placements can be detected in the figure as well, how-ever, this example serves as another demonstration of the meaningfulness of time-series clustering using K -Means.
As we do not have ground truth labels for the fish and leaves datasets, we perform clustering with differ-ent number of clusters for both data sets. We present the resulting label preservation in terms of the number of serie s that retain their label after quantization. These results a re shown in Table 2. In the table, we also present the results for clipping.

From the table, we see that the labels for the fish data set are very well preserved by the proposed moment pre-serving quantization (MPQ), with labels retained for 98.4% of the series when the number of clusters is 10. Note that reducing the number of clusters to 5 leads to the creation of more  X  X ll-behaved X  clusters, i.e., data from multiple cl us-ters gets forced into one umbrella cluster, and hence the la-bel preservation is not as good. Additionally, as the number of clusters increases beyond a certain level, there is insuf -ficient data to train the clusters, and hence identifying 15 clusters from the 247 fish series leads to poor clustering, and as a result poor label preservation. As opposed to the fish data set, the leaves data set has continuously in-creasing performance with the number of clusters. This is because the data set contains a much larger number of se-ries, avoiding the problem of overfitting. Furthermore, as the number of clusters decrease, there is a higher proba-bility of  X  X ll-behaved X  clusters reducing performance. Th e best results are achieved for this data set are when K = 15 when 96.8% of the series retain their label. Also, from the table we see that MPQ always outperforms Clipping, which performs reasonably on the simpler fish data set -76% of the series retain the same label before and after quantizati on -but performs significantly worse on the leaves data set -only 34% of the samples retain their label. Finally, in terms of sensitivity to the mismatch in initial centroid selectio n for quantized and unquantized datasets, the standard deviatio n for MPQ on the fish data set is 7 . 2% , while it is 6 . 8% the leaves dataset.
Lastly, we evaluate the compression efficiency of the quantization scheme. We report the compression efficiency  X   X  , as defined in equation (15), for the datasets used in our experiments.  X   X  is reported as percentage of the quantized dataset compared to the original one.

For computing these values, we assume that the unquan-tized data samples are represented by 4 bytes each, i.e., B = 32 . As can be seen from the table, the compression can be as small as 35%, a reduction in size by almost a fac-tor of 3. The compression efficiency varies with the number of clusters, deteriorating as K increases. Selecting the opti-mal trade-off between compression, clustering, and cluste r label preservation is an important practical consideratio n for this scheme.

We showcased compression schemes for high-dimensional data sets that preserve the outcome of K -Means clustering. Our analytic derivation indicates that a quantizer that retains the first two moments of each dimension of the data set, per cluster, does not change the optimization metric for K -Means and therefore can guarantee preservation of the underlying cluster structur e. Such a quantizer can be designed using 1-bit per dimension Moment Preserving Quantization. As future work, we plan to investigate the design of multi-bit quantizers, for providing fine-grained trade-offs between clustering guarantees and shape preservation. We also propose to investigate the interaction with transform domain (Wavele t, Fourier) based compression schemes and design extensions for non-separable distance functions.

