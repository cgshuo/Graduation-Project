 Narayan Gowraj, Prasanna Venkatesh Ravi, Mouniga V, and M.R. Sumalatha According to a recent presentation fro m the vice-president of Facebook Jeff Rothschild at the UC San Diego, Facebook has 30,000 servers supporting its operations which were 10,000 servers on April 2008. This drastic increase in the amount of data in large online organizations had led to the development of ex-tensive data processing applications like the Sql to Map-Reduce translators [1] [2] [3]. Due to the ever increasing size of data in these large clusters, traditional relational databases can be expensive; hence many companies have maneuvered their focus towards NoSql database framework such as BigTable, HadoopDB, Simple DB, Microsoft X  X  SDS cloud database, MongoDB, Apache CouchDB etc [4] [5] [6] [7]. The importance and uniqueness of cloud infrastructure is closely coupled with the development of the Map-Reduce paradigm [8] [9]. In summary the major contributions of this paper are: 1) Support intra-query correlations by building a Sql relationship tree to minimize redundant operations and com-putations.2) Build a spira l modeled database to stor e and retrieve the recently used query results to minimize the data transfer cost and network cost and in-crease the time efficiency.3) The spiral modeled database is adaptive to dynamic workload patterns.4) Sub-query generation to increase the throughput of the system.5) Use the concept of Views in da tabase to store intermediate results which reduces the amount of records to b e checked for every computation.6) Experiment our system with legacy systems like Hive, pig etc. The popularity of a hybrid system transforming Sql queries to Map-Reduce jobs has increased due to the emergence of big data in many data centric organiza-tions. S2MART uses nuoDB [10] [11] as its cloud storage as nuoDB provides full indexing support, easy and flexible Sql querying with DB replication, provides drivers for ODBC, JDBC and Hibernate, it can be deployed anywhere in the cloud, provides scalability upto 50 nodes and the most important thing is that it provides support for Hadoop HDFS. In this paper we focus on three important legacy Sql to Map-Reduce translators namely Pig, Hive and Scope. Companies and organizations which deal with big data in the form of click streams, logs and web crawls have focused their attention towards developing an open source high-level dataflow system. This has led yahoo develop an open source incarnation called Pig to support parallelization of data retrieval in the cloud environment [12]. Pig supports a simple language called Pig Latin [13] to support query formu-lations and data manipulation. Pig Latin is written using java class libraries and provides support for simple operations like filtering, grouping, projection and selection. The major issue concerned with Pig is that it provides limited flexibil-ity towards join operations and nested o perators. Hive developed by Facebook adds Sql like functions to Map-Reduce but follows syntax different from the conventional Sql and it is called as HiveQl [14] [15]. HiveQl transforms the Sql query into Map-Reduce jobs that are ex ecuted in Hadoop (an open source in-carnation of Map-Reduce). Hive provides the functionalities of a metastore that supports query compilations and optimizations [16]. Even though Hive follows a structured approach towards database concepts, it provides limited function-ality towards join predicate. The Sql to Map-Reduce translator developed by Microsoft was called as Scope [17]. The compiler and optimizer of Scope gen-erates a new scripting language for managing and retrieving data from large data repositories. The compiler and optimizer are responsible for automatic job generation and parallelization of job execution. However, Scope provides no sup-port towards user defined types and met astore which reduces the performance of the system. Hence Scope doesn X  X  provide a rich data model when compared to Hive and pig as it doesn X  X  include partitioning of tables. The other translators developed by Microsoft are Cosmos [18] and Dyrad [19]. The proposed system consists of two linchpin modules which are: 3.1 System Architecture The full-fledged system architecture is shown in fig.1 which is divided into three major layers.The architecture of S2MAR T uses the classic master/worker pattern of the Map-Reduce framework where the master node is referred as the query distributor and the worker nodes are referred as query analyzers. The input to the system is the Sql query from the user. This Sql query is given to the Sql query Parser which in turn is given to the IQ-RT (Intra-query relationship tree) parser which constructs the tree based on the input query. IQ-RT parser generates the tree structure which we call as the relationship tree for the input query from the Sql query parser. The tree is then given as an input to the IQ-RI (Intra-query relationship identifier). After identifying the existing relationships, the operations similar to each other are consolidated so that the number of scans of a particular database reduces drastically. The resultant IQ-RT is subjected to the Sub-Query generator engine which disintegrates the different operations in the IQ-RT into sub-queries rather than a single large query. Now each query are distinctly separated and every query is compared with the queries stored in the Spiral database which uses the FQR (Frequent Query Retrieval) algorithm to check whether the input query matches any one of the entries in the Spiral database. Now there exists two possibilities, the first scenario is the case where the input query and the results of the input query are cached from the Spiral database and the other scenario is that the query and the results are not found in the Spiral database. In the first case the results of the individual sub-query are cached from the Spiral database [20] and are given as an input to the query optimizer which optimizes the query results and gives the query results back to the user. In the other case, each individual query is given to the Map-Reduce framework which is built inside a Hadoop infrastructure [21][22]. Here in this case a single node alone is not subjected to retrieve data from the database nodes hence avoiding a bottleneck situation and no single node does the entire mapping job of a single query hence increasing th e efficiency of data retrieval from the database nodes without changing the underlying structure of the Map-Reduce framework. Data transfer cost is reduced in this method by the use of spiral DB. The frequently used queries are stored, thus reducing the need to transfer data unnecessarily. Network transfer cost is reduced by the use of buffers after the map-reduce step. 3.2 Methodology The entire working of the S2MART basically depends on the following modules: 3.2.1 SQL Query Parser The Sql Query Parser is a module which accepts Sql queries as input from the user, converts them into a format whic h is accepted by the S2MART and outputs the query to the IQ-RT module to generate the relationship tree for the input query. 3.2.2 Intra-query Relationship Tree The relationship tree is constructed based on a standard prototype where every operation (selection, aggregation, ordering and sorting) is represented within an oval and every database node is represented with a square. Let us consider an example where a large input Sql query shown in fig.2 gets converted into the re-lationship tree. The query is used to retrieve the department name (dept name), department id (dept id) and employee name (empname) where the employee is of Indian origin with a salary greater than 25000 who belongs to the IT de-partment and lives in the state Mumbai (MUM). Now given the input query, we construct the relationship tree based on the IQ-RT algorithm shown in fig.6. Fig.3 represents the relationship tree for the given input query where D, E, C represents the tuples which represent the Department, Employee and Country tables respectively. Here the total numb er of scans are 7 and requires 7 selection operations and 4 join operation to get the required query results.Each node in the fig.3 represents the individual database. 3.2.3 Intra-query Relationship Identifier The first step in IQ-RI module is to find out the intra-query relationship existing between the various operations performed by the queries. The intra-query rela-tionships are based on a set of procedures which are: Procedure 1: The selection operations on the same database can be grouped so as to eliminate redundant scans on the same table. Procedure 2: Two join operations involving the same partition key are combined into one join operation either using  X  X R X  or  X  X ND X . Procedure 3: Aggregation operations involving sum, max, min, avg etc on the same database can be grouped into one. Procedure 4: Combine individual outer join and inner join into a single query. Based on these procedures certain oper-ations are grouped together to avoid redundant scans on the same table and to optimize the query efficiency. Now the relationship tree is generated again based on all the groupings. The query which has been subjected to the IQ-RI module is shown in fig.4. Now, the IQ-RT is again generated for the query obtained which is shown in fig.5. The IQ-RT obtained represents 4 selection operations and 3 join operations totally, hence shows less number of operations compared to the IQ-RT obtained in fig.3. Hence the number of table scans and operations that has to be performed against the database has decreased which increases the throughput and reduces execution time. The algorithm used to find the intra-query relations and combine similar operations are shown in fig.7. 3.2.4 Sub-query Generator Engine The input to the sub-query generator engine is the relationship tree which has been subjected to the IQ-RI module. Now t he consolidated query obtained from the relationship tree is given a sequence number which is used as a reference. This query is checked with the queries present in the Spiral Database to find any match. If there is any match then the results of the query is directly given to the buffer or else the query is divided into a number of sub-queries which is done based on the GenQL algorithm. In order to bring concurrency between the queries to get the proper results we use t he concept of Views in database. After the sub-queries are generated, each sub-query represents a unique independent operation performed by the relationshi p tree. The number of views that has to be created is equal to the number of sub-queries generated for the same. The query which has been subjected to the IQ-RI module shown in the fig.4 can be divided into 4 sub-queries based on the GenQL algorithm shown in fig. 12. Since the number of sub-queries is 4 the number of views required is also 4. Every individual query is subjected to a view creation which reduces the amount of records or data that has to be accessed or referred for further operations on the table and the view creation can group operations that scans the same tables, minimizing the number of operations. Th e sub-queries generated for the query shown in fig.4 are shown in fig 8, 9, 10, 11.
 3.2.5 Spiral Database The spiral database acts as the cache storage for the S2MART system which is used to directly retrieve the most fre quently used Sql query and its results. Eventually, this reduces the data and network transfer cost for the overall system. The name Spiral database comes from the fact that the Sql queries are stored dynamically in the database based on the f requency count. In this case we store the six frequently referred queries along with its results so as to have a less space complexity for the system. If the numb er of queries is more than six then we use the FQR (Frequent Query Retrieval) to perform deletion of unused queries. Thus we perform an operation which involves dynamic inclusion and exclusion of queries and its results and hence the name Spiral Database. The input to the Spiral database is from the buffer which stores the results of the query executed. The results along with the sequence number of the query are stored in the database for caching purposes. The FQR (Frequent Query Retrieval) algorithm is implemented within the spiral database to dynamically store the different queries based on the frequency counts . In the FQR algorithm implemented, we have assumed six as the maximum number of queries that can be stored in the spiral DB. The number of queries that are stored in the spiral DB can be changed depending on our needs. The work flow of the Spiral Database has already been discussed in Section 3. The complete work procedure of the FQR algorithm is depicted in fig. 13.
 3.2.6 Query Distributor and Analyzer The query distributor and the query analyzer are set up inside the Hadoop framework where the query distributor represents the master node of the Map-Reduce framework and the query analy zers represent the worker nodes. The master node sends the individual sub-query to the query analyzers which are connected to the various mu ltifaceted database node s. The general format for the execution of every individual query is shown in fig.14. The final records are sent to the intermediate buffer once al l the sub-queries have been executed, to undergo the reduce function. The point of using the intermediate buffers is to increase the fault tolerance of the enti re system in situations where the buffer is subjected to single point of failure. In case of a bottleneck situation on the buffer, the query results can be retriev ed back from the intermediate buffer which eliminates the requirement of performing the entire process once again. This increases the robustness of the syst em. A copy of the results are sent back to the Spiral Database to associate with the sequence number of the query. 3.2.7 Query Optimizer The query results from the buffer in the Map-Reduce framework or results from the Spiral database are the two possibilities of query input to the query optimizer. The main function of the query optimizer is to check whether all the sub-queries which are generated from the relationship tree are executed successfully and the query results are optimized and are ordered in a presentable manner to be presented to the user. 3.2.8 Query Results The final step in the methodology of the S2MART is the query results which provide the user interface where the query results are presented back to the user. We have conducted various experiments to verify the sanity of S2MART. We have conducted our experiments by comparing our novel system with our benchmarks Hive and Pig [23]. A cluster with 40 nodes where each node uses a 4GB ram, 250 GB hard disk with 2.1GHz speed is used for implementation of various queries [24] [25].The size of the database used is 1GB with 7 attributes, 3 databases, and 10,000 tuples. Comprehensive evaluation of our novel system is done by using various paradigms of queries such as [26] [27] [28]: 4.1 Lightweight Query without Intra-query Relations and In this case we consider a simple query which cannot be subjected to any of the modules of the S2MART. The query shown in fig.15 is used to find the employee name (emp name) from the Employee table where employee id (emp id) is 101. The comparison between the various ex ecution time of the query in S2MART, Hive and Pig is shown in fig.19 where x-axis represents the number of nodes and y-axis represents the execution time in seconds. The graph shows that there is no drastic decrease in the execution ti me for simple light weighted queries. 4.2 Lightweight Query with Intra-query Relationships But No In this case we consider a query that can be subjected to the IQ-RT and the IQ-RI modules but it cannot be divided into sub-queries as it is atomic. The query shown in fig.16 is used to retrieve the Department name (dept name), Department id (dept id), Employee name (empno) from the tables Employee, Department and Country where the join operation is performed between the ta-bles using the primary key of each tabl e. Now the query which has been subjected to the IQ-RT and IQ-RI modules is shown in fig.17.

Fig.20 shows the comparison between S2MART and the benchmarks. The graph where x-axis represents the number of nodes and y-axis represents the execution time in seconds shows that the re is gradual decrease in the execution time of the query when compared to the other systems after 25 nodes. 4.3 Lightweight Query with Sub-query Generation and No In this case we consider a consolidated query which can be divided into a num-ber of Sub-queries but there are no intra-query relationships existing within the queries. The query shown in fig.18 is used to find the names of the Indian employees whose Department id (dept id) is 101 from the Employee and the De-partment table. This query is divided in to two sub-queries which are executed in two different views.

Fig.21 shows the experimental comparison between the systems. The x-axis of the graph represents the amount of data in thousands and y-axis represents the execution time of the query. The graph shows that the execution time drastically decreases after 20,000 records and thus it can be inferred that S2MART can be very useful when we deal with large amount of data in the data repositories. 4.4 Heavyweight Query with Both In tra-query Relationships and In this case we consider the same query which is shown in fig.4. The query is subjected to all the modules of the S2MART which includes IQ-RT, IQ-RI and Sub-Query Generation. Fig.22 shows the experimental comparison between the various systems where the x-axis repr esents the number of nodes and y-axis represents the execution time in seconds. The graph shows that the execution time gradually decreases as the number of nodes or the amount of data increases. We can infer that S2MART can be more e fficient than its benchmarks when we have a complex heavyweight Sql query as the input. 4.5 Query Results Cached from the Spiral Database In this case we consider the result of the query to be cached from the Spiral Database. In this case there is no query execution time spent for the query because the FQR algorithm finds whether a match is present before the execution of the query. Fig.23 shows the comparison between the execution time of the query for Hive and S2MART. We don X  X  consider Pig for the comparison as it doesn X  X  provide the functionalities of a metastore or caching of data. The graph shows that the execution time in hive[29] is very much greater when compared to S2MART. Execution of complex queries involving join operations between multiple tables, aggregation operations are very much nece ssary for data intensive applications, thus we developed a system that could support the concept of big data in data intensive applications and ease data retrieval to provide useful information. We implement many novel modules such as Intra-Query relationships (IQ-RT and IQ-RI), Sub-Query generation and Spiral Database in the S2MART system that makes the complete system a novel one. We verified the correctness of S2MART system by performing various experimen tal results using diff erent paradigms of queries and we achieved efficiency in all t he experimental results. The intended applications for S2MART are all data intensive applications which use big data analytics. Hence in the future we can build the system with more efficient query optimization for a particular application rather than being generic and enable secure cloud storage [30] for confidentiality and integrity of data.
