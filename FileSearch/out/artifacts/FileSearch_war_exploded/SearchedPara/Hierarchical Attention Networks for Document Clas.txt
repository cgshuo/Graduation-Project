 Text classification is one of the fundamental task in Natural Language Processing. The goal is to as-sign labels to text. It has broad applications includ-ing topic labeling (Wang and Manning, 2012), senti-ment classification (Maas et al., 2011; Pang and Lee, 2008), and spam detection (Sahami et al., 1998). Traditional approaches of text classification repre-sent documents with sparse lexical features, such as n -grams, and then use a linear model or kernel methods on this representation (Wang and Manning, 2012; Joachims, 1998). More recent approaches used deep learning, such as convolutional neural net-works (Blunsom et al., 2014) and recurrent neural networks based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn text representations. pork belly = delicious . jj scallops? jj I don X  X  even like scallops, and these were a-m-a-z-i-n-g . jj fun and tasty cocktails. jj next time I in Phoenix, I will go back here. jj Highly recommend.

Although neural-network X  X ased approaches to text classification have been quite effective (Kim, 2014; Zhang et al., 2015; Johnson and Zhang, 2014; Tang et al., 2015), in this paper we test the hypoth-esis that better representations can be obtained by incorporating knowledge of document structure in the model architecture. The intuition underlying our model is that not all parts of a document are equally relevant for answering a query and that determining the relevant sections involves modeling the interac-tions of the words, not just their presence in isola-tion.

Our primary contribution is a new neural archi-tecture ( x 2), the Hierarchical Attention Network (HAN) that is designed to capture two basic insights about document structure. First, since documents have a hierarchical structure (words form sentences, sentences form a document), we likewise construct a document representation by first building represen-tations of sentences and then aggregating those into a document representation. Second, it is observed that different words and sentences in a documents are differentially informative. Moreover, the impor-tance of words and sentences are highly context de-pendent, i.e. the same word or sentence may be dif-ferentially important in different context ( x 3.5). To include sensitivity to this fact, our model includes two levels of attention mechanisms (Bahdanau et al., 2014; Xu et al., 2015)  X  one at the word level and one at the sentence level  X  that let the model to pay more or less attention to individual words and sentences when constructing the representation of the document. To illustrate, consider the example in Fig. 1, which is a short Yelp review where the task is to predict the rating on a scale from 1 X 5. In-tuitively, the first and third sentence have stronger information in assisting the prediction of the rat-ing; within these sentences, the word delicious, a-m-a-z-i-n-g contributes more in implying the positive attitude contained in this review. At-tention serves two benefits: not only does it often result in better performance, but it also provides in-sight into which words and sentences contribute to the classification decision which can be of value in applications and analysis (Shen et al., 2014; Gao et al., 2014).

The key difference to previous work is that our system uses context to discover when a sequence of tokens is relevant rather than simply filtering for (se-quences of) tokens, taken out of context. To evaluate the performance of our model in comparison to other common classification architectures, we look at six data sets ( x 3). Our model outperforms previous ap-proaches by a significant margin. The overall architecture of the Hierarchical Atten-tion Network (HAN) is shown in Fig. 2. It con-sists of several parts: a word sequence encoder, a word-level attention layer, a sentence encoder and a sentence-level attention layer. We describe the de-tails of different components in the following sec-tions. 2.1 GRU-based sequence encoder The GRU (Bahdanau et al., 2014) uses a gating mechanism to track the state of sequences without using separate memory cells. There are two types of gates: the reset gate r t and the update gate z t . They together control how information is updated to the state. At time t , the GRU computes the new state as This is a linear interpolation between the previous state h t 1 and the current new state  X  h t computed with new sequence information. The gate z t decides how much past information is kept and how much new information is added. z t is updated as: where x t is the sequence vector at time t . The can-didate state  X  h t is computed in a way similar to a tra-ditional recurrent neural network (RNN): Here r t is the reset gate which controls how much the past state contributes to the candidate state. If r t is zero, then it forgets the previous state. The reset gate is updated as follows: 2.2 Hierarchical Attention We focus on document-level classification in this work. Assume that a document has L sentences s i and each sentence contains T i words. w it with t 2 [1 , T ] represents the words in the i th sentence. The proposed model projects the raw document into a vector representation, on which we build a classi-fier to perform document classification. In the fol-lowing, we will present how we build the document level vector progressively from word vectors by us-ing the hierarchical structure.
 Word Encoder Given a sentence with words w it , t 2 [0 , T ] , we first embed the words to vectors through an embedding matrix W e , x ij = W e w ij . We use a bidirectional GRU (Bahdanau et al., 2014) to get annotations of words by summarizing infor-mation from both directions for words, and therefore incorporate the contextual information in the anno-tation. The bidirectional GRU contains the forward GRU w iT and a backward GRU We obtain an annotation for a given word w it by concatenating the forward hidden state backward hidden state which summarizes the information of the whole sen-tence centered around w it .

Note that we directly use word embeddings. For a more complete model we could use a GRU to get word vectors directly from characters, similarly to (Ling et al., 2015). We omitted this for simplicity. Word Attention Not all words contribute equally to the representation of the sentence meaning. Hence, we introduce attention mechanism to extract such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector. Specif-ically, That is, we first feed the word annotation h it through a one-layer MLP to get u it as a hidden represen-tation of h it , then we measure the importance of the word as the similarity of u it with a word level context vector u w and get a normalized importance weight  X  it through a softmax function. After that, we compute the sentence vector s i (we abuse the no-tation here) as a weighted sum of the word annota-tions based on the weights. The context vector u w can be seen as a high level representation of a fixed query  X  X hat is the informative word X  over the words like that used in memory networks (Sukhbaatar et al., 2015; Kumar et al., 2015). The word context vector u w is randomly initialized and jointly learned during the training process.
 Sentence Encoder Given the sentence vectors s i , we can get a document vector in a similar way. We use a bidirectional GRU to encode the sentences: We concatenate sentence i , i.e., h i = [ neighbor sentences around sentence i but still focus on sentence i .
 Sentence Attention To reward sentences that are clues to correctly classify a document, we again use attention mechanism and introduce a sentence level context vector u s and use the vector to measure the importance of the sentences. This yields where v is the document vector that summarizes all the information of sentences in a document. Similarly, the sentence level context vector can be randomly initialized and jointly learned during the training process. 2.3 Document Classification The document vector v is a high level representation of the document and can be used as features for doc-ument classification: We use the negative log likelihood of the correct la-bels as training loss: where j is the label of document d . 3.1 Data sets We evaluate the effectiveness of our model on six large scale document classification data sets. These data sets can be categorized into two types of doc-ument classification tasks: sentiment estimation and topic classification. The statistics of the data sets are summarized in Table 1. We use 80% of the data for training, 10% for validation, and the remaining 10% for test, unless stated otherwise.
 Yelp reviews are obtained from the Yelp Dataset IMDB reviews are obtained from (Diao et al., Yahoo answers are obtained from (Zhang et al., Amazon reviews are obtained from (Zhang et al., 3.2 Baselines We compare HAN with several baseline meth-ods, including traditional approaches such as lin-ear methods, SVMs and paragraph embeddings us-ing neural networks, LSTMs, word-based CNN, character-based CNN, and Conv-GRNN, LSTM-GRNN. These baseline methods and results are re-ported in (Zhang et al., 2015; Tang et al., 2015). 3.2.1 Linear methods
Linear methods (Zhang et al., 2015) use the con-structed statistics as features. A linear classifier based on multinomial logistic regression is used to classify the documents using the features.
 BOW and BOW+TFIDF The 50,000 most fre-n-grams and n-grams+TFIDF used the most fre-Bag-of-means The average word2vec embedding 3.2.2 SVMs
SVMs-based methods are reported in (Tang et al., 2015), including SVM+Unigrams, Bigrams, Text Features, AverageSG, SSWE . In detail, Uni-grams and Bigrams uses bag-of-unigrams and bag-of-bigrams as features respectively.
 Text Features are constructed according to (Kir-AverageSG constructs 200-dimensional word vec-SSWE uses sentiment specific word embeddings 3.2.3 Neural Network methods
The neural network based methods are reported in (Tang et al., 2015) and (Zhang et al., 2015). CNN-word Word based CNN models like that of CNN-char Character level CNN models are re-LSTM takes the whole document as a single se-Conv-GRNN and LSTM-GRNN were proposed 3.3 Model configuration and training We split documents into sentences and tokenize each sentence using Stanford X  X  CoreNLP (Manning et al., 2014). We only retain words appearing more than 5 times in building the vocabulary and replace the words that appear 5 times with a special UNK token. We obtain the word embedding by training an un-supervised word2vec (Mikolov et al., 2013) model on the training and validation splits and then use the word embedding to initialize W e .

The hyper parameters of the models are tuned on the validation set. In our experiments, we set the word embedding dimension to be 200 and the GRU dimension to be 50. In this case a com-bination of forward and backward GRU gives us 100 dimensions for word/sentence annotation. The word/sentence context vectors also have a dimension of 100, initialized at random.

For training, we use a mini-batch size of 64 and documents of similar length (in terms of the number of sentences in the documents) are organized to be a batch. We find that length-adjustment can accelerate training by three times. We use stochastic gradient descent to train all models with momentum of 0.9. We pick the best learning rate using grid search on the validation set. 3.4 Results and analysis The experimental results on all data sets are shown in Table 2. We refer to our models as HN-f AVE, MAX, ATT g . Here HN stands for Hierarchical Network, AVE indicates averaging, MAX indicates max-pooling, and ATT indicates our proposed hi-erarchical attention model. Results show that HN-ATT gives the best performance across all data sets.
The improvement is regardless of data sizes. For smaller data sets such as Yelp 2013 and IMDB, our model outperforms the previous best baseline meth-ods by 3.1% and 4.1% respectively. This finding is consistent across other larger data sets. Our model outperforms previous best models by 3.2%, 3.4%, 4.6% and 6.0% on Yelp 2014, Yelp 2015, Yahoo An-swers and Amazon Reviews. The improvement also occurs regardless of the type of task: sentiment clas-sification, which includes Yelp 2013-2014, IMDB, Amazon Reviews and topic classification for Yahoo Answers.

From Table 2 we can see that neural network based methods that do not explore hierarchical doc-ument structure, such as LSTM, CNN-word, CNN-char have little advantage over traditional methods for large scale (in terms of document size) text clas-sification. E.g. SVM+TextFeatures gives perfor-mance 59.8, 61.8, 62.4, 40.5 for Yelp 2013, 2014, 2015 and IMDB respectively, while CNN-word has accuracy 59.7, 61.0, 61.5, 37.6 respectively. Exploring the hierarchical structure only, as in HN-AVE, HN-MAX, can significantly improve over LSTM, CNN-word and CNN-char. For exam-ple, our HN-AVE outperforms CNN-word by 7.3%, 8.8%, 8.5%, 10.2% than CNN-word on Yelp 2013, 2014, 2015 and IMDB respectively. Our model HN-ATT that further utilizes attention mechanism combined with hierarchical structure improves over previous models (LSTM-GRNN) by 3.1%, 3.4%, 3.5% and 4.1% respectively. More interestingly, in the experiments, HN-AVE is equivalent to us-ing non-informative global word/sentence context vectors (e.g., if they are all-zero vectors, then the attention weights in Eq. 6 and 9 become uniform weights). Compared to HN-AVE, the HN-ATT model gives superior performance across the board. This clearly demonstrates the effectiveness of the proposed global word and sentence importance vec-tors for the HAN. 3.5 Context dependent attention weights If words were inherently important or not important, models without attention mechanism might work well since the model could automatically assign low weights to irrelevant words and vice versa. How-ever, the importance of words is highly context de-pendent. For example, the word good may appear in a review that has the lowest rating either because users are only happy with part of the product/service or because they use it in a negation, such as not good . To verify that our model can capture context dependent word importance, we plot the distribution of the attention weights of the words good and bad from the test split of Yelp 2013 data set as shown in Figure 3(a) and Figure 4(a). We can see that the dis-tribution has a attention weight assigned to a word from 0 to 1. This indicates that our model captures diverse context and assign context-dependent weight to the words.

For further illustration, we plot the distribution when conditioned on the ratings of the review. Sub-figures 3(b)-(f) in Figure 3 and Figure 4 correspond to the rating 1-5 respectively. In particular, Fig-ure 3(b) shows that the weight of good concentrates on the low end in the reviews with rating 1. As the rating increases, so does the weight distribution. This means that the word good plays a more im-portant role for reviews with higher ratings. We can observe the converse trend in Figure 4 for the word bad . This confirms that our model can capture the context-dependent word importance. 3.6 Visualization of attention In order to validate that our model is able to select in-formative sentences and words in a document, we vi-sualize the hierarchical attention layers in Figures 5 and 6 for several documents from the Yelp 2013 and Yahoo Answers data sets.

Every line is a sentence (sometimes sentences spill over several lines due to their length). Red de-notes the sentence weight and blue denotes the word weight. Due to the hierarchical structure, we nor-malize the word weight by the sentence weight to make sure that only important words in important sentences are emphasized. For visualization pur-poses we display important words in unimportant sentences to ensure that they are not totally invisible.

Figure 5 shows that our model can select the words carrying strong sentiment like delicious, amazing, terrible and their corresponding sentences. Sentences containing many words like cocktails, pasta, entree are disre-garded. Note that our model can not only select words carrying strong sentiment, it can also deal with complex across-sentence context. For example, there are sentences like i don X  X  even like scallops in the first document of Fig. 5, if look-ing purely at the single sentence, we may think this is negative comment. However, our model looks at the context of this sentence and figures out this is a positive review and chooses to ignore this sentence.
Our hierarchical attention mechanism also works well for topic classification in the Yahoo Answer data set. For example, for the left document in Figure 6 with label 1, which denotes Science and Mathematics, our model accurately localizes the words zebra, strips, camouflage, predator and their corresponding sentences. For the right document with label 4, which denotes Computers and Internet, our model focuses on web, searches, browsers and their corresponding sentences. Note that this happens in a multiclass set-ting, that is, detection happens before the selection of the topic! Kim (2014) use neural networks for text classifi-cation. The architecture is a direct application of CNNs, as used in computer vision (LeCun et al., 1998), albeit with NLP interpretations. Johnson and Zhang (2014) explores the case of directly using a high-dimensional one hot vector as input. They find that it performs well. Unlike word level mod-elings, Zhang et al. (2015) apply a character-level CNN for text classification and achieve competitive results. Socher et al. (2013) use recursive neural networks for text classification. Tai et al. (2015) explore the structure of a sentence and use a tree-structured LSTMs for classification. There are also some works that combine LSTM and CNN struc-ture to for sentence classification (Lai et al., 2015; Zhou et al., 2015). Tang et al. (2015) use hierarchi-cal structure in sentiment classification. They first use a CNN or LSTM to get a sentence vector and then a bi-directional gated recurrent neural network to compose the sentence vectors to get a document vectors. There are some other works that use hier-archical structure in sequence generation (Li et al., 2015) and language modeling (Lin et al., 2015).
The attention mechanism was proposed by (Bah-danau et al., 2014) in machine translation. The en-coder decoder framework is used and an attention mechanism is used to select the reference words in original language for words in foreign language be-fore translation. Xu et al. (2015) uses the attention mechanism in image caption generation to select the relevant image regions when generating words in the captions. Further uses of the attention mechanism include parsing (Vinyals et al., 2014), natural lan-guage question answering (Sukhbaatar et al., 2015; Kumar et al., 2015; Hermann et al., 2015), and im-age question answering (Yang et al., 2015). Un-like these works, we explore a hierarchical attention mechanism (to the best of our knowledge this is the first such instance). In this paper, we proposed hierarchical attention net-works (HAN) for classifying documents. As a con-venient side-effect we obtained better visualization using the highly informative components of a doc-ument. Our model progressively builds a document vector by aggregating important words into sentence vectors and then aggregating important sentences vectors to document vectors. Experimental results demonstrate that our model performs significantly better than previous methods. Visualization of these attention layers illustrates that our model is effective in picking out important words and sentences. Acknowledgments This work was supported by Microsoft Research.
