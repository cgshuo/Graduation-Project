 meaning of content in a parsimonious way, it is commonplace t o classify webpages and ads into expedites the computation of an expensive to compute condit ional posterior. Gibbs sampling; however, it is not scalable due to slow conve rgence. data and data obtained from an internet advertising applica tion. Assume we have a hierarchy T consisting of L levels (root is level 0 ), for which m For node r in T , denote the parent of r as pa ( r ) , and the i th child of node r as c a descendent of r , we say r  X   X  r . Since the hierarchy has L levels, T in the hierarchy. Let y p -dimensional covariate vector associated with y is easily obtained from our algorithm). Consider the Gaussi an MLH defined by where  X  is a fixed effect parameter vector and  X  j to be where  X  = (  X  on V (  X  ( V )  X  1 /V ) and a mild quadratic prior on  X  a non-informative prior, i.e.,  X  (  X  )  X  1 .
 of specifying MLH is obtained by associating independent ra ndom variables b j nodes and replacing  X  L leaf node in the hierarchy. We denote this compactly as z  X  nodes in the hierarchy, and z compactly, let y = { y vectors x The problem is to compute the posterior mode of (  X  covariate vector. In this paper, we illustrate through a sca lar. 2.1 Model Fitting algorithm is computing the conditional posterior distribu tion of  X  = {  X  j given (  X  ,V,  X  ) . Since the parameters V and  X  are unknown, we estimate them through an EM mentioned above and is used in the inner loop of the EM.
 from root all the way down to the leaves.
 Filtering: Denote the current estimates of  X  ,  X  and V as  X   X  ,  X   X  , and  X  V respectively. Then, e are the residuals and V ar (  X  j effects. If the conditional posterior distribution  X  L step is to update  X  L for Gaussian models Next, the posteriors  X  j information from all the children.
 distribution of  X  j  X  1 Simple algebra provides the conditional expectation and va riance of  X  j  X  1 where B N (0 ,B j  X   X  j ) .
 assume that r is at level j  X  1 , and c Next, we combine information obtained by the parent from all its children. where k Smoothing: us with the posterior of each  X  j of For level 1 nodes, set  X   X  1 For node r at other levels, and let number of nodes.
 Expectation Maximization: parameters by maximizing the expected complete log-poster ior. For j = 1 , ,L, Updating  X   X  : mean of  X  as given in equation (16). where  X   X  L is the vector of  X   X  L 2.2 Simulation Performance Guatemalan children who belong to 1558 families who in turn live in 161 communities. The re-response as follows: y simulated 100 data sets and compared the estimates from Kalman filter to the one obtained from method converged rapidly and required at most 30 iterations. ized linear model family can be easily fitted using the proced ure. Let y defined as: procedure though expensive is easily parallelizable and ac curate. 3.1 Approximation Methods Let  X  approximation) around  X  response y Algorithm 1 The bootstrap procedure Let  X  = (  X  ,  X  ) .

Obtain  X   X  as an initial estimate of  X  . Bias b (0) = 0 . for i = 1 to N do end for and g ( x ) = 1 / (1 + exp(  X  x )) . Approximately, Now denote e tion (3) and (4), the resulting filtering step for the leaf nod es becomes: The step for estimating  X  becomes: where W = diag ( 1 3.2 Bias correction of
M = 50 with about 100  X  200 iterations worked well for us. The bias corrected estimates are (  X  are better than KF but worse than KF-B.
 Based on our findings, we recommend KF-B when computing resou rces are available (especially grid search using a small number of points around the initial estimate. estimate smooth click-rates of (page,ad) pairs. 4.1 Training and Test Data spans 23 days and consisted of approximately 11 M binary observations with approximately 1.9M coarser resolutions. 4.2 Results The MLH model which uses only 2 levels is inferior to the 3 leve l MLH while the general model that uses both covariates and hierarchy is the best. a graph.
 [8] G. Rodriguez and N. Goldman. Improved estimation proced ures for multilevel models with
