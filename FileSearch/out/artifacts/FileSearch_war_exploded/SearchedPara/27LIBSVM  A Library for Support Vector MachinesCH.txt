 Support Vector Machines (SVMs) are a popular machine learning method for clas-sification, regression, and other learning tasks. Since the year 2000, we have been developing the package LIBSVM as a library for support vector machines. 1 LIBSVM is currently one of the most widely used SVM software. In this article, 2 we present all implementation details of LIBSVM . However, this article does not intend to teach the in the package, the LIBSVM FAQ , 3 and the practical guide by Hsu et al. [2003].
LIBSVM supports the following learning tasks. (1) SVC: support vector classification (twoclass and multiclass); (2) SVR: support vector regression. (3) One-class SVM. and second, using the model to predict information of a testing dataset. For SVC and SVR, LIBSVM can also output probability estimates. Many extensions of LIBSVM are available at libsvmtools . 4
The LIBSVM package is structured as follows. (1) Main directory: core C/C++ programs and sample data. In particular, the file (2) The tool subdirectory. This subdirectory includes tools for checking data format (3) Other subdirectories contain prebuilt binary files and interfaces to other lan-
LIBSVM has been widely used in many areas. From 2000 to 2010, there were more than 250,000 downloads of the package. In this period, we answered more than 10,000 emails from users. Table I lists representative works in some domains that have suc-cessfully used LIBSVM .

This article is organized as follows. In Section 2, we describe SVM formulations sup-cation (  X  -SVC), distribution estimation (one-class SVM), -Support Vector Regression ( -SVR), and  X  -Support Vector Regression (  X  -SVR). Section 3 then discusses perfor-mance measures, basic usage, and code organization. All SVM formulations supported in LIBSVM are quadratic minimization problems. We discuss the optimization algo-rithm in Section 4. Section 5 describes two implementation techniques to reduce the running time for minimizing SVM quadratic problems: shrinking and caching. LIBSVM discusses our implementation for multiclass classification. Section 8 presents how to transform SVM decision values into probability values. Parameter selection is impor-tant for obtaining good SVM models. Section 9 presents a simple and useful parameter selection tool in LIBSVM . Finally, Section 10 concludes this work. references. We also show performance measures used in LIBSVM . y  X  R l such that y the following primal optimization problem: parameter. Due to the possible high dimensionality of the vector variable w , usually we solve the following dual problem: Q and the decision function is parameters in the model for prediction. lower bound of the fraction of support vectors.
 that y i  X  X  1 ,  X  1 } , the primal optimization problem is The dual problem is andonlyif so the usable range of  X  is smaller than (0 , 1].

The decision function is
It is shown that e T  X   X   X  can be replaced by e T  X  =  X  [Crisp and Burges 2000; Chang  X  may be too small due to the constraint  X  i  X  1 / l . If  X  is optimal for the dual problem (5) and  X  is optimal for the primal problem (4), Thus, in LIBSVM , we output (  X  / X , b / X  ) in the model. 6 class information, the primal problem of one-class SVM is The dual problem is where Q ij = K ( x i , x j ) =  X  ( x i ) T  X  ( x j ). The decision function is
Similartothecaseof  X  -SVC, in LIBSVM , we solve a scaled version of (7). form of support vector regression [Vapnik 1998] is The dual problem is where Q ij = K ( x i , x j )  X   X  ( x i ) T  X  ( x j ).

After solving problem (9), the approximate function is In LIBSVM , we output  X   X   X   X  in the model. control the number of support vectors. The parameter in -SVR becomes a parameter here. With ( C , X  ) as parameters,  X  -SVR solves The dual problem is The approximate function is can be replaced by an equality. Moreover, C / l may be too small because users often solves the following problem. Chang and Lin [2002] prove that -SVR with parameters (  X  C , ) has the same solution as  X  -SVR with parameters ( l  X  C , X  ). This section describes LIBSVM  X  X  evaluation measures, shows some simple examples of running LIBSVM , and presents the code structure. as y i ,..., y  X  l , we evaluate the prediction results by the following measures. 3.1.1. Classification.
 3.1.2. Regression. LIBSVM outputs MSE (mean squared error) and r 2 (squared corre-lation coefficient).
 While detailed instructions of using LIBSVM are available in the README file of the package and the practical guide by Hsu et al. [2003], here we give a simple example.
LIBSVM includes a sample dataset heart scale of 270 instances. We split the data to a training set heart scale.tr (170 instances) and a testing set heart scale.te . $ python tools/subset.py heart_scale 170 heart_scale.tr heart_scale.te The command svm-train solves an SVM optimization problem to produce a model. 7 $ ./svm-train heart_scale.tr * optimization finished, #iter = 87 nu = 0.471645 obj = -67.299458, rho = 0.203495 nSV = 88, nBSV = 72 Total nSV = 88 Next, the command svm-predict uses the obtained model to classify the testing set. $ ./svm-predict heart_scale.te heart_scale.tr.model output Accuracy = 83% (83/100) (classification)
The file output contains predicted class labels. two main subroutines are svm train and svm predict . The training procedure is more sophisticated, so we give the code organization in Figure 1.

From Figure 1, for classification, svm train decouples a multiclass problem to two-svm train one calls a corresponding subroutine such as solve c svc for C -SVC and suitable input values. The subroutine So, stoplve minimizes a general form of SVM in Sections 4 through 6. This section discusses algorithms used in LIBSVM to solve dual quadratic problems mization problems with one linear constraint, while the second part checks those with two linear constraints. We consider the following general form of C -SVC, -SVR, and one-class SVM: where clearly seen that C -SVC and one-class SVM are already in the form of problem (11). For -SVR, we use the following reformulation of Eq. (9). where We do not assume that Q is Positive semidefinite (PSD) because sometimes non-PSD kernel matrices are used. methods for SVM include, for example, Osuna et al. [1997b], Joachims [1998], Platt [1998], Keerthi et al. [2001], Hsu and Lin [2002b]. Subsequent developments include, for example, Fan et al. [2005], Palagi and Sciandrone [2005], Glasmachers and Igel [2006]. A decomposition method modifies only a subset of  X  per iteration, so only some columns of Q are needed. This subset of variables, denoted as the working set B , leads to a smaller optimization subproblem. An extreme case of the decomposition methods is the Sequential Minimal Optimization (SMO) [Platt 1998], which restricts B to have only two elements. Then, at each iteration, we solve a simple two-variable problem without needing any optimization software. LIBSVM considers an SMO-type decomposition method proposed in Fan et al. [2005].
 where Q is non-PSD.
 and only if there exists a number b and two nonnegative vectors  X  and  X  such that dual relationship,  X  , b ,and w generated by Eq. (3) form an optimal solution of the primal problem. The condition (14) can be rewritten as Since y i = X  1, condition (15) is equivalent to that there exists b such that where and That is, a feasible  X  is a stationary point of problem (11) if and only if From (16), a suitable stopping condition is: where is the tolerance.
 For the selection of the working set B , we use the following procedure from Section II of Fan et al. [2005].
 WSS 1 (1) For all t , s , define (2) Return B ={ i , j } .
 the term  X  b 2 it /  X  a it in Eq. (19). 4.1.3. Solving the Two-variable Subproblem. Details of solving the two-variable sub-problem in Eqs. (12) and (13) are deferred to Section 6, where a more general sub-problem is discussed. These two operations can be considered together because and where | B | | N | and Q : , B is the submatrix of Q including columns in B .Ifatthe Therefore, LIBSVM maintains the gradient throughout the decomposition method. is obtained, the variables b or  X  must be calculated as they are used in the decision function.

Note that b of C -SVC and -SVR plays the same role as  X   X  in one-class SVM, so average all these values. We take  X  the midpoint of the preceding range. because the zero vector is feasible, we select it as the initial  X  .
For one-class SVM, the scaled form (8) requires that convergence, List and Simon [2009] prove a result without making the assumption used in Chen et al. [2006]. From problems (6) and (10), both  X  -SVC and  X  -SVR can be written as the following general form: The main difference between problems (11) and (22) is that (22) has two linear con-straints y T  X  = 1 and e T  X  = 2 . The optimization algorithm is very similar to that for (11), so we describe only differences. problem (22). By the same derivation in Section 4.1.2, The KKT condition of problem (22) implies that there exist b and  X  such that Define If y i = 1, (23) becomes if y i = X  1, (23) becomes Hence, given a tolerance &gt; 0, the stopping condition is where The following working set selection is extended from WSS 1.
 WSS 2 (Extension of WSS 1 for  X  -SVM) (1) Find (2) Find implies Eqs. (25) and (26) according to y i = 1and  X  1, respectively. Now we consider In LIBSVM , for numerical stability, we average these values. If there is no  X  i such that 0 &lt; X  i &lt; C ,then r 1 satisfies We take r 1 the midpoint of the previous range.
 For the case of y i = X  1, we can calculate r 2 in a similar way.

After r 1 and r 2 are obtained, from Eq. (24), 4.2.3. Initial Values. For  X  -SVC, the scaled form (6) requires that y = X  1 is similar. The same setting is applied to  X  -SVR. This section discusses two implementation tricks (shrinking and caching) for the de-composition method and investigates the computational complexity of Algorithm 1. An optimal solution  X  of the SVM dual problem may contain some bounded elements (i.e.,  X  i = 0or C ). These elements may have already been bounded in the middle of to identify and remove some bounded elements, so a smaller optimization problem is solved [Joachims 1998]. The following theorem theoretically supports the shrinking a small set of variables is still changed.
 Theorem 5.1 T HEOREM IV IN F AN ET AL . [2005]. Consider problem (11) and assume Q is positive semi-definite. (1) The following set is independent of any optimal solution  X   X  .

If we denote A as the set containing elements not shrunk at the k th iteration, then instead of solving problem (11), the decomposition method works on a smaller problem. rearrangement are in Section 5.4.
 After solving problem (28), we may find that some elements are wrongly shrunk. When that happens, the original problem (11) is reoptimized from a starting point variables.

In LIBSVM , we start the shrinking procedure in an early stage. The procedure is as follows. (1) After every min( l , 1000) iterations, we try to shrink some variables. Note that (2) The preceeding shrinking strategy is sometimes too aggressive. Hence, when the (3) Once the stopping condition
For  X  -SVC and  X  -SVR, because the stopping condition (27) is different from (17), in the following set. For y t = X  1, we consider the following set. Caching is an effective technique for reducing the computational time of the decompo-sition method. Because Q may be too large to be stored in the computer memory, Q ij elements are calculated as needed. We can use available memory (called kernel cache) to store some recently used Q ij [Joachims 1998]. Then, some kernel elements may not contains these columns, we can save kernel evaluations in final iterations.
In LIBSVM , we consider a simple least-recent-use caching strategy. We use a circular list of structures, where each structure is defined as follows. struct head_t { }; A structure stores the first len elements of a kernel column. Using pointers prev and structures are ordered from the least-recent-used one to the most-recent-used one.
Because of shrinking, columns cached in the computer memory may be in different If condition (31) or (32) is satisfied, LIBSVM reconstructs the gradient. Because  X  i f ( tion, throughout iterations we maintain a vector  X  G  X  R l . Then, for i /  X  A , Note that we use the fact that if j /  X  A ,then  X  j = 0or C .
 differences next. (2) j first: let We may choose a method by comparing (35) and (36). However, the decision depends columns not in A are not used to solve problem (28). In such a situation, method 1 evaluations than l  X | F | .

Because method 2 takes an advantage of the cache implementation, we slightly lower Eq. (34).
 This rule may not give the optimal choice because we do not take the cache contents by making the following assumptions.  X  X  LIBSVM training procedure involves only two gradient reconstructions: The first is performed when the 10 tolerance is achieved; see Eq. (31). The second is in the end of the training procedure.  X  X ur rule assigns the same method to perform the two gradient reconstructions. Moreover, these two reconstructions cost a similar amount of time.
 We refer to  X  X otal training time of method x  X  X sthewhole LIBSVM training time (where as the time of one single gradient reconstruction via method x . We then consider two situations. (1) Method 1 is chosen, but method 2 is better.
 (2) Method 2 is chosen, but method 1 is better.
 Table II compares the number of kernel evaluations in reconstructing the gradient. method for both problems. We implement this technique after version 2.88 of LIBSVM . i  X  A ). Thus, a naive implementation does not access array contents in a continuous This approach allows a continuous access of array contents, but requires costs for the rearrangement. We decide to rearrange elements in arrays because throughout maybeanelementin A .

We rearrange indices by sequentially swapping pairs of indices. If t 1 is going to be shrunk, we find an index t 2 that should stay and then swap them. Swapping two elements in a vector  X  or y is easy, but swapping kernel elements in the cache is more expensive. That is, we must swap ( Q t make the number of swapping operations small, we use the following implementation: same procedure to identify the next pair. We summarize the shrinking procedure in Algorithm 2. We found that if the number of iterations is large, then shrinking can shorten the training time. However, if we loosely solve the optimization problem (e.g., by using a large stopping tolerance ), the code without using shrinking may be much faster. In this situation, because of the small number of iterations, the time spent on all decomposition iterations can be even less than one single gradient reconstruction.
Table II compares the total training time with/without shrinking. For a7a ,weuse the default = 0 . 001. Under the parameters C = 1and  X  = 4, the number of iterations Q
A , A is in the cache. Then, we need many kernel evaluations for reconstructing the gradient, so the implementation with shrinking is slower.

If enough iterations have been run, most elements in A correspond to free  X  i (0 &lt;  X  Table II), many bounded elements have not been shrunk and | F | | A | . Therefore, issue a warning message to indicate that the code may be faster without shrinking. rate of the decomposition method, in this section, we investigate the computational complexity.
From Section 4, two places consume most operations at each iteration: finding the elements. Therefore, the complexity of Algorithm 1 is (1) #Iterations  X  O ( l ) if most columns of Q are cached throughout iterations. Several works have studied the number of iterations of decomposition methods; see, for example, List and Simon [2007]. However, algorithms studied in these works are linear to the number of training data. Thus, LIBSVM may take considerable training time for huge datasets. Many techniques, for example, Fine and Scheinberg [2001], Lee and Mangasarian [2001], Keerthi et al. [2006], Segata and Blanzieri [2010], have been developed to obtain an approximate model, but these are beyond the scope of train a small subset. have proposed using different penalty parameters in the SVM formulation. For exam-ple, the C -SVM problem becomes where C + and C  X  are regularization parameters for positive and negative classes, dual problem of problem (40) is subproblem (12), which now becomes
Let  X  i =  X  k i + d i and  X  j =  X  k j + d j . The subproblem (41) can be written as function can be written as Minimizing the previous quadratic function leads to These two values may need to be modified because of bound constraints. We first consider the case of y i = y j and rewrite Eq. (42) as ( and If (  X  new i , X  new j ) is in region I, we set Other cases are similar. We have the following pseudocode to identify which region ( If y i = y j , the derivation is the same. LIBSVM implements the  X  X ne-against-one X  approach [Knerr et al. 1990] for multiclass classes, we solve the following two-class classification problem. to be in a class with the maximum number of votes.

In case that two classes have identical votes, though it may not be a good strategy, now we simply choose the class appearing first in the array of storing class names.
Many other methods are available for multiclass SVM classification. Hsu and Lin approach. mation. This section discusses the LIBSVM implementation for extending SVM to give and Weng [2004] for regression.

Given k classes of data, for any x , the goal is to estimate Following the setting of the one-against-one (i.e., pairwise) approach for multiclass classification, we first estimate pairwise class probabilities value at x , then we assume decision values before minimizing the negative log likelihood.
 p tion problem. The objective function in problem (44) comes from the equality and can be reformulated as where such that where e is the k  X  1 vector of all ones and 0 is the k  X  1 vector of all zeros.
Instead of solving the linear system (45) by a direct method such as Gaussian elimi-nation, Wu et al. [2004] derive a simple iterative method. Because the optimal solution p satisfies Using Eq. (46), we consider Algorithm 3.

Eq. (47) can be simplified to Algorithm 3 guarantees to converge globally to the unique optimum of problem (44). condition for Algorithm 3.
 Thus, we use a more strict stopping condition by decreasing the tolerance by a factor of k .

Next, we discuss SVR probability inference. For a given set of training data D = { model. tributed random noises. Given a test data x , the distribution of y given x and D ,  X  ( x that of the prediction error  X  .
 retained. On the contrary, distributions like Gaussian and Laplace, commonly used as distribution of  X  i  X  X  seems symmetric about zero and that both Gaussian and Laplace Gaussian and Laplace with mean  X  f ( x ).

Lin and Weng [2004] discuss a method to judge whether a Laplace and Gaussian a density function. Assuming that  X  i  X  X  are independent, we can estimate the scale parameter  X  by maxi-mizing the likelihood. For Laplace, the maximum likelihood estimate is Lin and Weng [2004] point out that some  X  X ery extreme X   X  i  X  X  may cause inaccurate which exceed  X  5  X  (standard deviation of the Laplace distribution). For any new data x , we consider that where z is a random variable following the Laplace distribution with parameter  X  .
In theory, the distribution of  X  may depend on the input x , but here we assume that To train SVM problems, users must specify some parameters. LIBSVM provides a simple tool to check a grid of parameters. For each parameter setting, LIBSVM obtains Cross-Validation (CV) accuracy. Finally, the parameters with the highest CV accuracy are returned. The parameter selection tool assumes that the RBF (Gaussian) kernel is used although extensions to other kernels and SVR can be easily made. The RBF kernel takes the form highest CV accuracy. Users then use the best parameters to train the whole training set and generate the final model.

We do not consider more advanced parameter selection methods because for only SVM problems under different ( C , X  ) parameters are independent, LIBSVM provides a simple tool so that jobs can be run in a parallel (multicore, shared memory, or distributed) environment.
 method to obtain the CV accuracy. Hence, the parameter selection tool suggests the LIBSVM outputs the contour plot of cross-validation accuracy. An example is in Figure 3. When we released the first version of LIBSVM in 2000, only two-class C -SVC was supported. Gradually, we added other SVM variants, and supported functions such as multiclass classification and probability estimates. Then, LIBSVM becomes a complete SVM package. We add a function only if it is needed by enough users. By keeping the system simple, we strive to ensure good system reliability.
 updating and maintaining this package. We hope the community will benefit more from our continuing development of LIBSVM .

