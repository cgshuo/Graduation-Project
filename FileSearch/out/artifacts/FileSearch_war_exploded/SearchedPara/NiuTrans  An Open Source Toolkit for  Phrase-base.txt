 We present NiuTrans, a new open source machine translation toolkit, which was developed for constructing high quality machine translation systems. The NiuTrans toolkit supports most statistical machine translation (SMT) paradigms developed over the past decade, and allows for training and decoding with several state-of-the-art models, including: the phrase-based model (Koehn et al., 2003), the hierarchical phrase-based model (Chiang, 2007), and various syntax-based models (Galley et al., 2004; Liu et al., 2006). In particular, a unified framework was adopted to decode with different models and ease the implementation of decoding algorithms. Moreover, some useful utilities were distributed with the toolkit, such as: a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training that allows for various evaluation metrics for tuning the system. In addition, the toolkit provides easy-to-use APIs for the development of new features. The toolkit has been used to build translation systems that have placed well at recent MT evaluations, such as the NTCIR-9 Chinese-to-English PatentMT task (Goto et al., 2011). 
We implemented the toolkit in C++ language, with special consideration of extensibility and efficiency. C++ enables us to develop efficient translation engines which have high running speed for both training and decoding stages. This property is especially important when the programs are used for large scale translation. While the development of C++ program is slower than that of the similar programs written in other popular languages such as Java, the modern compliers generally result in C++ programs being consistently faster than the Java-based counterparts. 
The toolkit is available under the GNU general public license 1 . The website of NiuTrans is http://www.nlplab.com/NiuPlan/NiuTrans.html . As in current approaches to statistical machine translation, NiuTrans is based on a log-linear model where a number of features are defined to model the translation process. Actually NiuTrans is not the first system of this kind. To date, several open-source SMT systems (based on either phrase-based models or syntax-based models) have been developed, such as Moses (Koehn et al., 2007), Joshua (Li et al., 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al., 2010), cdec (Dyer et al., 2010), Jane (Vilar et al., 2010) and SilkRoad 2 , and offer good references for the development of the NiuTrans toolkit. While our toolkit includes all necessary components as provided within the above systems, we have additional goals for this project, as follows: z It fully supports most state-of-the-art SMT z It offers a wide choice of decoding z It is easy-to-use and fast . A new system can The toolkit serves as an end-to-end platform for training and evaluating statistical machine translation models. To build new translation systems, all you need is a collection of word-aligned sentences 3 , and a set of additional sentences with one or more reference translations for weight tuning and test. Once the data is prepared, the MT system can be created using a sequence of commands. Given a number of sentence-pairs and the word alignments between them, the toolkit first extracts a phrase table and two reordering models for the phrase-based system, or a Synchronous Context-free/Tree-substitution Grammar (SCFG/STSG) for the hierarchical phrase-based and syntax-based systems. Then, an n -gram language model is built on the target-language corpus. Finally, the resulting models are incorporated into the decoder which can automatically tune feature weights on the development set using minimum error rate training (Och, 2003) and translate new sentences with the optimized weights. 
In the following, we will give a brief review of the above components and the main features provided by the toolkit. 3.1 Phrase Extraction and Reordering Model We use a standard way to implement the phrase extraction module for the phrase-based model. That is, we extract all phrase-pairs that are consistent with word alignments. Five features are associated with each phrase-pair. They are two phrase translation probabilities, two lexical weights, and a feature of phrase penalty. We follow the method proposed in (Koehn et al., 2003) to estimate the values of these features. 
Unlike previous systems that adopt only one reordering model, our toolkit supports two different reordering models which are trained independently but jointly used during decoding. z The first of these is a discriminative z The second model is the MSD reordering 3.2 Translation Rule Extraction For the hierarchical phrase-based model, we follow the general framework of SCFG where a grammar rule has three parts  X  a source-side, a target-side and alignments between source and target non-terminals. To learn SCFG rules from word-aligned sentences, we choose the algorithm proposed in (Chiang, 2007) and estimate the associated feature values as in the phrase-based system. 
For the syntax-based models, all non-terminals in translation rules are annotated with syntactic labels. We use the GHKM algorithm to extract (minimal) translation rules from bilingual sentences with parse trees on source-language side and/or target-language side 5 . Also, two or more minimal rules can be composed together to obtain larger rules and involve more contextual information. For unaligned words, we attach them to all nearby rules, instead of using the most likely attachment as in (Galley et al., 2006). 3.3 N -gram Language Modeling The toolkit includes a simple but effective n -gram language model (LM). The LM builder is basically a  X  X orted X  trie structure (Pauls and Klein, 2011), where a map is developed to implement an array of key/value pairs, guaranteeing that the keys can be accessed in sorted order. To reduce the size of resulting language model, low-frequency n -grams are filtered out by some thresholds. Moreover, an n -gram cache is implemented to speed up n -gram probability requests for decoding. 3.4 Weight Tuning We implement the weight tuning component according to the minimum error rate training (MERT) method (Och, 2003). As MERT suffers from local optimums, we added a small program into the MERT system to let it jump out from the coverage area. When MERT converges to a (local) optimum, our program automatically conducts the MERT run again from a random starting point near the newly-obtained optimal point. This procedure is repeated for several times until no better weights (i.e., weights with a higher BLEU score) are found. In this way, our program can introduce some randomness into weight training. Hence users do not need to repeat MERT for obtaining stable and optimized weights using different starting points. 3.5 Decoding Chart-parsing is employed to decode sentences in development and test sets. Given a source sentence, the decoder generates 1-best or k -best translations in a bottom-up fashion using a CKY-style parsing algorithm. The basic data structure used in the decoder is a chart , where an array of cells is organized in topological order. Each cell maintains a list of hypotheses (or items ). The decoding process starts with the minimal cells, and proceeds by repeatedly applying translation rules or composing items in adjunct cells to obtain new items. Once a new item is created, the associated scores are computed (with an integrated n -gram language model). Then, the item is added into the list of the corresponding cell. This procedure stops when we reach the final state (i.e., the cell associates with the entire source span). 
The decoder can work with all (hierarchical) phrase-based and syntax-based models. In particular, our toolkit provides the following decoding modes. z Phrase-based decoding . To fit the phrase-z Decoding as parsing (or string-based z Decoding as tree-parsing (or tree-based z Forest-based decoding . Forest-based In addition to the basic components described above, several additional features are introduced to ease the use of the toolkit. 4.1 Multithreading The decoder supports multithreading to make full advantage of the modern computers where more than one CPUs (or cores) are provided. In general, the decoding speed can be improved when multiple threads are involved. However, modern MT decoders do not run faster when too many threads are used (Cer et al., 2010). 4.2 Pruning To make decoding computational feasible, beam pruning is used to aggressively prune the search space. In our implementation, we maintain a beam for each cell. Once all the items of the cell are proved, only the top-k best items according to model score are kept and the rest are discarded. Also, we re-implemented the cube pruning method described in (Chiang, 2007) to further speed-up the system. 
In addition, we develop another method that prunes the search space using punctuations. The idea is to divide the input sentence into a sequence of segments according to punctuations. Then, each segment is translated individually. The MT outputs are finally generated by composing the translations of those segments. 4.3 APIs for Feature Engineering To ease the implementation and test of new features, the toolkit offers APIs for experimenting with the features developed by users. For example, users can develop new features that are associated with each phrase-pair. The system can automatically recognize them and incorporate them into decoding. Also, more complex features can be activated during decoding. When an item is created during decoding, new features can be introduced into an internal object which returns feature values for computing the model score. 5.1 Experimental Setup We evaluated our systems on NIST Chinese-English MT tasks. Our training corpus consists of 1.9M bilingual sentences. We used GIZA++ and the  X  X row-diag-final-and X  heuristics to generate word alignment for the bilingual data. The parse trees on both the Chinese and English sides were Moses: phrase 36.51 34.93
Moses: hierarchical phrase 36.65 34.79 Table 1: BLEU scores of various systems. t2s, t2t, and s2t represent the tree-to-string, tree-to-tree, and string-to-tree systems, respectively. generated using the Berkeley Parser, which were then binarized in a head-out fashion 6 . A 5-gram language model was trained on the Xinhua portion of the Gigaword corpus in addition to the English part of the LDC bilingual training data. We used the NIST 2003 MT evaluation set as our development set (919 sentences) and the NIST 2005 MT evaluation set as our test set (1,082 sentences). The translation quality was evaluated with the case-insensitive IBM-version BLEU4. 
For the phrase-based system, phrases are of at most 7 words on either source or target-side. For the hierarchical phrase-based system, all SCFG rules have at most two variables. For the syntax-based systems, minimal rules were extracted from the binarized trees on both (either) language-side(s). Larger rules were then generated by composing two or three minimal rules. By default, all these systems used a beam of size 30 for decoding. 5.2 Evaluation of Translations Table 1 shows the BLEU scores of different MT systems built using our toolkit. For comparison, the result of the Moses system is also reported. We see, first of all, that our phrase-based and hierarchical phrase-based systems achieve competitive performance, even outperforms the Moses system over 0.3 BLEU points in some cases. Also, the syntax-based systems obtain very 
Moses: phrase 36.69 34.99 0.11 + cube pruning 36.51 34.93 0.47
NiuTrans: phrase 37.14 35.47 0.14 + cube pruning 36.98 35.39 0.60 + cube &amp; punct pruning 36.99 35.29 3.71 + all pruning &amp; 8 threads 36.99 35.29 21.89 + all pruning &amp; 16 threads 36.99 35.29 22.36 Table 2: Effects of pruning and multithreading techniques. promising results. For example, the string-to-tree system significantly outperforms the phrase-based and hierarchical phrase-based counterparts. In addition, Table 1 gives a test of different decoding methods (for syntax-based systems). We see that the parsing-based method achieves the best BLEU score. On the other hand, as expected, it runs slowest due to its large search space. For example, it is 5-8 times slower than the tree-parsing-based method in our experiments. The forest-based decoding further improves the BLEU scores on top of tree-parsing. In most cases, it obtains a +0.6 BLEU improvement but is 2-3 times slower than the tree-parsing-based method. 5.3 System Speed-up We also study the effectiveness of pruning and multithreading techniques. Table 2 shows that all the pruning methods implemented in the toolkit is helpful in speeding up the (phrase-based) system, while does not result in significant decrease in BLEU score. On top of a straightforward baseline (only beam pruning is used), cube pruning and pruning with punctuations give a speed improvement of 25 times together 7 . Moreover, the decoding process can be further accelerated by using multithreading technique. However, more than 8 threads do not help in our experiments. We have presented a new open-source toolkit for phrase-based and syntax-based machine translation. It is implemented in C++ and runs fast. Moreover, it supports several state-of-the-art models ranging from phrase-based models to syntax-based models, and provides a wide choice of decoding methods. The experimental results on NIST MT tasks show that the MT systems built with our toolkit achieve state-of-the-art translation performance. The next version of NiuTrans will support ARPA-format LMs, MIRA for weight tuning and a beam-stack decoder which removes the ITG constraint for phrase decoding. In addition, a Hadoop-based MapReduce-parallelized version is underway and will be released in near future. This research was supported in part by the National Science Foundation of China (61073140), the Specialized Research Fund for the Doctoral Program of Higher Education (20100042110031) and the Fundamental Research Funds for the Central Universities in China. 
