
L F INESC-ID University of Pennsylvania University of Pennsylvania of the underlying model. We focus on the simple and tractable hidden Markov model, and as measured by both precision and recall of manually annotated alignments for six language pairs. We also report experiments on two different tasks where word alignments are required: phrase-based machine translation and syntax transfer, and show promising improvements over standard methods. 1. Introduction
The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1 X 5) for statistical machine translation and the concept of  X  X ord-by-word X  alignment, the correspondence between words in source and target languages.
Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn,
Och, and Marcu 2003] and rules [Galley et al. 2004; Chiang et al. 2005]) as well as for
MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and
Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008). which produce the target sentence one target word at a time by choosing a source word and generating its translation. IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several target words), resulting in probabilis-implement and extend. Many researchers use the GIZA++ software package (Och and
Ney 2003) as a black box, selecting IBM Model 4 as a compromise between alignment quality and efficiency. All of the models are asymmetric (switching target and source languages produces drastically different results) and the simpler models (IBM Models 1, 2, and HMM) do not enforce bijectivity (the majority of words translating as a single word). Although there are systematic translation phenomena where one cannot hope to obtain 1-to-1 alignments, we observe that in over 6 different European language pairs the majority of alignments are in fact 1-to-1 (86 X 98%). This leads to the common practice of post-processing heuristics for intersecting directional alignments to produce nearly bijective and symmetric results (Koehn, Och, and Marcu 2003).

Tillmann 1996), using a novel unsupervised learning framework that significantly boosts its performance. The new training framework, called Posterior Regulariza-tion (Grac  X a, Ganchev, and Taskar 2007), incorporates prior knowledge in the form of constraints on the model X  X  posteriors. The constraints are expressed as inequalities on the expected value under the posterior distribution of user-defined features. Although straints. We propose two such constraints: (i) bijectivity: one word should not translate to many words; and (ii) symmetry: directional alignments should agree. Both of these constraints significantly improve the performance of the model both in precision and recall, with the symmetry constraint generally producing more accurate alignments.
Section 3 presents the Posterior Regularization (PR) framework and describes how to encode such constraints in an efficient manner, requiring only repeated inference in the the alignments produced. The constraints over posteriors consistently and significantly outperform the unconstrained HMM model, evaluated against manual annotations.
Moreover, this training procedure outperforms the more complex IBM Model 4 nine times out of 12. We examine the influence of constraints on the resulting posterior dis-tributions and find that they are especially effective for increasing alignment accuracy for rare words. We also demonstrate a new methodology to avoid overfitting using a small development corpus. Section 5 evaluates the new framework on two different tasks that depend on word alignments. Section 5.1 focuses on MT and shows that the presented in Ganchev, Grac  X a, and Taskar (2008). Section 5.2 shows that the alignments we produce are better suited for transfer of syntactic dependency parse annotations. An implementation of this work (Grac  X a, Ganchev, and Taskar 2009) is available under a
GPL license. 1 482 2. Background
A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a target language (Brown et al. 1993b). There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one lan-in one language and a single word in the other language (e.g., agglutination such as
English weapons of mass destruction and German Massenvernichtungswaffen ), and expres-sions translated indirectly. Due to this inherent ambiguity, manual annotations usually distinguish between sure correspondences for unambiguous translations, and possible , for ambiguous translations (Och and Ney 2003). The top row of Figure 1 shows two word alignments between an English X  X rench sentence pair. We use the following nota-tion: the alignment on the left (right) will be referenced as source X  X arget (target X  X ource) and contains source (target) words as rows and target (source) words as columns. Each entry in the matrix corresponds to a source X  X arget word pair, and is the candidate for an alignment link . Sure links are represented as squares with borders, and possible links are represented as squares without borders. Circles indicate the posterior probability associated with a given link and will be explained latter.
 Table 1. The corpora are: the Hansard corpus (Och and Ney 2000) of English/French Canadian Parliamentary proceedings (En-Fr), and the English/Spanish portion of the
Europarl corpus (Koehn 2005) where the annotation is from EPPS (Lambert et al. 2005) (En-Es) using standard test and development set split. We also used the English/ Portuguese (En-Pt), Portuguese/Spanish (Pt-Es), Portuguese/French (Pt-Fr), and
Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described 40%/60%. Table 1 shows some of the variety of challenges presented by each corpus. For example, En-Es has longer sentences and hence more ambiguity for alignment.
Furthermore, it has a smaller percentage of bijective (1-to-1) alignments, which makes word fertility more important. Overall, the great majority of links are bijective across the corpora (86 X 98%). This characteristic will be explored by the constraints described in this article. For the evaluations in Section 4, the percentage of sure links (out of all links) will correlate with difficulty because only sure links are considered for recall. 2.1 HMM Word Alignment Model In this article we focus on the HMM for word alignment proposed by Vogel, Ney, and by introducing a first-order Markov dependence between consecutive alignment link decisions. The model is an (input X  X utput) HMM with I positions whose hidden state sequence z = ( z 1 , ... , z I )with z i  X  X  null ,1, ... , J word positions, where J is the source sentence length, and with null representing un-aligned target words. Each observation corresponds to a word in the target language x
The probability of an alignment z and target sentence x given a source sentence y can be expressed as: where p t ( x i | y z source word at position z i (translation probability), and p 484 word (observation) can be aligned to at most one source word (hidden state), whereas a source word could be used multiple times.
 probability p d ( z i | z i  X  1 ) depends only on the distance ( z sitions the states represent. Only distances in the range larger distances assigned equal probabilities. The probability of the initial hidden state, p ( z 1 | z 0 ) is modeled separately from the other distortion probabilities. To incorporate null links, we add a translation probability given null : p practice, null links also maintain position information and do not allow distortion. To implement this, we create position-specific null hidden states for each source position, and set p d ( null i | y i ) = 0and p d ( null i | null i complexity of inference O ( I  X  J 2 ). There are several problems with the model that arise from its directionality, however.
 2.2 Training
Standard HMM training seeks model parameters  X  that maximize the log-likelihood of the parallel corpus: over the N pairs of sentences { ( x 1 , y 1 ) ... ,( x rithm (Dempster, Laird, and Rubin 1977). EM maximizes q ( z | x , y ) (Neal and Hinton 1998):
To simplify notation, we will drop the dependence on y and will write p p ( x , z ), p  X  ( z | x , y )as p  X  ( z | x )and q ( z | x , y )as q ( z at iteration t + 1 are given by:
M :  X  t + 1 = arg max guaranteed to converge to a local maximum of L (  X  ) under mild conditions (Neal and
Hinton 1998). The E step computes the posteriors q t + 1 ( z variables (alignments) given the observed variables (sentence pair) and current param-eters  X  t , which is accomplished by the forward-backward algorithm for HMMs. The M step uses q t + 1 to  X  X oftly fill in X  the values of alignments z and estimate parameters  X 
This step is particularly easy for HMMs, where  X  t + 1 simply involves normalizing (ex-pected) counts. This modular split into two intuitive and straightforward steps accounts for the vast popularity of EM.
 ment link posterior for that particular word pair after training an HMM model with the
EM algorithm (see the experimental set up in Section 4.1). Note that the link posteriors are concentrated around particular source words (rare words occurring less than five times in the corpus) in both directions, instead of being spread across different words.
This is a well-known problem when training using EM called the  X  X arbage collector ef-fect X  (Brown et al. 1993a). A rare word in the source language links to many words in the target language that we would ideally like to see unaligned, or aligned to other words in the sentence. The reason this happens is that the generative model has to distribute translation probability for each source word among different candidate target words.
If one translation is much more common than another, but the rare translation is used in the sentence, the model might have a very low translation probability for the correct alignment. On the other hand, because the rare source word occurs only in a few sen-tences it needs to spread its probability mass over fewer competing translations. In this case, choosing to align the rare word to all of these words leads to a higher likelihood than correctly linking them or linking them to the special null word, because it increases the likelihood of this sentence without lowering the likelihood of many other sentences. 2.3 Decoding
Alignments are normally predicted using the Viterbi algorithm (which selects the single most probable path through the HMM X  X  lattice).
 decoding (Kumar and Byrne 2002; Liang, Taskar, and Klein 2006; Grac  X a, Ganchev, and
Taskar 2007). Using this decoding we include an alignment link i probability that word i aligns to word j is above some threshold. This allows the accumulation of probability from several low-scoring alignments that agree on one alignment link. The threshold is tuned on some small amount of labeled data X  X n our case the development set X  X o minimize some loss. Kumar and Byrne (2002) study 486 alignment having zero probability under the model, as many-to-many alignments can be produced in this way. MBR decoding has several advantages over Viterbi decoding.
First, independently of the particular choice of the loss function, by picking a specific threshold we can trade off precision and recall of the predicted word alignments. In fact, in this work when comparing different alignment sets we do not commit to any loss function but instead compare precision vs recall curves, by generating alignments for different thresholds (0..1). Second, with this method we can ignore the null word probabilities, which tend to be poorly estimated. 3. Posterior Regularization
Word alignment models in general and the HMM in particular are very gross over-simplifications of the translation process and the optimal likelihood parameters learned add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4+ (Brown et al. 1993b; Och and Ney 2003), and more recently by the LEAF model (Fraser and Marcu 2007). Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations use a learning framework called Posterior Regularization (Grac  X a, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model X  X  posteriors. The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features
On the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable. For example, enforcing that each hidden state of an HMM model should be used at most once per sentence would break the Markov property and make the model intractable. In contrast, we will show how to enforce the constraint that each hidden algorithm with the addition of solving an optimization problem similar to a maximum
Regularization framework, followed by a description of how to encode two pieces of prior information aimed at solving the problems described at the end of Section 2. 3.1 Posterior Regularization Framework
The goal of the Posterior Regularization (PR) framework is to guide a model during learning towards satisfying some prior knowledge about the desired latent variables key advantage of using regularization on posterior expectations is that the base model remains unchanged, but during learning, it is driven to obey the constraints by setting appropriate parameters  X  . Moreover, experiments show that enforcing constraints in ex-pectation results in predicted alignments that also satisfy the constraints. More formally, posterior information in PR is specified with sets Q x of allowed distributions over the hidden variables z which satisfy inequality constraints on some user-defined feature expectations, with violations bounded by  X  0:
Constrained Posterior Set : Q x = { q ( z | x ):  X   X  , E q
Q denotes the set of valid distributions where some feature expectations are bounded by b x and  X  0 is an allowable violation slack. Setting = 0 enforces inequality constraints with opposite signs. We assume that Q x is non-empty for each example x . linear inequalities because, as will be shown, subsequently this simplifies the learning model is penalized with the KL-divergence between the desired distribution space and the model posteriors, KL( Q x p  X  ( z | x )) = min larized objective is:
The objective trades off likelihood and distance to the desired posterior subspace (mod-ulo getting stuck in local maxima) and provides an effective method of controlling the posteriors.

L (  X  ) as a KL distance: KL(  X  ( x n ) p  X  ( x )) where  X  ( x objective is a sum of two average KL terms, one in the space of distributions over x and one in the space of distributions over z :  X  L (  X  ) + E [KL( Q This view of the PR objective is illustrated in Figure 2.
 488
Directly minimizing this objective is hard because there is an exponential number of alignments z ; however, the problem becomes easy to solve in its dual formulation (see Appendix A for derivation): solution is q ( z | x ) = p  X  ( z | x ) exp { X   X  f ( x , z ) pectation constraint, and the dual gradient at  X  = 0is  X  (  X  ) = b
Note that this primal X  X ual relationship is very similar to the one between maximum likelihood and maximum entropy. If b x corresponds to empirical expectations and p ( z | x ) is uniform, then Equation (10) would be a log-likelihood and Equation (14) (fol-lowing) would be a maximum entropy problem. As with maximum entropy, gradient computation involves computing an expectation under q ( z efficiently if the features f ( x , z ) factor in the same way as the model p constraints are linear. The conditional distribution over z represented by a graphical model such as HMM can be written as a product of factors over cliques
In an HMM, the cliques C are simply the nodes z i and the edges ( z correspond to the distortion and translation probabilities. We will assume f is factorized constraints can be expressed in this way):
Then q ( z | x ) has the same form as p  X  ( z | x ): Hence the projection step uses the same inference algorithm (forward X  X ackward for setting of  X  . Algorithm 1. Here  X  is an optimization precision,  X  is a step size chosen with the strong this only happens at the start of optimization and we use a sub-gradient for the first direction.
 and uses that inference as a subroutine. For HMM word alignments, we need to make several calls to forward X  X ackward in order to choose  X  . Setting the optimization pre-cision  X  more loosely allows the optimization to terminate more quickly but at a less accurate value. We found that aggressive optimization significantly improves alignment quality for both constraints we used and consequently choose  X  so that tighter values do not significantly improve performance. This explains why we report better results here in this paper than in Ganchev, Grac  X a, and Taskar (2008), which uses a more naive optimization (see Section 4.1). 3.2 Posterior Regularization via Expectation Maximization maximization (EM) algorithm. Recall from Equation (4) that in the E step, q ( z set to the posterior over hidden variables given the current  X  . To converge to the PR the constraint set Q x for each example x (Grac  X a, Ganchev, and Taskar 2007). hence to update the model X  X  parameters in the M step (Equation (5)), which remains unchanged. This scheme is illustrated in Figure 3 and in Algorithm 2. The only imple-mentation difference is that we must now perform the KL projection before collecting sufficient statistics. We found it can help to also perform this projection at test time, using q ( z | x ) = arg min Algorithm 2 : PR optimization via modified EM. E X -Step is computed using
Algorithm 1. 490 3.3 Bijectivity Constraints
We observed in Table 1 that most alignments are 1-to-1 and we would like to introduce this prior information into the model. Unfortunately including such a constraint in the model directly breaks the Markov property in a fairly fundamental way. In particular computing the normalization would require the summation of 1-to-1 or near 1-to-1 weighted matchings, which is a classic #P-complete problem. Introducing alignment degree constraints in expectation using the PR framework is easy and tractable. We word j that counts how many times it is aligned to a target word in the alignment z :
The second row of Figure 1 shows an example of the posteriors after applying bijectivity when compared with the EM-trained HMM. For example, in the top left panel, the word schism is used more than once, causing erroneous alignments. Projecting to the bijectivity constraint set prevents this and most of the mass is (for this example) moved to the correct word pairs. Enforcing the constraint at training and decoding increases the fraction of 1-to-1 alignment links from 78% to 97.3% for En-Fr (manual annotations have 98.1%); for En-Pt the increase is from 84.7% to 95.8% (manual annotations have 90.8%) (see Section 4.1). 3.4 Symmetry Constraints
The directional nature of the generative models used to recover word alignments con-language is source versus target matters and changes the mistakes made by the model independently and then intersect their predictions (Och and Ney 2003). However, we their posterior distributions over alignments to approximately agree. Let the directional models be defined as: directional alignments where ability zero according to the other model). We then define the following feature for each target X  X ource position pair i , j : with equal probability. We therefore impose the constraint E some small slack). Note that satisfying this implies satisfying the bijectivity constraint presented earlier. To compute expectations of these features under the model q we only need to be able to compute them under each directional HMM. To see this, we have by the definition of q  X  and p  X  , q  X  ( z | x ) = where we have defined:  X   X  q ( z | x ) = 1 Z  X   X   X   X  q ( z | x ) = 1 Z  X   X 
All these quantities can be computed separately in each model using forward X  X ackward models, and in most cases the probability mass was moved to the correct alignment links. The exception is the word pair internal/le . In this case, the model chose to incor-rectly have a high posterior for the alignment link rather than generating internal from null in one direction and le from null in the other.
 intersection to the size of the union. Symmetry constraints increase symmetry from 48% to 89.9% for En-Fr and from 48% to 94.2% for En-Pt (see Section 4.1). 4. Alignment Quality Evaluation
We begin with a comparison of word alignment quality evaluated against manually corpora with gold annotations described in the beginning of Section 2. 4.1 Experimental Setup more than 40 words. Following common practice, we added the unlabeled development and test data sets to the pool of unlabeled sentences. We initialized the IBM Model 1 translation table with uniform probabilities over word pairs that occur together in the same sentence and trained the IBM Model 1 for 5 iterations. All HMM alignment models 492 were initialized with the translation table from IBM Model 1 and uniform distortion probabilities. We run each training procedure until the area under the precision/recall curve measured on a development corpus stops increasing (see Figure 4 for an example of such a curve). Using the precision/recall curve gives a broader sense of the model X  X  performance than using a single point (by tuning a threshold for a particular metric). In most cases this meant four iterations for normal EM training and two iterations using posterior regularization. We suspect that the constraints make the space easier to search. of the gradient (gradient norm divided by number of constraints) being smaller than
For symmetric constraints,  X  and slack were set to 0.001. We chose  X  aggressively and lower values did not significantly increase performance. Less aggressive settings cause degradation of performance: For example, for En-Fr using 10k sentences, and running four iterations of constrained EM, the area under the precision/recall curve for the symmetric model changed from 70% with  X  = 0 . 1 to 85% using  X  = 0 . 001. On the other hand, the number of iterations required to project the constraints increases for smaller values of  X  . The number of forward X  X ackward calls for normal HMM is 40k (one for each sentence and EM iteration), for the symmetric model using  X  = 0 . 1was around 41k and using  X  = 0 . 001 was around 26M (14 minutes to 4 hours 14 minutes optimization methods, such as L-BFGS, or using a warm start for the parameters at each
EM iteration (parameters from the previous iteration), or training the models online, would potentially decrease the running time of our method.
 straints during learning, hence the main comparison is between HMM trained with normal EM vs. trained with PR plus constraints. We also report results for IBM Model 4, reference. However, we would like to note that IBM Model 4 is a more complex model, able to capture more structure, albeit at the cost of intractable inference. Because our approach is orthogonal to the base model used, the constraints described here could yielding similar improvements. We used a standard implementation of IBM Model 4 (Och and Ney 2003) and because changing the existing code is not trivial, we could not use the same stopping criterion to avoid overfitting and we are not able to produce precision/recall curves. We trained IBM Model 4 using the default configuration of the MOSES training script. 3 This performs five iterations of IBM Model 1, five iterations of
HMM, and five iterations of IBM Model 4. 4.2 Alignment Results In this section we present results on alignment quality. All comparisons are made using
MBR decoding because this decoding method always outperforms Viterbi decoding.
For the models with constraints we project the posteriors at decode time (i.e., we use q ( z | x ) to decode). This gives a small but consistent improvement. Figure 4 shows precision/recall curves for the different models on the En-Fr corpus using English as
Precision/recall curves are obtained by varying the posterior threshold from 0 to 1 and then plotting the different precision and recall values obtained.
 always above). Second, S-HMM performs slightly better than B-HMM. IBM Model 4 is comparable with both constraints (after symmetrization). The results for all language pairs are in Figure 5. For ease of comparison, we choose a decoding threshold for HMM models to achieve the recall of the corresponding IBM Model 4 and report precision. Our methods always improve over the HMM by 10% to 15%, and improve over IBM
Model 4 nine times out of 12. Comparing the constraints with each other we see that 494
S-HMM performs better than B-HMM in 10 out of 12 cases. Because S-HMM indirectly enforces bijectivity and models sequential correlations on both sides, this is perhaps not surprising.
 to achieve the recall of IBM Model 4. For small training corpora adding the constraints provides larger improvements (20 X 30%) but we still achieve significant gains even with a million parallel sentences (15%). Greater improvements for small data sizes indicate that our approach can be especially effective for resource-poor language pairs. 4.3 Rare vs. Common Words
One of the main benefits of using the posterior regularization constraints described is an alleviation of the garbage collector effect (Brown et al. 1993a). Figure 7 breaks down performance improvements by common versus rare words. As before, we use posterior decoding, tuning the threshold to match IBM Model 4 recall. For common words, this tuning maintains recall very close for all models so we do not show this in the figure. In the top left panel of Figure 7, we see that precision of common words follows the pattern we saw for the corpus overall: Symmetric and bijective outperform both IBM Model 4 and the baseline HMM, with symmetric slightly better than bijective. The results for common words vary more slowly as we increase the quantity of training data than they did for the full corpus. In the top right panel of Figure 7 we show the precision for rare words. For the baseline HMM as well as for IBM Model 4, this is very low precisely because of the garbage collector problem: Rare words become erroneously aligned to untranslated words, leading to low precision. In fact the constrained models achieve absolute precision improvements of up to 50% over the baseline. By removing these erroneous alignments the translation table becomes more accurate, allowing higher re-call on the full corpus. In the bottom panel of Figure 7, we observe a slightly diminished recall for rare words. This slight drop in recall is due to moving the mass corresponding to rare words to null . 4.4 Symmetrization
As discussed earlier, the word alignment models are asymmetric, whereas most appli-cations require a single alignment for each sentence pair. Typically this is achieved by a symmetrization heuristic that takes two directional alignments and produces a single alignment. For MT the most commonly used heuristic is called grow diagonal final (Och and Ney 2003). This starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points.
The alignment produced has high recall relative to the intersection and only slightly used, because one wants to have high precision links to transfer knowledge between languages. One pitfall of these symmetrization heuristics is that they can obfuscate the link between the original alignment and the ones used for a specific task, making errors based translation system, it is not clear when they will help and when they will hinder system performance. In this work we followed a more principled approach that uses 496 the knowledge about the posterior distributions of each directional model. We include a point in the final alignment if the average of the posteriors under the two models for that point is above a threshold. This heuristic is called soft union (DeNero and Klein 2007). Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.
The posterior regularization X  X rained models still performed better, but the differences get smaller after doing the symmetrization. This should not be very surprising, because the soft union symmetrization can be viewed as an approximation of our symmetry constraint applied only at decode time. Applying the symmetrization to the model with symmetry constraints does not affect performance. 4.5 Analysis
In this section we discuss some scenarios in which the constraints make the alignments collector effect and how both models address it. Both of the constraints also bias the model to have at most probability one in any row or column of the posterior matrix, encouraging 1-to-1 alignments. Obviously whenever alignments are systematically not 1-to-1 , this can lead to errors (for instance the examples described in Section 2). English/French sentence pair using the same notation as in Figure 1. In the top panel of
Figure 9, we see the baseline models, where the English word met is incorrectly being aligned to s  X  eance est ouverte . This makes it impossible to recover the correct alignment house/s  X  eance . Either constraint corrects this problem. On the other hand, by enforcing a 1-to-1 mapping the correct alignment met / est ouverte is lost. Going back to the first row (regular HMM) this alignment is correct in one direction and absent in the other (due to the many-to-1 model restriction) but we can recover that information using the symmetrization heuristics, since the point is present at least in one direction with high probability mass. This is not the case for the constraint-based models that reduce the mass of that alignment in both directions. Going back to the right panel of Figure 8, we can see that for low values of precision the HMM model actually achieves better recall type of problem, both with their caveats. One solution is to model the fertility of each word in a way similar to IBM Model 4, or more generally to model alignments of multi-ple words. This can lead to significant computational burden, and is not guaranteed to improve results. A more complicated model may require approximations that destroy option is to perform some linguistically motivated pre-processing of the language pair to conjoin words. This of course has the disadvantage that it needs to be specific to a language pair in order to include information such as  X  X nglish simple past is written using a single word, so join together French pass  X  e compos  X  e. X  An additional problem with joining words to alleviate inter-language divergences is that it can increase data sparsity. 5. Task-Specific Alignment Evaluation
In this section we evaluate the alignments resulting from using the proposed constraints restrict the number of possible minimal translation units; and syntax transfer, where alignments are used to decide how to transfer dependency links. 5.1 Phrase-Based Machine Translation
We now investigate whether our alignments produce improvements in an end-to-end phrase-based machine translation system. We use a state-of-the-art machine translation system, 5 and follow the experimental setup used for the 2008 shared task on machine translation (ACL 2008 Third Workshop on Statistical Machine Translation). The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and direction; (4) symmetrize directional word alignments; (5) build phrase table; (6) tune weights for the phrase table. For more details consult the shared task description. evaluate the quality of the produced alignments, we keep the pipeline unchanged, and use the models described earlier to generate the word alignments in Step 3. For Step 4, we use the soft union symmetrization heuristic. Symmetrization has almost no effect on alignments produced by S-HMM, but we use it for uniformity in the experiments. We 498 of precision vs. recall, and pick the best according to the translation performance on development data. Table 2 summarizes the results for the different corpora. For refer-ence we include IBM Model 4 as suggested in the task description. PR training always outperforms EM training and outperforms IBM Model 4 in all but one experiment.
Differences in BLEU range from 0.2 to 0.9. The two constraints help to a different extent for different corpora and translation directions, in a somewhat unpredictable manner.
In general our impression is that the connection between alignment quality and BLEU scores is complicated, and changes are difficult to explain and justify. The number of iterations for MERT optimization to converge varied from 2 to 28; and the best choice of threshold on the development set did not always correspond to the best on the test set.
Contrary to conventional wisdom in the MT community, bigger phrase tables did not always perform better. In 14 out of 18 cases, the threshold picked was 0.4 (medium size phrase tables) and the other four times 0.2 was picked (smaller phrase tables). When we include only high confidence alignments, more phrases are extracted but many of these are erroneous. Potentially this leads to a poor estimate of the phrase probabilities.
See Lopez and Resnik (2006) for further discussion. 5.2 Syntax Transfer
In this section, we compare the different alignments produced with and without PR based on how well they can be used for transfer of linguistic resources across languages.
We used the system proposed by Ganchev, Gillenwater, and Taskar (2009). This system uses a word-aligned corpus and a parser for a resource-rich language (source language) in order to create a parser for a resource-poor language (target language). We consider a parse tree on the source language as a set of dependency edges to be transferred. For each such edge, if both end points are aligned to words in the target language, then the edge is transferred. These edges are then used as weak supervision when training a generative or discriminative dependency parser. In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average changing the threshold in MBR decoding of alignments, we can trade off accuracy of the transferred edges vs. transferring more edges. We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005) trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and
Spanish. Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links between words with incompatible POS tags. Figure 10 shows our results for transferring from English to Bulgarian (En  X  Bg) and from English to Spanish (En results are based on a corpus of movie subtitles (Tiedemann 2007), and are consequently shorter sentences, whereas the En  X  Es results are based on a corpus of parliamentary proceedings (Koehn 2005). We see in Figure 10 that for both domains, the models trained using posterior regularization perform better than the baseline model trained using EM. 6. Related Work
The idea of introducing constraints over a model to better guide the learning process has appeared before. In the context of word alignment, Deng and Byrne (2005) use a state-duration HMM in order to model word-to-phrase translations. The fertility of each restrictions, likelihood prefers to always use longer phrases and the authors try to con-trol this behavior by multiplying every transition probability by a constant  X &gt; 1. This encourages more transitions and hence shorter phrases. For the task of unsupervised dependency parsing, Smith and Eisner (2006) add a constraint of the form  X  X he average of the dependencies are between adjacent words), using a scheme they call structural annealing. They modify the model X  X  distribution over trees p factor  X  changes from a high value to a lower one so that the preference for short edges (hence a smaller sum) is stronger at the start of training.
 posteriors take. However, the approaches differ substantially from PR. Smith and Eisner (2006) make a statement of the form  X  X cale the total length of edges X , which depending on the value of  X  will prefer to have more shorter/longer edges. Such statements are not data dependent. Depending on the value of  X  , for instance if  X   X  will push for more short edges. By contrast the statements we can make in PR are of the form  X  X here should be more short edges than long edges X . Such a statement is data-dependent in the sense that if the model satisfies the constraints then we do not need to change it; if it is far from satisfying it we might need to make very dramatic changes. 500 rently developed the idea of using penalties based on posterior expectations of features to guide semi-supervised learning. They call their method generalized expectation (GE) constraints or alternatively expectation regularization. In the original GE framework, discriminative model, using conditional likelihood on labeled data and an  X  X xpectation regularization X  penalty term on the unlabeled data:
Notice that there is no intermediate distribution q . For some kinds of constraints this objective is difficult to optimize in  X  and in order to improve efficiency, Bellare, Druck, and McCallum (2009) propose interpreting the PR framework as an approximation the problem of incorporating partial information about latent variables into a Bayesian framework using  X  X easurements, X  and after several approximation steps, they arrive at the objective we optimize.

Taskar, and Klein (2006), although under a very different formalization. They de-fine a joint objective max product distribution distribution as a product of marginals: q ( z ) = i , j clear what objective the approximate procedure actually optimizes. 7. Conclusion
In this article we explored a novel learning framework, Posterior Regularization, for incorporating rich constraints over the posterior distributions of word alignments. We focused on the HMM word alignment model, and showed how we could incorpo-rate complex constraints like bijectivity and symmetry while keeping the inference in the model tractable. Using these constraints we showed consistent and significant improvements in six different language pairs even when compared to a more complex model such as IBM Model 4. In addition to alleviating the  X  X arbage collector X  effect, we show that the obtained posterior distributions better reflect the desired alignments. Both constraints are biasing the models towards 1-to-1 alignments, which may be inappro-priate in some situations, and we show some systematic mistakes that the constraints introduce and suggest possible fixes.
 based MT and syntax transfer. For phrase-based MT, the improved alignments lead to a modest increase in BLEU performance. For syntax transfer, we have shown that the number of edges of a dependency tree that can be accurately transferred from one language to another increases as a result of improved alignments.
 straints that are directly applicable to word alignments, such as preferring alignments that respect dependency tree structure, part of speech tags, or syntactic boundaries. Appendix A: Modified E-Step Dual Derivation divergence:
Assuming the set Q x = { q ( z | x ):  X   X  , E q [ f ( x , z )] corresponding Lagrangian is max where derivative with respect to  X  ,weget: Replacing back into L (  X  ,  X  )wegetthedualobjective: Acknowledgments 502
