 In contrast to factoid question answering (QA), non-factoid QA is concerned with questions whose an-swer is not easily expressed as an entity or list of en-tities and can instead be quite complex  X  compare, for example, the factoid question Who is the secre-tary general of the UN? with the non-factoid manner question How is the secretary general of the UN cho-sen? A significant amount of research has been car-ried out on factoid QA, with non-factoid questions receiving less attention. This is changing, however, with the popularity of community-based question The ability of users to vote for their favourite an-swer makes these sites a valuable source of training data for open-domain non-factoid QA systems.
In this paper, we present a neural approach to open-domain non-factoid QA, focusing on the sub-task of answer reranking, i.e. given a list of can-didate answers to a question, order the answers according to their relevance to the question. We test our approach on the Yahoo! Answers dataset of manner or How questions introduced by Jansen et al. (2014), who describe answer reranking ex-periments on this dataset using a diverse range of features incorporating syntax, lexical semantics and discourse. In particular, they show how discourse in-formation (obtained either via a discourse parser or using shallow techniques based on discourse mark-ers) can complement distributed lexical semantic in-formation. Sharp et al. (2015) show how discourse structure can be used to generate artificial question-answer training pairs from documents, and test their approach on the same dataset. The best performance on this dataset  X  33.01 P@1 and 53.96 MRR  X  is re-ported by Fried et al. (2015) who improve on the lexical semantic models of Jansen et al. (2014) by exploiting indirect associations between words us-ing higher-order models.

In contrast, our approach is very simple and re-quires no feature engineering. Question-answer pairs are represented by concatenated distributed representation vectors and a multilayer perceptron is used to compute the score for an answer (the proba-bility of an answer being the best answer to the ques-tion). Despite its simplicity, we achieve state-of-the-art performance on this dataset  X  37.17 P@1 and 56.82 MRR. We attribute this improved performance to the use of paragraph vector representations (Le and Mikolov, 2014) instead of averaging over word vectors, and to the use of suitable data for training these representations. 2.1 Learning Algorithm We use a simple feedforward neural network, i.e. a multilayer perceptron, to predict the best answer. As shown in Figure 1, the first layer of the network is a projection layer that transforms question-answer pairs into their vector representations. The vector representation for a question-answer pair ( q, a ) is a concatenation of the distributed representations q and a for the question and the answer respectively. Each representation is a real-valued vector of a fixed dimensionality d , which is a parameter to be tuned. The projection layer is followed by one or more hid-den layers, the number of layers and units in each of these layers are also parameters to be experimentally tuned. We use the rectified linear (ReLU) activation function. Finally, a softmax layer is used to compute the output probability p , i.e. the probabilities p 1 and p 2 of the negative (i.e. not best answer) and posi-tive (i.e. best answer) classes respectively. For each question, all its user-generated answers are ranked according to their probability of being the best an-swer, as predicted by the network.

Given a question-answer pair ( q, a ) , the possible values for the ground-truth label are 1 (best answer) and 0 (not a best answer). The network is trained by minimizing the L2-regularized cross-entropy loss function between the ground-truth labels and the network predictions on the training set. We use stochastic gradient descent to minimize the loss over the training set. The development set is used for early stopping. 2.2 Document Representations Our approach requires question-answer pairs to be represented as a fixed-size vector. We experimen-tally evaluate the Paragraph Vector model (PV) pro-posed by Le and Mikolov (2014). The PV is an ex-tension of the widely used continuous bag-of-words (CBOW) and skip-gram word embedding models, known as word2vec . However, in contrast to CBOW and skip-gram models that only learn word embed-dings, the PV is able to learn representations for pieces of text of arbitrary length, e.g. sentences, paragraphs or documents. The PV includes (1) the distributed memory (DM) model, that predicts the next word using the concatenation of the previ-ous words and the paragraph vector, that is shared among all words in the same paragraph (or sen-tence); (2) the distributed bag-of-words (DBOW) model, that  X  similar to the skip-gram model  X  pre-dicts words randomly sampled from the paragraph, given the paragraph vector. We experiment with both DM and DBOW models, as well as their combi-nation. For comparison with recent work in answer reranking (Jansen et al., 2014; Sharp et al., 2015), we also evaluate the averaged word embedding vec-tors obtained with the skip-gram model (Mikolov et al., 2013) (henceforth referred to as the SkipAvg model). 3.1 Data In order to be able to compare our work with pre-vious research, we use the Yahoo! Answers dataset that was first introduced by Jansen et al. (2014) and was later used by Sharp et al. (2015) and Fried et al. (2015). This dataset contains 10K How ques-tions from Yahoo! Answers. Each question has at least four user-generated answers, and the average number of answers per question is nine. 50% of the dataset is used for training, 25% for development and 25% for testing. Further information about the dataset can be found in Jansen et al. (2014).
Our approach requires unlabelled data for unsu-pervised pre-training of the word and paragraph vec-tors. For these purposes we use the L6 Yahoo! An-swers Comprehensive Questions and Answers cor-about 4.5M questions from Yahoo! Answers along with their user-generated answers, and was provided as training data at the recent TREC LiveQA com-petition (Agichtein et al., 2015), the goal of which was to answer open-domain questions coming from ner question dataset prepared by Jansen et al. (2014) and described in the previous paragraph, was ini-tially sampled from this larger dataset. We want to emphasize that the L6 dataset is only used for unsu-pervised pretraining  X  no meta-information is used in our experiments.

We also experiment with the English Gigaword newswire sources. Jansen et al. (2014) used this cor-pus to train word embeddings, which were then in-cluded as features in their answer reranker. 3.2 Experimental Setup Following Jansen et al. (2014) and Fried et al. (2015), we implement two baselines: the baseline that selects an answer randomly and the candidate retrieval (CR) baseline. The CR baseline uses the same scoring as in Jansen et al. (2014): the ques-tions and the candidate answers are represented us-ing tf-idf (Salton, 1991) over lemmas; the can-didate answers are ranked according to their cosine similarity to the respective question.

We use the gensim 7 implementation of the DBOW and DM paragraph vector models. The word em-beddings for the SkipAvg model are obtained with word2vec . 8 The data was tokenized with the Stan-
To evaluate our models, we use standard imple-mentations of the P@1 and mean reciprocal rank (MRR) evaluation metrics. To evaluate whether the difference between two models is statistically sig-nificant, statistical significance testing is performed using one-tailed bootstrap resampling with 10,000 iterations. Improvements are considered to be sta-tistically significant at the 5% confidence level (p &lt; 0.05). 3.3 Results In Table 1, we report best development P@1 and MRR of the multilayer perceptron trained on Ya-hoo! Answers (Jansen et al., 2014) data. Early stopping is used to maximize P@1 on the develop-ment set. The distributed representations, including the SkipAvg model, beat both random and candi-date retrieval baselines by a large and statistically significant margin. Likewise, the multilayer per-ceptron with DBOW and DM representations sig-nificantly outperform the SkipAvg representations. Both paragraph vector representations initially pro-posed by Le and Mikolov (2014)  X  DBOW and DM  X  provide similarly high performance, how-ever the DBOW model performs slightly better, with the improvement over the DM model being sta-tistically significant. Different dimensionalities of the pretrained vectors provide similar results, with 200 outperforming the rest by a small margin. The multilayer perceptron with combinations of differ-ent distributed representations reach slightly higher P@1 and MRR on the development set. However, these improvements over the 200-dimension DBOW model are not statistically significant.

Table 2 presents the results on the test set. We only evaluate the 200-dimension DBOW model and its combinations with other models, comparing these to the baselines and the previous results on the same dataset (we use the same train/dev/test split as Jansen et al. (2014)). The DBOW outperforms the baselines by a statistically significant margin. The combination of the DBOW, DM and SkipAvg mod-els provides slightly better results, but the improve-ment over the DBOW is not statistically significant. 3.4 Analysis Jansen et al. (2014) report that answer rerank-ing benefits from lexical semantic models, and de-scribe experiments using SkipAvg embeddings pre-trained using the English Gigaword corpus. Here we compare the performance of the reranker with dis-tributed representations pretrained on a large  X  X ut-of-domain X  newswire corpus (Gigaword), versus a smaller  X  X n-domain X  non-factoid QA one (L6 Ya-hoo). Figure 2 shows the development P@1 and MRR of the multilayer perceptron with DBOW model on the Yahoo! Answers dataset pretrained on 30M random paragraphs from the English Gigaword corpus versus the multilayer perceptron with DBOW model pretrained on the Yahoo L6 corpus containing about 8.5M paragraphs. We also evaluate the com-bination of the two models. The results highlight the importance of finding a suitable source of unla-belled training data since vectors pretrained on rea-sonably large amounts of Yahoo! Answers data are more beneficial than using a much larger Gigaword dataset.

Even our best model is still, however, far from be-ing perfect, i.e. for about 60% of questions, the an-swer selected as best by the author of the question is not assigned the highest rank by our system. We be-lieve that one of the reasons for that is that the choice of the best answer purely relies on the question X  X  au-thor and may be subjective (see Table 3). A possible useful direction for future research is to incorporate the user-level information into the neural reranking model. This approach has been recently found ben-eficial in the task of sentiment analysis (Tang et al., 2015).

Another potential source of error lies in the user-generated nature of the data. Yahoo! Answers con-tains a large number of spelling and grammar mis-takes (e.g. how do i thaw fozen [sic] chicken? ), non-standard spelling and punctuation (e.g. Booorrrri-inng!!!!! ). A common way to deal with this prob-lem is normalization (Baldwin et al., 2015). To determine whether this might be helpful, we nor-malized the data following the strategy described by Le Roux et al. (2012). We trained the DBOW model with 200 dimensions and applied the MLP, as described in Section 2. The best development P@1 was only 33.95, with MRR 54.23 (versus 39.91 P@1 and 58.68 MRR without normalization). Even though our preliminary experiments show that ap-plying lexical normalization results in significantly lower performance, further study is needed. One di-rection is in using character-level embeddings that have been proven promising for user-generated con-tent because of their ability to better handle spelling variation (Kim et al., 2015). We have conducted answer reranking experiments for open-domain non-factoid QA and achieved state-of-the-art performance on the Yahoo! Answers manner question corpus using a very straightfor-ward neural approach which involves represent-ing question-answer pairs as paragraph vectors and training a multilayer perceptron to order candidate answers. Our experiments show that representing the question-answer pair as a paragraph vector is clearly superior to the use of averaged word vectors. We have also shown that a smaller amount of unla-belled data taken from a CQA site is more useful for training representations than a larger newswire set.
In this paper, we use general purpose distributed document representations provided by Paragraph Vector models to represent question-answer pairs. Then a machine learning algorithm is used to rank the pairs. One possible direction for future research is in learning distributed document representations and the ranking simultaneously and applying more sophisticated recurrent models such as long short-term memory (LSTM) (Hochreiter and Schmidhu-ber, 1997) neural networks, that have been shown to be effective in similar tasks (Wang and Nyberg, 2015; Zhou et al., 2015).
 Thanks to Yvette Graham and the three anony-mous reviewers for their helpful comments and sug-gestions. This research is supported by the Sci-ence Foundation Ireland (Grant 12/CE/I2267) as part of ADAPT centre ( www.adaptcentre.ie ) at Dublin City University. The ADAPT Centre for Digital Content Technology is funded under the SFI Research Centres Programme (Grant 13/RC/2106) and is co-funded under the European Regional De-velopment Fund.

