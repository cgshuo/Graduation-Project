 The development of disastrous flood forecasting techniques able to provide warnings at a long lead-time (5-15 days) is of great importance to society. Extreme Flood is usually a con-sequence of a sequence of precipitation events occurring over from several days to several weeks. Though precise short-term forecasting the magnitude and extent of individual pre-cipitation event is still beyond our reach, long-term forecast-ing of precipitation clusters can be attempted by identifying persistent atmospheric regimes that are conducive for the precipitation clusters. However, such forecasting will suf-fer from overwhelming number of relevant features and high imbalance of sample sets. In this paper, we propose an inte-grated data mining framework for identifying the precursors to precipitation event clusters and use this information to predict extended periods of extreme precipitation and subse-quent floods. We synthesize a representative feature set that describes the atmosphere motion, and apply a streaming fea-ture selection algorithm to online identify the precipitation precursors from the enormous feature space. A hierarchical  X  Co rresponding Author re-sampling approach is embedded in the framework to deal with the imbalance problem.

An extensive empirical study is conducted on historical precipitation and associated flood data collected in the State of Iowa. Utilizing our framework a few physically meaning-ful precipitation cluster precursor sets are identified from millions of features. More than 90% of extreme precipita-tion events are captured by the proposed prediction model using precipitation cluster precursors with a lead time of more than 5 days.
 I.5.2 [ PATTERN RECOGNITION ]: Design Methodol-ogy; I.5.4 [ PATTERN RECOGNITION ]: Applications X  Weather Forecasting Experimentation, Algorithm, Performance Flood Forecasting, Online Streaming Feature Selection, Spatial-temporal Data Mining Recent catastrophic floods in Australia, Brazil, Pakistan, Thailand and United States call for reliable flood forecasts and long-lead times so that we can better prepare and re-spond to disastrous events. With the advancement of obser-vation network and computational power, we can now pro-vide skilled short-term (1-5 days) weather forecasts. Long-lea d flood forecasting, on the other hand, works in a dra-matically increased resolution of spatial grids, length of fore-cast intervals, number of variables and parameters related to physical processes, parameterizations, and interactions. Existing atmospheric models, relying on simple nonlinear deterministic systems, cannot deal with such a huge feature space to provide accurate long-range (5-15 days) predictabil-ity of weather [13]. Existing operational flood forecasting systems usually rely on precipitation inputs from observa-tion networks (rain gauges) and radar. Because of the limi-tation on the predictability of individual weather events [12], such precipitation inputs limit the flood forecasting to only several days.

On the other hand,we have observed that extreme floods are consequences of long sequence of heavy precipitation events occurring over extended periods. Certain atmospheric regimes (e.g., blocking [16, 18]) can lead to sequence of precipitation events over periods of several days to several weeks. For example, in Pakistan, four very large precip-itation events occurred during July, 2010, and the last of which triggered the record flood that started around July 28th (Figure 1a). During this period of heavy precipitation, there is a clear signature of blocking upstream over Russia (Figure 1b). The atmospheric regime behavior (i.e. block-ing) may be more predictable than day-to-day precipitation events and the prediction of such regimes can lead to the possibility of long-lead (i.e. 5-15 days) extreme flood fore-casting.

Data mining techniques have great potential to identify the precursors to the atmospheric regimes that typically lead to the occurrence of flood events as a consequence of series of precipitation events. There are two major challenges when transforming from the day-to-day forecasting to long-lead prediction:
In this paper, we develop an integrated data mining frame-work to efficiently deal with complex, high-dimensional, im-balanced atmospheric data to forecast precipitation clusters. The key components are two-fold:
Specifically, we first construct a feature space to com-prehensively represent relevant spatial and temporal atmo-spheric variables. To untangle the problem of identifying the precursors of precipitation clusters from extremely high-dimensional data , we preform online feature selection using the Fast Online Streaming Feature Selection (Fast-OSFS) algorithm to process one feature at one time. The algorithm dynamically selects strongly relevant and non-redundant fea-tures on the fly, and is ideal in dealing with huge feature spaces with high efficiency and effectiveness. To deal with the imbalance issue, we design a hierarchical re-sampling approach for the feature selection and prediction process. Finally, by building the  X  X ost-correlated X  datasets we train classification models to predict precipitation clusters on the evaluation set.

In summary, the contributions of this paper include:
The rest of this paper is organized as follow. In Section 2 we review the related works on flood forecasting and data mining techniques. An overview of our forecasting frame-work is presented in Section 3. Section 4 describes the data preprocessing approaches. Section 5 lists the precursor iden-tification process including the Fast-OSFS algorithm and the hierarchical re-sampling approach. We show our experimen-tal results in section 6 and conclude the paper in section 7.
Recent years the ensemble numerical weather prediction systems (EPS) have drawn an increasingly attention from the hydrological community [2]. Many flood forecasting sys-tems rely on precipitation inputs that usually come from observation networks (rain gauges) and radar [3, 2]. But for medium term forecasts (2-15 days ahead), EPS mod-els must be used, especially when upstream river discharge data is not available [8]. Ensemble forecasts of precipitation are replacing single (deterministic) forecasts for extending streamflow predictions beyond 48 hours. However, improve-ments in the prediction of precipitation have lagged behind the much more significant improvements made by opera-tional NWP models in forecasting many aspects of the large-scale circulation [1]. For example, [4] found using the Global Forecast System (GFS) the precipitation total was underes-timated and that the spatial distribution of the rainfall was degraded by the resolution of the global model. Any uncer-tainty in the prediction of flood inundation and flood wave propagation will be amplified by the large uncertainties in the prediction of rainfall and subsequent runoff generation [2, 15]
Feature selection that aims to select a desirable subset of existing features for predictive modeling has received con-siderable attention in statistics and machine learning in the past decades [6]. Recently, a new research approach has been proposed to solve feature selection problem where the feature space in data is extremely large, sometimes even in-finite as not all features can be presented in the beginning. This is in contrast with most of the traditional feature selec-tion methods which assume that all features are static and available to a learner before feature selection takes place. So far several algorithms have been proposed to battle this challenging problem, such as Grafting, Alpha-investing and Fast-OSFS.
Class imbalance problem has been recognized to be exist-ing in lots of application domains [20, 11]. Under-sampling, the method trying to balance class distribution through the random elimination of majority class examples, is very pop-ular in dealing with such problems [5]. In the domain of test categorization, when training a binary classifier, all the sam-ples in the training set that belong to the category is consid-ered as relevant (positive) training data and all the samples belong to all the other categories as non-relevant (negative) training data. It is often the case that there is an over-whelming number of negative training data especially when there is a large collection of categories, which is typically an imbalanced data problem. To overcome this problem, in the work of [19] an under-sampling strategy is introduced by select a subset of most-relevant non-relevant data from the negative training set. The essential idea in this approach is to obtain more balanced positive and negative training data through under-sampling. In our work we adopt the about ideas by performing a hierarchical under-sampling process in both the feature selection process and the prediction mod-elling process.
In this section we give a overview of the proposed frame-work (Figure 2) for identifying the precipitation cluster pre-cursors and forecasting periods of extreme precipitations that resulting floods. We address our two goals in the fol-lowing steps: In step 3 only the features in the evaluation set are used to create the  X  X ost-correlated X  datasets and leaving the class label in the evaluation set X  X ntouched X (Section 4.3). We use the X  X ost-correlated X  X atasets in the validation and forecast-ing process based on a hypothesis that our advance sampling process can improve the prediction model performance. The hypothesis is examined in the experiments. The identified precipitation cluster precursors are demonstrated and the forecasting model is evaluated using Recall , Precision and the F-measure . The classifier of adaboost with 1-knn as the weak learner is used to build the prediction model for both the validation and evaluation processes.
As a forecasting model, our data mining framework deals with two types of data: predictor variables (features) and criterion variable (class label). The predictor variables come from meteorology variables and the criterion variable is cal-culated using the historical precipitation data. In this sec-tion we introduce the data preprocessing approaches for both the two kind variables.
The goal of our forecasting framework is to identify se-quences of extreme precipitation events with a lead time of more than 5 days. In other words, we are trying to forecast an upcoming time period that has extreme heavy precipita-tions. In practise, we define any 21 days periods as extreme precipitation clusters if during which the total amount of precipitations reaches a historical high level (i.e., above the 95% percentile of the historical records). For example, we consider the day of July 1st a positive example if the total amount of precipitations from July 1st to July 21st is above 95% percentile of any sum of 21 days X  precipitations in the historical records. We label such days as positive samples and our forecasting model aims to identify all the positive samples in the evaluation set using the precursors with lead times of more than 5 days.
The precursors we are looking for are meteorological pre-dictor variables with certain spatial and temporal informa-tion. As meteorological variables, we have chosen several fields from the NCEP-NCAR Reanalysis dataset [9] on con-stant pressure surfaces that are typically used by meteo-rologists for making forecasts. The variables (Table 1) are chosen based on their fundamental importance in the basic physical processes involved in maintaining persistent large-scale flow regimes or in the production of precipitation. Specif-ically, we apply reasoning based on quasi-geostrophic (QG) theory and the theory of baroclinic [7]instability.
First, we chose the 300hPa zonal (i.e. east-west) winds, a proxy for the location and strength of the jet stream, which is important for several reasons. First, the jet stream serves as a waveguide directing the flow of Rossby waves as they propagate across the mid-latitudes. Rosby waves are impor-tant in the maintenance of persistent atmospheric regimes because they represent one mechanism through which en-ergy propagates across the globe (over periods from days to weeks) and is transferred to the zonal mean flow (i.e. maintaining the westerly winds). The location of the jet stream is also important because storms require wind shear (strong change in wind speed with height) to develop, which is strongest near the core of the jet stream. By knowing the location of the jet stream, which is well known to ex-hibit persistence on scales much longer than individual storm events, we have information about the location where storms are likely to develop over multi-day periods. The geopoten-tial height at 1000hPa and 500hPa are chosen because the 500hPa field will contain information about Rossby wave propagation and the two fields taken together allow us to infer where large-scale rising motion (and therefore precipi-tation) is likely to take place. The difference in the geopo-tential height between two constant pressure surfaces (i.e. the thickness) is proportional to the temperature of the at-mospheric layer. The quasi-geostrophic (QG) vorticity is also proportional to the Laplacian of the geopotential height while the geostrophic component of the wind is proportional to the gradient of the geopotential height. The QG omega equation then relates vertical motion (needed for the produc-tion of precipitation) to the advection of thickness (i.e. tem-perature) and QG vorticity (at two levels) by the geostrophic wind, all of which can be inferred from the geopotential height at two levels. The 850hPa meridional (i.e. North-South) wind is chosen because it is extremely important for the transport of heat and moisture from the tropics into the mid-latitudes. We also include the precipitable water (i.e. total column water vapor) and 850hPa temperature so that we include explicit information about the transport of moisture and heat into the mid-latitudes. The moisture transport is needed to maintain the precipitation while the advection of temperature is crucial for strengthening (weak-ening) temperature gradients and the production (destruc-tion) of fronts, which are important in producing vertical (i.e. rising) motion.
The contribution of the meteorological predictors to cer-tain precipitation clusters vary across space and time. For example, a predictor X  X  value near the north pole may af-fect the atmospheric regimes over the Canada with a lead time of 2 days, but its effect to the atmospheric regimes over Mexico may have a lead time of 5 days. To counter this problem we build a feature space with the spatial and tempo-ral information of the predictors to achieve a comprehensive coverage of the potential precursors. Particular, we sample every variable from 5,328 locations evenly distributed be-tween the equator and the North pole (37 latitudes and 144 longitudes), and we do such sampling with a time span of 10 days to cover the period of 6 to 15 days lead time. For the identification of precipitation cluster precursors. T able 1: Candidate meteorological predictor vari-ables that contribute to the atmospheric regimes leading to extreme precipitation clusters example, we use the predictor variables sampled from July 1st to July 15th to predict whether there is an upcoming extreme precipitation clusters in July 21st. By doing so we construct an enormous spatial and temporal feature space of 479,520 features (9 predictor variables time 5328 locations time 10 days).

Finally, both the feature set and the class labels are di-vided to three parts: candidate set, validation set, and evalu-ation set (Figure 2) with the purpose of building, validating, evaluating the forecasting model, respectively.
The task of identifying precipitation cluster precursors is completed through the feature selection and model valida-tion processes using re-sampling and feature selection tech-niques. In this section we firstly give a series of formal no-tations and definitions related to the Fast Online Streaming Feature Selection (Fast-OSFS) algorithm. Then we intro-duce the streaming feature selection process by demonstrat-ing the pseudo-code of Fast-OSFS and the hierarchical re-sampling processes.
To characterize relevance between meteorological features and precipitation clusters, an input feature can be in one of three disjoint categories, namely, strongly relevant, weakly relevant or irrelevant [10]. Let F = { F 1 ; F 2 ; :::; F sent the full set of the meteorological features constructed in Section 4.3, C denotes the class attribute (sum of upcoming 21 days precipitations, Section 4.1 ) and F  X  X  F i } represent the feature subset excluding F i .

Definition 1 (Conditional Independence) Two dis-tinct features F i  X  F and F k  X  F are conditionally indepen-dent on a subset S  X  F  X  X  F i  X  F k } , iff P ( F i | F k or P ( F k | F i ; S ) = P ( F k | S )
Definition 2 (Strong Relevance) A feature F i is strongly relevant to C iff
Definition 3 (Weak Relevance) A feature F i is weakly relevant to C iff it is not strongly relevant, and Definition 4 (Irrelevance) A feature F i is irrelevant to C iff it is neither strongly nor weakly relevant, and
Weakly relevant features can be further divided into re-dundant features and non-redundant features [22].
Definition 5 (MB: Markov Blanket) The Markov blanket of feature F i , denoted as M i  X  F  X  F i makes every other feature independent of F i given its Markov blanket M i , that is,
Definition 6 (Redundant Features) A feature F i is redundant and hence should be discarded, iff it is weakly relevant and has a Markov blanket within the current set of features.
The pseudo-code of the Fast Online Streaming Feature Se-lection (Fast-OSFS) method is shown in Algorithm 1. Fast-OSFS employs a two-phase optimal subset discovery scheme: online relevance analysis (lines 5-8) and online redundancy analysis (lines 9-21). In the relevance analysis phase, Fast-OSFS discovers strongly and weakly relevant meteorological features and adds them into the set of best candidate pre-cursors so far ( BCF , Best Candidate Features). When a n ew meteorological feature arrives, Fast-OSFS assesses its relevance to the upcoming precipitation clusters ( C ) and decides to either discard the new feature or add it to BCF according to its relevance. Once a new feature is included into BCF, the redundancy analysis phase is triggered. If a subset exists within BCF to make any existing feature in BCF and the class attribute C conditionally independent, the previously selected candidate precursor Y ( Y  X  BCF ) becomes redundant and is removed from BCF (line 18). Al gorithm 1: The Hotspot Optimization Tool Data : X ; Y : features
BCF : the best candidate feature set 1 BCF = {} ; 2 repeat 3 added = 0; 4 X  X  get new f eature () 5 /*online relevance analysis */ 6 if Dep ( C; X | X  ) then 7 a dded=1; 8 end 9 /*Redundancy analysis 1:*/ 10 if added then 11 i f  X  S  X  BCF s:t:Ind ( C; X | S ) then 12 g o to Step 3 /*Discard X */ 13 end 14 BCF = BCF  X  X ; 15 /*Redundancy analysis 2: */ 16 for each feature Y  X  BCF  X  X do 17 if  X  S  X  B CF s:t:Ind ( C; X | S ) then 18 B CF = BCF  X  Y ; 19 end 20 end 21 end 22 until a prede ned accuracy satis ed ; 23 output BCF
As motioned in Section 1, the extreme precipitation clus-ters we aimed are rare events. Particular, the studied datasets built with the label of such clusters (Section 4.1) are ex-tremely imbalanced, with a positive sample and negative sample rate of 1:19. To deal with this problem we develop a hierarchical re-sampling approach that includes a under sampling process in the feature selection part and an ad-vanced sampling process in the model validation part.
As discussed in the work of [23], feature selection methods using two-sided metrics combine the positive and negative features so as to optimize the accuracy ( T P + F P In case of an imbalanced dataset with much more negative samples than the positive samples, two-sided metrics can-not ensure the optimal combination of positive and negative features according to F-measure ( 2 T P counter this problem we apply an systematic under-sampling approach to achieve balanced datasets for the best perfor-mance of our feature selection algorithm.

Particularly, we count the number of extreme precipita-tion clusters (positive samples)in the candidate set, and ran-domly choose the same amount of negative samples from the set and combine them to create a new balanced feature set. The major drawback of the under sampling process is that it may discard potentially useful meteorological features that could be important for forecasting. In our work we perform the under-sampling N times and use the results to construct N balanced features sets. Then we run the streaming fea-ture selection algorithm to identify candidate precursor sets from the balanced sets.
The subsets of features generated by the Fast-OSFS al-gorithm constitute a series of candidate precursor sets. To identify the best set of precursors, prediction models are built and evaluated using the validation set. Instead of us-ing all the samples in both the candidate precursor sets and the validation set, we hypothesize that using the  X  X ost-correlated X  datasets generated through an advanced sam-pling approach can improve the prediction performance in flood forecasting.

Particularly, we are looking for datasets (both the can-didate precursor set and the validation set ) in which the features are most-correlated with the evaluation set. For example, assuming we have a dataset D ( D can be a can-didate set or the validation set) containing 10 years X  data and a evaluation set T with 2 years X  data. Both of them contain the same two candidate precursors: feature a and b . To avoid confusion we name the two precursors in D Da and Db , and the ones in the evaluation set Ea and Eb . We firstly divide D into 9 partitions with each part having the same length of samples as the evaluation set (i.e. 1st-2nd years, 2nd-3rd years,...). For each partition we calculate the corre-lation coefficients (Formula 5) between Da and T a , Db and T b , respectively. Then we sum the absolute values of the two correlation coefficients for each partition and sort the partitions using the sum values from high to low. The top t partitions will be selected to construct the most-correlated dataset. We call this process  X  X dvance sampling X  instead of  X  X nder sampling X  because in the most-correlated dataset some of the samples may be over sampled, i.e. dataset con-taining the partitions of 2nd-3rd years and 3rd-4th years over samples the 3rd year data. where X and Y are two features and X a nd Y are the aver-ages of the features, respectively.

The most-correlated datasets are built for each candidate precursor sets and evaluated on the most-correlated valida-tion set. The candidate precursor set with the best pre-diction performance is selected. We test the hypothesis of using most-correlated datasets for better performance in the experiments.
In this section we present the experimental results from our forecasting framework. We first introduce the historical data used in the experiments. Then, we show and discuss the advantages of using most-correlated dataset using the N umber of Ra ndom Selected feature sets for comparison. experimental results. The performance of the forecasting model is evaluated at the end.
The dataset used in our study has 23,011 observations over 63 years (from January 1st, 1948 to December 31st, 2010) and each observation is described by a set of 479,520 features (9 variables time 5,328 locations time 10 days). Historical spatial average precipitation data (the mean of daily pre-cipitation totals from 22 stations divided by the standard deviation) of the state Iowa from the same time period is used to create the class label. The dataset of 1948-1998 (51 years) is used as the candidate set (Figure 2). The other 12 years data (1999-2010) are further divided into the val-idation set and the evaluation set in a rotated manner: 10 years for validation and the remained 2 years for evaluation.
For each pair of the validation and evaluation sets, we run the Fast-OSFS algorithm 10 ( N = 10) times with ran-dom under sampled balanced candidate feature sets. We also conduct random feature selection on the same candi-date feature sets for comparison. For each round, the most-correlated datasets are identified firstly using the advanced sampling process we proposed in section 4.3 with t = 1 for the most-correlated validation sets and t = 5 for the most-correlated candidate sets. Then the candidate sets (both from Fast-OSFS and random feature selection) having best performance during the validation processes are selected as the precursors and evaluated on the evaluation set (Table 2). As we are trying to predict a 21-days period of heavy precip-itation, all of the evaluation results are adjusted by using a tolerance zone of one day. For example, if in the evaluation set June 1 st is a positive example, we consider the positive prediction of May 31 st or June 2 nd a  X  X rue positive X .
The model evaluation results are shown in Table 2. On av-erage the forecasting models built using the most-correlated precursors capture 86% (Average Recall=0.86) extreme pre-cipitation clusters that are conducive for flooding in the eval-uation sets with a precision of 74% (Average Precision=0.74). The overall performance of the models is evaluated with the average F-measure (0.79). The precursors selected using the Fast-OSFS algorithm significantly improve the prediction compare to the random selected features(F-measure: 0.79 compared to 0.23)
The identified 12 precursors that contribute to the ex-treme precipitation clusters in the state of Iowa between 2009-2010 are demonstrated using the map shown in Figure 3. For example, the blue square on the map shows that dur-ing the year of 2009-2010, the 850hPa zonal wind in that location has a significant effect on the upcoming extreme precipitation clusters in the state Iowa with a lead time of 7 days. All the precursors come from 4 out of the 9 meteo-rological predictor variables used in the model. Some of the unselected features may also have considerable influence on target precipitation clusters. The reason of discarding such influence features is because they are considered redundant by the Fast-OSFS algorithm giving the selected precursors. Also, we are encouraged to note that several identified pre-cursors, like the precipitable water over the Gulf of Mexico, are physically meaningful.
We introduce the most-correlated datasets in our frame-work with two hypothesises: The two hypothesises are tested using the experimental data. Particularly, we construct random datasets having same length with the most-correlated sets and compare the model perfor-mance between them. In the tests, datasets from 1948-1998, 1999-2008, 2009-2010 are used as the candidate set, valida-tion set, and evaluation set, respectively. Based on the three parts 10 most-correlated candidate sets, 1 most-correlated validation set and 1 most-correlated precursor set are built.
To test the first hypothesis, we randomly selected a dataset (the  X  X andom validation set X ) from the validation set (1999-2008) that having the same length and no over-lapping with the most-correlated validation set. Then we build 10 mod-els using the 10 most-correlated candidate sets and evaluate them using the random validation set, the most-correlated validation set, and the evaluation set, respectively (Figure 4). Using the most-correlated validation set the best can-didate set (NO.2) is successfully identified and the random validation set fails to achieve this. In the test the models X  performance on the random validation sets are low. This is because the models are built using the most-correlated can-didate sets which are customized for the evaluation set and Fi gure 4: The models built using the 10 most-correlated candidate sets are evaluated on the ran-dom validation set, the most-correlated validation set, and the evaluation set, respectively. The best candidate set (set 2) is successfully identified using the most-correlated validation set. there are very limited correlations between the evaluation set and the random validation set.

Secondely, we randomly sampled 9 datasets from the 1948-1997 dataset using the same features and the same length (10 years) as the most-correlated precursor set. We evalu-ated the models built using the most-correlated precursors and the 9 datasets on the evaluation set (Figure 5). The model built using the most-correlated precursors achieves a F-measure that is much higher than the other models.
Improving the reliability and lead times of flood forecasts is critical for providing early earnings required to mobi-lize better preparedness for and response to disastrous flood events. In this paper, we discuss an integrated end-to-end data mining framework on precursor identification, dimen-sionality reduction, model validation and prediction to ana-lyze the flood triggering precipitation clusters. In our future work, we want to explore the impact of the sequential order Fi gure 5: The models built using the 9 random sets and most-correlated precursor set are evaluated on the evaluation set. The precursors X  model (right-most) has the best performance. of the streaming features on precursor identification. Also, we want to explore an alternative class labeling process in the training data. Finally, we plan to extend our analy-ses to other land. The framework is capable to be applied to other geographic areas because the candidate meteoro-logical variables are collected globally. The project team is currently participates in flood warning planning organized by the World Bank and the Government of Pakistan. [1] J. P. Charba and F. G. Samplatsky. High-resolution [2] H. Cloke and F. Pappenberger. Ensemble flood [3] A. P. de Roo, B. Gouweleeuw, J. Thielen, [4] S. Dravitzki and J. McGregor. Predictability of heavy [5] X. Guo, Y. Yin, C. Dong, G. Yang, and G. Zhou. On [6] I. Guyon and A. Elisseeff. An introduction to variable [7] I. M. Held, R. T. Pierrehumbert, S. T. Garner, and [8] T. M. Hopson and P. J. Webster. A 1-10-day ensemble [9] E. Kalnay, M. Kanamitsu, R. Kistler, W. Collins, [10] R. Kohavi and G. H. John. Wrappers for feature [11] S. Kotsiantis, D. Kanellopoulos, and P. Pintelas. [12] E. N. Lorenz. Deterministic nonperiodic flow. Journal [13] M. C. Morgan, D. D. Houghton, and L. M. Keller. [14] F. Pappenberger, K. J. Beven, N. Hunter, P. Bates, [15] F. Pappenberger and R. Buizza. The skill of ecmwf [16] J. L. Pelly and B. J. Hoskins. A new perspective on [17] S. Perkins and J. Theiler. Online feature selection [18] C. Schwierz, M. Croci-Maspoli, and H. Davies. [19] A. Singhal, M. Mitra, and C. Buckley. Learning [20] S. Visa and A. Ralescu. Issues in mining imbalanced [21] X. Wu, K. Yu, W. Ding, H. Wang, and X. Zhu. Online [22] L. Yu and H. Liu. Efficient feature selection via [23] Z. Zheng, X. Wu, and R. Srihari. Feature selection for [24] J. Zhou, D. P. Foster, R. A. Stine, and L. H. Ungar.
