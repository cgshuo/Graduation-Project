 The advent of Web 2.0 has determined the generation of a huge amount of user generated information such as online reviews; this phenomenon has increased in recent years, as more and more online users tend to write textual reviews to share their personal experiences and opinions on the purchased products. Online reviews are an invaluable information resource, and they have become increas-ingly important in decision making processes [ 2 ] and promoting business [ 15 ]. However, the volume of review data is overwhelming while the quality of reviews varies greatly. Hence, some researchers have made efforts to evaluate the review quality/helpfulness automatically [ 2 , 3 , 6 ]. In detail, existing approaches exam-ine a number of data features about the reviews such as writing quality and writer X  X  expertise to find the hidden association between the review quality and these defined features. However, such review quality prediction approaches usu-ally require considerable time and resources such as training and review data labelling. Meanwhile, another set of studies aim to extract a subset of useful reviews as a representation of the original review corpus [ 4 , 11 ]. For instance, Lappas et al. [ 4 ] proposed to extract a set of reviews that accurately reflect the proportion of opinions of each product feature (called opinion distribution) in a collection of reviews. One significant drawback of this method is that the utility of the selected reviews is only measured as a whole in terms of opinion distribu-tion, while the quality of each individual review has been largely ignored, which could allow some low-quality reviews to be included in the selected review set. Researchers have made a lot of efforts to find relationship between different terms or concepts more effectively and accurately. By making use of various techniques such as text mining, we are now able to generate product ontology or taxonomy about product features and relationships between features from data about products or even from user generated information such as tags and review text [ 1 , 10 ]. In this paper, we propose a method, called Quality-Aware Review Selection (QAS), to assess the review quality by using a product feature taxonomy to improve the performance of review selection. More specifically, we propose not only to select a small set of reviews which retain the same opinion distribution (i.e., positive and negative opinion proportion) as in the original review collection, but also we ensure that each of the selected reviews is of good quality by examining the comprehension of the features mentioned in the review. be discussed in Sect. 2 . Section 3 provides an insight about our proposed quality-aware review selection approach. The evaluation of our approach is reported in Sect. 4 . Finally, we conclude the paper in Sect. 5 . 2.1 Review Helpfulness Prediction A number of studies focused on identifying data features related to review text or review content, on which a review X  X  quality or helpfulness can be learnt [ 3 , 6 , 9 ]. In detail, these approaches formulated the review helpfulness prediction as a regression or classification problem. For instance, Liu et al. [ 7 ] made use of radial basis functions to predict the helpfulness of movie reviews based upon three factors: reviewer expertise, writing style and timeliness. O X  X ahony et al. [ 9 ] investigated a number of structural features related to review texts such as the percentage of uppercase characters and the number of sentences of reviews. They made use of a classifier to distinguish helpful and unhelpful reviews based upon identified structural features. In addition, Lu et al. [ 8 ] proposed to exploit and utilize contextual information about the writers X  identities and social networks to improve review quality estimation.
 Meanwhile, some researchers attempted to value the reviews by examining mentioned features and opinions from users. For instance, Liu et al. [ 6 ]con-sidered the number of product features in the review one of the most impor-tant indicators of the review helpfulness. They employed SVM (Support Vector Machines) as the model of classification to filter out low-quality reviews so that a more accurate opinion summarization (e.g., average feature rating score) can be achieved. Zhang et al. [ 13 ] proposed a supervised method to predict the utility of the reviews by taking sentiments into account. Specifically, the subjective words that express users X  opinions were considered one factor for learning the utility of the reviews. 2.2 Review Selection A number of recent works have focused on extracting reviews for the users based upon reviews X  utility. The aim is to preserve the integrity and immediacy of actual reviews. For instance, Tsaparas et al. [ 11 ] and Xu et al. [ 12 ] attempted to generate a subset of reviews which provide comprehensive information of the reviewed product by covering all product features and both positive and nega-tive opinions toward each feature. They formulated it as a maximum coverage problem. However, this formulation fails to capture the proportion of opinions in the review corpus since each opinion polarity on features is treated equally. In order to overcome this shortcoming, Lappas et al. [ 4 ] proposed to select a subset of reviews that emulate the opinion distribution in the original review collection as accurately as possible. They formulated this task as a combinato-rial optimization problem. In detail, the utility of the generated review set is measured based upon how the proportion of opinions in the selected reviews is close to that in the original review corpus. One major problem of this approach is that the utility of selected reviews is only benchmarked as a whole, while the quality of each individual review has been omitted. 2.3 Taxonomy or Ontology Learning Meanwhile, product classifications or taxonomies are often available, provided by product manufacture organizations or companies for promotion or marketing purposes. Moreover, ontology learning is a wide studied area. In recent years, some researchers seek to create a hierarchical structure about products or items from user generated content. Djuana et al. [ 1 ] proposed both to construct a tag ontology from folksonomy based on WordNet, and to personalize the tag ontology based on user clusters. Furthermore, Tian et al. [ 10 ] presented an approach to construct a hierarchical product profile which contains product features and relationships between them. Specifically, the proposed model identifies a group of frequent patterns as the potential features to assist selecting useful association rules for finding the relationships between features. Prior work has basically employed supervised learning approaches such as classi-fication to estimate the quality of the reviews based on statistical features related to review texts, which ignores the usefulness of information buried in reviews. For instance, a review written in a very professional style is still useless to a user if it lacks the discussion of the features concerned by the user. According to Liu et al. [ 6 ], a review of good quality should be a rather complete and detailed comment on a product, by presenting several aspects of a product. According to this observation, we attempt to find a way to formulate this characteristic for determining the quality of reviews. 3.1 Product Feature Taxonomy Reviews vary in terms of coverage and focus by considering different product features. More specifically, some reviews talk about a number of unrelated fea-tures, while others may focus on one or several specific features only by analysing them from different angles. We believe that a review X  X  helpfulness or quality can be better predicted if considering how it covers the product features than when analyzing its textual features or the writer X  X  reputation.
 a structural profile of the product that provides the relationships between its different features. It could be a standard ontology provided by domain experts or an ontology automatically generated from domain data such as reviews by using ontology learning methods. In this work, we make use of the product profile called product feature taxonomy proposed in [ 10 ], defined below, for assisting the review analysis. Figure 1 shows part of a feature taxonomy for a product (i.e., digital camera) generated from a collection of reviews. As shown, it is a tree structure describing the relationships between the product features. Definition 1 (Feature Taxonomy) : We formally represent a feature taxonomy as a set of features and their relationships, denoted as FT = has the following constraints and terms that are used in this paper: (1) The link which connects a pair of features represents the sub-feature rela-tionship. For f i ,f j  X  F ,if f j is a direct sub-feature of f a link in the taxonomy and ( f i ,f j )  X  L , which indicates that f specific than f i . f i is called the parent feature of f (2) Except for the root, each feature has only one parent feature. This means that the taxonomy is structured as a tree. (3) The root of the taxonomy represents the product itself. 3.2 Review Comprehension The product feature taxonomy provides a thorough picture of the hierarchical relation between different features of a certain product, which can be used as an indicator of review quality. For instance, if a user describes vivid color and high resolution of the pictures captured by a camera, the quality of picture is discussed in depth in this review. Based on the observation in [ 6 ], we believe such review is of good quality. Therefore, we first need to identify all features mentioned in the review. Due to the fact that the same feature may be represented in different ways, we make use of WordNet to find synonyms of a certain feature to address this problem. A feature is considered included in the review if a similar word to the feature name, according to a semantic similarity, is found in the review instead of the exact feature name. In order to measure how comprehensive a review is, we propose to examine the coverage of the features in the review. The coverage of a feature can be determined based on its sub features in the product feature taxonomy. In Definition 2 below, a notion of maximum sub tree is defined. In this paper, we propose to generate all maximum sub trees in a review according to its identified features, then measure the comprehension of the review based on its maximum sub trees.
 Definition 2 (Maximum Sub Tree) : Let FT = { F, L } be a product feature taxonomy, F r  X  F be a set of features identified from review r and f a specified feature. The maximum sub tree rooted at f is defined as MST {
SF ,SL f } which satisfies the following constraints:  X  f is the root of the sub tree, f  X  SF f  X  SF  X   X  g  X  SF feature taxonomy FT , f i  X  SF f , i =1 , ...n , and ( f, f
SL f , j =1 , ..., n  X   X  g  X  F g in the feature taxonomy FT and f i  X  SF f , i =1 , ..., n .
 These maximum sub trees are disjoint, i.e., there is no overlap between any two sub trees. The features in one sub tree are considered related in terms of F since they are linked in the sub tree, while the features from different sub trees are not considered related since there is no path or link between these features. Let MST r = { MST r,f 1 , ..., M ST r,f m } be a set of m maximum sub trees to F r . For example, if a review contains  X  X icture X  ,  X  X olor X  ,  X  X ode X  ,  X  X uto X  ,  X  X acro X  ; according to the feature taxonomy shown in Fig. 1 , we can generate two maximum sub trees: one consists of  X  X icture X  and  X  X olor X  , the other consists of  X  X ode X  ,  X  X uto X  and  X  X acro X  .
 review discusses the features of the considered product. We employ the sub-feature relation of the feature taxonomy to measure the degree of comprehen-sion of a review. Specifically, the more numerous are the sub features of a given feature in a review, the more comprehensive with respect to that fea-ture the review is. Therefore, we calculate the ratio between the number of the feature X  X  direct sub features appearing in the review and the total number of direct sub features in the feature taxonomy based upon the generated maximum sub trees. Let MST FT,f = { SF f ,SL f } be a maximum sub tree of which fea-ture f is the root; we can derive all direct sub features of f in the taxonomy: DSF f ( FT )= { sf | sf  X  Fand ( f, sf )  X  L } . Hence, the comprehension of f can be derived by the following equation: any sub feature. In such case, the comprehension degree with resepct to that feature is 1. As to the comprehension of a review r , we calculate the average comprehension value of the features in r as the comprehension value of the review: so that it can be employed to assess the review quality. This measure allows to assess the quality of both reviews that provide in-depth information on various product features and reviews that focus on one single feature. For instance, if a review discusses of a single feature by covering all or most of its sub features, the comprehension value of this review will be still quite good. The higher the comprehension value a review obtains, the better quality this review has. 3.3 Characteristic Review Selection Lappas et al. [ 4 ] believe that selecting a subset of reviews which preserves the opinion distribution statistics in the underlying review corpus brings a great balance between review summarization and review selection. Given a collec-tion of reviews R and a list of features F , a feature-opinion vector  X  ( R )is f = # Negative f i | R | measure the proportion of positive and negative opinion for each product feature in R .  X  ( R ) can be constructed by using sentiment analysis techniques such as [ 5 ]. To find the best set of n reviews SR as the representa-as much as possible. D (  X  ( SR ) , X  ( R )) is used to indicate the distance between two feature-opinion vectors. A Greedy algorithm based on opinion distribution distance has been introduced in their work.
 The selected reviews as a whole reflect the characteristics of the original review corpus and also avoid redundant information as other prior work does. Inspired by this motivation, we aim to combine our work and this idea together to improve the performance of the review selection task. 3.4 Quality-Aware Review Selection There are two obvious drawbacks for the above characteristic review selection method. Firstly, the utility of a review is only determined based on if it can min-imize the opinion distribution distance between the extracted review set SR and the review corpus R without examining the review X  X  actual quality. Therefore, a low-quality review could be selected if it can make the SR obtain a smaller opinion distribution distance value. Secondly, when multiple reviews make the current SR achieve the same minimum D (  X  ( SR ) , X  ( R )), this method fails to distinguish them to determine which review should be the most appropriate one to be chosen. In such case, the review with the top position according to the order of the reviews in the original corpus will be selected. In other words, the same review dataset with different order might lead to a different result. To overcome the aforementioned shortcomings, we propose to select the reviews based on both their review comprehension score and their opinion dis-tribution. In this work we employ the sentiment analysis algorithm proposed by Lau et al. [ 5 ] to identify all feature-sentiment pairs and to determine the orien-tation (positive or negative) of each sentiment of a feature so that the opinion distribution can be generated. Then, in the i th iteration of the selection process, we aim to select one review to form a set SR of i reviews that can make  X  ( SR ) close to  X  ( R ). In each iteration, the strategy is to first find a set of candidate reviews, then from the candidate reviews to select the most appropriate review. The number of candidate reviews is dynamically determined based on the opin-ion distribution distance between all potential review set SR and the original review set R .
 In the i th iteration, let SR i  X  1 be the current selected review set, for all remaining reviews in R \ SR i  X  1 , the minimum opinion distance between all poten-tial review set SR i and the original review set R is defined as: According to D i min , we extract a number of reviews whose derived opin-ion distribution distance value is close to D i min as the candidates for further selection. The following equation is defined to measure how close the opinion distribution distance D (  X  ( SR i  X  1  X  X  r } ) , X  ( R )) is to the minimum distance D can generate a set of candidate reviews as defined below in the i th iteration: where  X  is a threshold. In the experiments reported in Sect. 4 ,0 &lt; =  X &lt; = D 5 %. Among the identified candidate reviews, we calculate their comprehension value based upon the given product feature taxonomy and choose the one which obtains the highest comprehension score as the best review to be added to the algorithm employed for the reviews selection is sketched.
 Algorithm 1. Quality-Aware Review Selection Algorithm In this section we present the experiment that we performed to evaluate the effec-tiveness of our proposed approach. Our experiment is carried out using real data collected from one of the most popular e-commerce websites: www.amazon.com . On Amazon, users are able to vote each review as helpful or unhelpful from their perspective, and the form  X  X  of n people found the following review helpful X  indi-cates the helpfulness voting. Therefore, we use the ratio between the number of positive votes and the total number of votes as the gold standard of the review quality, which indicates if the review is preferred by users. For instance, the help-We use six datasets of reviews for testing (5 cameras and 1 laptop). Each dataset consists of all online users X  reviews for a certain product. In addition, there are more than 350 reviews in each dataset averagely. We use the review selection method proposed in [ 4 ] as the baseline for compar-ison. In detail, their proposed Greedy algorithm has a satisfactory performance in terms of running time and capturing the opinion distribution of the original review corpus. In order to provide a comprehensive and objective evaluation, we benchmark the effectiveness of our proposed method from three aspects: quality of the selected reviews, the opinion distribution distance between the selected reviews and the review corpus, and product rating results derived from the selected reviews. For each review dataset, both selection models are employed to generate the result set of n reviews ( n = 10, 20, 30) for 10 runs. Particularly, in each run, we shuffle the review dataset to change the order of the reviews. All experimental results are derived from the average of all runs.
 First of all, we compare the average helpfulness score of n selected reviews in the generated review set to check if our method is able to find reviews of better quality. More specifically, the ratio between helpfulness votes and total votes provided by the review is used as the helpfulness score to indicate the review quality. The average helpfulness score of all reviews in generated review set from both models is calculated. The higher the average helpfulness score obtained, the more the reviews of better quality have been selected. The experimental results are reported in Table 1 below.
 Table 1 illustrates the average helpfulness scores of 10, 20, and 30 reviews selected by the baseline and by our proposed method, respectively. From the results, we can see that the review set produced by our method always obtains better helpfulness score than the baseline does. It is because that in each iter-ation of the review selection process in which a new review is to be added into the existing review set, our method always chooses the review of best quality according to the comprehension value from a number of reviews instead of the one which achieves the minimum opinion distribution distance. As a result, this proves that making use of structural relationships between features to assess review quality is effective.
 can still preserve the characteristics of the original review corpus with respect to the opinion distribution. Thus, we calculate the opinion distribution distance between the selected reviews and the review corpus for both the proposed model and the baseline model. The experimental results are shown in Table 2 . and opinion distribution. Based upon the comparison provided by Table 2 ,we can see that our method selects reviews of much better quality without sacrificing the characteristic of opinion distribution in the original review collection, which is a satisfactory result. Thus, it is evident that our method could achieve a great balance between considering individual review quality and optimizing the utility of selected reviews as a whole.
 model is that it cannot obtain a consistent performance if the order of the reviews in the review dataset changes. Thus, we also undertake an experiment to test if our method has a stable performance on review selection. In detail, we calculate the standard deviation of the average helpfulness scores of selected reviews in all runs. The derived results are provided in Table 3 below.
 method are much lower than those of the baseline. This indicates that the review set generated by the baseline model is quite different in different runs with different orders. This is because the baseline selection method is heavily related to the review order in the dataset. In contrast, relatively low standard deviation scores of our method show that the selected reviews in each run tend to be quite similar, which indicates that our method is much less related to the review order in the dataset. To further demonstrate the impact of our approach, we aim to justify the performance with respect to product rating. We assume that a better selected review set should provide a more accurate product rating, which is in accordance with the objective evaluation of the products. Therefore, we seek to use the product rating score provided by authors of the reviews as the measurement in this regard. For the ground truth data, we look for the objective evaluation reference from external authoritative sources. A number of websites provide a professional  X  X xpert review X  for the products we test, which gives a score in the range of 1-10 for each product. Hence, we collect expert scores for the six for product rating comparison. We calculate the average product rating score of 30 selected reviews generated by both models. Since the product rating score on Amazon is in the range of 1-5 , we first rescale the derived average product rating scores into the range of 1-10 in order to undertake the comparison. Table 4 provides the comparison between the baseline model and our pro-posed method on product rating score for the six considered products, respec-tively. According to the results, we can see that the average product rating scores obtained by our proposed method are much closer to the golden standard compared to those of the baseline model for all products. This indicates that the review set produced by our approach can obtain more reliable opinion sum-marization results by providing a consistent product rating score with the one provided by the experts. We infer that the improvement is caused by the reviews of better quality in which more comprehensive information about the product has been provided in the selected reviews by our method. In this paper, we introduced a review selection method to extract a set of reviews for representing the review corpus. Different from existing review quality predic-tion methods, we proposed to assess the quality of a review in an unsupervised manner by utilizing the structural relationships between product features to cap-ture the review characteristic called comprehension which indicates how features have been covered in the review. The evaluation results on real world datasets have proven that our review selection method is effective and can be used to find reviews of good quality. In addition, the testing also shows that our proposed review selection method is able to optimize the selection results by providing a great balance between choosing the individual review of good quality and ensur-ing the extracted reviews as a whole to reflect the opinion distribution in the review corpus. In the future, we plan to improve our method and to undertake the evaluation on more datasets of different categories.

