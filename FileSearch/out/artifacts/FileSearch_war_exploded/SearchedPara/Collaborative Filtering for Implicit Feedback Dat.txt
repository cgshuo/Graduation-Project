
A common task of recommender systems is to improve customer experience through personalized recommenda-tions based on prior implicit feedback . These systems pas-sively track different sorts of user behavior, such as pur-chase history, watching habits and browsing activity, in or -der to model user preferences. Unlike the much more ex-tensively researched explicit feedback , we do not have any direct input from the users regarding their preferences. In particular, we lack substantial evidence on which products consumer dislike. In this work we identify unique proper-ties of implicit feedback datasets. We propose treating the data as indication of positive and negative preference asso -ciated with vastly varying confidence levels. This leads to a factor model which is especially tailored for implicit feed -back recommenders. We also suggest a scalable optimiza-tion procedure, which scales linearly with the data size. Th e algorithm is used successfully within a recommender system for television shows. It compares favorably with well tuned implementations of other known methods. In addition, we offer a novel way to give explanations to recommendations given by this factor model.
As e-commerce is growing in popularity, an important challenge is helping customers sort through a large variety of offered products to easily find the ones they will enjoy the most. One of the tools that address this challenge is rec-ommender systems, which are attracting a lot of attention recently [1, 4, 12]. These systems provide users with per-sonalized recommendations for products or services, which hopefully suit their unique taste and needs. The technology behind those systems is based on profiling users and prod-ucts, and finding how to relate them.

Broadly speaking, recommender systems are based on two different strategies (or combinations thereof). The con-tent based approach creates a profile for each user or prod-uct to characterize its nature. As an example, a movie pro-file could include attributes regarding its genre, the par-ticipating actors, its box office popularity, etc. User pro-files might include demographic information or answers to a suitable questionnaire. The resulting profiles allow pro-grams to associate users with matching products. However, content based strategies require gathering external infor ma-tion that might not be available or easy to collect.
An alternative strategy, our focus in this work, relies only on past user behavior without requiring the creation of ex-plicit profiles. This approach is known as Collaborative Filtering (CF), a term coined by the developers of the first recommender system -Tapestry [8]. CF analyzes relation-ships between users and interdependenciesamong products, in order to identify new user-item associations. For exam-ple, some CF systems identify pairs of items that tend to be rated similarly or like-minded users with similar history o f rating or purchasing to deduce unknown relationships be-tween users and items. The only required information is the past behavior of users, which might be their previous trans-actions or the way they rate products. A major appeal of CF is that it is domain free, yet it can address aspects of the dat a that are often elusive and very difficult to profile using con-tent based techniques. While generally being more accu-rate than content based techniques, CF suffers from the cold start problem, due to its inability to address products new to the system, for which content based approaches would be adequate.

Recommender systems rely on different types of in-put. Most convenient is the high quality explicit feedback , which includes explicit input by users regarding their inte r-est in products. For example, Netflix collects star ratings for movies and TiVo users indicate their preferences for TV shows by hitting thumbs-up/down buttons. However, explicit feedback is not always available. Thus, recom-menders can infer user preferences from the more abundant implicit feedback , which indirectly reflect opinion through observing user behavior [14]. Types of implicit feedback include purchase history, browsing history, search patter ns, or even mouse movements. For example, a user that pur-chased many books by the same author probably likes that author.

The vast majority of the literature in the field is focused on processing explicit feedback; probably thanks to the con -venience of using this kind of pure information. However, in many practical situations recommender systems need to be centered on implicit feedback. This may reflect reluc-tance of users to rate products, or limitations of the system that is unable to collect explicit feedback. In an implicit model, once the user gives approval to collect usage data, no additional explicit feedback (e.g. ratings) is required on the user X  X  part.

This work conducts an exploration into algorithms specifically suitable for processing implicit feedback. It re-flects some of the major lessons and developments that were achieved while we built a TV shows recommender engine. Our setup prevents us from actively gathering explicit feed -back from users, so the system was solely based on implicit feedback  X  analyzing watching habits of anonymized users.
It is crucial to identify the unique characteristics of im-plicit feedback, which prevent the direct use of algorithms that were designed with explicit feedback in mind. In the following we list the prime characteristics: 1. No negative feedback. By observing the users behav-2. Implicit feedback is inherently noisy. While we pas-3. The numerical value of explicit feedback indicates 4. Evaluation of implicit-feedback recommender requires
We reserve special indexing letters for distinguishing users from items: for users u, v , and for items i, j . The input data associate users and items through r ui values, which we henceforth call observations . For explicit feedback datasets, those values would be ratings that indicate the preference by user u of item i , where high values mean stronger pref-erence. For implicit feedback datasets, those values would indicate observations for user actions. For example, r ui indicate the number of times u purchased item i or the time u spent on webpage i . In our TV recommender case, r ui indicates how many times u fully watched show i . For ex-ample, r ui = 0 . 7 indicates that u watched 70% of the show, while for a user that watched the show twice we will set r ui = 2 .

Explicit ratings are typically unknown for the vast ma-jority of user-item pairs, hence applicable algorithms wor k with the relatively few known ratings while ignoring the missing ones. However, with implicit feedback it would be natural to assign values to all r ui variables. If no action was observed r ui is set to zero, thus meaning in our examples zero watching time, or zero purchases on record.
The most common approach to CF is based on neigh-borhood models. Its original form, which was shared by virtually all earlier CF systems, is user-oriented; see [9] for a good analysis. Such user-oriented methods estimate unknown ratings based on recorded ratings of like minded users. Later, an analogous item-oriented approach [13, 19] became popular. In those methods, a rating is estimated us-ing known ratings made by the same user on similar items. Better scalability and improved accuracy make the item-oriented approach more favorable in many cases [2, 19, 20]. In addition, item-oriented methods are more amenable to explaining the reasoning behind predictions. This is be-cause users are familiar with items previously preferred by them, but usually do not know those allegedly like minded users.

Central to most item-oriented approaches is a similarity measure between items, where s ij denotes the similarity of i and j . Frequently, it is based on the Pearson correlation coefficient. Our goal is to predict r ui  X  the unobserved value by user u for item i . Using the similarity measure, we iden-tify the k items rated by u , which are most similar to i . This set of k neighbors is denoted by S k ( i ; u ) . The predicted value of r ui is taken as a weighted average of the ratings for neighboring items: Some enhancements of this scheme are well practiced for explicit feedback, such as correcting for biases caused by varying mean ratings of different users and items. Those modifications are less relevant to implicit feedback datase ts, where instead of having ratings which are all on the same scale, we use frequencies in which items are consumed by the same user. Frequencies for disparate users might have very different scale depending on the application, and it is less clear how to calculate similarities. A good discussion on how to use an item-oriented approach with implicit feed-back is given by Deshpande and Karypis [6].

All item-oriented models share a disadvantage in regards to implicit feedback -they do not provide the flexibility to make a distinction between user preferences and the confi-dence we might have in those preferences.
Latent factor models comprise an alternative approach to Collaborative Filtering with the more holistic goal to un -cover latent features that explain observed ratings; exam-ples include pLSA [11], neural networks [16], and Latent Dirichlet Allocation [5]. We will focus on models that are induced by Singular Value Decomposition (SVD) of the user-item observations matrix. Recently, SVD models have gained popularity, thanks to their attractive accuracy and scalability; see, e.g., [3, 7, 15, 17, 20]. A typical model as -sociates each user u with a user-factors vector x u  X  R f , and each item i with an item-factors vector y i  X  R f . The pre-diction is done by taking an inner product, i.e.,  X  r ui = x T The more involved part is parameter estimation. Many of the recent works, applied to explicit feedback datasets, su g-gested modeling directly only the observed ratings, while avoiding overfitting through an adequate regularized model , such as: min Here,  X  is used for regularizing the model. Parameters are often learnt by stochastic gradient descent; see, e.g., [7, 15, 20]. The results, as reported on the largest available datas et  X  the Netflix dataset [4]  X  tend to be consistently superior to those achieved by neighborhood models. In this work we borrow this approach to implicit feedback datasets, which requires modifications both in the model formulation and in the optimization technique.
In this section we describe our model for implicit feed-back. First, we need to formalize the notion of confidence which the r ui variables measure. To this end, let us intro-duce a set of binary variables p ui , which indicates the pref-erence of user u to item i . The p ui values are derived by binarizing the r ui values:
In other words, if a user u consumed item i ( r ui &gt; 0 ), then we have an indication that u likes i ( p ui = 1 ). On the other hand, if u never consumed i , we believe no pref-erence ( p ui = 0 ). However, our beliefs are associated with greatly varying confidence levels. First, by the nature of th e data zero values of p ui are associated with low confidence, as not taking any positive action on an item can stem from many other reasons beyond not liking it. For example, the user might be unaware of the existence of the item, or un-able to consume it due to its price or limited availability. I n addition, consuming an item can also be the result of fac-tors different from preferring it. For example, a user may watch a TV show just because she is staying on the channel of the previously watched show. Or a consumer may buy an item as gift for someone else, despite not liking the item for himself. Thus, we will have different confidence levels also among items that are indicated to be preferred by the user. In general, as r ui grows, we have a stronger indication that the user indeed likes the item. Consequently, we intro-duce a set of variables, c ui , which measure our confidence in observing p ui . A plausible choice for c ui would be: This way, we have some minimal confidence in p ui for ev-ery user-item pair, but as we observe more evidence for pos-itive preference, our confidence in p ui = 1 increases ac-cordingly. The rate of increase is controlled by the constan t  X  . In our experiments, setting  X  = 40 was found to produce good results.

Our goal is to find a vector x u  X  R f for each user u , and a vector y i  X  R f for each item i that will factor user preferences. In other words, preferences are assumed to be the inner products: p ui = x T u y i . These vectors will be known as the user-factors and the item-factors, respective ly. Essentially, the vectors strive to map users and items into a common latent factor space where they can be directly compared. This is similar to matrix factorization techniqu es which are popular for explicit feedback data, with two im-portant distinctions: (1) We need to account for the varying confidence levels, (2) Optimization should account for all possible u, i pairs, rather than only those corresponding to observed data. Accordingly, factors are computed by mini-mizing the following cost function: min The  X  P u k x u k 2 + P i k y i k 2 term is necessary for regu-larizing the model such that it will not overfit the training data. Exact value of the parameter  X  is data-dependent and determined by cross validation.

Notice that the cost function contains m n terms, where m is the number of users and n is the number of items. For typical datasets m n can easily reach a few billion. This huge number of terms prevents most direct optimiza-tion techniques such as stochastic gradient descent, which was widely used for explicit feedback datasets. Thus, we suggest an alternative efficient optimization process, as f ol-lows.

Observe that when either the user-factors or the item-factors are fixed, the cost function becomes quadratic so its global minimum can be readily computed. This leads to an alternating-least-squares optimization process, wh ere we alternate between re-computing user-factors and item-factors, and each step is guaranteed to lower the value of the cost function. Alternating least squares was used for explicit feedback datasets [2], where unknown values were treated as missing, leading to a sparse objective function. The implicit feedback setup requires a different strategy t o overcome the dense cost function and to integrate the con-fidence levels. We address these by exploiting the structure of the variables so that this process can be implemented to be highly scalable.

The first step is recomputing all user factors. Let us as-sume that all item-factors are gathered within an n  X  f ma-trix Y . Before looping through all users, we compute the f  X  f matrix Y T Y in time O ( f 2 n ) . For each user u , let us define the diagonal n  X  n matrix C u where C u ii = c ui , and also the vector p ( u )  X  R n that contains all the preferences by u (the p ui values). By differentiation we find an analytic expression for x u that minimizes the cost function (3): A computational bottleneck here is computing Y T C u Y , whose naive calculation will require time O ( f 2 n ) (for each of the m users). A significant speedup is achieved by us-ing the fact that Y T C u Y = Y T Y + Y T ( C u  X  I ) Y . Now, Y T Y is independent of u and was already precomputed. As for Y T ( C u  X  I ) Y , notice that C u  X  I has only n zero elements, where n u is the number of items for which r ui &gt; 0 and typically n u  X  n . Similarly, C u p ( u ) contains just n u non-zero elements. Consequently, recomputation of x u is performed in time O ( f 2 n u + f 3 ) . Here, we assumed O ( f 3 ) time for the matrix inversion ( Y T C u Y +  X I ) even though more efficient algorithms exist, but probably are less relevant for the typically small values of f . This step is performed over each of the m users, so the total run-ning time is O ( f 2 N + f 3 m ) , where N is the overall number of non-zero observations, that is N = P u n u . Importantly, running time is linear in the size of the input. Typical val-ues of f lie between 20 and 200; see experimental study in Sec. 6.

A recomputation of the user-factors is followed by a re-computation of all item-factors in a parallel fashion. We arrange all user-factors within an m  X  f matrix X . First we compute the f  X  f matrix X T X in time O ( f 2 m ) . For each item i , we define the diagonal m  X  m matrix C i where uu = c ui , and also the vector p ( i )  X  R m that contains all the preferences for i . Then we solve: Using the same technique as with the user-factors, running time of this step would be O ( f 2 N + f 3 n ) . We employ a few sweeps of paired recomputation of user-and item-factors, till they stabilize. A typical number of sweeps is 1 0. The whole process scales linearly with the size of the data. After computing the user-and item-factors, we recommend to user u the K available items with the largest value of  X  p ui = x T u y i , where  X  p ui symbolizes the predicted preference of user u for item i .

Now that the basic description of our technique is com-pleted we would like to further discuss it, as some of our decisions can be modified. For example, one can derive p ui differently from r ui , by setting a minimum threshold on r for the corresponding p ui to be non-zero. Similarly, there are many ways to transform r ui into a confidence level c One alternative method that also worked well to us is setting Regardless of the exact variant of the scheme, it is impor-tant to realize its main properties, which address the uniqu e characteristics of implicit feedback: 1. Transferring the raw observations ( r ui ) into two sep-2. An algorithm that addresses all possible ( n m ) user-
It is well accepted [10] that a good recommendation should be accompanied with an explanation, which is a short description to why a specific product was recom-mended to the user. This helps in improving the users X  trust in the system and their ability to put recommenda-tions in the right perspective. In addition, it is an invalu-able means for debugging the system and tracking down the source of unexpected behavior. Providing explana-tions with neighborhood-based (or,  X  X emory-based X ) tech-niques is straightforward, as recommendations are directl y inferred from past users X  behavior. However, for latent fac -tor models explanations become trickier, as all past user actions are abstracted via the user factors thereby block-ing a direct relation between past user actions and the out-put recommendations. Interestingly, our alternating leas t squares model enables a novel way to compute explana-tions. The key is replacing the user-factors by using Eq. (4) : x u = ( Y T C u Y +  X I )  X  1 Y T C u p ( u ) . Thus, the pre-dicted preference of user u for item i ,  X  p ui = y T i x comes: y T i ( Y T C u Y +  X I )  X  1 Y T C u p ( u ) . This expression can be simplified by introducing some new notation. Let us denote the f  X  f matrix ( Y T C u Y +  X I )  X  1 as W u , which should be considered as a weighting matrix associated with user u . Accordingly, the weighted similarity between items i and j from u  X  X  viewpoint is denoted by s u ij = y T i W u y Using this new notation the predicted preference of u for item i is rewritten as: This reduces our latent factor model into a linear model that predicts preferences as a linear function of past actio ns ( r uj &gt; 0 ), weighted by item-item similarity. Each past ac-tion receives a separate term in forming the predicted  X  p and thus we can isolate its unique contribution. The actions associated with the highest contribution are identified as t he major explanation behind the recommendation. In addition, we can further break the contribution of each individual pas t action into two separate sources: the significance of the re-lation to user u  X  c uj , and the similarity to target item i  X  .

This shares much resemblance with item-oriented neigh-borhood models, which enables the desired ability to ex-plain computed predictions. If we further adopt this view-point, we can consider our model as a powerful pre-processor for a neighborhood based method, where item similarities are learnt through a principled optimization pro-cess. In addition, similarities between items become depen -dent on the specific user in question, reflecting the fact that different users do not completely agree on which items are similar.

Giving explanation through (7) involves solving a linear system ( Y T C u Y +  X I )  X  1 y j , followed by a matrix vector product, and can be done in time O ( f 2 n u + f 3 ) , assuming that Y T Y is precomputed. Data description Our analysis is based on data from a digital television service. We were able to collect data on about 300,000 set top boxes. All data was collected in ac-cordance with appropriate end user agreements and privacy policies. The analysis was done with data that was aggre-gated and/or fully anonymized. No personally identifiable information was collected in connection with this research .
We collected all channel tune events for these users, in-dicating the channel the set-top box was tuned into, and a time stamp. There are approximately 17,000 unique pro-grams which aired during a four week period. The training data contains r ui values, for each user u and program i , which represent how many times user u watched program i (related is the number of minutes that a given show was watched -for all of our analysis we focus on show length based units). Notice that r ui is a real value, as users may watch parts of shows. After aggregating multiple watches of the same program, the number of non-zero r ui values is about 32 million.

In addition, we use a similarly constructed test set, which is based on all channel tune events during the single week following a 4-week training period. Our system is trained using the recent 4 weeks of data in order to generate pre-dictions of what users will watch in the ensuing week. The training period of 4 weeks is chosen based on an experi-mental study which showed that a shorter period tends to deteriorate the prediction results, while a longer period d oes not add much value (since television schedules change sea-sonally, long training periods do not necessarily have an advantage, even though we found that our core model is robust enough to avoid being contaminated by the season-ality). The observations in the test set are denoted by r (distinguished with a superscript t ).

One characteristic of television watching is the tendency to repetitively watch the same programs every week. It is much more valuable to a user to be recommended programs that she has not watched recently, or that she is not aware of. Thus, in our default setting, for each user we remove the  X  X asy X  predictions from the test set corresponding to th e shows that had been watched by that user during the training period. To make the test set even more accurate, we toggle to zero all entries with r t ui &lt; 0 . 5 , as watching less than half of a program is not a strong indication that a user likes the program. This leaves us with about 2 million non-zero r t values in the test set.

The tendency to watch the same programs repeatedly also makes r ui vary significantly over a large range. While there are a lot of viewing events close to 0 (channel flip-ping), 1, 2 or 3 (watching a film or a couple of episodes of a series), there are also some viewing events that accumulate to hundreds (have the DVR recording the same program for many hours per day over a period of 4 weeks). Therefore we employ the log scaling scheme (6) with  X  = 10  X  8 .
One other important adjustment is needed. We observe many cases where a single channel is watched for many hours. It is likely that the initial show that was tuned into i s of interest to the viewer, while the subsequent shows are of decreasing interest. The television might simply have been left on while the viewer does something else (or sleeps!). We call this a momentum effect, and programs watched due to momentum are less expected to reflect real preference. To overcome this effect, we down-weight the second and sub-sequent shows after a channel tuning event. More specifi-cally, for the t -th show after a channel tune, we assign it a b = 6 to work well and is intuitive: the third show after the channel tune gets its r ui value halved, by the fifth show without a channel change, r ui is reduced by 99%. Evaluation methodology We evaluate a scenario where we generate for each user an ordered list of the shows, sorted from the one predicted to be most preferred till the least preferred one. Then, we present a prefix of the list to the user as the recommended shows. It is important to real-ize that we do not have a reliable feedback regarding which programs are unloved, as not watching a program can stem from multiple different reasons. In addition, we are cur-rently unable to track user reactions to our recommenda-tions. Thus, precision based metrics are not very appropri-ate, as they require knowing which programs are undesired to a user. However, watching a program is an indication of liking it, making recall-oriented measures applicable.
We denote by rank ui the percentile-ranking of program i within the ordered list of all programs prepared for user u . This way, rank ui = 0% would mean that program i is predicted to be the most desirable for user u , thus preceding all other programs in the list. On the other hand, rank ui 100% indicates that program i is predicted to be the least preferred for user u , thus placed at the end of the list. (We opted for using percentile-ranks rather than absolute rank s in order to make our discussion general and independent of the number of programs.) Our basic quality measure is the expected percentile ranking of a watching unit in the test period, which is: Lower values of rank are more desirable, as they indicate ranking actually watched shows closer to the top of the rec-ommendation lists. Notice that for random predictions, the expected value of rank ui is 50% (placing i in the middle of the sorted list). Thus, rank &gt; 50% indicates an algorithm no better than random.
 Evaluation results We implemented our model with dif-ferent number of factors ( f ), ranging from 10 to 200. In ad-dition, we implemented two other competing models. The first model is sorting all shows based on their popularity, so that the top recommended shows are the most popular ones. This naive measure is surprisingly powerful, as crowds tend to heavily concentrate on few of the many thousands avail-able shows. We take this as a baseline value.

The second model is neighborhood based (item-item), along the lines described in Sec. 3.1. We explored many variants of this scheme, and found the following two deci-sions to yield best results: (1) Take all items as  X  X eighbors  X , not only a small subset of most similar items. (2) Use cosine similarity for measuring item-item similarity. Formally, for an item i let us arrange within r i  X  R m the r ui values as-sociated with all m users. Then, s ij = r T i r j k r dicted preference of user u for show i is: P j s ij r uj . As a
Figure 1. Comparing factor model with popu-larity ranking and neighborhood model. side remark, we would like to mention that we recommend very different settings for neighborhood based techniques when applied to explicit feedback data.

Figure 1 shows the measured values of rank with dif-ferent number of factors, and also the results by the popu-larity ranking (cyan, horizontal line), and the neighborho od model (red, horizontal line). We can see that based only on popularity, we can achieve rank = 16 . 46% , which is much lower than the rank = 50% that would be achieved by a random predictor. However, a popularity based predictor is clearly non-personalized and treats all users equally. The neighborhood based method offers a significant improve-ment ( rank = 10 . 74% ) achieved by personalizing recom-mendations. Even better results are obtained by our fac-tor model, which offers a more principled approach to the problem. Results keep improving as number of factors in-creases, till reaching rank = 8 . 35% for 200 factors. Thus, we recommend working with the highest number of factors feasible within computational limitations.

We further dig into the quality of recommendations, by studying the cumulative distribution function of rank ui . Here, we concentrate on the model with 100 factors, and compare results to the popularity-based and the neighborhood-based techniques, as shown in Fig. 2. We asked the following: what is the distribution of percentile s for the shows that were actually watched in the test set? If our model does well, all of the watched shows will have low percentiles. From the figure, we see that a watched show is in the top 1% of the recommendations from our model about 27% of the time. These results compare favorably to the neighborhood based approach, and are much better than the baseline popularity-based model.

Here we would like to comment that results get much better had we left all previously watched programs in the test set (without removing all user-program events that already occurred in the training period). Predicting re-
Figure 2. Cumulative distribution function of the probability that a show watched in the test set falls within top x% of recommended shows. watching a program is much easier than predicting a first time view of a program. This is shown by the black dot-ted line in the figure, which evaluates our algorithm when previously watched shows are not taken out of the test set. Although suggesting a previously watched show might not be very exciting, it does come useful. For example, our system informs users on which programs are running to-day that might interest them. Here, users are not looking to be surprised, but for being reminded not to miss a favorite show. The high predictive accuracy of retrieving previousl y watched shows comes handy for this task.

We would also like to evaluate our decision to trans-form the raw observations (the r ui values), into distinct preference-confidence pairs ( p ui , c ui ). Other possible mod-els were studied as well. First, we consider a model which works directly on the given observations. Thus, our model (3) is replaced with a factor model that strives to minimize: min Notice that this is a regularized version of the dense SVD algorithm, which is an established approach to collabora-tive filtering [18]. Results without regularization (  X  1 were very poor and could not improve upon the popularity based model. Better results are achieved when the model is regularized  X  here, we used  X  1 = 500 for regularizing the model, which proved to deliver best recommendations. While results consistently outperform those of the popu-larity model, they were substantially poorer even than the neighborhood model. For example, for 50 factors we got rank = 13 . 63% , while 100 factors yield rank = 13 . 40% . This relatively low quality is not surprising as earlier we argued that taking r ui as raw preferences is not sensible. Therefore, we also tried another model, which factorizes the derived binary preference values, resulting in: min The model was regularized with  X  2 = 150 . Results are indeed better than those of model (9), leading to rank = 10 . 72% with 50 factors and rank = 10 . 49% with 100 factors. This is slightly better than the results achieved with the neighborhood model. However, this is still ma-terially inferior to our full model, which results in rank = 8 . 93%  X  8 . 56% for 50  X  100 factors. This shows the impor-tance of augmenting (10) with confidence levels as in (3).
We now analyze the performance of the full model (with 100 factors) on different types of shows and users. Dif-ferent shows receive significantly varying watching time in the training data. Some shows are popular and watched a lot, while others are barely watched. We split the positive observations in the test set into 15 equal bins, based on in-creasing show popularity, We measured the performance of our model in each bin, ranging from bin 1 (least popular shows) to bin 15 (most popular shows). As Fig. 3 (blue line) shows, there is a big gap in the accuracy of our model, as it becomes much easier to predict popular programs, while it is increasingly difficult to predict watching a non popu-lar show. To some extent, the model prefers to stay with the safe recommendations of familiar shows, on which it gathered enough data and can analyze well. Interestingly, this effect is not carried over to partitioning users accord -ing to their watching time. Now, we split users into bins based on their overall watching time; see Fig. 3 (red line). Except for the first bin, which represents users with almost no watching history, the model performance is quite similar for all other user groups. This was somewhat unexpected, as our experience with explicit feedback datasets was that as we gather more information on users, prediction qual-ity significantly increases. The possible explanation to wh y the model could not do much better for heavy watchers is that those largely represent heterogeneous accounts, wher e many different people are watching the same TV.

Finally, we demonstrate the utility of our recommenda-tion explanations. Explanations for recommendations are common for neighbor methods since the system can al-ways return the nearest neighbors of the recommended item. However, there is no previous work discussing how to do explanations for matrix decomposition methods, which in our experience outperform the neighbor based methods. Ta-ble 1 shows three recommended shows for one user in our study. Following the methods in Section 5, we show the top five watched shows which explain the recommended show (shown in bold). These explanations make sense: the reality show So You Think You Can Dance is explained by other re-
Figure 3. Analyzing the performance of the factor model by segregating users/shows based on different criteria. ality shows, while Spider-Man is explained by other comic-related shows and Life in the E.R. is explained by medical documentaries. These common-sense explanations help the user understand why certain shows are recommended, and are similar to explanations returned by neighbor methods. We also report the total percent of the recommendation ac-counted for by the top 5. In this case, the top five shows only explain between 35 and 40% of the recommendation, indicating that many other watched shows give input to the recommendations.
In this work we studied collaborative filtering on datasets with implicit feedback, which is a very common situation. One of our main findings is that implicit user observations should be transformed into two paired magnitudes: pref-erences and confidence levels. In other words, for each user-item pair, we derive from the input data an estimate to whether the user would like or dislike the item ( X  X ref-erence X ) and couple this estimate with a confidence level. This preference-confidence partition has no parallel in the widely studied explicit-feedback datasets, yet serves a ke y role in analyzing implicit feedback.

We provide a latent factor algorithm that directly ad-dresses the preference-confidence paradigm. Unlike ex-plicit datasets, here the model should take all user-item pr ef-erences as an input, including those which are not related to any input observation (thus hinting to a zero preference). This is crucial, as the given observations are inherently bi -ased towards a positive preference, and thus do not reflect well the user profile. However, taking all user-item values as an input to the model raises serious scalability issues  X  the number of all those pairs tends to significantly exceed the input size since a typical user would provide feedback mended due to a unique set of already-watched shows by this us er. only on a small fraction of the available items. We address this by exploiting the algebraic structure of the model, lea d-ing to an algorithm that scales linearly with the input size while addressing the full scope of user-item pairs without resorting to any sub-sampling.

An interesting feature of the algorithm is that it allows explaining the recommendations to the end user, which is a rarity among latent factor models. This is achieved by showing a surprising and hopefully insightful link into the well known item-oriented neighborhood approach.

The algorithm was implemented and tested as a part of a large scale TV recommender system. Our design method-ology strives to find a right balance between the unique properties of implicit feedback datasets and computationa l scalability. We are currently exploring modifications with a potential to improve accuracy at the expense of increasing computational complexity. As an example, in our model we decided to treat all user-item pairs associated with a zero preference with the same uniform confidence level. Since the vast majority of pairs is associated with a zero prefer-ence, this decision saved a lot of computational effort. How -ever, a more careful analysis would split those zero values into different confidence levels, perhaps based on availabi l-ity of the item. In our television recommender example, the fact that a user did not watch a program might mean that the user was not aware of the show (it is on an  X  X nusual X  channel or time of day), or that there is another favorite show on concurrently, or that the user is simply not inter-ested. Each of these correspond to different scenarios, and each might warrant a distinctive confidence level in the  X  X o preference X  assumption. This leads us to another possible extension of the model  X  adding a dynamic time variable addressing the tendency of a user to watch TV on certain times. Likewise, we would like to model that certain pro-gram genres are more popular in different times of the day. This is part of an ongoing research, where the main chal-lenge seems to be how to introduce an added flexibility into the model while maintaining its good computational scala-bility.

Finally, we note that the standard training and test setup is designed to evaluate how well a model can predict fu-ture user behavior. However, this is not the purpose of a recommender system, which strives to point users to items that they might not have otherwise purchased or consumed. It is difficult to see how to evaluate that objective without using in depth user study and surveying. In our example, we believe that by evaluating our methods by removing the  X  X asy X  cases of re-watched shows, we somehow get closer to the ideal of trying to capture user discovery of new shows. [1] G. Adomavicius and A. Tuzhilin,  X  X owards the Next [2] R. Bell and Y. Koren,  X  X calable Collaborative Fil-[3] R. Bell, Y. Koren and C. Volinsky,  X  X odeling Relation-[4] J. Bennet and S. Lanning,  X  X he Netflix Prize X , KDD [5] D. Blei, A. Ng, and M. Jordan,  X  X atent Dirichlet Al-[6] M. Deshpande, G. Karypis,  X  X tem-based top-N recom-[7] S. Funk,  X  X etflix Update: Try This At Home X , [8] D. Goldberg, D. Nichols, B. M. Oki and D. Terry,  X  X s-[9] J. L. Herlocker, J. A. Konstan, A. Borchers and John [10] J. L. Herlocker, J. A. Konstan, and J. Riedl.  X  X xplain-[11] T. Hofmann,  X  X atent Semantic Models for Collabora-[12] Z. Huang, D. Zeng and H. Chen,  X  X  Compari-[13] G. Linden, B. Smith and J. York,  X  X mazon.com Rec-[14] D.W. Oard and J. Kim,  X  X mplicit Feedback for Rec-[15] A. Paterek,  X  X mproving Regularized Singular Value [16] R. Salakhutdinov, A. Mnih and G. Hinton,  X  X e-[17] R. Salakhutdinov and A. Mnih,  X  X robabilistic Matrix [18] B. M. Sarwar, G. Karypis, J. A. Konstan, and J. Riedl, [19] B. Sarwar, G. Karypis, J. Konstan and J. Riedl,  X  X tem-[20] G. Takacs, I. Pilaszy, B. Nemeth and D. Tikk,  X  X ajor
