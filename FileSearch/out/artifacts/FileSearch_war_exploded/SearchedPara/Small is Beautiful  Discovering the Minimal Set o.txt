
Stern School of Business, New York University A drawback of most traditional data mining methods is that they do not leverage prior knowledge of users. In many bu siness settings, managers and analysts have significant intuition based on several years of experience. In prior work [11, 12] we proposed methods that could discover unexpected patterns in data by using this domain knowledge in a systematic manner. In th is paper we continue our focus on discovering unexpected patterns and propose new methods for discovering a minimal set of unexpected patterns that discover orders of magnitude fewer patterns and yet retain most of the truly interesting ones. We demonstrat e the strengths of this approach experimentally using a case study application in a marketing domain. A well -known criticism of many rule discovery algorithms in data mining is that they generate too many patterns, many of which are obvio us or irrelevant. It stands to reason that more effective methods are needed to discover fewer and more relevant patterns from data and KDD researchers have addressed this issue extensively. One way to approach this problem is by focusing on discovering unexpected patterns [4, 5, 6, 7, 11, 12, 13, 14, 17, 18], where unexpectedness of discovered patterns is usually defined relative to a system of prior expectations. In particular, we proposed in our prior research [11, 12] a characterization of unexpectednes s of a discovered pattern relative to the system of prior beliefs and developed efficient algorithms for the discovery of these unexpected patterns. Although these algorithms generate significantly fewer and more relevant patterns, still many of the gene rated unexpected patterns are redundant in the sense that they can be derived from other discovered unexpected patterns. Therefore, this paper focuses on minimality of unexpected patterns and on efficient algorithms that discover such minimal patterns. The power of the proposed approach lies in combining two independent concepts of unexpectedness and minimality of a set of patterns into one integrated concept that provides for the discovery of small but important sets of interesting patterns.
 The conce pt of minimality has been studied in AI for a long time and more recently in KDD. In particular in an early influential work [9], Mitchell addresses the problem of learning generalizations of a set of objects and presents a unifying approach to the problem of generalizing knowledge by viewing the generalization task as a search problem. In the context of discovering a minimal set of rules in data mining, the approach presented in [9] has the limitation that in most cases it may not be possible to have train ing examples that are classified into known generalizations. Therefore, rather than learning these generalization relationships among different objects, it is necessary to define them. Recent characterizations of various notions of minimality in the KDD li terature take this approach and we describe them below. In the KDD literature [2, 3, 8, 15, 16, 20] provide alternate approaches to characterizing a minimal set of discovered rules. In particular, [2] presents an approach that finds the  X  X ost interesting rules X , defined as rules that lie on a support and confidence frontier. Further, [2] proves that these rules necessarily contain the strongest rules discovered using several objective criteria other than just confidence and support. In [16] several heur istics for pruning large numbers of association rules has been proposed. One of these heuristics prunes out certain refinements of rules, thus, hinting at the concept of minimality of a set of rules. However, [16] focuses exclusively on heuristics that pru ne redundant rules from a discovered set of rules and does not explore the concept of minimality formally, nor proposes any algorithms for discovering a minimal set of patterns. In [8] a technique is presented to prune and then summarize an already disco vered set of association rules. In particular, [8] defines the concept of direction -setting rules and demonstrates how non -direction -setting rules can be inferred from them. Therefore, the set of direction -setting rules constitutes a set of rules that are  X  X inimal X  in some sense. This work is related to [16] in the sense that certain rule refinements are pruned out in the approach in [8] and therefore, they are not direction -setting. However, the method presented in [8] is different from [16] and 
Permission to make digital or hard copies of part or all of this work or personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 from our a pproach in the sense that not all refined rules are non -direction setting according to [8]. Moreover, [8] focuses on pruning already discovered rules and does not address the issue of direct discovery of minimal sets. An approach to eliminating redundant association rules is presented in [20]. In particular, [20] introduces a concept of the  X  X tructural cover X  for association rules and presents post -processing algorithms to find the structural cover. In this paper we, present an alternative formal characte rization of the minimal set of patterns that corresponds to structural covers of [20] for the association rules but is also broader and applicable to more general classes of rules. Moreover, [20] focuses on pruning already discovered rules and does not add ress the issue of direct discovery of minimal sets. Finally, the work of [3] and [15] is also related to the problem of discovering minimal sets of rules. In particular, [3] and [15] provide methods for eliminating rules such that the support and/or conf idence values of these rules are not unexpected with respect to the support and confidence values of previously discovered rules. However, this work is only marginally related to our approach because we focus on a more general definition of minimality that does not directly depend on confidence and support of discovered rules. In this paper we present a new approach for characterizing minimality of a set of unexpected patterns and present efficient methods to discover minimal unexpected patterns.
 In Sectio n 2 we present a few preliminaries regarding unexpectedness, followed by a characterization of minimality of unexpected patterns in Section 3. We then present two algorithms (one in detail and a sketch of the second, for lack of space) for discovering mini mal unexpected patterns in Section 4. We present experimental results and discussion in Section 5. We define an atomic condition to be a proposition of the form value for unordered attributes where value, value 1 to the set of distinct values taken by attribute in dataset D . In this paper we consider rules and beliefs defined as extended association rules of the form X  X  A , where X is the conj unction of atomic conditions (an itemset) and A is an atomic condition. We follow the definition of unexpectedness from [11] and define the rule A  X  B to be unexpected with respect to the belief X  X  Y on dataset D if the following conditions hold: user -specifi ed support threshold value. A key assumption in this definition, motivated in [10, 11], is that  X  B that we expect to hold on a dataset D , then monotonicity assumes the belief should also be expected to hold on any statistically large subset of D . Given the definition of unexpectedness, [11] presents algorithm ZoomUR that discovers all the unexpected rules with respect to a set of beliefs. In the first phase of ZoomUR, ZoominUR discovers all unexpected patterns that are refinements to any belief. More specifically, given any belief X  X  Y , ZoominUR discovers all unexpected r ules of the form X, A  X  B such that B AND Y |= FALSE. We refer to such rules as  X  X nexpected refinements X . In the second phase of ZoomUR, starting from all the unexpected refinements, ZoomoutUR discovers more general rules ( generalizations ) that are also un expected. As demonstrated in [10, 11], this approach generated far fewer and more interesting patterns than traditional approaches. Though ZoomUR discovers only unexpected rules and also far fewer rules than Apriori [1], 2 it st ill discovers large numbers of rules many of which are redundant in the sense that they can be inferred from other discovered rules under the monotonicity assumption stated in Section 2. In this sense, some of the discovered unexpected patterns are expecte d with respect to other discovered patterns and, thus, can and should be eliminated. For example, consider belief diaper  X  beer and two unexpected patterns diaper, weekday  X  not_beer and diaper, weekday, male  X  not_beer. Then the second unexpected pattern can be inferred from the first one under the monotonicity assumption. To address this issue, we formally characterize minimality of a set of unexpected patterns based on the monotonicity assumption. In order to do this, we first need to define inference of one rule from another under monotonicity. Before introducing minimal rules, we need to define formally which rules can be inferred to hold on a dataset due to the monotonicity assumption. Definition . Rule ( A  X  B ) |= M ( C  X  D ) if 1. C |= A , and 2. D = B . o In this definition, rule C  X  B can be inferred from rule A  X  B under the monotonicity assumption because if rule A  X  B holds on some data and C |= A then, by the monotonicity assumption, A  X  B should hold on the subset of data defined by C . all strong rules, while ZoomUR discovers only unexpected rules. Example . Consider the rules diaper, weekday  X  not_beer and diaper, weekday, male  X  not_beer . Since diaper, weekday, male |= diaper, weekday it follows that the rule diaper, weekday, male  X  not_beer is implied from the rule diaper, weekday  X  not_beer under the monotonicity assumption ( diaper, weekday  X  not_beer |=
M diaper, weekday, male  X  not_beer ), and therefore is redundant in that sense. o We next present a definition for the minimal set of rules, followed by the defin ition of the minimal set of unexpected patterns. Definition . Y is the minimal set of X if and only if the following conditions hold: (1) Y  X  X . (2)  X  x i  X  X ,  X  y i  X  Y | y i |= M x i . (3)  X  y 1 , y 2  X  Y , y 1 |  X  M y 2 .
 Proposition 3.1 . For any set of rules X , the minimal set of X is unique.
 Proof . To prove this proposition, we define a directed graph G =( V , E ) as follows. The set of nodes V consists of all the rules from X . Given two nodes n 1 = A  X  B and n 2 = C  X  D from V , there is an edg e from node n 1 to node n 2 in E if A  X  B |= D . Then it is easy to see that the minimal set of rules for X consists of all the nodes of G having no incoming edges (in -degrees of these nodes are 0). Then the claim follows from the observation that the se t G is unique. o We would like to reiterate that we use |= M instead of classical logical implication |= in the definition above because the concept of mininmality, as defined in this paper, is based exclusively on the inference under the monotonicity assu mption as specified in Section 3.1. Given the above definition, we introduce the minimal set of unexpected patterns as follows.
 patterns with respect to B , the minimal set of unexpected patte rns with respect to B is the minimal set of X . o In Section 4.1 we present an algorithm MinZoominUR that discovers the minimal set of unexpected refinements. We describe this algorithm because in many applications we are interested only in refinements of beliefs and also because MinZoominUR illustrates some important points used in MinZoomUR. We then present in Section 4.2 an overview of MinZoomUR, an efficient algorithm for discovering the minimal set of unexpected patterns. The inputs to algorithms MinZoominUR and MinZoomUR are: (1) a set of beliefs, B , (2) the dataset D , (3) minimum support and confidence values minsup and minconf and (4) minimum and maximum width for all ordered attributes. In the case of ordered attributes the width of any condition of the form value 1  X  value 2 is defined to be value 2 -value 1 . We take as user inputs the minimum and maximum width for all ordered attributes. Note that this is not a restrictive assumption in any way since the default can be the smallest width and largest width respectively for these two parameters. In this section we present MinZoominUR, an algorithm for discovering the minimal set of unexpected refinements to a set of beliefs. Consider the belief body  X  head , having the structure specified in Section 2. We use the term "CONTR(head)" to refer to the set of atomic conditions that contradict the atomic condition specified (sorted in ascending order if the attribute a is ordered) that a takes on in D . CONTR(head) is generated as follows: Algorithm MinZoominUR is based on Apriori [1] and ZoomUR [11] with several major differences. First, unlike in Apriori, generation of large itemsets starts with a set of bel iefs that seed the search. Second, unlike in Apriori and ZoomUR, MinZoominUR does not generate those itemsets that are guaranteed to produce non -minimal rules. Third, rule generation process is integrated into the itemset generation part of the algorithm  X  this process is immaterial for Apriori and ZoomUR but results in significant efficiency improvements for MinZoominUR. Before presenting MinZoominUR, we first present a broad overview of the algorithm. Each iteration of MinZoominUR generates itemsets in the following manner. In the k -th iteration we generate itemsets of the form { C,body,P }, where C  X  CONTR(head) and P is a conjunction of k atomic conditions. Observe that to determine the confidence of the rule body, P  X  C , the supports of both the itemset s { C,body,P } and { body,P } will have to be determined. Hence in the k -th iteration of generating large itemsets, two sets of candidate itemsets are considered for support determination: (1) The set C k of candidate itemsets. Each itemset in C { C , body ,P }) contains thresholds min_support and min_conf Outputs : For each belief, B , MinUnexp(B)
L , B) (2) A set C k ' of additional candidates. Each itemset in C { X,P }) is generated from an itemset in C k by dropping the condition, C , that contradicts the head of the belief.
 In each iteration, minimal unexpected rules are generated from the set of large itemsets. The main idea in MinZoominUR is that if an itemset generates an unexpected rule, it is deleted from consideration and therefore no superset of this itemset is even c onsidered in subsequent iterations. As we prove in Theorem 4.1, this step avoids generation of itemsets producing non -minimal rules and significantly improves the efficiency of the algorithm. We explain the steps of MinZoominUR in Fig. 4.1 now. The follo wing is a list of notations that are used in describing the algorithm:  X  UNORD is the set of unordered attributes.  X  ORD is the set of ordered attributes.  X  minwidth(a) and maxwidth(a) are minimum and maximum  X  Attributes(x) is the set of all attributes present in any of the  X  Values(a) is the set of distinct values the attribute a takes in First, given a belief, B , the set of atomic conditions that contradict the head of the belief, CONTR(head (B)), is computed (as described previously). Then, the first candidate itemsets generated in C condition from CONTR(head(B)) . Steps (6) through (20) in Fig. 4.1 are iterative: Steps 7 through 9 det ermine the supports in dataset D for all the candidate itemsets currently being considered and selects the large itemsets L L  X . Each itemset in L k contains the body and the head of a potentially unexpected rule, while each itemset in L onl y the body of the potentially unexpected rule. Steps 10 through 17 generate unexpected rules such that large itemsets that contribute to unexpected rules are subsequently deleted in Step 15. Specifically, for each large itemset in L k , if the unexpected ref inement rule that is generated from the itemset has sufficient confidence, then two actions are performed: 1. Step 14 adds this rule to the set of potentially minimal 2. Step 15 deletes the corresponding itemset from L k since any In step (19), function generate_new_candidates(L the set C k of new candidate itemsets to be considered in the next pass from the previously determined set of large itemsets, L with respect to the belief B ( X  x  X  y  X ) as described in ZoomUR [11]. In general we generate C 1 from L 0 by adding additional conditions of the form attribute = va lue for unordered attributes or of the form value1  X  attribute  X  value2 for ordered attributes to each of the itemsets in L when k &gt; 1 is similar to the apriori -gen function described in [1]. In step (20), as descri bed previously, we would also need the support of additional candidate itemsets in C k' to determine the confidence of unexpected rules that will be generated. The function generate_bodies(C k ,B) generates C k' by considering each itemset in C k and dropping the condition that contradicts the head of the belief and adding the resulting itemset in C Steps (22  X  27) are needed to detect any remaining non -minimal rules that arise due to the following special case of certain itemsets containing unordered attr ibutes. To illustrate this special case, consider the following two itemsets: {{ a=1 }, { 5  X  b  X  10 }} and {{ a=1 }, { 7  X  b  X  8 }}. The special case is that neither of these sets is a  X  X uperset X  of the other, yet ( 5  X  b  X  10  X  a=1 ) |=  X  b  X  8  X  a=1 should be eliminated in order to produce the minimal set of unexpected rules. Since Steps (6  X  21) of the algorithm do not eliminate such rules, the additional Steps (22  X  27) do this. Note th at in the case of only unordered attributes in the itemsets, Steps (22  X  27) of the algorithm are not needed since MinUnexp(B) after Step 21 is guaranteed to be minimal (see the proof of Theorem 4.1).
 The computational complexity of Steps (1  X  21) is dete rmined by the total number of candidate itemsets K generated in Steps (19 -20) taken over all the iterations of the While -loop. The computational complexity of the elimination procedure in Steps (22  X  27) is O( n 2 ), where n is the size of the set MinUnexp( B). In practice K &gt;&gt; n 2 . Therefore, the bottleneck of MinZoominUR algorithm lies in Steps (6  X  21). Moreover, the complexity of MinZoominUR in the worst case is comparable to the worst -case complexity of Apriori that is bounded by O(|| C || * || D ||), where | | C || denotes the sum of the sizes of candidates considered, and || D || denotes the size of the database [1]. However, in the average case, the computational complexity of MinZoominUR is significantly lower than that of Apriori. This is the case because the average number of candidates considered in MinZoominUR is significantly lower than that for Apriori due to (a) minimality -based elimination procedure, and (b) presence of the initial set of beliefs that seed the search process.
 Observe that a key strength of MinZoominUR, compared to ZoomUR [11] and Apriori [1], is that rule discovery is integrated into the itemset generation procedure in such a way that it can greatly reduce the number of itemsets generated in subsequent iterations. Theorem 4.1 . For any belief, B , MinZoominUR discovers the minimal set of unexpected rules that are refinements to the belief. Sketch of the Proof . We will first show that for the case where there are unordered attributes only, MinZoominUR generates the minimal set of unexpect ed patterns without needing to apply the minimal filter (Steps 22 through 27 of Figure 4.1). For unordered attributes only, it is easy to see that a rule X 1 =x 1 , X  X  Y = y 1 is non -minimal if and only if there is a rule of the form Z  X  Y = y 1 , w here Z  X  { X 1 =x 1 , X 2 =x 2 ,..., X observation it can be shown that, as done in MinZoominUR, itemset deletion immediately following the generation of an unexpected rule from the itemset is adequate to guarantee the generation of the minimal set o f unexpected refinements. However there is a special case involving ordered attributes that cannot guarantee only minimal rules before Steps 22 -27. This special case arises since a syntactic subset check cannot capture containment when dealing with ranges of values for ordered attributes. An example of this special case was given above in Section 4.2. Hence the filter in Steps 22 -27 removes any non -minimal rules remaining. A detailed proof of this theorem is in [10]. o In this section we focused on discov ering minimal set of unexpected refinements of beliefs. In the next section we present the main ideas of MinZoomUR, an algorithm that discovers the minimal set of unexpected patterns. Due to the space limitatio n, we present only an overview of the discovery algorithm. The complete description can be found in [10]. First we present a few preliminaries. We use the term parents(x) to denote the set of all subsets of x that contain the body of the belief and one con dition that contradicts the head of generation phase of the algorithm. Specifically, An itemset y is said to be a parent of x if y  X  parents(x). We use the term zoomin rules to denote unexpected rules that are refinements to beliefs and zoomout rules for unexpected rules that are more general unexpected rules. The large itemset x is said to generate a zoomin rule if confidenc e ( x -c  X  c ) &gt; min_conf , where c  X  CONTR(head(B)) . The large itemset x is said to generate a zoomout rule if x generates a zoomin rule x -c  X  c and confidence( x -c -d  X  c ) &gt; min_conf , where c  X  CONTR(head(B)) , d  X  body(B) and d is not empty. Associa ted with each itemset, x , are two attributes: x.rule , that keeps track of whether a zoomin rule is generated from x , and x.dropped_subsets , which keeps track of the subsets of body(B) that are dropped during the discovery of zoomout rules.
 Unlike what was done in MinZoominUR, an itemset that generates a zoomin rule in MinZoomUR cannot always be deleted from subsequent consideration since it is possible for minimal zoomout rules to be derived from non -minimal zoomin rules. 3 Note that this  X  X yntactic X  subset property is not true when dealing with ordered attributes , which is why the minimal filter in Steps 22 -27 are necessary. (Apriori, MinZoominUR and MinZoomUR) is iterative such that itemsets in subsequent iterations have greater cardinality (number of items). Consider the following example. Fo r a belief a, b  X  x , let a, b, c  X  y and a, b, c, d  X  y be two zoomin rules. Though a, b, c, d  X  y is a non -minimal zoomin rule, the rule may result in a zoomout rule such as b, c, d  X  y which may belong to the minimal set of unexpected rules. Extending this example one more step, we observe that the zoomout rule b, c, d  X  y can, however, be guaranteed to be non -minimal if the first zoomin rule a, b, c  X  y resulted in a zoomout proper subset of the body of the belief. Examples of such p are { b } and {} corresponding to the zoomout rules b, c  X  y and c  X  y respectively (generated from a, b, c  X  y ) . However if the first zoomin rule generated only the zoomout rule a, c  X  y , it may still be possi ble for the zoomout rule b, c  X  y to be minimal since b, c, d |  X  a, c .
 The discovery strategy of MinZoomUR is based on the following conditions under which some generated rules are guaranteed to be non -minimal and hence can be excluded from the minimal se t. These exclusion rules are integrated into the itemset generation phase of the algorithm (similar to the single exclusion rule integrated into MinZoominUR) and thus substantially reduce the number of itemsets considered in subsequent iterations. [10] pro ves that these conditions do indeed exclude only non -minimal rules, hence we only state these rules here for lack of space. The  X  X xclusion rules X  used in MinZoomUR are: 1. If x and y are two large itemsets such that x is a parent of y 2. If x is a large itemset that generates a zoomin rule and some 3. If x is a large itemset that generates zoomout rules p and q 4. If x and y are two large itemsets such that x is a parent of y , MinZoomUR generates candidate itemsets in the same manner as in MinZoominUR. A main difference in the algorithms is that MinZoomUR considers zoomout rules also for a given itemset immediately after the itemset generates a zoomin rule. This is necessary because some of the  X  X xclusion rules X  applied to an unexpected rule generated depe nds on knowing the zoomout rules generated for that itemset and its parents. After the four exclusion rules are applied, MinZoomUR also applies the minimal filter similar to the one specified in lines (22)  X  (27) of MinZoominUR. Moreover, as shown in (10), this minimal filter is necessary only when there are ordered attributes. In the case when all the attribures are unordered, the four exclusion rules are also 
For belief B and itemset x , if p is a single zoomout rule, then x.dropped_subsets(p) will contain only one element which is the subset of body(B) that was dropped to create the zoomout rule. sufficient conditions for generating only minimal rules (hence no minimal filter is necessary at the end).
 In summary, the following theorem states, MinZoomUR discovers all the minimal unexpected patterns. The proof of this theorem can be found in [10].
 Theorem 4.2. For any belief MinZoomUR discovers the minimal set of unexpected patterns.
 We would also like to note that the classical notion of  X  X inimality X  often assumes that it is possible to reconstruct the set of all objects having certain property from the minimal set of objects having this property. In our case also, the set of all unexpected pa tterns can be reconstructed from the minimal set of unexpected patterns. However, this can be done only using a computationally intensive process that requires extensive data manipulation, rather than through an immediate reconstruction procedure that does not require any additional data access. This limitation of our approach is the result of a development of efficient search algorithms that directly discover the minimal set of unexpected patterns without even examining all unexpected patterns. Moreover, t his limitation can also be circumvented by letting the domain expert examine the set of minimal unexpected patterns (that is small), select the most interesting minimal patterns, and use the system to automatically refine them to discover all the unexpecte d patterns obtained from this selected set. To illustrate the usefulness of our approach to discovering patterns, in this section we consider a case study application of applying the methods to consumer purchase data from a major market re search firm. We pre -processed this data by combining different data sets (transaction data joined with demographics), made available to us into one table containing 38 different attributes and 313409 records. For simplicity in generating beliefs and in mak ing comparisons to other techniques that generate association rules in these experiments we restrict our consideration to rules involving discrete attributes only. An initial set of 28 beliefs was generated by domain experts after examining 300 rules gener ated from the data using methods described in [10]. In this section we present some results from applying MinZoomUR, ZoomUR [11] and Apriori [1] to this dataset starting from the initial set of beliefs where applicable. Specifically we compare these method s in terms of the number of rules generated and provide some guidelines as to when each may be applicable and also present results from scale -up experiments. We refer the reader to [10, 11] for several examples of truly unexpected discoveries from applying our unexpected pattern discovery methods. For a fixed minimum conf. level of 0.6, Figure 5.1 through 5.3 show the number of patterns generated by Apriori, ZoomUR and MinZoomUR for varying levels of minimum support. Apriori generated 50,000 to 250,000 rules even for reasonably high minimum support values. This is not surprising since the objective of Apriori is to discover all strong association rules. For reasonable values of supp ort (5 to 10%), ZoomUR generates 50 to 5000 unexpected patterns. MinZoomUR on the other hand generated only 15 to 700 unexpected patterns even for extremely low values for minimum support. Figure 5.4 illustrates the comparison of the three methods in ter ms of the number of generated rules. Due to the order of magnitude difference in the number of generated rules, the graph plots the number of rules generated using a logarithmic scale for the Y axis.
 As we would expect, as the minimum support threshold is lowered, all the methods discover a greater number of rules. Despite this, MinZoomUR discovers orders of magnitude fewer patterns than both ZoomUR and Apriori. The graphs in Figures 5.1  X  5.3 also demonstrate that a majority of patterns generated by Zoo mUR are redundant. Observe that as the support threshold is lowered, the number of patterns generated by both ZoomUR and Apriori increase more than linearly. While this is the case for MinZoomUR in some regions, MinZoomUR plateaus out for lower regions of support. These plateaus signify that very few new minimal unexpected patterns are generated in these experiments despite the fact that in these experiments the number of unexpected patterns generated by ZoomUR keep increasing in that region. This observati on coupled with the comparison in the number of rules generated indicate that MinZoomUR is indeed effective in removing redundant patterns, which represent a large majority of the set of all discovered patterns. Based on these experiments we discuss below some possible tradeoffs between these methods and provide some guidelines to their usage.
 The clear advantage of MinZoomUR over ZoomUR is that it generates far fewer patterns and yet retains most of the truly interesting ones as shown in [10]. Since ZoomUR generates all unexpected patterns for a belief and MinZoomUR generates the minimal set of unexpected patterns, MinZoomUR will always generate a subset of patterns that ZoomUR generates. As shown above, this subset can be very small (fro m 15 to a few hundred patterns for the entire set of beliefs, while ZoomUR can generate an order of magnitude more). Moreover, domain experts can selectively refine some of the patterns in the minimal set to obtain all unexpected patterns that are refineme nts to the selected pattern. The drawback of MinZoomUR compared to ZoomUR is that MinZoomUR makes an implicit assumption that minimal unexpected patterns are the  X  X ost interesting X  patterns. From a subjective point of view this may not be necessarily tru e. Consider the following example of two unexpected patterns:  X  When coupons are available for cereals, they don't get used  X  On weekends, when coupons are available for cereals they MinZoomUR will not gene rate the second unexpected pattern since it is monotonically implied by the first pattern. However, the second unexpected pattern has a much higher confidence and may be considered "more unexpected" by some users. In a more general sense, the criteria impl ied by monotonicity and confidence are just two methods to rank unexpected patterns. In general there may be other criteria, some of which even depending on other subjective preferences of a user. Hence, since ZoomUR generates all the unexpected patterns, it is guaranteed to contain all the unexpected patterns that are "most unexpected" from any specific definition of the term "most unexpected". In the context of objective measures of interestingness, [2] discuss interesting approaches to finding the  X  X ost interesting X  patterns. In subsequent work, we will study the issue of generating the "most unexpected patterns" by characterizing the degree of unexpectedness for patterns along the lines of [2, 18]. Given the relative advantages of the two methods to discovering unexpected patterns, a practical implication of the above is that ZoomUR can be used to generate unexpected patterns for high levels of support values and MinZoomUR can be used if patterns of very low support need to be generated. As shown in Figure 5.3, MinZoomUR generates a reasonable number of unexpected patterns even for extremely small values of minimum support, as low as even 0.5%. Also the support of some beliefs abo ut a domain may be very low, perhaps reflective of some condition that occurs rarely. In such cases methods such as MinZoomUR that can find patterns at very low support values are necessary . As Figure 5.2 shows, for such low values of minimum support most methods may discover tens of thousands of patterns, resulting in a data mining problem of the second order.
 Apriori on the other hand has the drawback of generating a very large number of patterns since the objective is to discover all strong rules. As Fi gure 5.1 shows, for very low support values, this could easily result in a few million rules even on mid -sized problems. However there are two sides of the coin. Generating a very large number of patterns results in a data mining problem of a second orde r and is hence avoidable. At the same time it is possible that either of the two methods that seek unexpected patterns could miss other  X  X nteresting X  patterns that may be unrelated to domain knowledge. However the set of patterns generated by Apriori can, trivially, be guaranteed to have all the interesting patterns since it has all patterns. We believe that this tradeoff is in some sense unavoidable since the problem of generating all interesting patterns (not just  X  X nexpected X ) is a difficult problem to s olve. Below we experimentally examine the scalability of MinZoomUR with respect to the size of the database. For a sample of 10 beliefs, we ran MinZoomUR multiple times by varying the number of records in t he dataset from 40,000 to 200,000. Figure 5.5 shows the execution times for MinZoomUR. These experiments indicate that MinZoomUR scales, in the range considered, almost linearly with the size of the database. In this section we presented results pertaini ng to the effectiveness of MinZoomUR and compared it to Apriori and ZoomUR. We demonstrated that MinZoomUR can be used to discover far fewer patterns than Apriori and ZoomUR, yet finding most of the truly interesting patterns. In this pap er we presented a definition for the minimal set of unexpected patterns and proposed two algorithms for discovering the minimal set of such patterns. In a real -world application we demonstrated that the main discovery algorithm, MinZoomUR , discovered order s of magnitude fewer patterns than other comparable methods and yet retained most of the truly interesting patterns. We also discussed tradeoffs between various discovery methods and presented some guidelines for their usage.
 The power of this approach li es in combining two independent concepts of unexpectedness and minimality of a set of patterns into one integrated concept that provides for the discovery of small but important sets of interesting patterns. Moreover, MinZoominUR and MinZoomUR are efficien t since they directly discover minimal unexpected patterns. [1] Agrawal, R., Mannila, H., Srikant, R., Toivonen, H. and [2] Bayardo, R. and Agrawal, R., 1999. Mining the Most [3] Bayardo, R., Agraw al, R. and Gunopulos, D., 1999. [4] Berger, G. and Tuzhilin, A., 1998. Discovering Unexpected [5] Chakrabarti, S., Sarawagi, S. and Dom, B. Mining Surprising [6] Liu, B. and Hsu, W ., 1996. Post -Analysis of Learned Rules. [7] Liu, B., Hsu, W. and Chen, S, 1997. Using General [8] Liu, B., Hsu, W., and Ma, Y., 1999. Pruning and [9] Mitchell, T., 1982. Generalization as Search. Artificial [10] Padmanabhan, B., 1999. Discovering Unexpected Patterns in [11] Padmanabhan, B. and T uzhilin, A., 1998.  X  X  Belief -Driven [12] Padmanabhan, B. and Tuzhilin, A., 1999. Unexpectedness as [13] Suzuki, E., 1997. Autonomous Discovery of Reliable [14] Subramonian, R. Defining diff as a data mining primitive. In [15] Srikant, R. and Agrawal, R., 1996. Mining Quantitative [16] Shah, D., Lakshmanan, L.V.S, Ramamritham , K., and [17] Silberschatz, A. and Tuzhilin, A., 1995. On Subjective [18] Silberschatz, A. and Tuzhilin, A., 1996. What Makes [19] Srikant, R., Vu, Q. and Agrawal, R. Mining Association [20] Toivonen, H., Klemetinen, M., Ronkainen, P., Ha tonen, K. 
