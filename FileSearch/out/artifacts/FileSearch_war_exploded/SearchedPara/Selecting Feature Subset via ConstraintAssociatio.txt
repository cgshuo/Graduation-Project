 Feature subset selection is an important research issue in the domains of machine learning and data mining. Its purpose is to help the learning algorithm focus on those aspects of the data most useful for analysis and future prediction. Generally, feature subset selection is the process of identifying and removing as many irrelevant and redundant features as possible. As irrelevant features do not contribute to the predictive accuracy [13], and redundant features do not contribute to getting a better predictor for that the most information they provide is already present in other feat ure(s) [28], thus many feature subset selection algorithms have been proposed to handle the irrelevant features or/and redundant features.

However, feature interaction is not a negligible issue in practice [12]. For ex-ample, suppose F 1  X  F 2 = Y ,where F 1 and F 2 are two boolean variables, Y represents the target concept, and  X  represents the xor operation. F 1 and F 2 are irrelevant with Y when we consider their discrimination abilities for Y sep-arately, but they become very relevant when we combine them together. There-fore, removing the interactive features will lead to poor predictive accuracy. Thus a feature subset selection algorithm should consist of eliminating the irrelevant and redundant features while taking the feature interaction into consideration. Unfortunately, to our knowledge, only a few algorithms can deal with this situ-ation [12,29].

Association rule mining can discover interesting associations among data items [15], it has been used to build classifiers which show better classifica-tion accuracy compared with the other type s of classifiers [2,10,19]. Especially, it also has been employed for feature se lection recently by Xie et al. [26]. How-ever, Xie et al. only focus on relevant features and do not consider redundant and interactive features.
 An association rule is an expression of A  X  C ,where A (Antecedent) and C (Consequent) are itemsets. If we view A as the feature(s) and C as the fea-ture(s)/the target concept, association r ules can reveal the dependencies between either feature(s) and feature(s) or featu re(s) and the target concept. Therefore, it is reasonable and desirable to devise an association rule mining based method to choose feature subset.

In this paper, we propose a F eature subset sE lection A lgorithm based on aS sociaT ion rule mining (FEAST), which can eliminate the irrelevant and re-dundant features while taking the feature interaction into consideration. More-over, FEAST uses association as the measure to evaluate the relativity between feature(s) and the target concept, which is quite different from the traditional measures, such as the consistency measure [4,20,29], the dependence measure [9,27], the distance measure [18,21] and the information theory measure [17,23]. The association measure evaluates irrelevant, redundant and interactive features in a uniform way, it is at least a potential alternatives for feature subset selec-tion. The experimental results on the synthetic and real world data sets show the effectiveness of the proposed algorithm.

The rest of the paper is organized as follows: In Section 2, we introduce the related work. In Section 3 we describe some preliminaries. In Section 4, we present the new feature subset select ion algorithm FEAST. In Section 5, we provide the experimental results. Finally, in Section 6, we summarize our work and draw some conclusions. Feature subset selection has been an active research topic since 1970 X  X , and a great deal of research has been published.
Of the existing research work, most feat ure selection algorit hms can effectively identify the irrelevant features based on different evaluation functions. But not all of them can eliminate the redundant features and take the feature interaction into consideration [3]. Thus, the existing feature selection algorithms can gen-erally be grouped into several categories according to whether or not they can deal with irrelevant features, redundant features and the feature interaction.
Traditionally, feature subset selectio n research has focused on searching for relevant features. Feature weighting/ranking algorithms [8] weigh features indi-vidually and rank them based on their relevance to the target concept. Unfortu-nately, they are incapable of removing redundant features. Such as well-known Relief and its extension Relief-F [18].

Moreover, along with irrelevant features, redundant features also affect the speed and accuracy of learning algorithms and thus should be eliminated as well [16]. CFS [9], FCBF [27] and CMIM [5] are examples that take into consideration the redundant features. However, they do not handle the feature interaction [29].
Feature interaction has been drawin g more attention in recent years. There can be two-way, three-way or complex mu lti-way interactions among features [7]. Jakulin and Bratko [12] use interaction gain as a heuristic to detect feature interaction. Their algorithms can detect 2-way (one feature and the class) and 3-way (two features and the class) interactions. Zhao and Liu [29] demonstrate that feature interactions can be implicitly handled by a carefully designed feature evaluation metric and a search strategy with a specially designed data structure.
Recently, association rules have been us ed for feature selection. Xie et al. [26] propose an association rule-based featu re selection algorithm FSBAR. Unfor-tunately, it just detects relevant features and does not handle redundant and interactive features. In contrast, our algorithm aims to eliminate the irrelevant and redundant features, and takes the multi-way feature interactions into con-sideration, hence it is quite different from these algorithms above. 3.1 Strong, Classification and Atomic Association Rules Association rule mining searches for interesting relationships among items in a data set D .Let I = { i 1 ,i 2 ,  X  X  X  ,i k } be a set of items, an association rule is an implication of form A  X  B ,where A  X  I , B  X  I ,and A  X  B =  X  .

The support and confidence are two important measures of a rule X  X  interest-ingness. 1. The support of rule A  X  B is the percentage of instances in D that contain 2. The confidence of rule A  X  B is the percentage value that shows how fre-Typically, association rules are conside red interesting if they satisfy minimum support threshold ( minSupp ) and minimum confidence threshold ( minConf ). The minSupp and minConf can be set by users or domain experts. Based on these two thresholds, strong association rule (SAR) can be defined as follow. Definition 1. Strong association rule ( SAR ). A rule r of form A  X  C is a strong association rule if and only if: Where Supp(r) and Conf(r) represent the support and confidence of the associ-ation rule r , respectively.

For the sake of introducing classification association rule (CAR) and atomic association rule (AAR), we first give the concepts of feature value itemset (FVIS) and target value itemset (TVIS).

Let D = { d 1 ,d 2 ,  X  X  X  ,d n } beadatasetof n instances, F = { F 1 ,F 2 ,  X  X  X  ,F m } be the feature space of D with m features, where F i is the domain of i th feature and Y be the target concept. The instance d i of D can be denoted as a tuple ( X i ,y i ), where X i  X  F 1  X  F 2  X  X  X  X  X  F m ,and y i  X  Y . Then the feature value itemset FVIS = value item set TVIS = Y .

With the definitions of FVIS and TVIS, classification association rule (CAR) and atomic association rule (AAR) are defined as follows.
 Definition 2. Classification association rule ( CAR ). A rule r of form A  X  C is a classification association rule if and only if: Here, | X | denotes the cardinality of set X . All CARs constitute classification association rule set (CARset).
 Definition 3. Atomic association rule ( AAR ). A rule r of form A  X  C is an atomic association rule if and only if: All AARs excluding atomic classification rules constitute atomic association rule set (AARset). Here, an atomic classification rule is an AAR whose consequent is the target concept value. 3.2 Definitions of Relevant, Redundant and Interactive Features To define the relevant, redundant features and feature interaction based on con-straint association rules (i.e., classification and atomic association rules), we firstly give the definitions of relevant feature value, redundant feature value and feature value interaction based on association rules. Definition 4. Relevant feature value ( RelFV ). A specific value f ij of feature F i is relevant to the target concept Y if and only if: Otherwise, f ij is an irrelevant feature value ( iRelFV ).
 Where f ij denotes the j th (1  X  j  X | F i | ) value of feature F i ,and r.Ante repre-sents the antecedent of rule r . The same notations are employed in the following definitions.

From Definition 4 we can know that, the feature values appeared in the an-tecedent of a rule r  X  CARset are relevant feature values; on the other hand, the feature values never appeared in the antecedent of any rule r  X  CARset are irrelevant feature values.

We know that classification association r ules have been extensively employed in classification [2,10,19], and these cla ssifiers usually possess preferable classi-fication accuracy. This indicates that t he rules in CARset can be used to effec-tively explore the relationship between features and target concept. The feature values appeared in the antecedents of CARs are necessary and related to the target concept. Thus, it is reasonable to identify the relevant feature values by Definition 4.
 However, the feature values appeared in a rule X  X  antecedent maybe redundant. That is, two closely-correlated feature values will be simultaneously appearing in the rule X  X  antecedent. This is because t hat the association rules are gener-ated based on frequent itemset mining (FIM) [24], but FIM cannot detect the redundant items (i.e., feature values) since that, for a given feature value, if it is frequent and selected into a frequent itemset, then the value being redundant to it will be frequent and selected into an itemset as well. To handle this problem, the redundant feature value is defined as follow.
 Definition 5. Redundant feature value ( RedFV ). A specific value f of a feature value set ( FVset ) is redundant if and only if: Where r.Ante and r.Cons represent the antecedent and consequent of rule r , respectively.

From Definition 5 we can know that, of a given feature value set, a feature value is redundant when it appeared in the consequent of a rule in AARset and the rule X  X  antecedent is in the given feature value set as well.

As we known, for a redundant feature value, the information it provides is already present in other feature value. This indicates that it is closely related to and can be replaced by other feature value. What X  X  more, atomic association rule can be used to explore the correlation between two feature values. Thus, Definition 5 based on AAR can be used to detect redundant value.

It is noticed that Definition 5 only shows the two-way value redundancy (the redundancy between two values). Of course, there might exist multi-way feature value redundancy (the redundancy among multiple feature values). However, detecting all the multi-way value redundancy is a combination explosion problem since we need to list all possible combinations. This is impracticable even when the feature space is of a middle size. Therefore, we just focus on the two-way redundancy in this paper.
 Suppose FVset = { f 1 ,f 2 ,  X  X  X  ,f k } is a feature value set with k feature values. It is a value-assignment set of a feature set Fset with k features, that is, each member of FVset corresponds to exactly a value of the feature of Fset. Let ( A  X  FVset) =  X  and B =FVset  X  A , y be a value of the target Y , Conf ( r )bethe confidence of an association rule r ,and r F , r A and r B be the CARs of FVset  X  X  Y = y } , A  X  X  Y = y } and B  X  X  Y = y } , respectively. Then, the interactive feature value can be defined as follow.
 Definition 6. k -th feature value interaction. The k feature values in FVset are said to interact with each other if and only if: The confidence of an association rule shows how well the rule X  X  antecedent de-scribes its consequent. The higher confidence means the stronger description ability. In Definition 6, the confidence of rule r F is greater than those of rules r A and r B . This means that although either feature value set A or B is not helpful in describing the target concept, FVset = A  X  B works well in describing the target concept. In this ca se, feature value sets A and B are said to interact with each other.

According to Definition 2, the classification association rules usually have high confidence since their confidence should be at least greater than minConf . This implies that all the rules with high confidence are included in CARset. In Definition 6, it is impossible that r A or r B is a CAR but r F is not a CAR, since Conf ( r F ) is greater than both Conf ( r A )and Conf ( r B ). Therefore, the antecedents of rules in CARset will contain a ll possible feature value interactions according to Definition 6. That is, the feature value interaction can be reserved by the rules in CARset.

Based on the definitions of relevant feature value (RelFV), redundant feature value (RedFV) and feature value interaction, relevant feature, redundant feature and feature interaction are defined as follows.
 Definition 7. Relevant feature ( RelFea ). Feature F i is relevant to the target concept Y if and only if: Otherwise, F i is an irrelevant feature ( iRelFea ).
 Definition 7 shows that a feature is relevant when at least one of its values is a relevant feature value. On the other hand, for an irrelevant feature, all its values are irrelevant. Definition 8. Redundant Feature ( RedFea ). Feature F i is redundant if and only if: Definition 8 indicates that a feature is redundant due to two reasons: (i) each value of this feature is a redundant feature value; (ii) some values of this feature are redundant while others are irrelevant. As irrelevant values provide no infor-mation about the target concept and redundant values provide the information which is present by the other values, they are all useless in describing the tar-get concept. This is consistent with the p roperty of the classical definition of redundant feature [28].
 Definition 9. Feature interaction. Let Fset = { F 1 , F 2 ,  X  X  X  , F k } be a feature subset with k features, and VAset be its value-assignment sets. Features F 1 , F 2 ,  X  X  X  , F k are said to interact with each other if and only if:  X  fset  X  VAset , { fset is a FVset with k -th feature value interaction } =  X . (9) As we known, there is an intrinsic relationship between a feature and its values, and the properties of a feature subset ca n be studied by its value-assignment. Thus, for a given feature subset, it is reasonable that the feature interaction among this feature subset could be implied and studied by that among its value-assignment. Inspired by this, Definition 9 based on feature value interaction is proposed to identify feature interaction. Based on the definitions of relevant feature, redundant feature and feature in-teraction, we propose a novel feature subset selection algorithm FEAST, which searches for relevant features while taking into consideration redundant features and feature interaction. 4.1 FEAST Algorithm The algorithm FEAST consists of four steps: i) Association rule mining , ii) Relevant feature value set discovery , iii) Redundant feature value elimination and iv) Feature subset identification . 1) Association rule mining 2) Relevant feature value set discovery 3) Redundant feature value elimination 4) Feature subset identification Algorithm 1 shows the pseudo-code description of FEAST. Of the input param-eters, minSupp and minConf are used as the constraint conditions to achieve strong association rule SAR (Definition 1).

The pseudo-code of FEAST includes four parts, in part 1 (lines 1-2), classifi-cation association rule set CARset an d atomic association rule set AARset are mined by function FP growth [11] on the given data set D according to minSupp and minConf . In part 2 (lines 3-4), the union of the antecedents of the associa-tion rules in CARset constitutes the rel evant feature value set RFVset. Part 3 (lines 5-13) is used to eliminate the redundant feature values in RFVset, where function Sort sorts the rules in AARset in descending order of rule X  X  confidence. Firstly, the first rule (i.e. the rule with the highest confidence) r is chosen and removed from AARset. Then if its antecedent is a subset of the current RFVset, the value in r  X  X  consequent is eliminated from RFVset; meanwhile, the rules whose antecedents are equal to its conse quent are removed from AARset. This process repeats until that AARset is e mpty. Part 4 (lines 14-17) achieves the selected feature subset S according to the feature values in RFVset. Time Complexity Analysis. In part 1, the CARset and AARset are mined by function FP growth . Since the time consumption of FP-growth is closely related to the value of minSupp [11], the time complexity of this part can be represented as O(f(minSupp, D)) ,where f(minSupp, D) is a function of minSupp and D which increases with the decrease of minSupp /increase of the size of D .Forpart
Algorithm 1. FEAST 2, once a CAR is generated by FP-growth, its antecedent could be merged into RFVset meanwhile, so the consumed time of this part can be ignored. For part 3, since its main time consummation is the process of sorting the rules in AARset, the time complexity of this part is O ( V  X  log V ) (by quick sort), where V is the number of rules in AARset. The time complexity of part 4 is O ( K )where K is the number of feature values in the final RFVset whose maximum value is the number of all possible feature values in D .

Consequently, the time complexity of FEAST is O(f(minSupp, D) + O ( V  X  log V )+ O ( K ). Since part 1 is the major time consumer in the worst case, the efficiency of FEAST depends largely o n that of association rule mining. In this section, we empirically evaluate the performance of FEAST, and present the experimental results co mpared with the other four representative feature selection algorithms upon both synthetic and real world data sets. 5.1 Benchmark Data Sets Synthetic Data Sets. In order to directly evaluate how well FEAST deals with irrelevant, redundant features and feature interaction, five synthetic data sets with all the irrelevant, redundant and interactive features being known are employed.

The first two data sets synData1 and synData2 are generated by the data generation tool RDG1 of the data mining toolkit WEKA 1 . The other three data sets about MONK X  X  problems are available from UCI Machine Learning Repository [1]. The five data sets are described as follows. 1) synData1. There are 100 instances and 10 boolean features a 0 ,a 1 ,  X  X  X  ,a 9 .The 2) synData2. There are 100 instances, 11 boolean features denoted as a 0 ,a 1 ,  X  X  X  , 3) MONK1. There are 432 instances and 6 features a 1 ,a 2 ,  X  X  X  ,a 6 .Thetarget 4) MONK2. There are 432 instances and 6 features a 1 ,a 2 ,  X  X  X  ,a 6 .Thetarget 5) MONK3. There are 432 instances and 6 features a 1 ,a 2 ,  X  X  X  ,a 6 .Thetarget For each data set, the features appearing in the definition of the target concept are all relevant, while the absent features are either redundant or irrelevant. The conjunctive terms in the target concept X  X  definition imply feature interactions. Real World Data Sets. 14 extensively used real w orld data sets, which are available from from UC Irvine Machine Learning Repository [1], are employed. Table 1 summarizes the 14 data sets in ter ms of number of features (denoted as F), the number of instances (denoted as I) , the number of target concept values (denoted as T). The sizes of data sets vary from 57 to 20,000 instances, and the total number of original features is up to 240. Note that for the data sets containing continuous-value features, if needed, we apply the MDL discretization method (available in WEKA).
 5.2 Experimental Setup 1) Four representative feature selectio n algorithms were selected to be compared with FEAST.
 These algorithms include two well-known and frequently-used CFS [9] and FCBF [27]. They can effectively identify irrelevant features while taking consid-eration of the redundant features.
To further study the performance of FEAST in terms of handling feature interaction, an algorithm INTERACT [29], which is specifically proposed to address the feature interaction, is selected as one benchmark algorithm.
Moreover, since our proposed FEAST is an association-rule-based feature se-lection algorithm, a latest association-rule-based feature selection algorithm FS-BAR [26] is selected as well.

The parameters of these algorithms (including FEAST) were determined by the cross-validation strategy. 2) Classification accuracy over selected feature subset is extensively used as a measure to evaluate the performance of the feature selection algorithm in feature selection literature. This is due to the fact that the relevant features of real world data sets are usually not known in advance, and we can not directly evaluate how good a feature selection algor ithm is by the features selected.
However, different classification algorithms have different biases, and a fea-ture subset selection algorithm may be more suitable for some classification algorithms than others. With this in mind, three different types of well-known classification algorithms including probability-based Naive Bayes [14], decision tree-based C4.5 [22] and rule-based PART [6] were selected.

In order to make best use of the data set and get stable results, the classifica-tion accuracies before and after fe ature selection were obtained by a 5  X  10-fold cross-validation procedure. That is, for a given data set, each feature selection algorithm and each classifier were repeatedly performed on the data set with 10-fold cross-validation by five times. 3) All the experiments were conducted in the WEKA environment [25]. 5.3 Results on the Synthetic Data Sets Table 2 shows the feature subsets selected by the five feature subset selection algorithms on the five synthetic data sets. In this table,  X   X  indicates a missing relevant feature, and the letter in bold type indicates an irrelevant or a redundant feature selected by mistake. The last row  X  X elevant features X  reports the actual relevant features of each data set.

From Table 2, we observe that: (i) Only algorithm FEAST removes all ir-relevant features while reserving all relevant features for all the five data sets. The other algorithms identify the irrelevant on some but not all data sets. (ii) Except algorithm CFS, all other four algorithms can identify and remove the redundant feature r in the data set  X  X ynData2 X . (iii) Only algorithm FEAST reserves all the interactive features on all the five data sets. INTERACT works well on all the data sets except for  X  X ynData2 X . The other algorithms identify all the interactive features on some but not all the data sets. 5.4 Results on the Real World Data Sets In this section, we present the compar ison results of FEAST with other fea-ture subset selection algorithms in terms of (i) the classification accuracies after feature subset selection; (ii) the proportion of selected features; and (iii) the runtime.

Here, the proportion of selected features is the ratio of the number of features selected by a feature selection algorit hm to the original number of features of a data set.

What X  X  more, we also provide the sensitivity analysis results of the support and confidence thresholds on the proposed algorithm FEAST.
 Classification Accuracy Comparison. Table 3 records the classification ac-curacies of Naive Bayes, C4.5 and PART with the five feature subset selection algorithms, and the Win/Draw/Loss records, which are the numbers of data sets where the classification accuracy of the g iven classifier obtained with FEAST is greater than/equal to/lower than that with the compared feature selection al-gorithm.

From Table 3 we observe that: 1) For Naive Bayes, (i) compared to the original data set, the average accuracy 2) For C4.5, (i) compared to the original data set, the average accuracy of C4.5 is 3) For PART, (i) compared to the original data set, the average accuracy of Proportion of Selected Features Comparison. The reduction on the num-ber of features is an important metric us ed to evaluate feature subset selection algorithms. This can be measured throu gh the proportion of features selected by the feature selection algorithms.

Table 4 presents the propo rtion of features selected by each of the five feature selection algorithms over the 14 data sets. From this table we observe that: i) All the feature subset selection algorithms could significantly reduce the number of features on average. FCBF ranks 1 with proportion of selected features 35.63%, and INTERACT ranks last with 51.78%. ii) FEAST outperforms algorithms INTERACT and FSBAR in reducing the number of features.
 Runtime Comparison. Table 5 records the runtime of each feature subset selection algorithm upon the 14 data sets. From it we observe that (i) the average runtime of different algorithms is varying greatly, FCBF ranks 1 with 226.29 ms, and FSBAR ranks last with 28533.08 ms. (ii) FEAST is faster than INTERACT and FSBAR. Compared with the associative-based algorithm FSBAR, FEAST is much more efficient since it generates asso ciation rules by FP-growth algorithm which is more efficient than the Apriori algorithm used in FSBAR.

To summarize, the proposed algorithm FEAST outperformed other feature subset selection algorithms on the 14 UC I data sets in terms of average classifi-cation accuracy and Win/Draw/Loss record, and the runtime and the reduction rate are acceptable.
 Sensitivity Analysis of the Support and Confidence Thresholds. Sup-port threshold and confidence threshold are two important parameters in the pro-posed algorithm FEAST. To study how they affect the performance of FEAST, in this part, we give the sensitivity analysis of these two parameters on FEAST in terms of classification accuracy, propo rtion of selected features and runtime, respectively.
 Classification Accuracy. Fig. 1 shows sensitivity analysis results of the support and confidence thresholds on the classification accuracies of the three classifiers with respect to our proposed algorithm FEAST.
From Fig. 1(a) and 1(b) we observe that (i) for a given data set, the clas-sification accuracy varying trends of the three classifiers w.r.t FEAST are very similar for either the given support thresholds or the given confidence thresholds. This reveals that the FEAST has no bias for a special classifier, i.e. the results obtained by FEAST are generally suitable. (ii) The classification accuracy varies with both the support and confidence thresholds, and the thresholds correspond-ing to the highest classification accuracy are different for different data sets. For example, in Fig. 1(a), the support threshold corresponding to the highest classi-fication accuracy is about 10% for  X  X ustra X , while less than 5% for  X  X olic-orig X . In Fig. 1(b), the confidence threshold corresponding to the highest classification accuracy is greater than 95% for  X  X utos X , while about 70% for  X  X plice X . This implies that both support and confidence thresholds affect the feature subset selected by FEAST, and the best threshol ds are different for different data sets. Proportion of Selected Features. Fig. 2 shows sensitivity analysis results of the support and confidence thresholds on the proportion of features selected by the proposed algorithm FEAST.

From Fig. 2(a) we observe that for all the 14 data sets, with the increment of the support threshold, the proportion of the selected features decreases. The reason is that with the increment of the support threshold, the number of the frequent itemsets decreases. At the s ame time, FEAST chooses feature subset from itemsets that are at least frequent , thus the number of the selected features deceases, and the proportion of the select ed features decreases as well. We also observe that although the proportion of the selected features deceases with the increment of the support threshold, for the different data sets, the decrement extents are varying. Therefore, we should choose different support thresholds for the different data sets.

From Fig. 2(b) we observe that with the increment of the confidence thresh-old, the proportion of selected features either increases or decreases. The rea-son is that for a given confidence threshold, there are many support thresholds with varying values. Further, for the different confidence thresholds, the varying ranges of the support thresholds are different. This means the corresponding numbers of the frequent itemsets and fur ther the proportions of selected fea-tures are different as well. This reveals that both the support and confidence thresholds are affected by data set chara cteristics and we should select different thresholds for different data sets.
 Runtime Fig. 3 shows the sensitivity analysis results of the support and confi-dence thresholds on the runtime of our proposed algorithm FEAST.

From Fig. 3(a) we observe that for all the data sets, the runtime of FEAST decreases when the support threshold in creases. This is beca use with the incre-ment of the support threshold, the number of the frequent itemsets is decreased. So the time spending on mining the frequent itemsets is decreased as well. At the same time, FEAST chooses the feature subset from the itemsets that are at least frequent, thus the time consumed in the feature subset identification is also deceased.

From Fig. 3(b) we observe that the runtime of FEAST can increase, decrease and fluctuate when the confidence threshold increases. The reason is that for a given confidence threshold, there are many support thresholds with varying values. Further, for the different confidence thresholds, the varying ranges of the support thresholds are different. This means that the corresponding numbers of the frequent itemsets, and further the num bers of selected features are different as well. Thus, the time used to mine frequent itemsets and to identify feature subset is varying.

To summarize, the performance of the p roposed algorithm FEAST is directly affected by the selection of these two input-parameters: support and confidence thresholds. However, the appropriate thresholds for different data sets would be different. That is, there are no specific support and confidence thresholds which are the best choice for all the data sets. We should pick up different thresholds for different data sets. In this paper, we have presented a novel constraint association rule based feature selection algorithm FEAST. We have also compared FEAST with the other four representative feature selection algorithms, including two well-known algorithms CFS and FCBF, the algorithm INTERACT aiming at solving feature interaction, and an associative-rule-based algorithm FSBAR, upon both the five synthetic data sets and the 14 UCI data sets. The results on the synthetic data sets show that FEAST can identify relevant features and remove redundant ones while reserving feature interaction. The results on the real world data sets show that our proposed algorithm FEAST can reduce the number of features and outperforms all the other four feature selection algorithms in terms of the average accuracy improvement and the Win/Draw/ Loss records of all the three different types of classifiers Naive Bayes, C4.5 and PART.

We have also conducted a sensitivity analysis of support and confidence thresh-olds to FEAST. The results show that the support and confidence thresholds play a fundamental role in the proposed algorithm. Moreover, for different data sets, the appropriate thresholds could be different. Therefore, for further research, we plan to explore how to recommend the support and confidence thresholds for FEAST according to data set characteristics.

