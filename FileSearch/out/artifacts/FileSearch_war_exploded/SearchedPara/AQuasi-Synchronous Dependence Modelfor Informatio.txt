 Incorporating syntactic features in a retrieval model has had very limited success in the past, with the exception of binary term dependencies. This paper presents a new term depen-dency modeling approach based on syntactic dependency parsing for both queries and documents. Our model is in-spired by a quasi-synchronous stochastic process for machine translation [ 21]. We model four different types of relation-ships between syntactically dependent term pairs to perform inexact matching between documents and queries. We also propose a machine learning technique for predicting optimal parameter settings for a retrieval model incorporating syn-tactic relationships. The results on TREC collections show that the quasi-synchronous dependence model can improve retrieval performance and outperform a strong state-of-art sequential dependence baseline when we use predicted opti-mal parameters.
 H.3.3 [ Information System ]: Information Search and Re-trieval Algorithm, Experimentation, Performance Term Dependency, Quasi-synchronous Grammar, Weighting Retrieval Model
Term dependency has been studied for several decades to improve the effectiveness of information retrieval. Although independence assumptions simplify retrieval models, terms are actually dependent upon each other within documents and within queries. Terms are used together to make more specific meanings or, sometimes, totally different meanings. Despite this, it has been difficult to develop retrieval mod-els incorporating term dependence that show consistent im-provements over those that assume term independence. One of the challenging issues in modeling term dependence is that a concept can be expressed in different ways syntactically as well as lexically. In describing a concept in their queries, people use different vocabularies than authors use in de-scribing the same concepts in their documents [ 27]. Query term expansion techniques have been studied to deal with a lexical mismatch between queries and relevant documents.
Similarly, even if the vocabularies in queries and docu-ments are identical to each other, their relationships can differ in various ways: specifically, in order, proximity, and grammatical relation. In order to take account of the vari-ability in relationships of terms between queries and docu-ments, successful term dependence models in previous work allowed inexact matches in the order and proximity of depen-dent terms [ 16, 23]. Figure 1 shows example sentences con-taining the words  X  X hemical X  and  X  X eapons X . Even though the concept implied by  X  X hemical weapons X  is similar, their orders, distances, and syntactic relations vary between the example sentences. If we strictly regulate the order, dis-tance, and syntactic relation of dependent terms, we will miss  X  X hemical weapons X  in relevant documents. The se-quential dependence model (SDM) can match these impor-tant dependent terms by allowing differences in their order and proximity [ 16].

However, current models typically ignore term dependence based on syntactic relationships in queries and often restrict dependency relations to adjacent term pairs. If the example
A l-Rabta chemical weapons plant was uncovered and destroyed in a fire. . . . mentions ricin, sarin, soman, or anthrax as a toxic chemical used as a weapon . . . . its chemical and biological weapons and nuclear program.

He intends to produce not only chemical but also bac-teriological weapons .
 Fig ure 1: Example sentences that show syntactic variations in the term dependency of the concept  X  X hemical weapons X  sen tences in Figure 1 are used in a query, all dependencies except the first will be ignored because they are not adjacent. Gao et al. [ 7] propose an approach which tries to extract long-distance term dependencies by incorporating informa-tion from syntactic dependencies. Song et al. [ 22] and Lee et al. [12] also propose a model based on linguistic parsing. By choosing syntactically related term pairs in a query as dependent terms, these models attempt to overcome the lim-itation of sequential dependencies. However, the models are still limited to a head-modifier relation in which two depen-dent terms are directly linked in a dependency structure. Thus, they would still ignore all dependencies in Figure 1 except the first.

In this paper, we propose a term dependence model in-spired by a quasi-synchronous stochastic process developed by Smith and Eisner [ 21]. Synchronous grammars were pro-posed for machine translation to generate translated expres-sions or identify translation examples by aligning a parse tree in a source language to a parse tree in a target lan-guage [ 20 ]. Because of inherent incompatibility between a source language and target language, syntactic and lexical variations occur during translating from a source sentence to a target sentence. Thus, a synchronous model should be able to align a translation unit in a source language to differ-ent forms than that of the original [ 8]. Smith and Eisner [ 21] model several different types of syntactic configurations in a target language. A synchronous model allows parent-child words in a source language tree to be associated with words having different syntactic configurations in a target language tree.

This inexact matching process of the quasi-synchronous model has also shown significant improvements for other re-search tasks such as open domain question-answering(QA) [ 26] and paraphrasing [ 6]. The processes of selecting answer sen-tences and paraphrased sentences are interpreted as a free translation process between sentences in the same language. In a similar way, we adopt the quasi-synchronous stochastic approach and generalize it for information retrieval, where a target sentence (a query) is generated from a set of sen-tences (a document) instead of a single source sentence. By using the quasi-synchronous stochastic approach, we repre-sent term dependence more flexibly and incorporate trans-formations of dependence relationships.

The remainder of this paper is organized as follows. Sec-tion 2 introduces previous work about term dependence mod-els, syntactic features, and parameter optimization methods for integrating different retrieval models. In Section 3 we explain how our model covers syntactic variations between queries and documents. Next, Section 4 describes a method to predict optimal weights of models for a given query. In Section 5 we evaluate our proposed model. Finally, in Sec-tion 6 we conclude and outline future work.
Term dependency has long been studied in the field of information retrieval and has been based on various fea-tures such as co-occurrences of terms, proximities between terms, etc. [ 5, 7, 12, 16, 23, 25]. Dependence models based on syntactic parsing results have been proposed to capture long-distance term dependencies. Gao et al. [ 7] proposed a dependence language model in which term dependencies were selected based upon the linkage structure L of queries and documents. The fundamental idea of this model is that Fig ure 2: The sample linkage graph L of a query in [7]. They consider only a direct linkage between a word and its head word. queries and documents are represented in the form of the hidden variable, an acyclic, planar, undirected linkage graph L as shown Figure 2. The dependence language model gen-erates not only a query but also the linkages of the query as follows Then, dependent terms are defined by the edges in a linkage graph L and  X  (  X   X   X ,  X  ) is decomposed in the similar way with the bigram model. in which,  X   X  is the sentential head word of a query.  X   X  head word of  X   X  in L.

Maisonnasse et al. [ 13] extend this dependence language model using a syntactic and semantic analysis model. Lee et al. [12 ] also suggested a language model which is based on de-pendency parse trees generated by a linguistic parser. These models aim to solve the limitation of the language model approach in which models fail to detect long-distance de-pendencies when just replacing the unigram language model with a bigram or biterm language model. However, they still have the limitation that term dependencies are derived only from head-dependent term pairs or directly connected nodes in the linkage structure L. As shown Figure 1, a head-dependent relation of terms can be expressed using indirect relations without changing its meaning.

Song et al. [ 22] introduced variability , which represents the probability that a head-modifier term pair in a query will not have a head-modifier relation in a document. They observe that some head-modifier term pairs in queries have stronger relations with each other than other head-modifier term pairs. If a strongly tied term pair, e.g.  X  X utual X   X   X  X und X , is unseen in a document, it is unlikely to expect the strongly tied term pair be used in a different way. On the other hand, although a weakly tied term pair, e.g.  X  X extile X   X   X  X roduct X  is unseen in a document, we can expect that the term pair is used with a transformed relation in the document. Based on this assumption, they use predicted variabilities for an interpolation weight as follows. in which,  X   X  is the variability of a head-modifier  X   X  and  X  and  X  represent a document and a collection, respectively. Although the variability implies the possibility that there are the transformations of the syntactic relationships of depen-dent terms between queries and documents, the variability still does not consider the transformed term pairs explicitly. Fi gure 3: Example of syntactic parsing results (The description query of the TREC topic 625)
To identify dependencies of terms beyond head-modifier in both queries and documents and also to handle variations in dependence relations of terms, we adopt a relaxed alignment approach, specifically a quasi-synchronous grammar devel-oped by Smith and Eisner [ 21]. As in Berger and Lafferty [ 3], we view the probability  X  (  X   X   X  ) as the document-to-query translation process. We bring a quasi-synchronous approach to information retrieval and generalize it to adapt it for an alignment between a query and a document. In the next sec-tion, we briefly describe dependency parsing technique and then explain the quasi-synchronous model for information retrieval.
Dependency grammar describes linguistic structures di-rectly in terms of dependencies between words themselves [ 14]. This is especially useful in information retrieval, where the words themselves, and not additional phrasal or clausal struc-tures, are not our primary concern. Dependency parsers output directed trees over words: each word in the sentence has exactly one incoming edge, which comes from its  X  X ead X  or  X  X arent X , with the except of the  X  X oot X  word, which has no incoming edges. An example dependency tree for a query is depicted in Figure 3.

In information retrieval, we wish to measure the distance between a query and various candidate documents. By us-ing dependency syntax, we can cast this problem as one of distance between a query tree and document trees. Of espe-cial interest are the many approaches to comparing trees in the machine translation literature. In particular, Smith and Eisner proposed a quasi-synchronous stochastic process [ 21] to allow parent and child words in a source-language tree to be associated with words having different syntactic relations in a target-language tree.
Synchronous grammars, originally proposed for machine translation [ 20 ], jointly generate trees of a source and tar-get sentence. Depending on the size and complexity of the rewrite rules in a synchronous grammar X  X nd the compu-tational complexity required for inference X , the source and
The inspectorate searched chemical weapons. (a) parent-child
The inspectorate searched toxic chemicals which is used as weapons .

The inspectorate searched the chemical compounds, the weapons of mass destruction.

The inspectorate searched the chemical compounds which is used as weapons . (c) siblings (b) ancester-descendent (d) c-commanding Fig ure 4: Four types of syntactic dependency con-figurations for the quasi-synchronous model. The quasi-synchronous model matches terms in queries and documents along with transformations be-tween these dependence relations: (a) parent-child, (b) ascendant-descendant, (c) siblings, and (d) c-commanding. target trees can diverge more or less in their structures. For information retrieval and other problems, however, we are only interested in the conditional probability of one tree given another; moreover, we are usually not interested in the full generative process of either tree but only in the parts of the two trees that match with greater or lesser fi-delity. Words in a source tree (e.g., in a document) are not always translated into a target tree (e.g., a query) with the same syntactic structure. Some source words may be trans-lated into one target word, and others may be match more than one word or a phrase. To solve these disagreements in source and target languages, a quasi-synchronous model allows words in a target sentence, which are aligned with a words in a parent-child dependency relation in a source sentence, to have a different relationship to each other. In this paper, we consider the four following configurations: Figure 4 shows examples for terms, X  X hemical X  X nd X  X eapon X .
Like the language model framework, the basic idea of the quasi-synchronous model is to rank a document using the probability that a query is generated by the document model. However, we infer a document model from the de-pendency tree  X   X  of a document rather than the raw term sequence  X  as in the dependence language model [ 7]. The document model generates not an individual term or a de-pendent term pair but a fragment of the parsing tree  X   X  of a query  X  through the loose alignment  X  . where  X   X  and  X   X  are the parsing trees of a query and a document, respectively.  X  is a set of possible combinations of the four syntactic configurations between a query and a document.

In previous work [ 7, 12 , 22], valid alignments  X  contain only parent-child relations. Our quasi-synchronous approach, on the other hand, allows inexact matching from all four above-listed syntactic configurations to all other. More specif-ically, when we consider the transformation of the syntac-tic configurations, being aligned with a parent-child (  X  is different from being aligned with a child-parent (  X   X  Parent-child, ancestor-descendent, and c-commanding re-lations are different according to the order of two terms. Therefore, a dependent term pair in a query can be aligned not to four syntactic configurations but to seven syntactic configurations, and  X  has 4  X  7 elements for the combina-tions of syntactic configurations in a query and a document.  X  (  X   X   X   X  )  X  (  X   X   X   X ,  X   X  ) = where  X  X  X  X   X  and  X  X  X  X   X  are one of the four syntactic con-figurations and  X  (  X  X  X  X   X  ,  X  X  X  X   X   X   X   X , X  X  X  X   X  ) is the probability that a syntactic relation  X  X  X  X   X  in a document is used in the form of  X  X  X  X   X  in a query. Intuitively, the probabil-ity that  X  X  X  X   X  is transformed to  X  X  X  X   X  is different accord-ing to different terms as pointed out by [ 22]. For exam-ple, two words for a person X  X  name are less likely to be used in the forms of ancestor-descendant, siblings, or c-commanding. However, to estimate the probabilities for a quasi-alignment, the number of parameters becomes imprac-tically large. Therefore, for simplicity, we use a uniform distribution for  X  (  X  X  X  X   X  ,  X  X  X  X   X   X   X   X , X  X  X  X   X  ). awhere N is the number of elements in the set A.  X   X , X  X  X  X   X  and  X   X , X  X  X  X   X  represent a set of dependent terms having  X  X  X  X   X  and  X  X  X  X   X  in a document and a query, respec-tively. For example,  X   X , X  X  X  X  X  X  X  X  X   X   X  X  X  X  X  X  X  of the query in Figure 3 are (anthrax,sarin), (anthrax,ricin), (anthrax, soman), and (chemical,toxic). The probability  X  (  X   X , X  X  X  X   X   X   X ,  X   X , X  X  X  X  computed by the non-negative potential function of a term pair that has a syntactic relation  X  X  X  X   X  in a query and  X  X  X  X  in a document as follows: where  X   X  and  X   X  are dependent terms with relations  X  X  X  X   X  in the parse tree of a query. Because the model considers more complex term dependencies, a harmful term depen-dency is more frequently introduced by unimportant terms in a query. To circumvent this problem, we use the query term ranking score of [ 18 ].  X  (  X   X  ,  X   X  ) is the mean value of query term ranking scores of  X   X  and  X   X  .  X  (  X   X  ,  X   X   X   X  calculated in the same way of the potential function in [ 16 ].  X  X  X   X  , X   X  , X  X  X  X   X  is the term frequency of term pairs  X   X  and  X  the syntactic relation  X  X  X  X   X  in a document  X  . We smoothed the probability distribution using the Dirichlet smoothing algorithm.  X  X  X   X   X  , X   X  , X  X  X  X   X  is the collection frequency of term pairs  X   X  and  X   X  with the syntactic relation  X  X  X  X   X  in a collec-tion.

Compared to the quasi-synchronous models for machine translation, QA, and paraphrasing in previous work [ 21, 6, 26], our quasi-synchronous model: (1) aligns single query sentences with entire documents; (2) considers syntactic vari-ations, but not lexical translation, through quasi-synchronous matching; and (3) considers features other than head-modifier matching not only in documents but also in queries. We ex-pand on these characteristics below.

First, basically, our model matches terms of different units: a document and a query. On the other hand, in the previous work, matching is conducted between sentences. Therefore, terms of a query are allowed to match multiple times with terms in a document in information retrieval. As pointed out by Wang et al. [ 26], we are not interested in actual alignment between a query and a document. Rather, we interpret the process of matching as one in which a term pair with various dependency relations is generated by a document.
Second, our model focuses only on syntactic variations be-tween queries and documents. The original quasi-synchronous grammar is established to consider both syntactic and se-mantic variations. Smith and Eisner[ 21 ] allows the original quasi-synchronous model to have a NULL alignment and a 1-to-2 alignments. Wang et al.[ 26 ] extends a term pair in a question to consider 2 13  X  1 possible combinations of ex-panded term pairs by using a thesaurus, WordNet. A quasi-synchronous model for information retrieval must take into account an entire collection while previous work has tar-geted a relatively small number of sentences. It is imprac-tical to consider these kinds of semantic variations for all Wha t is the prognosis for new drugs? Find ways of measuring creativity.
 Wha t are commercial uses of Magnetic Levitation? Mexico City has the worst air pollution in the world
Wha t are the arguments for and against Great Britain X  X  approval of women being ordained as Church of England priests?
What are the industrial or commercial uses of cyanide or its derivatives? Fi gure 5: Example queries from the Robust 2004 collection that demonstrate better results when more weight is assigned to the query likelihood model, the sequential dependence model and the quasi-synchronous model, respectively. documents. Therefore, we make the model consider only term pairs which are lexically identical. We also ignore the transformations to a NULL node, an identical node and an arbitrary term pair in a sentence.

Third, most previous work [ 7, 13, 21, 22 ] treats only a parent-child relation or a head-modifier dependency in the parse tree of a query. On the other hand, in the quasi-synchronous model, we expect to cover various dependency relations of terms in both a query and a document. As all the four syntactic configurations in a document can have important meanings, dependent terms having the four syn-tactic configurations in a query would also be important to find relevant documents. Thus, we model the transforma-tion from all the four syntactic configurations to all the four syntactic configurations instead of counting only the direct head-modifier relation.
Although the quasi-synchronous model is based on the premise that syntactically dependent terms are important for retrieving relevant documents, a retrieval model based on independence assumptions or simpler dependence assump-tions can also be effective and may outperform our model for at least some queries. The quasi-synchronous model ranks a document based on complex syntactic term dependencies in a parsing tree, so it is less applicable for a query con-taining a single important keyword. Figure 5 shows some example queries. In the first group of queries, the impor-tant terms are not expected to be used with specific depen-dencies. Therefore, treating terms individually is sufficient to retrieve relevant documents. Because dependent terms in the second group of queries are placed near each other, the quasi-synchronous model may consider unnecessary de-pendencies and can assigns inaccurate scores to the docu-ments containing these dependencies. We need to combine the quasi-synchronous model with other retrieval models to address this problem.

When we combine several retrieval models, it is important to assign a proper weight to each retrieval model according to the characteristics of queries. Metzler [ 15 ] claimed that applying different types of models to new tasks typically requires an information retrieval expert to modify the un-derlying model in some way to properly account for the new types of features. He suggests an automatic feature selec-tion model which determines the optimal weight of a lin-ear feature-based model by using a greedy procedure. Zhai and Lafferty [ 29] also stressed an optimal parameter setting depends upon not only on document collections but also queries. Consider a linear interpolated model of the quasi-synchronous model and the sequential dependence model as follows:  X  (  X ,  X  ) =  X   X   X  X  X  X  (  X ,  X  ) + (1  X   X  )  X   X  X  X  X  X  X  X  X  X  X  X  X  X  (  X ,  X  ) (9) where  X  X  X  X  (  X ,  X  ) and  X  X  X  X  X  X  X  X  X  X  X  X  X  (  X ,  X  ) represent the log-scores of the document  X  given a query  X  measured by the sequential dependence model and quasi-synchronous model, respectively. In this preliminary experiment, we select an optimal parameter  X  value which maximizes the overall av-erage precision of the interpolated retrieval model. Fig-ure 6 demonstrates that the distribution of weights accord-ing to the length of queries. This result demonstrates that the sequential dependence model and the quasi-synchronous model have their own advantages for different types of queries. If we use a fixed parameter for all queries, the quasi-synchronous model improves the effectiveness for some queries but also adversely affects the performance for other queries.
We can find an optimal parameter setting for a new task and document collection although this may require excessive tuning. On the other hand, it is impossible to optimize a parameter setting for an unseen query. To solve this prob-lem, we exploit a machine learning approach to predict the optimal parameter settings for individual retrieval models based on a given query. Machine learning methods have been intensively studied for query term ranking approaches to predict the importance of an individual term or a set of terms in a given query [ 1, 2, 10, 11 , 18, 28]. We extend this general approach to weighting the combination of different retrieval models.

In training data for training a query term ranking model, each term or the set of terms is labeled by its optimal weights. Similarly, we train a prediction model to measure the im-The length of a query Fig ure 6: The distribution of optimal weight accord-ing to the length of a query from Robust 2004 col-lection when linear interpolating the sequential de-pendence model and the quasi-synchronous model. p ortance of a individual retrieval model for a given query. Training data for the prediction model consists of a query and the optimal weights of retrieval models, where  X   X  is a i th query and its feature vector.  X   X  X  X  is the optimal parameter setting of a j th sub retrieval model for the i th query.

A training label  X   X  X  X  , which is the optimal weight for a i th query and a j th retrieval model, are selected empirically. First, we retrieve an initial ranked list of documents for a specific query by using a baseline retrieval model. When we retrieve initial documents, we make the retrieval model re-trieve more documents because we wish training data cover documents out of the rank which may be retrieved by differ-ent parameter settings. Then, we choose optimal parameter values of retrieval models which maximize the performance of the initial document set. In this paper, we used mean av-erage precision (MAP) as the retrieval metric. The weights in Figure 6 were chosen in this way.
 The regression model we adopt in this paper is Support Vector Regression (SVR) [ 4]. Table 1 shows the list of fea-tures. Statistical features are an aggregation of feature val-ues representing the characteristics of an individual term. Syntactic features are derived from the dependency parsing result of a query. These features are used to measure the Table 1: A summary of features used to measure importance of models for an individual query.
  X   X   X  X  X  The length of query  X  X  X   X  X  X   X   X / X  X  X /  X  X  X  X  X  X  X   X   X  X  X  X  X  X   X   X  X  X  X /  X  X  X  X / X   X  X  X  X   X  X  X  X  X  X   X  X  X  X  X  X   X  The average of query term ranking scores  X  X  X  X  X  X  X   X  X  X  X  X  X  The ratio of stopwords in a query  X  X  X   X   X  X  X  X  X  X  X  X  X  X  Is a query a question?  X  X  X   X   X  Is a query a wh-question?  X  X  X  X   X   X  The number of a noun phrases in a query  X  X  X  X   X  X  X   X  X  X  X  X  X  The number of a dependent clauses in a query  X  X  X  X  X  X   X  X  X   X  X  X   X  X  X  The height of a parsing tree of a query  X   X  X  X  X  X  X   X   X / X  X  X  /  X  X  X / X  X  X  n umber of the complex syntactic relations of terms in the query have. The higher the number of noun phrases and the depth of a parsing tree, the more complex the syntactic structure of a query. The quasi-synchronous model aims to capture these complex syntactic relations of terms from both queries and documents. These features also reflect whether we need to consider syntactic variations between a query and a document. For example, if a query is a wh-question such as a factored question, a relevant document contains terms of the query without rephrasing it.
In this section, we describe experiments to evaluate the quasi-synchronous model. In particular, we aim to compare the effectiveness of the quasi-synchronous approach to mod-els based on the term-independence and sequential depen-dency assumptions. For this purpose, we interpolated the quasi-synchronous model with the query likelihood model [ 19] and the sequential dependence model [ 16 ] to compare the quasi-synchronous model in different settings.
We made use of the TREC Robust 2004 and Gov2 col-lections for experiments. Some statistics for these two col-lections are shown in Table 2. All documents were indexed using the Indri search engine [ 17]. Terms were stemmed by using the Porter stemmer and were stopped using a standard list of stopwords. Only the description queries were used be-cause our approach targets queries submitted in well-formed sentences 1 . We used Dirichlet smoothing for both the quasi-synchronous model and other retrieval models.
 Table 2: The statistics of the TREC Robust 2004 and Gov2 collections Co ll Desc ription # Doc. TR EC topics Ro 04 New wire articles 5 28,155 30 1-450, 601-700 Go v2 .go v web collection 25 ,205,179 70 1-850
F or the Robust 2004 collection, all documents and queries were parsed using the Stanford dependency parser [ 9]. The Stanford dependency parser internally includes the Stanford Part-of-Speech tagger [ 24 ]. Because the quasi-synchronous model needs an acyclic tree structure, we use the basic de-pendency representation form instead of the Stanford parser X  X  collapsed representation. For the Gov2 collection, it is im-practical to parse all documents. Therefore, we retrieve an initial document set using a baseline retrieval model. We then parsed documents in the initial set and evaluate the quasi-synchronous model only on the initial document set. Because the dependency parser accepts raw text format, we used lynx 2 to convert the documents of the Gov2 collection in the TREC web format to raw text format before parsing documents.

To predict optimal weights for the interpolation of re-trieval models, we used the support vector regression method
Co mpared to previous work, we use original description queries instead of refined queries from which command stop phrases are manually deleted. http://en.wikipedia.org/wiki/Lynx (w eb bro wser) three scores with fixed weights: the query-likelihood score  X  unordered window score  X   X  X  X  8 [16]. which predicts the approximate target value based on a given feature vector [ 4]. We trained the regression model for each query using leave-one-out cross-validation in which one query was used for test data and the others were used for training data. Among the features in Table 1, some features are not appropriate for a certain retrieval model. For example, the number of term pairs having parent-child, ancestor-descendent, siblings and c-commanding in a query may be not discriminative to weight the ordered window function and the unordered window function in the sequen-tial dependence model. Thus, we chose ten features for each interpolation strategy based on an self-evaluation result us-ing the training data.

The dependent term pairs of the quasi-synchronous model include 98 . 31% of adjacent term pairs (3,365) Among the 3,365 adjacent term pairs, 54 . 65% (1 , 839) of them have a parent-child relation. 19 . 67% (662), 6 . 03% (203) and 19 . 64% (661) have ancestor-descendent, siblings and c-commanding relations, respectively.
We combine the quasi-synchronous model with two base-line retrieval models using four different interpolation strate-gies. Figure 7 shows these four linear interpolation strate-gies. The first baseline model is the query-likelihood model (  X  X  X  ), a standard bag-of-word retrieval model based on the independent assumption [ 19]. The other baseline model is the sequential dependence model (  X  X  X  X  ), which consists of three factors: a query likelihood factor, an ordered window factor and an unordered window factor [ 16]. In the four in-terpolation strategies, the three factors of the sequential de-pendence model are interpolated in two ways. The  X   X  X  X  X  +  X  X  X  X  X  X  X  X  X  X  X  X  X   X  strategy interpolates three factors using fixed weights X   X   X  X  X  X  = 0 . 85  X   X   X  X  X  + 0 . 10  X   X   X  X  X  1 + 0 . 05  X   X  then, interpolates  X   X  X  X  X  and  X   X  X  X  X  X  X  X  X  X  X  X  X  X  using predicted op-timal weights. The X   X  X  X  +  X  X  X  1+  X  X  X  8+  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X trat-egy use predicted weights for all the individual factors:  X   X 
Table 3 shows the experimental results of the four inter-polation strategies. The statistical significance of the dif-ferences in the performance is determined using a two-sided Wilcoxon sign test, with  X  &lt; 0 . 05. As described in section 3, compared to the previous work, the quasi-synchronous model is different with respect to allowing inexact matching of syntactic relations between queries and documents. In Table 3,  X  Quasi-Synchronous Matching  X  represents the experimental results where we allow inexact matching while  X  Exact Matching  X  X hows the experimental results when we align between term pairs having only the same syntactic rela-tion. For all interpolation strategies, the quasi-synchronous approach shows better results than exact matching.
Among the four interpolation strategies in Figure 7, all the strategies with the quasi-synchronous model show significant improvements compared to a stat-of-art baseline model, the sequential dependence model.  X  X  X  X  +  X  X  X  X  X  X  X  X  X  X  X  X  X  achieves the best improvement. On the other hand, predicting the weights for the factors of the sequential dependence model fails to show improvement. The performance of  X  X  X  +  X  X  X  1+  X  X  X  8 is similar with that of the sequential dependence model. T able 3: Experimental results with the Robust 2004 with four interpolation strategies. Numbers in parentheses depict % improvement over the se-quential dependence model.
  X  X  X  +  X   X  1 +  X  X  X  8 0. 2462  X  X  X  +  X  X  X  X  X  X  X  X  X   X  X  X  X  0 .2754  X   X  X  X  +  X  X  X  X  X  X  X  X  X  X  X  X  X  0 .2786  X  X  X  +  X   X  1 +  X  X  X  8 0. 2765  X   X  0. 5440  X   X  0. 4582  X   X   X  X  X  +  X   X  1 +  X  X  X  8 0 .2583  X   X  0 .5218  X   X  0 .4361  X  Table 4: Mean Average Precision of the Robust 2004 collection when we use the four interpolation strate-gies using the true optimal weights of the training data.  X  X  X  X  X  X  X  X  X  X  X  means  X  X uasi-Synchronous Match-ing X  or  X  X xact Matching X  in the second and third column, respectively.
  X  X  X  +  X   X  1 +  X  X  X  8 +  X  X  X  X  X  X  X  X  X  X  X  0 .3165 0. 2936 The performance of  X  X  X  +  X  X  X  1 +  X  X  X  8 +  X  X  X  X  X  X  X  X  X  X  X  X  X  is slightly worse than that of  X  X  X  X  +  X  X  X  X  X  X  X  X  X  X  X  X  X  .
To see the potential of the quasi-synchronous model, we evaluate the four interpolation strategies using the training label as the interpolation weights. Table 4 shows the exper-imental results with the Robust collection.

When we use the training label or the true optimal weight, the (  X  X  X  +  X  X  X  1+  X  X  X  8+  X  X  X  X  X  X  X  X  X  X  X  X  X  ) strategy demonstrates the best results. The  X  X  X  +  X  X  X  1+  X  X  X  8 strategy is also bet-ter than the baseline. This demonstrates that the sequential dependence model still has a considerable margin for being improved by using a proper parameter setting instead of a fixed parameter setting.
 Comparing the MAP value of  X  X  X  +  X  X  X  X  X  X  X  X  X  X  X  X  X  or  X  X  X  X  + Table 5: Experimental results with the Gov2 col-lection based on an initial document set retrieved by the sequential dependence model. Numbers in parentheses depict % improvement in each evalua-tion measure.
  X  X  X  +  X   X  1 +  X  X  X  8+ 0. 2764  X  0. 5342  X  0. 5396  X  mea ns statistically significance difference with SDM  X  X  X  X  X  X  X  X  X  X  X  X  X  with  X  X  X  +  X  X  X  1+  X  X  X  8, the quasi-synchronous model has higher potential for taking into an account term dependencies than the sequential dependency model. Mean-while, (  X  X  X  +  X  X  X  1+  X  X  X  8+  X  X  X  X  X  X  X  X  X  X  X  X  X  ) shows considerable improvement compared to  X  X  X  X  +  X  X  X  X  X  X  X  X  X  X  X  X  X  .  X  X  X  X  +  X  X  X  X  X  X  X  X  X  X  X  X  X  assigns the same weights to adjacent term pairs while (  X  X  X  +  X  X  X  1 +  X  X  X  8 +  X  X  X  X  X  X  X  X  X  X  X  X  X  ) gives different weights based on the query. It means that certain types of dependency could prove superior for a given query. Thus, we expect further improvement by using a different proba-bility distribution for the alignment  X  (  X  X  X  X   X  ,  X  X  X  X   X   X   X  in Eq. 6.

On the other hand, the exact matching approach fails to show the potential to improve the effectiveness of a re-trieval model even though (  X  X  X  +  X  X  X  1+  X  X  X  8+  X  X  X  X  X  X  X  X  X  X  X  X  X  ) shows a significant improvement over  X  X  X  +  X  X  X  1 +  X  X  X  8. The sequential dependence model can take account of long-distance term dependencies on the document side by the unordered window factor  X  X  X  8 [16 ] and the exact matching approach considers long-distance term dependencies on the query side by extracting dependent terms having a parent-child, ancestor-descendent, siblings or c-commanding rela-tion. Because the exact matching approach does not con-sider the possibility of the transformation of dependency re-lations between queries and documents, the gap of MAP val-ues between  X  X  X  +  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  and  X  X  X  X  +  X  X  X  X  X  X  X  X  X  X  X  X  X  - X  X  X  X  X  X  is bigger than that of  X  X  X  +  X  X  X  X  X  X  X  X  X  X  X  X  X  and  X  X  X  X  +  X  X  X  X  X  X  X  X  X  X  X  X  X  . Only  X  X  X  +  X  X  X  1 +  X  X  X  8 +  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  achieves similar improvement to the quasi-synchronous model.
We also applied the quasi-synchronous model to a web collection, the Gov2 collection. For the Gov2 collection, it is impractical to parse all documents. We retrieve an initial document set (1,000 documents) using the sequential depen-dence model and then run experiments against this initial document set.

Table 5 shows the experimental results for the Gov2 col-lection. The performances of the interpolation strategies do not show as much improvement as the Robust 2004 collec-tion. Still,  X  X  X  X  +  X  X  X  X  X  X  X  X  X  X  X  X  X  and  X  X  X  +  X  X  X  1 +  X  X  X  8 +  X  X  X  X  X  X  X  X  X  X  X  X  X  interpolation strategies improve the effectiveness significantly. T able 6: Comparison of the MAP of the sequen-tial dependence model,  X  X  X  X  , and  X  X  X  X  +  X  X  X  X  X  X  X  X  X  X  X  X  X  . Statistics are collected from the experiments with the Robust 2004 collection. # queries is the num-ber of queries belong to each group and query length is the average length of queries.  X   X  X  X   X   X  X  X  +  X  X  X  1 +  X  X  X  8 1 63 1 7.12  X   X  X  X  &lt;  X  X  X  +  X  X  X  1 +  X  X  X  8 8 6 1 6.78  X   X  X  X   X   X  X  X  X  +  X  X  X  X  X  X  X  X  X  X  X  X  X  9 8 1 5.14  X   X  X  X  &lt;  X  X  X  X  +  X  X  X  X  X  X  X  X  X  X  X  X  X  1 51 1 8.21
Compared to the sequential dependence model, the quasi-synchronous model aims to capture long distance dependen-cies in queries. To test the impact of the quasi-synchronous model on long distance dependencies, we analyze queries for which this quasi-synchronous model shows better or worse results. Table 6 demonstrates comparison results between  X  X  X  X  ,  X  X  X  +  X  X  X  1 +  X  X  X  8 and  X  X  X  X  +  X  X  X  X  X  X  X  X  X  X  X  X  X  . The upper two rows are the comparison of the sequential de-pendence model with fixed and predicted weights. This re-sult demonstrates that the length of queries does not mat-ter for the sequential dependence model itself. On the other hand, the lower two rows are the comparison between the se-quential dependence model,  X  X  X  X  , and  X  X  X  X  +  X  X  X  X  X  X  X  X  X  X  X  X  X  . It shows that queries improved by the quasi-synchronous model tend to be longer than the other queries.

Based on this observation, we analyze the experimental results of the Robust 2004 collection according to the aver-age length of queries. Table 7 shows an experimental result in which we compared MAP of queries classified according to the length of queries. In this experiment, queries are split into three groups according their length. In the table, the interpolation strategies having the quasi-synchronous model demonstrate a clear tendency to larger improvements for longer queries while the strategy  X  X  X  +  X  X  X  1 +  X  X  X  8 does not. The longer a query is, the more long-distance term de-pendencies the quasi-synchronous model extracts from the query that are not considered by the sequential dependency model. These experimental results show that the quasi-synchronous model can help to capture long-distance term dependencies in not only documents but also queries.
We have proposed a novel term dependence model, the quasi-synchronous model, inspired by a quasi-synchronous stochastic process which constructs an inexact matching of syntactic relations between source and target sentences. As in query term expansion techniques that address lexical vari-ation between query and document, we aim to support syn-tactic divergence of term dependencies from documents to queries using an inexact matching approach. We general-ize these ideas from machine translation to the information retrieval task in which matching occurs between a sentence and an entire document. Experimental results show that the quasi-synchronous model can significantly improve effective-ness compared to a strong state-of-the-art retrieval model. Table 7: Experimental results with the Robust 2004 according to the length of queries. Length is the number of terms in a query and # queries is the number of queries belonging to each group. Num-bers in parentheses depict % improvement in each evaluation measure.
  X  X  X  +  X   X  1 +  X  X  X  8 0 .3056 0 .2380 0 .2232  X   X  X  X  +  X  X  X  X  X  X  X  X  X  X  X  X  X  0 .3255 0 .2668 0 .2738  X  X  X  +  X   X  1 +  X  X  X  8+ 0 .3251 0 .2649 0 .2698
Ea ch retrieval model, however, has its own strengths and weaknesses, which can differ query by query. A simpler re-trieval model may be superior to a more sophisticated model depending on the query. This is why most previous work using term dependencies has had problems showing consis-tent improvement. To address this issue, we used a machine learning approach to find an optimal parameter setting for a combination of retrieval models. By using a predicated optimal weight, we optimized the overall performance of the interpolation of several retrieval models. This interpolation technique, we found, is necessary for achieving the best re-sults with the quasi-synchronous model.
In this paper, we use a uniform distribution over align-ments between different syntactic relations in queries and documents. Intuitively, however, dependent terms are ex-pected to be used less frequently in a certain syntactic con-figurations and more frequently in others. For example, dependent terms in fixed phrases such as technical termi-nology, proper names, etc., will be used in the same way by both searchers and authors. Moreover, as shown in the experimental results, certain syntactic configurations could prove more important for evaluating the relevance of docu-ments. Thus, instead of a uniform distribution, employing a weighted alignment model could improve the effectiveness of the quasi-synchronous approach.
This work was supported in part by the Center for In-telligent Information Retrieval and in part by ARRA NSF IIS-9014442 . Any opinions, findings and conclusions or rec-ommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsor. [1] M. Bendersky and W. B. Croft. Discovering key [2 ] M. Bendersky, D. Metzler, and W. B. Croft. Learning [3] A. Berger and J. Lafferty. Information retrieval as [4] C.-C. Chang and C.-J. Lin. LIBSVM: a library for [5] W. B. Croft, H. Turtle, and D. Lewis. The use of [6] D. Das and N. A. Smith. Paraphrase identification as [7] J. Gao, J. Y. Nie, G. Wu, and G. Cao. Dependence [8] D. Gupta and N. Chatterjee. Study of divergence for [9] D. Klein and C. D. Manning. Accurate Unlexicalized [10] G. Kumaran and V. Carvalho. Reducing long queries [11] C. Lee, R. Chen, S. Kao, and P. Cheng. A term [12] C. Lee, G. G. Lee, and M.-G. Jang. Dependency [13] L. Maisonnasse, E. Gaussier, and J. P. Chevallet. [14] C. Manning and H. Sch  X  utze. Foundations of statistical [15] D. Metzler. Automatic feature selection in the markov [16] D. Metzler and B. W. Croft. A Markov random field [17] D. Metzler, T. Strohman, H. Turtle, and W. B. Croft. [18] J. H. Park and W. B. Croft. Query term ranking [19] J. M. Pont and W. B. Croft. A language modeling [20] S. M. Shieber and Y. Schabes. Synchronous [21] D. A. Smith and J. Eisner. Quasi-synchronous [22] Y. Song, K. Han, S. Kim, S. Park, and H. Rim. A [23] M. Srikanth and R. Srihari. Biterm language models [24] K. Toutanova, D. Klein, C. D. Manning, and [25] C. Van Rijsbergen. A theoretical basis for the use of [26] M. Wang, N. A. Smith, and T. Mitamura. What is the [27] J. Xu and W. B. Croft. Query expansion using local [28] X. Xue, S. Huston, and W. B. Croft. Improving [29] C. Zhai and J. Lafferty. Two-stage language models
