 Privacy-preserving data publication has been studied intensely in the past years. To date, all existing approaches transform data val-ues by random perturbation or generalization. In this paper, we in-troduce a radically different data anonymization methodology. Our proposal aims to maintain a certain amount of patterns , defined in terms of a set of properties of interest that hold for the original data. Such properties are represented as linear relationships among data points. We present an algorithm that generates a set of anonymi-zed data that strictly preserves these properties, thus maintaining specified patterns in the data. Extensive experiments with real and synthetic data show that our algorithm is efficient, and produces anonymized data that affords high utility in several data analysis tasks while safeguarding privacy.
 H.2.7 [ Database Management ]: Database Administration X  Se-curity, integrity, and protection ; H.2.8 [ Database Management ]: Database Applications X  Data Mining ; K.4.1 [ Computers and So-ciety ]: Public Policy Issues X  Privacy Algorithms, Security, Experimentation
Organizations often possess data that need to be made public for the common good. Yet such data may contain sensitive personal information. Several methodologies for data anonymization have been proposed, with the aim of protecting the privacy of persons involved while maintaining as much of the utility of the published data as possible. Existing approaches for data anonymization trans-form the data by either generalizing or perturbing data values. Gen-eralization-based approaches [10, 9, 2] group records into equiva-lence classes (ECs), and render the records within the same EC indistinguishable by generalizing their values on some pre-selected quasi-identifying attributes (QIs) to the same range(s).  X  Table 1 shows a sample of medical microdata records. Age and Weight are quasi-identifying attributes [12]; knowledge of those attributes X  values allows an adversary to re-identify the person in-volved. Disease is a sensitive attribute ; it contains information that entails a privacy risk for the persons concerned. Figure 1(a) visu-alizes these microdata in the two-dimensional space formed by the two quasi-identifiers [5], Age  X  Weight .An anonymization of these data by the generalization-based k -anonymity model with k could form two ECs out of them, one containing records { 1 and one out of { 4 , 5 , 6 } . Figure 1(b) presents these two ECs as rectangular regions in the two-dimensional QI-space. All records within the same generalized region become indistinguishable as far as their QIs are concerned. Following the paradigm of random per-turbation [1], an attribute value is modified by adding to the original value a random variable uniformly or normally distributed in a pre-defined interval [  X   X , +  X  ] . This perturbation effectively contains an adversary X  X  capacity to re-identify the record of a specific per-son. In the running example, Figure 1(c) presents an example of how the published data may look after random perturbation.
A careful examination of Figures 1(b) and (c) reveals that, though the anonymized data obtained from generalization and perturbation achieves privacy objectives, important topological relationships be-tween the data points are lost. In generalization, topological in-formation is completely obscured within the generalized regions; in perturbation, topological information is destroyed due to blind modification. However, topological information is essential in data mining tasks such as ranking, clustering or skyline queries. Hence, the data obtained from generalization or perturbation is not suitable for these tasks. Popular generalization-based approaches [12, 10, 8, 9, 2] and random perturbation-based approaches [1, 4, 11, 3, 14, 6] all inherit such shortcomings.

In this paper, we propose a novel data anonymization paradigm that addresses the above drawbacks. Given a microdata table define a process that specifies certain properties of interest (PoIs) among the QI attributes of T . The set of PoIs describes the data characteristics that the anonymization should maintain. Each PoI is expressed as a linear relationship between a subset of QI attribute values. We develop a scheme to obtain an anonymized table that satisfies all defined PoIs. T A is nearly randomly and uniformly sampled the space for all possible data that satisfy the PoIs, so as to afford privacy for the original data. With our approach, we get a result as in Figure1(d) for the sample medical data. Notably, the data pattern in (d) preserves the original pattern in (a) much more faithfully than that in (b) and (c). Still, the data values in (d) are modified in a way that preserves privacy, and they do not appear exactly the same as the original data.
Let T be a table with n data records and m QI attributes. En-try t i,j refers to the data value in the i th row and j th T . We first focus on describing the scheme for anonymzing 1D data, and later generalize it to work for a table with multiple QIs. Let D = { d 1 ,...,d n } be a particular 1D data vector (i.e., a QI col-umn of T ) that is subject to anonymization and X = { x 1 be the corresponding set of variables that express the anonymized form of D . Then we define a property of interest as follows.
D EFINITION 1. A property of interest (PoI) on data vector D is a linear relationship of the form d i  X  D i c i d i  X   X  among values in D , where c i is the coefficient of d i and  X   X  R + is a user-defined constant.

Our scheme operates in two phases, the properties extraction phase and the value substitution phase , to find a set of new data D A = { d A 1 ,...,d A n } for the variables in X that satisfy
Our overall proposal does not constraint the types of linear re-lationships that may be defined as PoIs. It is up to the data ven-dor and legitimate data recipients to decide what form of PoIs are important. However, in order to make our proposal concrete and illustrate the properties extraction phase , we introduce a particular type of PoIs that we use in the rest of this paper, locality .
D EFINITION 2(L OCALITY ). The locality of data values d i and d k with respect to data value d j , denoted as loc d j a linear relationship of the form | d i  X  d j || d j  X  d k  X  X  X  ,  X } makes the relationship true.

Locality captures relative distance information of two data values with respect to a third data value. The distance between d denoted as d i,j . Without loss of generality, we assume that d A locality property is most informative when d j lies between d and d k , i.e. d i  X  d j  X  d k . Otherwise, it suffices to know whether d &lt;d i or d j &gt;d k to deduce the property that holds, independently of the value of d j .
Without loss of generality, we assume that D is sorted in non-decreasing order. At first glance, each combination of i , j and k can form a locality ; thus, the total number of locality properties is 3 . A naive locality extraction algorithm would have to enumerate all possible combinations of i, j, k in O ( n 3 ) . Still, some of the localities generated by such a process would be redundant .For example, from d 1 , 3  X  d 3 , 4 we can infer d 1 , 3  X  d 3 the following two rules for pruning redundant localities:
Based on the two pruning rules, we propose an algorithm that generates a complete set of localities , P locs , whose size is O We use the following simple example to illustrate the rationale be-hind the algorithm. Figure 2 shows a set of data values D { d 1 ,...,d 6 } from which localities are to be extracted. Suppose we wish to retrieve localities for i =2 and j =3 . A naive approach would try all possible values of k&gt;j (i.e., k =4 , 5 or However, we can avoid this enumeration thanks to a simple geo-metrical observation: A circle centered at d j with radius d intersecting the D axis at breakpoint  X  , implies that for all k val-ues such that d k  X   X  ,itis d j  X  d i  X  d k  X  d j . Similarly, for all k values such that d k  X   X  , d j  X  d i  X  d k  X  d j . Let d k value in D less than  X  and d k + be the smallest value in D that is greater than  X  . In our example, d k  X  is d 4 and d k + is d 2). It then suffices to derive the localities d j  X  d i  X  d  X  d i  X  d Algorithm 1 shows a pseudocode for our locality extraction algo-rithm. Arguably, a small subset of localities P X  X  locs can provide a balance between utility and privacy, where P can be creating by sampling from P loc , or via other selection algorithms. Algorithm 1 : Locality Extraction Algorithm
In this section, we tackle the second step of our anonymization scheme, i.e., the value substitution phase. The problem we face is to find a set of values for the variables in X so that all the con-straints in Q are satisfied. In addition, we also aim to find a solution in which the correlation between the anonymized and the original data is weak. The value substitution algorithm should ideally give a solution randomly and uniformly sampled from the solution space.
To achieve our goal, we propose a Random Walk algorithm. In this algorithm, D and X are viewed as vectors in a n -dimensional space IR n , i.e. D =( d 1 d 2 ... d n ) T and X =( x 1 x In addition, we introduce a set of constraints H that defines the domain values of the attributes: H :  X  min  X  X T  X   X  max .For instance, we can use H to ensure Age values are bounded between 1 and 120 . Trivially, the set of all constraints on X ,  X  defines a bounded polyhedron S in IR n . Any point in S is a feasible assignment to X that satisfies the constraints  X  Q . Thus, our problem is to randomly select a point in S . Our algorithm exploits the fact that D is an already known solution in S ; it initiates a random walk from D and arrives at another internal point within S . We ensure that the random walk always stays within the bounds of S ; thus, the arrival point corresponds to an acceptable value assignment to all the variables in X . To minimize the correlation of the destination point to the original data D , the Random Walk algorithm operates in an iterative manner. As the number of iterations increases, the probability distribution of the location of the final destination tends to be uniform [13].
 We now elaborate on the details of the random walk within Each random walk iteration is characterized by two parameters: its direction,  X  X , and the length of walk in the direction,  X  . In the following we describe the derivation of  X  X and  X  :
In the following, we derive the maximum walking length l .
To calculate l , we convert the linear inequalities in Q to a set of linear equalities by adding a non-negative slack variable v left-hand side of each inequality:
Let V be the set of all slack variables, i.e. V = { v 1 ,...,v ( X,V ) represents the vector for all variables in the system.
Similar to the walking direction  X  X for the variables in X ,we can also introduce a direction vector  X  V =( X  v 1 ...  X  v the slack variables V .  X  V . Then ( X  X,  X  V ) is the direction vector in a particular random walk. In the following, we try to express each  X  v i in terms of  X  X . As the destination of the random walk is within S , the following equation holds after the walk:
From Equations 1 and 2, we derive: The above equality can be rewritten to express  X  v i as:
Since V T  X  0 is always required and the values in X should always be in their predefined ranges, the following system of in-equalities is formed:
Let [  X  min , X  max ] be the interval that defines the feasible range of  X  in the above system; then l =max { 0 , X  max } .
The anonymization scheme we have developed applies to 1D data vectors. To anonymize a table, we anonymize each QI attribute column independently. Moreover, instead of treating a single col-umn as a single 1D data set, we partition it to segments, and treat each segment independently. We emphasize that this approach does not contribute to the anonymization itself; it is only a mechanism to assist in defining PoIs and processing the data efficiently.
We now conduct experiments of our anonymization scheme, us-ing both real and synthetic data. Our first data set is a sample of the IPUMS USA census data 1 for the year 2008. It consists of 75K data records; we extract Age , Birth place , and Occupation as QIs. Our second data set is a synthetic one created by the randdataset tool We create a table with 3 columns and 10K rows, where the columns are independently selected from [0 , 1] , treated as normalized values of the same four attributes as in the IPUMS data. To compare with -diversification schemes, we employ the Adult dataset 3 ;weex-tract its first 30K tuples, and treat attributes Age , Final weight and Education years as QIs, and Occupation as the SA . All algorithms were developed in Java, and experiments ran on a 3GHz CPU, 2GB RAM machine running Windows XP.
We start by measuring the runtime of our algorithm. We increase the partition size from 100 to 900 , and measure the average time for extracting all the localities for a partition with Algorithm 1. Fig-ure 3(a) plots our results for both the census and synthetic data. As expected, time grows quadratically in partition size. Still, locality extraction runs in reasonable time even for large partition sizes.
In our next experiment, we fix the partition size to be 100 property extraction and randomly sample a number out of the set of all localities produced in a partition. We denote the percentage of sampled PoIs as  X  . Then, we run our value substitution algorithm with the chosen set of localities as constraints, taking 4000 walk iterations. Figure 3(b) shows our results. The runtime grows linearly in the number of PoIs, as our analysis in Section 4 predicts.
In this experiment, we evaluate our method by conducting a pop-ular data mining operation, k -means clustering, over the anonymi-zed data set. We produce anonymized forms T A of the same orig-inal data table T using our approach, and a random perturbation-based scheme [1], ensuring that both of them effect the same amount of distortion on the data. The distortion is assessed as follows:
This metric measures the average relative error in each entry of the anonymized data with respect to the original data. We first set the size of randomly sampled PoIs  X  for our scheme and measure the distortion Dst it incurs; then, we tune the perturbation interval in [1] so that it effects the same (or less) distortion Dst , allowing for a small divergence . Both compared schemes maintain ex-act data values, hence their results are amenable to clustering. As grounds of assessment we use the following clustering error metric: where C i ( T ) and C i ( T A ) are the sets of data records in the i th cluster based on the original data T and the anonymized data T A , respectively. The clustering error measures the percentage of data records that fail to be grouped in the correct cluster. We mea-sure this error as a function of the size of sampled PoIs  X  for our scheme, on which the amount of effected distortion for both meth-ods depends. The partition size is 100 , and the random walk makes 4000 iterations. The results are shown in Figure 4, for 3 different k values in k -means clustering, for both the census (c) and synthetic (s) data. Our scheme consistently outperforms the one based on random perturbation.
Next, we study the suitability of using the anonymized data gen-erated with our approach for answering aggregate queries using the Adult dataset. We compare the results derived with our scheme against the generalization-based Mondrian algorithm for -diversity [7]. We design three types of aggregate queries:
For each query, the parameters  X  age ,  X  fw and  X  edu take the val-ues that are randomly chosen from their attribute domains. The set { o 1 ,...,o b } is a random subset of all possible occupation values of random size b  X  [1 , 14] . Each query asks for the average value of one QI attribute based on predicates on other attributes.
We first anonymize the Adult dataset using generalization with =4 , 6 , 8 , 10 and 12 . We measure the relative errors obtained with generalization with respect to the distortion (Equation 6). To measure the distortion of generalized data, we select the mean value of each attribute within the EC as its representative value. Hence, for each version of anonymized dataset T under a particular value of , we can compute a distortion value Dst . Then, for each Dst value, we tune the amount of PoIs  X  used in our approach, until we arrive at an anonymized data set having the same or just a bit more distortion than Dst . The partition size is 20 , and the number of random walking iterations is 40 , 000 . Next, we create 2,000 instances for each of the three queries, execute them over these data, and average the accuracy results for each query type. When estimating the answers to range predicates with -diversified data, we assume that QI values are uniformly distributed within their ECs, and calculate the estimates accordingly. The accuracy of a query answer is assessed by the relative error |  X   X   X  A | (  X  A ) is the answer based on the original (anonymized) data.
Our results are shown in Figure 5. Remarkably, for all three queries, the results with our scheme are more accurate than those by -diversification, even though both incur the same distortion. This result shows that our method preserves more utility than Mondrian.
This paper has proposed a simple, yet effective, methodology for data anonymization, which allows a data owner to publish exact , instead of generalized, values, yet also preserves patterns among the data. Our scheme extracts a set of properties of interest as linear inequalities, which the anonymized data, generated by a random walk process, preserve. As opposed to traditional privacy-driven approaches, our approach is utility-driven . Our experiments verify that data anonymized by our approach allows for better or similar performance in data analysis tasks compared to data undergoing the same distortion under other anonymization methods.
