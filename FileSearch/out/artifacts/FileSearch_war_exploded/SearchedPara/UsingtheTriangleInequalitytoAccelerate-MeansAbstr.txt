 Charles Elkan ELKAN @ CS . UCSD . EDU Department of Computer Science and Engineering Uni versity of California, San Die go La Jolla, California 92093-0114 The most common method for finding clusters in data used in applications is the algorithm kno wn as -means. -means is considered a fast method because it is not based on computing the distances between all pairs of data points. Ho we ver, the algorithm is still slo w in practice for lar ge datasets. The number of distance computations is where is the number of data points, is the number of clusters to be found, and is the number of iterations re-quired. Empirically , gro ws sublinearly with , , and the dimensionality of the data.
 The main contrib ution of this paper is an optimized version of the standard -means method, with which the number of distance computations is in practice closer to than to The optimized algorithm is based on the fact that most dis-tance calculations in standard -means are redundant. If a point is far away from a center , it is not necessary to cal-culate the exact distance between the point and the center in order to kno w that the point should not be assigned to this center . Con versely , if a point is much closer to one center than to any other , calculating exact distances is not necessary to kno w that the point should be assigned to the first center . We sho w belo w how to mak e these intuitions concrete.
 We want the accelerated -means algorithm to be usable where ver the standard algorithm is used. Therefore, we need the accelerated algorithm to satisfy three properties. First, it should be able to start with any initial centers, so that all existing initialization methods can continue to be used. Second, given the same initial centers, it should al-ways produce exactly the same final centers as the standard algorithm. Third, it should be able to use any black-box distance metric, so it should not rely for example on opti-mizations specific to Euclidean distance.
 Our algorithm in fact satisfies a condition stronger than the second one abo ve: after each iteration, it produces the same set of center locations as the standard -means method. This stronger property means that heuristics for mer ging or splitting centers (and for dealing with empty clusters) can be used together with the new algorithm. The third condi-tion is important because man y applications use a domain-specific distance metric. For example, clustering to identify duplicate alphanumeric records is sometimes based on al-phanumeric edit distance (Monge &amp; Elkan, 1996), while clustering of protein structures is often based on an expen-sive distance function that first rotates and translates struc-tures to superimpose them. Ev en without a domain-specific metric, recent work sho ws that using a non-traditional norm with is beneficial when clustering in a high-dimensional space (Aggarw al et al., 2001). This paper is organized as follo ws. Section 2 explains how to use the triangle inequality to avoid redundant distance calculations. Then Section 3 presents the new algorithm, and Section 4 discusses experimental results on six datasets of dimensionality 2 to 1000. Section 5 outlines possible im-pro vements to the method, while Section 6 revie ws related work, and Section 7 explains three open research issues. Our approach to accelerating -means is based on the tri-angle inequality: for any three points , , and , all distance metrics possess.
 The dif ficulty is that the triangle inequality gives upper bounds, but we need lower bounds to avoid calculations. Let be a point and let and be centers; we need to kno w that the actual value of The follo wing two lemmas sho w how to use the triangle inequality to obtain useful lower bounds.
 Lemma 1: Let be a point and let and be centers. If Pr oof: We kno w that Lemma 2: Let be a point and let and be centers.
 Then Pr oof: We kno w that Note that Lemmas 1 and 2 are true for any three points, not just for a point and two centers, and the statement of Lemma 2 can be strengthened in various ways.
 We use Lemma 1 as follo ws. Let be any data point, let be the center to which is currently assigned, and let be any other center . The lemma says that if essary to calculate Suppose that we do not kno w kno w an upper bound need to compute If then the point must remain assigned to the center , and all distance calculations for can be avoided.
 Lemma 2 is applied as follo ws. Let be any data point, let be any center , and let same center . (That is, suppose the centers are numbered 1 through , and is center number ber iteration we kne w a lower bound Then we can infer a lower bound Informally , if distance between and the mo ved only a small distance, then tion to the updated distance.
 The algorithm belo w is the first -means variant that uses lower bounds, as far as we kno w. It is also the first al-gorithm that carries over varying information from one -means iteration to the next. According to the authors of (Kanungo et al., 2000):  X  X he most obvious source of in-efficienc y in [our] algorithm is that it passes no informa-tion from one stage to the next. Presumably in the later stages of Llo yd X  s algorithm, as the centers are con verging to their final positions, one would expect that the vast ma-jority of the data points have the same closest center from one stage to the next. A good algorithm would exploit this coherence to impro ve running time.  X  The algorithm in this paper achie ves this goal. One pre vious algorithm also re-uses information from one -means iteration in the next, but that method, due to (Judd et al., 1998), does not carry over lower or upper bounds.
 Suppose tance between and the center to which is currently assigned, and suppose on the distance between and some other center so it is necessary to calculate neither Note that it will never be necessary in this iteration of the accelerated method to compute essary to compute center Putting the observ ations abo ve together , the accelerated -means algorithm is as follo ws.
 First, pick initial centers. Set the lower bound for each point and center . Assign each to its closest initial center avoid redundant distance calculations. Each time is computed, set Ne xt, repeat until con vergence: 1. For all centers and 2. Identify all points such that 3. For all remaining points and centers such that 4. For each center , let 5. For each point and center , assign 6. For each point , assign 7. Replace each center by In step (3), each time its lower bound is updated by assigning Similarly , then essary . Step (3b) repeats the checks from (ii) and (iii) in order to avoid computing The fundamental reason why the algorithm abo ve is effec-tive in reducing the number of distance calculations is that at the start of each iteration, the upper bounds lower bounds . If these bounds are tight at the start of one iteration, the updated bounds tend to be tight at the start of the next it-eration, because the location of most centers changes only slightly , and hence the bounds change only slightly . The initialization step of the algorithm assigns each point to its closest center immediately . This requires relati vely man y distance calculations, but it leads to exact upper bounds man y start with each point arbitrarily assigned to one center . The initial values of calculated to this center only . With this approach, the ini-tial number of distance calculations is only , but lations are required later . (After each iteration each point is always assigned correctly to its closest center , regardless of how inaccurate the lower and upper bounds are at the start of the iteration.) Informal experiments suggest that both initialization methods lead to about the same total number of distance calculations.
 Logically , step (2) is redundant because its effect is achie ved by condition (iii). Computationally , step (2) is beneficial because if it eliminates a point from further consideration, then comparing separately is not necessary . Condition (iii) inside step (3) is beneficial despite step (2), because change during the execution of step (3).
 We have implemented the algorithm abo ve in Matlab . When step (3) is implemented with nested loops, the outer loop can be over or over . For efficienc y in Matlab and similar languages, the outer loop should be over since vectorized code that operates on all rele vant collecti vely . Step 4 computes the new location of each cluster center . Setting appropriate when the distance metric in use is Euclidean distance. Otherwise, example, with -medians the new center of each cluster is a representati ve member of the cluster . This section reports the results of running the new al-gorithm on six lar ge datasets, five of which are high-dimensional. The datasets are described in Table 1, while Table 2 gives the results.
 Our experimental design is similar to the design of (Moore, 2000), which is the best recent paper on speeding up the -means algorithm for high-dimensional data. Ho we ver, there is only one dataset used in (Moore, 2000) for which the raw data are available and enough information is given to allo w the dataset to be reconstructed. This dataset is called  X  X o vtype.  X  Therefore, we also use five other publicly available datasets. None of the datasets have missing data. In order to mak e our results easier to reproduce, we use a fix ed initialization for each dataset . The first center is initialized to be the mean of . Subsequent centers are initialized according to the  X  X urthest first X  heuristic: each new center is set of initial centers chosen so far (Dasgupta, 2002). Follo wing the practice of past research, we measure the performance of an algorithm on a dataset as the number of distance calculations required. All algorithms that ac-celerate -means incur overhead to create and update aux-iliary data structures. This means that speedup compared to -means is always less in clock time than in number of distance calculations. Our algorithm reduces the number of distance calculations so dramatically that its overhead time is often greater than the time spent on distance calculations. Ho we ver, the total execution time is always much less than the time required by standard -means. The overhead of the implementation than with the Matlab implementation used for the experiments reported here. For this reason, clock times are not reported.
 Perhaps the most striking observ ation to be made from Ta-ble 2 is that the relati ve adv antage of the new method in-creases with . The number of distance calculations gro ws only slo wly with and with (the number of passes over the data, called  X  X terations X  in Table 2). So much redundant computation is eliminated that the total number of distance calculations is closer to than to as for standard -means.
 A related observ ation is that for we obtain a much better speedup than with the anchors method (Moore, 2000). The speedups reported by Moore for the  X  X o vtype X  dataset are 24.8, 11.3, and 19.0 respecti vely for cluster -ing with 3, 20, and 100 centers. The speedups we obtain are 8.60, 107, and 310. We conjecture that the impro ved speedup for arises in part from using the actual cluster centers as adapti ve  X  X nchors,  X  instead of using a set of anchors fix ed in preprocessing. The worse speedup for remains to be explained.
 Another striking observ ation is that the new method re-mains effecti ve even for data with very high dimension-ality . Moore writes  X  X f there is no underlying structure in the data (e.g. if it is uniformly distrib uted) there will be lit-tle or no acceleration in high dimensions no matter what we do. This gloomy vie w, supported by recent theoreti-cal work in computational geometry (Indyk et al., 1999), means that we can only accelerate datasets that have in-teresting internal structure.  X  While this claim is almost certainly true asymptotically as the dimension of a dataset tends to infinity , our results on the  X  X andom X  dataset sug-gest that worthwhile speedup can still be obtained up to at least 1000 dimensions. As expected, the more clustered a dataset is, the greater the speedup obtained. Random pro-jection mak es clusters more Gaussian (Dasgupta, 2000), so speedup is better for the  X  X nist50 X  dataset than for the  X  X nist784 X  dataset. During each iteration of the algorithm proposed here, the lower bounds . These updates tak e of the algorithm remains at least number of distance calculations is roughly may be possible to avoid updating man y lower bounds in most iterations, and hence to reduce the nominal comple x-ity of the algorithm. Note that if a point is eliminated from further consideration in step (2), then used at all.
 In some clustering applications, . This is the case in particular for vector quantization for image compres-sion. For these applications the memory required to store the lower bounds Ho we ver, the entire matrix in main memory . If the data are streamed into memory at each iteration from disk, then the streamed into memory in synchron y.
 Moreo ver, the algorithm remains beneficial even if lower bounds are not used, so condition (ii) becomes When the algorithm is used with a distance function that is fast to evaluate, such as an norm, then in practice the time comple xity of the algorithm is dominated by the bookk eeping used to avoid distance calculations. There-fore, future work should focus on reducing this overhead. The point abo ve is especially true because a Euclidean dis-tance (or other distance) in dimensions can often be compared to a kno wn minimum distance in simple idea is to stop evaluating the new squared distance when the sum of squares so far is greater than the kno wn squared minimum distance. (This suggestion is usually as-cribed to (Bei &amp; Gray , 1985), but in fact it is first men-tioned in (Cheng et al., 1984).) Distance calculations can be stopped even quick er if axes of high variation are con-sidered first. Ax es of maximum variation may be found by principal component analysis (PCA) (McNames, 2000), but the preprocessing cost of PCA may be prohibiti ve. At the end of each iteration, centers must be recomputed. Computing means tak es This can be reduced to number of points assigned to a dif ferent center during the iteration. Typically in all except the first few itera-tions. As mentioned abo ve, the algorithm of this paper can also be used when centers are not recomputed as means. During each iteration, distances between all centers must be recomputed, so the minimum number of distance com-putations per iteration is vector quantization, this may be a dominant expense. Fu-ture research should investigate the best way to reduce this cost by computing approximations for inter -center dis-tances that are lar ge. Man y papers have been published on the topic of acceler -ating the -means algorithm, in several dif ferent research communities. Some of the most important of these papers are described briefly in this section. Most of the papers cited belo w only cite pre vious papers from the same re-search community , so one of the contrib utions of this paper is an attempt to collect references that otherwise cannot be found in one place. All the rele vant papers that we kno w of can be found by follo wing chains of citations from the papers mentioned here.
 A version of the -means algorithm was first published by (MacQueen, 1965). The history of dif ferent variants of the algorithm is discussed by (Faber , 1994). The basic algo-rithm used most commonly today , and used in this paper , where centers are recomputed once after each pass through the data, is usually attrib uted to a paper written by Llo yd in 1957 but not published until 1982 (Llo yd, 1982). Ho we ver, that paper only discusses quantization (i.e. clustering) for some special one-dimensional cases.
 The central operation in the -means algorithm is to find the nearest center for each data point. At least three gen-eral approaches have been developed for accelerating this operation.
 One general approach is based on locality-sensiti ve hash-ing (Indyk &amp; Motw ani, 1998), but these methods are not well-suited for finding exact nearest neighbors. A second general approach organizes points into trees where nearby points are in the same subtree. Approaches using -trees or similar have been proposed independently by several au-thors (Ramasubramanian &amp; Paliw al, 1990; Deng &amp; Moore, 1993; Pelle g &amp; Moore, 1999; Alsabti et al., 1998; Kanungo et al., 2000), but these methods are not effecti ve for about. By using metric trees Moore X  s  X  X nchors X  method is effecti ve for much lar ger (Moore, 2000).
 The third general approach to the nearest neighbor task is to use triangle inequalities to eliminate unnecessary distance calculations. Using Lemma 1 abo ve appears to have been proposed first by (Hodgson, 1988), then again indepen-dently by (Orchard, 1991; Montolio et al., 1992; Phillips, 2002) among others. Our application of Lemma 1 is more fine-grained than pre vious applications. The lemma says that if algorithm of (Hodgson, 1988) only considers the center that is closest to . If remains assigned to . Otherwise, no distance calcula-tions are eliminated. Our algorithm applies the lemma for every center dif ferent from , so for most some distance calculations are avoided, even if others must be performed. Variants of Lemma 2 have been used by man y authors, starting with (Burkhard &amp; Keller , 1973; Vidal, 1986), but using the lemma to update a lower bound on the distance between mo ving points appears to be novel.
 The triangle inequality applies to all distance metrics. Man y papers have also been published on speeding up -means or nearest neighbor search using inequalities that are specific for Euclidean distance, for example (W u &amp; Lin, 2000; Mielikainen, 2002).
 Man y papers have been published on on approximating -means quickly; well-kno wn papers include (Zhang et al., 1996; Farnstrom et al., 2000). Ho we ver, the exact algo-rithm presented here is so fast that it is not clear when an approximate algorithm is necessary . A basic open theoretical question is whether one can find a lower bound on how man y distance calculations are needed by any implementation of exact -means. Can one con-struct an adv ersary argument sho wing that if any algo-rithm omits certain distance computations, then an oppo-nent can choose values for these distances that, together with all other distances, satisfy the triangle inequality , yet also mak e the output of the algorithm incorrect? Perhaps the most fundamental practical question for future work is how to find better clusterings, i.e. better local op-tima. No w that we can run -means fast, how can we use additional computation to get answers of better quality? One common approach to finding better local optima is to run -means with man y dif ferent initializations. The al-gorithm abo ve allo ws man y more initializations to be tried in the same total time. Another widespread heuristic for finding better clusterings is to run -means with a lar ge value for , and then to mer ge or prune the clusters ob-tained into a good clustering with smaller . Since our al-gorithm mak es the running time of -means sublinear in , it is especially useful for this approach.
 A third important open question is how to accelerate clus-tering methods that use soft assignment of points to cen-ters. Two important methods in this class are Gaussian expectation-maximization (EM) (Dempster et al., 1977) and harmonic -means (Hamerly &amp; Elkan, 2002). In these methods each center is recomputed as the weighted aver-age of all points, where weights are related to distances. Can triangle inequalities (or other inequalities!) be applied to obtain upper bounds on weights that are close to zero, and hence to obtain approximate soft assignment solutions quickly? Ackno wledgments Thanks to Sanjo y Dasgupta, Ari Frank, Gre g Hamerly , Doug Turnb ull, and others for pro viding useful comments and datasets.

