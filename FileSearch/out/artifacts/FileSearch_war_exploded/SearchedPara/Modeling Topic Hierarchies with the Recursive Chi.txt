 Topic models such as latent Dirichlet allocation (LDA) and hierarchical Dirichlet processes (HDP) are simple solutions to discover topics from a set of unannotated documents. While they are simple and popular, a major shortcoming of LDA and HDP is that they do not organize the top-ics into a hierarchical structure which is naturally found in many datasets. We introduce the recursive Chinese restau-rant process (rCRP) and a nonparametric topic model with rCRP as a prior for discovering a hierarchical topic structure with unbounded depth and width. Unlike previous models for discovering topic hierarchies, rCRP allows the documents to be generated from a mixture over the entire set of topics in the hierarchy. We apply rCRP to a corpus of New York Times articles, a dataset of MovieLens ratings, and a set of Wikipedia articles and show the discovered topic hierarchies. We compare the predictive power of rCRP with LDA, HDP, and nested Chinese restaurant process (nCRP) using held-out likelihood to show that rCRP outperforms the others. We suggest two metrics that quantify the characteristics of a topic hierarchy to compare the discovered topic hierarchies of rCRP and nCRP. The results show that rCRP discovers a hierarchy in which the topics become more specialized to-ward the leaves, and topics in the immediate family exhibit more affinity than topics beyond the immediate family. G.3 [ Probability and Statistics ]: Nonparametric Statis-tics; H.3.3 [ Information Storage and Retrieval ]: Infor-mation Search and Retrieval X  clustering ; I.2.6 [ Artificial Intelligence ]: Learning Hierarchical Topic Modeling, Bayesian Nonparametric mod-els
Probabilistic topic models [5, 20] are important tools for discovering the latent semantic patterns in various data in-cluding text [11, 13], users [12, 24] and movie ratings [18]. A major limitation of these basic topic models and many of their extensions is that they discover topics in flat structures without organizing them into groups or hierarchies. This is a significant limitation because in many domains, topics can be naturally organized into hierarchies where the root topic of each hierarchy is the most general topic, and the topics become more specific toward the leaves. Consider, for exam-ple, the domain of movies, where there are genres (e.g., ac-tion ) and sub-genres (e.g., martial arts ) 1 . One branch of the tree may have the genre action as a topic, and children top-ics martial arts and James Bond series , and another branch may have the genre comedy and as children slapstick and black comedy . A user with preferences, then, should be al-lowed to be associated with both the action topic to indicate that his preferences span a wide variety of action movies, as well as the black comedy sub-genre to indicate that his pref-erences for comedy are limited to that sub-genre.

There have been previously proposed topic models that look at correlations among the topics [4] and hierarchical topic structure [2, 16], but these models do not fully ex-hibit the following three characteristics of an intuitive and flexible topic structure. First, the number of topics should be unbounded, and an optimal number should be automat-ically determined by the model. Second, topics should be structured in a hierarchy of unbounded depth from general to specific, and similar topics should form groups within the hierarchical structure. Third, a document should be com-posed of multiple topics from anywhere in the hierarchy of topics, not just a single topic, the topics of a single path, or the topics at the bottom of the hierarchy. Detailed com-parisons with other related models will be presented in the next section.

We propose a novel prior, recursive Chinese restaurant process (rCRP), and a hierarchical topic model with rCRP as a prior that can handle such flexible hierarchical topic modeling. The topic hierarchies found by rCRP are con-sistent with the general intuition that the topics start out quite general at the root level and become more specialized toward the leaves. Additionally, the topics within the im-mediate family (i.e., a parent topic and its direct children) are much more similar than the topics outside the family boundary. This characteristic of the topic hierarchy is more natural and fitting for many domains where each data point http://visual.ly/complete-list-film-sub-genres Figure 1: In PAM [14], a document is modeled as a distribution over the topics at the leaves of the topic hierarchy. In nested CRP [2], a document is modeled as a distribution over a single path from the root to the leaf node. In TS-SB [1], a document is modeled by a single node of the tree. In rCRP, a document has a distribution over all of the nodes of the hierarchy. is best explained by a variety of topics, general to specific and placed anywhere within the hierarchy. In addition to the flexibility of the model, rCRP outperforms LDA, HDP, and nCRP [2] on the predictive metric of heldout likelihood.
The rest of this paper is organized as follows. In Section 2, we discuss existing hierarchical topic models and how they differ from rCRP. In Section 3, we describe our model with the novel nonparametric prior, the recursive Chinese restau-rant process. In Section 4, we present a Markov chain Monte Carlo inference algorithm for approximating the posterior probability. In Section 5, we describe the three datasets used for experiments and visualize the topic hierarchies found for those datasets. We also compare our model against LDA, HDP, and nCRP on heldout likelihood. In Section 6, we propose two new metrics for quantifying the characteristics of a topic hierarchy and show the results of our model and nCRP. In Section 7, we conclude the paper with discussions and future directions.
Two classes of previously proposed models address hier-archical structures: ones that cluster each of the documents into the nodes of the hierarchy [9, 17, 23], and ones that place each of the topics into the nodes of the hierarchy [2, 14, 16]. Within the latter class of models, none are flexi-ble enough to accommodate the intuition that a document exhibits multiple topics, and those topics can come from anywhere in the hierarchy of topics, from the general root-level topic down to the most specific leaf node topic, and along any path of the tree.

The different assumptions of the related models, pachinko allocation model (PAM) [14, 16], nested Chinese restaurant process (nCRP) [2], tree-structured stick-breaking process (TS-SB) [1], as well as our rCRP model are illustrated in Figure 1. The figure shows the topic assignments for four fictitious documents highlighting the different assumptions of the four models. Each of the rows is a document, mod-eled by each of the four models in the columns. The differ-ent sized thick circles represent proportions of the document generated by a topic represented by that node. PAM [14] is a generalization of the LDA that enables learning of topics in a form of a directed acyclic graph. In [16], the model is further extended to explicitly identify the word distribution of super-topics and sub-topics. PAM and its extension as-sume that the documents are generated by only the leaf node topics. The nCRP [2] extends the original CRP represen-tation and constructs a tree-structured hierarchy of topics. This model assumes that a topic is represented by a path from the root to a particular leaf node, and each document is generated by a single path. TS-SB [1] can be used to discover a hierarchy of mixture components with each data point belonging to a component. This model assumes that each document is generated by only a single node which has its unique topic distribution. Our proposed model with recursive CRP (rCRP) as a nonparametric prior enables a document to have a distribution over the entire topic tree.
Chinese Restaurant Process (CRP) is a stochastic pro-cess that generates an exchangeable partition of data points. Due to its flexibility and extendability, CRP is widely used in nonparametric topic models. One of the most widely used model is hierarchical Dirichlet process mixture model [20]. It combines two levels of CRP to construct a mixture model of grouped data. The second level CRP partitions data points into homogeneous groups, while the first level CRP associates each group with a mixture component. Our model also employs two levels of partitioning process. However, we discover the hierarchical structure of the mix-ture components by replacing the first level CRP with a new stochastic process called recursive Chinese Restaurant Pro-cess (rCPR). In rCRP, mixture components are organized in an infinite tree, and each group of data points can be as-sociated with any node in the tree. As in [20], second level CRP is used to partition data points into groups.

With this setting, we propose a nonparametric Bayesian model that is capable of uncovering the hierarchical struc-ture of the mixture components. In Section 3.1 we present the metaphor and notations, and review CRP. In Section 3.2 we provide a detailed description of rCRP. In Section 3.3 we formalize the generative process of our model.
We maintain most of the basic assumptions and metaphors used in previous nonparametric probabilistic models as they have been proven to have strong explanatory power over data. A document is represented as a restaurant, and words are represented as customers in the restaurants. Words that convey homogeneous semantic theme are grouped together as customers of similar taste sit at the same table. A dish is chosen for each table from the global menu of dish tree, Figure 2: Our proposed model consists of two levels of partitioning process. In the diagram,  X  represents a dish,  X  represents a table, and  X  represents a cus-tomer. The first level rCRP associates each table to a dish. The second level CRP associates each customer to a table. which corresponds to topic assignment for each group of words. In our proposed model, the assignment of each cus-tomer to a table is determined by CRP. The association between tables and dishes is governed by rCRP.

CRP is a stochastic process that generates a random par-tition of discrete data. The table assignment probability distribution for a particular customer is as follows. The first customer always sits on the first table. The i th customer sits on a table depending on a draw from the following dis-tribution where n t is the number of customers already sitting at t table of the restaurant, and  X  is a parameter governing the likelihood of choosing of a new table. rCRP is an extension to CRP that assumes an infinite tree structure of the mixture components. Sticking to the metaphor, rCRP assigns a dish from the global menu for each table. The menu is an infinite tree of dishes unbounded in both branching factor and height. Indexing dishes in the infinite tree is nontrivial, as the number of potential dishes is uncountably infinite whereas the number of integers used for ordinary indexing scheme is only countably infinite. There-fore we utilize the index set of strings of integers. The root dish has an index of 1. Let k be an index string of a partic-ular dish on the menu, i  X  X h child of dish  X  k is named  X  This is visualized in Figure 2(a).

To find a dish for a particular table, we perform a recursive search beginning from the root dish. Let  X  k be the dish that is currently under examination. Then we make one of the three choices. The recursive search stops only when the first choice is made. 1. Choose  X  k 2. Choose one of the existing child dish of  X  k 3. Create a new child dish of  X  k , and choose it
We introduce the notations that will be used in the formal definition of the conditional probability of dish assignment. Let n jtk be the number of customers at table t of restaurant j eating dish k . We replace an index with dot to signify that the count is marginalized. For example, n jt  X  number of customers at table t of restaurant j , and n is the number of customers at restaurant j eating dish k . We use m jk to count the number of tables at restaurant j serving dish k . Likewise, m j  X  is the number of tables at restaurant j , and m .k is the total number of tables serving dish k at any restaurant. Finally, we use M .k to denote the cumulative counts of m .k summed over for all dishes that are descendants of  X  k including  X  k itself. The need for this cumulative counts is illustrated shortly.

Now we formalize the probability of three choices. To find the dish for table t in restaurant j , we perform recursive search from the root dish. Let  X  k be the current dish, then we draw from the following distribution where  X  k 0 is a direct descendent of  X  k , and  X  new is a new child dish of  X  k .

Dishes are equivalent to topic distributions used in the generation of documents, and each dish is drawn from a level-specific Dirichlet distribution. Let  X  k be the dish in-dexed by k , then it is generated as follows: where  X  ( k ) is a depth of current dish. We use  X  prior of Dirichlet distribution. Because symmetric Dirichlet distribution generates more sparse distribution with small values of parameter, we can expect more sparse topics with increasing depth of k when  X  is less than one.
We employ the two stages of generative process as de-scribed in Section 3.1 and Section 3.2. Now we formally describe the generative process.
 Topic Tree Generation The measure G tree of the global topic tree is drawn from the rCRP.
 Document Generation G j , the topic distribution of j th document, is distributed according to G tree .  X  ji denotes the topic of i th word in the j th document, and x ji denotes the word generated from the topic.

In this section, we develop a Markov Chain Monte Carlo algorithm for posterior sampling of table and dish assign-ments. Generally, computing an exact posterior of DP and its related models is intractable. Several approaches have been employed to compute the approximate posterior. Pos-sible approaches include (1) a P  X olya urn scheme based on the marginalization of unknown infinite-dimensions [15, 7], (2) a truncation approximation which limits the complexity of the model from infinite dimensions to finite dimensions [10], (3) a variational inference which converts inference al-gorithms into optimization problems [3, 21]. In this work, we employ the P  X olya urn scheme by incorporating the CRP metaphor for approximate inference.

Before we discuss the posterior inference algorithm, let us define variables of interest. x ji indicates the i th observed word of j th document, and  X  ji denotes the topic of x ji is an atom of G tree .  X  jt , which denotes the topic of t th table in the j th document, is an atom of G j . Note that each  X  associated with one  X  jt since the topic assigned to x ji correspond to the topic assigned to the table in which x ji seated. Likewise, each  X  jt is associated with one  X  k .
For the posterior inference, we marginalize out  X  k and  X  ji . Therefore we need to sample the assignment rela-tionship between these variables rather than sampling the quantities of variables themselves. For this purpose, we in-troduce two index variables. t ji is the index variable of tables such that  X  jt ji =  X  ji , and k jt is the index variable of topics such that  X  k jt =  X  jt .

First we write out the conditional density of x ji dish k in the inference steps for convenience. Each x drawn from some  X  k , and  X  k is drawn from its level dis-tribution, Dir(  X  | k | ). Therefore, by marginalizing out  X  we can simply compute the conditional density. Letting x only depends on the other x k already assigned to that dish and its decendents, and can be computed as follows We can further simplify the above equation by utilizing the Dirichlet-multinomial conjugacy as where V is the size of the vocabulary. With the P  X olya urn based sampling scheme, we can efficiently sample from the above distribution by marginalizing out unknown infinite dimensional distributions. Thus, our variables of interest are index variable of tables and dishes, namely t ji , and k It is natural to sample table t ji before sampling dish k the CRF metaphor, so we start with sampling t ji .
Sampling t The conditional distribution of t ji given x ji is proportional to the number of customers sitting at table t times the probability of x ji being observed under table t , which can be written as p ( t ji = t | rest )  X  where the probability of sitting at a new table can be found by marginalizing over all available dishes.
Sampling k The posterior sampling of k jt involves a se-quence of search along the menu tree. We begin from the root dish and move down along the tree until we find the dish. Suppose we want to sample a dish for customers at table t in restaurant j . We perform a recursive search be-ginning from the root dish as illustrated in Algorithm 1. The conditional probability of k jt is the prior probability of Algorithm 1 Sampling k jt by recursive algorithm function samplingK( k current ) else if k next = new child of k current then end if k times the likelihood of x jt being observed under dish k . The prior depends on k . If k = k current , it is proportional to the number of tables serving dish k . Otherwise, it is pro-portional to the number of tables serving dish k or any of its descendants.  X  Sampling k jt is important as it potentially changes the mem-bership of all data sitting at table t and leads to a well-mixed MCMC.

Estimating  X  For the rest of this paper and the experi-ments, we estimate  X   X  with a Maximum a posteriori (MAP) estimator.
We fit our rCRP model to discover and analyze the hi-erarchical topic structures in both synthetic and real data sets. We chose not only text data but also user-movie rat-ings data to show the generality of our model with respect to the type of data.
We generated a synthetic corpus that consists of 1,000 documents each having 1,000 word tokens. We used a three-level topic tree where the root topic has a uniform distribu-tion over the entire vocabulary. Topics at the second level are distributed over the terms in each of the columns. Top-ics at the third level have full probability concentrated at a single term from the column of its parent. The topic assign-ment process is performed by the two-level CRP and rCRP as described in Section 3.
New York Times The corpus consists of 1.8 million ar-ticles published between January 1, 1987 and June 19, 2007 . We randomly sampled 10,000 articles. We removed non-alphabetic characters and single-character words.

MovieLens The MovieLens dataset is a collection of movie ratings from 71,567 users on 10,681 movies. Users rated the movies on a scale of 1 to 5. We turned each user into a document made up of movies that he/she rated as 4 or 5. After this process, the dataset is equivalent to a text corpus consisting of 71,567 documents with 10,681 unique words.
Wikipedia Contemporary Art The WikiArt corpus consists of 3,600 web pages crawled by taking two hops from the Wikipedia Contemporary Art page 3 .

In both New York Times and Wikipedia, we applied porter stemming algorithm. We also removed words that occur too infrequently (less than 1%), and too frequently (more than 20%) in terms of the document frequency. These data statis-tics are illustrated in Table 1.
We visualize the result of inferring topic hierarchy from the synthetic data in Figure 3. The model successfully re-covers the original structure and topics. The first and second level topics are almost identical to the original topics. The http://archive.ics.uci.edu/ml/ en.wikipedia.org/wiki/Painting#Contemporary art Figure 3: Topic tree inferred from synthetic data. Each cell corresponds to a single word, and is shaded with intensity proportional to the probability of each word in the topic. The first and second level topics are almost identical to the original topics. There exist some noise in the third level topics. However, they are accounted by its direct parent. Figure 4: An example user from our MovieLens data. This user watched movies from the topics of Horror and Family from the genre level, and the topics of Action Thriller and Crime Thriller from the sub-genre level. third level topics show some noise, however most noise words in a topic are accounted by its direct parent topic. Figure 5 shows the topic trees inferred from NYTimes, MovieLens, and Wikipedia. Each topic tree is too large to fit in the space provided, so we take a subtree from each tree to illustrate the important points of the discovered topic hierarchies. Each topic is represented by the top ten highest probability words in that topic, and words with probability lower than 0.001 are not shown.

The root topic of each tree contains the most frequently used words in each corpus, and as we move down the tree, topics become more specialized. For example, in the topic tree of the NYTimes dataset, the Economy topic is followed by topics about Technology , Stocks , Prices , and Labor . For the MovieLens dataset, the root topic represents the generally popular movies, the movies with the most number of high ratings such as Star Wars and Forest Gump. One level down from the root, we can see the movies being clus-tered into genres such as Family, Horror, and Classics , and the Horror movie topic is separated into the more typ-ical Horror movies and movies in the Zombie sub-genre. As with any hierarchical taxonomy, some parts of the struc-ture are arguable, for example, whether the Drama topic should be a subtopic of Family or vice versa. Such arguable anomalies in the topic trees discovered by our model reflect the unique characteristics of the data.
The rCRP model allows each document to have a topic distribution over the entire topic tree. For example, using the topic tree in Figure 5(a), a user with high ratings for  X  X oy Story X  and  X  X ellraiser X  can be interpreted to be interested in both the Disney topic and the Horror topic. We show, in figure 4, an example user from our data whose topic pro-portion includes Horror, Family, Action Thriller , and Crime Thriller , with the movie titles that belong to those topics. Although the vanilla LDA and HDP models allow this, such flexible assignment of document-topic assignments is rare in models of topic hierarchies, and this shows that our model better reflects the nature of the hierarchical topic structure than previously proposed models.
Heldout likelihood, widely used as a comparative evalua-tion metric in topic modeling (cf. [5]), evaluates how well the trained model explains the heldout data. Heldout like-lihood is defined as log-likelihood of the heldout data given the trained model. Formally, where W heldout is the heldout data and M trained is the trained model. We use ten-fold cross validation.

We compare heldout likelihoods of our model with the baselines of LDA, HDP, and nCRP 4 . The result is shown in Figure 6. A model with higher explanatory power produces higher heldout likelihood. Note that the result of LDA with the optimal number of topics is very close to that of the HDP, which is natural because the HDP is designed to find the op-timal number of topics for LDA. Figure 6 shows that rCRP model outperforms LDA, HDP, and nCRP which confirms the intuition that the flexible hierarchical topic structure of rCRP explains the data better than the topic structures of HDP and nCRP.
A topic model is commonly evaluated by either directly calculating the perplexity or likelihood of held-out data [22], or applying the result to related tasks such as document clas-sification or recommendation [5]. To our knowledge, how-ever, there is no commonly used evaluation metric for mea-suring the goodness of a topic hierarchy. We suggest two fundamental characteristics of a topic hierarchy and propose concrete evaluation metrics. We then use these metrics to quantitatively compare the characteristics of the topic trees discovered by our model and by nCRP.
Studies on human semantic processing [6] find that in concept trees, the most general semantic category is placed at the top of the tree, and more specific categories toward http://www.cs.princeton.edu/ blei/downloads/hlda-c.tgz Figure 6: Heldout likelihoods of LDA, HDP, and rCRP for the three datasets. A higher value indi-cates that the model can explain better the heldout data. for better readability. the leaves. We assume the hierarchy of topics should fol-low this general principle and propose a metric to quantify the general-to-specific characteristic. We name this topic specialization and compute it by the semantic distance of a topic from the norm as defined below.

Let  X  Norm be the norm topic such that the probability of generating a particular word x i is proportional to the frequency of x i in the entire corpus. Formally, let freq ( x be the frequency of word x i in the entire corpus, V be the set of entire vocabulary, and  X  be the smoothing factor.  X  Norm is a topic such that for each word x i , As  X  Norm represents the word distribution of the entire cor-pus, we consider it to be the most general topic. For each topic  X  k , we measure how much it has drifted away from  X  Norm by measuring the cosine distance between the two. Formally, let  X (  X  k ) be the topic specialization of topic  X  then In rCRP, since customers at each table always visit the root topic first, the semantic distance between  X  Norm and  X  Root is zero.

We calculate and average the topic specialization of all topics at each level. From the definition of  X , a higher value indicates that the topic has drifted farther away from  X  Norm which implies that the topic has become more specialized. Figure 7 illustrates the concept of this topic specialization score, where we can see that the topic-word multinomial is near uniform for the root topic and becomes increasingly sparse toward the leaf topic.

In Figure 8, we summarize the topic specialization scores of rCRP and nCRP. In rCRP, topics at the second, third and fourth levels become increasingly more specialized. In nCRP, the general trend is the same, but the pattern is not so pronounced as the topics at all levels appear to be quite specialized. We conjecture this is because nCRP assumes that a document is generated only by the topics in a single path of the hierarchy, so all of the topics must be more spe-cialized to explain the data well. On the other hand, the topics discovered by rCRP do not have that restriction, so the model can focus more on finding an appropriate hierar-chy of increasingly more specialized topics, as shown in the topic trees in Figure 5.
Another important characteristic we expect to find from a hierarchical structure of topics is hierarchical affinity . That is, topics that descend from  X  k must be more similar to  X  than topics that descend from other topics. For clarity, we only use topics at the second level as the parent topics, and topics at the third level as the children topics, and compute the hierarchical affinity among them. Figure 9 illustrates this concept of hierarchical affinity, where the topic-word multinomial for the Lifestyle topic is similar to its children topics Music and Movie but quite different from its non-children topic Stocks .

Let  X  k be a topic at the second level, and let  X  ( k ) be the index set of all topics that have  X  k as a direct parent. Figure 7: Topic specialization. The topic-word multinomial is near uniform for the root topic and becomes increasingly sparse toward the leaf topic. Figure 8: Topic specialization scores of rCRP and nCRP. This shows the characteristic of rCRP to find general topics at the root and increasingly more spe-cialized topics toward the leaves. that do not have  X  k as a direct parent. In other words,  X  ( k ) are children of  X  k and  X   X  ( k ) are non-children of  X  measure the hierarchical affinity, we compare the average cosine similarity between  X  k and all topics in  X  ( k ) against the average cosine similarity between  X  k and all topics in  X   X  ( k ).

We compute this hierarchical affinity for all topics at the second level and compare the results for rCRP and nCRP. The results are illustrated in Figure 10. For all three data sets, rCRP clearly shows stronger hierarchical affinity for Figure 9: Hierarchical Affinity. Topics that form parent-child relationship show greater similarity in their word distribution than topics that are distant in the topic tree. children topics compared to the non-children topics. How-ever, in nCRP the affinity scores are not different for children compared to non-children .
We developed the recursive Chinese Restaurant Process, a new nonparametric prior that captures the hierarchical nature of mixture components. We used the rCRP to con-struct a nonparametric topic model that infers the hierar-chical structure of topics from discrete data. We applied our model to two text corpora and a user ratings dataset and visualized the inferred topic trees to show how our model can find intuitive hierarchical topic structures. We identified topic specialization and hierarchical affinity as two impor-tant characteristics of a hierarchical topic structure, and we suggested and tested evaluation metrics to quantify them. We also showed that our model outperformed LDA, HDP, and nCRP in terms of heldout likelihood.

Our model for discovering topic hierarchies with the re-cursive Chinese restaurant process describes a natural pro-cedure of finding a mixture component in a tree-structured way. This intuitive representation facilitates further exten-sions to our proposed model. One can relax the assumption that each table is assigned a single dish, and devise the In-dian Buffet Process [8] in with topics are in a hierarchical structure. The proposed model can also be deployed in vari-ous applications that rely on the latent structure of general-to-specific themes. An example would be recommendations based on collaborative filtering, extending our results with the movie ratings data, or social network search based on topics [19]. This research was supported by Basic Science Research Program through the National Research Foundation of Ko-rea (NRF) funded by the Ministry of Education, Science and Technology (2011-0026507). [1] R. Adams, Z. Ghahramani, and M. Jordan.
 [2] D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum. [3] D. Blei and M. Jordan. Variational inference for [4] D. Blei and J. Lafferty. Correlated topic models. In [5] D. Blei, A. Ng, and M. Jordan. Latent dirichlet [6] A. Collins and E. Loftus. A spreading-activation [7] M. Escobar and M. West. Bayesian density estimation [8] T. Griffiths and Z. Ghahramani. Infinite latent feature [9] K. Heller and Z. Ghahramani. Bayesian hierarchical [10] H. Ishwaran and L. James. Approximate dirichlet [11] M. Jeong and I. Titov. Multi-document topic [12] N. Kawamae. Latent interest-topic model: finding the [13] D. Kim and A. Oh. Accounting for data dependencies [14] W. Li and A. McCallum. Pachinko allocation: [15] S. MacEachern. Estimating normal means with a [16] D. Mimno, W. Li, and A. McCallum. Mixtures of [17] R. Neal. Density modeling and clustering using [18] T. Rubin and M. Steyvers. A topic model for movie [19] J. Tang, S. Wu, B. Gao, and Y. Wan. Topic-level [20] Y. Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical [21] Y. Teh, K. Kurihara, and M. Welling. Collapsed [22] H. M. Wallach, I. Murray, R. Salakhutdinov, and [23] C. Williams. A mcmc approach to hierarchical [24] G. Zheng, J. Guo, L. Yang, S. Xu, S. Bao, Z. Su,
