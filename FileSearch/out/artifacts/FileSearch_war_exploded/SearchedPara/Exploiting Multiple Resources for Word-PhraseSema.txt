 Semantic similarity calculation plays an important role in natural langua ge pro-cessing, such as information retrieve, automatic question answer ing, word sense disambiguation and machine translation. Most previous studies abou t semantic similarity calculation mainly focus on documents, sentences or conce pts, while ignore the fine-grained word-phrase semantic similarity which has be en a re-search hot topic. In this paper, we study the semantic similarity of w ords and compositional phrases.
 similarity of a word and a short sequence of words. In each word-ph rase pair, the word is a noun and the phrase is made up of an adjective and a nou n pair. The task can be treated as a binary classification which judge t he word-phrase pair is similar or not in semantic space. As the word-phrase pa ir lack of context, in this paper, we propose to extract rich features from multiple resources to represent word-phrase pair, including structured resource ( WordNet), semi-structured resource (Wikipedia) and unstructured resource (W eb). Totally, 27 features are obtained. Through feature selection, we finally get t he best feature subset. Then we use a supervised learning algorithm X  X upport Vect or Machine (SVM) to judge whether the word-phrase pair is similar or not in sema ntic space. We conduct experiments on SemEval 2013 Task5a. Our met hod achieves 82.9% in accuract beyond the best system (80.3%) that participate s in the task. Experimental results demonstrate the effectiveness of our appr oach. According to the technology used, semantic similarity mainly could be d ivided into three categories: purely statistical techniques , knowledge-based techniques and hybrid techniques[10]. The researching objects have been main ly focused on article, sentence and concept.
 distributional hypothesis [2 X 5] that words that occur in similar cont exts tend to have similar meanings. The distributional hypothesis could be expla ined as that statistical pattern of human word usage can be used to figur e out what people mean. When calculate word-word semantic similarity, VSM repr esents the word as a point of the high dimension of the text by counting the f requency of the word. Then the semantic similarity could be calculated by compa ring the distance of the point. VSM has been widely used in the information re-trieval area. VSM has been modified to calculate the semantic similarit y between words,documents and patterns[1]. Many methods have improved VS M , Deer-wester et al. (1990)[6] used truncated Singular Value Decompositio n (SVD) on the term-document matrix to improve similarity measurements. Lan dauer and Dumais (1997)[7] applied truncated SVD to word semantic similarity, a chiev-ing human-level scores on multiple-choice synonym questions from t he Test of English as a Foreign Language (TOEFL). Truncated SVD applied to do cument semantic similarity is called Latent Semantic Indexing (LSI), but it is ca lled Latent Semantic Analysis (LSA) when applied to word semantic similarit y. huge Web corpus and usually uses search engine to get the statistic al informa-tion. Based on the AltaVista search engine, Turney P (2001) [21] us ed pointwise mutual information(PMI) to recognize synonyms. Hsin-Hsi et al(2 006) [22] pro-posed a web search with double checking model to explore the web as a live corpus. GANG LU et al(2010) [23] proposed a semantic similarity meas urement method using the information page count and snippets. Danushka B ollegala et al(2008) [24] proposed an approach to compute semantic similarity u sing auto-matically extracted lexical-syntactic patterns from text snippets .
 tant relations between the words such as synonymy, hypernymy a nd meronymy. In WordNet text semantic similarity is mapped to word sense traced a s the con-cept. Methods [13 X 18] based on WordNet employ the structure of the WordNet, sometimes combing the information content.
 methods, Wiki-based methods [9 X 12] either use the content in the p age or the structure between the page. Explicit semantic analysis [9, 10] (ESA ) treats each article in Wikipedia as a concept and map any text into the high dimension of concept, besides, ESA has also measured the link between the page s. Hybrid methods have used both the statistical techniques and knowledge -based tech-niques. Mihalcea et al (2006) [25] and Bar et al (2012) [26] used hyb rid method to measure text semantic similarity.
 different methods focus on different lengths of texts and the effec t of each seman-tic similarity is mainly dependent on the resource. There are three ma in resources frequently used: structured resource, semi structured reso urce and unstructured resource. VSM and ESA perform better in long text while WordNet an d Web-based methods in word. Structured resource(e.g. WordNet) cou ld provide more information about semantic similarity but a lower coverage, Besides it is difficult to update. Unstructured resource(e.g. Web) have a higher cove rage but provide less information. Semi structured resource(e.g. Wikipedia) combin e the advan-tages of structured resource and unstructured resource. Bu t at the same time, it gets the drawbacks of them. Though the length of phrase is shor ter than long text, we can still treat a phrase as a long text. Besides the semant ic similarity of phrase can be calculated by word-word semantic similarity. To obt ain the ad-vantage of multiple resources and methods, we use multiple semantic similarities as the features of word-phrase pair and employ a supervised learn ing algorithm to calculate the semantic similarity of word-phrase. The word-phrase semantic similarity evaluation task is to judge whet her the word and phrase in a given pair is similar in semantic space or not. We tre at the task as a binary classification problem and employ SVM to solve it. T he representation of word-phrase pair has a significant influence on t he final perfor-mance. Therefore we focus on the feature crafting. In consider ation of the lack of sufficient context information, we propose to exploit three kinds of resources, i.e. structured resource (Wordnet), semi-structured resourc e (Wikipedia) and unstructured resource (Web), to extract rich features for wo rd-phrase pair repre-sentation. To combine the features from different resources, we employ a feature selection algorithm to select effective features. In the following, we first intro-duce three kind of features, and then give the details of the featu re selection algorithm. 3.1 WordNet Based features Adjective in English mostly act as a modifier, that is to say we can negle ct the adjective in phrase without losing much information. Besides WordNe t organizes same part of speech in a tree, it performs worse when used to meas ure the seman-tic similarity of words which have different part of speeches. Here to simplify the method to get the features, we use word-word semantic similarity a s the feature instead of word-phrase semantic similarity. Many methods have bee n proposed to calculate word-word semantic similarity using WordNet. These met hods are mainly based on structure in the WordNet or information content. S ince a word semantic similarity directly. Usually, we can use the following formula to get the word-word semantic similarity, by selecting for any given pair of word s those two meanings that lead to the highest concept-concept semantic similar ity. where w 1 and w 2 are the words which need to calculate the semantic similarity and w 1 and w 2 have the same part of speech, SC 1 and SC 2 are the synsets of w 1 and w 2 in the WordNet.
 semantic similarity method proposed by S&amp;P[13] is defined as: Where the depthM ax is the maximum depth of the taxonomy, len ( c 1 , c 2 ) is the length of the shortest path between the concepts c 1 and c 2 using node counting. two given concepts in the WordNet taxonomy and the depth of the le ast common subsume(LCS).
 the information content(IC) of the LCS of the two concepts.
 a normalization factor consisting of the information content of the two input concepts: rics can be easily achieved by using the WordNet-based implementatio n NLTK 3.2 Wikipedia Based features We get the Wikipedia based features mainly using the method Explicit Se man-tic Analysis (ESA) and Vector Space Model (VSM). ESA represents meaning of text in a high-dimensional space of concepts derived from Wikipedia, thus the meaning of any text can be represented in terms of Wikipedia-based concepts. The semantic similarity of the text can be valued by comparing the Wikip edia-based concepts vectors. Since ESA can process the text of arbit rary length, we can easily get the word-phrase semantic similarity. VSM builds a matrix of Term Frequency-Inverse Document Frequency (TF-IDF), in which row s correspond to terms and columns correspond to documents in the Wikipedia. Each w ord can be represented by a vector in the matrix of TF-IDF. We can get the word-word semantic similarity by comparing the vector in terms of the word. In o rder to get word-phrase semantic similarity, we use two methods. The first one is the same as the WordNet feature extraction, we use noun word-noun word semantic sim-ilarity to represent the word-phrase semantic similarity. The secon d one counts each word X  X  weight contributing to the text semantic similarity, it use word-word semantic similarity to represent word-phrase semantic similarity. Th e following is the formula[25] that we use to get the semantic similarity: Where maxsim ( w, T ) is the highest semantic similarity between word w and w  X  which is a word in text T . The Formula (8) builds a bridge from word-word semantic similarity to text-text semantic similarity when calculat ing the semantic similarity. In this paper, we use the Wikipedia snapshot as No vember 1, 2012 to implement our ESA and VSM model. Following procedures hav e been used to process the Wikipedia XML dump: 1. Turn the Wikipedia dump format from XML to text using the tool Wik ipedia 4. Filter out non-English characters and numbers; 5. Filter out the stop words; articles and 3,323,004 distinct terms, which served for representin g Wikipedia concepts as attribute vectors.
 ,manhattan and jaccard. Giving two vector: We can get the metrics using the following formula: we use four metrics to evaluate the distance between vectors. To tally, 12 features are obtained based on Wikipedia. 3.3 Web Based features Web Based features use the word co-occurrence[19]. The core ide a is that  X  X  word is characterized by the company it keeps X  [20]. Since the Web pa ge is too huge to get the date local for us, we mostly use the search engine t o get the statistical information about the term in the Web. So many metrics c ould be used to measure the degree of the word co-occurrence. In this p aper, we get the word statistical information through AltaVista and use Pointwis e Mutual Information (PMI), Jaccard coefficient and Dice coefficient to meas ure the word co-occurrence. If we define the p ( query ) as the following[21]: as follow formula: containing the query q; N is the number of the total web pages, usually N = are using the NEAR query (co-occurrence within a ten-word window ), which is a balance between accuracy (results obtained on synonymy test s) and effi-ciency (number of queries to be run against a search engine)[25]. Sp ecifically, the co-occurrence query could be defined as the follow to collect co unts from the AltaVista search engine. 1. Use noun word-noun word semantic similarity to represent word-phrase se-2. Use word-word semantic similarity to get the semantic similarity of w ord-3. Treat the phrase as  X  X ord X , use the Web-based method to get th e semantic achieve 9 features based on Web. 3.4 Feature Combination Based on three resources: Wikipedia, WordNet and Web corpus, We total get 27 features. According to the method used to get the semantic sim ilarity of word-phrase, we classify the features into three categories. To make the feature notations more readable, we use a capital letter as a prefix before method to represent the category and the letters after  X - X  to denote the m etric we use. 1. Use word-to-word semantic similarity to represent word-to-ph rase seman-2. Use noun word-to noun word semantic similarity to represent wor d-to-phrase 3. Directly calculate word-phrase semantic similarity, here we use pr efix X  X  X  X  each instance can be represented as a vector of 27 features.
 calculate the semantic similarity and the other is through feature se lection. We use the symbols FAll and FSubset to represent the ways of featur e combination separately.
 The forward search can be represented as follow: 1. Initialize F =  X  . 2. Repeat 3. Select and output the best feature subset that was evaluated during the is the set of all features, or when |F| exceeds some pre-set threshold(corresponding to the maximum number of features that you want the algorithm usin g). subset X  X Subset is as follow: FSubset = { NESA-JAC,WVSM-EUC,WVSM-MAN,DESA-EUC,DESA-JAC, 4.1 Data Set of 5861 negative instances and 5861 positive instances. In the tes t set, 3906 instances are provided. The format of instance (e.g. time particula r vs moment) in train and test data are the same. The first part of the instance is a noun and the second is a phrase which consists of an adjective and a noun. 4.2 Results and Discussion To see the performance of each previous semantic similarity on word -phrase, we use each semantic similarity of word-phrase as the feature vect or of word-phrase. Then we use the SVM to get the final results. This paper us e three main metrics in SemEval2013-Task5a to evaluate results. According to t he resources, we organize the results into three tables 1,2,3.
 DESA* methods perform better than others. It can be explained a s that ESA treats the phrase as a whole while others whether measure the par tial information or measure the semantic similarity indirectly. Among all the metrics us ed to compare vector X  X  similarity, cosine and jaccard have a better resu lt in all the methods.
 We find that most WIR* methods perform better than NIR* method s, We ex-plain this as that though adjacent pay less contributions to semant ic similarity than noun,it still has some contributions. DIR* methods preform wo rse than WIR* methods and NIR* methods, we conjecture that there is a ma in reason for the phenomena : this is due to the rare occurrence of phrase in the Web. WordNet-based methods perform better than other resources . It is because WordNet has much more information about semantic similarity than ot hers. three in SemEval 2013 task5a are also given. It is obvious that prev ious semantic similarity method is effective when we employ them to calculate the sema ntic similarity of word-phrase. We find that the result of FAll is significant ly improved and FSubset preforms even better. Different features of seman tic similarity based on different resources may figure out the different aspects of sem antic similarity of word-phrase. That is key point to explain why FAll obtains a bette r result. Because of the information redundancy between features, that is why FSubset perform better than FAll. We also find that both the FAll and FSubse t perform better than the top one in the Evaluating Phrasal Semantics task in three main metrics. That demonstrates the effectiveness of our approach. In this paper we focus on the task to judge whether the word-phr ase is similar or not in semantic space. Based on multiple resources, this paper ex tracts rich effective features to represent the word-phrase pair. After a f eature combination, we employ SVM to estimate whether the word and phrase is similar or no t in semantic space. Experimental results conducted on SemEval2013 -Task5a X  X  data set demonstrate the effectiveness of our approach.
 Phrasal Semantics task at SemEval-2013 in three main metrics, the re is still room for improvement. There may be more effective features to calculat e the semantic similarity. And other classifier excluding SVM, may have a better perf ormance. This work is supported by the Key Basic Research Foundation of She nzhen (JC201005260118A) and National Natural Science Foundation of China (61100094 &amp; 61300114).The authors are grateful to the anonymous re-view ers for their con-structive comments. Special thanks to Yaming Sun for insightful s uggestions.
