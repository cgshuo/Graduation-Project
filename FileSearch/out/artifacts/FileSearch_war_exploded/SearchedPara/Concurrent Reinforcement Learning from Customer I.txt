 David Silver d.silver@cs.ucl.ac.uk Leonard Newnham, Dave Barker, Suzanne Weller, Jason McFall Causata Ltd., 33 Glasshouse Street, London W1B 5DG In many commercial applications, a company or organ-isation interacts concurrently with many customers. For example, a supermarket might offer customers dis-counts or promotions at point-of-sale; an online store might serve targeted content to its customers; or a bank might email appropriate customers with loan or mortgage offers. In each case, the company seeks to maximise an objective function, such as revenue, cus-tomer satisfaction, or customer loyalty. This objective can be represented as the discounted sum of a reward function. A stream of interactions occurs between the company and each customer, including actions from the company (such as promotions, advertisements, or emails) and actions by the customer (such as point-of-sale purchases, or clicks on a website).
 Typically, thousands or millions of these interaction streams occur with different customers in parallel. Our goal is to maximise the future rewards for each cus-tomer, given their history of interactions with the com-pany. This setting differs from traditional reinforce-ment learning paradigms, due to the concurrent na-ture of the customer interactions. This distinction leads to new considerations for reinforcement learn-ing algorithms. In particular, when large numbers of interactions occur simultaneously, it is imperative to learn both online and to bootstrap , so that feedback from one customer can be assimilated and applied im-mediately to other customers.
 The majority of prior work in customer analytics, data mining and customer relationship management collects data after-the-fact (i.e. once interaction se-quences have been completed), and analyses the data offline (for example, Tsiptsis and Chorianopoulos 2010). However, learning offline or from complete in-teraction sequences has a fundamental inefficiency: it is not possible to perform any learning until interac-tion sequences have terminated. This is particularly significant in situations with high concurrency and delayed feedback, for example during an email cam-paign. In these situations it is imperative to learn online from partial interaction sequences, so that in-formation acquired from one customer is efficiently as-similated and applied in subsequent interactions with other customers. In these cases the final outcome of the sequence is unknown, and therefore it is necessary to bootstrap from a prediction of the final outcome. It is often also important to learn from the absence of customer interaction. For example, customer attrition (where a customer leaves the company due to an un-desirable sequence of interactions with the company) is often only apparent after months of inactivity by that customer. Waiting until a time-out event occurs is again inefficient, because learning only occurs af-ter the time-out event is triggered. However, the lack of interaction by the customer provides accumulating evidence that the customer is likely to attrite. Boot-strapping can again be used to address this problem; by learning online from predicted attrition, the com-pany can avoid repeating the undesirable sequence of interactions with other customers.
 In addition to concurrency, customer-based reinforce-ment learning raises many challenges. Observa-tions may be received, and decisions requested, asyn-chronously at different times for each customer. The learning algorithm must scale to very large quantities of data, whilst supporting rapid response times (for example, &lt; 10 ms for online website targeting). In ad-dition, each customer is only partially observed via a sparse series of interactions; as a result it can be very challenging to predict subsequent customer behaviour. Finally, there is usually a significant delay between an action being chosen, and the effects of that action occurring. For example, offering free shipping to a customer may result in several purchases over the fol-lowing few days. More sophisticated sequential inter-actions may also occur, for example where customers are channelled through a  X  X ales funnel X  by a sequence of progressively encouraging interactions.
 In this paper we formalise the customer interac-tion problem as concurrent reinforcement learning . This formalism allows for interactions to occur asyn-chronously, by incorporating null actions and null observations to represent the absence of interaction. We then develop a concurrent variant of temporal-difference (TD) learning which bootstraps online from partial interaction sequences. To increase computa-tional efficiency, we allow decisions to be taken at any time, using an instance of the options framework (Sut-ton et al., 1999); and we allow updates to be performed at any time, using multi-step TD learning (Sutton and Barto, 1998). We demonstrate the performance of concurrent TD on two large-scale test-beds for on-line and email interaction respectively, generated from real data about 300,000 customers. Reinforcement learning has previously been applied to sequential marketing problems (Pednault et al., 2002; Abe et al., 2002), cross-channel marketing (Abe et al., 2004), and market discount selection (Gomez-Perez et al., 2008). This prior work has found that model-free methods tend to outperform model-based meth-ods (Abe et al., 2002); has applied model-free meth-ods such as batch Sarsa (Abe et al., 2002) and batch Monte-Carlo learning (Gomez-Perez et al., 2008); us-ing regression trees (Pednault et al., 2002) and neural networks (Gomez-Perez et al., 2008) to approximate the value function. However, this prior work ignored the concurrent aspect of the problem setting: learn-ing was applied either in batch (incurring opportunity loss by not learning online), and/or by Monte-Carlo learning (incurring opportunity loss by waiting un-til episodes complete before learning). Our approach to concurrent reinforcement learning both learns on-line and bootstraps using TD learning, avoiding both forms of opportunity loss. In Section 5 we provide em-pirical evidence that both components are necessary to learn efficiently in concurrent environments. Much recent research has focused on a special case of the customer-based reinforcement learning frame-work, using contextual bandits. In this setting, ac-tions lead directly to immediate rewards, such as the click-through on an advert (Graepel et al., 2010), or news story (Li et al., 2010). A key assumption of this setting is that the company X  X  actions do not affect the customer X  X  future interactions with the company. However, in many cases this assumption is false. For example, advertising too aggressively to a customer in the short-term may irritate or desensitise a customer and make them less likely to respond to subsequent interactions in the long-term. We focus specifically on applications where the sequential nature of the prob-lem is significant and contextual bandits are therefore a poor model of customer behaviour.
 To avoid any ambiguity, we note that there has been significant prior work on distributed (sometimes also referred to as parallel ) reinforcement learning. This body of work has focused on how a serial (i.e. con-tinuing or episodic) reinforcement learning environ-ment can be efficiently solved by distributing the al-gorithm over multiple processors. Perhaps the best known approach is distributed dynamic programming (Bertsekas, 1982; Archibald, 1992), in which Bellman backups can be applied to different states, in par-allel and asynchronously; the value function is then communicated between all processors. More recently, a distributed TD learning algorithm was developed (Grounds and Kudenko, 2007). Again, this focused on efficient distributed computation of the solution, in this case applying TD backups in parallel to different states, and then communicating the value function be-tween processors. Other work has investigated multi-agent reinforcement learning, where multiple agents interact together within a single environment instance (Littman, 1994). Our focus is very different to these approaches: we consider a single-agent reinforcement learning problem that is fundamentally concurrent (because the agent is interacting with many instances of the environment), rather than a distributed solution to a serial problem (where the agent interacts with a single instance of the environment at any given time). We note that algorithms for concurrent reinforcement learning may themselves utilise distributed processing, however that is not the focus of this paper and is not discussed further. Reinforcement learning optimises long-term reward over the sequential interactions between an agent and its environment. Environments are typically divided into two categories: episodic and continuing environ-ments (Sutton and Barto, 1998). In an episodic en-vironment, the interaction sequence eventually termi-nates, at which point the environment resets and a new interaction sequence begins; whereas in continu-ing environments, there is a single interaction sequence that never terminates. In both cases, interactions oc-cur serially: the agent receives an observation, takes an action, and receives a reward.
 We introduce a third category for reinforcement learn-ing environments. In a concurrent environment, the agent interacts in parallel with many instances of the environment. In our motivating application, the agent is a company and the environment instances are cus-tomers. At any given time, the company may be involved in interaction sequences with many differ-ent customers; furthermore new customers may ar-rive, or old customers may leave, at any time. This is quite different from episodic reinforcement learning, in which one customer must complete its sequence be-fore a new customer arrives; or from continuing rein-forcement learning, which considers a single customer forever. The distinct nature of concurrent environ-ments leads to new challenges and considerations for reinforcement learning algorithms.
 One challenge that arises naturally in concurrent envi-ronments is that actions and observations may occur asynchronously : actions may be executed, and obser-vations or rewards received, at different times for each customer. We address this asynchronicity by repre-senting customer interaction sequences at a fine time-scale, and introducing null actions and observations to represent the absence of interaction with a given customer. Specifically, we define a i t to be the decision (or decisions) taken by the company with respect to customer i at time t . If the company does not take any action at time t , then that action is defined to be null, a = a  X  . Similarly, we define o i t to be the observation of the customer (which might include actions by the customer, or any new information acquired about the customer) i at time t ; if we do not observe anything for that customer, then that observation is defined to be null, o i t = o  X  . Finally, we define r i t to be the re-ward for customer i at time t ; in the absence of any response, the reward is defined to be zero, r i t = 0. Real (non-null) observations and real actions may oc-cur at different time-steps, with many intervening null interactions and zero rewards. Each time-step has a duration of d ; this is typically short and real inter-actions with each customer are typically sparse. 1 We assume that there are a total of N customers; if cus-tomers arrive in or exit the system, then their periods of inactivity are represented by null actions. A second challenge is partial observability. Reinforce-ment learning often focuses on the fully observable set-ting where observations o i t satisfy the Markov property words the agent is provided with full knowledge of the state of the environment. However, when interacting with a customer, the environmental state includes the latent  X  X ental X  state of the customer. Rather than attempting to model beliefs over this unwieldy envi-ronmental state  X  the approach taken by POMDPs (Kaelbling et al., 1995)  X  we represent the customer X  X  state directly by the history of their interactions. This is, by definition, a Markov state representation: the interaction histories form an infinite, tree-structured Markov decision process with the empty history at the root, and histories of length t at depth t .
 We make a simplifying assumption that customer be-haviour is identically distributed and conditionally in-dependent given their personal interaction history. In other words, customers are fully described by the ob-servable facts about that customer: interactions with one customer do not affect the behaviour of other customers (for example via communication between customers); and unobservable variations between cus-tomers (for example due to personality or mood) are modelled as noise. 2 Informally, our goal is to maximise future rewards for a customer given the interactions with that customer. We therefore focus on predict-ing future rewards directly from interaction histories, rather than marginalising over latent variables that model human decision-making behaviour.
 Formally, we define an interaction history to be a se-quence h t = { a 1 ,o 1 ,r 1 ,...,a t ,o t ,r t } , containing ac-tions a k  X  A  X  a  X  , observations o k  X  O  X  o  X  , and rewards r k  X  R . There are a total of N interac-tion histories, h t = [ h 1 t ; ... ; h N t ], occurring concurrently (the superscript i is henceforth used to denote the customer, which will be suppressed when discussing a single customer; and bold font to denote the vector of all customers). The observations and rewards are assumed to be i.i.d. given their interaction history, P We define a policy  X  ( a t +1 | h t ) = P ( a t +1 | h t ) to be a single strategy that can be applied to any customer; the next action is selected according to the interaction history for that customer only. We define a concurrent that depends on interaction histories for all customers. In particular, a concurrent algorithm can apply expe-rience gained from interactions with one customer so as to improve the policy for another customer. The return is the discounted sum of rewards for a sin-is the discount factor. This represents, for example, the total revenue from a customer. The action-value function Q  X  , is the expected return for a customer, given their interaction history followed by action a , Q  X  ( h t a ) = E ( R t :  X  | h t a ). The optimal value func-tion is the maximum action-value function achievable by any policy, Q  X  ( h t a ) = max  X  Q  X  ( h t a ); our goal is to approximate the optimal policy  X   X  ( a t +1 | h t ) that achieves this maximum, i.e. the best policy given the accumulated information about that customer.
 There are an infinite number of possible interaction histories, and therefore it is not feasible to repre-sent the value function for all distinct histories. We utilise linear value function approximation, Q  X  ( h t a )  X  Q  X  ( h t a ) =  X  ( h t a ) &gt;  X  to estimate the value function for the current policy  X  , where  X  ( h t a ) is a n  X  1 feature vector,  X  is an n  X  1 parameter vector. The feature vec-tor  X  ( h t a ) summarises the relevant information about both the interaction history h t and also the proposed action a into a compact representation. In concurrent reinforcement learning, the importance of bootstrapping is accentuated and it is crucial to use TD learning. However, naive application of TD(  X  ) is computationally inefficient and does not exploit the asynchronous structure of concurrent reinforcement learning. In this section we develop a simple variant of TD learning that is practical for large-scale concurrent reinforcement learning environments.
 If the time interval d is very small, and the number of customers N is large, it is computationally inefficient to update all customers at all time-steps. Typically, little changes between time-steps in which only null actions and null observations are observed. Learning at microscopic time-scales may also be data inefficient: for example when using TD(0), backups must propa-gate over many time-steps, and the algorithm X  X  be-haviour depends on the choice of time interval d . In our approach, the system only makes decisions at specific time-steps where a decision has been re-quested; and  X  X xecutes X  null actions (i.e. does noth-ing) at all other time-steps, without any further com-putation. Decisions may be requested for a variety of reasons: for online content serving, a real customer response will typically trigger a decision request; for email serving, a timer event might trigger a decision request (note that the decision may be a null action, i.e. choosing to do nothing). In addition, it is not nec-essary to perform learning updates at all time-steps, but only at those time-steps where an update has been requested. Again, updates might be triggered by real customer responses, but also by timer events. In gen-eral, our approach is fully asynchronous and there is no requirement that decisions and updates occur at the same time-step.
 To implement asynchronous decision requests, we de-fine a set of options {  X  ( a ) | a  X  A} , corresponding to actions a  X  A . Each option  X  ( a ) performs the corre-sponding real action a exactly once, then performs null actions at all subsequent time-steps until the next de-cision request or a time-out event. We seek the optimal policy over these options, taking account of the ran-domness in the decision requests; i.e. the best we can do under the constraint that non-null actions can only be taken on time-steps when decisions are requested. 3 To implement asynchronous update requests, we utilise multi-step TD learning (Sutton and Barto, 1998) to evaluate the current option-selection policy. K -step TD updates provide a consistent learning al-gorithm for any K ; mixing over different K leads to the well-known TD(  X  ) algorithm (Sutton, 1988). In our algorithm, we apply TD updates between succes-sive update requests at time-steps t and t 0 . In par-ticular, these time-steps need not correspond to de-cision requests, since the value of taking an option may legitimately be evaluated by intra-option value learning at any time-steps during its execution (Sut-ton et al., 1999). Specifically, we perform a t 0 step Sarsa update. This updates the action-value function Q ( h t a t +1 ), towards the subsequent action-value function Q ( h t 0 a t 0 +1 ), discounted t 0  X  t times, plus the discounted reward accumulated along the way, R The concurrent temporal-difference learning algorithm combines the options framework for asynchronous de-cision requests, with multi-step TD updates for asyn-chronous update requests (Figure 1). The implemen-tation of this algorithm is straightforward (Algorithm 1): at every decision request, an action is selected by -greedy maximisation of the action-value function Q ( h t a ); and at every update request, the action-value function is updated according to Equation 1.
 As discussed earlier, it is often desirable to learn from the absence of customer feedback. To allow for this possibility, we include updates for time-steps at which no real observation occurred (for example, the final update for each customer in Figure 1). These updates can be scheduled so as to minimise computational ex-pense, while ensuring that learning takes place for any significant change in customer value. In the following experiments these updates are scheduled at exponen-tially increasing time intervals after the last real obser-vation, eventually resulting in a time-out and a final update upon termination of the interaction sequence. To evaluate our concurrent reinforcement learning al-gorithms, we used a commercial simulator for inter-net targeting and email campaign scenarios. 4 The simulator was based on a nearest neighbour model, Algorithm 1 Concurrent TD Initialise  X 
Initialise u i  X  0 ,R i  X  0 ,  X  i  X  [1 ,N ] for each time-step t do end for constructed from a database containing 300,000 anonymised customer records from an online bank. Our experiments were designed to evaluate two ques-tions. First, how important is it to learn online, and to bootstrap, when operating at different levels of con-currency (i.e. when interacting with different numbers of customers in parallel)? Second, in a concurrent set-ting, how important is it to learn from delayed rewards (i.e. the full reinforcement learning problem), com-pared to learning from immediate rewards (i.e. the popular contextual bandit setting)? To answer these questions, we compared TD learning (which learns online from partial interaction sequences) to Monte-Carlo learning (which learns after-the-fact from com-plete interaction sequences) and to a contextual ban-dit algorithm (which learns online from immediate re-wards). The simulator included eight scenarios for in-ternet targeting and email campaigns, which varied considerably in their  X  X equentiality X  (i.e. whether ac-tions have long-term consequences). We investigated how the relative performance of the three algorithms changed across different levels of sequentiality. We now give details for both the simulator and algorithms. Each customer was represented by 30 variables such as session count, page view counters, and maximum credit. In addition, time-based variables have a spe-cial importance, due to the time-dependence of many customer interactions. For example, the probability of a positive customer response is typically dependent on the recency and frequency with which the company has been interacting with that customer: too little in-teraction and the customer is unlikely to respond; too much interaction and the customer is likely to be de-sensitised or annoyed; similarly, the current enthusi-asm of the customer is often captured by the recency and frequency with which the customer has been inter-acting with the company. To represent these factors, we also included two time-based variables that depend directly on the recent interaction history. One of these variables,  X  o , measured the time since the last real ob-servation; the second variable,  X  a , measured the time since the last real action.
 Each simulated customer was initialised with a real be-havioural history taken from the customer database. Subsequently, the reward and transition probabilities were generated from a commercial simulator. The sim-ulator included four internet targeting and four email campaign scenarios. In all scenarios, the agent was able to select amongst 7 real actions plus the null (do nothing) action a  X  . Customers were limited to a binary response at each time-step (i.e. they either respond or fail to respond). The reward was r = 1 for a positive response and r = 0 for no response. The probabil-ity of response is a Bernoulli random variable with a mean that depends on all 30 customer variables, based on a nearest-neighbour representation, multiplied by a function of the two time-based variables. Customers returned after a mean time interval of 13 time-steps, but with a probability of attrition of 0.05. Once a customer attrites, she never returns (this is therefore equivalent to a discount factor of  X  = 0 . 996). Action decisions were requested for each customer: once ini-tially, and subsequently once each time they return. In high-dimensional problems based on real customer data, it is common for some variables to be more sig-nificant than others; for some variables to be collinear; and for other variables to be predominantly noisy. This effect was modelled in the simulator by weighting the variables appropriately in the nearest neighbour response model. We combined two techniques to deal with the high-dimensional nature of the input. First, each of the 30 customer variables and 2 time-based variables was discretised into twelve bins with dynamically adapted bin boundaries, using a variant of online k -means in each dimension. The feature vec-tor  X  ( h t a ) contained one feature for each combination of bin and action (for customer variables), and one fea-ture for each combination of bin and real or null action (for time-based variables) to give a total of n = 2916 features, each of which captures an aspect of the rela-tionship between one variable and one action.
 Second, we automatically adapted the step-size for each feature, by stochastic gradient meta-descent (Sut-ton, 1992; Schraudolph, 1999). The key idea of this algorithm is to adapt a gain vector of step-sizes, by gradient descent, so as to minimise the mean-squared prediction error. Using this algorithm, the step-size of noise variables is gradually reduced, allowing more credit to be assigned to the significant variables. We compared three reinforcement learning algorithms, all of which used the adaptive discretisation and adap-tive step-size algorithm described above. The compu-tation required by each of these algorithms is min-imal, and they are therefore suitable for large-scale implementation with rapid response times. In each al-gorithm we used a naive exploration policy, based on -greedy with = 0 . 1. We note that, although more sophisticated exploration strategies have been consid-ered in the literature (especially in the contextual ban-dit setting), these do not always perform significantly better in real applications, and often less robustly, than a naive -greedy algorithm (Li et al., 2010). Monte-Carlo This algorithm learns after-the-fact Contextual bandit This was a simple instance of a Concurrent TD A ( t 0  X  t ) step TD update was ap-In all cases we compared a variety of constant step-size parameters  X  , and also an automatically adapted step-size, using a variety of meta-step-size parameters  X  . For each algorithm we report the results for the best-performing parameters.
 We compared the three algorithms on all eight scenar-ios of the simulator. To evaluate the performance of the algorithms, we measured their efficiency . We de-mean total reward over time-period [ t 1 ,t 2 ], normalised such that the uniform random policy has efficiency 0
A B C D A B C D 100 99 66 52 40 30 22 15 and the optimal policy 5 has efficiency 100. To esti-mate the sequentiality of each scenario, we used an or-acle that greedily maximises immediate reward, when provided with the true reward function. We measured the efficiency of this  X  X reedy oracle X , E greedy (0 ,T ), for large T (Table 1). We note that Internet A scenario is a true bandit scenario (i.e. actions have no effect on customers beyond their immediate response), whereas the other seven scenarios are increasingly sequential. Each algorithm was run approximately 100 times for each setting. In our first experiment, we measured the efficiency of the algorithms over time, using 100 concurrent customers and online updates. We mea-sured the efficiency over windows of 1,000 time-steps, E ( t,t + 1000), and plotted the learning curves (Fig-ure 2a). For the true bandit scenario, Internet A, the Contextual Bandit outperformed Concurrent TD. This is not surprising, since the contextual bandit ex-ploits prior knowledge that the immediate response by the customer is independent of subsequent visits. 6 However, whenever there was any significant degree of sequentiality, such as the Internet C, D and email scenarios, Concurrent TD outperformed the Contex-tual Bandit, by learning the sequential behaviour of customers during repeated visits. The advantage of Concurrent TD over the Contextual Bandit became more pronounced with increasing levels of sequential-ity. Monte-Carlo learning was more effective in the email scenario; however, it was slower to learn than Concurrent TD, due to the opportunity-loss incurred by waiting until a time-out before learning. 7 In our second experiment, we measured the efficiency of the algorithms at different levels of concurrency. For this experiment we continued until customers had re-turned 100,000 times in total (requiring 100,000 deci-sions to be made), but we varied the number of con-currently interacting customers at any given time. In other words we maintained a constant size pool of ac-tive customers; if one customer attrited then a new customer would join the pool. A concurrency level of 1 is just like an episodic environment: the company in-teracts with a single customer at a time, until 100,000 decisions have been completed. At the other extreme, at a concurrency level of 10,000, there is a pool of 10,000 customers interacting in parallel, each of which returns just 10 times. We started each run with a  X  X arm-up X  period (selecting actions randomly, with-out any learning updates) to ensure that the customer pool included customers at a variety of different stages of interaction. The efficiency was measured over the second half of each run (Figure 2c). The results show a general degradation of performance with larger con-currency, illustrating the increased opportunity loss. In all cases, the Monte-Carlo algorithm was most sensi-tive to concurrency; at high concurrency levels Monte-Carlo barely outperformed the uniform random base-line. This demonstrates that after-the-fact learning in-curs a prohibitively large opportunity loss at high con-currency. In contrast, the Concurrent TD algorithm is more robust at high concurrency, thanks to bootstrap-ping. In non-sequential scenarios such as Internet A, the Contextual Bandit was the most robust to high concurrency. However, for the non-sequential Email scenarios, the Contextual Bandit performed poorly, only occasionally stumbling on important sequences of null actions through random exploration. For all algorithms, the degradation of performance with con-currency was most apparent in the strongly sequential scenarios. In these scenarios customers must return several times before positive responses are observed, increasing the potential for opportunity loss. Finally, we compared the efficiency of Concurrent TD between online and batch learning. Rather than learn-ing online at each time-step, updates were batched together and executed every m time-steps, where m is the batch length ; m = 1 is closest to online learn-ing. Efficiency was measured over the full duration of each run. The results clearly show a very significant drop in performance for higher batch lengths. Further-more, the greater the concurrency level, the faster this drop occurred. At low concurrency, batch length was not too important; for example when the concurrency level is 1 (corresponding to an episodic environment in classic reinforcement learning) performance was main-tained up to m = 100000. However, at high concur-rency batching becomes much more problematic; for example at a concurrency level of 10,000 the perfor-mance started dropping significantly at m = 50. The natural conclusion is that, when interacting concur-rently with large numbers of customers, it is impera-tive to use online updates rather than the offline up-dates or large batch lengths used in prior work, e.g. (Pednault et al., 2002; Abe et al., 2002; 2004). Our experiments show the importance of bootstrap-ping online. We would expect similar performance for a well-tuned TD(  X  ) algorithm. Unfortunately, TD(  X  ) requires orders of magnitude more computation, as it updates all weights at every microscopic time-step. Finally, we note that the performance of all algorithms is significantly below the optimal policy, due largely to limitations of the linear function approximator; this could perhaps be addressed by a richer set of features. We have introduced a framework for reinforcement learning from customer interaction histories. Its cru-cial component, compared to traditional reinforce-ment learning approaches, is that the agent interacts with many customers concurrently . Our solution is a temporal-difference learning algorithm that bootstraps online from concurrent interactions. Our algorithm is computationally efficient and suitable for large-scale online learning. We evaluated Concurrent TD in a high dimensional commercial simulator against non-bootstrapping (Monte-Carlo), non-online (batch TD), and non-sequential (Contextual Bandit) algorithms re-spectively. Our results clearly demonstrate that, in highly concurrent and sequential scenarios, it is vi-tally important to bootstrap from partial interaction sequences, to learn online, and to use sequential rein-forcement learning algorithms. Unlike prior work, our algorithm combines all three of these key properties. Abe, N., Pednault, E., Wang, H., Zadrozny, B., Fan,
W., and Apte, C. (2002). Empirical comparison of various reinforcement learning strategies for sequen-tial targeted marketing. In International Conference on Data Mining , pages 3 X 10.
 Abe, N., Verma, N., Schroko, R., and Apte, C. (2004). Cross channel optimized marketing by re-inforcement learning. In International Conference on Knowledge Discovery and Data Mining (KDD) , pages 767 X 772.
 Archibald, T. (1992). Parallel dynamic programming.
In Kronsj  X o, L. and Shumsheruddin, D., editors, Ad-vances in parallel algorithms , pages 343 X 367. John Wiley &amp; Sons, Inc.
 Bertsekas, D. (1982). Distributed dynamic program-ming. Automatic Control, IEEE Transactions on , 27(3):610 X 616.
 Gomez-Perez, G., Martin-Guerrero, J. D., Soria-
Olivas, E., Balaguer-Ballester, E., Palomares, A., and Casariego, N. (2008). Assigning discounts in a marketing campaign by using reinforcement learning and neural networks. Expert Systems with Applica-tions , (doi: 10.1016/j.eswa.2008.10.064).
 Graepel, T., Candela, J. Q., Borchert, T., and Her-brich, R. (2010). Web-scale Bayesian click-through rate prediction for sponsored search advertising in microsoft X  X  bing search engine. In 27th International Conference on Machine Learning , pages 13 X 20.
 Grounds, M. and Kudenko, D. (2007). Parallel rein-forcement learning with linear function approxima-tion. In Adaptive Agents and Multi-Agent Systems , pages 60 X 74.
 Kaelbling, L., Littman, M., and Cassandra, A. (1995).
Planning and acting in partially observable stochas-tic domains. Artificial Intelligence , 101:99 X 134. Li, L., Chu, W., Langford, J., and Schapire, R. E. (2010). A contextual-bandit approach to per-sonalized news article recommendation. CoRR , abs/1003.0146.
 Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In 11th In-ternational Conference on Machine Learning , pages 157 X 163.
 Pednault, E., Abe, N., Zadrozny, B., Wang, H., Fan,
W., and Apte, C. (2002). Sequential cost-sensitive decision making with reinforcement learning. In In-ternational Conference on Knowledge Discovery and Data Mining (KDD) .
 Schraudolph, N. N. (1999). Local gain adaptation in stochastic gradient descent. In International Con-ference on Artificial Neural Networks , pages 569 X  574.
 Sutton, R. (1988). Learning to predict by the method of temporal differences. Machine Learning , 3(9):9 X  44.
 Sutton, R. (1992). Adapting bias by gradient descent: An incremental version of delta-bar-delta. In 10th
National Conference on Artificial Intelligence , pages 171 X 176.
 Sutton, R. and Barto, A. (1998). Reinforcement Learn-ing: an Introduction . MIT Press.
 Sutton, R., Precup, D., and Singh, S. (1999). Between
MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial In-telligence , 112(1-2):181 X 211.
 Tsiptsis, K. and Chorianopoulos, A. (2010). Data Min-ing Techniques in CRM: Inside Customer Segmen-
