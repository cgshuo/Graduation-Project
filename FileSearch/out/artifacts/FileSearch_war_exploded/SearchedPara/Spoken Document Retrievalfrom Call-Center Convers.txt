 We are interested in retrieving information from conversa-tional speech corpora, such as call-center data. This data comprises spontaneous speech conversations with low record-ing quality, which makes automatic speech recognition (ASR) a highly difficult task. For typical call-center data, even state-of-the-art large vocabulary continuous speech recogni-tion systems produce a transcript with word error rate of 30% or higher. In addition to the output transcript, ad-vanced systems provide word confusion networks (WCNs), a compact representation of word lattices associating each word hypothesis with its posterior probability. Our work exploits the information provided by WCNs in order to im-prove retrieval performance. In this paper, we show that the mean average precision (MAP) is improved using WCNs compared to the raw word transcripts. Finally, we analyze the effect of increasing ASR word error rate on search effec-tiveness. We show that MAP is still reasonable even under extremely high error rate.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms
The role of call-centers is becoming increasingly central to corporates in recent years. There are two main reasons for that. The first one is the increase of the importance of indi-viduals to companies, and the drive of the latter to acquire and keep customers for the long haul. The second relates to the rapid pace of advances in technology that creates an ever-growing gap between users and automated systems, prompting them to require more technical assistance. 70% accuracy in transcription of such noisy spontaneous telephone calls. In other words, for this kind of data, ap-proximately one out of three words is mis-recognized. In some circumstances like noisy channel, foreign accent, un-der trained engines etc., accuracy may fall to 50% or even less. The low accuracy of the data can have a dramatic effect on the precision and the recall of the search.

We present a novel scheme for information retrieval over noisy transcripts, that uses additional output from the tran-scription system in order to reduce the effect of recognition errors in the word transcripts. Our approach takes into con-sideration all word hypotheses provided by the transcription server as well as their posterior probabilities. We analyze the retrieval effectiveness at different word error levels and when different ranking models are used. We show that even for word error rate of about 50% our retrieval approach is able to perform reasonably well.

The paper is organized as follows. We describe the ASR engine and its output in Section 2. The Retrieval methods are presented in Section 3. Experimental setup and results are given in Section 4. In Section 5, we give an overview of related works. Finally, we conclude in Section 6.
We use the IBM research prototype ASR system, de-scribed in [16], for transcribing call-center data consisting of a single channel 6KHz speech (agent and caller are mixed). The output words are selected from a large US English vo-cabulary, which has a good coverage of the spoken language. The ASR engine works in speaker-independent mode, apply-ing the same models for all speakers (agents and callers) 1 . For best recognition results, a speaker-independent acoustic model and a language model are trained in advance on data with similar characteristics.

Typically, ASR generates word lattices that can be con-sidered as directed acyclic graphs. Each vertex is associated with a timestamp and each edge ( u, v ) is labeled with a word hypothesis and its prior probability , which is the probabil-ity of the signal delimited by the timestamps of the ver-tices u and v , given the word hypothesis. The 1-best path transcript is obtained from the word lattice using dynamic programming techniques.

Mangu et al. [11] and Hakkani-Tur et al. [9] propose a compact representation of a word lattice called word confu-sion network (WCN). Each edge ( u, v ) is labeled with a word hypothesis and its posterior probability , i.e. , the probability of the word given the signal. One of the main advantages of WCN is that it also provides an alignment for all of the words in the lattice. As explained in [9], the three main steps for building a WCN from a word lattice are as follows: 1. Compute the posterior probabilities for all edges in the 2. Extract a path from the word lattice (which can be 3. Traverse the word lattice, and align all the transitions
Some ASR systems train a specific speaker-dependent model, for each of the system agents. operator: thanks for calling the ibm customer service center this is john am i speaking with mark caller: yes operator: hey mark what X  X  going on caller: well i X  X  trying to connect to the at and t net client and i got an error it came back and gave me error one twenty it says invalid access list and i X  X  not sure what it means by that operator: one twenty invalid access list caller: uhhuh operator: let X  X  go and take a look at your setup real quick and see see what X  X  going on there caller: ok ... caller: well it ah trying to ah oh i got something um the t and t at and t net client got A GRAPHIC ON
MY SCREEN so it X  X  connecting and it X  X  counting the connect time so let X  X  see it X  X  got a little downer i think i can minimise it by clicking on it oops ah that X  X  not it ah yeah i can minimise it ok great and i X  X l just try to get into lotus notes and see what happens transcript of the same call, with 36.75% WER level . alternatives provided by the WCN. Consequently, these al-ternatives may have been spoken but were not the top choice of the ASR. Such an expansion might improve recall but will probably reduce precision.

In order to improve the recall without decreasing the pre-cision, we exploit two pieces of information provided by WCN concerning the occurrences of a term: its posterior probability and its rank among the other hypotheses. This additional information is used in order to improve the pre-cision of the results and consequently to improve search ef-fectiveness as measured by mean average precision (MAP) and precision at k ( P @ k ).
 The posterior probability reflects the confidence of the ASR in the hypothesis given the signal, thus the retrieval process will boost calls for which the query term occurs with higher probability. The rank of the hypothesis among other alternatives reflects the importance of the term relatively to other alternatives, thus, a call containing a query term that is ranked higher should be preferred over a call where the same term is ranked lower.
 Let D be a call-center conversation modeled by a WCN. We denote by P r ( t | o, D ) the posterior probability of a term t at the offset o in the WCN of D . We denote by rank ( t | o, D ) the rank of a term t at the offset o , where all hypotheses at offset o are sorted in decreasing order according to their posterior probabilities.

When terms are stemmed, the posterior probability of a stem t at offset o is the sum of all posterior probabilities of the terms having the same stem t at offset o . The rank of the stem is also reevaluated according to the new posterior prob-abilities. For example, in figure 3, the terms graphic and graphics both are stemmed to graphic ; hence the posterior probability of the stem is 0 . 22+0 . 13 = 0 . 35 and its rank is 1. The probability for the term glass is 0.27 and its rank is 2. It is interesting to note that the term graphic does not occur in the automatic 1-best path transcript although it occurs clarity.
Our search system also consider lexical affinities [3] dur-ing query evaluation. Lexical affinities are closely related query terms found close to each other in the document. The confidence level of a lexical affinity ( t 1 , t 2 ), is determined by the multiplication of the confidence levels of its terms: the lexical affinity (required for setting its boost) is deter-mined by the maximal rank of its terms: rank (( t 1 , t 2 ) | o, D ) = max ( rank ( t 1 | o 1 , D ) , rank ( t 2 | o 2 , D )).
The inverse document frequency ( idf ) of a term is a mea-sure of the relative importance of the term in the corpus. Usually, the inverse document frequency is evaluated by the ratio of the number of all the documents in the corpus to the number of documents in the corpus where the term appears. With WCNs, the occurrence of a term in a call is associated with a confidence level. We define idf ( t ), the inverse docu-ment frequency of a term t in the corpus C by the following formula: where O t reflects the number of occurrences of t in the cor-pus and is estimated by: and O reflects the overall number of occurrence in the corpus and is estimated by: (SUBR) is defined by Deletion error rate (DELR) and insertion error rate (INSR) are defined in a similar manner.

Table 1 gives the distribution of the error types over 1-best path transcripts extracted from automatic WCNs at different WERs. Different WER levels were achieved by providing different number of training examples to the ASR. The smaller the number of the training calls, the larger the error rate. The first row represents a limit on the lowest error rate that can be achieved by our system when all data is used for training.

The WER of the different corpora was also measured af-ter stop-words filtering and stemming; these corpora are de-noted by stem-stpw . Note that in our collection, around 70% of the terms are stop-words both in the manual and the automatic transcripts. This is typical for discussion tran-scripts where stop-words are much more frequent than in typical written text 2 . Note also the decrease in errors after stop-word filtering and stemming. The reason is that many errors relate to stop-words since shorter terms are harder to recognize. Furthermore, stemming reduces errors related to a mishmash between similar terms (e.g. table and tables ).
Additionally, the table shows that while the WER in-creases with smaller training data, the ratio between the different error types is preserved. Around 60% of the er-rors are substitution errors in all collections. The rest of the errors are split differently between insertion errors and dele-tion errors according to the global WER. When the WER increases, DELR increases and INSR decreases. If the ASR engine is under-trained, it does not output enough words so more deletions are expected and the chance to have extra words inserted is smaller. If the ASR engine is over-trained, it outputs many alternatives and more insertion errors than deletion errors are expceted.
As stated in [11], the 1-best path transcript obtained from a WCN is usually more accurate than the 1-best path tran-script obtained from the corresponding word lattice. How-ever, the distribution of the different types of errors is very similar between the two 1-best path transcripts.

In order to compare the retrieval effectiveness between these two representations, we have indexed the 1-best paths of all calls as extracted from the corresponding lattices, and the 1-best paths extracted from the WCNs, and run the same 120 queries against the two indices. The results were compared to the results obtained from an index of the man-ual transcripts. For this experiment, the term frequency reflects the number of occurrences of the term in the tran-script. Table 2 shows the MAP and P@10 of the results at two different WER levels.

We note that the retrieval effectiveness decreases with the increase in WER for both models. The results are very simi-lar between 1-best paths extracted from lattices and WCNs. This can be explained by similar WER levels. However, a WCN provides much more information that can be ex-ploited. In the following section, we show how the extra
Using the same set of stop-words, only about 50% of the terms in the TREC-8 corpus of textual documents are marked as stop-words. stop-words filtering.
 extracted from WCNs at different WER levels.
 Figure 4: Precision results at different error levels. levels, when all the hypotheses provided by the WCNs are indexed, however, precision is significantly reduced due to the added noise.
 The MAP and P@R of the search results are presented in Figure 6. The graphs show that all WCN CL boost always outperforms the other models, especially when the WER in-creases. Furthermore, recall and precision are the same for the 1-best WCN CL and 1-best WCN TF models, however, the first outperforms the second in MAP measure due to the usage of the confidence levels. This leads to the con-clusion that using confidence levels improves the retrieval effectiveness.

In the past decade, most of the research efforts on spo-ken data retrieval have focused on extending classical IR techniques to spoken documents. Some of these works are described by Garofolo et al. [7]. An ASR system is used to generate the transcription of the speech. This transcription is generally a 1-best path transcript. Most systems index the transcription as clean text and successfully apply a generic IR system over the text as described by Brown et al. [1] and James [10]. This strategy works well for transcripts like broadcast news stories that have a low WER (in the range of 15%-30%) and are redundant by nature (the same piece of information is spoken several times in different manners). Moreover, the algorithms have been mostly tested over long queries stated in plain English and retrieval for such queries is more robust against speech recognition errors.
An alternative approach consists of using word lattices in order to improve the effectiveness of SDR. Singhal et al. [14, 15] propose to add some terms to the transcript in order to alleviate the retrieval failures due to ASR errors. A classi-cal way to bring new terms from an IR perspective is doc-ument expansion using a similar corpus. Their approach consists of using word lattices in order to determine which words returned by a document expansion algorithm should be added to the original transcript. The necessity to use a document expansion algorithm was justified by the fact that the word lattices they worked with, lack information about word probabilities.

Saraclar and Sproat in [13] show improvement in word spotting accuracy, using phonemes and word lattices, where a confidence measure of a word or a sub-word in a speech document can be derived from the lattice. Their experi-ments also concern telephone conversations. Similarly to
This work studies how SDR can be performed efficiently over very noisy call-center data. This data comprises sponta-neous speech conversations with low recording quality, which makes automatic speech recognition (ASR) a highly difficult task. For typical call-center data, even state-of-the-art large vocabulary continuous speech recognition systems produce a transcript with word error rate of 30% or higher.
Our work exploits the additional information provided by a WCN in order to improve retrieval performance. In a WCN, a compact representation of the ASR X  X  word lattice, each word is associated with a confidence level reflecting its posterior probability given the speech signal. By taking the terms X  confidence levels into consideration, and by consider-ing the relative rank of the different hypotheses, our system is able to improve the search effectiveness.

Our experiments over true call-center conversations demon-strate the effect of increasing WER level on search effective-ness. As expected, a higher WER level hurts search results. The search recall is significantly improved by indexing all the hypotheses provided by the WCN. While the precision is decreased as expected, the MAP is improved compared to the 1-best path transcript, due to the confidence level con-sideration. Using our indexing scheme of WCN, the MAP is still reasonable even under extremely high error rate level, and thus effective search can be achieved.
 One of problems of ASR is out of vocabulary (OOV) words. OOV words are missing from the ASR system vocabulary and are replaced in the output transcript by alternatives that are probable, given the recognition acoustic model and the language model. In the experiments presented in this work all query terms are in-vocabulary so this problem has been ignored. However, in real practice, OOV queries re-quire special treatment. This is left for further research.
We are grateful to Olivier Siohan from the IBM T.J. Wat-son research center for providing the required data and for assistance on ASR topics. [1] M. Brown, J. Foote, G. Jones, K. Jones, and S. Young. [2] D. Carmel, E. Amitay, M. Herscovici, Y. S. Maarek, [3] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer. [4] C. Chelba and A. Acero. Indexing uncertainty for [5] C. Chelba and A. Acero. Position specific posterior
