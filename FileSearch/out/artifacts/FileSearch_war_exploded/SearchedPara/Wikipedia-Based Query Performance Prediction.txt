 The query-performance prediction task is to estimate re-trieval effectiveness with no relevance judgments. Pre-retrieval prediction methods operate prior to retrieval time. Hence, these predictors are often based on analyzing the query and the corpus upon which retrieval is performed. We pro-pose a corpus-independent approach to pre-retrieval predic-tion which relies on information extracted from Wikipedia. Specifically, we present Wikipedia-based features that can attest to the effectiveness of retrieval performed in response to a query regardless of the corpus upon which search is performed. Empirical evaluation demonstrates the merits of our approach. As a case in point, integrating the Wikipedia-based features with state-of-the-art pre-retrieval predictors that analyze the corpus yields prediction quality that is con-sistently better than that of using the latter alone.
There is a large body of work on query-performance pre-diction [5]. The goal is to estimate the effectiveness of ad hoc (query-based) retrieval with no relevance judgments.
Pre-retrieval prediction methods operate prior to retrieval time [12, 15, 16, 11, 22, 10]. Most of these methods an-alyze the query using information induced from the corpus upon which search is performed [8, 12, 11, 22]. Post-retrieval prediction methods analyze also the result list of the docu-ments most highly ranked [5]. Hence, their prediction qual-ity transcends that of pre-retrieval predictors. However, pre-retrieval predictors are much more efficient as they do not rely on performing the search. Accordingly, here we focus on pre-retrieval prediction.

We present a novel corpus-independent pre-retrieval query-performance prediction approach. Our approach is based on using information induced from Wikipedia so as to estimate the absolute query difficulty . That is, the Wikipedia-based features that we present attest to the resultant retrieval ef-fectiveness of using the query regardless of the corpus on which search is performed. Therefore, a clear advantage of our approach is for non-cooperative search settings where corpus-based information is not available [4].

Empirical evaluation performed with TREC datasets at-tests to the merits of our approach. First, when used alone, our approach yields decent prediction quality. Furthermore, we integrate the approach with previously proposed state-of-the-art pre-retrieval predictors that analyze the corpus used for retrieval. The resultant prediction quality is consistently better than that of using these predictors alone.
As noted, in contrast to our approach that is corpus-independent, most pre-retrieval prediction methods analyze the corpus that serves for retrieval [8, 12, 16, 11, 22, 10].
There is very little work on corpus-independent query-performance prediction [15, 11]. Query-syntactic features and WordNet-based features were used for prediction. In contrast to our work, Wikipedia-based information was not used and corpus-independent predictors were not integrated with corpus-dependent ones. Integrating these predictors [15, 11] with ours is a future venue we intend to explore.
There is some work on pre-retrieval query-performance prediction for the entity retrieval task which was performed over Wikipedia [18]. In contrast, we present predictors for the standard ad hoc retrieval task, and are the first, to the best of our knowledge, to use Wikipedia to this end. Fur-thermore, almost all of the features we use for prediction here were not used in this work on entity retrieval [18]
Finally, we note that most of the Wikipedia-based infor-mation sources that we use  X  titles, content, links and cat-egories  X  were used to improve retrieval effectiveness (e.g., [1, 19, 3, 13]) rather than to predict effectiveness.
Let q and D be a query and a document corpus, re-spectively. The task we pursue, a.k.a. pre-retrieval query-performance prediction [5], is estimating, prior to retrieval time , the effectiveness of a search that will be performed by some retrieval method over D in response to q .

The most effective pre-retrieval predictors utilize informa-tion induced from the corpus D [11]. In contrast, we devise predictors that use information induced from the Wikipedia corpus. We use the predictors as features in a regression framework to estimate the Average Precision (AP) of re-trieval performed over D . Thus, the approach we present is independent of D and is, in essence, designed to estimate the absolute difficulty of q .
 Notational conventions. Herein, we refer to a subset of q  X  X  terms as a sub-query. A Wikipedia page p is defined to be associated with a set of terms if its title contains at least one of the terms in the set. A q denotes the set of pages associated with q (i.e., with its set of terms { q i } ). A page p is exact match to a sub-query, if there is a 1-1 match between the sub-query X  X  terms and the page title. The maximal exact match length, M EM L , is the size of the largest sub-query (i.e., the number of its terms) for which an exact match holds. The set of Wikipedia pages for which the maximal match holds is referred to as M MEML . The set of pages that are an exact match to the entire query q is denoted E q .
We use S 1 , S 2 and S 3 to denote the sets of sub-queries that contain one, two and three terms, respectively. Predictors marked with * are computed separately for each of the sets. Predictors that are not marked with * are computed only for single query terms (i.e., sub-queries of size 1).
Computing some of our proposed features requires re-trieval over Wikipedia. However, we note that Wikipedia is a very small corpus (4 . 5 million documents), specifically, in comparison to large-scale Web corpora.

The predictors we propose can be categorized into three groups, based on the information provided by Wikipedia that they utilize: titles and content , links and categories and previously proposed corpus-based predictors adapted for Wikipedia . We next describe each of these groups.
The following predictors use information in the titles and content of Wikipedia pages to (mainly) measure aspects of the Wikipedia coverage of sub-queries of q .
The most common pages in Wikipedia are entity pages , which refer to people, objects, etc. Entity pages are as-signed with human-generated categories which constitute a rich source of structured information. In addition, al-most every page in Wikipedia contains links to other related pages. We next present predictors that extract information from links and categories.
We next adapt previously proposed pre-retrieval predic-tors to predict q  X  X  performance over Wikipedia. We hypoth-esize that this predicted performance can attest to q  X  X  per-formance over any corpus.
We turn to evaluate the prediction quality of the proposed methods. In Section 4.1 we evaluate the prediction qual-ity for a standard language-model-based retrieval. Then, in Section 4.2 we study the effectiveness of our methods in predicting the performance of runs submitted to TREC.
Table 1 presents the details of the TREC datasets used for evaluation. ROBUST consists of (mainly) newswire articles, WT10G contains Web pages that can be of low quality (e.g. spam), and GOV2 is a crawl of the .GOV domain; ClueWeb ( X 09) is a large scale noisy Web collection, which contains also Wikipedia pages.

We used the TREC topics titles as queries. Krovetz stem-ming and stopwords removal, using the INQUERY list, were applied to both documents and queries. The experiments were conducted using the Indri toolkit 1 . The query likeli-hood method [17] serves for the retrieval method. Document d  X  X  retrieval score with respect to query q is: log Q q p ( q i | d ) is the probability assigned to q i by a Dirichlet smoothed unigram language model induced from d with the smoothing parameter set to 1000 [21]. The same smoothing approach was applied to the language models used by our Wikipedia-based predictors, except for the query language model which was a simple maximum likelihood estimate.

As we integrate the proposed predictors in a regression framework, we follow previous recommendations [10] 2 , and past practice [10, 7, 2], and measure prediction quality by the root mean squared error (RMSE) between the predicted www.lemurproject.org
It was shown that measuring prediction quality using Pear-son correlation between the predicted and the actual value of average precision might not result in reliable results when integrating predictors using regression.
 Table 2: Prediction quality for language-model-based retrieval of the Wikipedia-based and corpus-based predictors and their integration.  X  X  X  ( X  X  X ) and  X  X  X  ( X  X  X ) mark statistically significant differences with the Wikipedia-based and corpus-based predic-tors, respectively, with p  X  0 . 05 ( p  X  0 . 1 ). Boldface marks the best result in a row. average precision (AP@1000) and the ground truth AP@1000 determined by using TREC X  X  relevance judgments. Statis-tically significant differences of prediction quality are deter-mined using the paired t-test.

We compared the prediction quality of using three sets of predictors. The first is our (42) proposed Wikipedia-based corpus-independent predictors. We also use the query length as a predictor which results in 43 predictors.

The second set of predictors is composed of three families of state-of-the-art pre-retrieval corpus-based predictors [11] applied to the corpus at hand ( D ). These use the following statistics for a single query term: (i) inverse-document fre-quency (IDF) [8, 12, 11], (ii) variability of its tf-idf value in documents in the corpus in which it appears (VAR.TF.IDF) [22], and (iii) its similarity to the corpus (SQC) [22]. To aggregate the term-based statistics across the query terms, for each of the three families of predictors, we used the av-erage, standard deviation and maximum functions. Overall, 9 predictors were created for this set.

The third set of predictors that we consider is a merge of the first two sets (i.e., our Wikipedia-based predictors and the previously proposed corpus-based predictors).

We integrate the predictors in each set using Lasso re-gression [9] so as to predict AP@1000. Leave-one-out cross validation is performed over the query set. The prediction quality numbers (RMSE) are presented in Table 2.

We see in Table 2 that the corpus-based predictors out-perform, to a statistically significant degree, the Wikipedia-based predictors for three out of four collections. This find-ing should come as no surprise as the Wikipedia-based pre-dictors do not analyze the corpus upon which search is per-formed. For ClueWeb, the Wikipedia-based predictors out-perform the corpus-based predictors, but not to a statisti-cally significant extent. (This could be the result of ClueWeb containing Wikipedia pages.) More importantly, the inte-grated set of Wikipedia-based and corpus-based predictors results in improved prediction quality. Specifically, for the Web collections, the integration of the two sets yields sub-stantial and statistically significant improvements of predic-tion quality over using either set alone.

Analysis of the relative effectiveness of the Wikipedia pre-dictors reveals interesting insights. The most effective pre-dictor was T N W P , which was used by the regressor for all four collections. The other leading predictors were LBG , P DC and P M I , each used for three out of the four collec-tions. We believe that the fact that all aspects of Wikipedia: page titles, links, categories and page text, were used for pre-diction attests to the merits of using Wikipedia for query-performance prediction. Table 3: Prediction quality for TREC runs.  X  X  X  ( X  X  X ) and  X  X  X  ( X  X  X ) mark statistically significant dif-ferences with the Wikipedia-based and corpus-based predictors, respectively, with p  X  0 . 05 ( p  X  0 . 1 ). Boldface marks the best result in a row.
Insofar, we predicted the performance of language-model-based retrieval. To evaluate prediction quality for a variety of retrieval methods, we use TREC runs. Specifically, we perform the evaluation with runs submitted to  X  X OBUST-based X  tracks (TREC7, TREC8, TREC12) and ClueWeb-based tracks from 2009-2011 (denoted Clue09, Clue10 and Clue11), as these represent two different types of corpora, namely, newswire and Web.

We focus on two  X  X seudo X  runs: (i) a median run, where the AP@1000 value per query is the median of the AP@1000 values of all runs for that query; b) a top5 run, where the AP@1000 per query is the average AP@1000 values for that query posted by the five best (MAP) performing runs in a track. Thus, the prediction quality for these two pseudo runs reflects the ability to predict the performance of median and highly effective retrieval methods.

To predict the performance of the runs in each track, we trained the Lasso regressor over the queries in the other two tracks that use the same collection; e.g., to predict the performance for TREC7, we trained over the TREC8 and TREC12 queries. For the training phase, we used the query-likelihood retrieval model as was the case in Section 4.1.
Table 3 presents the prediction quality numbers. Over-all, the results are consistent with those presented for the language-model-based retrieval. For the ROBUST-based tracks, the integration of Wikipedia-based and corpus-based predictors can be of merit, although the improvements are rarely statistically significant. For the ClueWeb tracks, the integration often posts statistically significant improvements. Furthermore, in some cases for ClueWeb, the Wikipedia-based predictors outperform the corpus-based predictors.
We presented a novel corpus-independent approach to pre-retrieval prediction of query performance. That is, the ef-fectiveness of retrieval performed in response to a query is predicted without analyzing the corpus upon which search is performed. Our approach is based on quantifying prop-erties of the query using Wikipedia. Empirical evaluation demonstrated the merits of our approach.

For future work, we plan to integrate our prediction meth-ods with state-of-the-art post-retrieval predictors [14]. In addition, we intend to evaluate the quality of our predictors in a cross-corpus experimental setting as in [6].
We thank the reviewers for their comments. This paper is based on work supported in part by the Israel Science Foun-dation under grant no. 433/12, by Google X  X  and Yahoo! X  X  faculty research awards, and by an IBM Ph.D. fellowship.
