 1. Introduction class of applications.
 superior performance compared to a sequential scan, irrespective of dimensionality.  X  of other types of queries on uncertain data using DuoWave. Our contributions are summarized as follows.  X   X  algorithm. The strong pruning power of the algorithms is validated in the experimental study.  X 
We show how DuoWave can be utilized to process a number of other types of queries.  X  scan, especially when the dimensionality is high.
 experimental results and Section 9 concludes the paper and presents future work. 2. Related Work 2.1. Processing queries on uncertain data without an index papers focus on computing query answers efficiently, but do not propose index structures. 2.2. Indexing techniques on uncertain data unclear how to extend the schemes to multidimensional data.
 uncertain values of the objects.
 for data with categorical domains, but do not apply to infinite data domains. dimensionality is more than ten. Moreover, the U-tree only addresses range queries. compare our technique with APLA-sequential in the experimental study. 2.3. Processing queries on certain multidimensional data
Lin [28] proposes a novel indexing structure of line segments based on compressed B address multi-granularity indexing and query processing to support web query expansion [32] . 2.4. Approximating with wavelets and histograms the correctness of the answers. 3. Data model and problem de fi nition summarized in Table 2 . 3.1. Data model represented by a single point x =  X  x 1 , x 2 , ... , x d values of X are within an uncertain region X . ur ={[ X . l pmf . Let X . m denote the number of possible values of X . Then we have denoted as pdf , to represent its probability distribution, and we have 3.2. Problem de fi nition
De fi nition 1. Given a d -dimensional rectangle R ={[ R . l 1 , R . u associated with continuous probability distribution ( X . pdf ( x )), the probability
If X is associated with discrete probability distribution X where The range query on uncertain objects is defined as follows.

De fi nition 2. Probabilistic range (PR) query Given a set DB of uncertain objects and a d -dimensional rectangle R probability of residing in R Q (i.e., P X R &gt; 0).
 follows.

De fi nition 3. Threshold probabilistic range (TPR) query Given a set DB of uncertain objects, a d -dimensional rectangle R in DB whose probability of residing in R Q is greater than P
To define the similarity query on the uncertain data model, we need to introduce the concept of object x and an uncertain object Q as follows.

De fi nition 4.  X  -Similarity between a certain object and an uncertain object
Given an uncertain object Q and a tolerance distance  X  i ( probability distribution ( Q . pdf ( q )), the probability of a certain object x = where
If Q is associated with discrete probability distribution ( Q . pmf ( q )), the probability of x being where  X  (,) is the same as de fi ned in Eq. (3) .
 distance  X  i in every dimension i . The smaller  X  i is, the more strict the similarity definition is. Now we can introduce the definition of  X  -similarity between two uncertain objects.
De fi nition 5.  X  -Similarity between two uncertain objects
Given an uncertain query object Q and a tolerance distance with continuous probability distribution ( X . pdf ( x )), the probability of X being
If X is associated with discrete probability distribution ( X . pmf ( x )), the probability of X being value of Q , e.g., q [1], we obtain a range R 1 centered at q [1] with the side length of 2  X  0 : 5 0 : 3  X  0 : 7  X  X  X  0 : 1 0 : 7  X  0 : 57.
 Now we can define the similarity query on uncertain objects.

De fi nition 6. Probabilistic  X  -similarity (PS) query
Given a database DB, a similarity query with an uncertain query object Q and a tolerance distance i , find every uncertain object X in DB that has a positive probability of being
Usually, we are also only interested in objects with P X S follows.

De fi nition 7. Threshold probabilistic  X  -similarity (TPS) query
Given a database DB, a similarity query with an uncertain query object Q , a tolerance distance and a threshold P Q (0  X  P Q  X  1)), find every uncertain object X in DB whose probability of being (i.e., P X S &gt; P Q ).
 algorithms for TPR and TPS queries. 4. The DuoWave index these steps are given as follows. 4.1. Approximating probability distribution approximate it.
 distributions. This further distinguishes our work from [19] .
 probability distributions of uncertain objects. buckets. Then we denote a histogram in dimension i by a probability sequence S of a probability density function or a probability mass function being approximated by the histogram S objects, and the sum probability of all other buckets is negligible. we denote a histogram in dimension i by a width sequence W bucket.
 no way to partition X into H buckets so that each bucket contains exact 1 which are the buckets with the maximum and minimum probability of X , respectively. Let p  X  portion  X  of X . Then we denote the equi-depth histogram in dimension i by a width sequence W p investigation as future work. 4.2. Compression with wavelet transform wavelet coefficients.

As stated above, in this subsection we mainly focus on the probability sequences ( S wavelet transform can be found in textbooks such as [38] . Consider the probability sequence S second) divided by 2 is stored as a coefficient, so we have four coefficients [0.01, on the resultant sequence until reaching resolution 1. Then we obtain the decomposition of S original sequence. The wavelet transform (we omit  X  Haar  X  of the leaf nodes from the internal nodes. A coefficient ^ leaf nodes the affected leaf nodes of ^ c i ; m . For the example in Fig. 3 , p ^ c affects its left and right subtree in the following way. The value of sequence element in the same resolution to derive the coefficients in ^ c
Lemma 1. Let leaves ^ c i ; m be the affected leaf nodes of a coefficient affected leaf node p i , j in the following way. If the value of level L is divided by  X 
T =0.005, then ^ c 1 ; 4 (0.004) and ^ c 1 ; 6 (0) are omitted. Further, we can always omit storing coefficients as the abridged wavelet coefficients . 4.2.1. Extension to equi-depth histograms
Then we normalize with a coe-bitmap, a threshold T a and a set of abridged wavelet coefficients. 4.3. Dimension selection strategies for selecting the most important dimensions are described below. X . ur ={[0.3,0.8],[0.7,0.8]} and a uniform probability distribution. Given a query range R and X . ur in both dimensions are the same, we have p 1 b chosen, because approximating p 2 with 1 brings less overestimation than approximating p implementation, we use the first strategy to select the dimensions and the second strategy to break ties. 4.4. Summary 4.5. Insertion, update and deletion 5. Processing TPR queries determine the final answer.
 A TPR query specifies a query range R Q and a probability threshold P an answer and we can safely discard it. If X . ur intersects R the probability of X being in R Q , denoted as P X R .If P X objects efficient due to sequential access.
 We defer the full investigation of such a possible multi-dimensional bound as future work. As stated in Section 4.1 , we focus on deriving the upper bound bound for the approach utilizing equi-depth histogram in Section 5.4 . 5.1. Deriving upper bound from histograms dimension, we can derive an upper bound of the probability of X being in R
For X . ur to intersect R Q , X . ur must intersect R Q in all dimensions. Let { p shows an example. In dimension 1, the probabilities in buckets intersected by R Similarly SIP X ; 2  X  X  X   X  7 j  X  3 p 2 ; j .
 Let the actual probability of X being in R Q be P X R . Then we have Lemma 2 . Lemma 2. min i =1 d SIP ( X , i ) is a tight upper bound of Proof 1. First, we prove that min i =1 d SIP ( X , i ) is an upper bound of For any of these possible values of X to be in R Q , this value has to be in the extent of R least P X R of X 's possible values in R Q in every dimension, which means that SIP X Second, we prove that min i =1 d SIP ( X , i ) is a tight bound of particular, we can always construct a query R Q and an uncertain object X that makes following way. Let i m be the dimension that has the min i =1 buckets that intersect R Q in dimension i m , let x satisfy x satisfies R Q in all dimensions. Therefore, there is at least a min with the same probability 0.1. In this example, SIP X ; 1  X  X  X  Dimension 1 has the min i =1 d SIP ( X , i ) which is 0.6. Every point that satisfies R uncertain object that satisfies the above description, since we know that there is at least min least min i =1 d SIP ( X , i ). On the other hand, min d i  X  1 Because there exists a case for which P X R  X  min d i  X  1 a tight bound of P X R . 5.2. Deriving upper bound from abridged wavelet coef fi cients
If there are no wavelet coefficients stored for a dimension, SIP X
Step 1 Recall that we obtain the abridged wavelet coefficients by omitting
Step 2 The second step is to derive the upper bound of SIP ( X , i ) from
Lemma 3. Given a TPR query b R Q , P Q &gt; , in dimension i a coefficient increases by  X  m , then SIP ( X , i ) changes to SIP ( X , i )
Proof 2. Let the probabilities in ^ c i ; m 's affected leaf nodes be p in both ^ c i ; m 's left subtree (right subtree) and { p
By our notation, lcnt =| LS |and rcnt =| RS |. The ( lcnt + rcnt ) elements in LS by ^ c i ; m .Basedon Lemma 1 , we know that when ^ c i ; m increases by
Similarly, the sum of the probabilities in RS changes to Then the value of SIP ( X , i ) changes to
Based on Lemma 3 ,if lcnt = rcnt , the value of ^ c i ; m c  X  T a respectively. If lcnt &gt; rcnt , SIP ( X , i ) increases as lcnt b rcnt , SIP ( X , i ) decreases as ^ c i ; m increases, so letting
After setting all the omitted coefficients in the above way, we obtain an from ^ S 's buckets intersected by R Q in dimension i and let us denote this sum as SIP ( X , i )
SIP ( X , i ), denoted as SIP X ; i  X  X  . The detailed steps for computing SIP X Lemma 4. SIP X ; i  X  X  is a tight upper bound of SIP ( X , i ).
 to prove that SIP X ; i  X  X  is a tight upper bound of SIP ( X , i ).
In particular, we prove that there exists a case for which SIP ( X , i ) can reach SIP X object X that makes SIP ( X , i ) reach the value of SIP X buckets, H , is a power of 2. Let each of the first H 2 probabilities of S p  X  4 5 H for j  X  0 ; 1 ; ... ; H 2  X  1 and p i ; j  X  6 5 H buckets of X in dimension i ,so R Q intersects  X  H  X  1 j  X  H
We perform wavelet transform on S i X and obtain
Algorithm 1 to derive the upper bound of SIP ( X , i ). First, we restore ^ c ^ S  X  1 H ;  X  1 5 H ; 0 ; 0 ; ... ; 0 . Second, we perform reverse wavelet transform on thesameastheoriginalhistogram S i X . Finally, we compute the sum of the probabilities of in S
Because there exists a case for which SIP X ; i  X  X   X  SIP X bound of SIP ( X , i ). 5.3. Whole process of computing upper bound
From the abridged wavelet coefficients, we can compute SIP X minimum SIP X ; i  X  X  among all dimensions as the upper bound of to prune unqualified objects. We can prove that this bound is tight. Theorem 1. min d i  X  1 SIP X ; i  X  X  is a tight upper bound of
Proof 4. Based on Lemma 4 , SIP X ; i  X  X   X  SIP X ; i  X  X  for every dimension i . Based on Lemma 2 , SIP X
Therefore, SIP X ; i  X  X   X  P X R for every dimension i , and so min d Now we prove that min d i  X  1 SIP X ; i  X  X  is a tight upper bound of object X that make P X R reach the value of min d i  X  1 SIP X the counter-examples constructed in Lemma 2 and Lemma 4 at the same time. Let i
Let R Q ={[ R . l 1 , R . u 1 ], ... ,[ R . l d , R . u d the second H 2 buckets in dimension i m be in R Q in all dimensions. The above described R counter-examples in the proofs of both Lemma 2 and Lemma 4 . Therefore,
SIP ( X , i m )and SIP ( X , i m )reachesthebound SIP X ;
Since there exists a case for which P X R  X  min d i  X  1 SIP X of
Algorithm 1. ComputeUPSIP( i , R . l i , R . u i ) 5.4. Extension to equi-depth histogram histogram of each dimension i with a width sequence W i X bucket j  X  i be the first and last buckets intersected by R bound of P X R .
 respectively. In particular, we derive the lower (upper) bound for j reverse wavelet transform based on (minimum) integer satisfying X : l i  X   X  j i  X  1 j  X  0 w i ; j
After deriving the lower bound of j i and upper bound of j efficiency of these algorithms as future works. 6. Processing TPS queries refine paradigm. A TPS query consists of an uncertain query object Q
Consider the hyper-rectangle R ext ={[ Q . l 1  X   X  1 , Q . u intersect R ext , X must not be an answer and we discard it. If the X . ur intersects R derive an upper bound of the probability of X being  X  -similar to Q , denoted as the answer set. We discuss how to use an index entry of DuoWave to derive an effective upper bound only help determine whether X satisfies the range requirement, but not the distribution requirement. values of Q . Next, we propose a bound which may not be tight, but is much lighter to compute. 6.1. Deriving an upper bound function
This function is a relaxed version of  X  ( x , q ) defined in Eq. (5) ;  X  ( x , q ) represents a set of x values that is a subset of that distributions, for any dimension i ( i =1,2, ... , d ), we have dimensional predicates to range queries.
 bucket as [ ql i , f , qu i , f ], where f =0,1, ... , H  X  (bucket) of Q one by one. Continue with Eq. (16) , we have for each dimension i .

Note that in Eq. (17) ,wecancompute  X  q  X  Q : ur ; q is actually the probability of X being in the range [ ql i , f bound for this probability from the DuoWave index of X . Then we can obtain an upper bound for given in Algorithm 2 .

Algorithm 2. ComputeSPUB( Q ,  X  , X ) 7. Other types of queries how DuoWave can be applied to process them. 7.1. Probabilistic dominating queries query object higher than a threshold P Q .
 probabilistic skyline query does not specify certain query object, while PD and TPD queries do. query. After computing the probability upper bounds, we can use them to filter unqualified objects. 7.2. Top-k probabilistic queries any object X  X  S and any object Y  X  DB  X  S , P X R  X  P Y R candidates as we go through the list. The actual probability of the k 8. Experiments generation and query generation are given in Section 8.1 .
 presented is the average of 100 queries.
 we deactivate the operating system buffering to achieve more rigorous test results. 8.1. Generating datasets and queries 8.1.1. Real data distribution of X is represented as m possible points, each associated with a probability 1 16, the value of the ( i +8) th dimension is generated by multiplying the value of the i as described above. To generate more objects, say a new object X add some variance to the attribute values of X 's records to obtain the records of X 8.1.2. Synthetic data 8.1.3. Queries
For TPR queries, the center points of query rectangles R Q queries, an uncertain query object is an uncertain object, randomly chosen from the database; probability threshold P Q is determined based on the uncertain query object and experiments. 8.2. Effects of parameters parameter values are used in the comparative studies in Sections 8.4 and 8.5 . 8.2.1. Effect of H
We vary H from 4 to 64 and test both kinds of queries on real and synthetic datasets. The values of flattens after H goes beyond 16 as shown in Fig. 6 (a) and (d). of the above experiments. 8.2.2. Effect of T a and  X 
DuoWave index needs to be kept small to retain a quick response time. When we vary T portion (5% in our implementation) of the dataset size by tuning are stored in the DuoWave index. Increasing T a and decreasing coefficients and decrease the size of DuoWave index.

As shown in Fig. 7 (a) and (d), for both types of queries when T decreases and then increases. Overall, the change is small. Note that increasing T for that dimension. Therefore given a fixed  X  , a larger T overall probability upper bound and stronger pruning power when T very little. Both types of queries considered the best pruning power appears when T corresponding  X  is in  X  .
 we use 0.01 and 0.75 as the chosen values of T a and  X  , respectively in all the remaining experiments. 8.2.3. Summary the datasets, 16 is the recommended value for H .

After setting value for H , the best performance of DuoWave appears when the values of T 8.3. Comparison on insertion, update and deletion time (a) insertion (b) update (c) deletion is even negligible. 8.4. Comparison on TPR queries segments in [9] ) (ii) using float form to store the numbers (originally 8.4.1. Dimensionality the constraint of space, the results of the anti-correlated synthetic dataset are omitted in the figure. number of I/Os and response time as dimensionality grows.
 performs better than APLA in both total response time and number of I/Os. the total response time of the U-tree is much larger than sequential scan.
DuoWave consistently beats sequential scan in all the tests, irrespective of the dimensionality. 8.4.2. Query selectivity We vary the selectivity of the TPR queries from 0.01% to 1% by changing the value of P present the results of the real dataset in Fig. 10 .
 in which case a large number of objects are visited in the refine phase. evidently as the selectivity increases.
 which leads to a much worse response time than sequential scan due to random accesses. 8.4.3. Dataset cardinality (a) Object visited, real data (b) Response time, real data (a) Object visited, real data (b) Response time, real data 8.5. Comparison on TPS queries 8.5.1. Dimensionality datasets have larger file size than the real dataset. 8.5.2. Query selectivity We vary the selectivity of the TPS queries from 0.01% to 1% by changing the value of P sequential scan in all tests. 8.5.3. Dataset cardinality sequential scan and the performance scales well with dataset cardinality. 9. Conclusion significantly outperforms state-of-the-art techniques, especially in high-dimensional spaces. (a) Object visited (b) Response time (c) I/O Acknowledgment supported in part by the National Science Foundation of China (NSFC Grant Nos. 60970124 and 61170034).
References (a) Object visited (b) Response time (c) I/O
