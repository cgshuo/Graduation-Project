 Simulation provides a powerful and cost-effective approach to explore and evaluate how interactions between a searcher and system influence search behaviour and performance. With a growing interest in simulation and an increasing number of papers using such an approach, there is a need for a flexible framework for simulation. Thus, we present SimIIR , an open-source toolkit for building and conducting Interactive Information Retrieval (IIR) experiments. The framework consists of a number of high level components, including the simulation, the searcher and the system, all of which must be configured. The SimIIR framework provides a series of interchangeable components. Examples of these components include the querying strategies (how simulated queries are formulated) and stopping strategies (the depth to which a searcher will examine snippets and documents) that a simulated searcher will employ. We have implemented various existing strategies so that they can be used by other researchers to not only replicate and reproduce past exper-iments, but also create new experiments. This paper de-scribes the SimIIR framework and the different components that can be configured and extended as required.
 Keywords: Simulation, Search Behavior, Strategy, Stop-ping Strategies, Continuation Strategies, Querying Strate-gies, User Modeling
The process of search is inherently interactive. During a search session, a searcher can issue a number of queries, examine a number of snippets, and assess documents for relevance to their information need. Despite this, the In-formation Retrieval (IR) community has centred much of its research around the so-called Cranfield Paradigm . Re-volving around the concept of standardised test collections and relevance judgements, the paradigm makes a number of key assumptions that are at odds with the interactive na-ture of search. Assumptions include a searcher: (i) issuing a single query; (ii) examining each document in turn and in-dependently; and (iii) examining snippets and documents to a fixed depth. Simulation, as outlined by Keskustalo et al. [10], provides a means to go beyond the limitations presented by the Cranfield paradigm without conducting an expensive and time consuming user study. Simulation also offers a number of other benefits over user studies. The approach provides a rapid means to exploring  X  X hat-if  X  sce-narios, and also facilitates a range of evaluations, such as a comparison between systems and understanding searcher behaviours [4]. Simulations enable a range and variety of searchers to be created who do not suffer from issues such as fatigue or learning effects, unless specifically coded to do so. Simulated searchers are therefore highly controlled, and can therefore yield reproducible results [4].

With a growing interest in simulation and a growing num-ber of researchers employing the technique in their work (e.g. [1, 2, 3, 6, 7, 9, 13, 14, 15, 17]), we argue that there is a need for an IIR-based simulation toolkit. A toolkit freely available to the IIR community will ensure that experiments conducted with it can be easily reproduced, as well as re-ducing the major overheads for creating simulations. To address this issue, we introduce an open-source framework for the simulation of IIR ( SimIIR ). Over the past two years, we have been building the SimIIR framework and used it to produce a number of different simulations [13, 14] -with more to come. The framework can be configured in a vari-ety of different ways to support different search processes, different searcher configurations, different experimental con-ditions, and different search engines.
Previous studies employing simulation have either: (i) considered aspects of the IIR process in isolation, such as query generation (e.g. [11]); or (ii) considered the search process as a whole (e.g. [14]). SimIIR is designed to capture and broadly reflect the complex processes involved within the wider IIR process, and has been developed from the ground up in the Python 1 programming language using a highly modularised, class-based framework. Each compo-nent broadly represents a major stage in the IIR process 2 Figure 1: An example of three topic descriptions used within the SimIIR framework. The first line denotes the topic title, with everything following the title considered the topic description. with new components simply inheriting from the relevant base class. This allows for the easy experimentation of dif-ferent simulated searchers using a variety of test collections, querying strategies, stopping strategies, and more. This also highlights one of the main advantages of SimIIR in that it can be easily adapted to suit ever more advanced compo-nents as the research in this area progresses. SimIIR has already been employed in several publications investigating stopping strategies by Maxwell et al. [13, 14].
 SimIIR Components: In order to successfully run an ex-periment on SimIIR, a simulation must first be defined. A simulation is comprised of a series of topics , a search in-terface/engine , an output controller , and a series of simulated searchers -all of which are elaborated on in subsequent sections. A simulation in essence is loosely asso-ciated with the concept of a real-world experiment, such as a user study. The experiment would be comprised of a series of searchers or subjects, examining documents for relevance over one or more topics. The parameters for each of these settings are specified in an XML simulation configuration file and passed to SimIIR. The experiment itself consists of a series of runs, the total number of which can be calcu-lated by summing the combinations of the specified number of searchers and topics. The final component comprising a simulation is the list of simulated searchers. Every simulated searcher is specified within a separate XML user configura-tion file . Within this file, a host of additional components are defined, namely: a querying strategy/generator ; a classifier/decision maker for both snippets and doc-uments ; a stopping strategy ; a logger ; and a search context . The components which comprise the searcher are encoded within the underlying searcher model , providing the various actions that are observed.
We define a topic as a description of what a simulated searcher is expected to find relevant. These are specified by a series of topic description files , consisting of a title and description . Figure 1 illustrates examples of such files. All the topic files that have been generated for SimIIR thus far are based upon the topics provided by various TREC tracks, such as the TREC 2005 Robust Track .
 Figure 2: An example snippet of the output log file generated by SimIIR. Included is the action per-formed by the simulated searcher (e.g. QUERY , SNIP-PET ), the total time available ( 1200 seconds), the elapsed time of the simulated session, and the judge-ment for the action (if applicable), such as SNIP-PET_REL for when a snippet is considered relevant.
We consider the search interface/engine component as an abstraction of a search engine and the Search Engine Results Page (SERP) . After issuing the search interface/engine with a query, the component provides SimIIR with access to the SERP -a ranked list of snippets and associated documents. As the interface is highly genericised, any search engine with a Python wrapper can be easily coupled to the SimIIR framework. Presently, only a wrapper for the Whoosh IR toolkit 3 has been implemented. Any additional configura-tion options for a particular search interface/engine can also easily be set in the simulation configuration file.
SimIIR also provides a flexible output controller with a variety of output options for the simulations that are run with the framework. Options are based upon the output files that are saved for each simulated run, and include op-tions to save the interaction log of the simulated searchers (e.g. queries issued, snippets examined, documents exam-ined), and whether to save the list of documents considered relevant by the simulated searchers. The file specified by the final option may then in turn be fed into an evaluation program such as trec_eval 4 to obtain the values for the various measures and metrics that can be computed.

For each run, output files include: a .log file, which con-tains the complete set of interactions undertaken by the sim-ulated searcher (refer to Figure 2 for an example of an output log); a .cfg file, containing the configuration of components for a given simulated searcher; an .out file, containing out-put from an evaluation program such as trec_eval (if en-abled); a .queries file, containing a line break separated list of queries issued; and a .res file, containing a list of the documents considered relevant (in the format of a standard TREC results file). Runs are identified by unique identi-fiers specified in the simulation configuration and searcher configuration files to avoid results being overwritten.
The querying strategy/generator is the component respon-sible for the generation and selection of queries. A variety of different simulated query generation strategies exist, such as the approaches discussed by Baskaya [5] and Keskustalo et al. [11]. Queries also need not be generated -a list of queries issued by real-world searchers can be used, if desired. This functionality has been already implemented through a PredeterminedQueryGenerator class, which takes as input a list of queries to be issued. This is useful for comparing the performance of real-world searchers and simulated searchers under similar conditions, as used by Maxwell et al. [14].
In addition to the generator detailed above, a number of query generation strategies have been operationalised as SimIIR classes. Several of the querying strategies proposed by Baskaya [5] and Keskustalo et al. [11] have already been implemented, such as the SingleTermQueryGenerator that generates single term queries, the BiTermQueryGenerator that generates two term queries, and the TriTermQuery-Generator that generates three term queries. All of these classes generate queries from the provided topic title and de-scription text. Work is also underway to incorporate terms from sources other than the topic title and description, such as previously examined snippets and documents. This will result in a wider potential vocabulary for query generation.
We next describe another major component of the SimIIR framework -classifiers . These components are responsible for determining the attractiveness of a given snippet -or the relevancy of a document -to the provided topic that the simulated searcher is tasked to examine. Snippet and doc-ument classifiers can be specified individually to enable the simulator to take into account the fact that searchers may judge snippets differently from documents. As an example, a searcher may be more liberal when deciding the attrac-tiveness of a snippet as opposed to a document X  X  relevancy.
A series of snippet/document classifiers have already been implemented and are ready for use. These include: a  X  X REC-style X  classifier, which judges everything to be relevant; an InformedTrecTextClassifier that uses TREC QRELS and probabilities (acting stochastically) to determine if a snip-pet/document should be considered attractive/relevant; and a LMTextClassifier which uses a language model (specified by optional configuration parameters) that acts determinis-tically to determine attractiveness and/or relevancy.
The stopping strategy decides where a simulated searcher should stop examining a list of ranked documents. This concept is called query stopping [14], as opposed to session stopping , which is presently determined by the logger com-ponent (refer to Section 2.7). A variety of stopping strategies have been previously proposed in the literature, such as the disgust and frustration point rules [8, 12].

As two recent publications used the SimIIR framework to simulate the effects of stopping strategies [13, 14], a va-riety of strategies have already been implemented. As an example, these include: a FixedDepthDecisionMaker , im-plementing a na  X   X ve approach where simulated searchers will stop after examining x snippets regardless of relevance; a NonrelDecisionMaker , which operationalises the aforemen-tioned disgust and frustration point rules [8, 12], where sim-ulated searchers will stop after judging x snippets non rel-evant; and an IftBasedDecisionMaker , a stopping strat-egy which operationalises the implicit stopping rule encoded within Information Foraging Theory (IFT) [14, 16].
A logger is the component of the SimIIR framework that determines the costs of interaction for the simulated searcher when performing actions, such as issuing a query. Interac-tion costs can be defined in the user configuration file. The logger therefore tracks and records all interactions that take place within a simulated search session, and is for example able to state when a predetermined time limit elapses -thus providing some form of session stopping. The details stored within the logger component can then be sent to the out-put controller component upon completion of the simulated search session to produce the .log output file.

To date, we have fully implemented a FixedCostLogger that considers each of the different actions undertaken by a simulated searcher, and as the name suggests, imposes a fixed cost upon each of them. Currently, a variable cost logger is in the early stages of development. This more ad-vanced logger will for example provide the ability for SimIIR to impose variable costs that are dependent upon factors such as query or document length, for example [7, 18].
The search context is the component of the SimIIR frame-work responsible for keeping a record of all issued queries, all examined snippets, documents (along with the judgements for each) and other interactions throughout a search ses-sion. The search context is in essence the  X  X emory X  of the simulated searcher, and interacts closely with the specified logger component to record all events (refer to Section 2.7). By referring to the search context, a snippet classifier can for example determine what snippets the simulated searcher has previously seen, whether they were considered relevant or non-relevant, and what text was observed.

From the basic search context, we have derived a relevance revision search context, as used by Maxwell et al. [13, 14]. When considering a snippet relevant, a simulated searcher using the revised relevance search context can then revise its judgement of said snippet if the associated document is subsequently deemed to be not relevant. Such a technique can influence the stopping point of the simulated searcher if using a stopping strategy based upon the frustration point and disgust stopping rules [8, 12] (refer to Section 2.6) for instance. We envisage that as work in the area of IIR simula-tion progresses, more complex search contexts can be imple-mented. For example, one such approach could be a  X  X ossy X  search context, where a simulated searcher would forget over time what snippets and documents have been examined.
When SimIIR is started, all the components are instanti-ated based upon the simulation and user configuration XML files. The process in which SimIIR simulates IIR is based upon the Complex Searcher Model (CSM) , a model of inter-action proposed by Maxwell et al. [14]. The model, repre-sented as a flowchart in Figure 3, is based upon the stochas-tic model presented by Baskaya et al. [6], but includes ad-ditional decision points as described by Thomas et al. [19]. While the CSM does not presently represent every aspect of the IIR process, it does provide a better representation than has been used previously. The model can be extended as research in this area progresses.

Essentially, the simulated searcher begins by (1) exam-ining the given topic and title description. From the title and description, the simulated searcher then (2) generates a series of queries which are issued to the underlying search engine. The simulated searcher then (3) issues a query from the generated list, and then (4) proceeds to examine the first/next snippet in the ranked list provided. The simulated searcher can also decide to issue a new query, thus returning to (3) . If the snippet is considered relevant by the simulated searcher, (5) the document is then examined in full. If the document is also considered relevant, (6) the document is then marked relevant. If either the snippet or document are considered non-relevant, the simulated searcher then returns to (4) with the document unmarked. In this demonstration paper, we have described our new IIR simulation framework, SimIIR. The highly modularised architecture of the framework makes it straightforward to implement new simulation components, and will aid in push-ing research forward in this area. Future work will see the development of more advanced components and the adap-tion of the CSM to facilitate this.

