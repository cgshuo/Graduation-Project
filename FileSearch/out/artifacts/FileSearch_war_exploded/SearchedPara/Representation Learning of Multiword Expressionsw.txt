 Multiword expressions (MWEs) are expressions consisting of two or more words, which can be noun phrases such as strong tea , verb phrases such as break up , or idioms such as kick the bucket [8]. Multiword expressions are important semantic units to express cer-tain specific meanings, especially the non-compositional MWEs. MWEs play important roles in diverse applications such as machine translation and sentiment analysis [15]. According to [19], 57% of sentences from web pages contain at least one MWE. Gen-erally speaking, MWEs can be categorized as either compositional such as traffic light, fresh air , whose semantics are composed from the semantics of its component words, or non-compositional such as idiomaticity such as couch potato and kick the bucket , whose semantics are not directly composed from its component words. In this paper, we use the term external context to refer to the context words surrounding a MWE. Recently, distributed representation of words as dense and low-dimensional vectors, referred to as word embedding or word vector , has achieved better results on vari-ous NLP tasks [4] than symbolic representations which have limited power to directly encode semantic information. Different models are also proposed to learn distributed representation of longer language units, such as phrases [12,25], sentences and docu-ments. Studies on learning MWE representation, which falls into the phrase category, are mainly divided into two approaches. distributional hypothesis that words appearing in similar context tend to have similar meanings [6]. This is also the same principle used for word embedding. To apply this principle for MWEs, a MWE is simply treated as a single term and its representation is inferred from its external context in the same way as learning word representation [12,24]. Since this approach treats a MWE as a non-divisible unit, its component words are completely ignored even though this information may be useful, especially for com-positional MWEs. For example, the MWE close interaction is semantically similar to contact , which can be reflected through the word embedding similarities between the component word interaction and contact . But, by treating close interaction as one non-divisible unit, its representation has to be learned independently which is more likely to suffer from data sparseness problem. In the case of an infrequently used MWE which would have insufficient context, this approach can fail to learn their representations. Compared to single words, the sparseness problem of MWEs is indeed more severe given the same corpus.
 principle of compositionality to determine the meaning of a MWE using the combined meanings of its constituents. Based on this principle, this approach employs certain composition function to obtain the representation of a MWE from the representations of its component words [14,26,25]. The representation of the component words is obtained using distributional models. This approach only uses information of component words and the information of its external context is not directly considered. One key problem of the compositional approach is that it can fail if a MWE is non-compositional because the semantics of non-compositional MWE cannot and should not be derived from its component words. For example, the MWE monkey business cannot be composed from monkey and business . In such a situation, the information of the component words can lead to an erroneous result. The unique meaning of non-compositional MWEs can be lost using current word embedding based compositional models.
 information to the representation of a MWE. Furthermore, the usefulness of the compo-nent words depends on the compositionality of the MWE. If there is a way to measure the compositionality of a MWE, the compositionality can then be used to measure the usefulness of the component words of a MWE. Based on the above analysis, we pro-pose a hybrid method to learn the representation of MWEs from their context with the compositionality constraint. This method can make use of both the external context and component words. Instead of simply combining the two kinds of information, we use the compositionality measures from lexical semantics to serve as a constraint. nation of external context with a weighted composition of the component words where the weight is based on automatically predicted compositionality. Compared to previous works, our model has the advantages of both previous methods while overcomes their drawbacks. Evaluation based on 3 MWE tasks first shows that compositional method does work better for compositional MWEs while the distributional method works better for non-compositional MWEs. Most importantly, the evaluation shows that our hybrid model gives the overall best performance and is more robust for both compositional and non-compositional MWEs.
 Section 3 presents our proposed model. Section 4 gives performance evaluation, and Section 5 concludes this paper. Learning of distributed representation of words can be categorized into counting-based methods and prediction-based methods [1]. Both methods are based on the distribu-tional hypothesis [6]. Counting-based methods perform matrix factorization on the word-context co-occurrence matrix. For example, Latent Semantic Analysis (LSA) per-forms Singular Value Decomposition (SVD) on the word-context co-occurrence ma-trix to obtain a low dimensional word vector representation. Glove [16] uses another kind of matrix factorization to directly factorize the co-occurrence matrix into two low-dimensional matrices so that the multiplication of the two matrices is close to the orig-inal entry in the co-occurrence matrix. On the other hand, prediction-based methods use neural network models to learn latent representations of words under certain ob-jective functions. For example, the Skip-Gram model tries to maximize the conditional probability of context given a target word and this probability is represented as a vec-tor function of the target word and its context [12]. It is proven that counting-based methods and prediction-based methods are essentially equivalent, differing only in a constant shift [11].
 phrase representations. One approach treats a phrase as a single unit and learns phrase representations from its external context using the same word representation learning model [12,24]. As for compositional models, there are a number of different methods including addition and multiplication of the component vectors [13]. Baroni [2] pro-poses to represent a noun as a vector and an adjective as a matrix and use matrix-vector multiplication to obtain the representation of adjective-noun phrases. Some proposed task specific and supervised composition models include recursive neural networks by matrix multiplication on concatenated component vectors [20]. Yu [25] proposes to ob-tain phrase representations by the weighted sum of word vectors and the weights are a tensor based composition model to learn phrase representations by vector-tensor-vector multiplication. The difference is that the tensor is phrase-type sensitive, which means that learning a tensor for each type of phrases, such as adjective-noun, noun-noun phrase, verb-noun phrase. However, all the above composition based methods assume that phrases and MWEs are compositional. The study by Sun [21] on MWE representa-tion actually makes use of both external context and component words by constraining a WME X  X  vector to be close to the vectors of both its two component words. However, the semantics of many MWEs are not necessarily similar to both of its component words. Some non-compositional MWEs have no relation to its component words. The work from [7] also considers both the external context and component words with compo-sitionality constraint, which is quite similar to our idea. However, this work can only handle verb-noun phrases. Note that all the above composition models focus on two-word MWEs.
 positionality prediction tasks. MWE detection aims to identify MWEs in the text, nor-mally a sequence tagging task [18]. Compositionality prediction aims to identify if a given MWE is compositional or not. Yazdani [23] proposes a semantic composition based method for compositionality detection. Noun compounds that cannot be well modeled by the composition model are considered non-compositional. Another method uses word embedding to predict the compositionality value of MWEs based on the co-sine similarity between the representation of the MWE and its component words [17]. Our proposed MWE representation model includes two parts. The first part is based on a distributional model to learn word and MWE representation using external context. The second part is the composition model to learn MWE representation from compo-nent words. Similar to previous works, we also focus on MWEs that consists of two component words. 3.1 Objectives Let us first introduce some notations. Given a large corpus S with a set of words w  X  V W and their context c  X  V C where V W and V C are the word and context vocabularies. Note that the vocabularies of V w and V C may be identical. The distinction is more for conceptual convenience. The context of word w i is defined as the words surrounding w frequency of word w , and #( c ) denote the occurrence frequency of context c . Now, let #( w,c ) denote the frequency of a word-context pair ( w,c ) . For MWEs,let V M denote the set of given MWEs where each MWE m  X  V M consists of two words. We use t m to denote the compositionality of m and the larger t m is, the more compositional is the WME. Let D denote the set of ( w,c ) and ( m,c ) pairs.
 representation c  X  R d for each context c  X  V C , and a vector representation m  X  R d for each m  X  V M . d is the vector dimension. 3.2 Skip-Gram with Negative Sampling (SGNS) Different models have been proposed to learn the dense word representation as already discussed in Section 2. Here we present the widely used SGNS model -the Skip-Gram model trained using negative-sampling [12]. Consider a word-context pair ( w,c ) . Let p ( D = 1 | w,c ) be the probability that ( w,c ) comes from D and let p ( D = 0 | w,c ) be the probability that ( w,c ) does not. The basic assumption of SGNS is that the conditional probability of p ( D = 1 | w,c ) should be high if c is the context of word w in corpus D and p ( D = 0 | w,c ) should be high otherwise. p ( c | w ) is computed as: The basic idea behind this is that if word w and context c co-occur, their vectors should have close correlation, modeled by the element-wise multiplication w  X  c . The objective of negative sampling is to maximize the conditional probability p ( D = 0 | w,c N ) =  X  (  X  w  X  c N ) by randomly samples negative context c N of w from V C . This can be translated to maximizing  X  (  X  w  X  c ) . So the objective for a single ( w,c ) pair is: where k is the number of negative samples and P D is the empirical unigram distribution P
D ( c ) = is used over the whole set of word-context pairs D in the corpus S . The obtained vector w is the learned representation of w and c is a by-product. Note that the SGNS may have data sparseness problem if the frequency of w is low.
 as a single term and representation learning is the same as learning the word representa-tion. However, data sparseness is an even bigger issue for MWEs, especially for MWEs. Also, for compositional MWEs, SGNS completely ignore the component words. 3.3 Composition Model In a compositional model, the representation of a MWE is inferred from that of its component words. Given a MWE m with two component words w 1 m and w 2 m and their respective vector representations w 1 m and w 2 m , the representation of m , denoted by m , can be computed by a function f : Different composition models are proposed for f [13]. The weighted additive compo-sition model with weights  X  and  X  is defined as a linear composition: The multiplicative composition model is defined by: Compared to SGNS, the compositional model can make use of component words infor-mation. However, this model can produce erroneous representation for non-compositional MWEs, such as couch potato whose meaning cannot be composed from its component words couch and potato . 3.4 The Hybrid Model The distributional model using SGNS suffers from data sparseness problem for MWEs and cannot make use of component words information. The compositional model alone does not make full use of external context and is not appropriate for non-compositional MWEs. In this work, we propose a hybrid model which makes use of the judgment on compositionality of MWEs and propose a model that can make proper use of the com-bined compositional model and the distributional model, denoted as the C&amp;S model. m  X  R d for every m  X  V M , and to learn the vector representation w  X  R d for every w  X  V W . Since learning include both words and MWEs, we first construct the candidate term set V T = V W  X  V M and then build the corresponding context set V C based on the window size L . For a word w , its representation can be learned according to Formula 1 . For a MWE m , the proposed C&amp;S model can be modeled as: In Formula 5 , the first two parts are identical to the SGNS model. The third part is for component words composition with a constant weight  X  to balance the overall contri-butions of the two models. Function h is defined as defines the correlation between the learned MWE representation m and the composed WME representation. The more they are correlated, the larger contribution the third part is to J S . t m is the compositionality of m , which can be computed using the method proposed by Salehi [17] based on word embedding as follows: g is the cosine similarity according to Salehi. m , w m are obtained using SGNS. Theo-retically speaking, Formula 6 has the following properties: 1. If the compositionality t m is low ( m being more non-compositional), the weight 2. If the compositionality t m is high ( m being more compositional), the weight of compositionality prediction model and the composition part f ( w 1 m , w 2 m ) can use any proposed composition model. By setting  X  to zero, our model degrades to the SGNS model. By setting t m to a constant, our model changes to a fixed weight model. 3.5 Model Training Our model can be trained through stochastic gradient descent (SGD) suggested by [12]. The gradient can be directly calculated for each training sample. Both the word vectors and MWE vectors are randomly initialized as what was used by Mikolov. Wikipedia August 2016 dump 1 is used as our training corpus. In pre-processing, pure digits and punctuations are removed, and all English words are converted to lowercase. The final corpus consists of about 3.2 billion words. During training, only words that occur more than 100 times are kept, resulting in a vocabulary of 204,981 words. The list of MWEs used in the evaluation are from 5 sources: (1) the set of 2,180 MWEs in the Noun-Modifier Composition dataset [22], (2) the DISCo set of 349 MWEs for the 2011 shared task in Distributional Semantics and Compositionality [3], (3) the set of 8,105 MWEs from the SemEval 2013 Task 5A [9], (4) the set of 1,042 MWEs from [5], and (5) the set of 56,850 phrases from [24]. The consolidated MWE list contained has a total of 60,315 after removal of duplication. Their representations are learned using the word2vecf source code [10]. 4.1 Evaluation Tasks We evaluate the representation of our MWEs on three evaluation tasks. The first task is called the SemEval 2013 Task 5 . The dataset for this task, denoted as SemEval , is pre-pared to judge whether a given bigram-unigram pair is semantically related or not [9]. For example, the bigram newborn infant is semantically related to the unigram neonate . so, the gold answer for this pair is (newborn infant, neonate, 1), where the label 1 indi-cates their relatedness . On the other hand, the bigram stable condition is not related to the unigram interview , So, in the gold answer, the entry is (stable condition, interview, 0) . The officially released data contains 7,814 test samples and 11,722 training sam-ples. 2 Since some of the bigrams/unigrams are not contained in the Wikipedia training corpus, we only use the 15,973 samples contained in Wikipedia in our evaluation. Since SemEval 2013 Task 5 is a binary classification problem, we simply use the cosine simi-larity between the learned bigram embedding and the unigram embedding as the feature and use an SVM to learn the threshold to perform 5-fold cross-validation classification. Accuracy, precision, recall and F-score are used in the evaluation metrics. The second task is called Phrase Similarity . This task provides a phrase pair similarity dataset with 324 samples 3 constructed using manually rated scores from 1 to 7 with 7 being the most similar [14]. For example, the phrase pair (hot weather, cold air) has a similarity score 2.22. The dataset contains three types of phrases: adjective-nouns, noun-nouns, and verb-objects with 108 samples for each type. All 324 samples are used in eval-uation, denoted as PS . Cosine similarity used to compare the different MWE vectors and Spearmans  X  correlation coefficient is used to evaluate the performance. The third task is called the Turney-5 . This dataset in this task is a 7-choice Noun-Modifier Ques-tion dataset built from WordNet [22] with 2,180 question groups. For example, in the sample (small letter, lowercase, small, letter, little, missive, ploughman, debt) , the first bigram small leter is the question and the latter 7 unigrams are the candidate answers. The task is to select the most similar unigram as the answer, which should be lower-case in this sample. To remove the bias towards MWEs by following Yu X  X  suggestion [25], we remove the two component words to construct a 5 choice single word ques-tions to form our evaluation dataset, denoted as T-5 . Again, by removing samples that are not contained in the Wikipedia training corpus, the final evaluation data contains 669 questions. The cosine similarity is used to measure the semantic closeness of a bi-gram MWE and the unigrams. The one with the highest similarity score is chosen as the answer. Accuracy is used as the evaluation metric. 4.2 Baselines and Experiment Settings We compare our model with the following baselines: 1. SGNS : the original vector representation model to take a MWE as a non-divisible 2. SEING : a modified SGNS model by treating component words as morphemes of 3. Comp-Add : a simple additive composition model to use the vectors of the compo-4. Comp-Mul : a simple multiplicative composition model to use the multiplication of 5. Comp-W1 : a composition model to use the vector of the first component word 6. Comp-W2 : a composition model to use the vector of the second component word Our proposed model has two settings for the compositionality t p . The first one directly sets t p as a constant, t p = 1 . This means the influence of compositionality is fixed rather than using Formula 7, denoted as C&amp;S-C . The second one uses the automatically computed t p which is MWE dependent, denoted as C&amp;S-t .
 is 5, and the word vector dimension is 300. We empirically set  X  to 8. For the composi-tion model, we empirically evaluate several kinds of combinations such as the additive model with  X  and  X  as 1, or the multiplicative model. Experiments show that the addi-tive composition model achieves the best, so we only report the results using the additive composition model. To obtain the compositionality t p , we first train the representation of MWEs using SGNS and compute its compositionality based on Formula 7 . 4.3 Performance Evaluation and Analysis Table 1 shows the performance using the three datasets of evaluation tasks for a total of six baseline systems and two variants of our proposed model. Table 1: Performance of different MWE representation models on 3 evaluation tasks. C&amp;S-C uses  X  = 8 and t p = 1 . C&amp;S-t uses  X  = 8 and learned t p . The top two are in bold. The best result is underlined.
 General Analysis Note that the first two models, SGNS, and SEING are distributional models, and compared to all the other models which have the compositional elements, the distributional models have no advantage at all. In other words, the semantics of MWE expressions are not fully recognized by using external context. Treating them as a single non-divisible unit obviously loses some semantic information.
 means that simple element-wise multiplication can introduce more noise than infor-mation. Comp-W1 and Comp-W2 have similar performance with Comp-W2 performs slightly better. A reasonable explanation is that the second component word is more likely the head word. For compositional MWEs, head words have a more defining role semantically.
 and PS. Theoretically speaking, however, our model considers information in both the external and the component words, which should lead to better performance. The reason that Comp-Add outperforms our model slightly on SemEval and PS may be the result of sparseness when getting the external context of MWE. Note that our model is the best performer on the T-5 data, outperforms COMP-Add with a larger margin (13.3%). cially dependent on the proportion of non-compositional MWEs. We randomly select 30 MWEs from the three datasets to manually annotate their compositionality and the result shows that the proportions of non-compositional MWEs are about 2.5%, 2.5% and 10% in SemEval, PS and T-5, respectively. Because the compositional model is more suitable on compositional MWEs, the Comp-Add model performs much better than SGNS on SemEval and the gap decreases on the T-5 as the proportion of non-compositional MWEs increases. Comp-Add indicates that the combined use of the vectors of the two component words is more comprehensive than using the external context of the compositional MWEs. On the T-5 dataset, the proportion of the non-compositional MWEs is larger than in the other two sets. So, there are more MWEs which would not work using the compositional method, which means their semantic cannot be composed from the component words. That is why the performance of SGNS increases and our model outperforms Comp-Add.
 C with fixed compositionality. This is mainly because the estimation of composition-ality t p is not sufficiently accurate. We evaluate the predicted compositionality on the MWEs dataset from Farahmand X  X  list [5] which has 1,042 MWES manually annotated with compositionality by calculating the Spearman X  X   X  correlation between the anno-tated compositionality and the predicted compositionality. The result shows that  X  only achieves 0.227, which means the current method to predict compositionality still has a lot of room for improvement.
 Compositionality Analysis To further explore the effects of compositionality on dif-ferent models, we further analyze the effect of the proportion of non-compositional MWEs on the SemEval semantic relation task. We manually select 20 non-compositional MWEs from Farahmand X  X  list [5]. Based on the 20 MWEs, we make 20 positive (se-mantically related) bigram-unigram pairs and 20 negative (not semantically related) bigram-unigram pairs to form a balanced non-compositional sample set for the SemEval task, denoted as N-Sem . We also randomly take 60 samples from the original SemEval dataset to form a compositional sample set, denoted as C-Sem . In the evaluation, we add the non-compositional MWEs from N-Sem to C-Sem to increase the proportion of non-compositional WMEs until all the non-compositional WMEs are used up (total of 100 samples). We then reduce the compositional portion so that the non-compositional portion reaches about 70% of the total set (57 samples). We add a C&amp;S-M model which takes the compositionality t p using manually annotated compositionality (1 or 0). F-score is used as the evaluation metric. Because of the limited data size, each model is run 10 times and the average is used.
 all the compositional models perform better, consistent with the result in Table 1 . As the non-compositional portion increases, Comp-Add degrades gradually whereas SGNS increases gradually. This indicates that external context is indeed useful for non-compositional MWEs and the composition model is ill-suited for non-compositional MWEs. The performance of SEING indicates that the constraint to force a MWE X  X  vec-tor to be similar to both of its components can actually bring adverse effect. Over the whole spectrum, our models give much more stable performance and are the overall top performers in all the automatic methods. C&amp;S-M, which gives the best performance, uses manually annotated compositionality. So, it can actually serve as the expected per-formance for automated systems. The difference between C&amp;S-M and both C&amp;S-t and C&amp;S-C indicates that there is room for improvement in compositionality prediction in the future. To conclude, our hybrid model gives an overall better performance as it is more robust on both kinds of MWEs. The fact that C&amp;S-M gives the best performance highlights the need for a more accurate estimation of compositionality. Future research on the evaluation of MWE datasets should consider the compositionality property. Fig. 1: Performance of different models as the proportion of non-compositional MWEs increases. Comp-Mul, Comp-Word1 and Comp-Word2 models are not included here because their performance is not comparable to Comp-Add. In this paper, we propose a hybrid method to learn the representation of MWEs from their context with the compositionality constraint. This method can make use of both the external context and component words of MWEs. Instead of simple combination of the two kinds of information, we use the compositionality measures from lexical semantics to serve as a constraint. Evaluations show that the composition model works better for compositional MWEs whereas the distributional model performs better on non-compositional MWEs. As we introduce compositionality measure, our proposed hybrid model is the most robust on both compositional and non-compositional MWEs. prediction method still has room for improvement. In the future, more study on ap-propriate compositionality prediction model can be investigated to provide this hybrid framework even better performance.

