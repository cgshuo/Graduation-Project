 1. Introduction
Language modeling (LM), providing a statistical mechanism to associate quantitative scores to sequences of words or to-kens, has long been an interesting yet challenging problem in the field of speech and language processing ( Jelinek, 1999; by Ponte and Croft (1998), Song and Croft (1999) and Miller, Leek, and Schwartz (1999) , indicating very good success, and was then extended in a number of publications.

The n -gram language model ( Bellegarda, 2004; Rosenfeld, 2000 ) that determines the probability of an upcoming word given the previous n 1 word history is the most commonly used model for speech recognition because of its neat formu-the search history beyond its order (e.g., a trigram language model is limited to two words of context).
In view of these problems, a wide variety of methods have been investigated to adapt or complement the n -gram lan-guage model with some success by searching for various information cues inherent in the search history. As an example,  X  trigger pairs are automatically generated beforehand to capture the co-occurrence information among words. By using TBLM, the associations between the words in the search history and the next occurring word can be modeled to some extent. 2001 ), are often considered to be two representative instantiations following this line of thought. A commonality among as a document) is not computed directly based on the frequency of the word occurring in the search history, but instead inference of model parameters: PLSA assumes the model parameters are fixed and unknown; while LDA places additional tions. LDA thus possesses fully generative semantics and could overcome the over-fitting problem to certain degree; how-shown that more sophisticated (or complicated) topic models, such as pachinko allocation model (PAM), do not necessary lematic for some highly inflecting or compounding languages (such as Finnish, German, Greek and Turkish), for which the these languages.

We recently introduced a new perspective on the problem of language modeling for speech recognition ( Chen &amp; Chen, 2011 ), saying that it can be approached with a framework built on the notion of relevance modeling, which shows good more in-depth elucidations of their modeling characteristics and associated speech recognition performance. Further, the utility of the methods deduced from our framework is verified by extensive comparisons with several widely-used LM methods.

The remainder of this paper proceeds as follows. We start by reviewing previous studies on language modeling using methods to be presented in this paper. In Section 3 , we shed light on the principles underlying the relevance modeling framework that can leverage lexical co-occurrence and topic cues in a systematic way for language modeling in speech rec-ognition, followed by an explanation of the connection between the methods instantiated from such a framework and the sented in Section 5 . Finally, Section 6 concludes our presentation and discusses avenues for future work. 2. Related work
To compensate for the weakness of n -gram models, a number of methods that dynamically discover topical or lexical co-occurrence information cues inherent in the search history have been investigated to complement the n -gram language
TBLM model that we will compare in this paper. Interested readers may also refer to ( Bellegarda, 2004; Zhai, 2008 ) for thorough and entertaining discussions of other major LM approaches that have been successfully developed and applied to speech recognition and IR.

As a common practice, topic modeling introduces a common set of latent topic variables to describe the  X  X  word-document  X  X  speech recognition, the dependence between an upcoming word and one of its search histories (usually regarded as a doc-ument) is not computed based on the frequency of the word occurring in the history, but instead based on the frequency of some sort of concept matching. PLSA and LDA are often considered to be the two best-known instantiations of this category. by H ) as a document (or history) topic model which can be used for predicting the occurrence probability of w : where T k is one of the latent topics; P  X  wT k j  X  and P  X  T and the probability of T k conditioned on the history H . The latent topic distribution P  X  wT ever, the search histories are not known in advance and their number could be enormous and varying during speech recog-nition. Thus, the corresponding topic mixture weight P  X  T has the advantage of taking account of the whole search history of an upcoming word, and to some extent can dynamically baseline n -gram (e.g., trigram) language model can be combined through a simple linear interpolation to guide the speech recognition process: where the interpolation parameter k controls the degree of reliance on the PLSA model rather than on the baseline n -gram language model.

On the other hand, LDA, having a formula analogous to PLSA for language modeling in speech recognition, is regarded as 2005 ). One essential difference between PLSA and LDA is in the inference of model parameters: PLSA assumes the model parameters are fixed and unknown; while LDA places additional a priori constraints on the model parameters, i.e., thinking of them as random variables that follow some Dirichlet distributions. Since LDA has a more complex form for model opti-approximation algorithm, the expectation propagation method and the Gibbs sampling algorithm, hence have been pro-posed for estimating the parameters of LDA ( Griffiths &amp; Steyvers, 2004 ).
 significantly and semantically related to unit B within a given context window. The complexity can be reduced by using words as the modeling units. Apart from using the average mutual information (MI) for the selection of trigger pairs ( Lau et al., 1993 ), the TF/IDF (Term Frequency/Inverse Document Frequency) measure which captures both local and global
The trigger pairs whose constituent words have the average MI scores or the TF/IDF scores higher than a predefined threshold can be selected for language modeling, and the associated conditional probability of the selected trigger pair ( w 0 , w ) can be estimated using the following equation: word w can be expressed by linearly combining the conditional probabilities of the trigger pairs ( w 0 , w ) as follows: where | H | is the length of the search history H ; w 0 is one of the words in H . The TBLM probability P bined with the baseline n -gram language model through a simple linear interpolation similar to (2) . 3. Relevance language modeling 3.1. Relevance class the information need expressed in the query are samples drawn from R . The document ranking problem then can be reduced to the problem of finding a mechanism to determine the relevance model (RM) or, more specifically, the probability P of observing words w in the query (and in the relevant documents) with respect to a particular information need. The rel-that we would observe a word if we were to randomly select a document from the relevance class and select the word from ranked documents obtained from an initial round of retrieval to approximate R . As such, the relevance model can be esti-mated in an unsupervised manner, and then works with various IR measures to distinguish relevant documents from irrel-evant ones.

The notion of relevance modeling has recently attracted much attention and been applied with success to many IR tasks nal into a series of subword units, and then a mapping function was used to translate the set of subword units to words. model (VSM) or LM, to name a few. Another example is that Siegler (1999) proposed the incorporation of speech recognition in this paper we strive to formalize (and instantiate) the notion of relevance modeling for speech recognition. 3.2. RM for speech recognition in which H is a search history, usually expressed as a sequence of words H = h diately succeeding words (i.e., an upcoming word). When RM is applied to language modeling in speech recognition, we hypothesize that each search history H has a relevance class R its immediately succeeding words w (the more relevant w to H the more likely that w is drawn alongside H from the rele-vance class R H of H ). The joint probability of H and w being generated by R conditional probability P  X  w j H  X  for speech recognition.

However, because the relevance class R H of each search history H is not known in advance, we may leverage a local feed-from the contemporaneous (or in-domain) corpus to approximate R ability of observing H together with w is given by where P ( D m ) is the probability that we would randomly select D taneously observing H and w in D m . If we further assume that words are conditionally independent given D of unigram probabilities of words generated by D m :
The probability P ( D m ) can be simply kept uniform or determined in accordance with the relevance of D probability P  X  w j H  X  is estimated by:
On the other hand, the baseline n -gram language model trained on a large general corpus can provide the generic con-gram) language model to form an adaptive language model for guiding the speech recognition process, which again can be achieved through a simple linear interpolation similar to (2) . 3.3. Incorporating latent topic information into RM
Not content with merely applying the RM model to speech recognition, we make a step forward to incorporate latent to-pic information into the RM modeling. For this idea to work, documents in the contemporaneous (or adaptation) collection are assumed to share a same set of latent topic variables { T acteristics. The probability that a word w is sampled from a relevant document D well as the likelihood that the document generates the respective topics:
As with PLSA and LDA, the probabilities P ( w | T k ) and P ( T like expectation X  X aximization (when with uniform priors) or variational approximation (when with Dirichlet priors) on the whole adaptation corpus. The joint probability of H and w being simultaneously observed in the relevance class R viously shown in (5) , is thus decomposed as or, alternatively, as
The main difference between (9) and (10) lies in that (9) assumes that H and w are simultaneously drawn from the same model (denoted, respectively, by TRM-I and TRM-II). In contrast to RM, TRM-I and TRM-II assume that the additional cues of how words are distributed across topics, which are gleaned from all documents in the adaptation corpus, can carry useful information for relevance modeling. 3.4. Modeling pairwise word associations
Furthermore, instead of using RM to model the association between an entire search history H and an upcoming word w , we can also use RM to render the pairwise word association between a word h By performing an algebra similar to (7) , we can arrive at the conditional probability P posite  X  X  conditional probability for the search history H to predict w can be obtained by linearly combining P the words h l in the history: where the values of the nonnegative weighting coefficients a apart from w and summed up to one ( P L l  X  1 a l  X  1), which have the form: where / l is set to a fixed value (between 0 and 1) for l =2, ... , L , and set to 1 for l = 1; and a ( Chen, 2009 ). By the same token, we can also introduce a set of latent topics { T occurrence relationships in a relevant document, and the pairwise word association between a history word h upcoming w is thus modeled by or, alternatively, by
Again, the difference between (14) and (15) lies in that (14) assumes that h same topics, while (15) relieves such an assumption instead. The language models formulated in (14) and (15) are referred to hereafter as TPRM-I and TPRM-II, respectively. 3.5. Leveraging document clusters for constructing RM
In addition to using single documents of the contemporaneous corpus as the semantic context for constructing the RM as word unigrams and bigrams with the aid of available clustering algorithms. During speech recognition, we then employ a document clusters from the contemporaneous corpus to approximate R the joint probability of observing H together with w is formulated as
We thus can estimate the corresponding RM model (denoted by RM-C) for H in a way analogous to that shown in (7) .In the same spirit, we can also derive the corresponding PRM model (denoted by PRM-C) for rendering pairwise word associ-ations with such document-cluster-based relevance modeling. 3.6. Implementation
As a final point, since the search histories typically are not known in advance and their number could be enormous and test utterance, the top-one word sequence hypothesis, output by the baseline speech recognition with the background clusters) from the contemporaneous (or in-domain) text collection. Empirical observations made on the development set re-
Croft, 1999 ), where each document (or document cluster) is respectively formulated as a unigram language model that can on the word occurrence frequencies in a document (or document cluster) and further combined with a background unigram language model using the Jelinek X  X ercer smoothing method ( Chen &amp; Goodman, 1996; Zhai, 2008 ) to model the general the proposed relevance modeling framework for speech recognition is shown in Fig. 1 . 4. Analytical comparisons
The various RM methods presented in this paper can be analyzed from several perspectives. First, RM (or RM-C) assumes that the entire history (or all the history words) and the upcoming word are sampled from the same relevant document (or document cluster), while PRM (or PRM-C) relaxes this assumption by considering individually the pairwise co-occurrence relationship between any history word and the upcoming word in each relevant document (or document cluster). Also note that during speech recognition, PRM additionally takes into account the distance (or proximity) between the history words and the decoded word through the use of exponentially decayed weights.

Second, PRM (or PRM-C) bears some resemblance to TBLM in modeling the long-distance co-occurrence relationships be-resentative trigger pairs remain an active area of research, and unlike PRM (or PRM-C), the derivation of TBLM probability distributions has not to do with the notion of relevance to the search histories (or the test utterance).
Third, analogously to PLSA and LDA, TRM (specifically, TRM-I and TRM-II) and TPRM (specifically, TPRM-I and TPRM-II) both make use of a set of latent topic variables to describe the co-occurrence relationships between words and documents (and also among words), which somehow addresses the important notions of synonymy and polysemy. Furthermore, PLSA and LDA have to estimate their component probability distributions on-the-fly for a new search history using expectation X  maximization or other more sophisticated algorithms, which would be time-consuming and thus limit their practical appli-the language model probability can be easily composed on the basis of the component probability distributions that were trained beforehand, without recourse to any complex inference procedure during the speech recognition (or rescoring) process.

Finally, the implementation of our proposed methods (e.g., RM in (7) ) can be quite efficient (compared to the conven-tional topic models), if the composition of language model probabilities is realized in the logarithmic domain. 5. Experimental results 5.1. Experimental setup The speech corpus consists of about 196 h of MATBN Mandarin broadcast news (Mandarin Across Taiwan Broadcast
News) ( ACLCLP, 2005 ). A subset of 25-h speech data compiled during November 2001 to December 2002 was used to boot-development set (1.5 h) and the test set (1.5 h). The front-end processing for speech recognition was performed with the
HLDA-based (Heteroscedastic Linear Discriminant Analysis) data-driven Mel-frequency feature extraction approach ( Kumar, 1997 ), and then processed by MLLT (Maximum Likelihood Linear Transformation) for feature de-correlation ( Saon, ment and test speech. Furthermore, the vocabulary size is about 72 thousand words. The n -gram (trigram) language model used in this paper was estimated from a background text corpus consisting of 170 million Chinese characters collected from Central News Agency (CNA) in 2001 and 2002 (the Chinese Gigaword Corpus released by LDC) using the SRI Language Modeling Toolkit (SRILM) ( Stolcke, 2000 ).

In this paper, we focus our attention on exploring extra lexical co-occurrence or topic information from an adaptation (contemporaneous) text corpus, which can provide complementary language cues to the n -gram language model that ex-topic methods and TBLM model were trained with a relatively small corpus collected from MATBN 2001, 2002 and 2003 (excluding the test set), which consists of one million Chinese characters (3643 documents) of orthographic broadcast news retrieved documents, used for language modeling were all tuned at optimum values by the held-out (development) set.
Chinese language thus often suffers from the word tokenization problems. The performance evaluation metric used in Man-character strings, divided by the total number of Chinese characters in the reference string ( Ref ):
In this study, the speech recognition system with the baseline trigram language model results in a CER of 20.08% and a perplexity of 682.10 on the test set. 5.2. RM with different granularities of context text being used to approximate the relevance class: single documents and document clusters. The corresponding CER results are shown in Table 1 , for which the numbers of relevant documents or document clusters being retrieved were determined based on the development set. It should be borne in mind that RM and PRM are obtained by using single documents as the
RM-C. Furthermore, RM-C and PRM-C seem to achieve results close to that of RM and PRM, respectively; this suggests that document clusters represent a good alternative to single documents for constructing the relevance language models. To re-cap, RM and PRM can lead to a CER reduction of 3.6% and 4.1%, respectively, as compared to the baseline trigram system. On relevance information indeed provides complementary cues to the n -gram language model that merely renders the generic word order and adjacency behaviors of the language.

Still another perspective is to use single sentences of the training corpus (usually determined by punctuation marks) to form the semantic context for relevance modeling, which again can lead to two relevance language models, denoted by RM-S (modeling the relationship between the entire search history and the upcoming word) and PRM-S (modeling the pairwise word association between the each history word and the upcoming word). The CER results of RM-S and PRM-S are 19.29% and 19.32%, respectively, which are quite on par with those results obtained using either single documents or doc-ument clusters for relevance modeling. 5.3. RM with latent topic information
In the second set of experiments, we try to measure the effectiveness of relevance modeling when a set of latent topics is additionally employed to describe the word X  X ord co-occurrence relationships in a relevant document, and the resulting information into relevance modeling brings additional performance gains for most cases, while assuming further that the achieves the best CER of 19.08%, which leads to a CER reduction of about 5.0%. 5.4. Comparisons with existing language models
Next, we compare our proposed various RM models with several well-practiced language models, including PLSA, LDA, the cache model (denoted by Cache), and TBLM, in terms of the CER reduction; the corresponding CER results of these models
TBLM was set optimally. As for the cache model, bigram cache was used since it yielded better results than the unigram or par with that obtained by TRM-I and TRM-II, but are slightly worse than that obtained by TPRM-I and TPRM-II. On the other hand, TBLM and the cache model have their respective best CER of 20.02% and 19.86%. According to these results, we can conclude that our proposed models can achieve comparable performance (in terms of the CER reduction) to the conventional models.

We also investigate the pairing of the proposed RM models (taking TRM-I and TPRM-I as examples) with the conventional models (taking the cache model and the LDA model as example) through linear interpolations. The corresponding results are illustrated in Table 4 , which reveal that the marriage of the proposed RM models and the other models can lead to better 18.72% (6.8% reduction than our baseline system) when TPRM-I (with the Dirichlet prior assumption), the cache model and LDA work together to obtain a better adapted language model.

Notably, when the local feedback-like procedure was conducted with the manual transcript, instead of the baseline rec-and 18.33%, respectively, for RM and PRM. They are considerably better than the original results of RM and PRM shown in test utterance remains an important issue that awaits further studies.
 5.5. Perplexity results In the last set of experiments, we compare our models with the existing topic models in terms of perplexity reduction. The corresponding results of the various language models, including RM, PRM, RM-C, PRM-C, TRM-I, TPRM-I, PLSA and forms slightly worse than TRM-I and TPRM-I in the CER reduction. In a nutshell, PRM can provide a perplexity reduction of 35.8% as compared to the baseline trigram system. 6. Conclusions and future work
In this paper, we have presented a relevance language modeling framework for speech recognition. The RM models de-duced from such a framework can exhibit an effective and efficient combination of relevance, topic and proximity modeling.
Furthermore, the utility of our proposed models have also been validated by extensively comparisons with several widely-used language models in terms of CER and perplexity reduction. We do not intend to claim that the proposed various RM models can really outperform the existing models by a big margin. Instead, we have elaborated on how the notion of rele-deemed complementary to those discovered by the existing language models. Our future research directions include: (1) ing the proposed RM models to other related speech and language applications including speech summarization and retrie-val tasks ( Chen, Lin, Chang, &amp; Liu, 2013; Chen et al., 2012 ).
 Acknowledgements
This work was sponsored in part by  X  X  X im for the Top University Plan X  X  of National Taiwan Normal University and Ministry of Education, Taiwan, and the National Science Council, Taiwan, under Grants NSC 101-2221-E-003-024-MY3, NSC 101-2511-S-003-057-MY3, NSC 101-2511-S-003-047-MY3, NSC 99-2221-E-003-017-MY3, and NSC 98-2221-E-003-011-MY3. References
