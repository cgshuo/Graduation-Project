 1. Introduction
The Web has become a major information source both in everyday and in professional life. Information sources available through the Web are regularly being referenced in scientific publications ( Herring, 2002;
Lawrence, 2001; Snyder &amp; Peterson, 2002; Zhang, 2001 ). In addition, the Web is used for gathering other types of professional information (e.g. calls for papers, tables of contents and abstracts of proceedings/jour-nal, information published by governments and different organizations).
The primary tools for locating information on the Web are the search engines. Currently Google is the most popular search tool and is one of the most visited sites on the Web ( Nielsen/NetRatings, 2004; Sul-livan, 2004a ). Even though large search engines cover only a fraction of the Web ( Bharat &amp; Broder, 1998;
Lawrence &amp; Giles, 1999 ), for most one or two-word queries (and these are the most frequently occurring queries), the number of retrieved results is in the thousands or in the millions. Users usually browse only the first or perhaps the second page of the search results (i.e., they usually only consider the top ten or twenty results) ( Silverstein, Henzinger, Marais, &amp; Moricz, 1999; Spink, Ozmutlu, Ozmutlu, &amp; Jansen, 2002 ). Thus ranking becomes crucial. In this paper we propose a new method for comparing the rankings of different search engines, based only on the documents that are listed by all the search engines that are compared.

The ranking algorithms of the search engines are not public: they are on the one hand trade secrets, and on the other hand the search engines fear that web site owners will misuse the available information in order to gain higher rankings for their pages. For example, Google is willing to disclose only that its ranking algo-rithm involves more than 100 factors, but  X  X  X ue to the nature of our business and our interest in protecting the integrity of our search results, this is the only information we make available to the public about our ranking system X  X  ( Google, 2004 ).

Since users would drown in information without reasonable ranking algorithms, it is of utmost importance to evaluate the rankings provided by different search tools. The usual method of evaluation is through human judgment, since rankings like relevance judgments are highly subjective and depen-dent on the context of the search carried out by the specific user. In an early study by Su, Chen, and
Dong (1998) , users were asked to choose and rank five most relevant items from the first 20 results retrieved for their queries. In fall 1999, Hawking, Craswell, Bailey, and Griffiths (2001) evaluated the effectiveness of 20 public Web search engines on 54 queries. One of the measures used was the re-ciprocal rank of the first relevant document (relevance was judged by humans) X  X  measure closely re-lated to ranking. The results showed significant differences between the search engines. In a recent study, Vaughan (2004) compared human rankings of 24 participants with those of three large commer-cial search engines, Google, AltaVista and Teoma on four search topics. The highest average correla-tion between the human-based rankings and the rankings of the search engines was for Google, where the average correlation was 0.72.

For judging the rankings produced by a specific search tool, the best method is human judgment. How-ever, for comparing rankings of different tools one can compute similarity measures without the involve-ment of human judges. Fagin, Kumar, and Sivakumar (2003) proposed a method for comparing the top k results retrieved by different search engines. One of the applications of the metrics proposed by them was comparing the rankings of the top 50 results of seven public search tools (some of them received their results from the same source, e.g., Lycos and AlltheWeb) on 750 queries. The basic idea of their method was to assign some reasonable, virtual placement to documents that appear in one of the lists but not in the other. The resulting measures were proven to be metrics. Bar-Ilan, Levene, and Mat-Hassan (2004) used three different measures to study the changes in search engine rankings over time.

We show that for the measures proposed by Fagin et al., when the two lists have little in common, the non-common documents have a major effect on the measure. Our experiments show that usually the over-lap between the top 10 results of two search engines for an identical query is very small. Here we propose a different method X  X omparing only the comparable documents, i.e. those appearing in both lists. 2. Methodology
Before discussing in detail the methods employed by us, we briefly discuss one of the Fagin et al. (2003) metrics (all the metrics introduced by them were shown to be equivalent). 2.1. The metrics introduced by Fagin et al.

It is relatively easy to compare two rankings of the same list of items X  X or this well-known statistical metrics such as Kendall  X  s tau or Spearman  X  s footrule can be easily utilized. The problem arises when the two search engines that are being compared rank non-identical sets of documents. To cover this case (which is the usual case when comparing top k lists created by different search engines), Fagin et al. (2003) extended the previously mentioned metrics. Here we discuss only the extension of Spearman  X  s footrule, but the exten-sions of Kendall  X  s tau are shown in the paper to be equivalent. A major point in their method was to devel-op measures that are either metrics or  X  X  X ear X  X  metrics. Spearman  X  s footrule, is the L permutations (where the rankings on identical sets can be viewed as permutations): F  X  r
P j r 1  X  i  X  r 2  X  i  X j . This metric is extended for the case where the two lists are not identical, documents appearing in one of the lists but not in the other an arbitrary placement (which is larger than the length of the list) is assigned in the second list X  X hen comparing lists of length k this placement can be k +1 for all the documents not appearing in the list. The rationale for this extension is that the ranking of those documents must be k + 1 or higher X  X agin et al. do not take into account the possibility that those docu-ments are not indexed at all by the other search engine (which is quite plausible because of the low overlap of the search engine databases). The extended metric becomes: where Z is the set of overlapping documents, and z is the size of Z , S is the set of documents that are only in the first list and T is the set of documents that appear in the second list only. Thus a lower bound on
F
This value is normalized by dividing the value by the maximum value of F normalized F ( k +1) is a distance measure with values between 0 and 1 (0 X  X he lists are identical). Note that the metric is heavily dominated by the non-overlapping elements, for z = k /5 (characteristic value we found in our experiments), the influence of the non-overlapping elements is considerable, since the minimum value of the normalized F ( k +1) in this case will be
Therefore in this characteristic case, the range of possible values of F the case z = k /10, the minimum normalized value tends to 0.81 as k goes to infinity, thus the range of pos-sible values is even smaller. 2.2. The proposed measures
The metrics proposed by Fagin et al. (2003) can be very useful for situations where the overlap between the two compared lists is large. However, in practice, this is not the case for the search results of the large public search engines on identical queries. The major reason is that as we mentioned in the introduction the overlap of the crawled pages by the different search engines is relatively small (see Bharat &amp; Broder, 1998; Lawrence &amp; Giles, 1999 ). Thus we propose measures where we consider only the overlapping documents.
For comparing rankings of two search engines on identical queries, we compute Spearman  X  s rho and test for significance. The measure is computed by considering only URLs that appear in both lists. Spearman  X  s rho is applied to ranked lists of n items, where the rankings are between 1 and n . Thus the set of intersecting URLs was reranked for both search engines, where each URL received its relative rank in this subset, based on the absolute rankings of the given search engine. Spearman  X  s rho ranges between 1 and 1, where 0 stands for no correlation between the two rankings, 1 for complete agreement and 1 for complete dis-agreement. It is the non-parametric version of Pearson  X  s r (see for example Garson, 2004 or Lowry, 2004 ). For more than two lists we calculate Kendall  X  sW  X  X lso called the coefficient of concordance that com-pares rankings of different judges (in our case different search engines) on a set of items (documents in our case). Kendall  X  sW ranges between 0 (no agreement) and 1 (complete agreement) (see for example Bove, 2002 ). Here too we consider the set of intersecting URLs only and rerank the lists like for the case of two search engines. For both Spearman  X  s rho and Kendall  X  sW , one has to consider both the strength of the correlation or of the agreement (the nearer these values are to 1, the rankings are more similar) and the significance of the result, that measures whether the differences between the rankings could have been random.

Identical queries were submitted to different search engines, and the lists of all displayed results were saved. For smaller queries (i.e., less than 1000 hits) most search engines display the complete lists, while for larger queries only the first 1000 results are shown usually. We only considered search engines that pro-vided complete rankings, thus search engines where only one or a few pages from a site are displayed, with-out an option to display complete lists (such options existed at the time of the data collection for Google and AltaVista) were discarded (e.g. Teoma). From the saved results pages, we filtered out the URLs of the hits and created a consolidated list of URLs that were retrieved by all the search engines for the given query. For every URL we listed the respective rank of that URL in the list of each search engine in which the URL appeared. For every pair of search engines we looked at those URLs only that appeared in both lists and reranked the URLs appearing in the intersection.

The Web search engine scene is rather dynamic, since the data collection AlltheWeb and AltaVista have undergone major changes, currently both are powered by Yahoo ( Sullivan, 2004b ). Thus, had we run these queries in October 2004, we probably would have received different results, however, the methods presented here are still valid, and based on smaller scale experiments we conclude that there are still huge differences between the ranking algorithms of Yahoo and Google. Search engines change their ranking algorithms all the time (see for example the reports on the Florida Google dance X  Sullivan, 2003 ) even without changes in their business models, thus in any case the specific results presented here were only valid at the time of the data collection. The specific results presented here demonstrate the applicability of the methods, and serve as an alert as to the huge differences in ranking by the different systems. 2.3. Data collection
For our experiment we chose 15 queries in the area of information retrieval. The queries were not chosen randomly, since we wanted to experience with different sizes of results sets, based on the number of results reported by Google X  X he first five queries resulted in less than 200 hits, for the next five the number of hits was between 200 and 1000, and the last five queries had more than 1000 hits X  X or this set only the first 1000 results were retrieved (1100 for AlltheWeb). We queried four search engines: Google, AlltheWeb, AltaVista and HotBot on December, 7, 2003. For Google we used the  X  X  X he repeat the search with the omitted results included X  X  option, and for AltaVista we marked the  X  X  X ite collapse off X  X  option in order to have complete rankings of the retrieved documents. Table 1 displays the queries and the number of URLs retrieved for each query by each search engine separately and by all the search engines together. For the large queries we also show the number of reported results in parentheses. In brackets is the relative coverage of each search engine in percentages out of the pool of URLs identified by the four search engines for the query.
For the small and medium queries the relative coverage of Google is considerably higher that those of the other search engines, while in December 2003, HotBot had the lowest coverage. As pointed out before, these results only present a momentary snapshot, and no conclusions should be drawn from the specific findings X  X hey are of anecdotal value only. 3. Results and discussion
We computed Spearman  X  s rho between every pair of search engines and for every query. The results appear also calculated the average correlation between pairs of search engines, based on the overlapping URLs.
Interesting to note that even though the highest correlation was between Google and AltaVista, the cor-relation was especially high (and in most of the cases significant) for most of the queries, there were a few exceptions, especially for query 11,  X  relevance ranking AND  X  X  X ink analysis X  X   X   X  X ere the correlation was ex-tremely low, only 0.004, even though both engines indexed 128 common documents. Another point to no-tice is that in spite of the low correlation between the rankings of Google and AlltheWeb, they both correlated relatively strongly with AltaVista.

For the sake of comparison we also computed F 51 measure introduced by Fagin et al. (2003) for queries 6 X 15 (for the first queries there were less than 50 hits for some of the search engines). The results were nor-malized so the numbers are between 0 and 1 (all values were divided by 2550 X  X he highest possible value), just like in ( Fagin et al., 2003 ). The results appear in Table 3 together with the size of the overlap (the z value in the formula for computing F ( k +1) ) for the two top 50-lists. Note that for query 10, HotBot retrieved only 46 results.

F is a distance measure, therefore, the smaller the value the more similar are the rankings of the com-pared search engines. Let us compare the rankings based on the average of Spearman  X  s rho with the ranking based on the F metric. The comparison appears in Table 4 .

Table 4 shows differences in rankings of similarity/correlation. The differences between the F values are small, compared to the far more emphasized differences in the average values for Spearman  X  s rho . Spearman  X  s rho is a well-known measure, and categorizations of the strength of the correlation exist. Some say that if the absolute value of the correlation is below 0.3 then the correlation is weak, between 0.3 and 0.49 it is medium and above 0.5 it is large ( Cohen, 1988 ). Others define the correlation as strong, only if the absolute value is above 0.7, and medium when the absolute value is between 0.4 and 0.7 ( Rowntree, 1981 ). In either case we see that the average correlation between Google and AltaVista and Google and Hotbot is medium-high, while the correlations between Google and AlltheWeb and AlltheWeb and Hotbot are very low.

The most striking difference between the two rankings is the placement of the Google-HotBot pair. We believe that the F measure is very strongly influenced by the size of the overlap, as we discussed in the meth-odology section. Actually the rankings for the F measure are exactly according to the size of the average overlap. Thus it seems that the F measure is useful in situations where the overlap between the results of every two pairs of compared search tools is considerable. Spearman  X  s rho , on the other hand, totally ignores the non-overlapping elements, and concentrates only on the differences in the rankings of the search engines being compared.

We also compared groups of three search engines on the set of URLs that were retrieved by all three engines. Finally we looked at the relative rankings of the URLs retrieved by all four search engines. For the case of three or more tools, we computed Kendall  X  sW that measures the agreement between raters and ranges between 0 and 1: 0 for no agreement and 1 for complete agreement. SPSS provides a significance measure associated with this value. The df in the table stands for degrees of freedom and it is the size of the intersection minus 1. The results appear in Table 5 .

We see that the average agreement for all the sets is around 0.5, and the results for the larger queries are mostly significant, i.e. the outcome is with high probability not accidental. For query 5,  X  X  X ultilingual re-trieval X  X  AND Hebrew, there was not a single document that was retrieved by all four search engines. 4. Conclusions
The findings of this study clearly show that search engines employ very different ranking algorithms. The obvious differences in rankings methods should be noted by experienced users, especially by scientists look-ing for a wide range of quality information on the Web. By submitting the same query to several search tools or to a meta search engine that sends the query to the major search engines, even by looking only at the top-10 or top-20 results retrieved by each of the search tools, one can increase the range of the results considerably.

Our method of taking into account only documents that are retrieved by all the search engines that are being compared, zooms in on the differences in the  X  X  X anking recipes X  X , without penalizing the search engines for having smaller indices or for crawling in places that the other search engines have not reached.
The method presented here compares rankings of different search engines; however it says nothing about the superiority of one ranking over the other. In order to decide which ranking is better in the  X  X  X yes of the users X  X , large-scale user studies have to be carried out along the lines of Vaughan  X  s research (2004) .
As for the future, we feel that larger scale studies are appropriate. Such studies should be carried out periodically, since both the Web and the ranking algorithms undergo constant changes.
 References
