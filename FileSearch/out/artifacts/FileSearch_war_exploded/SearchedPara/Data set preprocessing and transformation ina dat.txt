 Department of Computer Science, University of Houston, 4800 Calhoun Rd, Houston, TX, USA E-mail: ordonez@cs.uh.edu 1. Introduction
In this work we focus on data preprocessing, an important data mining topic that has received scant attention in the literature. Nowadays transactio n processing [6] and data warehousing [8] of large databases are performed mostly by relational database systems (DBMSs). Analyzing large databases with data mining techniques is a different story. Despite the existence of many data mining techniques offered by a database system [8,16], there exist many data mining tasks that are routinely performed with external tools [8,9]. This is due, among other reasons, to the existence of advanced data mining programs (i.e. open-source, third party commercial software), the availability of mathematical libraries (e.g. LAPACK, BLAS), the lack of expertise of data mining practitioners to write correct and ef fi cient SQL queries and legacy code. Therefore, the database system just acts as a data provider, and users write SQL queries to extract summarized data with joins and aggregations. Once  X  X aw X  data sets are exported they are further cleaned and transformed [8] depending on the task at hand, outside the database system. Finally, when the data set has the desired variables (features) and an appropriate number of records (observations, instances), data mining models [9] are iteratively computed, interpreted and tuned. From all the tasks listed above preparing, cleaning and transforming data for analysis is generally the most time-consuming task [8] because it requires signi fi cant effort to convert normalized tables in the database into denormalized tabular data sets [8], appropriate for data mining algorithms. Unfortunately, manipulating data sets outside the database system creates many data management issues: data sets must be recreated and re-exported every time required tables change, security is compromised (an essential aspect today due to the Internet). Data sets need to be re-exported when new variables (features) are created, models need to be deployed inside the database system, and different users may have inconsistent version of the same data set in their own computers. Therefore, we defend the idea of transforming data sets and computing models inside a modern database system, exploiting the extensive features of the SQL language and enjoying the extensive data managemen t capabilities provided by t he database system. Our motivation to migrate data mining preprocessing into the database system, yields the following bene fi ts. The database system provides powerful queryin g capabilities through SQL and 3rd party tools. Even further, the database system provides the best data management functionality (maintaining referential integrity, transaction processing, fault-tolerance, security). Additionally, the database system server is generally a fast computer and thus results can be obtained sooner and with less programming effort.
We assume the database system provides basic data mining functionality. That is, the database system is capable of performing common data mining tasks like regression [9], clustering [9], classi fi cation [19], PCA [3] or association rules [10,15], among others. On the other hand, we assume the database system supports the SQL language, which is used to store and retrieve information from the database system [6, 8]. Based on experience from data mining projects we have become aware that data mining practitioners have dif fi culty writing correct and ef fi cient SQL code to extract data from the database system or to transform their data sets for the data mining task at hand. To overcome such issues, practitioners resort to high level programming languages (e.g. Java, C++, SAS) to manipulate and transform their data sets. Thus, practitioners end up creating long programs mixing data transformation and modeling tasks together. Most of the  X  X edious X  programming effort is spent on manipulating and transforming input tables to build the required data set: this is the main aspect studied in this article. We identify practical issues as well as common solutions for data transformation tasks. We present evidence transforming data sets and building models on them is more ef fi cient to do inside the database system than outside with external data mining programs. We motivate much needed research on data preparation. Finally, we identify advantages and disadvantages between external data mining processing outside a database system and internal processing inside the database system. Our lessons can be valuable for data mining practitioners or AI people extracting data from a database system.

In this article, we provide practical guidelines to translate common data preparation tasks programmed in a traditional high-level programming language into SQL, which have quite different computing models behind. However, there are certain similarities a nd differences between both kinds of languages that make the problem interesting and challenging. A traditional language is an imperative language that manipulates data sets as record fi les, but not strictly as relational tables. A high-level programming language includes syntactic constructs for arithmetic expressions, fl ow control and function calls. In such languages, arrays can be used to implement matrix operations. In addition, some languages (e.g. C++) allow pointer manipulation. On the other hand, SQL is a set oriented language that manipulates data sets as tables, but which allows specifying relationships among tables with primary and foreign keys relationships. A set of tables is manipulated with join operations among their keys. In general, SQL does not support arrays and thus matrix manipulation is more dif fi cult. Even further, pointer manipulation is forbidden. Scanning the data set needs to be carefully programmed in a traditional language, whereas it is automatic in SQL. Query optimization is a fundament al issue in SQL, whereas in a traditional language the goal is to optimize mathematical operations and memory usage.

The article is organized as follows. Section 2 discusses important practical issues and common solutions when preprocessing and transforming data sets for data mining analysis. Section 3 presents guidelines to code common data transformations in SQL. Section 4 presents performance comparisons and users feedback from projects where external data preparation programs were successfully migrated into the database system. Related work is discussed in Section 5. Finally, Section 6 concludes the article. 2. Issues preprocessing and transforming data sets
We present important practical issues to preprocess and transform tables in a database in order to build a data set suitable for analysis. These issues have been collected from several projects helping users migrate analysis performed in external data mining tools into a database system. We present issues in three groups: (1) creating and transforming variables for analysis; (2) selecting ( fi ltering) records for analysis; (3) developing and optimizing SQL queries.
 In general, the main objection from users against using SQL is to translate existing legacy code. Commonly such code has existed for a long time (legacy programs), it is extensive (there exist many programs) and it has been thoroughly debugged and tuned. Therefore, users are reluctant to rewrite it in a different language, given associated risks. A second complaint is that, in general, the database system provides good, but still limited, data mining functionality, compa red to sophisticated data mining and statistical packages. Nevertheless, such gap has been shrinking over the years. As explained before, in a data mining environment most user time is spent on preparing data sets for analytic purposes. In the following paragraphs, we discuss some of the most important database issues when tables are manipulated and transformed to prepare a data set for data mining. We pay particular attention to building the data set with the right set of columns, getting correct results, and optimizing queries [6]. The issues we will discuss represent experience that has been gained by testing, adapting and optimizing SQL code that has been manually written or automatically generated by data mining tools. Certain data preparation tasks can be partially automated, whereas other tasks require signi fi cant manual work by the user. We will carefully distinguish both cases. Throughout this section, we present examples of typical SQL queries in a store. 2.1. Issues creating and transforming data set variables
We explain issue to create variables (features) for analysis. We also considerthe casewhere the variable is already available as a column, but needs to be transformed. Creating and transforming variables is the most important aspect to understand variables.
 Summarization and aggregation
Unfortunately, most data mining tasks require dimensions (variables) that are not readily available from the database. Such dimensions typically require computing aggregations at several granularity levels. This is because most columns required by data mining techniques require measures (or metrics), which translate as sums or counts computed with SQL. Unfortunately, granularity levels are not hierarchical (like cubes or OLAP [6]) making the use of separate summary tables necessary (e.g. summarization by product or by customer, in a retail database). Thus, the user needs to decide: which tables and columns need to be summarized, the primary key (s) of output tables (i.e. grouping columns) and which kind of aggregations (e.g. sum(), average(), min()) are required.

For a data mining practitioner it is best to create as many variables (features, dimensions) as possible to identify those that can help computing a more accurate model. Then summarization tends to create tables with many columns (sometimes hundreds), which make data mining analysis harder and query processing slower. This task is related to attribute relevance, which determines which attributes are more important for a given model. Attribute relevance is commonly determined by dimensionality reduction and feature selection techniques. Most data mining techniques, especially for classi fi cation and regression, are designed to perform variable (feature) selection [9]. A straightforward optimization is to compute as many dimensions in the same statement exploiting the same GROUP-BY clause, when possible.
 A typical query to derive dimensions from a transaction table is as follows: Transforming categorical variables
Many statistical and machine learning models require categorical variables to be transformed into numeric ones. One common approach to solve such problem is to code each categorical value as one binary attribute. Given k categorical values they are coded as k  X  1 binary attributes in order to create an uncorrelated set of variables (i.e. the k th value can be deduced from the other k  X  1 ). Such transformation is solved as a list of k  X  1 CASE statements in SQL. From a database perspective, this transformation is simple and fast to compute since the number of records remains the same.

This transformation does not require signi fi cant manual work by the user since the coding phase can be automated. Figuring out which categorical values are more relevant for a certain model requires computing correlations or contingency tables.
 Denormalization It is well known that on-line transaction processing (OLTP) databasesystems update normalized tables. Therefore, it becomes necessary to gather data from several tables and store attributes in one table. The explanation is simple. Normalization makes transaction processing faster and ACID [6] semantics are easier to ensure. Queries that retrieve a few records from normalized tables are relatively fast. On the other hand, analysis on the database requires precisely the opposite: a large set of records is required and such records gather information from many tables. Such processing typically involves complex queries involving joins and aggregations. Therefore, normalization works against ef fi ciently building data sets for analytic purposes. One solution is to keep a few essential denormalized tables from which specialized tables can be built. In general, such tables cannot be dynamically maintained b ecause they involve join computation with large tables. Therefore, they are periodically recreated as a batch process.
In general, denormalizing does not require signi fi cant user involvement since joins are computed by the database system and the logical model dictates which columns should be used to compute joins. Denormalization may introduce unnecessary attributes because those attributes may be repeated in multiple tables. T herefore, it is necessa ry to detect and remove redundant a ttributes: extra attributes having unitary correlation should be removed.
 Cross-tabulation and data set fl attening
This data transformation is a combination of summarization and denormalization. Data mining models generally require having all dimensions for analysis in a single data set containing a set of records having several attributes. The distinguishing characteristic of the table storing such data set is that in general it has only one column for its primary key (e.g. product id, customer id) and several numeric and discrete attributes. In certain cases the table may have a composite primary key formed by an identi fi er and some time-dependent column. Since tables tend to be available normalized, certain columns need to be aggregated whereas other columns are used to transpose (pivot) the input table. In other words, aggregation and pivoting are combined to build the data set. This transformation effectively transforms the input table into a  X  fl attened X  data set, where several values are cross-tabulated. Such kind of SQL statement has been called a horizontal aggregation [12]. A typical example is to aggregate numeric columns converting date and time values into multiple columns for one year of sales:
As noted above summarization requires a lot of user manual work, with the additional required denor-malization. Theref ore, getting all the pi eces together in one data set requi res careful code development and testing.
 Statistics columns
It is fair to say that univariate statistics are the main tool to understand the data set before computing a model, and even as the model is being validated and tuned. Such statistics include mean, variance, standard deviation and extreme values (min, max). Also, statistics include the mode and histograms for discrete variables. Unfortunately, some statistics like the mean are not distributive [17]. In other words, they cannot be computed from existing means or standard deviations. A simple solution is to keep sums statistics [2,16] prove useful for both simple statistics as well as sophisticated multidimensional models. This solution is represented by the suf fi cient statistics n, L, Q [16], explained in detail in Section 3.5. This task can be automated for the most part (i.e. little or no user manual work) since these suf fi cient statistics can be automatically computed as explained in the literature. 2.2. Issues selecting data set records for analysis
For the issues discussed below we assume we have an acceptable set of variables. So we now concentrate on explaining how to select the best records from the database for analysis. Selecting and fi ltering records is the second most important aspect to understand the data set. Many times selecting the right set of records produces much better models than using all existing records. In general, this task requires excellent domain knowledge on the problem. Thus it cannot automated. We should emphasize again preparing data is an iterative process. Therefore, it may be necessary to add variables and then select records again multiple times.
 Time window
In general, for a data mining task it is necessary to select a set of records from one of the largest tables in the database based on a date range. In general, this selection requires a scan on the entire table, which is slow. When there is a secondary index based on a date column it is more ef fi cient to select rows, but it is not always available. The basic issue is that such transaction table is much larger compared to other tables in the database. For instance, this time window de fi nes a set of active customers or bank accounts that have recent activity. Common solutions to this problem include creating materialized views (avoiding join recomputation on large tables) and lean tables with primary keys of the object being analyzed (record, product, etc) to act as fi lter in further data preparation tasks.

This task is generally guided by users with domain knowledge. Query writing is fairly simple since it only requires specifying date/time ranges.
 Level of detail
Transaction tables generally have two or even more levels of detail, sharing some columns in their primary key. The typical example is store transaction table with individual items and the total count of items and total amount paid. This means that many times it is not possible to perform data mining on one data set only. There may be unique pieces of information at each level. Therefore, such large tables need to be joined with each other and then aggregated at the appropriate granularity level, depending on the data mining task at hand. In general, such queries are optimized by indexing both tables on their common columns so that hash-joins can be used.

For instance, in a store there is typically one transaction table containing total amounts (sales, tax, discounts) and item counts, and another transaction detail (or line) table containing each individual item scanned at the register. For certain data mining analysis (market basket analysis the detailed purchase information may be required). On the other hand, in a bank there is one table with account summaries (current and by month) and another table with individual banking transactions (deposits, withdrawals, payments, balance inquiry).

More detailed data generally means more precise models, but also more manual work summarizing and aggregating. Therefore, the level of detail existing in the database is correlated to the amount of manual effort required for preparing data sets.
 2.3. Issues developing and optimizing SQL code
We now consider tasks related to developingSQL queries to transform and build data sets. Such aspects are relevant only in the context of a database system. As such these aspects do not help understanding the data set, but being able to ef fi ciently manipulate it. Manual work by the data mining practitioner can be signi fi cant if there is no tool to automate SQL query writing. Manual work can become more complicated if query optimization is involved, especially with large tables.
 Left outer joins for completeness
For analytic purposes, it is always best to use as much data as possible. There are strong reasons for this. Data mining models are more reliable, it is easier to deal with missing information, skewed distributions, discover outliers and so on, when there is a large data set at hand. In a large database with tables coming from a normalized database being joined with tables used in the past for analytic purposes may involve joins with records whose foreign keys may not be found in some table. That is, natural joins may discard potentially useful records. The net effect of this issue is that the resulting data set does not include all potential objects (e.g. records, products). The solution is de fi ne a universe data set containing all objects gathered with union from all tables and then use such table as the fundamental table to perform outer joins. For simplicity and elegance, left outer joins are preferred. Then left outer joins are propagated everywhere in data preparation and completeness of records is guaranteed. In general, such left outer joins have a  X  X tar X  form on the joining conditions, where the primary key of the master table is left joined with the primary keys of the other tables, instead of joining them with chained conditions (FK of table T1 is joined with PK of table T2, FK of table T2 is joined with PK of T3, and so on).
User involvement is small. The main issue is getting the main table that includes all records. This issue can be easily solved if there is one table that is guaranteed to have every record (e.g. every customer). Dependent SQL statements
A data transformation script is a long sequence of SELECT statements. Their dependencies are complex, although there exists a partial order de fi ned by the order in which temporary tables and data sets for analysis are created. To debug SQL code it is a bad idea to create a single query with multiple query blocks. In other words, such SQL statements are not amenable to the query optimizer because they are separate, unless it can keep track of historic usage patterns of queries. A common solution is to create intermediate tables that can be shared by several statements. Those intermediate tables commonly have columns that can later be aggregated at the appropriate granularity levels.
 Choosing between views and temporary tables
Views provide limited control on storage and indexing. It may be better to create temporary tables, especially when there are many primary keys used in summarization. Nevertheless, disk space usage grows fast and such tables/views need to be refreshed when new records are inserted or new variables (dimensions) are created.
 Filtering records at multiple data preparation stages
Selection of rows can be done at several stages, in different tables. Such fi ltering is done to discard outliers [13], to discard records with a signi fi cant missing information content (including referential integrity), to discard records whose potential contribution to the model provides no insight or sets of records whose characteristics deserve separate analysis. It is well known that pushing selection is the basic strategy to accelerate SPJ (select-project-join) queries [6], but such optimization is not easy to apply to multiple queries. A common solution we have used is to perform as much fi ltering as possible on one data set. This makes code maintenance easier and the query optimizer is able to exploit fi ltering predicates as much as possible.
 Multiple primary keys
Different subsets of large tables have different primary keys. This issue basically means such tables are not compatible with each other to join and aggregate directly. At some point in the SQL code such large tables with different primary keys must be joined and later summarized by the primary key of one of them. Join operations will be sl ow because indexing involves fore ign keys with larg e cardina lities, in contrast to common star joins in star schemas. Two solutions are common: creating a secondary index on the alternative primary key of the largest table, or creating a denormalized table having both primary keys in order to enable fast join processing.

For instance, consider a data mining project in a bank that requires analysis by customer id ,butalso account id . One customer may have mu ltiple acc ounts. An account may have m ultiple account holders. Joining and manipulating such tables is challenging given their sizes. Data mining models may be built by customer or by account, having some statistical variables in common.
 Model deployment
Even though many models are built outside the database system with data mining tools, in the end the model must be applied or deployed in the database [16]. When volumes of data are not large, it is feasible to perform model deployment outside: exporting data sets, applying the model and building reports can be done in no more than a few minutes. However, as data volume increases exporting data from the database system becomes a bottleneck [ 14,16]. This pr oblem gets com pounded with results interpretation when it is necessary to relate models back to the original tables in the database. Therefore, it is common to build models outside, frequently based on samples, and then once an acceptable model is obtained, then it is imported back into the database system. Nowadays, model deployment basically happens in two ways: using SQL queries if the mathematical computations are relatively simple or with UDFs [14,16], if the computations are more sophisticated. In most cases, such scoring process can work in a single table scan, providing good performance.
 Database system server resources usage This aspect includes both disk and CPU usage, with the second one being a more valuable resource. This problem gets compounded by the fact that most data mining tasks work on the entire data set or large subsets from it. In an active database environment running data preparation tasks during peak usage hours can degrade performance since, generally speaking, large tables are read and large tables are created. Therefore, it is necessary to use workload management tools to optimize queries from several users together. In general, the solution is to give data preparation tasks a lower priority than the priority for queries from interactive users. On a longer term strategy, it is best to organize data mining projects around common data sets, but such goal is dif fi cult to reach given the mathematical nature of analysis and the ever changing nature of variables (dimensions) in the data sets.
 Lessons learned: most useful queries
Data transformation is a time consuming task in any data mining project since the data set needs to be built tailored to the problem at hand. In general, data mining practitioners think writing data transformations in SQL is not easy. The reason is simple: despite the abundance of data mining tools users still need to understand the basics of query pro cessing. The most common queries needed to create data sets for data mining are listed below.  X  Left outer joins, which are useful to build a  X  X niversal X  data set containing all records (observations)  X  Aggregations, generally with sums and counts, to build a data set with new columns.  X  Denormalization, to gather columns scattered in several tables together.  X  Cross-tabulation, which combines transposition (pivoting) and aggregation to create a data with  X  Variable transformation in which the number of r ecords remains the same, but the number of columns  X  Rewriting complex queries to perform aggregations before joins, when query result correctness is 3. Translating data transformations in a high-level programming language into SQL queries
An important goal is to automate data transformation whenever there is code already written. This section gives guidelines to translate common transformations in a high-level programming language into SQL. We point out similarities and differences be tween both languages. Finally, we explain how to ef fi ciently compute common suf fi cient statistics in SQL that bene fi t several models, which effectively summarize large data sets. 3.1. De fi nitions In general, the goal of data preparation (and transformation) is to build a so-called  X  X nalytic X  data set. Let X = { x 1 ,...,x features). Each attribute can be numeric or categorical.
 In a high-level programming language the data set is manipulated as a fi le of records, as de fi ned above. each record values may be separated by a special character (a comma for instance) or they may have a fi xed length (i.e. values are located by position). We do not consider the case where data sets are stored as binary fi les for ef fi cient random access since such format is not commonly used by data mining practitioners. In SQL the terms used to manipulate a data set are table, row and column. A table contains a set of rows having the same columns. The order of rows is immaterial from a semantic point of view. A set of tables is linked by foreign key [6] relationships. 3.2. Data set storage
In a high level programming language (e.g. C++, Java, SAS) a program produces data sets which are basically fi les with a set of records, where each record has fi xed columns. Columns can be of numeric, string or date (time) data types. On the other hand, SQL manipulates data as tables, with the additional constraint that they must have a primary key (sometimes arti fi cially generated). In general, data sets in a high-level programming language are sequentially scanned, loaded and processed in main memory row by row. On the other hand, in SQL relational operations receive tables as input and produce one table as output, making internal processing in blocks of rows. 3.3. Statement translation
We distinguish two main classes of statements making a program that require different translation mechanisms: (1) Data manipulation (data set transformation). (2) Function calls (e.g. procedures, methods). Most statements we discuss are for data manipulation.
 Arithmetic equations
Columns in the data set are treated as variables. In a compiled programming language every variable must be declared, which can be used as columns in a temporary table. An assignment statement assigns a value to an existing variable. In SQL there is no assignment expression. Therefore, the assignment expression must be converted into SELECT statements with one column to receive the result of each expression. A sequence of variable assignments updates variables with arithmetic expressions. Most math functions in a high level programming language have one argument and they can be easily translated using a dictionary. String functions are more dif fi cult to translate because besides having different names they may have different arguments and some of them do not have an equivalent in SQL.
 If-then-else
A high-level programming language provides great fl exibility in controlling assignments. This is more restricted in SQL because only one column can be manipulated in a term. We consider three cases: (1) Chained IF-THEN statements with one assignment per condition; (2) Generalized IF-THEN-ELSE with nesting and two or more assignments per condition. A chained IF-THEN statement is translated into an SQL CASE statement where each IF cond ition is transformed in to a WHEN clause. It is important to watch out for new columns when new variables are declared. IF statements can be nested several levels. There may be loops with array variables. This case will be analyzed separately. Nested IF-THEN statements are fl attened into  X  X HEN X  substatements in a CASE statement. Comparison for numbers and strings use similar operators. However, date comparisons are different and therefore require special functions. Comparison operators have similar syntax in both languages, whose translation requires a simple equivalence table. Negation (NOT), parenthesis and strings, require similar translation (compilation) techniques.
 Looping constructs
We pay particular attention to FOR loops, which are particularly useful to manipulate matrices and vectors. In general, the number of iterations is fi xed. On the other hand, WHILE loops, with an undetermined number of iterations, are generally required to scan the input data set, which is done automatically by SQL. In a high-level programming language there are arrays used to manipulate variables with subscripts. SQL does not provide arrays, but they can be simulated by generating columns whose name has the subscript appended. A FOR loop is straightforward to translate when the subscript range can be determined at translation time; the most common case is a loop where the bounds are static. When an array is indexed with a subscript that has a complex expression (e.g. a [ i  X  10  X  j ] ) then the translation is more dif fi cult because the target column name cannot be known at translation time. Combining different data sets
We focus on two basic operations: (1) Union of two data sets; (2) Merging two data sets. Further combinations used these two operations as primitives.

Union: This is the case when the user wants to compute the union of data sets where most or all the columns are equal in each data set, but records are different. The main issue here is that such statement does not guarantee all data sets have the same columns. Therefore, the translation must make sure the result data set includes all variables from all data sets setting to null those variables that are not present for a particular data set. A total order must be imposed on column s so that each position correspond to one column from U . Then we just need to insert nulls in the corresponding variables when the result table is populated.

Merging: This is the case where two data sets have a common key, some of the remaining columns are the same and the rest are different. If there are other common columns (other than the key) between both data sets they must be renamed in order to have unique column names. Similarly, SQL requires the user to rename columns with a new alias. In general, in a high-level programming language both data sets must be sorted by the matching column (s). Such sorting process is unnecessary in SQL since the join operation serves that purpose. A fi ltering process must be performed to detect common non-key columns. This translates into SQL as a full outer join to include unmatched rows from either data set. Translating function and subroutine calls Translating subroutine calls requires de fi ning a stack-based calling mechanism, which is dif fi cult in SQL. A solution is to create separate SQL stored procedures for each major function and then integrate their calls with additional stored procedures. An alternative is to create a fl at script in SQL that contains all  X  X nfolded X  function calls (i.e. substitutin g the function call with corresponding SQL code). 3.4. Similarities and differences
In the previous sections, we explained how to translate code in a high-level programming language into equivalent SQL statements. We now provide a summary of common features of both languages and where they differ. Language similarities
We present similarities going from straightforward to most important. Values can be stored on a declared variable only. In both languages it is necessary to declare variables and columns, respectively. However, a high-level programming language may include dynamic storage allocation with pointers. Such cases require paying close attention to which variables get stored on disk. In a high level program-ming language each variable is updated as new assignment expressions appear in the code. In SQL it is necessary to create a column for each declared variable. In SQL a column cannot be referenced if it has not been previously created with the  X  X S X  keyword or it comes from some joined table. In most cases, each new functio n reading or writing an entire fi le, reads or creates a new data set. Therefore, this aspect can be taken as a guideline to create temporary tables to store intermediate results.
 Language differences
In a high-level programming language there is explicit code to scan and process the data set record by record. In contrast, in SQL there is no need to create a loop to process each row: any SQL statement automatically processes the entire table. However, in the database system sometimes it is necessary to perform computations without using SQL to exploit arrays. In other words, regular looping constructs are still required (stored procedures partially solve the issue).

High-level programming languages do not have support for managing missing information. SQL provides specialized syntax and operators for null values. Files do not necessarily contain column names, although it is common to include them as a header or in a separate metadata fi le. On the other hand, SQL requires de fi ning column names before populating them with values. To store results a table cannot contain columns with the same name. Th erefore, for practical purposes duplicate column names must be removed during the SQL code writing. In a high level programming language sorting procedures are needed to merge data sets. Sorting is not needed in SQL to join data sets. In fact, there is no pre-de fi ned order among rows. Merging works in a different manner to joins. A data set is always manipulated in memory, but new variables may not necessarily be saved to disk. In SQL a new table must be created for each processing stage. Tables are stored on disk. Some tables are created in permanent storage, whereas the rest of tables have to be created in temporary storage. 3.5. Suf fi cient statistics
We now explain fundamental statistics computed on the data set obtained from the data transformation process. These statistics bene fi t a broad class of data mining models. Their computation can be considered an intermediate step between preparing data sets and computing statistical models. In the literature the following matrices are called suf fi cient statistics [2,9,14,16] because they are enough to substitute the data set being analyzed in mathematical equations, preserving its most important statistical properties. Therefore, it is preferable they are available for the data set to be analyzed.
Consider the multidimensional (multivariate) data set de fi ned in Section 3.1: X = { x 1 ,...,x n points, where each point has d dimensions. Some of the matrix manipulations we are about to introduce are well-known in statistics, but we exploit them in a database context. We introduce the following two matrices that are fundamental and common for all the techniques described above. Let L be the linear sum of points, in the sense that each point is taken at power 1. L is a d  X  1 matrix, shown below with sum and column-vector notation.
 Each entry is the linear sum of each dimension: Let Q be the quadratic sum of points, in the sense that each point is squared with a cross-product. Q is a d  X  d matrix.
 Matrix Q has sums of squares in the diagonal and sums of cross-products off the diagonal: The most important property about L and Q is that they are much smaller than X ,when n is large (i.e. d&lt;&lt;n ). However, L and Q summarize a lot of properties about X that can be exploited by several data mining techniques. In other words, L and Q can be exploited to rewrite equations so that they do not refer to X , which is a large matrix. Techniques that directly bene fi t from these summary matrices include correlation analysis linear regression [9], principal component analysis [9,8], factor analysis [9] and clustering. These statistics also partially bene fi t decision trees [8] and logistic regression [9].
Since SQL does not have general support for arrays these matrices are stored as tables using dimension subscripts as keys. Summary matrices can be ef fi ciently computed in two ways: using SQL queries or using UDFs [14,16]. SQL queries allow more fl exibility, are portable, but incur on higher overhead. On the other hand, UDFs are faster, but they depend on the database system architecture and therefore may have speci fi c limitations such as memory size and parameter passing. Having an automated way to compute summary matrices inside the database system simpli fi es the translation process. The SQL code to derive suf fi cient statistics for a full matrix Q on X is:
This statement is fast, but may present limitations when d is high (given a maximum number of columns allowed by the database system), in which case a vertical layout for X is required. For further details consult [16]. 4. Experience and feedback from real-life projects
This section presents performance comparisons, advantages and disadvantages when migrating actual data mining projects into the database system. This discussion is a summary of representative successful projects. We fi rst discuss a typical data warehousing environment; this section can be skipped by a reader familiar with the subject. Second, we present a summary of the data minin g projects presenting their practical application and data mining techniques used. Third, we present time measurements taken from actual projects at each organization, running data mining software on powerful database servers. We conclude with a summary of the main advantages and accomplishments for each project, as well as the main objections or concerns against migrating external code in a high-level programming language into the database system. 4.1. Data warehousing environment
The computing environment was a data warehouse, where several databases were already integrated into a large enterprise-wide database. The database server was surrounded by specialized computers, performing OLAP and data mining analyses, connected by a high-speed network. Some of those computers were used for data mining.
 First of all, an entire set of high-level programming language programs was translated into SQL. Second, in every case the data sets were veri fi ed to have the same contents in the external fi les and in SQL. In most cases, the numeric output from data mining models was the same, but sometimes there were slight numeric differences, given variations in algorithmic improvements and advanced parameters (e.g. epsilon for convergence, step-wise regression procedures, pruning method in decision tree, outlier handling and so on). 4.2. Background on companies and their data mining projects
We now give a brief discussion about the companies where the code migration took place. We also discuss the speci fi c type of data mining techniques used in each case. Due to privacy concerns we omit discussion of speci fi c information about each company (i.e. their name), their databases (size, speci fi c table names) and the hardware con fi guration of their database system servers (in the range of terabytes). We can mention all companies had large database systems managed by an SMP (Symmetric Multi-Processing) Te radata server having several GB of mem ory on each node and sev eral terabytes of disk storage. Our projects were conducted on the data warehouse production systems, concurrently with other users and their applications (e.g. analysts, managers, DBAs, and so on).

The fi rst organization was an insurancecompany. The data mining goalinvolved segmentingcustomers into tiers according to their pro fi tability based on demographic data, billing information and claims. The data mining techniques used to determine segments involved histograms and clustering. The fi nal data set had about n = 300 k records and d = 25 variables. There were four segments, categorizing customers from best to worst.

The second organization was a cellular telephone service provider. The data mining task involved predicting which customers were likely to upgrade their call service package or purchase a new handset. The default technique was logistic regression [9] with stepwise procedure for variable selection. The data set used for scoring had about n = 10 M records and d = 120 variables. The predicted variable was binary.

The third organization was an Internet Service Provider (ISP). The predictive task was to detect which customers were likely to disconnect service within a time window of a few months, based on their demographic data, billing information and service usage. The data mining techniques used in this case were decision trees and logistic regression and the predicted variable was binary. The fi nal data set had n = 3.5 M records and d = 50 variables. 4.3. Users feedback
We summarize main advantages of projects migrated into the database system as well as objections from users to do so. Table 3 contains a summary of pros and cons. As we can see performance to score data sets and transforming data sets are positive outcomes in every case. Building the models faster turned out not be as important because users relied on sampling to build models and several samples were collected to tune and test models. A second important advantage was security: there was no need to export sensitive information outside the database system. A third advantage was simpli fi ed data management as databases and their analysis was centralized. We now summarize the main objections, despite the advantages discussed above. We exclude cost as a decisive factor since all companies had enough budget to implement a large data warehouse and we did not have speci fi c cost information on hardware and software. First, many users preferred a traditional programming language like Java or C++ instead of a set-oriented language like SQL. Second, some specialized techniques are not available in the database system due to their mathematical complexity; relevant examples include Support Vector Machines, non-linear regression, neural nets and time series models. Finally, sampling is a standard mechanism to get approximate models from large data sets, although model error needs to be controlled. 4.4. Processing time comparison
We focus on comparing processing time running data mining processing inside the database system with SQL code (generated by a data mining program) and outside with existing programs developed by each organization. Such programs were developed mainly in Java and SAS and ran on the data mining practitioner workstati on. The comparison is not fair because the d atabase system server was in general a powerful parallel computer and the external computer was slower and smaller. However, the comparison represents a typical enterprise environment where the most powerful computer is precisely the database system server.

We now describe the computers in more detail. The database system server was, in general, a parallel multiprocessor computer with a large number of CPUs, ample memory per CPU and several terabytes of parallel disk storage in high performance RAID con fi gurations. On the other hand, the external computer was generally a smaller computer with less than 500 GB of disk space with ample memory space. Data mining processing inside the Teradata DBMS was performed only with SQL and UDFs. In general, a workstation connected to each server with appropriate clie nt utilitie s. Client comput ers connected with ODBC/JDBC. Data sets were exported with fast bulk u tilities. All time measurements discussed herein were taken on 32-bit CPUs over the course of several years. Therefore, they cannot be compared with each other and they should only be used to understand performance gains within the same organization.
We discuss tables from the database in more detail. There were several input tables coming from a large normalized enterprise data warehouse that were joined, transformed and denormalized to build data sets used by data mining techniques. In short, the input were many tables and the output were a few tables (in many cases only one). No data sets were exported in this case: all processing happened inside the database system exploiting SQL and UDFs [16]. On the other hand, analysis on the data mining computer relied on bulk utilities, ODBC and SQL to extract data from the database system. Most transformations happened outside the database system. Data mining models were computed in an external data mining program (e.g. statistical package, data mining program, custom code). Once an acceptable was obtained, data sets were scored inside the database system with SQL code (manually written or generated by a tool). In general, data extraction from the database system was performed using fast bulk utilities which e xported data records in blocks.

Table 1 compares performance between both alternatives: inside and outside the database system. As explained above, we distinguish two phases in each project: building the model and scoring (deploying) the model on large data sets. he times shown in Table 1 include the time to transform the data set with joins and aggregations and the time to compute or apply the actual model. As we can see the database system is signi fi cantly faster. We must mention that to build the predictive models both approaches exploited samples from a large data set. Then the models were tuned with further samples. To score data sets the gap is wider, highlighting the ef fi ciency of SQL to compute joins and aggregations to build the data set and then compute model equations on the data set. In general, the main reason the data mining computer was slower was the time to export data from the database system and to a second extent its more limited computing power.

We decided to analyze if exporting data sets is i ndeed a bottleneck on a s eparate computer. The database system runs on a Windows Server with CPU running at 3.2 GHz, 4 GB on memory and 1 TB on disk. Table 2 compares time performance to compute a linear model inside the database system and the time to export the data set with the ODBC interface and bulk utilities. Clearly, there exists a bottleneck when exporting data from the database system to th e data mining server, ev en with bulk utilities. The linear models include PCA and linear regression, which can be derived from the correlation matrix of X in a single table scan using SQL and UDFs [16]. Clearly, exporting a data set is a bottleneck to analyze X outside the database system, regardless of how fast an external program is. Exporting a small sample from X can accelerate processing, but analyzing a large data set without sampling is much faster to do inside the database system. 5. Related work
Creating new attributes for analysis has received attention. One important technique is constructive induction [1,11], where relevant attributes to a learning task are created. Such attributes expand the representation space and enable a descriptive summarization of input records. In our case, building attributes with aggregations is the closest transformation to constructive induction since it provides a summarized representation of records. However, such construction is guided by the practitioner and domain knowledge. Deciding which new attributes are more relevant and setting the best aggregation granularity are aspects which could be improved with constructive induction. On the other hand, new numerical attributes are constructed from binary ones in the IGLUE system [11] through constructive in-duction; this approach is restricted to binary attributes (representing concepts). In our case, this technique could be applied to summarize combinations of categorical attributes, coded as binary attributes.
There exist many proposals that extend SQL with da ta mining functionality. Relational database systems provide advanced aggregate functions to compute linear regression and correlation, but it only does it for two dimensions. Most research proposals add syntax to SQL and optimize queries using the proposed extensions, but have limited applicability as they require extensive changes to the database system source code. UDFs implementing common vector operations are proposed in [16], which shows UDFs are as ef fi cient as automatically generated SQL queries with arithmetic expressions, proves queries calling scalar UDFs are signi fi cantly more ef fi cient than equivalent queries using SQL aggregations and shows scalar UDFs are I/O bound. Several SQL extensions to de fi ne, query and deploy data mining models are discussed in [8]; such extensions provide a friendly language interface to manage data mining models. This proposal focuses on managing models rather than computing them and therefore such extensions are complementary to our UDFs. Computation of suf fi cient statistics for classi fi cation in a relational database system is proposed in [7]. Suf fi cient statistics have been generalized and implemented as a primitive function using UDFs bene fi ting several data mining techniques [16]; this work explains the computation and application of summary matrices in detail for correlation, linear regression, PCA and clustering.

Some related work on exploiting SQL for data manipulation tasks includes the following. Data mining primitive operators are proposed in [4],including an operator to pivot a table and another one for sampling, useful to build data sets. The pivot and unpivot operators are useful to transpose and transform data sets for data mining and OLAP tasks [5], but they have not been standardized. Horizontal aggregations were proposed to create tabular data sets [12], as required by data mining techniques, combining pivoting and aggregation in one function. For the most part research work on preparing data sets for analytic purposes in a relational database system remains scarce. To the best of our knowledge there is scarce research work on the migration of data mining data preparation into a database system. For further reference on preprocessing data sets in practice, consult [8]. A preliminary version of this article appeared in [18]. 6. Conclusions
We presented our experience migrating data set preprocessing and transformation performed by ex-ternal programs into a database system exploiting t he SQL language. In general, data preprocessing is the most time consuming, not well planned and most error-prone task in a data mining project. Summa-rization generally has to be done at different aggregation granularity levels and such levels are generally not hierarchi cal with respect to each other (they are best rep resented by a lattice) . Input data records are selected based on a time window, which requires indexes on date columns. Row selection ( fi ltering) with complex predicates happens on many tables (including temporary tables), making code maintenance and query optimization dif fi cult. To improve performance it is necessary to create temporary denormalized tables with summarized data. In general, it is necessary to create a  X  X niverse X  data set to compute left outer joins in order to preserve every available record. Model deployment requires importing data mining models as SQL queries or UDFs to apply the model on new records. We brie fl y explained how to compute a powerful set of suf fi cient statistics on a data set, that bene fi t a broad class of data mining techniques, including correlation analysis, clustering, principal component analysis and linear regression. We presented a summary of main advantages when migrating programs written in traditional programming languages into the database system by translating them into optimized SQL code. Also, we presented a processing time comparison. We showed transforming and scoring data sets is much faster inside the database system, whereas building a model is also faster, but less signi fi cant because sampling can help analyzing large data sets. In summary, users can enjoy the extensive data management capabilities provided by the database system (querying, recovery, security and concurrency control) and data set preprocessing, transformation and analysis are faster inside the database system.
There are several opportun ities for future work. We would like to cre ate automated SQL code generation for common data preparation tasks like summarizing, cross-tabulation and denormalization to create data sets. Constructive induction can help creating relevant attributes. We would like to automate the translation of existing data transformation programs in high-level programming languages into SQL, identifying useful primitive operations. We would like to create a tool that can combine external processing in a high level programming language with SQL running on a database system, bene fi ting the user with both external mathematical libraries and the speed and query capabilities provided by SQL. Acknowledgments
This work was partially supported by National Science Foundation grants CCF 0937562 and IIS 0914861. This article summarizes experience and lessons learned while the author was working for Teradata Corporation, providing data mining consulting services to companies having large data ware-houses. The author thanks Michael J. Rote from Teradata Corporation for his valuable guidance and support to migrate many data mining projects into the DBMS.
 References
