 Based on the users X  education background, EP can give them different search results when the input the same the query by users X  active behavior, who know their own education background and set th e search area themselves, which make EP search result with high correction, but also leads to potential information loosing, for users have no idea of these knowledge exiting, for their limited ed-ucational. User also can get all search results from the homepage, independent of education background, but it made low correction and lots of time wasting to find what they really needed. There are mainly 6 branches is our home-pages for users who needs some information: (1)Advanced Search (including K-12 search, Higher Education search, Geocentric search), (2) Browse Learning Resources(including Subject/Disciplinary Content Areas, Special Topics, Grade Levels, Learning Resource Types, ABET Outcomes (Students, Learning), Host Collections, PR2OVE-IT Interventions), (3)K-12 Community, (4)Higher Edu-cation Community, (5)Disciplinary Communities, (6)Broadening Participation. And 2 branches for introduction about us: Premier Award and About us, and 2 branches used for interaction between user and us: (1)Submit Resource and (2)My workspace. The homepage as shown in Figure 1.

What EP can do now are: (1)Based on the users X  education background, give them different search results when they input the same the query (K-12, and so on), this function is implemented by users X  active behavior, they know their own education background, and set the search area by themselves. Pros: high correction; Cons: needs users active input; lose some information that may help them, they do not know the exiting of these knowledge, constrains by their lim-ited educational level; (2)User can get all the search results in the homepage, independent of their education background, In this function, user X  X  active behav-ior is no need, and the pros and cons are as follows: Pros are mostly, no useful information and knowledge will be missed, Cons are low correction and waste lots of time to find what they really needed.

So in this paper, we proposed a new EP that can give different and personal search recommendation for users with di fferent educational level and accom-plish this function automatically. For example, if the user is an undergraduate, he/she want to search some information about Advanced Mathematics, we can give his/her course ware, video and homework answers, or something can help them prepare for the test. What if we give him/her something in graduate, such as some papers and instructions about engineering application with advanced mathematics? If the user is a college teacher, and he/she also want to search some information about Advanced Mathematics, we can give him/her some-thing the same as the former user, such as course ware and video, to help them improve teaching, but we can also give him/her some research material, such as papers with higher theoretical level  X  X hich is contrary to the former under-graduate users. So what else we can do here, since we already have users X  log by powerful EP.

For data, firstly, we research semantic relationships among knowledge, then we classify or cluster them, finally we establis h the knowledge relationships models. For users, firstly, based on user log, we can set up user profile, then classify or cluster the users type, finally we establis h the users model, So, when a frequent user come to EP looking for something, we can give information directly related and recommend knowledge not directly related but can arouse their interest, based on the users model and knowledge relationships models. Based on users log mining, user model, data relationship analysis(semantic, classification, clus-tering), we will make EP a more excellent expert who know users well enough to guide them, according to the statistic information such as education back-ground. And by improving the collaborative recommend results in EP, our users can make the best use of their time by EP learning. In our previous works, we had done lots work on Digital Library[1,3]. How can electronic course ware meet the diverse n eeds of curricula am ongacrosssection of universities? How do educators adapt traditional teaching roles to fit new resources and delivery styles? What co urse ware access modes equally suit the needs of author, teacher, and student? Can an infrastructure designed for static course ware be adapted to dynamically changing information on the World Wide Web? In [4], for the experience of Synthesis/NEEDS( the National Engineering Education Delivery System) answered these questions while opening more is-sues in distance independent education by striving to integrate multidisciplinary, open-ended problem solving into the varied engineering curricu la of its members.
The focus of this research [5] lies in ascertaining tacit knowledge to model the information needs of the users of an engineering information system. It is proposed that the combination of reading time and the semantics of documents accessed by users reflect their tacit know ledge. By combining the computational text analysis tool of Latent Semantic Analysis with analyzes of on-line user transaction logs, we introduce the technique of Latent Interest Analysis (LIA) to model information needs based on tacit knowledge through user X  X  queries and prior documents downloaded; it was incorporated into our digital library to recommend engineering educ ation materials to users.

There are lots of new trends in recently i nformation ages, The digital library are improving with the change of information format as reported in the following research work. For many new services 2.0 are appearing everyday,eg., Facebook, Flickr, Jesus et.al[6] designs a system allows the reduction of the necessary time to find collaborators and information about digital resources depending on the user needs by using Google Wave to exten d the concept of Library 2.0. Social tagging or collaborative tagging is also a new trend in digital age. So,by linking social tags to a controlled vocabulary, Yi [7] did a study to investigate ways of predicting relevant subject headings for resources from social tags assigned to the resources based on different sim ilarity measuring techniques. Another trend is construction of semantic digital libraries. Jiang et al. [8] researched a clustering method based on normalized compression distance for  X  X ffiliation, X  its an important type of meta data in publications areas, and also a question hard to resolve when converts its meta data of digital resources into its semantic web data.

There are also many recent researches f ocus on the relationship among the retrieval system and users X  search behavior. Such as Catherine L. Smith and Paul B. Kantor [9], through adapting a two-way street, they did a factorial ex-periment by manipulated a standard search system to produce degraded results and studied how people solve the problem of search failure.

Like our EP, Lin and Smucker[10] researched a content-similarity browsing tool can compensate for poor retrieval results to help their users. The goal of educational digital library applications is to provide both teachers and students enriching and motivational educational resources [11, 12, 13] Jing and Qingjun [14]and Ankem[15]provide teachers and students a virtual knowledge environ-ment where students and teachers enjoy a high rate of participation. Alias et al.[16] describe an implementation of digital libraries that integrates semantic research. Mobile ad hoc networks are becoming an important part of the digital library ecology[17,13,18]. We have workspace for each register uses, when users using EP search find some-thing interested in, they can add them to their own workspace, For example, user Lucy login EP, and do some research for  X  X ata mining, X   X  X nformation retrieval X  in the  X  X dvance Search X  from the Discipline  X  X omputer Science X  and From grade  X  X ollege Freshman X  through  X  X ontinuing Education X  as shown in Figure 2, and we can get lots of search result as shown in Figure 3, Lucy choice the one with title  X  X sing the Data Warehouse X , click the link under the title, and she can get the detail information about this learning resource, as shown in Figure 4, then, if she interested in it ,she can save it to her work space, after saving the interested learning resource to her workspace, you can edit it or remove it from her workspace, as shown in Figure 5, the last one with title  X  X sing the Data Warehouse X , which her added it just now.

Based on so many users X  profiles in our data base, we can analyze the users X  interest and calculate the similarity be tween their interested resources and the others, finally give them what resources they may interested in but they have no idea of their exiting. For doing the database SQL query on three tables in our database: smete user , collection, and collection member, we can find out there are 102 users have more than 10 lea rning resources in their workspace, 33 users have more than 50 learning resou rces in their workspace, 20 users have more than 100 learning resources in their workspace, 5 users have more than 500 learning resources in their workspace, and the top 3 users have 4535,1738,1122 learning resources. There are three main algorithms we can use for large database clustering al-gorithm setting: BIRCH[19], MST[20], and CURE[21], we take the CURE for short text clustering. In ou r experiment, the short text clustering are the ti-tles in user X  X  workspace.We still take Lu cy X  X  profile as an example; there are 26 learning resources in her workspace, we using their titles as input, in our cluster part of recommendation system, we can see the results from Figure 6. And we can figure out that Lucy X  X  interest keywords maybe:  X  X esign X ,  X  X PC-A X ,  X  X om-puting X ,  X  X ducation X ,  X  X ollection X  and  X  X ohn Wiley Sons X ,according to these Lucy X  X  potential interesting area, we can get her potential interesting learning resources are as shown in Figure 7.

In our system, user can decide the number of cluster we want to get, or we can decide the number of item in each cluster, or we do not decide everything, but leave the algorithm to select the best number of cluster based on differ-ent evaluations for the distances between clusters: such as Euclidian distance, Jaccard similarity, Dice similarity, and Manhattan distance. The interface of our system as shown in Figure 8.

In the process of title clustering, we use cosine similarity to identify the correlated title pairs. The features of the first title to be clustered is X = &lt; x ,x 2 , ..., x n &gt; , and the features of the second title to be clustered is Y = &lt; y ,y 2 , ..., y n &gt; .The cosine similarity of titles to be clustered X and Y is nodes in the term graph.

Given the dataset D and the number of clusters K , C = &lt;C 1 ,C 2 , ..., C k &gt; , we can evaluate this cluster results by CH ( Calinski  X  Harabasz ) [28]index method, by doing Trial and error k , we can get the best cluster number when gets its maximum value. This index is computed as: V Algorithm 1. Title cluster based on CURE Algorithm Algorithm 2. EP Recom algorithm The maximum hierarchy level is used to i ndicate the correct nu mber of partitions in the data. traceB is the trace of the between cluster scatter matrix B ,and traceW is the trace of the within cluster scatter matrix W . The meanings of parameters for them are: n j is the number of points in cluster, $ ... $ means a certain distance ca lculation method, z is the centroid of the entire data set. z j is the centroid of the C j data set, x i is the i-th item in C j ,and the number is x in each C j is obviously n j . In this section, we present the result s of our experiment. The EPRecom dataset originally contained 120,000 ratings from 3125 users on 6250 learning resources. We gathered the top-20 recommendations for all users in the EPRecom dataset by using 4 methods, ItemRank, Tangent, PPTM, UPKR with c=0.01,0.001, c value is chosen experimentally. Since our method and Tangent require precom-puted relevance scores, we run Tangent and PPTM based on the relevance scores of ItemRank. 5.1 Popularity The Figure 9 shows comparison of recommendation from three algorithms in terms of popularity. In this figure, we present the difference between the distribu-tion of rating and recommendation. From the results, our method outperformed competitors in terms of popularity matching. As analyzed in Section III, existing methods tend to recommend more items having high popularity. However, the result of UPKR is quite similar to that of rating so the differences is small for all degree of popularity. The comparison results for each individual are dropped from this paper because they are similar to the overall comparison results. All results for each individual are published on our website. 5.2 Diversity We also measure the novelty of our method in terms of diversity in a way similar to the one we used in the analysis. The Figure 14 is the distribution of top-10 recommendation using UPKR with c = 0.002. The figure is generated in same way as we did in the investigation. Our method shows more diversified results and the over concentrated items set is sm aller than that of others. In addition, the inner line indicating the coverage of UPKR is also located higher than that of other methods.To compare the results more clearly, we measure the diversity and coverage of each method (Figure 10)quantitatively. First, we measure EMD distance for the distribution of each method from that of rating (Figure 12). Our method always shows a smaller EMD distance,which indicates more diversified results, than other methods regardless of the number of recommendations. Note that,in this evaluation, EMD distance measures the distance between distribu-tion of rating and recommendation. Thus, it X  X  independent from EMD distance for PPTs. Furthermore,the coverage of our method is also higher than that of other competitors (Figure 13). When we recommend 50 items for each user, the coverage of UPKR is over 60% but that of ItemRank is near 45% and that of Tangent is around 25%.All these evaluation results show that our method is better than other methods in terms of diversity. 5.3 Accuracy We evaluate the accuracy of our method a nd competitor. To compare the accu-racy, we run 3-fold cross validation 30 times for each 942 users and aggregated the result. For more detail, we divide rating history of a user into 3 folds. After that, we use 2 folders to infer the tast e of user,and measure the accuracy by comparing the recommendation result with remaining 1 fold, which is a relevant item set for test. We use recall to meas ure accuracy. It is because our method is a top-k recommendation method thus traditional accuracy measures, which are based on scores or the order of item, do not work. In this equation, the accuracy increases when the method recommends more items which are in the relevant item set for the test. We measure the accuracy change according to the change of the number of recommendations, and compare the accuracy of UPKR and Tangent to that of ItemRank (Figure 10). If the accuracy of a method is the same as ItemRank,it will be 1 for all k. It turns out that Tangent suffers from a performance degradation of around 15%-20%. On the other hand, our method shows better results than ItemRank when c is 0.001.Excessive engagement of PPT also results in a performance degradation of around 10 % but, still, it is better than Tangent. 5.4 Qualitative Evaluation We conduct a case study for user 585 to evaluate the recommendation results also in a qualitative way. To the best of our knowledge, the user prefers films having practical value and reputation. In terms of popularity, the user also study many courses that gross satisfy rate is high. The satisfy rates of most courses that learned are less than. The average gross was 80%. However, in the recom-mendations of ItemRank, there are already 3 out of 10 courses that given less than 75% satisfy rate. The average satisfy rate of ItemRank recommendation is higher than that of the learned list. In the case of Tangent, it is more severe. Only 2 courses, Introduction to Computers and Descriptive Introduction to Physics, given more than 75% satisfy rate. The average satisfy rate of UPKR is 81% which is higher than the average the learned list. Furthermore, newly added items do not seem to be suitable for the interest of the user. After this we can use our rec-ommendation system as shown in the bottom of Figure 13  X  X reate user Profile X , for these users, there are two methods for them who want to get recommended learning resources with sim ilar interests of they viewed before, the first one to tell our EP about more detail about themselves,such as major, education backgroud and so on, so we can get recommendation ,based on the similarity among profiles of users and profiles of learning resources, (see Figure 13). The other method is to cluster the learning resources in user X  X  workspace,based on the profiles of learning resources, we can infer a particular user X  X  profile.

Then we do some demographics of the users during an extended time period from January 1, 2011, to November 31, 2011 based on Google Analytic, there are 52,572 visitors with absolute unique 42,951 visitors came from 166 coun-tries/territories, as shown in Figure 6, they use 83 languages with 9,698 service providers, 31 browsers, 19 operating systems, and 66 browser and OS combina-tions, via 685 sources and mediums. All of the search are via 12,821 keywords, and the top-10 keywords as shown in Figure 14. We proposed the implementation of user recommendations to support deliver of materials from the Engineering Pathways (NSDL) website. a clustering mecha-nism is used as the basis for generating recommendations for users of the Engi-neering Pathway DL, and an experiment was conducted in order to assess the validity or potential of the proposed approach. By describing an optimal Cure clustering algorithms for short text are and summarizing web-log data from users, we can personalize s earch results based on users X  profiles. Based on our system, we can implement much more personal and much more precise learning resource recommendation,in fact, the CURE algorithm could also be used for discovering the outlier learning resource information,which in our case, the out-line learning resources must not relevan t with user X  X  major directly, but it may be another new research area this user ma y interested in, or maybe a potential information contributions to the user X  X  major, in this case it will help mining the relationship among multiple and different majors and educational background in our future work.
 Acknowledgments. This work was supported by a grant from the National Natural Science Foundati on of China (No. 60970018).

