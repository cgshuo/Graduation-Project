 Since its debut the Explicit Semantic Analysis (ESA) has received much attention in the IR community. ESA has been proven to per-form surprisingly well in several tasks and in different contexts. However, given the conceptual motivation for ESA, recent work has observed unexpected behavior. In this paper we look at the foundations of ESA from a theoretical point of view and employ a general probabilistic model for term weights which reveals how ESA actually works. Based on this model we explain some of the phenomena that have been observed in previous work and support our findings with new experiments. Moreover, we provide a theo-retical grounding on how the size and the composition of the index collection affect the ESA-based computation of similarity values for texts.
 Categories and Subject Descriptors : H.3.3 [Information Stor-age and Retrieval]: Information Search and Retrieval X  Retrieval models ; H.1.1 [Models and Principles]: Systems and Information Theory X  General systems theory General Terms : Experimentation, Standardization, Theory Gabrilovich and Markovitch introduced the concept of Explicit Semantic Analysis (ESA) in 2007 [4]. The idea underlying ESA is to represent and compare texts (from single terms to entire doc-uments) as vectors in a high dimensional concept space . Each di-mension in this space corresponds to an explicit semantic concept, where the concepts are considered to be  X  X hematically (or topically) orthogonal X . The entries in the concept vector of a given text quan-tify the associations between the text and the respective concepts. In order to compute such association values, each concept is rep-resented by an index document. Gabrilovich and Markovitch use Wikipedia articles as index documents since Wikipedia covers a wide range of topics, while each article is focused on one topic. This topical focus is needed for the orthogonality property of the concepts and is referred to as concept hypothesis .

Since its introduction the ESA model has been adopted quickly in the IR community, which might be attributed to the following facts: the basic idea is pretty simple, Wikipedia is freely available, and the concept representation of documents is easy to interpret. Table 1: Correlation of ESA-based similarities with human as-sessments, depending on index collection, number of index doc-uments, and weighting scheme (reproduced from [1]).
 However, recent work reveals that the central concept hypoth e-sis is not required [1]. It has been shown experimentally that in-stead of the conceptually orthogonal Wikipedia articles, using doc-uments from the Reuters corpus as well as randomly concatenated Wikipedia articles lead to a comparable performance. Even when using random term weights to construct the vectors representing concepts, ESA still achieves quite good performance values. These unexpected results cannot be aligned with the common explanation of ESA and motivate a deeper analysis of how ESA actually works.
The paper in hand takes a theoretical look at ESA and proposes a probabilistic term weight model to describe the computed simi-larity values. Our contributions are threefold: (1) we show which information of an index collection is actually exploited by ESA, (2) we investigate the impact of certain features of an index col-lection (such as size and composition), and (3) we explain the surprisingly good performance of ESA when using index vectors with random term weights. Altogether, the paper gives further evi-dence that ESA is a variation of the generalized vector space model (GVSM) [8].
ESA was introduced as an approach to compute the semantic relatedness of terms or short phrases [4] X  X e explain the tech-nical details in Section 2.2. Meanwhile, ESA has been adopted successfully in many applications. In [3] ESA contributes directly to the estimation of the relevance of documents for a given query. In other settings ESA is used to compute the semantic relatedness of terms [5], which are then used as parameters in other retrieval models (e.g., an extension of BM-25). A cross-lingual extension (CL-ESA) that exploits interlanguage links of Wikipedia articles is covered in [6] and [7]. In [2] it is shown that CL-ESA is superior to other retrieval models which are based on implicit semantics.
Anderka and Stein revisited ESA in [1] and found syntactic par-allels to the GVSM X  X  brief introduction of the GVSM is given in Section 2.1. They also present some initial analysis targeting the impact of the index collection on the performance of ESA, s ee Table 1. Two findings are quite interesting: First, employing Wikipedia as index collection does not perform best; e.g., using the Reuters corpus results in a better performance, even though the concept hypothesis is not satisfied in this case. Second, using index vectors with random term weights (Random Gaussian) performs nearly as good as using Wikipedia articles. This eventually led the authors to the conclusion, that the initial concept hypothesis in ESA does not hold.
Under the traditional vector space model (VSM) a document x is represented as a vector x based on a weighted combination of n -dimensional term vectors t j : where w ( t j , x ) is the weight of term t j in document x . The sim-ilarity between two documents x and y is measured via the inner product of the respective vectors x and y :
In the classical VSM, the term vectors t j have only a single non-zero entry, are orthogonal and of unit length. This allows to repre-sent documents as term weight vectors and the above product be-renders the VSM incapable of capturing interdependence of terms.
The generalized vector space model (GVSM) [8] considers term correlations. Based on term co-occurrence information, the values for h t j , t k i are estimated and stored as entries g jk This allows for maintaining the document representation as weight vectors and formulating the similarity of two documents x and y as follows:
Let D be an index collection of n documents, each of which describing a single concept. Further, let V be a vocabulary with m different terms that occur in D . In the original publication [4] the index collection is build from Wikipedia articles in order to cover a wide range of different concepts. Under the ESA model, a document x is represented as a concept vector u , where each entry in u corresponds to the inner product of x and d i , with d where x and d i are the VSM representations of x and d i , which are m -dimensional vectors containing a weight for each term in V . Here, the original publication suggests to use a standard tf-idf weighing scheme. The inner product h d i , x i corresponds to the similarity of x and d i under the VSM; it quantifies the association between x and the concept which is defined by d i .

Under the ESA model, the similarity between two documents x and y is defined by the cosine measure that is applied to the respec-tive concept vectors u and v :
The findings in [1] motivate a deeper analysis of ESA and the need to understand what actually happens when comparing vectors in the concept space. We start with a reformulation of sim Equation 7) using the definitions of the concept vector entries: structural reformulation of ESA as GVSM, which is also observed in [1]. The g jk are of particular interest as they contribute the en-tries in matrix G , see Equation 5. In Section 3.1 we will take a closer look at how these values evolve.

First we note that the g jk also appear in the length normalization:
The recurrence of the g jk is of interest because it allows us to scale G with a factor a &gt; 0 without affecting the similarity mea-sure. This can be seen by extending each g jk into a 1 a g factoring 1 a o ut of the sum: sim ESA ( x, y ) =
The same can be done in the calculations of the vector lengths for u and v in the denominator of the fraction marked with the asterisk (  X  ). The resulting i n the numerator. I.e., we can scale the g jk without changing counterbalance the increasing values in the entries of the matrix for growing n . Altogether, we will work with a scaled version G  X  of the matrix with the entries:
Based on G  X  , the ESA similarity measure can be formulated in analogy to the GVSM: where the only difference to Equation 5 is the length normalization factor; 1 | u | can be represented in the form
I n the original GVSM this factor is omitted since the weight vec-tors are considered to be normalized.
I n order to derive conclusions about the values of the g  X  model the term weights w ( t j , d i ) for the documents d dom variables. Typically, term weights are based on features such as term frequency, document frequency, or document length. In many (probabilistic) IR models, these features are modeled as ran-dom variables themselves. So, the idea to assume the term weights to be random variables is not far fetched.

The representation of a document d i is assumed to be a tuple of random variables ( W 1 i , W 2 i , . . . , W mi ) , where W weight of term t j . For each term t j we consider all W ji over the documents. This implies two assumptions: (1) the term weights in a particular document do not depend on weights in other documents, and (2) the joint distribution of all weights is the same for each document.

Both assumptions can be derived directly from our document model. We consider a document as the realization of the terms occurring in it; i.e., terms form a document X  X nd not the other way around. Of course, there will be latent influences in a document (author, topic, intended auditorium, language style, etc.) which in-fluence the choice and joint distribution of the words. These can be captured by the interdependence of the W ji within a document, about which we explicitly make no assumption. Based on this doc-ument model, the g  X  ik represent the mean over products of random variables W ji :
Since the W ji are i.i.d. with respect to the documents, the law of large numbers provides the almost sure convergence for n  X  X  X  : g
I.e., in general the entries g  X  jk converge to a representation that captures the covariance of the term weights. This suits the concept of the matrix G in the GVSM, where the entries are supposed to represent the correlation of term vectors; however, the approach to determine the correlation values is different. 2 A second interesting observation is that a single g  X  jk determines the measure of semantic relatedness for the terms t j and t k . So, ESA considers the common distribution of the weights of those two terms in the index collec-tion to measure semantic relatedness. The covariance denotes the deviation from a random and uncorrelated co-occurrence of terms. Hence, rather than exploiting orthogonal concepts, ESA actually benefits from redundant usage of terms in documents. This reveals that there is no substantive evidence for the benefit of the concept hypothesis.
The ESA formulation of the previous section will help us to in-vestigate properties of the index collection. In particular, we in-vestigate the document number and the topic composition, and we explain the fact that ESA still works with random weight vectors. Beyond explaining observations from previous work, we extend the results reported in [1] to empirically support our theoretical find-ings.
T he details of how the entries in the matrix G are calculated in the original GVSM approach go beyond the scope of this paper and can be found in [8].
From a practical point of view the index collection size affects the runtime of ESA, since a given document needs to be compared against all index documents. Given this constraint, a small index collection is favorable. However, given the convergence in Equa-tion 16, a large index collection is favorable since it reduces the variance of the g  X  jk entries. A small variance is desired because it eliminates random fluctuations, which might be caused by outlier documents in the index collection.

This hypothesis is supported by the experiment results shown in Table 1: With increasing number of index documents the per-formance of ESA becomes more stable. Only if randomly con-catenated Wikipedia articles are used as index documents (merged topics), the performance shows a significant increase at 200 , 000 documents. Within the merged topics collection each index docu-ments is generated by concatenating 10 randomly drawn Wikipedia articles with at least 1,000 words. However, given the size of the index collection several articles must have been reused; i.e., the as-sumption of the W ji to be independent across the documents can-not hold, which might explain this artifact in the data. 3
Equation 16 and the reformulation of ESA as a special case of the GVSM in Equation 14 show that the index collection essentially provides term correlation information. Moreover, ESA operates on the overlap of the vocabulary defined by the documents x and y , as well as the vocabulary defined by the index documents. The sum-mands in Equation 9 contain the term w ( t j , x ) w ( t k contributes to the similarity score if and only if w ( t j and g jk are nonzero. I.e., one will benefit from the correlation in-formation of two terms only if they occur in different documents x , y , as well as in the index collection. One hence might argue that an index collection that covers the same topic domain like the com-pared documents will be more beneficial than a collection about a different topic, just because of the vocabulary overlap. Again, this hypothesis is supported by the experimental results in Table 1. Note that using the Reuters corpus as index collection entails a better per-formance than using Wikipedia articles. A similar observation was also made by M X ller and Gurevich in [5]. The results in Table 1 are based on a test corpus of news documents from the Australian Broadcasting Corporation X  X  news mail service, which focus on in-ternational politics; see [1] for further details.

It is important to note that the wide topic range in Wikipedia can also be a disadvantage: even if the vocabulary overlap is large, homonyms from different topics will introduce noise and distor-tions in the correlation information. For instance, the term  X  X apital X  has several meanings (capital city, financial capital, capital letters or top part of a column). At a semantic level the meanings of this term are individually correlated with different other terms. At a syntactical level the semantic differences are lost and the correla-tions of all meanings with, for instance, the term  X  X overnment X , get blurred. In a narrow domain corpus such as Reuters it is likely that the number of different meanings of a term is low. By con-trast, Wikipedia contains most X  X f not all X  X ifferent meanings and thus introduces much more noise in the correlation weights of G  X  . I.e., the original motivation of using Wikipedia because of its topic range can be counterproductive when addressing a particular topic domain.
I n January 2010 Wikipedia contained less than 2,000,000 arti-cles with 1,000 words. http://stats.wikimedia.org/ EN/TablesWikipediaEN.htm Table 2: Correlation of ESA-based similarities with human s imilarity assessments, depending on index collection and num-ber of index documents. As weighing scheme tf-idf is used.
To substantiate this hypothesis we conduct a focused crawl of d ocuments from both the open directory project (DMOZ) and Wikipedia. We create different index collections of up to 100,000 documents with a topical focus on sports, science and news for DMOZ, as well as on arts, science and politics for Wikipedia. For each index collection we apply the same evaluation procedure as in [1], using the test corpus of news documents from the Aus-tralian Broadcasting Corporation X  X  news mail service. The results are shown in Table 2. The best results are achieved with index col-lections based on news documents and politics documents, which shows that ESA benefits from an index collection about the same topic domain like the documents that are to be compared.
The authors of [1] report that an index collection with term weights created by a random process yield good results as well. The term weights were chosen to be independently N (0 , 1) dis-tributed. We can directly use these parameter settings here in order to obtain Var ( W j ) = 1 , E ( W j ) = 0 and Cov ( W j , W j 6 = k . Using this in the Equation 16 results for j 6 = k in:
The elements g  X  jj on the main diagonal are treated differently:
For n  X   X  we observe that G  X  becomes a m  X  m unit ma-trix I . A unit matrix in the GVSM corresponds to the classical VSM with orthonormal term vectors. Also this theoretical finding is reflected by the results shown in Table 1. With increasing size of the randomly generated index collection (Random Gaussian) the performance converges towards a value of 0 . 717 , which is the same value one obtains with the classical VSM baseline.
As main contribution we develop a view of ESA that shows its conceptual equivalence with the GVSM, independent of the choice of term weights for the index documents. On the basis of this refor-mulation and by employing a probabilistic model for term weights, we show that ESA essentially captures term correlation information from the index collection. This correlation information is exploited to match different terms in compared documents, which also shows that the concept hypothesis has no (obvious) mathematical basis.
In addition, we develop a deeper understanding of the influence of the index collection: (1) Increasing the number of index doc-uments causes the correlation values for terms to converge, and
T he DMOZ category news comprises only 67,809 documents. hence, larger index collections provide more reliable correlation in-formation. (2) The application of ESA in a specific domain benefits from taking an index collection from the same topic domain while, on the other hand, a  X  X eneral topic corpus X  such as Wikipedia in-troduces noise. Finally, our model is able to explain the behavior of ESA when using random term weights in index documents: with increasing document number the ESA-based similarities converge to the classical VSM using the cosine measure.

Our theoretical findings are supported by empirical results pre-sented in related work. We also support our findings by new em-pirical analysis using different topically focused index collections. The paper also explains the behavior of ESA that seemed to be con-tradictory to the assumptions previously made on how ESA works.
Given these insights into ESA, future work will deal with the analysis and development of measures that can support the choice of a suitable index collection for a given task and context. Vocabu-lary overlap or a comparison of the distribution of term weights are first candidates to be considered. A second step is the composition within the index collection. Given the impact on runtime perfor-mance, it is of interest to reduce the size of the index collection while maintaining the effectiveness. Finally, also the impact of the findings presented here on CL-ESA should be explored. [1] M. Anderka and B. Stein. The ESA retrieval model revisited. [2] P. Cimiano, A. Schultz, S. Sizov, P. Sorg, and S. Staab. [3] O. Egozi, E. Gabrilovich, and S. Markovitch. Concept-based [4] E. Gabrilovich and S. Markovitch. Computing semantic [5] C. M X ller and I. Gurevych. Semantically enhanced term [6] M. Potthast, B. Stein, and M. Anderka. A Wikipedia-based [7] P. Sorg and P. Cimiano. Cross-lingual information retrieval [8] S. K. M. Wong, W. Ziarko, and P. C. N. Wong. Generalized
