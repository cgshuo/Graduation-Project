 The TREC .GO V collection mak es a valuable web testb ed for distributed information retriev al metho ds because it is naturally partitioned and includes 725 web-orien ted queries with judged answ ers. It can usefully mo del asp ects of gov-ernmen t and large corp orate portals. Analysis of the .gov data sho ws that a purely distributed approac h would not be feasible for pro viding searc h on a .gov portal because of the large num ber (17,000+) of web sites and the high prop or-tion that do not pro vide a searc h interface. An alternativ e hybrid approac h, com bining both distributed and cen tral-ized techniques, is prop osed and serv er selection metho ds are evaluated within this framew ork using web-orien ted eval-uation metho dology . A num ber of well-kno wn algorithms are compared against represen tativ es (highest anc hor rank ed page (HARP) and anc hor weigh ted sum (AWSUM)) of a family of new selection metho ds whic h use link anc hort-ext extracted from an auxiliary crawl to pro vide descrip-tions of sites whic h are not themselv es crawled. Of the previously published metho ds, ReDDE substan tially out-performed three varian ts of CORI and also outp erformed a metho d based on Kullbac k-Leibler Div ergence (extended) except on topic distillation. HARP and AWSUM performed best overall but were outp erformed on the topic distillation task by extended KL Div ergence.
 H.3.3 [ Information Storage and Retriev al ]: Information Searc h and Retriev al| sele ction process ; H.3.4 [ Information Storage and Retriev al ]: Systems and Soft ware| perfor-manc e evaluation (eciency and e e ctiveness); distribute d systems Exp erimen tation, Measuremen t, Performance
Distributed information retriev al (DIR) systems, also kno wn as metasearc hers, have long been prop osed as an alternativ e to large cen tralized indexes. They have been claimed to o er poten tial solutions to problems asso ciated with enormous data scale, with rapidly changing data, and with the need to pro vide integrated searc h over multiple sources including those sub ject to access con trols.
When the num ber of primary searc h services is large or when there is a cost asso ciated with querying serv ers, a pro-cess of query-sp eci c server sele ction is needed to main tain rapid query resp onse and to avoid unnecessary net work or searc h charges. A num ber of algorithms for serv er selection have been presen ted, including CORI [2], ReDDE [20], KL [21], and the GlOSS family [11]. In general, the performance of these techniques has been evaluated using TREC Ad Hoc resources 1 .
 The availabilit y of extensiv e relev ance judgmen ts for the TREC Ad Hoc corpus has made it the basis for most dis-tributed information retriev al (DIR) testb eds. However, ar-bitrary partitionings of a small collection of newspap er and governmen t documen ts don't seem to mo del likely real-w orld applications of DIR metho ds. For example, the Ad Hoc collections lack the link structure, URLs, anc hortext and clic k-through data whic h underpin successful Web 2 searc h engines.

In con trast, the .GO V and .GO V2 corp ora used in re-cen t TREC Web and Terab yte Track evaluations mo del a web environmen t in whic h an e ectiv e distributed searc h system migh t o er adv antages. A scheme in whic h govern-men t agencies (or eac h governmen t web site) indexed their own data and a governmen t metasearc her pro vided whole-of-governmen t searc h could poten tially replace the curren t cen-tralized service at the US governmen t portal http://www. firstgov.gov . Suc h an alternativ e migh t lead to greater coverage, reduced net work trac and more rapid resp onse to web site updates, as well as o ering the poten tial for uni-ed searc h access to a subset of resources de ned by the searc her's access righ ts.

A further adv antage of .GO V over other TREC collections in web applications is the availabilit y of queries and judg-men ts for searc h mo des more characteristic of web searc h: homepage nding, named page nding, and topic distilla-tion.

Searc hing the US governmen t domain is a speci c instance
See http://trec.nist.gov/ .
We follo w the con vention of capitalizing Web when refer-ring to the World Wide Web as opp osed to generic webs. Table 1: Sizes in pages of a selection of na-tional governmen t domains as estimated by Google ( www.google.com ) on 17 Jan 2005. The TREC .GOV2 corpus contains 25M pages; .GOV contains 1.25M. of a general class of portal searc h applications. Apart from the possibilit y of man y similar governmen t portals around the world (see Table 1), there are man y large-en terprise por-tals and man y multi-source topic portals covering domains suc h as health, chemistry and travel. The attractiv eness of distributed metho ds for building portals is greater when the sites to be included are widely distributed and where net work bandwidth is limited or exp ensiv e.

In what follo ws, we will focus on the problem of searc h-ing the .gov domain and use the .gov related resources from TREC. Our exp erimen ts are conducted on the .GO V corpus because of the web-sp eci c queries and judgmen ts available for it, but we use the much deep er crawl represen ted by .GO V2 as a reference for characteristics of the full .gov do-main.

We address a num ber of questions: First, is there a feasible mo del for DIR in the .gov domain? Second, how well do existing algorithms for serv er selection work in this domain? Third, can we devise metho ds whic h work better?
Man y individual serv er selection algorithms have been prop osed in the literature and evaluated using testb eds de-rived by partitioning TREC Ad Hoc corp ora | generally into one or two hundred collections, and with an eye to pro-ducing collections of appro ximately equal size. Examples include [20, 11, 25]. CORI (Callan et al. [2]) is probably the best kno wn. Nottelmann and Fuhr [16] have recen tly extended it with an estimation technique based on a general cost function, able to tak e accoun t of retriev al cost and time as well as estimated relev ance.

Similarly , Ipeirotis and Gra vano [14] extended the lan-guage mo delling techniques used by CORI, bGlOSS, and KL Div ergence, and were able to sho w signi can t impro ve-men ts in recall when tested with TREC Ad Hoc queries. Neither group were able to mak e use of web-st yle queries and judgemen ts.

Past work has often used long queries; for example, Xu and Croft [25] rep ort a series of exp erimen ts with a mean 34.5 terms per query . This is eviden tly much larger than the 2.35 words rep ortedly typical of queries to web searc h services. [22]
In a series of pap ers [10, 9, 17], Frenc h, Powell et al. have dev elop ed a set of testb eds based on the TREC Ad Hoc corpus and used it to evaluate CVV, gGlOSS, and CORI for serv er selection. The testb eds dev elop ed are based on data available at TREC-4 in 1995, and include SYM-236 (a division into 236 collections, arranged by source and pub-lication date); UBC-100 (100 collections, arranged to have appro ximately equal byte coun ts); and UDC-236 (236 collec-tions, arranged to have appro ximately equal num bers of doc-umen ts). Other testb eds included up to 921 databases, but again arranged to keep size appro ximately uniform. Queries in all these testb eds were based on TREC Ad Hoc topics and divided into \short" and \long" forms, with mean lengths of 3.5 and 21 terms resp ectiv ely, and in one investigation [9] the queries were rst fed through an automatic expansion pro cess.

Relativ ely little work has been done to con rm the ap-plicabilit y to web environmen ts of serv er selection results obtained on TREC Ad Hoc data.

Rasolofo et al. [18] compared CORI and their own metho d for serv er selection and for results merging using an eigh t-way division of the TREC WT10g collection of web data. The authors expressly considered the nature of web queries and used either a very short form of the TREC topics (two terms on average) or queries garnered from logs of the Excite searc h engine (2.4 terms on average).

In [23], Singhal and Kaszkiel evaluated the performance of an algorithm optimized for the TREC Ad Hoc task in a web environmen t. Their study does not consider serv er selection, but the di erences between TREC Ad Hoc data and the Web are ackno wledged and the authors use an 18 million page crawl of the Web as a testb ed. Using a web-speci c searc h task, they found ample room for impro vemen t in the documen t rankings pro duced by the TREC Ad Hoc algorithm, whic h suggests that published serv er selection algorithms may bene t from similar scrutin y.

Crasw ell et al. [5] evaluated CORI, vGlOSS, and CVV in a testb ed based on the 2GB, 956 serv er WT2g crawl of the Web. They concluded that CORI, and a mo di ed version of the CORI algorithm, performed reasonably e ectiv ely at the serv er selection task. This is similar to the presen t work, but with some di erences: the testb ed in [5] featured man y medium-sized serv ers and few small ones (the rev erse of the case in the web generally) and retriev al tasks typical of ad hoc rather than web searc h. They also considered every serv er to be searc hable, whic h we do not (see Section 3.3).
The .GO V2 corpus is believ ed to be a good snapshot of the .gov domain. Its page coun t of 25M is of similar magnitude to the Google estimate (Table 1) and is within 10% of the Yaho o! estimate of 27.4M obtained on 17 Jan 2005.
Unfortunately , at the time of writing, there were no judg-men ts for web-st yle queries against the .GO V2 corpus. Ac-cordingly we used the .GO V corpus after comparing its prop-erties against those of .GO V2. Table 1 sho ws that the .GO V corpus is represen tativ e of the size of a num ber of national governmen t domains.

The 18GB .GO V corpus includes 1.25 million documen ts, dra wn from 7792 serv ers in an early 2002 crawl of the .gov domain conducted by NIST 3 .
The appro ximately equal size distribution in man y pre-vious exp erimen ts is not typical of the .gov domain or of the Web, where documen ts are typically distributed between serv ers according to a power law (as describ ed by eg. Hu-berman and Adamic [13]). For some searc h applications, suc h as the .gov portal considered here, past partitionings
See http://es.csiro.au/TRECWeb/go vinfo.htm l Proportion of servers Figure 1: Distribution of documen ts amongst servers in the .GOV, .GOV2, and UBC-100 corpora.
 Table 2: Characteristics of some popular testb eds, and the .GOV corpus. also pro duce too few collections by an order of magnitude.
The .GO V corpus con tains documen ts from 7792 serv ers, compared with 17,186 for .GO V2. Figure 1 summarizes the distribution of documen ts across serv ers, for these col-lections and for the commonly-used UBC-100 TREC Ad Hoc testb ed. There are di erences between the .GO V and .GO V2 distributions (presumably due to incomplete crawl-ing of man y sites in .GO V) but the di erence between these collections and UBC-100 is overwhelming.

Summary statistics for distribution of documen ts in the .GO V corpus, and sev eral other testb eds used in the litera-ture, are given in Table 2.
Queries and relev ance judgmen ts available for the .GO V corpus are of three kinds, whic h are considered relev ant to web searc hes generally [8].

In the rst, \topic distillation", a broad query is inter-preted as a request for key homepages. For example, rel-evant documen ts for the topic \science" may include the homepages of governmen t science agencies. The same topic would be interpreted in the con text of the TREC Ad Hoc judgmen ts as meaning \ nd me all the pages you can about science", whic h would return man y more relev ant documen ts, eac h coun ted as having equal value. Topic distillation is con-sidered more like a typical web searc h task since web users tend to bro wse from an entry page to nd information they need (see, for example, Teev an et al. [24]).

The second type of searc h task considered, \homepage Proportion of servers Figure 2: Prop ortion of servers in the .GOV corpus which have a searc h interface. nding", and third, \named page nding", are similar. In both cases a query is interpreted as a request for a single web page, whic h may or may not be kno wn to exist. Homepage queries are tak en to be explicit requests for a homepage. Named page queries are for a page whic h is kno wn to exist, but whic h may not alw ays be a homepage. Both types of query have exactly one relev ant documen t, the page in ques-tion or duplicates of it. For this reason, standard evaluation measures for the Ad Hoc task, suc h as precision and recall at n , are not useful.
Not all serv ers in the .gov domain pro vide a public searc h interface; eviden tly, if a searc h interface is not available for a site then that site cannot be selected for metasearc h.
To disco ver whic h serv ers in the .GO V corpus pro vide a searc h facilit y, we implemen ted the classi er describ ed in Cop e et al. [4]. Giv en the HTML for a form, this classi er can determine, fairly reliably , whether the form is an inter-face to a searc h engine. We rst extracted all the serv er homepages from the corpus: the homepage for serv er x.gov is either the page at http://x.gov/ , or the page at one of eigh teen alternativ es suc h as http://x.gov/index.html , http://x.gov/home.htm or http://x.gov/start.asp . We found homepages for 6294 serv ers. Eac h form on eac h home-page was then passed to the classi er, and if a form app eared to be a searc h interface the serv er was mark ed as searc hable.
A num ber of serv ers do not mak e a searc h facilit y available on the homepage, but instead link to a separate searc h page. As a second phase, we also examined the HTML of those homepages with no apparen t searc h interface for links to pages with \searc h" in the name, or links with \searc h" in the anc hor text, and recorded those serv ers as searc hable.
Those serv ers with no searc h interface on the homepage and no searc h link from the homepage were mark ed as non-searc hable.

Overall, 31% of serv ers app ear searc hable, but there is a great deal of variabilit y. Searc habilit y in the .GO V corpus seems to correlate well with serv er size, as illustrated in Figure 2: while only 20% of serv ers with between four and sev en documen ts in the corpus pro vide a searc h interface, 75% of those with between 1024 and 2047 documen ts pro vide an interface as do all of those with 8192 documen ts or more. Since larger collections are more likely to be searc hable, 65% of documen ts in the corpus belong to searc hable serv ers.
In this section, we consider four possible mo dels for searc h-ing the .gov domain, and outline possible implications for qualit y, speed, and cost of the searc h. As extreme cases, we consider a cen tral index and \pure" metasearc h (the termi-nology in this section is from Crasw ell et al. [6]); we also consider \selectiv e" metasearc h. Finally , we presen t a hy-brid mo del whic h we believ e represen ts the most plausible alternativ e.
An obvious way to searc h is to crawl all of the .gov domain ahead of time, building a local index, and use this index to resp ond to queries. This is what most curren t web searc h engines do, including the service pro vided on the FirstGo v portal. However, the cost of crawling is signi can t, local optimizations may be lost, and crawling may result in a slow resp onse to site changes. If crawling is carried out frequen tly to main tain freshness, signi can t load may be imp osed on the web serv ers.
A ma jor problem faced by a crawling serv er is the net work trac generated by a large-scale crawl. Since some overhead is incurred in crawling (in HTTP headers, duplicates elim-inated, etc), the trac generated will exceed the total size of real data in .gov. Crasw ell et al. suggest an overhead of 70% 4 , whic h means we migh t reasonably exp ect eac h full crawl resulting in 426GB of text (as in the case of .GO V2) to require as much as 724GB net work trac.

Incremen tal crawling would reduce net work load consider-ably but the trac generated by frequen t incremen tal crawls is still substan tial. A num ber of other metho ds are available for reducing trac still further when cooperation and trust exists between website operators and the searc h engine. For example, on request, a local site could transmit a compressed form of changed pages only . Trust is more likely in the .gov domain than on the web at large.
Since individual serv ers can be exp ected to kno w some-thing about their documen ts, they can o er appropriate lo-cal optimizations. For example, they may translate terms (\la w" to \act" for a legislation searc h, or \exhaust" to \emissions" for a searc h of environmen tal information) or rank results according to frequency of use. This local kno wl-edge is lost if the site is searc hed via a crawl.
In the pure metasearc h mo del, eac h query is submitted to eac h .gov serv er, and the results are merged. This presen ts signi can t diculties to do with scale and availabilit y.
An obvious problem is the cost, in net work trac, of forw arding a user's query to eac h serv er and receiving the
We susp ect that this may be an under-estimate. replies. Follo wing [6], the net work trac generated is ( S S )( j C j + 1), where S q and S r are the size of eac h query and resp onse resp ectiv ely and j C j is the num ber of serv ers (col-lections). (The extra query-and-resp onse is the trac from and to the user.) Using gures of 1KB for S q and 20KB for S , with the 7792 serv ers of the .GO V corpus the cost per query is 160MB. This trac could be reduced by reducing the num ber of results fetc hed from eac h serv er [15] but the approac h is still infeasible for even the best-connected hosts.
Queries can only be forw arded after their receipt by the metasearc h agen t, and the nal results can be returned to the user only after every searc h engine has returned its re-sult list (or been timed out, with consequen t loss of e ectiv e-ness). This means the user is likely to exp erience signi can t dela ys, and the system as a whole will often be faced with una vailabilit y of one or more serv ers.
Since we cannot in general assume that eac h serv er pro-vides an iden tical searc h interface, we will need to pro vide a wrapp er for eac h interface | poten tially eac h serv er | whic h con verts the user's query , feeds it to the searc h inter-face, and extracts searc h results. Generating and main tain-ing these wrapp ers for a large num ber of serv ers represen ts a good deal of work, whic h could be dramatically reduced by adoption of standards.
Queries can only be forw arded to serv ers whic h mak e a searc h service available. As noted above, this is only 31% of serv ers in the case of .GO V. Without change of practice by the web serv ers, pure metasearc h is curren tly not feasible.
A well-studied, although little-used, alternativ e to pure metasearc h is a more selectiv e mo del. In this mo del, a searc her forw ards a query to a subset of the available serv ers, chosen by a serv er selection mec hanism suc h as CORI. The cen tral searc her must main tain adequate data to inform the selection, but in exc hange can dramatically reduce the time and trac needed to resp ond to a query . For example, if twenty serv ers are chosen instead of all 7792 in the .GO V corpus, the net work trac needed to reply to a query is reduced from 160MB to 440KB.

This is the mo del generally assumed in studies of serv er selection. However, as it stands it is not appropriate for searc hing the .gov domain: to work as required, every serv er must be searc hable, and an appropriate wrapp er must be main tained for every serv er. We suggest incorp orating se-lectiv e metasearc h into a hybrid mo del, describ ed below, whic h allo ws for this.
Both pure and selectiv e metasearc h seem to be impracti-cal for searc hing the .gov domain as it stands. As an alter-nativ e, we consider a hybrid system of the type suggested by [6], whereb y some or all of the serv ers with a searc h interface are considered candidates for metasearc h and the remainder of serv ers are crawled. To answ er a user's query , a num ber of searc hable serv ers are selected (as in selectiv e metasearc h) for searc h along with the local index. Since the largest serv ers in .gov are the most likely to pro vide a searc h interface, the volume of trac for a crawl can be substan-tially reduced; freshness of the index is also less of a concern.
There are a num ber of ways to choose from the .GO V corpus whic h serv ers are candidates for metasearc h. In the simplest baseline case, we can consider all and only those serv ers whic h seem to have a searc h interface according to our classi er. Alternativ ely, we can assume that by policy serv ers over a certain size will be searc hable, and are candi-dates. The chosen size determines the cost of a crawl and of a query in our hybrid system; it also determines the size of index and (in conjunction with a decision on crawl fre-quency) the possible staleness in our results.

In our exp erimen ts we assumed that the crawled serv ers would be indexed collectiv ely and treated by the metasearc her as a single serv er. We consider ten metho ds for serv er selection. Mo di ed KL Div ergence, CORI (three varian ts), and ReDDE repre-sen t state-of-the-art serv er selection algorithms, whic h have been well tested in other scenarios. We also introduce two ranking metho ds (HARP and AWSUM, see Section 5.4) suit-able for the hybrid mo del discussed for .gov. Finally , ran-dom ranking, size-based ranking, and crawl-rst ranking are presen ted as con trols. Kullbac k-Leibler Div ergence was suggested by Xu and Croft [25], and interpreted in a language mo delling con text by Si et al. [21]. We nd the serv ers C i with the highest probabilit y P given query Q , where: P ( Q ) dep ends only on the query , not the serv er, so can be ignored. P ( Q j C i ) can be estimated with a unigram language mo del as where G is the global language mo del and = 0 : 5. In the presen t work we consider an extension describ ed in [19], and assign P ( C i ) according to estimated serv er sizes:
The CORI algorithm [2] is an adaptation of the INQUER Y documen t ranking algorithm, treating eac h serv er as a com-pound \do cumen t" and using documen t frequency df in-stead of term frequency .

The score p for a serv er C i , for eac h query term t is given by p = b + (1 b ) T I , where:
I = log j C j + 0 : 5 Here df t is the num ber of documen ts in C i con taining the query term, cw is the num ber of words in C i , and avcw is the mean cw across all serv ers. The other terms are constan ts: b = 0 : 4, tf base = 50, and tf f actor = 150.

We also consider two extensions to CORI suggested by [19], whic h mak e use of the documen ts C isamp sampled from C i in the course of building the language mo del. In the rst, CORI ext1, we scale cw and df by ^ j C i j j C CORI ext2, we also scale tf base and tf f actor . Since these varian ts expressly adjust for estimated serv er size, it may be exp ected that they would perform better over the large variet y of serv er sizes in the .GO V corpus.
Si and Callan's ReDDE algorithm [20] estimates the dis-tribution of relev ant documen ts between serv ers by reference to sampled documen ts. The num ber of relev ant documen ts in eac h serv er is estimated with P (rel j d j ) is in turn estimated by ranking the sampled docu-men ts with relation to the query terms, and scoring a small constan t for eac h serv er eac h time one of its documen ts ap-pears in the rank ed list. The list is cut o at the point where the accum ulated serv er sizes, P d a constan t.
Because the portal applications considered in this pap er are wholly in the web domain, it is possible to mak e use of web-sp eci c information, suc h as link anc hortext, in serv er selection and to rely on the fact that web documen t names (URLs) include the full hostname of the web serv er whic h published the documen t.

In a fully crawled approac h to portal searc h, anc hortext can be used to iden tify the most valuable answ ers to queries and is kno wn to con tribute hea vily to retriev al e ectiv eness in web-st yle evaluations [7]. We hypothesized that anc hor-text deriv ed from something less than a full crawl migh t also be useful in serv er selection, given that the anc hortext of links to a web page allo ws us to kno w something about that page (and the web serv er whic h published it) without accessing its con ten t.

We prop ose a family of web serv er selection metho ds based on anc hortext extracted from an auxiliary crawl of web sites related in some way to the con ten t of the portal. In the con text of the hybrid approac h explored in this pap er, the obvious auxiliary crawl to use is that of the serv ers without searc h interfaces, since these sites have to be crawled anyway in order to pro vide searc h of their con ten t. To reduce poten-tial bias, we have excluded nep otistic (within-serv er) links as these are only available for sites in the cen tral crawled collection.

We rep ort results for two metho ds from this family using anc hortext deriv ed from the cen tral crawled collection. It would be interesting to apply the same metho ds to di eren t auxiliary collections suc h as a shallo w crawl over all the sites in the portal or indeed a full but long out-of-date crawl of those sites, but this remains for future work. The follo wing descriptions of the metho ds are sligh tly simpli ed for clarit y.
The anc hor text for all the non-nep otistic, within-domain links in the cen tral crawled collection is formed into sur-rogate documen ts according to the target URL of the link. Some of the targets corresp ond to real documen ts in the cen-tral crawled collection, others to documen ts on other serv ers and a prop ortion to documen ts whic h do not curren tly ex-ist. Surrogates for documen ts with man y incoming links will in general be much longer than surrogates for those with few incoming links. Note that documen t con ten t and other metadata is completely ignored.

The collection of surrogate documen ts is indexed and queries are pro cessed against the index using the AF1 scor-ing form ula [12] whic h includes no length normalization and in whic h the con tribution of high term frequencies ( tf not atten o as is appropriate for normal text documen ts. where w t is the weigh t con tribution of (Porter stemmed) query term t and n is the num ber of documen ts whose in-coming anc hortext con tains t . Term coordination is imp osed | fully matc hing documen ts are alw ays rank ed ahead of par-tial matc hes. Since only anc hor text is used, the value of doesn't a ect the ranking.

A ranking of up to 1000 web page results is returned and con verted to a ranking of serv ers using simple syn tactic pro-cessing of the URLs and lookups of a list of searc h serv er hostnames. Example: if a the URL of a page in the results is http://ks.water.usgs.gov/Kan sas/pubs/fact -sheets/fs. 022-98.html and ks.water.usgs.gov is in the the list of searc h serv ers, then ks.water.usgs.gov will be added to the tail of the rank ed list of serv ers unless it is already higher in the list. If the serv er (from .gov) does not app ear in the list of searc h interfaces, then the cen tral crawled collection will be app ended to the serv er list, if not already presen t.
In this way, searc h serv ers (and the cen tral crawled collec-tion) are rank ed by the score of their highest rank ed page.
Note that the cen tral crawled collection is added to the tail of the list if not already presen t, since the cost of searc hing it is low. This approac h also avoids returning an empt y list.
This is a varian t of HARP in whic h eac h page in the origi-nal ranking con tributes to the score of its serv er. In order to prev ent a serv er with man y low-v alue pages overwhelming another with few er but higher-v alue answ ers, eac h page's con tribution is its score divided by its rank. Serv ers are rank ed by descending score, with the cen tral crawled collec-tion at the tail of the list if not otherwise presen t.
For comparison baselines, we also consider three simple serv er selection metho ds: ranking serv ers randomly (\ran-dom"), ranking serv ers by their estimated size (\size"), and alw ays selecting the cen tral crawled serv er rst and others randomly (\cra wled").
When comparing new metho ds with those previously pub-lished, it is often dicult to be sure that the older metho ds have been correctly implemen ted. We have endea voured to do so by evaluating our implemen tations of CORI and ex-tensions, extended KL and ReDDE on a testb ed for whic h results have previously been published. We used the UBC-100 collection and TREC Ad Hoc queries 51{100, and com-pared R n scores at three key points with those read from the graphs in Figure 1 of [19]. The values we obtained were often higher than the read-o values and were alw ays within the margin of di erence to be exp ected given uncon trolled di erences in indexing, stemming etc.
We compared the ten metho ds using 725 queries and re-lated relev ance judgmen ts from the TREC Web Track 2002{ 2004. 125 of these were topic distillation queries, 375 home-page nding queries, and 225 named page queries.
Sev eral of the metho ds presen ted need an estimate of eac h serv er's size, and the CORI family and extended KL need an estimate of eac h serv er's language mo del.

To estimate language mo dels, we used single-term prob e queries in the manner of [3] until we had pro cessed 300 unique documen ts or 150 queries. The rst query term was selected at random from a list of common English words, then terms were chosen at random from the learned lan-guage mo del (or from the list of common words if the mo del was exhausted). The learned mo dels were ltered with the SMAR T stoplist.

Serv er sizes were estimated using the sample-resample metho d describ ed in [20], choosing query terms at random from the estimated language mo del and using ve queries.
We used full language mo del and accurate size for the cen tral crawled sev er as this information would be available in the mo deled setting.
Mo di ed average precision (MAP), a standard measure from the TREC Web Track, was used to compare the serv er selection metho ds. MAP is de ned by Where num rel ret is the num ber of relev ant serv ers in the ranking; rank( i ) is the rank of the i th relev ant serv er; and R is the total num ber of relev ant serv ers, or the requested length of the rank ed list if this is smaller. In eac h exp erimen t we rank ed up to 100 serv ers.

In the case where there is only one relev ant serv er (all named page and homepage queries, and some of the topic distillation queries), MAP is iden tical to mean recipro cal rank (MRR).
Table 3 sho ws results obtained for the most natural hybrid division | rely on local searc h interfaces where they exist and crawl the rest, treating the crawl as a single \serv er". 5
The num ber of serv ers (1972) in this partitioning is so large that random serv er selection scores very close to zero. Because the cen tral crawled serv er is by far the largest, crawled and size metho ds alw ays agree in their rst choice serv er. After that, choosing serv ers on the basis of size rather than randomly gives only small bene t. Giv en the homepage-dominated, tiny-relev ant-set evaluation we used, it may be that the reason the cen tral crawled serv er is so valuable may be more due to the fact that it covers a large
To enable replication of this work by others, the authors are happ y to pro vide on request their list of serv ers pro viding searc h interfaces. Table 3: MAP@100 results, for the hybrid model metasearc hing all .GOV servers known to provide a searc h interface plus a central crawl of the rest. num ber of sites rather than the fact that it includes a large num ber of pages. Here, selection metho ds are not rew arded for iden tifying serv ers with large num bers of documen ts whic h matc h the query but only for choosing serv ers holding one of a small set of answ ers.

The di erences between the performance of CORI and size are not signi can t at the 0.05 level (Wilco xon signed ranks text). CORI is solidly outp erformed overall by ex-tende d KL (Wilco xon p &lt; 0 : 01, except for the named page task where the di erence was not signi can t) whic h is itself outp erformed by ReDDE (Wilco xon p &lt; 0 : 05), although the adv antage on the topic distillation task is rev ersed (Wil-coxon p &lt; 10 8 ).

Overall, AWSUM was the best performing metho d, though it only sligh tly outp erforms HARP . Across all 725 queries AWSUM outp erformed ReDDE by 18% on MAP@100 (Wil-coxon p &lt; 10 3 ). Extended KL Div ergence performed best on the topic distillation queries, but AWSUM was within 5% on MAP@100 with ReDDE only sligh tly further behind.
The performance of the anc hortext based selection meth-ods is encouraging as the space of possible varian ts has only ligh tly been explored. It would be interesting to investigate the performance of this family of metho ds with other auxil-iary collections whic h migh t be available in practice, suc h as a long-out-of-date full crawl of the same domain or a recen t shallo w crawl of the domain covered by the portal (limited num ber of pages fetc hed per serv er). 6
It is not clear why scores for the named page task are substan tially lower than those for the homepage task for some metho ds but not others since both are navigational tasks in the sense of [1]. For HARP and AWSUM the dif-ference may be due to a higher prev alence of inter-serv er anc hortext referring to homepages. However, the relativ e di erence for the non-anc hortext CORI and KL metho ds, is even greater, while for ReDDE the di erence is very small. In all the sampling based metho ds, ranking of results for prob e queries made use of link coun ts and anc hortext.
We also compared the serv er selection metho ds on arti-cial partitionings of .GO V in whic h it is assumed that all serv ers publishing more than a threshold t were assumed to pro vide a searc h interface, corresp onding to the hypothetical
As previously suggested by Crasw ell, suc h collections could also be used for estimating language mo dels and sizes. enforcemen t of an edict. Space does not permit presen tation of full results, but for t = 256 (1053 searc hable serv ers in .GO V) the MAP@100 scores of the various metho ds were similar to those presen ted in Table 3.

Another natural partitioning of the .gov data would be by agency or jurisdiction rather than by serv er. For example NASA and the state of California migh t pro vide a searc h ser-vice covering all their web serv ers. 7 This would reduce the num ber of collections to be searc hed and the work required to implemen t wrapp ers but would require main tenance of a list of whic h serv ers corresp ond to whic h agencies and the URL of the appropriate searc h interface | otherwise results from a single serv er migh t be included more than once. The absence of suc h a list made it dicult for us to investigate agency-based partitioning. Partitioning by agency raises the interesting possibilit y of multi-lev el metasearc h.
A limitation of this study is that although the queries used were speci cally orien ted toward web searc h and the evalu-ation re ected the sort of beha viour web searc hers exp ect of web searc h engines, real queries and user relev ance/utilit y judgmen ts were not available. At the time of writing, we did not have access to query logs from FirstGo v. Even if we had suc h logs, there remains the dicult y of reconstructing the coresp onding searc her's need and the value they would assign to candidate answ ers. In pro duction use of a hybrid metasearc her, pseudo-judgmen ts against real queries could be obtained using clic k-through data collected in normal op-eration.

Clic k-through data could also possibly be exploited in an enhanced serv er selection metho d.
Using a generally available web-orien ted testb ed, whic h mo dels a range of plausible portal searc h applications, we have sho wn that ReDDE is sup erior to sev eral previously published serv er selection metho ds in this application, al-though Extended KL Div ergence work ed best on the topic distillation queries. The testb ed features a real partitioning into serv ers and a range of web query types.

ReDDE and KL Div ergence do not exploit the web-sp eci c information available in web searc h environmen ts (and avail-able in this testb ed). We prop ose a family of metho ds based on link anc hortext deriv ed from an auxiliary collection (suc h as the cen tral crawled collection in a hybrid implemen ta-tion) and sho w that represen tativ es of this family (HARP and AWSUM) are capable of out-p erforming ReDDE on the web-orien ted tasks we have studied.

A further adv antage of the web-orien ted metho ds using the cen tral crawled serv er as auxiliary collection is that, by virtue of outgoing links, they con tribute usefully to solving the problem of iden tifying whic h serv ers operate within the domain covered by the portal. Serv er selection information is also automatically kept up to date. By con trast, serv er selection metho ds relying on prob e queries would not auto-matically disco ver the commissioning of new serv ers or the decommissioning of old ones and would need to regularly fetc h sample documen ts to main tain up-to-date mo dels.
In the absence of an end-to-end e ectiv eness evaluation com bining serv er selection with results merging on real
Note that it is possible that the searc h interfaces we iden-ti ed within .GO V actually cover con ten t of more than one web serv er. queries, no conclusion can be reac hed about whether the hybrid mo del we have discussed could achiev e results com-petitiv e with those of a fully crawled service. In the absence of a full analysis of net work costs, update frequency require-men ts and other portal characteristics, it is not certain what economic or qualit y bene ts (eg. resp onsiv eness to con ten t change, pro vision of appropriate securit y mo del etc.) the hybrid mo del migh t bring. However, the fact that the best serv er selection metho ds studied are on average capable of nding the serv er con taining the desired page (or one of the key resources on a broad topic) within the rst two or three serv ers in the ranking pro vides encouragemen t to further investigation.

Despite the good performance of the anc hor text metho ds, the practical implemen tation of a hybrid cen tralized/ dis-tributed replacemen t for the curren t FirstGo v governmen t portal searc h would face formidable challenges, particularly in the accurate iden ti cation and characterization of avail-able searc h interfaces and the dev elopmen t and main tenance of wrapp ers for them. Because of the huge num ber of sites to be included, the success of a hybrid implemen tation would dep end on adoption of one or a small set of standard searc h interfaces across metasearc hed sites.

In the US, where net work bandwidth is plen tiful and traf-c costs are low, distributed or hybrid mo dels are unlik ely to be adopted for pro viding governmen t portal searc h to the general public. These metho ds are likely to be much more attractiv e when net work costs are higher and bandwidth is more restricted and/or when included sites are sub ject to individual or group based access restrictions.

Investigation of alternativ e auxiliary collections and re-nemen t of anc hortext based selection metho ds remains for future work as does the evaluation of results merging meth-ods on a web-orien ted testb ed. [1] Andrei Bro der. A taxonom y of web searc h. SIGIR [2] J. P. Callan, Z. Lu, and W. Bruce Croft. Searc hing [3] Jamie Callan, Margaret Connell, and Aiqun Du.
 [4] Jared Cop e, Nic k Crasw ell, and David Hawking. [5] Nic k Crasw ell, Peter Bailey , and David Hawking. [6] Nic k Crasw ell, Francis Crimmins, David Hawking, and [7] Nic k Crasw ell, David Hawking, and Stephen [8] Nic k Crasw ell, David Hawking, Ross Wilkinson, and [9] James C. Frenc h, Alison L. Powell, Jamie Callan, [10] James C. Frenc h, Allison L. Powell, Charles L. Viles, [11] Luis Gra vano, Hector Garc a-Molena, and Anthon y [12] David Hawking, Trystan Upstill, and Nic k Crasw ell. [13] Bernado A. Hub erman and Lada A. Adamic.
 [14] Panaglotis G. Ipeirotis and Luis Gra vano. When one [15] Ronn y Lemp el and Shlomo Moran. Optimizing result [16] Henrik Nottelmann and Norb ert Fuhr. Com bining [17] Allison L. Powell and James C. Frenc h. Comparing [18] Yves Rasolofo, Fa X za Abbaci, and Jacques Savoy. [19] Luo Si and Jamie Callan. The e ect of database size [20] Luo Si and Jamie Callan. Relev ant documen t [21] Luo Si, Rong Jin, Jamie Callan, and Paul Ogilvie. A [22] Craig Silv erstein, Monik a Henzinger, Hannes Marais, [23] Amit Singhal and Marcin Kaszkiel. A case study in [24] Jaime Teev an, Christine Alv arado, Mark S. Ackerman, [25] Jinxi Xu and W. Bruce Croft. Cluster-based language
