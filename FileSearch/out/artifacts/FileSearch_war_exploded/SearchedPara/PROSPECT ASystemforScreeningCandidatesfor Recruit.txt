 Companies often receive thousands of resumes for each job posting and employ dedicated screeners to short list qualified applicants. In this paper, we present PROSPECT, a deci-sion support tool to help these screeners shortlist resumes efficiently. Prospect mines resumes to extract salient aspects of candidate profiles like skills, experience in each skill, edu-cation details and past experience. Extracted information is presented in the form of facets to aid recruiters in the task of screening. We also employ Information Retrieval tech-niques to rank all applicants for a given job opening. In our experiments we show that extracted information improves our ranking by 30% there by making screening task simpler and more efficient.
 H.3.3 [ Information storage and retrieval ]: Information search and retrieval Algorithms, Design, Experimentation, Management resume search, resume information extraction
Hiring the right talent is a challenge faced by all com-panies. This challenge is amplified by the high volume of applicants if the business is labor intensive, growing and faces high attrition rates. One example of such a business is IT services run out of growth markets. In a typical services organization, professionals with varied technical skills and business domain expertise are hired and assigned to projects to solve customer problems. In the past few years, IT ser-vices including consulting, software development, technical support and IT outsourcing has witnessed explosive growth, especially in growth markets like India and China. For in-stance, according to a NASSCOM (National Association of Software and Services Companies of India) study, the total number of IT and IT enabled services professionals in In-dia has grown from 284000 in 1999-2000 to over 1 million in 2004-2005 [18]. More recent estimates suggest that this industry employs more than 2 million professionals in India alone. For organizations in the IT Services business, growth in business is synonymous with growth in the number of employees and recruitment is a key function.

Hiring large number of IT professionals in growth markets poses unique challenges. Most countries in growth markets have large populations of qualified technical people who all aspire to be part of the explosive growth in the IT Services industries. Thus, a job posting for a Java programmer can easily attract many tens of thousands of applications in a few weeks. Most IT Services companies are inundated with hun-dreds of thousands of applicants. For example, Infosys, one of the largest IT Outsourcing companies in India, received more than 1.3 million job applications in 2009. However, only 1% of them were hired 1 .

To give the context for our work, we now give an overview of a typical recruitment process. This is illustrated in Fig-ure 1. The process starts when a business unit decides to hire employees to meet its business objectives. The business unit creates a job profile that specifies the role, job cate-gory, essential skills, location of the opening and a brief job description detailing the nature of work. It might also spec-ify the total work experience that the prospective employee should possess, along with the desired experience level for each skill. The job openings are advertised through multi-ple channels like on X  X ine job portals, newspaper advertise-ments, etc. Candidates who are interested to apply for the job opening uploads their profile through a designated web-site. The website typically provides an on X  X ine form where http://www.infosys.com/newsroom/infosys-in-the-news/infosys-u/pages/index.aspx the candidate enters details about her application like per-sonal information, education and experience details, skills, etc. We call this Candidate Meta X  X ata. The candidates can also upload their resumes through the website. The objec-tive of allowing the candidate to enter meta X  X ata in an on X  line form is to capture the information in a more structured format to facilitate automated analysis. However, real life experience suggests that most candidates do not specify a lot of information in the on X  X ine forms and hence Candidate Meta X  X ata is often incomplete.
 Figure 1: Recruitment process with manual screen-ing
Once the applications of prospective candidates are re-ceived, they are subjected to careful scrutiny by a set of dedicated screeners. This screening process is crucial be-cause it directly affects the quality of the intake and hence, the company profits. The screeners typically proceed as be-low: 1. Understand the requirement for the job opening, in 2. Look through each of the applications, and reject those 3. Out of the remaining candidates, find the best match
The top few candidates who are shortlisted during the screening, undergo further evaluation in the form of inter-views, written tests, group discussions etc. The feedback from these evaluation processes is used to make the final hiring decision.

In this paper, our focus is the screening task which is a tedious and time consuming task. A candidate X  X  profile is multifaceted and often the various attributes in his pro-file are not directly comparable with others. For example, a candidate A may be highly experienced in the technical area but may not have the desired industry domain exper-tise. Another candidate B might have the required domain expertise, but may be slightly less experienced in the techni-cal area than candidate A. Yet another candidate, C, might be much more versatile, possessing skills in a large number of technical areas and business domains, but lacking the re-quired mastery of a specific technical area. The screener has to go through the resume of each applicant and quickly decide whether to shortlist the candidate or not. However, ideally, the screener should not make this decision without looking at the resumes of other candidates who have applied for the job. If the screener defers the decision, it is difficult to later come back and make a decision, especially when there are thousands of candidates who have applied for the same job. The screeners are also under immense pressure to hire people quickly, since the rapid growth of labor intensive companies is critically dependent on this. Often, because of these time pressures, screeners rely on small samples of the applicant pools or ranking provided by third party hiring firms to screen and shortlist candidates. In some cases, the best candidate for a job might not have been looked at by a screener! In this paper, we present a decision support system (dubbed Prospect) for helping screeners shortlist better candidates faster.

The key challenge in building a system to aid screening is that job requirements and resumes are written in natural language and job requirements often contain complex con-straints (e.g.  X  X t least 6 years of J2EE experience X ). It is clear that keyword matching of job requirements to resumes does not suffice to give a good ranking of candidates. In addition to improved ranking by weighting skill words more than others, Prospect allows screeners to quickly select can-didates that meet specified requirements via the user inter-face. Thus, we are able to achieve a more uniform screening process, lower cost to the organization, and identify better candidates.

This paper is organized as follows: In the next section, we outline and contrast our work with related work in retrieval on resumes. This is followed by Section 3 which gives the architecture and an overview of our systems features. Sec-tion 4 describes the information extraction techniques we use to mine important information useful in the screening process and gives experimental results for accuracies of ex-traction of various attributes. Section 5 describes our exper-iments on the retrieval method used for ranking candidates for jobs. Finally, we conclude by discussing some experiences of screeners using the system, and feedback from them that can guide future work.
There have been several attempts to automate various aspects of the recruitment process [16]. For example, [14] suggests using techniques like collaborative filtering to rec-ommend candidates matching a job. [27] describes a method that uses relevance models to bridge the vocabulary divide between job descriptions and resumes. In their method, re-lated job descriptions are identified by matching a given can-didate job description with a database of job descriptions. Then, resumes matching those job descriptions are used to capture vocabulary that is not explicitly mentioned in the job descriptions. These methods assume the existence of manually labeled relevance rankings. On the other hand, our method does not assume the presence of relevance rankings for training. In [9, 11], collaborative filtering measures are combined with content based similarity measures for better ranking. However, most of these studies are performed on synthetic data and not on real world unstructured resumes and job descriptions.

There is a body of work that takes into account individual preferences and team dynamics for staffing [17, 7]. In our work, we do not look at teaming aspects and decisions are made solely based on the skill match of the candidate to the job description.

There is also work focusing exclusively on information ex-traction from resumes [28]. In contrast to previous work, we describe an entire system for aiding screeners, using a combination of information extracted from resumes and re-trieval techniques to help the screeners accomplish their tasks faster. In particular, we show that we can improve retrieval performance using information extracted from re-sumes. Also unlike them we extract experience for each skill.
As described in Section 1, the manual screening process is highly cumbersome and inefficient. In this section, we describe the features of our system that facilitates a more effective screening process.

For each job profile, the system automatically ranks can-didates based on the similarity between job profile and the resume of the candidate. The screener can refine this rank-ing by adding multiple search and filtering criteria. The filtering criteria can be based on the information in the can-didate meta X  X ata as well as on the information that is au-tomatically extracted from the resumes. As new conditions are entered by the screener, Prospect refines the ranking to reflect the new constraints. This refining procedure is repeated until the screener is satisfied with the system gen-erated ranking. Subsequently, the screener can shortlist the top candidates in the ranked list for interviews, and reject in bulk, the remainder of the list.
 Figure 2 gives a screenshot of the user X  X nterface of Prospect. As can be noted from the screenshot, our system presents a multi X  X aceted view of the candidates thus helping the screen-ers quickly gauge the range of candidate profiles available. It also highlights the key aspects of the candidate profiles and displays relevant snippets of their resume.

In the sections below, we describe the features in Prospect that enable efficient screening of candidates, followed by a detailed system architecture .
In our initial ranking, we are looking for candidates that have a broad match of skills mentioned in the job descrip-tion. One way to achieve this is by computing a similarity score between the job description and the resume of a candi-date. In this paper we experimented with using various ad-hoc retrieval methods to provide a similarity score between the job description and the resume of a candidate. One is-sue we face is that the job description is long and usually contains lot of terms that are not important to match. We found that the retrieval method that performed the best for our task (TFIDF scoring model from Lucene) could be en-hanced by skills extracted from the job description. Details on our experiments are reported in Section 5.
Recruiters can enter search terms in the search box pro-vided in the system interface. This modifies the search criteria and those candidates whose resume mentions these search terms will have a higher rank.
A facet is a filter on a candidate attribute, which when chosen, will enforce the condition that, the candidates in the list should have the selected value for that particular attribute. For example, in the facet for education, if the screener chooses  X  X asters Degree X , then, the candidate X  X  list is refreshed to show only those candidates who have  X  X asters Degree X .

In addition to displaying the values, we also display the number of candidates against each of them. This gives the distribution of the values for the facet, on the current set of candidates. It helps the recruiters get a broad idea of the skills, education levels etc. of the candidates.

In many cases, the job role requires the candidates to have a minimum level of experience with a tool or skill. To enable the recruiters to filter candidates based on the minimum experience for a skill, Prospect extracts skills mentioned in the resumes and its corresponding experience level. The extraction process is detailed in Section 4.

Prospect provides the following facets:
Most positions require that the candidate X  X  educational qualification is from an accredited university. If the candi-date has graduated from a non-accredited university, then he or she may not be eligible to apply for the position and should be rejected. Similarly, there might be restrictions on hiring candidates from a set of specific organizations be-cause of recruiting policies. A typical example is when the candidate is working for a close competitor and has signed a non-compete clause with his current organization.
To assist the recruiters in this task, Prospect highlights such candidates. This is achieved by maintaining a list of no X  X ire companies and non-accredited universities in the system. Information from this list is compared with the candidate information and matching candidates are high-lighted. The recruiters can choose to reject these candidates without going through their profile in detail.
Typically, in large organizations, there are several hundred job openings that are active at any point of time. Candi-dates can apply to multiple job openings in parallel. The screener has to ensure that if a candidate has already been offered a position in the organization, or has been rejected earlier for the same job position, he should not be consid-ered again. In Prospect, we detect duplicate applications and alert the recruiter by highlighting corresponding candi-date profiles. We also provide information about the status of other applications submitted by the candidate.
Occasionally, the job descriptions are not specified in de-tail. In such cases, it becomes difficult for the recruiter to screen profiles. This problem is compounded when the re-cruiter is not well versed in the job domain. In such cases, automatically suggesting additional skills that are relevant to the job description will help them.

Prospect provides an automatic skill suggestion feature, which is based on the hypothesis that most of the candidates who have applied for the job have the relevant skills. The number of candidates possessing a skill is observed to be a good estimate of its relevance to the job role. Though this approach is simplistic, we found that it works well in practice, especially when the number of applicants for a job is large.

One could also use techniques like KL divergence [12] to suggest skills that differentiate various job profile.
Figure 3 outlines the architecture of Prospect. It com-prises of the following main components: Batch Processor, Query Processor and Resume Matcher.

New applications are first processed by the Batch Proces-sor. It stores the candidate meta X  X ata in the Master DB and extracts data from the resumes, which is saved in Extracted DB. This information is used by the Query Processor and the Resume Matcher to provide a ranked candidate list for a given user query.
This component executes a nightly job, which processes all resumes that have been received during the day. It consists of the following modules: 1. Text Index Generator: This component creates a full 2. Resume Miner: This is the core component of Prospect. 3. Duplicate Detector: As described in Section 3.5, can-
The Query Processor is responsible for interacting with various components like Text Index, Database and Resume Matcher. It takes as input, the Search Criteria and returns a ranked list of candidates as result. It comprises of query parser which parses user submitted query strings and con-verts it to a representative format suitable for Prospect. The query is then issued to text index and database by Query Processor. The results obtained are aggregated and submit-ted to Resume Matcher. Resume Matcher is the component which ranks the candidates based on extent of resume match with the job description. This ranked list is finally displayed to the user.
To support the features of Prospect described in Section 3, we extract various pieces of information from the unstruc-tured resumes. Information Extraction (IE) is the important task of deriving structured information from unstructured and semi X  X tructured sources. Over the years, IE capabili-ties have been successfully demonstrated across various tasks and domains [21]. As resumes can be written in a variety of styles, we prefer to use statistical data driven techniques rather than knowledge based techniques that use regular ex-pressions, dictionaries and rule matching. In this section we describe the techniques used in Prospect to extract informa-tion from resumes.
Conditional Random Fields (CRFs) are state-of-the-art probabilistic models for information extraction. CRFs were first proposed by [13] for segmenting and labeling sequence data. A CRF is an undirected graphical model that defines a single log-linear distribution over label sequences given an observation sequence. In the special case in which the ele-ments of the designated label sequence are linked by edges in a linear chain, CRFs make a first-order Markov indepen-dence assumption among output nodes. In this case CRFs can be thought of as conditionally-trained Hidden Markov Models.

Formally, let x = { x i } n i =1 and y = { y i } n i =1 be sequences of input and output variables respectively, where n is the length of sequence, x i  X  X  and y i  X  X  where X is the finite set of the input observations and Y is the output label state space. Then, a first-order linear-chain CRF is defined as: p ( y | x ) = 1 Z ( x )
 X  i ( x,y ) = exp X where s k is a state feature function that uses only the label at a particular position, t j is a transition feature function that depends on the current and the previous label, Z ( x ) is a normalization factor and  X  j is a learned weight for each feature function. Equation 1 can also be written as We train the parameters  X  to maximize the conditional log-likelihood on the training data T . The conditional log-likelihood of a set of training instances ( x m ,y m ) using pa-rameters  X  is given by: L  X  is concave in  X  and Quasi-Newton methods such as L-BFGS [5] can be used to maximize the likelihood.
For our problem of extracting various entities from re-sumes we use a hierarchical extraction model (Extractor). The rationale being, resumes exhibit a hierarchical structure where related concepts occur in close vicinity and there is a definite order in the overall structure. Once the information is extracted, it is normalized and checked for inconsistencies (Resolver). The Extractor along with the Resolver forms the resume extraction system henceforth referred to as Resume Miner.

Previous work in information extraction from resumes [28] have been based on hidden Markov models (HMM) [20]. Unlike HMMs, the main advantage of the CRF model is that it provides a compact way to integrate complex, overlapping and non-independent feature sets. In addition, CRFs avoid the label bias problem exhibited by HMMs that are based on directed graphical models [13]. A few studies have also shown that CRFs outperform HMMs on a number of real-world sequence labeling tasks [19, 23, 22]. In this work, we have used linear chain CRF to build our extraction models. Figure 4 depicts the outline of our resume extraction model. Analyzer (TA), Resume Segmenter(RS) and Concept Rec-ognizer(CR). TA is used to analyze and classify information compiled as tables in resume. Output of TA is provided as input to RS and CR as features. RS is responsible for partitioning resume into contiguous blocks of information. CR extracts various aspects of candidate profile using (along with other features) the output of RS. The extraction tem-plate in Figure 5 lists the attributes to be extracted. Table Analyzer (TA) Tables are a natural way of rep-lists education background lists past employer X  X  details like designation, employer name, employment duration Resume Segmenter (RS) It is responsible for segment-Concept Recognizer (CR) The segmented resume is then to be normalized. For example, one might encounter the same skill by different names across a resume (e.g. Microsoft Office XP, Office XP, MS Office XP). All these variations of the same skill mention should be mapped to a representative skill (i.e. office xp), so that the experience level across each mention is correctly summed up to give the overall experi-ence level for that skill. Prospect also has to flag candidates having affiliation with education institutes that are not ac-credited or employers that are in no X  X ire lists. How ever, the names of the institutes as mentioned in the resumes might not exactly match with the name in the lists. For example in the resume, the candidate may have mentioned  X  X P In-dia Pvt. Ltd. X , as an employer but in the no-hire list the same entity could be referred to as  X  X ewlett Packard India Pvt. Ltd. X . Resolver, a subsystem of Resume Miner is re-sponsible for resolving such cases. It uses string matching techniques [3] to tackle the data reconciliation problem. We now describe the various features that the Resume Segmenter and Concept Recognizer modules use for extrac-tion. These features can be broadly categorized as follows: we used various dictionary based features 4 . Jaro X  X inkler [25] distance metric was used for measuring similarity to words in an external dictionary. Some of these lexicons were compilation of attributes to be extracted (skill name, degree, etc.) while others contained keywords for specific blocks (e.g. common keywords for education block.) visual characteristics (layout and font) which can be lever-aged to identify them. For example, section headers might follow blank lines, or they might have different font charac-teristics as compared to a majority of text in the resume. Additionally, certain information like education, skill are mentioned inside tables.
 can be a strong indicator for identifying certain blocks. For example in a resume, a Date is mentioned in places like date of birth, employment date or project date. Similarly in many
Most of the dictionaries were compiled from open knowl-edge sources like Wikipedia Figure 6: Feature sets we use for Information Ex-traction. instances, a Duration 5 occurs inside an experience(project) block.
 tributes of the word itself (see Table 6 for details). form non-linear decision boundaries in the original feature space thereby capturing relationships that might not be pos-sible with a linear combination of features. In Resume Seg-menter, we use the conjunction of current line with the pre-vious line and the current line with the next line. In the Con-cept Recognizer, similar token level conjunctions are used. ognizer, we annotated around 110 English resumes using GATE (General Architecture for text Engineering) [8]. These resumes were carefully sampled across various job verticals span of time e.g. 12 months, 12 Sept to 14 Oct Figure 7: Statistics for the corpus of hand annota-tions. # Annotation done by more than one person 310 # Annotation with disagreement 41 Figure 8: Inter-annotator agreement on Skill name. and experience levels, so that learned model was not biased towards a particular job type. More than 98% of resumes were in Microsoft Word format. We used the Apache POI API 6 to parse these documents and extract data and visual (formating and layout) information. Figure 7 summarizes some important corpus statistics. We collected upwards of 7200 annotations from 3 volunteers. Volunteers were told to be as exhaustive as possible and tag all possible instances. This is very important in case of annotating skills, because we need to identify every occurrence of a skill in a resume to be able to calculate the experience level for that skill accurately. Figure 8 summarizes some statistics on inter X  annotator agreement for annotation on Skill name. Clearly, a considerable number of disagreements were found in the labeling wherein a segment was tagged by a volunteer while it was not tagged or given a different label by another vol-unteer. We believe similar cases would exist for other la-bels(attribute/segment annotations) too. We used 5 X  X old cross validation to evaluate our extraction system. To an-alyze the contribution of different kinds of features, we de-signed experiments with incremental addition of new feature types.
 cision, recall and F1-measure to evaluate experimental re-sults [6]. Precision and recall are macro-averaged across doc-uments and overall F1 computed from average precision and recall. Like [28], we used  X  X verlap X  criteria [15] (match if &gt; 90% overlap) to match ground truth with extracted data. man annotated tables as examples to learn the classification model. A combination of layout features (number of cells, number of columns, number of rows) and content features (header content, presence of date, presence of duration, ta-ble content, lexicon for designation, degree etc.,) achieved an averaged F1 of 0.958 . For identifying the type of column information, ColClass uses content based features similar to TabClass, along with other features like similarity of header http://poi.apache.org/ Figure 9: Segmentation results with different fea-ture sets. cells to a predefined label set (University, Degree, Year, Des-ignation etc.,) and achieved an averaged F1 of 0.941 . performance of Resume Segmenter. While VIS alone gives good results, combination of LEX, NE, VIS gives the best result. We found that Features 11, 14 and 18 were most effective in segmentation tasks. A large fraction of resumes had Education information embedded inside a table, so iden-tifying that was relatively easier. On the contrary, Project block appears in variety of formats (a sequence of project blocks, list of past employers each having a list of project block, etc.) and was difficult to extract. This was evident even during manual annotation, as there were several cases when volunteers were not easily able to decide the bound-aries of a project block.
 used as input features to the Concept Recognizer module. As shown in Figure 10, accuracy results for the Concept Recognizer followed a similar trend to Resume Segmenter. A combination of LEX, TEXT and VIS features gave the best overall F1 for all attribute extractors. Features 1, 5, 10 and 37 were key in identifying Skill names. For Education and past Employer details, features 19, 20 and 21 seemed to matter the most. On analysis we found that most of the incorrect extractions occurred when data was not compiled as tables in resume. Some of the common errors were the duration for which the project was executed. Sometimes this information is mentioned at the past employer level and not at the project level 7 . Any skill that is mentioned inside the project block gets this project experience. For exam-ple if Java is mentioned in project X  X  and project X  X  having duration of x and y months respectively, then overall expe-rience for Java is ( x + y ) months. This simplistic rule is used by the recruiters while screening for jobs having specific skill experience level requirement. This case can be handled easily Figure 10: Entity extraction results for various fea-ture sets.

Output of Resume Segmenter (project blocks) and Con-cept Recognizer(skill name), along with project duration in-formation is used to mine experience level information. To evaluate effectiveness of this simple strategy we collected around 148 human judgments, each comprising of a skill and corresponding experience level in months. For collect-ing these judgments we followed two strategies to nullify any bias due to resume structure. For evaluating the performance, we counted the number of instances for which inferred experience level was within a bound of human judgments. Figure 11 outlines the results. X X  X xis shows the error bounds for example, 10 refers to a bin where inferred level was within 10% of human judged ex-perience level. Y X  X xis records the number of skills that fall in that bin. We choose this evaluation method because job description generally require skill with range for experience level i.e 2 to 5 years experience in Java , so just using pre-cision may not indicate utility of extracted data. Detailed analysis highlighted the following reasons of failure: # skill instance Figure 11: Error histogram for extracting the expe-rience associated with skills.
Given a set of candidates that satisfy the filtering condi-tions, we would like to display candidates that better match the job requirements at the top. We create a query q from the job requirements and treat the text of the candidate re-sumes as documents d and apply standard ad-hoc retrieval techniques to rank the candidates. The different retrieval al-gorithms we use are Okapi BM25 (from the Lemur toolkit), Kullback X  X eibler divergence of language models with Dirich-let smoothing (also from the Lemur toolkit) and the TF-IDF scoring model in Lucene. Since the job descriptions are fairly verbose we also experiment with a retrieval model where cer-tain terms that are important to the quality of the match are weighted up in the TF-IDF scoring model. In our exper-iments we extracted the skills from the job description using a simple dictionary based extraction method where the dic-tionary consisted of a set of skills that we extracted using the system described in Section 4 from all the resumes in our system (across all the jobs).

For our experiments to compare the various retrieval meth-ods we used 8 job descriptions and retrieved the top 20 can-didate resumes for each method and judged the relevance of these against the job description. The jobs we chose had an average of 2000 candidates per job. Since we were interested in screening we also evaluated the 20 bottom ranked candi-dates. For the bottom ranked candidates, all of the methods performed equally well and we found no relevant candidates in that set for any of the methods. To compare performance of retrieval methods for the top results returned, we used both NDCG [10] and Precision @ k. Figure 12 summarizes our results and we see that Lucene TF-IDF scoring with boosting of skill terms performs the best. This agrees with results in [2] where it was found that finding and weighting up important concepts in long queries can improve retrieval performance.

While keyword based scoring functions do help in making the job of the recruiter easier, it is clear that they cannot capture requirements such as  X  X t least 3 years of Java de-velopment X  . Prospect allows recruiters to filter candidates based on the number of years of experience they have in cer-tain skills. In the next experiment we compared the quality of the ranking and with and without the filter when the job descriptions contained a requirement in terms of the number of years of experience for one or more skills. We see from the Figure 12: Comparison of various retrieval methods for ranking candidates Figure 13: Evaluation of ranking obtained with and without using the extracted years of experience results in Table 13 that the ranking performance is indeed significantly improved (around 30% ) by application of the filter.
In this paper we described Prospect a system that aids in the shortlisting of candidates for jobs. The key technical component that makes Prospect possible is the extraction of various pieces of information from resumes. Since job requirements often specify requirements such as  X  X t least 3 years of Java development X  we use information extracted from resumes to provide filters that allow screeners to find candidates that match such criteria. This allows us to over-come the limitations inherent in purely keyword based match-ing.

In piloting Prospect with screeners we estimated that us-ing the tool roughly sped up the screening process by a factor of 20 as compared to manual screening. This speedup can be attributed mainly to a combination of two factors. Firstly, ranking the candidates by match to the job description and use of the filters provided based on various information ex-tracted from the resume allows the screeners to inspect far fewer resumes to shortlist a given number of candidates. Sec-ondly, showing snippets of the resume based on its match to the requirements of the job and based on the information extracted from the resume allows a screener to shortlist or reject candidates much faster than if they had to scan the entire resume.

Despite these benefits, we did receive qualitative feedback that could help improving Prospect. One major area of im-provement is in the consideration of three factors that the screeners considered to be important in the ranking func-tion. In order of importance these are: As future work, we plan to learn to weight these factors (which is part of the information we extract from resumes) using relevance feedback data we collect from the screeners. [1] Apache Lucene. http://lucene.apache.org, 2009. [2] M. Bendersky and W. B. Croft. Discovering key [3] M. Bilenko, R. Mooney, W. Cohen, P. Ravikumar, and [4] A. Z. Broder. Identifying and filtering near-duplicate [5] R. H. Byrd, J. Nocedal, and R. B. Schnabel.
 [6] S. Chakrabarti. Mining the Web: Discovering [7] V. Chenthamarakshan, K. Dey, J. Hu, A. Mojsilovic, [8] H. Cunningham, D. Maynard, K. Bontcheva, and [9] F. Farber, T. Weitzel, and T. Keim. An automated [10] K. J  X  arvelin and J. Kek  X  al  X  ainen. Ir evaluation methods [11] T. Keim. Extending the applicability of recommender [12] S. Kullback. Information Theory and Statistics . Dover [13] J. D. Lafferty, A. McCallum, and F. C. N. Pereira. [14] S. Laumer and A. Eckhardt. Help to find the needle in [15] A. Lavelli, M. E. Califf, F. Ciravegna, D. Freitag, [16] I. Lee. An architecture for a next-generation holistic [17] J. Malinowski, T. Weitzel, and T. Keim. Decision [18] Nasscom. Knowledge professionals in india.
 [19] D. Pinto, A. McCallum, X. Wei, and W. B. Croft. [20] L. R. Rabiner. A tutorial on hidden markov models [21] S. Sarawagi. Information extraction. FnT Databases , [22] S. Sarawagi and W. W. Cohen. Semi-markov [23] F. Sha and F. Pereira. Shallow parsing with [24] Y. Wang and J. Hu. A machine learning based [25] W. Winkler. Matching and record linkage. 1995. [26] Y. Yang and W.-S. Luk. A framework for web table [27] X. Yi, J. Allan, and W. B. Croft. Matching resumes [28] K. Yu, G. Guan, and M. Zhou. Resume information
