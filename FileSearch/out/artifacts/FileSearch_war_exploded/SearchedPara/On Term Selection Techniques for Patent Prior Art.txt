 In this paper, we investigate the influence of term selection on retrieval performance on the CLEF-IP prior art test col-lection, using the Description section of the patent query with Language Model (LM) and BM25 scoring functions. We find that an oracular relevance feedback system that extracts terms from the judged relevant documents far out-performs the baseline and performs twice as well on MAP as the best competitor in CLEF-IP 2010. We find a very clear term selection value threshold for use when choosing terms. We also noticed that most of the useful feedback terms are actually present in the original query and hypoth-esized that the baseline system could be substantially im-proved by removing negative query terms. We tried four simple automated approaches to identify negative terms for query reduction but we were unable to notably improve on the baseline performance with any of them. However, we show that a simple, minimal interactive relevance feedback approach where terms are selected from only the first re-trieved relevant document outperforms the best result from CLEF-IP 2010 suggesting the promise of interactive meth-ods for term selection in patent prior art search. Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Query Formulation Keywords: Patent Search; Query Reformulation.
Patent prior art search involves finding previously granted patents, or any published work, such as scientific articles or product descriptions that may be relevant to a new patent application. The objective and challenges of standard for-mulations of patent prior art search are different from those of standard text and web search since [8]: (i) queries are reference patent applications, which consist of documents with hundreds or thousands of words organized into several sections, while typical queries in text and web search con-gorithm and English stop-word removal. We also removed patent-specific stop-words as described in [8]. We indexed each section of a patent (title, abstract, claims, and descrip-tion) in a separate field. However, when a query is processed, all indexed fields are targeted with an equal weight. We also used the International Patent Classification (IPC) codes as-signed to the topics to filter the search results by constrain-ing them to have common IPC codes with the patent topic as suggested in previous work [7]. Although this IPC code filter may prevent retrieval of relevant patents, we have cho-sen to keep it for the following reasons: (i) more than 80% of the patent queries share an IPC code with their associated relevant patents, and (ii) it makes the retrieval process much faster. System performance is evaluated using two popular metrics  X  Mean Average Precision (MAP) and Average Re-call  X  on the top-100 results for each query, assuming that patent examiners are willing to assess the top 100 patents [5]. We achieved the best performance while querying with the Description section as in previous work [16] and using either the LM or the BM25 scoring functions. We call this the Patent Query and use it as our baseline.

In addition, we compare our results to PATATRAS , a highly engineered system developed by Lopez and Romary [7], which achieved the best performance in the CLEF-IP 2010 competition. This system uses multiple retrieval models and exploits patent metadata and citation structures. All results in the paper use the 1348 English topic subset as reported in the PATATRAS evaluation [14]. Since the evaluation of our systems used a slightly smaller subset of 1281 topics as noted previously, we assume no relevant results were found by our systems for the 67 remaining topics of the 1348 topic subset in order to ensure a fair comparison to PATATRAS.
In this section we develop an Oracular Query to under-stand (a) the adequacy of the baseline Patent Query , (b) an upper bound on performance of the BM25 and LM models, and (c) the sufficiency of terms in the reference patent query.
We begin by defining an oracular relevance feedback sys-tem, which extracts terms from the judged relevant docu-ments. To this end, after an initial run of a given query, we calculate a Relevance Feedback ( RF ) score for each term t in the top-100 retrieved documents for query Q as follows: where Rel ( t , Q ) is the average term frequency in retrieved relevant patents and Irr ( t , Q ) is the average term frequency in retrieved irrelevant patents. We assume that words with a positive score are useful words since they are more frequent in relevant patents, while words with negative score are noisy words as they appear more frequently in irrelevant patents. We empirically seek to evaluate the threshold  X  on RF ( t,Q ) (defined below) yielding the best oracular query.

We formulate two oracular queries. The first query is formulated by selecting terms in the top-100 documents: We formulate the second query by selecting terms that also occur in the reference patent query as follows:
The gain achieved using the Oracular Patent Query method motivates us to explore various methods to approximate the terms selected by this query without  X  X eeking at the an-swers X  provided by the actual relevance judgements. We first attempt this via fully automated methods and then proceed to evaluate semi-automated methods based on interactive relevance feedback methods.
We use the following four simple approaches to reduce the initial Patent Query: (i) In standard IR approaches, removing terms appearing highly frequently across documents in the collection can im-prove retrieval effectiveness. Inspired by this fact, after an initial run of the query, we removed terms with a high aver-age document frequency (DF) over the top-100 documents ( DF ( t ) &gt;  X  ). As seen in Figure 2 (magenta line), such prun-ing hurts performance. DF pruning continues increasing and converges to the baseline as  X   X  X  X  (i.e., no pruning). (ii) Frequent terms inside long and verbose queries are con-sidered important [13]. Thus, we hypothesize that removing terms appearing infrequently in the Patent Query may help and hence propose to remove terms with query term fre-quency (QTF) below a threshold  X  ( QTF ( t )  X   X  ). Results in Figure 2 (blue line) indicate the performance is slightly better than the baseline when removing low QTF terms. The best MAP is achieved when  X  = 5 and it meets the baseline when  X  = 0 (i.e., all terms retained). (iii) We use Pseudo Relevance Feedback ( PRF ) to select query terms [13]  X  the same as we did for the Oracular Rele-vance Feedback system (Section 3). We assume that the top 5 retrieved documents are relevant and the rest are irrelevant (this performed best), then we calculate PRF score based on this assumption. Terms that have PRF score higher than the threshold  X  ( PRF ( t ) &gt;  X  ), are selected from the Patent Query to reformulate a reduced query. Figure 2 (red line) shows that this approach is also unsuccessful at achieving a notable improvement over the baseline. (iv) The titles of IPC codes indicate the intended content of patents classified under that code by using a single phrase or several related phrases linked together. We used words in IPC code titles for each patent query as stopwords to reduce the query, based on the assumption that these terms are common to all patents having the same IPC code label. As it can be seen in Figure 2 (black line), this approach slightly helps the performance.

Figure 3 shows an anecdotal example for a sample query about an invention related to  X  X mulsifier X  to help explain why these four approaches fail . It shows the raw abstract of the invention, and the top 20 high-scoring terms (except for IPC Title Terms which are not scored, but simply dis-played) and their associated RF scores for each approach. It can be seen that the four methods fail to clearly discrim-inate between useful and noisy terms. Important stemmed terms like  X  X nzym X  and  X  X tarch X  would be pruned according to DF; in contrast, QTF and PRF both score  X  X tarch X  highly and retain it, but also retain other noisy terms. Over half of the IPC Title Terms are noisy and appropriate to remove, but critical useful stemmed terms like  X  X mulsifi X  are also re-moved. Critically, all methods retain noisy terms (red/nega-Table 2: System performance using minimal rele-vance feedback.  X  is RF score threshold, and k indi-cates the number of top relevant patents. Figure 4: The distribution of the first relevant doc-ument rank over test queries. (Equation 3) derived from only the top-k ranked relevant documents identified in the search results (for small k )  X  we assume that the remaining documents in the top-100 are irrelevant. Using this approach, Table 2 shows that we can double the MAP in comparison to our baseline and also out-perform the PATATRAS system by identifying only the first relevant document.

Furthermore, to establish the minimal interaction required by this approach, Figure 4 indicates that the baseline meth-ods return a relevant patent approximately 80% of the time in the first 10 results and 90% of the time in the first 20 re-sults. Hence, such an interactive approach requires relatively low user effort while achieving state-of-the-art performance.
In this work, we focused on the development of an orac-ular query in order to address a number of fundamental questions regarding query reformulation and their efficacy in terms of approximating the oracular query. Previous works have not formulated such an oracular query, but nonethe-less have inspired our investigation of query reformulation techniques. Bashir et al. [1] proposed query expansion with pseudo-relevance feedback that used machine learning for term selection. Verma and Varma [15] used IPC codes in-stead of using the patent text to query, which are expanded using the citation network. Itoh et al. [4] proposed a new term selection method using different term frequencies de-pending on the genre in the NTCIR-3 Patent Retrieval Task. Mahdabi et al. [12] used term proximity information to iden-tify expansion terms. Ganguly et al. [3] adapted pseudo-relevance feedback for query reduction by decomposing a patent application into constituent text segments; the least similar segments to the pseudo-relevant documents are re-moved from the query. Kim et al. [6] provided diverse query suggestion using aspect identification from a patent query to increase the chance of retrieving relevant documents. Magdy et al. [9] and Bouadjenek et al. [2] studied different query expansion and reduction techniques for patent search on CLEF-IP 2010, and reported little improvement with au-tomatic methods. Magdy et al. [10] further compare the best two systems in CLEF-IP 2010.
