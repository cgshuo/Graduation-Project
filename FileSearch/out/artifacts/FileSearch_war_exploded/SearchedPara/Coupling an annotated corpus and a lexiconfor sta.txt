 Abstract This paper investigates how to best couple hand-annotated data with information extracted from an external lexical resource to improve part-of-speech tagging performance. Focusing mostly on French tagging, we introduce a maximum entropy Markov model-based tagging system that is enriched with information extracted from a morphological resource. This system gives a 97.75 % accuracy on the French Treebank, an error reduction of 25 % (38 % on unknown words) over the same tagger without lexical information. We perform a series of experiments that help understanding how this lexical information helps improving tagging accuracy. We also conduct experiments on datasets and lexicons of varying sizes in order to assess the best trade-off between annotating data versus developing a lexicon. We find that the use of a lexicon improves the quality of the tagger at any stage of development of either resource, and that for fixed performance levels the availability of the full lexicon consistently reduces the need for supervised data by at least one half.
 Keywords Part-of-speech tagging Maximum entropy models Morphosyntactic lexicon French Language resource development 1 Introduction Over recent years, numerous systems for automatic part-of-speech (POS) tagging have been proposed for a large variety of languages. Among the best performing systems are those based on supervised machine learning techniques (see Manning and Schu  X  tze 1999 for an overview). For some languages like English and other European languages, these systems have reached performance that comes close to human levels. Interestingly, the majority of these systems have been built without resorting to any external lexical information sources; they instead rely on a dictionary that is based on the training corpus (see however Hajic  X  2000 ). This raises the question of whether we can still improve tagging performance by exploiting this type of resource. Arguably, a potential advantage of using an external dictionary is in a better handling of unknown words (i.e., words that are not present in the training corpus, but that may be present in the external dictionary). A subsequent question is how to best integrate the information from a lexical resource into a probabilistic POS tagger. In this paper, we consider two distinct scenarios: (1) using the external dictionary as constraints that restrict the set of possible tags that the tagger can choose from, and (2) incorporating the dictionary tags as features in a probabilistic POS tagging model. Another interesting question is that of the relative impact of training corpora and of lexicons of various sizes. This issue is crucial to the development of POS taggers for resource-scarce languages for which it is important to determine the best trade-off between annotating data and constructing dictionaries.

This paper addresses these questions through various tagging experiments carried out on French, based on our new tagging system called MElt (Maximum-Entropy Lexicon-enriched Tagger). An obvious motivation for working on this language is the availability of a training corpus [namely, the French Treebank (Abeille  X  et al. 2003 )] and a large-scale lexical resource [namely, Le fff (Sagot et al. 2006 )]. Additional motivation comes from the fact that there has been comparatively little work in probabilistic POS tagging in this language. An important side contribution of our paper is the development of a state-of-the-art, freely distributed POS tagger for French. 1 Specifically, we here adopt Maximum Entropy Markov Models (MEMMs), an extension of MaxEnt models for sequence labeling. MEMMs remain among the best performing tagging systems for English and they are particularly easy to build and fast to train.

This paper is organized as follows. Section 2 describes the datasets and the lexical resources that were used. Section 3 presents a baseline MEMM tagger for French that is inspired by previous work, in particular (Ratnaparkhi 1996 ; Toutanova and Manning 2000 ), that already outperforms TreeTagger (Schmid 1994 ) retrained on the same data. In Sect. 4 , we show that the performance of our MEMM tagger can be further improved by incorporating features extracted from a large-scale lexicon, reaching a 97.75 % accuracy, which compares favorably with robustness of our approach, we apply it to various languages and of different sizes. Finally, Sect. 6 evaluates the relative impact on accuracy of the training data and the lexicon during tagger development by varying their respective sizes. These last two sections build on two previous conference publications, Denis and Sagot ( 2009 , 2010 ), respectively. 2 Resources and tagset 2.1 Corpus The morphosyntactically annotated corpus we used is a variant of the French TreeBank or FTB , (Abeille  X  et al. 2003 ). It differs from the original FTB in so far that all compounds that do not correspond to a syntactically regular sequence of categories have been merged into unique tokens and assigned a category corresponding to their spanning node; other compounds have been left as sequences of several tokens (Candito, p.c.). The resulting corpus has 350,931 tokens in 12,351 sentences.

In the original FTB , words are split into 13 main categories, themselves divided into 34 subcategories. The version of the treebank we used was obtained by converting subcategories into a tagset consisting of 28 tags, with a granularity that is intermediate between categories and subcategories. Basically, these tags enhance main categories with information on the mood of verbs and a few other lexical features. This expanded tagset has been shown to give the best statistical parsing results for French (Crabbe  X  and Candito 2008 ). 2 A sample tagged sentence from the FTB is given in Fig. 1 .

As in Candito et al. ( 2009 ), the FTB is divided into 3 sections: training (80 %), development (10 %) and test (10 %). The dataset sizes are presented in Table 1 together with the number of unknown words. 2.2 Lexicon One of the goals of this work is to study the impact of using an external dictionary for training a tagger, in addition to the training corpus itself. We used the morphosyntactic information included in the large-coverage morphological and syntactic lexicon Le fff , developed in the Alexina framework (Sagot 2010 ). 3
Although Le fff contains both morphological and syntactic information for each entry (including sub-categorization frames, in particular for verbs), we extracted only the morphosyntactic information. We converted categories and morphological tags into the same tagset used in the training corpus, hence building a large-coverage morphosyntactic lexicon containing 507,362 distinct entries of the form (form, tag, lemma) , corresponding to 502,223 distinct entries of the form (form, tag) .  X  X  X ategories X  X , these entries correspond to 117,397 (lemma, category) pairs (the relevance of these pairs will appear in Sect. 6 ) 3 Baseline MEMM tagger This section presents our baseline MaxEnt-based French POS tagger, MElt fr 0 .This tagger is largely inspired by Ratnaparkhi ( 1996 ) and Toutanova and Manning ( 2000 ), both in terms of the model and the features being used. To date, MEMM taggers are still among the best performing taggers developed for English. 4 An important appeal of MaxEnt models is that they allow for the combination of very diverse, potentially overlapping features without assuming independence between the predictors. These models have also the advantage of being very fast to train. 5 3.1 Description of the task process of assigning the maximum likelihood tag sequence ^ t n 1 2 T n to w 1 n . Following comprises the preceding tags t i i -1 and the word sequence w i n . 3.2 Model and features In a MaxEnt model, the parameters of an exponential model of the following form are estimated: f m are feature functions defined over tag t different tags. In this type of model, the choice of the parameters is subject to constraints that force the model expectations of the features to be equal to their empirical expectations over the training data (Berger et al. 1996 ). In our experiments, the parameters were estimated using the Limited Memory Variable Metric Algorithm (Malouf 2002 ) implemented in the Megam package. 6
The feature templates we used for designing our French tagging model is a superset of the features used by Ratnaparkhi ( 1996 ) and Toutanova and Manning ( 2000 ) for English (these were largely language independent). These features fall into two main categories. A first set of features try to capture the lexical form of the word being tagged: these include the actual word string for the current word w i , prefixes and suffixes (of character length 4 and less), as well as binary features testing whether w i contains special characters like numbers, hyphens, and uppercase letters. A second set of features directly model the context of the current word and tag: these include the previous tag, the concatenation of the two previous tags, as well as the surrounding word forms in a window of 5 tokens.
 The detailed list of feature templates we used in this baseline tagger is shown in Table 2 . 7
An important difference with Ratnaparkhi ( 1996 ) in terms of feature design is that we did not restrict the application of the prefix/suffix features to words that are rare in the training data. In our model, these features always get triggered, even for frequent words. We found that the permanent inclusion of these features led to better performance during development, which can probably be explained by the fact that these features get better statistics and are extremely useful for unknown words. These features are also probably more discriminative in French than in English, since it is morphologically richer. Another difference to previous work regards smoothing. Ratnaparkhi ( 1996 ) and Toutanova and Manning ( 2000 ) use a feature count cutoff of 10 to avoid unreliable statistics for rare features. We did not use cutoffs but instead use a regularization Gaussian prior on the weights, 8 which is arguably a more principled smoothing technique. 9 3.3 Testing and performance The test procedure relies on a beam search to find the most probable tag sequence for a given sentence. That is, each sentence is decoded from left to right and we maintain for each word w i the n highest probability tag sequence candidates up to w . For our experiments, we used a beam size of 3. 10 In addition, the test procedure utilizes a tag dictionary which lists for a given word the tags associated with this word in the training data. This drastically restricts the allowable labels that the tagger can choose from for a given word, in principle leading to fewer tagging errors and reduced tagging time.

The maximum entropy tagger described above, MElt fr 0 , was compared against two other baseline taggers, namely: UNIGRAM and TreeTagger. UNIGRAM works as follows: for a word seen in the training corpus, this tagger uses the most frequent tag associated with this word in the corpus; for unknown words, it uses the most frequent tag in the corpus (in this case, NC ). TreeTagger is a statistical, decision tree-based POS tagger (Schmid 1994 ). 11 The version used for this comparison was retrained on the FTB training corpus. The performance results of the three taggers are given in Table 3 ; scores are reported in terms of accuracy over both the entire test set and the words that were not seen during training.

As shown in Table 3 , MElt fr 0 achieves accuracy scores of 97 % overall and 86.1 % on unknown words. 12 Our baseline tagger significantly outperforms the retrained version of TreeTagger, with an improvement of over 10 % on unknown words. 13 There are several possible explanations for such a discrepancy in handling unknown words. The first one is that MaxEnt parameter estimation is less prone to data fragmentation for sparse features than Decision Tree parameter estimation due to the fact that it does not partition the training sample. A second related explanation is that TreeTagger simply misses some of the generalizations regarding lexical features due to the fact that it only includes suffixes and this only for unknown words. 4 Lexicon-enriched MEMM tagger coupling it with an external lexical resource, and compare two ways of integrating this new information: as constraints versus as features. 4.1 Integrating lexical information in the tagger The most natural way to make use of the extra knowledge supplied by a lexicon is to represent it as  X  X  X iltering X  X  constraints: that is, the lexicon is used as an additional tag dictionary guiding the POS tagger, in addition to the lexicon extracted from the training corpus. Under this scenario, the tagger is forced for a given word w to assign one of the tags associated with w in the full tag dictionary: the set of allowed tags for w is the union of the sets of its tags in the corpus and in Le fff . This approach is similar to that of Hajic  X  ( 2000 ), who applied it to highly inflected languages, and in particular to Czech.

In a learning-based tagging approach, there is another possibility to accommo-date the extra information provided by Le fff : we can directly incorporate the tags associated by Le fff to each word in the form of features. Specifically, for each word, we posit a new lexical feature for each of its possible tags according to the Le fff ,as well as a feature that represents the disjunction of all Le fff tags (provided there is more than one). Similarly, we can also use the Le fff to provide additional contextual features: that is, we can include Le fff tags for all the words in a window of 5 tokens centered on the current token. Table 4 summarizes these new feature templates.
Integrating the lexical information in this way has a number of potential potential errors in the lexicon or simply mismatches between the corpus annotations and the lexicon categories). Furthermore, some of the above features directly model the context, while the filtering constraints are entirely non contextual. 4.2 Comparative evaluation We compared the performance of the Le fff -constraints based tagger MElt fr c and these taggers, UNIGRAM Le fff , like UNIGRAM , is a unigram model based on the training corpus, but it uses Le fff for labeling unknown words: among the possible Le fff tag for a word, this model chooses the tag that is most frequent in the training corpus (all words taken into account). Words that are unknown to both the corpus and Le fff are assigned NC . The second tagger, TreeTagger Le fff is a retrained version of TreeTagger to which we provide Le fff as an external dictionary. Finally, we also compare our tagger to F -BKY , an instantiation of the Berkeley lexicalized parser adapted for French by Crabbe  X  and Candito ( 2008 ) and used as a POS tagger. The performance results for these taggers are given in Table 5 .

The best tagger is MElt fr f , with accuracy scores of 97.75 % overall and 91.36 % for unknown words. This represents significant improvements of .75 and 5.26 % over MElt fr 0 , respectively. 14 By contrast, MElt fr c achieves a rather limited (and statistically insignificant) performance gain of .1 % overall but a 2.9 % improve-ment on unknown words. Our explanation for these improvements is that the Le fff -based features reduce data sparseness and provide useful information on the right context: first, fewer errors on unknown words (a direct result of the use of a morphosyntactic lexicon) necessarily leads to fewer erroneous contexts for other words, and therefore to better tagging; second, the possible categories of tokens that are on the right of the current tokens are valuable pieces of information, and they are available only from the lexicon. The lower result of MElt fr c can probably be explained by two differences: it does not benefit from this additional information about the right context, and it uses Le fff information as hard constraints, not as (soft) features.

Accuracy scores put MElt fr f above all the other taggers we have tested, including the parser-based F -BKY , by a significant margin. To our knowledge, these scores are the best scores reported for French POS tagging. 15 Other taggers have been proposed for French, some of which have been evaluated during the GRACE evaluation campaign. 16 Although a direct comparison is difficult, given the differences in terms of reference corpus and tagsets, it is worth mentioning that the best scores during this campaign, approximately 96 %, have been obtained by parser-based taggers (Adda et al. 1999 ). Finally, Nasr and Volanschi ( 2004 ) report a 97.82 % accuracy on the FTB , but their tagger/chunker does not take unknown words into account. 4.3 Error analysis In order to understand whether the 97.75 % accuracy of MElt fr f could still be improved, we decided to examine manually its first 200 errors on FTB -DEV , and classify them according to an adequate typology of errors. The resulting typology and the corresponding figures are given in Table 6 .

These results show that the 97.75 % score can still be improved. Indeed, standard named entity recognition techniques could help solve most errors related to named entities, i.e., more than one out of four errors. Moreover, simple regular patterns could allow for replacing automatically all numbers by one or several placeholder(s) both in the training and evaluation data. Indeed, preserving numbers as such inevitably leads to a sparse data problem, which prevents the training algorithm from modeling the complex task of tagging numbers X  X hey can be determiners, nouns, adjectives or pronouns. Appropriate placeholders should significantly help the training algorithm and improve the results. Finally, no less than 13.5 % of MElt fr f  X  X  apparent errors are in fact related to FTB -DEV  X  X  annotation, because of errors (9 %) or unclear situations, for which both the gold tag and MElt fr f  X  X  tag seem valid.

Given these facts, we consider it feasible to improve MElt fr f from 97.75 to 98.5 % in the future. 4.4 Impact of various sets of Le fff -based lexical features In order to understand better the relative impact on MElt fr f  X  X  model of various types of information extracted from the Le fff , we have run a series of ablation experiments on the set of features described in 4. Specifically, we have evaluated the 8 possible configurations consisting in including (or not) internal lexical features ( INT ), external lexical features defined on the left context ( LEFT ), and external lexical features defined on the right context ( RIGHT ). The results of these experiments performed on the development corpus FTB -DEV are given in Table 7 . Note that the experiments named here [ and INT ? LEFT ? RIGHT , correspond respectively to the variants MElt fr 0 and MElt fr f of our system.

These results indicate that it is the combination of internal and right external lexical features that brings the most information to the tagger. Indeed, the subset INT ? RIGHT yields the best results after MElt fr f itself, both on all words and on unknown words only. These two subsets of lexical features are complementary: INT features improve the lexical coverage of the tagger (some unknown words, i.e., unseen in the training corpus, are covered by the lexicon), whereas RIGHT features provide important information about the right context that MElt fr 0  X  X  features only model in a rough way. 5 Varying tagsets and languages In order to validate the robustness of our approach, we have trained two series of taggers with the same architecture as MElt fr f :  X  several other taggers trained on the same corpus and the same lexicon, but with  X  two other taggers trained on corpora and lexicon for other languages, namely Results are shown in Table 8 . Note that the last three experiments (French large , English, Spanish) rely on lexicons that do not use the same tagset as the corpus. 17 Indeed, since lexical information extracted from the lexicon is used in the form of features, there is no particular need for having the same tags in the lexicon as in the corpus.

These results are satisfying not only for MElt fr f itself. For example, the state-of-the-art for English on the same corpus (not necessarily split in the same way, though) is approximately 97.4 % (Spoustova  X  et al. 2009 ), a figure that is reached by combining several taggers. 6 Varying training corpus and lexicon sizes 6.1 Motivations and experimental setup The results achieved by MElt fr f have been made possible by the (relatively) large size of the corpus and the broad coverage of the lexicon. However, such resources are not always available for a given language, in particular for so-called under-resourced languages. Moreover, the significant improvement observed by using Le fff shows that the information contained in a morphosyntactic lexicon is worth using. The question arises whether this lexical information is able to compensate for the lack of a large training corpus. Symmetrically, it is unclear how various lexicon sizes impact the quality of the results.

Therefore, we performed a series of experiments by training MElt fr f on various sub-corpora and sub-lexicons. Extracting sub-corpora from FTB -TRAIN is simple: the first s sentences constitute a reasonable corpus of size s . However, extracting sub-lexicons from the Le fff is less trivial. We decided to extract increasingly large sub-lexicons in a way that approximately simulates the development of a morphosyntactic lexicon. To achieve this goal, we used the MElt fr f tagger described in the previous section to tag a large raw corpus. 18 We then lemmatized the corpus by assigning to each token the list of all of its possible lemmas that exhibit a category consistent with the annotation. Finally, we ranked all resulting ( lemma, category ) pairs w.r.t. frequency in the corpus. Extracting a sub-lexicon of size n then consists in extracting all ( form, tag, lemma ) entries whose corresponding ( lemma, category ) pair is among the l best ranked ones. We reproduced the same experiments as those described in Sect. 4 , but training MElt fr f on various sub-corpora and various sub-lexicons. We used 9 different lexicon sizes and 8 different corpus sizes, summed up in Table 9 . For each resulting tagger, we evaluated on FTB -TEST the overall accuracy and the accuracy on unknown words. 6.2 Results and discussion Before comparing the respective relevance of lexicon and corpus manual development for optimizing the tagger performance, we need to be able to quantitatively compare their development costs, i.e., times.

In Marcus et al. ( 1993 ), the authors report a POS annotation speed that  X  X  X xceeds 3,000 words per hour X  X  during the development of the Penn TreeBank. This speed is approximately 60 h) during which the POS tagger used for pre-annotation was still improving. The authors also report on a manual tagging experiment (without automatic pre-annotation); they observed an annotation speed that is around 1,300 words per hour. Therefore, it is probably safe to assume that, on average, the creation of a manually validated training corpus starts at a speed that is around 1,000 words (30 sentences) per hour, and increases up to 3,000 words (100 sentences) per hour once the corpus has reached, say, 5,000 sentences.

For lexicon development, techniques such as those described in Sagot ( 2005 ) allow for a fast validation of automatically proposed hypothetical lemmas. Manual intervention is then limited to validation steps that take around 2 X 3 s per lemma, i.e., about 1,500 lemmas per hour.
Figure 2 compares contour lines 19 for two functions of corpus and lexicon sizes: tagger accuracy and development time. 20 These graphs show different things:  X  during the first steps of development (less than 3 h of manual work), the  X  in later stages of development, the optimal approach is to develop both the  X  using a morphological lexicon drastically improves the tagging accuracy on  X  for fixed performance levels, the availability of the full lexicon consistently
These results demonstrate the relevance of developing and using a morphosyntactic lexicon for improving tagging accuracy both in the early stages of development and for long-term optimization. 7 Conclusions and perspectives We have introduced a new MaxEnt-based tagger, MElt, that we trained on the FTB for building a tagger for French. We show that this baseline, named MElt fr 0 , can be significantly improved by coupling it with the French morphosyntactic lexicon knowledge, the best figures reported for French tagging, including parsing-based taggers. More precisely, the addition of lexicon-based features yield error reductions of 25 % overall and of 38 % for unknown words (corresponding to accuracy improvements of .75 and 5.26 %, respectively) compared to the baseline tagger.
We also showed that the use of a lexicon improves the quality of the tagger at any stage of lexicon and training corpus development. Moreover, we approximately estimated development times for both resources, and show that the best way to optimize human work for tagger development is to work on the development of both an annotated corpus and a morphosyntactic lexicon.
In future work, we plan on trying and demonstrating this result in practice, by developing such resources and the corresponding MElt fr f tagger for an under-resourced language. We also intend to study the influence of the tagset, in particular by training taggers based on larger tagsets. This work should try and understand how to benefit as much as possible from the internal structure of tags in such tagsets (gender, number, etc.).
 References
