 Black-box identification approaches are widely used to learn dynamic models from a finite set of systems that involve a wide amount of variables and find important applications in many different domains such as chemical engineering, economic systems and computer vision [2]. In this scenario a key point is that the identification procedure should be sparsity-favouring, i.e. able to extract from the large number of subsystems entering the system description just that subset which influences significantly the system output. Such sparsity principle permeates many well known techniques in machine learning and signal processing such as feature selection, selective shrinkage and com-pressed sensing [3, 4].
 In the classical identification scenario, Prediction Error Methods (PEM) represent the most used approaches to optimal prediction of discrete-time systems [1]. The statistical properties of PEM (and Maximum Likelihood) methods are well understood when the model structure is assumed to be known. However, in real applications, first a set of competitive parametric models has to be postu-lated. Then, a key point is the selection of the most adequate model structure, usually performed by AIC and BIC criteria [5, 6]. Not surprisingly, the resulting prediction performance, when tested on experimental data, may be distant from that predicted by  X  X tandard X  (i.e. without model selection) statistical theory, which suggests that PEM should be asymptotically efficient for Gaussian innova-tions. If this drawback may affect standard identification problems, a fortiori it renders difficult the study of large scale systems where the elevated number of parameters, as compared to the number of data available, may undermine the applicability of the theory underlying e.g. AIC and BIC. Some novel estimation techniques inducing sparse models have been recently proposed. They in-clude the well known Lasso [7] and Least Angle Regression (LAR) [8] where variable selection is performed exploiting the ` 1 norm. This type of penalty term encodes the so called bi-separation feature, i.e. it favors solutions with many zero entries at the expense of few large components. Con-sistency properties of this method are discussed e.g. in [9, 10]. Extensions of this procedure for group selection include Group Lasso and Group LAR (GLAR) [11] where the sum of the Euclidean norms of each group (in place of the absolute value of the single components) is used. Theoreti-cal analyses of these approaches and connections with the multiple kernel learning problem can be found in [12, 13]. However, most of the work has been done in the  X  X tatic X  scenario while very little, with some exception [14, 15], can be found regarding the identification of dynamic systems. In this paper we adopt a Bayesian point of view to prediction and identification of sparse linear sys-tems. Our starting point is the new identification paradigm developed in [16] that relies on nonpara-metric estimation of impulse responses (see also [17] for extensions to predictor estimation). Rather than postulating finite-dimensional structures for the system transfer function, e.g. ARX, ARMAX or Laguerre [1], the system impulse response is searched for within an infinite-dimensional space. The intrinsical ill-posed nature of the problem is circumvented using Bayesian regularization meth-ods. In particular, working under the framework of Gaussian regression [18], in [16] the system impulse response is modeled as a Gaussian process whose autocovariance is the so called stable spline kernel that includes the BIBO stability constraint.
 In this paper, we extend this nonparametric paradigm to the design of optimal linear predictors for sparse systems. Without loss of generality, analysis is restricted to MISO systems so that we inter-pret the predictor as a system with m + 1 inputs (given by past outputs and inputs) and one output (output predictions). Thus, predictor design amounts to estimating m + 1 impulse responses mod-eled as realizations of Gaussian processes. We set their autocovariances to stable spline kernels with different (and unknown) scale factors which are assigned exponential hyperpriors having a common hypervariance. In this way, while GLAR uses the sum of the ` 1 norms of the single impulse re-sponses, our approach favors sparsity through an ` 1 penalty on kernel hyperparameters. Inducing sparsity by hyperpriors is an important feature of our approach. In fact, this permits to obtain the marginal posterior of the hyperparameters in closed form and hence also their estimates in a robust way. Once the kernels are selected, the impulse responses are obtained by a convex Tikhonov-type variational problem. Numerical experiments involving sparse ARMAX systems show that this ap-proach provides a definite advantage over both GLAR and PEM (equipped with AIC or BIC) in terms of predictive capability on new output data.
 The paper is organized as follows. In Section 2, the nonparametric approach to system identification introduced in [16] is briefly reviewed. Section 3 reports the statement of the predictor estimation problem while Section 4 describes the new Bayesian model for system identification of sparse linear systems. In Section 5, a numerical algorithm which returns the unknown components of the prior and the estimates of predictor and system impulse responses is derived. In Section 6 we use simu-lated data to demonstrate the effectiveness of the proposed approach. Conclusions end the paper. 2.1 Kernel-based regularization A widely used approach to reconstruct a function from indirect measurements { y t } consists of min-imizing a regularization functional in a reproducing kernel Hilbert space (RKHS) H associated with a symmetric and positive-definite kernel K [19]. Given N data points, least-squares regularization in H estimates the unknown function as where {  X  t } are linear and bounded functionals on H related to the measurement model while the positive scalar  X  trades off empirical error and solution smoothness [20].
 Under the stated assumptions and according to the representer theorem [21], the minimizer of (1) is the sum of N basis functions defined by the kernel filtered by the operators {  X  t } , with coeffi-cients obtainable solving a linear system of equations. Such solution enjoys also an interpretation in Bayesian terms. It corresponds to the minimum variance estimate of f when f is a zero-mean Gaussian process with autocovariance K and { y t  X   X  t [ f ] } is white Gaussian noise independent of f [22]. Often, prior knowledge is limited to the fact that the signal, and possibly some of its deriva-tives, are continuous with bounded energy. In this case, f is often modeled as the p -fold integral of Figure 1: Realizations of a stochastic process f with autocovariance proportional to the standard Cubic Spline kernel (left), the new Stable Spline kernel (middle) and its sampled version enriched by a parametric component defined by the poles  X  0 . 5  X  0 . 6 white noise. If the white noise has unit intensity, the autocorrelation of f is W p where W p ( s,t ) = splines [23]. In particular, when p = 2 , one obtains the cubic spline kernel. 2.2 Kernels for system identification In the system identification scenario, the main drawback of the kernel (2) is that it does not account for impulse response stability. In fact, the variance of f increases over time. This can be easily Gaussian process with autocovariance proportional to W 2 . One of the key contributions of [16] is the definition of a kernel specifically suited to linear system identification leading to an estimator with favorable bias and variance properties. In particular, it is easy to see that if the autocovariance of f is proportional to W p , the variance of f ( t ) is zero at t = 0 and tends to  X  as t increases. However, if f represents a stable impulse response, we would rather let it have a finite variance at t = 0 which goes exponentially to zero as t tends to  X  . This property can be ensured by considering autocovariances proportional to the class of kernels given by where  X  is a positive scalar governing the decay rate of the variance [16]. In practice,  X  will be unknown so that it is convenient to treat it as a hyperparameter to be estimated from data. In view of (3), if p = 2 the autocovariance becomes the Stable Spline kernel introduced in [16]: Proposition 1 [16] Let f be zero-mean Gaussian with autocovariance K 2 . Then, with probability one, the realizations of f are continuous impulse responses of BIBO stable dynamic systems. The effect of the stability constraint is visible in Fig. 1 (middle) which displays 100 realizations drawn from a zero-mean Gaussian process with autocovariance proportional to K 2 with  X  = 0 . 4 . y t  X  R and { u t } t  X  Z , u t  X  R m a pair of jointly stationary stochastic processes which represent, Figure 2: Bayesian network describing the new nonparametric model for identification of sparse respectively, the output and input of an unknown time-invariant dynamical system. With some sample value. The same holds for u t . Our aim is to identify a linear dynamical system of the form unknown system impulse responses while e t is the Gaussian innovation sequence.
 Following the Prediction Error Minimization framework, identification of the dynamical system (5) the predictor impulse response associated with the k -th input { u k t } t  X  Z , one has where h m +1 := { h m +1 t } t  X  N is the impulse response modeling the autoregressive component of the h k is (BIBO) stable. Under such assumption, our aim is to estimate the predictor impulse responses, in a scenario where the number of measurements N is not large, as compared with m , and many measured inputs could be irrelevant for the prediction of y t . We will focus on the identification of ARMAX models, so that the zeta-transforms of { h k } are rational functions all sharing the same denominator, even if the approach described below immediately extends to general linear systems. 4.1 Prior for predictor impulse responses We model { h k } as independent Gaussian processes whose kernels share the same hyperparameters apart from the scale factors. In particular, each h k is proportional to the convolution of a zero-mean Gaussian process, with autocovariance given by the sampled version of K 2 , with a parametric impulse response r , used to capture dynamics hardly represented by a smooth process, e.g. high-frequency oscillations. For instance, the zeta-transform R ( z ) of r can be parametrized as follows nent of the model, Fig. 1 (right panel) shows some realizations (with samples linearly interpolated) drawn from a discrete-time zero-mean normal process with autocovariance given by K 2 enriched by by enriching the Stable Spline kernel with the poles  X  0 . 5  X  0 . 6 The kernel of h k defined by K 2 and (7) is denoted by K : N  X  N 7 X  R and depends on  X , X  . Thus, letting E [  X  ] denote the expectation operator, the prior model on the impulse responses is given by 4.2 Hyperprior for the hyperparameters The noise variance  X  2 will always be estimated via a preliminary step using a low-bias ARX model, as described in [24]. Thus, this parameter will be assumed known in the description of our Bayesian model. The hyperparameters  X  ,  X  and {  X  k } are instead modeled as mutually independent random vectors.  X  is given a non informative probability density on R + while  X  has a uniform distribution on  X  . Each  X  k is an exponential random variable with inverse of the mean (and SD)  X   X  R + , i.e. with  X  the indicator function. We also interpret  X  as a random variable with a non informative prior on 4.3 The full Bayesian model Let A k  X  R N  X  X  where, for j = 1 ,...,N and i  X  N , we have: In view of (6), using notation of ordinary algebra to handle infinite-dimensional objects with each h k interpreted as an infinite-dimensional column vector, it holds that In practice, y -is never completely known and a solution is to set its unknown components to zero, see e.g. Section 3.2 in [1]. Further, the following approximation is exploited: i.e. the past y -is assumed not to carry information on the predictor impulse responses and the hyperparameters. Our stochastic model is described by the Bayesian network in Fig. 2 (left side). The dependence on y -is hereafter omitted as well as dependence of the { A k } on y + or u k . We start reporting a preliminary lemma, whose proof can be found in [17], which will be needed in propositions 2 and 3.
 stationary stochastic processes, each operator { A k } is almost surely (a.s.) continuous in H K . 5.1 Estimation of the hyper-parameters We estimate the hyperparameter vector  X  by optimizing its marginal posterior, i.e. the joint density that derives from simple manipulations of probability densities whose well-posedness is guaranteed by lemma 1. Below, I N is the N  X  N identity matrix while, with a slight abuse of notation, K is now seen as an element of R  X  X  X  X  , i.e. its i -th column is the sequence K (  X  ,i ) , i  X  N . Then, under the approximation (11), the maximum a posteriori estimate of  X  given y + is where J is almost surely well defined pointwise and given by The objective (13), including the ` 1 penalty on {  X  k } , is a Bayesian modified version of that con-nected with multiple kernel learning, see Section 3 in [25]. Additional terms are log det[ V [ y + ]] and log(  X  ) that permits to estimate the weight of the ` 1 norm jointly with the other hyperparameters. An important issue for the practical use of our numerical scheme is the availability of a good start-ing point for the optimizer. Below, we describe a scheme that achieves a suboptimal solution just solving an optimization problem in R 4 related to the reduced Bayesian model of Fig. 2 (right side). 5.2 Estimation of the predictor impulse responses for known  X  Let H K be the RKHS associated with K , with norm k X k H K . Let also  X  h k = E [ h k | y + , X  ] . The following result comes from the representer theorem whose applicability is guaranteed by lemma 1. Proposition 3 Under the same assumptions of Proposition 2, almost surely we have where k X k is the Euclidean norm. Moreover, almost surely we also have for k = 1 ,...,m + 1 After obtaining the estimates of the { h k } , simple formulas can then be used to derive the system impulse responses f and g in (5) and hence also the k -step ahead predictors, see [1] for details. We consider two Monte Carlo studies of 200 runs where at any run an ARMAX linear system with 15 inputs is generated as follows In the first Monte Carlo experiment, at any run an identification data set of size 500 and a test set experiment, the prediction on new data is more challenging. In fact, at any run, an identification data set of size 500 and a test set of size 1000 is generated via the MATLAB function idinput.m using, respectively, independent realizations of a random Gaussian signal with band [0 , 0 . 8] and [0 , 0 . 9] (the interval boundaries specify the lower and upper limits of the passband, expressed as fractions of the Nyquist frequency). We compare the following estimators: Figure 3: Boxplots of the values of COD 1 obtained by PEM+Or, Stable Spline, GLAR and PEM+BIC in the two experiments. The outliers obtained by PEM+BIC are not all displayed. The following performance indexes are considered: Figure 4: COD k , i.e. average coefficient of determination relative to k -step ahead prediction, ob-tained during the Monte Carlo study #1 (top) and #2 (bottom) using PEM+Oracle (  X  ), GLAR (  X  ) Stable Spline based on the full (  X  ) and the reduced ( + ) Bayesian model of Fig. 2. Notice that, in both of the cases, the larger the index, the better is the performance of the estimator. In every experiment the performance of PEM+BIC has been largely unsatisfactory, providing strongly negative values for COD k . This is illustrated e.g. in Fig. 3 showing the boxplots of the 200 values of COD 1 obtained by 4 of the employed estimators during the two Monte Carlo studies. We have also assessed that results do not improve using AIC. In view of this, in what follows other results from PEM+BIC will not be shown.
 Table 1 reports the percentage of the predictor impulse responses equal to zero correctly estimated as zero by the estimators. Remarkably, in all the cases the Stable Spline estimators not only outperform GLAR but the achieved percentage is close to 99 % . This shows that the use of the marginal posterior function of the prediction horizon obtained during the Monte Carlo study #1 (top) and #2 (bottom). The performance of Stable Spline appears superior than that of GLAR and is comparable with that of PEM+Oracle also when the reduced Bayesian model of Fig. 2 is used. We have shown how identification of large sparse dynamic systems can benefit from the flexibility of kernel methods. To this aim, we have extended a recently proposed nonparametric paradigm to identify sparse models via prediction error minimization. Predictor impulse responses are modeled as zero-mean Gaussian processes using stable spline kernels encoding the BIBO-stability constraint and sparsity is induced by exponential hyperpriors on their scale factors. The method compares much favorably with GLAR, with its performance close to that achievable combining PEM with an provide a theoretical analysis characterizing the hyperprior-based scheme as well as to design new ad hoc optimization schemes for hyperparameters estimation.
