 We describe a general result ranking approach for multi-phase, multi-strategy information systems, which has been applied to the task of question answering (QA). Many in-formation systems incorporate multiple steps and each step or phase may incorporate multiple component algorithms to achieve acceptable robustness and overall performance. Such systems may produce and rank a large number of can-didate results. Prior work includes many models that rank a particular type of information object (e.g. a retrieved doc-ument, a factoid answer) using features specific to that in-formation type, without attempting to make use of other non-local features (e.g. features of the upstream information source). We propose an approach that allows each phase in a system to leverage information propagated from preceding phases to inform the ranking decision. This is accomplished by a system object graph which represents all of the ob-jects created during system execution, object dependencies (e.g. provenance), and ranking feature values extracted for a specific object. We evaluate the effectiveness of the pro-posed ranking approach in a multi-phase question answering system built by recombining pre-existing software modules. Experimental results show that our proposed approach sig-nificantly outperforms comparable answer ranking models. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Question answering; answer ranking; learning to rank
Merging the output of multiple component algorithms to provide a single output in a processing phase is a fundamen-tal idea in computing, and is especially relevant for com-plex information processing tasks where no one approach or combination of approaches is likely to produce a satisfac-tory output for all inputs. This has been particularly rele-vant for the development of advanced Question Answering (QA) systems, which combine multiple knowledge sources from both structured databases and unstructured sources to validate the correctness of answer candidates [21], imple-ment multi-strategy approaches for answer extraction, rang-ing from simple factoid database lookup to complex machine learning-based named entity extraction [18, 8], or support a mixture of experts for question and content analytics [5, 4].
A multi-phase, multi-strategy system has the capacity to produce a significant number of both correct and incor-rect candidate results. Previous ranking approaches include: a) ranking via answer-specific features (answer validation and similarity) [11], b) logic reasoning by verifying seman-tic relationships between the question and its candidate an-swers [17], and c) heuristic-based ranking approaches [22, 13]. These approaches can be less effective when multiple strategies are combined in each phase, especially when there is a large variance in recall and precision across the combined strategies for any given input question. Localized ranking based on heuristic features is sub-optimal when some up-stream sources are much better sources of answer candidates, for some discernable subset of the inputs.

To address this problem, our proposed ranking model fo-cuses on correlating ranking features drawn from every phase in a pipelined system. Intuitively, an approach which at-tempts to leverage  X  X lobal X  information from every step in the answer generation pipeline has the potential to learn how to rank answers more effectively than an approach that is limited to just  X  X ocal X  features which are calculated over candidate answers. Our proposed approach uses a general form of feature propagation from prior phases when ranking candidate answers, and is not prone to the weakness de-scribed above. Specifically, a multi-phase, multi-strategy is represented by a system object graph, which offers a unified representation for propagation of ranking information, espe-cially useful when training ranking functions for each phase of the system. We hypothesize that our phased approach, using the system object graph to create new features and feature propagation for ranking, can improve answer accu-racy when compared to common answer ranking approaches, which typically ignore information from prior phases or de-pendencies among system objects (with the possible excep-tion of local context drawn from the answer bearing pas-are linearized and arranged by processing order. sage). In our ranking experiments, we validate our claims using multiple TREC data sets 1 .
The system object graph represents the dependences be-tween an input object, a component implementing the first phase in a solution, the output of that component, and so on. The special distinguishing feature of a system object graph is that all its nodes are arranged in sequential order from left to right. Let G ( V , E ) be a directed acyclic graph, where V are the nodes and E are the edges of the graph. Each node v  X  V belongs to a particular phase, and a di-rected edge e  X  E denotes the process order from phase to phase, or dependency from model object to result object. In this graph, we denote the input object and the result objects by shading the corresponding nodes to distinguish them from model object (component) nodes. A rectangle is used to capture the replication (multiple instatiation) of the model object (component) nodes, result object nodes, and their descendants when multiple strategies or components are present in a phase.

In Figure 1, we present a system object graph which de-scribes the general flow of a QA system. This pipelined architecture consists of sequential processing phases. The input object is a question Q. Each phase takes the outputs from the previous phase as input. The input will be pro-cessed though multiple modules per phase, in parallel, to generate a candidate set of results. This process ends at the T -th phase which generates the final outputs A . A phased ranking model may be used at the end of a phase to merge all the results generated by the different upstream modules.
Given a system object graph, we formalize the result rank-ing problem as that of producing a ranking on a collection of result objects of the final phase. By sequentially construct-ing the phases, we are able to rank the result objects at any phase. More formally, we address the following problem:
Since we used a QA system that is not state-of-the-art as the baseline system for the experiment, we measure signifi-cant improvement from that baseline, rather than absolute comparison to published TREC results. Put more simply, the goal of the paper is to evaluate ranking approaches, not QA systems.
 Given a T -phase system object graph and a collection of n result objects of the T -th phase, R T = { R T 1 ,...,R T n wish to find a ordering  X  of R T such that any k -length pre-fix of  X  ( R T ) is the best possible k results in the T -th phase. Therefore, we want to maximize result precision, where R Q T i is the i -th ranked result in the T -th phase S the question Q , and I is the indicator function.
As described in Section 2, the system object graph con-sists of a sequence of phases from S 1 to S T , where each phase S contains multiple model objects M t which produce a col-lection of result objects R t as output. In this way, it is easy to trace the derivation path of a result object at any phase, and those different result objects in the same phase can be compared by collecting evidence from their derivation paths. As well, those similar result objects produced in the same phase provide additional evidence, for the reason that rel-evant information may be presented in different ways and tend to be redundant [12]. To rank result objects, the above notions are captured by two steps. First, we apply a cascade model to rank result objects. This model receives the sys-tem object graph as input, extracts features for each phase, and combines those features linearly. Second, we learn a ranking model by jointly considering the cascade model and a voting model which scores a result object by consulting its similar neighbors. We now describe each aspect of the proposed phased model approach in more detail.
We describe a cascade model that uses features derived from each phase of the system object graph to produce a normalized score for comparing different result objects. Generally, the proposed model is a ranking model r : R d 7 X  R such that the relevance score of a set of result objects is specified by the real values that r takes, specifically, r ( R r ( R j ) is taken to mean that the model asserts that R i relevant than R j .

More specificially, the cascade model consists of a sequence of feature sets {  X  1 ,...,  X  T } , where each feature set  X  tracted from the t -th phase of the system object graph. The cascade model for a result object R T from the T -th phase therefore has the following form: where  X  t ( R T ) is a set of feature functions scoring R t -th phase, and  X  k is the weight vector to be learned.
The cascade model includes three kinds of features: a) features computed for a model in a current phase (Model Features), b) features computed for an output of the cur-rent phase (Result Features), and c) features which propa-gate information from preceding phases which can be useful for ranking a current phase model or result (Dependency Features). The most important innovation of the proposed model is the inclusion of Dependency Features, along with a means to automatically compute their weights from training data; these features make it possible to learn when perfor-mance of a module in the current phase depends on the nature of of the original input, a preceding model or a prior result object. When applied in a solution where multiple modules are applied in a given phase, these features make it possible to learn when one model X  X  output should be pre-ferred over that of another, given their derivation graphs and features extracted from them.
If a result object R t is produced at the t -th phase, its generation path can be represented as Q  X  M R t 1  X  R R t ...  X  M R t T  X  R t . Using this path, we can estimate the trustworthiness of a model, and also collect evidence for the correctness of a result object. These dependencies can be used to model the relationship between the generation path and the likelihood of a correct result.

Based on those intuitions, features are extracted from the three types of objects associated with each phase: the model objects, the result objects, and their dependencies. We com-bine the model object feature  X  M , the result object features  X 
R , and the dependency features  X  D into a single feature vector  X  t = [  X  M |  X  R |  X  D ] at each phase S t for the cascade model. Now we describe the details of features derived from the different types of objects in the generation path.
From the model perspective, features use evidence from the model objects in a phase to model the likelihood of a correct output. The model features are binary-valued fea-tures which indicate which model(s) a result object is gen-erated from. For example, given the TREC11 question 1496  X  X hat country is Berlin in? X , assume a Wikipedia-based search agent retrieves the passage  X 1936 Summer Olympics held in Berlin, Germany X , and a Twitter-based agent returns the passage  X  X cDonalds in Berlin sells McRib all year and serves beer! I thought America was the greatest country! X . In this case, we may expect a ranking model to learn that passages retrieved from Wikipedia are more highly weighted than passages retrieved from Twitter, when used as a source of answers for geographical location questions.
From the result perspective, features are derived from re-sult objects with prior knowledge, reflecting how likely a decision (or result object) in a phase can lead to a correct result. Two types of features can be extracted depending on whether the result objects are drawn from predefined cate-gories. If a result object is not drawn from predefined cate-gories, we represent a result object via suitable relevance fea-tures. For example, when retrieving documents from a web search engine, useful information about each document re-turned (bag-of-words, rank position) can be normalized and represented as a feature value. If a result object is drawn from predefined categories (e.g., detected answer types), we use a binary vector to indicate the category value(s).
Note that the features defined from both model and re-sult perspectives are based on the assumption that objects in the system object graph are conditionally independent, which is not strictly the case. For example, in a factoid QA system, the correct answer text is likely to appear in close proximity to at least some of the question keyterms. This dependency was modeled using web snippet search as an ev-idence gathering technique in prior work [12, 11], but this dependency is specific to QA and does not solve this type of dependency problem in a general way that applies to all information systems.

We propose several general variants of feature types for objects of different phases, each with different underlying dependency assumptions, as shown in Table 1. According to the different object types, length-2 dependency types are enumerated for seven relationship types. Those features can be extracted by the following four approaches.

Type I. Dependency features can be a binary valued vec-tor if both the objects within a dependency are drawn from predefined categories. One such example is the relationship M  X  M 0 , where the total number of features is | N M | X | N
Type II. Dependency features can be a real valued vec-tor if only one of the two objects is drawn from predefined categories. For example, result relevance score could be es-timated for objects R . In this case, the number of features for the relationship R  X  M should be | N M | .

Type III. We label result objects as  X  X orrect/incorrect X  by using binary classification with local features, if neither of the objects is drawn from predefined categories, or there are variant local features (mainly result object features) that we want to combine together. Using this type of classification model, a relationship of the third type is converted into a Type I or Type II relationship. The relationship R  X  R 0 could be one example (e.g., answer candidates ( R ) extracted from relevant passages ( R 0 ) may be more likely to be correct ones than those detected from irrelevant passages).

Type IV. Relationships which depend on an input ob-ject cannot be classified, as it is not appropriate to assign a  X  X orrect X  or  X  X ncorrect X  label to a primary input. Therefore, we represent an input object as a set of features (e.g. an input question represented as a bag-of-words model).
Previous work [10] shows that it is advantageous to esti-mate the probability of answers in a joint manner; however, the empirical results reported in [10] were limited to the case where only 10 answer candidates were considered per ques-tion (to limit complexity of the required computations over a joint graphical model). Such a constraint is too limiting for complex systems that produce thousands of candidate results. To overcome this limitation, we describe a rele-vance voting approach to model the relationship between results. The relevance voting approach is based on two hy-potheses. First, the probability of that a result is correct is dependent on the result X  X  relevance and the relevance of its similar neighbors. Second, the relevance derived from two text contexts for a result is independent (e.g., the fact that  X  X esuvius destroyed the ancient city of Pompeii X  can be found in many documents, but those documents are unlikely to be connected).

We assume the graph G consists of n result objects R T from the T -th phase. The relevance of all the results from r ( R t 1 ) to r ( R tn ) which are known variables modeled by Equation 2. We define the conditional probability over the random variables in G as P ( R ti | r ( R t 1 ) ,...,r ( R tn )) = 1 where n is the total number of the results produced in the t -th phase, Z = P n i =0 exp P n k =0 w ki r ( R tk ) normalizes the distribution, and w ki is the weight given from R tk to R indicating how much relevance from node R tk can be trans-ferred to node R ti in the current phase.

Result similarity functions can be defined to decide the weights ~w (Equation 3). For example, in the answer ranking stage, we define the weight of relevance transferred from A to itself as 1. If two answer candidates A i and A j represent different concepts, then they are mutually exclusive answer candidates and their transfer weights to each other should be 0. If we detect that they may represent the same concept or related concepts, their weights can be proportional to the similarity. Based on a result similarity function, we can replace w ki with Sim( R k ,R i ) in Equation 3. In this paper, we define the similarity function as where R k equals R i implies identical surface text. There are two advantages when using this similarity function: a) near-redundant information can provide stronger evidence when identifying the most likely result, and b) by merging redundant results, the final ranked result has higher value when there are multiple correct answers.
Given both the cascade model in Section 4.1 and the prop-agation model in Section 4.3, we now turn to the problem of answer ranking by integrating the two models.

The cascade model defined in Equation 2 is a linear model consisting of m features from the three perspectives: the system object graph features, the local result features, and the global result features. We rewrite the Equation 2 as where f j is the feature function extracted for ranking results and  X  i is the weight given to that particular feature function. Substituting this back into Equation 3, we have the following ranking function,
In P ( R ti | r ( R t 1 ) ,...,r ( R tn ))  X  Equation 6 suggests that instead of directly estimating rel-evance r t ( R t ), we can learn the ranking model by merging result objects through aggregating the features of similar results: for real valued features, we sum up the values; for binary features, we define this sum as the  X  X nd X  operator. In this way, we are able to jointly optimize result relevance and result similarity by using a learning to rank approach.
For the T -th phase, the result set R T is ranked according to the ranking score of each result, and the top ranked results are selected as the final output. In particular, during final answer selection, the top ranked answer candidate would be selected as the final answer (and final system output).
To compare the performance of our model with previous work on answer ranking, we built a baseline QA system using existing software (OpenEphyra 2 and some other available resources). In this section, we provide details of the sys-tem object graph derived from the baseline QA system, and how we applied the phased ranking model for final answer selection.
The system object graph for open domain factoid question answering contains three phases: question analysis ( S 1 ), pas-sage retrieval ( S 2 ), and answer extraction ( S 3 ); each phase incorporates multiple options (model nodes).

Question Analysis. This component identifies and clas-sifies the type of a given question among a set of predefined question types. The type of a question typically guides the search strategy at the phase of passage retrieval [2] or de-cides which types of named entities to extract at the phase of answer extraction [3]. To improve recall, one strategy
OpenEphyra, http://www.ephyra.info/ passage retrieval ( S 2 ), and answer extraction ( S 3 ). is to use multiple question analysis models at the phase of question analysis; we incorporated these strategies: 1. A model based question analyzer, as presented in [21]. 2. A rule-based question analyzer [21] that uses the same 3. A default question analyzer that uses a question X  X  in-
Passage Retrieval. In this phase, the question is trans-formed into a set of search queries, which are passed to the search agents to retrieve relevant passages from a collection of knowledge sources. We integrated the following multiple web and local search agents to retrieve passages: 1. The AQUAINT corpus originally used for the TREC
Indri, http://www.lemurproject.org/indri/ 2. An encyclopedia corpus, a Wikipedia 4 data dump (a 3. Two search agents were used to retrieve relevant short 4. Two local indexes were built by using documents re-5. Similar to web search engine based agents, we retrieved 6. The microblog website Twitter 7 Search API was con-
Answer Extraction. This component detects named entities as answer candidates after retrieving the results from knowledge sources. Named entities can be locations such as city, airport, and mountain; time and duration such as hour, month, day, and year; proper names of person, organization, music, etc. Eight extractors were integrated into our system to extract the answer candidates that match the detected question types. 1. List based NER (Named Entity Recognizer) was a part 2. Pattern based NER is also a part of OpenEphyra X  X  in-3. The OpenNLP 8 NER has a number of pre-trained 4. Stanford NER contains 3 class named entity recogniz-5. An item based NER and a proper name based NER
Wikipedia, http://www.wikipedia.org/
Bing Search, http://www.bing.com/
Yahoo! Answers, http://answers.yahoo.com/
Twitter, http://twitter.com/
OpenNLP, http://opennlp.apache.org/
In Section 4.2, our features are extracted from three per-spectives and consist of nine feature types (one for model object, one for result object and seven for object dependen-cies). We implemented features per phase by using those feature types. Feature details are summarized in Table 2.
For the result types, two features R 1 .type and R 1 .level are extracted in question analysis phase. The R 1 .type feature indicates which question type the original question was as-signed to. Since question types can be categorized by their levels of the question type hierarchy, the R 1 .level type has three binary features  X  X eneral X ,  X  X oarse X  and  X  X ine X  corre-sponding to the 3-level question hierarchy.
 For the dependency types with question object Q (e.g., M  X  Q ), we applied feature extraction method Type IV de-scribed in Section 4.2.3 to represent original input question Q with a bag-of-words model that contains question X  X  inter-rogative word, question focus [21], and four named entities (date, location, person and organization).

We also applied feature extraction method Type III de-scribed in Section 4.2.3 by employing logistic regression to classify result objects. There are two reasons. First, in the phase of passage retrieval, passage relevance scores are cal-culated by different methods (in our case, R 2  X  Q .count and R 2  X  Q .all). We want to combine different relevance scores to reduce the number of dependency features. Second, when exploring dependency R 3  X  R 2 under the hypothesis that an-swer candidate is dependent on passage, we want to examine whether  X  X orrect X  and  X  X ncorrect X  passages have different ef-fects on answer candidates. We used the following procedure to build a training set for each phase. First, we sample a set of training questions with answer keys for answer selection. Second, a single training question is processed by the QA system. If the result (passage or answer candidate) matches the answer key, we treat it as a positive training instance; otherwise, a negative training instance. To classify a passage R , local features, R 2 and R 2  X  Q are employed; to classify an answer candidate, local features, R 3 and R 3  X  Q , are uti-lized. The passage correctness score and answer correctness score are estimated probabilities of a  X  X orrect X  result.
We evaluated the phased ranking model on TREC QA datasets, and compared the performance of different answer ranking models. Specifically, we sought to answer the fol-lowing questions. First, what is the measurable impact of using the proposed method versus a traditional QA rank-ing approach with final answer ranking? Second, why does the fully implemented phased ranking model perform sig-nificantly better than the other models? To answer these research questions, we first compared the proposed phased ranking method with two answer ranking models, and then analyzed the effects of the proposed feature types.
The proposed method was evaluated on factoid questions from TREC 9 through TREC 12. The questions and corre-sponding answer keys can be obtained from the NIST web-site. We use the same experimental settings presented in [20], in that NIL questions without answer keys are removed. Often there is more than one acceptable answer for a ques-tion. Since our goal is to measure and compare the learning capacity of different models, we only use the answer keys found in the NIST website as the acceptable answers to train and test our models.
We compared the proposed phased ranking method with two answer ranking models. All of the methods used the same ranking framework (cascade model, relevance voting, parameters, learning algorithms, training and testing set, and the number of training instances) as the phased rank-ing model. The only difference is in the feature types em-ployed. The baseline is the independent prediction model (IPM) [11], an approach that optimizes both answer vali-dation and answer similarity. There are two reasons for us to choose this baseline. The first reason is that it employs answer validation, which is commonly used by previous QA systems in TREC QA task. Answer validation searches the web via different search agents to find relevant answer infor-mation. It is straightforward to implement answer validation by using our adapted search agents with the validity scoring method proposed by Magnini et al. [12]. The second reason is that previous successful QA systems [16] reported simi-lar answer ranking methods as IPM. We selected the IPM as a representative of the previous answer ranking methods, which relied on heuristic evidence features to score answer candidates without leveraging non-local information about upstream objects.

The second method for comparison is a simplified form of our phased ranking model, which we call the hypothesis se-lection model (HSM). This approach collects features from prior phases and makes them available for ranking in the current phase, by using only feature types M , R , and R  X  Q from Table 2. This approach represents a general form of IPM extended with shallow ranking features, such as those used in CHAUCER [8] (e.g., the strategy used to extract the answer, the question type of the original question, and similarity between the original question and an answer can-didate).

All the ranking models were trained by the ranking SVM method based on the cutting-plane method [9]. We applied the ranking models to each dataset separately, and 5-fold cross validation was used to evaluate the performance. In 5-fold cross-validation, we randomly partitioned the original questions into 5 equal size subsets. Among the subsets, a single subset was retained for testing the model, and the remaining 4 subsets were used as training data. The 5 tested subsets are finally combined together to provide an overall evaluation score.
 We evaluate our results using three measures: TOP@1, MRR@5, and Recall@10. Our TOP@1 measure is consis-tent with the QA accuracy presented in [20]: the number of top 1 correct answers divided by the total number of ques-tions. MRR is the average of mean reciprocal rank of the top 5 answers. Recall@10 is the QA accuracy in the top 10 positions (the score is 1 if at least one correct answer exists among the top 10 answers; otherwise, the score is 0).
Table 3 shows the end-to-end system performance for our proposed models and the baseline. Statistical significance was calculated using a one-sided sign test on questions. From the performance comparison, we notice two trends. First, HSM either significantly outperforms or is statistically indistinguishable from IPM in all cases. This highlights the advantage of the phased ranking approach: by propagating features from every phase of a system object graph to the final phase ranking, it allows for the use of richer features to form more complex ranking functions for higher accu-racy than extracting local features heuristically. Second, in all datasets, PhRank yields consistent and significant (p &lt; 0.001) improvements over the baseline IPM, and outper-forms HSM in terms of TOP@1, MRR@5 and Recall@10 in all cases. This illustrates that exploring system object graph dependencies between phases can significantly improve rank-ing performance.

We observed that performance numbers vary across differ-ent datasets. There are two reasons. First, most of the orig-inal development effort on OpenEphyra focused on TREC11 questions, and the components we used don X  X  have specific strategies to answer the large number of definition questions in the earlier TREC datasets (definition questions were not present in the TREC11 dataset). Second, the web search snippets retrieved for the TREC11 dataset have better qual-ity than those retrieved for the other datasets, for some yet undetermined reason.

We conducted a set of experiments to verify the effec-tiveness of propagating features from every phase to answer ranking. The results are summarized in Table 4, and the fea-tures grouped by phase are denoted as S 1 , S 2 and S 3 sepa-rately. It can be clearly observed that none of the individual set of features performed better than the combination of all the features. The features derived from the question anal-ysis phase performed the worst among all the three phases. This is reasonable because most of the answer candidates share the same question evidence, which provides the least information to distinguish the correct answers from a large number of noisy answer candidates. The features derived from the passage retrieval phase only will capture passage relevance (e.g., a keyword match between a passage and the original question), and therefore are not effective enough to train a satisfying model for answer ranking by themselves. Features derived from the answer extraction phase provided the best performance gain, indicating that direct evidence for answers (e.g., answer validation features, answer similar-ity features) provided the best source of ranking information.
To further study the effectiveness of the proposed features, we trained ranking models by leaving out features per phase. Based on results, we expect the PhRank approach to achieve the best results by combining features from all the phases. The results confirmed our hypothesis. None of the ranking models trained by leaving one feature type out is better than the model using all of the features. In the experiment, omit-ting S 3 features (no. S 3 ) leads to the most significant drop in performance. Although the features from the question anal-ysis phase ( S 1 ) were not effective when used in isolation, a ranking model that uses them in combination with the other features worked better than one leaving them.

The PhRank approach integrates nine types of feature ev-idence detailed in Table 2. We analyzed each feature set X  X  contribution and the results are shown in Table 5. The PhRank approach combining all features significantly out-performs the one with the best single-type features. Sur-prisingly, both the independent features M and R did not perform well. In contrast with their performance in isola-tion, they are the building blocks for the best performing dependency feature type R  X  M (+ M 0 ) (a combination of type R  X  M and type R  X  M 0 , for better illustration of the feature effectiveness).

In the feature ablation studies, we do a  X  X eave one feature type out X  analysis. Our motivation is to identify the key factors of the dependency features in PhRank. Results are presented in Table 6. We observed that omitting M  X  Q fea-tures leads to the most significant drop in performance. This illustrates that models developed in a system can strongly depend on features of the original question. One might have expected that as long as the question type is detected, the original question words are less important. But these re-sults show that original question provides additional vital evidence for ranking, since classified question types have al-ready been modeled in the M  X  R 0 relationship. Ablating the feature type R  X  M (+ M 0 ) also leads to a significant drop in performance. This implies that a result object and its preceding model object are strongly connected, and this relationship helps to inform the ranking decision. However, dropping feature type M leads to a performance improve-ment, suggesting that the dependency feature types can be an effective replacement. We also observed that our imple-mentation of R  X  R 0 (Type III in Section 4.2.3) does not works well, possibly because no new information can be ob-tained by training a classifier using the same training data. The comparative advantage of the other features is less pro-nounced than that of M  X  Q features, but they still con-tribute to the overall performance.

We wish to further understand when and why the datasets favor one approach over the other. Two concrete examples are shown to explain why HSM outperforms IPM, and how PhRank reinforces HSM in terms of answer accuracy. HSM clearly performs well at its intended purpose, learning a cas-cade model with three-level features extracted from each phase to rank the candidates. Consider the TREC11 ques-tion 1847  X  X hat is Tina Turner X  X  real name? X  In the phase of question analysis, the first model detects the expected answer type as  X  X erson X ; the second model generates an in-terrogative word  X  X hat X  strategy as an alternative. In the answer extraction phase, the former model may find the can-didate  X  X nna Mae Bullock X  (the correct answer) and the lat-ter may lead to an incomplete answer  X  X ullock X . IPM favors the incomplete answer because both of the candidates have similar text context and the candidate  X  X ullock X  has a higher redundancy than the other candidate  X  X nna Mae Bullock X . As opposed to the independent prediction model, HSM pro-gressively collects the supporting evidence from each phase, and hence it can more correctly estimate whether a candi-date answer is a correct one. By learning from the training data, HSM assigns a system object graph feature of 0.9 to the candidate  X  X nna Mae Bullock X  extracted by the first model, and assigns the same feature a value of 0.1 for the candidate  X  X ullock X  extracted by the second model.
In order to answer why PhRank outperforms HSM, we show an example in Figure 2 where dependency features are able to estimate supporting evidence scores of candidate an-swers more accurately. The candidates  X  X iam X  and  X  X humi-bol Adulyadej X  are derived from the component algorithms M a and M b separately. In this example, HSM only uses fea-tures f 1 and f 2 , while PhRank employs dependency features f to f 8 in addition to f 1 and f 2 . Because M b derives cor-rect answers more often than M a , the ranking approach gives higher confidence score to M b ( f 1 &lt; f 2 ), leading to the in-correct answer for HSM. By adding dependency features, we Figure 2: Example of the proposed dependency fea-tures M 1  X  Q in PhRank for scoring two candidates  X  X iam X  and  X  X humibol Adulyadej X  to answer the TREC11 question 1828  X  X hat was Thailand X  X  origi-nal name? X  In this example, the aggregated evidence scores for  X  X iam X  and  X  X humibol Adulyadej X  are 0.458 and -0.656 separately, suggesting that  X  X iam X  is more likely to be the correct answer. can see that if a named entity of  X  LOC X  ( X  X hailand X  in this case) appears in the question, component M a is preferred than component M b to derive the candidate answers. This observation explains that those dependency features form more complex ranking functions and can provide higher an-swer accuracy than HSM.
Answer ranking models can be broadly categorized into three types: heuristic based, logical reasoning based, and machine learning based models. Early answer ranking mod-els mainly adopted heuristic based methods. Those methods attempted to verify answer correctness according to vari-ous methods, ranging from simple rules that eliminate obvi-ously wrong answers [13], to more complex answer validation methods which measure the redundancy of Web information [12]. Logical reasoning based methods focus on deep under-standing of the relationship between the question and an-swer text. For example, Moldovan et al. [17] proposed an approach that transformed questions and answer passages into logic representations based on syntactic, semantic and contextual information. With heuristic based methods, an-swer ranking models are typically tailored especially for spe-cific system components, and are not effective as a general answer ranking solution. Logical reasoning based methods require significant human effort to implement, and are not easy to generalize and tune to new domains.

More recent work focuses on machine learning based algo-rithms. Several methods provide answer ranking frameworks which estimate the probability that an answer is correct given the question. Ittycheriah et al. [14] modeled the dis-tribution of the correctness of an answer and the question by introducing a hidden variable representing the answer type. Ravichandran et al. [19] compared classifier with re-ranker by using maximum entropy approaches with a set of answer features (frequency, answer type, question word absent and word match). Ko et al. [11] combined together evidence in-cluding answer validation scores from multiple sources and different answer similarity measures. Logistic regression was learned from training data to predict whether an answer can-didate was correct or not. An undirected graphical model was developed later by Ko et al. [10] to estimate the joint probability of the correctness of all answer candidates, given the pairwise similarity between each pair of answer candi-dates and the validity score for each candidate. The most recent direction is a learning to rank framework that ex-ploits rank preferences induced from the data. Watson [7, 1] proposed a cascading ranking method where the rank-ing produced by one ranker is used as input the next stage. Most of the features are measures of verifying answers for the question based on passages or knowledge base information.
Our proposed method follows the learning to rank frame-work. A key difference from previous work is that the pro-posed method concentrates on linking result ranking to the system object graph defined in Section 2. In this way, depen-dencies of system objects (input, models, and results) can be explored exhaustively for result ranking, for any information processing pipeline.
In this paper we proposed a general result ranking ap-proach for multi-phase, multi-strategy information process-ing system, and evaluating its effectiveness by comparing to baseline approaches using an existing QA system as a testbed. Our research contributes several meaningful re-sults. First, most prior work in answer ranking has stud-ied features related to question-answer pairs for ranking an-swers. The use of features in a general form that propagates from every phase has not been throughly studied, although Watson [7] incorporates one specific way to cascade rank-ing information in a particular information system. Our ranking approach, which consists of both a cascade model and a result voting model, allows us to effectively learn a final answer ranking model to combine rich features of dif-ferent types (more than 100,000 features). Second, most prior work on question answering has focused on extracting features heuristically. None of the previous work formal-ized the problem of extracting features from a general set of system object graph dependencies. Our results show that significant improvement can be achieved by exploring the non-local relationships among system objects and the mod-els that produce them. We are working to apply the pro-posed ranking method to other QA domains and other NLP applications which incorporate multiple phases and multiple strategies for each phase. [1] A. Agarwal, H. Raghavan, K. Subbian, P. Melville, [2] M. W. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. [3] A. Celikyilmaz, M. Thint, and Z. Huang. A [4] J. Chu-Carroll, K. Czuba, J. Prager, and [5] D. A. Ferrucci, E. W. Brown, J. Chu-Carroll, J. Fan, [6] J. R. Finkel, T. Grenager, and C. Manning.
 [7] D. Gondek, A. Lally, A. Kalyanpur, J. W. Murdock, [8] A. Hickl, K. Roberts, B. Rink, J. Bensley, T. Jungen, [9] T. Joachims. Advances in kernel methods. chapter [10] J. Ko, E. Nyberg, and L. Si. A probabilistic graphical [11] J. Ko, L. Si, and E. Nyberg. A probabilistic [12] B. Magnini, M. Negri, R. Prevete, and H. Tanev. Is it [13] A. F. Majid Razmara and L. Kosseim. Concordia [14] A. I. Martin, M. Franz, and S. Roukos. Ibm X  X  [15] D. Metzler and W. B. Croft. A markov random field [16] D. Moldovan, C. Clark, and M. Bowden. Lymba X  X  [17] D. I. Moldovan, C. Clark, S. M. Harabagiu, and [18] E. Nyberg, R. E. Frederking, T. Mitamura, M. W. [19] D. Ravichandran, E. Hovy, and F. J. Och. Statistical [20] N. Schlaefer, J. Chu-Carroll, E. Nyberg, J. Fan, [21] N. Schlaefer, J. Ko, J. Betteridge, G. Sautter, [22] H. Yang and T.-S. Chua. The integration of lexical prediction model.
 Table 4: Feature analysis per phase: question analysis ( S 1 ), passage retrieval ( S 2 ), and answer extraction ( S 3 ), evaluated on TREC11.
 no. S 1 no. S 2 no. S 3 All no. S 1 no. S 2 no. S 3 All no. S 1 no. S 2 no. S 3 All 0 R  X  Q R  X  M (+ M 0 ) R  X  R 0 All 0 R  X  Q R  X  M (+ M 0 ) R  X  R 0 All 0 R  X  Q R  X  M (+ M 0 ) R  X  R 0 All
