 We present a Bayesian tensor factorization model for infer-ring latent group structures from dynamic pairwise interac-tion patterns. For decades, political scientists have collected and analyzed records of the form  X  X ountry i took action a toward country j at time t  X  X  X nown as dyadic events  X  X n order to form and test theories of international relations. We represent these event data as a tensor of counts and develop Bayesian Poisson tensor factorization to infer a low-dimensional, interpretable representation of their salient pat-terns. We demonstrate that our model X  X  predictive perfor-mance is better than that of standard non-negative tensor factorization methods. We also provide a comparison of our variational updates to their maximum likelihood counter-parts. In doing so, we identify a better way to form point estimates of the latent factors than that typically used in Bayesian Poisson matrix factorization. Finally, we showcase our model as an exploratory analysis tool for political sci-entists. We show that the inferred latent factor matrices capture interpretable multilateral relations that both con-form to and inform our knowledge of international affairs. I.5.1 [ Pattern Recognition ]: Models; J.4 [ Computer Ap-plications ]: Social and Behavioral Sciences Algorithms, Experimentation Poisson tensor factorization, Bayesian inference, dyadic data, international relations
Social processes are characterized by pairwise connections between actors, such as people, organizations, corporations, and countries. In some social processes, actors declare their connections and researchers can directly study them X  X .g., friendships on Facebook or co-authorships in academia. In other processes, however, these connections are not explic-itly declared. Rather, they are evidenced over time via dy-namic interaction patterns. Inferring social processes from such implicit data is a challenging and important task. This task is especially motivated in international relations. For decades, scholars have collected and analyzed records of pairwise interactions between countries of the form  X  X ountry i took action a toward country j at time t , X  known as dyadic events . These data sets, e.g., [28], which are traditionally small and well-curated, help them form and test theories of international relations, which often concern the multilateral behavior of groups of countries. Recently, there has been new interest in studying less structured, larger scale sources of pairwise interaction data. Researchers have created sev-eral large data sets, e.g., [20], by automatically extracting and encoding dyadic events from Internet news archives.
These modern data sets differ substantially from their smaller counterparts, which previously dominated the field. Rather than documenting high-level, aggregate behaviors, such as formal wars and military alliances, they document micro-level behaviors at a day-to-day granularity. Although this new view of the world potentially paints a more accurate and nuanced picture of international relations, these data are too noisy and disaggregated to analyze effectively using tra-ditional techniques. We need new methods to uncover the latent multilateral relations that underlie these events.
In this paper, we introduce Bayesian Poisson tensor fac-torization (BPTF) for inferring latent multilateral relations from observed dyadic events. We present a scalable varia-tional inference algorithm and demonstrate our method, via both predictive and exploratory analyses, on large-scale in-ternational relations data. Figure 1 illustrates our approach; our model infers both ongoing multilateral relations, such as the Six-Party Talks from 2003 through 2009 (left), as well as relations precipitated by temporally localized anomalous activity, such as the September 11, 2001 attacks (right). Technical summary: A data set of dyadic events can be represented as a four-way tensor by aggregating (i.e., count-ing) events within discrete time steps. Each element of the tensor is a count of the number of actions of type a taken by country i toward country j at time t . Our model decomposes such a tensor into a set of latent factor matrices that provide a low-dimensional representation of the salient patterns in the counts X  X n this case, latent multilateral relations.
Tensors derived from dyadic event data are often very sparse since most countries rarely interact with one another. Additionally, the non-zero counts, for countries that do in-teract, are highly dispersed X  X .e., their mean is greatly ex-ceeded by their variance. Traditional tensor factorization methods, involving maximum likelihood estimation, are un-stable when fit to sparse count tensors [2]. Bayesian Pois-son tensor factorization (section 3) builds on previous work on Bayesian Poisson matrix factorization with Gamma pri-ors [1, 21, 11, 38, 25, 10] to avoid these instabilities. We val-idate our model by comparing its out-of-sample predictive performance to non-Bayesian tensor factorization methods (section 5); BPTF significantly outperforms other models when decomposing sparse, highly dispersed count data.
We present an efficient variational inference algorithm to fit BPTF to data (section 4) and outline the relationship between our algorithm and the traditional maximum likeli-hood approach (section 7). This relationship explains why BPTF outperforms other methods without any sacrifice to efficiency. It also suggests that when constructing point esti-mates of the latent factors from the variational distribution, researchers should use the geometric expectation instead of the arithmetic expectation commonly used in Bayesian Pois-son matrix factorization. We show that using the geometric expectation increases the sparsity of the inferred factors and improves predictive performance. We therefore recommend its use in any subsequent work involving variational infer-ence for Bayesian Poisson matrix or tensor factorization.
Finally, we showcase Bayesian Poisson tensor factoriza-tion as an exploratory analysis tool for political scientists (section 6). We demonstrate that the inferred latent fac-tor matrices capture interpretable multilateral relations that conform to and inform our knowledge of international affairs.
Over the past few years, researchers have created large data sets of dyadic events by automatically extracting them from Internet news archives. The largest of these data sets is the Global Database of Events, Location, and Tone (GDELT), introduced in 2013, which contains over a quarter of a bil-lion events from 1979 to the present, and is updated with new events daily [20]. In parallel, government agencies (e.g., DARPA) and their contractors have also started to collect and analyze dyadic events, in order to forecast political in-stability and to develop early-warning systems [24]; Lock-heed Martin publicly released the Integrated Crisis Early Warning System (ICEWS) database in early 2015. Ward et al. provide a comparison of GDELT and ICEWS [31].
 GDELT and ICEWS use the CAMEO coding scheme [6].
 A CAMEO-coded dyadic event consists of four pieces of in-formation: a sender, a receiver, an action type, and a times-tamp. An example of such an event (top) and a sentence from which it could have been extracted (bottom) is Dec. 25, 2014:  X  X urkish jets bombed targets in Syria. X  CAMEO assumes that senders and receivers belong to a sin-gle set of actors, coded for their country of origin and sector (e.g., government or civilian) as well as other information (such as religion or ethnicity). CAMEO also assumes a hier-archy of action types, with the top level consisting of twenty basic action classes. These classes are loosely ranked based on sentiment from Make Public Statement to Use Uncon-ventional Mass Violence . Each action class is subdivided into more specific actions; for example, Make Public State-ment contains Make Empathetic Comment . When study-ing international relations using CAMEO-coded data, re-searchers commonly consider only the countries of origin as actors and only the twenty basic action classes as action types. In ICEWS, there are 249 unique country-of-origin actors (which include non-universally recognized countries, such as Taiwan and Palestine); in GDELT, there are 223. Dyadic events as tensors: A data set of dyadic events can be aggregated into a four-way tensor Y of size N  X  N  X  A  X  T , where N is the number of country actors and A is the number of action types, by aggregating the events into T time steps on the basis of their timestamps. Each element y ijat of Y is a count of the number of actions of type a taken by country i toward country j during time step t . As described in section 6, we experimented with various date ranges and time step granularities. For example, in one set of experiments, we used the entire ICEWS data set, spanning 1995 through 2012 (i.e., 18 years) with monthly time steps X  i.e., a 249  X  249  X  20  X  216 tensor with 267,844,320 elements. Tensors derived from ICEWS and GDELT are very sparse. For the 249  X  249  X  20  X  216 ICEWS tensor described above, only 0.54% of the elements (roughly 1.5 million elements) are non-zero. Moreover, these non-zero counts are highly dispersed with a variance-to-mean ratio (VMR) of 57. Any realistic model of such data must therefore be robust to spar-sity and capable of representing high levels of dispersion.
Tensor factorization methods decompose an observed M -way tensor Y into M latent factor matrices  X  (1) ,...,  X  ( M ) that provide a low-dimensional representation of the salient patterns in Y . There are many different tensor factorization methods; the two most common methods are the Tucker de-composition [30] and the Canonical Polyadic (CP) decom-position [13]. These methods can both be viewed as tensor generalizations of singular value decomposition. Here, we fo-cus on the CP decomposition, as it performs better than the Tucker decomposition when modeling sparse count data [18]. For a four-way count tensor Y of size N  X  N  X  A  X  T , the CP decomposition treats each observed count y ijat as for i,j  X  [ N ], a  X  [ A ], and t  X  [ T ], where  X  (1) ik and  X  (4) tk are known as factors ,  X  y ijat is known as the recon-struction of count y ijat , and  X  Y is the reconstruction of the entire tensor Y . The set of all factors used to model Y can be aggregated into four latent factor matrices ; for ex-each factor matrix has K columns, a single index k  X  [ K ] indexes four columns (one per matrix). These columns are collectively known as a component ; for example, component i.e., a length-N vector of sender factors, a length-N vector of receiver factors, a length-A vector of action-type factors, and a length-T vector of time-step factors. Figure 1 visually depicts two components inferred from ICEWS and GDELT.
When viewed from a probabilistic perspective, the recon-the mean of the distribution from which the observed count y ijat is assumed to have been drawn. If this distribution is a Poisson X  X .e., if y ijat  X  Pois( y ijat ;  X  y ijat ) X  X hen the process of decomposing Y into its latent factor matrices is known as Poisson tensor factorization (PTF), and can be performed via maximum likelihood estimation (MLE) of  X  (1) ,  X   X  (3) , and  X  (4) . For sparse count data, PTF often yields better estimates of the latent factor matrices than those ob-tained by assuming each count to have been drawn from a Gaussian distribution X  X .e., y ijat  X  X  ( y ijat ;  X  y ijat
In this paper, we also assume that each observed count y ijat is drawn from a Poisson distribution with mean  X  y ijat however, rather than obtaining point estimates of the factor matrices using maximum likelihood estimation, we impose prior distributions on the latent factors and perform full Bayesian inference. Bayesian inference for Poisson matrix factorization (PMF) was originally proposed by Cemgil [1] and has been successfully used for several tasks including image reconstruction [1], music tagging [21], topic model-ing [25], content recommendation [11], and community de-tection [9]; here, we generalize Bayesian PMF to tensors. Since the Gamma distribution is the conjugate prior for a Poisson likelihood, Bayesian PMF typically imposes Gamma priors on the latent factors [1, 11, 39]. The Gamma distri-bution, which has support on (0 ,  X  ), is parameterized by a shape parameter a &gt; 0 and a rate parameter b &gt; 0; if  X   X  Gamma (  X  ; a,b ), then E [  X  ] = a b and Var[  X  ] = a when a 1 and b is small, the Gamma distribution concen-trates most of its mass near zero yet maintains a heavy tail and can therefore be used as a sparsity-inducing prior [1, 11]. We show the effects of different a and b values in figure 2.
To define Bayesian Poisson tensor factorization (BPTF) for a four-way tensor, we impose four sparsity-inducing Gamma priors over the latent factors. For a single factor, e.g.,  X 
Beyza and Cemgil [5] described the same model in a paper written concurrently to a previous version [27] of this paper. terization of the Gamma distribution, where the rate pa-rameter is the product of the shape parameter and  X  (1) , the mean of the prior is completely determined by  X  (1) E data [1, 21]. The shape parameter  X  , which determines the sparsity of the latent factor matrices, can be set by the user. Throughout our experiments, we use  X  = 0 . 1 to encourage sparsity and hence promote interpretability of the factors.
Given an observed tensor Y , Bayesian inference of the latent factors involves  X  X nverting X  the generative process de-scribed in the previous section to obtain the posterior dis-tribution of the latent factor matrices conditioned on Y and the model hyperparameters H X {  X , X  (1) , X  (2) , X  (3) , X  (4)
The posterior distribution for BPTF is analytically in-tractable and must be approximated. Variational inference turns the process of approximating the posterior distribution into an optimization algorithm. It involves first specifying a parametric family of distributions Q over the latent vari-ables of interest, indexed by the values of a set of variational parameters S . The functional form of Q is typically chosen so as to facilitate efficient optimization of S . Here, we use a fully factorized mean-field approximation and define Q to be the product of N  X  N  X  A  X  T  X  K independent Gamma distributions X  X ne for each latent factor X  X .g., for  X  (1) ik parameters is thus S  X {S (1) , S (2) , S (3) , S (4) } . This form of Q is similar to that used in Bayesian PMF [1, 25, 11].
The variational parameters are then fit so as to yield the closest member of Q to the exact posterior X  X nown as the variational distribution . Specifically, the algorithm sets the values of S to those that minimize the KL divergence of the exact posterior from Q . It can be shown that these values are the same as those that maximize a lower bound on P ( Y |H ), known as the evidence lower bound (ELBO): B ( S ) = E Q h log P ( Y ,  X  (1) ,  X  (2) ,  X  (3) ,  X  (4) where H ( Q ) is the entropy of Q . When Q is a fully factorized approximation, finding values of S that maximize the ELBO can be achieved by performing coordinate ascent, iteratively updating each variational parameter, while holding the oth-ers fixed, until convergence (defined by relative change in the ELBO). The update equation for each parameter can be derived easily using an auxiliary variable as shown for Bayesian PMF [1, 25, 11]; we therefore omit derivations.
For parameters  X  (1) ik and  X  (1) ik , the update equations are where E Q [  X  ] and G Q [  X  ] = exp ( E Q [log (  X  )]) denote arithmetic and geometric expectations. Since Q is fully factorized, each expectation of a product can be factorized into a product of individual expectations, which, e.g., for  X  (1) ik are where  X (  X  ) is the digamma function. Each expectation X  a sufficient statistic X  X an be cached to improve efficiency. Note that the summand in (4) need only be computed for those values of j , a , and t for which y ijat &gt; 0; provided Y is very sparse, inference is efficient even for very large tensors.
The hyperparameters  X  (1) ,  X  (2) ,  X  (3) , and  X  (4) can be op-timized via an empirical Bayes method, in which each hyper-parameter is iteratively updated along with the variational parameters according to the following update equation:
Update equations (4), (5), and (7) completely specify the variational inference algorithm for BPTF. Our Python im-plementation, which is intended to support arbitrary M -way tensors in addition to the four-way tensors described in this paper, is available for use under an open source license 2
We validated our model by comparing its predictive per-formance to that of standard methods for non-negative ten-sor factorization involving maximum likelihood estimation. Baselines: Non-Bayesian methods for CP decomposition find values of the latent factor matrices that minimize some cost function of the observed tensor Y and its reconstruc-tion  X  Y . Researchers have proposed many cost functions, but most often use Euclidean distance or generalized KL divergence, preferring the latter when the observed tensor consists of sparse counts. Generalized KL divergence is on the observed data only. The standard method for esti-mating the values of the latent factors involves multiplicative update equations, originally introduced for matrix factoriza-tion by Lee and Seung [19] and later generalized to tensors by Welling and Weber [32]. The multiplicative nature of these update equations acts as a non-negativity constraint on the factors which promotes interpretability and gives the algo-rithm its name: non-negative tensor factorization (NTF).
Some cost functions also permit a probabilistic interpreta-tion: finding values of the latent factors that minimize them is equivalent to maximum likelihood estimation of a prob-abilistic model. The log likelihood function of a Poisson tensor factorization model X  y ijat  X  Pois( y ijat ;  X  y ijat https://github.com/aschein/bptf where constant C  X  P i,j,a,t  X  log ( y ijat !) depends on the observed data only. Since equation (8) is equal to the nega-tive of equation (10) up to a constant, maximum likelihood estimation for Poisson tensor factorization is equivalent to minimizing the generalized KL divergence of  X  Y from Y .
To validate our modeling assumptions, we compared the out-of-sample predictive performance of BPTF to that of non-negative tensor factorization with Euclidean distance (NTF-LS) and non-negative tensor factorization with gen-eralized KL divergence (NTF-KL or, equivalently PTF). Experimental design: Using both ICEWS and GDELT, we explored how well each model generalizes to out-of-sample data with varying degrees of sparsity and dispersion. For each data set X  X CEWS or GDELT X  X e sorted the country actors by their overall activity (as both sender and receiver) so that the N  X  N sender X  X eceiver slices of the observed tensor were denser toward the upper-left corner. Figure 3 depicts this property. We then divided the observed tensor into a training set and a test set by randomly constructing an 80% X 20% split of the time steps. We defined training set Y train to be the N  X  N  X  A slices of Y indexed by the time steps in the 80% split and defined test set Y test to be the N  X  N  X  A slices indexed by the time steps in the 20% split.
We compared the models X  predictive performance in two scenarios, intended to test their abilities to handle different levels of sparsity and dispersion: one in which we treated the denser upper-left N 0  X  N 0 (for some N 0 &lt; N ) portion of each test slice as observed at test time and predicted its complement, and one in which we observed the complement at test time and predicted the denser N 0  X  N 0 portion.
In each setting, we used an experimental strategy analo-gous to strong generalization for collaborative filtering [23]. During training, we fit each model to the fully observed training set. We then fixed the values of the variational parameters for the sender, receiver, and action-type factor matrices (or direct point estimates of the factors, for the non-Bayesian models) to those inferred from the training set. For each test slice, indexed by time step t , we used the observed upper-left N 0  X  N 0 portion (or its complement) to infer variational parameters for (or direct point estimates of) its time-step factors {  X  (4) tk } K k =1 . Finally, we reconstructed the missing portion of each test slice using equation (1). For the reconstruction step, we can obtain point estimates of the la-tent factors by taking their arithmetic expectations or their geometric expectations under the variational distribution. In this section, we report results obtained using geometric expectations only; we explain this choice in section 7.
The time-step factors inferred from the observed portion of a given test slice capture the extent to which the sender, receiver, and action-type factors for each component inferred from the training set describe (the observed portion of) that slice. For example, if component k summarizes the Israeli X  Palestinian conflict, with Israel and Palestine as top actors and Fight as a top action type, then if Israeli X  X alestinian hostilities were intense during test time step t and if Israel and Palestine belong to the observed portion of each test slice, the inferred value of  X  (4) tk is very likely to be large.
We used the entire ICEWS data set from 1995 through 2012 (i.e., 18 years), with events aggregated into monthly time steps. The resultant tensor was of size 249  X  249  X  20  X  216. Since GDELT covers a larger date range (1979 to the present) than ICEWS, we therefore selected an 18-year subset of GDELT spanning 1990 through 2007, and aggregated events into monthly time steps to yield a tensor of size 223  X  223  X  20  X  216. Since we are interested in interactions between countries, we omitted self-actions so that the diagonal of each N  X  N sender X  X eceiver slice was zero. Ranking the country actors by their overall activity (as both sender and receiver), the top four actors in the ICEWS tensor are USA, Russia, China, and Israel, while the top four actors in the GDELT tensor are USA, Russia, Israel, and Iraq. The GDELT tensor contains many more events than the ICEWS tensor (26 million events versus six million events). It is also much denser (1.6% of the elements are non-zero, as opposed to 0.54%) and exhibits a much higher level of dispersion (VMR of 100, as opposed to 57). Summary of results: The out-of-sample predictive per-formance of each model is shown in table 1. We experi-mented with several different values of K and found that all three models were insensitive to its value; we therefore report only those results obtained using K = 50. We computed three types of error: mean absolute error (MAE), mean absolute error on only non-zero elements (MAE-NZ), and Hamming loss on only the zero elements (HAM-Z). HAM-Z corresponds to the fraction of true zeros in the unobserved portion of the test set (i.e., elements for which y whose reconstructions were (incorrectly) predicted as being greater than 0.5. For each data set, we generated three training X  X est splits, and averaged the error scores for each model across them. For each experiment included in ta-ble 1, we display the density and dispersion of the corre-sponding test set. When we treated the dense upper-left N 0  X  N 0 portion as observed at test time (and predicted its complement), all models performed comparably. In this sce-nario, NTF-LS consistently achieved the lowest MAE score and the lowest HAM-Z score, but not the lowest MAE-NZ score. This pattern suggests that NTF-LS overfits the spar-sity of the training set: when the unobserved portion of the test set is much sparser than the training set (as it is in this scenario), NTF-LS achieves lower error scores by sim-ply predicting many more zeros than NTF-KL (i.e., PTF) or BPTF. In the opposite scenario, when we observed the complement at test time and predicted the denser N 0  X  N 0 portion, NTF-LS produced significantly worse predictions than the other models, and our model (BPTF) achieved the lowest MAE, MAE-NZ, and HAM-Z scores X  X n some cases by an order of magnitude over NTF-KL. These results sug-gest that in the presence of sparsity, BPTF is a much better model for the  X  X nteresting X  portion of the tensor X  X .e., the dense non-zero portion. This observation is consistent with previous work by Chi and Kolda which demonstrated that NTF can be unstable, particularly when the observed tensor is very sparse [2]. In section 7, we provide a detailed dis-cussion comparing NTF and BPTF, and explain why BPTF overcomes the sparsity-related issues often suffered by NTF.
In this section, we focus on the interpretability of the latent components inferred using our model. (Recall that each latent factor matrix has K columns; a single index k  X  [ K ] indexes a column in each matrix X   X  (1) ik N i =1 ,  X   X  nent.) We used our model to explore data from GDELT and ICEWS with several date ranges and time step granularities, including the 18-year, monthly-time-step tensors described in the previous section (treated here as fully observed).
When inferring factor matrices from data that span a large date range (e.g., 18 years), we expect that the inferred com-ponents will correspond to multilateral relations that per-sist or recur over time. Figure 1 shows two such compo-nents, inferred from the 18-year GDELT and ICEWS ten-sors. The first component corresponds to ongoing negotia-tions over North Korea X  X  nuclear program, while the second corresponds to a decade-long war (though precipitated by a sudden anomalous event). We found that many the com-ponents inferred from 18-year tensors summarize regional relations X  X .e., multilateral relations that persist due to ge-ographic proximity X  X imilar to those found by Hoff [15].
We found a high correspondence between the regional components inferred from GDELT and the regional compo-nents inferred from ICEWS, despite the five-year difference in their date ranges. Figure 4 illustrates this correspondence. We also found that components summarizing regional rela-tions exhibited the least sparsity in their sender, receiver, and time-step factors. For example, the component depicted in figure 4 has near-uniform values for the top ten sender and receiver actors (all of whom are regional to Central Asia), while the time-step factors possess high activity throughout. In contrast, the time-step factors for the component shown in the second plot of figure 1 (i.e., the War on Terror) exhibit a major spike in October 2001. This component X  X  sender and receiver factors also exhibit uneven activity over the top ten actors, with the US, Afghanistan, and Pakistan dominating.
These  X  X egional relations X  components conform to our un-derstanding of international affairs and foster confidence in BPTF as an exploratory analysis tool. However, for the same reason, they are also less interesting. To explore tem-porally localized multilateral relations X  X .e., anomalous in-teraction patterns that do not simply reflect usual activity X  we used our model to infer components from several subsets of GDELT and ICEWS, each spanning a two-year date range with weekly time steps. We ranked the inferred components by the sparsity of their time-step factors, measured using the Gini coefficient [3]. Ranking components by their Gini coefficients is a form of anomaly detection : components with high Gini coefficients have unequal time-step factor values X  i.e., dramatic spikes. Figure 6 shows the highest-ranked (i.e., most anomalous) component inferred from a subset of GDELT spanning 2011 X 2012. This component features an unusual group of top actors and a clear burst of activity around June 2012. To interpret this component, we per-formed a web search for ecuador UK sweden june 2012 and found that the top hit was a Wikipedia page [34] about Ju-lian Assange, the editor-in-chief of the website WikiLeaks X  an Australian national, wanted by the US and Sweden, who sought political asylum at the Ecuadorian embassy in the UK during June through August 2012. These countries are indeed the top actors for this component, while the time-step factors and top action types (i.e., Consult , Aid , and Appeal ) track the dates and nature of the reported events.
In general, we found that when our existing knowledge was insufficient to interpret an inferred component, performing a web search for the top two-to-four actors along with the top time step resulted in either a Wikipedia page or a news article that provided an explanation. We present further examples of the most anomalous components inferred from other two-year date ranges in figure 5, along with the web searches that we performed in order to interpret them.
Previous work on Bayesian Poisson matrix factorization (e.g., [1, 25, 11]) presented update equations for the varia-tional parameters in terms of auxiliary variables, known as latent sources , and made no explicit reference to geometric expectations. In contrast, we write the update equations for Bayesian Poisson tensor factorization in the form of equa-tions (4) and (5) in order to highlight their relationship to Lee and Seung X  X  multiplicative updates for non-negative ten-sor factorization X  X  parallel also drawn by Cemgil in his paper introducing Bayesian PMF [1] X  X nd to show that our update equations suggest a new way of making out-of-sample predictions when using BPTF. In this section, we provide a discussion of these connections and their implications.
When performing NTF by minimizing the generalized KL divergence of reconstruction  X  Y from observed tensor Y (which is equivalent to MLE for PTF), the multiplicative update equation introduced by Lee and Seung for, e.g.,  X  (1) ik is
These update equations sometimes converge to locally non-optimal values when the observed tensor is very sparse [8, 22, 2]. This problem occurs when factors are set to inadmissible zeros ; the algorithm cannot recover from these values due to the multiplicative nature of the update equations. Several solutions have been proposed to correct this behavior when minimizing Euclidean distance. For example, Gillis and Glineur [7] add a small constant to each factor to prevent them from ever becoming exactly zero. For KL divergence, Chi and Kolda [2] proposed an algorithm X  X lternating Pois-son Regression X  X hat  X  X cooches X  factors away from zero more selectively (i.e., some factors are still permitted to be zero).
In BPTF, point estimates of the latent factors are not estimated directly. Instead, variational parameters for each parameters then define a Gamma distribution over the fac-tor as in equation (2), thereby preserving uncertainty about its value. In practice, this approach solves the instability issues suffered by MLE methods, without any efficiency sac-rifice. This assertion is supported empirically by the out-of-sample predictive performance results reported in section 5, but can also be verified by comparing the form of the update in equation (11) with those of the updates in equations (4) and (5). Specifically, if equations (4) and (5) are substituted into the expression for the arithmetic expectation of a single equation is very similar to the update in equation (11): outside the sum in the numerator and letting  X   X  0, yields which is exactly the form of equation (11), except that the point estimates of the factors are replaced with two kinds of expectation. This equation makes it clear that the prop-erties that differentiate variational inference for BPTF from the Lee and Seung updates for PTF are 1) the hyperparam-eters  X  and  X  (1) and 2) the use of arithmetic and geometric expectations of the factors instead of direct point estimates.
Since the hyperparameters provide a form of implicit cor-rection, BPTF should not suffer from inadmissible zeros, unlike non-Bayesian PTF. It is also interesting to explore the contribution of the geometric expectations. The fact that each  X  y ijat is defined in terms of a geometric expec-tation suggests that when constructing point estimates of the latent factors from the variational distribution (e.g., for use in prediction), the geometric expectation is more appro-priate than the arithmetic expectation (which is commonly used in Bayesian Poisson matrix factorization) since the in-ference algorithm is implicitly optimizing the reconstruction as defined in terms of geometric expectations of the factors.
To explore the practical differences between geometric and arithmetic expectations of the latent factors under the vari-ational distribution, it is illustrative to consider the form of Gamma (  X  ; a,b ). Most relevantly, the Gamma distribution is asymmetric, and its mean (i.e., its arithmetic expectation) is greater than its mode. When shape parameter a  X  1, Mode (  X  ) = ( a  X  1) b ; when a &lt; 1, the mode is undefined, but most of the distribution X  X  probability mass is concentrated near zero X  X .e., the pdf increases monotonically as  X   X  0. This property is depicted in figure 2. In this scenario, the Gamma distribution X  X  heavy tail pulls the arithmetic mean away from zero and into a region of lower probability.
The geometric expectation is upper-bounded by the arith-metic expectation X  X .e., G [  X  ] = exp ( X ( a )) b  X  a b = E like the mode, it is well-defined for a  X  (0 , 1) and grows quadratically over this interval, since exp ( X ( a ))  X  a  X  (0 , 1); in contrast, the arithmetic expectation grows lin-early over this interval. As a result, when a &lt; 1, the geomet-ric expectation yields point estimates that are much closer to zero than those obtained using the arithmetic expectation. When a  X  1, exp ( X ( a ))  X  a  X  0 . 5 and the geometric expec-tation is approximately equidistant between the arithmetic properties are depicted in figure 7; the key point to take away from this figure is that when a &lt; 1, the geometric expecta-tion has a much more probable value than the arithmetic expectation, while when a  X  1, the geometric and arith-metic expectations are very close. This observation suggests that the geometric expectation should yield similar or bet-ter point estimates of the latent factors than those obtained using the arithmetic expectation. In table 2, we provide a comparison of the out-of-sample predictive performance for BPTF using arithmetic and geometric expectations. Indeed, these results show that the performance obtained using geo-metric expectations is either the same as or better than the performance obtained instead using arithmetic expectations.
Over the past fifteen years, political scientists have en-gaged in an ongoing debate about using dyadic events to study inherently multilateral phenomena. This debate, as summarized by Stewart [29], began with Green et al. X  X  demon-stration that many regression analyses based on dyadic events were biased due to implausible independence assumptions [12]. Researchers continue to expose such biases, e.g., [4], and some have even advocated eschewing dyadic data on princi-ple, calling instead for the development of multilateral event data sets [26]. Taking the opposite viewpoint X  X .e., that dyadic events can be used conduct meaningful analyses of multilateral phenomena X  X ther researchers, beginning with Hoff [16], have developed Bayesian latent factor regression models that explicitly model unobserved dependencies as occurring in some latent space, thereby controlling for their effects in analyses. This line of research has seen an increase in interest and activity over the past few years [14, 29, 15].
In this paper, we too take this latter viewpoint, but in-stead of focusing on latent factor models for regression, we present a Bayesian latent factor model for predictive and exploratory data analysis X  X pecifically, for identifying and characterizing the  X  X omplex dependence structures in inter-national relations X  [17] implicit in dyadic event data. Our ex-ploratory analysis revealed interpretable multilateral struc-tures that capture both persistent regional relations and temporally localized anomalies. As evidenced empirically by our predictive experiments and analytically by a com-parison of our variational inference algorithm with tradi-tional algorithms for performing non-negative tensor factor-ization, Bayesian Poisson tensor factorization overcomes the instability issues exhibited by standard non-negative tensor factorization methods when decomposing sparse, dispersed count data. We provided additional analysis and empiri-cal results demonstrating that when constructing point esti-mates of the latent factors from the variational distribution, the geometric expectation is a more appropriate choice than the arithmetic expectation. We therefore recommend its use in any subsequent work involving variational inference for Bayesian Poisson matrix or tensor factorization.
Thank you to Mingyuan Zhou, Brendan O X  X onnor, Bran-don Stewart, Roy Adams, David Belanger, Luke Vilnis, and Juston Moore for very helpful discussions. This work was partially undertaken while Aaron Schein was an intern at Microsoft Research New York City. This work was sup-ported in part by the UMass Amherst CIIR and in part by NSF grants #IIS-1320219, #SBE-0965436, and #IIS-1247664; ONR grant #N00014-11-1-0651; and DARPA grant #FA8750-14-2-0009. Any opinions, findings, and conclu-sions or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsor. [1] A. Cemgil. Bayesian inference for nonnegative matrix [2] E. Chi and T. Kolda. On tensors, sparsity, and [3] R. Dorfman. A formula for the Gini coefficient. The [4] R. Erikson, P. Pinto, and K. Rader. Dyadic analysis in [5] B. Ermis and A. Cemgil. A Bayesian tensor [6] D. Gerner, P. Schrodt, R. Abu-Jabr, and  X  O. Yilmaz. [7] N. Gillis and F. Glineur. Nonnegative factorization [8] E. Gonzalez and Y. Zhang. Accelerating the [9] P. Gopalan and D. Blei. Efficient discovery of [10] P. Gopalan, S. Gerrish, M. Freedman, D. Blei, and [11] P. Gopalan, J. Hofman, and D. Blei. Scalable [12] D. Green, S. Kim, and D. Yoon. Dirty pool.
 [13] R. Harshman. Foundations of the PARAFAC [14] P. Hoff. Equivariant and scale-free Tucker [15] P. Hoff. Multilinear tensor regression for longitudinal [16] P. Hoff and M. Ward. Modeling dependencies in [17] G. King. Proper nouns and methodological propriety: [18] T. Kolda and J. Sun. Scalable tensor decompositions [19] D. Lee and S. Seung. Learning the parts of objects by [20] K. Leetaru and P. Schrodt. GDELT: Global data on [21] D. Liang, J. Paisley, and D. Ellis. Codebook-based [22] C. Lin. On the convergence of multiplicative update [23] B. Marlin. Collaborative filtering: A machine learning [24] S. O X  X rien. Crisis early warning and decision support: [25] J. Paisley, D. Blei, and M. Jordan. Bayesian [26] P. Poast. (Mis)using dyadic data to analyze [27] A. Schein, J. Paisley, D. Blei, and H. Wallach. [28] D. Singer and M. S. (producers). Correlates of war [29] B. Stewart. Latent factor regressions for the social [30] L. Tucker. Some mathematical notes on three-mode [31] M. Ward, A. Beger, J. Cutler, M. Dickenson, [32] M. Welling and M. Weber. Positive tensor [33] Wikipedia. Cruise missile strikes on Afghanistan and [34] Wikipedia. Embassy of Ecuador, London. Accessed [35] Wikipedia. Japanese Iraq reconstruction and support [36] Wikipedia. Jyllands-Posten Muhammad cartoons [37] Wikipedia. Six-party talks. Accessed June 8, 2015. [38] M. Zhou and L. Carin. Negative binomial process [39] M. Zhou, L. Hannah, D. Dunson, and L. Carin.

