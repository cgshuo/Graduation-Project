 1. Introduction
Although the web continues to attract much research activity, the spread of the information technology to all aspects of life implies that other fields deserve also attention. Among others, stock exchange markets, geographic information systems or the forthcoming electrical grids (SmartGrid) are fields that require addressing the problem of dealing with large amounts of floating point data. For example, the forthcoming electrical grids will produce huge amounts of floating point data, given that any electricity consumer or producer will continuously report information in form of streams of real numbers ( Zicari, 2012; IBM Software, 2012 ).

The use of compression techniques allows to reduce both the flow of information through the network and the storage needs. Furthermore, once the data are stored, in many scenarios it is mandatory to access these data efficiently to help real-time decision-making processes, which, in turn, requires the development of suitable indexing techniques.  X 
Existing indexes provide adequate performance for most classical computer applications. However, these techniques require additional space that, when dealing with large amounts of information, might become prohibitive. Compression has become a so common solution that even commercial database management systems have included it to save space sion techniques that permit random decompression from any position ( Moura, Navarro, Ziviani, &amp; Baeza-Yates, 2000 ), this both saves space and permits indexes to be built over the compressed data.
 Unfortunately, compression techniques specifically designed for floating point data ( Burtscher &amp; Ratanaworabhan, 2009;
Ratanaworabhan, Ke, &amp; Burtscher, 2006 ), or general purpose techniques that obtain reasonable compression values with this type of data (like P7zip ), require decompression to start at the beginning of the compressed data, thereby preventing the indexation of such data.

Therefore, we have a trade-off between space and performance. On the one hand, to obtain sub-linear search time over sequences of real numbers, we have to store the data uncompressed in order to be able to build an index over them, thus increasing the space requirements. On the other hand, if we need to save space, we have to store the data in a compressed form and then, to perform searches, we must first decompress such data, and then run a linear-time search algorithm.
The outbreak of the web has promoted the development of data structures that have several appealing characteristics to do not suffer from frequent changes. However, floating point data have a large alphabet (a real number hardly ever repeats twice) and a low biased distribution, yet many scenarios can hold the last requirement, since it is usual to produce data that never change and are only stored to, for example, run decision-making processes. In this work, we aim at applying those data structures to the floating point data scenario, trying to keep their good features as much as possible.
In recent years, the research community has worked extensively on inverted indexes ( Knuth, 1973 ) due to their use to index text and more specifically to index the web. The main problem of this structure is the consumption of space, since it spends considerable amounts of space in addition to the original information.

At the same time, a new family of data structures, called self-indexes, have merged compression with indexing. They require space proportional to the compressed text, replace it, and permit fast indexed searches on it without any additional tern string in a time that depends on the pattern length and the output size (number of occurrences), but not on the text size (that is, the search process is not sequential). Most of them are also able to count the number of occurrences of a pattern string much faster than just locating them.

In this work, we present four contributions. First, we study the adaptation of inverted indexes to the indexation of real numbers. In our experiments, we will show that a structure whose size (including the original data plus the whole index) is between 10% smaller and 20% larger than the original sequence obtains locate times that are up to around 8000 times fas-ter than the sequential search. Second, we study the use of self-indexes to index and store sequences of real numbers. The structure that allows both to store and index data. Our second proposal based on self-indexes uses a wavelet tree on byte-performance in previous scenarios. The third self-index is based on the recent integer-based Compressed Suffix Arrays (iCSA) successfully handle large alphabets, as those that arise when indexing real numbers. In our experiments, the self-index based on the classical WTs occupies around the same size as the original collection (from 10% less to around 10% more), whereas the locate times are up to 12 times faster than the sequential search. However, in the case of the WT, the extract operation, needed to recover a portion of the original sequence, requires some computational effort, whereas that time is negligible in ation, whereas checking if a data source surpassed a certain threshold will be the usual case. In a general scenario the best space/time trade-off of our study is achieved by using an index based on the WTBC, which obtains locate times up to 100 times faster than the sequential search, with a structure up to 15% smaller. Yet, when one aims at saving as many space as possible, the iCSA-based self-index becomes the best choice. It permits to save up to 30% of the size of the collection and permits to count the occurrences of a given search pattern up to 10 performance at locating them is poor.

The capabilities of these structures can be useful in different scenarios, for example, assume that the measures of the elec-tricity consumption of end consumers are stored in one or multiple arrays, or simply, they are stored consecutively. Those the main targets of the SmartGrid is to determine the production of the power plants over time, since the electricity cannot be stored, and therefore an excess of production represents a waste of money. Consider the time series of the aggregated consumption of a certain region, the SmartGrid could be interested in the past periods where the consumption was within a certain range, assuming that when the consumption is between the boundaries of that range, this requires a particular configuration of the system, including that certain power plants should be on while others must be disconnected from the network and turned off. The study of the conditions of those past periods, like the hour of the day and the weather conditions, helps to determine which is the suitable configuration for the current conditions.

In most scenarios, like in a SmartGrid system, we expect that exact searches of floating point values will be unlikely, and we expect that range queries will be the usual ones. However, our indexes can search for exact values if it is required. We access a given position of the sequence in sublinear time as well.

The outline of this paper is as follows. Section 2 presents some related work and Section 3 discusses the 64-bits IEEE 754 format and a preliminary study of the characteristics of real numbers. Section 4 presents our first contribution, showing how to adapt inverted indexes to successfully index sequences of real numbers. In Section 5 , we describe how we adapted three well-known self-indexing structures (a classical WT, a WTBC, and an iCSA) to deal with real numbers. Section 6 shows our experiments. Finally, our conclusions and directions for future work are shown in Section 7 . 2. Related work 2.1. Inverted indexes
Inverted indexes ( Knuth, 1973 ) are composed of two elements: a list of search keys and a collection of posting lists. The the positions in the original sequence where that particular key can be found (see Fig. 1 ).

The inverted index is the most common structure to index text. Its importance for the web has increased the interest of the investigation of this structure. The focus of the research community is in the main problem of the inverted index, it requires too much space. Observe that, as a classic index, it is an auxiliary structure that should be added to the data. To attenuate this problem, two strategies can be followed: to compress the source data and to compress the posting lists.
In the case of text, several compression techniques have been used to compress the source data. Among them, the word-oriented Huffword is a well-known alternative ( Witten, Moffat, &amp; Bell, 1999 ) that combines good compression and perfor-mance. Other more recent techniques such as Tagged Huffman ( Moura et al., 2000 ) or the Dense codes ( Brisaboa, Fari X a, the decompression from any point of the compressed data. For repetitive data, grammar-based compressors ( Larsson &amp; Moffat, 2000 ) proved also to be successful.

Observe that a posting list is a sequence of increasing numbers, therefore incremental encoding is the obvious choice to obtain compression. Additional savings are obtained using different techniques to encode the gaps, like for example, Byte responding posting list has fewer pointers. However, now the index is only useful to filter out blocks and then, a sequential scan within those blocks is needed to obtain the exact positions of the searched key. 2.2. Self-indexing structures
Self-indexes are a more recent approach to both store and index sequences of symbols S  X  1 ; n over an alphabet R of size r , which were developed for relatively small alphabets and for sources with a rather skewed distribution. They efficiently support three basic operations: counting and locating all the occurrences of a given pattern in S , as well as extracting any representation of S , since S  X  1 ; n can be recovered via extract operation.
 In the sequel we focus on two well-known self-indexing structures such as the wavelet-tree ( Grossi et al., 2003 ) and
Sadakane X  X  Compressed Suffix Array ( Sadakane, 2003 ). We also describe two variations of them that were shown to success-fully handle large alphabets such as the Wavelet Tree on Bytecodes ( Brisaboa et al., 2012 ) and the Word Compressed Suffix 2.2.1. Wavelet trees symbols drawn from an alphabet R of size r . Although such alphabets were usually small, WTs have succeeded at dealing with sequences composed of words ( Brisaboa et al., 2012 ), and also at managing sequences built over even larger alphabets the symbol at any given position. Besides, it is capable of obtaining all the positions of S where a given symbol is located.
WTs were firstly proposed for solving rank and select queries over sequences on large (non-binary) alphabets. Given a sequence of symbols S ; rank b  X  S ; i  X  X  y if the symbol b appears y times within S  X  1 ; i , and select of the symbol b in the sequence S appears at position x .
 node v , only a bitmap is stored. It is denoted by B  X  v  X  . When
The symbols S  X  v  X  X  i from S that belong to R 0 go to the left child of way, if S  X  v  X  X  i belongs to R 1 ; B  X  v  X  X  i is set to 1. Recursively, each child denoted as S  X  v 0  X  . That is, B  X  v 0  X  X  i 0 iff S  X  v 0 obtained when traversing the tree from the root down to a node that handles the symbols of S that have the binary label L  X  the binary representation of the symbols of R .

The WT has been used for different purposes, and with different shapes. For instance, if the frequency of each symbol is bitmaps of the WT is the same as the number of bits output by a Huffman compressor. This is upper bounded by n  X  H being H 0  X  S  X  the zero-order empirical entropy defined as H each symbol.

Using an uncompressed bitmap representation that takes n  X  o  X  n  X  bits, the total space of the WT is at most pression effectiveness but also to access time. Grossi, Gupta, and Vitter (2004) showed that if symbols are accessed with a frequency proportional to their number of occurrences in the sequence S , then the average access time is improved from
O  X  log r  X  , for a plain WT, to O  X  H 0  X  S  X  X  1  X  in a Huffman shaped WT, holding H The shaded numbers are not actually stored in the WT, and are only displayed for illustration purposes. In this example, the
WT is a balanced binary tree, since the coding used to shape the WT consists in the 3-bits binary representation of each symbol.

The WT reduces both recovering the original data and searching for a given symbol to rank and select operations on the bitmap in the root node ( B  X  V ;  X  X  4 ), obtaining a 1. Now we have to check which is the order of that 1 among all the 1 node, in our case, it is the second occurrence of a 1 (that is, rank of the bitmap of the node V 1 , that is, the node handling the symbols of the second half of the alphabet (those whose binary representation starts with 1). We obtain again a 1 ( B  X  V ( rank 1  X  B  X  V 1  X  ; 2  X  X  2). Finally, we move to node V the bit value 0. Therefore, the binary representation of S  X  4 is 110, and we recover the symbol 6.

We can also search for the position in S of the j th occurrence of a symbol. First, we have to obtain the codeword of the symbol using the same encoder used to shape the WT. Then, we must reach the leaf that contains such codeword by travers-the left child otherwise. Once the leaf is reached, we start an upward transversal using the nodes that are already in the node is the left child of its parent, we set pos select 0 of the current node. Once we reach the root, the sought position of the j th occurrence of the symbol is pos .
If we want to retrieve all the positions where a symbol occurs, the process is similar but once we reach the leaf, we have to carry out an upward transversal for each occurrence of the symbol (the number of occurrences is known once we reach the leaf). 2.2.2. Wavelet trees on bytecodes
Wavelet trees on bytecodes (WTBC) ( Brisaboa et al., 2012 ) are based on a family of compression codes known as byte-target alphabet (that is, the codewords that replace the original symbols are sequences of one or more bytes). This change obtain faster times at compression and especially at decompression. The first technique, called Plain Huffman (PH), is just a
Huffman code assigning byte rather than bit sequences to the codewords, that is, this is just the d -ary Huffman code that appears in Huffman X  X  original paper ( Huffman, 1952 ), with d = 256.

More recently, Brisaboa et al. (2012) proposed reordering the bytes in the codewords from a text compressed with a given bytecode following a wavelet-tree-like strategy. At the expense of a small space overhead over the text compressed with that bytecode, such a reorganization turns the compressed data into a self-index: the WTBC structure. In particular, the WTBC with PH shape was shown to perform the best.

In WTBC, instead of representing the compressed data as a sequence of codewords, each one representing one original symbol, the data is represented with a wavelet tree where the different bytes of each codeword are placed at different nodes. sequence. The root has as many children as different bytes can be the first byte of a codeword. In the second level, the node at position i in node V x is the second byte of the codeword corresponding to the i th original symbol whose codeword starts with the byte x . The same procedure is followed in the lower levels of the tree.

We use again L  X  v  X  to denote the sequence of labels obtained when traversing the tree from the root down to a node
B  X 
 X  to denote the sequence (in this case) of bytes in the node elements are not stored and are only included for clarity. The process starts by computing the codeword corresponding to codewords representing the symbols in the ordering of the original sequence. The second byte of those codewords is con-tained in the corresponding child. For example, since in the 8th position of the root node we have the byte b byte of S  X  8 is in the child corresponding to the codewords that have b original sequence that has b 4 as the first byte of its codeword, therefore in the second position of V of S  X  8 . Since the codeword representing 0 (the symbol in S  X  8 ) only has two bytes, the process ends here.
Now, suppose that we want to recover the symbol S  X  5 from the WTBC. We start at the 5th position of the root node, and we obtain the byte b 3 . This is the second occurrence of that byte in the root node (we know that by performing rank b 3  X  B  X  V ;  X  ; 5  X  X  2). Therefore, we access the second position of V that byte value in V b 3 , since rank b 3  X  B  X  V b 3  X  ; 2  X  X  1. Then, we access the first position of V value b 1 . Therefore, the symbol S  X  5 is represented by the codeword b containing the correspondence between the original symbols and the codewords and we obtain that S  X  5 is a 5.
We can also search for the positions in the original sequence of a given symbol as in a binary WT. Hence, in a first step, we use the bytes from the codeword of that symbol to traverse the WTBC downwards by performing rank operations. Once the corresponding leaf is reached, we search for the positions of the occurrences of that symbol by traversing the WTBC upwards performing select operations.

The sum of the space needed by the WTBC is the same as the size needed by the codewords of the bytecode used to encode the symbols, plus a negligible amount of extra space to store the pointers that keep the shape of the WTBC. 2.2.3. The Compressed Suffix Array: Sadakane X  X  CSA permutation of  X  1 ; n , so that for all the suffixes S  X  i ; n ; 1 ordering (as S typically contained text).
 term appears because at each step of the binary search, one could need to compare up to m symbols from P with those in the suffix S  X  A  X  i ; A  X  i  X  m 1 . Unfortunately the space needs of A are high.

To reduce these space requirements, Sadakane X  X  CSA ( Sadakane, 2003 ) uses another permutation W  X  1 ; n defined in ( Grossi have E  X  rank 1  X  D ; l  X  X  E  X  rank 1  X  D ; x  X  8 x 2 X  l ; r 1 . Note that rank ( Jacobson, 1989; Munro, 1996 ).
 By using W ; D , and E it is possible to perform binary search without the need of accessing A nor S . Note that, the symbol
S  X  A  X  i  X  1 as E  X  rank 1  X  D ; W  X  i  X  ; S  X  A  X  i  X  2 as E  X  rank we could need to compare with P  X  1 ; m in each step of the binary search.

However, in principle, W would have the same space requirements as A . Fortunately, since W is formed by r subsequences or c -codes ( Elias, 1975 ). In practice, absolute sampled positions W  X  1  X  i t values. Further research showed that coupling Huffman and run-length coding of gaps succeeded at reduce W size ( Navarro &amp; M X kinen, 2007; Fari X a et al., 2012 ).
 extract any subsequence S  X  i ; j we also need to keep A 1 which we could start the extraction mechanism using W ; E , and D showed above. In practice, only sampled values of A and A are stored. In the former case, the source sequence S is sampled at positions t ; 2 t ; ... ; n , and an array A values of A (in suffix array order) pointing to those sampled positions. In addition, a bitmap B tions from A . Therefore, sampled values A  X  i are computed as A  X  i A retrieved by starting with i 0 i and then applying i 0 W  X  i B  X  x  X  1). At this point we compute A  X  i A  X  x k . Similarly, samples of A 1  X  j t before b ( j  X  b 1  X  = t  X  1). From here on we know that S  X  1  X  j t is pointed from A  X  i ; i A use the usual extraction mechanism to recover the subsequence S  X  1  X  j t ; e which contains S  X  b ; e . need A anymore to perform searches.
 Note that different sampling values t can be used for A ( t time trade-off for a 1 GB text. In this case, a CSA requires nH to 2 nH k  X  O  X  n  X  . In our experiments we will use several values t  X  t offs in the CSA-based indexes. Fig. 4 shows a description of the structures needed in a CSA. 2.2.4. The Word-based Compressed Suffix Array: WCSA and iCSA
The original CSA was designed to typically index characters within a text sequence, so that CSA was able to count , locate , and extract any substring of the indexed text. Fari X a et al. (2012) modified CSA to index words rather than characters and created the so called Word-based CSA (WCSA). It allows to perform the same operations, but in a word-wise fashion. That is, around n = 5 words, WCSA has to index less symbols than a character-based CSA. This permits it to greatly reduce space needs. by its corresponding id to make up the sequence S that is actually self-indexed with an integer-based CSA (iCSA). Note that, the original CSA handles an alphabet of at most 256 values (characters), whereas the number of different words in a text is much larger. Therefore, iCSA is just an adaptation of CSA that handles integers, hence allowing us to deal with very large alphabets. 3. The IEEE 754 format: basics and preliminary analysis
We consider the double precision 64-bits IEEE 754 format ( IEEE Std 754-200, 2008 ). This standard divides the floating numbers), (ii) 11 bits to represent the exponent of the number, and (iii) 52 bits to represent the mantissa.
Using the following expression we can transform a binary floating point representation into the real number representation: where constant bias equals 1023 (in the case of 64-bits double precision numbers).

The exponent indicates the magnitude of the number while the mantissa contains the integer part (if it has one) and the decimal part. This format usually normalizes the number to a number such that 1 sented and the bits that are closer to the decimal point are stored in the leftmost bits reserved for the mantissa.
When we have a collection of double precision real numbers it is likely that most of those numbers are unique, and very because most decimal numbers could not be represented since the format is designed for discrete machines, while the nature of the decimal numbers is purely continuous. Hence, the arithmetic algorithms must deal with rounding problems that can generate very different representations for similar numbers.

However, collections of floating point numbers are usually measures or results of some computational process or real phenomenon. Therefore, we expect that, despite the numbers could be significantly different, their magnitude can be similar ( Engelson et al., 2000 ). In such a case, in a sequence of real numbers, the number of different exponents is usually smaller Ratanaworabhan, 2009; Ratanaworabhan et al., 2006 ). Hence, the first bits are the most compressible part of each number.
The question is now how many bits must we index to achieve a compact representation? As explained, since it is rare that a real number appears twice in a sequence, if we index all the bits in each real value, the index will have an entry for each number in the sequence, and therefore the index will be even larger than the original data.

Fortunately, the nature of most data and the characteristics of the IEEE 754 format permit us to appreciate a biased dis-over those initial bytes.

To show this, we studied six collections of real numbers downloaded from Martin Burtscher X  X  site.
The collection named brain contains results from a numeric simulation of a velocity field of a human brain during a head NAS Parallel Benchmark. Table 1 shows the characteristics of these collections.

Table 2 shows a study of the zero-order empirical entropy of those collections. We do not tackle larger orders, given that the cost of storing the contexts limits the character-oriented compressors with k th-order modelers to small values of k (not usually larger than 16). Therefore, if we deal with larger alphabets this cost is much worse and then prohibitive in space terms.

In the table, columns from the second to the sixth show the entropy (in bits/symbol) of the first x bytes. That is, when displaying the entropy of the first byte, we consider the first byte of each number in the collection as an input symbol, and we compute the entropy of those symbols. When we show the entropy of the first 2 bytes, the symbols are 2-byte num-bers, and so on.

The last five columns give a lower bound of the compression achieved if we compress only the first x bytes and leave the of bytes are stored verbatim, we need 7 additional bytes, in total, 58 : 38 bits/symbol to represent each number.
However, the previous study gives an unrealistic level of compression, since any statistical compressor would require an additional table to hold the correspondence between the original numbers and the codewords representing them. The size of such table might also give us an idea of the size of an index over that collection, as an index should have to handle all the table as: where m is the number of unique values in the collection and x the number of bytes being compressed from those numbers. word representing the original symbol in the compressed text. Recall that we only compress the first x bytes, and the rest are stored verbatim.

Table 3 includes our estimation of the size of the table that maps original numbers and codewords, as well as the esti-mation of the size of the compressed data including the remainders from Table 2 . We can see that the best average compres-sion ratio is achieved by compressing 3 bytes and leaving the rest uncompressed. As expected, the exception is in highly repetitive collections, in our example sppm . In our tests, this combination of compressed bytes and non-compressed bytes is the most suitable one to meet the two basic requirements to obtain good compression with a statistical compressor: (i) the distribution of the source data must be biased enough to take advantage of the repetitiveness and (ii) the number of different values cannot be excessively large, otherwise the table that maps the original symbols to codewords would be too large, hence spoiling the compression. Observe that if we compress fewer bytes, the distribution is less biased, whereas if we compress more bytes, the number of different values increases significantly.

A more realistic way to check the correctness of our study is to use a Huffman previous study, we measured the compression achieved taking also into account the remainders, which are not compressed. respondence between original symbols and codewords is large, the real Huffman compressor gives better values. The reason could be that our Huffman compressor compresses that table with a char-based bit-oriented Huffman, hence obtaining addi-tional compression.

However, the general idea does not change. Except in collection sppm , which is highly repetitive, compressing the first 3 bytes and leaving the other 5 bytes uncompressed achieves the best balance between compression ratio and indexing as order Huffman. Therefore, in a general scenario we do not expect self-indexes (including those achieving high-order compres-sion as well as grammar-based or Lempel X  X iv-based self-indexes) to be successful in space in a general scenario.
Of course, if the domain is totally different from our datasets, the optimal number of bits to index could change, but, in any case, our proposals can be adapted with no effort to index any number of bits. 4. Inverted Indexes for sequences of real numbers
As explained in the previous section, if we build an inverted index over a sequence of real numbers, it is likely that the inverted index will be composed by posting lists having in most cases only one value. This implies that the problem of space key in the list of keys plus, around d log  X  n  X e bits per occurrence only the first x bytes of each number (the more compressible ones), that is, the list of keys contains key values of x bytes.
The idea is to reduce the number of entries in the list of keys from a value that could be close to 2 coupled with the gap encoding will be able to reduce the space.

In addition, we expect that the search times would not suffer too much since it is likely that the search of exact real num-bers will be rare, whereas the issue of range queries will be the usual case. With our approach, a range search can be solved first accessing the index, which implies a search that involves the sign, the exponent and the first  X  x 8  X  12 bits of the mantissa (from left to right), since as explained, the leftmost bits hold the most significant part of the number. Obviously, positions, and then those positions must be inspected. For this sake, we store the remaining 8 x bytes of each number in the source sequence in an array of fixed length numbers. Finally, if the index holds more than the bits needed to answer the
Recall that the inverted index is an auxiliary structure, then the original data must be kept. However, the array of fixed length numbers storing the remaining 8 x bytes of each number already store part of the original data. Yet, we still have to store the first x bytes of each number, just those indexed. Again those bytes are stored in another fixed length array, but saves space.
 case, we indexed the first 3 bytes of each number. To do this, we extract the first 24 bits of each real number, and then we length remainders (5 bytes each). In the lower part, we can see the inverted index, formed by the list of keys , that is, the different values found in the first 3 bytes of each number (sorted increasingly), and their corresponding posting lists .
Finally, the first 3 bytes of each number are stored in a k -bit array. In our example, there are only four different values
Finally, to compress the posting lists, we encoded the gaps of the incremental values with Rice codes. 4.1. Tuning the inverted indexes
Using the same experimental framework described in Section 6 , we present a brief study where we compare the space/ time trade-off obtained by our inverted indexes when we use Rice codes inverted index (II); and (ii) a block-addressing inverted index (II-blocks) tuned to use blocks of 512, 2048, 8192, and 32768 shows average time needed to locate patterns of 5.5 bytes. Fig. 7 shows that among this three encoding methods Rice is the technique obtaining the best compression whereas the time performance is quite similar in all of them. That is the reason why we will use Rice codes in advance in our inverted indexes. 5. Self-indexing sequences of real numbers
In this section, we present how we have adapted the three self-indexing structures described in Section 2.2 to successfully deal with sequences of real numbers. We follow the same guidelines presented in the previous section, that is, splitting the real numbers into an indexable and a non-indexable part ( x and 8 x bytes respectively), and briefly describe the resulting structures. 5.1. Using Wavelet Trees
A WT is a binary tree that self-indexes a sequence of symbols S  X h s numbers of x bytes. We use WTs with Huffman shape, that is, the represented values in the WT are the codewords that the (binary) Huffman algorithm assigns to each original symbol in S .

We used Francisco Claude X  X  libcds library ( Claude, 2012 ). libcds includes several compressed data structures, including an implementation of a WT with Huffman shape that occupies nH libcds uses pointers to build the structure of WTs with Huffman shape and stores at each leaf the number of occurrences of performing the navigation through the tree by means of rank and select operations, yet this slows down the searches.
To allow the WT to store and index real numbers, we apply the same idea used for the inverted indexes: we only index the not need to be stored separately in an array of x -bytes numbers, since as we are using a self-index, those bytes are already implicitly stored within the WT.

Furthermore, if we only index the first 3 bytes, there are chances of finding runs of equal consecutive numbers in S .To avoid representing those repeated values several times and to avoid a WT traversal for each repeated symbol at those runs, instead of indexing the original sequence h s a ; s a ; s only stores a number when a change in the original sequence is found. Obviously, this would lose information. Yet, to main-
At search time, once we found a match, for example the second element ( s know its exact position in S , we first compute select 1  X  R ; 2  X  X  4, which gives the position of the first s number of repetitions of that run is obtained as select 1 To further reduce the space we compress the bitmaps using the technique in Raman et al. (2002) . It represents a bitmap
Fig. 8 depicts the final structure, again only the non-shaded areas are the data structures actually stored, whereas the shaded areas are only included for illustration purposes. For simplicity, we represent the WT with regular shape (rather than using a Huffman-shaped WT) and assume that each indexed number has only 4 bits (instead of the 24 bits of a 3-byte num-case of using a WT with Huffman shape, an additional table storing the correspondence between the original symbols and the corresponding codewords is also needed. 5.2. Using Wavelet Trees on bytecodes
The idea is basically the same as in previous cases. We index only the first x bytes of the numbers and we store the remainders in verbatim. Even though any bytecode could be used in our experiments to create the WTBC, we chose a Plain-Huffman-based WTBC, which is known to be the best one ( Brisaboa et al., 2012 ).

As in the previous section, by using Plain Huffman, we obtain a wavelet tree with Huffman shape, and therefore the space consumption is related to the zero-order entropy. As explained, by using bytes rather than bits as the target alphabet the compression worsens. This is easy to see since Huffman encoders, as any statistical method, need a model of the source data statistical methods give shorter codewords to the most frequent original symbols and larger codewords to the least frequent ones. Therefore, the classical Huffman code might assign a codeword of just 1 bit to the most frequent symbol, whereas Plain
Huffman code would assign a codeword of at least 1 byte to that symbol. This could become an important drawback if we were dealing with small alphabets, yet it is attenuated in the scenario for which Plain Huffman was designed, large texts. By using a word-based modeler and by the Heaps X  law, that implies that the frequency of words is much more biased than that of characters ( Moura et al., 2000 ), the byte-oriented Huffman compresses only around 10% worse than the bit-oriented Huffman.

Yet, let us recall what was the target of bytecodes. The use of bytes as target alphabet was proposed to obtain better com-assume the worst case with x  X  3; a source data where the data distribution is completely uniform and the 2 present. This would give a completely balanced tree and H the same scenario using Plain Huffman, H 0 would be 3 target symbols (bytes) per original symbol.

We can make the reasoning in another way. As any other index tree, like a B-tree, the bigger fan-out, the lower the tree, and therefore the faster the search. The WTBC with Plain Huffman has a 256-ary Huffman shape, having a fan-out of 256, with three levels, we could keep the 2 24 values, whereas a classical WT would need 24 levels. Therefore, the traverals during the searches in WTBC would have length O  X  3  X  1  X  , while those traversals in the classical WT with Huffman shape would have length O  X  24  X  1  X  .

The assumption above is very close to reality in most of our real numbers collections. For example, considering the first 3 bytes of each number of the lu collection (presented in Section 3) , a search for a number in a binary Huffman shaped WT requires to traverse O  X  16 : 39  X  1  X  levels on average, whereas the same search in a WTBC using Plain Huffman requires to move from the O  X  7 : 12  X  1  X  levels of the binary WT to the O  X  0 : 89  X  1  X  levels of the WTBC.

To sum up, the WTBC would have search times related to the zero-order entropy, considering bytes as target alphabet. On the other hand, since the classical WT with Huffman shape is a binary tree, it would have search times related to the zero-order entropy considering bits as the target alphabet.

The structure of the WTBC follows the same guidelines showed in Section 5.1 . Yet, the small height of WTBC makes it unnecessary to use bitmap R in order to avoid traversals during searches. Even though including R could help to slightly improve space requirements in very repetitive collections, the expected search performance would be similar. Note that the most frequent symbols will be given a 1-byte codeword that will be located in the root node of the WTBC. Consequently no traversal will be needed when searching for them, what nullifies the benefits of handling runs. In addition, in non-repetitive collections using R would even worsen space needs. Fig. 9 shows an example with the final structure of WTBC. 5.3. Using integer-based Compressed Suffix Arrays
As in the previous sections, we used the iCSA to index the sequence of integers formed by the first x bytes of each number, and we stored the remainders verbatim. The success of this self-index will depend, in part, on the repetitiveness present in the sequence of integers, since the space consumed by this structure is close (in practice) to 2 nH restricted to low values of k . To be successful, the k th order entropy requires the presence of sequences of symbols that repetitively appear throughout the sequence being indexed. The other factor that affects the compression achieved by the as the number of applications of the W function to solve searches is smaller. On the contrary, with a sparse sampling the compression is better, but obviously, the searches are slower. 6. Experimental evaluation
In our tests, an isolated Intel  X  Xeon  X  -E5520@2.26 GHz with 72 GB DDR3@800 MHz RAM was used. It ran Ubuntu 9.10 (kernel 2.6.31-19-server), using gcc version 4.4.1 with -O9 presented in Section 3 .
 Our experiments focus on showing the memory utilization and search performance of our indexes at search time. Yet, in Section 6.4 , we also compared their construction time and memory usage at indexing.

We include experiments 8 for count , locate , extract , and range queries for: (i) a WT-based index with Huffman shape (WTH), Rice codes for compressing the posting lists of the inverted indexes.

We performed count and locate for 1000 numbers and show average times. That is, we count the number of occurrences of a given pattern or also locate them within each structure. We randomly chose patterns of 3 bytes (those indexed) and 5.5 bytes (3 indexed plus 2.5 bytes). The former patterns can be directly searched for by the index, whereas the latter force a further search over the array of fixed length remainders, using the candidate positions provided by the index.
For range queries we took a randomly chosen 3-byte indexed value and we search for all the patterns that start by that value or by the next three consecutive (3-byte) indexed values. In our experiments we show the average times of performing 100 range-queries.
 of 200 consecutive elements from the collection S  X  p 99 ; p  X  100 centered at a random position p such that 100 6 p &lt; n 100. By providing average extraction time per number, the latter experiment permits us to show the cost of random access plus extraction , in comparison with extract all operation. 6.1. Comparison on space needs
Table 5 includes the space needs of two baseline compressors and those of the proposed indexes built on the described datasets. Even though the datasets are rather heterogeneous, we also include an average row to summarize the behavior of each technique. For the parameterizable structures (IIB, WTH, WTBC, and iCSA) we include two different setups obtained by tuning their parameters respectively with both the most dense and sparse sampling configurations described above, so that we achieve either a larger (and faster) or a more compact (and slower) index.

The baseline compressors included are: (i) Huff-3b , a Huffman compressor run over the 3 first bytes of each number and leaving the rest verbatim and (ii) P7zip run over the whole sequence. In the compression ratio of our indexes we consider all the structures that each index requires at query time (including the original data in the case of II and IIB).
The II occupies around 15% more than the original collection, whereas IIB yields values that range from around the same size as the original collection when the block size ( b ) is set to 512, to a size around 10% smaller when using b  X  524, 288.
WTH typically obtains a compression around 5 percentage points worse than IIB. Yet, in the highly repetitive collection ( sppm ) the R bitmap leads to an improvement around 6% over IIB. This backs our hypothesis that WT-based indexes would stock market. WTBC yields better compression than II and clearly overcomes WTH in most scenarios. These values are not too far from those achieved by the Huffman compressor, and even in the non-repetitive collections, they are not exaggerat-edly far from those achieved by P7zip , being the values of the WTBC only around 10 percentage points worse. Finally, we can see that a lightweight setup of iCSA makes it the smaller indexing structure in all scenarios. In addition, it behaves particularly well in the repetitive datasets, yielding compression ratios under 70%. 6.2. Comparison on searching performance
We compared the searching performance of our indexes in both a non-repetitive and a repetitive scenario. In the former we datasets. Note that we tried to capture de average behavior for each scenario by summing the times obtained over the (either 4 or 2) datasets that compose it.

In addition, a sequential search (SS) over the original sequence of real numbers is included as a baseline. 6.2.1. Searching performance: count and locate
Fig. 10 (a) and (b) show the average times for counting the occurrences of 1000 patterns of 3 bytes, which consist in the first 3 bytes of the indexed numbers. A similar behavior is observed in both the repetitive and non-repetitive scenarios.
As expected, iCSA obtains by far the best space/time trade-off (count takes O  X  log n  X  time). WTs perform also fast at count-ing as it consists only in a top-down traversal followed by 2 rank operations to count the occurrences of the searched symbol in the corresponding leaf. Among the WT-based techniques, WTBC is not only smaller than WTH, but also clearly overcomes WTH by around 10 times at count operation.

The count time in II and IIB is proportional to the number of occurrences. This makes them slower than the self-indexing structures. Yet, II still obtains times close to those of WTH. In the case of IIB searching times worsen as the block-size increases. In particular, when using very large blocks (containing more than 2 sequential search over the uncompressed sequence. Yet it also requires only around 85% of its space. be solved directly with the index which only has to find the relevant vocabulary entries and then fetch all the values within the corresponding posting lists. As shown, the price is a structure around 15% larger than the original sequence.
IIB shows different space/time trade-offs. When using small blocks IIB is around 10 X 20 times faster than the sequential search (with similar space usage). WTH is also around 10 times faster than the sequential search. In the non-repetitive sce-nario WTH is slightly overcome by IIB. However, in repetitive datasets WTH overcomes IIB when we tune the indexes so that they use less than 98% of the space of the original data.

As shown before, iCSA is the most compact self-index. This makes it the best choice in the repetitive scenario when we want a small index (it is the only one yielding compression ratios under 80%). However, when we allow the indexes to use space over 85% of the original collection, WTBC is the clear winner.

In a non-repetitive scenario WTBC obtains the best space/time trade-off (only II improves its search times). iCSA is still a good choice (particularly as space decreases) but its locate times are around 5 X 10 times worse than those of WTBC for compression ratios over 95%.
Fig. 10 (e) and (f) show the performance when the indexes locate patterns formed by the first 5.5 bytes (2.5 bytes more than those indexed). We can observe similar results to those obtained when searching for just the 3 indexed bytes. Yet, the displayed search times are around 100 X 1000 times slower. Note that this huge difference is mainly in part due to the fact that we show time per occurrence, and now the number of occurrences is much smaller. For example, in the repetitive sce-nario, in datasets lu and sp we find 391,232 and 23,466,182 occurrences when searching for 3-byte patterns and respectively only 2500 and 3435 when searching for 5.5-byte patterns. Therefore, our indexes have to pay for locating all 3-byte patterns, and then also filter out those occurrences that do not match the array of fixed-length remainders.

In terms of overall time needed to locate all the occurrences of a given pattern, the gap between locating either a 3-byte ders is small.
 Dependency on the number of bits indexed :
We also present an experiment where we show how the number of bits indexed determines the space X  X ime trade-off obtained by our indexes. In this case, we set the block size of our IIB to 512 bytes; we tuned the sampling gap in WTH to 11; and we set the sampling rate in the iCSA to t 8. Fig. 11 shows the results obtained when we index locate searches for 5.5 byte patterns. In general terms, indexing more bits leads to faster indexes, yet space usage also increases. The main exception to this is iCSA in the repetitive scenario, where the more bits are indexed the more repetitive-ness is found and better compression is obtained. We can see that (particularly in the non-repetitive scenario) indexing 3-bytes (24 bits) is typically a reasonable choice. 6.2.2. Searching performance: dealing with range queries
Our next experiment focuses on performing range queries. Recall that now our indexes must retrieve all the occurrences of four consecutive 3-byte indexed numbers. That is, it is expected that we have around four times the cost as for locate, but also around four times its number of occurrences. Therefore, the average time per occurrence should be similar in both cases.
This is exactly what we can see if we compare the results in Fig. 12 (a) and (b) with those in Fig. 10 (c) and (d). 6.3. Recovering the original data: extract operation
We measured the time needed to recover the original data ( extract ). We only present results for the non-repetitive sce-subsequence of 200 numbers from 1000 random positions. Fig. 13 (a) displays the average time needed to extract each num-ber (in msec/number). II and IIB require a small amount of time to merge the first x bytes and the other 8 x bytes to make up the final numbers. Therefore, it is around 10 times slower than the straightforward extraction of the plain representation. ture at extract being around 10 times faster than iCSA. WTH is the slowest technique. Fig. 13 (b) shows exactly the same behavior. This implies that the random access does not hurt significantly the extraction speed, showing that this important requirement is nicely handled by all the indexes.

Our IIs obtain the best space/extract time trade-off. They recover around 7 : 5 10
WTs is more expensive, as the WTs have to perform rank operations from the root to the leaves to recover the original num-recover S  X  1 ... n ). WTH recovers around 0 : 25 10 6 numbers per second and, due to its lower tree height and a more refined implementation, WTBC around 10 7 numbers per second. Finally, iCSA with a regular sampling (i.e. a compression ratio around 90%) recovers around 10 6 numbers per second, showing the cost of accessing to a compressed W (plus a rank oper-ation over D ). 6.4. Memory usage and time at indexing
We present additional information to show the construction cost of our indexes. We tuned the indexes in their fastest (and most space consuming) configuration, and measured both cpu user-time (in seconds) and peak memory usage MB).

Fig. 14 presents the results for each dataset. The x axis shows the size of the datasets (sorted increasingly). Note that our indexing programs were not heavily optimized to minimize memory utilization and construction time. Yet, we can see that, except WTH, which requires around 100 s, all the other structures are built in less than 15 s. Regarding memory consump-tion, our indexes use around 2 X 4 times the size of the source dataset. 7. Conclusions
In this work, we have presented succinct indexes for collections of IEEE 754 double precision floating point numbers. The and a compressible part (the first 3 bytes) that can be indexed with any general purpose integer-based indexing technique.
By doing so, we have presented five different indexing alternatives from three well known families of indexes. Two of them are based on inverted indexes (II and IIB), WTH and WTBC are based on wavelet trees, and finally we also included a variant based on Compressed Suffix Arrays (iCSA). Yet, note that any integer-based indexing structure from the state-of-the-art can be used. Apart from indexing, this approach is applicable to compression. Indeed, following the ideas in Brisaboa, Ladra, and
Navarro (2013) , we are working in a new method to compress sequences of real numbers that will permit direct access to any position.

The size of the resulting structures (including both the data and the index) is between 30% shorter and 20% larger than the original sequence. As expected, the improvements in search time are remarkable in the case of inverted indexes with full positional information, being up to around 8000 times faster than sequentially scanning the original collection. The more compact indexes, such as the block addressing IIs, the wavelet-trees, and iCSA yield different interesting space/time trade-offs.

According to the results of our experiments in the most repetitive collection, we expect that the WTBC-based indexes will obtain better results when applied to rather stable collections, as expected in the case of SmartGrid or stock exchange mar-
To sum up, if one aims at obtaining a good search speed with reasonable (high) space consumption, the II is the clear win-ner. Obviously, it is only beaten by the plain representation in the extract operation. However, if one is mainly concerned ested in fast extraction of the original data, and reasonably good space requirements and searches, the block-addressing IIs are a good choice. Finally, considering a general scenario, WTBC, is probably the best choice. It obtains good compression (10 X 15% less space than the original data), it yields fairly good performance in search operations, and is reasonably fast at extract operation.
 Acknowledgments y Competitividad [TIN2013-46238-C4-3-R and TIN2013-47090-C3-3-P]; CDTI, AGI, and Ministerio de Econom X a y Competi-tividad (Spain) [CDTI-00064563/ITC-20133062]; and (for Alberto Ord X  X ez) by Ministerio de Ciencia e Innovaci X n [AP2010-6038 (FPU Program)].
 References
