 Sapienza University of Rome Sapienza University of Rome
Web search result clustering aims to facilitate information search on the Web. Rather than the a different meaning of the input query, thus taking into account the lexical ambiguity (i.e., polysemy) issue. Existing Web clustering methods typically rely on some shallow notion of textual similarity between search result snippets, however. As a result, text snippets with no word in common tend to be clustered separately even if they share the same meaning, whereas snippets with words in common may be grouped together even if they refer to different meanings of the input query.
 automatic discovery of word senses from raw text, a task referred to as Word Sense Induction.
Key to our approach is to first acquire the various senses (i.e., meanings) of an ambiguous induced. Our experiments, conducted on data sets of ambiguous queries, show that our approach outperforms both Web clustering and search engines. 1. Introduction
The Web is by far the largest information archive available worldwide. This vast pool of text contains information of the most wildly disparate kinds, and is potentially capable of satisfying virtually any conceivable user need. Unfortunately, however, in this setting retrieving the precise item of information that is relevant to a given user search can be like looking for a needle in a haystack. State-of-the-art search engines such as Google and Yahoo! generally do a good job at retrieving a small number of relevant results from
Such systems today, however, still find themselves up against the lexical ambiguity issue (Furnas et al. 1987; Navigli 2009), that is, the linguistic property due to which a single word may convey different meanings.
 (Miller et al. 1990; Fellbaum 1998) and Wikipedia 1 queries are ambiguous (Sanderson 2008), as also confirmed in later studies (Clough et al. 2009; Song et al. 2009). An example of an ambiguous query is Butterfly effect , which could refer to either chaos theory, a film, a band, an album, a novel, or a collection of poetry. Similarly, black spider could refer to either an arachnid, a car, or a frying pan, and so forth.
 solved by expanding the initial query with unequivocal cue words. Interestingly, the average query length is continually growing. The average number of words per query is now estimated around three words per query, 3 a number that is still too low to eradicate polysemy.
 been tackled by diversifying search results, an approach whereby a list of heterogene-nous results is presented, and Web pages that are similar to ones already near the top are prevented from ranking too highly in the list (Agrawal et al. 2009; Swaminathan,
Mathew, and Kirovski 2009). Today even commercial search engines are starting to rerank and diversify their results. Unfortunately, recent work suggests that diversity
Artiles 2010), but it undoubtedly has the potential to do so (Chapelle, Chang, and Liu 2011).
 ing engines (Carpineto et al. 2009), such as Carrot 4 and Yippy. search results by providing a cluster for each specific meaning of the input query. Users can then select the cluster(s) and the pages therein that best answer their information needs. These approaches, however, do not perform any semantic analysis of search results, clustering them solely on the basis of their lexical similarity.
 the snippets reported in Table 1. 6 In the third column of the table we provide the correct meanings associated with each snippet (i.e., either the operating system or the animal sense). Although snippets 2, 4, and 5 all refer to the same meaning, they have no content word in common apart from our query words. As a result, a traditional Web clustering engine would most likely assign these snippets to different clusters. Moreover, snippet 6 shares words with snippets referring to both query meanings (i.e., snippets 1, 2, and 3 in Table 1), thus making it even harder for Web clustering engines to group search 710 results effectively. Finally, none of the top-ranking snippets refers to The Snow Leopard , a popular 1978 book by Peter Matthiessen.
 explicitly addresses the language ambiguity issue. Key to our approach is the use of
Word Sense Induction (WSI), that is, techniques aimed at automatically discovering the different meanings of a given term (i.e., query). Each sense of the query is represented as a cluster of words co-occurring in raw text with the query. Each search result snippet returned by a Web search engine is then mapped to the most appropriate meaning (i.e., cluster) and the resulting clustering of snippets is returned.

Di Marco and Navigli 2011) by performing a novel, in-depth study of the interactions between different corpora and several different WSI algorithms, including novel ones, within the same framework, and, additionally, by providing a comparison with a state-of-the-art search result clustering engine.

Section 3 we illustrate our approach, end-to-end experiments are reported in Section 4, analysis in Section 6, and conclude the paper in Section 7. 2. Related Work
Our work is aimed at addressing the difficulties arising within the different approaches to the issue of lexical ambiguity in Web Information Retrieval. Given the large body the topic. 2.1 Web Directories
In Web 1.0 X  X ainly based on static Web pages X  X he solution to clustering search results was that of manually organizing and categorizing Web sites. The resulting repositories are called Web directories and list Web sites by category and possible subcategories. These categories are sometimes organized as taxonomies (like in the Open Directory Project, ODP 7 ).
 therein. So, given a query, the returned search results are organized by category. For instance, given the query snow leopard the ODP returns the categories shown in Table 2 712 (the number of matching Web pages is reported in the second column). As can be seen from this example, the Web directory approach has evident limits: 1. It is static, thus it needs manual updates to cover new pages and new 2. It covers only a small portion of the Web (e.g., we only have one Web page 3. It classifies Web pages using coarse categories. This latter feature of tackled effectively (Bennett and Nguyen 2009), these approaches are usually supervised and still suffer from reliance on a predefined taxonomy of categories. Finally, it has been reported that directory-based systems are among the most ineffective solutions to Web information retrieval (Bruza, McArthur, and Dennis 2000). 2.2 Semantic Information Retrieval
A second approach to query ambiguity consists of associating explicit semantics (i.e., word senses or concepts) with queries and documents, that is, performing Semantic
Information Retrieval (SIR). SIR is performed by indexing and searching concepts rather than terms, that is, by means of Word Sense Disambiguation (WSD; Navigli 2009), thus potentially coping with two linguistic phenomena: expressing a single meaning with different words ( synonymy ) and using the same word to express various different meanings ( polysemy ). The main idea is that assigning concepts to words can potentially overcome these two issues, enabling a shift from the lexical to the semantic level to be achieved.
 1992; Voorhees 1993; Mandala, Tokunaga, and Tanaka 1998; Gonzalo, Penas, and Verdejo 1999; Kim, Seo, and Rim 2004; Liu, Yu, and Meng 2005, inter alia). Contrasting results have been reported on the benefits of these techniques, however: It has been shown that WSD has to be very accurate to benefit Information Retrieval (Sanderson 1994) X  X  result that was later debated (Gonzalo, Penas, and Verdejo 1999; Stokoe, Oakes, and Tait 2003). Also, it has been reported that WSD has to be very precise on minority senses and uncommon terms, rather than on frequent words (Krovetz and Croft 1992; Sanderson 2000).
 to perform WSD (typically, WordNet) and thus suffers this dictionary X  X  static nature and its inherent paucity of most proper nouns. This latter problem is particularly important for Web searches, as users tend to retrieve more information about named entities (e.g., singers, artists, cities) than concepts (such as abstract information about singers or artists). Although lexical knowledge resources that integrate lexicographic senses with named entites on a large scale have recently been created (Navigli and Ponzetto 2012), it is still to be shown that their use for SIR is beneficial. Moreover, these resources do not yet tackle the dynamic evolution of language.
 both lexicographic and encyclopedic senses of a query (including new ones), thus taking into account all of the mentioned issues. 2.3 Search Result Clustering
A more popular approach to query ambiguity is that of search result clustering .Typ-one or more commonly available search engines and clusters them on the basis of some notion of textual similarity. At the root of the clustering approach lies van Rijsbergen X  X  cluster hypothesis (van Rijsbergen 1979, page 45):  X  X losely associated documents tend to be relevant to the same requests, X  whereas results concerning different meanings of the input query are expected to belong to different clusters.
 description-centric (Carpineto et al. 2009). The former focus more on the problem of of clusters and, after the selection of a group, performs clustering again and proceeds iteratively. Developments of this approach have been proposed that improve on cluster quality and retrieval performance (Ke, Sugimoto, and Mostafa 2009). Other data-centric approaches use agglomerative hierarchical clustering (e.g., LASSI [Maarek et al. 2000]), rough sets (Ngo and Nguyen 2005), or exploit link information (Zhang, Hu, and Zhou 2008).
 produce for each cluster of search results. Among the most popular and successful approaches are those based on suffix trees. Suffix trees are rooted directed trees that contain all the suffixes of a string s . The label of each edge is a non-empty substring of considered as a set of documents that share a phrase (i.e., the label of the vertex itself) and therefore the vertices represent a set of base clusters B = ( b
Suffix Tree Clustering (STC; Zamir et al. 1997; Zamir and Etzioni 1998) algorithm obtains the final clustering by merging the clusters in B with a high overlap in the documents they contain. A scoring function is defined, based on both the number of documents in the base cluster and the length of the common phrase, with the aim of returning only the top k clusters.
 document X  X ocument similarity scores in order to overcome the low scalability of the original approach (Branson and Greenberg 2002). Crabtree, Gao, and Andreae (2005) identified an issue in the original scoring function whereby unreasonably high scores base clusters. To solve this problem, they proposed the Extended Suffix Tree Clustering algorithm (ESTC) with a novel scoring function and a new procedure for selecting the top k clusters to be returned.
 714 (Bernardini, Carpineto, and D X  X mico 2009; Carpineto, D X  X mico, and Bernardini 2011). are based on formal concept analysis (Carpineto and Romano 2004), singular value decomposition (Osinski and Weiss 2005), spectral clustering (Cheng et al. 2005), spectral geometry (Liu et al. 2008), link analysis (Gelgi, Davulcu, and Vadrevu 2007), and graph connectivity measures (Di Giacomo et al. 2007). Search result clustering has also been viewed as a supervised salient phrase ranking task (Zeng et al. 2004).
 use of lexical semantics, in our work we show how to exploit search result clustering as the common evaluation framework of both semantic and non-semantic clustering engines. 2.4 Diversification
Rather than clustering the top search results by their similarity, one can aim at reranking them on the basis of criteria that maximize their diversity, so as to present top results which are as different from each other as possible. This technique, called diversification of search results, is a recent research topic that, yet again, deals with the query ambiguity issue. To some extent, today X  X  search engines, such as Google and Yahoo!, apply some diversification technique to their top-ranking results.
 similarity functions to measure the diversity between documents and between docu-ment and query (Carbonell and Goldstein 1998). Other diversification techniques use conditional probabilities to determine which document is most different from higher-ranking ones (Chen and Karger 2006), or use affinity ranking (Zhang et al. 2005), based on topic variance and coverage.
 has been proposed that aims to reduce information redundancy and returns Web pages that maximize coverage with respect to the input query. In this approach the Web search results for a query q are transformed into bags of words containing the terms occurring in the corresponding Web page. Frequency information from raw corpora is then used to find relevant words for q , that is, words which are generally infrequent, but occur often in the results retrieved for q . The coverage score of a search result r is then calculated as a function of the number of terms relevant for q and contained in greedy algorithm is proposed that balances between relevance and diversity of the search results. The algorithm is evaluated using generalizations of classical Information
Retrieval metrics that are based on statistical considerations and take into account the intentions of the user.
 diversity in search results (Santamar  X   X a, Gonzalo, and Artiles 2010). Web page results are represented as vectors and compared against vector representations of encyclopedic entries available from Wikipedia using cosine similarity. Search results are diversified accordingly.
 to perform diversification, as proposed by Ma, Lyu, and King (2010), who make use of
Markov random walks on query-URL bipartite graphs, and Chandar and Carterette identify the subtopics of the returned documents. 2.5 Word Sense Induction
A fifth solution to the query ambiguity issue is Word Sense Induction (WSI), namely, the automatic discovery of word (i.e., query) senses from raw text (see Navigli [2009, 2012] for a survey). WSI allows us to go beyond the surface similarity of Web snippets (which hampers the performance of Web search result clustering) by dynamically acquiring an inventory of senses of the input query. The core idea is to then use these query senses to cluster the Web snippets returned by a traditional search engine.
 improve bag-of-words ad hoc Information Retrieval (Sch  X  utze and Pedersen 1995) and preliminary studies (Udani et al. 2005; Chen, Za  X   X ane, and Goebel 2008) have provided interesting insights into the use of WSI for Web search result clustering. A more recent attempt at automatically identifying query meanings is based on the use of hidden topics (Nguyen et al. 2009). In this approach, however, topics (estimated from a uni-versal data set) are query-independent and thus their number needs to be established beforehand. In contrast, we aim to cluster snippets on the basis of a dynamic and finer-grained notion of sense.
 present work we take this preliminary finding to the next level, by studying the impact of several graph-based WSI algorithms on a large scale and by integrating them into a
Web search result clustering framework. As a result, we are able not only to perform an end-to-end evaluation of WSI approaches, but also to compare them with traditional search result clustering techniques, which instead lack explicit semantics for the query meanings. 2.6 Aspect Identification
Retrieval that makes use of query logs and clickthrough information to identify and model the aspects of a given query in terms of the user intents for that query. Aspects can be identified by exploiting those queries in the past that enabled the user to retrieve documents that are close to the current input query (Wang and Zhai 2007). A different approach aims, instead, at extracting related queries from query logs as candidate aspects and discarding duplicate and redundant aspects using search results. Wikipedia
InfoBoxes are used to cluster candidate aspects into classes (Wu, Madhavan, and Halevy 2011). Latent aspects of queries can also be extracted from query reformulations within historical search session logs (Wang, Chakrabarti, and Punera 2009). More recently, a topic modeling approach based on query logs and click data has been proposed that aims at discovering generic aspects pervading manually fixed categories of named entities (Xue and Yin 2011). The implicit user-specific aspect of a query can be obtained from short query log sessions of other users using a Markov logic learning model. This results in the documents that best model the user X  X  intentions when entering a query (Mihalkova and Mooney 2009). Finally, a semi-supervised approach has recently been applied to create class labels that are later assigned to latent clusters of queries using a
Hierarchical Dirichlet Process (Reisinger and Pasca 2011). 716 differences: future work, in the hope that the previously mentioned issues of privacy and availability will somehow be mitigated. 3. Semantically Enhanced Search Result Clustering
Web search result clustering is usually performed in three main steps: 1. Given a query q , a search engine is used to retrieve a list of results 2. A clustering C = ( C 1 , ... , C m )oftheresultsin R is obtained by means 3. The clusters in C are optionally labeled with an appropriate algorithm (Section 3.1). Next, to inject semantics into search result clustering, we propose improving Step 2 by means of a WSI algorithm: Given a query q , we first dynamically induce, from a text corpus, the set of word senses of q (Section 3.2); next, we cluster the
Web results on the basis of the word senses previously induced (Section 3.3). We show our framework in Figure 1. 3.1 Preprocessing of Web Search Results
As a result of submitting our query q to a search engine, we obtain a list of relevant each result r i is processed by means of four steps aimed at transforming it into a bag of words b i : 1. We obtain the snippet s i corresponding to the result r 2. We apply tokenization to s i , thus splitting the string into tokens and 3. We augment the current token set with multi-word expressions obtained 4. We remove the stopwords (e.g., get , on , be , as ) and the target query words
An example of the application of the four steps to a snippet returned for the query snow leopard is shown in Table 3. As a result of this process, we obtain a list of bags of words
B = ( b 1 , ... , b n ), where b i is the bag of words of the search result r 3.2 Graph-Based Word Sense Induction
The next step is to dynamically discover the senses of the input query q and provide a representation for them that will later be used for semantically clustering the snippets preprocessed in the previous step. WSI algorithms are unsupervised techniques aimed at automatically identifying the set of senses denoted by a word. These methods induce word senses from text by clustering word occurrences on the basis of the idea that a 718 given word X  X sed in a specific sense X  X ends to co-occur with the same neighboring words (Harris 1954). Several approaches to WSI have been proposed in the literature (see Navigli [2009, 2012] for a survey), ranging from clustering based on context vectors (Brody and Lapata 2009), latent semantic models (Van de Cruys and Apidianaki 2011), and co-occurrence graphs (e.g., Widdows and Dorow 2002).
 two reasons: ii) Other approaches are either based on syntactic dependency statistics (Lin We therefore integrated the following algorithms into our framework: occurrence graph construction (described in Section 3.2.1) and a second step, namely, the discovery of word senses, whose implementation depends on the specific algorithm adopted. We discuss the second phase of each algorithm separately (Section 3.2.2). 3.2.1 Step 1: Graph Construction. Given a target query q , we build a co-occurrence graph
G = ( V , E )suchthat V is the set of words 8 co-occurring with q ,and E is the set of undirected edges, each denoting a co-occurrence between pairs of words in V .We harvest the statistics for co-occurring words V from a text corpus (we used two different corpora, see Section 4.1.2), which was previously tokenized and lemmatized. the number of times c ( w , w )that w occurs together with some word w in the same context (to this end, we use the lemmas corresponding to inflected forms in the text).
For instance, in Table 4, assuming w = lion , we show the absolute count c ( w )ofsome words (second column) together with the joint co-occurrence count c ( w , w )ofwords w occurring with w = lion in the same context (third column). Note that the co-occurrences w may refer to different senses of word w  X  X or example, africa and savannah refer to the animal sense of lion , whereas technology and software to the operating system sense.
Moreover, w may be ambiguous itself in the context of w (e.g., tiger as either an animal or an operating system).
 between any two words w and w : 9 simple co-occurrence count such as c ( w , w ), is that dividing by the average of the total 720 counts of the two words drastically decreases the ranking of words that tend to co-occur frequently with many other words ( home , page ,etc.).
 graph construction procedure is shown in Algorithm 1 and consists of the following steps: a. Initialization with snippet words ( lines 1 X 2 ) : Initially we set V to b. Adding first-order co-occurrences ( lines 3 X 5 ) : We augment V with the c. Adding second-order co-occurrences ( lines 6 X 11 ) : Optionally, we create an Algorithm 1 The graph construction algorithm.
 d. Creating the co-occurrence graph ( lines 12 X 15 ) : For each pair of words duced. Consider again the target word lion and let us assume that the words in
Table 4 are the only co-occurrences of lion . In Figure 2 we show the execution of the four steps of our graph construction algorithm for the input query lion , assuming simulation simulation 722 words in the snippets returned for lion (Figure 2a), next we add the words co-occurring with the query (Figure 2b), then second-order co-occurrences, that is, words co-occurring with those just added to the graph (Figure 2c), and finally we add those edges between word pairs whose Dice value is above a threshold (Figure 2d). 3.2.2 Step 2: Sense Discovery. All the graph-based WSI algorithms that we implemented in our framework are designed to discover the senses of an input term, which in out through the use of the relational and structural information contained in the co-occurrence graph we have just created. In fact, a co-occurrence graph G a query q contains: (i) vertices w  X  V corresponding to words highly related to q ,and (ii) edges e  X  E representing co-occurrence relations between vertices (i.e., words) in V .
The key idea behind graph-based WSI is to obtain a partition S = ( S vertices (i.e., words). In other words, each vertex set V related to a specific sense of q .Asaresult S is the sense inventory for the query q and each S i is a sense cluster.

Curvature. The curvature algorithm aims at quantifying how strongly the neighbors of a vertex are related to each other. To measure this degree of correlation, the curvature coefficient for a vertex w is calculated as follows: where a triangle is a cycle of length 3. The numerator of Equation (4) is trivially calcu-lated as the number of links between neighbors of w , and the denominator is calculated by counting all the possible pairs of neighbors. According to Equation (4), the curvature coefficient can assume values between 0 and 1. A vertex whose neighbors are highly connected (i.e., with a high value of curvature) is assumed to be part of a component that represents a specific meaning of the target query. Conversely, a vertex with low curvature acts as a connection between different meanings.
 of the removal of all vertices whose curvature is below a certain threshold  X  . For ex-ample, we can attribute two different meanings to the word Napoleon , namely, a French emperor and an American city. By looking at the graph in Figure 3 we can easily find that Napoleon participates in two triangles (represented by continuous lines) and it po-tentially could also participate in four additional triangles (i.e., those including dashed results in two components (respectively, containing the vertices and { Ohio , America } ) representing the two mentioned meanings.

SquaT++. The curvature clustering algorithm is based on the hunch that local connec-proposing a more elaborate local connectivity approach that exploits three different graph patterns, namely: triangles (i.e., cycles of length 3, like in curvature clustering), squares (i.e., cycles of length 4) and diamonds (i.e., graphs with 4 vertices and 5 edges, forming a square with a diagonal), hence the name SquaT++ (Squares, Triangles, and occurrence graph as follows: where w is a vertex. Then we linearly combine the three measures as follows: where  X  +  X  +  X  = 1. Similarly to the curvature algorithm, the sense clusters are ob-tained by removing all those vertices whose SquaT++ value is below a threshold  X  .In
Figure 4(a) we show in bold the vertices selected for removal, and in Figure 4(b) the sense clusters obtained after removal, namely: { videogame , simulation , software apple , mac } ,and { cat , animal , predator , africa , savannah pattern to calculate curvature, and (ii) it disconnects the graph using the same algorithm as curvature. SquaT++ is a novel algorithm, however, that extends the previously proposed SquaT (Navigli and Crisafulli 2010), based on triangles and squares, by in-troducing a new pattern, namely, the diamond, whose clustering coefficient is linearly combined with the other two. Moreover, in our experiments we tested two versions of SquaT++: the traditional one in which the coefficient is calculated on vertices (like in
Equation (8)), and a variant calculated on edges. Our hunch here is that removing low-ranking edges rather than vertices might produce more informative clusters, because no word is removed from the original graph. In what follows, we refer to the vertex version as SquaT++ V and to the variant on edges as SquaT++ general algorithm as SquaT++.
 B-MST. A more global approach to the identification of sense components is the
Balanced Maximum Spanning Tree (B-MST), which is based on the computation of 724 the Maximum Spanning Tree (MST) of the co-occurrence graph. Cluster meanings are identified by iteratively removing the edges which represent structurally weak rela-tions, i.e., those with lower weight in the MST. The procedure is as follows:
B-MST (i.e., the maximum number N of meanings to be identified) is set to 3, we obtain the clusters in Figure 4(d).

HyperLex. Another option for sense discovery is that of HyperLex, which identifies the most interconnected vertices in the graph G q , called hubs . Each hub acts as the  X  X oot X  of a specific component of G q and, correspondingly, a meaning of the target query q . count c ( w ) in decreasing order. Each vertex w  X  L is then selected as hub if it satisfies the following conditions: cident on w must be, respectively, above the thresholds  X  and  X  . Once it has been selected, the hub and all its neighbors are removed from L so as to avoid neighboring next vertex in the sorted list does not satisfy either of the Equations (9) or if the list L is empty.
 in the graph is created, sorted by c ( w ), as shown in Table 4. For the purpose of our 726 example, let us assume  X  = 0 . 5and  X  = 0 . 015. The first hub to be selected is animal . removed from the list. The next hub is videogame (its neighbors simulation , software ,and technology are also removed from the list). The last hub is mac ; after the removal of its neighbor from the list ( apple ) the last vertex to be examined is iPod , which cannot be selected as hub because it does not satisfy the second condition of Equation (9). The selected hubs are shown as rectangles in Figure 4(e).
 of vertices V of graph G q and each hub is connected to q with an infinite-weight edge spanning tree T q of the graph is calculated starting from vertex q (see the bold edges in Figure 4(e)). As a result, T q will include all the infinite-weight edges from q to its direct descendants, namely, the hubs. Vertex q is then removed from the graph so that each subtree rooted at a hub in T q represents a word sense for the target query q (see
Figure 4(f)). In our example, three clusters are produced:
Note that, in our example, HyperLex and SquaT++ found the same meanings for the query word lion (namely, the animal, the operating system, and the videogame), but produced different clusters (e.g., HyperLex assigns the word tiger to the animal cluster whereas SquaT++ removes it from the graph). Finally, notice that in HyperLex the number of senses is dynamically chosen on the basis of the co-occurrences of q and the algorithm X  X  thresholds.
 using the PageRank algorithm to sort the vertices of the co-occurrence graph and choose the best ranking ones as hubs of the target word (Agirre et al. 2006b). Given that the performance of this variant is comparable to that of HyperLex, in this work we focus on the original version of the induction algorithm.

Chinese Whispers. All the previously presented algorithms work in a top X  X own fashion, until a number of partitions are obtained. The last algorithm we consider, called Chinese
Whispers, works, instead, bottom X  X p. The pseudocode, shown in Algorithm 2, consists of the following two steps: 1. First, the algorithm assigns a distinct class i to each vertex v 2. Second, a series of iterations is performed aimed at merging the clusters Algorithm 2 The Chinese Whispers algorithm.
 stops and outputs the final clustering (line 12). In contrast to the previous algorithm,
Chinese Whispers is parameter-free. Figure 4(g) shows an output example for this algorithm on the lion co-occurrence graph. 3.3 Clustering of Web Search Results
We are now ready to semantically cluster our Web search results R , which we previously transformed into bags of words B (cf. Section 3.1). To this end we use the automatically discovered senses for our input query q (cf. Section 3.2). We adopt different measures, each of which calculates the similarity between a bag of words b clusters { S 1 , ... , S m } acquired as a result of Word Sense Induction. meaning of r i . Formally: where sim ( b i , S j ) is a generic similarity value between b is assigned to result r i ). As a result of sense assignment for each r clustering C = ( C 1 , ... , C m )suchthat: that is, C j contains the search results classified with the j -th sense of query q . and sense clusters (cf. Equation (11)), which we implemented in our framework. 728
Word Overlap. It calculates the size of the intersection between the two word sets: where S j = ( V j , E j ) as defined in Section 3.2.2.

Degree Overlap. It calculates the sum of the degrees in the co-occurrence graph compo-nent of S j of the snippet X  X  words in b i : where degree ( w , S j ) is the number of edges incident on w in the S co-occurrence graph.

Token Overlap. The third measure is similar in spirit to Word Overlap, but takes into account each token occurrence in the snippet bag of words b where c ( w , r i ) is the number of occurrences of the word w in the result r 3.4 Cluster Sorting
As a natural consequence of the different similarity values between snippet results and a given cluster, first, not all the snippets will have the same degree of relevance for the cluster, and second, the produced clusters will show a different  X  X uality X  depending on the relevance of the search results therein. We thus sort the clusters in our clustering
C using a similarity-based notion of cluster  X  X uality. X  For each cluster C termine its similarity with the corresponding meaning S j formula: the search results r i in cluster C j and the corresponding sense cluster S function sim is the same as that stated in Equation (11) and defined in Section 3.3.
We note that the ranking and optimality of clusters can be improved with more sophis-ticated techniques (e.g., Crabtree, Gao, and Andreae 2005; Kurland 2008; Kurland and
Domshlak 2008; Lee, Croft, and Allan 2008). This is beyond the scope of this article, however. 4. In Vivo Experiments: Web Search Result Clustering
We now present two extrinsic experiments aimed at determining the impact of WSI when integrated into Web search result clustering. We first describe our experimental set-up (Section 4.1). Next, we present a first experiment focused on the quality of the diversification of semantically enhanced versus non-semantic search result clustering algorithms (Section 4.3). 4.1 Experimental Set-up 4.1.1 Lexicon. In all our experiments our lexicon was given by the entire WordNet vocabulary (Miller et al. 1990; Fellbaum 1998) augmented with the set of queries in our test data sets. 4.1.2 Corpora. To calculate the co-occurrence strength between words we need a large corpus to extract co-occurrence counts and calculate the Dice values (cf. Equation (1)). To this end we performed separate experiments on two different corpora and constructed the corresponding co-occurrence databases:
Web1T is a very large corpus, but with very narrow contexts (5-grams) with a mini-mum occurrence frequency; ukWaC represents a smaller portion of the Web, but with larger contexts. This enabled us to observe the behavior of WSI algorithms when co-occurrences were extracted from different kinds of textual source. In Table 5 we show examples of the contexts available in the two corpora for the same word (i.e., lion )and the content words that are found to co-occur with it (shown in italics in Table 5). 730 4.1.3 Tuning Set. Given that our graph construction step and our WSI algorithms have parameters, we created a data set to perform tuning. In order to fix the parameter values independently of our application we created this data set by means of pseudowords (Sch  X  utze 1992; Yarowsky 1993). A pseudoword is an ambiguous artificial word created by concatenating two or more monosemous words. Each monosemous word represents a meaning of the pseudoword. For example, given the words pizza and blog we can create the pseudoword pizza*blog . The list of pseudowords we used is reported in Table 6.
 tion of sense-tagged corpora with virtually no effort. In fact, we automatically created our tuning data set as follows: 1. First, we collected the top 100 results retrieved by Yahoo! for each meaning 2. We created a set of 100 snippets for the  X  X seudoword X  query (e.g., 3. Finally, within each of the 100 snippets, we replaced each monosemous 4.1.4 Parameters. We used our tuning set to select, first, the optimal values of the pa-rameters needed to perform graph construction, and, second, to choose the parameter values specific to each graph-based WSI algorithm. To find the best configurations we performed tuning by combining the three evaluation measures of Adjusted Rand Index,
Jaccard Index, and F1 (introduced in Section 4.2.1).
Graph construction. Because all our WSI algorithms draw on the co-occurrence graph, we first tuned the parameters for graph construction for each of the two corpora (cf. Section 3.2.1), namely: the maximum length of the compounds extracted from the corpus (  X  ), the minimum number of co-occurrences (  X  ) and minimum Dice value (  X  ) for vertex addition, and the minimum weight for a graph edge (  X  ) and vertex addition using first versus second-order co-occurrences. In Table 7 we show the values for these parameters that optimize the performance of each WSI algorithm on the two corpora.
In all our runs we used the Word Overlap as a similarity measure for Web search result clustering.
 construction were stable across algorithms, whereas they changed across corpora due to the different scales of the two corpora. Instead, the maximum compound length and the co-occurrence order were fixed for all configurations. For the former we observed no performance increase with longer compound lengths. For the latter we found negligible improvements with second-order co-occurrences, at the cost, however, of increasing the size of the resulting graph exponentially. Given the large number of experiments that would be involved, we decided to avoid this additional workload and use first-order co-occurrences in all our experiments.

WSI algorithms. Next, for each graph-based WSI algorithm, we kept the given optimal varying the parameter values of the WSI algorithm, using Word Overlap as similar-ity measure for Web search result clustering. In Table 8 we show the optimal values for each algorithm when using Web1T (third column) and ukWaC (fourth column) to build the co-occurrence graph. Chinese Whispers is not shown as it is parameter-free (cf. Section 3.2.2). For SquaT++, together with the  X  threshold, we also tuned the three coefficient values  X  ,  X  ,and  X  , that is, we needed to find the best values for the coefficients in Equation (8). The optimal coefficient combinations are shown in Table 9 for SquaT++ on vertices and edges, when using the two corpora for graph construction.
The values indicate that all the three graph patterns provide a positive contribution to the algorithm X  X  performance, with the same coefficients for SquaT++ on vertices and 732 edges. Interestingly, we observe that, whereas the contribution of triangles (weighted by  X  ) is the same across corpora, the respective weights of squares (  X  ) and diamonds (  X  ) are flipped. After inspection we found that the graphs obtained with Web1T are less interconnected than those produced with ukWac. Consequently, diamonds are sparser but more reliable in the Web1T setting, whereas they are much more frequent, and thus noisier, in the ukWaC setting. 4.1.5 Test Sets. We conducted our in vivo experiments on two test sets of ambiguous queries: and Pasca (2006), Mihalcea (2007), and Gabrilovich and Markovitch (2009). Santamar  X   X a,
Gonzalo, and Artiles (2010) have investigated in depth the benefit of using Wikipedia much more sense coverage for search results than other resources such as WordNet. that the snippets could possibly be annotated with more than one Wikipedia subtopic, we also determined the average number of subtopics per snippet. This amounted to 1.01 for AMBIENT and 1.04 for MORESQUE for snippets with at least one subtopic annotation. We can thus conclude that multiple subtopic annotations are infrequent.
Finally, we analyzed how the different subtopics are distributed over the snippet results for each query. To do this we calculated the standard deviation of the subtopic popula-tion for each individual query, which we show in Figure 5. We observed a considerable difference in the standard deviations of shorter and longer queries (e.g., between those from the AMBIENT data set [from 1 to 44 in the figure] and the MORESQUE data set [from 45 to 158]). We further calculated the average standard deviation over the two data sets X  queries, obtaining 6.5 for AMBIENT and 13.1 for MORESQUE. Therefore we anticipate that the longer the query length, the more unbalanced will be the distribution of its subtopics over the top-ranking results.
 not contain monosemous queries for two reasons: (i) we are interested in queries with multiple meanings, and (ii) monosemous queries would increase the performance of our experiments because no diversification would be needed for them. 4.1.6 Systems. We performed a comparison of our semantically enhanced search result clustering systems with nonsemantic ones. 734
Semantically enhanced systems. We integrated our graph-based WSI algorithms (Curva-ture, SquaT++, B-MST, HyperLex, and Chinese Whispers; cf. Section 3.2) into our search result clustering framework. We tested each algorithm when combined with any of the snippet-to-sense similarity measures introduced in Section 3.3.

Nonsemantic systems. We compared our semantically enhanced systems with four Web clustering engines, namely: our framework. Conversely, for Yippy we used the on-line output provided by the Web search engine. 4.1.7 Baselines. We compared the four systems against three baselines: a bias towards very small (singletons) or big clusters (all-in-one). The third baseline, based on Wikipedia, is a tough one in that X  X n contrast to our systems X  X t relies on a predefined sense inventory (which is the same as that used in the manual classification the senses, but just classifies (or labels) each snippet with the best-matching Wikipedia sense of the input query. 4.2 Experiment 1: Evaluation of the Clustering Quality of the output produced by our search result clustering systems. Unfortunately, the clustering evaluation problem is a notably hard issue, and one for which there exists no unequivocal solution. Many evaluation measures have been proposed in the literature (Rand 1971; Zhao and Karypis 2004; Rosenberg and Hirschberg 2007; Geiss 2009, inter alia) so, in order to get exhaustive results, we tested three different clustering quality measures, namely, Adjusted Rand Index, Jaccard Index, and F1-measure, which we introduce hereafter. Each of these measures M ( C , G ) calculates the quality of a clustering
C , output for a given query q , against the gold standard clustering then determine the overall results on the entire set of queries Q in the test set according to the measure M by averaging the values of M ( C , query q  X  Q .

Adjusted Rand Index. Given a gold standard clustering G , the Rand Index (RI; Rand 1971) of a clustering C is a measure of clustering agreement commonly used in the literature, calculated as follows: where TP is the number of true positives (i.e., snippet pairs) that are in the same cluster both in C and G , TN is the number of true negatives (i.e., pairs which are in different positives and false negatives. For the gold standard G we use the clustering induced by the sense annotations provided in our data sets for each snippet (i.e., each cluster subtopic, of the query). 736 figuration in both C and G , but its main weakness is that it does not take chance into account. In fact, the expected value of the RI of two random clusterings is not a constant value (e.g., 0). This issue is addressed by the Adjusted Rand Index (ARI; Hubert and
Arabie 1985), which corrects the RI for chance agreement and makes it vary according to expectation: where E ( RI ( C , G )) is the expected value of the RI. Given two clusterings and G = ( G 1 , G 2 , ... , G g ), we first quantify the degree of overlap between the contingency table reported in Table 11, where n ij denotes the number of objects in common between G i and C j (i.e., n ij = | G i  X  C j | )and a the number of objects in G i and C j . Now, Equation (18) can be reformulated as follows (Steinley 2004): between  X  1and + 1 and is 0 when the index equals its expected value. Given the issues with RI, in our experiments we focused on ARI.
 Jaccard Index. The ARI compares a clustering C with a gold standard different clusters (TN). There are typically many TN in a clustering, however; therefore this measure tends to overweight the usefulness of snippets placed in different clusters. The Jaccard Index (JI) is a measure that addresses this issue. JI is calculated as follows: nator of JI include the TN term.

F1-Measure. The ARI and the JI calculate the clustering quality using snippet pairs as the basic unit. Instead, a clustering C can be evaluated by focusing on the precision of the single clusters and the topics recalled by them, that is, we evaluate its precision (P) and recall (R) against a gold standard accurately the clusters of C represent the topics in the gold standard measures how accurately the topics in G are covered by the clusters in Andreae 2005): where t is the majority topic in C j for a given query, 17
C which are tagged with subtopic t in the gold standard G . The recall of a topic t is, instead, calculated as: where C t is the subset of clusters of C whose majority topic is t ,and n snippets tagged with subtopic t in the gold standard. The total precision and recall of the clustering C are then calculated as: where T is the set of subtopics in the gold standard G for the given query. The two values of P and R are then combined into their harmonic mean, namely, the F1 measure (van Rijsbergen 1979): consider untagged gold standard snippets. 4.2.2 Results and Discussion. We show the results of the WSI algorithms in Table 12.
With few exceptions, the results obtained on the two corpora are comparable. SquaT++, which extends Curvature with the Square and Diamond patterns, obtains higher perfor-mance. Although integrating three different graph patterns is beneficial, the difference between using edges and vertices to do so is mostly marginal.
 consistent across corpora and similarity measures (i.e., WO, DO, and TO), thus showing the robustness of the WSI algorithms when co-occurrences are extracted from different textual sources. The pairwise evaluation measures (i.e., ARI and JI), however, rank the 738
WSI algorithms differently from the F1 measure. In fact, when we focus on pairwise evaluation measures, SquaT++ outperforms all other systems on both corpora, with
Chinese Whispers ranking second. B-MST and HyperLex obtain lower results. When we look into the precision of the output clusters and the recall of the gold-standard topics (i.e., we calculate F1), however, we observe an inverse trend: HyperLex, B-MST and, to a lesser extent, Chinese Whispers achieve the best performance, whereas Curvature and
SquaT++ obtain lower F1. This is because, assuming comparable precision, producing more clusters (as is done by HyperLex, B-MST, and Chinese Whispers) implies more chances to obtain higher recall, thus better diversifying among the topics of the retrieved search results. More specifically, B-MST and, especially, HyperLex benefit from the use of ukWaC in terms of F1 performance, with HyperLex gaining around 5% when moving from Web1T to ukWaC.
 similarity measures (i.e., WO, DO, TO, cf. Section 3.3), with some exceptions concerning B-MST, HyperLex, and Chinese Whispers.
 those of nonsemantic systems (i.e., Lingo, STC, and KeySRC, cf. Section 4.1.6) and our three baselines (i.e., all-in-one, singleton, and Wikipedia, cf. Section 4.1.7). For the WSI algorithms we show the results when using the WO measure, because, first, DO uses graph information and thus cannot be applied to nonsemantic systems, and, second, in most cases (as remarked earlier) there are negligible differences between the two other similarity measures (i.e., WO and TO, see Table 12).
 other approaches across all evaluation measures on the two corpora, except for KeySRC and the singleton baseline when using the F1 measure. We note, however, that although
KeySRC outperforms the WSI algorithms based on graph patterns in terms of F1, it attains very low ARI and JI results. Even worse, the singleton baseline produces trivial, meaningless clusterings, as measured by ARI and JI. The all-in-one baseline, instead, obtains non-zero JI (thanks to the true positives taken into account), but again zero ARI.
Further, its F1 is lower than singleton, because of its lower recall. The Wikipedia baseline fares well compared with the other baselines in terms of ARI and JI, but achieves lower
F1, again because of low recall. Finally, KeySRC consistently outperforms the other SRC systems in terms of ARI and F1.
 version of our test set that retains only the Yahoo! results that were also returned by
Yippy. The average number of results over all queries in the resulting data set is 24.4, with a minimum and maximum number of 3 and 56 results per query, respectively. result clustering systems, Yippy performs worse in terms of ARI and JI. Instead, when we focus on the precision and recall of the output clusters, Yippy outperforms all other nonsemantic systems, while lagging behind all WSI algorithms (which use Web1T). One finding here is that, even in the presence of a smaller number of snippets per query, semantic systems perform best, whereas other approaches, which rely (like KeySRC) on the availability of a sufficient number of snippets, fall short. 4.3 Experiment 2: Evaluation of the Clustering Diversity 4.3.1 Evaluation Measure. Most of today X  X  search engines return a flat list of search results.
We thus performed a second experiment aimed at quantifying the impact of our Web search result clustering systems on flat-list search engines. In other words, our goal was 740 to determine how many different meanings of a query are covered in the top-ranking results shown to the user. One natural way of measuring such performance is given by S-recall@K (Subtopic recall at rank K ) and S-precision@r (Subtopic precision at recall r ) (Zhai, Cohen, and Lafferty 2003). S-recall@K counts the number of different sub-topics retrieved for q in the top K results returned: where subtopics ( r i ) is the set of subtopics manually assigned to the search result r m is the number of subtopics for query q in the gold standard. In order to cut out some noise, we calculated the S-recall@K considering only the subtopics assigned to at least two snippets.
 the first K r documents, where K r is the minimum number of top results for which the system achieves recall r . Formally:
S-precision@r quantifies the ratio of distinct subtopics covered by the minimal set of results returned for which the system obtains a specific recall r . Note that unambiguous queries would perform with S-precision@r = S-recall@K = 1 for all values of r and K . (such as Yahoo! and Essential Pages). In order to apply them to search result clustering systems, we flatten each clustering to a list of search results. To do so, given a clustering
C = ( C 1 , C 2 , ... , C m ), we add to the initially empty list the first element
C ( j = 1, ... , m ); then we iterate the process by selecting the second element of each cluster C j such that | C j | X  2, and so on. The remaining elements returned by the search engine, but not included in any cluster of C , are appended to the bottom of the list in their original order. 4.3.2 Results and Discussion. The results in terms of S-recall@K are shown in Table 15. The first key finding is that, independently of the adopted corpus for graph construction, each of the WSI algorithms outperforms all nonsemantic systems, including the state-of-the-art search resulting clustering engine (KeySRC), Essential Pages (see Section 2.4), and the Yahoo! baseline. This result provides strong evidence that inducing senses for all WSI algorithms perform the same, however: In fact, we observe that exploiting local graph patterns (as done by Curvature and SquaT++) typically leads to worse results compared with other graph-based approaches. We do not observe substantial differences between Curvature and SquaT++ on edges and vertices. We hypothesize pattern-based WSI algorithms is due to the lower number of clusters they produce (in the order of around two to three clusters, cf. Table 12). 742 by B-MST and Chinese Whispers. HyperLex is more complex and requires the tuning of many parameters (Agirre et al. 2006a), however. Interestingly, we observe that the ranking of WSI algorithms according to S-recall@K closely matches that obtained with the F1 measure for clustering quality. Finally, among the nonsemantic alternatives, Yahoo! fares well and surpasses KeySRC and EP.
 ure 6 we graphed the values of S-recall@K for representative systems, namely, B-MST,
SquaT++ V , and the three nonsemantic systems. The results shown in the figure are those obtained with Web1T (top) and ukWaC (bottom). We can see that SquaT++ behind B-MST especially for low values of K . As also remarked previously, Yahoo! tends to perform better than KeySRC.
 nonsemantic systems. The general trend observed for S-recall@K is confirmed here:
HyperLex generally achieves the best values of S-precision@r, with good performance for all other semantic systems. All in all, HyperLex has the best balance between recall and precision, with better diversification performance on ukWaC, and therefore looks like the most suitable choice. B-MST, however, is much simpler and requires just one parameter (i.e., the number of clusters), which can also be exploited by the user to get finer-or coarser-grained search result groups. As was previously done for S-recall@K, we also graphed the values of S-precision@r for the same representative systems in
Figure 7. 5. In Vitro Experiment: Evaluating the Induced Senses
Although the primary aim of this work was to demonstrate a relevant, end-to-end appli-cation of sense discovery techniques, we performed an additional in vitro experiment which they are used.
 clear hint as to which algorithm performs best (Manandhar et al. 2010). In fact, some
WSI algorithm will discover more fine-grained sense distinctions. To provide further insights into the clusters produced by our graph-based WSI algorithms, we performed 744 a qualitative evaluation of the output clusters. To this end we randomly selected 17 queries from our query data set. For each query, we submitted in random order the output of three representative WSI algorithms on the ukWaC corpus, namely, Curvature, HyperLex, and B-MST, to five annotators.

Table 17. On the left side of the table we propose an example of an anonymized set of three clusterings (i.e., one for each algorithm, shown in columns 2 X 4) presented to our annotators. Each algorithm produced a group of clusters, each of which consisted of a set of words strictly related to the meaning conveyed by the cluster itself, as discussed in
Section 3.2.2. The annotators were asked to rank the three clusterings according to their own preference (ties were allowed). On the right side of Table 17 we show an example of ranking for the three clusterings. In the example, clustering B was deemed to be more representative, because it better models three meanings of excalibur , namely: the film-novel meaning, the sword meaning, and the hotel casino meaning, whereas clustering A mixes the movie and the casino meaning within cluster 1, and, even worse, clustering C just provides a singleton cluster.
 ranking obtained by each WSI algorithm. The overall results are shown in Table 18 corroborates the findings obtained from our extrinsic experiments: Curvature is the worst-ranking system (probably because of the low number of induced senses), whereas
HyperLex and B-MST are more apt to discriminate between the meanings of an input query. It is worth noting that the annotators often assigned the same rank to the clus-ters produced by B-MST and HyperLex, confirming our extrinsic finding that the two algorithms tend to have a similar behavior, compared with local graph pattern WSI. 6. Time Performance Analysis
Finally, because we are interested in the real-world application of the WSI techniques we discussed, we decided to collect statistics about the execution times of each system on the AMBIENT and MORESQUE data sets. We carried out this performance analysis on a workstation using Sun Java 1.6 VM running on OpenSuse 11.4 (64 bit) with 16 GB PC3-15000 RAM, Intel Xeon E3-1240@3.30 GHz, and 1.5 TB hard disk space.
 80%) of the computational load is due to the interaction with the database management system (DBMS, we used MySQL 5.1), and the remaining CPU time is used for popu-lating the graph. On average constructing a co-occurrence graph takes 10 X 12 seconds per query. We note, however, that our algorithms were not engineered to work in an enterprise, possibly distributed, environment, with a commercial DBMS. Moreover, a fully engineered architecture might appropriately precalculate and cache the graphs concerning the most frequent queries. 746 snippet clustering (but excluding graph construction) are shown in Table 19, expressed in average number of seconds per query for both corpora. These numbers are compared with the time performance of nonsemantic systems (bottom part of the table). cost, due to the heavy calculation of three different graph patterns. SquaT++ particularly onerous in the presence of large amounts of edges, which is the case onerous to compute. Interestingly, the algorithms which we experimentally found to perform best (i.e., B-MST, HyperLex, and Chinese Whispers) have a much lower computational load compared with graph-pattern based algorithms. We found that
HyperLex is particularly fast, with an average time of 0.1 seconds per query. Finally, we observe that the cost of the best WSI algorithms is not very far off that of nonsemantic
SRC systems. 7. Conclusions
In this article we have presented a novel approach to Web search result clustering based on the automatic discovery of word senses from raw text. Key to our approach is the idea of, first, automatically inducing senses for the target query and, second, clustering the search results based on their semantic similarity to the word senses induced. already exists at the intersection between lexical semantics and information retrieval.
That research, however, has focused almost exclusively on classical Word Sense Dis-we provide clear indication on the usefulness of a looser notion of sense to cope with ambiguous queries.
 approach outperforms all nonsemantic approaches to Web search result clustering. The main advantage of using Word Sense Induction lies in its dynamic production of word senses that cover both concepts (e.g., beagle as a specific breed of dog) and instances (e.g., beagle as a specific instance of a space lander). This is in contrast with static dictionaries such as WordNet that are typically used in Word Sense Disambiguation and which, by their very nature, mainly encode concepts.
 clustering, surpasses its nonsemantic alternatives, but we have also provided an end-to-end evaluation framework that enables fair comparison of WSI algorithms. As a result, we are able to overcome many of the issues with the evaluation of clustering algorithms (von Luxburg, Williamson, and Guyon 2012), including the lack of a single unbiased intrinsic measure (Manandhar et al. 2010). Moreover, new WSI algorithms can be added at any time and compared with those already integrated into the framework. Building upon this, we are currently organizing a Semeval-2013 task for the extrinsic evaluation of WSI algorithms. 20 As of today, we are releasing a new data set of 114 ambiguous queries and 11,400 sense-annotated snippets. 21 Given the present paucity of ambiguous query data sets available (Sanderson 2008), we hope our data set will be useful in future comparative experiments.
 other ways, including the addition of new snippet similarity measures, text corpora, query data sets, evaluation measures, and so on. Although our graphs are centered on words (as vertices), we are also interested in testing new graph construction procedures based on the use of collocations as vertices, as done by Korkontzelos and Manandhar (2010). Furthermore, the framework is independent of the target language, in that it just requires a large-enough corpus for co-occurrence extraction in that language and some basic tools for processing text (i.e., a stopword list, a lemmatizer, and a compounder). 748
As future work, the framework might be integrated with distributional semantics models and techniques (Baroni and Lenci 2010; Erk, Pad  X  o, and Pad  X  o 2010; Mitchell and
Lapata 2010; Boleda, im Walde, and Badia 2012; Clarke 2012; Silberer and Lapata 2012, inter alia).
 semous queries only, nothing prevents it from being used to perform experiments at different levels of sense granularity. A qualitative evaluation of preliminary experiments in aspect identification (cf. Section 2.6), which requires the detection of very fine-grained subsenses of possibly monosemous queries, showed that WSI also seems to perform well in this task. Given the high number of monosemous queries submitted to Web search engines, we believe that further investigation in this direction may well reveal additional benefits of WSI for Web Information Retrieval.
 Acknowledgments References 750 752
