 Recommender Systems (RS) attempt to discover users X  pref-erences, and to learn about them in order to anticipate their needs. The main task normally associated with a RS is to offer suggestions for items. However, for most users, RSs are black boxes, computerized oracles that give advice, but can-not be questioned. In order to improve the quality of predic-tions and the satisfaction of the users, explanations facilities are needed. We present a novel methodology to explain rec-ommendations: showing predictions over a set of observed items. Our proposal has been validated by means of user studies and lab experiments using MovieLens dataset. H.3 [ Information Storage and Retrieval ]; H.5 [ Information Interfaces and Presentation ] Explanation, Effectiveness, Collaborative Filtering
Usually, when working with RSs, the users do not have an-other option to trust. The need of justifications and expla-nations has started to gain attention during the last decade, being nowadays more crucial due to shilling attacks favoring a given item [1, 2]. Although we can find several alterna-tives for explaining recommendations, it has been widely ac-cepted the use of simple interfaces (generally, because they can be well understood by ordinary users): For example, it can be used explanations as  X  X sers with similar tastes rate this product as ... X ,  X  X ustomers who bought this item also bought ... X , etc. These simple explanations might be useful when recommending in low-risk domains (such as movies, songs or books), but they are not of much help in a domain where the cost of a wrong decision is high, as electronics, holiday packages or investments portfolios. In such domains, an explanation interface given understandable, effective and acceptable information will be preferred.

In this paper we investigate the use of more informative explanation interfaces that could assist the users to make decisions. Having in mind that the proposed approach must be generalizable, and in order to mitigate the domain de-pendence, we will only consider collaborative information (nowadays widely available) in the explanations.

The idea motivating our proposal is that by showing ex-plicitly predictions of this system for some items previously observed, the user can understand the reasoning behind the recommendations. Besides, we also show that the informa-tion presented in the interface can be useful also for improv-ing the recommendations. As far as we know there is no explanation approach that used this information, which is an example of hybrid Human/Item explanations [2].
We are looking for an effective explanation that would help the user to evaluate the quality of the recommenda-tion, according to their own preferences. Obviously, this explanation strongly depends on the used recommendation model. In this paper we will assume a neighborhood-based RS predicting how an active user, a , might rate a target item, t . In general, user-based rating prediction is a process in which each neighbor, v , suggests a rating for the target item, sg ( v, t ), which are combined by weighting the contri-bution of each one, i.e.  X  r a,t  X  P v  X  N w ( a, v ) sg ( v, t ).
Figure A shows a classical interface (bar chart histogram to explain the predicted rating. This interface is the most convincing explanation of why a movie was recommended in MovieLens [1]. In this case the explanation says that  X  X he system suggest 3 ? because it has been rated by other similar users as ... X . Although this interface explains in some way how the predicted rating is computed, there is a second problem that we are going to consider in this paper:  X  X hy I shall believe the system X  X  3 ? prediction? X  .

In case of asking to my friend, I believe (or not) her rec-ommendations because usually I know her preferences over a set of common rated items. We will try to include this simple idea in our explanations. A simple approach, also explored in [1], would be to say  X  X ecause the system made correct predictions in over x % of all products X . This ex-planation is independent of the target item, moreover in a neighborhood-based approach, these predictions may be ob-tained with quite different neighbors. So, what we propose is to ask this neighborhood for their prediction for some items in the active user X  X  profile. This information can give an idea of the quality of this neighborhood and must be
We use a bar chart for each possible category (1 ? to 5 ? ) plus an additional bar (denoted by  X ? X ) representing that we have had some problems when finding a minimum set of neighbors who rated the target item. showed to the user in a graphical way, but there exists some questions that remain to be solved.

Q1. What items can be used in the explanation? This point is particularly important because if we show many items the user might suffer for information overload whereas if we show a few of them, it might be useless. Particularly, in this paper we show the predictions for those items rated by at least 70% of these neighbors. These items represent the support of the explanations 2 , denoted as S , in our ex-perimentation a mean of 20.6 movies were selected.

Q2. How to show the selected items, S ? They will be disposed as points in a two-dimensional graph (Figure B): On the one hand, the x-axis relates each item, i  X  S , with the target one, t . Since we do not use content informa-tion, this relationships is obtained by considering how far are their predictions in such a way that those items with similar suggestions will be placed at the left. In order to measure this criterion we use a distance measure d ( i, t )  X  P v  X  N | sg ( v, i )  X  sg ( v, t ) | . On the other hand, the y-axis represents the quality of the suggestions, i.e. whether the neighbors suggestions match the rating given by the active user to the target item, r ( a, i ). Particularly, we compute way, those items predicted properly by all the members of the neighborhood will be disposed in top positions of the y-axis. For these items we aware of a consensus among the individuals predictions.

Q3. What information is showed for each item in S ? To show the quality of a prediction we use squares , or di-amond 3 , to represent that this particular neighborhood hits, or not, the given rating, respectively, and the color used to fill these figures represents the rating given by the active user to each movie (we use the same colors as the ones used in Figure A). By hovering within these figures, an informational balloon (Figure C) appears showing the par-ticular movie, the real user X  X  rating (4 ? ) and the prediction proposed by the system (3 ? ).

By means of this explanation the user has information about a wide spectrum of situations, ranging from those that reinforce the system prediction to those explaining a wrong one. For instance, in our example the user can figure out the
In case of considering a model-based RS we can use predic-tions for items related to the target one and/or those items that best describe the user X  X  tastes, but how to select the best ones in this approach remains as open problem. trend of t hese neighbors to underestimate the rating, and move the decision towards a 4 ? value.
 Evaluation. In order to evaluate this approach we fol-low a dual-strategy: Firstly, a classical user study 3 using 40 users who rated at least 60 movies in the 100M-MovieLens dataset. After a training step, each user has to judge the explanations for 20 anonymous predictions 4 . Then, they have to confirm whether they agree with the predicted rat-ing or not (in this case, we asked for an alternative). The main conclusions from this experimentation are (i) that our interface gives more information to the user (it helps to re-consider the given rating the 34% of the times whereas the bar histogram helped in the 26%), and (ii) that this infor-mation is more useful because it helps to predict the given rating (the original MAE of the system was 0.76, using the histogram it increases to 0,79 and with our proposal it is reduced to 0.71). We have also recommended for each user 10 unobserved movies (in this case we show the title of the movies) and asked then if they would watch the movie. For this purpose, our explanation interface was useful to the user (changing a previous decision the 17% of the times).
We also asked the user for their opinion about the inter-faces. The results are consistent with our hypothesis: Expla-nations facilities are necessary in a RS and users definitively have a look at them. Besides, although in the users X  opinions prevails subjective elements associated with the simplicity of the histograms, they prefer to consult the most informative interface (joining both approaches) in case of doubts.
Secondly, we have performed an empirical study trying to evaluate if the information showed might give some trends about the quality of the proposed recommendation. In order to find tendencies, we have decided to correlate the explain-ing information with the error obtained in the prediction for all the users in 100M-MovieLens. We found that there is no a global pattern that can be applied, the trends de-pend on the user (what is valid for a user does not have to be good for a different user). Nevertheless, we found signifi-cant trends (p &gt; 0.05) for the 53% of the users. We consider these results interesting since represents a different piece of information that can be included in recommending models. Acknowledgements. This work was supported by the Spanish Ministerio de Educaci  X on y Ciencia (TIN2008-06566-C04-01 and TIN2011-28538-C02-02) , Spanish Research Pro-gram Consolider Ingenio 2010: MIPRCV (CSD2007-00018) and Junta de Andaluc  X  X a Excellence Project TIC-04526. [1] J. L. Herlocker, J. A. Konstan, and J. Riedl. Explaining [2] A. Papadimitriou, P.Symeonidis and Y. Manolopoulos.
In this paper we only present an snapshot of our study.
The users neither know that they rated these movies nor that we show the same movies in both interfaces.
