 One of the major challenges for automated question answer-ing over Knowledge Bases (KBQA) is translating a natural language question to the Knowledge Base (KB) entities and predicates. Previous systems have used a limited amount of training data to learn a lexicon that is later used for question answering. This approach does not make use of other poten-tially relevant text data, outside the KB, which could supple-ment the available information. We introduce a new system, Text2KB, that enriches question answering over a knowledge base by using external text data. Specifically, we revisit dif-ferent phases in the KBQA process and demonstrate that text resources improve question interpretation, candidate generation and ranking. Building on a state-of-the-art tra-ditional KBQA system, Text2KB utilizes web search results, community question answering and general text document collection data, to detect question topic entities, map ques-tion phrases to KB predicates, and to enrich the features of the candidates derived from the KB. Text2KB significantly improves performance over the baseline KBQA method, as measured on a popular WebQuestions dataset. The results and insights developed in this work can guide future efforts on combining textual and structured KB data for question answering.
It has long been recognized that searchers prefer concise and specific answers, rather than lists of document results. In particular, factoid questions have been an active focus of research for decades due to both practical importance and relatively objective evaluation criteria. As an important ex-ample, a large proportion of Web search queries are looking for entities or their attributes [19], a setting on which we focus in this work.

Two relatively separate approaches for Question Answer-ing (QA) have emerged: text-centric, or Text-QA and knowl-edge base-centric, or KBQA. In the more traditional, Text-QA approach, systems use text document collections to re-trieve passages relevant to a question and extract candidate answers [14]. Unfortunately, a passage of text provides a limited amount of information about the mentioned enti-ties, which has to be inferred from the context. The KBQA approach, which evolved from the database community, re-lies on large scale knowledge bases, such as DBpedia [1], Freebase [9], WikiData [24] and others, which store a vast amount of general knowledge about different kinds of enti-ties. This information, encoded as [subject, predicate, object] RDF triples, can be effectively queried using struc-tured query languages, such as SPARQL.

Both approaches eventually deal with natural language questions, in which information needs are expressed by the users. While question understanding is difficult in itself, this setting is particularly challenging for KBQA systems, as it requires a translation of a text question into a structured query language, which is complicated because of the com-plexity of a KB schema, and many differences between nat-ural language and knowledge representations. For example, Figure 1 shows a SPARQL query that retrieves the answer to a relatively simple question  X  X ho was the president of the Dominican Republic in 2010? X  from Freebase.

KBQA systems must address three challenges, namely question entity identification (to anchor the query process); candidate answer generation; and candidate ranking. We will show that these challenges can be alleviated by the ap-propriate use of external textual data. Entity identification seeds the answer search process, and therefore the perfor-mance of the whole system greatly depends on this stage [28]. Question text is often quite short, may contain typos and other problems, that complicate entity linking. Exist-ing approaches are usually based on dictionaries that contain entity names, aliases and some other phrases, used to refer to the entities [21]. These dictionaries are noisy and incom-plete, e.g., to answer the question  X  X hat year did tut became king? X  a system needs to detect a mention  X  X ut X  , which refers to the entity Tutankhamun . If a dictionary doesn X  X  contain a mapping  X  X ut X   X  Tutankhamun , as happens for one of the state of the art systems, it will not be able to answer the question correctly. Such less popular name variations are often used along with full names inside text documents, for example, to avoid repetitions. Therefore, we propose to look into web search results to find variations of question entity names, which can be easier to link to a KB (Figure 2). This idea has been shown effective in entity linking for web search queries 1 [12].

After question entities have been identified, answer candi-dates need to be generated and ranked to select the best an-swer. A candidate query includes one or multiple triple pat-http://web-ngram.research.microsoft.com/ERD2014/ republic in 2010? X  from Freebase Figure 2: Search results for the question  X  X hat year did tut became king? X  , which mention both the full name of the king and the correct answer to the ques-tion terns with predicates, corresponding to words and phrases in the question. Existing knowledge base question answer-ing approaches [3, 6, 7, 8, 10, 29] rely on a lexicon, learned from manually labeled training data, and supported by addi-tional resources, such as question paraphrases [7] and weakly labeled sentences from a large text collection [30]. Such training data tends to be small compared to the number of different predicates in a KB, and therefore the coverage of these lexicons is limited. By our estimate, in a popular WebQuestions KBQA dataset, the answers to  X  5.5% of test questions (112 out of 2032) involve a predicate that does not appear as a ground truth in the training set. For example, an RDF triple [Bigos, food.dish.type_of_dish1, Stew] answers the question  X  X hat are bigos? X  , but no other exam-ples in the training set involve this predicate. In addition, a lexicon needs to cover all different ways a predicate can be asked about. For example, questions  X  X ho did jon gos-selin cheat with? X  and  X  X ho is the woman that john edwards had an affair with? X  are answered by the same KB predi-cate, but use different language. Therefore, presence of the first question in a training set may not help to answer the second question. On the other hand, traditional Text-QA systems benefit from the redundancy of the information on the Web, where the same facts are stated multiple times in many different ways [17]. This increases the chances of a good lexical match between a question and answer state-ments, which makes even some relatively simple counting-based techniques quite effective [11]. We propose to adapt these ideas from text-based question answering for KBQA. The right part of the Figure 3 shows web search results, a community question answering page, and text fragments mentioning pairs of entities, that can be useful to answer the question about John Edwards X  affair.

To summarize, our contributions are three-fold:
Taken together, this work introduces novel techniques for using external text to significantly improve the performance of the KBQA approach. More broadly, our work bridges the gap between Text-QA and KBQA worlds, demonstrating an important step forward towards combining unstructured and structured data for question answering. In this section, we overview the existing approaches to Knowledge Base Question Answering (KBQA), as our ap-proach, described in the next section, builds upon and ex-tends some of these efforts.

Over time, KBQA systems have converged to two major approaches: semantic parsing , and information extraction (IE) [29]. The former focuses on question understanding, and attempts to parse sentences into their semantic repre-sentations, e.g., logical forms [6, 7, 8]. IE approaches [3, 31, 30] are based on identifying topic entities in the ques-tion, and then, using pre-defined templates for mapping the question to predicates, explore the neighborhood of these entities in a KB. Theoretically, semantic parsing-based sys-tems would be capable of generating any required queries, and would apply to any question, seen or unseen in training, whereas the template-based approach is less likely to gener-alize. In practice, however, answers to most of the questions lie within two edge traversals in a KB, making the template-based approaches quite effective.
Recent resurgence of interest in KBQA coincides with the availability of large scale knowledge bases such as Freebase and DBPedia, as well as commercial efforts from Google, Microsoft, Facebook and Yahoo; which make it possible to answer many real questions. Additionally, the creation of the WebQuestions dataset [6], provided a common bench-mark which is large enough to allow both comprehensive evaluation, and training machine learning methods. In this work, we chose to extend an existing information extraction KBQA system  X  Aqqu [3]  X  which achieves one of the high-est scores among publicly available systems. However, as we will show, our approach is general and can be incorporated into other IE-based systems as well.

We will first describe an information extraction approach to KBQA in more detail using Aqqu as an example. In Section 3 we present our system Text2KB, which extends this approach by incorporating external text-based data at various stages of the question answering process.
First, the system identifies question entities, which are used as sources for the answer search process. For concrete-ness, consider a question from the WebQuestions dataset  X  X ho is the woman that john edwards had an affair with? X  . Here, the entity John Edwards with Freebase id /m/01651q is the main question entity. However, Freebase contains mil-lions of entities and it can be difficult to identify the topical ones ( e.g., entities Woman and Affair are also present in Freebase), or to disambiguate and choose between John Ed-wards a politician ( /m/01641q ), an American racing driver ( /m/06zs089 ) and other people with the same name. Aqqu considers all spans of question words under certain condi-tions on part of speech tags and uses an entity names lexicon [21] to map phrases to potential entities. Most reported sys-tems, including Aqqu, do not disambiguate entities at this stage, but rather keep a set of candidates along with some in-formation about their popularities ( e.g., number of mentions in the collection), and mention scores p ( entity | mention text ).
At the next stage, SPARQL query candidates are gener-ated by exploring the neighborhood of the question topic entities using a predefined set of query templates. Each query template has question entities, predicates and answer placeholders. The majority of the answers in the WebQues-tions dataset can be covered by just 3 templates (q entity -question entity, a entity -answer entity, cvt node -Free-base mediator node, which represent tuples with more than 2 arguments):
The first template retrieves a set of entities that are di-rectly connected to the given question entity via a certain predicate. The second template accounts for the presence of a mediator node, that groups together arguments of a multi-argument relation. And the last template looks for cases, when a question also mentions another argument of a multi-argument relation, e.g., Captain Kirk and Star Trek for the question  X  X ho played captain kirk in star trek movie? X  .
Each query candidate is represented with a set of fea-tures, that includes the scores for linked question entities, various scores for matching between question term n-grams and query predicates, the size of the results list, etc. The final stage of the question answering process is filtering and ranking. The Aqqu system employs a pairwise learning-to-rank model, trained on part of the dataset. For each pair of candidate answers Aqqu creates an instance, which con-tains 3 groups of features: features of the first, the second candidate in the pair and the differences between the corre-sponding features of the candidates. Specifically, a Random Forest model is used in the provided Aqqu implementation. A pair where the first candidate is better than the second belongs to class +1, and -1 otherwise. To reduce the num-ber of pairs for the final ranking, Aqqu includes a simplified linear filtering model, which is trained to detect incorrect answers with high precision.
Before introducing our text-based improvements, we de-scribe some basic extensions to the original Aqqu system. First, we noticed that since Aqqu does not use information about the answer entity Freebase types, in many cases it re-turns an answer that is incompatible with the question: e.g., state instead of county etc. Therefore, we trained a model to return a score which measures compatibility between the question and answer entities, based on the entity notable types and question uni-and bi-grams as features, similar to Aqqu X  X  relations score model. A second extension intro-duced a new date range query template, which helps solve cases like  X  X hat team did david beckham play for in 2011? X  , where we need to look at the ranges of dates to determine whether an answer candidate satisfies the question.
We now introduce our system, called Text2KB 2 , that ex-pands upon the basic KBQA model by incorporating exter-nal textual sources throughout the QA process. The gen-eral architecture and an example use case of Text2KB is presented on Figure 3. The left part of the figure roughly corresponds to the architecture of existing information ex-traction approaches to KBQA. The right part introduces additional external text data sources, namely Web search results, community question answering (CQA) data, and a collection of documents with detected KB entity mentions. We demonstrate how these data sources can help with the main challenges in KBQA, i.e., question topical entity iden-tification, predicate scoring and answer candidates ranking. http://ir.mathcs.emory.edu/projects/text2kb/ As described in detail next, this information is used to de-velop novel features for detecting entities and ranking candi-date answers. The final ranking is performed using the same learning-to-rank method as the baseline Aqqu system [3], which uses the Random Forest model.
Traditional Text-QA systems rely on search results to re-trieve relevant documents, which are then used to extract answers to users X  questions. Relevant search results men-tion question entities multiple times and in various forms, which can be helpful for entity linking [12]. Furthermore, retrieved document set often contains multiple statements of the answer, which can be a strong signal for candidate ranking [17].

To obtain related web search results, Text2KB issues the question as a query to a search engine 3 , extracts top 10 result snippets and the corresponding documents. Next, Text2KB uses Aqqu entity linking module to detect KB entity men-tions in both snippets and documents.

Question entity identification . Question text provides only a limited context for entity disambiguation and linking; additionally, the entity name can be misspelled or an uncom-mon variation used. This complicates the task of entity iden-
In our experiments we use the Bing Web Search API https://datamarket.azure.com/dataset/bing/search and lo-cal Wikipedia search using Lucene tification, which is the foundation of KB question answer-ing process. Fortunately, web search results help with these problems, as they usually contain multiple mentions of the same entities and provide more context for disambiguation. Text2KB uses the search result snippets to expand the set of detected question entities. More specifically, we count the frequencies of each entity mentioned in search snippets, and most popular ones with names similar to some of the ques-tion terms are added to the list of topical entities. The goal of this similarity condition is to keep only entities that are likely mentioned in the question text, and filter out related, but different entities. To estimate the similarity between a name and question tokens, we use Jaro-Winkler string dis-tance. An entity is added to the list of question entities if at least one of its tokens e t has high similarity with one of the question tokens q t excluding stopwords ( Stop ): Answer candidate features . The information stored in KBs can also be present in other formats, e.g., text state-ments. For example, on Figure 2 multiple search snippets mention the date when Tutankhamun became a king. Text-QA systems use such passages to extract answer to users X  questions. However, text may not provide sufficient context information about the mentioned entities, and systems have to infer the useful details, e.g., entity types, which can be problematic [31]. On the other hand, KBQA systems can Figure 4: Example of a question and answer pair from Yahoo! Answers CQA website utilize all the available KB knowledge about the entities in a candidate answer, and would benefit from additional text-based information to improve ranking. More specifically, Text2KB proceeds as follows: 1. Precompute term and entity IDFs. We used Google 2. Each snippet s i and document d i are represented by 3. In addition, vectors of all snippets and all documents 4. Each answer candidate a j is also represented as TF-5. Cosine similarities between answer and each of 10 snip-6. We also compute answer similarities with the com-
Recall that a major challenge in KBQA is that natural language questions do not easily map to entities and predi-cates in a KB. An established approach for this task is su-pervised machine learning, which requires labeled examples of questions and the corresponding answers to learn this mapping, which can be expensive to construct. Researchers have proposed to use weakly supervised methods to extend a lexicon with mappings learned from single sentence state-ments mentioning entity pairs in a large corpus [30]. How-ever, the language used in questions to query about a certain predicate may differ from the language used in statements. A recent work [20] demonstrated how distant supervision can be applied to question-answer pairs from CQA archives for a related task of information extraction for knowledge base completion. In a similar way, we use weakly labeled collection of question-answer pairs to compute associations between question terms and predicates to extend system X  X  lexicon (Figure 4). We emphasize that this data does not re-place the mappings learned from single sentence statements, which are already used by our baseline system, but rather introduces the new ones learned from the CQA data.
For our experiments we use 4.4M questions from Yahoo! http://lemurproject.org/clueweb09/FACC1/
Term Predicate PMI born people.person.date of birth 3.67 kill people.deceased person.cause of death 1.70 currency location.country.currency formerly used 5.55 school education.school.school district 4.14 win sports.sports team.championships 4.11 Table 1: Examples of term-predicate pairs with high PMI scores, computed using distant supervi-sion from a CQA collection WebScope L6 dataset 5 . Question and answer texts were run through an entity linker, that detected mentions of Freebase entities. Next, we use distant supervision assumption to label each question-answer pair with predicates between en-tities mentioned in the question and in the answer. This labels are used to learn associations between question terms and predicates by computing pointwise mutual information scores (PMI) for each term-predicate pair. Examples of scores for some terms are given in Table 1.

In Text2KB we evaluate candidate answer predicates by using the association (e.g., PMI) scores between predicates and the question terms (missing pairs are given a score of 0). The minimum, average and maximum of these values are used as features to represent a candidate answer. Such asso-ciations data can be sparse, we also use pretrained word2vec word embeddings 6 . We compute predicate embeddings by taking a weighted average of term vectors from predicate X  X  PMI table. Each term vector is weighted by its PMI value (terms with negative score are skipped). Then, we com-pute cosine similarities between predicate vector and each of the question term vectors and take their minimum, aver-age, maximum as features. Finally, we average embeddings of question terms and compute its cosine similarity with the predicate vector.
A key step for ranking candidate answers is to estimate whether the question and answer entities are related in a way asked in the question. Existing KBQA approaches usu-ally focus on scoring the mappings between question phrases and KB concepts from a candidate SPARQL query. How-ever, textual data can provide another angle on the problem, as question and answer entities are likely to be mentioned together somewhere in text passages. For example, in the bottom right corner of Figure 3 we can see some passages that mention a pair of people, and the context of these men-tions explains the nature of the relationships. This data can be viewed as additional edges in a KB, which connect pairs of entities, and have associated language models, estimated from text phrases, that mention these entities. Such edges do not have to coincide with the existing KB edges, and can connect arbitrary pairs of entities, that are mentioned together in text, therefore extending the KB. https://webscope.sandbox.yahoo.com/ https://code.google.com/p/word2vec/ Entity 1 Entity 2 Term counts John Edwards John Edwards John Edwards John Edwards Table 2: Example of entity pairs along with the most popular terms mentioned around the entities
We use the ClueWeb12 corpus with existing Freebase en-tity annotations and count different terms that occur in the context of a mention of a pair of different entities (we only consider mentions within 200 characters of each other). To compute this unigram language model we use the terms sep-arating the entities, as well as the terms within a small win-dow (e.g., 100 characters) before and after the entity men-tions. A small sample of this data is presented in Table 2.

We use this data to compute candidate ranking features as follows. Consider question words Q and an answer can-didate, which contains a question entity e 1 and one or more answer entities e 2 . For each answer candidate, we compute a language model score: and use the minimum, average and maximum over all an-swer entities as features. To address the sparsity problem, we again use embeddings, i.e., for each entity pair a weighted (by counts) average embedding vector of terms is computed and minimum, average and maximum cosine similarities be-tween these vectors and question token embeddings are used as features.
In addition to external text data, many knowledge bases, including Freebase, contain text data as well, e.g., Freebase includes a description paragraph from Wikipedia for many of its entities. These text fragments provide a general de-scription of entities, which may include information relevant to the question [22]. For completeness, we include them in our system as well. Each entity description is represented by a vector of tokens, and a vector of mentioned entities. We compute cosine similarities between token and entity vectors of the question and description of each of the answers, and use the minimum, average and maximum of the scores as features.
This section reports the experimental setup, including the dataset and metrics, as well as the main methods compared for evaluating the performance of our Text2KB system. Ad-ditionally, we describe a series of ablation studies to analyze contribution of different system components.
We compare our system, Text2KB, to state-of-the-art ap-proaches, notably: Additionally, other previously published results on WebQues-tions are included to provide context for the improvements introduced by our Text2KB system. We followed the standard evaluation procedure for the WebQuestions dataset, and used the original 70-30% train-test split (3,778 training and 2,032 test instances). Within the training split, 10% was set aside for validation to tune the model parameters and only the best-performing set of parameters selected on the validation data was used to re-port the results on the official test split.
Recent papers using the WebQuestions dataset have pri-marily used the average F1-score as the main evaluation metric, defined as: avg F 1 = 1 | Q | P q  X  Q f 1( a  X  q , a and a q are correct and given answers to the question q, which can be lists of entities. Additionally, we report average preci-sion and recall, to gain better understanding of the tradeoffs achieved by different methods.
The results of existing approaches and our Text2KB sys-tem are presented in Table 4. We should note, that text-based QA systems typically return a ranked list of answers, whereas many answers on WebQuestions dataset are lists, which complicates the comparison between KBQA and text-based systems. The result reported for YodaQA system is F1 score at position 1.

As we can see, Text2KB significantly improves over the baseline system and reaches the current best published result -STAGG [31]. We believe that this system will also benefit from the ideas of our work (Section 5).
To analyze the contribution of the features and datasources we introduced, we report results from a series of ablation studies. For convenience, we introduce the following short-hand notations for different components of our system: (+1.6%) 0.563  X  (+3.1%) 0.522  X  (+5.7%)
In our results table we will use the notation + &lt; comp &gt; for a system with a certain component added, and -&lt; comp &gt; when it is removed. For example, the baseline system will be denoted as  X  Aqqu  X . The same system with additional date range filter query templates and notable types score model is denoted as  X  Aqqu +DF+T  X , which represents the same system as  X  Text2KB -WebEnt-Web-CQA-CL  X  (we will call it Text2KB (base)). Our full system  X  Text2KB  X  can be also denoted as  X  Aqqu +DF+T+WebEnt+Web+CQA+CL  X .
 Components : First, we analyze the improvements intro-duced by different components of our system (Table 4). As we can see, additional date range filters and notable types model ( Aqqu+DF+T ) are responsible for an increased recall and a drop in precision compared to the baseline model. Fea-tures generated from Wikipedia search results, CQA data and ClueWeb entity pair language models ( +Wiki+CQA+CL ) improve average F1 by 0.007 (+1.4%) compared to the base model, adding entity linking using Wikipedia search results improves results even more (+3%).

Web search results ( +Web+CQA+CL ) turned out to be more helpful than Wikipedia results ( +Wiki+CQA+CL ), which is nat-ural since Wikipedia is a subset of the web. This was one of the reasons we didn X  X  combine Wikipedia and Web search together. Finally, entity linking and all text-based features combined achieves an even higher score, proving that their contributions are independent.
 Data Sources : We now anylize the contribution of the dif-ferent data sources. We will remove a group of web search, CQA or Clueweb-based features and see how the perfor-mance of the whole system changes (Table 5). As we can see, all data sources have an impact on the system perfor-mance, and web search results based features provide the most useful signal for answer ranking.
 Feature Importance for Ranking : Figure 5 plots a sub-set of features ranked by their Gini index-based importance scores. The figure supports the observation that web search results features are the most useful, however, other text data sources also contribute to the improvement.

In summary, Text2KB significantly outperforms the base-line system, and each of the introduced components con-tributes to this improvement. Web search results data turned System R P F1 Aqqu 0.604 0.498 0.494
Text2KB (base) = Aqqu+DF+T 0.617 0.481 0.499 +Wiki+CQA+CL 0.623 0.487 0.506 +WikiEnt +Wiki+CQA+CL 0.632 0.498 0.514 +WebEnt 0.627 0.492 0.508 +Web+CQA+CL 0.634 0.497 0.514 +WebEnt +Web+CQA+CL 0.635 0.506 0.522 Table 4: Average Recall (R), Precision (P), and F1 of Aqqu and Text2KB system with and without dif-ferent components. +A means that a component A is added to the Text2KB (base) system. The list of components is given in Section 4.5.
 Table 5: Average Recall (R), Precision (P), and F1 of Text2KB with and without features based on web search results, CQA data and ClueWeb collection. out to be the most useful resource, and it significantly im-proves the quality by helping with question entity identifi-cation and candidate ranking. Next, we analyze the system performance in more detail, and investigate factors for fu-ture extension.
We now investigate how our system would compare to other systems on the same benchmark; then, we investigate in depth the different error modes (Section 5.1), which helps identify the areas of most substantial future improvements.
We took an existing KBQA systems and demonstrated that by combining evidence from knowledge base and exter-nal text resources we can boost the performance. A reason-able question is whether the same approach will be helpful to other systems, e.g., the currently best system  X  STAGG [31]. STAGG differs from our baseline system Aqqu in the components: entity linking algorithm, a set of query tem-marked * are not text-based and are provided for comparison) Table 6: Average F1 for combinations of Text2KB and STAGG using a simple heuristic based on the length of the answer list and Oracle upper bound plates and ranking methods. Therefore, our approach is complementary and should be helpful for STAGG as well. To support this claim, we made an experiment to combine answers of STAGG and Text2KB. One of the advantages of the former is its set of filters, that restricts list results to en-tities of certain type, gender, etc. Therefore, we combined answers of STAGG and Text2KB using a simple heuristic: we chose to use the answer returned by STAGG if the num-ber of answer entities is less than in the Text2KB answer, otherwise we use the answer of our approach. Table 6 gives the results of the experiment, and as we can see the combi-nation achieves a slightly better average F1 score. Alterna-tively, we can look at the Oracle combination of the systems, which always selects the answer with the higher F1. As we can see such a combination results in a performance of 0.606, which is much higher than either of the systems.

As we mentioned earlier, answers to 112 of the test ques-tions in the WebQuestions dataset involve predicates that weren X  X  observed in the training set, which may be a prob-lem for approaches that rely on a trained lexicon. We eval-uated both systems on these questions, and indeed the per-formance is very low, i.e., the average F1 score of Text2KB is 0.1640 compared to 0.1199 for STAGG 7 .
To get a better insights into the problems that remain, we collected 1219 questions for which Text2KB didn X  X  return completely correct answer, i.e., F1 score &lt; 1. We manually looked through a couple of hundreds of these examples and grouped the problems into several clusters (Figure 6).
As we can see candidate ranking is still the major prob-lem, and it accounts for  X  31% of the cases. The second problem is incorrect ground truth labels (almost 25% of re-ported errors). Another set of questions has incomplete or overcomplete ground truth answer list. Typical examples are questions asking for a list of movies, books, landmarks,
Unfortunately, the number of questions is too low to show statistical significance (p-value=0.16) of the difference Figure 6: Distribution of problems with questions, where Text2KB returns an answer with F1 &lt; 1 etc. The ground truth answer usually contains  X  10 enti-ties, whereas the full list is often much larger. This seems to be an artifact of the labeling process, where the answer was selected from the Freebase entity profile page, which shows only a sample of 10 entities, while the rest are hidden be-hind the  X  X  values total X  link. About 20% of the questions are ambiguous, i.e., questions have no strict 1-1 correspon-dence with any of the predicates and can be answered by multiple ones without any obvious preferences. For exam-ple, the question  X  X hat did hayes do? X  can be answered by profession, occupied position or some other achievements. Another problem is when there is no predicate that answers the question. For example, the question  X  X hat do people in france like to do for fun? X  doesn X  X  have a good match among the facts stored in Freebase. The ground truth en-tity Cycling comes from the list Olympic sport competitions country participated 8 .

Text2KB components were quite effective in resolving some of the problems. Web search results helped identify the right question topical entity in a number of cases, e.g.,  X  X hat did romo do? X  mentions only the last name of the Dallas Cowboys quarterback and the baseline system were unable to map it to the right entity. Web search results provides more than enough evidence that  X  romo  X  refers to Tony Romo . However, there are a number of loses, introduced by added unrelated entities. For example, the entity I Love Lucy was added for the question  X  X hat was lucille ball? X  , because the term lucy had high similarity with lucille . A portion of these problems can be fixed by a better entity linking strat-egy, e.g., [12]. An interesting example, when external text resources improved the performance is the question  X  X hat ship did darwin sail around the world? X  . This is actually a hard question, because the ship entity is connected to olympics.olympic_participating_country.athletes the Charles Darwin entity through the  X  X nownFor X  predi-cate along with some other entities like Natural selection . Thus, the predicate itself isn X  X  related to the question, but nevertheless, the name of the ship HMS Beagle is mentioned multiple times in the web search results, and entity pair model computed from ClueWeb also has high scores for the terms  X  X hip X  and  X  X orld X .

There are several major reasons for the loses, introduced by features based on external text resources. Some entities often mentioned together and therefore one of them gets high values of cooccurrence features. For example, the base-line system answered the question  X  X hen did tony romo got drafted? X  correctly, but since Tony Romo is often followed by Dallas Cowboys , Text2KB ranked the team name higher. Another common problem with our features is an artifact of entity linking, which works better for names and often skips abstract entities, like professions. For example, the correct answer to the question  X  X hat did jesse owens won? X  is an entity with the name Associated Press Male Ath-lete of the Year , which is rarely mentioned or it X  X  hard to find such mentions. Some problems were introduced by a combination of components. For example, for  X  X here buddha come from? X  a topical entity Buddhism was introduced from search results, and it generated Gautama Buddha as one of the answer candidates. This answer was ranked the highest due to large number of mentions in the search results.
In summary, we show that ideas behind Text2KB could be integrated into other systems and improve their perfor-mance. The error analysis suggested that even though a sig-nificant number of questions in the WebQuestions dataset have incorrect or ambiguous ground truth labels, there is still a room for improvement. In particular, the future work for Text2KB will include a better strategy for entity link-ing using external data sources and a better context model for entity mentions in text documents, which can put more weight on entities mentioned in the context related to the question.
One well known annual benchmark in knowledge base question answering is Question Answering over Linked Data (QALD), started in 2011 [23]. These benchmarks use the DBpedia knowledge base and usually provide a training set of questions, annotated with the ground truth SPARQL queries. In QALD-3 a multilingual task has been introduced, and since QALD-4 the hybrid task is included. This task asks participants to use both structured data and free form text available in DBpedia abstracts. The formulation of the hybrid task is the most relevant to our work, but there are some key differences. Questions in the hybrid track are man-ually created in such a way, that they can only be answered using a combination of RDF and free text data. Secondly, the hybrid task focuses on text data already present in a KB, whereas we are exploring external text resources. In general, because of the expensive labeling process, QALD datasets are relatively small, for example, QALD-5 training set for multilingual question answering includes 300 exam-ples and 40 for the hybrid task, with 50 and 10 test questions correspondingly. Therefore, due to the scale of datasets and slightly different focus of tasks, we did not evaluate our tech-niques on the QALD benchmarks, but intend to explore it in the future.

Another benchmark dataset  X  WebQuestions  X  was intro-duced by Berant et al. [6]. The approaches proposed since then differ in the algorithms used for various components, and, what is more relevant to our work, the use of external datasets. WikiAnswers corpus of question clusters can be used to learn a question paraphrasing model, which helps to account for different ways a question can be formulated [7]. Another approach to learn term-predicate mappings is to mine them from a large text corpus [30], weakly labeled using distant supervision [18]. In the current paper, we build on this idea in two ways: by introducing a new data source (CQA archives), and by mining a language model for each mentioned entity pair, rather than predicates. Another ap-proach to generate more training data is to automatically convert RDF triples to questions using entity and predicate names [10]. Finally, many systems work with distributed vector representations for words and RDF triples and use various deep learning techniques for answer selection [10, 31]. In all of these works, external resources are used to train a lexicon for matching questions to particular KB queries. In our work, we use external resources in a different way: we are targeting better candidate generation and ranking by considering the actual answer entities rather than predi-cates used to extract them.

In general, combining different data sources, such as text documents and knowledge bases, for question answering has been attempted before, and it has been already implemented in hybrid QA systems [2, 5]. Such systems typically have dif-ferent pipelines that generate answer candidates from each of the data sources independently, and merge them to select the final answer at the end. We make a step towards inte-gration of approaches, by incorporating text resources into the different stages of knowledge base question answering process. This is similar to the work of [22], who explored the use of entity types and descriptions from a KB for text-based question answering, and [13] explored such semantic annotations for ad-hoc document retrieval.

An alternative approach to QA is by using Open Infor-mation Extraction [15], which extract semi-structured data from text. OpenIE repositories can be queried using struc-tured query languages, and at the same time allows keyword matching against entities and predicates [16]. In this work, we are borrowing an idea of learning about entity relation-ship via natural language phrases connecting them. How-ever, since we do not need to extract clean set of relation tuples, we can keep all kinds of phrases, mentioned around entity pairs.

M. Yahya et al [27] proposed extending SPARQL triple patterns with text keywords, and using certain query relax-ation techniques to improve the robustness of KBQA sys-tems. Query relaxation drops certain triple patterns from SPARQL query and adds the corresponding question words as keywords to other triple patterns. The idea of query re-laxations and using text in SPARQL queries was extended in [26], which proposed a framework for querying extended knowledge graphs, comprising of a combination of KB and OpenIE triples. These ideas are complimentary to our work, because our use of text data improves the matching between question phrases and KB concepts, whereas query relax-ations are applied when a good match wasn X  X  found. An-other KB-Text hybrid approach, proposed in [25], utilizes text resources as a post-processing step for answer valida-tion and filtering. In contrast, Text2KB integrates external textual information into all stages of question answering, re-sulting in more robust and overall higher performance than previously explored enhancements done in isolation.
Our work showed that unstructured text resources can be effectively utilized for knowledge base question answering to improve query understanding, candidate answer gener-ation and ranking. We focused on three particular tech-niques and associated text information sources: web search results for query understanding and candidate ranking, com-munity question answering data for candidate generation, and text fragments around entity pair mentions for ranking. Certainly, there are more resources that could be poten-tial adapted, e.g., entity profile pages like Wikipedia, news sources, textbooks, and many others. However, we believe that the proposed approach is general enough that it could be extended and successfully incorporate these other diverse text sources.

In the future, we plan to extend our work to the more open setup, similar to the QALD hybrid task, where ques-tions no longer have to be answered exclusively from the KB. This would require extending the described techniques, and creating new QA benchmarks.
 ACKNOWLEDGMENTS : This work was partially sup-ported by the Yahoo Labs Faculty Research Engagement Program (FREP). We also thank the anonymous reviewers for their constructive comments. [1] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, [2] K. Barker. Combining structured and unstructured [3] H. Bast and E. Haussmann. More accurate question [4] P. Baudi X s. Systems and approaches for question [5] P. Baudi X s and J.  X  Sediv`y. Modeling of the question [6] J. Berant, A. Chou, R. Frostig, and P. Liang. [7] J. Berant and P. Liang. Semantic parsing via [8] J. Berant and P. Liang. Imitation learning of [9] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and [10] A. Bordes, S. Chopra, and J. Weston. Question [11] E. Brill, S. Dumais, and M. Banko. An analysis of the [12] M. Cornolti, P. Ferragina, M. Ciaramita, H. Sch  X  utze, [13] J. Dalton. Entity-based Enrichment for Information [14] H. T. Dang, D. Kelly, and J. J. Lin. Overview of the [15] A. Fader, S. Soderland, and O. Etzioni. Identifying [16] A. Fader, L. Zettlemoyer, and O. Etzioni. Open [17] J. Lin. An exploration of the principles underlying [18] M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant [19] J. Pound, P. Mika, and H. Zaragoza. Ad-hoc object [20] D. Savenkov, W.-L. Lu, J. Dalton, and E. Agichtein. [21] V. I. Spitkovsky and A. X. Chang. A cross-lingual [22] H. Sun, H. Ma, W.-t. Yih, C.-T. Tsai, J. Liu, and [23] C. Unger, C. Forascu, V. Lopez, A.-C. N. Ngomo, [24] D. Vrande X ci  X c and M. Kr  X  otzsch. Wikidata: A free [25] K. Xu, Y. Feng, S. Reddy, S. Huang, and D. Zhao. [26] M. Yahya, D. Barbosa, K. Berberich, Q. Wang, and [27] M. Yahya, K. Berberich, S. Elbassuoni, and [28] X. Yao. Lean question answering over freebase from [29] X. Yao, J. Berant, and B. Van Durme. Freebase qa: [30] X. Yao and B. Van Durme. Information extraction [31] W.-t. Yih, M.-W. Chang, X. He, and J. Gao. Semantic
