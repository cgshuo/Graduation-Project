 Ben London  X  blondon@cs.umd.edu Bert Huang  X  bert@cs.umd.edu Ben Taskar  X  taskar@cs.washington.edu Lise Getoor  X  getoor@cs.umd.edu
University of Maryland, College Park, MD 20742 USA University of Washington, Seattle, WA 98195 USA Before proceeding, we recall a general form of McDi-armid X  X  inequality.
 Theorem 7 (McDiarmid, 1989, Corollary 6.10) . Let f : Z n  X  R be a measurable function for which there z Then, for any &gt; 0 , Note that the above does not require independence. P the Hamming metric, then P n i =1  X  2 i  X  nc 2 k  X   X  n (Though the published results only prove this for countable spaces, Kontorovich later extended this analysis to continuous spaces in his thesis (2007).) If f is c -Lipschitz with respect to the normalized Ham-completes the proof.
 E Z . Recall that each Z 0 l is independent and identically distributed according to P ( Z ). By linearity of expec-tation, we have that To complete the proof, we simply apply Theorem 2 to
E [ L ( h, Z 0 )], using the fact that k  X   X  mn k  X  = k  X  because the dependency matrix  X   X  mn is block diagonal. i th coordinate, missibility condition that Combining this with the second term, we have that where we have used the second admissibility condition and uniform collective stability.
 that The last inequality follows from uniform collective sta-conditions of Theorem 1, with c =  X  . Recalling that  X ( F ) = E [ X ( F , Z )], we therefore have that Assigning  X  probability to this event and solving for completes the proof.
 For the following, we use variables Z and Z 0 to distin-guish between realizations of the training and testing Jensen X  X  inequality, we have that let and P ( Z , Z 0 ) = P ( T (  X  i ) |  X  i ) P ( T 0 (  X  i ) |  X  i try,
 X ( F )  X  E which completes the proof.
 We begin with a technical lemma, which is a general-ization of Talagrand X  X  contraction lemma (Ledoux &amp; Talagrand, 1991) to vector-valued functions and arbi-trary norms.
 Lemma 10. Let F be a class of functions from a do-main Z to R k . Let {  X  i } n i =1 be a set of Rademacher any p  X  1 , then, for any z  X  X  n , E " Conditioned on  X  1: n  X  1 , we know that there must exist two functions f + ,f  X   X  X  such that where the last line follows from the Lipschitz condition. f j ( z n )), and note that This yields
E By induction on n , we therefore have that where s i,j disappears because of symmetry.
 The proof of Lemma 4 follows directly from this lemma, since the second admissibility condition en-sures that ` is  X  -Lipschitz under the 1-norm. The fact that h : X n  X   X  Y n is irrelevant. Because Lemma 10 holds for any realization z  X  Z n , we obtain the (non-empirical) Rademacher complexity by taking the ex-pectation over Z .
  X  (1  X   X  ) equality is preserved when this term is dropped. Thus, dividing both sides by  X  X / 2, we have that (1  X   X  ) is maximized at  X  = 0.
 Let  X  a , arg min a  X  X   X  (  X ,a ) and  X  a 0 , we could state this in terms of  X  0 .) Using Lemma 5, we have that Taking the square root completes the proof.
 Using Cauchy-Schwarz, we have that | E w ( x , a )  X  E w ( x 0 , a ) | because, by definition, k w k 2 is uniformly upper-( x 0 , a ) only differ at any clique involving node i . The number of such cliques is Q i , which is uniformly upper-bounded by Q G , so at most Q G features will change. Further, since the norm of any feature function is, by definition, uniformly upper-bounded by B , we have that k f ( x , a )  X  f ( x 0 , a ) k 2 =  X   X   X  2 BQ i  X  2 BQ G , which completes the proof.
 edge length (2 / etry, one can show the hypercube [0 , 2 / scribed in a ball of radius ; therefore, the Euclidean distance from any point in [0 ,  X ] d to the center of the nearest cell is at most . To find the value of k that -covers [0 ,  X ] d , we let k (2 / k .
 The following is a consequence of Massart X  X  finite class lemma.
 Theorem 8. Let F be a class of functions from Z n to R n . For any n  X  1 and p  X  1 , The ramp function is defined as for any y,y 0  X  Y and  X  y  X   X  Y , | ` 1 ( y,  X  y )  X  ` 1 which establishes the first admissibility condition. y  X  X  and  X  y,  X  y 0  X   X  Y , we have that Further, for any a,a 0  X  R , Combining these inequalities, we have that | `  X  ( y,  X  y )  X  `  X  ( y,  X  y 0 ) |  X  (1 / X  ) k  X  y  X   X  y 0 tablishes the second admissibility condition. L.1. Collective Regression interval on the real number line. Since the output can Lemma 11. The quadratic loss ` q is (1 , 2) -admissible. Proof. First, since both Y and  X  Y are upper-bounded ond argument. Therefore, by the mean value theorem, there exists a  X   X  [0 , 1] such that, for any y  X  Y and  X  y,  X  y 0  X   X  Y , with  X  X  y ,  X  y 0  X   X  y , which establishes the second condition.
 We can thus obtain bounds on the quadratic risk for the class of TSM regressors with strongly convex reg-ularizers, similar to how we obtained Theorem 6. Kontorovich, L. Measure Concentration of Strongly Mixing Processes with Applications . PhD thesis, Carnegie Mellon University, 2007.
 Kontorovich, L. and Ramanan, K. Concentration in-equalities for dependent random variables via the martingale method. Annals of Probability , 36(6): 2126 X 2158, 2008.
 Ledoux, M. and Talagrand, M. Probability in Banach
Spaces: Isoperimetry and Processes . Ergebnisse der Mathematik und ihrer Grenzgebiete. Springer-Verlag, 1991.
 McDiarmid, C. On the method of bounded differences. In Surveys in Combinatorics , volume 141 of London
Mathematical Society Lecture Note Series , pp. 148 X 
