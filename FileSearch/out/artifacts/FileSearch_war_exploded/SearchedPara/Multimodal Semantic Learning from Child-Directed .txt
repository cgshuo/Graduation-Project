 Computational models of word learning typically approximate the perceptual context that learners are exposed to through artificial proxies, e.g., represent-ing a visual scene via a collection of symbols such as cat and dog , signaling the presence of a cat, a dog, etc. (Yu and Ballard, 2007; Fazly et al., 2010, inter alia ). 1 While large amounts of data can be generated in this way, they will not display the complexity and richness of the signal found in the natural environment a child is exposed to. We take a step towards a more realistic setup by intro-ducing a model that operates on naturalistic images of the objects present in a communicative episode. Inspired by recent computational models of mean-ing (Bruni et al., 2014; Kiros et al., 2014; Silberer and Lapata, 2014), that integrate distributed linguis-tic and visual information, we build upon the Multi-modal Skip-Gram (MSG) model of Lazaridou et al. (2015). and enhance it to handle cross-referential uncertainty. Moreover, we extend the cues com-monly used in multimodal learning (e.g., objects in the environment) to include social cues (e.g., eye-gaze, gestures, body posture, etc.) that reflect speak-ers X  intentions and generally contribute to the un-folding of the communicative situation (Stivers and Sidnell, 2005). As a first step towards developing full-fleged learning systems that leverage all signals available within a communicative setup, in our ex-tended model we incorporate information regarding the objects that caregivers are holding. Like the original MSG, our model learns multimodal word embeddings by reading an utterance sequen-tially and making, for each word, two sets of pre-dictions: (a) the preceding and following words, and (b) the visual representations of objects co-occurring with the utterance. However, unlike Lazaridou et al. (2015), we do not assume we know the right object to be associated with a word. We consider instead a more realistic scenario where multiple words in an utterance co-occur with multiple objects in the corresponding scene. Under this referential uncer-tainty , the model needs to induce word-object as-sociations as part of learning, relying on current knowledge about word-object affinities as well as on any social clues present in the scene.

Similar to the standard skipgram, the model X  X  pa-rameters are context word embeddings W 0 and tar-get word embeddings W . The model aims at opti-mizing these parameters with respect to the follow-ing multi-task loss function for an utterance w with associated set of objects U : where t ranges over the positions in the utterance w , such that w t is t th word. The linguistic loss function is the standard skip-gram loss (Mikolov et al., 2013). The visual loss is defined as: ` where w t stands for the column of W corresponding to word w t , u s is the vector associated with object U , and g the penalty function which is small when projections to the visual space w t of words from the utterance are similar to the vectors representing co-occurring objects, and at the same time they are dissimilar to vectors u 0 repre-senting randomly sampled objects. The first term in Eq. 2 is the penalty g weighted by the current word-object affinity  X  , inspired by the  X  X ttention X  of Bah-danau et al. (2015). If  X  is set to a constant 1 , the model treats all words in an utterance as equally rel-evant for each object. Alternatively it can be used to encourage the model to place more weight on words which it already knows are likely to be related to a given object, by defining it as the (exponentiated) cosine similarity between word and object normal-ized over all words in the utterance: The second term of Eq. 2 is the penalty weighted by the social salience h of the object, which could be based on various cues in the scene. In our experi-ments we set it to 1 if the caregiver holds the object, 0 otherwise.
 We experiment with three versions of the model. With  X  = 1 and  X  frozen to 1, the model reduces to the original MSG , but now trained with referen-tial uncertainty. The Attentive MSG sets  X  = 1 and calculates  X  ( w t , u s ) using Equation 4 (we use the term  X  X ttentive X  to emphasize the fact that, when processing a word, the model will pay more atten-tion to the more relevant objects). Finally, Attentive Social MSG further sets  X  = 1 tance of socially salient objects.

All other hyperparameters are set to the values found by Lazaridou et al. (2015) to be optimal af-ter tuning, except hidden layer size that we set to 200 instead of 300 due to the small corpus (see Sec-tion 3). We train the MSG models with stochastic gradient descent for one epoch. Frank et al. (2007) present a Bayesian cross-situational learning model for simulating early word learning in first language acquisition. The model is tested on a portion of the Rollins section of the CHILDES Database (MacWhinney, 2000) consist-ing of two transcribed video files ( me03 and di06 ), of approximately 10 minutes each, where a mother and a pre-verbal infant play with a set of toys. By in-specting the video recordings, the authors manually annotated each utterance in the transcripts with a list of object labels (e.g., ring , hat , cow ) correspond-ing to all midsize objects judged to be visible to the infant while the utterance took place, as well as vari-ous social cues. The dataset includes a gold-standard lexicon consisting of 36 words paired with 17 object labels (e.g., hat = hat , pig = pig , piggie = pig ). 2
Aiming at creating a more realistic version of the original dataset, akin to simulating a real visual scene, we replaced symbolic object labels with ac-tual visual representations of objects. To construct such visual representations, we sample for each ob-ject 100 images from the respective ImageNet (Deng et al., 2009) entry, and from each image we ex-tract a 4096-dimensional visual vector using the Caffe toolkit (Jia et al., 2014), together with the pre-trained convolutional neural network of Krizhevsky obtain a single visual representation of each object. Concerning social cues, since infants rarely follow the caregivers X  eye gaze but rather attend to objects held by them (Yu and Smith, 2013), we include in our corpus only information on whether the care-giver is holding any of the objects present in the scene. Note however that this signal, while infor-mative, can also be ambiguous or even misleading with respect to the actual referents of a statement. Figure 1 exemplifies our version of the corpus, the Illustrated Frank et al. Corpus (IFC).
 Several aspects make IFC a challenging dataset. Firstly, we are dealing with language produced in an interactive setting rather than written discourse. For example, compare the first sentence in the Wikipedia entry for hat (  X  X  hat is a head covering X  ) to the third utterance in Figure 1, corresponding to the first oc-currence of hat in our corpus. Secondly, there is a large amount of referential uncertainty, with up to 7 objects present per utterance (2 on average) and with only 33% of utterances explicitly includ-ing a word directly associated with a possible ref-erent (i.e., not taking into account pronouns). For instance, the first, second and last utterances in Fig-ure 1 do not explicitly mention any of the objects present in the scene. This uncertainty also extends to social cues: only in 23% of utterances does the mother explicitly name an object that she is holding in her hands. Finally, models must induce word X  object associations from minimal exposure to input rather than from large amounts of training data. In-deed, the IFC is extremely small by any standards: 624 utterances making up 2,533 words in total, with 8/37 test words occurring only once. We follow the evaluation protocol of Frank et al. (2007) and Kievit-Kylar et al. (2013). Given 37 test words and the corresponding 17 objects (see Table 2), all found in the corpus, we rank the objects with respect to each word. A mean Best-F score is then derived by computing, for each word, the top F score across the precision-recall curve, and averaging it across the words. MSG rankings are obtained by di-rectly ordering the visual representations of the ob-jects by cosine similarity to the MSG word vectors.
Table 1 reports our results compared to those in earlier studies, all of which did not use ac-tual visual representations of objects but rather ar-bitrary symbolic IDs. Bayesian CSL is the orig-inal Bayesian cross-situational model of Frank et al. (2007), also including social cues (not limited, like us, to mother X  X  touch). BEAGLE is the best semantic-space result across a range of distributional models and word-object matching methods from Kievit-Kylar et al. (2013). Their distributional mod-els were trained in a batch mode , and by treating ob-ject IDs as words so that standard word-vector-based similarity methods could be used to rank objects with respect to words. Plain MSG is outperforming nearly all earlier approaches by a large margin. The only method bettering it is the BEAGLE+PMI com-bination of Kievit-Kylar et al. (PMI measures direct co-occurrence of test words and object IDs). The latter was obtained through a grid search of all pos-sible model combinations performed directly on the test set, and relied on a weight parameter optimized on the corpus by assuming access to gold annotation. It is thus not comparable to the untuned MSG.
Plain MSG, then, performs remarkably well, even without any mechanism attempting to track word-object matching across scenes. Still, letting the model pay more attention to the objects currently most tightly associated to a word (AttentiveMSG) brings a large improvement over plain MSG, and a further improvement is brought about by giv-ing more weight to objects touched by the mother (AttentiveSocialMSG). As concrete examples, plain MSG associated the word cow with a pig, whereas AttentiveMSG correctly shifts attention to the cow. In turn, AttentiveSocialMSG associates to the right object several words that AttentiveMSG wrongly pairs with the hand holding them, instead.

One might fear the better performance of our models might be due to the skip-gram method be-ing superior to the older distributional semantic ap-proaches tested by Kievit-Kylar et al. (2013), in-dependently of the extra visual information we ex-ploit. In other words, it could be that MSG has sim-ply learned to treat, say, the lamb visual vector as an arbitrary signature, functioning as a semantically opaque ID for the relevant object, without exploit-ing the visual resemblance between lamb and sheep. In this case, we should obtain similar performance when arbitrarily shuffling the visual vectors across object types (e.g., consistently replacing each occur-rence of the lamb visual vector with, say, the hand visual vector). The lower results obtained in this control condition (ASMSG+shuffled visual vector) confirm that our performance boost is largely due to exploitation of genuine visual information.

Since our approach is incremental (unlike the vast majority of traditional distributional models that op-erate on batch mode), it can in principle exploit the fact that the linguistic and visual flows in the corpus are meaningfully ordered (discourse and vi-sual environment will evolve in a coherent man-ner: a hat appears on the scene, it X  X  there for a while, in the meantime a few statements about hats are uttered, etc.). The dramatic quality drop in the ASMSG+randomized sentences condition, where AttentiveSocialMSG was trained on IFC after ran-domizing sentence order, confirms the coherent sit-uation flow is crucial to our good performance. Minimal exposure. Given the small size of the in-put corpus, good performance on the word-object association already counts as indirect evidence that MSG, like children, can learn from small amounts of data. In Table 2 we take a more specific look at this challenge by reporting AttentiveSocialMSG perfor-mance on the task of ranking object visual represen-tations for test words that occurred only once in IFC, considering both the standard evaluation set and a much larger confusion set including visual vectors for 5.1K distinct objects (those of Lazaridou et al. (2015)). Remarkably, in all but one case, the model associates the test word to the right object from the small set, and to either the right object or another relevant visual concept (e.g., a ranch for moocows ) when the extended set is considered. The exception is kitty , and even for this word the model ranks the correct object as second in the smaller set, and well above chance for the larger one. Our approach, just like humans (Trueswell et al., 2013), can often get a word meaning right based on a single exposure to it. Generalization. Unlike the earlier models relying on arbitrary IDs, our model is learning to associate words to actual feature-based visual representations. Thus, once the model is trained on IFC, we can test its generalization capabilities to associate known words with new object instances that belong to the right category. We focus on 19 words in our test set corresponding to objects that were normed for visual similarity to other objects by Silberer and Lapata (2014). Each test word was paired with 40 ImageNet pictures evenly divided between images of the gold object ( not used in IFC), of a highly visually simi-lar object, of a mildly visually similar object and of a dissimilar one (for duck : duck , chicken , finch and garage , respectively). The pictures were represented by vectors obtained with the same method outlined in Section 3, and were ranked by similarity to a test word AttentiveSocialMSG representation.

Average Precision@10 for retrieving gold object instances is at 62% (chance: 25%). In the major-ity of cases the top-10 intruders are instances of the most visually related concepts (60% of intrud-ers, vs. 33% expected by chance). For example, the model retrieves pictures of sheep for the word lamb , or bulls for cow . Intriguingly, this points to classic overextension errors that are commonly reported in child language acquisition (Rescorla, 1980). While there is work on learning from multimodal data (Roy, 2000; Yu, 2005, a.o.) as well as work on learning distributed representations from child-directed speech (Baroni et al., 2007; Kievit-Kylar and Jones, 2011, a.o.), to the best of our knowledge ours is the first method which learns distributed rep-resentations from multimodal child-directed data. For example, in comparison to Yu (2005) X  X  model, our approach (1) induces distributed representations for words, based on linguistic and visual context, and (2) operates entirely on distributed represen-tations through similarity measures without posit-ing a categorical level on which to learn word-symbol/category-symbol associations. This leads to rich multimodal conceptual representations of words in terms of distributed multimodal features, while in Yu X  X  approach words are simply distributions over categories. It is therefore not clear how Yu X  X  ap-proach could capture phenomena such as predicting appearance from a verbal description or representing abstract words X  X ll tasks that our model is at least in principle well-suited for. Note also that Frank et al. (2007) X  X  Bayesian model we compare against could be extended to include realistic visual data in a sim-ilar vein to Yu X  X , but it would then have the same limitations.

Our work is also related to research on reference resolution in dialogue systems, such as Kennington and Schlangen (2015). However, unlike Kennington and Schlangen, who explicitly train an object recog-nizer associated with each word of interest, with at least 65 labeled positive training examples per word, our model does not have any comparable form of supervision and our data exhibits much lower fre-quencies of object and word (co-)occurrence. More-over, reference resolution is only an aspect of what we do: Besides being able to associate a word with a visual extension, our model is simultaneously learn-ing word representations that allow us to deal with a variety of other tasks X  X or example, as mentioned above, guessing the appearance of the object de-noted by a new word from a purely verbal descrip-tion, grouping concepts into categories by their sim-ilarity, or having both abstract and concrete words represented in the same space. Our very encouraging results suggest that multi-modal distributed models are well-suited to simu-lating human word learning. We think the most pressing issue to move ahead in this direction is to construct larger corpora recording the linguistic and visual environment in which children acquire lan-guage, in line with the efforts of the Human Spee-chome Project (Roy, 2009; Roy et al., 2015). Having access to such data will enable us to design agents that acquire semantic knowledge by leveraging all available cues present in multimodal communicative setups, such as learning agents that can automati-incorporate this knowledge into the semantic learn-ing process.
 We thank Marco Marelli for useful advice and Brent Kievit-Kylar for help implementing the Best-F mea-sure. We acknowledge the European Network on In-tegrating Vision and Language for a Short-Term Sci-entific Mission grant, awarded to Raquel Fern  X  andez to visit the University of Trento.

