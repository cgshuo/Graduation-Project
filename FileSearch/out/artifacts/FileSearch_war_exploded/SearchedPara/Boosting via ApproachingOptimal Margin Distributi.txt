 Behind the success, the working mechanism of AdaBoost has not been explained In the statistics community, researchers have devoted much effort to study Bayesian classifiers under some limitations [ 3 , 10 ]. However, these theories can X  X  explain the resistance to overfitting of boosting. Margin theory is another pop-ular explanation proposed by Schapire et al. [ 16 ], which argues that AdaBoost reduces the generalization error via improving the margin. Breiman [ 2 ]proveda minimum margin bound that is sharper than the bound given by Schapire et al. [ 16 ], thus they considered that minimum margin is more relevant to the boost-ing algorithms. Thus, much works [ 2 , 9 ] focused on maximizing the minimum margin. However, these algorithms do not always yield better performance. In fact, more often the opposite is true, and margin theory suffered serious doubt. Later, Koltchinskii et al. [ 11 , 12 ] showed the bound in Schapire et al. [ 16 ]canbe improved based on Rademacher and Gaussian complexities, but these bounds can not be proved to be sharper than Breiman X  X  minimum margin bound. Reyzin and Schapire [ 15 ] duplicated the experiments of Breiman X  X  and they observed some flaws that lead to poor control in model complexity. They emphasized that margin distribution rather than minimum margin is crucial to boosting. bution called Emargin bound, which was proved to be sharper than previously well-known bounds [ 2 , 16 ]. In particular, they showed that if a boosting algorithm minimizes the Emargin bound, the learned classifier would converges to the opti-from which they reformulated Emargin bound as the infimum of all the k th margin bound, that is, they proved that the Emargin bound is sharper than the minimum margin bound [ 2 ] from a new perspective. These results suggest a new boosting approach via optimizing Emargin bound. However, the Emargin bound can not be optimized easily. Although Wang et al. [ 20 ] designed the EEM algorithm to verify Emargin theory and obtained exciting results, the algorithm is not suitable in real applications due to the computational complexities.
 expectation and variance, and they proved that AdaBoost approximately max-imizes the unnormalized average margin and minimizes the margin variance. However, they provided no generalization error bound to support their method. tribution called k  X  -optimization margin distribution, and demonstrate that it would lead to a sharper generalization error bound than that of AdaBoost. From the definition we then develop two approximate algorithms, KM-Boosting and MD-Boosting. Both of the two algorithms present good results on benchmark datasets. In particular, the classifier generated by MD-Boosting empirically has a sharper generalization error bound than that of AdaBoost almost surely. More-over, MD-Boosting can be viewed as an effective method to reduce redundancy because of its limited computational cost and less loss of accuracy. is a given hypothesis space, and  X  h  X  X  is a mapping from X to Y .Let C ( H ) t is the corresponding weight of h t .
 For an example ( x i ,y i ), the margin y i f ( x i ) reflects the difference between Denote Bernoulli relative entropy function by , then with probability at least 1  X   X  over the random choice of sample with The minimum margin bound is the trivial condition of k th margin bound For convenience, we denote the Emargin bound and the k th bound of f by EB( f ) and KB( f, k ), respectively. Thus, we have We first define a kind of margin distribution that has good property according the margin theory of boosting.
 Definition 1. For  X  f, g  X  X  ( H ) , f is produced by AdaBoost, MD( g ) is the k  X  -optimization margin distribution if the inequality: holds for k  X  = arg min k  X  X  1 ,...,n } KB( f, k ) .
 following lemma.
 Lemma 1.  X   X  1 ( u ; v ) ia a monotone increasing function of v .
 Proof. Since for  X  u  X  v 1  X  v 2 &lt; 1, we have where the inequality holds from the relation This completes the proof of the lemma.
 Theorem 2. If MD( g ) is k  X  -optimization margin distribution, g has sharper Emargin bound than f , i.e., EB( g )  X  EB( f ) .
 and from Lemma 1 and formulae ( 2 ) and ( 3 ) we can easily get then This completes the proof.
 bution, and the purpose of this paper is to develop a new boosting algorithm with the k  X  -optimization margin distribution. We first discuss the second step. First run the standard AdaBoost T steps, Actually, formula ( 6 ) can be understood to maximize the minimum margin Algorithm 1. KM-Boosting samples as outliers, choosing k  X  means confirming the percentage of outliers. Here we consider the percentage of outliers in training set as follows: i.e., the Bayes error, where  X  y is the prediction of y given by Bayes classifier, and the approximation comes from the fact that S drawn independently from D . Further more, we approximate it by the validation error ve of AdaBoost. That is, we take is highly consistent with margin theory. Remind that  X  X ood X  margin distribution can be simply described as the distribution in which most examples has large margins. This idea is completely reflected in KM-Boosting, namely, the n  X  k  X  +1 examples are viewed as the  X  X ost X  examples. We maximize the minimum margin of them and don X  X  care the margins of the other k  X   X  1 points. 4.2 MD-Boosting KM-Boosting directly optimize the k  X  th margin, however, it must save some points as validation sets. Here we present another method that could obtain k -optimization margin distribution simply.
 the growth of iteration when k is small. However, when k reaches a certain value, the k th margin always decrease. We have mentioned that larger k th margin would produce smaller k th margin bound for a fixed k . This means, it X  X  the increased k th margin that really help to improve the Emargin bound. Based on This proposition is easy to get from formula ( 8 ). Therefore, the object turns Solving equation ( 9 ) is intractable, and we approximately formulate it as where and od( y i f ( x i )) is the order of y i f ( x i ) in all the margins. the result of AdaBoost, while the weights of easy points quickly vanish and give no (asymptotic) contribution. Actually, formula ( 10 ) can be viewed as a method to improve the margins of the hard points.
 1 shows that the average margin decreases while the iteration increases. In fact, this phenomenon appears in all datasets we have tested. So empirically we have for  X  k  X  K  X  , where  X  S ( Yf ( X )) is the average margin of f in S . Shen et al. [ 18 ]provedthe margin of AdaBoost follows the Gaussian distribution, thus | K  X  | X  n 2 , approxi-mately.
 computation and storage costs. Actually, it X  X  unnecessary since we find K  X  is not very sensitive to the result, that is, as long as | K  X  | reaches some certain value, the result will be good. So we replace the original K  X  with where q 0 =Pr S [ yf ( x )  X  0]. Therefore, the problem size of formula ( 10 )isat most 0 . 1  X  n + T when n is very large while other modified boosting algorithms usually require to solve a linear program with size n + T . The goal of this paper is not to find a boosting algorithm that outperforms all the other variants of boosting, but to show a way to approach the optimal margin distribution. So we only compare the proposed algorithms with AdaBoost. We verify the proposed algorithms on 13 benchmark datasets with two types of base classifiers, decision stumps and three-layer decision trees. We train on a randomly drawn subset of 40% of the examples in a data sets and validate on a subset of 20% of the examples, which is disjointed with the above training set, Algorithm 2. MD-Boosting Figure 2 and 3 present the comparison of the margin distributions on different gin theory of boosting. As far as we know, MD-Boosting is the first boosting algorithm that has this property. For all the other modified boosting algorithms, such as LP Reg -AdaBoost [ 14 ], MDBoost [ 17 ], and the KM-Boosting, the margin distribution curve usually appear above that of AdaBoost for some  X &gt; 0. Boosting, where the best performances are marked in bold face. The last row is the average ranking of the three methods on all 13 datasets. We can see both MD-Boosting and KM-Boosting have a better performance than that of AdaBoost on decision stumps, while KM-Boosting performs a little worse than AdaBoost does on decision trees. This may be due to the fact that some leaf nodes have no sam-ple, causing uncontrolled complexities. In consideration of both of accuracy and performance, it seems that decision stumps are more suitable for KM-Boosting. by MD-Boosting, which is a decision stump and a decision tree respectively. The sizes of both base classifiers could be reduced, especially the decision stump, the size does not exceed 50 on ten datasets. That is, MD-Boosting with decision stump is more effective for feature selection where small ensembles are needed. It is widely accepted that margin distribution plays an important role in the success of AdaBoost. Following this line, we first define the k  X  -optimization margin distribution, which is closer to the optimal margin distribution than that of AdaBoost. Then we propose two boosting algorithms: KM-Boosting and MD-Boosting, both of which can approximate the k  X  -optimization margin distri-bution and improve the accuracy of AdaBoost. Especially, MD-Boosting almost surely has a sharper generalization error bound than that of AdaBoost, and can
