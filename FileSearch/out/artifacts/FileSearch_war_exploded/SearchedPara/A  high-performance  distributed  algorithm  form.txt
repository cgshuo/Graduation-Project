
The economic value of data mining is today well established. Most large organiza-tions regularly practice data-mining techniques. One of the most popular techniques is association rule mining (ARM), which is the automatic discovery of pairs of elem-ent sets that tend to appear together in a common context. An example would be to discover that the purchase of certain items (say tomatoes and lettuce) in a super-market transaction usually implies that another set of items (salad dressing) is also bought in that same transaction.

Like other data-mining techniques that must process enormous databases, ARM is inherently disk-I/O intensive. These I/O costs can be reduced in two ways: by reduc-ing the number of times the database needs to be scanned or through parallelization, by partitioning the database between several machines which then perform a dis-tributed ARM (D-ARM) algorithm. In r ecent years, much progress has been made in both directions.
 frequently appear together X  X he frequent itemsets. The number of database scans required for the task has been reduced from a number equal to the size of the largest itemset in Apriori (Agrawal and Srikant 1994), to typically just a single scan in modern ARM algorithms such as Sampling and DIC (Toivonen 1996; Brin et al. 1997).
 tecture of the parallel system plays a key role. For instance, many of the proposed algorithms take advantage of the fast interc onnect, or the shared memory, of parallel computers. Notable examples include Han et al. (2000) and Zaki et al. (1997b). The latest development is Zaiane et al. (2001), in which each process makes just two passes over its portion of the database.
 were shown to scale up to 128 processors, few organizations can afford to spend such resources on data mining. The alternative is distributed algorithms, which can be run on cheap clusters of standard, off-the-shelf PCs. Algorithms suitable for such systems include the CD and FDM algorithms (Agrawal and Shafer 1996; Cheung et al. 1996), both parallelized versions of Apriori, published shortly after it was described. However, while clusters may easily and cheaply be scaled to hundreds of machines, these algorithms were shown not to scale well (Cheung and Xiao 1998).
The DDM algorithm (Schuster and Wolff 2001), which overcomes this scalability problem, was recently described. Unfortuna tely, all the D-ARM algorithms for share-nothing machines scan the database as man y times as Apriori. Since many business databases contain large frequent itemsets (long patterns), these algorithms are not competitive with DIC and sampling.

D-sampling. The algorithm is intended for clusters of share-nothing machines. The main obstacle of this parallelization, that of achieving a coherent view of the dis-tributed sample at reasonable communication costs, was overcome using ideas taken from DDM. Our distributed algorithm scans the database once, just like the sam-pling algorithm, and is thus more efficient than any D-ARM algorithm known today.
Not only does this algorithm divide the disk-I/O costs of the single scan by parti-tioning the database among several mach ines, it also uses the combined memory to linearly increase the size of the (global) sample. This increase further improves the performance of the al gorithm because the safety mar gin required in sampling decreases accordingly.
 is superior to previous algorithms in every way. When compared with sampling X  one of the best sequential algorithms known today X  X t offers superlinear speed-up.
When compared with FDM, it improves runtime by orders of magnitude. Finally, on scalability tests, an increase in both the number of computing nodes and the size of the database does not degrade D-sampling performance.
 some notations and a formal definition of the D-ARM problem. In the next section, we present relevant previous work. Sectio n 3 describes the D-sampling algorithm, and Sect. 4 provides the required statistical background. Section 5 describes the ex-periments we conducted to verify D-sampling performance. We conclude with some open research problems in Sect. 6.
Let I ={ i 1 , i 2 , ..., i m } be the items in a certain domain. An itemset is a subset of I . A transaction t is also a subset of I that is associated with a unique trans-action identifier X  TID . A database DB is a list of such transactions. Let DB {
DB 1 , DB 2 , ..., DB n } be a partition of DB into n parts. Let S be a list of trans-actions that were sampled uniformly from DB ,andlet S partition of S induced by DB . For any itemset X and any group of transactions A ,
Support ( X , A ) is the number of transactions in A that contain all the items of X and Fr e q ( X , A ) = Support ( X , A ) | A | . We call Fr e q partition i and Fr e q ( X , DB ) its global frequency; likewise, we call Fr e q estimated local frequency of X in partition i and Fr e q frequency.

For some frequency threshold 0  X  Min Freq  X  1, we say that an itemset X is frequent in A if Fr e q ( X , A )  X  Min Freq and infrequent otherwise. If A is a sample, we say that X is estimated frequent or es timated infrequent. If A is a partition, we say that X is locally frequent, and if A is the whole database, then X is globally frequent. Hence, an itemset may be estimated locally frequent in the k th partition, globally infrequent, etc. The group of all itemsets with frequency above or equal to fr in A is called F fr [ A ] . The negative border of F are not themselves in F fr [ A ] but have all their subsets in F of globally frequent itemsets X and Y such that X  X  Y threshold 0 &lt; MinConf  X  1, we say the rule X  X  Y is confident if and only if Fr e q ( X  X  Y , DB )  X  MinConf  X  Fr e q ( X , DB ) .

Definition 1.1. Given a partitioned database DB and given MinFreq and MinConf , the D-ARM problem is to find all the confident rules between frequent itemsets in F
Since its introduction in 1993, the ARM problem (Agrawal et al. 1993) has been studied intensively. Many algorithms, represe nting several differ ent approaches, were suggested. Some algorithms, such as Apriori, Partition, DHP, DIC, and FP-growth (Agrawal and Srikant 1994; Savasere et al. 1995; Park et al. 1995a; Brin et al. 1997; Han et al. 1999), are bottom-up, starting from itemsets of size 1 and working up.
Others, like Pincer-Search (Lin and Kedem 1998), use a hybrid approach, trying to guess large itemsets at an early stage. Most algorithms, including those cited above, adhere to the original problem definition, while others search for different kinds of rules. These may be implication rules (Brin et al. 1997), generalized rules (Srikant and Agrawal 1994; Han and Fu 1995), quantitative rules (Srikant and Agrawal 1996) or rules constrained to some meta-form (Srikant et al. 1997; Pei and Han 2000;
Thomas and Chakravarthy 2000). Finally, the algorithms also differ in the way the data are stored: horizontally as a TID with the list of items in that transaction, vertically as an itemset with the list of TIDs in which it appears (Savasere et al. 1995; Ananthanarayana et al. 2000), or a combination of the two (Zaki et al. 1997a).
Algorithms for the D-ARM problem usually can be seen as parallelizations of sequential ARM algorithms. The CD, FDM, FPM and DDM (Agrawal and Shafer 1996; Cheung et al. 1996; Cheung and Xiao 1998; Schuster and Wolff 2001) algo-rithms parallelize Apriori (Agrawal and Srikant 1994), and PDM (Park et al. 1995b) parallelizes DHP (Park et al. 1995a). The major difference between parallel algo-rithms is in the architecture of the parallel machine. This may be shared memory, as in the case of Zaki et al. (1996), Cheung and Xiao (1998) and Zaiane et al. (2001), distributed shared memory, as in Jarai et al. (1998), or shared nothing, as in Agrawal and Shafer (1996), Cheung et al. (1996) and Schuster and Wolff (2001).
It first mines a sample of the database a nd then validates the result. It can thus be seen as a parallelization of the sampling algorithm (Toivonen 1996). The sam-(1995) and Ananthanarayana et al. (2000), a nd it is mined using modifications of the DDM (Schuster and Wolff 2001) algorithm, which is Apriori based. We thus include a short description of Apriori and its parallelizations and of the sequential sampling algorithm.

Apriori: A year after the 1993 paper that introduced the ARM problem, Agrawal and Srikant presented Apriori (Agrawal and Srikant 1994). Apriori is a levelwise algorithm for identifying frequent itemset s. It begins by assuming that each item is a candidate to be a frequent itemset of size 1. Then Apriori performs several rounds of a two-phased computation. In the first phase of the k th round, the database is scanned and frequency counts are calculated for all k -sized candidate itemsets (itemsets containing k items). Those candidate itemsets with a frequency above the user-supplied MinFreq threshold are inserted into F Min Freq candidate itemsets of size k + 1 are generated from the frequent itemsets of size k if and only if all their size-k subsets are frequent. The rounds terminate when there are no candidates of size k + 1. Because it is a levelwise algorithm, Apriori performs exactly k database scans.

Sampling: In 1996, Toivonen presented a single-scan algorithm called sampling (Toivonen 1996). The idea behind sampling is simple. A random sample of the database is used to predict all the frequent itemsets, which are then validated in a single database scan. Because this appro ach is probabilistic and therefore fallible, not only are the frequent itemsets counted in the scan but also their negative border.
If the scan reveals that itemsets that were p redicted to belong to the negative border are frequent, a second scan is performed to discover whether any superset of these itemsets is also frequent. To further reduce the chance of failure, Toivonen suggests that mining be performed using some lo w _ fr &lt; Min Freq and the results reported only if they pass the original Min Freq threshold. He also gives a heuristic that can be used to determine lo w _ fr . The cost of using lo ber of candidates. The sampling algorithm and the DIC algorithm (Brin et al. 1997) are the only single-scan ARM algorithms known today. Their performance is thus unrivaled by any other sequential ARM algorithm.

FDM: Also in 1996, Cheung et al. presented an algorithm called FDM (Cheung et al. 1996). FDM is a parallelization of Apriori to n shared-nothing machines, each with its own partition of the database. At every level and on each machine, the database scan is performed independently on the local partition. Then a distributed pruning technique is employed. The pruning technique is based on the inference that, in order for an itemset to appear in the database at a certain frequency, it must appear with at least that frequency in at least one partition of the database. Thus, in
FDM, every party first names those candidate itemsets that are locally frequent in its partition. Next, support counts are globally summed for those candidate itemsets that were named by at least one party. According to the global counts, itemsets are identified as globally frequent. Those frequent itemsets are used to generate the next level candidates.

If the probability that an itemset has the potential to be frequent is Pr then FDM only communicates Pr potential | C | of the itemsets, where C is the group of all candidate itemsets considered by Apriori. The communication complexity of
FDM is thus O ( Pr potential | C | n ) . The main problem with FDM is that Pr scalable in n . It has been shown by Cheung and Xiao that Pr to1as n increases (Cheung and Xiao 1998). The convergence to 1 is especially fast in nonhomogeneous databases: as the nonhomogeneity of the database (measured by a skewness measure) increases or the number of partitions grows, FDM pruning techniques are rendered increasingly ineffective.

DDM: In a previous paper (Schuster and Wolff 2001), we described another Apriori-based D-ARM algorithm X  X DM. As in FDM, candidates in DDM are generated levelwise and are then counted by each node in its local database. The nodes then perform a distributed decision protocol in order to find out which of the candidates are frequent and which are not. DDM differs from FDM in that the DDM protocol allows some of the nodes to choose to publish the local frequency of a candidate and others not to. The protocol is directed by two hypotheses that are maintained about each candidate: in one, called the public hypothesis, each node assumes that the global frequency of the itemset is equal to the average of the local frequencies published for it thus far (or zero if none was published); in the other, called the private hypothesis, each node assumes that its local frequency is shared by all those that have not published their own local frequency for the candidate. If a node finds that the public and private hypotheses about an itemset disagree (i.e. one predicts that the local frequency. It is easy to show that, when the protocol dictates that no node should publish the local frequency of a certain itemset, the public hypothesis for that itemset correctly predicts whether it is frequent or infrequent. DDM improves the communication complexity of previous solutions to O ( is the chance of an itemset being locally frequent at a specific partition. Pr by definition smaller than Pr potential and is also independent of n . DDM is thus far more communication efficient, scalable, and resilient to data skewness.
The distributed algorithms described in the previous section are based on Apriori. In-deed, all parallel algorithms that have been presented until now are levelwise and re-quire multiple database scans 1 . The reason why no distributed form of sampling was suggested in the 6 years since its presentation may lie in the communication com-plexity of the problem. As we have seen, the communication complexity of D-ARM algorithms is highly dependent on the number of candidates and on the noise level in the partitioned database. When the sampling algorithm samples the database and lowers the Min Freq threshold, it greatly increases both the number of candidates and the noise level. This may render a distributed algorithm useless. offer an opportunity. The main idea of D-sampling is to utilize DDM to mine a dis-tributed sample using lo w _ fr instead of Min Freq .After F fied, the partitioned database is scanned once in parallel to find the actual frequencies of F lo w _ fr [ S ] and its negative border. Those frequencies can then be collected and rules can be generated from itemsets more frequent than Min Freq .
 levelwise, here it is executed on a memory-resident sample. Thus, we could modify
DDM to develop new itemsets on the fly and calculate their estimated frequency with no disk-I/O. Second, a new method for the reduction of Min Freq to lo additional benefits: it uses a rigorous e rror bound, compared with the heuristic one used in sampling, and it produces far fewer candidates than the rigorous method sug-gested previously. Third, after scanning the database, it would not be wise to merely collect the frequencies of all candidates. B ecause these candidates were calculated according to the lowered threshold, few o f them are expected to have frequencies above the original Min Freq . Instead, we run DDM once more to decide which can-didates are frequent and which are not. We call the modified algorithm D-sampling (Algorithm 1).
 Algorithm 1 D-sampling
D-sampling begins by loading a sample into memory. The sample is stored in a trie X  a lexicographic tree. This trie is the main data structure of D-sampling and is ac-information (parents, descendants etc.), the list of TID s of those transactions that include the itemset associated with this node. These lists are initialized from the sample for the first level of the trie; when a new trie node X  X nd itemset X  X re de-veloped, the TID lists of two of the parent nodes are intersected to create the TID list of the new node.

Figure 1 describes the development of the trie throughout D-sampling. The first step of D-sampling is to run a modification of DDM on the distributed sample.
Then, in order to set lo w _ fr , the algorithm enters a loop; in each cycle through the loop, it calls another DDM derivative called M-Max to mine the next M estimated frequent itemsets. M is a tunable parameter we set to about 100. After it finds those additional itemsets, D-sampling reduces lo w _ fr to the estimated frequency of the least frequent one and re-estimates the error probability using a formula described in Sect. 4. When this probability drops below the required error probability, the loop ends. Then D-sampling creates the final candidate set C by adding to F negative border.

Once the candidate set is established, each partition of the database is scanned exactly once and in parallel, and the actual frequencies of each candidate are calcu-lated. With these frequencies, D-sampling performs yet another round of the modified DDM. In this round, the original Min Freq is used; thus, unless there is a failure, no candidates outside the negative border need to be used. If indeed no failure oc-curs, then all frequent itemsets will be eval uated according to the actual frequencies that were found in the database scan. Hence, after this round, it is known which of the candidates in C are globally frequent and which are not. In this case, rules are generated from F Min Freq [ DB ] using the known global frequencies.
 frequent, this means that D-sampling has failed: a superset of that candidate, which was not counted, might also turn out to be frequent. In this case, we suggest the same solution offered by Toivonen: to create a group of additional candidates that includes all combinations of anticipated an d unanticipated frequent itemsets, and then perform an additional scan. The size of this group is limited by the number of antic-ipated frequent itemsets times the number of possible combinations of unanticipated frequent itemsets. Because failures are ve ry rare events and the probability of mul-the same scale as the first scan.
The original DDM algorithm, as described i n Sect. 2, is levelwise. When the database is small enough to fit into memory, the levelwise structure of the algorithm becomes superfluous. Modified Distributed Decision Miner, or MDDM (Algorithm 2), there-fore starts by developing all the locally frequent candidates regardless of their size.
It then continues to develop candidates whenever they are required, i.e. when all their subsets are assumed frequent (according to the local hypothesis X  P )orwhen another node refers to the associated itemset.
 itemsets for which the global hypothesis and local hypothesis disagree and commu-nicate their local counts to the rest of the parties. When no such itemset exists, the ties pass, the algorithm terminates and the itemsets that are predicted to be frequent according to the public hypothesis H are the estimated globally frequent ones. ceived. First, the locally frequent itemsets are developed, their TID lists calculated and their public hypothesis and private hypothesis evaluated ( H and P , respectively).
The starting value of H is zero and that of P is the local frequency. As messages are received, those values change. I temsets are sent when their H and P are on opposite sides of Min Freq . Therefore, in this toy example, where Min Freq is 0 { 1 } is sent (not all eligible candidates have to be sent on each communication cycle).
When a message is received about an itemset that has already been developed (as is is received for an itemset that has not yet been developed (as is the case for it is developed on the fly and its local frequency is calculated. The modified DDM algorithm identifies all itemsets with frequency above Min Freq .
D-sampling, however, requires a further decrease in the frequency of itemsets that are included in the database scan. The reason fo r this, as we shall see in Sect. 4, is that three parameters affect the chances for f ailure. These are the size of the sample, N , Algorithm 2 Modified distributed decision miner the size of the negative border, and the estimated frequency of the least frequent candidate. The first parameter is given, the second we can calculate or bound and the last parameter is the one we can control.

The frequency of the least frequent ca ndidate can be controlled by reducing lo w _ fr . However, this must be done with care: lowering the frequency threshold increases the number of candidates. This increase depends on the distribution of itemsets in the database and is therefore nondeterministic. The larger number of can-didates affects the scan time: the more candidates you have, the more comparisons must be made per transaction. In a distributed setting, the number of candidates is also strongly tied to the communication complexity of the algorithm.

To better control the reduction of lo w _ fr , we propose another version of DDM called M-Max (Algorithm 3). M-Max increases the number of frequent itemsets by a given factor rather than decreasing the th reshold value by an arbitrary value. Al-though worst-case analysis shows that an increase of even one frequent itemset may require that any number of additional candidates be considered, the number of such candidates tends to remain small and r oughly proportional to the number of addi-tional frequent itemsets. We complement this algorithm with a new bound for the error (presented in Sect. 4). The combined scheme is both rigorous and economical in the number of candidates.
 Algorithm 3 M-Max threshold to the H value of the M largest itemset 2 every time an itemset is de-veloped or a hypothesis value is changed will result in all parties agreeing on the
M most frequent itemsets when DDM termin ates. This is easy to prove. Take any final state of the modified algorithm. The H value of each itemset is equal in all parties; hence, the final Min Freq is equal in all parties as well. Now compare this state with the corresponding state under DDM, with the static Min Freq value set to the one finally agreed upon. The state attained by M-Max is also a valid final state for this DDM. Thus, by virtue of DDM corr ectness, all parties must be in agreement on the same set of frequent itemsets.
As a stand-alone ARM algorithm, M-Max may be impractical because a node may be required to refer to itemsets it h as not yet developed. If the database is large, this would require an additional disk scan whenever new candidates are de-veloped. Nevertheless, at the lo w _ fr correction stage of D-sampling, the database is the memory-resident sample. It is thus possible to evaluate the frequency of arbitrary itemsets with no disk-I/O.
Two statistical issues should be settled in o rder to validate that D-sampling has the required failure probability. The first is bounding the probability of failure that fol-lows the error adjustment phase. The second is showing how a distributed database can be sampled uniformly.
Let 0 &lt; fr &lt; 1 be the frequency of some arbitrary itemset X in DB . Consider a random sample S of size N from DB . We will assume that transactions in the sample are independent. Hence, the number of rows in S that contain X can be seen as a random variable, x  X  Bin ( N , fr ) .

The frequency of X in N transactions, s _ fr = x / N , is an estimate for fr ,which improves as N increases. The best-known way to bound the chance that s _ fr will deviate from fr is with the Chernoff bound. We use a tighter bound for the case of binomial distributions (see Hagerup and Rub (1989/90)):
Lemma 4.1. Given a random uniform sample S of N transactions from DB , a fre-quency threshold Min Freq , the lowered frequency threshold lo border of F lo w _ fr [ S ] , denoted NB , the probability p frequency larger than or equal to Min Freq (hence causing failure) is bounded by:
Proof. For any specific itemset in NB , the probability that this itemset will cause failure is the probability that its estimated frequency is below lo frequency is above Min Freq . Substituting Min Freq for fr and lo bound gives us to relax this bound by substituting | I || F lo w _ fr in F lo w _ fr [ S ] can only be extended by at most | I | holds.

Corollary 4.1 (Toivonen 1996). If none of the itemsets in the negative border caused failure, then no other itemset can cause failure.
 Proof. Any other itemset X outside F lo w _ fr [ S ] and NB must include a subset from NB . Hence, its frequency must be less than or equal to the frequency of this subset.
It follows that, if the frequency of each itemset in NB is below Min Freq ,soisthe frequency of X .
Uniform sampling is not a simple task in any database. At worst, it may require as much as a full scan of the database to ensure uniformity. Partitioning the database, as we do, adds a further complication. Here we show that any existing method for uniformly sampling a single database can be leveraged into a scheme for sampling partitioned databases.
 from the partitioned database, we first uniformly choose a partition formly choose a transaction from the chosen partition. Extending this to a sample of size | S | , we first choose randomly, for each transaction in the sample, the partition from which it will be sampled. Then, knowing exactly how many transactions should be sampled from each partition, we randomly choose that number of transactions.
Note that the theoretical bound we use allows sampling with repetitions; the algo-rithm, however, will require slight modifications for a single TID to appear twice in the sample.
 ple. Because local sample sizes are select ed randomly, one of these local samples may be small. Small samples are, by definiti on, noisier than large ones. Because the performance of DDM depends on Pr abo v e and hence on the noisiness of the data, a sample that is biased against a specific partition may result in a longer run time. distributed multinomially. The expect ed number of transactions from each of the n partitions is hence | S | n . Because we choose the partitions independently, we can apply the Chernoff bound to the size of the sample from a specific partition,
Taking = 10%, we get Pr | S i | X  0 . 9 | S | n  X  e  X  80,000  X  n . This is based on the size of the sample in Toivonen X  X  experiments: be-tween 20,000 and 80,000 transactions. The chance of having a 10% smaller sam-ple with these figures is negligible: less than e  X  400 sample size will not have any noticeable eff ect on the noise level or on the run time.
Because the chances of a sample that is lar gely biased toward a specific partition are slim, the best thing to do if such a sample does occur is to sample once again.
Moreover, in many practical scenarios, it is known that the partitioning of the data was random. In that case, it is justified to simply sample an equal portion of each partition. In our experiments, we used this last method.
We carried out four sets of experiments. The first set tested D-sampling to see how much faster it is to run the algorithm with the database split among n machines than to run it on a single node. The second set compared D-sampling and FDM on a range of Min Freq values. The third set checked s cale-up: the change in runtime when the number of machines is increased together with the size of the database.
The last one examined the number of redundant itemsets D-sampling generates and compared it with FDM, which generates no redundant candidates.

We ran our experiments on two clusters: the first cluster, which was used for the first, second and fourth sets of experiments, consisted of 15 Pentium computers with dual 1.7-GHz processors. Each of the computers had at least 1 gigabyte of main memory. The computers were connected via an Ethernet-100 network. The second cluster, which we used for the scale-up experiments, was composed of 32 Pentium computers with a dual 500-MHz processo r. Each computer had 256 megabytes of memory. The second cluster was also connected via an Ethernet-100 network.
All of the experiments were performed with synthetic databases produced by the standard gen tool (Srikant 1993). The databases were built with the same parameters that were used by Toivonen in Toivonen (1996). The only change we made was to enlarge the databases to about 18 gigabyt es each; had we used the original sizes, the whole database would fit, when partitioned, into the memory of the computers.
The database T5.I2.D600M has 600 M trans actions, each containing an average of five items, and patterns of length two. T10.I4.D375M and T20.I6.D200M follow the same encoding. When the database was to be partitioned, we divided it arbitrarily by writing transaction TID into the TID % n partition.
The speed-up experiments were designed to demonstrate that parallelization works well for sampling. We thus ran D-sampling with n = reverts to sampling) on a large database. Then we tested how splitting the database between n computers affects the algorithm X  X  performance.

As Fig. 3 shows, the basic speed-up of D-sampling is slightly sublinear. However, when the number of candidates is large, the speed-up becomes superlinear. This is because the global sample size increases with the number of computers. This larger sample size translates into a higher lo w _ fr value and thus to a smaller number of candidates than with n = 1.

The second set of experiments (Fig. 4) investigates D-sampling X  X  performance de-pendency on Min Freq , which determines the number and size of the candidates. We compared the D-sampling runtime with that of both DDM and FDM. D-sampling turned out to be insensitive to the reduction in Min Freq ; its runtime increased by no more than 50% across the whole range. On the other hand, the runtime of DDM and FDM increased rapidly as Min Freq decreased. This is because of the addi-tional scans required as inc reasingly larger itemset s become frequent. Because it performs just one database scan, D-sampling is expected to be superior to any lev-elwise D-ARM algorithm, just as sampling is superior to all levelwise ARM algo-rithms.
The third set of tests was aimed at testing the scalability of D-sampling. Here the partition size was fixed. We used a database of about 1.5 gigabytes on each computer.
A scalable algorithm should have the same runtime regardless of the number of computers.

D-sampling creates the same communi cation load per candidate as DDM. How-ever, because it generates more candidates, it uses more communication. As can be for midrange numbers of computers, D-sampling runs even faster than with n due to the superlinear speed-up discussed earlier. The mild slowdown seen in Fig. 5c is due to the smaller average pattern size and the smaller number of candidates in
T5.I2.D1200M. The larger the number of candidates, the greater the saving in can-didates when the number of computers increases. If there are enough large patterns, this saving will compensate for the increasing communication overhead. Such is not the case, however, with T5.I2.D1200M.
Because the main disadvantage of the seque ntial ing algorithm is the large number of candidates it generates, our last set o f experiments was aimed at testing how many of the candidates are actually redundant. We first obtained the optimal number of candidates by running FDM on a set of small databases and then ran D-sampling on these databases. As bef ore, we used samples of 80 K transactions and maximum error probability  X  = 0 . 001.
 Figure 6 compares the number of candidates resulting from Chernoff and from Hagerup error bounds in D-sampling, as opposed to the number of candidates in
FDM. It can be seen that the number of candidates in D-sampling is strongly tied to the bound the algorithm used for calcula ting the probability of error. The Cher-noff bound suggested by Toivonen in sequential sampling produces relatively many candidates to satisfy the error probability condition. The Hagerup bound we use is tighter and produces significantly fewer ca ndidates. The table summarizes the over-head of candidates posed by D-sampling for some databases and values of Min Freq .
Our experiments show that D-sampling does not pose large candidate overhead when compared with the number of candidates generated by FDM.
We presented a new D-ARM algorithm that uses the communication efficiency of the DDM algorithm to parallelize the single-scan sampling algorithm. Experiments prove that the new algorithm has superlinear speed-up and outperforms FDM with any Min Freq value. The exact improvement in relation to FDM or DDM depends on the number of database scans they require . Experiments demonstrate good scalability, provided the database scan is the major bottleneck of the algorithm. titioning the database until every partition becomes memory resident. This approach may lead to a D-ARM algorithm that mines a d atabase by loading it into the mem-ory of a large number of computers and then runs with no disk-I/O at all. Second, it would be interesting to have a parallelized version of the other single-scan ARM algorithm X  X IC X  X n a share-nothing cluster, or of the two-scans partition algorithm.
Finally, we feel that the full potential of the M-Max algorithm has not yet been re-alized; we intend to research additional applications for this algorithm.
