 ments. In order to help a user in such a case, the query expansion approach becomes very important which focus on generating new queries by adding words to the original query. 
Query expansion is the approach of adding words to the original query to improve retrieval performance in information retrieval operations. It can discover some related property between words and original query au tomatically by analyzing a collection of documents or some resources. There are two main challenges in the query expansion process. One is where should the additional words be found. As an external resource, engine log data offers excellent opportunities for additional words mining. The other one is what types of association are useful to help improve the queries quality. In the them as candidate terms. generated by URLs when a user inputs a query. The query-click graph is a weighted bi-partite graph [3], with queries on one side and URLs on the other. Then, the term-relationship graph which was obtained by several transformations from the query-vised algorithm normalized weight SimRank (NWS). Term pairs with high similarity scores are with high confidence and then used for query expansion. 
In the next section, we discuss the related work of query expansion and query logs rithm and our normalized weight SimRank algorithm. Section 5 shows the candidate last section is conclusion. 2.1 Query Expansion In query expansion, the query is expanded using words or phrases with similar meaning to those in the query and the chances of matching words in relevant documents are there-fore increased. One of the earliest studies of query expansion was carried out by [5] who clustered words based on co-occurrence in documents and used those clusters to expand the queries. The techniques that have been used recently can be described as being based on either global or local analysis of the documents in the corpus being searched. produce consistent effectiveness improvements through automatic expansion. Other dexing) or LDA (Latent Dirichlet allocation), and have also reported good results [6]. Among all the local analysis approaches, pseudo-relevance feedback (PRF) exploiting the retrieval result has been the most effective [7]. It has been implemented in differ-framework [10]. In these works, the top ranked documents were assumed to be rele-vant as a special case of relevance feedback. 
Compare with PRF approaches a crucial question is the expansion terms deter-external collection enrichment approaches have been proposed, such as search engine query logs [2], WordNet [12], Wikipedia [13] etc. Our work follows this strategy of a query expansion approach using query logs as a resource of query expansion terms. 2.2 Query Graphs Baeza-Yates [14] identifies five different types of query graphs based on query logs. In all cases, the nodes are queries, a link is introduced between two nodes respectively if: (1) the queries contain the same word(s) (word graph), (2) the queries belong to the same session (session graph), (3) users clicked on the same URLs in the list of their link graph), (5) there are l common terms in the content of the two URLs (link graph). they behave after a query and the content distribution of what they look at. Moreover the authors study several characteristics of click graphs, i.e., Query-click graphs (also URL that was an answer for the query. This framework is used to infer semantic rela-several topics or a single very general topic. Query-click graphs are presenting more in detail in Section 3. 3.1 Query-Click Graph Query-click graph (also known as query-document graph) is introduced by [3]. It is a weighted bi-partite graph which the nodes in the one set are queries and the nodes in query q and a URL u if the user that issued the query, clicked on the URL in the list of results. Let and URLs clicked for those queries, q v is a node of query and u v is a node of URL in the is the number of clicks there are for a query q on a URL u . 
Specifically, the query-click graph constructed by the following four steps: 1. Remove all noisy records and stopwords from the query logs (details in 2. Add a new query node to the query-click graph for every unique query in the 3. In the same way, add a new URL node to the graph for every unique URL in 4. Make a directly connection edge form every query-URL node, if they appear 3.2 Term-Relationship Graph directed edge which appears between two term nodes, represent a kind of direct rela-tionship, if the user input them in different queries, and clicked the same URL in their associated degree between two terms i and j . 
The term-relationship graph can be obtained by two transformations from the query-click graph. 3.2.1 Query Nodes Substitution But in this paper, we propose to mine query logs for query expansion at the level of nodes for query nodes by following four steps: 1. For every query node q v in the query-click graph, replace it with all term 2. If the node does not exist, build this node. 3. Copy all adjacency edges from every query node to all terms nodes include in 4. Delete all query nodes and their adjacency edges. Then we obtain a graph composed by term nodes, URL nodes and directed edges between them. 3.2.2 URL Nodes Elimination Query expansion technique needs the relations at the level of terms, so the URL nodes are unnecessary, and what X  X  more, it will cause lots of extra cost. For this reason, we decide to eliminate them by following steps: 2. Delete all URL nodes and their adjacency edges. pute by following formula: form query-click graph into term-relationship graph, and the transformation has many practical significance: 1. The term-relationship graph is conducive to mine the association between 2. The analysis is at the level of terms rather than at the level of queries, and it is 3. Compared to the number of queries and URLs, the number of terms is quite 4. Compared to the bi-partite graph model, the process of SimRank and optimi-4.1 Na X ve Method SimRank [4] is a method for computing object similarities, applicable in any domain behind it is that, in many domains, similar objects are related to similar objects. More themselves. each pair of objects. In our case, we can consider the terms as one type of objects and use SimRank to compute similarity scores for each term-term pair. naive equation: between 0 and 1, gives the rate of decay as similarity flows across edges. In the Sim-equations always exists and is unique. Al so notice that the SimRank scores are sym-metric, i.e. ( , ) ( , ) ab ba Sim v v Sim v v = . 
According to the Equation (2), we can use the SimRank score as the association for query expansion instead of co-occurrence degree and probability of translating. 4.2 Normalized Weight SimRank In the term-relationship graph, there may be many  X  X npopular X  terms, i.e., term nodes with very few in-degree. Although the scarcity of contextual information makes them difficult to analyze, these terms are often the most useful, since they tend to be harder terms with little contextual information in sparse graph. properly identify term similarities in our application. In the naive form SimRank, the pairs by just using the term-relationship graph X  X  structure. relationship graph by following the next equation: the new form SimRank, named normalized weight SimRank (NWS), we modify the Equation (2) as follows: NWST I v I v I v v I v v NWS I v I v  X  X  = X   X  (5) from a v to b v . 4.3 Pruning computing. The space required is simply 2 () n  X  to store the results. The time required is 22 () Kn d the query logs. Thus, in this subsection we do briefly consider pruning techniques that reduce both the time and space requirements. 
Our way to reduce the resource requirements is to prune the term-relationship it. Thus one pruning technique is to set the similarity between two nodes far apart to works, we consider only node-pairs within a radius r of n from each other, and there become 2 () r Knd d  X  and ( ) r nd  X  respectively. and then the time complexities become 2 () rm Knd d  X  . 
Of course, the quality of the approximation needs to be verified experimentally for the actual datasets. For the case of scientific papers, our empirical results suggest that found in Section 6. the approach select candidate terms and do query expansion by following three steps: 3. Join all remaining candidate terms in the original query, and then retrieval us-In the follow sections, the method above is called NWSE (Normalized Weight Sim-Rank based Expansion). 6.1 Dataset Our experiments are based on the query logs distributed by AOL, containing 36,389,567 records, 10,154,742 distinct queries and 657,426 users. Most of them are query, a timestamp, and the results (for each result, the position on the result page is also provided). 
All evaluation is done using three standard TREC collections: Robust04 and Gov-2, containing 528,155 documents and 25,205,179 documents respectively. It should tion, while the Gov-2 is a web collections. 
We used 150 queries associated with each collection for evaluation (No.301-450 queries for Robust04 and No.701-850 queries for GOV-2 respectively). In particular, we treat all topic titles as queries and neglect their description. 6.2 Design of Experiments 6.2.1 Baseline baseline for comparison. One is the language model implemented by Indri toolkit and the Dirichlet smoothing prior  X  was set to 1500 empirically. This method is denoted by LM-Dir. The other one, we implemented a Relevance-Based Language Model, one of the PRF expansion approaches, based on the method by Lavrenko and Croft [10], denoted by RM. 
We used precision at 10 (P@10), precision at 20 (P@20) and MAP to measure re-trieval effectiveness for all experiments. 6.2.2 Query Expansion with NWSE In order to remove noisy records, we remove all index queries and meaningless que-ries from AOL query logs, i.e., queries such as  X  X idelity.com X  and  X  X u 20v 20----- X . number of adult queries, and we did not use it in our experiments, though. For creat-ing the term-relationship graph, we creating the query-click graph first, that contains approximately 4 million query nodes and 2 million clicked URL nodes. After several tained finally, containing 413,013 term nodes. Table 1 shows that how the m d (the maximum number of in-edges considered by the NWS) influences the NWS scores, When M (the number of candidate terms re-mained) is set to 5 empirically. We find that the NWS scores nearly no longer grow, if the value of m d is bigger than 500, so the m d is set to 500 in all experiments. iterations) to 2 and 5 respectively. 
According to the description in section 5, we can expand the original query weight candidate terms. The candidate terms and the original query terms are combined using  X #weight X  operator and  X #combine X  operator implemented in Indri, and Equation 6 is implemented by creating an Indri query of the form: creating the final query. One is all candidate terms are considered equally good in the Q , named NWSE-Unweighted. The other one is adding their NWS scores in the final query as weights, named NWSE-Weighted. 6.3 Results well when 0.3  X  = and 0.6  X  = . On the other hand, the MAP of the NWSE-Weighted method and NWSE-Unweighted method is higher than the RM method X  X . The NWSE-Weighted method and NWSE-Unweighted method performed the best when  X  is set to 0.5 for Robust 04, 0.9 and 0.8 respectively for Gov-2. Finally, we set the parameter  X  to optimal values for each of methods. 
The Figure above show the results of assigning di ff erent value to M for all meth-ods on three TREC collections. The MAP of our methods are 6.19% and 1.81% higher than the query expansion based on pseudo relevance feedback, 7.49% and 5.44% higher on P@10, and 10.96% and 3.73% higher on P@20 for Robust 04 and Gov-2 respectively. 
According to Figure 2, we can see the NWSE-Weighted and NWSE-Unweighted methods preform as well as the RM model does in the MAP evaluation. We also can pansion terms selected from query logs by NWSE is more relevance than them ex-tracted from pseudo-relevance documents. 
Figure 2 shows the trend of MAP, P@10 and P@20 for all methods. Comparing to very great scope enhancement in the evaluating metrics. The NWSE-Weighted and NWSE-Unweighted methods perform well when 20 40 M &lt;&lt; , but RM method does process can be finish in a short time by using the NWSE methods, just because more terms in query cause more time and space cost. Specially, we notice that the NWSE-Weighted works better and more stable than NWSE-Unweighted, and it proves that the weights provide by NWS scores is quality. In this paper, we have explored the AOL query logs as a new resource for query ex-side and URLs on the other, and then, obtained the term-relationship graph by several transformations from it. We also proposed new methods for mining expansion terms from AOL query logs. Our methods named normalized weight SimRank are based on a revised SimRank algorithm, and we measure the importance of expansion terms according to their NWS with the original query terms. Our experiments show that the expansion terms extracted from AOL query logs are better than those from the feed-back documents, using a simple statistical method, on the di ff erent TREC collections. click-throughs instead of real relevance judgments, so a future work would be to test the proposed methods with real relevance data. Second, query logs have more mean-ingful information of sessions was neglected in our experiments. The last, because of the great capacity for query log, we do not have enough time and equipment for prun-ing radius optimization. So we will pay more attention on this in our future work. China (No. 60673039 and 60973068), the National High Tech Research and Devel-opment Plan of China (2006AA01Z151), the Doctoral Fund of Ministry of Education of China 20090041110002. 
