 Collaborative filtering is a fundamental building block in many rec-ommender systems. While most of the existing collaborative filter-ing methods focus on explicit, multi-class settings (e.g., 1-5 stars in movie recommendation), many real-world applications actually belong to the one-class setting where user feedback is implicitly expressed (e.g., views in news recommendation and video recom-mendation). The main challenges in such one-class setting include the ambiguity of the unobserved examples and the sparseness of existing positive examples.

In this paper, we propose a dual-regularized model for one-class collaborative filtering. In particular, we address the ambiguity chal-lenge by integrating two state-of-the-art one-class collaborative fil-tering methods to enjoy the best of both worlds. We tackle the sparseness challenge by exploiting the side information from both users and items. Moreover, we propose efficient algorithms to solve the proposed model. Extensive experimental evaluations on two real data sets demonstrate that our method achieves significant im-provement over the state-of-the-art methods. Overall, the proposed method leads to 7.9% -21.1% improvement over its best known competitors in terms of prediction accuracy, while enjoying the lin-ear scalability.
 H.2.8 [ Database Management ]: Database applications X  Data min-ing Recommender systems, one-class collaborative filtering, dual reg-ularization
Recommender systems have become increasingly indispensable in many applications including movie recommendation [16], hash-tag recommendation [2], music recommendation [5], news recom-mendation [6], etc. Collaborative filtering, which aims at predicting the preferences of users towards items based on the historical user feedback, plays a central role in these recommender systems. To reflect the users X  preferences for items, the feedback can be explic-itly expressed as different ratings (e.g., 1-5 stars in Netflix). The vast majority of existing work focuses on such explicit, multi-class recommendation problem. However, in many real situations, the feedback could be implicitly expressed with examples like views of news, clicks of webpages, purchases of products, downloads of mu-sic, etc. Recommendation with implicit feedback naturally forms the one-class collaborative filtering (OCCF) problem [24].
Despite its importance and ubiquity, there is much sparser liter-ature on OCCF, compared with the extensive machinery for multi-class collaborative filtering (MCCF). This is mainly due to the fol-lowing two challenges. The first challenge rises from the ambigu-ity of the unobserved data: the unobserved data is not necessarily negative examples, but a mixing of negative examples and missing positive examples. Different from MCCF, which often focuses on the observed data only, special treatments are needed for the miss-ing/unobserved examples in OCCF. Existing solutions for OCCF differ in terms of how they treat the unobserved data, including weighting-based, imputation-based, and sampling-based methods (see Section 6 for a review). While each of them has its own ratio-nality and advantage, it is unclear how to integrate them together to maximally improve the recommendation performance.

Another challenge of OCCF lies in data sparseness (i.e., only an extremely small percentage of data is labeled as positive examples). This challenge compromises the full power of collaborative filter-make satisfactory recommendations for the cold-start users and/or cold-start items. To tackle this challenge in the MCCF setting, sev-eral researchers propose to incorporate the side information such as the demographical information about users, item content, and the social relationships between users (see Section 6 for a review). However, it is unclear how to migrate these techniques to the one-class setting, especially when there is side information from both users and items.

In this paper, we propose a dual-regularized model for one-class collaborative filtering. In particular, we address the ambiguity chal-lenge by integrating two state-of-the-art one-class collaborative fil-tering methods (weighting-based and imputation-based methods) to enjoy the best of both worlds. The intuition behind this inte-grated method is two-fold. First, notice that users might prefer a variety of items including those unseen ones. Thus, we impute the unobserved data to indicat e the proba bility that a user would prefer an unseen item. Second, we assign a weight for the imputed data to indicate its reliability. Furthermore, we tackle the sparseness challenge by encoding the side information from both users and items as two graph regularization terms. Such a treatment (dual-regularization) could help alleviate the cold-start problem and im-prove the overall performance of recommender systems as well.
In summary, the main contributions of this paper include:
The rest of the paper is organized as follows. Section 2 defines the problem. Section 3 and Section 4 present the proposed for-mulations and algorithms, respectively. Section 5 presents the ex-perimental results. Section 6 reviews related work, and Section 7 concludes the paper.
Table 1 lists the main symbols we use throughout the paper. Fol-lowing conventions, we use bold capital letters for matrices. For example, we use an m  X  n matrix R to denote the user-item feed-back, where R ( u , i ) = 1ifuser u had provided positive feedback on item i ,and R ( u , i ) = 0 if the feedback is unknown/unobserved. Without loss of generality, we use u and v to index users, and we use i and j to index items. We use calligraphic font O and Symbol Definition and Description R the implicit, one-class feedback matrix W the weighting matrix for R P the imputation matrix for R F , G the low-rank approximations for R M , N the user-user graph and the item-item graph R the transpose of matrix R R ( u , :) the u th row of R R ( u , i ) the element at the u th row and i th column of R I O , I U the indicator matrices for observed and missing data I A the full indicator matrix I A = I O + I U O the set of observed data between users and items
U the set of unobserved data between users and items m , n the number of users and items u , v the users i , j the items w the global weight for W p the global imputation value for P r the low rank for F and G l the maximum iteration number  X  the threshold to terminate the iteration indicate the set of observed and unobserved user-item data, respec-tively. Further, we denote I O and I U as the indicator matrices for O and U , respectively. Similar to Matlab, we denote the i th row of matrix R as R ( i , :), and the transpose of a matrix with a prime (i.e., R  X  R T ).
 With these notations, we first define the basic OCCF problem. P ROBLEM 1. Basic OCCF Problem [24] Given: (1) an m  X  n implicit, one-class feedback matrix R ,(2)a Find: the estimated preference of user u on item i.

In Problem 1, to estimate the preference of a given user on a given item, the only input needed is the feedback matrix R .In addition to this feedback matrix, there might exist side informa-tion (e.g., the social relationships between users, the item similar-ity, etc.) that can be exploited in many applications. In this work, we focus on the pair-wise link information from both users and items. For example, a link between two users may indicate their similarity, intimacy, or friendship, while a link between two items may indicate their similarity or category. We use two symmetric matrices/graphs M and N to denote all the links between users and items, respectively. With these additional notations, we can define the following OCCF problem.
 P ROBLEM 2. OCCF Problem with Side Information Given: (1) an m  X  n implicit, one-class feedback matrix R , (2) an Find: the estimated preference of user u on item i.
Before presenting our proposed solutions in the next two sec-tions, let us first briefly review two representative OCCF methods.
The first one is the weighting-based method, which formulates the following minimization problem [24] min where W ( u , i ) indicates the weight of the corresponding example, F and G are the non-negative low-rank approximations of R ,and is used to control the regularization of || F || 2 F and || smaller weight is assigned for those unobserved examples. That is, W ( u , i ) = 1if R ( u , i ) is observed, and W ( u , i ) oriented, and item oriented. For simplicity, we adopt the uniform strategy, i.e., we set W ( u , i ) = where wZAN in this paper.

Another approach is imputation based. The intuition behind this method is that the unobserved data in OCCF setting may contain many missing positive examples; therefore, we can impute a value for the unobserved example to indicate the possible feedback that the user would give to the item. Formally, the imputation-based method can be formulated as min where P stands for the imputation matrix for the unobserved data. Typically, P ( u , i ) = 0if R ( u , i ) = 1, and P ( u , 0. To fill in P , we can resort to similar strategies as those for wZAN . For example, we can fill in P ( u , i ) uniformly with a global value p  X  [0 , 1] for the unobserved data. We can also adopt the user-oriented strategy or the item-oriented strategy. In this paper, we focus on the uniform strategy, and will refer to this approach as iZAN in the following. Note that the similar idea was also implicitly explored in [31].
In this section, we present our formulations for the basic OCCF problem (i.e., Problem 1) and the OCCF with dual side information problem (i.e., Problem 2).
For Problem 1, the main challenge is from the ambiguity of the unobserved data, i.e., the unobserved data is not necessarily nega-tive examples but a mixing of negative examples and missing posi-tive examples. To deal with the ambiguity problem, the weighting-based method (e.g., wZAN in Eq. (1)) adds a weight for unob-served examples, while the imputation-based method (e.g., iZAN in Eq. (2)) imputes the unobserved data. The key observation here is that these two methods are complementary to each other: we are more certain about the observed data, and we have a relatively lower confidence of the imputed data. This leads to a natural choice of integrating these two methods by putting a smaller weight on the contribution of the imputed data, i.e., tively. In this formulation, we introduce the imputed value P ( u of an unobserved example to indicate the likelihood that the user would favor the corresponding item. In the meanwhile, we intro-refer to this approach as wiZAN .

Scalability issues. As we can see from Eq. (3), wiZAN optimizes over a dense matrix ( R + P )ofsize m  X  n , and introduces another m n matrix W . Directly optimizing Eq. (3) might be computationally prohibitive in terms of both time and space in many large-scale applications. We will propose scalable algorithms to tackle this issue in the next section.
For Problem 2, we include both the user-user graph and the item-item graph as additional inputs. Effectively leveraging such side information might not only help alleviate the cold-start problem, but also improve the overall performance for recommender sys-tems. Here, we describe how we incorporate the side information. We take the user-user side information as an example, but similar treatment can be applied on the item-item side information.
For user side, the basic idea is to employ the Homophily ef-fect [22], i.e., similar users tend to share similar preferences for items. In other words, if two users are connected (i.e., a non-zero and/or larger M ( u , v )), they might have similar latent preferences fore, for two users u and v , we add the following constraint on their preferences where m is the total number of users, and M is the m  X  m user-user graph for these users. As we can see from Eq. (4), the link M ( u would encourage the corresponding latent preferences (i.e., F ( u and F ( v , :)) to be close to each other. Notice that it has no penalty for two dis-connected users (i.e., M ( u , v ) = 0).

We can further formulate the constraint in Eq. (4) as a graph regularization term with the following equations where tr(  X  ) stands for the matrix trace, r is the rank of F ,and D the degree matrix for M with D M ( u , u ) = m v = 1 M ( u Similarly, we have the item-side regularization term tr( G ( D N ) G ), where N is the symmetric item-item graph which represents item-item similarity, and D N is the degree matrix for N .
Finally, we incorporate the two regularization terms into Eq. (3), resulting in our dual-regularized OCCF model for Problem 2 min where  X  F and  X  G are used to control the importance of the two regu-larization terms. Notice that the two regularization terms, ( D and ( D N  X  N ) are the graph Laplacian of the user-user graph and item-item graph, respectively. Actually, such a dual regularization can be plugged into many existing methods (e.g., wZAN and iZAN ), as we will show in the experimental section.

With the F and G matrices derived from the above formulations (e.g., Eq. (6) and Eq. (3)), we can estimate the preference of user u on item i as  X  R ( u , i ) = F ( u , :) G ( i , :) .
In this section, we present the algorithm ( wiZAN-Dual )tosolve the OCCF problem with side information in Eq. (6), followed by some effectiveness and efficiency analysis. The algorithm ( wiZAN ) for Eq. (3) can be derived from wiZAN-Dual by ignoring the dual regularization terms.
Unfortunately, the optimization problem in Eq. (6) is not jointly convex due to the coupling between F and G . Therefore, instead of seeking for a global optimal solution, we aim to find a local minimum by alternatively updating F and G while fixing the other. Next, we show how to update F when G is fixed. The update of G can be done in a similar way.

When G is fixed, the optimization problem in Eq. (6) becomes the minimization problem of the following equation (by dropping some constant terms) wrt the matrix F
J = || W (( R + P )  X  FG ) || 2 F +  X  r || F || 2 F +  X  F where is the Hadamard product with [ A B ]( u , i ) = A ( u for any two matrices with the same size.
Since both the user-user graph M and its degree matrix D M symmetric, the derivative of J wrt F can be computed as
In Eq. (8), M is a sparse non-negative matrix, and D M is a di-agonal non-negative matrix. A fixed-point solution of Eq. (8) with the non-negativity constraint l eads to the following multiplicative updating rule for F F ( u , k )  X  F ( u , k )
The ( R + P ) matrix in Eq. (9) is extremely large in many recom-mender systems, causing severe scalability issues in terms of both time and storage. To tackle this issue, we propose an efficient al-gorithm to scale up the updating process in terms of both time and storage. Before presenting our algorithm, we need to further define matrix  X  R 1 . We denote  X  R 1 as the sparse matrix whose elements are predicted by F and G on the observed examples in R .Thatis,
Basedonthe  X  R 1 matrix, we present the new updating rule for F as follows where A 1 and B 1 are defined as Here, ples, p is the global imputation value, and 1 1  X  n is a 1 with all 1s.
 Following similar steps, we can have the updating rule for G with A 2 and B 2 in the following form where N is a sparse, non-negative matrix containing the item links, and D N is the diagonal degree matrix for N .

Finally, we summarize the overall algorithm for solving Eq. (6) in Alg. 1. As we can see from the algorithm, after we initialize the F and G matrices (Step 1), the algorithm begins the iteration procedure. In each iteration, the algorithm first computes the matrix (Step 3), and then alternatively updates F and G (Steps 4-9 and Steps 10-15, respectively). We use the following criteria to ter-minate the iteration procedure: either the Frobenius norm between successive estimates of both F and G is below our threshold the maximum iteration step l is reached. Finally, we can predict the preference of user u on item i by F ( u , :) G ( i , :) .
Here, we briefly analyze the optimality, convergency, and com-putational complexity of our algorithm.
 Algorithm 1 The wiZAN-Dual Algorithm.
 Input: R , M , N ,rank r , global weight w , and global imputation Output: F and G 1: initialize F and G randomly; 2: while not convergent do 3: compute  X  R 1 as defined in Eq. (10); 4: compute A 1 and B 1 as defined in Eq. (12); 5: for u =1: m do 6: for k =1: r do 7: update F ( u , k ) as defined in Eq. (11); 8: end for 9: end for 10: compute A 2 and B 2 as defined in Eq. (14); 11: for i =1: n do 12: for k =1: r do 13: update G ( i , k ) as defined in Eq. (13); 14: end for 15: end for 16: end while 17: return F and G ;
We first show the correctness of Eq. (11) for updating F ,byprov-ing that the fixed-point solution of Eq. (11) satisfies the KKT con-dition. The correctness of Eq. (13) for updating G can be proved analogously.

T HEOREM 1. Correctness of Eq. (11) . The fixed-point solu-tion of Eq. (11) satisfies the KKT condition.
 P ROOF . In order to prove the theorem, we will first show that Eq. (9) satisfies the KKT condition, and then show the equivalence between Eq. (9) and Eq. (11). We start with Lagrangian function of Eq. (7) L where  X  is the Lagrange multiplier. Let the derivative of the above equation L J equal to 0, we have From the KKT complementary slackness condition, we have Clearly, a fixed point of the updating rule in Eq. (9) satisfies the above equation.

Next, we show the equivalence between Eq. (9) and Eq. (11). We first introduce several notations. We denote  X  R 2 as the matrix whose values are predicted by F and G on the unobserved examples in R , observed and missing data, respectively. The full indicator matrix is I
A = I O + I U . Then, we have the following two equations: and Finally, we can get the updating rul e in Eq. (11) by substituting the above two equations into Eq. (9), which completes the proof.
In Theorem 1, we have shown that the updating rule in Eq. (11) yields a correct solution for minimizing Eq. (7) at convergence. Next, we prove that the updating rule in Eq. (11) is guaranteed to converge.

T HEOREM 2. Convergence of Eq. (11) . Under the updating rule of Eq. (11) ,Eq. (7) decreases monotonically.
 P ROOF . See the appendix.

Combining Theorem 1 and Theorem 2 together, we can have the following corollary, which states that Alg. 1 finds a local optimum for Eq. (6). Given that the original optimization problem in Eq. (6) is not jointly convex wrt F and G , such a local minimum is accept-able in practice.

C OROLLARY 1. Effectiveness of Alg. 1 . Alg. 1 finds a local minimum for the optimization problem in Eq. (6) .
 P ROOF . Omitted for brevity.

The time complexity and space complexity of the proposed al-gorithm are summarized in the following lemmas, which basically state that Alg. 1 scales linearly wrt the total number observed ex-(i.e., m + n ) in both time and space. Notice that all the three matrices ( R , M ,and N ) are often very sparse (e.g., | R | &lt;&lt; etc). In contrast, if we directly use the updating rule in Eq. (9), it would cost us quadratic complexity (e.g., O ( mn )) in both time and space.
 L EMMA 1. Time complexity of Alg. 1 . The time complexity of Alg. 1 is O (( | R | + | M | + | N | ) rl + ( m + n ) r 2 l ) . P ROOF . Omitted for brevity.

L EMMA 2. Space complexity of Alg. 1 . The space complexity of Alg. 1 is O ( | R | + | M | + | N | + ( m + n ) r ) .
P ROOF . Omitted for brevity.
In this section, we present the experimental evaluations. The experiments are designed to answer the following questions.
We use two real data sets: Ciao and Epinions [33, 35]. For both data sets, we randomly select 50% ratings as the training set and use the rest as the test set. For one-class experiments, we remove all the ratings that are no greater than 3, and relabel ratings 4 and 5 as 1 (positive examples). For the user-side regularization, we use the trust relationships between users. In particular, we assign a trust link M uv = 1 if either u trusts v or v trusts u . The resulting M matrix is sparse (see Table 2). For the item-side regularization, we aggregate the reviews as a document for each item, and then compute the cosine similarity between the TF-IDF vectors of these documents. To keep the item-item graph sparse, we assign N if the cosine similarity between item i and item j is larger than 0.4. The statistics of the two data sets are summarized in Table 2.
First, we compare with three existing methods for explicit, multi-class collaborative filtering. Notice that both GWNMF and SR also formulate the user and item side information as graph regulariza-tion terms. Despite their own success in the multi-class case, their algorithms would lead to trivial solutions in the one-class case (with the two latent matrices F and G being all 1 / soon, they inevitably result in poor recommendation performance in the one-class case.
We also compare the following algorithms for one-class setting:
For the results reported in this section, we use the same initial-izations of F and G , and we fix the global weight global imputation value p = 0 . 01, and rank r = 10 unless otherwise stated. For other parameters, we set maximum iteration l = termination threshold  X  = 10  X  6 , and the regularization parameters  X  = 0 . 1 , X 
To evaluate the effectiveness of the compared methods, we adopt three widely used evaluation metrics for OCCF.

The first metric is Half-Life Utility (HLU) [3, 24]. HLU estimates how likely a user will view/choose an item from a ranked list, with the assumption that the user will view each consecutive item in the list with an exponential decay of possibility. A larger HLU indicates better recommendation performance.

The second metric is Mean Average Precision (MAP) [18]. MAP measures the overall performance based on precision at different recall levels. It calculates the mean of the average precision (AP) over all users in the test set. A larger MAP indicates better recom-mendation performance.
 The third metric is a recall-oriented metric Mean Percentage Ranking (MPR) [12]. MPR measures the user satisfaction of items in a ranked list. It is expected that a randomly produced list would have a MPR of 50%. A smaller MPR indicates better recommenda-tion performance.

For efficiency experiments, we simply report the wall-clock time of the proposed algorithms. All the experiments were run on a machine with eight 3.4GHz Intel Cores and 24GB memory. (A) Effectiveness Comparisons. We first compare the overall ef-fectiveness performance of the proposed methods with that of the existing methods. The results on Ciao data and Epinions data are shown in Table 3 and Table 4, respectively. Larger HLU / MAP and smaller MPR are better. ldNMF is computationally prohibitive on Epinions data due to its quadratic complexity.

There are several observations from the tables. First of all, the proposed wiZAN-Dual outperforms all the compared methods in all evaluation metrics on both data sets. For example, on the Ciao data, wiZAN-Dual outperforms the best existing competitors by 15.0%, 21.1%, and 10.1% wrt HLU , MAP ,and MPR , respectively; on the Epinions data, wiZAN-Dual outperforms the best existing competitors by 9.9%, 17.3%, and 7.9% wrt HLU , MAP ,and MPR , respectively. Second, as expected, the first three methods ( ZAM , Table 3: Effectiveness results on Ciao data. Larger HLU / MAP and smaller MPR are better. wiZAN-Dual significantly outper-forms all the compared methods.
 Table 4: Effectiveness results on Epinions data. Larger HLU / MAP and smaller MPR are better. wiZAN-Dual signif-icantly outperforms all the compared methods. The ldNMF method is computationally prohibitive on Epinions data. GWNMF ,and SR ) that are designed for the MCCF setting perform poorly in the OCCF setting. Third, compared to the other meth-ods that are proposed for the OCCF setting, our wiZAN can already achieve better or close performance. For example, on both data sets, wiZAN is better than all the existing competitors in all three evaluation metrics except the MPR metric of the MSCMF method. Recall that the MSCMF method also uses the side information. This indicates that wiZAN outperforms all the compared methods for the basic OCCF problem defined in Problem 1. Forth, the overall per-formance of all the methods on Ciao data is better than that on the Epinions data. This is probably due to the fact that the Epinions data is much sparser than the Ciao data (e.g., 0.034% sparsity of Epinions data vs. 0.160% sparsity of Ciao data). (B) Effectiveness of Dual Regularization. The proposed dual regularization is applicable to many existing methods. We have already shown the usefulness of dual regularization by incorporat-ing them into our own wiZAN as shown in Table 3 and Table 4. Next, we further verify the effectiveness of the dual regularization terms by adding them into wZAN and iZAN . The results on the two data sets are shown in Table 5 and Table 6, respectively. As we can see, both wZAN-Dual and iZAN-Dual perform better than the corresponding cases when the regularization terms are not added. For example, wZAN-Dual improves wZAN by 13.4%, 18.9%, and 14.1% wrt HLU , MAP ,and MPR on the Ciao data, respectively. This result indicates that dual regularization indeed helps in OCCF recommendation. methods for both cold-start users and cold-start items on both data sets. Table 5: Effectiveness of side information on Ciao data. Larger HLU / MAP and smaller MPR are better. Our dual regulariza-tion improves the prediction accuracy in all cases.
 (C) Effectiveness in Cold-start Scenarios. Next, we put our focus on the cold-start scenarios. As mentioned in introduction, one of the advantages of our method is to alleviate the cold-start problem in recommender systems. Here, we compare the effectiveness of wiZAN-Dual in the cold-start scenarios with several best competi-tors including wiZAN , MSCMF , RG ,and wZAN . We also compare with an additional Pairwise method [26]. The Pairwise method is specially designed for cold-start users/items, and it requires the user features and item features as input. To apply the Pairwise method to our problem setting, we perform a spectral decomposition method to translate M and N to the feature representation for users/items. The results are shown in Fig 1, where x-axis indicates the number of positive feedback given by the cold-start users or received by the cold-start items in the training set, and y-axis indicates the MPR metric.
 Table 6: Effectiveness of side information on Epinions data. Larger HLU / MAP and smaller MPR are better. Our dual regu-larization improves the prediction accuracy in all cases.
As we can see from the figures, wiZAN-Dual outperforms all the compared methods for both cold-start users and cold-start items on both data sets. Specially, wiZAN-Dual is better than wiZAN ,which directly indicates the importance of dual regularization in the cold-start scenarios. The MSCMF method performs the second best in the compared methods. The reason is that although in a different way, MSCMF also considers the side information from both users and items.
Finally, we evaluate the efficiency of wiZAN-Dual by reporting the wall-clock time of the training stage (i.e., Alg. 1). We use the subsets of the data sets to test the scalability of th e proposed algo-rithm. The results are shown in Fig. 2. As we can see from the figures, our algorithm scales linearly wrt the total number of obser-(a) Wall-clock time vs. | R | + |
M | + | N | on Ciao data (c) Wall-clock time vs. | R | + |
M | + | N | on Epinions data Figure 2: Scalability of the proposed algorithm. It scales lin-early wrt the data size. (i.e., m + n ), which is consistent with our analysis in Lemma 1. Additionally, the algorithm is very efficient, finishing the training stage within 30 seconds for both data sets.
In this section, we briefly review the related work including the existing methods for multi-class c ollaborative filtering and one-class collaborative filtering, and the existing solutions for the cold-start problem.

Multi-Class Collaborative Filtering . Most of the existing col-laborative filtering methods are p roposed for explicit, multi-class recommender systems. Typically, collaborative filtering methods are categorized into memory-based methods and model-based meth-ods [13, 14]. A combination of memory-based method and model-based method has also been explored [15, 19].

Considering the wide existence of social links between users, many researchers begin to incorporate these social links into col-laborative filtering [20, 21, 36, 30, 34]. The key idea of these meth-ods is that the linked users tend to have similar latent preferences for items. By applying similar idea to the item side, Gu et al. [9] employ the link information from both user side and item side.
One-Class Collaborative Filtering . Although one-class collab-orative filtering is less visited compared to the multi-class setting, it is widely applicable in many real situations. According to how they treat the unobserved data, existing solutions can be categorized into three classes: weighting-based methods and sampling-based meth-ods [24, 23], as well as imputation-based methods. For example, Hu et al. [12] propose a weighting-based method for recommend-ing TV shows, and they obtain the weight from the number of min-utes that a given show was watched. Paquet and Koenigstein [25] propose a sampling-based method where the degree distributions of users/items are preserved. Si ndhwani et al. [31] propose to treat the unobserved data as optimization variables, which is essentially the imputation-based method.

Side information is also exploited by several researchers. For ex-ample, Li et al. [18] propose to leverage the users X  past queries to construct the user-item similarity, and use such similarity to im-prove the recommendation performance; Zheng et al. [38] pro-pose to employ multiple similarity matrices between users/items for drug-target interaction prediction. Other related proposals for one-class setting include the rank-based optimization objective [28], the matrix co-factorization method [8], the combination of senti-ment analysis and neighborhood method [27], etc.

Cold-Start Problem . Cold-start problem is one of the key chal-lenges in recommender systems. In cold-start scenarios, it is rela-tively difficult to provide accurate recommendations for cold-start users [39], cold-start items [1], or both [26]. Existing solutions for cold-start problem can be categorized into three classes: interview based , adjustment based ,and side-information based .

In the interview-based methods, an additional set of items is usu-ally provided in the sign-up phase to collect the preferences of the cold-start users [11, 39, 32]. For example, Zhou et al. [39] use deci-sion tree to select the set of interview items; Harpale and Yang [11] identify the interview set by active learning. One problem of the interview-based methods is that they bring additional burdens to the cold-start users. The second class of adjustment-based meth-ods mainly focus on how to make full use of the small amount of ratings from cold-start users or for cold-start items. For exam-ple, Hacker and Ahn [10] introduce an online game during which the preferences of cold-start users can be adjusted. Methods in the third class exploit the side information to alleviate the cold-start problem. In this class, existing methods can be further categorized into attribute-based methods and link-based methods according to the type of side information they used. For attribute-based meth-ods, Schein et al. [29] use item attributes such as item content; Park et al. [26] and Zhang et al. [37] leverage both user attributes and item attributes such as the demographical information. In contrast to attribute-based methods, link-based methods mainly employ the the social relationships between users [20, 30]. Our method falls into this sub-category, and we encode the link-based side informa-tion from both users and items.
In this paper, we have proposed a unified model wiZAN-Dual for one-class collaborative filtering. In wiZAN-Dual , we address (1) the ambiguity challenge by imputing each unobserved user-item pair to indicate the probability that the user would prefer the item, together with a weight to control the contribution/reliability of the imputed data; and (2) the sparseness challenge by exploiting the relationships between users/items (i.e., the user/item side informa-tion). We propose efficient algorithms for wiZAN-Dual , and ana-lyze our algorithms in terms of optimality, correctness, and com-plexity. Our experimental evaluations on real benchmark data sets show that the proposed method leads to significant improvement over the state-of-the-art methods in prediction accuracy, while en-joying the linear scalability in both time and space.
This work is supported by the National 863 Program of China (No. 2012AA011205), and the National Natural Science Founda-tion of China (No. 91318301, 61321491, 61100037). This mate-rial is partially supported by the National Science Foundation un-der Grant No. IIS1017415, IIS-1162374, and IIS-1218036, by the Army Research Laboratory under Cooperative Agreement Num-ber W911NF-09-2-0053, by Defense Advanced Research Projects Agency (DARPA) under Contract Number W911NF-11-C-0200 and W911NF-12-C-0028, and by Region II University Transportation Center under the project number 49997-33 25.

The content of the information in this document does not nec-essarily reflect the position or the policy of the Government, and no official endorsement should be inferred. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on. [1] S. S. Anand and N. Griffiths. A market-based approach to [2] P. Bogdanov, M. Busch, J. Moehlis, A. K. Singh, and B. K. [3] J. S. Breese, D. Heckerman, and C. Kadie. Empirical [4] O. Chapelle, V. Sindhwani, and S. S. Keerthi. Optimization [5] H.-C. Chen and A. L. Chen. A music recommendation [6] A. S. Das, M. Datar, A. Garg, and S. Rajaram. Google news [7] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal [8] Y. Fang and L. Si. Matrix co-factorization for [9] Q. Gu, J. Zhou, and C. H. Ding. Collaborative filtering: [10] S. Hacker and L. Von Ahn. Matchin: eliciting user [11] A. S. Harpale and Y. Yang. Personalized active learning for [12] Y. Hu, Y. Koren, and C. Volinsky. Collaborative filtering for [13] M. Jamali and M. Ester. A matrix factorization technique [14] M. Jiang, P. Cui, F. Wang, Q. Yang, W. Zhu, and S. Yang. [15] Y. Koren. Factorization meets the neighborhood: a [16] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization [17] D. D. Lee and H. S. Seung. Algorithms for non-negative [18] Y. Li, J. Hu, C. Zhai, and Y. Chen. Improving one-class [19] H. Ma. An experimental study on implicit social [20] H. Ma, H. Yang, M. R. Lyu, and I. King. Sorec: social [21] H. Ma, D. Zhou, C. Liu, M. R. Lyu, and I. King.
 [22] M. McPherson, L. Smith-Lovin, and J. M. Cook. Birds of a [23] R. Pan and M. Scholz. Mind the gaps: weighting the [24] R. Pan, Y. Zhou, B. Cao, N. N. Liu, R. Lukose, M. Scholz, [25] U. Paquet and N. Koenigstein. One-class collaborative [26] S.-T. Park and W. Chu. Pairwise preference regression for [27] A. Popescu-Belis and N. Pappas. Sentiment analysis of user [28] S. Rendle, C. Freudenthaler, Z. Gantner, and [29] A. I. Schein, A. Popescul, L. H. Ungar, and D. M. Pennock. [30] Y. Shen and R. Jin. Learning personal+ social latent factor [31] V. Sindhwani, S. S. Bucak, J. Hu, and A. Mojsilovic. [32] M. Sun, F. Li, J. Lee, K. Zhou, G. Lebanon, and H. Zha. [33] J. Tang, H. Gao, and H. Liu. mTrust: discerning [34] J. Tang, X. Hu, H. Gao, and H. Liu. Exploiting local and [35] J. Tang, H. Liu, H. Gao, and A. Das Sarmas. etrust: [36] S.-H. Yang, B. Long, A. Smola, N. Sadagopan, Z. Zheng, [37] M. Zhang, J. Tang, X. Zhang, and X. Xue. Addressing cold [38] X. Zheng, H. Ding, H. Mamitsuka, and S. Zhu. Collaborative [39] K. Zhou, S.-H. Yang, and H. Zha. Functional matrix By ignoring constant terms, we can re-write Eq. (7) as J ( F ) =  X  2tr[( W W ( R + P )) GF ] + tr[( W W ( FG )) GF ]
Following the auxiliary function approach [17], an auxiliary func-tion H ( F ,  X  F )of J ( F ) must satisfy We define Then, by construction, we have
In the remainder of proof, we need to find 1) an appropriate aux-iliary function, and 2) the global minimum solution of the auxiliary function.

We start with the auxiliary function, and show that the following equation is one of the auxiliary functions for Eq. (15) For convenience, we name the five terms in Eq. (19) as E 1, E 2, E 3, E 4and E 5, respectively. Then, for E 3wehave Using the inequality z 1 + log z ,wehave and
E 2  X  For E 5, we use the following inequality [7] and B are symmetric. Therefore, we have Finally, for E 4, let F ( u , k ) =  X  F ( u , k ) Q ( u , E 4 = By substituting Eq. (20)-(24) into Eq. (19), we have H ( F J ( F ).

Next, we need to find the global minimum solution of H ( F The gradient is 1 2  X  H ( F ,  X  F )  X  F ( u , k ) We can further show that the Hessian matrix of H ( F ,  X  onal matrix with positive diagonal elements. Therefore, the global minimum can be obtained by setting Eq. (25) as zero, which results in F ( u , k ) =  X  F 2 ( u , k ) rule in Eq. (9) decreases monotonically. Further, with equivalence between Eq. (9) and Eq. (11) as shown in the proof of Theorem 1, we have that Eq. (7) decreases monotonically under the updating rule of Eq. (11), which completes the proof.
