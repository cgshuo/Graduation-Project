 Data streams have emerged as an important paradigm for processing data that arrives and needs to be processed continuously. For instance, military applications that monitor streams of stock data reported from various stock exchanges, temperature Stream Management System (DSMS) [1, 2, 3, 4] is running. Continuous queries from the applications are registered to DSMS and are evaluated over the incoming data continuously. 
A salient feature of continuous queries injected to DSMS is the sophisticated real-time constraint, for the streams are usually short-lived and to be immediately consumed. Therefore, unlike a traditional DBMS where people are mostly interested in the correctness of query results, queries in a DSMS generally require to be higher uncertainty, and sometimes the delay exceeding certain threshold value may manufacturing status catastrophic results. Therefore, we formally put forward the term, deadline , as real-time constraint of data stream processing. We associate deadline with continuous queries in DSMS as the maximum response delay tolerated we address the problem of deadline-sensitive data stream processing in this paper. 
Since streams are potentially unbounded in size, computation over the entire streams seems not practical. And for many stream-based applications, recent elements of a stream are more important than those arrived long ago. This preference for recent elements is commonly expressed using a sliding window [5, 6], which identifies a portion of the stream that arrived between "now" and some recent time in the past. So this paper focuses on a deadline-sensitive approach for processing of sliding window. 
When processing high-volume, bursty data streams, even though the average arrival rate is within computational limits, there may be bursts of high load leading to accumulation of input data to be processed, due to finite CPU and memory capacity. increase, and the overall system throughput will decrease accordingl y. Therefore, it is particularly important for DSMS to be able to automatically adapt to unanticipated bursts in input data rates. In addition, an overloaded system will be unable to process discarding some fraction of the unprocessed data, becomes necessary in order for the system to continue to provide up-to-date response. In our deadline-sensitive approach for real-time processing of sliding windows, when the data rate fluctuates, chances are that the system may predict the arrived wind ows could satisfy their deadline or not. If system. In summary, we make the following contributions in this paper:  X 
We summarize the real-time constraint of stream processing, and formally put forward the term deadline for real-time processing of data streams. Particularly, we focus on deadline-sensitive sliding window modeling and processing.  X 
We introduce a H-D-P model to analyze adaptativity of sliding window based proposed for real-time processing over data stream sliding window.  X 
A deadline-sensitive processing architecture with feedback control is proposed, in which DSAT prediction is applied. Once deadline missing is predicted, a proposed load shedding strategy, HopDrop , is applied to relieve the system in advance. 
The rest of this paper is organized as follows. We begin in Section 2 by formalizing the problem and establishing notations and models. Section 3 presents our Experimental results are presented in Section 4. Section 5 discusses the related work. Finally, we end the paper with conclusions and future work in Section 6. In this section, we introduce some key concepts and notation. Without loss of generality, we assume that the timestamp referred in this paper is defined in a nonnegative integer domain. 2.1 Definition of Deadline In an ideal case, at any point of time t , the answers resulting from the input at time t would be available instantaneously. We refer to t in this case as the output create-time , denoted by t create . However, real systems produce the output with certain delay. We refer to the output time in this case as the output release-time , denoted by t release . And the real-time constraint of a query is to minimize the response delay (i.e., t delay tolerated by the application, denoted by D Q . For any output of Q :  X  t  X  t
For stream-based applications, some are wi th soft deadline semantics, in which deadline semantics are common in many real-time applications. 
The deadline satisfy ratio ( DSR ) is one of the most important performance metrics in real-time applications, and we give definition to deadline satisfy ratio in real-time number of tuples that have missed and met their deadlines, respectively. 2.2 Deadline-Sensitive Sliding Window Model A sliding window over a stream always contains a bag of the most recent tuples seen time-based window and count-based window [8]. Here we focus on the time-based window, and it can be easily applied to the tuple-based window. assuming 0&lt; H  X  Z . At each point of time t , the new arrived tuples are inserted into the window, and the expired tuples are deleted from the window. Thus, a new window respectively. For each window instance W t :  X  t  X  t represent the number of window instances that meet and miss deadlines, respectively. 
In our sliding window processing model, we employ the  X  X ncremental evaluation X  added. Therefore, as shown in Equation (2), the output of W t , denoted by Output ( W t ), can be achieved by adding the effect of inserted data and that of deleted data over the Noted that both insertion and deletion over a window may incur addition and/or overall output or delete tuples from overall output. 
For the cost of each window instance W t , besides the window maintenance cost P m , the computation cost P ( W t ) consists of the cost of adding the effect of increased data and that of deleting the effect of decreased data in the window, denoted by P window instance is shown as following in Equation (3). 
For queries over deadline-sensitive sliding windows, the absolute goal is to minimize the processing cost of each wind ow instance to meet its deadline. (Of course, waiting time is another cause of response delay, which does not belong to the computation cost model discussed in this paper.) 2.3 D-H-P Model processing cost is called a H-D-P model. P-H Relation. To keep up with window updating, processing system should make effort to finish processing each window in H time units, or windows will pile up and the system will become too overloaded to hold in memory or to satisfy deadline. Accordingly, an overload-factor is introduced to describe overload effect. Definition 2: overload-factor of W t , O ( W t ), is defined in Equation (4). With the assumption that W t is DSAT :  X  O ( W negative overload burden on the processing;  X  O ( W positive overload burden to the processing. In other words, the processing of W t takes up some time of subsequent windows. Definition 3: accumulated overload-factor from W t to W s , AO ts . As given in Equation Definition 4: balance point . Balance point is given definition in recursion as follows. 1) Time point Z is the first balance point; 2) If time point t is a balance point, then time point s is a balance point if and only if 
Known from Definition 2, 3 and 4, at a balance point t , W t-H has finished computation and W t has been built up. So that the accumulated overload burden factor accumulation begins from balance point t . current round of overload-factor accumulation. Here, we introduce a DSAT-factor deprived from P-D relation. Definition 5: DSAT-factor of W t ,  X  ( W t ) is defined in Equation (6).  X  0 &lt;  X  ( W  X   X  ( W
Moreover, in the range of (0,1], the closer  X  ( W t ) is to 1, the more urgent deadline is. On the contrary, the closer  X  ( W t ) is to 0, the slacker deadline is. D-H Relation. D-H relation is studied in the following two aspects: with window updating, or even ahead of window updating. Therefore, time point that each window built up is a balance point.  X  D &gt; H . In this case, slacker deadline constrai nt allows window computation to lag behind window updating. Under this circumstance, H-D-P model presents a kind of adaptivity to the fluctuation of data rate among consecutive window instances. There are three possible scenarios for each window instance W t : 
Given that D &gt; H , effect and positive overload effect can compensate each other during accumulation. Hence, when the bursts of high arrival rate have receded, the piled unprocessed light load may compensate for that under heavy load, which is a kind of adaptivity to the fluctuation of data rate in H-D-P model. In this section, we propose a novel deadline-sensitive approach for processing of sliding windows. We give a deadline-sensiti ve processing architecture with feedback control, and present the Deadline-SAT prediction and HopDrop strategy. 3.1 Deadline-Sensitive Processing Architecture with Feedback Control Here we give an overview of our deadline-sensitive processing architecture. The general outline of the architecture in given in Figure 2. 1) Arrived windows are placed in the ready queue . 2) The query engine continuously processes the admitted input windows and 3) The monitor measures DSR and predict failure ratio ( PFR ), collects statistics of 4) Based on the metadata of arrived input and information from the monitor, the 5) If a window is predicted DMISS , load shedding will be applied by load shedder . 6) To provide robustness and adaptivity to the processing architecture, predictor 7) Moreover, if DSR or sample rate deteriorate to the extreme value, alarm informs 3.2 Deadline-SAT Prediction Strategy subsequent k windows would have arrived during the computation of W t . Chances are load shedding would be applied over it in advance. Look at the computation cost model of a window instance given in Equation (3), P factors: (1) the number of newly arrived tuples and that of expired tuples, which are plan. Based on whether the processing cost is correlated to the window size Z or not, the operations can be simply classifies into two categories: Operations whose processing cost is unrelated to window size, such as SUM, COUNT, AVERAGE, etc. ; and the operations whose processing cost is somehow correlated to window size, such as Window Join, MAX, MIN, DISTINCT, etc . (3) Physical implementation policy of determined, (2) and (3) can be reflected by the average processing cost of new arrived periodically collects the average processing co st of new arrived tuples and the expired tuples, denoted by P + 0 and P  X  0 , and forwards the information to the predictor . Based on the metadata of arrived input and information from the monitor, the predictor estimates the processing cost of window instance W t , i.e., () predict model given in Equation (7), where the cost adjust factor  X  is initially set to 1. () DSAT or not according to Definition (5). Detailed algorithm is shown in Table 1. 
To provide robustness and adaptivity to our DSAT prediction strategy, we apply feedback-control based prediction adaptation . As far as the estimated cost of window instance is concerned, it is evaluated based on the runtime statistics and input metadata. And the runtime statistics is where the predict error may come from mostly. When the cost of window instance is overestimated, load shedding would be applied cost of window instances is underestimated, windows would prefer to miss their PFR would increase remarkably. Accordingly, when DSR deteriorates we make slight adjustment to the cost adjust factor  X  with respect to sample ratio and DMR . 3.3 A Load Shedding Strategy: HopDrop Once a window is predicted DMISS , load shedding should be applied over it to lighten the system load in advance without loss of output performance. 
In our  X  X ncremental evaluation X  philosophy, load shedding can be customized as W  X  W
Compared with Equation (3), the load shedding of W t saves system time as much as () be used to process subsequent windows. And there is no additional work for the posterior window W t+H to do. As it is shown above, the load shedding strategy used in our deadline-sensitive approach is just to skip the new arrived hop, so we term it as HopDrop . 
Noted that after applying HopDrop over W t , if W t+H has not built up, we can buffer the tuples to be dropped and process them as much as possible within their deadline; and if the load of W t+H is relatively small, the buffered tuples could also be processed with best effort within their deadline. Moreover, load-shedding strategy used in DSAT prediction algorithm is not limit to HopDrop merely. experiment results and analysis are presented in Section 4.2. 4.1 Experiment Setup The Internet Traffic Archive [10] is a good source of real-world stream data sets. One of their races, named  X  LBL-PKT-4  X , includes 863,000 records representing one hour X  X  worth of all wide-area traffic between Berkeley Lab and the rest of the world. We use rate fluctuation. Each record contains on e timestamp column and five integer valued columns, srcHost, destHost, srcPort, destPort , and size . 
The sliding windows in the queries were based on the timestamps presented in the fair comparisons, all the experiments are conducted with the same materialized randomization. Once windows are predicted DMISS , HopDrop is applied to make experiment environment. In BEA , if a window finishes computation within deadline, a DSAT window is contributed. Otherwise, if it does not finish computation till deadline, the unfinished tuples would be discarded. 
Furthermore, it is known from Section 3.2 that the window processing cost is somehow related to the operation over the sliding window. Accordingly, we conduct two groups of experiments. In Group (a), operation cost of window instance is unrelated to the window size. We ask for the average network traffic from different IP time period specified by window size. We vary the window size, hop and deadline for each group of experiment to see about DSR and PFR under different circumstances. 4.2 Experiment Analysis Firstly, we study our analytical results regarding the H-D-P model discussed in whose processing cost is unrelated to window size, window size has no effect on the of window size, as illustrated in Figure 4(a). And for the operation whose processing increases accordingly, which means that wind ows are easier to miss deadline. This is accordingly. Therefore,  X  ( W t ) will increase, and windows prefer to miss deadline. As a result, DSR in both Figure 5(a) and Figure 5(b) deteriorate as window hop increases. (3) Deadline D . Obviously, longer deadline provides slacker real-time constraint, and Hereby, the experimental results validate the correctness of our H-D-P model. 
Secondly, PFR in Figure 4-6 are quite low, which shows the precision of our DSAT predict strategy. As addressed in DSAT prediction algorithm (Section 3.2), when estimating processing cost of window instances, errors would be involved. Assuming the error of estimated processing cost of a window as  X  , we have: 
Therefore, the error of ()
Accordingly, it is quite easy to explain the PFR curves of DSA in Figure 4-6. For the operation whose cost is not affected by wi ndow size, the error is also unrelated to the variation of window size, while for the operation whose cost is related to window size, the error is magnified as P ( W t ) increases. When window hop increases, however, From the other point of view, the magnified error is somehow in proportion to DSAT . Therefore, though PFR may increase with the variation of window size, more failures occur only when DSR is lower, which makes feeble influence to most applications. 
Another performance concerned is the boost of DSR won by DSA over BEA . We predict chances are more, more DIMSS window can be discarded in advance, so that time can be saved to compute more DSAT windows. In a word, more predict chances may bring improvement of DSR , and vice versa. As for window size, it has no effect on the chances of prediction, which is coincided by Figure 4. As for Figure 5-6, since our prediction strategy is only practicable in the case of D  X  H , when D draws near to H or even below H , chances of prediction come down, so DSA provides less benefit or even the same performance as BEA . On the other side of the spectrum, when DSR of both algorithms improve, they all draw towards the ideal case, i.e. , DSR =1. and data stream algorithms. Some systems that incorporate stream processing include Aurora [4, 11], Niagara [1], STREAM [2], and Telegraph [3], among many others. 
Stream-based applications require answers of continuous queries should be delivered in a timely fashion. Real-time databases, which have real-time constraint on Transactions that cannot be finished with in deadline would make no value or even negative value to the system. Here we associate deadline with each continuous query stream processing is to make best effort to deliver answers with minimum average response delay [14, 15, 16, 17]. However, deadline constraint of continuous query has not been definitely termed in data stream processing, and little attention has been paid to deadline sensitive adaptive processing or deadline satisfaction prediction. 
Another relevant research area deals with load shedding. There are two main approaches of load shedding. The first relies on random load shedding, i.e ., tuples are removed based on arrival rates, but not their actual values [5]. The second proposes to semantic load shedding. Reference [6] approximates the output of an operator by maximizing a user-defined similarity measure between the exact answer and the approximate answer returned by the system. Reference [18] includes QoS first. Load shedding for sliding window has also been discussed in [5, 6]. In this paper, we focus on real-time constraint of continuous query over data streams, termed as deadline . A H-D-P model is proposed, based on which DSAT prediction feedback control is proposed in which DSAT prediction strategy is included. Once DIMSS is predicted, a proposed load shedding strategy, HopDrop , is applied to relieve the system in advance. Experiment investigations show the performance our deadline-sensitive approach for real time processing of sliding windows. At present, our operator level. Besides, we hope to extend our deadline-sensitive approach to multi-query environment with more complicated scheduling strategy in the future work. 
