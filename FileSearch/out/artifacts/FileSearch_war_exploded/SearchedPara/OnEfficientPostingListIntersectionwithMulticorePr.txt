 H.3.3 [ Information Storage and Retriev al ]: Information Searc h and Retriev al| Retrieval models Algorithms, Design, Performance multicores, parallel query pro cessing, list intersection
The size of indexable Web and the num ber of searc h queries submitted by users have been gro wing consisten tly through-out the past decade. With suc h a gro wth, ecien t and scalable metho ds to implemen t information retriev al (IR) systems become critical for user satisfaction. Thus far the performance of IR with resp ect to query throughput and query latency has been impro ved by designing new list in-tersection algorithms [2] and by dev eloping novel cac hing strategies [1]. In con trast to these techniques, we explore a new researc h direction to impro ve IR eciency: design-ing algorithms that leverage mo dern computer architectures suc h as multicore systems.

Multicores, primarily motiv ated by energy and power con-strain ts, pac k two or more cores on a single die. They typi-cally share on-c hip L 2 cac he as well as the fron t-side bus to main memory . As these systems become more popular, the general trend has been from single-core to man y-core: from dual-, quad-, eigh t-core chips to the ones with tens of cores. So far, however, very little has been done to exploit the full poten tial of these chips in the con text of IR.

Strohman and Croft used 64-bit mac hines and four-core chips to sho w mo dest impro vemen ts in throughput [6]. Their techniques su er from bandwidth issues, and as a result, pro vide only limited scalabilit y. Bonacic et al. used syn-chronous strategies to group the queries into batc hes, and thereafter to pro cess them sequen tially [3]. Ding et al. par-allelized the posting list intersections using graphics pro ces-sors (GPUs) [4]. These techniques, however, fail to give good performance as the num ber of cores increases.

In this article, we presen t and discuss two di eren t parallel query pro cessing mo dels for multicore systems { inter-query parallelism and intr a-query parallelism . While the former ex-plores the parallelism betwe en the queries, the latter exploits the parallelism within a given query . With the intra-query mo del, we are able to impro ve both the throughput and the query resp onse time sim ultaneously { a rst of its kind, to the best of our kno wledge.

Commercial searc h engines are typically driv en by query latency , so impro ving the time to pro cess queries individ-ually is crucial. The query latency includes the time for decompression, posting list intersection, documen t scoring, and result page generation. Herein, we consider only the time for decompression and list intersection { we refer to this time as the query latency or the query response time .
Dev eloping ecien t parallel query pro cessing mo dels is quite challenging. Posting lists are usually kept in com-pressed format (to reduce the storage requiremen ts) mak-ing it dicult to supp ort random accesses on lists. Parallel strategies need to partition the work across cores to balance load more evenly , i.e. , to reduce idle time per core. Further-more, the memory accesses of individual cores should be minimized so that the memory bandwidth is not saturated.
The posting list of eac h term is a sorted list of documen t iden ti ers that is stored as a skip list [5]. A skip is a pointer i ! j between two non-consecutiv e documen ts i and j in the posting list. The num ber of documen ts skipp ed between i and j is de ned as skipsize . For a term t , the posting list L ( t ) is a tuple ( S t ; C t ) where S t = f s 1 a sequence of skips and C t con tains the remaining docu-men ts (between skips) stored using P F orDelta compression scheme [7]. While skips are popularly used to speed up the list intersection pro cess, we leverage skips to pro vide random access over compressed posting lists.
 Figure 1a sho ws the di erences between our two parallel IR mo dels. The inter-query mo del exploits the parallelism among queries by handling eac h query on a di eren t core. Here, the posting lists of a given query are intersected using the standard merge-based technique with appropriate prun-ing strategies based on skips. The documen ts within a skip pointer are decompressed on demand.

The intra-query mo del, on the other hand, exploits the parallelism within a query by dividing the asso ciated work into indep endent tasks (see Figure 1b). Eac h task holds a sequence of documen ts from both posting lists on whic h the intersection is performed. Consider a query q with two terms a and b , whose posting lists are L ( a )=( S a ; C L ( b )=( S b ; C b ) with m = j S a j and n = j S b j . Assume, with-out loss of generalit y, that the query terms are sorted in the increasing order of their posting list size, i.e., m n . For eac h skip pointer in L ( a ), we create a task with one or more skip pointers from L ( b ) suc h that intersection is performed on resulting sequences of posting lists.

More speci cally , we generate a set of indep enden t tasks f t 1 ; t 2 ; :::; t m g where t i = ( s i ; s i +1 ; s j ; s 1 i m and s j ; s k 2 S b for 1 j k n . Note that, s i +1 is unde ned when i = m . For a given s i and s i +1 L ( a ), the skips from L ( b ) are chosen suc h that s i s i +1 s k . In other words, all the documen ts within a skip pointer s i ! s i +1 fall in the documen t interv al given by [ s ; s k ]. To nd common elemen ts from these skips lists, we apply typical list intersection metho ds. It is straigh tforw ard to extend this approac h for queries with more terms.
The tasks are generated by applying a mo di ed merge-based or searc h-based list intersection algorithm on skips from S a and S b . Eac h task fully speci es the portion of posting lists whic h need to be intersected. Thus, once we have tasks created and pushed into a task pool , di eren t cores pro cess them indep enden tly (see Fig. 1b). Common documen ts are then fed to the ranking phase for further pro cessing. Since documen t scores are indep enden t of eac h other, we can easily parallelize the ranking phase { eac h core tak es a documen t from rank pool, ranks it, and pro ceeds to the next one. The eciency can further be impro ved by integrating both intersection and ranking phases.
 Figure 1: (a) parallel query processing models, (b) architecture of the intra-query model
Let Q be the given query workload. We consider three di eren t performance measures. ( i ) Speedup that is de ned with one pro cessor and T P is the time using P pro cessors. ( ii ) Thr oughput that is measured as the ratio between total num ber of queries j Q j and the total time spent in pro cess-ing them. ( iii ) Average Query Latency that is computed as tion pro cess for the i th query is started, and f i is the time at whic h the pro cess is complete.

Our data consists of a crawl of documen ts from the UK do-main, and Alta vista query log with 200 ; 000 queries. These queries are pro cessed in a streaming fashion. The skipsize of skip lists is set to 512. With resp ect to speedup and through-put, the inter-query mo del ( Inter ) achiev es almost linear scalabilit y due to very simple parallelization (see Fig. 2a). On the other hand, the intra-query mo del ( Intr a ) incurs Figure 2: (a) Query throughput (b) Average query latency run time overhead due to task creation and task pool main-tenance, resulting in sub-linear performance.

The average query latency of Inter is almost constant since only the parallelism between di eren t queries is explored. In con trast, it reduces con tinuously for Intr a as num ber of cores is increased. It is imp ortan t to note that the drop in throughput and speedup due to the intra-query mo del when compared to that of the inter-query mo del is less than 20%. However, the impro vemen t in query latency is more than 5-fold. In a nutshell, Inter only impro ves the query throughput and speedup whereas Intr a pro vides excellen t impro vemen t in query latency by sacri cing some perfor-mance with resp ect to throughput and speedup. Unlik e ex-isting approaches [6], we found that the memory accesses in our mo dels, esp ecially in the intra-query mo del due to small tasks, are small and uniform. Thus, it is highly unlik ely that the memory bandwidth reac hes its saturation.

We omit some of the results obtained by varying query length and skipsize due to lack of space. We are curren tly evaluating more sophisticated intersection algorithms to see if they pro vide any bene t over the simple merge-based metho d. In the future, we plan to investigate these two par-allel mo dels with resp ect to power and energy managemen t techniques suc h as DVFS and core-hopping.
 Acknowledgmen ts: This work has been partly supp orted by NSF gran ts NGS-CNS-0406386, CAREER-I IS-0347662, RI-CNS-0403342, and CCF-0702587.
