 The ability to deal with uncertain information is becoming increas-ingly important for modern database applications. Whereas a con-ventional (certain) object is usually represented by a vector from a multidimensional feature space, an uncertain object is represented by a multivariate probability density function ( PDF ). This PDF can be defined either discretely (e.g. by a histogram) or continuously in parametric form (e.g. by a Gaussian Mixture Model). For a database of uncertain objects, the users expect similar data analy-sis techniques as for a conventional database of certain objects. An important analysis technique for certain objects is the skyline op-erator which finds maximal or minimal vectors with respect to any possible attribute weighting. In this paper, we propose the concept of probabilistic skylines, an extension of the skyline operator for uncertain objects. In addition, we propose efficient and effective methods for determini ng the probabilistic sk yline of uncertain ob-jects which are defined by a PDF in parametric form (e.g. a Gaus-sian function or a Gaussian Mixture Model). To further accelerate the search, we elaborate how the computation of the probabilistic skyline can be supported by an index structure for uncertain ob-jects. An extensive experimental evaluation demonstrates both the effectiveness and the efficiency of our technique.
 H.3 [ Information Storage And Retrieval ]: Information Search and Retrieval Algorithms, Performance Gaussian Mixtrue Model, Skyline Query, Uncerain Data
Skyline queries and probabilistic queries on uncertain data are two vital areas of current database research which recently have attracted increasing attention. The classical example of a skyline query is a user looking for cheap hotels which are close to the beach. Typically, it is unknown to the system how important the is a conflict between these conditions (available are expensive ho-tels close to the beach and cheap hotels far away from the beach), it is unclear how the user X  X  trade-off between these conditions would really look like. The system could ask the user to express his pref-erences by a weighting factor. However, such a weighting factor is information on the database content. Moreover, complex decision making on real world data usually involves several dimensions of may be of interest. This means, the skyline does not only contain the cheapest hotel and the hotel closest to the beach but also all hotels providing an outstanding combination of price and distance, which makes them more attractive to the user than any other ho-tel in the database. If one hotel A is both, closer to the beach and cheaper than hotel B ,wesay A dominates B (in symbols A  X  because A is better than B with respect to any possible weighting of the criteria distance and price . More generally, the skyline contains all objects which are not dominated by any other object in the data set. Such conventional skyline queries on exact data have attracted a huge volume of attention in the recent years, e.g. [3, 18, 9, 14, 11] where most papers focus on efficient algorithms for query pro-cessing. More recent approaches study skyline queries for special applications, e.g. time series data [10] and skyline queries from the perspective of a moving query object [20].

Uncertainty is a natural factor in many applications, which is not restricted to moving objects [8, 20]. The classical example of uncertain data modelled by probability density functions (PDFs) are sensor networks. In numerous applications, sensor networks are collecting data to monitor complex systems, for example the mental conditions in a region [12], the medical conditions of a pa-tient [16], or the production of a chemical reactor [21]. A common characteristic of all sensor networks is that the individual sensors collect uncertain data. Every obtained piece of information is as-sociated with some measurement error. Usually there is a trade-off between the accuracy, size and price of a sensor. Very precise sen-sors are typically more expensive and also larger in size. Wearable sensors for example need to be small and the measurement error is partly due to inaccurate placemen t and other individual conditions such as the electrical conductance of the skin. Based on extensive testing, manufacturers of sensors can usually provide close bounds on the measurement error.
Most complex decision processes based on real world data in-volve uncertainty which could be effectively supported by skyline queries on uncertain data : As a concrete application scenario of a sensor network with a large amount of sensors considers a chem-ical reactor producing polyvinyl chloride (PVC). Depending on the aiming application area of the PVC the reactor has to have a cer-tain temperature. The temperature can either be controlled by heat-ing up the reactor, or by adding some substances cooling down the whole production process. Due to the size of the reactor adding some chemical product in order to increase the temperature needs a certain time to take effect. Furthermore the pressure inside the reactor is also another critical and monitored indicator in quality assurance of the resu lting product. Gas can be injected into the reactor or valves can be opened in order to control the pressure.
Producing PVC is only possible if the temperature of the reactor at any point may not drop below a certain value. Furthermore the pressure is not allowed to drop below some critical threshold. Of course the combination of low temperature and decreased pressure is also a critical combination. The reactor is equipped with hun-dreds of sensors, which measure different inaccurate phenomena of the production process. In general all outstanding combinations of the monitored sensors can be critical to the production phase. Those extraordinary values can be recognized as a skyline. There-fore an efficient skyline computation on uncertain data is necessary.
Uncertainty is not only restricted to sensor networks and moving objects. Due to space limitation, we can only mention a few exam-ples. In privacy preserving data mining for example, uncertainty is often deliberately introduced to mask sensitive information. In all these applications, decision making can effectively be supported by our technique for processing probabilistic skyline queries.
The remainder of the paper is organized as follows: Section 2 in-troduces the related work on skylines and probabilistic query pro-cessing. In Section 3, the mathema tical foundations for probabilis-tic skyline computation on uncertain data are derived, first for gen-eral PDFs and then for the important case of Gaussian PDFs and Gaussian Mixture Models. Section 4 gives the details on efficient processing of probabilistic skyline queries in presence of an index structure for uncertain objects in a filter-and-refinement architec-ture. Section 5 is dedicated to the algorithms for computing the skyline objects with and without index structures. Section 6 gives an extensive experimental evaluation and Section 7 concludes our paper.
Skyline Queries on Certain Data. The skyline operator has been introduced to the database community by B X rzs X nyi et al. [3]. Triggered by the practical relevance of skyline queries in many do-mains including e.g. customer segmentation, decision support and bio-medical applications, a huge volume of papers focus on effi-cient query processing, e.g. [6, 18, 9] and [11]. Most of these approaches can be classified into three major categories: nested-loop-based, divide-and-conquer-based and index-based methods. The Block-Nested-Loops algorithm (BNL) and the Extended Di-vide&amp;Conquer algorithm (D&amp;C) by B X rzs X nyi et al. [3] have been the fundamental approaches in the first two categories. More effi-cient approaches have recently b een proposed, e.g. the Sort-Filter-Skyline algorithm (SFS) by Chomicki et al. [6] which improves BNL by pre-sorting the data set. Index-based approaches focus on pruning strategies based on approximations to avoid scanning the whole data set. The Bitmap and the Index technique by Tan et al. [18] fall into this category. The first method uses bit-vectors as approximations, whereas the second applies a transformation into one-dimensional space to exploit the B + -tree for efficient re-trieval. Another index-based algorithm is presented by Kossmann et al. [9], which is based on nearest neighbor search. This method does not compute the skyline in a batch-oriented way but progres-sively outputs results and thus allows the user to change preferences during runtime. Papadias et al. [14] developed a progressive algo-rithm BBS (branch-and-bound skyline), based on a nearest neigh-bor search technique supported by an R-tree. Only nodes that may contain skyline points are accessed, and each node is processed only once. Lin et al. [11] developed an efficient skyline computa-tion method by determining the skyline for the most recent N el-ements in a data stream. All of these methods aim at the skyline computation of certain data.

Uncertain Data. In many applications, e.g. sensor networks [8] and moving objects, the features of the objects are not exactly known, but can be specified with some uncertainty, e.g. the current location of a moving object. A lot of work focuses on efficient in-dexing and query processing on uncertain data, e.g. [7, 17, 4, 5, 19, 2], just to mention a few. Typically, uncertain objects are modelled by probability density functions. Most query types on conventional databases, especially K-nearest neighbor and range queries have been extended to the probabilistic case. Cheng et al. [5] intro-duce R-tree-based index structure s to support probabilistic thresh-old queries on uncertain objects. An uncertain object is represented by a so-called uncertainty interval which covers the unknown true appearance of the object and is associated with a PDF. All objects which are answers with a probability exceeding a threshold are re-turned to the user. Similarly, Tao et al. [19] represent uncertain ob-jects by an uncertainty region associated with a PDF. The authors propose the U-tree, a hierarchical index structure to efficiently sup-port probabilistic range queries. Hype r-rectangular core parts of the uncertainty regions can be associated with bounds for the appear-ance probability of an uncertain object, which allows for effective pruning. In [2], uncertain objects are represented by Gaussians, which allows for probabilistic identification queries. Given an un-certain query object, the identification query returns those uncer-tain objects which most likely represent the same object (e.g. for a database of facial images: Find the images most likely showing the same person as portrayed on the query image). The Gauss-tree is a hierarchical index structure on the parameter space of Gaussians: Gaussians with similar mean and variance are stored in a common MBR. The Gauss-tree, the U-tree and most other index structures for the efficient management of uncertain data are fully dynamic in-dex structures which can efficiently handle insertions and deletions of uncertain objects. These index structures are very general and support many types of queries (e.g. identification queries). There-fore, it can be assumed that in a database system for uncertain ob-jects, such an index structure is available, and it is not required to construct a particular index for each single query.

Probabilistic Skylines on Multi-Instance Data. In contrast to common query types, such as range or identification queries, prob-abilistic skyline queries have been nearly untouched so far. To the best of our knowledge, only the work of Pei et al. [15] goes into this direction. The author s propose a probabilistic skyline model for multi-instance data, where each object is part of the skyline with a certain probability. Each object consists of multiple instances. The skyline probability of an object is determined by counting the num-ber of its instances which are not dominated by the instances of any other object in the database. This is the first approach dealing with uncertain data in the area of skyline queries, but it is not directly adaptable to objects which are initially represented by probability density functions. Approaches involving uncertain data represent uncertainty by a PDF because the true representation of an uncer-tain object is not available.
In this section, we introduce the basic definitions of probabilistic skylines. In the following we consider uncertain objects, each given by a PDF denoted by f ( x ) or g ( x ) , and the complete database consists of a set of such PDFs, DB = { f 1 ( x ) , ..., f n = | DB | is the number of uncertain objects in DB . The vec-sponding uncertain object that follows the given distribution func-tion ( x  X  f ( x ) ). Unlike in [15], x is not an instance given from the application.
The original concept of skylines (for d -dimensional certain fea-ture vectors) defines the notion of dominance in the following way: Object x dominates object y (in symbols x  X  y ) if the following two conditions hold: (1) x i  X  y i for all coordinates i :1  X  i  X  d ,and (2) x i &lt;y i for at least one coordinate i :1  X  i  X  d . The generalization from non-pr obabilistic skylines on certain ob-jects to probabilistic skylines on uncertain objects is given in the following definitions. We start with the generalization of the no-tion of the dominance : f ( x ) ,g ( y )  X  DB be uncertain objects. The probab ility that f dominates g ( y ) corresponds to th e probab ility that x generated from the probability density function f ( x ) dominates y generated by g ( y ) . In symbols: P ( x  X  y )= Instead of P ( x  X  y ) we will also often write P ( f ( x The symbol ume integral evaluated over the complete data space R d : Z Similarly to the dominance, also the skyline property of an uncer-tain object in the database can only be defined in terms of a proba-bility, the so-called skyline probability P S ( f ( x )) : ability of an uncertain object f ( x )  X  DB corresponds to the prob-ability with which x taken from the PDF f ( x ) is not dominated by any of the y each of which is independently taken from one of the remaining uncertain objects stored in the database g ( y )  X  where DB = DB \{ f ( x ) } : P
S ( f ( x )) = P ( Most of the considered PDFs (such as Gaussians) have infinite sup-port (i.e., the domain in which the probability density is different from 0 ). Hence in principle every object is in the skyline, though the skyline probability of most objects is very close to zero. In practice, usually only objects with a skyline probability signifi-cantly greater than zero are of interest. This is formalized by a user-defined threshold  X  : D EFINITION 3(  X  -S KYLINE ). Let  X   X  [0 .. 1] be a threshold. for which the following property holds: In the next section, we will derive the closed formulas for the dom-inance and skyline probability of uncertain objects, provided that the PDF is a normal distribution.
The Gaussian distribution (or normal distribution) is by far the most important and most commonly used distribution function in statistics. Its extarordinary role is mainly due to the central limit theorem . The probability density function f ( x ) corresponds to the well-known multivariate Gaussian function with mean  X  (  X  x, 1 , ...,  X  x,d ) T and a diagonal (in principle, our work can also be extended to the non-diagonal case) covariance matrix  X  diag (  X  2 x, 1 , ...,  X  2 x,d ) : f ( A second object g ( y ) can be defined applying separate parameters  X  y and  X  y .Since  X  x and  X  y are diagonal, the distribution func-tions are independent in the coordinates, and we can write them in the following way: f ( x )= The dominance probability can be rewritten as follows: P ( x  X  y )= Since all the factors in the products of this formula correspond to independent variables x i and since the conditions in the case dis-tinctions are independent, we can rearrange the terms in the follow-ing way and then exchange product and integral: P ( x  X  y )=
This means, we can do the whole determination of the dominance probability independently in each dimension (the two-fold integral) and then combine the individual dimension-wise probabilities to an overall probability (the product symbol in the above formula). Now we are going to derive the solution for each of the dimensions. In Figure 1, we see on the left side two different Gaussians. The task is to determine the probability with which x i  X  y i .

We substitute a new variable i such that y i = x i + i . Then, the half space which we have to integrate (in which the case distinc-tion yields 1) is exactly defined by i  X  0 . Then, our dominance probability co rresponds to:
P ( x i  X  y i )= where the formula (*) for the convolution product has already been proven in [2]. This can be simply rewritten using the cumulative distribution function, in the following form: We can now conclude that integrating the product of the two Gaus-sians exactly corresponds to integrating one Gaussian with the added variances of the two. As Figure 1 demonstrates, the original prob-lem on the left side has been transformed into an easy-to-solve problem on the right side, where only one Gaussian needs to be integrated. The integral is marked in color.
Now, we want to derive an analogous formula, provided that the sian Mixture Models rather than simple Gaussian functions. Gaus-sian Mixture models provide a highly flexible and accurate repre-sentation of uncertain objects. Gaussian Mixture Models are for example suitable to represent sensor data with non-Gaussian error distributions [1]. A Gaussian Mixture Model M x is given as a set of triplets where k = | M x | is the number of components. Each component is defined by its weight w x and its means  X  x and its diagonal variance matrix  X  x . The sum of the weights equals one:
The probability density function of the GMM is given by:
The GMM M y and its probability density function is defined analogously. The number of components l = | M y | may differ from that of M x .
 The dominance probability can now be written as:
P ( x  X  y )= We can apply the distributive law and then exchange summation and integration, and include our results obtained in Section 3.2 to obtain: =
When uncertain objects are stored in an index structure such as the U-tree [19] or the Gauss-tree [2], then it is an interesting ques-tion how to accelerate the search for the skyline by exploiting this index structure. If, for instance, a few objects belonging to the skyline are already known (or at least a few good candidates for skyline objects) then these objects may dominate not only other single database objects but also complete branches (subtrees) of the index. On the other hand, to determine the exact probability of an uncertain object f ( x ) to belong to the skyline, theoretically we have to examine all objects g ( y )  X  DB which have a probability P ( y x ) &gt; 0 of dominating the object. However, many objects g ( y ) may have a probability to dominate f ( x ) which is close to zero, and the same may be true for all objects g ( y ) in a complete subtree. This leads us to the question, how to estimate the high-est and lowest possible probability with which an uncertain object dominates an arbitrary object stored in a subtree, and, inversely, what is the highest and lowest probability that an arbitrary object stored in a subtree dominates another uncertain object.
Note that index structures such as the U-tree or the Gauss-tree are very general indexing methods efficiently supporting different kinds of queries (such as probabilistic identification queries) and efficiently supporting insertion of new uncertain objects and dele-tion of uncertain objects. Therefore, it can be assumed that such an index is available in a database system for uncertain objects. The details how insertions and deletions can be processed and how the tree is maintained (e.g. to keep balance) for the U-tree can be found in [19], for the Gauss-tree in [2].

An important principle for indexing probability density functions is the principle of parameter space indexing where the parameters (in the case of Gaussian PDFs (  X  1 , X  1 , ...,  X  d , X  d ) points from a (2  X  d ) -dimensional data space rather than spatially extended objects. For Gaussian Mixture Models, each uncertain object is represented by a set of such (2  X  d ) -dimensional points. The most prominent example for such a parameter space indexing method is the Gauss-tree [2]. Like in R-trees and similar tree struc-tures for high-dimensional data, objects which are close to each other in the (2  X  d ) -dimensional data space are grouped together into a common subtree, and the whole subtree is represented by an axis-parallel minimum bounding rectangle of the stored points. Be-fore accessing a node, query processing algorithms have to check according to the minimum bounding rectangle, whether or not the corresponding subtree is able to contain objects to answer the query. Figure 2: Maximum Probability that Object n  X  nates M y .

In our context, we need to determine the maximum and mini-mum probability of an object stored in a tree (  X  contained in the rectangle) to dominate a given uncertain object, and to be domi-nated by a given uncertain object, respectively.
 In the following sections we restrict the formulas to the simple Gaussian functions only for better readability, the exact same for-mulas hold for the Mixture Model as well, only the weighting factor is missing.
To clearly define the problem, we are given an uncertain object as a probability density function and a minimum bounding rectangle which defines limits for the parameters of other probability density functions g ( y ) which may be stored in the subtree. Therefore, g may be an arbitrary Gaussian function where the parameters fulfill the constraints We call the functions g ( y ) fulfilling the above constraints the al-lowed functions in the subtree. Note that a function does not have to be stored in the subtree to be allowed. The allowed functions are rather those uncertain objects which potentially could be stored in the subtree.

We want to determine among all allowed functions those which are dominated by f ( x ) with maximum and minimum probability, respectively. In symbols:  X  P ( f ( x )  X  MBR )= max  X  (  X  i :1  X  i  X  d ) is the maximum dominance probability, and  X  P ( f ( x )  X  MBR )= min (  X  i :1  X  i  X  d ) is the minimum dominance probability. Our first observation is that here the maximization (minimization) can be done individually in each dimension because the terms are again independent in the dimensions. We can add the variance of f that of g ( y ) :  X  P ( f ( x )  X  MBR )= (and analogously for the minimu m dominance probability). It is convenient to exchange  X  x,j and  X  y,j in the above equation by fol-lowing the rule N  X , X  2 ( x )=1  X  N x, X  2 (  X  ) :  X  P ( f ( x )  X  MBR )= Now, we can draw the four extreme curves of the MBR with the added variance of f ( x ) , as depicted in the top and middle diagram of Figure 2. We have also marked for two possible positions of  X  x,i , in the top diagram a position which is less than  X   X  diagrams, we have marked the dominated area, i.e. right from  X  The probability corresponds to the are a under the Gaussian, starting from  X  x,i until +  X  which exactly corresponds to 1  X  N... We will also show in the next paragraph how the parameters of N must be set in order to maximize (minimize) the dominance proba-bility.

No matter at what position  X  x,i in Figure 2 is, the maximally dominated function is always one of the rightmost possible func-tions, i.e.  X  y,i = X   X  i . The reason is that if we want to determine that curve, which has most of its area right from the point  X  then we have to shift the curve as far as possible to the right side. This intuition is also confirmed by the bottom diagram in Figure 2 which depicts for each of the extreme functions of the MBR the cor-responding function 1  X  N... (  X  x,i ) which gives for each position of  X  x,i the area under the Gaussian right from  X  x,i : The two func-tions centered at  X   X  i are everywhere greater than the correspond-ing functions centered at  X   X  i and all other allowed functions (with same variance). A little bit more complex is the determination of the variance  X  2 y,i of the function g ( y ) maximizing the dominance probability. We have to distinguish the two cases where  X  (case A) or right (case B) from  X   X  i .

Case (A) is depicted in the top diagram of Figure 2. In this case, we can see that the maximizing variance is defined by the most nar-row (and highest) function with  X  y,i = X   X  i , because the integral of a Gaussian is monotonically increasing (positive derivative) with increasing variance  X  2 if (and only if) the upper integration bound-ary is less than the mean, and monotonically decreasing (negative derivative) if (and only if) the upper integration boundary is greater than the mean. If the integration boundary is equal to the means, then the integral is constantly 0.5 no matter how large the variance is (and its derivative is equal to zero). Formally: Therefore, in case (A), 1  X  N... (  X  x,i ) is maximal for the smallest possible variance (i.e. the most narrow Gaussian), in this case  X  Figure 2 where the maximum among all allowed functions is N
Case (B) is depicted in the middle diagram of Figure 2. In this case, we see that  X  y,i = X   X  i defines the maximum because the marked area becomes larger the more  X  y i is increased. For the case  X  x,i = X   X  i , we would mark 50 percent of the curve, no matter how large  X  y i is. Summarizing, we obtain the following formula for an upper bound estimation of the maxi mum dominance probab ility:  X  P ( f ( x )  X  MBR )= We have 1  X  N... (  X  x,i ) because here we have to integrate from  X  x,i to +  X  rather than the integration boundaries from Section 3.2 (from  X  X  X  to  X  x,i ). The overall function  X  P ( f ( x depicted in the bottom diagram of Figure 2. Here we can see the four integral functions corresponding to the four extreme Gaussians representing the corner points of the MBR. The maximum (which changes at  X   X  i from 1  X  N  X   X  is marked in color. Analogously, the minimum dominance proba-bility is given by:  X  P ( f ( x )  X  MBR )=
In the bottom diagram of Figure 2, the minimum function is not particularly marked but it can also be recognized that minimal are those functions which are centered by  X   X  i and that the case distinc-tion of the above formula is also correct.
Inversely, we are also interested in the question what is the max-imum and minimum probability with which an arbitrary function from the MBR may prune a given uncertain object. This informa-tion will later be used to avoid unnecessary node refinements when confirming that the uncertain object is indeed a skyline object. The derivation of the following equations is analogous to those in Sec-tion 4.1:  X  P (
MBR  X  f ( x )) =  X  P (
MBR  X  f ( x )) =
We can define the dominance probability also at the level of two minimum bounding rectangles MBR x and MBR y and determine the maximum and minimum probability with which an arbitrary function stored in MBR x dominates an arbitrary curve stored in MBR y , respectively.  X  P (
MBR x  X  MBR y )=  X  P (
MBR x  X  MBR y )=
Until now, we have only given the details how the dominance probability can be determined for Gaussian PDFs but not for the skyline probability. An obvious idea to determine the skyline prob-ability of an uncertain object f ( x ) would be directly to use the results of Section 4.2 to 4.3 and to multiply the probabilities of f ( x ) . However, this approach requires the independence of the dominance property. As we will show in this section, the prob-ability that one object g ( y ) dominantes f ( x ) and the probability negatively) correlated. As we will see, negative correlations are ex-cluded due to the transitivity of the dominance. The consequence is, that we could under-estimate (but not over-estimate) the skyline probability by ignorin g this correlation.
 f ( x ) ,g ( y ) , and h ( z )  X  DB be uncertain objects. The conditional that f ( x ) is dominated by g ( y ) is greater or equal to the uncon-ditional pr obability that f ( x ) is dominated by h ( z ) conditional pr obability that f ( x ) is not dominated by h to the unconditi onal probab ility:
P ROOF . Consider Figure 3 where we have two stochastic vari-ables y i  X  g ( y i ) and z i  X  h ( z i ) Those values of x dominated by z i , are in the area marked by A = The values which are dominated by y i are in the areas A  X  B = whole Gaussian PDF). The unconditional probability with which x is dominated by z i is: In contrast, if we know that x i is dominated by y i (conditional probability), then the event space only consists of the areas A and, therefore, the conditional probability is: If the roles of y i and z i are exchanged, then the conditional prob-ability even becomes 1 because all instances which are dominated by z i are also dominated by y i . But also in this case the statement of the lemma holds (trivially). It also holds in the multivariate case ( h ( z )  X  f ( x ) ), because in the conditiona l probability , the area of negative events is always less or equal compared to the uncondi-tional probability whereas the positive events are unchanged. The same applies for the conditional probability for h ( z )  X  This lemma provides us some possibilities for the estimation of up-per and lower boundaries of the s kyline probability, mostly due to the product rule o f probab ilites ( P ( A  X  B )= P ( A )  X  for arbitrary, not necessarily independent stochastic events A and B which is related to Bayes X  theorem ) which allows the decompo-sition of the formula for the skyline probability. Let Q  X  (where again DB = DB \{ f ( x ) } ) be a suitable subset of the database, (e.g. those objects that have the largest probabilities to dominate f ( x ) ). Then according to the product rule of probabili-ties we have: P ( f ( x )) = P ( = P ( of which we can get an upper bound by completely cancelling the conditional term (which is  X  1 ), and a lower bound by replacing it by the unconditional term (which is a lower bound due to Lemma 1): P
S ( f ( x ))  X  P
S ( f ( x ))  X  and the term the independence assumption skyline estimation with respect to Q . Note that, if Q corresponds to the complete database DB ,then and the independence assumption estimation (which can be com-puted analytically in a very inexpensive way) can e.g. be used to rank the objects of the database (cf. Section 3.2). For the partial Montecarlo integration method: Let X = { x 1 , ..., x s } (with a number s = | X | of elements) of values i.i.d. generated from f ( x ) . Then:  X 
P Q S ( f ( x ))  X  P Q MC ( f ( x )) = This Montecarlo integration is not very expensive either since (1) it generates instances x only for the single PDF f ( x ) , and not for the (many) other functions g ( y )  X  Q (i.e. our method requires O ( s ) computations), and (2) this integration method is updatable whenever Q changes (e.g. when a new uncertain object is inserted into Q ). (e.g. when a new uncertain object is inserted into Q ). Our method has no demand to store generated instances permanently.
In the following sections we describe algorithms for efficient computation of the  X  -skyline.
The baseline algorithm for determining the  X  -skyline iterates over the number n of objects in the database, and determines the skyline probability of each object by generating s instances. The skyline probability is then computed according to the formula for P 4.4. An object f ( x ) belongs to the  X  -skyline if holds. Since each database object has to be compared to each other, the runtime complexity of the baseline algorithm is O ( s s being the sample rate. The baseline algorithm has only a linear runtime complexity with respect to the number of instances. But no filtering steps from Section 3 are applied to accelerate the skyline operator.
In order to avoid the computation of the exact skyline probability for each object, we introduce our priority algorithm. The main idea of the priority algorithm is to use the equations derived in Section 4.4 to exclude as many objects as possible from further considera-tion. Compared to the numerically determined skyline probability P extremly fast even on large databases. This fact is used to deter-mine for object f ( x ) the approximation P DB I ASP ( f ( resulting dominance probability 1  X  P ( g ( y )  X  f ( x )) comparison step is stored together with a reference to g ( ority queue in descending order. In the following iteration step ,in order to decide whether f ( x ) belongs to the  X  -skyline, we use the priority queue and get one object after the other out of the queue. For each dequeued object h ( z ) , we update the skyline probability P ority queue. When processing object h ( z ) we update P DB by cancelling out the dominance probability which is stored in the priority queue. This results in the probability P DB \ SF Figure 4: Pseudocode of the Priority Algorithm for Skyline Construction. representing the maximal possible influence of the remaining ob-jects in the priority queue on object f ( x ) . This means, the more objects h ( z ) are processed, the smaller P SF MC ( f ( x f ( x ) can be returned if one of the following conditions are ful-filled: If we are only interested in objects belonging to the  X  -skyline, we can stop the computation as soon as we know that f ( x ) belongs to the skyline. But then the exact skyline probability value of f can not be returned.

We can typically exclude object f ( x ) from the skyline very early, i.e. after comparing it with only a few other objects h ( ority queue, since the queue is sorted in descending order according the highest influence on the skyline probability of f ( x cessed first.

The pseudocode for the priority  X  -skyline algorithm is depicted in Figure 4. Here, the computation of P DB IASP ( f ( x )) refers to the formula in Section 4.4. The worst case runtime com-plexity of the priority algorithm is: with s being the sample rate, n the number of objects in the database, and | SF | being the maximum number of objects considered in the set SF . In Section 6 we will demonstrate the superiority of the pri-ority algorithm over the baseline approach. The main disadvantage of the priority algorithm is its I/O costs.
When objects are stored in a database a common indexing tech-nique uses minimal bounding rectangles (MBR). The indexed ap-proach uses the MBR technique as described in section 4. By using the index the  X  -skyline can prune several MBRs without looking at the exact objects.

The indexed approach is backed by a priority queue held in main memory. The priority queue is sorted descending according to the  X  P
MC ( f ( x )) of an object f ( x ) and according to MBR R . The first element to be inserted in the priority queue is the root of the index. The next element to be processed is the one with the highest current skyline probability. If the next element is a node with MBR R , it is removed from the queue and all its child elements are inserted into the queue. By inserting the child elements, the values of  X  P MC ( f ( x )) and  X  P MC ( f ( f ( x ) as well as  X  P MC ( R ) of all MBRs R are updated.

If the next element is an object f ( x ) , it can be returned provided that one of the two following conditions are fulfilled: When we exclude an object f ( x ) from the skyline we can stop searching for further objects in the queue, since all other objects have smaller skyline probabilities than f ( x ) . If we cannot decide for the current object if it belongs to the skyline or not, we process the priority queue X  X  next MBR and insert its children. Thus, the skyline probabilities for all objects contained in the queue will be refined.

The pseudocode for the indexed  X  -skyline algorithm is depicted in Figure 5. The update of the upper and lower bounds of the sky-line probability (  X  P pq MC ( f ( x )) ,  X  P pq MC ( f ( of the priority queue in line 14 can be done in one iteration step over the priority queue when inserting all the child elements of the former MBR. In this iteration the computation of the upper bound (line 15) of the independenc e assumption sky line probability (  X 
P well.

Our experiments presented in the next section demonstrate sub-stantial performance gains by indexing. As with any index struc-ture, the magnitude of improvement depends on data distribution and dimensionality. Thus, an acceptable worst case runtime com-plexity is an important characteristic. The indexed approach has a worst case runtime complexity of: with s being the sample rate and n the number of objects in the database, since the objects are retrieved from the tree and after-wards in the worst case compared to each other.
In this section, we present an extensive experimental evaluation on synthetic and real world data. Uncertain objects are provided by single Gaussian distributions or Gaussian Mixture Models. Un-less otherwise specified, the following experiments have been per-formed on 4,000 uniformly distributed 3-dimensional uncertain ob-jects with a sample-rate of 100 and a threshold of  X  =0 . tainty is simulated by representing each object as Gaussians with mean between 0 and 1,000 and a variance between 0 and 1000
Besides synthetic data we used the NBA game-by-game tech-nical statistics from 1991 to 2005 which is available at the NBA website www.NBA.com . The data set contains the performance statistics of 1,313 players. We represented each player as an un-certain object including 3 important statistics on his performance: number of points, number of assists, and number of rebounds (each defined by mean and variance ).
 We implemented all of our algorithms single threaded in Java. Unless otherwise noted, all experiments were performed on a SUN Fire X4600 with SunOS which is equipped with four Dual-Core AMD Opteron Processor and has 32 GByte of RAM. But in all ex-periments our algorithms only used 256 MB of Ram and a single CPU core.

Runtime w.r.t. Sample Rate. Figure 6 displays the effect of varying sample rates on NBA data. It is evident that all algorithms scale linearly with increasing sample rate (cf. Figure 6(a)). How-ever, the linear factor is much larger for the baseline algorithm than for the other two. For 10,000 samples the baseline algorithm needs 198 minutes, whereas the priority algorithm needs 13 min-utes. The indexed algorithm taking approximately 2 minutes is the fastest method. The reason for this is that relatively costly Mon-tecarlo integrations are performed for each pair of objects in the baseline approach. Exploiting t he lower bounding approximation even the simple priority algorithm saves much time. In the indexed approach, whole sub-trees can be pruned based on this efficient ap-proximation, leading to superior runtime as displayed in detail in Figure 6(b).

Another experiment for the sample rate was evaluated on a syn-thetic generated Gaussian Mixture Model data set with 1500 ob-jects in the database. As depicted in figure 7(a) the differences in runtime especially for the baseline are tremendous, here the priority and the indexed algorithms show their superiority.

Semantically the sample rate determines the level of abstraction which is used to represent an uncertain object. A higher sample rate may lead to more accurate results. Our experiments on NBA data demonstrate that a sample rate of 100 is sufficient to achieve a stable result. The skyline probabilities of the NBA players are displayed in Table 1. LeBron James is in all our results the most LeBron James 0.525133 Karl Malone 0.414282 Shaquille O X  X eal 0.496458 Allen Iverson 0.387400 Jason Kidd 0.484001 Chris Webber 0.383983 Charles Barkley 0.482588 Michael Jordan 0.379210 Dennis Rodman 0.466790 John Stockton 0.318612 Kevin Garnett 0.458139 Grant Hill 0.317689 Tim Duncan 0.420447 Kevin Johnson 0.312901 Table 1: NBA Players with a Skyline Probability of at least 0.3. outstanding player. He is always several percentages better than the second best player according to the skyline probability. Players with almost equivalent skyline probability differ only in the 3 imal place e.g. Jason Kidd and Charles Barkley or Grant Hill and John Stockton. As depicted in Figure 7(b) the skyline probabilities of the NBA-players show highly stable results and only vary on average about 0.7% when repeating the experiment multiple times with sample rates ranging from 1000 to 4000. Figure 7: Variance of Results and Runtime for Sample Rate.
Runtime w.r.t. Number of Objects. Figure 8 shows the runtime behavior of the three algorithms on different database size. We gen-erated synthetic data sets with varying number of objects. As it can be seen in Figure 8(b), the runtime of the baseline algorithm heav-ily depends on the database size n , whereas the priority and the indexed demonstrate their superiority. Even at moderate database sizes of 4,000 objects the baseline approach is outperformed by a factor of 57 compared to the priority algorithm. Figure 8(a) com-pares the runtime of priority and indexed algorithm. The indexed shows a speedup of factor 4 for 10,000 objects. The response time is only 52 seconds whereas the priority algorithm needs 213 sec-onds.

I/O Costs of the Indexed Approach. Another experiment demon-strates the I/O cost of the indexed algorithm. We defined a page size of 2 kByte and assumed a cache of 20% of the pages for both algo-rithms. In Figure 9(b) the resulting page accesses are depicted. It can be seen that the indexed outperforms the priority algorithms by magnitudes especially for large number of objects in the database. Comparing the data set with 10,000 objects, the indexed algorithm only has to access 186 pages whereas the priority has to load 6.8 times more pages, resulting in 1,280 page accesses. Figure 8: Runtime Analysis with Increasing Database Size.
Runtime w.r.t.  X  . Figure 9(a) depicts the runtime on 1500 synthetic generated Gaussian Mixture Model data with increasing values for the skyline-threshold  X  , ranging from 0.1 to 0.9. The runtime for the baseline algorithm is not included in this diagram, since its runtime does not depend on  X  . Recall that the baseline approach first computes the skyline for all objects and just scans in a postprocessing step the result for objects having a higher skyline probability than  X  . The runtime of the baseline approach is about 24 minutes for all  X  values. The performance of the priority algo-rithm starts to degrade for thresholds lower than approximately 0.4, whereas the indexed approach scales only slightly sub-linear with decreasing  X  . This indicates that pruning is effective even for very small threshold values. Even for the extreme case of  X  =0 query is processed in 4 seconds, which is a speedup of a factor of 24 w.r.t. the priority algorithm. (a) different  X  threshold values Figure 9: Runtime Analysis on Changing  X  Threshold and Page Accesses.
In this paper, we have introduced the probabilistic skyline query which is a powerful building block for data mining and decision support applications on uncertain data. This is the first approach extending the skyline operator to uncertain objects which are mod-elled by probability density functions. Analogously to the con-ventional skyline query, we provide definitions for dominance and skyline probability fo r uncertain objects. We propose an efficient algorithm to compute the skyline probability of each uncertain ob-ject and introduce the  X  -skyline query, together with an efficient algorithms for query processing. The result set of the  X  -skyline query contains all objects with a skyline probability of more than  X  %. Our algorithms relies on a very simple data structure based on MBRs and thus can be integrated into many indexing techniques. In an extensive experimental evaluation on synthetic and real data we demonstrate that the proposed algorithms are highly efficient and scalable to large data sets. [1] R. S. Blum, Y. Zhang, B. M. Sadler, and R. J. Kozick. [2] C. B X hm, A. Pryakhin, and M. Schubert. The gauss-tree: [3] S. B X rzs X nyi, D. Kossmann, and K. Stocker. The skyline [4] R. Cheng, D. V. Kalashnikov, and S. Prabhakar. Evaluating [5] R. Cheng, Y. Xia, S. Prabhakar, R. Shah, and J. S. Vitter. [6] J. Chomicki, P. Godfrey, J. Gryz, and D. Liang. Skyline with [7] N. N. Dalvi and D. Suciu. Answering queries from statistics [8] A. Faradjian, J. Gehrke, a nd P. Bonnet. Ga dt: A probability [9] D. Kossmann, F. Ramsak, and S. Rost. Shooting stars in the [10] Q. Li, B. Moon, and I. Lopez. Skyline index for time series [11] X. Lin, Y. Yuan, W. Wang, and H. Lu. Stabbing the sky: [12] K. Lu, Y. Qian, D. Rodr X guez, W. Rivera, and M. Rodriguez. [13] A. M. Mainwaring, D. E. Culler, J. Polastre, R. Szewczyk, [14] D. Papadias, Y. Tao, G. Fu, and B. Seeger. An optimal and [15] J. Pei, B. Jiang, X. Lin, and Y. Yuan. Probabilistic skylines [16] B. Sarikaya, M. A. Alim, and S. Rezaei. Integrating wireless [17] A. D. Sarma, O. Benjelloun, A. Y. Halevy, and J. Widom. [18] K.-L. Tan, P.-K. Eng, and B. C. Ooi. Efficient progressive [19] Y. Tao, R. Cheng, X. Xiao, W. K. Ngai, B. Kao, and [20] A. K. H. Tung, Z. Huang, H. Lu, and B. C. Ooi. Continuous [21] D.-L. Yu and D.-W. Yu. Detecting sensor faults for a
