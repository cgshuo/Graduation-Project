 We address the problem of predicting new drug-target interactions from three inputs: known interactions, similarities over drugs and those over targets. This setting has been considered by many meth-ods, which however have a common problem of allowing to have only one similarity matrix over drugs and that over targets. The key idea of our approach is to use more than one similarity matri-ces over drugs as well as those over targets, where weights over the multiple similarity matrices are estimated from data to auto-matically select similarities, which are effective for improving the performance of predicting drug-target interactions. We propose a factor model, named Multiple Similarities Collaborative Matrix Factorization (MSCMF), which projects drugs and targets into a common low-rank feature space, which is further consistent with weighted similarity matrices over drugs and those over targets. These two low-rank matrices and weights over similarity matrices are es-timated by an alternating least squares algorithm. Our approach allows to predict drug-target interactions by the two low-rank ma-trices collaboratively and to detect similarities which are important for predicting drug-target interactions. This approach is general and applicable to any binary relations with similarities over ele-ments, being found in many applications, such as recommender systems. In fact, MSCMF is an extension of weighted low-rank approximation for one-class collaborative filtering. We extensively evaluated the performance of MSCMF by using both synthetic and real datasets. Experimental results showed nice properties of MSCMF on selecting similarities useful in improving the predictive perfor-mance and the performance advantage of MSCMF over six state-of-the-art methods for predicting drug-target interactions. G.1.2 [ Numerical Analysis ]: Approximation X  least squares ap-proximation ; I.2.6 [ Artificial Intelligence ]: Learning X  knowledge acquisition, parameter learning Algorithm, Experimentation, Performance Chemoinformatics; Drug-target interaction; Weighted low-rank ap-proximation; Multiple types of similarities over drugs and targets
Pharmaceutical sciences are an interdisciplinary research field of fundamental sciences, including biology, chemistry and physics, and a successfully developed engineering field, which has created a major industry of our society. The objective of pharmaceutical sciences is drug discovery, which starts with finding effective inter-actions between drugs and targets, where drugs are chemical com-pounds and targets are proteins (amino acid sequences). Known drug-target interactions are however limited to a small number [8], say only less than 7,000 compounds in fact having target protein in-formation in PubChem [22], one of the largest chemical compound databases with currently around 35 million entries. Furthermore nowadays drug discovery, i.e. finding new drug-target interactions, needs much more cost and time, because relatively similar drugs (or targets) to those in known interactions have been already ex-amined thoroughly. In this light efficient computational methods for predicting potential drug-target interactions are useful and long-awaited [11].

Two major computational approaches are docking simulation and data mining (or machine learning). Docking simulation is biolog-ically well-accepted but has two serious problems: 1) simulation always needs three-dimensional (3D) structures of targets which are often unavailable [4] and 2) simulation is heavily time con-suming. On the other hand, machine learning is much more effi-cient, by which a large number of candidates can be tested within a very short period of time. A straight-forward approach in machine learning is to set up a standard classification problem over a ta-ble of instances and their features, where instances are drug-target pairs, and features are chemical descriptors (for drugs) and amino acid subsequences (for targets). Any classification methods, such as support vector machine (SVM), can be applied to this table [18].
Drug-target interactions can be represented by a binary-labeled matrix Y of drugs and targets, where an element is 1 if the corre-sponding drug and target interact; and 0 if they do not interact. The problem of predicting drug-target interactions is to estimate labels of unknown elements from known elements in Y . For this problem, similarities between drugs and those between targets are helpful to predict drug-target interactions, assuming that similar drugs tend to share similar targets and vice versa [14]. A relatively simple idea of using the similarities is the pairwise kernel method (PKM) [13], which generates similarities (or kernels) between drug-target pairs from those between drugs and those between targets. PKM has to generate a huge matrix of all possible combinations of drug-target pairs, causing a serious drawback in computational efficiency. In-stead, a typical procedure of similarity-based approaches is to use drug and target similarities to generate kernels over drugs and those over targets, respectively, from which drug-target interactions are estimated by kernel methods, such as SVM [2] and kernel regres-sion [28, 26]. These approaches however have a common short-coming that the kernel from drugs is generated independently from that from targets, meaning that predictions are done twice sepa-rately and the final result is obtained by averaging over the two predictions (See Section 3.2.3 for detail). This indicates that drug-target  X  X nteractions X  are not captured well enough by the current similarity-based approaches. Furthermore so far similarity-based methods have used only one type of similarity for drugs and that for targets. In fact, chemical structure similarity and genomic se-quence similarity are the most major metrics for drugs and targets, respectively. However, both drugs and targets have different types of similarity measures, and considering different types of similari-ties might enhance the predictive performance of drug-target inter-actions. We thus need to develop a method, which can incorporate multiple types of similarities from drugs and those from targets at once, to predict drug-target interactions.

Our proposed approach is to approximate the input drug-target interaction matrix Y by two low-rank matrices A and B , which share the same feature space, so that A and B should be the space to be generated by the weighted similarity matrices of drugs and those of targets, respectively. In other words, Y is collaboratively approximated by the inner products of the feature vectors of drugs, i.e. A , and those of targets, i.e. B , where the weighted drug (target) similarity matrices are also approximated by the inner products of drug (target) feature vectors themselves. We name this formulation multiple similarities collaborative matrix factorization (MSCMF) . We further propose an alternating least squares algorithm to esti-mate A , B and weights over drug and target similarities, by which MSCMF can select similarities which are the most consistent with the given drug-target interactions, resulting in the performance im-provement for predicting drug-target interactions.

Low-rank approximation with respect to the Frobenius norm can be solved easily by singular value decomposition (SVD), if there are no constraints on factorized matrices. In data mining, the low-rank approximation is a starting point of many different disciplines. First, under no regularization terms, several variants of SVD, such as the generalized low-rank approximation model (GLRAM) [30] and the probabilistic matrix factorization (PMF) model [21], are proposed, being applied to information retrieval, particularly rec-ommender systems [17]. Another series of related work are  X  X on-negative X  matrix factorization (NMF) [6, 7], in which factorized matrices must keep the elements nonnegative. On the other hand, MSCMF is a weighted low-rank approximation (WLRA) with reg-ularization terms including a L2 (Tikhonov) regularization term over the low-rank matrices A and B , and there are no nonnega-tive constraints on A and B . This means that MSCMF is different from both simple low-rank approximation formulation and NMF. An EM algorithm for estimating the factorized matrices A and B under WLRA is already presented [24]. WLRA with Tikhonov regularization over A and B is also equivalent to one formalization for recommender systems, called one-class collaborative filtering, where an alternating least squares algorithm was presented for es-timating A and B [20, 19, 15]. The key difference of MSCMF from one-class collaborative filtering is that MSCMF further in-corporates regularization terms to consider similarity matrices over drugs and those over targets, particularly multiple similarities of drugs and targets. In fact, similarity matrices over users and those over items are also considered in recommender systems [15, 10]. However, first, in [15], similarities are rather preprocessed and in-corporated into the weight of WLRA, by which the formulation of one-class collaborative filtering is still used. Second, in [10], sim-ilarities are processed in a rather similar way to MSCMF in terms that the formulation contains a graph regularization term for simi-larity matrices, while the main factorization is weighted NMF, by which factorized matrices must be nonnegative. On the other hand, our formulation is WLRA with Tikhonov regularization and regu-larization terms over drug and target similarities, resulting in a new, original formulation, and we present an alternating least squares algorithm for estimating parameters in this formulation. In fact, alternating least squares as well as stochastic gradient descent are currently the two major and well-accepted approaches for comput-ing matrix factorization.

We empirically evaluated the performance of MSCMF by using both synthetic and real datasets. We first examined MSCMF in terms of the performance improvement in adding similarity matri-ces and the selectivity of similarity matrices by using various types of synthetic data. We then evaluated the predictive performance of MSCMF under four benchmark datasets, comparing with six state-of-the-art similarity-based drug-target interaction prediction meth-ods, in three settings of predicting 1) new (unknown) interactions (pair prediction), 2) new drugs (drug prediction) and 3) new tar-gets (target prediction). Experimental results showed that MSCMF outperformed all competing methods, in terms of AUPR (Area Un-der the Precision Recall curve), for all three prediction settings and four datasets. In addition, we checked the performance difference by paired t -test, and the performance advantage of MSCMF was statistically significant in all 56 cases except only three cases at the significance level of 0.01 against p -values of the paired t -test. This result indicates a clear performance advantage over the competing methods.
Let D = { d 1 ; d 2 ; : : : ; d N d } be a given set of drugs and T = { t 1 ; t 2 ; : : : ; t N t } be a given set of targets, where N number of drugs and targets, respectively. Let { S 1 d ; S be a set of drug similarity matrices, where each is a N d and M d is the number of drug similarity matrices. We denote the score between drugs d i and d j in the k -th drug similarity matrix S matrices, where each is a N t  X  N t matrix, m t is the number of target similarity matrices, and the ( i; j ) -element of S and t j in S k t . Let Y be a N d  X  N t binary matrix of true labels of drug-target interactions, where Y ij = 1 if drug d i and target t interact with each other, and Y ij = 0 if they do not interact. The input of our method is the above two sets of similarity matrices and Y . Let F be a score function matrix, where the ( i; j ) -element of F , i.e. F ij , shows the score that drug d i and target t with each other. The problem is to estimate F so that F should be consistent with Y .
The main idea of MSCMF is to project drugs and targets into two low-rank matrices, corresponding to feature spaces of drugs and targets, respectively.

We thus factorize Y into two low-rank feature matrices A and B so that one drug-target interaction should be approximated by the inner product between the two feature vectors of the corresponding drug and target as follows: where A and B are N d  X  K and N t  X  K feature matrices of drugs and targets, respectively, and K is the dimension of the feature spaces.

Estimating A and B leads us to reconstruct Y , by which un-known interactions can have prediction scores. Figure 1 illustrates the process of Eq. (1). To estimate A and B in the matrix fac-torization, a reasonable approach is to minimize the squared error which can be our objective function: where  X  X  X  X  2 F is the Frobenius norm.

In order to distinguish known drug-target pairs from unknown pairs, we consider one formulation, weighted low-rank approxi-mation, which introduces a N d  X  N t weight matrix W , in which W ij = 1 if Y ij is a known drug-target pair, i.e. an interacting or non-interacting pair; otherwise W ij = 0 . W is given as an input and used as follows: where W  X  Z denotes the element-wise product of matrices W and Z .

Then to avoid overfitting of A and B to training data, we ap-ply L2 (Tikhonov) regularization to Eq. (2) by adding two terms regarding A and B . arg min where l is a regularization coefficient.

Suppose that we have only one similarity matrix for drugs, S and that for targets, S t . Our idea is that the generated low-rank ma- X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  Input: Output: 1: Initialize A , B , ! d and ! t randomly; 2: repeat 3: Update each row vector of A using Eq. (4); 4: Update each row vector of B using Eq. (5); 5: Update weight vector ! d using Eq. (6); 6: Update weight vector ! t using Eq. (7); 7: Update F using Eq. (8); 8: until Convergence; 9: Output F ;  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  Figure 3: Pseudocode of Parameter Estimation in Multiple Similarities Collaborative Matrix Factorization trices should be factorized matrices of the drug and target similari-ties. That is, the similarity between drugs should be approximated by the inner product of the corresponding two drug feature vectors, and this is also the case with the target similarity, as follows: Figure 2 shows a schematic picture of Eqs. (3).

Here we have a set of similarity matrices, instead of only one similarity matrix, for drugs and also for targets. We can then re-place one similarity matrix with the one which combines multiple similarity matrices linearly as follows: where ! k d and ! k t are weights over multiple similarity matrices for drugs and targets, respectively, ! d = ( ! 1 d ; : : : ; ! ( ! t ; : : : ; ! M t t ) T .
 We thus minimize the squared error between S d ( S t ) and AA ( BB T ), resulting in the two regularization terms. Thus the entire objective function (loss function L ) can be written as follows: where d , t and ! are regularization coefficients. Our regular-ization terms are all additive, and this manner of using regulariza-tion terms is typical (e.g. [27]). (a) True clusters (b) Input interaction matrix (c) Similarity matrix (low noise) (d) Similarity matrix (high noise) Figure 4: (a) True clusters, (b) the input interaction matrix, (c) a sample similarity matrix with low noise and (d) a sample similarity matrix with high noise, all for five balanced clusters
We select alternating least squares to estimate A , B , ! ! , which minimize L . Here let a i and b j be the i -th and j -th row vectors of A and B , respectively. We take the partial derivative of as follo ws: where I K is the K  X  K identity matrix.

Similarly, according to @ L @ b B as follows:
Again, updating rules of ! d and ! t are given as follows: where 1 K is the v ector, in which all K elements are 1, letting ( i; j ) and t ( i; j ) be the ( i; j ) -elements of d and tively, d and t are given as follows:
Furthermore letting d ( k ) and t ( k ) be the k -th elements of vec-tors d and t , d and t are given as follows:
Fig. 3 shows a pseudocode of the alternating least squares algo-rithm for estimating A , B , ! d and ! t . In this algorithm, we first initialize A , B , ! d and ! t randomly and repeat updating A , B , ! d and ! t , according to Eqs. (4), (5), (6) and (7), respectively, until convergence. Finally, the matrix of predicted drug-target in-teractions F is given as follows: F ij is the predicted interaction score of drug d i and target t the drug and target in a highly ranked pair in terms of the scores are predicted to interact with each other.
Drug-target interactions as well as user-item collaborations would have latent clusters (or factors) from which interactions (or collabo-rations) can be generated. Thus we first embedded true clusters into the interaction matrix and then simulated a real-world situation by adding a certain amount of noise to the interaction matrix so that the true clusters should not be easily retrieved from the interaction matrix only. For simplicity, we focused on disjoint clusters (by which it is easy to generate synthetic similarity matrices). Figure 4 shows an example of true clusters and the input interaction matrix generated by using the true clusters with noise, for five balanced clusters. We further generated drug similarity matrices and target similarity matrices by first incorporating the cluster information of drug-target interaction matrix (so clusters are on the diagonal of the similarity matrices) and then adding a certain amount of noise, so that multiple similarity matrices have a different amount of noise. Figure 4 shows two samples of similarity matrices (with low and high noise), again for five balanced clusters. We then checked how well the true clusters can be estimated from the interaction matrix and how much this performance can be improved by adding simi-larity matrices to the interaction matrix. Note that we added simi-larity matrices keeping the diversity in terms of noise in the similar-ity matrices. At the same time we examined what type of similarity matrices will be selected, where we expect that less noisy matrices will be selected and more noisy matrices will be discarded.
We here show more detailed experimental settings below: All in-teraction matrices we used have 200 drugs and 150 targets. We first embedded several true clusters in the interaction matrix by which the ( i; j ) -element of this matrix is 1 if d i and t j are in the same cluster; otherwise this element value is 0. We then completed the input interaction matrix by considering two types of noise: 1) we simply replaced 80% of 1 of the interaction matrix with 0 randomly, and 2) we randomly flipped 2% of all 0 to 1. Then similarity ma-trices are generated in the following manner: We first generated a true similarity matrix S true so that the ( i; j ) -element of S is 1 if d i and d j (or t i and t j ) belong to the same true cluster (note that this is easy because true clusters are disjoint); other-wise this value is 0. We then generated a random matrix S where each element of this matrix randomly takes a value between (a) Interaction matrix only (b) 1 similarity matrix added (c) 4 similarity matrices added (d) 16 similarity matrices added Figure 5: Estimated interactions (a) without any similarity ma-trices, with (b) one similarity matrix, (c) four similarity matri-ces and (d) 16 similarity matrices, when five true clusters are balanced. 0 and 1 and the diagonal elements are forced to be 1. We fi-nally generated similarity matrices (for both drugs and targets) by S true  X  ( noise_level )  X  S random , where noise_level is a value being changed from 0.15 to 0.9 by the interval of 0.05, resulting in 16 similarity matrices, all having different noise levels totally.
We started our experiments for the five balanced clusters shown in Figure 4, in which each cluster has 40 drugs and 30 targets. Fig-ure 5 shows the estimated drug-target interactions (clusters) from the input interaction matrix and similarity matrices. From this fig-ure, we can see that clusters (interactions) was not clearly predicted if the input was the interaction matrix only, while these clusters were made clearer by adding a larger number of similarity matri-ces. This indicates that clusters (interactions) were more clearly predicted by adding a larger number of similarity matrices, and MSCMF works well for this addition of similarity matrices to the input interaction matrix. We then made the size of clusters un-balanced, keeping the number of clusters at the same, where the sizes of five true clusters were (70, 10), (55, 20), (40, 30), (25, 40), (10, 50) for drugs and targets, respectively. Figure 6 shows the estimated drug-target interactions (clusters) from the input interac-tion matrix and similarity matrices. This figure also clearly shows the advantage of adding similarity matrices and the efficiency of MSCMF for selecting the similarity matrices useful to improve the performance.

Instead of checking the obtained interaction matrices directly, we then checked normalized mutual information (NMI) between the true clusters and the obtained interactions for both the balanced and unbalanced cases (Note that NMI is a standard measure to check the performance of clustering methods). Figure 7 shows the NMI when we changed the number of added similarity matrices. This figure reveals that NMI was clearly bigger by adding similarity ma-trices for the both cases, while NMI was saturated when around six similarity matrices were added. This would be because low noise similarity matrix was already included when we added around six or so similarity matrices, keeping the diversity of noise level in the (a) Interaction matrix only (b) 1 similarity matrix added (c) 4 similarity matrices added (d) 16 similarity matrices added Figure 6: Estimated interactions (a) without any similarity ma-trices, with (b) one similarity matrix, (c) four similarity matri-ces and (d) 16 similarity matrices, when five true clusters are unbalanced. (a) Balanced case (b) Unbalanced case Figure 7: NMI for the (a) balanced 5 clusters and (b) unbal-anced 5 clusters. matrices. We thus checked the change of weights over similarity matrices during the iteration of our alternating least squares algo-rithm. Figure 8 shows the resultant weight change during the it-eration, when we used five similarity matrices with different noise levels: 0.15, 0.3, 0.5, 0.7 and 0.9. From this figure, we can see that the initial weights were 0.2 (we used an uniform distribution for this case), while in both cases, only two similarity matrices had high weights (around 0.5) finally. These selected similarity matri-ces were with the noise level of 0.15 and 0.3, indicating that low-noise similarity matrices were selected by MSCMF automatically and high-noise similarity matrices were not, implying the efficiency of MSCMF for selecting better similarity matrices.

Finally we changed the number of clusters from 3 to 7, keeping the cluster unbalancedness. In fact, the sizes of clusters we tested were, for drugs and targets, (110, 20), (60, 50) and (30, 80) for 3 clusters, (80, 10), (60, 20), (40, 40) and (20, 80) for 4 clusters, (65, 5), (50, 10), (40, 20), (25, 30), (15, 40) and (5, 45) for 6 clusters, and (70, 5), (50, 10), (30, 15), (20, 20), (15, 25), (10, 35) and (5, 40) for 7 clusters. Figures 9 and 10 show the NMI when we changed the number of added similarity matrices and the weights over five similarity matrices (with the same noise levels as the case of five clusters) during iterations of our algorithm, respectively. The re-sults of these figures were totally consistent with those obtained (a) Balanced 5 clusters (b) Unbalanced 5 clusters Figure 8: Change of weights over similarity matrices during iteration of our algorithm for the (a) balanced 5 clusters and (b) unbalanced 5 clusters. (a) 3 clusters (b) 4 clusters (c) 6 clusters (d) 7 clusters Figure 9: Variation of NMI by adding similarity matrices for (a) 3 clusters, (b) 4 clusters, (c) 6 clusters and (d) 7 clusters, all being unbalanced clusters. when we used five clusters, and for example, only two similarity matrices were finally selected regardless of the number of clusters in Figure 10. One additional item of note is that when we used random initial weights, the final weights were almost the same as those of uniform distributions which are shown in Figures 8 and 10. This indicates that our optimization algorithm is stable against the change of initial values. Overall these results indicate that the efficiency of MSCMF is robust against the cluster size as well as cluster unbalancedness. We used four real benchmark datasets, called Nuclear receptor , GPCR , Ion channel and Enzyme , which were originally provided by [29] 1 . These datasets were collected from four general databases and frequently used in predicting drug-target interactions [2, 28, 26, 9]. Table 1 shows the statistics of these four datasets.
All datasets are downloadable from http://web.kuicr.kyoto-u.ac.jp/supp/yoshi/drugtarget/ (a) 3 clusters (b) 4 clusters (c) 6 clusters (d) 7 clusters Figure 10: Variation of weights during the algorithm iteration for (a) 3 clusters, (b) 4 clusters, (c) 6 clusters and (d) 7 clusters, all being unbalanced clusters.

We used the following two and four types of similarities for drugs and targets, respectively, by considering the effectiveness of predicting drug-target interactions [12].
 Drugs: Chemical structure similarity (CS) is computed by the number of shared substructures in chemical structures between two drugs. ATC similarity (ATC) is computed by using a hierarchical drug classification system, called ATC (Anatomical Therapeutic Chem-ical) [23]. We used a general method in [16] to compute the simi-larity between two nodes (drugs) in this classification tree. Targets: Genomic sequence similarity (GS) is computed by a normalized Smith-Waterman score [29] between two target sequences. Gene Ontology (GO) similarity is the overlap of GO annotations [1] of two targets for which we simply used GOSemSim [31]. We con-sidered two options of GO: molecular functions (MF) and biologi-cal processes (BP).
 Protein-protein interaction (PPI) network similarity (PPI) is computed from the shortest distance between two targets in a hu-man protein-protein interaction (PPI) network [25].
 We note that CS and GS are the most standard similarities (which have been used for predicting drug-target interactions) and so we just downloaded the data of CS and GS along with the drug-target interaction data, while we computed the other similarities by using the procedure described above. We further note that these similari-ties are diverse. For example, GS is derived from static (sequence) information, while PPI is more dynamic and noisy, and GO is rather in between these two types of similarities.
We here briefly review six state-of-the-art similarity-based meth-ods for predicting drug-target interactions, all being compared with our method in this experiment.

Pairwise Kernel Method (PKM) [13] generates similarities (ker-nels) over drug-target pairs, which can be the input instances of SVM. The similarity between two drug-target pairs, say ( d; t ) and ( d ; t  X  ) , is computed from given drug and target similarities, s and s t ( t; t  X  ) , as follows:
Bipartite local model (BLM) [2] also uses SVM. To predict a score of drug-target pair ( d; t ) , first drug d is fixed, and a SVM is trained by using known interactions of drug d as instances where the kernel over instances is the target similarity matrix. Second target t is fixed, and a SVM is trained by using interactions of target t . In [2], the prediction is obtained by the maximum of the results of the two trained SVM, while in our experiments we used the average over them, since this is standard in other methods, such as [28, 26].
Net Laplacian regularized least squares (NetLapRLS) [28] min-imizes the least squared error between Y and F . Note that F is obtained twice, i.e. drugs and targets, separately. For drugs, we first compute a N d  X  N d matrix O d , showing how many targets are overlapped between two drugs. We then have matrix V d linear combination of O d with S d : V d = t S d +(1  X  t ) O then given by F d = V d d , where d is the parameter matrix to be estimated. Note that NetLapRLS uses only one similarity matrix over drugs. The entire formulation is given as follows: where n is a weight and L d is a normalized graph Laplacian of S . This is a convex optimization problem, which gives the fol-lowing direct solution: The same operation is done for the target side to have ^ F final result is obtained by the average: ^ F = ^ F d + ^ F Regularized Least Squares with Gaussian Interaction Profiles (RLS-GIP) [26] is similar to but simpler than NetLapRLS in terms of reg-ularization. For drugs, we can first compute a N d  X  N d matrix Q in which the ( i; j ) -element is exp(  X   X  Y i  X  Y j  X  2 ) , where is a parameter and Y i is the i -th row vector of Y . We then have matrix V d by a linear combination with S d : V d = t S d + (1  X  t ) Q least square classifier with simpler regularization leads to a direct and simpler solution for estimating F : Again the final result is obtained by averaging over ^ F d
RLS-GIP with Kronecker product kernel (RLS-GIP-K) [26] uses the regularized least square but incorporates the idea of PKM, i.e. a kernel over drug-target pairs, which is, in [26], a (Kronecker) product kernel from V d and V t , as follows: We then use the regularized least square, being the same as RLS-GIP, to estimate the final F as follows: where Y and F are vectors (with N d N t elements), corresponding to Y and F , respectively.

Kernelized Bayesian matrix factorization (KBMF2K) [9] has a similar idea to our method in the sense that drug and target similar-ities are both projected onto low-dimensional spaces with the same dimension so that they can reconstruct true drug-target interactions. More concretely, S d and S t are reduced into low-dimensional ma-trices G d and G t , respectively, so that Y  X  G d G T t . The entire scheme is a graphical model that makes G d and G t latent variables, which are estimated by a variational estimation algorithm. Graphi-cal models are likely to have unavoidable constraints. So drug and target similarity matrices cannot be weighted in KBMF2K.
We compared the performance of MSCMF with six latest com-peting methods, BLM, PKM, NetLapRLS, RLS-GIP, RLS-GIP-K and KBMF2K. In addition, as a reference, we checked the per-formance of two downgraded variations of MSCMF: 1) OCCF: MSCMF without any similarities, meaning weighted low-rank ap-proximation with Tikhonov regularization only, being equivalent to the formulation of one-class collaborative filtering [20] and 2) CMF: MSCMF with only one type of similarity, i.e. chemical structure similarity for drugs and genomic sequence similarity for targets. The evaluation was done by 5  X  10 -fold cross-validation (CV). That is, we repeated the following one CV five times: the entire dataset was randomly divided into ten folds, from which we repeated training by nine folds and testing for the rest, ten times, changing the test fold. The results were averaged over the total 50 ( = 5  X  10 ) runs. We considered three different types of pre-diction by randomly dividing 1) all drug-target interactions ( pair prediction ), 2) all drugs ( drug prediction ) and 3) all targets ( target prediction ). Note that RLS-GIP, RLS-GIP-K and OCCF cannot be applied to drug and target prediction, by which we compared four methods for drug and target prediction. We evaluated the perfor-mance by AUPR (Area Under the Precision-Recall curve) instead of a more standard measure, AUC (Area Under the ROC Curve), because AUPR punishes highly ranked false positives much more than AUC [5], this point being important practically since only highly ranked drug-target pairs in prediction will be biologically or chemically tested later in an usual drug discovery process, meaning that highly ranked false positives should be avoided.

MSCMF has five parameters, K , l ; d , t and ! . For each pair of training and test datasets in cross-validation, we selected parameter values, by using an usual manner of (10-fold) cross-validation: only a part (nine folds) of the training dataset for es-timating parameters of MSCMF and the rest (one fold) for evalua-tion. In this parameter value selection, we considered all combina-tions of the following values: { 50 ; 100 } for K , { 2  X  , { 2  X  3 ; 2  X  2 ; : : : ; 2 5 } for d and t , { 2 1 ; 2
We implemented PKM and BLM by using LIBSVM [3], where the regularization parameter of SVM was set at 1, according to [13] and [2]. To train PKM, the number of negative examples (interac-tions) was set at the same number of positive interactions, due to limitations of the main memory size. We implemented NetLapRLS exactly according to [28] and set its parameter values as specified in [28]. RLS-GIP and RLS-GIP-K were run by using the software originally developed in [26], where the parameter setting we used was also the same as those in [26]. Similarly KBMF2K was run by the software in [9] with the same parameter setting as that in [9].
Table 2 shows resultant AUPR (with p -values of paired t -test be-tween each method and the best method in the same column) which The p -values between the best method and each method are in parentheses. (a) Pair prediction ) 0.592 ( 6 : 68  X  10  X  42 ) 0.496 ( 2 : 07  X  10  X  54 ) ) 0.663 ( 1 : 20  X  10  X  42 ) 0.627 ( 2 : 86  X  10  X  45 ) ) 0.900 ( 2 : 25  X  10  X  20 ) 0.874 ( 1 : 46  X  10  X  18 ) ) 0.904 ( 1 : 21  X  10  X  18 ) 0.880 ( 7 : 43  X  10  X  15 ) ) 0.898 ( 2 : 00  X  10  X  20 ) 0.884 ( 1 : 20  X  10  X  10 ) ) 0.876 ( 3 : 00  X  10  X  22 ) 0.796 ( 1 : 34  X  10  X  41 ) ) 0.883 ( 5 : 03  X  10  X  25 ) 0.775 ( 8 : 96  X  10  X  42 ) ) 0.937 ( 4 : 76  X  10  X  1 ) 0.887 ( 2 : 51  X  10  X  13 ) (b) Drug prediction ) 0.167 ( 1 : 82  X  10  X  25 ) 0.092 ( 1 : 12  X  10  X  27 ) ) 0.328 ( 3 : 64  X  10  X  8 ) 0.254 ( 3 : 18  X  10  X  17 ) ) 0.343 ( 2 : 50  X  10  X  13 ) 0.298 ( 7 : 51  X  10  X  16 ) ) 0.296 ( 6 : 19  X  10  X  9 ) 0.253 ( 4 : 52  X  10  X  17 ) ) 0.342 ( 8 : 68  X  10  X  10 ) 0.326 ( 8 : 00  X  10  X  12 ) (c) Tar get prediction ) 0.641 ( 2 : 51  X  10  X  25 ) 0.611 ( 1 : 03  X  10  X  23 ) ) 0.659 ( 1 : 30  X  10  X  21 ) 0.587 ( 9 : 33  X  10  X  29 ) ) 0.762 ( 2 : 67  X  10  X  9 ) 0.787 ( 4 : 90  X  10  X  5 ) ) 0.725 ( 2 : 16  X  10  X  12 ) 0.607 ( 2 : 90  X  10  X  27 ) ) 0.785 ( 5 : 20  X  10  X  3 ) 0.795 ( 4 : 21  X  10  X  1 ) Table 3: A typical case of resultant similarity weights under pair prediction. (a) Similarities over drugs Similarities Nuclear receptor GPCR Ion channel Enzyme CS 0.6042 0.68 0.5804 0.5626
ATC 0.3958 0.32 0.4196 0.4374 (b) Similarities o ver targets Similarities Nuclear receptor GPCR Ion channel Enzyme GS 0 0.5297 0 0 GO (MF) 0.4409 0.1286 0.5262 0.3827 GO (BP) 0.5591 0 0.4738 0.3652
PPI 0 0.3417 0 0.2521 were all obtained by 5  X  10 -fold CV. For pair prediction, MSCMF outperformed all six competing methods, being statistically signif-icant in all cases. This directly indicates the clear performance advantage of our approach over existing state-of-the-art methods for predicting drug-target interactions. In addition, MSCMF com-pletely outperformed OCCF for all four datasets, being statistically significant, and achieved higher values than CMF, being statisti-cally significant for two datasets (GPCR and Enzyme). This re-sult implies that adding similarity matrices is generally useful but sometimes insignificant. For drug prediction, again MSCMF out-performed all four competing methods, all being statistically sig-nificant in all datasets. In addition, this case, MSCMF clearly out-performed CMF for all four datasets. This result shows that the scheme of MSCMF worked for prediction, and using more than one similarity matrices is useful for drug prediction. For target pre-diction, MSCMF outperformed all four competing methods, being statistically significant except three cases in the Nuclear Receptor dataset at the significant level of 0.01. On the other hand, CMF outperformed MSCMF in all four datasets, two cases being statis-tically significant. Thus we can say that the framework of CMF or MSCMF works for target prediction, while incorporating more similarity matrices might not be necessarily useful for improving the performance in this case.

Finally we checked resultant weights over similarity matrices of drugs and targets. Table 3 shows a typical set of resultant weights under pair prediction. In this table, for drugs, chemical structure similarity (CS) always had larger weights than ATC code similarity (ATC) for all four datasets, being consistent with the fact that chem-ical structure similarity is the most well-used similarity. On the other hand, for targets, interestingly, the most popular genomic se-quence similarity (GS) had weights of zero for three datasets (Nu-clear receptor, Ion channel and Enzyme), implying that this simi-larity might not work well for prediction. Instead, two GO-based similarities both achieved high values for the three datasets, im-plying that GO-based similarities were very useful and should be used more than genomic sequence similarity. However, genomic sequence similarity achieved the highest weight value for GPCR, which is in reality the most major target in drug discovery (more than 50% of all targets are GPCR), which might be the reason why genomic sequence similarity has been used so far.
We have presented a new formulation based on weighted low-rank approximation for predicting drug-target interactions. The key feature of our approach is to use multiple types of similarity matri-ces for both drugs and targets. In particular we stress that multiple similarity matrices are explicitly incorporated into our optimization formulation as regularization terms, by which our method can se-lect similarity matrices, which are the most useful for predicting drug-target interactions, by which the predictive performance can be improved. We have demonstrated the advantage of our proposed method by using both synthetic and real datasets. Synthetic data experiments have revealed the favorable selectivity on similarity matrices, and real data experiments have shown the high predic-tive performance over the six current state-of-the-art methods for predicting drug-target interactions.
This work has been partially supported by MEXT KAKENHI (24300054), ICR-KU International Short-term Exchange Program for Young Researchers, SRF for ROCS, SEM and National Natural Science Foundation of China (61170097). [1] M. Ashburner et al. Gene ontology: tool for the unification of [2] K. Bleakley and Y. Yamanishi. Supervised prediction of [3] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support [4] A. C. Cheng, R. G. Coleman, K. T. Smyth, Q. Cao, [5] J. Davis and M. Goadrich. The relationship between [6] I. S. Dhillon and S. Sra. Generalized nonnegative matrix [7] C. H. Q. Ding, T. Li, and M. I. Jordan. Convex and [8] C. M. Dobson. Chemical space and biology. Nature , [9] M. G X nen. Predicting drug-target interactions from chemical [10] Q. Gu, J. Zhou, and C. H. Q. Ding. Collaborative filtering: [11] A. L. Hopkins. Drug discovery: predicting promiscuity. [12] M. Iskar, G. Zeller, X. M. Zhao, V. van Noort, and P. Bork. [13] L. Jacob and J. P. Vert. Protein-ligand interaction prediction: [14] T. Klabunde. Chemogenomic approaches to drug discovery: [15] Y. Li, J. Hu, C. Zhai, and Y. Chen. Improving one-class [16] D. Lin. An information-theoretic definition of similarity. In [17] H. Ma, H. Yang, M. R. Lyu, and I. King. Sorec: social [18] N. Nagamine and Y. Sakakibara. Statistical prediction of [19] R. Pan and M. Scholz. Mind the gaps: weighting the [20] R. Pan, Y. Zhou, B. Cao, N. N. Liu, R. Lukose, M. Scholz, [21] R. Salakhutdinov and A. Mnih. Probabilistic matrix [22] E. W. Sayers et al. Database resources of the National Center [23] A. Skrbo, B. Begovic, and S. Skrbo. Classification of drugs [24] N. Srebro and T. Jaakkola. Weighted low-rank [25] C. Stark, B. J. Breitkreutz, T. Reguly, L. Boucher, [26] T. van Laarhoven, S. B. Nabuurs, and E. Marchiori. Gaussian [27] F. Wang, X. Wang, and T. Li. Semi-supervised multi-task [28] Z. Xia, L. Y. Wu, X. Zhou, and S. T. Wong. Semi-supervised [29] Y. Yamanishi, M. Araki, A. Gutteridge, W. Honda, and [30] J. Ye. Generalized low rank approximations of matrices. [31] G. Yu, F. Li, Y. Qin, X. Bo, Y. Wu, and S. Wang.

