 Researchers read academic papers related to their own research to acquire knowl-edge and/or cite them when they write their own papers. Doing so makes their research more sophisticated and objective. Here, we call researchers X  examina-tions of papers related to their work,  X  X cholarly surveys. X  Scholarly surveys are an important means for researchers, especially those who have recently changed their field or who do not have much experience, to get an understanding of their field of study. However, it is very difficult for researchers to read all the papers related to their research because their information processing ability is limited. Therefore, there is a need for automated techniques to make scholarly surveys easier.
 Progress in Web technologies has reached a point where we can now download and use many kinds of publications such as books and pictures electronically. The trend in digitization of publications is especially evident in academic jour-nals. Researchers used to conduct scholarly surveys by reading journals directly. However, since the advent of search engines that are customized to search aca-demic literature (called  X  X cademic search engines X  hereafter), they can retrieve and read papers on the Web. Popular academic search engines include Google Scholar 1 and Microsoft Academic Search 2 . After a user inputs a query (a sequence of keywords), the academic search engine searches for and displays a list of relevant papers. The search results are ranked according to the rel-evance to the input query, the number of citations of the paper, etc. Results deemed more relevant have a higher rank and are located at a higher position on the list. In addition, they include bibliographic information such as title, author, published year, and journal as a snippet, along with links to the papers. Figure 1 illustrates the user interface of a search engine results page of a conventional academic search engine.
 user as relevant, i.e., the targets of the survey, while other papers will be judged irrelevant. The user makes such judgments by checking the snippets of the search results. However, in some cases, the user cannot do so because the search results page has limited space to display snippets. To address this problem, we propose a method that considers the citing and cited relationship between academic papers.
 a user surveys one paper, he/she tends to survey the other paper. If we could visualize such a relationship and display it on the search results page, users could easily distinguish the papers relevant to their research from the non-relevant ones, and this would improve the efficiency of conducting scholarly surveys. Conventional search engines produce rankings according to the relevance to the query and display lists of search results; however, they do not explicitly consider the relationship between search-result papers.
 based on citing and cited relationships between papers into the results page of an academic search engine. Our method incorporates a citation graph, whose nodes represent papers in the search results or the papers that cite or are cited by them and whose edges represent citation relationships between them, and it displays this graph to users in an understandable fashion. Representing the cita-tion relationship as a graph allows users to understand the relationship between papers intuitively. Citation graphs may help newcomers to a research field to find papers useful for their initial scholarly surveys. Also, they may help experts of their fields find recently published papers easily. To the best of our knowledge, no other study has proposed a method of incorporating a graph that aggregates search results into an academic search engine X  X  results pages. The contributions in this paper are twofold: (1) We describe the idea and merits of incorporating a graph based on citation relationships into the search results page of an academic search engine. We also propose a method of doing so and implemented it. (2) We describe the results of a user study examining the effectiveness of the proposed method.
 The remainder of this paper is organized as follows. Section 2 introduces pre-vious work related to this research. Section 3 describes the features of scholarly surveys using academic search engines and the merits of incorporating citation relationships into the interface of an academic search engine. Section 4 explains the method of incorporating a citation graph into the search results page of an academic search engine and the interface of the academic search engine we imple-mented. Section 5 describes the content and results of the experiment performed to examine the effectiveness of the proposed method. Section 6 concludes this paper and describes future work. An academic search engine is a search engine that has been customized to search academic literature; representatives are Google Scholar [ 1 ], Microsoft Academic Search, and CiteSeerX 3 . Beel and Gipp [ 2 , 3 ] reported that popular academic search engines rank their search results based on the relevance between the query and the retrieved paper, number of citations of the paper, and names and reputations of the authors and the journals. Verberne et al. [ 4 ]proposeda method of suggesting additional terms to specify the topic when reformulating the input query.
 Our research visualizes the relationship between academic papers by using network graphs [ 5 ]. Nanba et al. [ 6 ] proposed a system that visualizes the state-ments on why the citing paper cites the cited paper along with the citing and cited relationships between papers and displays them to users. Here, Microsoft Academic Search introduced a similar function called the  X  X itation Graph. X  It displays a set of papers that cite the target paper appearing in the search results as a graph. He et al. [ 7 ] extracted and visualized the time-series process of the evolution of a research field by analyzing the abstracts and citation information of papers of the field. Dunne et al. [ 8 ] proposed a system that visualizes a sum-mary of collected papers by using the citations and automatic summarization to help researchers explore papers rapidly. The goal of the above research is to support browsing of papers, whereas our goal is to apply a graphical method to the results pages.
 visualizing the search results understandably. One of its visualization methods improves the interface of snippets.  X  X nippets X  in Web search engines are sum-mary sentences displayed on the search results page. Users check each snippet and decide whether they should follow the link to the corresponding Web page or not. Some researchers have attempted to improve the content of summary sentences in snippets [ 9 ], while others have surveyed the influence of snippets on user behavior [ 10 ]. These researches are relevant to ours in that they discuss the information contained in search engine results pages. Apart from these, other studies have proposed methods of displaying clustered search results to users [ 11 , 12 ]. Scaiella et al. [ 13 ] categorized the set of search results obtained from a Web search engine by using the categories of Wikipedia and displayed them with labels (the representative name of each cluster). Mirylenka and Passerini [ 14 ] proposed a method of organizing papers in the search results of an academic search engine and displaying them as a graph whose node represents a research category. These researches are relevant to ours in terms of their organizing the search results of an academic search engine; however, they are different in that their goal is to extract research topics by clustering papers in search results, while ours is to support users X  judgment of the relevance of search results. In this section, we describe the characteristics of scholarly surveys using acad-emic search engines and summarize their search process and problems. Then we explain the advantages of incorporating citing and cited relationships into the results pages. 3.1 Characteristics of Scholarly Surveys Using Academic Search Current scholarly surveys are usually performed using an academic search engine. Different from general Web searches, we think that scholarly surveys using an academic search engine have the following characteristics: (1) They need to retrieve various relevant documents. (2) They can determine the papers that they should survey next with reference With regard to (1), some researchers have claimed that recall is an important factor in a scholarly survey [ 15 ]. For example, in a normal Web search, when users want to know tomorrow X  X  weather, they are usually satisfied after checking only a few search results. On the other hand, a scholarly survey needs to search and check a large number of papers ranging from famous papers in the field to the papers relevant to the user X  X  own research because he/she needs to acquire enough knowledge about the field. With regard to (2), we think that two papers having a citing and cited relationship may be similar in content to each other. This allows users of an academic search engine to select the papers that they should survey next from the papers that cite or are cited by the relevant paper. In this way, they can efficiently survey the studies related to their own research. 3.2 Problems in the Process of Scholarly Surveys on Academic Search Engines The user looks for the information they need by inputting a sequence of words, i.e., a query, and checks the snippets of the corresponding search results from the top of the list to the bottom. When a user judges a paper to be related to his or her own research, they follow the link to it and read it. They repeat the process until they are satisfied with the results of the survey, or else they reformulate the input query because the search results are not relevant to their information needs.
 However, search results pages have limited space for displaying snippets, and thus, they may not be able to show enough information for users to be able to make good judgments on the relevance of the papers. Therefore, users may miss the papers that they should survey or conduct unnecessary surveys because of misjudgments they make about the relevance of the search results. 3.3 Incorporating Citation into Search Engine Results Pages To address the problem mentioned in Sect. 3.2 , It is possible to exploit the idea of relevance feedback. For example, we can devise a system that reranks the search results when users notify the search engine that the paper is relevant to their information needs. However, this method sometimes forces users to provide feedback, which may be a burden.
 As mentioned in Sect. 3.1 , with regard to two papers that have a citing and cited relationship, if one is a target of the users X  survey, the other also tends to be a target. We thought the method of aggregating citing and cited relationships whose heads or tails are papers in the search results (called  X  X roximate citing and cited relationships X  after this) is effective and considered displaying it to users understandably. This method allows users to grasp the relationships between papers in the search results and distinguish the ones relevant to their needs before they follow the links to them. If we suppose there is a search-result paper Z that is relevant to the users X  needs and they can see that Z and X have a citing and cited relationship, they can recognize X as a target and not overlook it. Likewise, if users can see that Z and Y have no citing and cited relationship, they perhaps can exclude Y from the targets of the survey. An example of this process is illustrated in Fig. 2 . In this way, displaying proximate citing and cited relationships can help users make relevance judgments and improve the efficiency of scholarly surveys. In this section, we describe the method of incorporating a graph based on cit-ing and cited relationships into the search results page of an academic search engine. First, we consider what kind of information should be included in the citation graph. Then we explain the features and functions of the graph we have implemented. 4.1 Visualization Using a Citation Graph One possible method of visualizing proximate citing and cited relationships is to incorporate a list of the citing papers and the cited papers into the snippet of each search result. However, this method makes it difficult for users to understand the citing and cited relationship because it requires them to check the citing papers and the cited papers for each search result. Instead, we propose a more understandable visualization that works by summarizing the citing and cited relationships between these papers as a graph whose nodes represent individual papers and whose edges represent citing and cited relationships.
 if we incorporate all the citing and cited relationships into a graph, the amount of displayed information would be excessive. This causes various problems in that it takes up too much space, imposes a heavy cognitive load, etc. Thus, we categorize citing and cited relationships from the viewpoint of whether they support the users X  judgments of relevance and consider what type of relationship should be displayed to them. We propose three types of relationship: (1) Direct citing and cited relationships between papers in the search results. (2) Citing and cited relationships between papers in the search results via a paper not in the search results. (3) Citing and cited relationship except for (1), (2) (via more than one paper not in the search results).
 Figure 3 summarizes this categorization. With regard to type (1), two papers in the search results that have a citing and cited relationship are likely to be similar in content. Displaying such relationships is important for supporting users X  relevance judgments. Next, type (2) means that papers A or B in the search results cite or are cited by paper C, which is in or not in the search results. We consider that although this type of relationship is less important than type (1), it is still important. In addition, papers in search results tend to be similar in content to the papers that cite or are cited by them. This tendency implies that such papers can be new targets of the users X  surveys. For these reasons, it is important for users to display this type of relationship. Finally, type (3) has a problem in that it is much less important than type (2), and too many nodes and edges satisfy this rule.
 The above consideration led us to include only types (1) and (2); that is, we excluded type (3). 4.2 Features and Functions of Citation Graph In this section, we explain our implementation of the interface with citation graph. The interface receives query inputs by users and makes retrievals from the literature databases. It displays the graph based on the criteria in Sect. 4.1 , along with a list of papers that are ranked based on their citation count, rele-vance to the query, and other indicators. Figure 4 shows examples of the citation graph obtained by the proposed method. We create the list by using Microsoft Academic Search and display the top ten search results on the first page. In addition, this interface has links to other search results pages at the bottom of the page. For example, if users want to check the page that contains the 11th to the 20th search results, they can click link  X 2. X  Each snippet contains the title of the paper, a link to the page including the details of the paper in Microsoft Academic Search, and summary sentences of the paper.
 Determination of the Size of a Node According to Its Importance.
 Changing the size of a node displayed in a graph can highlight the paper cor-responding to the node. Hence, it is likely that changing the size according to the importance of the paper will help users to pay more attention to impor-tant papers. Moreover, although there are various indicators that represent the importance of papers such as the journals that published the paper, the authors of the paper, etc., one of the most important is the citation count (i.e., the number of citations) [ 16 ]. Thus, in our method, if the paper has a large citation count, the size of the corresponding node becomes large, and vice versa. This function allows the interface to express the importance of a single paper on the graph as well as the relationship between papers. For example, it helps users to understand that the relationship between two papers must be important because the individual papers are likely to be important.
 paper in the search results is less than a threshold, we decided not to display the node corresponding to that paper and the surrounding edges. We suppose the threshold is given by users.
 Arrangement of Nodes. If the nodes and edges of the citation graph are disordered, users looking at the interface of the graph may experience significant cognitive load. Here, arranging the nodes in the citation graph on the basis of time would be a familiar way to relieve the load because the cited paper always precedes the citing paper. Hence, we decided to position the nodes according to the published year. For example, the node of a paper published in 1990 is located on the left of another paper published in 2000. This arrangement eases the users X  cognitive load by keeping the graph in order. Apart from this, arranging the papers as a time series allows users to understand the developmental history of their research topic. It enables them to improve their understanding of their research field. Here, we explain the user study examining the effectiveness of the proposed method. We asked actual users of an academic search engine to participate the experiment, and to conduct scholarly surveys using the interface that we imple-mented on the existing academic search engine (Microsoft Academic Search) and using the same engine but without our interface. Figure 5 shows the interfaces used in the experiment. The two interfaces were as follows: (a) Academic search engine with no additional information
This interface was the baseline in this experiment; it was a conventional academic search engine. (b) Academic search engine with a graph based on citing and cited relationship
This was the interface based on the method described in Sect. 4 . 5.1 Procedure We asked nine students in our laboratory, eight graduate students and an under-graduate student, to participate in the experiment. They had their own research fields related to data mining or text processing and surveyed related papers if needed. The procedure of this experiment is listed below: (1) We divided the participants into three groups so that each group had three (2) A participant was selected from each group and asked to prepare a research (3) Before the surveys, all of the participants were asked to use the two interfaces (4) All the participants in group 1 were asked to complete their surveys accord-(5) Using the same procedure as (4), all participants in group 1 were asked to (6) After the participants had performed each survey, they were asked to answer (7) The participants in groups 2 and 3 were also asked to survey the research 5.2 Results Efficiency of Survey. First, we show the results of the study related to the efficiency of surveys obtained from the log data. Table 1 shows the average num-ber of papers judged to be  X  X elevant X  or  X  X artially relevant X  by the participants. Numbers in parentheses represent standard deviations. The results suggest that the participants using interface (b) judged more papers to be relevant compared with those using interface (a), excluding the surveys of topic 3. This indicates that the relationships displayed in the citation graph helped them choose rel-evant search results. The surveys of topic 3 with interface (b) obtained fewer relevant papers because the scope of the topic was narrow, and the participants could not find relevant papers easily.
 Next, we describe the ratio of papers judged to be relevant by the participants to all papers whose links they followed and judged the relevance of during the surveys. This indicator represents how many unnecessary surveys the proposed method could reduce, which is one of the problems of scholarly surveys men-tioned in Sect. 3.2 . We can also consider it as a precision based on the papers the participants read. The average numbers of all papers that a participant judged the relevance (followed the link) per topic in the survey were about 20. Table 2 shows the average ratio of papers judged to be  X  X elevant X  or  X  X artially relevant X  to all papers of which he/she judged the relevance. Numbers in paren-theses represent standard deviations. We can observe that the participants using interface (b) could find a higher ratio of papers relevant to their needs compared with the participants using interface (a), excluding the surveys of topic 3. The reason is that displaying a graph enabled them to understand the relationship between the papers in the search results easily and thus made it easier to find only the relevant papers from a large number of papers.
 Subjective Preferences. Next, we briefly review the participants X  subjective preferences from the results of the post-survey and exit questionnaires for each interface. We asked the participants about the usability of the interfaces, the cognitive load when they used the interface, the confidence on the relevance judgments before they read a paper, and so on. The results indicated that inter-face (b) was better than interface (a) as the confidence in the relevance judgments was higher with reasonable cognitive load.
 Also, we got some comments which reveal that the citation graph improved the satisfaction and usability of the interface. On the other hand, some partici-pants pointed out the possibility that the citation graph had a bad influence on the interface when the research topic of the survey was limited to a narrow one or they had little knowledge on the topic. We proposed a method of supporting researchers X  scholarly surveys using acad-emic search engines. This method extracts citing and cited relationships between papers and incorporates them into the search results pages in an intuitive fash-ion. We explained the implementation of an interface based on the proposed method and described a user study in which actual users simulated scholarly surveys.
 There is still a need to consider how best to deal with citing and cited rela-tionships as there are some patterns on the relationships. We also need to refine the definition of scholarly surveys so that they can express the users X  information needs more clearly, conduct another user study that includes more participants and research topics, and analyze the results deeply.

