 ORIGINAL PAPER Partha Pratim Roy  X  Umapada Pal  X  Josep Llad X s Abstract In graphical documents (e.g., maps, engineering drawings), artistic documents etc., the text lines are anno-tated in multiple orientations or curvilinear way to illustrate different locations or symbols. For the optical character rec-ognition of such documents, individual text lines from the documents need to be extracted. In this paper, we propose a novel method to segment such text lines and the method is based on the foreground and background information of the text components. To effectively utilize the background infor-mation, a water reservoir concept is used here. In the pro-posed scheme, at first, individual components are detected and grouped into character clusters in a hierarchical way using size and positional information. Next, the clusters are extended in two extreme sides to determine potential candi-date regions. Finally, with the help of these candidate regions, individual lines are extracted. The experimental results are presented on different datasets of graphical documents, cam-era-based warped documents, noisy images containing seals, etc. The results demonstrate that our approach is robust and invariant to size and orientation of the text lines present in the document.
 Keywords Multi-oriented text line segmentation  X  Artistic documents  X  Graphical document analysis  X  Foreground-background information 1 Introduction In optical character recognition (OCR), the text lines in a document must be segmented before recognition. When the text lines in a document image are parallel to one another (sin-gle oriented documents) simple techniques like the projec-tion profile, component nearest neighbor clustering method [ 1 , 2 ] etc. are good enough to segment individual lines. But there are many documents where text lines may be printed in several orientations (multi-oriented documents) or the text lines may be curved in shape. Examples of some such doc-uments are shown in Fig. 1 . This paper deals with a novel technique to extract (segment) individual text lines from such multi-oriented or curved documents.

There are many techniques to extract text lines from single-oriented documents [ 3 ], but the published works on extraction of multi-oriented and curved text lines are few [ 20 , 21 ]. Li et al. [ 4 ] proposed an approach for handwritten textline segmentation using level sets. Here, Gaussian filter-ing is used to estimate the probability density function (PDF) of pixel values, and then level sets are initialized on these high PDF values. Growing and merging of level sets is then per-formed iteratively. Goto and Aso [ 5 ] proposed a local linear-ity based method to detect text lines in English and Chinese documents. In the method proposed by Hones and Litcher [ 6 ], line anchors are first found in the document image and then text lines are generated by expanding the line anchors. These methods cannot handle variable sized text, which is the main drawback of the methods. Loo and Tan [ 7 ] proposed a method using irregular pyramids for text line segmentation. This algorithm uses the inclusion of background information, concept of  X  X loseness X , density of a word region, majority  X  X in X  strategy, and the directional uniformity and continuity among words in a sentence. Recently, Bukhari et al. [ 8 ]pro-posed a line segmentation approach for camera-based warped documents using active contour models. The segmentation technique uses several active contour models (baby snakes) and their convergence. Gatos et al. [ 9 ] proposed an algorithm based on text line and word detection for warped documents. The binary document is de-warped based on word rotation and translation according to upper and lower word baselines. Bai et al. [ 10 ] used a traditional perceptual grouping-based algorithm for extracting curved line from logos and slogans. Chains of connected components are extended to the left and the right according to the local orientations of the text lines. Pal and Roy [ 11 ] proposed a head-line based technique for multi-oriented and curved text lines extraction from Indian documents containing Bangla and Devnagari scripts. Since the head-line feature is missing in English, this method can not be used for English line extraction. In other work, Pal et al. [ 12 ] developed a system for English multi-oriented text line extraction estimating the equation of the text line from the character information. Main drawback of this method is that, it will not work for documents of curved text lines.
In graphical documents, however, text characters do not appear isolated. These documents usually contain many graphical lines besides text characters information and the text often touch/overlap with these long graphical lines. Detection of such text characters is difficult even there exist many text/graphics separation algorithms in literature [ 13 , 14 ]. Text line segmentation techniques may provide use-ful information in this point of view. If some of the characters in a text line are missing, the remaining portion of text line (well-extracted characters) can direct us to locate the miss-ing text characters. Thus, more emphasis can be provided to segment the touching/missing character.

Extraction of individual text lines from artistic or tech-nical documents having multi-oriented or curved text is a difficult problem. In graphical documents, the components of text lines can be of different sizes and font styles. Again, the inter-character distance varies time to time for annota-tion purpose. For example, in maps shown in Fig. 1 c, the inter-character distance in the word  X  X CEAN X  is different for annotation and it has different orientation in different location to annotate the graphical lines.

In this paper, we propose a line extraction technique from such artistic/technical documents. To handle documents of wide variations in terms of size, font, orientation, layout, etc., the proposed technique is based on the foreground and background information of the characters in a text line. Most researchers focus their attention only on foreground com-ponents and discard the background part (background part refers to those white areas of a binary image surrounding the actual black foreground). Here, we show how the background information can be utilized for multi-oriented line extraction.
The proposed approach is based on perceptual properties of text components. Text is a sequence of cavities following a regular distribution in a smooth path. These cavities/back-ground information play an important role in our proposed method and guide our algorithm to extract individual lines from the documents containing multi-oriented and curved text lines. We use the background portion obtained between two consecutive characters of a line. To get this background portion, we apply the water reservoir concept. Water reservoir is a metaphor to illustrate the cavity region of a component [ 15 ]. The use of background information and its computa-tion through the water reservoir concept guides our proposed algorithm to segment text lines. In the proposed scheme, at first, individual components are detected and grouped into initial character clusters using their inter-component dis-tance, size, and positional information. Merging these initial character clusters, a proximity graph is created to have larger clusters. Using inter-character background information, ori-entations of the extreme characters of a larger cluster are determined, and based on these orientations, two candidate regions are formed from the cluster. Finally, with the help of these candidate regions, individual lines are extracted. The organization of the rest of the paper is as follows. A brief discussion on water reservoir concept used for line extraction is given in Sect. 2 . The line extraction procedure is detailed in Sect. 3 . We demonstrate our proposed algorithm on a variety of datasets including graphical documents and camera-based documents in Sect. 4 . Conclusions and future work are presented in Sect. 5 . 2 Water reservoir concept The water reservoir principle [ 15 ]isasfollows.Ifwateris poured from a side of a component, the cavity regions of the background portion of the component where water will be stored are considered as reservoirs of the component. Details of water reservoir and its different properties can be obtained in [ 15 ]. However, here, we give a short description of different properties used in our proposed scheme to ease readability.
Top (bottom) reservoir: By top (bottom) reservoirs of a component, we mean the reservoirs obtained when water is poured from the top (bottom) of the component. A bottom reservoir of a component is visualized as a top reservoir when water is poured from top after rotating the component by 180  X  .

Left (right) reservoir: If water is poured from the left (right) side of a component, the cavity regions of the compo-nent where water will be stored are considered as left (right) reservoirs. A left (right) reservoir of a component is visual-ized as a top reservoir when water is poured from the top after rotating the component by 90  X  clockwise (anti-clockwise).
Water reservoir area: The area of a reservoir is defined by the area of the cavity region where water will be stored. The number of points (pixels) inside a reservoir is computed, and this number is considered as the area of the reservoir.
Water flow level: The level from which water overflows from a reservoir is called the water flow level of the reservoir (see Fig. 2 ).

Reservoir surface width: The width of the reservoir at the flow level of the reservoir is the reservoir surface width. In Fig. 2 , AC is the surface width of the reservoir.

Mid-point of water flow surface: This is defined as the mid-point of the reservoir surface width. In Fig. 2 , M is the mid-point of the water flow surface.

Height of a reservoir: By height of a reservoir, we mean the depth of water in the reservoir.

These background region-based features obtained using the water reservoir concept help our line extraction scheme. 3 Text line extraction An English text line can be divided into 3 zones: Upper zone, Middle zone or Busy zone, and Lower zone. The Busy zone for an English text line is the zone between the mean line and the base line as shown in Fig. 3 a. Busy zone is shown for curve line in Fig. 3 b. This height information is important for the line extraction algorithm. It is used in our approach to determine the search zone (explained later) for the multi-oriented text line extraction.

In our line-detection algorithm, we assumed that, the inter-character distance in a word of a line is smaller than the dis-tance of this line from its neighboring lines. The proposed line detection process is divided into 4 steps. These steps are (a) initial character clustering, (b) grouping of character clusters to form larger cluster, (c) candidate point selection of large clusters, and (d) extension of large clusters for line extraction. The flow chart of text line segmentation is given in Fig. 4 .The documents are digitized in gray tone with 300 dpi. We apply Otsu binarization [ 16 ] method to convert a gray document into two-tone image, and component labeling is performed on this binary image to detect individual components. The proposed scheme uses these connected components to extract the separate text lines. The main part of the line segmentation approach is illustrated within the dashed rectangle. In the fol-lowing sections, we detail each step. 3.1 Initial character clustering Our approach of line segmentation is similar to perceptual grouping. Text line is a sequence of similar sized character components following a regular distribution in a smooth path. The grouping of characters in text line is done in hierarchi-cal ways in our approach. After detecting the text characters by connected component labeling, we group the neighbor characters in small clusters using the size and local linearity. Neighbor characters are selected using boundary-growing algorithm that is performed as follows.

The external contour points of each character compo-nent are detected by a contour tracing algorithm. These contour points are expanded outwards iteratively by one pixel (8-pixel neighbor configuration) for boundary growing. Number of iteration to find nearest components is decided considering the size of the characters of the word. It is decided as Q = q  X  S c . The multiple factor q is selected based on the type of dataset. We considered different values of q (e.g., 2,3,...,8) in the experiment of different datasets to set q. In text documents, q is set to 4. In graphical documents (e.g., maps) where text characters are very sparse, this value is set to 6. These values are set based on the experiment. If after Q iterations of boundary growing, a character component does not touch other components, then no neighbor is found for the character.

Next, the size (S c ) of individual components (characters) is computed for character clustering. The size S c is calcu-lated by finding the radius of a minimum enclosing circle of the component. For each component (say, D 1 ) , we find its two nearest components using a boundary-growing algo-rithm (discussed later). Let the two nearest components of D be D 2 and D 3 .Alsolet,c 1 , c 2 and c 3 be the centers of mini-mum enclosing circle (MEC) of the components D 1 , D 2 and D a valid 3-character cluster if they satisfy both the size simi-larity and the local linearity. These are described as follows. (a) Size similarity: the size similarity is tested as follows. (b) Local linearity: Local linearity (orientation) is tested
Initial 3-character clustering of the image Fig. 5 aisshown in Fig. 5 b. Each of these 3-character clusters is marked by circular ring. From the figure, it is to be noted that, there is no cluster among the components (o-p-e) and (r-l-i) because they fail to satisfy the angular criteria. The clusters (r-s-C), (s-C-r), and (n-g-R) are not formed because the inter-char-acter spacing is large. 3.2 Grouping of initial clusters From the initial clustering, several 3-character clusters are obtained, which will be grouped together by iterative grow-ing. From the second text line of Fig. 5 b, we have different 3-character clusters such as (D-a-r), (a-r-l), (l-i-n), and (i-n-g). A graph analysis is performed to grow these clusters. A graph (G) described below is used here with all the components of different clusters, where components of all the clusters are considered as nodes. An edge between two component nodes is added if they are from the same initial cluster.
Formally, a graph G = (V, E) is created for growing these small 3-character clusters. Here, V = component and E = edges between components. Let there be T s components in the document, thus, | V |= T s . An edge e ij in E is formed by a component pair (C i , C j ) if components C i and C j neighbors.

Sometimes, by adding edges in such a way, a node may have 3 or more edges. This situation occurs when two or more text lines cross each other or they are very close. The nodes having 3 or more edges are not considered in this step. For illustration, see Fig. 6 . Here, the cluster including the charac-ter  X  X  X  of the word  X  X lgorithm X  has two neighbors  X  X  X  and  X  X  X  from this word. Similarly, the character  X  X  X  of word  X  X naly-sis X  has neighbors  X  X  X  from this word and  X  X  X  from the word  X  X lgorithm X . After grouping into initial clusters,  X  X  X  will have 3 edges that are connected to its 3 neighbors ( X  X -t X ,  X  X -m X , and  X  X -s X ). This node  X  X  X  is marked for removal and is not considered for initial cluster grouping, since it may generate erroneous results in line extraction.

The angle of each node (of degree 2) is calculated with respect to two connected nodes. If the angle is less than T this node is also marked for removal. For example, in Fig. 5 c, the component  X  X  X  of (D,a,r,l,i,n,g) does not satisfy the angle criterion with its neighbor components  X  X  X  and  X  X  X .
Thus, if for a node, either number of edges is greater than 2 or its angle with neighbor characters is less than T ang the node is removed and all the edges of the correspond-ing node are deleted from G. Thus local linearity criterion between each of 3 character clusters is validated. Because of removal of some edges from the graph, G will be split into sub-graphs or a set of components. In each of these sets, we will have a chain of components that are linear in fashion. In other words, paths of characters are searched in the graph according to piece-wise linearity criterion. Each sub-graph is considered as a large cluster.
 These large clusters represent different parts of text lines. These clusters are grown to merge other clusters in the same text line. Since, the clusters in the same line can be in different orientations, we use the cluster growing in local curvilinear direction. The continuation path of each cluster is followed by extending two extreme sides of the cluster. Hence, the orientation of the extension is computed from the extreme characters of the cluster group. For this extension purpose, we select candidate points from both ends of each cluster to follow the orientation. The selection of candidate points and the extension of the cluster are described in the following. 3.3 Candidate point selection Using inter-character background information, orientations of the extreme characters of a cluster group are decided, and based on these orientations, two candidate regions are formed from the cluster. For each cluster group, we find one pair of characters from both of the two extreme sides of the cluster. Let,  X  X G X  and  X  X N X  be two such pairs of extreme characters of a cluster group and these pairs are shown in the 1st column of Fig. 7 . To find background information, the water reservoir concept is used. To do so, first convex hull of each character is found (a convex hull is shown in the 2nd column of Fig. 7 ) and this is done to fill up the cavity regions of the character, if any. Next, the resultant components are joined (shown in 3rd column of Fig. 7 ) by a straight line through their centers of MEC (minimum enclosing circle). This joining is performed to make the character pair into a single component to find the water reservoir in the background part between them. It may be noted that, the line obtained by joining the centers of the MEC of two characters does not show the proper ori-entation of the characters always (e.g., when one character has ascender/descender parts and the other does not have ascender/descender parts) whereas water reservoir concept can take care such characters properly. Because of this, the water reservoir concept has been used here.
Subsequently, the water reservoir area of this joined char-acter is computed in 8 directions at 45  X  intervals as shown in Fig. 8 . Computation of the water reservoir when water is poured in the N 7 ( N 3 ) direction is equivalent to the computa-tion of top (bottom) reservoir. The areas of water reservoirs in direction N 3 and its opposite direction N 7 are added to get the total background area with respect to the orienta-tion N 3 N 7 . This is done for other orientations N 1 N 5 and N 4 N 8 . The orientation, in which the maximum area is found, is detected and water flow-lines of the corresponding reservoirs are computed. The mid-points of the water flow-lines (discussed earlier in Sect. 2 ) of the two reservoirs are the candidate points. For illustration, see the last four col-umns of Fig. 7 , where water reservoirs in four orientations (N ure, it can be noted that, the maximum reservoir area is found for the pair  X  X G X  in N 3 N 7 (top X  X ottom) directions whereas for the pair  X  X N X , the maximum reservoir area is obtained in the N 1 N 5 (left X  X ight) direction. This is because of the dif-ferent orientations of these two extreme pairs of the cluster  X  X NTRODUCING X .
 For a pair, we get two candidate points and an orientation. The candidate points obtained from the character pairs  X  X G X  and  X  X N X  are shown in Fig. 9 . The orientation is obtained from the directions in which we get a maximum reservoir. For example, if we get a maximum reservoir area in the N 3 and N 7 directions, then N 3 N 7 is the orientation. This orien-tation represents the direction of the extreme characters of a cluster, and it helps us to extend the cluster group for text line extraction. Generally, cluster groups should be extended in perpendicular to this orientation to obtain its neighboring clusters of a text line. We also compute the distance between two candidate points, and this distance gives the height infor-mation of the busy zone of the text line. The number of water reservoir direction is tested with different values (4, 8, and 12) of discrete angles in the experiment. If we use more direc-tion for water reservoir than 8 directions, then computation time will be more. From our experiment, we noted that water reservoir with 8 directions solves our purpose. 3.4 Extension of cluster group For each extreme character pair of a cluster group, we can know its candidate points and orientation. Let two candidate points of one extreme side of a cluster be M 1 and M 2 and its orientation be D. The distance between M 1 M 2 is noted, and this distance is called height info ( HI ). We find a line that is perpendicular to D and passing through the mid-point of M 1 M 2 . Let this line be XY and we call it the estimated line of extension. Note that, this line is detected based on the information of the extreme characters of a cluster, and as a result, our line extraction scheme gives good results. Now, the line XY is extended in an outward direction until it reaches the bounding box of the cluster group. The point where the extended line meets the bounding box is noted, and we call it a key point. This is done for the other parts of the other extreme sides of the cluster, and we detect another key point. So, for a cluster, we have two key points. Key points of the cluster  X  X NTRODUCING X  are shown in Fig. 10 c and marked by  X  X  X .

Now, for each extreme side of a cluster, we have the fol-lowing: (a) estimated equation of the line of extension and (b) a key point (K). For cluster extension to get individual line, we generate a candidate region for each key point of a cluster. Candidate region generation is performed as follows.
Generation of candidate regions: Estimated equation of line of extension is extended up to a distance TR = q  X  HI in the outwards direction from the key point K. The path obtained during this extension is called as extended candi-date path of K of the cluster. The value of q is discussed in Sect. 3.1 . A rectangular mask of length q  X  HI and width HI is placed on the extended candidate path in such a way that the mask will be divided into two halves by the extended candidate path. This region of the rectangular mask is con-sidered as the candidate region of the cluster for the key point K. In a similar way, the candidate region of the cluster for the other key point is computed. Candidate regions for two key points of a cluster  X  X NTRODUCING X  are shown in Fig. 10 c, and they are marked by hatched line.
 The reason to choose a TR equal to q  X  HI is as follows. In printed text, HI generally represents the busy-zone height of a text line. So, if we assume TR as q  X  HI ,itismost likely that a part of the neighboring cluster will fall in one of the candidate regions of the current cluster.

Line extraction: Candidate regions are used for line extrac-tion, and line extraction is performed as follows. Let  X  be the set of candidate clusters and isolated characters of an input image. We use a bottom-up approach for line extraction and the approach is as follows. First, an arbitrary cluster (say, topmost left cluster) is chosen from  X  , and a line-group L is formed using this cluster. Two candidate regions are detected from this cluster. For each line-group, we maintain two candi-date regions ( CR ): left and right CR . Next, we check whether there exists any extreme character of a cluster or individual component whose portion falls in these CR s of the line-group L . Suppose, from  X  , we get a cluster (say, Nc) of which an end character falls in CR .Ifthe HI of that end character of Nc and that of corresponding CR is similar and their corre-sponding orientation are also similar, then we include it in L . The candidate regions of the cluster Nc are noted and the CR s of L are modified based on the new cluster Nc. The CR sare modified as follows. If the right (left) candidate region of Nc falls in the left (right) CR of L then the left (right) candidate region of Nc is assigned as left (right) CR of L . Sometimes a portion of some isolated characters of  X  may also fall in the CR s. We consider such isolated characters for inclusion in L . If a portion of an isolated component falls in any CR of L , the component will be selected to join the line-group L , if the size similarity and linearity condition discussed in Sect. 3.1 are satisfied. If an isolated character is included in L , then the corresponding CR is updated considering the included character as the extreme character of the line-group. The extension of this line-group continues at both sides, till it does not find any component/cluster in any CR , or it reaches the border of the image.

When several clusters are found in the candidate region, we check size and orientation similarity for each cluster with that of candidate region of L . The cluster having best match-ing in terms of size and orientation is selected to be included. To decide the best matching cluster, at first, the cluster having minimum height difference is chosen. If the chosen cluster satisfies the local linearity properly with that of L then the chosen cluster is considered as best matching cluster. Algo-rithmic steps of the extension of clustering group of the pro-posed scheme are given in Algorithm 1.

Components clustered into a single line-group are the members of a single text line. To get other text lines, we fol-low the same steps and finally we get T number of line-groups if there are T text lines in a document. Finally, the punctu-ation symbols/small components that were left before are included into the nearest text lines according to the proxim-ity. In pre-processing of line extraction, small symbol compo-nents ( X . X ,  X , X  etc.) and background noise are filtered out based on their aspect ratio and pixel density. To include these punc-tuation symbols/small components in the line-group of their respective text lines, we use a boundary-growing technique. A punctuation symbol/small component is grown along its border until it touches any component of a line. A compo-nent is included to the text line to which it touches first dur-ing its boundary growing. Different results of the process are described in Fig. 11 .
 Algorithm 1: Extension of cluster group (G c ).
 Step 1: Let M 1 and M 2 be the candidate points for one extreme side of a cluster (G c ) and D be the orientation of line obtained by joining M 1 and M 2 .Let HI be the distance between M 1 and M 2 . An equation XY is estimated perpen-dicular to D and passing through the mid-point of M 1 M 2 Step 2: The intersection point of XY and bounding box of G are detected to find the key point (K) as shown in Fig. 10 c. Step 3: XY is extended up to length TR = q  X  HI in the outwards direction from the key point K; q is the multiplying factor of neighbor search area to get candidate region. Step 4: A candidate region ( CR ) of rectangular size ( TR HI ) is considered at K as shown in Fig. 10 c.
 Step 5: If extreme character of another cluster falls in CR , we check the height and orientation information of the new cluster with respect to CR . If their sizes are similar and orien-tation of the candidate points of the new cluster is also similar to that of orientation D of G c , then the cluster is included with G . When several clusters fall in the CR , the best matching cluster is considered for inclusion. Instead of a cluster, if an isolated character falls in CR , then it will be included in G the character has similar size and local linearity with respect to G c . Update G c to get new M 1 , M 2 and D from the resultant cluster. If no cluster is found to include with G c then  X  X xit X . Else,gotoStep1. 4 Experimental results: different case studies For the experimentation of the present work, we considered real data from maps, newspapers, magazines etc. as well as synthetic data generated through the computer. The scanned text images that are obtained in gray scale are transformed into binary images. We have used a histogram-based global binarization algorithm [ 16 ] to convert the data image into two-tone (0 and 1) images. (Here,  X 1 X  represents object point and  X 0 X  represents background point.) The digitized image may contain spurious noise points, small break points, and irregularities on the boundary of the characters, leading to undesired effects on the system. Since our method is based on cavity, so if there is a broken part, our method may not work. Hence, we used a method to join the broken parts using the algorithm due to Roy et al. [ 17 ]. Since we consider in our work only small broken part, if there is a small broken part in the component, this algorithm can join that part and our proposed method works well. We have done a quantita-tive experiment on this and noted that our method could not join 0.74% of the broken characters. We also noted that our method wrongly joins 0.32% cases.

We have used different datasets for our experimental results computation. To get the idea of the font sizes, we considered 10, 12, 16, 20, 26, and 30 point-size characters for the experiment. The results are discussed in the following sections. Some of the datasets have ground truth, and some do not have ground truth. If ground truth is not available, then the result is checked manually by viewing the results. To check visually whether a text line is extracted correctly or not, we render all components that are clustered in an indi-vidual line by a single color. The colors for different text lines are selected with a random function generator. 4.1 Text documents Our experiments involve text documents that are obtained both from scanner and camera. The scanned dataset is taken from documents where text lines are parallel to each other and the text lines are horizontal. We considered 50 such doc-uments to test. The camera-based documents are taken from the CBDAR 2007 document image dewarping contest [ 18 ]. This document dataset consists of images captured with a hand-held camera in uncontrolled environments. From this dataset, we considered 30 images containing text data. Some of them include graphical diagrams also. As our system can accept graphical diagrams, we used these documents with-out any other pre-processing and evaluated our approach on these documents. Segmented results of some of the images are shown in Fig. 12 . Detailed quantitative results obtained from our different dataset (text documents, geographical maps, engineering drawings, and seal documents) are given in Fig. 17 .

The errors occurred mainly due to the presence of bro-ken text components. If characters in a text line are broken, and those broken components cannot be joined through pre-processing [ 17 ], then our approach may fail. In this case, neighborhood component selection will not be proper due to the size similarity feature. Hence, direction from the water reservoir concept cannot determine the candidate region properly and errors occur. 4.2 Graphical documents We have considered 20 different real geographical maps and 6 engineering drawing images to test our method. Images were digitized by a flatbed scanner at 300 dp X . They contain text lines of different scale and orientation. We have used one dataset of synthetic maps that are generated using an automatic system described in Delalandre et al. [ 19 ]. The backgrounds (graphical lines, boundaries etc.) of these maps are taken from a few real maps, and the text words of coun-try/river names are placed in the foreground using a set of rules. We show two different test images of the same back-ground map in Fig. 13 a, b. The graphical long lines, i.e., geographical borders and rivers, shown in these maps are kept fixed. The text portions are randomly placed and ori-ented to generate different synthetic documents.

Some real geographical maps are selected from histori-cal archive. See Fig. 13 c, where a portion of the document is shown. These images contain text printed in dark color compared to the background. To obtain the text image, we get foreground information by converting the color image to a gray-level one and apply a threshold to this image for segmenting. The conversion to gray level is performed by transforming the RGB color model to YIQ model, where the luminance channel ( Y ) represents the gray-level image. It is achieved by the conversion equation, Y = 0 . 299  X  R + 0 . 587  X  G + 0 . 114  X  B
In maps, long graphical lines touch or overlap with text in some places. To extract all the text components from graph-ical documents, we need to remove long graphical objects that are present in the image as shown in Fig. 1 c. In the lit-erature [ 13 , 14 ], different algorithms for extraction of these text characters are explained. Connected component analy-sis on the image will be useful for extracting isolated char-acters in the document. For each connected component, we use a minimum enclosing boundary box that describes the height and width of the character shape. The text components are separated analyzing the rectangular size (size of bound-ing box) and the area of the connected components. For this purpose, histogram of the bounding box size of the compo-nents is generated. Through a correct threshold selection ( T ) obtained dynamically from the histogram, the large graphi-cal components are discarded, leaving the smaller graphics and text components. In our experiment, the threshold T is considered as, T = n  X  max ( A mp , A avg ) , where A mp and A avg are the mode and average of the bounding box size. The value of  X  X  X  was set to 3 empirically according to Tombre et al. [ 13 ]. This resultant image is fed to our system for line segmentation.

As mentioned before, there may exist some small graph-ical objects that are not text components. Also, some text components that are touched by graphical objects may be removed by the size threshold criterion. Due to this reason, we may not extract all the text characters by the text/graph-ics separation approach mentioned earlier. The text charac-ters that touch graphical lines are missed out from the text lines extracted in the final result. To get an idea of our text line extraction results in graphical documents, some of the images are shown in Fig. 14 . Most of the errors in maps occurred due to missing of character components in their corresponding lines. It is noticed that, if some characters are in middle of a word, then our algorithm can extract the rest of the characters and group them in a single line. For example in Fig. 14 b, the word  X  X EDITERRA_EAN SEA X  is recov-ered though some of its characters like  X  X  X  in the middle are missing. We can use this extracted text line information to obtain the missing characters. Using some post-processing steps such as top-bottom line fitting of the text line, the miss-ing characters of the words could be found. We get some false positives in maps due to the presence of small components (non-text) in a line. In the engineering drawing dataset, the errors occurred when words were located very close together. As we are not considering the graphical information that is separated from the text portion, the boundary/box informa-tion of the text is ignored. Due to this reason, the words from different text blocks are combined together. 4.3 Seal documents For experiments with real noisy data, documents containing seals were considered (see Fig. 15 ). These were collected from different sources, for example, historical documents, postal letters, official documents from universities etc. In these documents, text lines are in different orientations and they overlap with seal many times. We tested a database of 50 documents containing seals of English characters. The documents were digitized by a flatbed scanner at 200 dpi. In our database, there are mainly 3 types of seal shapes, e.g., circular, elliptical, and rectangular. Text information is anno-tated in straight or curve way according to the shapes of seals. The seals are posted in different orientations and in different locations. Frequently, there is missing of seal information due to noise or overlapping signatures. Sometimes, due to stamping on to text documents, many additional text char-acters of the document itself are also present inside the seal region.

The characters in a string may touch frequently due to background noise, which creates problems for the extrac-tion of isolated text characters. When more than one char-acters in a string touch, they make a touching component. The size of a touching component is usually bigger than an isolated component. As our text line segmentation approach is performed by clustering the character components based on their size and positional information, touching compo-nents may not be clustered in the text line because of the size difference of touching components and isolated char-acters. To tackle this problem, the touching components are decided first using the number of valley formed in these components. For the touching components, the ori-entations of each extreme side are used for character clus-tering. To decide a component as touching or isolated, we checked the number of concavity regions of each compo-nent. If the number of concave regions in a single compo-nent is more than 4, then we assume that the component is a touching component. Here, the height information (height of minimum enclosing rectangle) is considered for checking size similarity criteria mentioned in Sect. 3.1 for character clustering. 4.4 Results with synthetic noise We have tested our approach with the documents added with synthetic noises. A set of graphical documents are degraded with Gaussian noise of different noise levels (10, 20, and 30%). For lines segmentation in noisy documents, our approach is as follows. In the binarized image, we apply a Gaussian smoothing technique to remove some of the noises. Since our method is based on cavity, so if there is a broken part, our method may not work. Hence we used a method to join the broken parts using the algorithm due to Roy et al. [ 17 ]. If there is a small broken part in the component, this algorithm can join the broken part and our proposed method work well. Next, we use our approach for line segmentation. To get an idea of such line segmentation results, we show a document image in Fig. 16 a where the document is added with 20% Gaussian noise. We have also tested method with Kanungo noise with parameter (0.9, 0.9, 0.5, 0.2, 0.0, 1.0), and result is provided in Fig. 16 b. 4.5 Discussion and error analysis To give an idea about different ranges of accuracy of the sys-tem on different types of documents, we divide the accuracy into three categories: (a) 100%, (b) 95 X 99.9%, and (c) &lt;95%. The accuracy of the line extraction module is measured according to the following rule. If out of N components of a line, M components are extracted in favor of that line by our scheme, then the accuracy for that line is ( M  X  100 )/ N %. So if all components of a text line are extracted correctly by the proposed algorithm, we say that the accuracy for the line is 100%.

Figure 17 shows the overall result of line segmentation using our approach. To evaluate the performance of the sys-tem with the retrieved text lines, we use common ratio of pre-cision (P) and recall (R). The precision measures the quality of the extracted lines in terms of the ability of the system to include only relevant text lines in the result. For a given retrieval result, the precision measure (P) is defined as the ratio between the number of relevant extracted lines and the number of retrieved lines. Whereas recall measures the effec-tiveness of the system in retrieving the relevant text lines. The recall (R) is defined as the ratio between number of relevant retrieved lines to the total number of relevant lines in the collection. Precision and recall are computed as follows. Precision = {relevant line}  X  {retrieved line}/{retrieved line} Recall = {relevant line}  X  {retrieved line}/{relevant line}
We show in Table 1 , the precision recall accuracy of dif-ferent datasets obtained from our experiments. The precision recall performance is computed with the text lines where components in text lines are more than 95%.

In our proposed methodology of text line separation, the assumption was taken that the text is present in foreground layer of the documents. Thus, the images were binarized by Otsu threshold to convert the image into two-tone images. If the image is in reverse contrast, i.e., black background and white foreground, our present method can not handle. Our method, thus, depends on efficiency of binarization approach. Also, our system will not work well with color information in the document. We have converted color images to binary images using a typical color to gray image conversion tech-nique and used the binary images for different experiments. More focused color segmentation methods, precisely for doc-uments can be useful for better results.
Most of the errors in our approach are due to over-segmentation of the characters in the text line. We show in Fig. 17 , the text line segmentation accuracy in different dataset used in our experiment. In the datasets of normal scanned text documents, the text lines are segmented very well. In CBDAR dataset, the warped lines are segmented properly and separated from graphical portions if they exist. The orientation of text line information can be useful in future for de-warping the document. In seal document dataset, text line segmentation accuracy is affected mainly due to bro-ken text components in the seal and the presence of long graphical lines over text regions. Also, the text lines in these documents are not separated by large inter-line distance. Sometimes, more than one character clusters appear in the candidate region. Thus, error occurs when a cluster end is very close to another cluster, which is a member of a dif-ferent text line. Hence, text lines are over segmented. From the quantitative analysis, we noted that only 1.12% errors occur because of such situation. To reduce such errors, we have considered height and orientation information. In maps, when the characters of a word are distantly spaced, then our line extraction method sometimes fails to join the cluster ends due to threshold selection. In future, we plan to work to take care of such errors.

Instead of the few drawbacks that occur mainly from connected components detection, our line segmentation approach can handle documents where the characters are sparse and dense. As the algorithm depends on the back-ground information for growing of the text lines, the method is invariant to character font and style. Also, another advan-tage of the method is that, it does not depend on the ori-entation of the text lines. We have tested it in graphical documents containing maps, electrical diagrams, etc. The extracted text line can be useful for post-processing to find missing/touching characters in graphical documents. We have demonstrated the robustness of the approach with a range of documents from degraded text documents to com-plicated seal document. 5 Conclusion We have presented a new approach for text line segmentation. A key characteristic of our approach is the use of foreground and background information. The background information is taken care using water reservoir approach, which guides our process to follow the text line. The approach is based on perceptual laws of text characters in a line and it is easy to apply.

We have demonstrated our approach of invariance toward rotation, scale, affine to real world images. We demonstrated the robustness of this approach with different dataset with high degree of curvilinearity. For this purpose, we consid-ered camera-based documents, graphical documents (map, engineering drawing), and seal documents. Our experiment shows that the approach can be used without de-warping or rotating the documents. One of the significant advantages of the proposed method is its flexibility. Our scheme is inde-pendent of font, size, and style of the text characters. References
