 Social media holds a considerable amount of user-generated content describing the opinions of customers on products and services through reviews, blog, tweets, etc. These reviews are valuable for customers to make purchasing decisions and for companies to guide the business activities. Consequently, the advent of social media has stirred much excitement and provided abundant opportunities for opinion mining and sentiment analysis [12, 6, 23].
 tion [19, 18] or recommender systems [7, 14], recognizing product aspects from the product reviews is usually treated as the first step. Afterwards, we collect relevant sentences and analyze opinions for each product aspect. Thus product two sub-tasks for this task, one is aspect extraction, and the other one is aspect clustering. Aspect extraction aims to extract the entities, which the users write comments on. For example, it can extract the  X   X   X   X  ( X  X icture X  in English) as product aspect from the review  X   X   X   X   X   X  ( X  X he picture is great X ). On the other hand, aspect clustering aims to cluster the aspects that have the similar meaning into the same groups. For example, the word  X   X   X   X  ( X  X icture X  in En-glish) and  X   X   X   X  ( X  X hoto X  in English) express the same meaning, we need to group them.
 sub-task including many kinds of methods, such as rule-based [1, 3, 22, 13], su-pervised [20, 4, 8], and topic model-based [10, 2, 15] methods. Unfortunately, just a few work is done on the second sub-task. Due to the importance of the second sub-task, we need to pay more attention to it and this paper is mainly focusing on this sub-task. In the previous work, some researchers used topic model based methods [17, 10, 16] to cluster domain-specific aspects. However, topic models always jointly modeled topics and sentiment words. Mukherjee et al. [11] used a switch variable trained with Maximum-Entropy to separate topic and sentiment words. Some researchers consider this task as a traditional clustering task, the key part of which is similarity computation. Zhai et al. [21] modeled this task as a semi-supervised learning problem using lexical similarity. However, it needed some manually selected seeds as the input, which are random and accordingly hard to handle or reproduce in the experiments.
 applies two effective aspect relations. Specifically, we treat this task as a typical clustering problem, which mainly emphasizes on the similarity computation. However, the common similarity measures are usually based on literal meaning of two aspects, which is far from enough. To address this issue, we find two interesting phenomena. One is summarized as relevant aspect phenomenon . aspects are relevant and can be grouped into a same cluster . For example, in the sentence  X   X  8 U  X   X  [ &lt;  X   X  fi ]  X   X   X  [  X  fi ]  X  E  X   X   X  B  X  ( X  X  bought fi  X  ( X  X ortrait lens X  in English) and  X   X  fi  X  ( X  X ens X  in English) shows relevant aspect phenomenon. Thus the two aspects can be classified into a same group. The relationship between them is called relevant aspect relation .
 different forms, even though this aspect can be expressed in other forms . Based on this phenomenon, the aspects appearing in the same sentence can be considered sentences as an example:  X  Sentence 1:  X  3 &gt; M  X   X  A Z U 600D [  X   X  ]  X  a  X  [  X   X  ]  X   X   X   X  &lt;  X   X  Sentence 2:  X  3 &gt; M  X   X  A Z U 600D [  X   X  ]  X  a  X  [  X   X  ]  X   X   X   X  &lt;  X  people always use the same word. Thus, the word  X   X   X   X  ( X  X icture X  in English) used in Sentence 1 shows the phenomenon. However, we seldom use different word form to express a same meaning in a same sentence, such as the word  X   X   X   X  ( X  X icture X  in English) and  X   X   X   X  ( X  X hoto X  in English) used in Sentence 2. Therefore, in Sentence 1, since the aspect  X   X   X   X  and aspect  X   X  E  X   X  do not contain each other, they can be considered as different aspects. That is to say, they belong to different groups. Thus, the relationship between them is called irrelevant aspect relation .
 relevant aspect set and an irrelevant aspect set from a large corpora respectively. That is to say, we provide two kinds of background knowledge for each aspect. On one hand, the relevant aspect set is to help the given aspect to get the domain synonyms more accurately. On the other hand, the irrelevant aspect set is to help separate this aspect from other irrelevant aspects that do not refer to the same aspect.
 ing method is applied to classify the aspects into different groups based on their relevant aspect sets and irrelevant aspect sets. Several similarity computation methods are used to compute the similarity between two aspects.
 Experimental results show that the both two kinds of aspect relations achieve significant performance that gains over the baseline clustering method without using these two relations.
 two kinds of aspect relations, and constructs the relevant and irrelevant aspect set for each product aspect. Section 3 shows the hierarchical clustering algorithm based on the two aspect relations. Section 4 presents the experiments and results. Finally we conclude this paper in Section 5. For each aspect, two aspect sets, irrelevant aspect set and relevant aspect set can be built. Figure 1 shows an example consisting of a Chinese review, which is tagged with all the appearing product aspects.
 aspect  X   X  fi  X  ( X  X ens X  in English) as an example, since  X   X  fi  X  is the suffix of  X  1 C  X  fi  X ,  X  1 C  X  fi  X  is the relevant aspect of the given aspect  X   X  fi  X  and can be concluded into the relevant set. On the other hand,  X  1  X  and  X   X   X  J  X  are totally different from  X   X  fi  X  literally, thus they are concluded into the irrelevant set. where set R is a set that stores items relevant to a and set IR is a set that stores items irrelevant to a . aspects and m irrelevant aspects to generate the final aspect clustering.  X  relevant aspect r i : aspect a and r i is relevant, if a and r i are appearing in  X  irrelevant aspect ir j : aspect a and ir j is irrelevant, if a and ir j are ap-accordingly acquire lots of relevant aspect sets and irrelevant aspect sets from a domain-specific corpus. Then a final set R and set IR with more aspect elements can be further built.
 aspects can be found from 138 reviews. Based on these background knowledge, we can design new hierarchical clustering algorithms to classify the domain aspects into different groups. Since aspect clustering is a typical clustering problem, we can use many kinds of clustering algorithms. In this paper, we take hierarchical clustering algorithm as a case of study. During the process, similarity computation between aspects is the main part. Traditional similarity measures are using the thesaurus dictionaries or just computing the similarity between two aspect literally. However, they are far from sufficient due to a few reasons. First, many aspects are domain words or phrases, which are not included in the traditional thesaurus dictionaries. For example, the aspect  X  1 C  X  fi  X  ( X  X ptical zoom lens X  in English) is not appearing in any dictionaries. Secondly, many aspects are not synonyms in a dictionary, but indicating the same aspect under the given domain.
 for each aspect, including the relevant aspect sets and the irrelevant aspect sets. That is to say, we can use more knowledge to compute the similarity between two aspects besides their similarity literally.
 poses three parts.  X  Literal Similarity (LS): Similarity between a i and a j literally, which is record- X  Relevant Set Similarity (RSS): Similarity between the relevant aspect set- X  IRrelevant Set Similarity (IRSS): Similarity between the irrelevant aspect as follows: where similarity s 2 reflects the relevant aspect phenomenon and s 3 reflects the irrelevant aspect phenomenon respectively.
 Figure 2 in detail.
 and c j = { a j 1 ,...,a j q ,...,a j m } is computed as follows.
 4.1 Experimental Setup Corpus We conducted the experiments on a Chinese corpus of digital camera domain, which came from the corpora of the Chinese Opinion Analysis Evalua-tion (COAE). Table 1 describes the corpus in detail. reduplication removing, and 1,189 aspects are left after reduplication removing. Besides, each aspect averagely appears about 3.4 times. Therefore, for each as-pect, we can collect its two effective aspect relation sets from many sources, because this aspect may appear multiple times in the corpus.
 Evaluation We follow the evaluation metrics of Zhai et al. [21] to evaluate the clusters in this study. The evaluation metrics include two parts: Entropy and k is the given number of clusters. Suppose our method can group DS into k disjoint subsets, that is, DS = DS 1 ,...,DS i ,...,DS k , Entropy and Purity can be defined as follows.
 Equation (6), where P i ( g j ) is the proportion of g j data points in DS i . The total entropy of the whole clustering (which considers all clusters) is calculated by Equation (7). one gold-partition. The cluster purity is computed with Equation (8). The total purity of the whole clustering (all clusters) is computed with Equation (9). Comparative systems Similarity computation between aspects is the main part during the clustering procedure. According to the three similarity compu-tation measures between aspects, we designed four comparative systems to show the performance of each similarity measure when clustering.  X  Literal Similarity (LS): We compute the similarity between two aspects a i  X  Relevant Set Similarity (RSS) + LS: We compute the similarity between two  X  IRrelevant Set Similarity (IRSS) + LS: We compute the similarity between  X  RSS + IRSS +LS: We combine the three kinds of similarities between two 4.2 Results product aspect clustering task. Here, LS (Literal Similarity) is the baseline sys-tem, which is computed without any background knowledge. All the other three systems are based on the baseline system, and computed with different kinds of background knowledge.
 describe each aspect, two kinds of background knowledge can be expanded ac-cordingly.
 relevant aspect set as the new background knowledge can yield better results, with the Entropy of 1.39 and the Purity of 0.95. This can illustrate that the a , besides the knowledge of a i  X  X  literal meaning, its relevant aspect set expand-ed from multiple sentence contexts is another good dimension to measure the similarity between two aspects.
 new background knowledge can also yield better results, with the Entropy of 1.40 and the Purity of 0.95, compared with LS . This proves that the irrelevant aspect set can also be treated as another important evidence for aspect clustering. Obviously, if the aspect a i appears in the irrelevant aspect set of the aspect a j , a and a j cannot be grouped together. This background knowledge can naturally avoid a part of the situation that a i and a j are literally similar, but in fact they do not belong to a same group.
 be considered as a supplement of the literal similarity ( LS ), and the dimension of irrelevant aspect similarity ( IRSS ) can be considered as a filter to reduce some wrong cases. Therefore, the two aspect relations RSS and IRSS are comple-mentary to each other, we combine them into a new system LS+RSS+IRSS based on the baseline LS . Table 2 shows that LS+RSS+IRSS performs best among all the comparative systems, with the Entropy of 1.37 and the Purity of 0.96. Aspect extraction and aspect clustering are both critical for the applications of sentiment analysis and opinion mining. However, the research on the aspect clus-tering task is far from enough. In this paper, we propose an easy and effective unsupervised method based on two effective aspect relations, namely, relevant aspect relation and irrelevant aspect relation. These two kinds of relations can expand the background knowledge of each aspect, and improve the performance of the similarity computation between two aspects. Experimental results on cam-era domain show that our method achieves better performance than the baseline without using the aspect relations, which proves that the two proposed relations are useful. As the future work, in order to capture more background knowledge for each aspect, we will expand them from the Web.
 We thank the anonymous reviewers for their helpful comments. This work was supported by National Natural Science Foundation of China (NSFC) via grant 61300113, 61133012 and 61273321, and the Ministry of Education Research of Social Sciences Youth funded projects via grant 12YJCZH304.

