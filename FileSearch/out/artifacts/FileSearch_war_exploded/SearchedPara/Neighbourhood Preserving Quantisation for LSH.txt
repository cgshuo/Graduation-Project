 We introduce a scheme for optimally allocating multiple bits per hyperplane for Locality Sensitive Hashing (LSH). Exist-ing approaches binarise LSH projections by thresholding at zero yielding a single bit per dimension. We demonstrate that this is a sub-optimal bit allocation approach that can easily destroy the neighbourhood structure in the original feature space. Our proposed method, dubbed Neighbour-hood Preserving Quantization (NPQ), assigns multiple bits per hyperplane based upon adaptively learned thresholds. NPQ exploits a pairwise affinity matrix to discretise each di-mension such that nearest neighbours in the original feature space fall within the same quantisation thresholds and are therefore assigned identical bits. NPQ is not only applica-ble to LSH, but can also be applied to any low-dimensional projection scheme. Despite using half the number of hy-perplanes, NPQ is shown to improve LSH-based retrieval accuracy by up to 65% compared to the state-of-the-art. H.3.3 [ Information Systems ]: Information Search and Re-trieval Algorithms, Experimentation, Measurement Locality Sensitive Hashing, Image Retrieval, Approximate Nearest Neighbour Search, Hamming Distance, Manhattan Distance
Approximate nearest neighbour (ANN) search using hash-ing techniques is widely used to find objects of interest within large-scale datasets. In this scenario similarity preserving hash functions map nearby points in the original space into similar and substantially more compact binary codes. Us-ing binary codes as surrogates for the original data points reaps two important computational benefits: firstly storage requirements are vastly reduced given the more compact na-ture of the binary codes; and secondly ANN search can be performed in sublinear time by treating the binary codes as hash keys.

The primary requirement for an effective hashing scheme is the generation of codes that use as few bits as possi-ble while also mapping similar data points to binary codes. There are two distinct stages to most existing hashing schemes: projection and quantisation . A common first step in many binary encoding methods is to project the data into a lower dimensional space, for example, by using principal compo-nent analysis (PCA) or by using a Gaussian random matrix (LSH). The projected data points are then subsequently en-coded into binary using a quantisation step. Both steps should ideally preserve the neighbourhood structure between the points in the original feature space.

There has been a significant amount of prior research that has sought to improve the projection stage. For example, by introducing machine learning methods to learn compact codes [11][2]. In contrast, the quantisation stage has at-tracted substantially less attention from the research com-munity, but is arguably equally as important. Binarising real-valued features can lead to significant information loss if not performed judiciously. In this paper we focus on im-proving quantisation performance. Our primary hypothesis in this paper is that a pairwise affinity matrix (for example, based upon the Euclidean distance between points in the original feature space) can be an effective signal for quantiz-tains the affinity between the data points.
The seminal work on Locality Sensitive Hashing (LSH) demonstrated how random projections could be applied in the generation of binary codes that approximately maintain the distance between points in the original feature space [3]. LSH function families have the property that objects that are close in the original feature space have a higher proba-bility of being allocated identical binary codes than objects that are further apart. The LSH hash codes permit sub-linear retrieval time by acting as an index into the bucket of a hash table. Many LSH hash function families have been 1 I n this paper we define a projected dimension as the normal vector of the hyperplane. Figure 1: Partitioning of a projected dimension b ased on thresholds (solid vertical lines) and the as-sociated binary encoding of each region. Top: bit allocation for single bit quantisation. Middle: NPQ with 2 bits per dimension and three thresholds. Bot-tom: NPQ with 3 bits per dimension and seven thresholds. This is the same bit encoding as in [5]. developed. For example, LSH for cosine similarity [1] inter-sects the space with random hyperplanes, with each result-ing subspace forming a bucket of the hashtable. The number of LSH hyperplanes directly equate to the number of bits in the binary code for a data point. More precisely, if x . n i  X  X  1 . . . k } for data point x and hyperplane normal vector n , then the i-th bit is set to 0, and 1 otherwise. This ef-fectively thresholds the projected dimension at zero. The k bits generated from k hyperplanes are concatenated to form the hash key for use in ANN search.

LSH uses data-independent random projections , which de-spite having some attractive theoretical guarantees, is not generally conducive to the production of compact hash codes. This problem has recently been circumvented by using ma-chine learning techniques to find appropriate hash functions through optimization of an underlying hashing objective func-tion for compact binary codes. Many of the more recent methods in this area, including Spectral Hashing [11] and Iterative Quantization [2] have demonstrated improvements on LSH in terms of the number of bits required to find good approximate nearest neighbours. These approaches are also based upon the idea of thresholding a projected dimension at zero to generate bits.
Our approach, dubbed Neighbourhood Preserving Quanti-sation (NPQ), allocates multiple bits per hyperplane based upon a novel adaptive thresholding scheme. In contrast, vanilla LSH thresholds at zero and assigns a single bit per hyperplane. Existing multiple-bit quantisation schemes seek to position the thresholds based solely on information within the projected space, for example by using a k-means cluster-ing on each projected dimension [5][4]. In this work we show that the affinity between the data points in the original space can be a valuable signal for optimal threshold positioning.
We use X  X  X  X  n  X  m , to denote the data matrix, where n is the number of instances and m is the number of features. We are given n data points { x 1 , x 2 , . . . , x n } , x form the rows of the data matrix X . The objective is to learn a binary code matrix B  X  X  0 , 1 } n  X  b , where b is the total number of bits in our binary representation. In previous work the binary encoding function for bits k = { 1 . . . b } is given by h k ( x ) = sgn ( n k x ), where n k is a vector normal to the hyperplane and sgn ( v ) = 1 if v  X  0 and 0 otherwise.
As argued by previous authors [5] [4], thresholding at zero in this manner is suboptimal given that the area of high-est data density typically occurs in the region around zero. There is therefore a high chance that neighbouring points may fall on different sides of the threshold, and therefore be assigned different bits in our encoding. NPQ seeks to over-come this problem by using multiple thresholds to partition the dimension in such a way that points close in the orig-inal feature space are more likely to fall between the same thresholds, and therefore are assigned identical bit codes.
Let T = { t 1 , t 2 , . . . , t u } be a set of thresholds, where u  X  1, t i  X  X  X  with t 1  X  t 2 . . .  X  t u . We denote by t and t u +1 the leftmost and rightmost ends of the sorted di-mension 2 , respectively. NPQ directly leverages a pairwise affinity matrix , S  X  X  X  n  X  n , where S ij = 1 if x i and x  X  -nearest neighbours in the original feature space, and 0 oth-erwise, to guide the positioning of the thresholds in T along the projected dimension. In our terminology a pair of data points x i and x j with S ij = 1 are deemed a positive pair , and a pair of points with S ij = 0 a negative pair . Intuitively, the pairwise affinity matrix specifies which points should fall between the same thresholds, and therefore be assigned the same bit codes. NPQ seeks to exploit this valuable signal.
We define a region r a as all the projected values y i be-tween thresholds t a and t a +1 , where a  X  X  0 , . . . , u } . For u thresholds, there are u +1 regions. NPQ counts the number of true positives (TP), false positives (FP) and false nega-tives (FN) across all regions, r a , of the projected dimension.
Intuitively, TP are the number of positive pairs that are found within the same region, FP is the number of negative pairs found within the same region, and FN are the number of positive pairs found in different regions of the threshold partitioned projected dimension. We combine the TP, FP and FN counts by computing the F 1 score. The overall NPQ objective function Z npq is a convex combination of the F score and the regularization term  X ( T 1: u ): Here we define  X ( T 1: u ) as:
Where  X  = P n i =1 { y i  X   X  d } 2 ,  X  d denotes the mean of the dimension and  X  r a is the mean of the points in region r that is, between thresholds t a and t a +1 . NPQ varies the po-sition of the thresholds along the projected dimension until a maximum of Z npq is attained. The optimisation approach is shown in Figure 2. We use random restarts for the thresh-old positions to ensure that the optimisation remains linear in the number of data-points rather than O ( n u ). projections of all data-points for one particular hyperplane Figure 2: Top: data points in the original 2 dimen-si onal space. Middle: Projection of points onto the normal vector n of a hyperplane. This illustrates a good positioning of the thresholds. In this case most pairs receive similar binary bits. Bottom: a poor positioning of the thresholds. Here the thresh-olds divide neighbouring pairs into different regions thereby causing differences in their bit assignments. NPQ favours the threshold positioning that max-imises the F1 score.
We evaluate the effectiveness of NPQ in the domain of im-age retrieval, although our approach is general and can be used for other types of data (for example, text, video). To do so, we test against three publicly available image datasets: 22k Labelme consisting of 22,019 images represented as 512 dimensional Gist descriptors [8]; CIFAR-10 a dataset of 60,000 images represented as 512 dimensional Gist descrip-tors; and 100k TinyImages a collection consisting of 100,000 images, represented by 384 dimensional Gist descriptors, randomly sub-sampled from the original 80 million tiny im-ages dataset. The datasets and and associated features are identical to that used in previous related work [5] [4]. This ensures that our results are directly comparable to previ-ously published figures.
We evaluate NPQ quantisation performance with five pro-jection schemes: LSH-based projections [3], Shift-invariant Kernel-based Locality-Sensitive Hashing (SIKH) [9], Prin-cipal Components Analysis Hashing (PCAH) [10], Spectral Hashing (SH) [11] and Iterative Quantisation (ITQ) [2]. All five projections are assigned 2 bits per dimension from the encoding scheme in Figure 1.
NPQ quantisation performance is compared against four state-of-the-art quantisation schemes in addition to the stan-dard threshold at zero technique: single bit quantisation (SBQ) [3], Manhattan Hashing (MQ) [5], Double Bit Quan-tisation (DBQ) [4] and Hierarchical Quantisation (HQ) [6].
In all experiments we follow previously accepted proce-dure [5] and randomly select 1000 data points as queries, with the remaining points being used to train the hash func-tions. All experimental results are averaged over 10 random training and testing partitions, and the average result re-ported.

We define the  X  -neighbours of each data-point based upon a data-dependent threshold  X  which is computed by sam-pling 100 training data-points at random from the training dataset. The threshold  X  is set to the Euclidean distance at which these training points have 50 nearest neighbours on average. This threshold is used to determine the nearest neighbours for training and testing. To evaluate the quality of retrieval we use the Euclidean ground truth to compute precision, recall and the area under the precision-recall curve The NPQ affinity matrix is computed by thresholding the Euclidean distance matrix by the average distance to the 50th nearest neighbour. The affinity matrix is computed us-ing the training dataset only. The parameter  X  (Equation 1) contribution of NPQ lies in our novel objective function we adopt both the binary bit encoding method (Figure 1) and Manhattan distance metric used by MQ [5].
Experimental results are presented in Table 1 and Fig-ure 3. Table 1 demonstrates that NPQ outperforms all state-of-the-art quantisation schemes at 32-bits across five different projection methods, and three different datasets. We find a 33% performance gain over MQ for LSH-based projections for 22k Labelme. This result is statistically sig-nificant based upon a paired t-test across 10 random train-ing/testing partitions of the dataset ( p-value :  X  1 . 7  X  10  X  5 ). We also find statistically significant gains in performance on the larger CIFAR-10 and 100k TinyImages datasets. For CIFAR-10, NPQ obtains a substantial 65% increase in AUPRC over MQ for LSH at 32 bits ( p-value :  X  1 . 0  X  10  X  7 ). For 100k TinyImages NPQ achieves a 37% increase in AUPRC over MQ with LSH at 32 bits ( p-value :  X  2 . 5  X  10  X  12 ). Figure 3 presents the precision-recall (PR) curves (at 32 bits) and AUPRC vs number of bits for all three datasets. NPQ obtains improved precision at all recall levels, in addi-tion to decisively dominating both SBQ and MQ in terms of AUPRC between 8 to 128 bits. We confirm our primary hy-pothesis that a pairwise affinity matrix is a beneficial signal for guiding threshold-based quantisation.
This paper presents the neighbourhood preserving quan-tization (NPQ) method for approximate similarity search. NPQ leverages adaptively learned thresholds to quantize LSH projections into multiple bits based upon a pairwise affinity matrix. Our results show that retrieval performance can be significantly enhanced by using the neighbourhood information in the original feature space to inform the place-Precision (MAP). 4 We find a dependence of  X  on bit length:  X  = 1 . 0 is effective for bit lengths  X  128 bits, with settings of  X  &lt; 1 . 0 effective for higher bit lengths. Dataset 22k LabelMe CIFAR-10 100k TinyImages ment of quantisation thresholds. NPQ is orthogonal to ex-isting approaches for improving the accuracy of LSH, for example multi-probe LSH [7], and can be applied alongside these techniques to further improve retrieval performance. An interesting avenue for future work would be the de-velopment of a principled method for selecting a variable number of bits per dimension that does not rely on either a projection-specific measure of hyperplane informativeness (e.g. magnitude of the eigenvalues) or computationally ex-pensive cross-validation.
 We are grateful to Weihao Kong for making available his feature sets for the CIFAR-10 and 100k Tiny Image datasets. [1] M. S. Charikar. Similarity estimation techniques from [2] Y. Gong and S. Lazebnik. Iterative quantization: A [3] P. Indyk and R. Motwani. Approximate nearest [4] W. Kong and W.-J. Li. Double-bit quantization for [5] W. Kong, W.-J. Li, and M. Guo. Manhattan hashing [6] W. Liu, J. Wang, S. Kumar, and S.-F. Chang.
 [7] Q. Lv, W. Josephson, Z. Wang, M. Charikar, and [8] A. Oliva and A. Torralba. Modeling the shape of the [9] M. Raginsky and S. Lazebnik. Locality-sensitive [10] J. Wang, S. Kumar, and S.-F. Chang. Semi-supervised [11] Y. Weiss, A. Torralba, and R. Fergus. Spectral
