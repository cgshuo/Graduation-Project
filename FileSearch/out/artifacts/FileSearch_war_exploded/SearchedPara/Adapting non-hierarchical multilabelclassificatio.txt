
Institute of Mathematical Sciences and Computation, University of S  X  ao Paulo, S  X  ao Carlos, Brazil School of Computing, University of Kent, Canterbury, UK 1. Introduction since the classes are hierarchically structured and an instance can simultaneously belong to more than one class at the same hierarchical level. These problems are very common in applications like protein and gene function prediction and text classi fi cation.

HMC problems can be treated using two major approaches, referred to as local (or top-down) hierar-a top-down fashion to classify each new (test) instance. In contrast, the global approach induces a single classi fi cation model considering the class hierarchy as a whole.

According to [39], there are different training versions for the local approach: a local classi fi er per procedure to predict the class of a new instance. The experiments presented in this work use only the local classi fi er per parent node version. In this version, for each parent node in the class hierarchy, a than two classes, a multiclass classi fi cation approach must be used.

For the classi fi cation of a new instance, the system initially predicts its most generic class, which is child classes at the next level, de fi ning its subclasses. Thus, in the test phase, when an instance is are propagated to the deeper levels of the class hierarchy. However, it has the positive aspect that any traditional (non-hierarchical) classi fi cation algorithm can be used at each node of the class hierarchy.
In the global approach, after inducing a single classi fi cation model using the whole training set, the classi fi cation of a new instance occurs in just one step. Since global methods must consider the adaptations are made to consider the whole class hierarchy. As a result, global methods have a more complex implementation. However, they avoid the error propagation problem associated with the local global rule set tends to be less complex (with much fewer rules) than the collection of all local rule sets generated by classi fi ers following the local approach [2,47].
 This article proposes and evaluates two local methods, named HMC-Label-Powerset and HMC-Cross-Training. These new methods are hierarchical vari ations of non-hierarchi cal multilabel m ethods found in the literature: Label-Powerset [44] and Cross-Training [38]. The HMC-Label-Powerset method uses a label combination strategy to combine sibling classes assigned to an instance into a new class, transforming the original HMC problem into a hierarchical single-label problem. The HMC-Cross-Training method applies a label decomposition strategy, which transforms the original HMC problem methods produce solutions to the ori ginal hierarchical multilabel problem. Thus, the main difference between the proposed methods is:  X  HMC-Label-Powerset: based on local label combination, where the set of labels assigned to an  X  HMC-Cross-Training: based on local label decomposition, where the HMC problem is decomposed
In this work, these two methods are compared with three well-known HMC methods: the well-known local binary-relevance method (HMC-Binary-Relevance) [39], used as baseline in many works, and two global methods, HC4.5 [11] and Clus-HMC [47]. The main aspects of these three methods are:  X  HMC-Binary-Relevance: local method based on local binary classi fi cation, where a classi fi er is  X  HC4.5: global hierarchical multilabel variation of the C4.5 algorithm [34], where the entropy  X  Clus-HMC: global method based on the concept of Predictive Clustering Trees (PCTs) [6], where a
An experimental comparison between the global and local approaches is another contribution of this methods and to the development of new methods. Besides, there are few empirical comparisons of these two approaches in the literature [14,47], since mo st of the works compare a proposed method with a non-hierarchical counterpart.

The fi ve methods investigated are evaluated using 10 datasets related with gene function prediction for the Saccharomyces cerevisiae (a speci fi c type of Yeast ) model organism, regarding different data confor-mations. Speci fi c metrics developed for the evaluation of HMC classi fi ers are used. The experimental results show that the proposed methods, specially the HMC-Label-Powerset method, can provide a good alternative to deal with HMC problems.

This article is organized as follows: Section 2 introduces the basic concepts of hierarchical and proposed methods are explained in details in Section 4; the experimental setup and the analysis of the results are presented in Sections 5 and 6, respectively; fi nally, Section 7 discusses the main conclusions and future research directions. 2. Hierarchical and multilabel classi fi cation with hierarchical single-label problems followed b y non-hierarchical multilabe l problems. These types of problems are then combined in the discussion of hierarchical multilabel classi fi cation problems, which are formally de fi ned. 2.1. Hierarchical single-label classi fi cation each instance x or subclasses. However, in many real classi fi cation problems, one or more classes can be divided into tree or a Directed Acyclic Graph (DAG). These problems are known in the literature of Machine Learning (ML) as hierarchical classi fi cation problems. In these problems, new instances are classi fi ed into the class(es) associated with one or more nodes in a class hierarchy. When each new instance must be node of the class hierarchy, the task is named non-mandatory leaf node classi fi cation [22].
Figure 1 illustrates examples of two text classi fi cation problems, one whose classes are structured as a tree and the other as a DAG. The main difference between the tree 1(a) and the DAG 1(b) structures is that, in the tree structure, each node (except the root) has exactly one parent node, while, in the DAG structure, each node can have more than one parent node. In tree structures, each class has a unique depth, because there is only one path from the root to any given class node. In DAG structures, however, the depth of a class is no longer unique, since there can be more than one path between the root and a given class node. These characteristics must be taken into account in the design and evaluation of classi fi cation models based on these structures.

In both structures, classes associated with deeper nodes in the hierarchy usually have lower prediction accuracy. This occurs because these classes are more speci fi c and the classi fi ers for these nodes are trained with fewer instances than classes associated with shallower nodes. 2.2. Non-hierarchical multilabel classi fi cation
In multilabel classi fi cation problems, each instance x the same time. A multilabel classi fi er can be represented by a function H : X  X  2 C , which maps an instance x
Figure 2 illustrates a comparison between a conventionalclassi fi cation problem, where instances can be problem in which a document can belong to one of the two classes  X  X iology X  or  X  X omputer Science X , but never to both classes at the same time. Figure 2(b) shows a classi fi cation problem in which a document can be simultaneously assigned to the classes  X  X iology X  and  X  X omputer Science X , referred to as  X  X ioinformatics X  documents.

Similar to hierarchical problems, where local and global approaches can be used to solve the clas-si fi cation task, two major approaches have been used in multilabel problems, referred to as algorithm independent and algorithm dependent [9]. The algorithm independent approach transforms the original multilabel problem into a set of single-label problems and, as in the local approach, any traditional classi fi cation algorithm can be used. In the algorithm dependent approach, as the name suggests, new mechanisms of existing traditional algorithms. The global approach for hierarchical problems can be seen as an algorithm dependent approach, as new or modi fi ed algorithms are used.
 Fuzzy classi fi cation is used to deal with ambiguity between multiple classes for a given instance. It is not used to achieve a multilabel classi fi cation. Usually, a defuzzi fi cationstepisusedtoderiveacrisp can have properties of multiple classes, and these classes can be very distinct. Additionally, the use of membership functions in both problems is different. While, in fuzzy systems, for each instance, the sum of the degrees of memberships in all  X  X uzzy classes X  is 1, this constraint in not applied to multilabel problems, where each instance can be assigned to more than one class, belonging 100% to each class, which would be equivalent to a  X  X otal degree of membership X  much larger than 1. 2.3. Hierarchical multilabel classi fi cation
In hierarchical multilabel classi fi cation problems, the multilabel and hierarchical characteristics are combined, and an instance can be assigned to two or more subtrees of the class hierarchy. The HMC problem is formally de fi ned by [47] as follows: Given:  X  a space of instances X ;  X  a class hierarchy ( C,  X  aset T of instances ( x  X  a quality criterion q that rewards models with high accuracy and low complexity.
 Find:  X  a function f : X  X  2 C ,where 2 C is the powerset of C , such that c  X  f ( x )  X  X  X  c
The quality criterion q can be the mean accuracy of the predicted classes or the distances between the and the induction time can be take n into account as quality criteria.

An example of HMC problem is illustrated in Fig. 3, in which the class hierarchy is structured  X  X iology/Bioinformatics X , and  X  X omputer Science/Arti fi cial Intellig ence X . When a prediction is made in this subtree is reduced to a path.

It is important to notice the difference between hierarchical single-label problems and multilabel problems. A hierarchical single-label problem can be seen as being naturally multilabel in a kind of trivial way, due the fact that a path in the hierarchy has more than one class. When the class  X  X iology/Bioinformatics X  is assigned to an instance, this prediction means that the instance belongs to two classes:  X  X iology X  and  X  X ioinformatics X . However, in this paper, a hierarchical problem is considered multilabel only in the non-trivial case where classes from more than one path in the hierarchy are assigned to an instance. 3. Related work
Many methods have been proposed in the literature to deal with HMC problems. The majority of them are applied to protein and gene function prediction and text classi fi cation. This section reviews some of the recent methods, organizing them according to the taxonomy proposed by [39]. In this taxonomy, a hierarchical classi fi cation algorithm is described by a 4-tuple &lt;  X  ,  X  ,  X  ,  X  &gt; ,where:  X   X  : indicates ifthe algorithmis hierarchicalsingle-label(SPP  X  Single Path Prediction)orhierarchical  X   X  : indicates the prediction depth of the algorithm  X  MLNP (Mandatory Leaf-Node Prediction) or  X   X  : indicates the taxonomy structure the algorithm can handle  X  T (tree structure) or D (DAG  X   X  : indicates the categorization of the algorithm under the proposed taxonomy  X  LCN (Local
A LCN method was proposed in [4], where a hierarchy of SVM classi fi ers [46] is used for the classi-fi cation of gene functions according to the biological process hierarchy of the GO (Gene Ontology) [3]. Classi fi ers are trained for each class separately. The predictions are combined using a Bayesian network model, with the objective of fi nding the most probable consistent set of predictions.

Another LCN method, [10], proposed a Bayes-optimal classi fi er and applied it to two document datasets: the Reuters Corpus Volume 1, RCV1 [29] and a speci fi c subtree of the OHSUMED corpus of medical abstracts [25]. In this method, the relationships between the classes in the hierarchy are seen as a forest. Trees in this forest represent a class taxonomy G . The method starts by putting all nodes of G in a set S . The nodes are then removed from S one by one. Every time an instance is assigned to a class c , it is also assigned to the classes in the path from the root of its tree to the class c
Also based on the LCN strategy, an ensemble where each base classi fi er is associated with a class of the hierarchy was investigated in [45]. It was applied to datasets with genes annotated according to the FunCat scheme developed by MIPS [30]. The base classi fi ers were trained to become specialized probabilities  X  p  X  X onsensus X  global probability p The dataset used was composed by MedLine articles associated with GO codes. The training process value, representing the probability th at the input instance belongs to a class c probability is higher than a given threshold are assigned to the instance.
 A second LCPN method was proposed by [20]. This method is a hierarchical variation of the AdaBoost [37], named TreeBoost.MH. It was applied to the hierarchical multilabel classi fi cation of documents, using a hierarchical version of the Reuters-21578 corpus, generated in [42], and the Reuters Corpus Volume 1  X  version 2 (RCV1-v2) [29]. The TreeBoost.MH is a recursive algorithm that uses the AdaBoost.MH as its base step, and is called recursively over the hierarchical class structure.
There also works proposing GC methods. The work of [36], for example, investigated a kernel based algorithm for the hierarchical multilabel classi fi cation. This method was applied to text and biological classi fi cation problems (RCV1, WIPO-alpha patent dataset [48] and Enzyme classi fi cation dataset [32]). The classi fi cation method is based on a variation of a framework named Maximum Margin Markov Network [41,43], where the hierarchical structure is represented as a Markov tree. For the learning process, the authors de fi ned a joint feature map  X  ( x, y ) over the input and output spaces. In the HMC context, the output space can be de fi ned as all possible subtrees or subgraphs of the class hierarchy.
Another GC method was proposed in [11]. This global method, named HC4.5, is based on decision tree induction. It was applied to the classi fi cation of gene functions of the Saccharomyces cerevisiae are made. In the original C4.5 algorithm, the entropy is used to decide the best split in the decision tree. The authors X  variation of C4.5 employed the sum of the entropies of all classes to choose the best attribute to label an internal node of the tree.

The GC strategy was also used in the work of [8], where two new HMC methods were proposed and applied to the task of document classi fi cation, also using the WIPO-alpha patent dataset. The fi rst method is a generalized version of the Perceptron algorithm, and the second is a hierarchical multilabel SVM. For the multilabel version of the SVM, the authors generalized the multiclass formulation [15] to a multilabel formulation similar to [19]. The propos ed hierarchical Percep tron algorithm uses the minimum-overlap (Minover) learning rule [28], so that the instance that most violates the desired margin is used to update the separating hyperplane.
 The work of [2] also followed the GC strategy to developed an Arti fi cial Immune System (AIS), named Multilabel Hierarchical Classi fi cation with an Arti fi cial Immune System (MHC-AIS), for the prediction of protein functions described in the GO. The proposed algorithm is able to fi nd a set of rules that are given protein (instance). The algorithm training is divided into two basic procedures, named Sequential Covering (SC) and Rule Evolution (RE). These procedures produce candidate classi fi cation rules, each and a consequent (THEN part), represented by a set of predicted classes.

Still based on the GC approach, the work of [47] compared three methods that use decision trees, based on PCTs [6], for HMC problems. The methods are compared using datasets related to functional genomics. The authors compared the performance ofthe Clus-HMC method thatinducesa single decision tree making predictions for all the classes of the hierarchy at once, with other two methods that induce a decision tree for each hierarchical class, named Clus-SC and Clus-HSC. Clus-SC de fi nes an independent the classes. Clus-HSC explores the hierarchical relationships to induce a decision tree for each class in the hierarchy. The authors also applied the methods to class hierarchies structured as DAGs, discussing the issues that arise when dealing with these kinds of structures and the modi fi cations required to the algorithms to be able to deal with such hierarchies.

Table 1 presents the methods reviewed in this section, organized according to the taxonomy presented by [39]. 4. Proposed methods This section presents the HMC methods proposed in this work. The fi rst method, named Hierarchical Multilabel Classi fi cation with Label-Powerset (HMC-LP), performs a combination of labels (classes), where all labels assigned to an instance, at a speci fi c level, are combined into a new and unique label. This creates a hierarchical single-label problem . The second method, named Hierarchical Multilabel Classi fi cation with Cross-Training (HMC-CT), carries out a decomposition of labels, creating many hierarchical single-label problems. According to the taxonomy proposed by [39], these new methods can be classi fi ed as &lt; M P P, M LN P, T, LCP N &gt; . 4.1. Hierarchical multilabel classi fi cation with label-powerset ( HMC-LP )
The HMC-LP method uses a label combination process to transform the HMC problem into a hi-erarchical single-label problem. It is a hierarchical adaptation of the Label-Powerset method used for non-hierarchical multilabel classi fi cation in [7,44]. Different from the HMC-Binary-Relevance (HMC-BR) method, HMC-LP considers the sibling relationships between classes.

In the Label-Powerset method used fo r multilabel non-hierarchical classi fi cation, all classes assigned to each instance are combi ned into a new and unique class. Figure 4 illustra tes an example of the application of the label combination process. This fi gure illustrates a combination of the classes in the instances 1 and 3. For each of these two instances, a new class was created, labeled  X  X iomedicine X .
In the HMC-LP method, labels are combined at each level of the class hierarchy. This occurs by combining all classes assigned to each instance, at a speci fi c level, creating a new class.
To illustrate this process, consider an instance belonging to the classes A.D and A.F , and another instance belonging to the classes E.G , E.H , I.J and I.K ,where A.D , A.F , E.G , E.H , I.J and I.K are hierarchical structures, such that A E and I belonging to the fi rst level and D , F , G , H , J and K belonging to the second level, as shown in Fig. 5. When the HMC-LP method is applied, the resulting combination of classes for the two instances would be a new hierarchical structure with the label paths C the fi rst instance, C level. In the second instance, C at the second level. Figure 5 illustrates this process of label combination. To the best of our knowledge, this type of adaptation was not yet reported in the literature.

As can be seen in Fig. 5, after the label combination process, the original problem is transformed into a hierarchical single-label problem. During the training and test phases, the local approach is used, process, the predictions referring to the combined classes are transformed into predictions of their original, individual classes. The label combination procedure is presented in Algorithm 1.
One problem with the label combination process is that it can considerably increase the number of classes in the dataset. As a small example, Fig. 4 shows a multilabel problem with three classes. After the label combination procedure, the number of classes is increased to four. If there are many possible multilabel combinations in the dataset, the new formed classes may have few positive instances, resulting instead of binary classi fi ers, the induction time might decrease considerably when compared with the HMC-Binary-Relevance method. 4.2. Hierarchical multilabel classi fi cation with cross-training ( HMC-CT )
The HMC-CT method uses a label decompositionprocess to modify the original hierarchical multilabel problem, transforming it into a set of hierarchical single-label problems. In the decomposition process, if the maximum number of labels per instance is N , the original problem is decomposed into N single-label instances participate more than once in the training process. As an example, if a dataset has multilabel instances belonging to the classes c multilabel instances that has the class c class c was originally proposed by [38] for non-hierarchical multilabel classi fi cation.

The label decomposition process o f the Cross-Training method for non-hierarchical classi fi cation is  X  X iology X , all multilabel instances th at belong to the class  X  X iology X  become single-label instances for that class, and the same procedure is adopted for the other classes.

When using multiclass classi fi ers, the number of classi fi ers used in the Cross-Training method is equal to the number of classes assigned to, at least, one multilabel instance. When using binary classi fi ers, however, the number of classi fi ers is equal to the number of classes in the problem. The method allows the original multilabel problem to be recovered from the single-label problems generated. In the example of Fig. 6, three classi fi ers were used, because the three classes  X  X iology X ,  X  X edicine X  and  X  X hysics X  are assigned to a multilabel instance. It is important to notice in the fi gure that the method does not consider all possible cross-combinations of labels. As explained, the number of classi fi ers used is equal to the number of classes assigned to, at least, one multilabel instance. This method is named Additive Cross-Training. If all possible cross-combinations are considered, the method is named Multiplicative Cross-Training.

In the new hierarchical variation of the Cross-Tr aining method proposed here , the label decomposition process is applied to all hierarchical levels, and the local approach is used during the test and training phases. Figure 7 illustrates a label decomposition p rocess performed by the HMC-CT method. In this fi gure, when an instance belongs to more than one class, these classes are separated by a slash (/).
It is important to notice the difference between the HMC-CT and HMC-BR methods. In the HMC-CT method, a classi fi er is not associated with each class. Thus, it does not transform the original problem classi fi ers. Therefore, given a multilabel instance x process occurs twice, fi rst considering x to class c
A problem with this method is its computational cost, which is higher than the cost for the HMC-BR and HMC-LP methods. It is higher because the training process uses each instance several times, since it considers all its classes. 5. Experimental setup 5.1. Datasets
The datasets used in the experiments are related to gene functions of the Saccharomyces cerevisiae (a speci fi c type of Yeast ), often used in the fermentation of sugar for the production of ethanol. This organism is also used in the fermentation of wheat and barley for the production of alcoholic beverages. It is one of the biology X  X  classic model organisms, and has been the subject of intensive study for many years [47].
 The gene functions (classes to be predicted) in these datasets are structured as a tree following the FunCat annotation scheme (http://mip s.gsf.de/projects/funcat) developed by MIPS [30]. These datasets are freely available at http://www.cs.kuleuven.be/dtai/clus/hmcdatasets.html. The FunCatscheme has 28 main categories for the functions, including cellular transport, metabolism and cellular communication. Its tree structure has up to six levels and a total of 1632 functional classes. To reduce computational cost, for each dataset, four classes of the FunCat scheme (01, 02, 10, and 11) were randomly selected at experiments. Tables 2 and 3 show the main characteristics of the reduced datasets. 5.2. Evaluation of the classi fi cation methods
For the experimental evaluation, the real and predicted sets of classes are represented as boolean vectors, where each pos ition repre sents a class in the dataset. If an instance belongs to a class c is used for the predicted set of classes.

The datasets were divided using k-fold cross-validation, with k =5 . Statistical tests were applied to Friedman [23] and Nemenyi [31], which are recommended for comparisons involving many datasets and several classi fi ers [16].
 were those proposed by [40], named Hierarchical Micro Precision and Recall. A combination of these metrics, the Hierarchical-F Precision and Recall metrics have the same importance in the calculation of the Hierarchical-F
These metrics are calculated by computing, for each class, the contribution of the instances erroneously assigned to the class. For such, it is necessary to de fi ne an acceptable distance ( Dis classes, which must be higher than zero. Equations (1) and (2) de fi ne the contribution of an instance x Dis ( c ,c i ) is the distance between two classes in the hierarchy, which is given by the number of edges between these two classes.  X  If x  X  If x
The contribution of an instance x denoted by RCon ( x
The total contribution of False Positives (FP) ( FpCon instances, is de fi ned by Eqs (4) and (5), respectively.
 Precision and Recall for each class, respectively.

The extended values of Hierarchical Precision and Recall (Hierarchical Micro Precision and Recall) are presented in Eqs (8) and (9), respectively, where m represents the number of classes.
According to the value of Dis function is applied to the numerators of the Eqs (8) and (9) to make their values positive. As FpCon | FP i | ,if | TP i | + | FP i | + FnCon i 0 , the numerator max (0 , | TP i | + FpCon i + FnCon i )=0 . The  X  Pr  X CD value can be considered zero in this case. The same rule is applied to the calculation of  X 
The Hierarchical Micro Precision and Recall metrics can then be combined in the Hierarchical-F metric Eq. (10).

In the experiments, the value Dis the calculation of the Micro Precision and Recall metrics. Thus, if the number of edges (in the class as a false positive or false negative. On the other hand, if the number of edges between a predicted class Dis  X  =2 was also used in the experiments reported by [40].

As the objective of the evaluation metric is to consider that closer classes in the hierarchy are more similar to each other, the use of Dis a real class is equal to 2 , this error should not contribute negatively to the metric value, because the metric considers that these two classes are similar. When the distance is larger than 2 , the error should contribute negatively to the value of the metric.
 not counted as errors. However, this is not a problem for the analysis of the results, since all algorithms being compared in our experiments have been evaluated according to the same metric. 5.3. Comparison with other local and global approaches
In the experiments carried out, t he proposed local hierarchical multilabel m ethods were compared against the well-known HMC-Binary-Relevance [49] method, based on the local approach, and the HC4.5 [11] and Clus-HMC [47] methods, based on the global approach. Five ML techniques were used as the base classi fi ers for the local methods: SVM [46], BayesNet [24], Ripper [12], C4.5 [34] and KNN [1].

To compare the local and global hierarchical classi fi cation methods, modi fi cations were needed in the vectors of predicted classes produced by the global methods. In these methods, the membership of an instance x vector of predicted classes obtained using the local and global methods. In the Fig. 8(a), an instance x so on. To assign classes of the vector to an instance, a threshold value can be used. Thus, if a threshold value of 0.6 is chosen, only those classes with a value higher than or equal to 0.6 are assigned to the vectors of predicted classes of the local methods contain only the values 0 and 1 (Fig. 8(b)), indicating if an instance belongs (1) or does not belong (0) to a class.

In this paper, fi ve different threshold values were used in the evaluation of the global methods (0.0, and recall. As the threshold value increases, the precision value tends to increase, and the recall value tends to decrease.

It is important to notice that there is no best threshold value. For example, suppose that after building a given decision tree (using the HC4.5 or Clus-HMC method), a given leaf node has 50 instances belonging new instance, since it can belong to both classes A and B ,onlytoclass A ,oronlytoclass B . There can also be a leaf node with 97 class A instances and 3 class B instances. In this case, instances from class B can represent noisy data or truly rare and important information. Thus, it is dif fi cult to know if the A with probability of 0.97 and in the class B with probability 0.03, or just in the class A . Due to these issues, different threshold values were used.

Since the main goal of this paper is to experimentally compare global and local approaches in a way as controlled as possible, the HC4.5 method used in the experiments was modi fi ed to work as described in [14]. This version of the algorithm includes the restriction of mandatory leaf node prediction ,which is the prediction process used in this work. 5.4. Software tools
The local methods used in this work were implemented using the R language [35], which has many ML-related packages. The e1071 [17] package was used to generate the SVM classi fi ers. The RWeka [26] package was used to induce the classi fi ers BayesNet, Ripper, C4.5 and KNN. 6. Experiments and discussion
Table 4 presents the ranking of the algorithms, considering their average hierarchical f-measure values across the 10 datasets used. The table shows in each row the ranking obtained at a speci fi clevelof the class hierarchy. Bold numbers represent the algorithms that achieved the top three positions in the ranking.

According to Table 4, the HMC-CT method achieved the best overall performance at the fi rst hierar-chical level. At the second level, the best overall performance was obtained by the HMC-LP method, followed by the Clus-HMC and the HMC-BR methods, in this order. At the third and fourth hierarchical levels, the fi rst, second and third best overall performances were achieved by the HMC-LP, Clus-HMC and HC4.5 methods, respectively.

The accuracy results of the experimental comparisons of the local and global approaches, for each dataset, are shown in Tables 5, 6, 7 and 8, each one for a different hierarchical level. The best results, for each dataset, are shown in bold face and the standard deviations are shown between parentheses.
It can be observed in these tables that, in most of the results, the performance of the methods decreases number of classes involved and the smaller the number of instances per class, making the classi fi cation process more dif fi cult. In addition, the error propagation problem is associated with the methods based levels, contributing to decrease the classi fi cation performance at these levels.

The HMC-CT, Clus-HMC and HC4.5 methods made more errors in all class levels, probably due to the large number of class predictions made for each instance. These methods achieved high Micro Hierarchical Recall values and low Micro Hierarchical Precision values. High Recall values and low Precision values can indicate that the methods make more errors because of the increasing number of false positives and the decreasing number of true positives.

Despite the higher number of errors, Table 5 shows that the best performance at the fi rst level was obtained by the HMC-CT method. Although less accurate at the fi rst level, making more errors, the HMC-CT method achieved a better coverage on the instances of the dataset, obtaining a higher Micro Hierarchical Recall value. Thus, this method achieved a better balance between the Micro Hierarchical Precision and R ecall metrics, resu lting in a better pe rformance for the Hierarch ical-F
The smaller number of classes at the fi rst level also seemed to favor the HMC-CT method. When using the data more than once during the training process, in a reduced number of classes (four classes
At the second, third and fourth levels (Tables 6, 7 and 8), the best results were obtained by the HMC-LP method. The evaluation metrics used consider the distances between the classes of the hierarchy, which may have contributed to this best performance.

An analysis of the errors committed by the met hods showed that the HMC-LP method committed more errors in the subtrees rooted at the classes  X 01 X  and  X 10 X . In the datasets, these subtrees have most (the class hierarchy has four levels). The subtrees rooted by the classes  X 02 X  X nd  X 11 X  have fewer classes. Besides, the subtree rooted at the class  X 02 X  does not have classes at the fourth level of the hierarchy.
When an instance x is a false positive for a class c , the false positive and false negative contributions are calculated through the sum of the distances between all real classes of x and the predicted class c assigned to it. When the instance is classi fi ed as a false negative for the class c , the contributions are calculated through the sum of the distances of all classes assigned to the instances and the class c . Therefore, it may be the case that the distances between the predicted and real classes, in the fi nal classi fi cation produced by the HMC-LP method, were smaller than in the other methods. As the metric better classi fi cation performance.

The methods based on the global approach had the worst performance because, depending on the threshold value used, they predicted a larger number of classes than the methods based on the local approach. The Hierarchical Precision and Recall metrics re fl ect the variation in the classi fi cation performance of the methods according to the threshold values selected. In many cases, the global methods obtained values for these metrics higher than values obtained by the methods based on the local approach. The larger number of predicted classes is also a feature of the HMC-CT method. In this method, the training instances are used several tim es due to the class decompos ition process, increasing the number of classi fi ers, and, as a result, increasing the number of predictions.

Despite the worst predictive performance overall, it is possible to observe that, in some cases, the methods based on the global approach achieved better performance than the methods based on the local approach in the last hierarchical levels, especially in comparison with the HMC-CT method. A possible propagated to the deeper levels. This error propagation problem is not present in the methods based on the global approach.

The variation of the threshold values used in the methods based on the global approach is another characteristic that in fl uences their predictive performance. It is possible to see that, in some cases, the use of different thresholds values signi fi cantly in fl uenced the accuracy obtained by the HC4.5 and Clus-HMC methods, as can be seen in the Church and Phenotype datasets. Therefore, the value selected for the threshold may be an important parameter in such methods, and it can be adjusted according to lead to classi fi ers with a higher coverage (recall) of the instances.

It is important to consider that, although the worst values of predictive performance were obtained by the methods based on the global approach, these methods produce a less complex (smaller) classi fi er, which may result in classi fi cation models easier to be interpreted by users. The HC4.5 and Clus-HMC methods both produce decision trees that can be translated into a set of rules, making the model more interpretable for specialists in the problem domain. Such aspects were not taken into consideration in this work, but a multi-objective evaluation could be used to consider the interpretability of the generated classi fi ers.

Although this paper addressed a problem related to functional genomics, there are other application domains which could be investigated, like, for example, human diseases. In fact, examples of HMC problems can be found in many domains, like protein function prediction [2,33,36,47], text classi fi ca-tion [5,21], and image classi fi cation [18]. The multilabel hierarchical classi fi cation methods proposed in this work are generic enough to be applied to all these and other types of problems, in the same sense that a single-label fl at classi fi cation algorithm can be applied to any application domain.
The comparisons of the algorithms X  predictive accuracies using statistical signi fi cance tests are shown in Tables 10, 11, 12 and 13. To facilitate the understanding of the tables, the symbols presented in Table 9areused. 7. Conclusions and future work
This paper investigated the Hierarchical Multilabel Classi fi cation (HMC) problem. In this problem, classes are structured in a hierarchy, where classes can be superclasses or subclasses of other classes, and each example can simultaneously belong to more than one class. These two aspects increase the dif fi culty of the classi fi cation task.

The hierarchical classi fi cation methods investigated in this paperare divided into two approaches: local sibling classes of the hierarchy at each step, using a  X  X ivide and Conquer X  strategy. The local approach allows the development of methods using conventional ML algorithms as base classi fi ers. In this work, three methods based on this approach were implemented. The HMC-BR method, previously proposed in the literature; and two new methods proposed in this work, named HMC-LP and HMC-CT. These methods were evaluated using fi ve ML algorithms as base classi fi ers: KNN, C4.5, Ripper, BayesNet and SVM. Methods following the global approach, unlike the local approach, induce classi fi ers considering conventional ML algorithms. In this work, two existing methods based on this approach were evaluated: HC4.5 and Clus-HMC.

Experiments were performed in order to compare the local and global approaches. As HMC problems are common in bioinformatics, the experiments used datasets related with functional genomics. In par-developed by MIPS, and describe different aspects of the genes in the Yeast genome, like sequence statis-tics, phenotype and expression.

The experimental results were evaluated with the use of speci fi c metrics for HMC problems, based on the distances between the real and predicted classes in the hierarchy. All results were reported separately of the differences in the predictive accuracy performances of the different HMC methods.
The results show that the proposed local HMC methods can obtain predictive accuracy better than, or similar to, the global methods investigated. These results were observed mainly for the HMC-Label-Powerset method proposed in this work.

As future work, HMC methods for non-mandatory leaf node classi fi cation will be investigated. In the classi fi er, without the requirement for the instances to be always assigned to classes represented by leaf nodes.

In the local approach, an important improvement that can be incorporated is a mechanism for the correction of the error propagation problem. This mechanism could detect, at each step of the local The incorporation of such mechanism could improve the predictive performance of the local approach.
Techniques for combining classi fi ers can also be employed to improve the predictive accuracy of methods based on the local approach. Ensemble strategies have been already adopted in the development predictive performance.

Different local approaches can be combined to improve the classi fi cation performance. According to the experiments reported in this work, the best performance at the fi rst level was obtained by the HMC-CT method. Therefore, this method could be combined with the HMC-LP method, through the use of HMC-CT at the fi rst level and the HMC-LP method at the other levels.
 The development of methods based on the global approach is also a promising research direction. Although more complex to develop, these methods usually produce a simpler and more interpretable classi fi cation model than the models induced by methods based on the local approach, especially if the generated model is a decision tree or a set of classi fi cation rules, such as the HC4.5 and Clus-HMC methods used in this work.
 The consideration of other types of hierarchical structures, such as hierarchies structured as Directed Acyclic Graphs (DAG), is also a topic that deserves future research. In DAG structures, a node can have class hierarchies structured as DAGs, modi fi cations are necessary in the classi fi cation methods. Although several metrics have been proposed in the lite rature, many considerations can still be made with respect to their performance. Moreover, new evaluation metrics can be developed, and modi fi cations can be incorporated into the existing metrics.

Finally, in addition to the analysis of different evaluation metrics, one can analyse how different clas-This study can support the improvement of existing methods and the development of new methods. Acknowledgments
The authors would like to thank the Brazilian research councils CAPES, CNPq and FAPESP for their fi nancial support, the Katholieke Universiteit of Leuven X  X  Machine Learning Research Group for the Clus-HMC program and the datasets used, Amanda Clare for the HC4.5 program, and Thiago Cov X es for the codes of the statistical tests.
 References
