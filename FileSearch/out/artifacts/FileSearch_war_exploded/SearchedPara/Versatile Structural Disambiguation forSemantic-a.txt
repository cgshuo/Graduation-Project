 In this paper, we propose a versatile disambiguation ap-proach which can be used to make explicit the meaning of structure based information such as XML schemas, XML document structures, web directories, and ontologies. It can be of support to the semantic-awareness of a wide range of applications, from schema matching and query rewriting to peer data management systems, from XML data clustering to ontology-based automatic annotation of web pages and query expansion. The effectiveness of the achieved results has been experimentally proved and is founded both on a flexible exploitation of the structure context, whose extrac-tion can be tailored on the specific application needs, and of the information provided by commonly available thesauri such as WordNet.
 Categories and Subject Descriptors: H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing  X  linguistic processing General Terms: Algorithms.
 Keywords: word sense disambiguation, semantic web, struc-ture based information.
In recent years, knowledge based approaches, i.e. ap-proaches which exploit the semantics of the information they access, are rapidly acquiring more and more importance in a wide range of application contexts. We refer to  X  X ot X  research topics, like schema matching and query rewriting [9, 15], also in peer data management systems (PDMS) [13], XML data clustering and classification [19, 20] and ontology-based annotation of web pages and query expansion [8, 10], all going in the direction of the Semantic Web  X ... an exten- X 
This work is partially supported by the Italian Council co-funded project WISDOM.
 sion of the current web in which information is given well-defined meaning, better enabling computers and people to work in cooperation X  [4]. In these contexts, most of the pro-posed approaches share a common basis: They focus on the structural properties of the accessed information, which are represented adopting XML or ontology based data models, and their effectiveness is heavily dependent on knowing the right meaning of the employed terminology. For instance, the graph matching algorithm proposed in [16] converts the schemas to be matched into directed labelled graphs, as-sumes that a similarity measure between nodes has been defined and the matching computation essentially relies on a mechanism of similarity spreading. Obviously, the more the similarity measure is able to quantify the semantic closeness of the node X  X  labels, the more the obtained mappings are effective. In the same way, the approach for automatic clas-sification of XML data proposed in [20] views tags as high-quality features of XML documents and exploits their mean-ing in the classification process. Generally speaking, due to the ambiguity of natural languages, terms describing infor-mation usually have several meanings and making explicit the semantics of information goes through the tricky task of deriving from the context the most appropriate meanings. For example, Fig. 1 shows the hierarchical representation desktop PC components of a portion of the categories offered by eBay ,oneofthe most famous world X  X  online marketplaces (nodes are univo-cally identified by their pre-order values). It contains many polysemous words, from string to batteries and memory , to which commonly available vocabularies associates several meanings. The information given by the surrounding nodes allows us to state, for instance, that string are  X  X tringed instruments that are played with a bow X  and batteries are electronic devices and not a group of guns or whatever else.
For most of the knowledge based approaches present in the literature, the problem of making explicit the meanings of words is usually demanded to human intervention and till now machine-based solutions have only been marginally ad-dressed in the context of structure based information. On the other hand, in the most cutting edge semantic-aware ap-plication contexts the role of humans is limited to that of user while, when some kind of human intervention is pro-vided for, unassisted semantic annotation is a quite tedious task.

In this paper, we propose a generic approach for the dis-ambiguation of graph-like structured information, mainly fo-cusing on trees. It can be used to make explicit the meaning of a wide range of structure based information, including XML schemas, the structures of XML documents, web di-rectories, and ontologies. Starting from the lesson learnt in the word sense disambiguation (wsd) field [11], where several solutions have been proposed for free text, we have conceived a versatile approach which tries to disambiguate the terms occurring in the nodes X  labels by analysing their context and by using an external knowledge source. More precisely, starting from a given node, we support several ways of navigating the graph in order to extract the con-text which can thus be tailored on the specific application needs. Moreover, the disambiguation method does not de-pend on training data or extensions, which are not always available. For instance, in a PDMS, peers not necessarily store actual data. We follow instead a different approach: The exploitation of the information provided by commonly available thesauri such as WordNet [17]. In particular, dis-ambiguation is founded on the hypernymy/hyponymy hi-erarchy, as suggested by most of the classic wsd studies, and the sense contexts, extracted from the thesaurus, can be compared against the graph context to refine the results. The outcome of the overall process is a ranking of the plausi-ble senses for each term. In this way, we are able to support both the assisted annotation and the completely automatic one whenever the top sense is selected. The disambiguation approach has been implemented and extensively evaluated through tests performed on three groups of trees differing in the level of specificity and polysemy. Experimental results show the good effectiveness of the proposed approach, also in particularly involved settings.

The rest of the paper is organized as follows: Section 2 presents an overview of our disambiguation approach, while the proper disambiguation algorithm is presented in Section 3. Experimental evaluation is provided in Section 4 and related works in Section 5. Section 6 concludes the paper.
In this section we present the functional architecture of a generic tree disambiguation service (see Fig. 2) and in-troduce relevant terminology. Being trees particular kinds of graphs, without loss of soundness, in the following we will use indifferently the terms tree and graph. Indeed, at the end of the present section, we will show that the service can be straightforwardly extended to graphs. We emphasize that no extension or training data is required for our disam-biguation purposes as they are not always available. The only external source is a thesaurus associating each word with the concepts or senses it is able to express.
The service is able to disambiguate XML schemas, the structures of XML documents, web directories, and, in gen-eral, such information descriptions which can be represented as trees. As a particular case, XML schemas are represented as trees which make explicit the structural relationships be-tween the involved elements, thus capturing the element con-text, and abstract from the complexity of the language syn-tax. The tree contains a set of nodes whose labels must be disambiguated and a set of arcs which connect pairs of nodes and which may as well be labelled (e.g. type , property ). The individuation of the correct sense for each label can be possible by analysing the context of the involved terms and by using an external knowledge source. Arcs are particu-larly important as they connect each label with its context. Each arc label is associated with two weights between 0 and 1 (default value 1), one for each crossing direction (direct and inverse). Weights will be used to compute the distance between two nodes in the graph and the lower the weight of an arc is the closer two nodes connected by such arc are.
The  X  X erms/senses selection X  component in Fig. 2 takes the label of each node N of the tree, extracts the contained terms (which can also be more than one as for instance desktop PC components in Fig. 1) and associates each of these terms ( t, N ) 1 with a list of senses Senses ( t, N )= [ s ,s 2 ,...,s k ]. In principle, such list is the complete list of senses provided by the thesaurus but it can also be a shrunk version suggested either by human or machine experts or as feedback of a previous disambiguation process.

Each polysemous term ( t, N ) is then associated with its context. The context is first extracted from the tree but it does not necessarily coincide with the entire tree. In-deed, different applications require different contexts. For instance, while disambiguating the term string in the musi-cal instruments category of eBay , using categories such as women X  X  clothing would be quite misleading. Thus we support different contexts by means of different crossing set-tings. By default, the nodes reachable by the term X  X  node N through any arc belong to the term X  X  context. The set of crossable arc labels and the corresponding crossing direc-tions is shrinkable, that is it is possible to specify which kinds of arcs are crossable, in which direction and the maximum number of crossings (distance from the term X  X  node). More-over, as we deal with trees, we also provide the possibility of including the siblings of the term X  X  node in the context. The above options can be freely combined. As a special case, let us consider trees having no label on the arcs. It actually represents the conceptual structure of the most common ap-plication contexts such as web directories, XML documents, and XML schemas. When the only crossing direction is the direct one, the context is defined by the descendants or sub-tree of the term X  X  node. Conversely, it is represented by the ancestors. For instance, for the eBay example, one of the best crossing settings is to include ancestors, descendants, and siblings whereas the whole structure would be useful for structures dealing with more  X  X ontextualized X  topics such as book descriptions.

Given a crossing setting, the  X  X raph context extraction X  component in Fig. 2 contextualizes each polysemous term ( t, N ) by extracting its graph context Gcontext ( t, N )from the set of terms belonging to the reachable nodes. Not all
Notice that the same term could be included more than once and that the disambiguation is strictly dependent on the node each instance belongs to. nodes contribute with the same weight to the disambigua-tion of a term. In principle, the more one node is close to the term X  X  node and is connected by arcs with low weights the more it influences the term disambiguation. For this reason, we associate each reachable node N c in the context with aweight weight ( N c ) computed as follows. Given the path from the node N c to the term X  X  node N ,wecountthenum-ber of instances corresponding to each pattern specified in the crossing setting (i.e. arc label and arc crossing direction) and we define the distance d between N and N c as the sum of the product of the weights associated to each pattern and the corresponding number of instances. Then, weight ( N c is computed by applying a gaussian distance decay function defined on d : Thus each element of the graph context is a triple (( t c Senses ( t c ,N c ) ,weight ( N c )) defined from each term t longing to each reachable node N c . For instance, assume that in the eBay example the context is made up of the siblings and ancestors, that the weight of the parent/child arcs is 1 in the direct direction and 0.5 in the opposite one, and that the maximum number of crossings is 2. The graph context of the term ( mouse , 7) is made up of the terms ( computers , 2), ( desktop , 3), ( PC , 3), ( components , 3), ( memory , 4), ( speaker , 5), and ( fan , 6). The distance be-tween node 7 and nodes 2, 3, 4 (5 and 6) are 1 (i.e. 2 arcs crossed in the opposite direction with weight 0.5), 0.5 (i.e. 1 arc crossed in the opposite direction with weight 0.5), and 1.5 (i.e. 1 arc crossed in the opposite direction with weight 0.5 and 1 arc crossed in the direct direction with weight 1), respectively. Then, weight (2) = 0 . 91, weight (3) = 0 . 95, and weight (4) = weight (5) = weight (6) = 0 . 8.
The context of each term ( t, N ) can be expanded by the contexts Scontext ( s )ofeachsense s in Senses ( t, N ). It is particularly useful when thegraph context provides too little information. In particular for each sense we consider the definitions, the examples and any other explanation of the sense provided by the thesaurus. As most of the semantics is carried by noun words [11], the  X  X ontext expansion X  module in Fig. 2 defines Scontext ( s ) as the set of nouns contained in the sense explanation.

Finally, each term ( t, N )withitssenses Senses ( t, N )is disambiguated by using the previously extracted context. The proper disambiguation process is the subject of the fol-lowing Section. The result is aranked version of Senses ( t, N ) where each sense s  X  Senses ( t, N ) is associated with a con-fidence  X  ( s )inchoosing s as a sense of ( t, N ).
The overall approach is quite versatile. It supports sev-eral disambiguation needs by means of parameters which can be freely combined, from the weights to the graph con-text. Moreover, the ranking approach has been conceived in order to support two types of graph disambiguation services: The assisted and the completely automatic one. In the for-mer case, the disambiguation task is committed to a human expert and the disambiguation service assists him/her by providing useful suggestions. In the latter case, there is no human intervention and the selected sense can be the top one. Moreover, the above approach can be straightforwardly applied also to graphs. Indeed, only the context extraction phase accesses the submitted structure whereas the actual disambiguation algorithm is completely independentfrom it. The only problem is in the weight computation where more than one path can connect a pair of nodes. In this case, the one with the lower distance could be selected. In this way, we would be able to disambiguate ontologies written in dif-ferent languages such as OWL and RDF where arc labels are quite frequent (e.g. in RDF arcs can be of subClassOf type or range type or many other types). However, at present, trees are our main focus of interest and, in particular, trees having no label on the arcs which have been subject of our tests, while we plan to deal with general graphs in the future.
The algorithm for disambiguation we devised follows a relational information and knowledge-driven approach. In-deed, the context is not merely considered as a bag of words but other information such as their distance from the poly-semous term to be disambiguated and semantic relations are also extracted. Moreover we use additional information pro-vided by thesauri: The hypernymy/hyponymy relationships among senses and the sense explanations and frequencies of use. In particular, for the disambiguation algorithm we used WordNet [17] which is a fairly comprehensive common-sense thesaurus.

The algorithm is shown in Fig. 3. It takes in input a term ( t, N ) to be disambiguated and produces a vector  X  of confidences in choosing each of the senses in Senses ( t, N ). In particular, given Senses ( t, N )=[ s 1 ,s 2 ,...,s k ],  X  is a vector of k values and  X  [ i ] is the confidence in choosing s as sense of ( t, N ). The obtained confidence vector tunes two contributions (line 11): That of the context, whose weight is expressed by the constant  X  and which is subdivided in the graph context (confidence vector  X  G ,weight  X  )andthe expanded context (confidence vector  X  E ,weight ),  X  + =1, and that of the frequency of sense use in English language (confidence vector  X  U )withweight  X  ,  X  +  X  =1. 2
The terms surrounding a given one provide a good infor-mational context and good hints about what sense to choose for it. The contribution of the graph context is computed from step 1 to step 6. In particular,  X  G is the sum of the values measuring the level of semantic correlation between the polysemous term t and the ones in the graph context Gcontext ( t, N ) (step 4). The contribution of each context term ( t c ,N c ) is weighted by the relative position in the graph of the t c  X  X  node, N c , w.r.t. N (i.e. weight ( N c )). Finally in step 6 the whole vector  X  G is divided by the norm value in order to obtain normalized confidences.
The basis of function TermCorr () (see Fig. 4) derives from the one in [18]. As in [18], the confidence in choosing one of the senses associated with each term is directly pro-portional to the semantic similarities between that term and each term in the context; the intuition behind the similarity is that the more similar two terms are, the more informa-tive will be the most specific concept that subsumes them both. However, our approach differs in the semantic sim-ilarity measure sim ( t, t c ) as it does not rely on a training phase on large pre-classified corpora but exploits the hyper-nymy hierarchy of the thesaurus. In this context, one of the most promising measures is the Leacock-Chodorow [12],
All operations on the vectors are the usually defined ones. which has been reviewed in the following way: sim ( t, t c )= where len ( t, t c ) is the minimum among the number of links connecting each sense in Senses ( t, N ) and each sense in Senses ( t c ,N c )and H is the height of the hypernymy hi-erarchy (in WordNet it is 16). Moreover, we define the min-imum common hypernym c ( t, t c )of t and t c as the sense which is the most specific (lowest in the hierarchy) of the hypernyms common to the two senses (i.e. that crossed in the computation of len ( t, t c )). For instance, in WordNet the minimum path length between the terms  X  X at X  and  X  X ouse X  is 5, since the senses of such nouns that join most rapidly are  X  X at (animal) X  and  X  X ouse (animal) X  and the minimum common hypernym is  X  X lacental mammal X . Obviously these two values are not computed within the function but once for each pair of the involved terms. Eq. 1 is decreasing as one moves higher in the taxonomy thus guaranteeing that  X  X ore abstract X  is synonymousof  X  X ess informative X . There-fore, function TermCorr () increases the confidence of such senses in Senses ( t, N ) which are descendants of the min-imum common hypernym (lines 3-4) and the increment is proportional to how informative the minimum common hy-pernym is (line 5). At the end of the process (Fig. 3, line 6), the value assigned in  X  G to each sense is then the proportion of support it receives, out of the support possible which is kept updated by function TermCorr () (line 6) and in the main algorithm (Fig. 3, line 5).
Beside the contribution of the graph context, also the ex-panded context can be exploited in the disambiguation pro-cess (Fig. 3, lines 7-9). In this case, the main objective is to quantify the semantic correlation between the context Gcontext ( t, N ) of the polysemous term ( t, N )andtheex-planation of each sense s in Senses ( t, N ) represented by Scontext ( s ). In particular, the confidence in choosing s is proportional to the computed similarity value (Fig. 3, line 9). The pseudocode of function ContextCorr () is shown in Fig. 5. It essentially computes the semantic similarity between each term t i in the graph context and the terms in the sense context Scontext ( s ) (lines 3-7) by calling the TermCorr () function for each term t s j in Scontext ( s ) (line 6) and then by computing the maximum of the obtained confidence vector  X  T . The returned value (line 8) is the mean of the similarity values computed for the terms in Gcontext ( t, N ).

The last contribution is that of function decay (), exploit-ing the frequency of use of the senses in English language (Fig. 3, line 9). In particular, WordNet orders its list of senses WNSenses ( t )ofeachterm t on the basis of the fre-quency of use (i.e. the first is the most common sense, etc.). We increment the confidence in choosing each sense s in Senses ( t, N ) in a way which is inversely proportional to its position, pos ( s ), in such ordered list: where 0 &lt; X &lt; 1 is a parameter we usually set at 0 . 8and |
WNSenses ( t ) | is the cardinality of WNSenses ( t ). In this way, we quantify the frequency of the senses where the first sense has no decay and the last sense has a decay of 1:5. Such an adjustment attempts to emulate the common sense of a human in choosing the right meaning of a noun when the context gives little help.

As a final remark, notice that for the sake of simplicity of presentation, algorithm Disambiguate() takes one term at a time. However, for efficiency reasons, in the actual implementation the sim () computation is performed only once for a given pair of terms (also swapped as sim () is a symmetric measure).
In this section we present the results of an actual imple-mentation of our disambiguation approach.
Tests were conceived in order to show the behavior of our disambiguation approach in different scenarios. In particu-lar we tested 3 groups of trees characterized by 2 dimensions of interest. The first dimension, specificity , indicates how much a tree is contextualized in a particular scope; trees with low specificity can be used to describe heterogeneous concepts, such as a web directory, whereas trees with high specificity are used to represent specialized fields such as data about movies and their features and staff. The second dimension, polysemy , indicates how much the terms are am-biguous. Trees with high polysemy contain terms with very different meanings: For instance, rock and track whose meanings radically change in different contexts. On the other hand, trees with low polysemy contain mostly terms whose senses are characterized by subtle shades of mean-ing, such as title . For each feasible combination of these properties we formed a group by selecting the three most representative trees. Group1 is characterized by a low speci-ficity and a polysemy which increases along with the level of the tree; it is the case of web directories in which we usu-ally find very different categories under the same root and a low polysemy at low levels and high polysemy at the leaf level. The trees we selected for Group1 are a small portion of Google  X   X  X  and Yahoo  X  X  web directories and of eBay  X  X  catalog. Group2 is characterized by a high specificity and a high polysemy; we chose structures extracted from XML documents of Shakespeare X  X  plays, Internet Movie Database (IMDb , www.imdb.org ) and a possible On Line Music Shop (OLMS). Finally, Group3 is characterized by a high speci-ficity and a low polysemy and contains representative XML schemas from the DBLP and SIGMOD Record scientific digital libraries and the Dublin Core Metadata Initiative (DCMI , dublincore.org ) specifications. Low specificity and high polysemy are hardly compatible, therefore we will not consider this one as a feasible combination.

Tab. 1 shows the features of each tree involved in our experimental evaluation. From left to right: The number of terms, the mean and maximum number of terms X  senses, the percentage of correct senses between all the possible senses and the average similarities among the senses of each given term in the tree (computed by using a variant of Eg. 1). Notice that our trees are composed by 15-40 terms. Even though they are not particularly big, their composition al-lows us to generate a wide and significant variety of graph contexts. The other features are instead important in or-der to understand the difficulty of the disambiguation task: For instance, higher is the number of senses of the involved terms more difficult will be their disambiguation. The mean number of senses of Group2 and Group3 is almost double than that of Group1, thus we expect their disambiguation to be harder. This is confirmed by the percentage of correct senses between all the possible senses, which can be consid-ered an even more significant  X  X ase factor X  and is higher in Group1. The last feature partially expresses how the trees are positioned w.r.t. the polysemy dimension: the higher is the average of the sense similarity the lower is the polysemy and the different senses have a closer meaning. This is true in particular for Group1 and Group3 trees, confirming the initial hypothesis.
In our experiments we evaluated the performances of our disambiguation algorithm mainly in terms of effectiveness. Efficiency evaluation is not crucial for a disambiguation ap-proach and is beyond the goal of this article so it will not be deepened (in any case, the disambiguation process for the analysed trees required at most few seconds). Traditionally, wsd algorithms are evaluated in terms of precision and recall figures [11]. In order to produce a deeper analysis not only of the quality of the results but also of its possible motiva-tions w.r.t. the different tree scenarios, we considered the precision figure along with a number of newly introduced indicators. Recall parameter is not considered because its computation is usually based on frequent repetitions of the same terms in different documents, and we are not interested in evaluating the wsd quality from a single term perspective.
The disambiguation algorithm has first been tested on the entire collection of trees using the default graph context: all the terms in the tree. Fig. 6 shows the precision results for the disambiguation of the three groups. Three contri-butions are presented: The graph context one (Graph), the expanded context one (Exp) and the combined one (Comb). In general, precision P is the mean of the number of terms correctly disambiguated divided by the number of terms in Figure 6: Mean precision levels for the three groups the trees of each group. Since we have at our disposal com-plete ranking results, we compute precision P(M) at differ-ent levels of quality, by considering the results up to the first M ranks: For instance, P(1) will be the percentage of terms in which the correct senses are at the top position of the ranking. Combination of graph context and expanded context contributions produces good P(1) precision levels of 90% and of over 83% for groups Group1 and Group2, respec-tively. Precision results for Group3 are lower (nearly 70%), but we have to consider the large number and higher simi-larity between the senses of the involved terms; even in this difficult settings, the results are quite encouraging, particu-larly if we notice that P(2) is above 88%. As to the effective-ness of the context expansion, notice that its contribution alone (Exp) is generally very near to the graph context one, particularly in the complex Group3 setting, meaning a good efficacy of this approach too; further, in all the three cases the combination of the two contributions (Comb) produces better results than each of the contributions alone. This is achieved by using optimal values for the  X  ,  X  (0.7)  X  ,and (0.3) weights, as obtained from a series of exploratory tests.
The next step was to evaluate the different behaviors of the trees disambiguation by varying the composition of their terms X  context. We tested an extensive range of combina-tions for all the available trees, with  X  X elected X  contexts in-cluding only ancestor, descendant and/or sibling terms, and discovered two main behaviors: Group1 trees respond well to a more careful context selection, while Group2 and Group3 show an opposite trend. Fig. 7 shows two illustrative com-parisons between complete and selected contexts for Yahoo tree (Group1, Fig. 7-a) and IMDb tree (Group2, Fig. 7-b). Notice that, in the first case, the combined precision P(1) raises from 86% to a perfect 100% for a selected setting in-volving only ancestors, descendants and siblings. This is due to the fact that Group1 concepts are very heterogeneous and including in the context only directly related terms reduces the disambiguation  X  X oise X  produced by completely uncor-related ones. For instance, when the complete Yahoo tree is used to disambiguate the term hygiene in the health cate-gory, the top sense is that related to the health science as the process is wrongly influenced by terms like neurology and cardiology containedinthe medicine category. Instead, whenthetreetermsarespecificandmorecontextualized, such as in the other two groups, the result is the opposite: Notice the IMDb combined precision dropping from nearly 88% to 80% when only ancestors and descendant terms are kept (Fig. 7-b).

Precision figures are the fundamental way to evaluate a wsd approach, however, in our case, we wanted to analyze the results more in depth and from different perspectives. For instance, precision P(1) might be high thanks to the ef-fectiveness of the approach but also for the possibly small number of senses of the involved terms (think of terms with just one sense). In order to deepen our analysis, we com-puted additional  X  X elta X  parameters (see Tab. 2): The left part of the table shows delta values between rank positions, while the right part shows delta values between confidences. Delta rank values express the mean difference between the position in the ranking of the correct sense and that of the last one; we computed them when the right senses appear in the first (rank1 in table), second (rank2) and third (rank3) position. For a given rank, we indicate by a  X - X  the situa-tion where there are no correct senses with that rank. In general, the higher the  X  X elta to last X  rank value is, the harder the disambiguation task should be. At a first glance, Group2 and Group3 confirm their inherent complexity w.r.t. Group1, where rank1 delta values are nearly double. Also notice the very high rank1 delta of some trees, such as the Shakespeare one, meaning that our approach correctly dis-ambiguates also terms with very high number of senses. Fur-ther, we wanted to analyze the actual confidence values and, in particular: How much the right senses X  confidences are far from the incorrect ones, i.e. how much the algorithm is con-fident in its choices (delta confidence to the followings, first column of the right part of the table), and, when the choice is not right, how much the correct sense confidence is far from the chosen one (delta confidence from the top). We see that the  X  X o the followings X  values are sufficiently high (from 14% of Group3 to over 24% of Group1), while the  X  X rom the top X  ones are nearly null, meaning very few and small mistakes. Notice that the wsd choices performed on Group1, which gave the best results in terms of precision, are also the most  X  X eliable X  ones, as we expected.
In Tab. 2 we showed aggregate delta values for each group, however we also found interesting to investigate the visual trend of the delta confidences of the terms of a tree. Fig. 8 shows the double histogram composed bythe delta tothe fol-lowings (top part) and the delta from the top (bottom part) values, where the horizontal axis represents the 21 terms of the On Line Music Shop tree. Notice that for two terms no contributions are present: This is due to the fact that these terms have only one available sense and, thus, their disam-biguation is not relevant. Further, the graph shows that, when the upper bars are not particularly high (low confi-dence), the bottom bars are not null (wrong disambiguation choices), but only in a very limited number of cases. In most cases, the upper bars are evidence of good disambiguation confidence and reliability, with peaks of over 40%.
Up to now we have not considered the contribution of the terms/senses X  feedback to the overall effectiveness of the results, in particular in the disambiguation of the most am-biguous terms in the tree. For illustration, suggesting the correct meaning of the term volume in the DBLP tree as a book helps the algorithm in choosing the right meaning for number as a periodic publication. Moreover, suggesting the correct meaning of the term line (part of character X  X  speech) in the Shakespeare tree produces better disambiguation re-sults, for instance for the speaker term, where the position of the right sense passes from second to first in the ranking. Notice that, in this case, and in many others, the feedback on the term merely confirms thetop sense in the ranking (i.e. our algorithm is able to correctly disambiguate it); nonethe-less, this has a positive effect on the disambiguation of the near terms since the  X  X oise X  produced by the wrong senses is eliminated. The flexibility of our approach allows also to benefit from a completely automatic feedback, where the re-sults of a given run are refined by automatically disabling the contributions of all but the top X senses in the following runs. We can generally choose a very low X value, such as 2 or 3, since the right sense is typically occupying the very top positions in the ranking. For instance, by choosing X =2 in the SIGMOD tree, the results of the second run show a precision increment of almost 17%, and similar results are generally obtainable on all the considered trees.
Before discussing the few approaches proposed for the  X  X tructural X  disambiguation problem, we will first briefly review our disambiguation approach in the more  X  X lassic X  and well studied field of wsd for free text.

The necessity of looking at the context of a word in order to correctly disambiguate it is universally accepted, nonethe-less two different approaches exist: The bag of words ap-proach, where the context is merely a set of words next to the term to disambiguate, and the relational informa-tion approach, which extends the former with other infor-mation such as their distance or relation with the involved word. The disambiguation algorithm developed in this pa-per adopts therelational information approach which is more complex but generally performs much better. In the litera-ture, a further distinction is based on the kind of informa-tion source used to assign a sense to each word occurrence [11]. Our disambiguation method is a knowledge-driven method as it combines the context of the word to be disam-biguated with additional information extracted from an ex-ternal knowledge source, such as electronically oriented dic-tionaries and thesauri. Such approach often benefits from a general applicability and is able to achieve good effectiveness even when it is not restricted to specific domains. WordNet is, with no doubt, the most used external knowledge source [6] and its hypernym hierarchies constitute a very solid foun-dation on which to build effective relatedness and similarity measures, the most common of which are the path based ones [12].

Further, the descriptions and glosses provided for each term can deliver additional ways to perform or refine the disambiguation: The gloss overlap approach [3] is one of them. Among the alternative approaches, the most com-mon one is the corpus-based or statistic approach where the context of a word is combined with previously disam-biguated instances of such word, extracted from annotated corpora [1, 2]. Recently, new methods relying on the entire web textual data, and in particular on the page count statis-tics gathered by search engines like Google [7, 8] have also been proposed. However, generally speaking, the problem of such approaches is that they are extremely data hungry and require extensive training, huge textual corpora, which are not always available, and/or a very large quantity of manual work to produce the sense-annotated corpora they rely on. This problem prevents their use in the application contexts we refer to, as even  X  X aw X  data are not always available (e.g. in a PDMS, peers not necessarily store actual data).
Structural disambiguation is acknowledged as a very real and frequent problem for many semantic-aware applications. However, to our best knowledge, up to now it has only been partially considered in two contexts, schema matching and the XML data clustering, and few actual structural dis-ambiguation approaches have recently been presented. In many schema matching approaches, the semantic closeness between nodes relies on syntactic approaches, such as sim-ple string matching possibly considering its synonyms (e.g. [16, 14]). Also, a good number of statistical wsd approaches have been proposed in the matching context (e.g. [13]). However, as we already outlined, they rely on additional data which may not always be available. As to the proper structural disambiguation approaches, in [20] the authors propose a technique for XML data clustering, where disam-biguation is performed on the documents X  tag names. The local context of a tag is captured as a bag of words contain-ing the tag name itself, the textual content of the element and the text of the subordinate elements and then it is en-larged by including related words retrieved with WordNet. This context is then compared to the ones associated to the different WordNet senses of the term to be disambiguated by means of standard vector model techniques. In a similar sce-nario, the method proposed in [19] performs disambiguation by applying a shortest path algorithm on a weighted graph constructed on the terms in the path from each node to the root and on their related WordNet terms. For the graph con-struction, WordNet relations are navigated just one level. In a schema matching application, [5] presents a node disam-biguation technique exploiting the hierarchical structure of a schema tree together with WordNet hierarchies. In order for this approach to be fully effective, the schema relations have to coincide, at least partially, with the WordNet ones, and this appears as a quite strong requirement.

Generalizing, our approach differs from the existing struc-tural disambiguation approaches as it has not been con-ceived in a particular scenario but it is versatile enough to be applicable to different semantic-aware application contexts. It fully exploits the potentialities of the context of a node in a graph structure and its extraction is flexible enough to include relational information between the nodes and differ-ent kinds of relationships, such as ancestors, descendants or siblings. Further, we fully exploit WordNet hierarchies, and in particular the hypernym ones which are the most used for building effective relatedness measures between terms in free text wsd.
Structural disambiguation is acknowledged as a very real and frequent problem for many semantic-aware applications. As to our knowledge, this is the first work which proposes a versatile approach for the disambiguation of graph-like structured information. Our main aim was to provide a sig-nificant improvement to the semantic-awareness of a wide range of knowledge based applications. The experimental results, showing the very good effectiveness of the approach, are quiteencouraging and induce us to continue in this direc-tion. In our future work, we will deeply analyse the ontology disambiguation problem, also by evaluating the performance on generic graphs, and the feedback process. Moreover, we will consider the use of different and more specific thesauri. [1] E. Amitay, R. Nelken, W. Niblack, R. Sivan, and [2]J.Artiles,A.Penas,andF.Verdejo.WordSense [3] S. Banerjee and T. Pedersen. Extended gloss overlaps [4] T. Berners-Lee, J. Hendler, and O. Lassila. The [5] P. Bouquet, L. Serafini, and S. Zanobini. Semantic [6] E. Budanitsky and G. Hirst. Semantic distance in [7] R. Cilibrasi and P. Vitanyi. Automatic meaning [8] P. Cimiano, S. Handschuh, and S. Staab. Towards the [9] H. Do, S. Melnik, and E. Rahm. Comparison of [10] M. Ehrig and A. Maedche. Ontology-focused crawling [11] N. Ide and J. Veronis. Introduction to the Special [12] C. Leacock and M. Chodorow. Combining local [13] J. Madhavan, P. A. Bernstein, A. Doan, and A. Y. [14] J. Madhavan, P. A. Bernstein, and E. Rahm. Generic [15] F. Mandreoli, R. Martoglia, and P. Tiberio.
 [16] S. Melnik, H. Garcia-Molina, and E. Rahm. Similarity [17] G. A. Miller. WordNet: A Lexical Database for [18] P. Resnik. Disambiguating Noun Groupings with [19] A. Tagarelli and S. Greco. Clustering Transactional [20] M. Theobald, R. Schenkel, and G. Weikum.

