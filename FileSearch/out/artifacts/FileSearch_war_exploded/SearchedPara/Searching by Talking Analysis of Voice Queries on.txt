 The growing popularity of mobile search and the advance-ment in voice recognition technologies have opened the door for web search users to speak their queries, rather than type them. While this kind of voice search is still in its infancy, it is gradually becoming more widespread. In this paper, we examine the logs of a commercial search engine X  X  mobile interface, and compare the spoken queries to the typed-in queries. We place special emphasis on the semantic and syntactic characteristics of the two types of queries. We also conduct an empirical evaluation showing that the lan-guage of voice queries is closer to natural language than typed queries. Our analysis reveals further differences be-tween voice and text search, which have implications for the design of future voice-enabled search tools.

The popularity of search from mobile devices ( mobile search ) has rapidly increased in recent years [34]. In fact, the num-ber of mobile queries has already exceeded the number of those submitted from desktop devices in the United States and other countries [32]. A prominent characteristic of the advancement in mobile search is the emergence of voice search , allowing users to input queries in a spoken language and then retrieve the relevant entries based on system-generated tran-scriptions of the voice queries [19]. Recent developments in speech recognition, backed by high bandwidth coverage and high-quality speech signal acquisition, are enabling higher quality voice search [8]. Already in 2010, Google presented a case study stating that their goal is to make voice search ubiquitously available and that a level of performance was achieved such that usage is growing [30]. Since then, fur-ther enhancements to automatic speech recognition (ASR) for web search have been reported [31, 43], taking advantage of the large data that started to accumulate on voice search logs [8], and applying advanced learning methods [17]. The use of voice has also been promoted by the increasing pop-ularity of voice-activated intelligent assistants, such as Siri, Google Now, and Cortana. These assistants provide context-based query-less personalized advice for mobile users, but also enable web search [18]. A recent survey of 1400 U.S. smartphone users found that 55% of the teenagers use voice search every day [3, 14]. It is therefore becoming important for information retrieval researchers and practitioners to un-derstand this new medium of search and its differences from traditional text search .

Using voice as a means to search holds various potential advantages. Although typing usability has improved in re-cent years, querying by voice is still likely to be substantially easier and faster for the vast majority of mobile users. For users with visual or manual impairment, or with limited lit-eracy skills, voice search may break down the entry barrier into web search. In addition, as searching by voice does not require visual attention or the use of hands, it can be per-formed in situations such as driving, cooking, or exercising, where typed search might be especially cumbersome, error-prone, and even dangerous. In the aforementioned survey, 78% of the teens who used voice search pointed out its use-fulness for multitasking as a key motivating factor [14].
In spite of its growing popularity, the area of voice search has not received much attention in the IR literature. Early work compared voice and text queries in a laboratory study, however these did not represent typical web search queries, but rather complex long questions [10]. More recent work has mostly focused on voice recognition [1, 8, 27, 31, 38, 43] and query reformulation [3, 19, 33]. A few studies revealed more details about how voice search is performed on com-mercial search engines [30, 40], however we are not aware of a systematic log analysis of voice queries as of yet.
In this work, we perform a query log analysis of half a mil-lion voice queries, issued to the mobile application of a com-mercial web search engine, over a period of six months. The log includes English-only queries, from the United States, transcribed from voice to text using high-quality ASR. We compare the voice queries with a similar-size sample of mo-bile text queries, typed on the same mobile application. Our comparison inspects characteristics of context, clicks, ses-sions, and, primarily, the query text itself. We examine both semantic and syntactic features and compare them for voice versus text queries. In the final part of our analy-sis, we directly compare the similarity of the voice and text query language to natural language corpora, which include traditional news articles and the titles of questions in a large community question answering (CQA) website.
Our work has the following key contributions:  X  To the best of our knowledge, we present the most com-prehensive analysis of a web search engine voice query log.  X  We combine a semantic analysis using novel methods, such as analyzing a broad set of triggered cards, with an in-depth syntactic analysis, to shed more light on the common and different between voice and text queries.  X  We provide empirical evidence, based on language mod-eling, that voice queries are closer to natural language than text queries, yet are still distant from natural ques-tion language.
 Our findings suggest different ways for search systems to en-hance their support and take advantage of the unique char-acteristics of voice queries. We conclude the paper by sum-marizing the key findings and discussing their implications and future research directions.
Studies of mobile query log analysis have been published throughout the past decade, ever since mobile devices be-came ubiquitous. One of the early studies [20] compared search patterns on 12-key keypad cellphones, PDAs, and desktop (PC) computers. It found that the diversity of queries on mobile was substantially lower than on desktop and that the most popular query in each of the three device types was different. Baeza-Yates et al. [4] compared mobile and desktop search queries on Yahoo Japan and found that mobile queries included fewer characters, more queries in the Business category, and fewer in Art. Yi et al. [41] performed a large-scale query log analysis of the Yahoo OneSearch mo-bile service, and found that mobile query patterns were dy-namic, as users were exploring how to use the devices. With the evolution of mobile devices into smartphones, mobile search has also been shown to change. Kamvar et al. [21] examined search behavior on iPhones and found it was more similar to desktop search than to search on basic mobile phones. Song et al. [34] performed a broad 3-month log anal-ysis of Bing search on desktop, iPad, and iPhone. Due to the significant differences between user search patterns on the three platforms, they proposed a ranking system that considered platform-specific features.

With the advancement of speech recognition technologies, studies of mobile search using voice started to emerge. Many of the studies focused on voice recognition challenges. Wang et al. [38] defined voice search as  X  X he technology underlying many spoken dialog systems that provide users with the in-formation they request with a spoken query X  , and reviewed key challenges, such as environmental noise, pronunciation variance, and linguistic issues. Acero et al. [1] described the architecture of the speech recognition interface of  X  X ive Search for Mobile X . Moreno-Daniel et al. [27] discussed the interleaving of ASR with IR systems and suggested to com-bine acoustic and semantic models to enhance performance. In recent years, alongside the enhancement of ASR tech-nologies with deep learning [17], various studies suggested advanced methods for voice search ASR and reported fur-ther performance enhancements. Chelba et al. [8] leveraged the data on Google X  X  voice search logs to enhance language modeling and achieved  X  X mall but significant X  gains in speech recognition performance. Shan et al. [31] described a system for Mandarin Chinese voice search and reported  X  X xcellent performance on typical spoken search queries under a variety of accents and acoustic conditions. X  Zweig and Chang [43] found that the use of Model M (exponential n-gram language model) with personalization features improved the speech recognition performance on Bing voice search. In this work, we take advantage of the advancement in speech recognition, to explore a high-quality transcribed query log, but do not delve into speech recognition aspects.

Some of the recent work has focused on voice query re-formulation, showing that users sometimes respond to voice recognition errors by different reformulation patterns, such as repeating a query or refining it [19]. Classifiers were built to predict and categorize voice reformulations, extending text-based approaches with features such as voice recogni-tion time and confidence [3]. Researchers also found that users do not tend to switch between voice and text when reformulating queries [33]. While our study does not focus on query reformulation, we report related statistics for voice versus text queries in our session analysis.

Most closely-related to our research are three studies that directly referred to the comparison between voice and text queries. The first described a case study of the develop-ment of  X  X oogle Search by Voice X  [30]. While most of the report is focused on describing the technology and the eval-uation of the voice recognition component, a section is ded-icated to evaluating the user experience based on a 4-week query log analysis. It was found that the query categories  X  X ood &amp; drink X  and  X  X ocal X  (e.g., place names or business listings) were more popular with voice searches. Also, short queries, in particular 1-and 2-word queries, were relatively more frequent in voice searches than in typed searches, while longer queries (5+ words) were far rarer. A poster by Yi and Maghoul [40] inspected the change in mobile search on Yahoo from 2007 to 2010 and provided a short comparison of 79K voice queries to typed mobile and desktop queries, which examined query length and categories. The most com-prehensive comparison between voice and text queries was performed in a lab study from a decade ago (pre-smartphone era) [10]. The 12 participants were students from the local research lab, as voice search was in its infancy and required IR experience. They were asked to formulate 10 TREC top-ics as queries in a process that took 1 to 3 minutes per query. The resulting queries did not reflect typical web queries and were complex and long (23.1 words for an average voice query, 9.5 for text). Moreover, the study did not involve a search system and participants were not exposed to search results. In addition to query characteristics, such as length and typing duration, the retrieval effectiveness of typed ver-sus spoken queries was evaluated. In our analysis, when relevant, we tie to the results reported in these three studies and discuss the common and different with our own findings.
Our analysis is based on a random sample of 500,000 queries from the Yahoo mobile search application, performed by over 50,000 unique users of the voice interface along a period of exactly six months (April-October 2015) in the United States. The mobile search application transcribes a voice query into a text query using state-of-the-art ASR, and from this point onward treats it as a text query. In other words, the multi-modal interface allows inputting queries by voice, but returns results using the same information re-trieval techniques and the standard mobile search user in-terface. For comparison, we collected an identical number of queries performed using the  X  X egular X  keyboard-based in-terface of the same mobile application. We refer to the for-mer set of queries as voice queries and to the latter as text queries . The text queries were collected along the same pe-riod of six months for a similar number of users. Moreover, we sampled an identical number of voice and text queries in each day of the experimental period. When inspecting day-of-week distribution and session statistics, we compared all queries from all users in our voice sample with all queries from all users in our text sample, during two months of the experimental period, to allow suitable analysis.

Each query in the log, either voice or text, included, in addition to the query itself, a timestamp (adapted to the timezone in which it was performed), a location in the form of city and state, and, for logged-in users, the user X  X  age and gender. In addition, for each query we had information about its associated clicks, if any were performed, including the corresponding URLs and ranks within the search results page (SERP).

Our analysis is organized as follows. Section 4 compares basic characteristics of voice and text queries, including con-text, query length, and session characteristics. Section 5 examines the query semantics by inspecting different cate-gories as well as specific queries and terms. Section 6 looks into click behavior and distribution of clicked domains, re-flecting on the findings in the query semantics analysis. Sec-tion 7 examines query syntax as reflected in characteristics of parsing and distribution of part-of-speech tags. The se-mantic and syntactic analyses reveal various differences be-tween voice and text queries, of which many indicate that voice queries are phrased closer to natural language than text queries. The final part of our evaluation therefore ex-plicitly compares the similarity of both types of queries to natural language corpora, and is described in Section 8.
In this section, we compare basic characteristics of voice and text queries, including context aspects, basic query fea-tures, and session characteristics.
We found similar contextual characteristics for voice and text queries in terms of searcher X  X  age and geography (cities and states). There was a slight tendency towards male searchers in the voice log compared to the text log (up 3%).
The distribution across day-of-week was similar for voice and text queries: in both, there was a slight peak on week-ends compared to weekdays (5% more searches on average). In contrast, there was a noticeable difference between voice and text queries with regards to time-of-day, as depicted in Figure 1. Voice queries were more frequent during day hours (from 8am to 8pm), while higher portions of the text queries (relative to voice) were performed during evening, night, and early morning hours (8pm to 8am). These differences were consistent during weekdays and weekends.

In our analysis, we inspected the results while controlling for factors that were found to be different between voice and text queries, including time-of-day and gender. When rele-vant, we report the influence of these factors on the results.
The average query length was significantly higher for voice queries at 4 . 2 (std: 2 . 96, median: 4, max: 109) versus 3 . 2 for text (std: 2 . 38, median: 3, max: 308). Query length was measured by the number of words, using white-space tok-enization. The substantially higher maximum value in text queries is likely due to the use of the copy-paste feature, which does not exist for voice. Figure 2 shows the detailed distribution of voice versus text queries by length. It can be seen that one-word queries were particularly rarer on voice (12 . 2% vs. 21% for text), perhaps implying a lower portion of navigational queries. Voice queries were more common starting at queries of 5 words, which have been recently re-ferred to as  X  X erbose X  queries [15]. Overall, 34 . 5% of the voice queries were of 5 words or more, compared to only 21 . 2% of the text queries. The length difference between voice and text queries has a major effect on query syntax, which we examine more closely in Section 7.

Previous work was somewhat inconsistent with regards to voice versus text query length. While a Google case study found that voice queries tend to be shorter than typed queries (2 . 5 versus 2 . 9 on average, respectively) [30], other studies found voice queries to be longer (e.g., 3 . 4 versus 2 . 2 on a Yahoo study) [10, 40]. Jiang et al. [19] pointed out this discrepancy and stated that  X  X urther studies are needed to identify the characteristics of queries in voice search. X  . Our findings evidently support voice queries being substan-tially longer than text queries (while also indicating a gen-eral trend of queries becoming longer on mobile search).
The portion of unique queries for voice was 73 . 9%, com-pared to 77 . 6% for text. This stands somewhat in contrast to voice queries being longer, as we would expect more repe-tition for shorter queries. We believe this finding stems from two main reasons. First, the lower query diversity for voice search characterizes search in its earlier stages, as has been the case for web and mobile search [40]. Second, the use of abbreviations, spelling variants, and punctuation marks, makes text queries more diverse. We will further demon-strate this in Section 5.
A session is a series of queries issued by an individual user in close succession, often with all queries being related to the same topic. Using a common approach for defining sessions [35], we considered queries that occur in a sequence without 15 minutes of inactivity as part of the same session. Table 1 shows session statistics. The average session length on voice and text queries was very similar, with higher stan-dard deviation for voice. About two thirds of the sessions, on both voice and text, included only one query. Average and median idle times were shorter on voice queries, likely as a result of the faster inputting enabled by voice. This gap may also reflect the fact that voice queries often focus on topics that require little interaction with the results, as our analysis will later demonstrate. Finally, we inspected the relation of a query in a session to its previous query (when such exists): the bottom of Table 1 shows the portion of identical and refining queries (a query q2 refines a query q1 if q1 is a prefix of q2, but not identical to q2). Overall, the numbers are similar, with somewhat higher portions of identical queries on voice and refining queries on text.
In this section, we compare voice and text query seman-tics. We first examine higher level query categories by in-specting the triggering of vertical cards. We then examine the query themselves more closely, by comparing the most popular queries and the most distinctive query terms.
The recent evolution of web search has introduced richer experience of the SERP, with cards (also referred to as  X  X neboxes X  or  X  X irect displays X ) showing results of verticals such as weather, direct factual answers, or live sport scores [32]. These cards extend organic search results ( X  X en blue links X ) by addressing a user X  X  specific information need directly on the SERP [9], often sparing the need for a click [23]. Commercial search en-gines trigger cards upon the identification of a relevant user X  X  intent. The card triggering technology largely relies on query pattern matching that provides high precision, since the pre-sentation of a wrong card might substantially degrade the user experience and is therefore highly undesirable. In the following analysis, we examine the distribution of presented cards for voice versus text queries, relying on the search engine X  X  technology for card triggering to shed light on se-mantic differences between the two types of queries. The card triggering technology allows us to build on the ability to capture clear and specific user intent based on different patterns of language. For example, the dictionary card is definition X , or  X  X eaning of &lt; term &gt;  X .

Overall, the portion of queries for which at least one card was triggered was 43 . 3% for voice queries and 40 . 6% for text. The higher portion for voice may reflect a more frequent use for information needs that can be directly satisfied on the SERP and require less interaction from the user.

Table 2 presents the  X  X riggering ratio X  for 16 different cards (i.e., the ratio between the portion of voice queries for which the card was presented and the portion of text queries for which it was presented). We excluded very broad cards (news, shopping, and digital magazines) and cards that re-flect a vague or ambiguous intent (e.g., company, which also includes websites such as Facebook or Amazon). The left side of the table shows the ratio for frequent cards, trig-gered for at least 0 . 5% of both voice and text queries, while the right side focuses on narrower less common cards that still appeared for at least 0 . 1% of either voice or text queries. Among the frequent cards, music videos, which are triggered by queries such as song names and singer names, were the most common for voice compared to text (highest trigger-ing ratio). CQA cards, which include direct (inline) answers from community question answering sites, recipe cards, and map cards, were also more common for voice. On the other hand, sports and people cards were considerably more com-mon for text queries. The latter is a particularly common card, triggered for celebrity names and sometimes a refining keyword such as age or height.

Inspecting specific queries that refer to popular celebrities, we indeed observed a ratio smaller than 1 between voice and text, which becomes particularly low for queries that only include the celebrity X  X  name, without any refinements. For example, the voice/text ratio for queries that included  X  X e-bron james X  was 0 . 85, but this ratio decreases to 0 . 57 when only considering the exact query  X  X ebron james X . Similarly, for  X  X ruce jenner X  these ratios were 0 . 97 and 0 . 47, respec-tively; for  X  X im kardashian X  0 . 63 and 0 . 48; and for  X  X onald trump X , 0 . 83 and 0 . 42.

Reviewing the less frequent cards, on the right side of table 2, the time card was largely more common on voice queries, with a triggering ratio of over 5 . 5. This card is typically triggered by queries that ask for the time in a specific loca-tion (e.g.,  X  X avanna time X  or  X  X hat is the hour in chicago X ). Countries (country names, sometimes with refinements such as  X  X apital of X  or  X  X opulation X ), dictionary, and weather, were also more popular on voice, whereas lottery and even more so horoscope were more popular on text. Overall, we see that many of the infrequent cards are more commonly triggered for voice. Additionally, it appears that cards with concise answers (time, definition, weather) are more com-monly triggered for voice queries, while cards that require higher user engagement, as they present richer content or more likely to require interaction (horoscope, lottery), are more commonly triggered for text queries.
 Some of the findings in this section coincide with the Google case study [30], which identified  X  X ood &amp; drink X  and  X  X ocal X  as the more popular categories for voice (out of a to-tal of 8), corresponding with our findings w.r.t the  X  X ecipe X  and  X  X aps X  cards. That study also found that the  X  X nline communities X  and  X  X dult X  categories were less frequent for voice, to which we show support in the next section.
Table 3 shows the most popular queries for text versus voice (popularity is measured by the number of unique users who issued the query at least once). The top query in each list is already different:  X  X acebook X  for text versus  X  X outube X  for voice. The difference between the two is substantial: on text queries,  X  X acebook X  was issued by a number of users larger by a factor of 1 . 7 than the number of users who queried for  X  X outube X , while on voice it was issued by less than half (0 . 44). It can also be seen that adult site queries ( X  X orn X ,  X  X nxx X ,  X  X edtube X ) appear only on the top text list. On the other hand, the voice list includes more retail brands ( X  X almart X ,  X  X ome depot X ) and the query  X  X ello X , which is likely used for experimenting with the voice system. In ad-dition, the voice list includes the use of the suffix  X .com X  for popular navigational queries, e.g., both  X  X ahoo X  and  X  X a-hoo.com X  are on the top list for voice (and similarly for  X  X ma-zon X ). Across all queries, however, the  X .com X  suffix was less common on voice than on text (ratio of 0 . 6), likely due to substantially sparser use of full URLs on voice queries: the prefixes  X  X ww X  and  X  X ttp X  appeared much more commonly on text queries, with a voice/text ratio of 0 . 07 and 0 . 01, respectively.
To further inspect semantic differences, we set out to ex-plore which terms mostly characterize voice versus text queries. To this end, we used Kullback-Leibler (KL) divergence, which is an asymmetric distance measure between two given distri-butions [6]. Specifically, we calculated the terms that con-tribute the most to the KL divergence between the voice and text query language models, for unigrams, bigrams, and trigrams 1 . Table 4 reports the terms with the highest KL divergence for text queries (w.r.t voice queries) and for voice queries (w.r.t text). Inspecting the unigrams, the terms on the voice list mostly include common function words (deter-miners, prepositions), question words, and pronouns. The only two nouns on the list are  X  X nd X  and  X  X umber X . For  X  X nd X , closer inspection verified that this is due to the ASR often confusing it with the more common  X  X nd X  (e.g.,  X  X an i eat onions end garlic while breast-feeding X  or  X  X he preacher end the bear song X ). The text unigram list, on the other hand, includes site names (especially social media and adult) and common abbreviations (states, e.g.,  X  X x X ,  X  X a X ,  X  X c X ; and also  X  X t X ,  X  X s X ;  X  X r X ), which are hardly ever used on voice.
For bigrams and trigrams, the text lists include website and entity names, sometimes with extending keywords such as  X  X ign up X ,  X  X ogin X ,  X  X nline X ,  X 2015 X ,  X  X ews X , or  X  X cores X . On the other hand, the voice list includes many common parts of natural language ( X  X hat is X ,  X  X n the X ), requests phrased in natural language ( X  X how me X ,  X  X ake me to X ,  X  X  X  X  looking for X , We elaborate on the smoothing method in Section 8.
  X  X hone number for X ), and also a few state names that appear in their standard form ( X  X ew york X ,  X  X orth carolina X ).
Finally, we also used the KL analysis to examine distinc-tive unigrams positioned at the beginning and at the end of a query. The most distinctive words to open a voice query were the question words  X  X hat X  and  X  X ow X  and the most dis-tinctive for text queries were the site names  X  X ornhub X  and  X  X acebook X . The most distinctive word to terminate a voice query was  X  X lease X , again indicating the use of natural lan-guage, while for text it was  X 2015 X , perhaps as it is easy to write but relatively long to pronounce.

The difference in use of question words is one of the most prominent between voice and text queries. A recent study examined this form of  X  X uestion queries X , which  X  X ake the form of natural language X  [39]. We used a similar methodol-ogy to identify this type of queries for voice and text. Over-all, 9 . 9% of the voice queries begin with a wh-word (one of the 5W1H), compared to only 3 . 7% of the text queries (ra-tio 2 . 67). Adding yes/no questions (start with  X  X oes X ,  X  X id X ,  X  X an X , etc.), the portions grow to 11 . 9% and 4 . 7%, respec-tively (ratio 2 . 55). The two most popular question words, by a large margin, were  X  X ow X  (3 . 6% of all voice queries) and  X  X hat X  (3 . 5%); while for  X  X hat X  the voice/text ratio was 3 . 1, for  X  X ow X  it was lower at 2 . 3. The lowest ratio among the 5W1H was for  X  X hy X  queries at 1.8 (these queries account for 0 . 4% of all voice queries), probably as these are more open-ended questions, often characterized by longer answers that require more exploration on the part of the user [37]. On the other hand, common prefixes for factoid questions were substantially more common on voice, e.g.,  X  X ow old is X  (ratio 5 . 74) or  X  X ho is the X  (5 . 36) [16].

Thus far, we have seen many quantitative characteristics by which voice queries differ from text queries. Next, we show a few anecdotal examples of voice and text queries used to perform a semantically-similar search. To this end, we in-spected queries in our voice sample that landed (i.e., resulted in a click) on CQA pages, as these often reflect a specific in-formation need. For such queries, we matched text queries that landed on the same CQA page during a period of one week (our text sample did not contain such matches, thus we had to inspect a larger log). Table 5 presents seven exam-ples of voice and text query pairs that landed on the same CQA page and express a similar information need. These examples nicely demonstrate some of the findings pointed out during this section. We note, however, that we cherry-picked examples where the voice query was especially differ-ent from the corresponding text query. In other cases, voice queries were similarly phrased to text queries. For instance, Table 5: Example voice and text queries that landed on the same CQA page.
 inspecting our original samples, 13 . 1% of the voice queries were completely identical to a query in the text sample.
Table 6 shows the ratio for various click characteristics between voice and text queries 2 . It can be seen that the click-through rate (CTR; the portion of queries for which at least one click was made) and average number of clicks per query were substantially lower for voice queries. We conjecture that voice queries are often conducted in a situ-ation that allows less interaction with the device, including clicking on search results. The mean reciprocal rank (MRR) across all queries is also substantially lower for voice queries. The MRR across clicked queries only is similar for voice and text queries, indicating that the difference in the general MRR is mostly due to the lower CTR of voice queries.
Inspecting the clicked domains (a domain is determined by the  X  X ost X  part of the clicked URL), the portion of unique do-mains out of all clicks is substantially lower for voice queries, indicating lower diversity. Further analysis indicated that a higher portion of the voice clicks are performed on top do-mains, determined using a list of the 100 most commonly-clicked domains during our experiment X  X  period.

Table 7 shows the top clicked domains for text and voice queries. The rightmost column shows, for each of the top voice domains, the  X  X lick ratio X , i.e., the ratio between its number of clicks on the voice query sample and number of clicks on the text query sample. Differences emerge between the two lists from their top. While the most clicked domains for text queries are Wikipedia and Facebook, they are only 2nd and 5th, respectively, on the voice click list, with low click ratios, especially for the social network, at 0 . 51. In-stead, the top of the voice query list is dominated by video domains: video.search.yahoo.com, with more than double the clicks as on text queries, and popular video sharing site We cannot disclose actual values due to business sensitivity. Youtube. Also higher on the voice list are CQA sites, such as Yahoo Answers at 4th (6th on text) and Answers.com at 10th (not among the top 12 for text). On the top voice list only, with high click ratios, are also maps.yahoo.com and Yellowpages, reflecting more specific information needs (we have already seen  X  X hone number X  is a distinctive term for voice queries). Another evident difference is with adult sites: while four (Pornhub, Xvideos, Xnxx, and Redtube) are on the top text domains, only two make the top voice domains, with low click ratios. As we saw, text queries are more pop-ular during night hours, which could explain the difference. Inspecting the portions of adult site clicks by the hour of the day, we indeed observed a sharp increase during night hours compared to day hours, however, the gap between text and voice clicks persists throughout all hours of the day.
Further inspecting the lists of top clicked domains revealed differences towards voice queries in other CQA sites (e.g. wikiHow with a click ratio of 1 . 3), maps (MapQuest 1 . 75; ap-pearances of  X  X aps X  within all clicked domain strings with a ratio of 1 . 74), weather (AccuWeather 2 . 22;  X  X eather X  string 1 . 48), dictionary (dictionary.reference.com 1 . 43; thefreedic-tionary.com 1 . 75,  X  X ictionary X  1 . 35), recipes (allrecipes.com 1 . 58,  X  X ecipe X  1 . 53), video streaming (screen.yahoo.com 1 . 97;  X  X creen X  1 . 77), and music (iTunes 1 . 88). On the other hand, more dominant on text queries were shopping sites (eBay 0 . 92, Craiglist 0 . 54,  X  X hopping X  0 . 84), health (WebMD 0 . 88, drugs.com 0 . 77, nih.gov 0 . 64), news (news.yahoo.com 0 . 82,  X  X ews X  0 . 7), sports (sports.yahoo.com 0 . 72, espn.go.com 0 . 8,  X  X port X  0 . 83), finance (finance.yahoo.com 0 . 48,  X  X inance X  0 . 56,  X  X ank X  0 . 89), celebs (celebs.yahoo.com 0 . 63,  X  X eleb X  0 . 81), and social network sites with a particularly low ratio be-tween voice and text clicks (Twitter 0 . 17, Linkedin 0 . 37). Despite the differences in favor of voice clicks for audio and video results, there was no such difference for photo sites (Photobucket 0 . 97, Pinterest 0 . 65,  X  X hoto X  0 . 87).
In Section 4.2, we saw that voice queries tend to be longer than text queries. In this section, we delve deeper into syn-tactic analysis of voice versus text queries. Our analysis includes two parts: we first inspect the characteristics of syntactic parsing of both types of queries and then examine the distribution of key part-of-speech (POS) tags. In our analysis, we used two corpora for additional comparison: the first is the Wall Street Journal (WSJ) corpus (sections 2-23) [26], which primarily includes business and financial news articles, broken into sentences (a total of 42,248 sen-tences). The second is a collection of 500,000 question titles in English, randomly sampled from the Yahoo Answers CQA website. We only considered question titles of one sentence (over 90% of all titles on the site).
For this analysis, individual sentences from each of the four corpora  X  WSJ, question titles, voice queries, and text queries  X  were tokenized, pos-tagged, and syntactically parsed using the Stanford parser 3 . The parser first generates an unlexicalized PCFG parse [22] and then produces typed de-pendencies by matching patterns on CFG trees [11].

Table 8 reports four measures of syntactic complexity (four middle columns) for each of the four corpora. The first col-umn shows the median and average number of tokens per parsed item 4 . The second column depicts the median and mean dependency tree depth, defined as the number of edges in the longest path from the root node to a leaf in the tree. The third and fourth columns present the fraction of depen-dency tree root edges that go to tokens POS-tagged as nouns or verbs, respectively. We use these two measures as proxies to the syntactic category of the input text, with noun roots often indicating simple noun phrases and verb roots often indicating more complex syntactic forms [28]. Finally, the rightmost column of Table 8 presents the median and av-erage length-normalized log probability score of the PCFG parse, which serves as a proxy for grammaticality (a more negative score reflects a lower probability of the parse).
These results indicate substantial differences between voice and text queries: voice queries have more tokens, higher tree depth, higher portion of root nodes that govern a verb, lower portion of root nodes that govern a noun, and higher parse score. All of these differences make voice queries more simi-lar to question titles (which are, in turn closer to news arti-cles), relative to text queries. This analysis suggests that on the scale where text queries are at one extreme (shorter, less grammatical) and natural-language news articles are at the other (longer, better-formed), voice queries are positioned somewhere in-between text queries and question titles.
The second part of the query syntax evaluation focuses on the distribution of part-of-speech tags using the Stanford POS tagger 5 [36]. In our analysis, we removed all punc-tuation tokens. Mobile queries in general include very few punctuation marks: only 0 . 7% of the text tokens and 0 . 3% of the voice tokens were punctuation marks, compared to 12 . 4% and 12 . 8% for question titles and WSJ, respectively (largely due to the use of question marks at the end of ques-tion titles and periods at the end of WSJ sentences). We worked with a lower-case version of all four corpora, as all the voice queries and the vast majority of text queries were lower case in their original form.

Table 9 displays the portion of primary POS tags (as the portion out of of all tokens in the corpus) for the four cor-pora. The first row refers to nouns, which are prevalent in queries, as previously shown [5]. Yet, for voice queries the portion of nouns is substantially lower than for text queries http://nlp.stanford.edu/software/lex-parser.shtml The number of tokens is slightly higher than reported in Section 4.2, due to the use of the Stanford tokenizer instead of white-space tokenization. http://nlp.stanford.edu/software/tagger.shtml (52 . 4% vs. 64 . 3%), although still considerably higher than for WSJ (34 . 5%) and question titles (30 . 6%). Adjectives are also somewhat more common for queries, with similar por-tions for text (9 . 9%) and voice (9 . 6%). For the other parts of speech, it can be seen that the portions for voice queries are higher than text queries, and closer to the portions for titles and WSJ, although not quite as high. These differ-ences are consistent across all five lower POS types in the table, and especially salient, with more than a double ratio between voice and text, for determiners and pronouns.
The richer language used in voice queries is largely due to their length: as we saw, voice queries are a token longer on average than text queries. Yet, differences also emerge when comparing queries of the same length. Table 10 shows the POS tag portions for voice versus text across queries of 2-7 tokens. There is a clear general trend in POS distribution by query length: the portion of nouns decreases as the num-ber of tokens increases, the portion of adjectives remains stable, while the portion of other POS types increases with the length of the query. For queries with a fixed length, dif-ferences can still be observed between voice and text in the number of nouns, on the one hand, which is higher for text queries, and verbs, prepositions, determiners, pronouns, and adverbs, which are higher for voice queries. These differences somewhat diminish as query length grows, yet such queries are quite rare, especially for text (e.g., only 2 . 7% of the text queries are of 7 tokens). The differences for determiners and pronouns remain solid even for queries of 7 tokens. Over-all, we see that even for queries of the same length, there is a difference in POS distribution between voice and text queries, with more diverse language used in voice.
Our analysis so far has revealed semantic and syntactic differences between voice and text queries. In this section, we set out to validate that the language of voice queries is indeed closer to natural language than text queries. To this end, we built two natural language models (LMs). The first is based on the WSJ corpus (sections 2-23, 42,248 sentences). The second, marked QT, was built based on a random sam-ple of 50M question titles from the Yahoo Answers CQA site, posted between 2006 and 2015. Yahoo answers is a large and diverse website, acting not only as a medium for sharing technical knowledge, but as a place where one can seek advice, gather opinions, and satisfy curiosity about a wide variety of topics [2]. We opted to use these two corpora since one (WSJ) represents classic formal language, com-monly used in natural language processing research, while the other (QT) represents a more up-to-date web language contributed by the  X  X rowd X  in order to ask questions. We built unigram, bigram, and trigram LMs, with Jelinek-Mercer smoothing [42],  X  =0 . 8, as learned throughout our experiments 6 . Smoothing unknown unigrams was done us-ing the standard of 1 over the vocabulary size.

For measuring the similarity to a natural language model we used perplexity, which is perhaps the most commonly used measure of model quality in speech, natural language processing, and information retrieval research [29]. Perplex-ity quantifies the error between the predicted probability of
In our experiments, we worked with  X  =0 . 5 , 0 . 6 ,.., 0 . 9. We only report the results with  X  =0 . 8 for clarity of presentation, but note that the respective outcomes for other values of  X  were very similar. Table 10: Part-of-speech distribution for text (T) vs. voice (V) by query length (number of tokens). an event proposed by a language model, compared to the empirical probability of the event. In our case, we used per-plexity to measure the quality of a natural language model (WSJ or QT) with regards to a corpus of queries (i.e., the events) from either voice or text. In other words, we mea-sured how likely the set of voice versus text queries is to originate from the given language model.

More formally, given a language model LM and a set of observed probabilities P , the perplexity of LM is defined as 2
H ( P ; LM ) where H ( P ; LM ) is the cross entropy of the prob-ability model LM with respect to the observed probabilities P , summed over all events in P . The closer the estimated probabilities for each event to the actual probabilities, the lower the perplexity. In our case, each event is a query q in a corpus Q (either voice or text), with an observed probability | Q | . Therefore, the cross entropy can be calculated as: H ( Q ; LM )=  X  X where log 2 LM ( q ) is the length-normalized log probability of the query q based on the language model LM . The perplex-ity itself is calculated by using the cross entropy as the expo-nent  X  the lower its value, the better is the language model for  X  X redicting X  the generation of the given query corpus.
The first two rows of Table 11 show the perplexity results for the WSJ and QT LMs. It can be seen that the perplexity for voice queries is considerably lower than for text queries, for unigrams, bigrams, and trigrams. The  X  X /T X  columns explicitly show the proportion between the two. It can also be seen that the ratio decreases, i.e., the gap between voice and text queries grows, when moving from unigrams to bi-grams and then to trigrams, indicating that the difference is largely due to the structure of the voice query language, rather than merely due to its vocabulary.

The general perplexity values for bigrams and trigrams are substantially higher for WSJ than for QT. This can be either due to the WSJ language being less similar to query language than QT, or due to the large volume of training data for QT, which produced a better language model. To further explore this, we randomly sampled 42,248 question titles from the 50M QT corpus  X  identical to the number of sentences in the WSJ corpus  X  and trained unigram, bi-gram, and trigram LMs based on this smaller corpus. The lowest row of Table 11 shows the respective results. It can be seen that the bigram and trigram perplexity values are higher than for the massively trained QT model, but are still lower by roughly an order of magnitude for bigrams, and two orders of magnitude for trigrams, compared to the WSJ 7 . This gives a stronger indication that the queries are more likely to be generated from the QT LM than the WSJ. The differences between voice and text queries remain  X  the perplexity ratios are similar to the full QT LM.

While being longer is an inherent characteristic of voice queries, we set out to explore whether for queries of the same length, there are still differences in the perplexity be-tween text and voice queries. Table 12 shows the results for both WSJ and QT, as measured for text and voice queries of length 3, 5, and 7 tokens. Generally, the perplexity val-ues indeed decrease as query length increases, indicating, as conjectured, that longer queries are closer to natural lan-guage. For both WSJ and QT, the perplexity across all n-grams is still noticeably lower for voice versus text queries of the same length. While the ratio is not as sharp as for the general query population, the difference is still clear and the trend of decreasing ratio from unigrams to bigrams and from bigrams to trigrams persists. This indicates that when we take a voice query and a text query of the same length, the former will have a language closer to natural.
Our study disclosed various differences between voice and text search. In this section, we summarize the key findings, discuss implications, and suggest directions for future work.  X  Query Categories. Both our query semantics and click analysis revealed that voice queries are more focused on audio-video content, such as from music channels or video sharing websites. It seems that voice search is more often used when the result is also expected to include voice. In addition, we saw that higher portions of the voice queries triggered the  X  X irect answer X  card and yielded clicks on popular CQA sites. This may be a result of the fact that higher portions of the voice queries were phrased as ques-
The unigram perplexity is substantially lower for the smaller training set. We suspect that the large training set adds many rare unigrams to the vocabulary that do not at all appear in the query sets, but reduce the probability of other unigrams that do appear in the queries.
Table 12: Perplexity by query length for text and voice. tions: White et al. [39] found that such  X  X uestion queries X  typically have informational intent and often result in visits to CQA sites. We also found evidence for higher portions of recipe-related queries and clicks, perhaps implying that voice search is used while cooking. On the other hand, lower portions of voice queries referred to social network-ing and adult sites, which may represent more sensitive or personal content [13]. These noticeable differences in query categories suggest that search services that build on query classification, such as vertical selection, card triggering, ad targeting, query expansion, and even result ranking, may need to be adapted when used for voice search. For ex-ample, as voice queries are often phrased as questions, the identification of CQA queries (e.g., for presenting a CQA vertical on the SERP) may need to change.  X  Device interaction. Voice search tends to focus on topics that require less interaction with the device X  X  touch-screen. This was reflected by a substantially lower number of clicks, higher portions of queries that triggered cards, and more queries that expressed a narrow information need (time, dictionary, weather, and phone numbers). We also saw this trend on celebrity queries, where the more open-style queries that only include the person X  X  name, were rel-atively less common than queries that refer to a refined as-pect, such as age or spouse. On the other hand, queries for research topics (e.g., health) and, as already mentioned, so-cial network sites, which require higher level of engagement, were less frequent on voice. These results have two key implications. First, they suggest that voice search should enable voice-based result presentation, to support a com-plete hand-free interaction with the user. While short voice answers have started to emerge on commercial web search engines, we believe these capabilities should be further ex-tended, to support interaction with more result types, ex-ploration of different search results, query suggestion, and ultimately a complete dialogue with the user, as already done by intelligent assistants for personal advice [18]. The second implication relates to the IR evaluation process. Re-cent studies have argued that with modern search, espe-cially on mobile devices, the merit of clicks as a primary evaluation measure decreases [23]. Other measurements, such as  X  X ood abandonment X , have been proposed [24]. Our findings show that voice search clicks, as a form of inter-action, are even rarer than text search clicks on mobile devices. Thus, new metrics for evaluating user satisfaction of voice queries should be developed.  X  Pronouncing vs. writing. In our analysis, a variety of issues were observed that relate to the difference between speaking queries and typing them. We saw more frequent use of words that are easy to pronounce but hard to write (e.g., long state names), and on the other hand less frequent use of abbreviations or calendar years (2015), which are easy to type but harder to pronounce. Related to this is the absence of a copy-paste feature, reflected by the rare use of URLs in voice queries. In addition, some typing styles are  X  X tandardized X  by the transcription process, e.g., the use of apostrophe for possession (on text,  X  X  X  is commonly used both with and without the preceding apostrophe) or the use of diacritical marks, such as in  X  X eyonc  X e X . Finally, the use of punctuation marks and upper case letters, which is infrequent on mobile search, is even rarer on voice search.  X  Voice query language. Our syntactic analysis, based on parsing and POS tagging, showed that voice queries are not only longer, by a token on average, than text queries, but also use richer language. Our semantic analysis demon-strated this with the use of natural language phrases such as  X  X  X  X  looking for X ,  X  X ake me to X , and  X  X lease X . The perplexity-based analysis showed that the language of voice queries is indeed closer to natural language, even when controlling for query length. While it has long been claimed that voice is a richer, more expressive media than written text [7, 10], our study demonstrates it for the domain of web search queries.
Having said that, the language of voice queries was found to still be far from natural-language questions and even far-ther from news articles. We also saw that voice queries may often be as short and identically-phrased as text queries.
These findings suggest that voice queries pose their own type of language, in-between traditional text queries and natural-language questions.

One question that follows, which we did not explore in this work, is how the length and richer language of voice queries can help improve the search process. Previous studies found that longer queries, closer to natural lan-guage, do not necessarily improve the retrieval effective-ness compared to typical keyword-based queries [10, 39].
On the other hand, taking advantage of the general growth of query length on web search, recent studies proposed vari-ous methods for applying linguistic analysis on long queries, including part-of-speech tagging, dependency parsing, and entity and relation extraction, in order to enhance search performance. Linguistic analysis can also be applied on the document side and matched against the query X  X  analysis, to resolve ambiguity and further enhance query-document match calculation. A recent book on  X  X erbose X  queries (5 words or more; 34.5% of all voice queries) summarizes this body of research and explains that for such queries, not only is it more feasible to apply linguistic analysis, but it is also essential for the understanding of the specific intent and the relevance of the returned results [15].

Previous work has tried to automatically transform queries into questions, by adding missing functional words or using question templates [12, 25], motivated by a variety of rea-sons, such as helping with intent disambiguation, improv-ing search over CQA archives, enhancing query expansion, and allowing to post a query directly as a question on a
CQA site when the answer cannot be found. Since voice search queries are more often phrased as questions, they may directly enable these benefits.

There is plenty of room (and need) for further research, such as exploring the use of voice queries X  phonetic charac-teristics, e.g., the speaker X  X  stress, speed, and intonation for search personalization, or conducting user studies to gain in-depth understanding of the voice search process. Our analysis suggests that voice search is still in its early stages, as reflected by the smaller portion of unique queries and the lower diversity of clicked domains. Future research should follow the dynamics of voice search as it is poised to become ubiquitous and further evolve. My thanks to Alex Nus, Dan Pelleg, Yuval Pinter, and Gilad Tsur for their useful comments. Special thanks to Avihai Mejer for his useful advice throughout this research.
