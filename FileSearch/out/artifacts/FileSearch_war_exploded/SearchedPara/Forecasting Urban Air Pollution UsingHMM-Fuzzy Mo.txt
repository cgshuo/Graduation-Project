 Urban air pollution (UAP) [1], [2], [3] has great impact on health and lives in society. It is getting a lot of attention because of it is actually changing the environment as we watch. Over the last decade, it has been studied in terms of measurements, physics, chemistry and modeling. The modeling approach is an important tool to study pollution in an urban air shed, and specifically, to measure the air pollution control policies [4]. In most instances, the deterioration of health can be traced back to air pollution [1]. This makes it imperative that air pollution be stopped, or at least, controlled. An effective forecasting tool that can accurately help us cont rol pollution and keep it within acceptable levels is a dire necessity. In order that air quality does not deteriorate any further, scientific plans for analytical methods and pollution control are needed.

Different techniques exist that can for ecast environmental events, though only a few have been applied to forecasting air pollution. Air quality phenomena have been traditionally modelled using physical reality as a start, and, for instance, this information has been coded into diff erential equations. Methods of Com-putational Intelligence (CI) [5] are a paradigm shift from the current approach which puts a model together based only on measured data. CI is fairly new to the environmental scientists and engineers, which is based on the hypothesis that reasoning can be realised using computation [6]. Methods used in computational intelligence include a number of forms of computation, the most famous of which are neurocomputing and fuzzy logic. Neurocomputing works on principles that were discovered studying the brain and its organizational structure [7], whereas fuzzy logic is based on the fuzzy set theory, which extends traditional bivalent logic into continuous group membership with truth values between 0 and 1 [8]. Challenges: Amongst the major challenges in forecasting UAP, the prediction of episodes with high pollutant concentration in urban areas in order that author-ities can provide appropriate means to counter potential problems. Authorities and the public demand precise forecasts of urban air quality, especially during episodes where the pollution levels are above the threshold values of acute health effects, and this demand has turned into an outcry during the last few years af-ter the introduction of higher air quality standards (EC/92/72, EC/96/62 and EC/99/33, for example) [2]. Authorities in many European cities have estab-lished emergence preparedness systems t hat handle air pollution episodes and it is quite likely that we shall see more such systems in times to come.
UAP forecast quality depends mainly on three factors: the mapping of emis-sions, the UAP model and the quality o f meteorologica l forecast data. Contribution: Our study presented here aims at attaining a better under-standing of phenomena associated with air pollution at a location related to its traffic volume and meteorological variable. The objective is to show how Hidden Markov Model (HMM) [9] along with fuzzy logic [10] can be used to create a model that can forecast concentrations of airborne pollutant variables. Organization: The remainder of the paper is organized as follows. In Section 2, we briefly discuss work related to this area. Our model is formally described in Section 3. Section 4 provid es a description of the dataset, design of the exper-iment carried out in this study and the result of the experiment. Finally in Section 5, we discuss the results and conclude the paper. Artificial Neural Networks (ANNs) are a common tool used in most of the similar applications. For instance, neural models for ozone concentrations have been constructed [11] and a model that predicts hourly NO x and NO 2 concentrations has been successfully applied [12]. Most of the work has focused on comparing feed-forward ANN, especially multi-layer perceptrons (MLP), with traditional methods such as the ARIMA model and linear regression. The results show in general that neural models perform as we ll as these methods if not better [13]. When applied, however, such  X  X lack box X  modeling offers too little support for understanding the physical phenomena that are being considered.

Kolehmainen et al . [1] introduced a model using the Self-Organizing Map (SOM) algorithm, Sammon X  X  mapping and fuzzy distance metrics. The MLPs were used to forecast environmental p ollution. Actual levels of individual pollutants were then computed using a combination of MLP models which were appropriate in that situation. Neagu et al . [3] presented a unified approach that integrated implicit and explicit knowledge in neurosymbolic systems as a com-bination of neural and neuro-fuzzy modules. We introduce a novel HMM-based fuzzy rule generation tool: HMM-fuzzy model 1 here. In our model, a HMM is used to sort the data vectors in the multivariate dataset and divide the input space into a number of subspaces to form fuzzy rules.

The model comprises three phases:  X  Phase 1: The HMM is used to partition the input dataset based on the  X  Phase 2: An iterative top-down(divide and conquer) algorithm is used to  X  Phase 3: A gradient descent method is applied to fine tune the obtained
It should be noted that, before using the HMM to partition the input data vectors, the HMM is trained using the Baum-Welch algorithm [15] and available training data vectors. 3.1 Sorting Training Dataset To partition the input dataspace, we first sort the data vectors/patterns using a single HMM based on the similarities among the patterns. The initial HMM was built by using random parameter values. Our approach of sorting the data patterns differs from the usual approach using HMM. Therefore, a detailed ex-planation is presented in the following subsection.
 HMM as a data-pattern sorting tool In our model, a single HMM has been used to sort the available training data based on the HMM-loglikelihood value.
 Lemma 1. For a given HMM  X  =( A, B,  X  ) , the probability of a k -dimensional vector is Pr( X |  X  ) where, x i  X  X . A single HMM generates a scalar value for each of the vectors such that
Pr( X |  X  )= The generated scalar value known as the log-likelihood value, determines the sim-ilarity between two data patterns of k -dimensional vectors is used for sorting the data patterns.
 Proof. According to Rabiner [9], after training the HMM, which acts a reference point, it becomes suitable to compute the probability that the vector was produced by the model. They have also shown that the probability acts as an indicator for how well a given model matches a given vector. Therefore, any vector can be transformed into a scalar log likelihood value.
 Consider the log-likelihood value for the three data vectors are l 1 , l 2 and l 3 . If the values of l 1 and l 3 are close within a tolerance level, the data vectors that correspond to these two log-likelihood values are similar. If the value of l 3 is not close to the value of l 1 and l 2 , it indicates that data vector corresponding to l 3 is not similar to the data vectors corresponding to l 1 and l 2 . Thus, data values with similar log-likelihood values would belong to same group.
 Here, the HMM was used as a pattern matching tool only, where no time depen-dency is assumed among the data variables (features). Each data vector fed into the HMM are formed using a number of distinct variables. Given the HMM the probability of generating a k -dimensional data pattern, x 1 ,x 2 ,x 3 ,x 4 ,...,x k , is calculated using the following set of equations [9]: where, Q = State sequence q 1 ,q 2 , ..., q k (for a k -state HMM),  X  =TheHMMmodel,
X = Input data vector x The values of Pr( X | Q,  X  )andPr( Q |  X  ) is calculated using the following equa-tions [9]: where, b i ( x i ) = Emission probability of the feature x i from state i . where,  X  = Prior probability matrix, a i,j = Transition probability from state i to state j .
 Bucketing to group similar data vector The range of log-likelihood values ( l 1 to l m ,where l i = log-likelihood value produced for the i th data vector and m=total data vectors) is split into equal sized buckets. The data vecotrs in each bucket produce similar log-likelihood values. Each of the bucket has a start point and an end point corresponding to the log-likelihood values. The size of the bucket,  X  , is a parameter of the model that is used to guide the rule extraction process. These buckets were generated so that they can be used to generate fuzzy rules at a later phase. 3.2 Fuzzy Rule Generation In this phase of the model, we divide the dataset using the buckets and a di-vide and conquer approach to generate appropriate number of fuzzy rules. To begin with, we create only one fuzzy rule that represents the entire input space of the training dataset. At this point, all log-likelihood values contained in the individual buckets may be perceived as belonging to one global bucket. In the process of rule generation, we calculate the mean  X  x i and standard deviation  X  i to define the membership function for each features follows:
The prediction error for the training dat a vectors is calculated using the gener-ated fuzzy rule. A mean squared error (MS E) is used to quantify the performance of the developed model for the training dataset. If the prediction error for the training dataset is less than or equals a threshold value  X  the algorithm is ter-minated and no further rules are extracted. On the other hand, if the prediction error is greater than  X  then the input space is split into two parts with the help of buckets produced in previous section. The splitting of the input space is done by dividing the total buckets into two equal parts. Data in the respective parts constitute the splitted input space. Each splitted partition has individual rules created for it. Finally, the total number of rules is increased by one. The prediction error for the training dataset is recalculated using the extracted rule set. Should the error threshold  X  not be reached then the buckets containing the datasets responsible for the left part of the rule are divided into two rules, and the process is iterated. Again, if the error threshold  X  is still not met, the right part of the rule is partitioned and the process undertaken again. This cycle continues until eith er the error threshold  X  is met or the number of rules equals the number of buckets. 3.3 Optimization of Extracted Fuzzy Rules The parameters of the generated fuzzy rules are further fine tuned using a gradi-ent descent algorithm and training dataset. At this stage, the mean and standard deviation for each of the membership functions of all fuzzy rules are fixed more precisely so that it can predict with be tter accuracy. We follow the gradient descent methodology as in ANFIS [16].
 4.1 Dataset The dataset used are a subsample of 500 observations from a dataset that was originally put together as part of a study on air pollution related to traffic vol-ume and meteorological variables on a road, conducted by the Norwegian Pub-lic Roads Administration. The response variable (column 1) comprised hourly values of the logarithm of the concentration of PM 10 (particles) measured at Alnabru in Oslo from October 2001 to August 2003. The predictor variables (columns 2 to 8) are the logarithm of the number of cars per hour, wind speed (m/s), temperature 2 meters above the ground (  X C ), the temperature deference between 25 and 2 meters above ground (  X C ), wind direction (within the range of 0  X  -360  X  ), hour of day and day number as counted from October 1, 2001. 4.2 Experiment Design The experiment was designed using the HMM-fuzzy model. The size of a bucket was chosen to be,  X  =0 . 5 (while the bucketing was done using log-likelihood values) and the desired MSE was chosen to be 0.001. To optimize the extracted rules, 500 epochs were chosen while execu ting the gradient descent algorithm.
In this experiment, the number of states in HMM was chosen to be 7 based on a previous study by Hassan et al . [14]. For the HMM, the initial values of transi-tion probability matrix and the prior probability matrix were chosen randomly. As time series datasets are continuous, the observation emission probability ma-trix of HMM is considered to follow normal distributions where the means and variances are initially chosen randoml y. HMM-fuzzy model tool was executed in 10-fold cross validation (CV). 4.3 Results of HMM-Fuzzy Model Various numbers of rules were generat ed in each fold of the execution of HMM-fuzzy model. There were an average of 2 . 9  X  1 . 3703 rules with confidence level of 95% or over.

Figure 1 shows the effect of fuzzy rules in the dataspace. In Fig. 1(a), we see how the dataspace is being divided by the generated rules (figure with respect to the first three attributes only); while in Fig. 1(b), we can see the fuzzy rule that actually divides the dataspace shown in Fig. 1(a). Figure 1(c) shows a membership function of the first attribute shown in Fig. 1(b). 4.4 Results Comparison We used two other tools in order to comp are the results generated by our HMM-fuzzy model. All the tools were executed in 10-fold CV.
 As ANNs are commonly used in similar applications, we try to minimize the MSE of ANN as far as possible. We have em pirically chosen the architecture of ANN that has the smallest MSE in training data. The ANN had 7 nodes in the input layer, 21 nodes in the hidden layer and 1 node in the output layer. We used the LM algorithm along with tan-sigmoid activation function. The epochs and training goal were chosen to be 500 and 0.001, respectively. The average training error occurred for the ANN was only 0.002.

We have also generated a forecasting model using the subtractive clustering-based fuzzy model reported in [17]. In this approach, we must provide the radius of the cluster before generating fuzzy rules from the available dataset. Based on the given parameter, i.e., the cluster radius, the model generates a number of clusters in an unsupervised way. We have empirically chosen the cluster radius to be 0.6. Each cluster obtained corresponds in generating a fuzzy rule, i.e., each fuzzy rule relates a region in the input space to an output class.

MSE of HMM-fuzzy model is compared with the MSE and the number of fuzzy rules with confidence level of 95% or over of two other techniques using 10-fold CV is presented in Tab. 1. This study has demonstrated that HMM-fuzzy approach technique followed by gradient descent method is effectivel y able to model an hourly air pollution forecasting system that can predict concentrations of airborne pollutants. This hybrid technique clearly outperforms the other popular tools, such as ANN and fuzzy rule extraction (see Tab. 1). Mor eover, the HMM-fuzzy approach generates a significantly fewer number of rules than the technique described in [17].
This system has reduced complexity and s imultaneously improved forecasting accuracy. This is most likely because H MM X  X  accuracy in identifying similari-ties within air pollutant data sequences (i.e. traffic volume and meteorological variables on a road) that consequently provides improved partitions in the input space. Besides, while partitioning the input space using HMM, the similarities among the feature attributes are identified by HMM in terms of fluctuations in magnitude. The end result is an improved set of fuzzy rules that can predict the concentration of PM 10 particles.

To determine the efficiency of the HMM-fuzzy model proposed in this paper, it is vital to compare it with other well-performed fuzzy rule finding methods, for instance [17]. From the comparison, it is evident that other techniques consider the individual input features to be independent of each other and, this may generate extra rules making the overall system complex. The increased number of rules without taking into consideration the interdependencies among the input variables does not always lead to a more e ffective model. Experimental results presented in this paper support this observation.

This paper demonstrates that the hybrid HMM-fuzzy model has the poten-tial to achieve high levels of performan ce when it designs an hourly air pollution forecasting system that can effectively t rack and forecast concentrations of air-borne pollutant variables. We recommend that higher sample sizes and various meteorological variables should be used in continual research a long these lines. The results will forecast air pollution, e xtrapolate its effects and can help decide on a proper course of action to combat the problem.

