 We present a hierarchical kernelized classification model for the automatic classification of general questions into their corresponding topic categories in community Question An-swering service (cQAs). This could save many efforts of manual classification and facilitate browsing as well as better retrieving of questions from the cQA archives. To deal with the challenge of short text message of questions, we explore and optimally combine various cQA features by introduc-ing multiple kernel learning strategy into the hierarchical classification framework. We propose a hybrid regulariza-tion approach of combining orthogonal constraint and L 1 sparseness in our framework to promote the discriminative power on similar topics as well as sparsing the model pa-rameters. The experimental results on a real world dataset from Yahoo! Answers demonstrate the effectiveness of our proposed model as compared to the state-of-the-art methods and strong baselines.
 H.3.3 [ Information Systems ]: Information Storage and Retrieval Question Topic Categorization; Kernel Learning; Sparse Or-thogonal Regularization
Community Question Answering services (cQAs) have be-come popular alternative for users to seek knowledge and find solutions when their queries fail to find good answers in the traditional search engines [1, 26, 19]. In cQAs such as Yahoo! Answers and AnswerBag , the existing questions are often organized into a category hierarchy in which the questions with the same semantic topic are grouped togeth-er. Figure 1 presents a partial view of the Yahoo! Answers Categories. All questions posed to the Yahoo! Answers Fi gure 1: Partial hierarchy of the Yahoo! Answers categories. community will be manually assigned a suitable leaf node category label which fits the intrinsic topic of the question most. The category hierarchy not only helps people to easi-ly browse questions/answers that they are interested in, but also improve the performance of question retrieval within the same or similar semantic categories [8, 7].

However, manually labeling the questions at the posting time undermines the X  X ser experience X  X f cQA service. More severely, if people are not familiar with the category hier-archy in whole, they are prone to be confused and would take more time to assign the correct category label. For instance, there are three categories in Yahoo! Answers all named Ireland  X  Dublin under three top classes Dining out , Local businesses and Travel . If users are not aware of this, it is highly likely that frustrations will occur in their labelling of Dublin related questions. Therefore, automatic classifi-cation of questions into the category hierarchy will ease the burdens of cQA users by assisting them in making the cor-rect decision of category labels.

The state-of-the-art question topic categorization meth-ods are mainly based on the statistical learning framework [2, 6, 23]. Qu et al. [23] made a systematic evaluation of existing methods on a large set of questions from Yahoo! An-swers, and showed that the single path hierarchical support vector machines (hSVM) could achieve better performance than other methods in effectiveness. Different from the tra-ditional text document classification, one of the challenges of the question topic categorization is the  X  X ata sparseness X  problem, in which the textual part of community question-s is often much shorter in length. Existing solutions are mainly based on the use of additional external resources, such as the relevant feedback of information retrieval [22] Fi gure 2: An example of different parts of a question from Yahoo! Answers. or general knowledge-base like Wikipedia [6]. In real cQA applications, questions often come with more auxiliary in-formation than the traditional TREC QA benchmark. For example, as illustrated in Figure 2, a question typically con-tains a possible question content field describing the user X  X  specific context or information needs and a best answer giv-en by other users often with high star rating of quality. In each field of the cQAs question, we can extract various kind-s of features covering useful information such as the lexical and syntactic structures. Therefore, in this work we com-bine these abundant heterogeneous cQA features together to alleviate the data sparseness problem of question topic cate-gorization. More specifically, we introduce a multiple kernel learning strategy into the hierarchical learning framework in order to optimally combine various kernelized cQA features to improve the performance of question topic classification.
Under the kernelized framework, the question is classified via the decision function consisting of weighted kernel sim-ilarities. In the category hierarchy, sibling nodes under the same parent often talk very similar topics and are more hard to discriminate. Thus we adopt an orthogonal constraint to regularize the kernelized parameters in node siblings of the hierarchy, forcing these similar categories to use different combination of kernel similarities. However, the estimated model often becomes very dense when the category hierar-chy scales up (even for moderate size) and the case that orthogonal regularization is adopted. It means that more space is needed for the storage of model parameters and higher computational cost is required for predicting the un-seen questions. Therefore, we propose a hybrid regulariza-tion approach of combining L 1 sparse regularization and the orthogonal constraint in the kernelized hierarchical classifi-cation framework. The experimental results show that our proposed hybrid regularization is more powerful in effective-ness compared to other methods, and also brings a more parsimonious model for saving model space and inference time.
 The main contributions of our approach are two-fold:
First, we explore various kinds of kernels, such as key-words, n-grams, Part of Speech (POS), syntactic tree ker-nels from different components of community questions for dealing with the problem of data sparseness, and obtain the optimal weighting of these kernels through multiple kernel learning under the hierarchical classification framework.
Second, we introduce a hybrid sparse orthogonal regular-ization into our model, augmenting its discriminative pow-er on similar topics as well as sparsing model parameters, which could further enhance the classification performance and reduce the storage cost and inference time.

We conduct experiments on real world application dataset from Yahoo! Answers to evaluate our model. Compared with the strong baselines and the state-of-the-art question categorization methods, such as Naive Bayes (NB), SVM, hi-erarchical SVM (hSVM), Orthogonal Transfer SVM (OTSVM) and Orthogonal Transfer Logistic Regression (OTLR), the experimental results demonstrate the effectiveness of our method.

The rest of the paper is organized as follows: Section 2 shows a brief history of related work, and Section 3 presents the proposed question topic categorization framework. The parameters learning and inference process of our method are presented in Section 4. The experiments and some dis-cussions are given in Section 5 and Section 6 respectively. Finally we conclude this paper in Section 7.
Recent work for question classification (QC) mainly con-sists of two types: one is to classify questions based on the answer focus, such as the QC task on TREC QA track [18, 28, 3, 14], the other is question topic categorization, which organizes questions into a predefined semantically meaning-ful category hierarchy. Our work belongs to the second type.
To our best knowledge, there is little previous literature [2, 6, 23] dealing with the community question topic cat-egorization problem. Blooma et al. [2] used the unigram and bigram features of both the question and sub questions (question content) under a hierarchical SVM classifier, and showed that the introduction of sub questions obtains lim-ited improvement in classification accuracy for 23 leaf cate-gories. Qu et al. [23] compared different classification mod-els like Naive Baynesian (NB), Maximum Entropy (ME), and SVM. By evaluating the classification performance on the Yahoo! Answer dataset, they found that the single path hierarchical SVM with bag of words features in all combined parts of question achieved the best performance. Cai et al. [6] explored the whole Yahoo! Answers Categories and used more than 2 million questions to train the classifiers. To overcome the large-scale data problem, they used a search step to prune most unrelated categories to focus only on a small related subset. To tackle the data sparseness prob-lem they enriched the short questions by leveraging external Wikipedia semantic knowledge such as hypernyms and as-sociative concepts.

For hierarchical classification of ordinary text documents, a lot of research work considering different parameter reg-ularization or loss functions emerged [5, 9, 29]. Cai and Hofmann [5] proposed a transfer regularization encouraging the classifiers of nodes to be as close as possible to its an-cestors in the hierarchy, while Cesa-Bianchi et al. [9] used a tree-induced loss that penalizes two misclassifying classes to be proportional to the length of the undirected graph path of these classes. Zhou et al. [29] proposed an orthogonal reg-ularization to the parameters of classes, ensuring that each node and its ancestors to be classified via different features o r feature combinations, and reported better classification performance.

In the traditional TREC QA classification, Li and Roth [18] first proposed a two-layered question taxonomy which consists of 6 coarse classes in the top level and 50 fine sub classes in the secondary level. The taxonomy was designed to categorize questions into different classes based on the possible semantic types of the answers. Zhang and Lee [28] utilized five machine learning methods to classify the factoid questions on TREC task within this taxonomy and found that SVM with syntactic tree kernel could achieve the best performance. For general questions Bu et al. [3] recon-structed a new taxonomy according to their functions (such as Fact , List , Reason , etc) instead of contents, and proposed a Markov logic network question classification methodolo-gy which combines both rule-based and statistical methods. Moschitti et al. [20, 21] presented various syntactic and shallow semantic kernels for short text such as the ques-tion/answer pair classification, and showed that some se-mantic kernels derived from predicate argument annotation are more efficient for extracting the correct answers for a given question.

Different from previous work, our method aims at com-munity question topic categorization. To deal with the chal-lenge of data sparseness, we explore and optimally combine various cQA features by integrating multiple kernel learn-ing into our hierarchical classification model. Moreover, we propose a hybrid regularization approach of combining L 1 sparseness and orthogonal constraints, promoting the dis-criminative power on similar topics, while at the same time sparsing the model parameters in order to improve the per-formance of cQA topic categorization.
Hierarchical models have been proved to be more efficient and sometimes more effective than their flat counterparts in text classification when the class labels form a hierarchy [15]. In this section, we will present a general hierarchical classification framework. First, some notations are given: children, descendants and siblings of a tree node i in the hi-erarchy, and F + ( i ) = F ( i ) be the domain of observations (such as the d -dimensional feature vectors of questions) and let Y = { 1 ; 2 ;:::;m } the set of class labels of the hierarchy (except the root la-bel). The training process of hierarchical question topic categorization is defined as follows: given a set of training questions D = { ( x 1 ;y 1 ) ;:::; ( x N ;y N ) } ;x k  X  X  { 1 ;:::;N } , learn m classifiers w = { w i } m i =1  X  X  d , each for a class node i in the hierarchy, by solving the following opti-mization problem, The equation consists of two terms, where R ( w ) is a regular-ization term preventing the model from over-fitting, L h ( w;y ) denotes the hierarchical loss function and C controls the bal-ance of the two terms. The loss function is used to penalize the misclassification cases  X  often with a large value for the incorrect inferences but with no impact for the correct ones [16]. Particularly, the loss for hierarchical Support Vector Machines (hSVM) has the following form, L h ( w;y k ;x k ) = max For the correct leaf node label y k and all its ancestors from bottom to up along the hierarchy, the loss will penalize the cases when the margin of the correct output and other offensive sibling labels in the current level is less than 1. The smaller the margin, the more loss it contributes. Rewrite Equ.1 and Equ.2 by adding some slack variables , we get the constrained formulation as follows: It is tolerant for the linearly inseparable case by setting some slack variables k greater than 1. If we substitute the loss to the logistic form, L h ( w;y k ;x k ) = max it becomes the hierarchical Logistic Regression (hLR) with an implicit margin (we will apply it as one of the baselines in the experiments).

The general procedure of question topic categorization is presented in Algorithm 1. For an input question, it initial-Al gorithm 1 Hierarchical Question Topic Categorization Inpu t: a question feature vector x  X  X  d Output: the label of the question y 1: y = 0 2: while C ( y ) is not empty do 4: end while 5: return y iz es the question label with the root 0, and then recursive-ly searches its child nodes to find the one with the maxi-mal classification output using a top-down manner, until it reaches a leaf node, and the final leaf category label is as-signed to the input text. Since it classifies the input along a single path from the root to leaf in the hierarchy, it is termed as single path hierarchical (SPH) SVM [15].
Kernel based methods have shown great effectiveness for short text classification problems by projecting an input x into a more general reproducing kernel Hilbert space (RKHS)  X ( x ). A kernel function  X ( x i ;x j ) intuitively computes the similarity between two samples x i and x j in the new space with no need for giving the explicit mapping function  X . Th e kernelized solution to classifiers admits the representor theorem : That is, every classifier is based on some linear combination of the projected training examples (support vectors), and { ij } ;i = 1 ;:::;m;j = 1 ;:::;N are the combination param-eters. Note that the new parameters i has a dimension of R N rather than the original w i in R d , and the kernelized form of Equ.3 becomes, where  X  k denotes the k -th row of  X , and  X  = {  X ( x i ;x is the N  X  N kernel similarity matrix of training samples.
To deal with the question categorization problem, we pro-pose a hierarchical kernelized classification method which integrates multiple kernel learning with sparse hierarchical regularization. It can be described with the following opti-mization formulation, minimize J ( ; ) = C 1 where the first three terms are the hybrid sparse orthogonal regularization, and the fourth term is the hinge loss in ker-nelized form. N k denotes the total number of kernels, and  X  is the combination of all questions kernels. C 1 , C 2 and matrix K = { K ij } m i;j =1 are parameters controlling the bal-ance of regularization terms and loss function. It has been proved [29] that by properly choosing the matrix K 1 , the second and third regularization term of Equ.7 is convex. S-ince the L 1 norm is also convex (although not smooth), and the combined kernel  X  is positive semidefinite thus it will not change the characteristic of the convexity . By fixing
Th e matrix K = { K ij } m i;j =1 should be entry-wise nonnega-tive and its comparison matrix should be positive semidefi-nite. The comparison matrix K is defined as: K ii = | K ii i = j ; K ij =  X  X  K ii | , if i  X  = j . the kernel combination parameters , it is not difficult to prove that the objective function of Equ.7 is still a convex problem for variables . Therefore, we can readily use an effective algorithm to solve the optimization problem.
We design our question categorization model from two aspects: the multiple kernel learning strategy and spare or-thogonal regularization in the hierarchical form. More de-tails and the motivations will be given in the following two subsections.
Multiple kernels learning provides flexibility to involve multiple data sources and heterogeneous structures. Fur-thermore, it can lead to an ideal method for us to interpret and deeper understand the importance of different kernels. In this paper, we explore and discuss the cQA question k-ernels for question topic categorization. Different from the factoid questions in TREC benchmark, a question of real cQA application such as Yahoo! Answers often comes with some auxiliary information. As illustrated in Figure 2, the community question often contains a question subject that asks the main question, a possible question content that describes the detailed information need or the personal con-text, and a best answer chosen by the asker or voted by community members with relatively high quality. 2 For ev-ery component of questions, we explore various kinds of k-ernels based on keywords, n-grams, Part of Speech (POS) and syntactic trees to capture the basic lexical and syntactic information: Keywords Kernel  X  1 : the keywords kernel value  X  1 ( i;j ) is computed as the cosine similarity between the TF-IDF (term frequency-inverse document frequency) vectors of sin-gular words in text i and text j , with stopwords removed. N-grams Kernel  X  2 : the n-grams kernel value  X  2 ( i;j ) is computed as the cosine similarity between the TF-IDF vec-tors of n-grams in text i and text j . Here we only consider the case of n = 2, because for n  X  3, it is too sparse for our dataset.
 POS Kernel  X  3 : the POS kernel value  X  3 ( i;j ) is com-puted as the number of common subsequences based on the original words along with their Part of Speech tags. It is previously used for extracting relations between entities in natural language text [4]. The input text is assigned with the Part of Speech tags first, such as: Who/WP deletes/VBZ Dave/NNP 's/POS poems/NNS ?/. We use the Stanford POS Tagger 3 with trained model on WSJ for tagging. We then use the original words together with their POS tags as basic units for substring sequence kernel computing. Tree Kernel  X  4 : the syntactic tree kernel value  X  4 ( i;j ) computes the number of common substructures [11, 28] be-tween two syntactic trees (forests) of text i and text j . We obtain the syntactic tree from the text by using the Char-niak X  X  parser 4 . Figure 3 shows two syntactic trees for the question:  X  Feedback on my poem please ? X  X nd the question:  X  What do you think of my poem ? X . A pair of matching le-
Ot her answers are not considered here since there are many none-best answers in quite different quality for one question. http://nlp.stanford.edu/software/tagger.shtml http://bllip.cs.brown.edu/resources.shtml#software Fi gure 3: The syntactic trees of two questions; the dashed lines denote two matching subtrees. gal subtrees is highlighted with dashed box. Note that the subtrees should only obey the syntactic rules of the original tree, with some of the original text on the leaf nodes (e.g. on and of ) ignored.

We normalize the value of these kernels to be within 0 and 1, and linearly combine the 12 different question ker-nels which consists of four types (keywords, 2-gram, POS and syntactic tree kernel) each for the question subject S , question content C and best answer fields A respectively via parameters ,
Generally speaking, the more effective the kernel, the big-ger weight it should be assigned with. In our method, we use parameters { i } to denote the optimal kernel weights. Under the multiple kernel learning framework, they should meet two requirements  X  summing to 1 and staying non-negative. At the training process, we use a gradient descent algorithm to get the optimal in alternate iterations. De-tails on parameter estimation will be given in Section 4. In our framework, we adopt a hybrid regularization method. The regularization terms are as follows: R ( ) = C The first term is L 1 regularization to sparse the classifica-tion models, while the second and third terms are orthogonal constraint for hierarchical regularization. The orthogonal constraint has shown better performance compared to other regularization types [29]. In our model, we adapt the or-thogonal regularization to penalize the sibling classes of the same parent in the same hierarchy level, which have very similar topic distributions and hard to tell from each other in our problem; However, in the case of multi-class hierar-chical classification, the support vectors are dense [17], while after introducing the matrix K in orthogonal regularization, parameters are even denser. From the perspective of sparse learning and to reduce the space for storage and classifica-tion time, we prefer to learn a parsimonious model in which every classifier is based on a sparse linear combination of training instances. Therefore we introduce L 1 sparse reg-ularization for parameters penalization in our hierarchical learning framework.
The main parameters that need to be estimated in our model include classifier parameters: , and weights of ker-nels: . Although the joint optimization for both and are not convex, we can still use an alternate iterating algo-rithm by fixing one and solving the other in which case it is convex.
 Kernel weights estimation ( ): first we fix and obtain an optimal value for . Since the objective function J for is smooth and convex, we directly compute the gradient of parameters as, where  X  k;l denotes the k -th row of kernel  X  l . At iteration t + 1, the new parameters can be updated as, where t is the step size of iteration t , note that the elements of t +1 should be greater than 0 and re-normalized to sum 1.
 Classifiers parameter estimation ( ): when fixing the parameters , it is more complex for optimizing . We rewrite the Equ.7 with the equivalent unconstraint optimiza-tion problem as, where  X ( ), H ( ), r ( ) are respective to the orthogonal transfer, hinge loss and L 1 regularization, Because of the non smooth absolution operation in  X ( ) and r ( ), the optimization of Equ.12 becomes a little more com-plicated. However, we can still compute the subgradients of  X ( ) and H ( ) [29]. Let g  X  = ( g  X  1 ;:::;g  X  m ) denote the subgradient of  X ( ), then, if x =0. For H ( ), let a nd compute a vector h k = ( h k 1 ;:::;h k m ) as: h k = 0, if 1 components of h k are zero, otherwise. Then the subgradient of H ( ) can be computed as,
We propose a two-step algorithm to deal with the sparse r ( ) regularization. That is, for each iteration t we first put aside r ( ) and use the regularized dual averaging (R-DA) method [25] for t updating on  X ( ) and H ( ) and get the temporary result t + 1 2 . Then we solve the L 1 regular-ization r ( ) by the FOBOS updating [12], that is, the new parameters of iteration t + 1 is obtained by, where e is a small positive threshold controlling the spare-ness and [ z ] + denotes max { 0, z } .

For RDA we introduce two auxiliary terms ( ) and  X ( ) to get a simple solution to Equ.12, We summarize the proposed optimization solution to Equ.12 as Algorithm 2.
 A lgorithm 2 Alternate iterating algorithm for parameters estimation of our Equ.12 Inp ut: number of kernels N k , kernel matrix of training sam-ples  X  i ,i=1,..., N k , constant C 1 , C 2 and e , max iterations T Output: the optimal parameters  X  ,  X  1: (1) = 0, g (0) = 0, (1) = 1 =N k 2: for t=1,...,T do 3: compute the subgradient g ( t ) of ( ) using Equ.13, 4: co mpute the temporary vector 5: update the next parameters ( t + 1) using Equ.16 6: compute the new parameters ( t + 1) using Equ.11 7: end for 8:  X  = ( T + 1) 9:  X  = ( T + 1) 10: return  X  ,  X 
When we begin to classify a new question, it computes from the root to the leaf node along a single path of the hierarchy using the learned parameters. The process is al-most the same as in Algorithm 1. except in Step 3, where we use the estimated optimal kernelized parameters rather than the original weights of features, where  X ( X t ;x ) denotes the kernel similarity vector between the testing instance x and all training samples X t . Note that th e decision function under the kernelized form is based on combinations of kernel similarities weighted by the estimated parameters through the training process. We crawl real world community questions from Yahoo! Answers website by using the getByCategory API 5 , and we select six top categories, among which the most popular cat-egory ( Sports ) and the least popular one ( Environment ) are included. We also set the number of questions in each cat-egory to better reflect the real distribution statistics of the whole archives as the work of [23]. The statistical infor-mation about the dataset is presented in Table 1. The final dataset consists of 11,354 questions from 127 leaf categories. Every question corresponds to a unique leaf category label. The average length of question title is 9.1 words, while the question content and best answer field are much longer, with 60.3 and 77.3 words respectively. All the questions have the title field, plus 9,992 additional question content and 11,351 best answer fields. We split the whole dataset into ten folds, and randomly select eight of them for training, one of them as the validation data for tuning the parameters, and the rest for testing. We change all texts to the lowercase and filter the stop words 6 . Since the questions often get no an-swers when submitted, we just use the question subject and question content for testing. For evaluation, we compare the Micro F1 and Macro F1 [27] metrics which are widely used in multi-class question classification task. The Micro F1 treats every document equally while the Macro F1 treats every category equally no matter how many documents in it.
We use the following methods for comparison of effective-ness: Naive Bayes (NB) : We utilize the implementation of the Weka [13] toolkit. Since NB is not kernel based classification method, we use the keywords and 2-grams as features. Support Vector Machine (SVM) : we use the SMO for SVM in the Weka toolkit, and choose the linear kernel. For comparison we also use the same features as NB. hierarchical SVM (hSVM) : we use the default L 2 regu-larization and the hinge loss in hSVM [23].
 Orthogonal Transfer SVM (OTSVM) : OTSVM [29] re-places the L 2 regularization with the regularization in Equ. 9.
 Orthogonal Transfer Logistic Regression (OTLR) : it h ttp://developer.yahoo.com/answers/ http://truereader.com/manuals/onix/stopwords1.html T able 2: The classification accuracy in both Micro F1 and Macro F1 of five state-of-the-art methods and our strong baseline kSVM are compared with the proposed kOTSVM. The best performance are highlighted, and * indicates that the improvements of our KOTSVM to all other methods are statisti-cally significant under p -value of 0.05 using t -tests. Fi gure 4: The classification results of each top cate-gory. has the same formulation as in OTSVM except that it uses the logistic loss in Equ.4.

For verifying the effectiveness of the hybrid sparse orthog-onal regularization, we also implement a strong baseline: kernelized SVM (kSVM) with the default L 2 regulariza-tion, and combine all kernels of our discussed cQA features for kernel learning. We name our method the kernelized Spase Orthogonal SVM (kSOSVM) . For all methods, we tune the parameters for the best performance on the val-idation data. The classification accuracy of these methods in Micro F1 and Macro F1 are presented in Table 2.
From Table 2, we can see that our method kSOSVM achieves the best performance for both Micro F1 and Macro F1 compared to other methods. It shows that NB perform-s much worse than the SVM families, which confirms the results of other researchers [28, 23]. The hierarchical SVM obtains some improvement to the flat one. The OTSVM works better than the ordinary SVM and hSVM because of the introduction of orthogonal regularization, and the OTL-R achieves almost the same performance as compared to OTSVM. By utilizing the integrated kernel learning and hy-brid regularization, our kSOSVM achieves much higher per-formance improvement as compared to other methods.
Moreover, as illustrated in Table 3, by using our regular-ization more than 80% of the components in (classifier parameters) are zeros (sparseness), while in the case of tra-ditional orthogonal regularization [29], there are almost all none zeros. The sparse representation can lead to a more parsimonious model not only saving 72% memory require-ment for testing instances but also reducing as much as 61% Table 3: The comparison between our sparse orthog-onal regularization and Zhou X  X  in [29] in kernelized framework.
Fi gure 5: The learnt weights for every kernels. in inference time, at the cost of sacrificing a little training time.

Figure 4 presents the detailed Micro F1 scores for each top-level category. We will give some misclassification anal-ysis in Section 6.3.
To explore the heterogeneous data sources and auxiliary information (the keywords, n-grams, POS tags and syntactic trees of different question fields), we use the multiple kernel learning process to estimate an optimal weighting of these kernels. The estimated weights of various kernels are pre-sented in Figure 5. 7 In general, the keywords and 2-grams based kernels get bigger weights than the POS and syntac-tic tree based kernels. Particularly, the POS kernels have nearly zero weights; this could be because the relaxation of exact word to the combined text/POS leads to more inac-curate matching. The second observation is that the kernels on question subject obtain relatively higher weights than the other question components (question content and best answer). This indicates that it contains the most important information for question categorization task. In other words, the kernel learning leads to a better method to interpret the results and give us a deeper understanding of the effects of kernels. In Section 6.2 we will study the kernels separately, and verify the conclusions of kernel weighting.
We vary the size of training data, and show the tenden-cy of classification accuracy. We choose OTSVM which achieves the best performance among other previous meth-ods and our proposed kSOSVM for comparison. The results are illustrated in Figure 6. It is observed that the perfor-mance increases rapidly at first when training instances is increased for both methods, and the improvement becomes slower when more than 40% training data is used. Our method kSOSVM beats OTSVM for all cases, and the im-provement is significant. This is mainly because when more
F or evaluating all the kernels, we also include the best an-swer part for testing instances. Fi gure 6: The tendency of Micro F1 for our method kSOSVM and OTSVM by varying the rate of train-ing data.
 Fi gure 7: The Micro F1 and Sparseness of L 1 or-thogonal regularization by varying parameter C 1 . abundant training instances are available, our method can utilize more appropriate structural information in the kernel learning process.
There are some empirical parameters that need to be tuned in our model, including the matrix K for orthogo-nal constraint, the control weights of C 1 and C 2 , the L parameter e , the step size t and the max iteration number T . The matrix K which controls the orthogonal strength should satisfy the non-negative and semidefinite constraint in its comparison matrix. Through empirical study we find it can affect the convergence of the training process, and set-ting its minimal eigenvalue to be close to 1 can speed up the convergence. Thus we set K ij = 1 : 25 if i = j ; K ij = 0 : 05 if Parent( i ) = Parent( j ) and K ij = 0 otherwise. We find that by changing C 2 the results remain nearly unchanged, thus we just choose C 2 = 1 in the experiments. We set the step size t = 0 : 05  X  0 : 8  X  t from empirical experience. For max iteration number T , we set it to 100 for all the ex-periments, since we observe that the classification accuracy becomes stable for that. Tuning from the validation data we get e = 10  X  4 . Finally, we study the impact of parameter C , for both the spareness (rate of zero components) of the parameters and the Micro F1, and illustrate the results in Figure 7. It can been seen that by increasing C 1 , the learnt parameters becomes sparser, while the accuracy even rises a little until about 80% of parameters become zero. Thus it can save more space and time for storage and classification (see Table 3).
 Table 4: The classification results of different ques-tion components.
 su bject+content+best answer+asker 0. 636 0 .637
In this section we conduct further experiments to sepa-rately evaluate the effectiveness of different question com-ponents and various kernel types, and analyze some mis-classification cases.
For evaluation on the effectiveness of different compo-nents we use the three question components X  question sub-ject , question content and the best answer separately for question topic categorization. For fairness we remove the questions which do not contain the question content or best answer field, and use all four types of kernels for them.
The experimental result (Table 4) shows that the three components have different contributions for question topic categorization. It is clear that although the question subjec-t field is the shortest in length, it is still the most effective field for the intrinsic topic identification. In comparison, al-though the question content and best answer parts are much longer, they are not as effective as the question subject. This is mainly because the askers often state the question subjec-t with a concise sentence in some specific topic, while they just depict the problems or requirements in particular con-text and the answerers only provide the answers targeting at answering the detailed parts, without explicitly repeating the topics. At the posting time, because the subject and content of a question both are available, combining them together can significantly improve the classification perfor-mance. Moreover, it is shown that by adding the best answer component, it achieves even better performance. This result is important for the case when the question has received sev-eral answers and a best answer is chosen but the category label is still unassigned or neglected by the asker.
As the user related information has been shown useful in some applications such as answer quality evaluation [24] and answer summarization [10] in cQAs, we also consider includ-ing the asker X  X  field for question topic categorization. The result is given in the last line of Table 4. We find that asker information is almost of no help, since there is no statisti-cal significance compared to the case of removing it (see the 6th line of Table 4). We note that a similar result is also reported in [23]. It is possibly that in the scenario of cQA, an asker can pose multiple questions in different categories, and questions within the same topic are often asked by dif-ferent users, thus there is no apparent relationship between users and question topics.
The importance of various kernels can be obtained from the estimated optimal kernel weights. In this subsection we evaluate four types of kernels in the most effective question T able 5: Two POS-similar questions from different categories.
 Fi gure 8: Different syntactic trees for the two POS-similar questions in different categories. title field, and show the impacts of them separately in our framework. The result is illustrated in Table 6. From the table we can see that the keywords kernel achieves the best classification accuracy, which is consistent with the results of Section 5.3, where the keywords kernel gets the biggest weights. The next important kernel is the syntactic tree k-ernel. It performs better than the 2-gram kernel. This is in accordance with the result of Figure 5, in which the syntac-tic tree kernel gets comparative bigger weight than 2-gram kernel in the question title field. It is likely that the length of the title is too short thus the obtained 2-grams are too sparse, while the syntactic tree could capture more struc-tural information besides the texts. The result also shows that the syntactic tree kernel is much better than the POS k-ernel. The reason can be illustrated in Table 5 and Figure 8, in which the two questions comes from different categories. Although they share few exact words, most POS tags are the same. However, the syntactic tree structures for these two questions are quite different and easy to discriminate. This is why the syntactic tree kernel is more effective than POS kernel in our question categorization problem.
We show some misclassification cases which are difficult for automatic classification. The first case is illustrated in Table 7(a), the manual label is Global Warming , while our model categorizes it to Politics . Although Al Gore is both a politician and an environmentalist, just from the simple question we can not determine which category it is contained with. The second example is Table 7(b), in which the ques-tion with manual label Civic Participation are misclassified Table 6: The classification results of various kernels in the question subject field. T able 7: Some examples of misclassification cases from our Yahoo! Answers dataset.
 as Government . It is mainly because with the fast growth of questions the topics in one category becomes incohesive and shifting, and it is easy to confuse with other semantic related categories. This case occurs frequently in the top categories Environment and Politics &amp; Government , and this explains why the overall performances of them are relatively poor in Figure 4.

Another misclassification case is illustrated in Table 7(c), in which the question comes from the Software category is misclassified to Computer Networking . In this case, just us-ing the question subject is enough for classification. Howev-er, after introducing the question content and best answer fields, other words such as youtube , Mac and Windows which might be significant features in other categories has misled the question to the wrong category labels. It shows that the including of other question parts can sometimes introduce noises for this case. Nevertheless, the results of Table 4 show that combining them together can still get significant im-provement for overall question categorization performance.
In this paper, we presented a new hierarchical kernelized method for community question topic categorization. We exploited various kinds of cQA feature kernels through a kernel learning process. To make the classifiers use discrim-inative and parsimonious training instances, we introduced a hybrid sparse orthogonal regularization into our model. Experiments on Yahoo! Answers showed that our method outperforms the state-of-the-art methods. Furthermore, we ev aluated the effect of different question fields and various kernel types for question topic categorization, which can be useful in designing a more effective and practical cQA sys-tem. For the future work, we intent to explore more possible effective kernels to capture more information about the com-munity questions. This work was partially supported by the National Basic Research Program of China (973) under Grant 2014CB347600, the Program for New Century Excellent Talents in Univer-sity under grant NCET-12-0632, the Natural Science Foun-dation of China under Grant 61073002, National Research Project(MJ-Y-2011-39) and Shanghai High-Tech Project(11-43). [1] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and [2] M. J. Blooma, D. H.-L. Goh, and A. Y. K. Chua. [3] F. Bu, X. Zhu, Y. Hao, and X. Zhu. Function-based [4] R. Bunescu and R. J. Mooney. Subsequence kernels [5] L. Cai and T. Hofmann. Hierarchical document [6] L. Cai, G. Zhou, K. Liu, and J. Zhao. Large-scale [7] X. Cao, G. Cong, B. Cui, and C. S. Jensen. A [8] X. Cao, G. Cong, B. Cui, C. S. Jensen, and C. Zhang. [9] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni. [10] W. Chan, X. Zhou, W. Wang, and T.-S. Chua [11] M. Collins and N. Duffy. New ranking algorithms for [12] J. Duchi and Y. Singer. Efficient online and batch [13] M. Hall, E. Frank, G. Holmes, B. Pfahringer, [14] F. M. Harper, J. Weinberg, J. Logie, and J. A. [15] D. Koller and M. Sahami. Hierarchically classifying [16] Y. LeCun, S. Chopra, R. Hadsell, R. Marc , a  X rAurelio, [17] Y.-J. Lee and O. L. Mangasarian. Rsvm: Reduced [18] X. Li and D. Roth. Learning question classifiers. In [19] Q. Liu, E. Agichtein, G. Dror, Y. Maarek, and [20] A. Moschitti. Exploiting Syntactic and Shallow [21] A. Moschitti. Syntactic and semantic kernels for short [22] X.-H. Phan, L.-M. Nguyen, and S. Horiguchi.
 [23] B. Qu, G. Cong, C. Li, A. Sun, and H. Chen. An [24] Chirag Shah and Jefferey Pomerantz. Evaluating and [25] L. Xiao. Dual averaging methods for regularized [26] X. Xue, J. Jeon, and W. B. Croft. Retrieval models [27] Y. Yang and X. Liu. A re-examination of text [28] D. Zhang and W. Lee. Question classification using [29] D. Zhou, L. Xiao, and M. Wu. Hierarchical
