 Pseudo relevance feedback has dem onstrated to be in general an effective technique for improving re trieval effectiveness, but the noise in the top retrieved documents still can cause topic drift problem that affects the performance of certain topics. By viewing a document as an interaction of a set of independent hidden topics, we propose a novel semantic clustering technique using independent component anal ysis. Then within the language modeling framework, we apply the obtained semantic topic clusters into the query sampling process so that the sampling depends on the activated topics rather than on the individual document language model. Theref ore, we obtain a semantic cluster based relevance language model, which uses pseudo relevance feedback technique without requiring any relevance training information. We applie d the model on five TREC data sets. The experiments show that our model can significantly improve retrieval performance ove r traditional language models including relevance-based and clus tering-based retrieval language models. The main contribution of the improvements comes from the estimation of the relevance model on the semantic clusters that are closely related to the query. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Relevance Feedback Algorithms, Experimentation Relevance based language model, pseudo relevance feedback, query expansion, semantic clus tering, independent component analysis
Researchers realize that queries, especially short queries, do not provide a complete specification about users X  needs. To obtain more query related information, pseudo relevance feedback (PRF) is among the most often used me thods in various statistical language models like relevance-based language models [4] and cluster-based techniques [5, 6]. PRF assumes that top retrieved documents are all relevant, but not all those documents are really relevant. The noise introduced by non-relevant documents could cause the expansion query drifting away from the original topic. It is, therefore, important in appl ying PRF to firstly know how to identify relevant documents in th e top retrieved set without any relevance information, and then how to perform query expansion based on the obtained relevant information. 
This paper addresses the topic drift problem in PRF by introducing a semantic cluste ring method within language modeling framework. Our method us es a high order statistical method called independent compone nt analysis (ICA) [1] to construct a latent semantic space. In ICA, documents are viewed as the interactions of a set of independent hidden topics, which are the most highly structured dimensions with minimal entropy in the text data. With this understanding, we can perform documents clustering in the latent semantic space according to each latent topic. We hypothesize that this clustering method will generate better clusters on the t op retrieved documents so that relevant and non-relevant documen ts can be easily separated. 
Our method also extends docum ent generation model. We do not simply view that a query is directly generated as a random sample from the document language model estimated from each top retrieved document. Our f eedback documents are chosen based on a query topic activation pr ocess. That is, a user X  X  query activates one of the topics iden tified by ICA, and the documents associated with that activated topic are used as the relevant documents for estimating docum ent language models. This activation process not only forms doc ument models that are closer to the query model by filtering out more noise than using all top N retrieved documents, it also activ ates highly relevant keywords in the topic to form a semantic keywords cluster model that can be interpolated linearly to smooth a document model. 
In summary, inspired by releva nce based language models, we design a language model that explicitly represents relevance in matching the query and documents, and uses semantic clusters obtained from ICA to better select documents and terms for PRF. We call our model the semantic clustering based relevance language model. Similar to the relevance based language model, our model can construct a relevance model from the query without any relevant training data. 
In the remainder of this paper, Section 2 describes the principle ideas of ICA and how the seman tic clustering-based relevance model can be constructed. Secti on 3 reports experimental results on five TREC datasets, and compares to that of traditional language model, cluster-based and relevance-based language models. Finally, Section 4 c oncludes with future work. 
We view a document as be ing generated based on an interaction of a set of independe nt hidden topics, thus an ICA model in text data analysis can be used to separate these independent hidden topics: X = AS , where X is a term by document matrix that holds m observed signals (terms) in each row with n document samples in each column. A is the unknown m by k mixing matrix with non-orthogonal transformation basis, and the unknown matrix S holds the k latent topics with n document samples. Each S i ( i = 1,2,..., k ) is mutually independent and non-Gaussian. By using softmax normali zation [2], the value in matrix S can be converted into a probability that describes the degree that a document belongs to a latent topic. With such probability information, a document can be assigned to a topic according to the highest probability, as shown in the follows: 
A topic can be expressed by a set of keywords, called  X  X emantic term cluster X , and the probability of the keywords can be obtained by the back projection technique [2] shown as follow: where i = (1,2,..., m ), j = (1,2, ..., k ). The transformation TA holds the mixing proportions for a term from the semantic term space to the semantic topic space. T is the term matrix from the semantic space found by singular value decomposition (SVD), that is, the result, ICA helps us to cluster documents in latent semantic space, and it also helps to identify a set of representative terms for each latent topic. This is what we called  X  semantic clustering  X . 
Inspired by relevance based la nguage models [4], our method works on representing explicitly users X  relevance in the calculation of language models. A user X  X  need will activate certain latent topic in a set of documents, we believe that the representation of relevance model can be better constructed using the documents in the semantic cl usters instead of all the top retrieved documents. 
We rely on the query to identif y the semantic clusters whose documents are most likely to generate the query. If we consider each cluster as an abstract concept and assume that the query consists of a sequence of independe nt terms, we can calculate the joint probability of the cluster and the query, and then choose the semantic cluster with highest joint probability. where MS j is the semantic cluster that is the most similar to the user X  X  query, and q i is the i th term in the query. With the selected semantic model MS , we formally estimate the joint probability of a term t and the query q 1 ... q k , P ( t , q 1 ... q joint probability will be used as query expansion terms. Based on Bayes rule and expressed in the log likelihood, we get, 
The estimation of P ( q i | t ) is as follows, where MS j are the semantic models selected in equation (3), P ( MS j | t ) is as follows: P ( MS j ) can be represented either by the entropy of each cluster or by assuming uniform over the distri bution of all semantic clusters. In this paper, we adopt uniform distribution over the universe of semantic clusters. P ( q i | MS j ) in equation (5) or P ( t | MS (6) have the same calculation as follow: 
As shown in equation (7), we select the candidates of the expansion terms based on two probabilities: P ( MD | MS ), which is the probability of a document model MD being generated given a semantic model MS that is the most similar to the query; and P ( t | MD ), which is the probability of term t being generated given a selected document model. The calculation of P ( MD | MS ) has been given in equation (1). 
The semantic term cluster model Term _ Clu MS as well as the collection model Coll is linearly interpolated to smooth the selected document model MD as follows: 
The term prior probability P ( t ) is estimated as the summation over all semantic models: We conducted experiment s on five TREC data sets: Associated Press Newswire (AP) 1988-90 w ith query topics 51-200, Wall Street Journal (WSJ) 1987-92 with query topics 51-200, Financial Times (FT) 1991-94 with query topics 301-400, San Jose Mercury News (SJMN) 1991 with query t opics 51-150, and Los Angeles Times (LA) with query topics 301-400. For all collections, the title field of the TREC topics was used as the query. Queries that have no relevant documents in the judged pool for a specific collection had been removed from the query set for that collection. A summary of the five collections and the query sets can be found in [6]. Indri 2.9 system was used for indexing and retrieval. All collections and queries were ste mmed using Porter stemmer, and stop-words were removed as well. 
We used queries 51-150 and the AP collection for parameters training, and used the remaining queries 151-200 on AP collection and all other queries and the rest of the collections for testing. The initial query results were generated using the basic query likelihood language model. The implementation of ICA algorithm was from DTU : Toolbox [3]. Our baselines include the basic query likelihood language model, Indri X  X  implementation of Lavrenko X  X  relevance based la nguage model, and two other cluster-based retrieval models from the related literature. 
Top 50 documents from the initial retrieval results were selected for semantic clusteri ng using ICA. The optimal number k of the topic was estimated by Bayes information criterion. A term is selected to be in a semantic term cluster if its probability in equation (2) is greater than thre shold 0.3. According to equation (4), a number n of terms t 1 ... t n , with probabilities p selected for query expansion. Th e expansion query in indri query form is: where parameter  X  controls the relative importance between the expansion terms and the original query. For reducing computation load, we set a fixed  X  = 0.5 which can generate reasonable and stable performance according to [7]. The terms number n is then determined by an exhaustive search to look for the optimal values [5, 6]. The parameters training results were: 1) with semantic clustering-based relevance language model (SRM), the number of feedback documents d = 50, and the number of feedback terms n = 75; 2) with relevance language model (RM), d = 25, n = 100. as 1 0.45  X  = , 
Rret 
MAP 0.2106 +56.32 0.2307 +42.70 11Avg 0.2285 +50.90 0.2440 +41.31 Table 1 gives the retrieval performance of SRM against that of RM and the basic query likelihood language model (LM) on the training topics. It shows that SRM has statistically significant improvements over LM and RM on all the measures. However, there is no significant difference between RM and LM on all the measures except the number of relevant retrieved documents. Table 2 gives the performance comparisons using MAP among SRM, RM, LM, CBDM method [6] and Resampling method [5] on the five test collections. The Uppe r in the last column refers to the upper bound performance on each collection when using SRM. We selected 50 true relevant documents based on ground truth as the feedback documents to form such upper bound. This upper bound helps us to establish the best results that our semantic clustering-based relevance model can achieve when all provided are true relevant documents. 
Table 2 shows that SRM met hod has significant improvements over LM and RM on all the co llections whereas RM has no significant difference over LM on all the collections. Although no Wilcoxon test was conducted between our results and that of CBDM or Resampling method, from the large difference between the results, we believe that our method has clear advantages in choosing better relevant documen ts over these two methods. A large gap to the upper bound indicates that there is still a large room for further improving SRM method if we concentrate on better selection of topic-rela ted documents. Current SRM still either misses true relevance documents or selects some non-relevant documents during the feedback. Figure 1. Comparison of retrie val performance in training and testing. In each plot, the RM and SRM methods are compared with the basic language model. 
Figure 1 shows the comparison of retrieval performance in precision-recall curves. In both tr aining and testi ng phases, SRM always shows an overwhelming better performance than LM and RM. Only at high recall side on WSJ and FT collection, SRM shows a little bit worse performance than that of LM and RM. The LA collection appears to be a specific collection for relevance based methods. RM could hardly improve the performance over LM on this collection, and achieved even worse results than LM at the low recall side. SRM also struggled on the LA collection because its performance had a far distance to the upper bound, and its improvements ove r RM and LM were not as salient as that of on other coll ections. This observation indicates that different collections require different relevant feedback parameters, and how to obtain bette r training parameters that fit well with different collections is still an open problem. 
In brief, SRM works better than LM and RM at low recall and high precision side in all cases. This indicates that almost all documents in our semantic cluster are relevant, which help to boost the expanded queries to be mo re relevant to the information need so that more relevant documents are pushed to the top of the ranked lists. Our semantic clusteri ng relevance model, therefore, is an attractive choice in high precision applications. However, when high recall is needed (larger than 0.7), our method, like other methods in comparison, drew too much noise in the feedback so that the precision of the results falls quickly. A better ICA algorithm may be needed in this situation. 
By assuming that a set of rele vant expansion terms would be topically closer to the true topic than the non-relevant expansion term set, we can evaluate the quality of the semantic clusters. To define a topic distance, we examine the topics containing the expansion terms generated from either RM or SRM and two assumed topics. One assumed topi c stands for a random non-topical model, approximated by the bac kground collection model, written topic, written as C oll rel . The terms are sampled from topic model Topic SRM or Topic RM. generated by SRM or RM respectively. The distance can be measured by KL divergence. where V is the vocabulary of a collection, Topic is either Topic SJMN, FT, LA}. We expect that better expansion terms would generate smaller distance to Coll rel but larger distance to Coll Coll 
Table 3 gives the results of the comparison. We observe that SRM Table 3) than RM, which indicate that our expansion terms are far from random topic than that of RM. Meanwhile, SRM has a smaller average topic distance to Coll rel than RM, which indicates that our expansion terms are closer to the true topic than that of RM. This demonstrates that the feedback documents selected by our method are closer to the true relevan ce documents. We think that it is because SRM can filter out the collection noise with its observation of keeping distance to random non-relevant topics, which lacks in the RM method. 
In this paper, we proposed a se mantic cluster based relevance language model that uses pse udo relevance feedback without requiring relevance training informa tion. Our experiments show that it is more effective to sample terms from documents belonging to the semantic topic clusters activated by user X  X  query than to sample blindly from all top ranked documents. We think that the main contribution of the improvement of retrieval performance comes from the estimation of relevance model using the activated clusters that are semantically related to the query. Our future work include: studying better ICA algorithms that are more stable and can better separate latent topics in documents ; exploring our intuition that the probability of a cluster being generated can play a positive role in relevance model estimation; and finally examining the tuning of the parameters to be closer to the upper-bound of the performance. This work was partially supported by China Scholarship Council, the University of Pittsburgh and NSF under NSF/IIS 0704628. 
