 Statistical mac hine translation tak es a source se-quence, S = [ s 1 s 2 . . . s K ], and generates a target sequence, T  X  = [ t 1 t 2 . . . t L ], by finding the most likely translation given by: 1.1 Block selection Recen t statistical mac hine translation (SMT) al-gorithms generate suc h a translation by incorp o-rating an inventory of bilingual phrases (Oc h and Ney , 2000). A m -n phrase-pair, or blo ck, is a se-quence of m source words paired with a sequence of n target words. The inventory of blo cks in cur-ren t systems is highly redundan t. We illustrate the redundancy using the example in Table 1 whic h Figure 1: Example of Arabic snip et and alignmen t to its English translation. sho ws a set of phrases that cover the two-w ord Arabic fragmen t  X  lljnp Almr kzyp  X  whose align-men t and translation is sho wn in Figure 1. One notices the significan t overlap between the vari-ous blo cks including the fact the output target se-quence  X  of the centr al committee  X  can be pro-duced in at least two differen t ways: 1) as 2-4 blo ck  X  lljnp Almr kzyp j of the centr al committee  X  cov-ering the two Arabic words, or 2) by using the 1-3 blo ck  X  Almr kzyp j of the centr al  X  follo wed by covering the first Arabic word with the 1-1 blo ck  X  lljnp j committee  X . In addition, if one adds one more word to the Arabic fragmen t in the third posi-tion suc h as the blo ck  X  AlSyny j chinese  X  the over-lap increases significan tly and more alternate possi-bilities are available to pro duce an output suc h as the  X  of the centr al chinese committee . X 
In this work, we prop ose to only use 1-n blo cks and avoid completely the redundancy obtained by the use of m-n blo cks for m &gt; 1 in curren t phrase-based sys-tems. We discuss later how by defining appropriate features in the translation mo del, we capture the im-portan t dep endencies required for pro ducing n -long fragmen ts for an m -word input sequence including the reordering required to pro duce more fluen t out-put. So in Table 1 only the blo cks corresp onding to a single Arabic word are in the blo ck inventory . To differen tiate this work from previous approac hes in Table 1: Example Arabic-English blo cks sho wing possible 1-n and 2-n blo cks rank ed by frequency . Blo ck coun t is given in () for 2-n blo cks. direct mo deling for mac hine translation, we call our curren t approac h DTM2 (Direct Translation Mo del 2). 1.2 Statistical modeling for translation Earlier work in statistical mac hine translation (Bro wn et al., 1993) is based on the  X  X oisy-c hannel X  form ulation where T  X  = arg max where the target language mo del p ( T ) is further de-comp osed as where k is the order of the language mo del and the translation mo del p ( S j T ) has been mo deled by a sequence of five mo dels with increasing complexit y (Bro wn et al., 1993). The parameters of eac h of the two comp onen ts are estimated using Maxim um Lik e-liho od Estimation (MLE). The LM is estimated by coun ting n-grams and using smo othing techniques. The translation mo del is estimated via the EM algo-rithm or appro ximations that are bootstrapp ed from the previous mo del in the sequence as introduced in (Bro wn et al., 1993). As is well kno wn, impro ved results are achiev ed by mo difying the Bayes factor-ization in Equation 1 above by weighing eac h distri-bution differen tly as in: This is the simplest MaxEn t 1 mo del that uses two feature functions. The parameter  X  is tuned on a dev elopmen t set (usually to impro ve an error met-ric instead of MLE). This mo del is a special case of the Direct Translation Mo del prop osed in (Pap-ineni et al., 1997; Papineni et al., 1998) for language understanding; (Foster, 2000) demostrated perplex-ity reductions by using direct mo dels; and (Oc h and Ney , 2002) emplo yed it very successfully for language translation by using about ten feature functions: Man y of the feature functions used for translation are MLE mo dels (or smo othed varian ts). For example, if one uses  X  1 = log ( p ( T )) and  X  2 = log ( p ( S j T )) we get the mo del describ ed in Equation 2. Most phrase-based systems, including the baseline deco der used in this work use feature functions: The weigh t vector  X  is estimated by tuning on a rather smal l (as compared to the training set used to define the feature functions) dev elopmen t set using the BLEU metric (or other translation error met-rics). Unlik e MaxEn t training, the metho d (Oc h, 2003) used for estimating the weigh t vector for BLEU maximization are not computationally scalable for a large num ber of feature functions. Most recen t state-of-the-art mac hine translation de-coders have the follo wing asp ects that we impro ve upon in this work: 1) blo ck style, and 2) mo del pa-rameterization and parameter estimation. We dis-cuss eac h item next. 2.1 Block style In order to extract phrases from alignmen ts available in one or both directions, most SMT approac hes use a heuristic suc h as union , interse ction , inverse pro-jection constr aint , etc. As discussed earlier, these approac hes result in a large overlap between the ex-tracted blo cks (longer blo cks overlap with all the shorter sub comp onen ts blo cks). Also, sligh tly re-stating the adv antages of phrase-pairs iden tified in (Quirk and Menezes, 2006), these blo cks are effec-tive at capturing con text including the enco ding of non-comp ositional phrase pairs, and capturing local reordering, but they lack variables (e.g. embedding between ne . . . pas in Frenc h), have sparsit y prob-lems, and lack a strategy for global reordering. More recen tly, (Chiang, 2005) extended phrase-pairs (or blo cks) to hierarc hical phrase-pairs where a grammar with a single non-terminal allo ws the embedding of phrases-pairs, to allo w for arbitrary embedding and capture global reordering though this approac h still has the high overlap problem. However, in (Quirk and Menezes, 2006), the authors investigate mini-mum translation units (MTU) whic h is a refinemen t over a similar approac h by (Banc hs et al., 2005) to eliminate the overlap issue. The MTU approac h picks all the minimal blo cks sub ject to the condition that no word alignmen t link crosses distinct blo cks. They do not have the notion of a blo ck with a vari-able (a special case of the hierarc hical phrase-pairs) that we emplo y in this work. They also have a weak-ness in the parameter estimation metho d; they rely on an n-gram language mo del on blo cks whic h inher-ently requires a large bilingual training data set. 2.2 Estimating Model Parameters Most recen t SMT systems use blo cks (i.e. phrase-pairs) with a f ew real valued  X  X nformativ e X  features whic h can be view ed as an indicator of how proba-ble the curren t translation is. As discussed in Sec-tion 1.2, these features are typically MLE mo dels (e.g. blo ck translation, Mo del 1, language mo del, etc.) whose scores are log-linearly com bined using a weigh t vector,  X  f where f is a particular feature. The  X  f are trained using a held-out corpus using maxim um BLEU training (Oc h, 2003). This metho d is only practical for a small num ber of features; typ-ically , the num ber of features is on the order of 10 to 20.

Recen tly, there have been sev eral discriminativ e approac hes at training large parameter sets includ-ing (Tillmann and Zhang, 2006) and (Liang et al., 2006). In (Tillmann and Zhang, 2006) the mo del is optimized to pro duce a blo ck orien tation and the target sen tence is used only for computing a sen tence level BLEU. (Liang et al., 2006) demonstrates a dis-criminativ ely trained system for mac hine translation that has the follo wing characteristics: 1) requires a varying update strategy (local vs. bold) dep ending on whether the reference sen tence is  X  X eac hable X  or not, 2) uses sen tence level BLEU as a criterion for se-lecting whic h output to update towards, and 3) only trains on limited length (5-15 words) sen tences.
So both metho ds fundamen tally rely on a prior deco der to pro duce an  X  X -b est X  list that is used to find a target (using max BLEU) for the training al-gorithm. The metho ds to pro duce an  X  X -b est X  list tend to be not very effectiv e since most alternativ e translations are minor differences from the highest scoring translation and do not typically include the reference translation (particularly when the system mak es a large error).

In this pap er, the algorithm trains on all sen tences in the test-sp ecific corpus and crucially , the algo-rithm directly uses the target translation to update the mo del parameters. This latter point is a critical difference that con trasts to the ma jor weakness of the work of (Liang et al., 2006) whic h uses a top-N list of translations to select the maxim um BLEU sen tence as a target for training (so called local update). In (Bro wn et al., 1993), multi-w ord  X  X epts X  (whic h are realized in our blo ck concept) are discussed and the authors state that when a target sequence is sufficien tly differen t from a word by word transla-tion, only then should the target sequence should be promoted to a cept. This is in direct opp osition to phrase-based deco ders whic h utilize all possible phrase-pairs and limit the num ber of phrases only due to practical considerations. Follo wing the per-spectiv e of (Bro wn et al., 1993), a minimal set of phrase blo cks with lengths ( m, n ) where either m or n must be greater than zero results in the follo wing types of blo cks: 1. n = 0, source word pro ducing nothing in the 2. m = 0, spontaneous target word (insertion 3. m = 1 and n 1, a source word pro ducing n 4. m 1 and n = 1, a sequence of source words 5. m &gt; 1 and n &gt; 1, a non-comp ositional phrase In this pap er, we restrict the blo cks to Types 1 and 3. From the example in Figure 1, the follo wing blo cks are extracted: These blo cks can now be considered more  X  X eneral X  and can be used to generate more phrases compared to the blo cks sho wn in Table 1. These blo cks when utilized indep enden tly of the remainder of the mo del perform very poorly as all the adv antages of blo cks are absen t. These adv antages are obtained using the features to be describ ed below. Also, we store with a blo ck additional information suc h as: (a) alignmen t information, and (b) source and target analysis. The target analysis includes part of speech and for eac h target string a list of part of speech sequences are stored along with their corpus frequencies.

The first alignmen t sho wn in Figure 1 is an exam-ple of a Type 5 non-comp ositional blo ck; although this is not curren tly addressed by the deco der, we plan to handle suc h blo cks in the future. A classification problem can be considered as a map-ping from a set of histories, S , into a set of futures, T . Traditional classification problems deal with a small finite set of futures usually no more than a few thousands of classes.

Mac hine translation can be cast into the same framew ork with a much larger future space. In con-trast to the curren t global mo dels, we decomp ose the pro cess into a sequence of steps. The pro cess begins at the left edge of a sen tence and for practical rea-sons considers a windo w of source words that could be translated. The first action is to jump a distance, j to a source position and to pro duce a target string, t corresp onding to the source word at that position. The pro cess then marks the source position as hav-ing been visited and iterates till all source words have been visited. The only wrinkle in this relativ ely sim-ple pro cess is the presence of a variable in the tar-get sequence. In the case of a variable, the source position is mark ed as having been partially visited. When a partially visited source position is visited again, the target string to the righ t of the variable is output and the pro cess is iterated. The distortion or jump from the previously translated source word, j in training can vary widely due to automatic sen tence alignmen t that is used to create the parallel corpus. To limit the sparseness created by these longer jumps we cap the jump to a windo w of source words (-5 to 5 words) around the last translated source word; jumps outside the windo w are treated as being to the edge of the windo w.

We com bine the above translation mo del with a n -gram language mo del as in This mixing allo ws the use of language mo del built from a very large monolingual corpus to be used with a translation mo del whic h is built from a smaller parallel corpus. In the rest of this pap er, we are concerned only with the translation mo del.

The minim um requiremen ts for the algorithm are (a) parallel corpus of source and target languages and (b) word-alignmen ts. While one can use the EM algorithm to train this hidden alignmen t mo del (the jump step), we use Viterbi training, i.e. we use the most likely alignmen t between target and source words in the training corpus to estimate this mo del. We assume that eac h sen tence pair in the training corpus is word-aligned (e.g. using a MaxEn t aligner (Itt ycheriah and Rouk os, 2005) or an HMM aligner (Ge, 2004)). The algorithm performs the follo wing steps in order to train the maxim um entrop y mo del: (a) blo ck extraction, (b) feature extraction, and (c) parameter estimation. Eac h of the first two steps requires a pass over the training data and param-eter estimation requires typically 5-10 passes over the data. (Della Pietra et al., 1995) documen ts the Impro ved Iterativ e Scaling (IIS) algorithm for train-ing maxim um entrop y mo dels. When the system is restricted to 1-N type blo cks, the future space in-cludes all the source word positions that are within the skip windo w and all their corresp onding blo cks. The training algorithm at the parameter estimation step can be concisely stated as: 1. For eac h sen tence pair in the parallel corpus, 2. At eac h source word, the alignmen t iden tifies the 3. Form a windo w of source words and allo w all 4. Apply the features relev ant to eac h blo ck and 5. Form the MaxEn t polynomials(Della Pietra et We will next discuss the prior distribution used in the maxim um entrop y mo del, the blo ck extraction metho d and the feature generation metho d and dis-cuss differences with a standard phrase based de-coder. 4.1 Prior Distribution Maxim um entrop y mo dels are of the form, where p 0 is a prior distribution, Z is a normalizing term, and  X  i ( t , j, s ) are the features of the mo del. The prior distribution can con tain any information we kno w about our future and in this work we utilize the normalized phrase coun t as our prior. Strictly , the prior has to be uniform on the set of futures to be a  X  X axim um X  entrop y algorithm and choices of other priors result in minim um div ergence mo dels. We refer to both as a maxim um entrop y mo dels.
The practical benefit of using normalized phrase coun t as the prior distribution is for rare transla-tions of a common source words. Suc h a translation blo ck may not have a feature due to restrictions in the num ber of features in the mo del. Utilizing the normalized phrase coun t prior, the mo del is still able to penalize suc h translations. In the best case, a fea-ture is presen t in the mo del and the mo del has the freedom to either boost the translation probabilit y or to further reduce the prior. 4.2 Block Extraction Similar to phrase deco ders, a single pass is made through the parallel corpus and for eac h source word, the target sequence deriv ed from the alignmen ts is extracted. The  X  X nverse Pro jection Constrain t X , whic h requires that the target sequence be aligned only to the source word or phrase in question, is then chec ked to ensure that the phrase pair is consisten t. A sligh t relaxation is made to the traditional target sequence in that variables are allo wed if the length of their span is 3 words or less. The length restriction is imp osed to reduce the effect of alignmen t errors. An example of blo cks extracted for the romanized ara-bic words  X  X ljnp X  and  X  X lmrkzyp X  are sho wn Figure 2, where on the left side are sho wn the unsegmen ted Arabic words, the segmen ted Arabic stream and the corresp onding Arabic part-of-sp eech. On the righ t, the target sequences are sho wn with the most fre-quen tly occuring part-of-sp eech and the corpus coun t of this blo ck.

The extracted blo cks are pruned in order to min-imize alignmen t problems as well as optimize the speed during deco ding. Blo cks are pruned if their corpus coun t is a factor of 30 times smaller than the most frequen t target sequence for the same source word. This results in about 1.6 million blo cks from an original size of 3.2 million blo cks (note this is much smaller than the 50 million blo cks or so that are deriv ed in curren t phrase-based systems). 4.3 Features The features investigated in this work are binary questions about the lexical con text both in the source and target streams. These features can be classi-fied into the follo wing categories: (a) blo ck internal features, and (b) blo ck con text features. Features can be designed that are specific to a blo ck. Suc h features are mo deling the unigram phrase coun t of the blo ck, whic h is information already presen t in the prior distribution as discussed above. Features whic h are less specific are tied across man y transla-tions of the word. For example in Figure 2, the pri-mary translation for  X  X ljnp X  is  X  X ommittee X  and occurs 920 times across all blo cks extracted from the corpus; the final blo ck sho wn whic h is  X  X f the X committee X  occurs only 37 times but emplo ys a lexical feature  X  X ljnp committee X  whic h fires 920 times. 4.3.1 Lexical Features
Lexical features are blo ck internal features whic h examine a source word, a target word and the jump from the previously translated source word. As dis-cussed above, these are shared across blo cks. 4.3.2 Lexical Context Features
Con text features enco de the con text surrounding a blo ck by examining the previous and next source word and the previous two target words. Unlik e a traditional phrase pair, whic h enco des all the infor-mation lexically , in this approac h we define in Ta-ble 2, individual feature types to examine a por-tion of the con text. One or more of these features may apply in eac h instance where a blo ck is relev ant. The previous source word is defined as the previously translated source word, but the next source word is alw ays the next word in the source string. At train-ing time, the previously translated source word is found by finding the previous target word and utiliz-ing the alignmen t to find the previous source word. If the previous target word is unaligned, no con text feature is applied. 4.3.3 Arabic Segmen tation Features An Arabic segmen ter pro duces morphemes; in Arabic, prefixes and suffixes are used as prep ositions, pronouns, gender and case mark ers. This pro duces a segmen tation view of the arabic source words (Lee et al., 2003). The features used in the mo del are formed from the Cartesian pro duct of all segmen tation to-kens with the English target sequence pro duced by this source word or words. However, prefixes and suffixes whic h are specific in translation are limited to their English translations. For example the pre-fix  X  X l# X  is only allo wed to participate in a feature with the English word  X  X he X  and similarly  X  X he X  is not allo wed to participate in a feature with the stem of the Arabic word. These restrictions limit the num-ber of features and also reduce the over fitting by the mo del. 4.3.4 Part-of-sp eech Features
Part-of-sp eech taggers were run on eac h language: the English part of speech tagger is a MaxEn t tag-ger built on the WSJ corpus and on the WSJ test set achiev es an accuracy of 96.8%; the Arabic part of speech tagger is a similar tagger built on the Ara-bic tree bank and achiev es an accuracy of 95.7% on automatically segmen ted data. The part of speech feature type examines the source and target as well as the previous target and the corresp onding previ-ous source part of speech. A separate feature type examines the part of speech of the next source word when the target sequence has a variable. 4.3.5 Coverage Features
These features examine the coverage status of the source word to the left and the source word to the righ t. During training, the coverage is determined by examining the alignmen ts; the source word to the left is unco vered if its target sequence is to the righ t of the curren t target sequence. Since the mo del em-ploys binary questions and predominan tly the source word to the left is already covered and the righ t source word is unco vered, these features fire only if the left is open or if the righ t is closed in order to minimize the num ber of features in the mo del. A beam searc h deco der similar to phrase-based sys-tems (Tillmann and Ney , 2003) is used to translate the Arabic sen tence into English. These deco ders have two parameters that con trol their searc h strat-egy: (a) the skip length (ho w man y positions are al-lowed to be untranslated) and (b) the windo w width, whic h con trols how man y words are allo wed to be considered for translation. Since the ma jorit y of the blo cks emplo yed in this work do not enco de local re-ordering explicitly , the curren t DTM2 deco der uses a large skip (4 source words for Arabic) and tries all possible reorderings. The primary difference be-tween a DTM2 deco der and standard phrase based deco ders is that the maxim um entrop y mo del pro-vides a cost estimate of pro ducing this translation using the features describ ed in previous sections. An-other difference is that the DTM2 deco der handles blo cks with variables. When suc h a blo ck is pro-posed, the initial target sequence is first output and the source word position is mark ed as being partially visited and an index into whic h segmen t was gener-ated is kept for completing the visit at a later time. Subsequen t extensions of this path can either com-plete this visit or visit other source words. On a searc h path, we mak e a further assumption that only one source position can be in a partially visited state at any point. This greatly reduces the searc h task and suffices to handle the type of blo cks encoun tered in Arabic to English translation. The UN parallel corpus and the LDC news corp ora released as training data for the NIST MT06 eval-uation are used for all evaluations presen ted in this pap er. A variet y of test corp ora are now available and we use MT03 as dev elopmen t test data, and test results are presen ted on MT05. Results obtained on MT06 are from a blind evaluation. For Arabic-English, the NIST MT06 training data con tains 3.7M sen tence pairs from the UN from 1993-2002 and 100K sen tences pairs from news sources. This represen ts the univ erse of training data, but for eac h test set we sample this corpus to train efficien tly while also observing sligh t gains in performance. The training univ erse is time sorted and the most recen t corp ora are sampled first. Then for a given test set, we obtain the first 20 instances of n-grams from the test that occur in the training univ erse and the resulting sam-pled sen tences then form the training sample. The con tribution of the sampling technique is to pro duce a smaller training corpus whic h reduces the compu-tational load; however, the sampling of the univ erse of sen tences can be view ed as test set domain adapta-tion whic h impro ves performance and is not strictly done due to computational limitations 2 . The 5-gram language mo del is trained from the English Giga word corpus and the English portion of the parallel corpus used in the translation mo del training.

The baseline deco der is a phrase-based deco der that emplo ys n -m blo cks and uses the same test set specific training corpus describ ed above. 6.1 Feature Type Experimen ts There are 15 individual feature types utilized in the system, but in order to be brief we presen t the re-sults by feature groups (see Table 3): (a) lexical, (b) lexical con text, (c) segmen tation, (d) part-of-sp eech, and (e) coverage features. The results sho w im-pro vemen ts with the addition of eac h feature set, but the part-of-sp eech features and coverage features are not statistically significan t impro vemen ts. The more complex features based on Arabic segmen tation and English part-of-sp eech yield a small impro vemen t of 0.5 BLEU points over the mo del with only lexical con text.
Table 4: Errors on last 25 sen tences of MT-03. We analyzed the errors in the last 25 sen tences of the MT-03 dev elopmen t data using the broad categories sho wn in Table 4. These error types are not indep en-den t of eac h other; indeed, incorrect verb placemen t is just a special case of the word order error type but for this error analysis for eac h error we tak e the first category available in this list. Word choice er-rors can be a result of (a) rare words with few, or incorrect, or no translation blo cks (4 times) or (b) mo del weakness 3 (22 times). In order to address the mo del weakness type of errors, we plan on investigat-ing feature selection using a language mo del prior. As an example, consider an arabic word whic h pro-duces both  X  X he X  (due to alignmen t errors) and  X  X he conduct X . An n-gram LM has very low cost for the word  X  X he X  but a rather high cost for con ten t words suc h as  X  X onduct X . Incorp orating the LM mo del as a prior should help the maxim um entrop y mo del focus its weigh ting on the con ten t word to overcome the prior information. We have presen ted a complete direct translation mo del with training of millions of parameters based on a set of minimalist blo cks and demonstrated the abilit y to retain good performance relativ e to phrase based deco ders. Tied features minimize the num-ber of parameters and help avoid the sparsit y prob-lems asso ciated with phrase based deco ders. Uti-lizing language analysis of both the source and tar-get languages adds 0.8 BLEU points on MT-03, and 0.4 BLEU points on MT-05. The DTM2 deco der achiev ed a 1.7 BLEU point impro vemen t over the phrase based deco der on MT-06. In this work, we have restricted the blo ck types to only single source word blo cks. Man y city names and dates in Ara-bic can not be handled by suc h blo cks and in future work we intend to investigate the utilization of more complex blo cks as necessary . Also, the DTM2 de-coder utilized the LM comp onen t indep enden tly of the translation mo del; however, in future work we intend to investigate feature selection using the lan-guage mo del as a prior whic h should result in much smaller systems.
