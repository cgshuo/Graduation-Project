 Boosting is a very successful classification algorithm that produces a linear combination of  X  X eak X  classifiers (a.k.a. base learners) to obtain high quality classification models. In this paper we propose a new boosting algorithm where base learners have structure relationships in the functional space. Though such relationships are generic, our work is partic-ularly motivated by the emerging topic of pattern based classification for semi-structured data including graphs. To-wards an efficient incorporation of the structure information, we have designed a general model where we use an undi-rected graph to capture the relationship of subgraph-based base learners. In our method, we combine both L 1 norm and Laplacian based L 2 norm penalty with Logit loss function of Logit Boost. In this approach, we enforce model sparsity and smoothness in the functional space spanned by the basis functions. We have derived efficient optimization algorithms based on coordinate decent for the new boosting formulation and theoretically prove that it exhibits a natural grouping effect for nearby spatial or overlapping features. Using com-prehensive experimental study, we have demonstrated the effectiveness of the proposed learning methods.
 H.2.8 [ Database Management ]: Database Applications-Data Mining Algorithms, Experimentation L 1 Regularization, Graph Classification, Semi-Structured Data, Boosting, Feature Selection
Boosting is a very successful classification algorithm that produces a linear combination of  X  X eak X  classifiers (a.k.a. Figure 1: Three subgraph features in three graphs. Dashed edge means that the two nodes are con-nected by a path with varying length &gt; 1. base learners) to obtain high quality classification models [7, 9, 34, 35]. Recently, the boosting algorithm has been suc-cessfully extended to tasks such as multi-class classification [23], multi-label classification [39], cost sensitive learning [26], semi-supervised learning [43], manifold learning [24], classification with missing-value [12], and transfer learning [3] among others.

In this paper we propose a new boosting algorithm where base learners have structure relationships in the functional space. Our work is particularly motivated by the emerging topic of pattern based classification for semi-structure data including graphs [16, 31, 36, 38, 40]. For example, Kudo et al. [20] recently applied boosting to graph classification using subgraphs as base learners and showed the connec-tion of graph boosting to support vector machine with the R -convolution kernel. Nowozin et al. [28, 32] combined sub-graph mining and graph boosting for classifying graphs rep-resenting images.

Though graph boosting has demonstrated promising re-sults, the limitations of the current algorithms are that they totally ignore the structure relationships among subgraph base learners and hence may not provide the optimal results for graph classification. We illustrate the point with the following example:
Consider the three labeled graphs G 1 , G 2 , G 3 and three subgraph features F 1 , F 2 , F 3 shown in Figure 1. Suppose that the class labels for graphs G 1 , G 2 , G 3 are Y = [1 , 1 , We may construct three base learners h 1 ( G ), h 2 ( G ) and h ( G ) in the format h i ( G ) = 1 if F i  X  G and h i ( G ) = otherwise ( i  X  { 1 , 2 , 3 } ). These decision rules are derived based on a majority voting of subgraph coverage on posi-tively and negatively labeled graphs.
Considering a boosting algorithm that iteratively selects base classifiers to build ensemble models, since h 1 is perfectly correlated with class labels as evaluated on the three training samples, h 1 will be selected first. h 2 and h 3 produce the same prediction for all the graphs in the training data set and hence may be perceived to have the same discriminative power. This is not true in this example. Subgraph F 1 and F 2 occur in every positive graph sample and are clustered with a consistent relative spatial position. F 3 occurs in every graph, but in contrast to F 1 and F 2 , it has quite different spatial distribution as compared to F 1 and F 2 and hence we consider F 3 as a spurious pattern. Once F 1 is selected, we argue that we should select F 2 rather F 3 to build more stable and interpretable classification models. However, current boosting methods are not designed to perform such model selection since the structure relationships of base learners are not considered in any case.

The spatial relationship is special cases of possible rela-tionships of base learners. Another example is the partial overlapping relationship. We call the possible information regarding to the relationships of base learners as structure relationships. Here we hypothesize that the structure rela-tionship of subgraph features carries important information regarding the importance of the base learners in boosting. Towards an efficient incorporation of such information, we design a general model where we use an undirected graph to capture the relationship of subgraph-based base learn-ers. We combined L 1 norm and Laplacian based L 2 norm penalty with Logit loss function of Logit Boost [9]. In this approach, we enforce model sparsity and smoothness in the functional space spanned by the basis functions. We de-rive efficient optimization algorithms based on coordinate decent for the new boosting formulation and theoretically prove that it exhibits a natural grouping effect for nearby spatial or overlapping features. Using comprehensive exper-imental study and comparing with the state-of-the-art, we have demonstrated the effectiveness of the proposed learning method.

We believe the new formalization is applicable to a vari-ety of boosting applications where (i) base learners have a known structure relationship and (ii) the optimal ensemble of base learner functions is sparse in the functional space. The proposed method can be naturally extended to other semi-structured data such as sequences and trees where pat-terns such as frequent subsequences and frequent subtrees are widely used for classification [21].
Subgraph based supervised learning on graphs has re-cently attracted extensive research interest [16, 31, 36, 38, 40]. For example, Yan et. al [40] proposed Leap algorithm with two concepts: structural leap search and frequency de-scending to reduce search space and mine informative pat-terns faster than previous methods. However, LEAP only considers individual pattern rather than a set of patterns [16]. Moreover, the discriminate power of a pattern is eval-uated entirely on the occurrence information of the pattern and misses interaction among patterns. gPLS [31] applies partial least square regression to graph mining and performs feature selection and classifier construction simultaneously, but the model interpretability is low due to the use of latent variables [16]. In addition, the structure relationship among features is neglected. COM [16] is a newly proposed method that mines co-occurrence rules. COM is prone to giving high number of false positives and fails to consider the structure information among features as well.

Recently, a significant amount of progress has been made on developing supervised learning algorithms for feature se-lection from data with structured features [4, 15, 18, 22, 33, 37, 41, 42]. In these models, features may be naturally partitioned into groups [4, 15, 41] or ordered in some mean-ingful way, such as a chain [18, 37], a tree [42] or a graph [22, 33]. These approaches demonstrate the importance of incorporating prior structure information among features to build highly accurate and interpretable models. However, all these algorithms handles vector data and hence are not applicable to graphs.

Though subgraph based feature selection on graph data has been studied for a long time, none of the existing method considers the structure relationships among subgraph fea-tures and hence may not provide the optimal results for graph classification. The objective of this paper is to incor-porate the structural information on features into learning and build a more accurate and interpretable graph boosting model.

The rest of the paper is organized as following. In Section 2, we provide some preliminaries on Boosting. In Section 3 we show our details of our methodology. In Section 4 we apply the algorithm and theory developed in Section 3 to graph data and graph classification. In Section 5 we present the experimental study of our algorithm, followed by a con-clusion and a discussion of the future work.
We use the following notations throughout the rest of the paper. We use lowercase letters to represent scalar values, lower-case letters with an arrow to represent vectors (e.g.  X   X  ), uppercase letters to represent matrices, {  X ,  X  1 ,  X  represent Lagrange multiplier, and uppercase calligraphic letters to represent sets. Unless state otherwise, all vectors in this paper are column vectors.

Given training instances T = { x i , y i } n i =1 where y x i  X  X , we construct a set of base learners H = { h j : X 7 X  { X  1 , +1 } , j = 1  X  X  X  p } . In this paper, we do not as-sume any type of X ; it may be a vector space, or simply a set. The objective of boosting is to train a composite binary classifier with weight vector  X   X  taking the form of h ( x i ) = sgn ( pirical loss function  X  ( X ,  X  X  ;  X   X  ) is minimized. where l is a loss function.

AdaBoost [8] takes the exponential loss function: and LogitBoost [9] takes the logit loss function: Duchi et. al [4] modified AdaBoost by imposing L 1 / L 2 or L / L  X  penalty on weight vectors in a multi-task learning framework. However, they neglect the structure informa-tion among base learners. We consider a simple yet effective modification to Logit Boost [9] that incorporates a com-posite penalty with L 1 and L 2 regularization encoding the structural information among base learners on the weight vector, which is detailed in the following section.
We capture the structure relationships among base learn-ers as an undirected graph G , whose nodes correspond to the set of p base learners. Edges in the graph G are weighted, with w i,j indicating the X  X loseness X  X etween the two features and 0 indicating that the two features have no relationship. We call the graph G  X  X eature graph X  and explore approaches for building a feature graph in Section 4.2.

We incorporate the priori domain knowledge by adding a Tikhonov regularization factor 1 2 convex fitness function  X  ( X ,  X  X  ;  X   X  ) to enforce that the fea-ture coefficients vary smoothly for neighboring features. The Tikhonov regularization factor could be conveniently writ-ten in matrix format  X   X  T L  X   X  where L is the Laplacian of G given by: L = D  X  W . W is the p by p edge weight matrix W = ( w i,j ) p i,j =1 , and D is the density matrix of W , defined
To avoid having any feature  X  X ominate X  the penalization function, we use the normalized Laplacian L following [2] to normalize the weight of each feature, where the elements of L are defined by
Tikhonov regularization does not lead to the sparsity of the model. To obtain a sparse solution, we add the L 1 norm of  X  identify a vector  X   X  that minimizes the following loss function: tation, we use the logitloss [9]: The major challenge in fitting the model described in Equation (5,6) to data is to estimate the parameter  X   X  effi-ciently and accurately. In the following subsection, we pro-vide the optimization algorithm.
We discuss the optimization algorithm for Equation (5) bellow. We first show that the structurally regularized boost-ing with logit loss function can be interpreted as an additive logistic regression with the same regularization in the func-tional space spanned by base learners. We then provide the optimization algorithm based on coordinated decent to solve the equivalent regularized logistic regression towards the base learners. For simplicity, let F ( x ) = decision function on the sample x . For a fixed training data set, we denote all the predicted labels for the training data using functions in H as an n by p matrix H , where n is the sample size, p is the number of base learners. H i,j = h j is the label given by base learner h j  X  H on the training sample x i . We call H  X  X bject-prediction X  matrix. We use H i to denote i th row of object-prediction matrix H (the pre-dictions of all the base learners on the sample x i ) and H to represent j th column of H (the predictions of h j on the training data).

We use the following Lemma to show that the minimizer of the expected loss function J ( F ) = E (log(1+exp (  X  yF ( x )))) is the symmetric logistic transform of P ( y = 1 | x ). Lemma 3.1. E (log(1 + exp (  X  yF ( x )))) is minimized at P ( y =  X  1 | x ) = 1
Proof. Since E imposes expectation over the joint dis-tribution of y and x , we have E (log(1 + exp  X  yF ( x ))) = P ( y = 1 | x ) log(1 + exp (  X  yF ( x ))) + P ( y =  X  1 | exp ( yF ( x ))). Then it is sufficient to minimize J ( F ) by com-puting the first derivative with respect to F ( x ):  X  X  ( F ) ting the derivative to zero.

With Lemma 3.1, the structurally regularized boosting can be interpreted as logistic regression with the same reg-ularization function. Let y  X  = ( y + 1) / 2, taking values of 0, 1, and parameterize the binomial probabilities by P ( y = 1 | x ) = p ( x ) = 1 logit loss function is equivalent to negative binomial log-likelihood: l b ( y log(1 + e F ( x ) )  X  y  X  F ( x ). Now we rewrite (5) in terms of negative binomial log-likelihood with y  X  : g ( X ,  X  X  ;  X   X  ) =
After transforming logit loss to negative binomial log-likelihood, we followed the general framework of coordinated decent algorithm proposed in [10] recently proposed by Fried-man et al. for L 1 norm regularized logistic regression. Their approach relies on the connection between the Newton X  X  method for optimizing logistic regression and the least square formulation. The Newton X  X  method amounts to using Tay-lor expansion, up to a quadratic function, to approximate the logit function. In this way, applying Newton X  X  method can be viewed as solving a series of least squares problem (also called iterative reweighted least squares tting [10]). Applying Taylor X  X  expansion at current estimate  X   X   X  to nega-tive log-likelihood function (7), we have the reweighted least square problem: where z i = H i  X   X   X  + ( y  X  i  X   X  p ( x i )) / (  X  p ( x  X  p ( x i )(1  X   X  p ( x i )) and C (  X   X   X  ) is a constant.
In the remaining discussion, we show an extension of Fried-man X  X  work to solve a reweighted least square fitting (9) with Laplacian weighted L 2 and L 1 norm regularization. To han-dle the new mixture penalty, we derive a modified coordinate descent scheme in Lemma 3.2 extending the work presented in [10].

Lemma 3.2. Suppose that the data set contains n obser-vations and p predictors, with the response vector Y = ( y , y n ) T and the data matrix X = (  X  X  1 ,  X  X  X  ,  X  X  n ) T . We also as-sume that the predictors are standardized and the response is centered so that for all j , i =1 y i = 0 . The Lagrange form of the network constrained objective function (with least squares tness function) is: The coordinate-wise update has the form (for each  X  j ):  X  S (  X   X  y contribution from x ij and S ( z,  X  ) = sign ( z )( | z | X  soft thresholding operator where: sign ( z )( | z | X   X  ) + =
Suppose that we have estimates of  X   X  l for l  X  = j and we wish to partially optimize the objective function with respect to  X  . We would like to compute the gradient at  X  j =  X   X  j , equation 10 is given by Since X is standardized, by setting 11 to 0, we obtain  X  j exists for  X   X  j &lt; 0. Combining two cases we will get Lemma 3.2.
 We notice that our solution is not constrained in L 1 and L 2 penalty, but can be extended to L  X  , which recently at-tracted research interest [4], since L  X  norm is differentiable everywhere except singular points (  X   X  = 0) [45].
We summarize what is discussed previously in the algo-rithm called LPGB. Given the training data T = {X ,  X  X  } , the n by p object-prediction matrix H = { h i,j } = { h j ( x structed from base learners, regularization parameters  X  1  X  2 and convergence parameter  X  , our algorithm iteratively solves (8). Here we transform  X  X  to  X  y  X  using 0/1 to repre-sent the outcome and p ( x ) = P ( y = 1 | x ) = P ( y  X  = 1 1 / (1 + exp (  X 
As evaluated in our experimental study in Section 5, the regularized LPGB algorithm usually has better classification performance and are insensitive to outliers and class label noises, comparing to the unregularized gBoosting [20]. We believe that these advantages are contributed to the capa-bility of LPGB to select clustered base learners in the func-tional space. We call this phenomenon the  X  X rouping effect X  Algorithm 1 LPGB(  X  1 ,  X  2 , H,  X  X   X  , M axIteration,  X  ) 1: Initialize  X   X   X  (0) =  X  0; 2: for i=1 to MaxIteration do 3: Compute the quadratic approximation for (7); 4: Use the coordinate descent method in lemma 3.2 to 6: Break; 7: end if 8: end for 9: return  X   X   X  =  X   X   X  ( i ) ; and we provide theorems to explain the  X  X roup effect X  be-low. Our proof is similar to that presented in [22] where we consider a simple case of two base learners that are linked. We show that the related L 2 regularization ensures that the difference of the estimated coefficients have an upper bound based on the sample size and the regularization coefficients.
We derive an upper bound of the difference of coefficients between two neighboring features. Motivated from a similar proof in [22] where a linear regression framework with L 1 and L 2 regularization, we study the special case in which only two features are connected to each other in the feature graph.

Theorem 3.3. Give training data T = { x i , y i } n i =1 where x  X  X  and xed scalars  X  1 ,  X  2 and let  X   X   X  (  X  1 ,  X  2 ) be the op-timal solution to (8), we suppose that  X   X  i (  X  1 ,  X  2 ) 0 , and the two features F i and F j are only linked to each other on the feature graph. De ne D  X  1 , X  2 ( i, j ) = |  X   X   X  (  X  1 ,  X  2 ) | , then D  X  1 , X  2 ( i, j )  X  2 n/ X  2 .
Proof. Since  X   X  0. More specifically, for  X   X  i and  X   X  j , we have where  X  X  ( X ) = 1 / (1 + exp (  X  H  X   X  )),  X  y  X  = (  X  X  + H is the object-prediction matrix. Subtracting (12) from (13)and taking the absolute value with the assumption that d i,i = d j,j = w i,j and sgn (  X   X  i ) = sgn (  X   X  j ) gives Now consider the worst case, for any x  X  X h i ( x )  X  = h also |  X  y  X   X   X  X  ( X ) | X   X  1, hence D  X  1 , X  2 ( i, j ) = 2 n/ X  2
The upper bound of D  X  1 , X  2 ( i, j ) provides two insights of our method: 1) smoothness: the coefficients of neighboring base learners are close to each other due to the L 2 norm reg-ularized feature graph Laplacian penalty term. 2) Grouping effect: Once a base learner is selected, its spatially neigh-boring base learners will be more likely selected. Thus our boosting algorithm can select groups of spatially neighboring base learners.
We show how to apply the LPGB algorithm to graph clas-sification bellow.
In our model, we use frequent subgraphs as features and construct base learners (decision stamps) from these fea-tures. Given training data {X ,  X  X  } and a set of frequent subgraphs, the decision stamp classifier for subgraph F i given by: The prediction  X  y for F i given training data X is found by: This criteria is to perform a majority voting to obtain pre-diction of the decision stamp based on the percentage of pos-itive (or negative) graphs where the feature occurs. gBoost-ing [20] uses a similar strategy to construct base learners.
One challenge of processing graph data is that there is no natural approach to define the structure relationship of base learners. We notice a few recent studies that are mov-ing towards the direction of defining the relationship among features in graphs and sets. For example in the recently defined graph Graphlet Spectrum kernel [19], the spatial re-lationship of graph feature (called graphlets) are explored in an algebraic framework for measuring the structure sim-ilarity of graph adjacency matrices. In addition, recently developed association net uses a graph model to represent a set of association rules [29]. However, these work could not be directly applied in our current framework since the graphlet spectrum method models the spatial relationship of graphlet in an implicit approach and the association rule net only explore the overlapping relationship of features.
Here we adopted our previous work [5, 6] to construct fea-ture graphs. In [5], we formalize a concept which we called  X  X eature consistency map X . A feature consistency map is a undirected graph in which each node represents a feature and each edge encodes the spatial consistency relationship between two features. We measure the minimum distance between two features using the average shortest path con-necting a node in one feature to a node in the other feature. We compute the variance of the minimal distance between the occurrences of the two subgraphs in the training data. If the variance is bellow a threshold, we consider the two features are in a consistent spatial relationship. In our ex-periment study, we adopt the feature consistency map as an approach to construct a feature graph.

In addition, we also explored the possibility of evaluating the structure-overlapping relationship of features as did in [6]. Towards that end, we compute a kernel function for the set of features. A graph kernel function is a positive semi-definite function that maps graphs to a Hilbert space in order to evaluate the similarity of graphs in the space. Many kernel functions have been designed for graphs and we use the random walk based Marginalized Graph kernel function [17] to compute the kernel function for the set of subgraph features. We convert such kernel matrix to a feature graph where nodes are features and edges are labeled with the inner product (as evaluated with a graph kernel function) of the two features. To avoid a complete connected graph, we use a threshold. If the inner product between two features is less than a threshold, we set the weight of the edge to zero (and hence canceling the edge). The aforementioned approach provides another way to construct a feature graph.
We have performed a rigorous evaluation of our algorithm in terms of modeling accuracy and feature selection perfor-mance using 6 Protein structure data sets, obtained from [16]. We implemented a prototype of our method in Mat-lab. We have compared our method with state-of-the-art methods including Support Vector Machine Recursive Fea-ture Elimination (SVM-RFE) [11], gBoosting [20], graph partial least square regression (gPLS) [31], graph classifica-tion based Pattern Co-occurrence (COM) [16]. We obtained the SVM-RFE executable along with the spider machine learning toolbox from http://www.kyb.tuebingen.mpg.de/ bs/people/spider/ . For gBoosting, we use the gboost tool-box [30]. We obtained gPLS and COM directly from the original authors of the methods. All the experiments were conducted on a PC with a 2.8Ghz duo core CPU and 3GB memory.
To evaluate our methods, we utilized 6 protein-structure graph data sets that were originally studied in [16]. Each data set is a set of geometric graphs representing a set of three-dimensional protein structures. Nodes in such graphs represent amino acids in a protein structure and are labeled with the amino acid type. Edges represent the pairwise Eu-clidian distance of amino acids (defined between C  X  atoms) and are labeled with the discretized distances.

Graphs in the data sets are labeled. Positive samples are sampled from a selected protein family. Negative samples are randomly sampled from the Protein Data Bank. On av-erage a graph contains 250 nodes and 1600 edges. Protein-structure graphs are much larger than chemical-structure graphs, which usually contain about hundreds of nodes and thousands of edges, and contain much large number of pat-terns. Working with protein structure graphs are hence more challenging for constructing sparse predictive models.
In Table 1, we summarize the characteristics of the 6 protein-structure graph data sets. For each data set, we list the data set index, the related protein family ID in the SCOP database [27], the description of the protein family, the number of positive samples and the number of negative samples. See [16] for a comprehensive description of the data collection process. Table 1: Data set: the symbol of the data set. P : total number of positive samples, N : total number of negative samples
We use standard cross validation to generate training and testing data sets. We apply FFSM [14] to generating fre-quent subgraphs from the training data set with min sup = 0 . 30 and with subgraph size between 2 and 6. Such sub-graphs are used as feature for feature based classification(e.g. SVM, SVM RFE) or as base learners for boosting based clas-sification including gBoosting and our methods.

For SVM RFE, we encode each graph sample as a binary feature vector, indexed by the mined subgraphs, with values indicate the presence (1) or absence (0) of the related fea-tures. We perform feature selection using SVM RFE and use LibSVM [1] with linear kernel to construct the best model. We use 5-fold cross validation in the training data set to select important parameter C for SVM.

For COM, we set t p = 0 . 3 and t n = 0 as proposed in [16], where t p is the minimal positive frequency for a classifica-tion rule and t n represents the maximal negative frequency permitted. For gPLS, we use min sup = 0 . 3 and examine the combinations of n = { 2 , 4 , 8 , 16 } and k = { 2 , 4 , 8 , 16 optimal setting. For gBoosting, we also set min sup = 0 . 30 and search the optimal parameter  X  (misclassification cost) in the range of { 0 . 04 , 0 . 06 , . . . , 0 . 18 , 0 . 20 ter selection are based on another 5-fold cross validation on the training data only.

For our own methods, we utilize two approaches to model the spatial correlation of base learners (i.e. subgraphs). The first approach, LPGBK, is to construct a kernel func-tion for the subgraphs, utilizing the the Marginalized ker-nel [17]. The second approach, LPGBCMP, is to construct the feature consistency map, as investigated in [5]. We fix max var = 1 for feature consistency map building thresh-old and  X  = 0 . 25 for overlapping threshold. Empirical study shows that there is no significant change if we change these two parameters within a wide range. Further details of the two spatial correlation computation methods can be found in [5, 6].

Below we summarize the model construction and model evaluation.

Model Construction. For each data set, we partition the data set into 5-folds to perform 5-fold cross-validation (CV) with 4 folds for training and 1 fold for testing. We use another 5-fold CV on the training data set to select the optimal parameters for each method. We then generate a single model from the entire training set with the selected parameters and apply the model to the testing data set for prediction.

Model Comparison. For model comparison, we collect the sensitivity (TP/(TP+FN)), specificity (TN/(TP+FP)) and accuracy ((TP+TN)/ S ) of the trained model, where TP stands for true positive, FP stands for false positive, TN stands for true negative, FN stands for false negative, and S stands for the total number of samples. All the values reported are collected from the testing data set only and are averaged across 10 replicates of the 5-fold cross validation in a total of 50 experiments.
In this subsection, we show the performance of our meth-ods compared with SVM-RFE, gPLS, gBoosting and COM. The accuracy is shown in Fig 2. Since the standard devia-tion is around 2%-5% for all these methods, we do not list it here. Figure 2: Accuracy comparison of on 6 data sets.

In Fig 2, we observe that the accuracy of all these meth-ods has the same trend with different data sets. gBoosting and gPLS have comparable performance in the 6 data sets. SVM RFE outputs gBoosting, gPLS, and COM in three out of six data sets and have comparable performance for the rest. Comparing two versions of our methods, LPGBCMP outperforms LPGBK on all data sets. In fact LPGBCMP performs best among all the evaluated data sets though the margin may be small for 3 data sets when compared with SVM RFE.

To better understand the accuracy differences, we plot the average sensitivity and average specificity of all meth-ods in Fig 3. It is clear that COM provides the best sen-sitivity among the majority of data sets. COM utilizes a rule-based classification algorithm where it classifies a graph sample as positive if a co-occurrence pattern-rule is satisfied. This algorithm is not specific enough, as compared to other methods (shown in the right panel of Fig 3). Interesting enough, all boosting based methods, including gBoosting, LPGBCMP, and LPGBK, have very high specificity com-paring to the rest of the methods. Overall, the regularized boosting methods such as LPGBCMP and LPGBK seem to have a good compromise between specificity and sensitivity. This observation provides experimental evidence supporting our hypothesis that structure information among base clas-sifier should be considered in order to build a highly accurate predictive model for semi-structure data such as graphs.
To evaluate the capability of the LPGBCMP algorithm for selecting grouped base learners, we visualize the spatial distribution of selected base learners in original graphs. By 6HQVLWLYLW\ 6SHFLILFLW\ ranking the base learners by the learned coefficients, we se-lect the top three features for LPGBCMP and gBoosting. We plot the embedding of the three subgraphs in two pro-teins: protein 1EGI and protein 1H8U belonging to the same protein family in Fig 5. We rotate the protein structures and highlight the occurrence of the features with circles for a better demonstration.

In Fig 5, the upper row shows the spatial distribution of the top three features for LPGBCMP in two proteins and the lower row shows the distribution for the top three features from gBoosting. Each column uses the same protein for demonstration. From Fig 5, we observe that F 1 , F 2 and F 3 from LPGBCMP have a consistent spatial distribution on the two proteins. F 1 and F 2 are clustered and both are close to F 3 . In contrast, features from gBoosting do not have a stable spatial distribution in the two proteins. The observation supports our claim that our method can select grouped features with stable spatial distribution among the graph data.
A common concern with boosting is that the method is usually sensitive to outliers and errors in the training data set due to the exponential loss function. We use logit loss function that is less sensitive to outliers. However, as claimed in [25], any convex loss function may degenerate to random guess with a certain level of random classification noise. L regularization in linear regression has been shown to stabi-lize the learning function [13]. In our algorithm design, we used the Laplacian based L 2 regularization and this may reduce the boosting algorithms X  sensitivity to outliers and random classification noise. To test the robustness of our method experimentally, we singled out the P 4 data set and performed 5 fold cross validation with class label errors. In particular, for each fold, we change certain percentage of the class labels in the training data, train a model with changed training data, and apply trained model to normal test data. In Fig 4, we report the average accuracy with error rate ranging from 0% to 25% for LPGBCMP, gPLS, gBoosting, SVM RFE and COM.

From Fig 4, a clear trend is that the accuracy of all meth-ods decreases as more errors are introduced in the train-ing data set. There is a sharp deceasing from 0 to 5% for SVM RFE and COM. The regularized boosting method re-mains the best over all the settings, even though the per-formance gain is not significant. From the test, we conclude that LPGBCMP is at least as sensitive (if not less) to noises Figure 4: Average accuracy with different percentage
Figure 6: Average accuracy with different max var as other classifiers including SVM and partial least square based methods.

In addition, we evaluate the robustness of the regularized boosting algorithm by changing different parameter values. Among the parameters that may affect the performance of the regularized boosting algorithm, we test the parameter max var , which is used to derive the feature consistency map. With a large value of max var , the edge number of feature consistency map increases and with smaller value of max var , the edge number of feature consistency map decreases.

Fig 6 indicates average accuracy on 5 fold cross validation for each value of threshold max var from 0.5 to 8. From the result, we observe that the accuracy remains stable within a relatively wide range of threshold and the best accuracy can be obtained around 1 to 2. Furthermore, the relation-ship between the performance and parameter mar var is revealed. When max var is quite small, the structure in-formation among features is ignored and our method will degenerate to regular logit boosting with elastic net regular-ization [44]; when max var is large, the feature graph will be a complete graph and our method may possibly intro-duce less discriminative features hence undermine the per-formance.

Overall, the regularized boosting method is effective and achieves good accuracy within a wide range parameters and a certain number of outliers.
In this paper, we presented a novel boosting algorithm that considered the structure relationship of base learners in the functional space. We model the structure relation-ship as an undirected graph and incorporate such informa-tion by introducing a L 2 norm regularized graph Laplacian to standard boosting formalization. Though the new al-gorithm may be applied to many applications, we specifi-cally focus on constructing supervised graph learning mod-els in this paper. Using a comprehensive experimental study with protein structure graphs and comparing with current state-of-the-art, we demonstrate that the new algorithm se-lects clustered features with stable spatial relationship, and achieves better predictive performance. In the future we plan to extend the current work to constructing boosting al-gorithms for other types of data such as trees and sequences, where the base learners have a structure relationship in the functional space.
 This work has been supported by an Office of Naval Re-search award (N00014-07-1-1042) and the National Science Foundation under Grant No. 0845951. [1] C. Chang and C. Lin. Libsvm: a library for support [2] F. Chung. Spectral graph theory. CBMS Reginal [3] W. Dai, Q. Yang, G. rong Xue, and Y. Yu. Boosting [4] J. Duchi and Y. Singer. Boosting with structural [5] H. Fei and J. Huan. Structure feature selection for [6] H. Fei and J. Huan. L2 norm regularized feature kernel [7] Y. Freund. Boosting a weak learning algorithm by [8] Y. Freund and R. Shapire. A decision-theoretic [9] J. Friedman, T. Hastie, and R. Tibshirani. Additive [10] J. Friedman, T. Hastie, and R. Tibshirani.
 [11] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene [12] G. Haffari, Y. Wang, S. Wang, G. Mori, and F. Jiao. [13] T. Hastie, R. Tibshirani, and J. Friedman. The [14] J. Huan, W. Wang, and J. Prins. Efficient mining of [15] L. Jacob, G. Obozinski, and J.-P. Vert. Group lasso [16] N. Jin, C. Young, and W. Wang. Graph classification [17] H. Kashima, K. Tsuda, and A. Inokuchi. Marginalized [18] S. Kim and E. P. Xing. Structured feature selection in [19] R. I. Kondor, N. Shervashidze, and K. M. Borgwardt. [20] T. Kudo, E. Maeda, and Y. Matsumoto. An [21] C. Leslie, E. Eskin, and W. Noble. The spectrum [22] C. Li and H. Li. Newwork-constrained regularization [23] P. Li. Adaptive base class boost for multi-class [24] N. Loeff, D. Forsyth, and D. Ramachandran.
 [25] P. M. Long and R. A. Servedio. Random classification [26] A. C. Lozano and N. Abe. Multi-class cost-sensitive [27] A. Murzin, S. Brenner, T. Hubbard, and C. Chothia. [28] S. Nowozin, K. Tsuda, T. Uno, T. Kudo, and [29] G. Pandey, S. Chawla, S. Poon, B. Arunasalam, and [30] H. Saigo and et. al. gboost: Graph learning toolbox [31] H. Saigo, N. Kr  X  amer, and K. Tsuda. Partial least [32] H. Saigo, S. Nowozin, T. Kadowaki, T. Kudo, and [33] T. Sandler, P. P. Talukdar, and L. H. Ungar. [34] R. Schapire. The strength of weak learnability. [35] R. Schapire and Y. Singer. Improved boosting [36] M. Thoma, H. Cheng, A. Gretton, J. Han, H.-P. [37] R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and [38] K. Tsuda. Entire regularization paths for graph data. [39] R. Yan, J. Tesic, and J. R. Smith. Model-shared [40] X. Yan, H. Cheng, J. Han, and P. Yu. Mining [41] M. Yuan and Y. Lin. Model selection and estimation [42] P. Zhao and B. Yu. Grouped and hierarchical model [43] L. Zheng, S. Wang, C. hoon Lee, and Y. Liu.
 [44] H. Zou and T. Hastie. Regularization and variable [45] H. Zou and M. Yuan. F  X  norm support vector
