 We consider the nonparametric problem of estimating R  X  enyi  X  -entropy and mutual information (MI) based on a finite sample drawn from an unknown, absolutely continuous distribution over R d . There are many applications that make use of such estimators, of which we list a few to give the reader 2005), parameter estimation in semi-parametric models (Wolsztynski et al., 2005), studying fractal random walks (Alemany and Zanette, 1994), and texture classification (Hero et al., 2002b,a). Mu-tual information estimators have been used in feature selection (Peng and Ding, 2005), clustering (Aghagolzadeh et al., 2007), causality detection (Hlav  X  ackova-Schindler et al., 2007), optimal exper-imental design (Lewi et al., 2007; P  X  oczos and L  X  orincz, 2009), fMRI data processing (Chai et al., 2009), prediction of protein structures (Adami, 2004), or boosting and facial expression recogni-tion (Shan et al., 2005). Both entropy estimators and mutual information estimators have been used for independent component and subspace analysis (Learned-Miller and Fisher, 2003; P  X  oczos and L  X  orincz, 2005; Hulle, 2008; Szab  X  o et al., 2007), and image registration (Kybic, 2006; Hero et al., 2002b,a). For further applications, see Leonenko et al. (2008); Wang et al. (2009a).
 In a na  X   X ve approach to R  X  enyi entropy and mutual information estimation, one could use the so called  X  X lug-in X  estimates. These are based on the obvious idea that since entropy and mutual information are determined solely by the density f (and its marginals), it suffices to first estimate the density using one X  X  favorite density estimate which is then  X  X lugged-in X  into the formulas defining entropy and mutual information. The density is, however, a nuisance parameter which we do not want to estimate. Density estimators have tunable parameters and we may need cross validation to achieve good performance.
 The entropy estimation algorithm considered here is direct  X  X t does not build on density estimators. It is based on k -nearest-neighbor (NN) graphs with a fixed k . A variant of these estimators, where each sample point is connected to its k -th nearest neighbor only, were recently studied by Goria et al. (2005) for Shannon entropy estimation ( i.e. the special case  X  = 1 ) and Leonenko et al. (2008) for R  X  enyi  X  -entropy estimation. They proved the weak consistency of their estimators under certain conditions. However, their proofs contain some errors, and it is not obvious how to fix them. Namely, Leonenko et al. (2008) apply the generalized Helly-Bray theorem, while Goria et al. (2005) apply the inverse Fatou lemma under conditions when these theorems do not hold. This latter error originates from the article of Kozachenko and Leonenko (1987), and this mistake can also be found in Wang et al. (2009b).
 The first main contribution of this paper is to give a correct proof of consistency of these estimators. Employing a very different proof techniques than the papers mentioned above, we show that these estimators are, in fact, strongly consistent provided that the unknown density f has bounded support and  X   X  (0 , 1) . At the same time, we allow for more general nearest-neighbor graphs, wherein as opposed to connecting each point only to its k -th nearest neighbor, we allow each point to be connected to an arbitrary subset of its k nearest neighbors. Besides adding generality, our numer-ical experiments seem to suggest that connecting each sample point to all its k nearest neighbors improves the rate of convergence of the estimator.
 The second major contribution of our paper is that we prove a finite-sample high-probability bound on the error ( i.e. the rate of convergence) of our estimator provided that f is Lipschitz. According (1996) who proved the root-n consistency of an estimator of the Shannon entropy and only in one dimension.
 The third contribution is a strongly consistent estimator of R  X  enyi mutual information that is based on NN graphs and the empirical copula transformation (Dedecker et al., 2007). This result is proved for d  X  3 1 and  X   X  (1 / 2 , 1) . This builds upon and extends the previous work of P  X  oczos et al. (2010) where instead of NN graphs, the minimum spanning tree (MST) and the shortest tour through the sample ( i.e. the traveling salesman problem, TSP) were used, but it was only conjectured that NN graphs can be applied as well.
 There are several advantages of using k -NN graph over MST and TSP (besides the obvious concep-tual simplicity of k -NN): On a serial computer the k -NN graph can be computed somewhat faster than MST and much faster than the TSP tour. Furthermore, in contrast to MST and TSP, computa-tion of k -NN can be easily parallelized. Secondly, for different values of  X  , MST and TSP need to be recomputed since the distance between two points is the p -th power of their Euclidean distance where p = d (1  X   X  ) . However, the k -NN graph does not change for different values of p , since p -th power is a monotone transformation, and hence the estimates for multiple values of  X  can be calculated without the extra penalty incurred by the recomputation of the graph. This can be advan-tageous e.g. in intrinsic dimension estimators of manifolds (Costa and Hero, 2003), where p is a free parameter, and thus one can calculate the estimates efficiently for a few different parameter values. The fourth major contribution is a proof of a finite-sample high-probability error bound ( i.e. the rate of convergence) for our mutual information estimator which holds under the assumption that the copula of f is Lipschitz. According to the best of our knowledge, this is the first result that gives a rate for the estimation of R  X  enyi mutual information.
 The toolkit for proving our results derives from the deep literature of Euclidean functionals, see, (Steele, 1997; Yukich, 1998). In particular, our strong consistency result uses a theorem due to Red-mond and Yukich (1996) that essentially states that any quasi-additive power-weighted Euclidean functional can be used as a strongly consistent estimator of R  X  enyi entropy (see also Hero and Michel 1999). We also make use of a result due to Koo and Lee (2007), who proved a rate of convergence result that holds under more stringent conditions. Thus, the main thrust of the present work is show-ing that these conditions hold for p -power weighted nearest-neighbor graphs. Curiously enough, up to now, no one has shown this, except for the case when p = 1 , which is studied in Section 8.3 of (Yukich, 1998). However, the condition p = 1 gives results only for  X  = 1  X  1 /d .
 Unfortunately, the space limitations do not allow us to present any of our proofs, so we relegate them into the extended version of this paper (P  X  al et al., 2010). We instead try to give a clear explanation of R  X  enyi entropy and mutual information estimation problems, the estimation algorithms and the statements of our converge results.
 Additionally, we report on two numerical experiments. In the first experiment, we compare the empirical rates of convergence of our estimators with our theoretical results and plug-in estimates. Empirically, the NN methods are the clear winner. The second experiment is an illustrative applica-tion of mutual information estimation to an Independent Subspace Analysis (ISA) task.
 The paper is organized as follows: In the next section, we formally define R  X  enyi entropy and R  X  enyi mutual information and the problem of their estimation. Section 3 explains the  X  X eneralized nearest neighbor X  graphs. This graph is then used in Section 4 to define our R  X  enyi entropy estimator. In the same section, we state a theorem containing our convergence results for this estimator (strong consistency and rates). In Section 5, we explain the copula transformation, which connects R  X  enyi entropy with R  X  enyi mutual information. The copula transformation together with the R  X  enyi entropy estimator from Section 4 is used to build an estimator of R  X  enyi mutual information. We conclude this section with a theorem stating the convergence properties of the estimator (strong consistency and rates). Section 6 contains the numerical experiments. We conclude the paper by a detailed discussion of further related work in Section 7, and a list of open problems and directions for future research in Section 8. R  X  enyi entropy and R  X  enyi mutual information of d real-valued random variables 2 X = ( X 1 ,X 2 ,...,X d ) with joint density f : R d  X  R and marginal densities f i : R  X  R , 1  X  i  X  d , are defined for any real parameter  X  assuming the underlying integrals exist. For  X  6 = 1 , R  X  enyi entropy and R  X  enyi mutual information are defined respectively as 3 For  X  = 1 they are defined by the limits H 1 = lim  X   X  1 H  X  and I 1 = lim  X   X  1 I  X  . In fact, Shannon (differential) entropy and the Shannon mutual information are just special cases of R  X  enyi entropy and R  X  enyi mutual information with  X  = 1 .
 The goal of this paper is to present estimators of R  X  enyi entropy (1) and R  X  enyi information (2) and study their convergence properties. To be more explicit, we consider the problem where we are density f : R d  X  R and marginal densities f i : R  X  R and our task is to construct an estimate b H  X  ( X 1: n ) of H  X  ( f ) and an estimate b I  X  ( X 1: n ) of I  X  ( f ) using the sample X 1: n . The basic tool to define our estimators is the generalized nearest-neighbor graph and more specifi-cally the sum of the p -th powers of Euclidean lengths of its edges.
 Formally, let V be a finite set of points in an Euclidean space R d and let S be a finite non-empty set of positive integers; we denote by k the maximum element of S . We define the generalized nearest-neighbor graph NN S ( V ) as a directed graph on V . The edge set of NN S ( V ) contains for each i  X  S an edge from each vertex x  X  V to its i -th nearest neighbor. That is, if we sort k x  X  y 1 k X k x  X  y 2 k X  X  X  X  X  X k x  X  y | V | X  1 k then y i is the i -th nearest-neighbor of x and for each i  X  S there is an edge from x to y i in the graph.
 For p  X  0 let us denote by L p ( V ) the sum of the p -th powers of Euclidean lengths of its edges. Formally, where E ( NN S ( V )) denotes the edge set of NN S ( V ) . We intentionally hide the dependence on S in the notation L p ( V ) . For the rest of the paper, the reader should think of S as a fixed but otherwise arbitrary finite non-empty set of integers, say, S = { 1 , 3 , 4 } .
 The following is a basic result about L p . The proof can be found in P  X  al et al. (2010). Theorem 1 (Constant  X  ) . Let X 1: n = ( X 1 , X 2 ,..., X n ) be an i.i.d. sample from the uniform distribution over the d -dimensional unit cube [0 , 1] d . For any p  X  0 and any finite non-empty set S of positive integers there exists a constant  X  &gt; 0 such that The value of  X  depends on d,p,S and, except for special cases, an analytical formula for its value is not known. This causes a minor problem since the constant  X  appears in our estimators. A simple and effective way to deal with this problem is to generate a large i.i.d. sample X 1: n from the uniform distribution over [0 , 1] d and estimate  X  by the empirical value of L p ( X 1: n ) /n 1  X  p/d . We are now ready to present an estimator of R  X  enyi entropy based on the generalized nearest-neighbor graph. Suppose we are given an i.i.d. sample X 1: n = ( X 1 , X 2 ,..., X n ) from a distribution  X  over R d with density f . We estimate entropy H  X  ( f ) for  X   X  (0 , 1) by and L p (  X  ) is the sum of p -th powers of Euclidean lengths of edges of the nearest-neighbor graph NN S (  X  ) for some finite non-empty S  X  N + as defined by equation (3). The constant  X  is the same as in Theorem 1.
 consistent and gives upper bounds on the rate of convergence. The proof of theorem is in P  X  al et al. (2010).
 Theorem 2 (Consistency and Rate for b H  X  ) . Let  X   X  (0 , 1) . Let  X  be an absolutely continuous distribution over R d with bounded support and let f be its density. If X 1: n = ( X 1 , X 2 ,..., X n ) is an i.i.d. sample from  X  then Moreover, if f is Lipschitz then for any  X  &gt; 0 with probability at least 1  X   X  , Estimating mutual information is slightly more complicated than estimating entropy. We start with a basic property of mutual information which we call rescaling . It states that if h 1 ,h 2 ,...,h d : R  X  R are arbitrary strictly increasing functions, then A particularly clever choice is h j = F j for all 1  X  j  X  d , where F j is the cumulative distribution function (c.d.f.) of X j . With this choice, the marginal distribution of h j ( X j ) is the uniform distri-bution over [0 , 1] assuming that F j , the c.d.f. of X j , is continuous. Looking at the definition of H  X  and I  X  we see that I In other words, calculation of mutual information can be reduced to the calculation of entropy pro-vided that marginal c.d.f. X  X  F 1 ,F 2 ,...,F d are known. The problem is, of course, that these are not as their estimates. Given an i.i.d. sample X 1: n = ( X 1 , X 2 ,..., X n ) from distribution  X  and with density f , the empirical c.d.f X  X  are defined as Introduce the compact notation F : R d  X  [0 , 1] d , b F : R d  X  [0 , 1] d , Let us call the maps F , b F the copula transformation , and the empirical copula transformation , copula (Dedecker et al., 2007). Note that j -th coordinate of b Z i equals where rank( x,A ) is the number of element of A less than or equal to x . Also, observe mation I  X  by where b H  X  is defined by (5). The following theorem is our main result about the estimator b I  X  . It states that b I  X  is strongly consistent and gives upper bounds on the rate of convergence. The proof of this theorem can be found in P  X  al et al. (2010).
 Theorem 3 (Consistency and Rate for b I  X  ) . Let d  X  3 and  X  = 1  X  p/d  X  (1 / 2 , 1) . Let  X  be an sample from  X  then Moreover, if the density of the copula of  X  is Lipschitz, then for any  X  &gt; 0 with probability at least 1  X   X  , b
I In this section we show two numerical experiments to support our theoretical results about the con-vergence rates, and to demonstrate the applicability of the proposed R  X  enyi mutual information esti-mator, b I  X  . 6.1 The Rate of Convergence In our first experiment (Fig. 1), we demonstrate that the derived rate is indeed an upper bound on the convergence rate. Figure 1a-1c show the estimation error of b I  X  as a function of the sample size. Here, the underlying distribution was a 3D uniform, a 3D Gaussian, and a 20D Gaussian with randomly chosen nontrivial covariance matrices, respectively. In these experiments  X  was set to 0 . 7 . For the estimation we used S = { 3 } (kth) and S = { 1 , 2 , 3 } (knn) sets. Our results also indicate that these estimators achieve better performances than the histogram based plug-in estimators (hist). The number and the sizes of the bins were determined with the rule of Scott (1979). The histogram based estimator is not shown in the 20D case, as in this large dimension it is not applicable in practice. The figures are based on averaging 25 independent runs, and they also show the theoretical upper bound (Theoretical) on the rate derived in Theorem 3. It can be seen that the theoretical rates are rather conservative. We think that this is because the theory allows for quite irregular densities, while the densities considered in this experiment are very nice. 6.2 Application to Independent Subspace Analysis An important application of dependence estimators is the Independent Subspace Analysis problem (Cardoso, 1998). This problem is a generalization of the Independent Component Analysis (ICA), where we assume the independent sources are multidimensional vector valued random variables. The formal description of the problem is as follows. We have S = ( S 1 ; ... ; S m )  X  R dm , m inde-we assume that S is hidden, and only n i.i.d. samples from X = AS are available for observation, where A  X  R q  X  dm is an unknown invertible matrix with full rank and q  X  dm . Based on n i.i.d. observation of X , our task is to estimate the hidden sources S i and the mixing matrix A . Let the estimation of S be denoted by Y = ( Y 1 ; ... ; Y m )  X  R dm , where Y = WX . The goal of ISA is to calculate argmin W I ( Y 1 ,..., Y m ) , where W  X  R dm  X  q is a matrix with full rank. Following the ideas of Cardoso (1998), this ISA problem can be solved by first preprocessing the observed quan-tities X by a traditional ICA algorithm which provides us W ICA estimated separation matrix 5 , and then simply grouping the estimated ICA components into ISA subspaces by maximizing the sum of the MI in the estimated subspaces, that is we have to find a permutation matrix P  X  { 0 , 1 } dm  X  dm which solves where Y = PW ICA X . We used the proposed copula based information estimation, b I  X  with  X  = 0 . 99 to approximate the Shannon mutual information, and we chose S = { 1 , 2 , 3 } . Our experiment shows that this ISA algorithm using the proposed MI estimator can indeed provide good estimation of the ISA subspaces. We used a standard ISA benchmark dataset from Szab  X  o et al. (2007); we generated 2,000 i.i.d. sample points on 3D geometric wireframe distributions from 6 different sources independently from each other. These sampled points can be seen in Fig. 2a, and they represent the sources, S . Then we mixed these sources by a randomly chosen invertible matrix A  X  R 18  X  18 . The six 3-dimensional projections of X = AS observed quantities are shown in Fig. 2b. Our task was to estimate the original sources S using the sample of the observed quantity X only. By estimating the MI in (12), we could recover the original subspaces as it can be seen in Fig. 2c. The successful subspace separation is shown in the form of Hinton diagrams as well, which is the product of the estimated ISA separation matrix W = PW ICA and A . It is a block permutation matrix if and only if the subspace separation is perfect (Fig. 2d). As it was pointed out earlier, in this paper we heavily built on the results known from the theory of Euclidean functionals (Steele, 1997; Redmond and Yukich, 1996; Koo and Lee, 2007). However, now we can be more precise about earlier work concerning nearest-neighbor based Euclidean func-tionals: The closest to our work is Section 8.3 of Yukich (1998), where the case of NN S graph based p -power weighted Euclidean functionals with S = { 1 , 2 ,...,k } and p = 1 was investigated. Nearest-neighbor graphs have first been proposed for Shannon entropy estimation by Kozachenko and Leonenko (1987). In particular, in the mentioned work only the case of NN S graphs with S = { 1 } was considered. More recently, Goria et al. (2005) generalized this approach to S = { k } and proved the resulting estimator X  X  weak consistency under some conditions on the density. The estimator in this paper has a form quite similar to that of ours: Here  X  stands for the digamma function, and e i is the directed edge pointing from X i to its k th nearest-neighbor. Comparing this with (5), unsurprisingly, we find that the main difference is the use of the logarithm function instead of | X | p and the different normalization. As mentioned before, Leonenko et al. (2008) proposed an estimator that uses the NN S graph with S = { k } for the purpose of estimating the R  X  enyi entropy. Their estimator takes the form is the volume of the d -dimensional unit ball, and again e i is the directed edge in the NN S graph starting from node X i and pointing to the k -th nearest node. Comparing this estimator with (5), it is apparent that it is (essentially) a special case of our NN S based estimator. From the results of Leonenko et al. (2008) it is obvious that the constant  X  in (5) can be found in analytical form when S = { k } . However, we kindly warn the reader again that the proofs of these last three cited articles (Kozachenko and Leonenko, 1987; Goria et al., 2005; Leonenko et al., 2008) contain a few errors, just like the Wang et al. (2009b) paper for KL divergence estimation from two samples. Kraskov et al. (2004) also proposed a k -nearest-neighbors based estimator for the Shannon mutual information estimation, but the theoretical properties of their estimator are unknown. We have studied R  X  enyi entropy and mutual information estimators based on NN S graphs. The estimators were shown to be strongly consistent. In addition, we derived upper bounds on their convergence rate under some technical conditions. Several open problems remain unanswered: An important open problem is to understand how the choice of the set S  X  N + affects our estimators. Perhaps, there exists a way to choose S as a function of the sample size n (and d,p ) which strikes the optimal balance between the bias and the variance of our estimators.
 Our method can be used for estimation of Shannon entropy and mutual information by simply using function of the sample size n (and d,p ) such that the resulting estimator is consistent and converges as rapidly as possible. An alternative is to use the logarithm function in place of the power function. However, the theory would need to be changed significantly to show that the resulting estimator remains strongly consistent.
 In the proof of consistency of our mutual information estimator b I  X  we used Kiefer-Dvoretzky-Wolfowitz theorem to handle the effect of the inaccuracy of the empirical copula transformation (1 / 2 , 1) and the dimension to values larger than 2 . Is there a better way to estimate the error caused by the empirical copula transformation and prove consistency of the estimator for a larger range of  X   X  X  and d = 1 , 2 ? Finally, it is an important open problem to prove bounds on converge rates for densities that have higher order smoothness ( i.e.  X  -H  X  older smooth densities). A related open problem, in the context of of theory of Euclidean functionals, is stated in Koo and Lee (2007).
 This work was supported in part by AICML, AITF (formerly iCore and AIF), NSERC, the PAS-CAL2 Network of Excellence under EC grant no. 216886 and by the Department of Energy under grant number DESC0002607. Cs. Szepesv  X  ari is on leave from SZTAKI, Hungary.

