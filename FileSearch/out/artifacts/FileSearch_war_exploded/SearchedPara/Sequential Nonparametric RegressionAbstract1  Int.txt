 Haijie Gu haijieg@cs.cmu.edu John Lafferty lafferty@galton.uchicago.edu Bandwidth selection is arguably the most important aspect of nonparametric regression using smoothing kernels. It is well understood how the optimal band-width for regression depends on the sample size. Con-sidering the one dimensional case, let where the X i s are independent and identically dis-tributed, m : R  X  R is the unknown function to estimate, and  X  i  X  N (0 ,  X  2 ). Assuming m  X  X  is abso-lutely continuous and the Nadaraya-Watson kernel regression estimator with bandwidth h has the form where is defined as the risk of the estimate b m n on a sample of size n . Here c 1 and c 2 are constants that depend on the kernel and the distribution of X . The opti-mal bandwidth that minimizes (1.2) thus has order h  X  = O ( n  X  1 / 5 ), which leads to the optimal minimax More generally, if we assume further smoothness of m so that the d -th derivative of m exists and is bounded, local polynomial regression of order d  X  1 with a band-1996). Thus, bandwidth selection is of the essence in nonparametric regression, and a large body of research has been devoted to this problem in various settings. These classical results assume that a training data set of size n is given; the sample size n formally increases to infinity in the theoretical analysis. In an online set-ting, however, the data arrive sequentially, and the size of the data set is always changing. In this case the bandwidth needs to adapt somehow to the changing sample size. A simple variation of the classical meth-ods would carry out a batch regression on the entire data set seen up to the current time T , with an appro-priately sized bandwidth. However, this would require quadratic complexity O ( T 2 ) to compute the estimates after T points are observed, and is prohibitive for large sample sizes. This motivates the problem studied in this paper X  X o develop computationally efficient esti-mators in the online setting that preserve the statis-tical efficiency of the classical batch estimators. We propose an algorithm for sequential regression that re-quires linear computational cost, and prove that the al-gorithm achieves the optimal minimax rate of conver-gence. The essential idea is to avoid recomputation by shrinking the bandwidth for each new observed data point. Our analysis assumes that the data are iid, and the algorithm assumes they are obtained sequentially. Although online local polynomial smoothing has not been previously studied, significant previous work has been devoted to related problems. Steland (2010) investigates a cross-validation scheme for sequential data and establishes theoretical results. However, this does not consider the cost of recomputing the en-tire model for each new bandwidth; performing leave-one-out cross-validation adds extra computation and would be impractical for many online settings. Kivi-nen et al. (2004) develop variants of stochastic gradi-ent descent for online learning in a reproducing ker-nel Hilbert space. The algorithms require linear com-putation cost at each step, and their RKHS analysis does not consider selection of tuning parameters for the kernel, or adaptation to the unknown smoothness of the regression function. A great deal of work has been carried out for the problem of adaptation in the classical batch setting. Fan &amp; Gijbels (1995) consider using Residual Squares Criteria (RCS) for perform-ing data-driven bandwidth selection; Ruppert et al. (1995) propose a plug-in bandwidth selection scheme for local linear kernel estimators. These are effective methods for adaptive estimation; however, they do not take into account the computational cost for online updating. The mixing expert framework has been a popular strategy for online prediction, and there is a rich literature on this topic (Cesa-Bianchi &amp; Lugosi, 2006). Results by Bunea &amp; Nobel (2008) give oracle inequalities for regression in terms of generalized sim-plex combinations of a set of fixed estimators in the online setting. However, this analysis does not allow the experts to change over time, as we require with dy-namically changing bandwidths. Further related work is discussed below.
 In the following section we present the algorithm for sequential local polynomial regression. In Section 3 we outline a theoretical analysis of the risk achieved by this algorithm for both sequential density estima-tion (Theorem 3.2) and regression (Theorem 3.3). In Section 4 we briefly discuss the problem of adapting to unknown smoothness, using the expert mixing frame-work and also the extension to additive models. In Section 5 we present experimental results showing that our algorithm is comparable to the batch algorithm but much more computationally efficient, and adapts to unknown smoothness of the true function. Our efficient sequential estimator is based on local polynomial regression with a sequence of shrinking bandwidths. Among the various nonparametric re-gression methods, local polynomial regression enjoys strong minimax properties, and gracefully deals with the problem of boundary bias (Fan et al., 1993). Let according to the model given in equation (1.1). We assume that the pairs ( X i , Y i ) are independent and identically distributed random variables. Throughout we assume for simplicity that the domain of m ( x ) is x  X  [0 , 1]. An extension to higher dimensional x is straightforward. We discuss an extension to additive models for the high dimensional case in Section 4.2. 2.1. Sequential kernel regression We first present the simplest version of the method. Recall that the kernel regression (Nadaraya-Watson) estimator in the batch setting is where h is the bandwidth. Under standard assump-tions that the regression function is in a second-order Sobolev space, the bandwidth is chosen as h = c  X  n  X  1 / 5 and the estimator achieves the minimax rate of conver-gence R ( b m n , m ) = O ( n  X  4 / 5 ). Now, suppose the data arrive sequentially. Our sequential estimator takes the form where the bandwidth h t = c  X  t  X  1 / 5 is used only for the t th point. The algorithm incrementally computes the numerator and denominator, shrinking the band-width for each new term added. A corollary of our main technical result is that this estimator achieves the same rate of convergence as the Nadaraya-Watson estimator.
 This result is surprising since the first examples in the widths h t = c  X  t  X  1 / 5 that are too large, introducing a bias in the estimate. However, our analysis shows that this bias is asymptotically  X  X ashed out. X  To gain some intuition for how this happens, note that we can write the estimate as Define f : R  X  [0 , 1] to be the marginal proba-bility density function of X . As we show below, the denominator is a consistent density estimate of f ( x 0 ), and moreover it attains the optimal rate of convergence. The early contributions to the series K tively weighted by 1 /n b f ( x 0 ), which removes the bias they introduce as n increases.
 2.2. Sequential local polynomial smoothing In the classical batch estimation case, the order-d lo-cal polynomial regression at a prediction point x 0 is computed by minimizing the locally weighted squared error where K (  X  ) is the kernel. Denote by b  X  ( x 0 ) the ( d +1)-vector that minimizes this objective. The regres-sion estimate is then given by the intercept b m ( x 0 ) = b  X  ( x 0 ). Let X n be the n  X  ( d + 1) design matrix Furthermore, let y n = ( Y 1 , Y 2 , . . . , Y n ) T , and let W n = diag { K h n ( X t , x 0 ) } 1  X  t  X  n be an n  X  n diago-nal matrix of weights, where K h ( X t , x ) = 1 h K ( X t Then the solution that minimizes (2.4) is given as function at x 0 (Fan et al., 1993).
 If we were to adapt h n +1 to the increased sample size n +1 at the next time step, we would have to recompute the entire X T W X matrix. To save computation, we allow a variable bandwidth in W . In other words, we select a new bandwidth h n +1 that only applies to Specifically, we propose the following sequential local polynomial regression algorithm. Let where h t = c  X  t  X  1 / (2 d +1) is the bandwidth that would be asymptotically optimal with respect to a sample of size t , with c a constant. Our proposed online estimate after n samples are observed is then where e 1 = (1 , 0 , . . . , 0). Denote by S n the ( d + 1)  X  ( d + 1) matrix X T f W n X , with ( i, j ) entry To update the model after having observed where x n +1 is the ( d + 1) vector (1 , ( X n +1  X  x and the Woodbury formula for the inverse of a rank-one matrix update, we have that updating S  X  1 n +1 given S  X  1 n has complexity O ( d 2 ). Similarly, updating has O ( d ) cost. Therefore, the complexity of updating the estimate from a sample of size n to one of size n +1 at each point x 0 in a grid G of size |G| is O ( |G| d 2 ), independent of n . However, the update cost in the batch setting is O ( |G| ( nd 2 + d 3 )) because changing the bandwidth forces re-evaluating equation (2.5). In this section, we give a risk analysis of sequential density estimation and regression. Assuming the re-gression function m lies in C d , the class of functions with d continuous derivatives, our goal is to show that the asymptotic risk of the online algorithm given in the previous section achieves the statistical rate of conver-gence of n  X  2 d/ (2 d +1) . This is the minimax optimal rate for this function class.
 We begin by analyzing the risk of sequential kernel density estimation and kernel regression ( d = 2), be-cause the analysis is simpler and more transparent for these special cases than for the general case. We then generalize the results to order d  X  1 sequential polyno-mial regression in C d .
 We assume that the true density function f and the true regression function m have d  X  2 continuous derivatives. The kernel K is assumed to satisfy the following properties: 0,  X  2 K  X  quence of bandwidths { h t | t = 1 , 2 , 3 , . . . } to satisfy 3.1. Sequential Kernel Density Estimation Our sequential kernel density estimator e f is given by e f ( x ) = 1 n crementally according to the update rule e f Thus, updating e f n has cost O ( |G| ), where |G| is the size of the grid of x values at which the estimates are made.
 Lemma 3.1. The risk of the sequential density esti-mate e f n at time t = n is + + o The proof of Lemma 3.1 is given in Section A.1. We note in passing that a similar algorithm for sequential density estimation appears, without theoretical analy-sis, in Kristan et al. (2010).
 Theorem 3.1. Let h t = c t  X  1 / 5 for some constant c . Then the risk of sequential kernel density estimator e f n satisfies R ( f, e f n ) = O ( n  X  4 / 5 ) .
 Proof. Let c 1 = 1 4 (  X  2 K ) 2 R K 2 ( u ) du . Then Lemma 3.1 states that
R ( f, e f n ) = bandwidth sequence
R ( f, e f n ) = Minimizing over k , we find that k  X  = 1 5 , and optimal The above analysis assumes that the density f has a continuous second derivative. The result can be ex-tended to the case where f is in C d , using a higher order kernel satisfying  X  j K = j &lt; d .
 Theorem 3.2. If the density function f lies in C d , then the optimal risk of the sequential kernel density estimator e f n satisfies R  X  = O ( n  X  2 d/ 2 d +1 ) when the bandwidth sequence is taken to be h  X  t = c t  X  1 / 2 d +1 Proof sketch. By a Taylor expansion and calculations similar to those in the proof of Lemma 3.1, we have
E e f n ( x ) = + o ( h d t ) .
 Assuming a higher order kernel, we have  X  j K = 0 for j &lt; d , so the leading order of the bias is Bias( e f n ( x )) = The variance satisfies Var( e f n ( x )) = f ( x ) of Lemma 3.1. Thus, using the bias-variance decom-position the risk takes the form Assuming that h t = c t  X  k for k &gt; 0 and optimizing 3.2. Sequential Local Polynomial Regression Similar results hold for the sequential local polynomial regression estimator. In particular, for d = 2 we use where e f n ( x ) = 1 n sult is proved in Section A.1.
 Lemma 3.2. The risk of the estimator e m n ( x ) in (3.9) satisfies
R ( e m n , m ) (3.10) = +  X  2 + o The analogue of Theorem 3.2 for density estimation can also be obtained in the regression setting. In-stead of choosing a special kernel to cancel out the lower order terms in the Taylor X  X  series, we leverage the minimax optimality of local polynomial regression as introduced in Section 2.
 Theorem 3.3. Suppose that the regression function m has d continuous derivatives. Let h t = c t  X  1 / 2 d +1 Then at t = n , the order d  X  1 sequential local poly-nomial regression attains the optimal minimax risk The proof of Theorem 3.3 is given in Section A.3. In this section we extend the above analysis in two ways. First, we show how the expert mixing frame-work can be used to adapt to the smoothness expo-nent. Second, we show how the procedure can be ex-tended to additive models. 4.1. Adapting to unknown smoothness The theoretical performance of the sequential estima-tors presented above hinges on selecting the correct order d of the local polynomial, and in practice it de-pends on the constant c in the bandwidth as well. Tra-ditional statistical model selection methods, e.g. AIC and cross validation, are not practical in an online sce-nario.
 In order to maintain a reasonable computational cost, we combine estimators that use different parame-ters (order d and constant c ) through an exponential weighting strategy. Leveraging our analysis in Sec-tion 3, it can be shown that the procedure adapts to the unknown smoothness at the optimal rate, while maintaining a linear computational cost.
 In more detail, our mixing online regression estimates procedure forms an exponential weighting of a set of sequential local polynomial regression estimates with different orders d and bandwidth constants c . CD to be the size of the family of sequential regression estimates M = { e m ( i,j ) ,t } where 1  X  i  X  C and 1  X  j  X  D . At time t , the bandwidth of the regression e m corresponding estimator is computed as in equation (2.6); specifically, where The double index ( i, j ) is used to illustrate the con-struction of the expert set; in the following, for simplic-ity, we use a single index k to index the M sequential estimators.
 mulative loss of estimator k at time t . At each time s , the prediction of Y s is made with the es-timator e m k,s  X  1 constructed using the previous data { ( X 1 , Y 1 ) , ( X 2 , Y 2 ) , . . . , ( X s  X  1 , Y s  X  1 M  X  1 , and for t  X  1 let where  X  is a positive learning rate, to be chosen later. The combined estimator e m  X  at time t is the convex combination given by Note that the weight at time t can be updated in linear time O ( M ) using w A large literature is devoted to the study of regret-based bounds for online learning, which hold for any realization of data (Cesa-Bianchi &amp; Lugosi, 2006). However, we are primarily interested in analyzing the statistical risk of our estimators. In particular, it is of interest to obtain an oracle inequality of the form
E k e m  X  ,n  X  m k 2  X  min where  X  ( n ) is the additive penalty paid for adapta-tion. Having already established the minimax rate of will enable us to show that the weighted combination adapts to achieve a (near) minimax rate.
 Theorem 4.1. Let m  X  C d  X  . Assume that 1. e m k,n  X  [  X  A, A ] , for some constant A , for all Suppose that the optimal order d  X  is contained in the set of candidate degrees D . Then, for sufficiently large n , where  X  is chosen to be a small constant depending on A . In particular, we have that R ( m, e m  X  ,n ) = This result is proved by adapting analysis by Yang (2004) (Theorem 5, see also Catoni (2004)). 4.2. Additive models It is also possible to derive a sequential extension to the backfitting algorithm for additive models (Hastie &amp; Tibshirani, 1990). Assuming now that X is p dimen-sional, an additive regression model takes the form Y = m 0 + where m 0 is an overall mean or intercept, the nonpara-metric regression functions m j are one-dimensional, and  X  i is mean zero noise. The backfitting algorithm is a type of coordinate descent or Gauss-Seidel procedure that iteratively computes the residuals for a variable j , and then smoothes those residuals to get a nonpara-metric estimate of the component function.
 In our sequential setting, a complication comes from the fact that the residuals must be updated sequen-tially as well X  X or computational efficiency we cannot afford to compute the residuals over all of the previous data. Since the residuals for all of the p variables de-pend on one another, we update them iteratively using our sequential regression estimator until convergence. We then cycle through the variables to sequentially update each component function in terms of the con-verged residuals.
 This algorithm is made explicit below, where update ( m j , X, resid, t ) updates the j th model m j with the ( X, resid ) pair, using the appropriate bandwidth at time t . This can be viewed as an incremental version of the smoothing step in the classical backfitting algo-rithm and can be efficiently computed using (2.8) and (2.10) in the case of local-polynomial smoothing, us-ing (2.3) in the case of kernel regression. We note that the numerical convergence of even the classical back-fitting is difficult to analyze. Example simulations of our sequential backfitting algorithm are given in Sec-tion ?? , where it compares favorably to the classical batch backfitting algorithm.
 To illustrate the performance of a single online esti-mator (descirbed in  X  2), we compare it with the batch algorithm using the same bandwidth h = cn  X  1 / 5 . We consider the following 4 regression functions of differ-ent smoothness on [0 , 1] used by Yang (2001): f 1 ( x ) = exp(  X  x ) , f 2 ( x ) = 1 + 2 x 2 + exp(  X  5( x  X  0 . 5) 0 . 2) 2 ) / sample size is 150 and  X  2 = 0 . 5.
 We choose the bandwidth constant c from C = a comparison to a batch estimator that does leave one out cross validation at each time step to choose the bandwidth constant from C . The first 50 samples are used to initialize the batch estimators.
 Figure 1 (first row) shows the average loss as a func-tion of the sample size for the best sequential estimator (with optimal constant), the best batch estimator, and the batch cross-validation estimator for the functions. In each case, the best sequential estimator has very similar loss to the best batch estimator, which sup-ports our theory showing that the sequential estima-tor achieves the same statistical rate of convergence as the classical batch algorithm. Note that in each case the average loss converges to around the noise level  X  2 = . 5. The middle row compares the fits of the se-quential, batch, and batch with cross validation, and indicates that the fits are comparable, and very close to the true function. The third row shows the stan-dard errors for the sequential estimators running the simulations 100 times for each function.
 Here we study the estimator X  X  risk under unknown smoothness. We use the following parameterized set of ness of the function m  X  is H  X older  X  continuous. We consider four  X  X xperts X  of online kernel estimators us-ing the parameter set  X  for their bandwidths: e m  X  takes the variable bandwidth h t = 0 . 4  X  t  X  1 / (2  X  +1) . We use the estimator, and set the noise to be  X  2 = 0 . 01. Al-though we have not included the analysis due to lack of space, it can be shown that our sequential estima-regression functions. The task here is to adapt to the unknown exponent  X  .
 Figure 2 shows the risk at the critical point x 0 = 0 . 5 of the four experts under the four functions, having different degrees of smoothness. We run simulations for each of the functions 1000 times, and show a plot of the average risk as a function of sample size. For each true function m  X  , the expert with the correct  X  obtains the lowest risk for any sample size. This suppports the analysis in  X  4, showing the best expert has the highest weight when the experts are mixed to-gether. As a result, the mixing expert framework of online estimators adapts to the unknown smoothness of the true function. While the sequential algorithm makes a tradeoff between performance and computa-tional efficiency, its performance is quite comparable to that of the optimal batch estimator.
 We proposed and analyzed an efficient sequential al-gorithm for local polynomial smoothing in nonpara-metric regression. The first contribution of this work is the online algorithm that shrinks the bandwidth for each new point that arrives. The second is the anal-ysis showing that order d sequential local polynomial smoothing achieves the optimal minimax rate of con-vergence n  X  2 d/ 2 d +1 . Finally, we show that exponential weight mixing of a family of such sequential estimators adapts to unknown smoothness at the optimal rate, and extended the algorithm to sequential backfitting for nonparametric additive models. Our experimen-tal results confirm the theoretical analysis, and show that little loss in statistical efficiency is sacrificed by the computationally efficient online procedure. While we have shown adaptation to global smoothness, an interesting direction for future work is to consider se-quential estimators that adapt to spatially inhomoge-neous function classes. One promising direction is to adapt the variable bandwidth estimator of Lepski et al. (1997) to online regression for Besov spaces. Research supported in part by NSF grant IIS-1116730 and AFOSR contract FA9550-09-1-0373.

