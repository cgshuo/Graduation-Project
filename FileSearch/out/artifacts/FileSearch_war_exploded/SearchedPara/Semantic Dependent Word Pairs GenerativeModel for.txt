 Challenge in opinion mining is to identify the precise object on which the opinion is expressed that is finding fine-grained product features from the user reviews. For example,  X  X  do not like the camera with low memory capacity. X , the au-thor expressed its negative opinion on the camera memory capacity rather than the camera as a whole, but the classical a pproach will classify the sentence into negative category. To overcome the shortcomings, feature-based opinion mining [1] proposed to extract such a pair of n oun and adjective that is adjacent to each other so that noun can indicate a product feature and adjective indicates the opinion orientation. But standard approaches take each noun as a unique product feature rather than categorize the nouns into clusters by their semantic relevance to each other so that each of the c lusters could represent a discrimina-tive product feature. To extract fine-grained product feature, similarity measures between the nouns are designed based on manual annotated tags and ontology dictionary such as WordNet [3,4]. Then classical machine learning approaches are used to categorize the nouns into different product features. In text processing domain, generative models are efficient f or document clustering and word clus-tering, e.g., latent Dirichlet allocation (LDA) [10]. A local version of LDA [3] has been proposed to categorize the noun words as the product feature. A multi-level latent semantic association model [4] ext racted fine-grained product features by exploring the sliding window based context of each noun but it does not consider semantic relevance between the noun and the words in the context. Though there is some work [6] taking advantage of semantic relations for sentiment classifica-tion, most of the previous works [1,2,3] on extracting product features only take account of nouns or with a little bit utilization of non-nominal terms such as ad-jectives. Even if non-nomin al terms are considered, but their semantic relevance to the nouns gets less attention. To utilize the non-nominal terms, a typical ap-proach [5] defined the noun X  X  context features as its co-occurring words in noun phrases and then use a graph clustering method on the co-occurrence relations between nouns and their context features to categorize the noun words. Another similar approach [7] is to apply non-negative matrix factorization method on the co-occurrence of nouns and their semantic relevant context features. But both of them disregard the direct co-occurrence between nouns. To combine the co-occurrence of nouns and semantic dependencies between nouns and other terms, this paper contributes to propose a semantic dependent word pair generative model to take advantage of nouns, adjectives and their semantic relevance for the extraction of fine-grained product features. Bag-of-word is a popular representation of document in text processing but is not sufficient for feature-based opinion mining [2]. To extract relation between product feature and opinions on the feature, noun-adjective pairs are extracted from sentences in [1,2]. In their work, mo ving window method are used to capture text pieces following a pattern where the word at window X  center is a noun and adjectives within the window are opinionated terms to evaluate the product feature indicated by the noun. For example, it can be extracted that pairs such as  X  X estaurant X ,  X  X ood X  ,  X  X ervice X ,  X  X riendly X  and  X  X rice X ,  X  X easonable X  from the sentence shown in Fig.1. But syntactical approaches may extract noisy pairs which do not have a dependency relation. For example, bad pairs like  X  X ood quality X  , X  X riendly X  and  X  X ervice X ,  X  X easonable X  would be included by moving window of size two, which may deliver negative effect on extraction of product feature. Another problem is that unigram noun is not sufficient to represent a product feature such as  X  X ood quality X  as shown in Fig.1.

A dependency tree is represented as a set of dependencies { w h i , w m i , r i } where head node w h i and modifier node w m i forms a head-modifier dependency of r i type relation as shown in the top half of Fig.1. So the noun compound is defined with syntactical rules by largest seque nce of consecutive noun words with each word tagged as NN by parts-of-speech (POS) or such a noun sequence including word  X  X f X . By merging each word in a noun compound, the dependency tree obtains a new node representing the noun compound, denoted as noun node. And semantic neighbors of noun node are the nodes with adjacent dependency relation to the associated noun compound in the dependency tree, e.g.,  X  X ood X  and  X  X eans X  as the semantic neighbor of  X  X estaurant X  in Fig.1. Thereafter a semantic structure comprised of noun compound and their semantic neighbor nodes can form a semantic neighborhood structure, e.g., the bottom half in Fig.1 where an arrow suggests a neighboring relation between a noun fragment and a semantic neighbor with associated POS tag following the lexicons for each node. Here POS Tag is specific to denote the noun fragments instead of noun word only.

We use word  X  X oun X  to denote noun or noun compounds discussed above and  X  X eighbor X  is used to denote the semantic neighbor. To concentrate on the generative process to produce pairs of noun and neighbor, only the most typical neighbors are considered, including neighbors with POS tag  X  X J X ,  X  X JR X , X  X JS X ,  X  X BN X  and  X  X BG X , namely, adjectives and adjectival-verbs, as shown in Table 1. The resultant semantic dependent word pairs from Fig.1 are  X  X estaurant X ,  X  X ood X  ,  X  X ood quality X ,  X  X xcellent X  ,  X  X ervice X ,  X  X riendly X  and  X  X rice X ,  X  X easonable . Given a collection of sentences related to some product domain such as  X  X estaurant X ,  X  X otel X , the sentences are assumed to be prepro-index, noun phrase drawn from a vocabulary V NN and the associated semantic neighbors drawn from vocabulary V SNei respectively. Also a sentence s can be problem is given as below.
 Problem Definition: Given the triple stream { d i , w n i , w s i } , the problem is how to extract representative and seman tic relevant lexicons from the noun vo-cabulary V NN and adjective vocabulary V S to represent and distinguish different fine-grained product features.

Different with the unigram representation, our problem should deal with pairs of noun and adjective with semantic rel evance, from which cluster of nouns and adjectives should be obtained for each fine -grained product feature. Unlike pre-vious work [3] to formalize the noun words clustering as a bi-clustering problem and appeal to LDA, our problem is to generate words from two independent vocabularies of noun and adjective. Our institution is to extend the LDA into a  X  X ariant X  with semantic dependency bet ween pairs of Part-of-speech tokens, which will be discussed in detail in the following sections. To extract the fine-grained product feature, we propose our approach with a generative mode. The semantic dependent word pair generative model (SDWP) can be regarded as an extension of Latent Dirichlet Allocation (LDA). Each cluster learned by the model would be regarded as a product feature. The orig-inal LDA models document as a mixture of topics (clusters) with each topics comprised of words in a vocabulary. As sentence based analysis is more suitable for fine-grained product feature extraction, LDA mentioned in this paper will be referred as sentence based LDA. The basic assumption made in LDA is the ex-changeability between words in a sentence. To include the semantic dependency between words into LDA, the SDWP model assumes that sentence is compose by a mixture of exchangeable nouns and another mixture of exchangeable seman-tic neighbors. Based on such assumption, SDWP could be proposed to generate word pairs { w n i , w s i } for each sentence.

Suppose we have a stream of triples { d i , w n i , w s i } from N D sentences, given hyper-parameters H = {  X ,  X  NN , X  S } for Dirichlet smoothing and the pre-defined number of clusters k , generative process of SDWP can be depicted as below. 1. For each cluster f ,draw multinomial distribution  X  ( f )  X  Dirichlet (  X  NN ) 2. For each cluster f ,draw multinomial distribution  X  ( f )  X  Dirichlet (  X  S ) 3. Given sentence s = d i ,draw multinomial distribution  X  ( s )  X  Dirichlet (  X  ) The graphical model of SDWP can be illustrated in Fig.2. Recalling the graphical model definition in Fig.2, given the cluster index z, the sentence index s are independent with w n i and w s i respectively as in the case of LDA. It is consist with the results in [7] that w n i and w s i are independent conditional on the cluster index.
 As described above about the SDWP, given the hyper-parameter H and a collec-tion of sentences S = { d s } s , we can calculate the evidence likelihood of tokens streams comprised by noun-neighbor pairs as W = { w n i , w s i }| i =1:1: N below. The sentence index for each pair w n i , w s i is denoted by d i . We would start the study used for inference is listed in Table 2.

As shown in Fig.2, given the sentence s , z i is drawn from  X  ( s ). Then given the from a Dirichlet distribution conditional on hyper-parameter  X  = {  X  1 , X  2 ,..., X  K } as in (1): where  X  is Gamma function,  X  i is positive meanwhile  X  s,i &gt; 0 and subjected to Similarly, given parameter H with  X  NN = {  X  NN 1 , X  NN 2 ,..., X  NN L {  X  as in (2) and (3), eters of multinomial distribution of the i th cluster, associated with noun and semantic neighbor vocabulary res pectively, with constraints of j  X  i,j =1and j  X  i,j = 1 while each  X  i,j and  X  i,j are positive, for each cluster i .Here L NN and L S are the size of nouns vocabulary V NN and semantic neighbor terms vocabulary V S respectively.

Given the parameters  X  , for each associated pair w n i , w s i , it can be drawn the latent variables z i indicating which cluster the pair belong to. The stream of { z i } is denoted by Z . Based on the model illustrated in Fig.2, the joint probability of Z and W and the parameters {  X ,  X ,  X  } can be written as From the multinomial distribution with parameter  X  ( s ), it is easy to know that p ( z i |  X , s )equalsto  X  s,z i . Similarly p ( w n i | z i , X  )equalsto  X  z i ,w n
Substituted with (1) (2) and (3), (4) can be rewritten as where C is the normalized factor calculated as With the probability discussed above, we can infer the latent variable { z i } and parameters H with details in the subsections. 4.1 Latent Variable Inference Similar to LDA, the latent variables is intractable for inference. So approximating approaches have been proposed to infer the latent variables with LDA such as Gibbs sampling and variant method. In this paper, we employ collapse Gibbs sampling for inference of the latent variables. Parameters {  X ,  X ,  X  } could be integrated out on the joint probability (6) as which can be computed as In Gibbs sampler, the essence is to randomly reserve one variable for sampling while assuming the others are the true samples drawn from the model. We denote z i as subset of Z excluding z i . The posterior probability of z i conditioned on H , W and z  X  i canbecomputeas 4.2 Parameter Estimation Similar to LDA, giving the sampling results of Z , the joint probability of parameters {  X ,  X ,  X  } can be written as which implies the conditional independency among  X  ,  X  and  X  in between and indicates that
Using expectation of Dirichlet distribution, E ( Dir (  X  ))=  X  1 , X  2 ,..., X  K / i  X  i , on the results obtained above, the parameters can be estimated as 4.3 Hyper-parameter Estimation As discussed in [14], LDA is effective and robust enough but sensitive to different setting of hyper-parameters such as  X  . To eliminate the influence of improper setting of hyper-parameter to the models, we follow the rules of equation (8) to update the hyper-parameter  X  . In this approach, hyper-parameters {  X  i } is represented by  X  0 and m i where  X  0 is the sums of {  X  i } and m i =  X  i / X  0 ,where probabilistic distribution { m i } is assumed to be drawn from a Poisson distribu-tion with  X  0 drawn from a Gamma distribution. So given the topical variable samples from Gibbs sampling, hyper-parameters can be updated by where | s | is the number of pairs in sentence s . The SDWP model is approximated by Gibbs sampling method to infer the clus-ter index of each pairs w n i , w s i . With the samplers drawn from Gibbs sampling process, the model X  X  paramete rs can be estimated by (9.1).  X  s can be inter-preted as the mixture weights o f cluster occurring in sentence s .  X  i and  X  i are the nouns and semantic neighbor X  X  occurring probability distribution in the i th cluster respectively. To evaluate the clu stering results, thr ee measures are used including perplexity, average clustering entropy (ACE) and normalized mutual information index (NMI) which will be discussed below. 5.1 Perplexity Perplexity is an effective measure for gen erative language models. It is defined as the exponential of geometrical mean of the probability of each word-pair X  X  occurrence. The model learned from the training corpus is denoted by M Train = {  X 
T , X  T , X  T } . The likelihood of the testing sentences W Test given the hyper-parameters and models can be calculated by integrating out the latent variables and the parameters as Here  X  Test is estimated by applying equation (9.1) on samples drawn from Gibbs sampling on the testing dataset. So the perplexity on joint probability of noun and semantic neighbor can be computed as
PPX ( W Test )= exp (  X  i where PPX is the perplexity function and N Test is the total number of occurrence of pairs in the testing sentences W Test . This perplexity is a word occurrence perplexity averaging between noun and adjective. Square of the perplexity in (11) is a perplexity of the occurrence of a word pair of noun and adjective.
To calculate the perplexity of noun features, there are two ways, including marginalize the semantic neighbor variables and calculate the likelihood of noun features conditional on semantic neighbor feature.

To represent the noun and semantic feature, W Test can be rewritten as { marginal perplexity and conditional perplexity can be computed as below: 5.2 Average Cluster Entropy After running Gibbs sampling on the training sentences W Train , the model X  X  distribution of nouns and semantic neigh bor X  X  terms respectively. So the entropy of nouns in topic i can be computed as Similarly the entropy of sem antic neighbors for cluster i is The average cluster entropy of nouns and semantic neighbors are the mean of Ent NN and Ent S respectively. Low average cluster entropy means more discrimi-native between clusters, more cohesive w ithin cluster and so better performance. 5.3 Normalized Mutual Information Index The normalized mutual information (NMI) is an important external cluster eval-uation measure for the density of clusters. To calculate NMI, mutual entropy be-tween cluster and the feature should be computed in advance as I ( C, F )where C is the cluster variable and F is the data X  X  features. In our case, there are two set of features for each sentences including the nouns words and semantic neighbor terms. Based on the samples of { z i } from the Gibbs sampling process, a cluster assignment statistics matrix A = { a i,j } can be built by taking a i,j as for semantic neighbors. The mutual entr opy between cluster variable and noun W NN is calculated on A as By normalization, the NMI can be computed as H ( C )and H ( W NN ) are the entropy of cluster and noun features, written as  X  tries in a row and column of matrix c (  X  ,  X  ) respectively. Similarly, NMI between cluster variable and semantic neighbor feature can be computed by alternating matrix A with B in (14). We conduct the experiments on two public datasets.  X  X itySearch X  dataset used in [12] contains user reviews of res taurants in US, which is crawled from newyork.citysearch.com . And  X  X ripAdvisor X  dataset [13] includes user re-views on hotels, which is crawled from tr ipadvisor.com. Pr eprocessing steps include first POS tagging with OpenNLP packages [9], and then dependency parsing by Malt Parser [11]. Then a standard stop-word list [15] is used and dozens of corpus frequent terms is removed. After removal of nouns, neighbors and sentences with only one occurrence, a dictionary V NN of 3377 noun fragments and a dictionary V SNei of 2583 neighbors are obtained from 16456 sentences for CitySearch. The details of the two datasets are shown in Table 3.
 We introduce two baseline models based on conventional LDA for comparison. The first model is a sentence based unigram LDA to generate the noun occur-rence regardless of the adjectives. The second baseline mod el takes account of the adjectives into the sentence based unigram LDA by treating nouns and ad-jectives exchangeable in the sentence and merging the two vocabularies V NN and V
SNei into a united vocabulary V accordingly. The second model can be inter-preted as running a unigram LDA on a non-discriminating mixture of nouns and adjectives.
To evaluate the general performance of our models, we conduct 46 independent runs of Gibbs sampling of 6000 iterations to train the three models to extract from variant number of clusters while ten percent sentences are left out for test-ing. All evaluation measures discussed on Section 4 are computed with samples drawn after 500 iterations of Gibbs sampling on the test set. Hype-parameter is set to be update every 20 Gibbs sampling iterations. The performance of per-plexity results, average cluster entro pies and normalized mutual entropy index measure are shown in Fig. 3, 4 and 5 respectively. In Fig. 3, lowest perplexity is achieved by SDWP on noun marginally which implies that SDWP is superior to the unigram LDA as SDWP combines naturally the two views of semantic depen-dency between nouns and neighbors and the association of nouns co-occurring in one sentence. The average perplexity of SDWP is also lower than the bag-of-word based LDA ( X  X DA on mixture X ) due to the effective introduction of dependency relation between noun and neighbors. It is a promising direction in future to ex-tend the work to include all pairs of dependency relation. Lowest average cluster entropies attained by SDWP as shown in Fig. 4 indicates the clusters generated by SWDP is more cohesive and clusters give more probability mass to terms which is more discriminative and representative for the clusters, also evident from the extracted topic words in Table 5. The higher normalized mutual en-tropy index of SDWP in Fig. 5 is also another indication of the consistence and higher cohesion of the clusters of SDWP than that of the baseline models. All the three measures demonstrate consistently that our model SDWP outperforms the unigram LDA for the task of finger-grain product feature mining. Top 10 terms of each cluster extracted by SDWP on restaurant review dataset is shown in Table 5, which is obtained by ranking the terms with their cluster conditional probability learned by the model. Similarly, top 10 terms of each cluster can be obtained by sentence based unigram LDA from nouns of CitySearch dataset. It can be found that the clusters extract ed by SDWP are more descriptive and informative by both the groups of nouns and neighbors for each cluster. For example, it is clear to indicate product features as staff &amp; service, grilled food and atmosphere in cluster 2, 3 and 5 in Ta ble 5 respectively while the associated clusters in Table 4 are not so clear, e.g. cluster 3 is the mixture of staff and atmosphere. There are some interesting new clusters special in Table 5, such as  X  X nternal environment X  in cluster 7,  X  X omparison of cuisine X  in cluster 14 and etc. Furthermore, there are more descriptive and discriminative fine-grained clusters in Table 5, e.g.  X  X ood X  and  X  X ad X  staff i s differentiated between cluster 3 and 15,  X  X nternal X  and  X  X utdoor X  environm ent differentiated between cluster 1 and 7 and etc. To analysis the disadvantage of conventional LDA, it is found that noise is brought in when conventional LD A takes into account the co-occurrence of nouns and adjectives without dependency relation.
 Highly cohesive fine-grained product features are obtained as shown in Table 5. Furthermore, the associated group of semantic neighbors helps to discriminative the context of the features, e.g. cluster 3 and 15 in Table 5, which cannot be achieved by the conventional methods. The semantic neigh-bors of each SDWP cluster contain subj ective and objective terms where ob-jective terms provide fact context of ea ch product feature and the subjective terms are good semantic candidates to build feature-specific opinionated words vocabulary for further sentiment analysis. The clusters generated by LDA on the non-discriminative mixture of noun and adjective are shown in Table 6, where the terms are not as informative and cohesive as that of SDWP in Table 5. Due to limitation of paper number,Comparison between results on TripAdvisor dataset obtained by SDWP and unigram LDA will not be shown in this paper. Interested readers could find full results on both dataset at  X  http://www.comp.hkbu.edu.hk/ ~ tjzhan/SDWP_Results.html  X .

To measure the quality of representative terms for the clusters in a quantita-tive manner, a semantic relevance score is defined as ratio of number of manual evaluated semantic relevant items, namely, the underlined items, to the number of items representing each cluster in the Table 4, 5 and 6. For example, the se-mantic relevance score of cluster 1 in Table 5 is 15/20. Comparison of averaged semantic relevance scores of clusters e xtracted by SDWP and the two baseline models is shown in Table 7, where the each cluster is represented by ten top items of noun and (or) adjective. The results in Table 7 are consisting with the topical results and discussion above, which demonstrate that SDWP outperforms the other two unigram LDA baseline models.
 Different from the bag-of-word approach in this paper, we model sentence as pairs of noun and adjective with semantic dependency. The semantic dependency between nouns and adjectives are combined with the sentential co-occurrence between nouns to develop a semantic dependent word pair generative model to extract semantic cohesive clusters of n ouns and of adjectives for representing fine-grained product feature. Gibbs sampling is applied to infer the hidden vari-ables and to infer the parameters. To evaluate the performance of the model, we computes the perplexity, average cluster entropies and normalize mutual en-tropy index on the samples drawn from the Gibbs sampling. The sentence based LDA is employed as the baseline mode represented by bag-of-noun corpus and non-discrimination mixture of nouns and adjectives corpus respectively. The ex-perimental results demonstrate the advantage of our model and show promising direction on further research.
 Acknowledgments. This work is partially supported by HKBU research grant FRG2/09-10/052.

