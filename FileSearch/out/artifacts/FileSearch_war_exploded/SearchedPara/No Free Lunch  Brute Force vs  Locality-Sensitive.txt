 This work explores the problem of cross-lingual pairwise sim-ilarity, where the task is to extract similar pairs of doc-uments across two different languages. Solutions to this problem are of general interest for text mining in the multi-lingual context and have specific applications in statisti-cal machine translation. Our approach takes advantage of cross-language information retrieval (CLIR) techniques to project feature vectors from one language into another, and then uses locality-sensitive hashing (LSH) to extract similar pairs. We show that effective cross-lingual pairwise similar-ity requires working with similarity thresholds that are much lower than in typical monolingual applications, making the problem quite challenging. We present a parallel, scalable MapReduce implementation of the sort-based sliding win-dow algorithm, which is compared to a brute-force approach on German and English Wikipedia collections. Our central finding can be summarized as  X  X o free lunch X : there is no sin-gle optimal solution. Instead, we characterize effectiveness-efficiency tradeoffs in the solution space, which can guide the developer to locate a desirable operating point based on application-and resource-specific constraints.
 Categories and Subject Descriptors : H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms : Algorithms, Performance Keywords : LSH, machine translation, Wikipedia
In its most general form, pairwise similarity computation deals with finding pairs of objects in a large dataset that are similar according to some measure. This problem is frequently encountered in text processing applications, for example, clustering for unsupervised learning [28, 26], gen-eration of similarity lists for  X  X ore-like-this X  queries [18], and near-duplicate detection in the web context. Instances of this problem are encountered in other domains such as bioinformatics as well [22].

This work focuses on pairwise similarity in text collec-tions, but introduces a new twist X  X e wish to mine similar pairs of documents that are in different languages. At a high level, this problem is driven by an evolution toward more multi-lingual societies, which makes the ability to commu-nicate across language barriers increasingly important. Ma-chine translation (MT) is one technological solution, and modern systems are heavily driven by statistical methods dependent on large amounts of training data [16]. Tradition-ally, algorithms have required texts that are mutual trans-lations of each other X  X alled parallel corpora X  X ut there has also been work on exploiting comparable corpora X  X r texts in different languages that are similar, but not necessarily mutual translations [21, 29, 24, 31]. In our view, parallel and comparable corpora lie on a continuum of similarity, and so-lutions to the cross-lingual pairwise similarity problem can unlock new sources of training data for MT systems.
The specific application we focus on is automatically gen-erating links between Wikipedia articles in different lan-guages (so-called  X  X nterwiki X  language links). In addition to contributing to the overall goal of data mining for machine translation, we believe the task is independently interesting. For example, there presently exists a link between the article  X  X riedensnobelpreis X  in German and  X  X obel Peace Prize X  in English. However, as far as we know, these links are manu-ally created, and therefore suffers from many shortcomings: link creation requires volunteers who know both languages and is a labor-intensive, error-prone endeavor. As a result, link coverage is relatively sparse. A cross-lingual pairwise similarity algorithm may help: its output can feed into a manual verification process that improves both speed and accuracy (e.g., perhaps aided by crowdsourcing).

Our approach uses locality-sensitive hashing (LSH) [4, 5, 2], which represents documents as bit-signatures, such that two similar documents are likely to have similar signatures. A sort-based sliding window algorithm on permutations of bit signatures is used to extract similar pairs. LSH pro-vides a tradeoff between efficiency and effectiveness by user-controlled parameters, and can be straightforwardly paral-lelized since multiple randomizations run independently.
Although LSH is a well-explored solution to the pair-wise similarity problem, this work has several contributions. First, we introduce and motivate the cross-lingual variant of the pairwise similarity problem, which has received lit-tle attention in the literature. Second, we demonstrate that the cross-lingual variant of the problem is considerably more challenging since it requires mining document pairs that have low similarities, far lower than thresholds typically used in monolingual solutions. Third, we describe a scal-able MapReduce [10] implementation of the sort-based slid-ing window LSH algorithm for solving the problem. The solution exhibits linear scalability and a high degree of par-allelism that makes it suitable for large collections. Finally, we present an analytical and empirical analysis of our ap-proach across a wide range of parameter settings, using the brute-force N 2 approach as a baseline. Our central finding can be summarized as  X  X o free lunch X : there is no single op-timal solution to the cross-lingual pairwise similarity prob-lem. Instead, we characterize effectiveness-efficiency trade-offs within the solution space, which can guide the developer to locate a desirable operating point based on application-and resource-specific constraints. This characterization ad-dresses a weakness of LSH approaches in general, in that they present a bewildering number of parameters that need to be set, and provide little guidance for an application de-veloper approaching new problems. To this end, we show that the efficiency and the effectiveness of our algorithm can be analytically estimated, thus allowing the developer to characterize the tradeoff space for a particular problem without actually needing to run any experiments.

The remainder of the paper is organized as follows. We begin with related work in Section 2. A formal definition of the cross-lingual pairwise similarity problem is provided in Section 3. Our LSH-based approach is detailed in Section 4, followed by an analytical model in Section 5. Experimental results are presented in Section 6 before concluding.
The problem of efficiently computing pairwise similarity has been extensively studied by text processing and data mining researchers; in the database community, this is bet-ter known as the  X  X et similarity join X  problem [12, 33]. An-other variant that has received attention is computing pair-wise  X  X unctional X  computations (not necessarily similarities) between elements in large datasets [23, 15].

In general, two approaches to the problem exist: the index-based approach focuses on building an inverted index and pruning it to achieve efficient similarity computations [3, 12, 32, 33]; and the signature-based approach that transforms the data into a more compact representation for performing similarity comparisons. The LSH [4, 5, 2] techniques we adopt exemplifies the second approach.

There is also extensive work on near-duplicate detection of web pages, which can be viewed as a special case that aims to detect highly-similar pairs, sometimes without a prede-fined similarity threshold or even a similarity measure. Both index-based [32, 35] and signature-based [6, 20, 17, 13, 14] approaches have been proposed to address this problem.
All of the above focus on monolingual or homogeneous similarity, either similarity within one language or similarity within a homogeneous collection of objects. We are aware of some work that attempts to solve the cross-lingual version of the problem, but in a slightly different setting. Anderka et al. [1] concluded that both signature-based and index-based approaches require at least a linear scan of the collection due to the low similarity thresholds. However, the authors do not describe an algorithm or show any experimental results. In another recent paper, Platt et al. [27] described techniques for projecting multi-lingual documents into a single vector space: training  X  X ncourages X  comparable document pairs to have similar vector representations. This and other work on extracting parallel sentences [24, 31] focus primarily on rep-resentational issues and do not explicitly address questions of scale. Thus, these techniques can be viewed as comple-mentary to our own.

Our work takes advantage of Hadoop, the open-source im-plementation of MapReduce [10], which has recently emerged as a popular model for large-scale data processing. The problem of pairwise similarity computation has been stud-ied in MapReduce previously [18, 11, 33]. However, these algorithms adopt an index-based approach, which stands in contrast to our signature-based approach.
Assuming a  X  X ag of words X  model, where a document d is represented as a vector of term weights w t,d , one for each term t in the vocabulary space V , similarity between two document vectors u and v is computed via cosine similarity: Sim( u,v ) = cos(  X  ( u,v )). There are, of course, many possible weighting schemes for w t,d ; here, we adopt BM25 [30], which has been shown to be effective for many retrieval tasks. In this scheme, each weight is a function of document frequency ( df ) and term frequency ( tf ) values.

The pairwise similarity problem is that of finding all pairs of documents ( u,v ) with cosine similarities above a certain threshold  X  . In cross-lingual pairwise similarity, we addi-tionally stipulate that the document vectors represent doc-uments in different languages, thus introducing additional complexity. Due to different vocabulary spaces, document vectors in different languages are not directly comparable.
We experimented with two different solutions to overcome vocabulary mismatch: translation and vector projection.
The first approach is to translate documents from one lan-guage into the other using an off-the-shelf machine transla-tion system. If we are working with German and English as the language pair, we could translate all German doc-uments into English, and then perform pairwise similarity in the English vocabulary space. The advantage of this ap-proach is that modern machine translation systems produce quite reasonable results, especially for European languages. The downside, however, is that machine translation is typi-cally time-consuming and resource-intensive.

The second approach is to project document vectors from one language into another using cross-language information retrieval (CLIR) techniques. Adopting the approach pro-posed by Darwish and Oard [8], a document vector v in some foreign language F can be translated into a document vector v  X  in the target language E by computing df  X  and tf  X  values for every term e  X  E as follows: where we assume df and tf values are represented in v . Since Figure 1: Histogram of cosine similarities from 505 semantically similar documents in German and En-glish, comparing Google Translate with CLIR. term weight is a function of df  X  and tf  X  values, this infor-mation is sufficient to construct the vector v  X  . The quantity p ( f | e ) is the conditional probability of a foreign language word f being the translation of a target language word e ; this distribution captures lexical translation ambiguities X  the fact that words in one language can translate to multiple words in another language. This distribution can be esti-mated from parallel corpora using unsupervised techniques, as the output of the word alignment problem [34, 25]. So-lutions to this problem are well understood, and there exist multiple off-the-shelf implementations.
Typical instances of the monolingual pairwise similarity problem involve extracting pairs with high cosine similari-ties (e.g., 0.9 or higher). For the cross-lingual case, we ex-pect similar documents in different languages to have lower similarities, since the translation process is noisy.
To empirically determine appropriate similarity thresh-olds for the cross-lingual case, we experimented with the German and English portions of the Europarl corpus, which contains proceedings from the European Parliament (1996 X  2009). We constructed artificial  X  X ocuments X  by concatenat-ing every 10 consecutive sentences into a single document. In this manner, we sampled 505 document pairs that are mutual translations of each other (and therefore semanti-cally similar by construction). This provides ground truth to evaluate the effectiveness of the two translation approaches discussed above: machine translation (in this case, we used Google Translate 1 ) and direct vector projection using the CLIR approach. In the first case, we translated each Ger-man document into English and then processed the resulting translated English document into vector form. In the second case, we first processed the German document into vector form, and then projected the vector over into English. We then computed the cosine similarities of the document pairs, under both conditions.

The distribution of the cosine similarities is shown in Fig-ure 1, binned in 0.1 intervals. We make two important ob-servations. First, the cosine similarities are surprisingly low, and we expect even lower values in practice, since these are artificially created perfect document translations. This im-plies that in order to mine semantically similar document pairs in different languages, we need to extract documents that have low cosine similarities. Second, cosine similar-ity scores are higher for Google Translate, but vector pro-jection seems reasonably competitive, especially considering the computational tradeoff: running an MT system on mil-lions of documents is computationally expensive. For this reason, the remainder of this paper uses the CLIR vector projection technique.
Our approach, which is based on LSH [4, 5, 2], consists of three steps. First, all documents are preprocessed into doc-ument vectors (Section 4.1). Second, document signatures are generated from document vectors (Section 4.2). Finally, a sliding window algorithm applied to the signatures finds all cross-lingual pairs above a similarity threshold (Section 4.3).
In the preprocessing step, documents from both languages are parsed, tokenized, and represented as weighted docu-ment vectors. All terms that occur only once in the collec-tion are removed and each remaining term is converted into an integer for efficiency reasons. Document vectors of the foreign language (i.e., German) are projected into the target language (English) by the CLIR approach explained in Sec-tion 3. Thus, the collections in two languages are converted into a single collection of document vectors in the target lan-guage. Since we are only interested in cross-lingual pairs, we use document identifiers (docids) to determine the original language of a document vector.
At the core of any pairwise similarity algorithm is the sim-ilarity calculation between pairs of documents. The major drawback of cosine similarity is that the calculation requires a number of floating point operations linear to the number of terms in u and v , which is computationally expensive. Although it is possible to accurately approximate floating point values with integers using quantization methods, the similarity calculation still remains a bottleneck when dealing with many documents and large vocabularies.

Signatures are an efficient and effective way of represent-ing large feature spaces. We explore three well-known meth-ods to generate signatures from document vectors: random projection [5], Simhash [20], and Minhash [7].
 Random projection (RP) signatures [5] use a series of random hyperplanes as hash functions to encode document vectors as fixed-size bit vectors. Let us assume a collection with vocabulary V , so that each document vector is from the domain R | V | . To obtain a signature of D bits using this approach, D randomly generated real-valued vectors of length | V | are used to map each document vector u onto a RP signature s u  X  [0 , 1] D . The i th bit of s u is determined by an inner product of u and the i th random vector.
Given D random vectors r 1 ,...,r D , the signature s u computed as follows: The cosine similarity between two documents can be com-puted via hamming distance between their signatures, ac-cording to the following relationship [5]: Simhash signatures are essentially a  X  X ash X  of the docu-ment vector. This approach relies on a hash function that generates hash values for every term in the document vec-tor. Given a document vector u and a hash function h that maps string terms to D -bit hash values, we generate a D -bit signature s as follows: where every term-weight pair ( t,w ) in u contributes + w to s [ i ] if h ( t )[ i ] = 1 and  X  w otherwise. If the sum of contribu-tions to s [ i ] is greater than 0, then s [ i ] is set to 1, otherwise, it is set to 0.

For this approach, signature quality depends on the qual-ity of the hash function, which also dictates the signature length (which cannot be set arbitrarily). In this work, we use the hash function in Manku et al. [20], where it was applied to detect near-duplicate web pages. We leave exploration of more recent methods along similar lines, such as self-taught hashing [36] for future work.
 Minhash signature for a document vector u requires K ran-dom orderings of terms in u . Given a hash function, terms can be ordered by their hash value, or alternatively, terms can be ordered by a random permutation on the vocabulary. For each of the K orderings, the term in a document that has lowest order is picked as the  X  X inimum hash X  of that document. The probability that two documents u and v have the same  X  X inimum hash X  term for a given ordering is | u  X  v | (i.e., Jaccard similarity). This procedure is repeated for K different randomly selected orderings to reduce the risk of false positives; thus, the Minhash signature of a doc-ument vector consists of all K  X  X inimum hash X  terms. The cosine similarity between u and v is approximated by the proportion of terms they share. Chum et al. [7] showed its effectiveness on near-duplicate image detection.
 In order to compare the precision of these three signature generation techniques, we selected a sample of 1064 docu-ment vectors from German Wikipedia. For every pair of doc-ument vectors, we computed the true cosine similarity and estimated values using each of the three signature methods and varying signature lengths. In each case, we computed the average absolute difference between the true and esti-mated cosine similarity value, shown in Table 1. For com-parison, the last column shows the average time (in millisec-onds) taken to generate one signature on a laptop with an Intel Core 2 Duo 2.26 GHz processor.

Simhash signatures generated by the approach in Manku et al. [20] are 64 bits. Since Minhash signatures are repre-sented as a list of integers (corresponding to term ids), they take up 32 times more space than a Simhash or RP signa-ture of the same length. We included 2-term (64 bits) and 32-term (992 bits) Minhash signatures in the comparison.
We performed three sets of experiments: First, we com-puted error for all possible pairs, then filtered out pairs with cosine similarity less than 0.1, and finally less than 0.2. Precision at very low similarities is not very impor-tant because relevant documents usually have values higher Table 1: Average cosine similarity error per signa-ture with different methods than 0.2 (based on Figure 1). We noticed that Minhash sig-natures perform worse as we filter out low-similarity pairs because it simply predicts 0.0 for many pairs. In contrast, RP signatures are more accurate when the cosine similarity is higher, an indicator of the robustness of the method. In all cases, 64-bit RP signatures are more precise than Simhash signatures, and this becomes more apparent once the low-similarity pairs are filtered out. The only drawback of RP signatures is the amount of time necessary to generate them: an inner product is needed to determine every bit. The average time to create a 64-bit RP signature is about 5 times slower than other methods with the same number of bits and generating 1000-bit RP signatures is more than 10 times slower than 32-term Minhash signatures. However, the running time of the signature construction step is relatively brief compared to the actual pairwise similarity algorithm. Based on this analysis, we selected 1000-bit RP signatures for our experiments.
The sliding window algorithm [5] is a probabilistic method which uses randomization and approximation heuristics to provide a tradeoff between efficiency and effectiveness. In this section, we first describe a straightforward parallel im-plementation of the original version of this algorithm in MapReduce (Figure 2). Then, we discuss modifications to exploit the framework and take advantage of greater paral-lelism (Figures 3 and 4). For space considerations, we as-sume that the reader has at least passing familiarity with the sliding window algorithm, as it is a well-known LSH tech-nique. We further assume that the reader is already familiar with the MapReduce programming model.
In the sliding window algorithm, Q random permutation functions p 1 ,...,p Q are created as a preprocessing step. put is the sequence of (docid, signature) pairs for the entire collection. Each mapper takes a docid n and signature s as input, and emits an intermediate key-value pair (  X  i,s i for each permutation function p i ,i = 1 ...Q . 3
Each key sent to a reducer is a pair of the permutation group number p and the signature s (denoted as  X  p,s  X  ) and the values are docids sharing that pair. The reduce step is designed so that all keys with the same permutation group number are sent to the same reducer (by appropriately parti-tioning the key space), and they are sorted according to the
Mapper 1: method Map (docid n, signature s ) 2: for all permute func p i  X  [ p 1 ,p 2 ,...,p Q ] do 3: Emit (  X  i,s. Permute ( p i )  X  ,n )
Reducer 4: method Initialize 5: docids  X  new Queue ( B ) 6: sigs  X  new Queue ( B ) 7: method Reduce (  X  permno p, sig s  X  , docids [ n 1 ,n 2 8: for all docid n  X  [ n 1 ,n 2 ,... ] do 9: for i = 1 to sigs. Size () do 10: distance  X  s. Distance ( sigs. get ( i )) 11: if distance  X  T then 12: Emit (  X  docids. get ( i ) ,n  X  ,distance ) 13: sigs. Enqueue ( s ) 14: docids. Enqueue ( n ) 15: if sigs. Size () &gt; B then 16: sigs. Dequeue () 17: docids. Dequeue () Figure 2: Pseudo-code of the initial version of the sliding window algorithm. signature bits (comparison of two signatures is from most significant to least significant bit). Therefore, each of the Q reducers receive a sorted list of permuted signatures, paired with respective docids, which we call a  X  X able X .

Based on LSH, more similar documents have higher prob-abilities of being closer in a table. Given a large enough Q and B , it can be shown that signatures of similar docu-ments will be at most B positions apart in at least one table with high probability. Since sorted order depends on the positions of bits, having multiple permutations of the same signature increases the chance of retrieving a similar pair.
A queue of size B is used to implement the idea above (
Reduce method in Figure 2): in each table, we compare all signatures that are at most B positions away from each other. The reducer iterates over the signatures and keeps the last B in the queue (lines 13 X 17). At each iteration, the current signature is compared to all signatures in the queue, and hamming distances are emitted as output (lines 8 X 12).
A drawback of the basic algorithm is the inability to in-crease the number of reducers, because each reduce group processes a single permuted list of all the signatures in the collection. In other words, in the reduce phase, only Q reducers can operate in parallel, which may be an under-utilization in very large clusters that are commonly used for MapReduce. In this case, being able to increase the number of reducers arbitrarily can help the algorithm better scale.
We achieved this by modifying the previous algorithm in the following way. The map function is exactly the same, but the reduce step divides each table into an arbitrary num-ber of consecutive blocks (Figure 3). Each of the Q reducers iterates over sorted signatures, stores them in a list (lines 9 X  11), and outputs the list when its size reaches a pre-specified size, M (lines 12 X 14). We denote each of these M -sized lists as a chunk , and refer to this as the chunk generation phase. The actual comparison and similarity computation is performed in an additional map-only MapReduce task (Fig-ure 4), called the detection phase, where the input is the chunks and each mapper finds all similar pairs in one chunk.
Mapper 1: method Map (docid n, signature s ) 2: for all permute func p i  X  [ p 1 ,p 2 ,...,p Q ] do 3: Emit (  X  i,s. Permute ( p i )  X  ,n )
Reducer 4: method Initialize 5: docids  X  new List 6: sigs  X  new List 7: chunk  X  new SignatureChunk 8: method Reduce (  X  permno p, sig s  X  , docids [ n 1 ,n 2 9: for all docno n  X  [ n 1 ,n 2 ,... ] do 10: sigs. Add ( s ) 11: docids. Add ( n ) 12: if sigs. Size () = M then 13: chunk. Set ( sigs,docids ) 14: Emit ( p,chunk ) 15: sigs  X  sigs. SubList ( M  X  B + 1 ,M ) 16: docids  X  docids. SubList ( M  X  B + 1 ,M ) 17: method Close 18: chunk. Set ( sigs,docids ) 19: Emit ( p,chunk ) Figure 3: Pseudo-code of the chunk generation phase of the sliding window algorithm.
 Only pairs that have a distance less than the threshold T are emitted by the algorithm. We use the docids to make sure only cross-lingual pairs are included in the output. In this phase, there can be as many mappers as there are chunks, which allows full utilization of computational resources.
Note that some comparisons will be missed at the bound-ary of chunks (e.g., the last element of a chunk is not com-pared to the first element of the next chunk). We solve this problem by appending the last B signatures of the previous chunk at the beginning of the current chunk (Figure 3, lines 15 X 16). This results in redundancy in the chunks emitted to disk (the last B signatures of a chunk are the same as the first B signatures of the next one), but the redundancy is negligible especially since B M . In this section, we provide a theoretical analysis of our LSH algorithm, which provides a model for estimating ef-fectiveness analytically. The starting point is Equation 1, which shows how random vectors are used to generate bit signatures. For any two vectors u and v , the probability that a single random projection h r collides is The proof can be found in [5], which we omit here. The in-tuition is that the hyperplane defined by the random vector divides our collection of vectors into two disjoint sets, and the probability that any two vectors will land in the same set is determined by the angle  X  between them, according to the relation above.
 responding to the subsets of our collection of vectors that share the same hash value. Given a vector u , we can com-pute h r ( u ) and perform a linear scan in the corresponding set to find similar vectors. Given that vectors u and v have
Mapper 1: method Map (permno p, signatureChunk chunk ) 4: distance  X  chunk. Signature ( i ) . Distance ( chunk.
Signature ( j )) 6: Emit (  X  chunk. Docid ( j ) ,chunk. Docid ( i )  X  ,distance ) cosine similarity t , the probability that they are in the same subset is given by 1  X  cos  X  1 ( t ) / X  .

We can extend this basic approach by selecting n random vectors { r 1 ,r 2 ,...r n } and construct corresponding hash func-we can divide up the collection into 2 n disjoint sets: Suppose we wish to find document vectors that are simi-lar to u : we can apply the hash functions to determine the correct candidate set of vectors, and then perform a linear scan through all those candidates to compute the actual dot product. With n random projections, the probability that we X  X l find similar vectors becomes (1  X  cos  X  1 ( t ) / X  ) random projections reduce the number of comparisons we must make, but at the cost of compounding the error intro-duced by each random projection.

To alleviate this issue, we can repeat the entire process m times using m sets of n different random projections. The probability that we X  X l identify a valid similar pair becomes:
Pr[ u,v in same set in at least one trial | cos( u,v ) = t ] The above equation quantifies the error when searching for similar vectors using LSH. The second source of error is the hamming distance computation. As shown in Section 4.2, cosine similarity can be estimated from the hamming dis-tance with Equation 2.

We use this similarity estimate to efficiently decide which documents to return during the linear scan of the candidate set. For a given vector u and similarity threshold  X  , we calculate the corresponding hamming distance threshold T using Equation 2 and return all candidates with hamming distance less than or equal to T . The precision of this deci-sion is given by the following: Pr[hamming( u,v )  X  T | cos( u,v ) = t  X  u,v share n -bit prefix] = How does this relate to our sliding window algorithm? Let us first consider a variation of our algorithm. Suppose in-stead of applying a sliding window B over the sorted bit sig-natures, we performed a pairwise N 2 comparison of all bit signatures that share n prefix bits. Another way to think about this is to dynamically adjust B so that it encompasses only those signatures that share the prefix. In this case, the expected probability of extracting a pair of vectors with similarity t (hamming distance  X  T ) is quantified by the product of Equations 4 and 5 above. The number of tables (permutations) is equal to the number of trials m and the prefix length is equal to the number of random projections n in the above analysis. The probability of successfully ex-tracting a similar pair has contributions from two sources: LSH (the two documents sharing the same signature prefix) and accuracy of hamming distance.

In actuality, we scan a fixed window B . However, we approximate n to be between b log 2 ( C/B ) c and d log 2 where C is the total number of vectors in the collection. We use both estimates to obtain a range of the estimated recall values. Although this is merely a rough model, it provides us with a basis for analytically estimating effectiveness, which we demonstrate later.
We evaluated our cross-lingual pairwise similarity algo-rithm on English and German Wikipedia, selected because they are the largest Wikipedia collections available and be-cause significant amounts of parallel corpora exist for the language pair. We used the German Wikipedia dump from 1/31/2011, which contains 2.41m articles totaling 8.5 GB. We used the English Wikipedia dump from 1/15/2011, which contains 10.86m articles totaling 30.6 GB. For both collec-tions we discarded redirect pages and stub articles. Our system is implemented on top of Ivory, an open-source Hadoop toolkit for web-scale information retrieval [19]. Ger-man articles were projected into English document vectors, as described in Section 3. For translation probabilities, we trained a word alignment using the Berkeley Aligner 4 on the Europarl German-English corpus, containing 1.08m sentence pairs from European parliament speeches. For tokenization, we used the Java-based OpenNLP toolkit. 5 After projection, document vectors containing fewer than five terms were dis-carded. We arrived at 1.47m German articles and 3.44m English articles. In the next step, RP signatures were gen-erated from document vectors. Finally, we ran the improved sliding window algorithm (Section 4.3.2) to extract all docu-ment pairs with cosine similarity above a specified threshold. We only output pairs in which one document is from German Wikipedia and the other is from English Wikipedia.

All experiments were conducted on a Hadoop cluster (run-ning Cloudera X  X  distribution, version 0.20.2+320) with 16 nodes, each having 2 quad-core 2.2 GHz Intel Nehalem Pro-cessors, 24 GB RAM, and three 2 TB drives.
The parameters in our algorithm are: the length in bits of each signature ( D ), the number of tables ( Q ), the chunk size ( M ), the window size ( B ), and the hamming distance threshold ( T ). We fixed D to 1000 based on the discussion Figure 5: Running times of the detection phase of the sliding window LSH algorithm for  X  = 0 . 3 , using 1000-bit signatures. Data points are annotated with recall and relative cost. in Section 4.2, and fixed M to 0.49m so that each table is split into 10 chunks. For the rest of the parameters, we explored a number of choices and observed the changes in efficiency and effectiveness. Note that we need to run the preprocessing step and signature generation step only once for these experiments (this takes 1.67 hours). The chunk generation phase of the sliding window algorithm must be executed once for every value of Q (this takes 0.28, 0.53, and 0.75 hours, respectively, for Q = 100 , 200 , 300). The detection phase needs to be run for every different set of the remaining parameters, for which running times on the entire German-English Wikipedia collection are plotted in Figure 5 (varying window sizes for each value of Q ). The regression lines clearly show that the algorithm scales lin-early as Q and B increase ( R 2 &gt; 0 . 999 in all three cases). Note that the hamming distance threshold T does not af-fect the running time, but only determines which pairs are extracted; therefore, it is not included in this figure.
To assess effectiveness, we selected a sample of 1064 Ger-man Wikipedia articles. For every article, we calculated its true cosine similarity with all other documents in English Wikipedia and retained all pairs above threshold  X  = 0 . 3 to serve as the ground truth, informed by the similarity distribution in Figure 1. This corresponds to a hamming distance threshold of T = 400 for 1000-bit signatures, since cos(400  X / 1000)  X  0 . 3. Note that a hamming distance of 500 corresponds to no similarity at all, so we are looking for documents that may be very dissimilar. From this, we are able to measure the quality of the pairs extracted by our algorithm. We argue that recall is the most salient evalua-tion metric, i.e., the fraction of pairs in the ground truth set that are actually extracted, since the task is recall-oriented by definition. If precision is desired, one could always filter the extracted pairs and discard results that fall below the similarity threshold X  X he time necessary for this is negligi-ble compared to the time for extracting the pairs to begin with. For instance, with parameters Q = 300 ,B = 2000, the number of extracted pairs is 64 million, 0.0013% of all possible cross-lingual pairs in the collection. This gives an idea of how much of the search space is filtered by the algo-rithm. Moreover, from an application point of view, it may not even be necessary to filter, since our  X  threshold of 0.3 is somewhat arbitrary, and pairs with lower similarity scores may nevertheless be useful (see Figure 1).

The data points in Figure 5 are annotated with recall and a measure we call relative cost , defined as follows: for each condition, we can analytically compute the total number of similarity comparisons (i.e., in terms of hamming distance) necessary over the entire run. We can express this as a percentage of the total number of comparisons that a brute-force approach would require, which is the product of the size of the two collections: with 3.44m English articles and 1.47m German articles, this equals 5.05 trillion comparisons. The cost of the chunk generation process is also taken into account in these calculations (by translating that time into an equivalent in terms of number of comparisons). A rela-tive cost lower than 100% means we X  X e  X  X aving work X  by not considering pairs unlikely to be similar X  X he entire point of LSH. In essence, each pair in the graph delineates a point in the tradeoff space: the level of recall and efficiency that can be expected for a particular parameter setting.

To better understand these results, it is necessary to quan-tify the upper bound on effectiveness. Note that our algo-rithm has two sources of error: those introduced by the bit signatures (cf. Table 1) and those introduced by the sliding window algorithm (i.e., failure to find similar pairs within B ). We can isolate the error introduced by the sliding win-dow and compute an effectiveness upper bound by repeating the procedure that generated the ground truth, but using signatures instead. For every sample document X  X  signature, we computed the hamming distance with all other signa-tures in English Wikipedia and extracted all pairs that have a distance less than T = 400. We then compared these pairs to the ground truth set and obtained 0.76 recall. 6 We see that the 0.04 absolute error in cosine similarity (cf. Table 1) introduced by 1000-bit random projections, which is negli-gible for other tasks that adopt high similarity thresholds (e.g., [28]), has a large impact for our task. Another way to understand this is that absolute error has an increasing impact as the similarity threshold is lowered (i.e., 0.04 abso-lute error is equal to 13% relative error when we are dealing with cosine similarity values of 0.3). The conclusion to draw from this analysis is that the upper bound on recall for our sliding window algorithm is not 1.0, but a more modest 0.76.
The adage  X  X o free lunch X  best characterizes our experi-mental results. There does not appear to be a single optimal solution to the cross-lingual pairwise similarity problem us-ing LSH. Gains in efficiency inevitably come at a cost in ef-fectiveness, and the extent to which it is worthwhile to trade off one for the other depends on application-and resource-specific constraints: i.e., how much recall is necessary, how much computational resources are available, etc. We pro-vide a guide that helps the application developer locate a desirable operating point in the solution space.

We additionally explored the impact on upper bound ef-fectiveness of different representations. Increasing the num-ber of random projections produces longer and therefore more precise signatures, at the cost of linearly increasing running times. This is shown in Table 2, where we re-peated the same experiment for 2000-and 3000-bit signa-tures and show the average time (in ns) to process a single pair. Increasing signature length, however, does not appear Table 2: Comparing different representations: aver-age time for a similarity comparison and the level of effectiveness achieved. to have much of an impact on recall. As a reference, we also show results with the original document vector. Even with 3000 bits, hamming distance calculations are still more than 5 times faster than computing similarity directly on document vectors (which require floating point operations). Once again, there X  X  no free lunch: representations that sup-port faster similarity comparisons sacrifice fidelity.
It would be desirable to more exhaustively explore the parameter space, beyond the experiments in Figure 5. The major impediment, however, is the long running times of the experiments. Nevertheless, it is possible to separately eval-uate effectiveness and efficiency in a meaningful way. The amount of work required by our sliding window algorithm can be quantified by the number of comparisons required. This measure is convenient since it can be computed ana-lytically without actually running experiments, and in some sense is better than (wall clock) running time since it ab-stracts over hardware differences and other natural varia-tions. Since the results in Figure 5 confirm that our algo-rithm scales linearly, we can derive a straightforward map-ping from number of comparisons to actual running time. To compute effectiveness, we can greatly speed up the ex-periments by considering only documents in the test set.
The results of separately evaluating effectiveness and ef-ficiency are shown in Figure 6, where we vary the number of tables Q for B = 200 , 500 , 1000. Once again, this charac-terizes the tradeoff space by showing the recall and relative cost (labeled on the points) that is realized by a particular parameter setting.

Figure 6 also shows recall estimates based on our ana-lytical model from Section 5. As previously discussed, we bound n by b log 2 ( C/B ) c and d log 2 ( C/B ) e , where C is the total number of vectors in the collection: these are shown in the figure as dotted lines (but note overlaps). Despite the simplifications made in that model, we see that the es-timates are fairly good. This rough model allows the appli-cation developer to tackle arbitrary collections, and without performing any actual experiments , estimate the effective-ness that can be achieved with different parameter settings. Combined with the ability to compute efficiency analytically, as we have shown above, the developer can characterize the tradeoff space with minimal effort. Such an analysis ad-dresses a general weakness of LSH approaches: they present a bewildering number of parameters that need to be set, and provide little guidance for a developer approaching new problems. The ability to quantify efficiency and analytically estimate effectiveness provides a powerful tool for tackling the pairwise similarity problem.

In Figure 7, we push our analysis even further with a wider setting of parameters, enabled by separating effective-ness and efficiency measures as above. Each data point is annotated with actual recall achieved. The thick horizontal Figure 6: Effectiveness-efficiency tradeoffs of the sliding window LSH algorithm for  X  = 0 . 3 , using 1000-bit signatures. Figure 7: Number of distance calculations and cor-responding recall values. line indicates the number of comparisons required for the brute-force approach. This particular analysis holds impor-tant implications for the type of underlying hardware that can be used to run our algorithm. Since the number of tables Q dictates the amount of parallelism that can be extracted, this suggests that larger clusters with more processing ca-pacity may benefit from larger values of Q to increase cluster utilization. On the other hand, a smaller cluster with faster processors might benefit from larger values of B , since the size of the sliding window determines how fast an individual chunk is processed. Once again, there is no one-size-fits-all solution: the most optimal operating point in the tradeoff space will depend on the specifics of a particular application and the computational resources at hand.

One interesting observation is that the brute-force solu-tion should not be so readily dismissed. In fact, to achieve very high levels of recall, the brute-force approach actually requires less work. This is shown in Figure 8: the x -axis plots relative cost, while on the y -axis we normalize effec-tiveness with respect to the recall upper bound to obtain a relative effectiveness measure. Each of the lines corresponds to a different setting of Q under different settings of B . This Figure 8: Relative effectiveness vs. relative cost of the sliding window algorithm shows that if a relative effectiveness of greater than 99.5% is desired, it is more efficient to simply use the brute-force approach. On the other hand, the cost can be reduced sig-nificantly by trading off effectiveness. For instance, only 35% of the cost is required to achieve close to 93% relative effectiveness. The algorithm obtains 95% and 98% relative effectiveness with a cost about 60% and 50% less than the brute force approach, respectively. This analysis shows that Q = 1000 appears to be a good setting X  X ut once again, there are many practical concerns that need to be taken into account. But inevitably, there is no free lunch, as efficiency can only be gained at the cost of effectiveness.
Finally, we analyze the benefits and drawbacks of our end-to-end system, on the task of matching relevant cross-lingual documents in Wikipedia. This can be done quantitatively by comparing the algorithm X  X  output to  X  X nterwiki X  language links. These links are created manually by users, and are supposed to connect pages about the same entity across lan-guages. However, there has been some work showing that these links are inaccurate [9]. Therefore, we find these links unsuitable for use as ground truth, and present the following results with this strong caveat.

To establish a reference on output quality, for each sam-ple German article, we extracted the English article with the highest cosine similarity (using document vectors). The actual  X  X nterwiki X  links are available from the source of each Wikipedia page, but only 401 of the sample articles had one listed in our downloaded version of Wikipedia. Among those links, our algorithm identified 130 of them (33%) as the most similar article.

A deeper, qualitative analysis shows that our proposed links are generally helpful, and in many cases they are more comprehensive than the existing Wikipedia links. Table 3 shows examples of article pairs that were assigned a co-sine score above 0.3. For the German article titled  X  X eta-daten X , the algorithm correctly puts  X  X etadata X  at the top of the list and includes two other related articles:  X  X emantic Web X  and  X  X ile format X . Although  X  X ierre Curie X ,  X  X arie Curie X  and  X  X   X el`ene Langevin-Joliot X  are all physicists from the same family, and therefore similar, the algorithm places Marie above Pierre in terms of similarity for the German Table 3: Examples of relevant German X  X nglish ar-ticle pairs found by our algorithm. version. This may be due to the richer content of the article about Marie Curie. For the German article  X  X irgisistan X , 10 articles about recent events in Kyrgyzstan, its leaders and neighbors are in the output (we show the top five matching articles in Table 3). In short, linking cross-lingual articles via similarity has the potential to discover related articles, not just articles on exactly the same entity.

It is interesting that our system was not able to return any similar article for 571 of the 1064 sample German arti-cles. For the remainder, it may simply be the case that a similar article does not exist. This is true especially for ar-ticles about specific locations in Germany (e.g.,  X  X ortmund-Aplerbeck X ). After analyzing some of these cases by hand, we discovered that there are two common situations where the algorithm struggles. The first is when there is a large gap between the document lengths and contents of the arti-cles in different languages. For instance, the German version of the article about the music show on German TV chan-nel ZDF ( X  X DF-Hitparade X ) is far more comprehensive than the English version. Since cosine similarity measures how similar documents are, it falls below 0.3 in cases like this. Secondly, our system has difficulty matching highly techni-cal articles (e.g.,  X  X arbeindringpr  X  ufung X , English  X  X ye pen-etrant testing X ), due to out-of-domain vocabulary: the vec-tor projection has difficulty matching technical terms across languages. This could be solved with more data or special processing of named entities. We will address these issues as part of future work.
The cross-lingual pairwise similarity problem requires ex-tracting pairs that have low cosine similarity values X  X  re-quirement imposed by the nature of the task. In this work, we explored an LSH-based approach to the problem and analyzed two different components of the solution: the sig-nature generation process and the sliding window algorithm for extracting similar pairs. We showed that 1000-bit sig-nature vectors with random projections are not sufficient to achieve anywhere close to perfect recall but this is the cost of a representation that is significantly faster to process. Our solution is implemented as a parallel, scalable Map-Reduce algorithm and demonstrated to scale linearly on the two largest Wikipedia collections. We experimentally and analytically quantified the effectiveness-efficiency tradeoff of the sliding window approach with respect to a multitude of parameters. This characterization provides a guide to the application developer in selecting the best operating point for a specific situation. A somewhat surprisingly finding is that a brute-force approach should not be readily dismissed as a viable solution, especially when high recall is desired.
Part of our future work is to improve the representation and translation of documents by including named entity recognition and using broader and larger vocabularies, as well as techniques from natural language processing. Ger-man word translations may be very noisy due to compound-ing and other morphological phenomena, so we plan on ex-perimenting with other language pairs. Using learning meth-ods to find better random projections has been shown to work well [27] and we are interested in adapting that ap-proach to our system. As a logical next step, we would like to go beyond cross-lingual pairwise similarity at the docu-ment level to directly extract bilingual sentence pairs, and then use these as an additional source for training machine translation systems. Finally, we hope to show the scalability of our algorithm on even larger datasets.
This work has been supported in part by DARPA con-tract HR0011-06-2-0001 (GALE); NSF under awards IIS-0836560, IIS-0916043, and CCF-1018625. Any opinions, findings, conclusions, or recommendations expressed are the authors X  and do not necessarily reflect those of the sponsors. The last author is grateful to Esther and Kiri for their loving support and dedicates this work to Joshua and Jacob. [1] M. Anderka, B. Stein, and M. Potthast.
 [2] A. Andoni and P. Indyk. Near-optimal hashing [3] R. Bayardo, Y. Ma, and R. Srikant. Scaling up all [4] A. Broder. On the resemblance and containment of [5] M. Charikar. Similarity estimation techniques from [6] A. Chowdhury, O. Frieder, D. Grossman, and [7] O. Chum, J. Philbin, and A. Zisserman. Near [8] K. Darwish and D. Oard. Probabilistic structured [9] G. de Melo and G. Weikum. Untangling the [10] J. Dean and S. Ghemawat. MapReduce: Simplified [11] T. Elsayed, J. Lin, and D. Oard. Pairwise document [12] M. Hadjieleftheriou, A. Chandel, N. Koudas, and [13] M. Henzinger. Finding near-duplicate web pages: a [14] L. Huang, L. Wang, and X. Li. Achieving both high [15] T. Kiefer, P. Volk, and W. Lehner. Pairwise element [16] P. Koehn. Statistical Machine Translation . Cambridge [17] A. Kolcz, A. Chowdhury, and J. Alspector. Improved [18] J. Lin. Brute force and indexed approaches to pairwise [19] J. Lin, D. Metzler, T. Elsayed, and L. Wang. Of Ivory [20] G. Manku, A. Jain, and A. Das Sarma. Detecting [21] H. Masuichi, R. Flournoy, S. Kaufmann, and [22] S. Matthews and T. Williams. MrsRF: An efficient [23] C. Moretti, J. Bulosan, D. Thain, and P. Flynn. [24] D. Munteanu and D. Marcu. Improving machine [25] F. Och and H. Ney. A systematic comparison of [26] P. Pantel, E. Crestan, A. Borkovsky, A.-M. Popescu, [27] J. Platt, K. Toutanova, and W.-t. Yih. Translingual [28] D. Ravichandran, P. Pantel, and E. Hovy. Randomized [29] P. Resnik and N. Smith. The web as a parallel corpus. [30] S. Robertson, S. Walker, S. Jones, [31] J. Smith, C. Quirk, and K. Toutanova. Extracting [32] M. Theobald, J. Siddharth, and A. Paepcke. SpotSigs: [33] R. Vernica, M. Carey, and C. Li. Efficient parallel [34] S. Vogel, H. Ney, and C. Tillmann. HMM-based word [35] H. Yang and J. Callan. Near-duplicate detection by [36] D. Zhang, J. Wang, D. Cai, and J. Lu. Self-taught
