 n hidden-output vector h of length H , leading to the final output y : Here, fictitious outputs x thresholds p (final) output; v first hidden layer; w vectors , p , v , w are denoted by n , n For parameter optimization, one may attempt to minimize the squared error over m data where t where J r r , an m n Jacobian matrix of r , and the d th row of J is denoted by r r T omitted, and its search direction is found by solving J GN = r (or , J T J GN = r E ). Under the Gauss-Ne wton method (see p.1404 [1]; p.1031 [2]) in the nonlinear least squares sense. Since It would be of great value to understand the weaknesses of such Gauss-Ne wton-type methods. Sampson (pp.65 X 66 [5]) described an overlap-singularity situation involving a redundant model; specifically , a classical (linear -output) model of exponentials with h Eq.(1): If the tar get data follo w the path of a single exponential then the two hidden parameters, v rank-deficient, then the search direction GN can be numerically orthogonal to r E at some distant point from ; consequently , no progress can be made by searching along the Gauss-Ne wton direc-in solving a particular system of nonlinear equations, for which the merit function is defined as Eq.(3), where m = n . Another weak point of the Gauss-Ne wton-type method is a so-called lar ge-nonlinear , or its norm is lar ge at solution . Those dra wbacks of the Gauss-Ne wton-type meth-escape from singularity plateaus, thereby enhancing the learning capacity . ture often arises. This is true with an arbitr ary MLP model, but to mak e our analysis concrete, we consider a single terminal linear -output two-hidden-layer MLP: f ( ; x ) = P H 2.1 An existence of the 4 4 indefinite Hessian block H in r 2 E hidden layer . The weights connecting to those two nodes are p in the follo wing manner: where v residual Jacobian matrix J ( @ r z = 1 ) at the first hidden layer where only four entries are sho wn that are associated with four weights: p locations of those four weights in the n -vector are denoted by l Given J , we interchange columns 1 and l finally columns 4 and l which can be readily accomplished by P T r 2 E P , where four permutation matrices P are emplo yed as P P first four leading rows and columns of P T r 2 E P has the follo wing structure: The posed Hessian block H is associated with a vector of the four weights [ p If lated, two columns h Hessian block is almost always indefinite (so is r 2 E of size n n ) to be pro ved next. 2.2 Case 1: v yielding the gradient vector g of length 4 and the 4-by-4 Hessian block H [see Eq.(9)] associated with the four weights [ p where the entries are given belo w with B P m Notice here that the subscript d implies datum d ( d = 1 ; :::; m ) ; hence, h on datum d (but not the d th hidden-node output) common to both nodes j and k due to v Pr oof: A similarity transformation with T , a 4-by-4 ortho gonal matrix ( T T = T 1 ), obtains where 1 The eigen values of the 2-by-2 block at the lower -right corner are obtainable by which yields 1 2.3 Case 2: v The result in Case 1 becomes simpler: For a given set of m (training) data, of
S ; namely , 1 2 ( d Pr oof: Proceed similarly with the same orthogonal matrix T as defined in Eq.(12), where b has the sign-dif ferent eigen values determined by 2 d e 2 = 0 . 2 QED 2 (when d 6 = 0 ); hence, H becomes singular .
 Theor em 4 : If r E ( ) = 0 , and e = 0 , but d &lt; 0 [see Eq.(13)], then is a saddle point. the entire Hessian matrix r 2 E of size n n is indefinite 2 QED 2 howe ver, we could alter the eigen-spectrum of H by changing linear parameters p in conjunction with scalar for p held fix ed (to be confirmed in simulation; see Fig.1 later), leading to the follo wing Theor em 5 : If D 6 = 0 and C &gt; 0 [see the definition of C and D for Eq.(11)] and v p = 2 p and p k = 2(1 ) p can render H and thus r 2 E indefinite.
 of : The (3,3)-entry of H , H (3 ; 3) = 2 p (2 pC + D ) ; has two roots: 0 and D (4,4)-entry , H (4 ; 4) = 2(1 ) p [2(1 ) pC + D ] ; has two roots: 1 and 1 + D by Cauch y X  X  interlace theorem, so is r 2 E . 2 QED 2 Example 1 : A two-e xponential model in Eq.(5).
 Given two sets of five data pairs ( x a minimizer 0 = [ p ; v ] T of a two-weight 1-1-1 MLP , and then expand it with scalar as = [ p ; (1 ) p ; v ; v ] T to construct a four -weight 1-2-1 MLP that produces the same input-to-by solving r E = 0 , which yields p = 2 ; v = 0 ; E ( 0 ) = 1 as = [ p which realizes the same input-to-output relations as the 1-1-1 MLP . Fig.1 sho ws how changes the eigen-spectrum (see solid curv e) of the 4 4 Hessian r 2 E (supported by Theorem 5). Sk etch of Pr oof: Choose a node j among H hidden nodes, and add a hidden node (call node k ) by duplicating the hidden weights by v (due to p value ( x -axis) in = [ p ; (1 ) p ; v ; v ] T , the four weights of a 1-2-1 MLP with exponential weights), any local minimum point may not be found by optimizing a 2-1-1 MLP (five weights), since the hidden weights tend to be divergent (or weight-1 attractors). Here is another example: node function ( x ) 1 Hessian argument for Blum X  s line in Sec.3.2.
 We continue with Example 2 to verify the abo ve theorems. We set = [ p ; (1 ) p ; v ; v ] in is added to v , then r 2 E becomes indefinite (see Theorem 2). In contrast, when = 1 : 5 , r 2 E became indefinite (minimum eigen value 0 : 2307 ); this situation was similar to Fig.1(left). mix ed linear and nonlinear) optimization problems. As a small non-MLP model, consider , for in-Expressed belo w are the gradient and Hessian of F : procedure described abo ve fails because D = 0 (see Example 3 belo w also). Some other types of our paper did not claim anything about local minima, and our approach is totally dif ferent. Example 3 : A linear -output five-weight 1-1-2-1 MLP with = [ p H = O , a 4 4 block of zeros, r 2 E would be indefinite (ag ain by the interlace theorem) as long then the negative-curv ature information will be lost due to r 2 E + I . where v linear since both hidden and final outputs are produced by sigmoidal logistic function ( x ) 1 3.1 Insensiti vity to the initial weights in the singular XOR problem The world-reno wned XOR problem (in volving only four data of binary values: ON and off) with a standard nine-weight 2-2-1 MLP is ine vitably a singular problem because the Gauss-Ne wton Hes-MLP can develop insensiti vity to initial weights, always solving the posed XOR problem. 3.2 Blum X  s linear manif old of stationary points In the XOR problem, Blum [10] found a line of stationary points by adding constraints to as leading to a weight-sharing MLP of five weights: [ L; w; L in [10]. Using four XOR data: ( x hidden-node outputs: h are independent of input data. Then, for a given tar get value  X  X f f X  (e.g., 0.1), set so that those tar get values  X  X f f X  and  X  ON  X  must approximate XOR . Blum X  s Claim (page 539 [10]): Ther e are many stationary points that are not absolute minima. in the ( w; L ) -plane . Actually , these points are local minima of E , being 1 all zeros; hence, S = O . Consequently , no matter how (see Theorem 5) is changed to update w must hold for the Hessian argument to work.
 The 5-by-5 Hessian matrix r 2 E at a stationary point = [ L; w; 0 ; 0 ; 0] is given by We thus obtain two non-zero eigen values of r 2 E , No w, the smaller eigen value can be rendered negative when the follo wing condition holds: ON = 2 (2) off 1 : 6616 by Eq.(17). Because r 2 E is indefinite, the posed stationary point is a saddle point with E = 1 singular saddle point. 3.3 Two-class patter n classification problems of Gori and Tesi We next consider two two-class pattern classification problems made by Gori &amp; Tesi: one with in [18 ]). We shall sho w a descending negative curv ature direction.
 with m = 5 is given by E = 1 Fig.3, the solution to the linear system belo w yields p = [ p The resulting point [ p be given by the two dotted lines with sol [0 ; 1 ; 1; 0 : 5 ; 1 ; 1; 0 : 5 ; 1 ; 1] T . negative curv ature is ortho gonal to r E , the steepest-descent direction. Claim : Line search from init to sol monotonically decr eases the squared error E ( init + ) as the step size (scalar) changes from 0 to 1; hence, no plateau .
 a property that ( x ) = 1 ( x ) : including some problems where no method was developed to alle viate plateaus. In simulation, we exploit negative curv ature. This approach is suitable for up to medium-scale problems, for which to matrix-free Krylo v subspace methods: Among them, the truncated conjug ate-gradient (Krylo v-dogle g) method tends to pick up an arbitrary negative curv ature (hence, slo wing down learning; interest such as a Lanczos type [21 ] and a parameterized eigen value approach [22 ]. Ackno wledgments The work is partially supported by the National Science Council, Taiw an (NSC-99-2221-E-011-097).
