 User generated social annotations provide extra information for describing document contents. In this paper, we pro-pose an effective method to model the categorization prop-erty of social annotations and explore the potential of com-bining it with classical language models for improving re-trieval performance. Specifically, a novel TR-LDA model is presented to take annotations as an additional source for generating document contents apart from the document it-self. We provide strategies for representing and weighting the categorization property and develop an efficient infer-ence algorithm, where space saving is taken into account. Experiments are carried out on synthetic datasets, where documents and queries come from the standard evaluation conference TREC and annotations come from the website Delicious.com. Our results demonstrate the effectiveness of theproposedmethodonthead-hocretrievaltask,which significantly outperforms state-of-art baselines. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Design, Experimentation Social Annotations, Categorization, Topic Models, Language Models
Social bookmarking has become one of the most impor-tant and popular activities for online users to manage and share resources. Typical systems like Delicious 1 and Bibson-omy 2 allow people to annotate documents with freely chosen terms, called tags. The tagging process for a document can be seen as somewhat a classification of the document into categories represented by the annotations and this is also referred to as categorization property of annotations[22].
A lot of studies have been conducted to analyze social an-notations, e.g., their motivations, types, quantities and sta-tistical characteristics[11, 3, 5, 10], and all these works serve for almost the same purpose, that is to apply this resource in real web search applications. However, there are limited approaches proposed in direct use of annotations[25, 22, 2, 13] and most existing works model tags as additional key-words[19, 25]. How to utilize the categorization property to improve search effectiveness is an interesting research ques-tion that remains unexplored.

Modeling annotations as categories are prominent in the following aspects which previous methods cannot handle well: (1) Many tags are composed of multiple terms, such as  X  X arpal-tunnel-syndrome X  or  X  X omputer storage X . Tak-ing them as additional words would reduce their descriptive power for documents regardless of tokenization. (2) Many tags are from personal vocabulary but are meaningful on describing topic connections of documents. Taking them as words would not generate such an effect. (3) The tags on the Web are often multilingual and it is easy to treat syn-onyms from tags and words as multiple terms. The weights of these tags on topic estimation would be further reduced if they are used as words.

To leverage the categorization property, the following is-sues need to be addressed: category representation, category weighting and search-oriented category utilization. In this paper, we take all the above three aspects into consideration. First we propose a new topic model called TR-LDA based on the categorization property which combines the advantages of global text analysis and local categorization knowledge in-ferred from annotations. The learned topic model can then http://www.delicious.com http://www.bibsonomy.org be used to smooth the classical language model to improve IR performance. In our proposed TR-LDA model, category representation is an independent component and category weights can be specified according to their relative accura-cies on describing the document. We explore two methods for the representation according to whether users are con-sidered as one element in category representation and their performances are further compared. We conclude that an-notations working as categories provide stronger constraints for the learned topic model than working as keywords for which lots of tags are needed.

Our contributions in this paper lie in the following points. (1) We bring up a new method for modeling documents based on the categorization property of annotations and ex-plore the impact of incorporating this factor on retrieval performance. (2)We compare several strategies to represent and weight annotation categories. (3)We develop an effec-tive evaluation scheme as well as construct several evaluation datasets for the general social tag based retrieval task.
The rest of the paper is organized as follows: Section 2 reviews the existing work related to this paper. Section 3 presents our proposed TR-LDA model and its implementa-tion issues. Section 4 discusses the TR-LDA based retrieval method in more detail. Section 5 introduces the evaluation process and presents experimental results. Finally, Section 6 concludes and discusses possible directions for future work.
Social annotations have been utilized for estimating web page importance[13, 2] as well as estimating document mod-els[19, 25]. These two aspects are the major concerns of web search and we target the document modeling aspect in this paper. On modeling annotations, most existing works em-phasize the keyword property, where tags are taken as obser-vations at the same level as words. Specifically, [25] used one common topic specific distribution for words and tags while [19] demonstrated that independent modeling of word and tag distributions is desired, but both methods need a com-parative number of tags to guarantee their performances.
Besides, methods on extracting one kind of data given its alternative counterpart have also been proposed, such as extracting content-related annotations[14] and annotation-related contents[18]. However, the common strategy used in the above works is that the topic distribution of extracted data is limited within the scope of the given data. This may not be suitable for the general IR task, because in this context, the goal is to find the best topic model to describe the document collection without any prior knowledge about which data source is more desirable; trusting one source only may be biased without fully exploring the information con-tained in the other.

Another related work concerns language models[17], for which smoothing is critical and has been well studied[24, 15, 21, 12]. Recent researches show that sophisticated topic models may provide no additional gains over the basic LDA model for smoothing[23]. We believe this is because using only the statistical information from document collections may not be sufficient and additional domain knowledge is needed to train a more accurate model.

Wikipedia categories have also been explored for ad hoc retrieval[14], where they are taken as references to align queries and documents. However, it is difficult to apply the same method to annotation categories because their quality is much lower than Wikipedia categories.
This section starts with the introduction of proposed method for modeling documents by leveraging categorization prop-erty of annotations. Then we describe an inference frame-work for learning the model parameters. Finally, we discuss the implementation issues involving parameter settings and parameter updates.
Since tags are mostly accurate topic descriptors, it is nat-ural to consider tags as potential categories. Under this interpretation, a document with certain tags is assumed to be consistent with the topic(s) embodied by the tag term(s), and naturally tags should lie in a higher level compared to the traditional use of taking them at the word level. But note that the same tag term from different users may have different topic granularities and thus categories may be bet-ter represented by raw annotations, which are defined as &lt; user,tag &gt; tuples. The two kinds of category representa-tions with or without user involvement are discussed in Sec-tion 3.3 and hereafter, we simply use annotation categories to refer to them.
 Formally, we use D to represent the document collection. For each document d  X  D , suppose C d is the annotation categories of the document d , then the union of C d forms the global category set C of document collection D .Note that a proportion of categories in C would appear in multi-ple documents as used for the purpose of organization. We use D c to represent the set of documents associated with a certain annotation category c .

To reflect the constraint effect of categories, we take a certain proportion of word observations as generated from the annotation source. Specifically, suppose this propor-tion is 1  X   X  (  X   X  1) and if document d has N d words, then (1  X   X  ) N d words are generated by its annotation categories, and the remaining  X N d are generated from the document itself. In this process, each word variable is in fact replaced with two random variables representing its generative pat-terns from the document source and annotation source re-spectively. The two variables having their appearances of  X  and 1  X   X  are independent though they share the same word form. Similarly, we can further model the generation from the annotation source using | C d | category specific ran-dom variables and define the appearance proportion of each as  X  dc . Note that the observation unit is now continuous rather than discrete as adopted in traditional LDAs and the generated random variables are assumed to be exchangeable as in the  X  X ag-of-words X  assumption[1]. This generative pro-cess is further illustrated in Figure 1 and Table 1.
As we can see, the proposed TR-LDA model maintains a global category component, which makes the documents with the same annotations have partial topic consistence since they may share the same  X  c .

In the TR-LDA model,  X  and  X  are parameters to be spec-ified beforehand. These two parameters control how much we trust the social annotation source and its different an-notation categories. The details of setting up them are dis-cussed in Section 3.3. All other parameters can be inferred from the document collection. You may also easily observe Figure 1: Graphic representation of TR-LDA Model For each annotation category c  X  C : Generate  X  c  X  Dirichlet(  X |  X  c ) For each document d  X  D Generate  X  d  X  Dirichlet(  X |  X  d ) For each word index n  X  1 ,...,N d : that LDA can be seen as a special case of TR-LDA, where  X  =1.
Given the parameters  X  = {  X  c , X  d , X  d , X , X  } , the likeli-hood of a word w appearing in a document d is then the product of two source related word likelihoods, P ( w )= P ( w | d )  X  P ( w | C d ) 1  X   X  = P ( w | d )  X  follows: where
As for TR-LDA, we would like to know the posterior dis-tribution of P (  X  d , X  c ,z | C, D,  X ) and the parameter  X  .The former is used to estimate the document topic(s), while the latter is to estimate the topic-specific word distribution.
A variety of algorithms have been proposed to estimate the parameters in topic models. The two most common methods are Gibbs sampling [9] and variational inference [4]. Here we develop a variational inference method to ap-proximate the posterior distribution of latent variables. The approximate distribution used in our algorithm is given as follows: q (  X  d , X  c ,z |  X ) = where  X  = {  X  c , X  d , X  dn | c  X  C, d  X  D, dn  X  1 ,...,N d variational parameters. Specifically,  X  d and  X  c are Dirichlet parameters and  X  dn are multinomial parameters. Using the variational EM algorithm, we get the following parameter updates
E-step
M-step
In this paper, we use fixed values for  X  c and  X  d for simplic-ity, though they can also be estimated by Newton-Raphson algorithm.  X  is a parameter which controls how likely the observed word is sampled from the document source. A smaller  X  (which means bigger 1  X   X  ),indicatesthatwehavemoretruston the annotation source and the learned topic representation would be prone to reflect the underlying annotation cate-gories. In our experimental setting,  X  is closer to 1 rather than to 0 with the assumption that the statistical informa-tion contained in document is more important.  X  d controls the proportion of words sampling from an-notation categories for document d . We assume  X  dc ,the proportion sampling from a certain category c , is consistent with its importance to the document d .

According to previous analysis, the categories can be rep-resented by only tags after aggregating users out or by raw annotations . For the first representation, two heuris-tic methods are developed to compute  X  d accordingtodiffer-ent assumptions. Specifically, we first compute the weights for each tag and then use L1 norm to generate its corre-sponding proportion. The tag weights can be calculated using (1) the TF method. By aggregating annotations with the same tag term, we can calculate the term frequency of each tag assigned to the document. (2) the TF-IDF method. This method punishes the common tag terms appearing in multiple documents and emphasizes rare tag term(s) for a document. For the second representation(i.e. with user in-formation), no prior knowledge on the relative importance of raw annotations is assumed, therefore uniform distribution is employed for  X  d . It is found that using tags as categories and TF based weighting scheme is the most effective through our experiments.
In the above inference algorithm, at E-step, the estimation for variational parameters is done through cycling over all theparametersandreplacingeachinturnwithanupdated estimate by using the current estimates of other related pa-rameters. Obviously, the associated parameters have to be kept in memory so as to update each other. In TR-LDA, each document update needs the parameters of its annota-tion categories and each category update needs all its as-sociated document parameters(shown in Eq.(2) and Eq.(3) respectively). In fact, annotations and documents consti-tute a bipartite graph. The maximum memory needed de-pends on the size of the maximum subgraph. Suppose the maximum subgraph contains D s documents and C s anno-tation categories, the space required for inference is at least the maximum subgraph could often consist of tens of thou-sands of nodes, therefore space saving is an immediate issue which needs to be addressed.

A basic idea to control the memory consumption is to remove unnecessary edges from the graph. Here, we adopt a principled graph cut approach to solve this problem. Graph cut is a method which can find the best graph partition with the least cut cost based on the graph structure[16, 8]. We use it here to split document-annotation subgraphs into smaller and balanced pieces according to the memory constraint. One can see that the edges removed would not participate in estimating related document and category topics, but we believe the negative impact is limited. Specifically, each edge is assigned the same weight, and we use the technique of normalized cut[20] to split the graph.

Besides, for those documents without annotations, we set  X  = 1 to update  X  dni in Eq.(1).
The basic language model(LM) uses query likelihood to score documents. Formally, for a query Q ,thescoreofa document d is given as follows: where q is a query term in Q and P ( q | d ) is the estimated document model from the document observation. Obviously, the estimation of document model is the key element for LM based retrieval and it is demonstrated that some sort of smoothing is a must[21]. LDA based smoothing methods use the Eq.(6) to estimate P ( w | d ).
 P ( w | d )=  X  N d
P
P ml ( w | d )and P ( w | coll ) are the maximum likelihood es-timates of word w in the document and in the entire col-lection respectively.  X  is the Dirichlet prior while  X  controls the preference for exact term matching and inexact semantic matching. P tp ( w | d ) is the component estimated from topic models such as LDA and PLSI, etc.
 In our proposed method, we use TR-LDA to calculate the P tp ( w | d ) component and the corresponding formula is shown in Eq.(8). Specifically, posterior distributions for  X  d and z are estimated based on the TR-LDA model. The rest of the retrieval process is the same as described before. The semantic matching provided by TR-LDA is assumed to be better because human categorization knowledge is incorpo-rated in addition to the unsupervised analysis purely based on statistical information such as LDA.

To evaluate the performance of tag based retrieval meth-ods, the dataset should consist of two components: a docu-ment collection and its related annotations, and a query set and its relevance judgements. Here we resort to the standard evaluation conference TREC 3 and try to build test collec-tions based on its tasks. We select the ad-hoc retrieval tasks of Web Track 2009[7] and Web Track 2010[6] as bases to construct our datasets. It is mainly because TREC datasets are carefully constructed and the used document collection ClueWeb09 dataset 4 was crawled from January to Febru-ary in 2009, whose documents could be found in current bookmarking systems such as Delicious. Particularly, the ClueWeb09 dataset contains two types, the complete  X  X at-egory A X  dataset and the smaller  X  X ategory B X  dataset.
Our construction process is composed of the following se-quential steps: 1. The documents appearing in the relevance judgements of  X  X ategory B X  dataset were taken as seeds to crawl re-lated annotations. Specifically, the seeds can be divided into two portions: the relevant document set D ( rel )andthe non-relevant document set D ( nonrel ). Their corresponding annotation sets crawled are A ( rel )and A ( nonrel ). Each annotation is in the form of a &lt; user,tag &gt; tuple. 2. The annotations in A ( rel )and A ( nonrel )werefur-ther submitted to the Delicious website to fetch web pages, which form two expanded document collections DE ( rel )and DE ( nonrel ). New annotations of DE ( rel )+ DE ( nonrel ) were added to A ( rel )+ A ( nonrel ). 3. By intersecting DE ( rel )and DE ( nonrel ) with the doc-ument set of  X  X ategory B X , we got two new document sets DC ( rel )and DC ( nonrel ). The annotations of those doc-uments not in DC ( rel )+ DC ( nonrel ) were deleted from A ( rel )+ A ( nonrel ).
 Now we define the dataset type R as follows:
Note that the documents in DC ( rel ) are likely to be rel-evant due to the fact that they share the same annotations with D ( rel ). Since no human judgements are available for DC ( rel ), we removed this portion from R . DC ( nonrel )may http://trec.nist.gov/ http://boston.lti.cs.cmu.edu/Data/clueweb09/
Table 2: Statistics about Web Track adhoc tasks. also contain relevant documents, but the possibility would be much lower and therefore we assume that all its docu-ments are not relevant.

Specifically, two evaluation datasets are constructed and used in the experiments which are derived from Web Track 2009 and Web Track 2010. The statistics about Web Tracks and the constructed datasets are shown in Table 2 and Table 3 respectively. For the relevance judgements provided by Web Track 2010, only those with documents appearing in  X  X ategory B X  dataset were included, resulting in 48 queries for evaluation.
In this section, we examine the performance of TR-LDA based retrieval method(TBDM) to answer the following two questions: (1) Can annotations improve search effectiveness? (2) Does the TR-LDA model have advantage over other tag based models for IR?
For answering the first question, we implemented the fol-lowing baselines: (1) Language model with Dirichlet prior(LM); (2) Language model using LDA smoothing (LBDM) [21]. In fact, LBDM is a strong baseline which is currently the best document smoothing method based on our knowledge[23, 21]. For answering the second question, we implemented three baselines. One is the language model using LDA smooth-ing (LBDM-T), where each document is represented by the set of its words and associated tags at inference. The other two baselines are WT-LDA and WT-QDAU+ from[25] which is also the most related work to ours. 09 R is our training collection to estimate parameters and 10 R is used for testing whether the optimized parameters are consistent in other collections. The queries and rele-vance judgements in ad-hoc retrieval tasks are used and doc-uments not appearing in the relevance judgements are taken as non-relevant at evaluation. The performance of each run is evaluated based on its top 100 documents instead of tradi-tional 1000 documents. All the experiments were done with lemur toolkit 5 . Documents are indexed as trecweb type and stemmed using Porter stemmer. Queries are taken from the  X  X uery X  field of the initial topic release. The performance is measured using mean average precision(MAP). In fact, for comparison, the whole experiment part follows the paradigm of [21], including the selection of parameters and the usage of training set and testing set.
For the retrieval methods mentioned above, each has a topic component which needs to be estimated before com-bining with the language model. We primarily examine the http://www.lemurproject.org performances of LBDM and TBDM to study the impact of topic model estimation on IR. Other methods show similar properties as LBDM .

We use tags as categories and the TF based strategy for  X  d according to previous conclusions. This leads to a new document-tag bipartite graph. Considering the memory lim-itation, we select 20 for 09 R and 50 for 10 R as the Ncut par-tition number respectively. This makes around 7,000 doc-uments have to be kept in memory at inference and the proportion of documents with tags is 68%for 09 R and 63% for 10 R after the partition. To estimate topic models, we use the most frequent 100,000 word terms in the collection and hyper parameters  X  d ,  X  c and  X  prior are set to 1.
There are mainly four factors that determine the actual performance of the proposed TR-LDA method for retrieval: (1) The iteration number. Generally, more iterations mean that the learned model tends to have stable predictive abil-ity, however, for the retrieval task, better topic representa-tion may be generated in the process of iterations, not in the final converged model. We found that the retrieval perfor-mance based on 5 iterations is generally good enough and the later iterations may even harm the performance. (2) The local maximum problem. We ran 10 times for a given topic number with various random initializations for TR-LDA and LDA respectively. The average MAP across 10 runs was used as the performance measure for later com-parison. (3) The number of topics. The number of topics deter-mines the granularity of the learned topics. To find an op-timal topic number K ,wevaryitfrom10to80with10 as the step. Both TBDM and LBDM reach their optimal performances at K = 20. (4) The  X  parameter.  X  controls the preference of infor-mation source for generating document words. We set it from 0.6 to 1.0 with 0.05 as step. The optimal performance is achieved at  X  =0 . 85.
Parameters  X  and  X  are selected through exhaustive search on 09 R .  X  =0 . 9 is the best value for both TBDM and LBDM. The optimal  X  is a little bit different, that is  X  = 1750 for TBDM and  X  = 1500 for LBDM. The retrieval results on 09 R are presented in Table 4. Statistically significant improvements are found for TBDM over LBDM and QL at multiple recall levels. With the trained parameter setting from 09 R , we examined the performances of these methods on 10 R . As shown from Table 5, TBDM still outperforms LBDM and QL significantly. This demonstrates that social annotations could bring benefits for web search.
To further compare with other tag based retrieval meth-ods, we implemented LBDM-T, WT-LDA and WT-QDAU+, where the topic estimations were done in a similar way as LBDM. Their parameters were also tuned at 09 R .Table6 shows that TBDM outperforms WT-LDA and WT-QDAU+ significantly, which demonstrates the benefit of modeling an-notations as categories.
In this paper, we explore the use of the categorization property of social annotations for modeling document con-tents. We have proposed a new topic model TR-LDA, where annotations and documents determine the latent topic dis-Table 4: Comparison of QL, LBDM and TBDM in the 09 R dataset.  X  indicates a statistically sig-nificant improvement with a one-tailed Wilcoxon signed-rank test( p&lt; 0 . 05 ).
 Rel. 4002 4002 4002 Rel. 1844 1844
Retr. 1877.9 +1.79 +1.79 0.00 0.5800 0.6371 0.6380 +10 . 00  X  +0.15 0.10 0.4631 0.4972 0.5125 +10 . 67  X  +3 . 09  X  0.20 0.4406 0.4630 0.4803 +9 . 00  X  +3 . 72  X  0.30 0.4156 0.4206 0.4401 +5.90 +4 . 64  X  0.40 0.3447 0.3406 0.3558 +3.23 +4 . 47  X  0.50 0.1888 0.2224 0.2243 +18 . 80  X  +0.82 0.60 0.1050 0.1384 0.1577 +50 . 10  X  +13 . 91  X  0.70 0.0788 0.0715 0.0760 -3.51 +6.29 0.80 0.0370 0.0343 0.0387 +4.78 +12.98 0.90 0.0138 0.0178 0.0198 +42 . 80  X  +10.88 1.00 0.0115 0.0120 0.0119 +3.36 -0.83
Avg 0.2134 0.2312 0.2414 +13 . 12  X  +4 . 41  X  tribution together. The proposed model is demonstrated effective in the ad hoc retrieval task.

Future directions include the exploration of other alterna-tive approximation methods to identify a better annotation-document partition. We are also interested in the perfor-mance of applying the proposed TR-LDA to document col-lections with much fewer annotations.
This work is supported by the Major State Basic Re-search Project of China under Grant No . 2007CB311103 and the National Science Foundation of China under Grant No. 61070111 and 60873166. Table 5: Comparison of QL, LBDM and TBDM in the 10 R dataset.  X  indicates a statistically sig-nificant improvement with a one-tailed Wilcoxon signed-rank test( p&lt; 0 . 05 ).

Retr. 1333.4 +1.94 +2.29 0.00 0.4357 0.4627 0.4751 +9.04 +2.68 0.10 0.3720 0.3677 0.3884 +4.04 +5 . 63  X  0.20 0.3316 0.3149 0.3284 -0.96 +4 . 29  X  0.30 0.2927 0.2839 0.2890 -1.26 +1.80 0.40 0.2287 0.2393 0.2452 +7 . 21  X  +2.47 0.50 0.1625 0.1763 0.1843 +13 . 41  X  +4.54 0.60 0.1434 0.1307 0.1412 +1.53 +8.03 0.70 0.0695 0.0657 0.0708 +1.87 +7.76 0.80 0.0237 0.0250 0.0289 +21 . 94  X  +15 . 6  X  0.90 0.0000 0.0009 0.0027  X   X  1.00 0.0000 0.0002 0.0000  X   X  Avg 0.1623 0.1626 0.1716 +5 . 73  X  +5 . 53  X  Table 6: Comparison of LBDM, LBDM-T, WT-LDA, WT-QDAU+ and TBDM.
 09R 0.2312 0.2332 0.2092 0.2079 0.2414 10R 0.1626 0.1621 0.1538 0.1578 0.1716
