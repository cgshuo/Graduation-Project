 Morphosyntactic labels for words are commonly used in a variety of NLP applications. For this rea-son, part-of-speech (POS) tagging and supertagging have drawn significant attention from the commu-nity. Combinatory Categorial Grammar is a lexical-ized grammar formalism that is widely used for syn-tactic and semantic parsing. Supertagging (Clark, 2002; Bangalore and Joshi, 2010) assigns complex syntactic labels to words to enable fast and accurate parsing. The disambiguation of correctly labeling a word with one of over 1,200 CCG labels is dif-ficult compared to choosing on of the 45 POS la-bels in the Penn Treebank (Marcus et al., 1993). In addition to the large label space of CCG supertags, labeling a word correctly depends on knowledge of syntactic phenomena arbitrarily far in the sentence (Hockenmaier and Steedman, 2007). This is be-cause supertags encode highly specific syntactic in-formation (e.g. types and locations of arguments) about a word X  X  usage in a sentence.
 In this paper, we show that Bidirectional Long Short-Term Memory recurrent neural networks (bi X  LSTMs) (Graves, 2013; Zaremba et al., 2014), which can use information from the entire sentence, are a natural and powerful architecture for CCG su-pertagging. In addition to the bi X  X STM, we create a simple yet novel model that outperforms the pre-vious state-of-the-art RNN model that uses hand-crafted features (Xu et al., 2015) by 1.5%. Con-current to this work (Lewis et al., 2016) introduced a different training methodology for bi-LSTM for supertagging. We provide a detailed analysis of the quality of various LSTM architectures, forward, backward, and bi-directional, shedding light over the ability of the bi X  X STM to exploit rich sentential con-text necessary for performing supertagging. We also show that a baseline feed-forward neural network (NN) architecture significantly outperforms previ-ous feed-forward NN baselines, with slightly fewer features, achieving better accuracy than the RNN model from (Xu et al., 2015).

Recently, bi X  X STMs have achieved high accu-racies in a simpler sequence labeling task: part-of-speech tagging (Wang et al., 2015; Ling et al., 2015) on the Penn treebank, with small improve-ments over local models. However, we achieve strong accuracies compared to (Wang et al., 2015) using feed-forward neural network model trained on local context, showing that this task does not require bi X  X STMs. Our strong feed-forward NN baselines show the power of feed-forward NNs for some tasks.
Our main contributions are the introduction of a new bi X  X STM model for CCG supertagging that achieves state-of-the-art, on both CCG supertagging and parsing, and a detailed analysis of our results, including a comparison of bi X  X STMs and simpler feed forward NN models for supertagging and POS tagging, which suggests that the added complexity of bi X  X STMs may not be necessary for POS tagging, where local contexts suffice to a much greater extent than in supertagging. We use feed-forward neural network models and bidirectional LSTM (bi X  X STM) based models in this work. 2.1 Feed-Forward For both POS tagging and our baseline supertagging model, we use feed-forward neural networks with two hidden layers of rectified linear units (Nair and Hinton, 2010). For supertagging, we use a slightly smaller set than Lewis and Steedman (2014a), us-ing a left and right 3-word window with suffix and capitalization features for the center word. However, unlike them, we train on the full set of supertag cat-egories observed during training.

In POS tagging, when tagging word w i , we con-sider only features from a window of five words, with w i at the center. For each w j with i  X  2  X  j  X  i + 2 , we add w j lowercased and a string that encodes the basic  X  X ord shape X  of w j . This is com-puted by replacing all sequences of uppercase letters with A , all sequences of lowercase letters with a , all sequences of digits with 9 , and all sequences of other characters with  X  . Finally, we add two and three let-ter suffixes and two letter prefix for w i only. 2.2 LSTM models We experiment with two kinds of bi X  X STM models. We train a basic bi X  X STM where the forward and backward LSTMs take input words w i and produce hidden state duce  X  h i , where where  X  ( x ) = max(0 ,x ) is a rectifier nonlinear-ity, and where W  X  X  X  learned. The unnormalized likelihood of an output supertag is computed using supertag embeddings D t i and biases b t i as p ( t i | final softmax layer computes normalized supertag probabilities.

Although bidirectional LSTMs can capture long distance interactions between words, each output la-bel is predicted independently. To explicitly model supertag interactions, our next model combines two models, the bi X  X STM and a LSTM language model (LM) over the supertags (Figure 1). At position i , the LM accepts an input supertag t i  X  1 produc-ing hidden state h LM i , and a second combiner layer, parametrized by matrices W LM and W  X   X  h i and h LM i to h i similar to the combiner for (Equation 1). Output supertag probabilities are com-puted just as before, replacing replacing  X  h i with h i We refer to this model as bi X  X STM X  X M. For all our LSTM models, we only use words as input features. 2.3 Training We train our models to maximize the log-likelihood of the data with minibatch gradient ascent. Gradi-ents of the models are computed with backpropa-gation (Chauvin and Rumelhart, 1995). Since gold supertags are available during training time and not while decoding, a bi X  X STM X  X M trained on gold su-pertags might not recover from errors caused by us-ing incorrectly predicted supertags. This results in the bi X  X STM X  X M slightly underperforming the bi X  LSTM (we refer to training with gold supertags as g X  X rain in Table 1). To bridge this gap between train-ing and testing we also experiment with a sampling training regime in addition to training.

Scheduled sampling : Following (Bengio et al., 2015; Ranzato et al., 2015), for each output token, with some probability p , we use the most likely pre-dicted supertag ( arg max t model in position i  X  1 as input to the supertag LSTM LM in position i and use the gold supertag with probability 1  X  p . We denote this training as ss X  train X 1. We also experiment with using the 5 -best previous predicted supertags from the output distri-bution at position i  X  1 and feed them to the LM as input in position i as a bit vector. Additionally, we use their probabilities (re-normalized over the 5 -best tags) and scale the input supertag embeddings with their re-normalized probability during look-up. We refer to this setting as ss X  X rain X 5. In this work, we use an inverse sigmoid schedule to compute p , where s is the epoch number and k is a hyperpa-the development set training with scheduled sam-pling improves the perplexity of the gold supertag sequence when using predicted supertags, indicat-ing better recovery from conditioning on erroneous supertags. For both ss-train and g-train, we use gold supertags for the output layer and train the model to 2.4 Architectures Our feed-forward models use 2048 rectifier units in the first hidden layer, 50 and 128 rectifier units in the second hidden layer for POS tagging and Supertag-ging respectively, and 64 dim. input embeddings. Our LSTM based models use 512 hidden states. We pre-train our word embeddings with a 7 -gram feed-forward neural language model using the pus (Charniak et al., 2000) and WSJ sections 02 X 21 of the Penn Treebank. 2.5 Decoding We perform greedy decoding. For each position i , we select the most probable supertag from the output distribution. For the bi X  X STM X  X M models trained with g X  X rain and ss X  X rain X 1, we feed the most likely supertag from the output distribution as LM input in the next position. We decode with beam search (size 12) for bi X  X STM X  X Ms trained with g X  X rain and ss X  X rain X 1. For the bi X  X STM X  X Ms trained with ss X  X rain X 5, we perform greedy decoding similar to training, feeding the k -best supertags from the out-put supertag distribution in position i  X  1 as input to the LM in position i , along with the renormal-ized probabilities. We don X  X  perform beam decoding for ss X  X rain X 5, as the previous k -best inputs already For supertagging, experiments were run with the standard splits of CCGbank. Unlike previous work no features were extracted for the LSTM models and rare categories were not thresholded. Words were lowercased and digits replaced with @.

CCGbank X  X  training section contains 1,284 lexi-cal categories (394 in Dev). The distribution of cate-gories has a long tail, with only a third of those cate-Ling et al. (2015) Bi-LSTM 97.36 Wang et al. (2015) Bi-LSTM 97.78 S X gaard (2011) SCNN 97.50 This work Feed-Forward 97.40 gories having a frequency count  X  10 (the threshold used by existing literature). Following (Lewis and Steedman, 2014b), we allow the model to predict all categories for a word, not just those with which the word was observed to co-occur in the training data. Accuracies on these unseen (word, cat) pairs are pre-sented in the third column of Table 1. Table 3 presents our Feed-Forward POS tagging re-sults. We achieve 97.28% on the development set and 97.4% on test. Although slightly below state-of-the-art, we approach existing work with bi X  X STMs,
Table 1 shows a steady increase in performance as the model is provided additional context. The for-ward and backward models are presented with infor-mation that may be arbitrarily far away in the sen-tence, but only in a specific direction. This yields weaker results than the Feed Forward model which can see in both directions within a small window. The real gains are achieved by the Bidirectional LSTM which incorporates knowledge from the en-tire sentence. Our addition of a language model and changes to training, further improve the perfor-
Wenduan et al. (2015) 86.25 87.04 + new POS Tags &amp; C&amp;C 86.99 87.50 bi X  X STM X  X M +ss X  X rain X 1 87.75 88.32 mance. Our final model (bi X  X STM X  X M+ss X  X rain X 1 model with beam decoding) has a test accuracy of 94.5%, 1.5% above state-of-the-art. 4.1 Parsing Our primary goal in this paper was to demonstrate how a bi X  X STM captures new and different in-formation from uni-directional or feed-forward ap-proaches. This advantage also translates to gains in parsing. Table 4 presents new state-of-the-art parsing results for both (Xu et al., 2015) and our bi X  X STM X  X M +ss X  X rain X 1. These results were at-tained using our part-of-speech tags (Table 3) and the Java implementation (Clark et al., 2015) of the 4.2 Error Analysis Our analysis indicates that the information follow-ing a word is more informative than what preceded it. Table 2 compares how well our models recover common and syntactically interesting supertags. In particular, the Forward and Backward models, moti-vate the need for a Bi-directional approach.
The first two rows show prepositional phrase at-tachment decisions (noun and verb attaching cate-gories are in rows one and two, respectively). Here the forward model outperforms the backward model, presumably because knowing the word to be modi-fied and the preposition, is more important than ob-serving the object of the prepositional phrase (the information available to the backward model).
Conversely, the backward model outperforms the forward model in most of the remaining categories. (Di-)transitive verbs (lines 4 &amp; 5) require knowledge of future arguments in the sentence (e.g. separated by a relative clause). Because English has strict SVO word-order, the presence of a subject is more pre-dictable than the presence of an (in-)direct object. It is therefore not surprising that the backward model is often comparable to the Feed Forward model.
If the information missing from either the forward or backward models were local, the bidirectional model should perform the same as the Feed-Forward model, instead it surpasses it, often by a large mar-gin. This implies there is long range information necessary for choosing a supertag.
 Embeddings In addition, we can visualize the in-formation captured by our models by investigating a category X  X  nearest neighbors based on the learned embeddings. Table 5 shows nearest neighbor cate-ward, Backward, and Bidirectional models.

We see see that the forward model learns inter-nal structure with the query category, but the list of arguments is nearly random. In contrast, the back-ward model clusters categories primarily based on the final argument, perhaps sharing similarities in the subject argument only because of the predictable SVO nature of English text. However, due to its lack of forward context the model incorrectly asso-ciates categories with less-common first arguments (e.g. S [ qem ] ). Finally, the bidirectional embeddings appear to cleanly capture the strengths of both the forward and backward models.
 Consistency and Internal Structure Because su-pertags are highly structured their co-occurence in a sentence must be permitted by the combinators of CCG. Without encoding this explicitly, the lan-guage model dramatically increases the percent of predicted sequences that result in a valid parse by up to 15% (last column of Table 2).
 Sparsity One consideration of our approach is that we do not threshold rare categories or use any tag dictionaries; our models are presented with the full space of CCG categories, despite the long tail. This did not did not hurt performance and the models learned to successfully use several categories which were outside the set of traditionally-thresholded fre-quent categories. Additionally, the total number of categories used correctly at least once by the bi-directional models was substantially higher than the other models (  X  270 vs.  X  220 of 394), though the large number of unused categories (  X  120) indicates that there is still substantial room for improvement. Because bi X  X STMs with a language model encode an entire sentence at decision time, we demonstrated large gains in supertagging and parsing. Future work will investigate improving performance on rare cat-egories.
 This work was supported by the U.S. DARPA LORELEI Program No. HR0011-15-C-0115. We would like to thank Wenduan Xu for his help.
