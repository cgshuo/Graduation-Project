 This paper proposes an algorithm called Imprecise Spectrum Analysis (ISA) to carry out fast dimension reduction for doc-ument classification. ISA is designed based on the one-sided Jacobi method for Singular Value Decomposition (SVD). To speedup dimension reduction, it simplifies the orthog-onalization process of Jacobi computation and introduces a new mapping formula for transforming original DOCument-term vectors. To improve classification accuracy using ISA, a feature selection method is further developed to make inter-class feature vectors more orthogonal in building the initial weighted term-document matrix. Our experimental results show that ISA is extremely fast in handling large term-document matrices and delivers better or competitive classification accuracy compared to SVD-based LSI. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Algorithms, Experimentation, Performance Feature Selection, LSI, SVD, Dimension Reduction
Latent Semantic Indexing (LSI) with SVD is an effective dimension reduction method for document classification and other information analysis tasks. The computational over-head of SVD is known to be a bottleneck in dealing with large data sets. Even there are various advancements of the computational methods for SVD (e.g. [12, 7, 10, 1, 5]), SVD computation is still very expensive for larger matrices [11].
In this paper, we propose a fast dimension reduction al-gorithm called imprecise spectrum analysis (ISA) for docu-ment classification with training data. There are three key optimization strategies in ISA to speedup computation while sustaining a good accuracy.

First, we start with the one-sided Jacobi method which is more accurate numerically than the QR method [4]. Given a weighted term-matrix matrix H , the most time consuming part of the Jacobi method is an orthogonalization process ex-pressed as HV = B where V contains right-singular vectors and B is further decomposed as B = U  X where U contains left-singular vectors and  X  is a diagonal matrix containing singular values of H . We simplify the orthogonalization pro-cess with an approximation.

Second, to minimize the negative impact of imprecise com-putation, we consider a weighting design in constructing the initial matrix H . We make column vectors of H more or-thogonal by utilizing terms X  global scores and class-oriented characteristics.

Finally, we use a fast mapping formula in transforming original feature vectors into a reduced space. Given an orig-inal document-term vector d , the new dimension-reduced vector d lsi is computed as d T H . We show this formula has the functional equivalence to d lsi = d T U  X  and has com-petitive or better accuracy compared to the other existing methods.
A document-term vector d can be transformed into a low dimension vector d lsi in an LSI space with the following standard mapping formula [3]: d lsi = d T U  X   X  1 where d is the original document vector d m  X  1 , U is a column-orthogonal matrix m  X  r ,  X  is a diagonal matrix r  X  r ,and d lsi is the target pseudo-document which has a much lower dimension k (  X  r ) than the original space H m  X  n . Some work [10, 9] uses another mapping formula: d lsi = d T U.

The main cost of LSI in those mapping models is deriving the matrix U and  X  in decomposing the matrix H .Among those computing SVD methods, one-sided Jacobi method has the form HV = B for a given matrix H ( m  X  n )of rank r . Namely the Jacobi method starts from matrix H and performs a sequence of sweeps which essentially post-multiplies H with V and yields matrix B whose columns are orthogonal. The above iterative computation of the Jacobi method performs the following sequence of sweeps to make columns in H orthogonal. Each sweep uses the following plane rotation: where ( H k ) i and ( H k +1 ) i are the i -th column in H before and after the k -th plane rotation, and  X  is a rotation an-gle that is designed to produce two orthogonal columns [6]. When all pair vectors are orthogonal under a precision value, by normalizing columns of B by the norm of vectors, we can get B = U  X andthen H = U  X  V T .

It is very time-consuming to perform the above orthogo-nalization computation in the Jacobi method. For exam-ple, for a sparse document-term matrix H from the 20-newsgroup corpus, we found that before the first sweep, there were only 5% non-orthogonal pair vectors while this sparse matrix has 5% of non-zero elements. Then many sweeps of the Jacobi method are spent with a plane rota-tion on getting a 100%-orthogonal matrix B ,andtheentire process is slow. This uneconomic practice encouraged us to challenge the fundamental problem: can LSI for feature se-lection perform well when we relax the rigorously orthogonal constraint on matrix B ?
An analysis on the LSI space [10] shows that feature vec-tors of those two documents from different topic classes should be nearly orthogonal. In this way, LSI does a partic-ularly good job of classifying documents. Our approach is to relax the orthogonal constraint of B in the above one-sided Jacobi algorithm. Meanwhile, we make those pairs of vec-tors from different classes to be orthogonal as much as pos-sible through feature scoring by extending a scoring formula called CFC [8] in constructing the initial matrix H .With this in mind, we can consider key pairs of vectors in H are near orthogonal, and thus we remove the time-consuming matrix rotation process in the Jacobi computation. Then the one-sided Jacobi computation can be simplified as the following steps:
To carry out dimension reduction which uses U k and di-agonal matrix  X , we can select top k norms and take them as top k scalar values in the diagonal matrix  X . and use cor-responding normalized vectors as the matrix U k .Intherest of this section, we present our weighting formula for initial matrix construction and an error upper-bound when using top k vectors to approximate. We will also present our de-sign for mapping document vectors into the reduced space with analytic results that justify the choice of mapping.
We construct an initial matrix H in a way such that two document vectors in different topic classes have an orthogo-nal trend. To do this, we extend CFC score [8] by exploiting summarized class information extracted from training data.
The terms used in this paper is defined in Table 1. Our scoring formula to construct a term-document matrix H is called CFC-E, where weight h i,j for term t i and document j in matrix H is defined as:
After construction of the initial matrix H ,weusethis matrix H as the target matrix B . Then ISA uses the top k vectors with the maximum norm values from the normal-ized and sorted matrix H . This essentially means that we approximate the original matrix H with H k which contains these top k vectors. We can derive the bound of errors raised from this approximation. Given an m  X  n matrix H ,therel-ative error ratio of top k vectors to approximate H can be given as where || H || F is the the Frobenius norm of H . Using top k vectors is consistent with the strategy used in SVD-based matrix approximation. The more important a vector in matrix U or V is, the bigger the corresponding  X  in matrix  X  will be and thus top k singular values with their corresponding singular vectors are selected in SVD-based matrix approximation.
In general, we model the mapping formula as With F ( X ) =  X   X  1 , it is the traditional formula d d
U  X   X  1 . For mapping original vectors, the selected vector u i is multiplied by a weight factor of 1  X  i .With F ( X ) = I , we have d lsi = d T U . We find from our experiments that the accuracy of mapping model with F ( X ) =  X   X  1 is not as good as the one with F ( X ) = I when the dimension of pseudo vectors k is large. This is because the singular value ( X ) becomes small for a large k value and inverse  X   X  1 be-comes too large in the LAS2 algorithm [2], which leads to a big numerical error.

From the above experimental finding, we infer that it is reasonable to let the mapping formula multiply weights to important vectors u i proportionally according to values of  X  . Thus we propose a mapping formula with F ( X ) =  X , namely, d lsi = d T U  X . This gives each selected vector u positive weight  X  i in the mapping process, which is consis-tent to the heuristic that the more important vector u i is, the bigger weight  X  i is applied. Our experiments presented in 4.1 verify that classification performance of mapping with F ( X ) =  X  is close to that with F ( X ) = I .

We will show that d lsi = d T U  X  has the same effect as d lsi = d T H for replacing the part d i d T j of Equation 4 shown below in SVM-based classification. This means that we can choose d lsi = d T H to map vectors with an advantage that H can be used directly without any expensive computation other than a normalization process.

We analyze the functional equivalence of the above map-ping formulas for classification tasks as follows. Given an SVM-based classifier, for a dual Wolfe optimization prob-lem L D : where y i is the label of the vector d i and  X  i is the La-grange multipliers, if dimension reduction is carried out in LSI space, the part d i d T j will be replaced by ( d lsi we use the mapping formula d lsi = d T U  X , the part d i d Equation 4 can be substituted by d T i U  X ( U  X ) T d j . In the sin-gular value decomposition H = U  X  V T for an initial matrix H , we have the transformation
We can get HH T = U  X ( U  X ) T because V T V = I . Conse-quently, the part d i d T j can be replaced in d T i U  X ( U  X ) d
HH T d j . We get an equivalent mapping formula d lsi = d
H from the mapping formula d lsi = d T U  X .
Table 2 lists different algorithms and approaches com-pared in our experiments and our evaluation has the fol-lowing objectives:
To assess the performance of different mapping formu-las in classifying tasks, we use an SVM classifier to carry out classifying tasks, and use standard macro-averaging F1 (MacroF1) and micro-averaging F1 (MicroF1) as the accu-racy metrics to evaluate the performance. As for SVD com-putation, we use LAS2 algorithm in the latest SVDLIBC library 1 . Three datasets used for experiments are summa-rized below.

WebKB. WebKB 2 contains seven categories and 8,203 pages (7,031 training and 1,172 testing). For the X  X itle X , X  X 1 X , and  X  X RL X  parts in the page, we give the weight 5 times more than those in  X  X ody X . We kept 28,473 unigram terms that occurred at least once in the training set. 20-newsgroup. This dataset from 20 Usenet newsgroups 3 consists of 19,899 messages (13,272 training and 6,627 test-ing) and 31,138 unigram terms. We only keep  X  X ubject X ,  X  X eywords X , and  X  X ontent X  while words in  X  X ubject X  and  X  X eywords X  are given the weight 5 times more than those in  X  X ontents X .

Reuters-21578. This dataset 4 contains 6,495 training texts, 2,557 testing texts and 11,430 unique unigram terms in this 52-category corpus. Words in titles are given the weight 5 times more than those in abstracts. http://tedlab.mit.edu/  X  dr/SVDLIBC/ http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data http://kdd.ics.uci.edu/databases/20newsgroups http://ronaldo.cs.tcd.ie/esslli07/sw/step01.tgz
For above three corpora, we used the tokenizer tool pro-vided in the Trinity College sample. Stemming and word clustering were not applied. Entropy score for initial matrix H are extracted from the whole corpus. Original document vectors are constructed with TF-IDF score.
 Parameter Settings. The SVMTorch package 5 and SV M multiclass package 6 are used in classifying tasks. The two SVM classifiers can perform multi-class tasks directly with one-vs -others decomposition and default parameter val-ues. Experiments were performed on a machine with Intel Core2 Duo 2.8 GHz CPU, 4 GB memory.
On the spectrum of pseudo-document X  X  dimension, the precision in classifying tasks is used to describe the per-formance of different mapping formulas. We construct H with e  X  cf in Reuters-21578, e in WebKB and tf  X  e in 20-newsgroup respectively because different scores for H can il-lustrate the stable performance of SVD1 more convincingly. Figure 1: Performance of three mapping formulas for dimension reduction in Reuters and WebKB.

Fig. 1 shows that SVD and SVD1 have produced a com-parable performance while they outperform SVD-1 signifi-cantly especially for a larger dimension size. For MicroF1, the difference between SVD and SVD1 is relatively small, and is getting bigger with larger dimension size. In WebKB, after dimension is bigger than 2000, SVD1 exhibits more stable performance while SVD-1 X  X  performance drops. For MacroF1, SVD is slightly better than SVD1, and SVD1 is sometime slightly better, especially for a larger dimension. Overall speaking, SVD1 X  X  performance is close to SVD and they perform better than SVD-1 for a higher dimension.
Fig. 2 shows that ISA can become competitive to SVD1 after dimension 5,000 in Reuters. If we use the full size of H , Fig. 2 shows that ISA can perform as well as SVD1 on high dimensions. ISA X  X  performance is lower on low dimensions because training data is not sufficient, while SVD1 uses full training matrix and performs better.

Table 3 lists the results of comparison for the Reuters dataset. For example, we select the top k=1000 vectors in matrix H according to the norm of those vectors, then, LAS2 is used to carry out singular value decomposition for H 1000 ISA uses H 1000 fully in its mapping formula d lsi = d T H http://www.idiap.ch/  X  bengio/projects/SVMTorch.html http://svmlight.joachims.org/svm multiclass.html Figure 2: SVD1 vs. ISA in Reuters-21578 and 20-newsgroup.
 Table 3: Performance comparison between SVD1 and ISA in Reuters-21578 after SVD is applied to top k vectors.
 k=1000 0.8868 0.7162 0.8858 0.7164 k=2000 0.9116 0.7651 0.9116 0.7653 k=3000 0.9214 0.7858 0.9221 0.7866 k=4000 0.9245 0.7996 0.9245 0.7996 k=5000 0.9280 0.8028 0.9280 0.8001 k=6000 0.9292 0.8016 0.9292 0.8010 Table 3 shows that ISA can still perform as well as SVD1 on all dimensions when SVD also only uses the same set of top vectors.
For LSI-based classification, two key steps are dimension reduction computation and training using SVM. For LAS2, Table 4 illustrates the cost of a few choices of targeted di-mensions and processing time is extremely slow for larger dimensions. ISA only needs to sort the initial matrix H after normalization, so ISA took less than one second to perform matrix decomposition for three corpora.
 Table 4: Elapsing seconds of LAS2 and ISA for ma-trix computation with different targeted dimensions.
In Table 5, for each sample dimension, the training pro-cess had been carry out 10 times with SV M multiclass in 20-newsgroup. We extracted the smallest time-cost for LAS2 while the biggest time-cost for ISA. Table 5 shows that SVD X  X  vectors with dimension 3,000 need 9.77 seconds in training while ISA X  X  results with dimension 12,000 need only 5.22 seconds. The pseudo-document vectors mapped using SVD are dense while the pseudo-document vectors produced by ISA are sparse. For example, over 92% of elements in transformed vectors are zero in ISA for 20-newsgroup and the SVM code has taken this advantage.

Three tested benchmarks show that our approach can per-form extremely fast dimension reduction while have an ac-curacy competitive to SVD-based LSI in document classifi-cation. ISA X  X  complexity is proportional to the number of non-zero elements in H . Though ISA has a bigger suitable dimension (  X  0.6n in the tested cases), vectors transformed by ISA are highly sparse and a classification learning al-gorithm such as SVM can exploit sparse computation to achieve low training time. Our future work is to study ISA for other applications.
 This work is support in part by NSFC 60811130528. We thank anonymous referees for their valuable comments. [1] N. Ailon and B. Chazelle. Faster dimension reduction. [2] M.W. Berry. Multiprocessor sparse SVD algorithms [3] S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. [4] James Demmel and Kresimir Veselic. Jacobi X  X  method [5] Zlatko Drma X  candKre X  simir Veseli  X  c. New fast and [6] G.R. Gao and S.J. Thomas. An optimal parallel [7] Jing Gao and Jun Zhang. Sparsification strategies in [8] H. Guan, J. Zhou, and M. Guo. A [9] H. Kim, P. Howland, and H. Park. Dimension [10] C.H. Papadimitriou, H. Tamaki, P. Raghavan, and [11] Xiaoguang Qi and Brian D. Davison. Web page [12] Llyod N. Trefethen and David Bau, III. Numerical
