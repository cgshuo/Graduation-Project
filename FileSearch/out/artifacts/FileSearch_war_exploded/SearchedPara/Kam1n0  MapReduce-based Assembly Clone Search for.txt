 Assembly code analysis is one of the critical processes for de-tecting and proving software plagiarism and software patent infringements when the source code is unavailable. It is also a common practice to discover exploits and vulnerabilities in existing software. However, it is a manually intensive and time-consuming process even for experienced reverse engi-neers. An effective and efficient assembly code clone search engine can greatly reduce the effort of this process, since it can identify the cloned parts that have been previously analyzed. The assembly code clone search problem belongs to the field of software engineering. However, it strongly depends on practical nearest neighbor search techniques in data mining and databases. By closely collaborating with reverse engineers and Defence Research and Development Canada ( DRDC ), we study the concerns and challenges that make existing assembly code clone approaches not practi-cally applicable from the perspective of data mining. We propose a new variant of LSH scheme and incorporate it with graph matching to address these challenges. We implement an integrated assembly clone search engine called Kam1n0 . It is the first clone search engine that can efficiently identify the given query assembly function X  X  subgraph clones from a large assembly code repository. Kam1n0 is built upon the Apache Spark computation framework and Cassandra-like key-value distributed storage. A deployed demo system is publicly available. 1 Extensive experimental results suggest that Kam1n0 is accurate, efficient, and scalable for handling large volume of assembly code.
 Assembly clone search; Information retrieval; Mining soft-ware repositories
Code reuse is a common but uncontrolled issue in software engineering [15]. Mockus [25] found that more than 50% of files were reused in more than one open source project. Sojer X  X  survey [29] indicates that more than 50% of the de-velopers modify the components before reusing them. This massively uncontrolled reuse of source code does not only introduce legal issues such as GNU General Public License (GPL) violation [36, 17]. It also implies security concerns, as the source code and the vulnerabilities are uncontrollably shared between projects [4].

Identifying all these infringements and vulnerabilities re-quires intensive effort from reverse engineers. However, the learning curve to master reverse engineering is much steeper than for programming [4]. Reverse engineering is a time consuming process which involves inspecting the execution flow of the program in assembly code and determining the functionalities of the components. Given the fact that code reuse is prevalent in software development, there is a press-ing need to develop an efficient and effective assembly clone search engine for reverse engineers. Previous clone search approaches only focus on the search accuracy. However, designing a practically useful clone search engine is a non-trivial task which involves multiple factors to be considered. By closely collaborating with reverse engineers and Defence Research and Development Canada ( DRDC ), we outline the deployment challenges and requirements as follows:
Interpretability and usability: An assembly function can be represented as a control flow graph consisting of con-nected basic blocks. Given an assembly function as query, all of the previous assembly code clone search approaches [7, 6, 18, 26] only provide the top-listed candidate assembly func-tions. They are useful when there exists a function in the repository that shares a high degree of similarity with the query. However, due to the unpredictable effects of differ-ent compilers, compiler optimization, and obfuscation tech-niques, given an unknown function, it is less probable to have a very similar function in the repository. Returning a list of clones with a low degree of similarity values is not useful. As per our discussions with DRDC, a practical search en-gine should be able to decompose the given query assembly function to different known subgraph clones which can help reverse engineers better understand the function X  X  composi-tion. We define a subgraph clone as one of its subgraphs that can be found in the other function. Refer to the example in Figure 1. The previous clone search approaches cannot address this challenge. Figure 1: An example of the clone search problem. Basic blocks with a white background form a sub-graph clone between two functions. Three types of code clones are considered in this paper. Type I: literally identical; Type II: syntactically equivalent; and Type III: minor modifications.

Efficiency and Scalability: An efficient engine can help reverse engineers retrieve clone search results on-the-fly when they are conducting an analysis. Instant feedback tells the reverse engineer the composition of a given function that is under investigation. Scalability is a critical factor as the number of assembly functions in the repository needs to scale up to millions. The degradation of search performance against the repository size has to be considered. Previous approaches in [6, 7], which trade efficiency and scalability for better accuracy, have a high latency for queries and are thus not practically applicable.

Incremental updates: The clone search engine should support incremental updates to the repository, without re-indexing existing assembly functions. [7] requires median statistics to index each vector and [26] requires data-dependent settings for its index, so they do not satisfy this requirement.
Clone search quality: Practically, clones among assem-bly functions are one-to-many mappings, i.e., a function has multiple cloned functions with different degrees of similar-ity. However, all previous approaches [6, 7, 18, 26] assume that clones are one-to-one mappings in the experiment. This is due to the difficulty of acquiring such a one-to-many la-beled dataset. Moreover, they use different evaluation met-rics. Therefore, it is difficult to have a direct comparison among them with respect to the search quality. We need to develop a one-to-many labeled dataset and an unified eval-uation framework to quantify the clone search quality.
To address the above requirements and challenges, we pro-pose a new variant of LSH scheme and incorporate it with a graph matching technique. We also develop and deploy a new assembly clone search engine called Kam1n0 .Our major contributions can be summarized as follows:
The remainder of this paper is organized as follows. Sec-tion 2 situates our study within the literature of three differ-ent research problems. Section 3 formally defines the studied problem. Section 4 provides an overview of our solution and system design. Section 5 presents the preprocessing steps and the chosen vector space. Section 6 introduces our pro-posed locality sensitive hashing scheme. Section 7 presents our graph search algorithm. Section 8 presents our bench-mark experiments. Section 9 provides the conclusion.
Locality sensitive hashing. Locality sensitive hash-ing ( LSH ) has been studied for decades to solve the -approximated Nearest Neighbor ( NN) problem, since exact nearest neighbor is not scalable to high dimensional data. One of the prevalent problems of LSH is the uneven data dis-tribution issue , as LSH equally partitions the data space. To mitigate this issue, several approaches have been proposed including the LSH-Forest [2], LSB-Forest [32], C2LSH [10] and SK-LSH [22]. It has been shown that the cosine vector space is robust to different compiler settings [18] in assembly code clone search. However, LSH-Forest, C2LSH and SK-LSH are designed for the p -stable distribution, which does not fit the cosine space. LSB-Forest dynamically and un-equally partition the data space. As pointed out by [33], it requires the hash family to possess the ( , f ( )) property. However, to our best knowledge, such a family in cosine space is still unknown. There are other learning-based ap-proaches [11], which do not meet our incremental require-ment. Wang et al. [35] provide a more comprehensive sur-vey on LSH. To satisfy our requirements, we propose the ALSH scheme specifically for the cosine space. Different to the LSH-Forest, ALSH takes more than one bit when going down the tree structure and does not require the ( , f ( )) property for the LSH family to have theoretical guarantee. Unlike LSB forest [2], we dynamically construct the buckets to adapt to different data distributions.

Subgraph isomorphism. Ullmann [34] proposed the first practical subgraph isomorphism algorithm for small graphs. Several approaches were proposed afterwards for large scale graph data, such as TurboISO [13] and STwig [31]. It has been shown that they can solve the subgraph iso-morphism problem in a reasonable time. However, they do not completely meet our problem settings. The subgraph isomorphism problem needs to retrieve subgraphs that are isomorphic to the graphs in the repository and identical to the query. However, we need to retrieve subgraphs that are isomorphic to the graphs in the repository and isomorphic to the graph of the query, which significantly increases the complexity. More details will be discussed in Section 7. Such a difference requires us to propose a specialized search algo-rithm. Lee et al. [21] provide a comprehensive survey and performance benchmark on subgraph isomorphism.

Assembly code clone search. The studies on the as-sembly code clone search problem are recent. Only a few approaches exist [6, 7, 18, 26]. They all rely on the inexact text search techniques of data mining. BinClone [7] models assembly code into an Euclidean space based on frequency values of selected features. It is inefficient and not scal-able due to the exponential 2-combination of features which approximates the 2-norm distance. LSH-S in [26] models assembly code into a cosine space based on token frequency and approximate the distance by hyperplane hashing and banding scheme. It equally partitions the space and suffers from the uneven data distribution problem. Graphlet [18] models assembly code into a cosine space based on extracted signatures from assembly code. However, it cannot detect any subgraph clones that is smaller than the graphlet size. Tracelet [6] models assembly code according to string edit-ing distance. It compares function one by one, which is not scalable. Kam1n0 is fundamentally different to the previous approaches. It is an integration of inexact assembly code Figure 2: The overall solution stack of the Kam1n0 engine. search and the subgraph search. It enables clone subgraph search of any size.
Reverse engineering starts from a binary file. After being unpacked and disassembled, it becomes a list of assembly functions. In this paper, function represents an assembly function; block represents a basic block; source function rep-resents the actual function written in source code, such as C++; repository function stands for the assembly function that is indexed inside the repository; target function denotes the assembly function that is given as a query; and corre-spondingly, repository blocks and target blocks refer to their respective basic blocks. Each function f is represented as a control flow graph denoted by ( B,E ), where B indicates its basic blocks and E , indicates the edges that connect the blocks. Let B (RP) be the complete set of basic blocks in the repository and F (RP) be the complete set of functions in the repository. Given an assembly function, our goal is to search all its subgraph clones inside the repository RP. We formally define the search problem as follows: Definition 1. ( Assembly function subgraph clone search ) Given a target function f t and its control flow graph ( B the search problem is to retrieve all the repository func-tions f s  X  RP, which share at least one subgraph clone with f t . The shared list of subgraph clones between f s f is denoted by sg s [1 : a ], where sg s [ a ] represents one of them. A subgraph clone is a set of basic block clone pairs sg s [ a ]= { b t ,b s ,... } between f s and f t , where b t b  X  B s ,and b t ,b s is a type I, type II, or type III clone (see Figure 1). Formally, given f t , the problem is to retrieve all { f s | f s  X  RP and | sg s | &gt; 0 } .
The Kam1n0 engine is designed for general key-value stor-age and the Apache Spark 4 computational framework. Its solution stack, as shown in Figure 2, consists of three layers. The data storage layer is concerned with how the data is stored and indexed. The distributed/local execution layer manages and executes the jobs submitted by the Kam1n0 engine. The Kam1n0 engine splits a search query into mul-tiple jobs and coordinates their execution flow. It also pro-vides the RESTful APIs. We have implemented a web-based user interface and an Hex-Rays IDA Pro plugin 5 as clients. IDA Pro is a popular interactive disassembler that is used by reverse engineers.

Figure 3 depicts the data flow of the clone search pro-cess. It consists of the following steps. Preprocessing :After parsing the input (either a binary file or assembly functions) into control flow graphs, this step normalizes assembly code into a general form, which will be elaborated in Section 5. Find basic blocks clone pairs: Given a list of assembly basic blocks from the previous step, it finds all the clone pairs of blocks using ALSH. Search the subgraph clones: Given the list of clone block pairs, the MapReduce module merges and constructs the subgraph clones. Note that this clone search process does not require any source code.
We choose the cosine vector space to characterize the se-mantic similarity of assembly code. It has been shown that the cosine vector space is robust to different compiler set-tings [18]. It can mitigate the linear transformation of as-sembly code. For example, to optimize the program for speed, the compiler may unroll and flatten a loop struc-ture in assembly code by repeating the code inside the loop multiple times. In this case, the cosine similarity between the unrolled and original loop is still high, due to the fact that the cosine distance only considers the included angle be-tween two vectors. The features selected in Kam1n0 include mnemonics, combinations of mnemonics and operands, as well as mnemonics n -gram, which are typically used in as-sembly code analysis [7, 26]. The equivalent assembly code fragments can be represented in different forms. To miti-gate this issue, we normalize the operands in assembly code during the preprocessing. We extend the normalization tree used in BinClone [7] with more types. There are three nor-malization levels: root , type ,and specific . Each of them corresponds to a different generalization level of assembly code. More details can be found in our technical report. 6
In this section, we introduce an Adaptive Locality Sensi-tive Hashing ( ALSH ) scheme for searching the block-level semantic clones. As discussed in Section 2, exact nearest neighbor search is not scalable. Thus, we started from the reduction of the -approximated k -NN problem:
Definition 2. ( -approximated NN search problem )Given a dataset D  X  R d ( R denotes real numbers) and a query point q ,let r denote the distance between the query point q and its nearest neighbor o  X  . This problem is to find an approximated data point within the distance  X  r where &gt; 1
The -approximated k -NN search problem can be reduced to the NN problem by finding the k data points where each is an -approximated point of the exact k -NN of q [10]. The locality sensitive hashing approaches do not solve the NN problem directly. NN is further reduced into another prob-lem: ( , r )-approximated ball cover problem [1, 14].
Definition 3. (( , r ) -approximated ball cover problem )Given adataset D  X  R d and a query point q ,let B ( q,r )denote aballwithcenter q and radius r . The query q returns the results as follows: Onecansolvethe( , r ) ball cover problem by using the Locality Sensitive Hashing (LSH) families. A locality sen-sitive hashing family consists of hashing functions that can preserve the distance between points.

Definition 4. ( Locality Sensitive Hashing Family )Givena distance r under a specific metric space, an approximation ratio , and two probabilities p 1 &gt;p 2 , a hash function family H X  X  h : R d  X  U } is ( r, r, p 1 ,p 2)-sensitive such that:
LSH families are available for many metric spaces such as cosine similarity [3], hamming distance [14], Jaccard coef-ficient, and p -stable distributions [5]. Based on our chosen cosine vector space, we adopt the random hyperplane hash [3] family, where sign (  X  ) output the sign of the input. By substituting the random vector a we can obtain different hash functions in the family. The collision probability of two data points o 1 and o 2 on Equation 1 can be formulated as:  X  o 1 ,o 2 is the included angle between o 1 and o 2 . The prob-ability that two vectors have the same projected direction on a random hyperplane is high when their included angle is small.

Theorem 1. The random hyperplane hash function is a ( r, r, 1  X  r/ X , 1  X  r/ X  ) sensitive hashing family.
Proof. According to Definition 4 and Equation 2: p 1 = 1  X  r/ X  and p 2 =1  X  r/ X 
To use the locality sensitive hashing families to solve the ball cover problem, it needs a hashing scheme to meet the quality requirement. The E2LSH approach was originally proposed by [14] and extended by[5]. It concatenates k dif-ferent hash functions [ h 1 ,...,h k ] from a given LSH family into a function g ( o )=( h 1 ( o ) ,...,h k ( o )), and adopts l such functions. The parameters k and l are chosen to ensure the following two properties are satisfied:
Property 1. ( P 1 ): if there exists p  X   X  B ( q,r ), then g g ( q ) from some j =1 ...l .

Property 2. ( P 2 ): the total number of points /  X  B ( q,r ) that collides with q is less than 2 l .
It is proven that if the above two properties hold with constant probability, the algorithm can correctly solve the ( , r )-approximated ball cover problem [14]. For E2LSH, by picking k = log p 2 (1 /n )and l = n  X  where  X  = ln 1 /p 1 Properties 1 and 2 hold with constant probability.
However, the ball cover problem is a strong reduction to the NN problem since it adopts the same radius r for all points. Real-life data cannot always be evenly distributed. Therefore, it is difficult to pick an appropriate r .Wede-note this as the uneven data distribution issue .In[12], a magic r m is adopted heuristically. But as pointed out by [32], such a magic radius may not exist. A weaker re-duction was proposed in [14], where the NN problem is re-duced to multiple ( r, )-NN ball cover problems with varying r = { 1 , 2 , 3 ,... } . The intuition is that points in different density areas can find a suitable r . However, such a reduc-tion requires a large space consumption and longer response time. Other indexing structures have been proposed to solve this issue. Per our discussion in Section 2, existing tech-niques do not meet our requirement. Thus, we customize the LSH-forest approach and propose the ALSH structure.
We found that the limitation of the expanding sequence of r in previous section is too strong. It is unnecessary to exactly follow the sequence r = { 1 , 2 , 3 ,... } ,aslongas r is increasing in a similar manner to r t +1 = r t  X  .Thus,we customize the -approximated NN problem as follows:
Definition 5. ( f ( r ) -approximated NN search problem )Given a dataset D  X  R d and a query point q ,let r denote the dis-tance between the query point q and its nearest neighbor o The problem is to find an approximated data point within the distance f ( r ), where f ( r ) /r &gt; 1
Instead of using a fix approximation ratio, we approxi-mate the search by using a function on r . We issue a differ-ent sequence of expanding r . The expanding sequence of r is formulated as r 0 ,r 1 ,...,r t ,r t +1 ,...,r m , where r Similar to the E2LSH approach, we concatenate multiple hash functions from the random hyperplane hash family H into one. However, we concatenate different number of hash functions for different values of r . This number is de-noted by k t for r t , and the sequence of k is denoted by k ,k 1 ,...,k t ,k t +1 ,...,k m , where k t &gt;k t +1 . Recall that the concatenated function is denoted by g .Consequently, there will be a different function g at position t , which is de-noted by g t . Yet, function g t and function g ( t +1) can share k +1 hash functions. With p m to be specified later, we set the r value at position t as follows: This allows us to have the effect of increasing the r value by decreasing the k value. We calculate the value of k at position t as follows: By getting t t +1 from Equation 3, substituting k t using Equa-tion 4, and substituting p m using Equation 3, we have: By setting c equals to 2, we can get an approximately similar curve of r sequence to the original sequence r t +1 = r t Figure 4: The index structure for the Adaptive Lo-cality Sensitive Hashing (ALSH). There are m +1 levels on this tree. Moving from level t to level t +1 is equivalent of increasing the search radius from r t where equals to 2. Following the aforementioned logic, we construct an Adaptive Locality Sensitive Hashing (ALSH) index in the form of prefix trees.

As shown in Figure 4, the index structure is a prefix tree of the signature values calculated by G = { g m ,g m  X  1 ,...,g Level t corresponds to the position t in the r expanding sequence. By introducing different values of k t , each level represents a different radius r t . Each level denotes a different g function and the g t function is a concatenation of k t hash functions. Moving up from a node at level t to its parent at level t +1 indicates that it requires a shorter matched prefix. The nodes which have the same parent at level t share the same prefix that is generated by g t .

To locate the leaf for a given data point q  X  R d ,ALSH dynamically constructs the hash functions by trying g t  X  G in sequence. The signature of g t can be generated by padding additional hash values to g t +1 since k t = c  X  k Following [14, 32], with l to be specified later, we adopt l such prefix trees as the ALSH X  X  index. Given a query point q , we first locate the corresponding leaves in all prefix trees. With l to be specified later, we collect the first 2 l points from all the leaf buckets. To index a point, we locate its corresponding leaf in each tree and insert it to the leaf bucket. Suppose a leaf is on level t t +1 .Ifthenumberof points in that leaf is more than 2 l , we split all the data points of that leaf into the next level t by using g t . All the trees are dynamically constructed based on the incoming points to be indexed in sequence. Therefore, they can be incrementally maintained. Unlike the learning-based LSH [11], Kam1n0 does not require the whole repository to estimate the hash functions to build the index.

It can be easily proved that g t is a ( r t ,r t +1 ,p m ,p tive hash family and g t can correctly solve the ( r t +1 approximated ball cover problem by setting p c m =1 /n and l = n 1 /c . The proof follows [14]. Details and implemen-tation on key-value data store can be found in our techni-cal report. 6 Another parameter r m controls the starting k value at the root value. It indicates the maximum distance that two points can be considered as valid neighbors. For sparse points far away from each other, they are not consid-ered as neighbors unless their distance is within r m .
For a single ALSH tree, the depth in the worst case is k , and all the leaves are at level 0. In this case, the tree is equivalent to the E2LSH with k = k 0 . Therefore, the Figure 5: The MapReduce-based subgraph clone construction process. space consumption for l = n 1 /c ALSH trees are bounded by O ( dn + n 1+1 /c ), where O ( dn ) is the data points in a dataset and O ( n 1+1 /c ) is the space of indexes for the trees. The query time for a single ALSH prefix tree is bounded by its height. Given the maximum k value k 0 and the minimum k value k m , its depth in the worst case is log c ( k 0 /k Thus, the query time for l = n 1 /c prefix trees is bounded by O ( log c ( k 0 /k m )  X  n 1 /c ). The ALSH index needs to build n prefix trees for the full theoretical quality to be guaranteed. Based on our observation, setting l to1and2isalready sufficient for providing good quality assembly code clones.
Even subgraph isomorphism is NP-hard in theory [21, 28], many algorithms have been proposed to solve it in a reason-able time. Formally, the subgraph isomorphism algorithm solved by most of these systems [13, 21, 31] is defined as:
Definition 6. ( Subgraph Isomorphism Search )Agraphis denoted by a triplet ( V,E,L ) where V represents the set of vertices, E represents the set of edges, and L represents the labels for each vertex. Given a query graph q =( V,E,L )and adatagraph g =( V ,E ,L ) a subgraph isomorphism (also known as embedding ) is an injective function M : V  X  V such that the following conditions hold: (1)  X  u  X  V,L ( u ) L ( M ( u )), (2)  X  ( u i ,u j )  X  E, ( M ( u i ) ,M ( u j L ( u i ,u j )= L ( M ( u i ) ,M ( u j )). The search problem is to find all distinct embeddings of q in g .

The difference between this problem and ours in Defini-tion 1 is two-fold. First, our problem is to retrieve all the subgraph clones of the target function f t  X  X  control flow graph from the repository. In contrast, this problem only needs to retrieve the exact matches of query graph q within g .Re-fer to Conditions 1, 2, and 3 in Definition 6, or the termi-nation condition of the procedure on Line 1 of subroutine SubgraphSearch in [21]. Our problem is more challenging and can be reduced to the problem in Definition 6 by issuing all the subgraphs of f t as queries, which introduces a higher algorithmic complexity. Second, there is no such L data la-bel attribute in our problem, but two types of edges: the control flow graph which links the basic blocks and the se-mantic relationship between basic blocks which is evaluated at the querying phase. Existing algorithms for subgraph iso-Algorithm 1 Mapper morphism are not directly applicable. Assembly code con-trol graphs are sparser than other graph data as there are less number of links between vertices and typically, basic blocks are only linked to each other within the same func-tion. Given such properties, we can efficiently construct the subgraph clones respectively for each repository function f if it has more than one clone blocks in the previous step.
We adopt two functions in the Apache Spark MapRe-duce execution framework, namely the map function and the reduce-by-key function. In our case, the map function transforms the clone pairs generated by ALSH (refer to the data flow in Figure 3) and the reduce-by-key function con-structs subgraph clone respectively for different repository function f s . Figure 5 shows the overview of our subgraph clone search approach.

The signature for the map function (Algorithm 1) is b t ,b  X  f s ,sg s [1 : a ] . Each execution of the map function takes a clone pair b t ,b s produced by ALSH and transforms it to f s ,sg s [1 : a ] , which is a pair of repository function id f and its list of subgraph clones sg s in Definition 1. The map functions are independent to each other.

The outputs of the map functions correspond to the first row in Figure 5. A red circle represents a target basic block b and a green triangle represents a source basic block b t . The link between them indicates that they are a block-to-block clone pair b t ,b s , which is produced in the previous step. A white rectangle represents a list of subgraph clones and the colored rectangle inside it represents a subgraph clone. Algorithm 1 maps each clone pair into a list of sub-graph clones which contains only one subgraph clone. Each subgraph clone is initialized with only one clone pair.
After the map transformation functions, the reduce-by-key function reduces the produced lists of subgraph clones. The reducer merges a pair of lists sg 1 s and sg 2 s into a single one, by considering their subgraph clones X  connectivity. The reduce process is executed for the links from the second row to the last row in Figure 5. Only the lists of subgraph clones with the same f s will be merged. As indicated by the links between the first and second row in Figure 5, only rectangles with the same orange background can be reduced together. Rectangles with other background colors are reduced with their own group.

Algorithm 2 shows the reduce function in details. Given two lists of subgraph clones under the same repository func-tion f s , the reduce function compares their subgraph clones (Lines 1 and 2) and checks if two graphs can be connected (Lines 4 to 13) by referring to the the control flow graph edges E s and E t . If the two subgraph clones can be con-nected by one clone pair, then they can be merged into a single one (Lines 6 and 7). If a subgraph clone from sg 2 cannot be merged into any subgraph clones in sg 2 s ,itwillbe appended to the list sg 1 s (Lines 20 and 21). At the end of the Algorithm 2 Reducer graph search algorithm, we solve the problem in Definition 1. In order to obtain a ranked list of repository functions for f , we calculate the similarity value by checking how much its subgraphs sg s cover the graph of the query f t : sim ( | uniqueEdges( sg s ) | + | uniqueNodes( sg s ) | ) / ( | B
Compared to other join-based or graph-exploration-based search approach, our MapReduce-based search procedure avoids recursive search and is bounded by polynomial com-plexity. Let m s be the number of clone pairs for a target function f t . There are at most O ( m 2 s ) connectivity checks between the clone pairs (no merge can be found) and the map function requires O ( m s )executions. m s corresponds to the number of rectangles in the second row of Figure 5. Refer from the second row to the last one. The reduce func-tion are bounded by O ( m 2 s ) comparisons. m s is bounded by O ( | B t | X | B s | ), which implies that each basic block of f is a clone with all the basic blocks of f s . However, this extreme case rarely happens. Given the nature of assembly functions and search scenarios, m s is sufficiently bounded by O ( max ( | B t | , | B s | )). According to the descriptive statis-tics of our experiment, 99% of them have less than 200 basic blocks.
This section presents comprehensive experimental results for the task of assembly code clone search. First, we ex-Table 1: The assembly code clone dataset summary. plain how to construct a labeled dataset that can be used for benchmarking in future research. Then, we evaluate the effect of assembly code normalization. Although normaliza-tion has been extensively used in previous works, its effects have not been thoroughly studied yet. Next, we present the benchmark results which compares Kam1n0 with state-of-the-art clone search approaches in terms of clone search quality. Finally, we demonstrate the scalability and capac-ity of the Kam1n0 engine by presenting the experimental results from a mini-cluster.
One of the challenging problems for assembly code clone search is the lack of labeled (ground truth) dataset, since the most effective labeled dataset requires intensive manual identification of assembly code clones [7]. To facilitate future studies on assembly clone search, we have developed a tool to systematically generate the assembly function clones based on source code clones. The tool performs four steps: Step 1: Parse all the source code functions from different branches or versions of a project and identify all the function-to-function clones using CCFINDERX [16], which estimates source code similarity based on the sequence of normalized tokens. Step 2: Compile the given branches or versions of a project with an additional debug flag to enable the compiler output debug symbols. Step 3: Link the source code functions to the assembly code functions using the compiler output debug symbols, where such information is available. Step 4: For each pair of source code clone, generate a pair of assembly function clone and transfer the similarity to the new pair.
The intuition is that the source code function-level clones indicate the functional clones between their corresponding assembly code. In [7, 25], the source code and assembly code are manually linked with an injected identifier in the form of variable declarations. However, after the automa-tion of such process, we find that the link rate is very low due to the impact of compiler optimizations. The generated assembly code clone is in fact the combined result of source code patches and compiler optimizations. The source code evolves from version to version, and different versions may have different default compiler settings. Thus, the labeled dataset simulates the real-world assembly clone evolvement. This tool is applicable only if the source code is available.
Refer to Table 1 for some popular open source libraries with different versions. We applied the aforementioned tool on them to generate the labeled dataset for the experiments. There are 63,939 assembly functions which successfully link to the source code functions. The labeled dataset is a list of one-to-multiple assembly function clones with the trans-ferred similarity from their source code clones.

See Figure 6c and Figure 6d. The assembly function basic block count follows a long-tail distribution. Most of them have between 0 and 5 assembly basic blocks, and 99% of them is bounded by 200. We find that this is the typical distribution of assembly function block count. This distri-bution facilitates our graph search because the worst case Figure 6b shows the cosine similarity distribution of each basic block X  X  20 th -nearest neighbor. It reflects variations of density in the vector space and calls for an adaptive LSH. 74% of the source code clones given by CCFINDERX are exact clones (see Figure 6a). However, by applying a strong hash on their assembly code, we find that only 30% of them are exact clones (Type I clones). Thus, the total percentage of inexact clones is 70%  X  74%+26% = 77 . 8% . CCFINDERX classifies tokens in source code into different types before clone detection. If two source code fragments are identified as clones with a low similarity, there is a higher chance that the underlying assembly code is indeed not a clone due to the normalization of the source code. To mitigate this issue, we heuristically set a 0.4 threshold for clones to be included in our dataset. Thus, we have 66.8% of inexact clones. Table 3: Paired t-test on the normalization level.
Assembly code normalization is used in [7, 27]. However, its effects were not formally studied. In this section, we present the results of the statistical test on the effects of the normalization level. Details on normalization can be found in our technical report. 6 Westartbyusingastronghash clone search with different normalization levels on each of the generated datasets. Then, we collect the corresponding precision value to the given normalization level as samples and test the relationship between precision and the chosen normalization level. Normalization can increase the recall, but we want to evaluate the trade-off between the precision and different normalization levels. According to the ANOVA Figure 7: Scalability study. (a): Average Indexing Time vs. Number of Functions in the Repository. (b): Average Query Response Time vs. Number of Functions in the Repository. The red line represents the plotted time and the blue line represents the smoothed polynomial approximation. test ( p&lt; 2 e  X  16 ), the difference of applying normalization or not is statistically significant. Consider Table 3. However, the difference of applying different levels, namely Root , Type , or Specific , is not statistically significant.
In this section, we benchmark twelve assembly code clone search approaches: BinClone [8, 19], Graphlets [17, 18], LSH-S [29], and Tracelet [6]. [18] includes several approaches: mnemonic n -grams (denoted as n -gram), mnemonic n -perms (denoted as n -perm), Graphlets (denoted as Graphlet), Ex-tended Graphlets (denoted as Graphlet-E), Colored Graphlets (denoted as Graphlet-C), Mixed Graphlets (denoted as Mix-Graph), Mixed n -grams/perms (denoted as MixGram), Con-stants, and the Composite of n -grams/perms and Graphlets (denoted as Composite). The idea of using Graphlet orig-inated from [20]. We re-implemented all these approaches under a unified evaluation framework and all parameters were configured according to the papers. We did not in-clude the re-write engine in [6] because it is not scalable.
Several metrics are used in previous research to evaluate the clone search quality, but there is no common agreement on what should be used. Precision, recall and F1 are used in BinClone [7], while [36] maintains that a F2 measure is more appropriate. However, these two values will change as the search similarity threshold value changes. To evaluate the trade-off between recall and precision, we use three typical information retrieval metrics, namely Area Under the Re-ceiver Operating Characteristic Curve ( AUROC ), Area Un-der the Precision-Recall Curve ( AUPR ), and Mean Average Precision at Position 10 ( MAP@10 ). These three metrics flavor different information retrieval scenarios. Therefore, we employ all of them. AUROC and AUPR can test a clas-sifier by issuing different threshold values consecutively [9, 24, 30], while MAP@10 can evaluate the quality of the top-ranked list simulating the real user experience [23].
Table 2 presents the benchmark results. The highest score of each evaluation metric is highlighted for each dataset. Also, the micro-average of all the results for each approach isgiveninthe Avg column. Kam1n0 out-performs the other approaches in almost every case for all evaluation metrics. Kam1n0 also achieves the best averaged AUROC, AUPRC, and MAP@10 scores. The overall performance also suggests that it is the most stable one. Each approach is given a 24-hour time frame to finish the clone search and it is only allowed to use a single thread. Some results for BinClone and Tracelet are empty, which indicate that they are not scalable enough to obtain the search result within the given time frame. Also, we notice that BinClone consumes more memory than the others for building the index, due to its combination of features which enlarges the feature space. We notice that the experimental results are limited within the context of CCFinder. In the future, we will investigate other source code clone detection techniques to generate the ground truth data.
In this section, we evaluate Kam1n0 X  X  scalability on a large repository of assembly functions. We set up a mini-cluster on Google Cloud with four computational nodes. Each of them is a n1-highmem-4 machine with 2 virtual cores and 13 GB of RAM. We only use regular persistent disks rather than solid state drives. Each machine is given 500 GB of disk storage. All the machines run on CentOS . Three machines run the Spark Computation Framework and the Apache Cas-sandra Database and the other runs our Kam1n0 engine. To conduct the experiment, we prepare a large collection of binary files. All these files are either open source li-braries or applications, such as Chromium . In total, there are more than 2,310,000 assembly functions and 27,666,692 basic blocks. Altogether, there are more than 8 GB of as-sembly code. We gradually index this collection of binaries in random order, and query the zlib binary file of version 2.7.0 on Kam1n0 at every 10,000 assembly function index-ing interval. As zlib is a widely used library, it is expected that it has a large number of clones in the repository. We collect the average indexing time for each function to be in-dexed, as well as the average time it takes to respond to a function query. Figure 7 depicts the average indexing and query response time for each function. The two diagrams suggest that Kam1n0 has a good scalability with respect to the repository size. Even as the number of functions in the repository increases from 10,000 to 2,310,000, the impact on the response time is negligible. There is a spike up at 910,000 due to the regular compaction routine in Cassandra, which increases I/O contention in the database.
Through the collaboration with Defence Research and De-velopment Canada (DRDC), we learned that scalability, which was not considered in previous studies, is a critical issue for deploying a successful assembly clone search engine. To ad-dress this, we present the first assembly search engine that combines LSH and subgraph search. Existing off-the-shelf LSH nearest neighbor algorithms and subgraph isomorphism search techniques do not fit our problem setting. Therefore, we propose new variants of LSH scheme and incorporate it with graph search to address the challenges. Experimen-tal results suggest that our proposed MapReduce-based sys-tem, Kam1n0, is accurate, efficient, and scalable. Currently Kam1n0 can only identify clones for x86/amd64 processor. In the future, we will extend it to the other processors and investigate approaches that can find clones between different processors. Kam1n0 provides a practical solution of assem-bly clone search for both DRDC and the reverse engineering community. The contribution is partially reflected by the award received at the 2015 Hex-Rays Plug-In Contest.
The authors would like to thank the reviewers for the thor-ough reviews and valuable comments. This research is sup-ported by Defence Research and Development Canada (con-tract no. W7701-155902/001/QCL) and Canada Research Chairs Program (950-230623). [1] A. Andoni and P. Indyk. Near-optimal hashing [2] M. Bawa, T. Condie, and P. Ganesan. LSH forest: [3] M. Charikar. Similarity estimation techniques from [4] P. Charland, B. C. M. Fung, and M. R. Farhadi. [5] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. [6] Y. David and E. Yahav. Tracelet-based code search in [7] M. R. Farhadi, B. C. M. Fung, P. Charland, and [8] M. R. Farhadi, B. C. M. Fung, Y. B. Fung, [9] T. Fawcett. An introduction to roc analysis. Pattern [10] J. Gan, J. Feng, Q. Fang, and W. Ng.
 [11] J. Gao, H. V. Jagadish, W. Lu, and B. C. Ooi. Dsh: [12] A. Gionis, P. Indyk, and R. Motwani. Similarity [13] W. Han, J. Lee, and J. Lee. Turbo iso : towards [14] S. Har-Peled, P. Indyk, and R. Motwani. Approximate [15] E. Juergens et al. Why and how to control cloning in [16] T. Kamiya, S. Kusumoto, and K. Inoue. Ccfinder: a [17] W. M. Khoo. Decompilation as search. University of [18] W. M. Khoo, A. Mycroft, and R. J. Anderson.
 [19] V. Komsiyski. Binary differencing for media files. 2013. [20] C. Kruegel, E. Kirda, D. Mutz, W. Robertson, and [21] J. Lee, W. Han, R. Kasperovics, and J. Lee. An [22] Y. Liu, J. Cui, Z. Huang, H. Li, and H. T. Shen. [23] C. D. Manning, P. Raghavan, H. Sch  X  utze, et al. [24] C. D. Manning and H. Sch  X  utze. Foundations of [25] A. Mockus. Large-scale code reuse in open source [26] A. Saebjornsen. Detecting Fine-Grained Similarity in [27] A. S X bj X rnsen, J. Willcock, T. Panas, D. J. Quinlan, [28] R. Shamir and D. Tsur. Faster subtree isomorphism. [29] M. Sojer and J. Henkel. Code reuse in open source [30] S. Sonnenburg, G. R  X  atsch, C. Sch  X  afer, and [31] Z. Sun, H. Wang, H. Wang, B. Shao, and J. Li. [32] Y. Tao, K. Yi, C. Sheng, and P. Kalnis. Quality and [33] Y. Tao, K. Yi, C. Sheng, and P. Kalnis. Efficient and [34] J. R. Ullmann. An algorithm for subgraph [35] J. Wang, H. T. Shen, J. Song, and J. Ji. Hashing for [36] H. Welte. Current developments in GPL compliance,
