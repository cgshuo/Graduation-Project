 1. Introduction
Prototype based neural networks are trained to obtain a representative set of points with the same characteristics as the original data set, this is called vector quantization. A classic algorithm for this purpose is k-means ( MacQueen, 1967 ) that reassigns them to the mean of the data points included in the Voronoi region they de fi ne. In this algorithm only the closest prototype is modi fi ed by each data point, so a  X  winner-takes-all philosophy is used.

Neural Gas (NG) is another vector quantization algorithm, presented in Martinetz et al. (1993) , which reaches an optimum distribution of prototypes. The main difference between NG and k-means is the use of a neighbourhood ranking between prototype vectors and each data point. It is a cooperative  X  competitive algorithm which avoids local minima. To improve the convergence speed and the robustness of the algorithm, a batch version is presented in Cottrell et al. (2006) . NG reaches optimum distribu-tions at the cost of having no order between prototypes, thus it does not offer output maps.

Supervised versions of NG have been also developed, specially for classi fi cation. A supervised version based on a combination of NG and Generalized Learning Vectors Quantization ( Sato and Yamada, 1995 )isde fi ned in Hammer et al. (2005) , which updates all the prototypes of the respective class depending on the NG ranking. A different Supervised Neural Gas was developed includ-ing the class labels as additional dimensions and weighting them with a mixing parameter in Hammer et al. (2006) .
 Another important quantization algorithm is the Self-Organizing Map (SOM), also called Kohonen Map ( Kohonen, 2001 ). Instead of using free moving prototypes, its learning rule is based on a prede fi ned lattice and the learning rule depends also on the position of each unit in that lattice and not only on the distance to the data points. This algorithm has suppose a great advance in machine learning because of its many features, mainly the dimensionality reduction and the topological preservation. In data mining, SOM is widely used for visualization purposes. The main tools for visualization are the component planes and the U-matrix. Component planes are a representation of the prede-fi ned lattice with a colour scale for the values of each dimension in the input space. These component planes allow the user to non-linear relationships between different variables. The U-matrix represents the distance between adjacent prototypes in the lattice, so different clusters can be de fi ned using the boundaries de by signi fi cantly higher distances. A detailed work of the use of U-matrix can be found in Ultsch (2003) .

SOM has been used for multiple tasks, and speci fi c modi tions were created for different purposes. In order to create a continuous map based on the lattice, an interpolation method, called  X  Interpolating Self-Organizing Map  X  (I-SOM) was started in G X ppert and Rosenstiel (1993) and developed in different phases until G X ppert and Rosenstiel (1997) , where a soft interpolating method was used to obtain the  X  Continuous Interpolating Self-Organizing Map  X  (CI-SOM) . Both I-SOM and CI-SOM are based on interpolation using the closest prototypes to the winning neuron, i.e. D + 1 neurons were required for a D-dimensional data vector.
These interpolating SOMs are proposed for function approxima-tion specially if these can be reduced to a low dimension map. were developed, like the Temporal Kohonen Map (TKM), the
Recurrent SOM and Recursive SOM. Temporal Kohonen Map, de fi ned in Chappell and Taylor (1993) , uses  X  leaky integrators select the best matching unit (bmu), so the previous state is taken in care to select the new winning neuron. Recurrent SOM, proposed in Varsta et al. (1997) , applies an IIR fi lter to the sequence instead of fi ltering the output. Recursive SOM, described in Voegtlin (2002) , uses both the actual map and a copy of the previous one. So a new data entry is a combination of the input vector and a context vector based on the copy of the map in the previous instant. They all have the advantage of using chunks of temporally ordered data instead of having to create a special data set including ordered fi elds. A more detailed review on temporal extensions can be found in Salhi et al. (2009) .
 e.g. surface reconstruction or optimization algorithms. There are many approaches for estimating the gradient; mathematical lit-erature has different estimation methods based on local estima-tors designed for speci fi c problems ( Meyer et al., 2001; Hazen and Gupta, 2009 ).
 main goal of having better quantization without losing the good properties of having a prede fi ned lattice. This algorithm was presented in Mach X n-Gonz X lez et al. (2010) and an intermediate behaviour between the free moving prototypes in NG and the rigid order of SOM maps was obtained. As it was an interesting tool for quantization and clustering, a supervised version was developed in Crespo Ramos et al. (2011) using a linear model for each prototype in the codebook.
 meaning as gradient components instead of being just numerical tools for estimation. Gradient norms provide variability and stability information of the represented system.
 that the new gradient analysis method is proposed in Section 3 , experimental testing is performed in Section 4 and fi nally the results will be discussed in Section 5 . 2. The SOM  X  NG algorithm hybrid algorithm SOM  X  NG was proposed in Mach X n-Gonz X lez et al. (2010) . The main feature of the algorithm is the neighbour-hood function, which is based on both SOM and NG neighbour-hood functions. The expression of the hybrid neighbourhood function is where v is the data vector and w i is a prototype vector in the input space, h SOM is the SOM-based neighbourhood function and h the NG-based neighbourhood function.
 h SOM  X  v ; w i  X  X  exp  X  s  X  r i n where s  X  r i n ; r i  X  is the distance ranking in the output map and the neighbourhood radius in epoch t . Distance ranking for output map is de fi ned as the number of neurons closer to the best matching unit r i n than the output representation of w i .

The formal de fi nition is s  X  r i ; r i n  X  X jf r j = d  X  r j ; r i n  X  o d  X  r i ; r where d is a distance function between two given points in the lattice, usually the squared Euclidean distance. It is important to notice that this way of measuring distance is different to the usual in SOM. In SOM squared Euclidean distance is used as distance measure, but a distance ranking was needed in order to measure neighbourhood in the same units for both expressions. Using a ranking in a lattice changes the meaning of neighbourhood, instead of measuring a distance around the best matching unit, i.e.  X  s neurons away  X  , it measures the amount of prototypes to be included in the neighbourhood, i.e.  X  s closest neurons  X  of this measure the neighbourhood radius will be narrower than the usual one. Some other minor effects can appear as a con-sequence of this measure, like having a non-symmetric distance matrix because of map limits or having a rougher decrease in the neighbourhood function.
 the following expression: h NG  X  v ; w i  X  X  exp  X  k  X  v where k  X  v ; w i  X  is the position in the distance ranking of prototype w i , starting in 0 for the closest prototype, or best matching unit, and m  X  1 for the furthest one, with m being the number of prototypes in the codebook. Parameter  X  is the topology preserva-tion parameter, which controls the in fl uence of the NG term. hood functions will be expressed as h SOM and h NG , assuming that they are de fi ned for prototype w i and data vector v j , respectively. 2.1. Unsupervised version tization algorithm is proposed in Mach X n-Gonz X lez et al. (2010) .
The energy cost function for this algorithm is the weighted sum of the distance between data points and prototypes. Using the squared Euclidean distance as distance measure, the following expression is obtained:
E  X   X  Newton ' s method, with the expression
 X  w i  X   X  J  X  w i  X  H  X  1  X  w i  X  X  6  X  where J is the Jacobian matrix and H is the Hessian matrix. As h is constant, h NG can be considered locally constant and the training data is de fi ned before training, only variations in the prototype positions are considered, so J and H respectively are
J  X   X  2  X 
H  X  2  X  prototype w i is w i  X  experimental results is included in Mach X n-Gonz X lez et al. (2010) . quantization and topological order. Parameter  X  controls how strong is the topological restriction. Low values of  X  give big in fl uence of the h NG term, giving prototypes freedom to move in the input space, getting close to a NG behaviour. Otherwise high values of  X  reduce the in fl uence of h NG so the result is similar to a pure SOM, specially for values above 80 where the h NG term is negligible. Intermediate values of  X  , i.e. between 20 and 40, keep topological order while giving prototypes some freedom to move. These values offer the best advantage of the hybrid algorithm as prototypes give a good approach to the data distribution keeping the lattice order.

Computational cost of the unsupervised algorithm has linear complexity with the number of data elements, the number of prototypes and the dimension of them.

In Mach X n-Gonz X lez et al. (2010) the algorithm is tested for quantization, dimensionality reduction and clustering, offering competitive results for these features. 2.2. Supervised version
Based on the unsupervised SOM  X  NG algorithm, a supervised version was created in Crespo Ramos et al. (2011) . The supervised version was developed to make a local linear model for every Voronoi region using the following expression: ^ f  X  v  X  X  y i n  X  a i n  X  v  X  w i n  X  X  10  X  where y i n and a i n are the reference value and the estimated gradient of f ( v ) in the position of the best matching unit w respectively.
 The energy cost function to optimize (10) is E  X   X  where f  X  v j  X  is the exact value of the output function f obtained from the data set and ^ f  X  v j  X  is the estimated value using (10) .
Following a process for (11) similar to the one done for (5) for both y i and a i , the new training equations are obtained. For the reference value y i an expression similar to prototype position is obtained: y  X 
However for gradient vector a i same simpli fi cations cannot be done, and the updating rule is  X  a
The result of (12) is the fi nal result of y i for next epoch but (13) is an updating rule, so  X  a i has to be added to the previous value.
In Crespo Ramos et al. (2011) the supervised algorithm is explained and implemented. It was tested using a trigonometric function and a real data set comparing the quantization and estimation capabilities with SOM and NG. The value of parameter  X  has a great in fl uence on the behaviour of the algorithm, specially for tuning quantization and topological order. Estimating quality is less affected by  X  but a good position of prototypes produces a more accurate estimation.

The supervised version of the algorithm has a linear computa-tional cost with the number of data elements and the number of prototypes in the map. 3. Proposed application
The SOM  X  NG algorithm has been proven to be a useful tool for quantization and dimensionality reduction, as well as a competi-tive estimator based on local linear models. Now it will be presented as a data mining tool analysing the gradients and obtaining useful information with them.

Having a set of locally calculated gradients the evolution of the estimated function can be studied from them. A fi rst approach is to represent a gradient norm plane where the norm of the gradient is represented in each point of the lattice. This map represents the variability of the output function in the Voronoi region of the corresponding prototype vector. Higher values represent zones where the output function can be easily changed and low values represent zones where the output value is steadier. The gradient map is useful for studying steadiness of the output variable for the different zones.

Different components of the gradient, i.e. all a i dimensions, can be represented too, creating the gradient component planes. In this map the in fl uence of each variable can be studied and compared to different variables and in different working zones. Gradient component maps can be used to fi nd relevant variables for control techniques, data mining or variable selection.
It is also useful to make a gradient map, representing the input space gradient components a i ; a and a i ; b in positions w respectively, where a and b are the tags for two different variables or dimensions in the data vectors. It allows to detect local tendencies in the input space. If the amount and distance of the prototypes allow a clear image, drawing the borders of the Voronoi regions in the same image is suggested. 3.1. Peaks example
A toy example will be used to demonstrate the new application of the algorithm. This data set is created using three Gaussian curves. They form three peaks and two pits. All the points with j z j o 0 : 1 were removed and pits were arti fi cially moved away to create separated clusters. The data set is drawn in Fig. 1 .
A standard training was done with that data set. Selected data is a set of 11 029 three-dimensional vectors, having x and y as input variables and z as output. The chosen lattice is a 12 9 hexagonal grid and  X  is equal to 1. Component planes of the obtained codebook are in Fig. 2 . The z component is the output variable, and the peaks and pits can be identi fi ed. The left and bottom sides represent the pits, the zone between them repre-sents the small peak and the upper right zone is assigned to the two bigger peaks. In the x and y components it can be seen that the prototypes are not distributed linearly along the input space. The last plane is the U-matrix, it represents distances between the prototypes in the input space. The border between the zones is marked in the map, so the pits are easily located as different zones or clusters.

At this point the algorithm has been used as in previous works, just as a quantization and estimation tool. Clusters are also identi fi ed. Next step is to represent the gradient norm and the gradient components, as it is done in Fig. 3 . prototypes are located in plain zones, so they have low values of gradient norm while the norm in the peaks and pits zone gradient norms are greater. Using this tool is easy to identify plain (steady) zones and hills (unsteady) zones. In the gradient component planes it can be seen that slopes change their sign depending if the positive direction of each variable moves uphill or downhill.
It is also interesting to pay attention to the colour scales because the y component of the gradient vector has higher values than the x component.
 plane with a colour representation of the output value in Fig. 4 .
It can be seen how the gradient fi eld points to the higher value of z , i.e. out of the pits and towards the top of the peaks. 4. Real data application: luminous ef fi cacy of the algorithm, including the old and the new ones. This data set was created following the steps described in Mayhoub and Carter (2011) . In this study the direct luminous ef fi cacy is modelled using geographical and meteorological data. Direct luminous ef fi different locations is modelled calculating K  X  E = I , where E is the direct illuminance measured in lux and I is the direct irradiance measured in W = m 2 . Both variables were obtained in S@tel-Light (2011) . Input variables are Latitude ( Lat ), Longitude ( Lon ), Cloud
Index ( CI ) and Solar Altitude (  X  ). See Table 1 for a complete variable list. Latitude and Longitude are the geographic coordi-nates where measures were done. Cloud index is the fraction of the sky obscured by clouds, observed from a satellite for that location. Cloud index data was obtained from NASA . Solar altitude is the measure of the angle between the sun and the horizon. different input values, solar altitude (  X  ) and cloud index ( CI ).
Plotting Latitude and Longitude is not relevant as it only provides the location of the cities in a map.
Several values of  X  were used to compare the obtained models using model quality measures. The most important measure is the quantization error q e , which represents the average distance for every data point v i to its best matching unit w i n : q  X  1 N  X  N where N is the amount of data points.

Other important measure is the topographic error  X  t , which represents the percentage of data points whose two closest prototypes are not adjacent in the output grid:  X   X  1 N  X  N where u  X  v i  X  is 0 when the two closest prototypes are adjacent in the output map and 1 otherwise. In Fig. 7 both the quantization error and the topographic error are shown for a wide variety of values for  X  .

An intuitive idea of the effect of  X  can be seen comparing the neighbourhood in the input and output spaces. The path repre-sented in Fig. 8 is continuous in the output space. In the input space the prototypes will be placed closer if topographical pre-servation is better. To make a comparison the same path is drawn for different values of  X  in Fig. 9 .

In Fig. 9 planes with high values of  X  , i.e. over  X   X  10 have clear paths which can be easily seen but low values, i.e. below have messy representations where it is dif fi cult to fi nd the next point in the route. Detailed information on the effect of found in Mach X n-Gonz X lez et al. (2010) .

Neighbourhood radius s is also involved during the learning phase. This variable is not constant, as it should decrease in every epoch to generate local models. Exponentially decreasing radii are recommended ( Mach X n-Gonz X lez et al., 2010 ), using the expression s  X  t  X  X  s t 0 where s  X  t  X  is the neighbourhood radius in epoch t , s 0 value and s t max is the fi nal value for the last epoch. Recommended values are s 0  X  m units = 2, where m units is the amount of prototypes in the model and s t max  X  0 : 001. A comparison between different values of s 0 is shown in Fig. 10 . Quantization is close to constant when modifying initial neighbourhood radius but small neigh-bourhoods led to worse topological preservation, so very small values should be avoided even when good topological preserva-tion is not required.

Using the hybrid SOM  X  NG algorithm with  X   X  10, the compo-nent planes of Fig. 11 were obtained. Data was normalized to obtain zero mean and unity standard deviation in every variable. Normalization is necessary to avoid weighting variables because of the units instead of the real relevance. Colour scales keep the normalized values for a better understanding of the process, but component planes are usually denormalized for visualization. Training was done using a 10 8 rectangular lattice with random values for initialization and  X  equal to 10. For location components, i.e. latitude and longitude, the position of prototypes is the position of the measured places in most of the cases. Cloud index has high values for most of them, but there is no clear relationship among them. The CI plane has different tendencies to all the others, as CI is not directly correlated with any other variable. Finally,  X  seems to be the most important one, as the luminous ef fi cacy has a very similar shape to the solar altitude. In the upper left and right sides the values of  X  and K are both low. The higher the  X  value, the higher the output value, but not in a linear relationship, as it seems to reach an upper limit soon.
For this case, the component planes offer very few additional information than just representing the variables in scatter plots.
Local model estimations are going to be compared with those made using the models proposed in Mayhoub and Carter (2011) .
The proposed global models are listed in Table 2 . Errors are measured using the mean of absolute deviations (MAD), the root mean square deviations (RMS) and the mean bias deviations (MBD). They are de fi ned with the following equations:
MAD  X   X  N
RMS  X 
MBD  X   X  N estimated function and N is the total amount of samples.
The values for MAD, RMS and MBD are represented for all the models in Fig. 12 . The SOM  X  NG model has more accurate results for all the measured errors. Accuracy for local models can be adjusted modifying the number of prototypes in the map. In this case there are 80 prototypes, i.e. 80 local models.

After comparing the SOM  X  NG model with analytical models, a different study will be done. The SOM  X  NG algorithm will be compared with two popular neural networks: the multi-layer perceptron (MLP) and the radial basis function neural networks (RBF-NN). To make a fair comparison the number of neurons in the hidden layer is limited to 80, the number of prototypes used in the SOM  X  NG model. MLPs use a 4 n 1 topology, where 4 is the input layer size, same as the number of input variables, n is the amount of neurons in the hidden layer, and 1 is the size of the output layer, same as the number of output variables. In the comparison chart different models are labelled using MLP( n ), i.e. MLP(10) is a 4 10 1 model. RBF models are also limited to 80 points and they need to get a radius parameter. RBF models were created with different radii and labelled as RBF( s ), with sigma being the radius, i.e. RBF(10) is a model with maximum 80 points and radius equal to 10. All the models reached the limit of points. In Fig. 13 a comparison between the different models is shown. MLP and RBF errors are slightly worse to the SOM  X  NG ones. This shows that even these popular estimators with global models are not as accurate as local models.

Gradient norm and gradient component planes are represented in Fig. 14 , values in the colour bar are used to obtain normalized values for K . These values represent the approximate partial derivative of K with respect to that variable. As variables were normalized during data preprocessing the value of each partial derivative is a direct indicator of the in fl uence of each variable, both in positive and in negative. In the norm plane there are three main zones, one for the high values of luminous ef fi cacy and two corresponding to the upper and right sides where output values are lower. This plane means that the luminous ef fi cacy is easily modi fi ed when it has low values, but when the value is high it is not so easy to change it.

In addition to gradient component planes a distribution analy-sis can be done too, usually with histograms or box plots. In Fig. 15 a violin plot, which combines both the histogram and the box plot, is represented. Narrow distributions around zero like in Longitude mean in which the variable can be eliminated from the linear model without a signi fi cant loss of accuracy. Latitude gradients are slightly more distributed, but also with small relevance. Most values of CI gradient are in the range between  X  1 and 0, which means it has mostly a negative in fl uence, i.e. cloudy days have lower luminous ef fi cacy. All those variables have quite uniform gradient maps, so the in fl uence is similar for all the situations. Again the most signi fi cant variable is the solar altitude, which has high values in the low ef fi cacy zones and close to null ones in the high ef fi cacy ones. Violin plots, or just box plots, can help to make an extensive search for relevant variables. In a data set with tens or maybe hundreds of variables the most relevant ones are easily detected because of the gradient distribution. In Fig. 15 solar altitude  X  is clearly identi fi ed as the most relevant variable. A gradient map for location variables is represented in Fig. 16 . Most of the gradient vectors are short in that bi-dimensional plane. There is one exception in 24.8 1 ,37.15 1 where the gradient length is longer than all the others. This vector can be ignored because the Voronoi region for that prototype is very thin in that dimension, so its real effect will be very small even with such a big derivative.

Fig. 17 is a very illustrative one. Gradients in this plane have very different lengths and directions depending on the values of and CI , so their in fl uence varies depending on the state. For low solar altitudes, i.e. in the dawn, the luminous ef fi cacy is low but it is increased easily as the solar altitude grows. In moderate solar altitudes the effect is less important until it reaches steadiness. If CI is taken in care, low solar altitudes are affected negatively and for moderate solar altitudes, CI gets more importance. The main conclusion for Fig. 17 is that luminous ef fi cacy increases quickly as the sun goes up until it reaches a steady value of about 110 lm The solar altitude for that value depends on CI , so the maximum is reached earlier if CI is low.

Using gradient analysis for this data set several outcomes were obtained:
Geographical location has small in fl uence over luminous ef cacy if solar altitude is known.

Solar altitude is the most important variable to estimate luminous ef fi cacy, as it is important to determine the value and the derivative.

Luminous ef fi cacy is variable in some states, i.e. low solar altitude, but is very steady for others.

At this point the luminous ef fi cacy is better understood, and the effect of clouds over it is known.
 maps. Figs. 16 and 17 were done with topic criterion but the importance of a plane can be measured with the length of the gradient vector projection in that plane. To quantify that length a plane relevance parameter  X  i ; j is proposed, but different expres-sions can be used depending on the situation. The following expressions are proposed: where  X  a  X  is the norm of the gradient using all the components and a i and a j are the gradient components for variables i and j respectively. Expression (20) is appropriate for gradient maps with many close-to-zero norms to avoid dividing by small numbers.
Otherwise (21) takes in care the length of the gradient to compare the full norm and the plane projection. In this data set there are many stable zones with small gradients so (21) can give false positives. Table 3 was calculated using (20) . As there is a for each prototype the mean and the standard deviation are calculated.
 standard deviation represents the similarity between that plane and the norm. As  X  i ; j was obtained using subtraction the most relevant plane will be the one with the least value. In this dataset the CI vs.  X  plane is the one with the closest to zero mean value and the smaller deviation. Such a small deviation means that the norm of the gradient is proportional to that plane projection in most cases. Many other heuristics can be de fi ned, using different de algorithm can be used if the system complexity makes an unaffordable exhaustive search, but the computational cost of these heuristics is low after creating the models. 5. Conclusions
In this work a new use for the previously proposed algorithm is presented. The algorithm was proven to be useful for quantization and estimation but it has more capabilities, such as data mining.
Previously presented features are also useful for data mining uses. Quantization and component planes are useful for state de fi nition and obtaining representative points of data. Component planes can fi nd relationships between different variables and distance maps are useful for clustering purposes.

In this paper, gradient analysis is also done using the hybrid algorithm. Representing the gradient norm in a component map identi fi es the steadiness of the different zones. Large gradient norms mean zones where the output function has signi fi cant variations with small input changes and small gradient norms represent steady zones where the output value has small changes when input variables are modi fi ed.

Using gradient component maps the in fl uence of each indivi-dual variable can be studied. Variables can have direct in over the output value or determine the state of the system and in fl uence the importance of other variables. Gradient component maps are useful to determine which variable should be modi to obtain the desired output value with minimal effort. Also, the colour gradients of component maps determine which variables are cooperative, i.e. colour gradients have the same direction, opposite, i.e. colour gradients have opposite directions, or unre-lated, i.e. directions are neither the same nor opposite.
The obtained knowledge allows creating fuzzy models based on local relevancy determining input changes to optimize the output variable variation. Expert knowledge is not needed if using this methodology.

The algorithm can also be used to generate gradient maps without using a speci fi c tool for each fi eld of knowledge. Gradients can be estimated for any kind of data and simple gradient maps can be drawn using the prototype vectors and the gradient vectors.
To conclude, the algorithm gives a lot of information about the in fl uence of variables and it can be extracted with known graphical representations of the obtained models.
 References
