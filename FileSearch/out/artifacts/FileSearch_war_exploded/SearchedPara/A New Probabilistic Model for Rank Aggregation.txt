 Rank aggregation aims at combining multiple rankings of objects to generate a better ranking. It is the key problem in many applications. For example, in meta search [1], when users issue a query, the query is sent to several search engines and the rankings given by them are aggregated to generate more comprehensive ranking results.
 Given the underlying correspondence between ranking and permutation, probabilistic models on permutations, originated in statistics [19, 5, 4], have been widely applied to solve the problems of rank aggregation. Among different models, the Mallows model [15, 6] and the Luce model [14, 18] are the most popular ones.
 The Mallows model is a distance-based model, which defines the probability of a permutation ac-cording to its distance to a location permutation. Due to many applicable permutation distances, the Mallows model has very rich expressiveness, and therefore can be potentially used in many different applications. Its weakness lies in the high computational complexity. In many cases, it requires a time complexity of O ( n !) to compute the probability of a single permutation of n objects. This is clearly intractable when we need to rank a large number of objects in real applications. The Luce model is a stagewise model, which decomposes the process of generating a permutation of according to a probability based on the scores of the unassigned objects. The product of the selection probabilities at all the stages defines the probability of the permutation. The Luce model is highly efficient (with a polynomial time complexity) due to the decomposition. The expressiveness of the Luce model, however, is limited because it is defined on the scores of individual objects and cannot leverage versatile distance measures between permutations.
 In this paper, we propose a new probabilistic model on permutations, which inherits the advantages of both the Luce model and the Mallows model and avoids their limitations. We refer to the model as coset-permutation distance based stagewise (CPS) model. Different from the Mallows model, the CPS model is a stagewise model. It decomposes the generative process of a permutation into sequential stages, which makes the efficient computation possible. At the k -th stage, an object is selected and assigned to position k with a certain probability. Different from the Luce model, the CPS model defines the selection probability based on the distance between a location permutation and the right coset of (referred to as coset-permutation distance) at each stage. In this sense, it is also a distance-based model. Because many different permutation distances can be used to induce the coset-permutation distance, the CPS model also has rich expressiveness. Furthermore, the coset-permutation distances induced by many popular permutation distances can be computed with polynomial time complexity, which further ensures the efficiency of the CPS model. We then apply the CPS model to supervised rank aggregation and derive corresponding algorithms for learning and inference of the model. Experiments on public datasets show that the CPS model based algorithms can achieve state-of-the-art ranking accuracy, and are much more efficient than baseline methods based on previous probabilistic models. 2.1 Rank Aggregation There are mainly two kinds of rank aggregation, i.e., score-based rank aggregation [17, 16] and order-based rank aggregation [2, 7, 3]. In the former, objects in the input rankings are associated we focus on the order-based rank aggregation, because it is more popular in real applications [7], and score-based rank aggregation can be easily converted to order-based rank aggregation by ignoring the additional score information [7].
 Early methods for rank aggregation are heuristic based. For example, BordaCount [2, 7] and median rank aggregation [8] are simply based on average rank positions or the number of pairwise wins. In the recent literature, probabilistic models on permutations, such as the Mallows model and the Luce model, have been introduced to solve the problem of rank aggregation. Previous studies have shown that the probabilistic model based algorithms can outperform the heuristic methods in many settings. For example, the Mallows model has been shown very effective in both supervised rank aggregation and unsupervised rank aggregation, and the effectiveness of the Luce model has been demonstrated in the context of unsupervised rank aggregation. In the next subsection, we will describe these two models in more detail. 2.2 Probabilistic Models on Permutations In order to better illustrate the probabilistic models on permutations, we first introduce some con-cepts and notations.
 { object assigned to position i . We usually write and 1 as vectors whose i -th component is ( i ) and 1 ( i ) , respectively. We also use the bracket alternative notation to represent a permutation, i.e., =  X  1 (1) ; 1 (2) ; : : : ; 1 ( n )  X  .
 The collection of all permutations of n objects forms a non-abelian group under composition, called the symmetric group of order n , denoted as S n . Let S n k denote the subgroup of S n consisting of all permutations whose first k positions are fixed: The right coset S n k = { |  X  S n k } is a subset of permutations whose top-k objects are exactly the same as in . In other words, 2 , : : : , and i k in position k .
 The Mallows model is a distance based probabilistic model on permutations. It uses a permutation distance d on the symmetric group S n to define the probability of a permutation: where  X  S n is the location permutation,  X  R is a dispersion parameter, and There are many well-defined metrics to measure the distance between two permutations, such as Spearman X  X  rank correlation d r ( ; ) =  X  if x is true and 0 otherwise. One can (and sometimes should) choose different distances for different applications. In this regard, the Mallows model has rich expressiveness.
 in the Mallows model, for many other distances (such as d r and d f ), there is no known efficient method to compute Z ( ; ) and one has to pay for the high computational complexity of O ( n !) [9]. This has greatly limited the application of the Mallows model in real problems. Usually, one has to employ sampling methods such as MCMC to reduce the complexity [12, 11]. This, however, will affect the effectiveness of the model.
 The Luce model is a stagewise probabilistic model on permutations. It assumes that there is a the object 1 (1) is assigned to position 1 with probability exp( ! 1 (1) )  X  n until a complete permutation is formed. In this way, we obtain the permutation probability of as follows, The computation of permutation probability in the Luce model is very efficient, as shown above. Actually the corresponding complexity is in the polynomial order of the number of objects. This is a clear advantage over the Mallows model. However, the Luce model is defined as a specific function of the scores of the objects, and therefore cannot make use of versatile permutation distances. As a result, its expressiveness is not as rich as the Mallows model, which may limit its applications. As discussed in the above section, both the Mallows and the Luce model have certain advantages and limitations. In this section, we propose a new probabilistic model on permutations, which can inherit their advantages and avoid their limitations. We call this model the coset-permutation distance based stagewise (CPS) model. 3.1 The CPS Model As indicated by the name, the CPS model is defined on the basis of the so-called coset-permutation distance. A coset-permutation distance is induced from a permutation distance, as shown in the following definition.
 Definition 1. Given a permutation distance d , the coset-permutation distance ^ d from a coset S n k to a target permutation is defined as the average distance between the permutations in the coset and the target permutation: where | S n k | is the number of permutations in set S n k .
 It is easy to verify that if the permutation distance d is right invariant, then the induced coset-permutation distance ^ d is also right invariant.
 With the concept of coset-permutation distance, given a dispersion parameter  X  R and a location permutation  X  S n , we can define the CPS model as follows. Specifically, the generative process of a permutation of n objects is decomposed into n sequential stages. As an initialization, all the objects are placed in a working set. At the k -th stage, the task is to select the k -th object in the original permutation out of the working set. The probability of this selection is defined with the coset-permutation distance between the right coset S n k and the location permutation : where S n k ( ; k; j ) denotes the right coset including all the permutations that rank objects From Eq. (6), we can see that the closer the coset S n k is to the location permutation , the larger the selection probability is. Considering all the n stages, we will obtain the overall probability of generating , which is shown in the following definition.
 Definition 2. The CPS model defines the probability of a permutation conditioned on a dispersion parameter and a location permutation as: where S n k ( ; k; j ) is defined in the sentence after Eq. (6).
 It is easy to verify that the probabilities P ( | ; ) ;  X  S n defined in the CPS model naturally form a distribution over S n . That is, for each  X  S n , we always have P ( | ; )  X  0 , and  X  In rank aggregation, one usually needs to combine multiple input rankings. To deal with this sce-nario, we further extend the CPS model, following the methodology used in [12].
 where = { 1 ; : : : ; M } and = { 1 ; : : : ; M } .
 The CPS model defined as above can be computed in a highly efficient manner, as discussed in the following subsection. 3.2 Computational Complexity According to the definition of the CPS model, at the k -th stage, one needs to compute ( n  X  k ) coset-permutation distances. At first glance, the complexity of computing each coset-permutation intractable. The good news is that the real complexity for computing the coset-permutation distance induced by several popular permutation distances is much lower than O (( n  X  k )!) . Actually, they can be as low as O ( n 2 ) , according to the following theorem.
 Theorem 1. The coset-permutation distances induced from Spearman X  X  rank correlation d r , Spear-man X  X  footrule d f , and Kendall X  X  tau d t can all be computed with a complexity of O ( n 2 ) . More specifically, for k = 1 ; 2 ; : : : ; n  X  2 , we have 2 According to the above theorem, each induced coset-permutation distance can be computed with a time complexity of O ( n 2 ) . If we compute the CPS model according to Eq. (7), the time complexity the following theorem.
 Theorem 2. For the coset distances induced from d r , d f and d t , the CPS model in Eq. (7) can be computed with a time complexity of O ( n 2 ) . 3.3 Relationship with Previous Models The CPS model as defined above has strong connections with both the Luce model and the Mallows model, as shown below.
 The similarity between the CPS model and the Luce model is that they are both defined in a stage-wise manner. This stagewise definition enables efficient inference for both models. The difference between the CPS model and the Luce model lies in that the CPS model has a much richer expres-siveness than the Luce model. This is mainly because the CPS model is a distance based model while the Luce model is not. Our experiments in Section 5 show that different distances may be appropriate for different applications and datasets, which means a model with rich expressiveness has the potential to be applied for versatile applications.
 The similarity between the CPS model and the Mallows model is that they are both based on dis-tances. Actually when the coset-permutation distance in the CPS model is induced by the Kendall X  X  tau d t , the CPS model is even mathematically equivalent to the Mallows model defined with d t . The major difference between the CPS model and the Mallows model lies in the computational efficiency. The CPS model can be computed efficiently with a polynomial time complexity, as dis-cussed in the previous sub section. However, for most permutation distances, the complexity of the Mallows model is as huge as O ( n !) . 3 According to the above discussions, we can see that the CPS model inherits the advantages of both the Luce model and the Mallows model, and avoids their limitations. In this section, we show how to apply the extended CPS model to solve the problem of rank aggrega-tion. Here we take meta search as an example, and consider the supervised case of rank aggregation. That is, given a set of training queries, we need to learn the parameters in the CPS model and apply the model with the learned parameters to aggregate rankings for new test queries. Algorithm 1 Sequential inference 4.1 Learning query q l , and ( l ) is the set of M input rankings.
 In order to learn the parameters in Eq. (8), we employ maximum likelihood estimation. Specifi-cally, the log likelihood of the training data for the CPS model can be written as below, It is not difficult to prove that L ( ) is concave with respect to . Therefore, we can use simple optimization techniques like gradient ascent to find the globally optimal . 4.2 Inference In the test phase, given a new query and its associated M input rankings, we need to infer a final ranking with the learned parameters .
 A straightforward method is to find the permutation with the largest probability conditioned on the M input rankings, just as the widely-used inference algorithm for the Mallows model [12]. We call it cannot handle applications with a large number of objects to rank. Considering the stagewise definition of the CPS model, we propose a sequential inference algorithm. The algorithm decom-poses the inference into n steps. At the k -th step, we select the object j that can minimize the coset-permutation distance position. The procedure is listed in Algorithm 1.
 In fact, sequential inference is an approximation of global inference, with a much lower complexity. Theorem 3 shows that the complexity of sequential inference is just O ( M n 2 ) . Our experiments in the next section indicate that such an approximation does not hurt the ranking accuracy by much, while significantly speeds up the inference process.
 Theorem 3. For the coset distance induced from d r , d f , and d t , the stagewise inference as shown in Algorithm 1 can be conducted with a time complexity of O ( M n 2 ) . We have performed experiments to test the efficiency and effectiveness of the proposed CPS model. 5.1 Settings We take meta search as the target application, and use the LETOR [13] benchmark datasets in the experiments. LETOR is a public collection created for ranking research. 4 There are two meta search datasets in LETOR, MQ2007-agg and MQ2008-agg. In addition to using them, we also composed a smaller dataset from MQ2008-agg, referred to as MQ2008-small, by selecting queries with no more than 8 documents from the MQ2008-agg dataset. This small dataset is used to perform detailed investigations on the CPS model and other baseline models.
 We used NDCG [10] as the evaluation measure in our experiments. NDCG is a widely-used IR measure for multi-level relevance judgments. The larger the NDCG value, the better the aggregation accuracy.
 section are the average results over the five folds.
 For the CPS model, we tested two inference methods: global inference (denoted as CPS-G) and se-quential inference (denoted as CPS-S). For comparison, we implemented the Mallows model. When applied to supervised rank aggregation, the learning process of the Mallows model is also maximum likelihood estimation. For inference, we chose the permutation with the maximal probability as the final aggregated ranking. The time complexity of both learning and inference of the Mallows model with distance d r and d f is O ( n !) . We also implemented an approximate algorithm as suggested by [12] using MCMC sampling to speed up the learning process. We refer to this approximate al-gorithm as MallApp. Note that the time complexity of the inference of MallApp is still O ( n !) for distance d r and d f . Furthermore, as a reference, we also tested a traditional method, BordaCount [1], which is based on majority voting. We did not compare with the Luce model because it is not straightforward to be applied to supervised rank aggregation, as far as we know.
 Note that Mallows, MallApp and CPS-G cannot handle the large datasets MQ2007-agg and MQ2008-agg, and were only tested on the small dataset MQ2008-small. 5.2 Results First, we report the results of these algorithms on the MQ2008-small dataset.
 The aggregation accuracies in terms of NDCG are listed in Table 1(a). Note that the accuracy of Mallows( d t ) is the same as that of CPS-G( d t ) because of the mathematical equivalence of the two models. Therefore, we omit Mallows( d t ) in the table. We did not implement the sampling-based learning algorithm for the Mallows model with distance d t , because in this case the learning algorithm has already been efficient enough.
 From the table, we have the following observations. In addition to the comparison of aggregation accuracy, we have also logged the running time of each model. For example, on our test machine (with 2.13Ghz CPU and 4GB memory), it took about 12 training process. The inference of the Mallows model based algorithms and the global inference of the CPS model based algorithms took more time than sequential inference of the CPS model, although the difference was not significant (this is mainly because n  X  8 for MQ2008-small). From these results, we can see that the proposed CPS model plus sequential inference is the most efficient one, and its accuracy is also very good as compared to other methods.
 Second, we report the results on MQ2008-agg and MQ2007-agg in Table 1(b). Note that the results of the Mallows model based algorithms and that of the CPS model with global inference are not available because of the high computational complexity for their learning or inference. The results show that the CPS model with sequential inference outperforms BordaCount, no matter which dis-tance is used. Moreover, the CPS model with d t performs the best on MQ2008-agg, and the model with d r performs the best on MQ2007-agg. This indicates that we can achieve good ranking per-formance by choosing the most suitable distances for different datasets (and so applications). This provides a side evidence that it is beneficial for a probabilistic model on permutations to have rich expressiveness.
 To sum up, the experimental results indicate that the CPS model based learning and sequential inference algorithms can achieve state-of-the-art ranking accuracy and are more efficient than other algorithms. In this paper, we have proposed a new probabilistic model, named the CPS model, on permutations for rank aggregation. The model is based on coset-permutation distance and defined in a stagewise manner. It inherits the advantages of the Luce model (high efficiency) and the Mallows model (rich expressiveness), and avoids their limitations. We have applied the model to supervised rank aggregation and investigated how to perform learning and inference. Experiments on public datasets demonstrate the effectiveness and efficiency of the CPS model.
 As future work, we plan to investigate the following issues. (1) We have shown that three induced coset-permutation distances can be computed efficiently. We will explore whether other distances also have such properties. (2) We have applied the CPS model to the supervised case of rank aggre-gation. We will study the unsupervised case. (3) We will investigate other applications of the model, and discuss how to select the most suitable distance for a given application. [1] J. Aslam and M. Montague. Models for metasearch. In Proceedings of the 24th SIGIR , pages [2] J. A. Aslam and M. Montague. Models for metasearch. In SIGIR  X 01: Proceedings of the 24th [3] M. Beg. Parallel Rank Aggregation for the World Wide Web. World Wide Web. Kluwer Aca-[4] D. Critchlow. Metric methods for analyzing partially ranked data . 1980. [5] H. Daniels. Rank correlation and population models. Journal of the Royal Statistical Society. [6] P. Diaconis. Group representations in probability and statistics . Institute of Mathematical [7] C. Dwork, R. Kumar, M. Naor, and D. Sivakumar. Rank aggregation methods for the web. [8] R. Fagin, R. Kumar, and D. Sivakumar. Efficient similarity search and classification via rank [9] M. Fligner and J. Verducci. Distance based ranking models. Journal of the Royal Statistical [11] A. Klementiev, D. Roth, and K. Small. Unsupervised rank aggregation with distance-based [12] G. Lebanon and J. Lafferty. Cranking: Combining rankings using conditional probability [13] T. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. LETOR: Benchmark dataset for research on learning [14] R. D. Luce. Individual Choice Behavior . Wiley, 1959. [15] C. L. Mallows. Non-null ranking models. Biometrika , 44:114 X 130, 1957. [16] R. Manmatha, T. Rath, and F. Feng. Modeling score distributions for combining the outputs [17] M. Montague and J. A. Aslam. Relevance score normalization for metasearch. In CIKM  X 01: [18] R. L. Plackett. The analysis of permutations. Applied Statistics , 24(2):193 X 202, 1975. [19] L. Thurstone. A law of comparative judgment. Psychological review , 34(4):273 X 286, 1927.
