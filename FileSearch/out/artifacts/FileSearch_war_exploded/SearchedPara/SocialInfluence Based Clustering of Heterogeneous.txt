 Social networks continue to grow in size and the type of infor-mation hosted. We witness a growing interest in clustering a so-cial network of people based on both their social relationships and their participations in activity based information networks. In this paper, we present a social influence based clustering framework for analyzing heterogeneous information networks with three u-nique features. First, we introduce a novel social influence based vertex similarity metric in terms of both self-influence similarity and co-influence similarity. We compute self-influence and co-influence based similarity based on social graph and its associat-ed activity graphs and influence graphs respectively. Second, we compute the combined social influence based similarity between each pair of vertices by unifying the self-similarity and multiple co-influence similarity scores through a weight function with an iterative update method. Third, we design an iterative learning al-gorithm, SI-C luster , to dynamically refine the K clusters by con-tinuously quantifying and adjusting the weights on self-influence similarity and on multiple co-influence similarity scores toward-s the clustering convergence. To make SI-C luster converge fast, we transformed a sophisticated nonlinear fractional programming problem of multiple weights into a straightforward nonlinear para-metric programming problem of single variable. Our experiment results show that SI-C luster not only achieves a better balance be-tween self-influence and co-influence similarities but also scales extremely well for large graph clustering.
 H.2.8 [ Database Applications ]: Data Mining Graph Clustering, Heterogeneous Network, Social Influence
Social influence studies the impact of a group of people on an in-dividual member of the group by their opinions or actions. Social influence analysis has great potential for understanding the ways in which information, ideas, experiences and innovations are spread across social networks. As more and more people are engaged in social networks, we witness many forms of heterogeneous social networks in which entities are of di ff erent types and are intercon-nected through heterogeneous types of links, representing di kinds of semantic relations. Analyzing and mining heterogeneous social networks can provide new insights about how people inter-act with and influence each other and why ideas and opinions on di ff erent subjects propagate di ff erently on social networks.
Clustering a heterogeneous social network with multiple types of links, entities, static attributes and dynamic and inter-connected ac-tivities demands for new clustering models and distance functions to address the following new challenges.  X 
The large scale heterogeneous social network analysis often dis-plays features of social complexity and involves substantial non-trivial computational cost. For example, a full version of the D-
BLP bibliography data contains 964 , 166 authors, 6 , 992 confer-ences, 363 , 352 keywords and 31 , 962 , 786 heterogeneous links.  X 
Each type of entities usually associates to one primary social world but participates in many other social worlds, each with domain-specific semantics. How to make good use of the infor-mation from various social worlds to provide more informative views of how people influence one another in a given social net-work? For instance, we may want to utilize the original facebook people network as well as the associated activity networks in the facebook dataset to generate a better clustering of people based on their social influence in terms of both their circle of friends (i.e., self-influence) and their participations in multiple domain specific activity networks (i.e., multiple types of co-influence).  X 
The information flow between two social worlds may be bidirec-tional so that we should be careful in di ff erentiating them when we integrate the results from di ff erent information networks. For example, Bob may influence his circle of friends (direct or indi-rect) by his blogs on certain subject and his participation in some tennis tournaments. On the other hand, direct links from a blog (or a tournament) to other blogs (or tournaments) can serve as a recommendation by Bob to its circle of friends.  X 
As multiple social networks may be from arbitrary domains, it is challenging to e ffi ciently integrate the multiple types of influ-ences from multiple information networks into a unified distance space simultaneously. Moreover, social network clustering can be more meaningful if it is context aware and only the activity networks that are relevant to the context of interest will be uti-lized to perform the social influence based clustering analysis.
With these new challenges in mind, in this paper we develop an innovative social influence based graph clustering approach for het-erogeneous information networks, SI-C luster . It captures not only the complex attributes of people (vertices) in the social collabora-tion network but also the nested and complex relationships between people and other types of entities in di ff erent information network-Concretely, we categorize the social influence based graph model work or the activity networks, (2) the single-valued or multi-valued vertices in the social network (such as name, sex, age, and multiple education degrees a person may achieve), (3) the nested and com-plex relationships between the social network and the activity net-works (such as multiple activities one may have participated). We show that the social influence based graph clustering for heteroge-neous networks demands for a dynamic graph clustering method in contrast to conventional graph clustering algorithms. SI-C is designed to cluster large social network with two new criteri-a: (1) it takes into account both the complex vertex properties and the topological structure to define the initial influence of a vertex and the weights of its influence propagation to its circle of friends; (2) it computes pairwise vertex closeness by considering not only the social influence patterns (influence-based similarities) based on both direct and indirect social connections existing in the relevant social and activity networks but also the potentially new interac-tions that have high propagation probabilities based on the existing interactions. A unique characteristics of SI-C luster is its ability of integrating the self-influence and multiple types of co-influences into a unified influence-based similarity measure through iterative-ly clustering and dynamic weight tuning mechanism.

This paper makes the following original contributions.  X 
We integrate di ff erent types of links, entities, static attributes and dynamic activities from di ff erent networks into a unified influence-based model through the intra-network or inter-network social influences.  X 
We compute influence-based vertex similarity in terms of heat di ff usion based influence propagation on both social graph (self-influence) and each of activity graphs (co-influence).  X 
A dynamic weight tuning method is provided to combine vari-ous influence-based similarities through an iterative learning al-gorithm, SI-C luster , for social influence based graph cluster-ing. To make the clustering process converge fast, a sophisti-cated nonlinear fractional programming problem with multiple weights is transformed to a straightforward parametric program-ming problem of a single variable.  X 
We perform extensive evaluation on real datasets to demonstrate that SI-C luster can partition the graph into high-quality clusters with cohesive structures and homogeneous social influences.
The most closely related work to this research falls into three ar-eas: social influence analysis, heterogeneous social network anal-ysis and graph clustering. Social influence analysis is gaining at-tention in recent years. [1] proposed the first provable approxima-tion algorithm for maximizing the spread of influence in a social network. [2] proposed a cascading viral marketing algorithm. [3] proposed a heat-di ff usion based viral marketing model with top K most influential nodes. [4] used a user X  X  implicit social graph to sented a model in which information can reach a node via the links of the social network or through the influence of external sources.
Recent works on heterogeneous social network analysis [6 X 10] combine links and content into heterogeneous information network-s to improve the quality of querying, ranking and clustering. [6] proposed a method to model a relational database containing both attributes and links. [7] proposed to learn an optimal linear com-bination of di ff erent relations on heterogeneous social networks in terms of their importance on a certain query. [9] groups objects into pre-specified classes, while generating the ranking information for each type of object in a heterogeneous information network. [10] presented a query-driven discovery system for finding semantically similar substructures in heterogeneous networks.
 Graph clustering has attracted active research in the last decade. Most of existing graph clustering techniques have focused on the topological structure based on various criteria, including normal-ized cuts [11], modularity [12], structural density [13], stochastic flows [14] or clique [15]. K-SNAP [16] and CANAL [17] present-ed OLAP-style aggregation approaches to summarize large graphs by grouping nodes based on the user-selected attributes. [18] ex-ploited an information-theoretic model for clustering by growing a random seed in a manner that minimizes graph entropy. [19] p-resented a clustering method which integrates numerical vectors with modularity into a spectral relaxation problem. SA-Cluster [20] and BAGC [21] perform clustering based on both structural and at-tribute similarities by incorporating attributes as augmented edges PathSelClus [22] utilizes limited guidance from users in the form of seeds in some of the clusters and automatically learn the best weights for each meta-path in the clustering process. GenClus [23] proposed a model-based method for clustering heterogeneous net-works with di ff erent link types and di ff erent attribute types.
To our knowledge, this work is the first one to address the prob-lem of social influence based clustering over heterogeneous net-works by dynamically combining self-influence from social graph and multiple types of co-influence from activity graphs.
We consider three types of information networks in defining a so-cial influence based graph clustering method: (1) the social collab-oration network, which is the target of graph clustering and typical-ly a social network of people, such as friend network, co-author net-work, to name a few; (2) the associated activity networks, such as product purchasing activity network, sport activity network or con-ference activity network; (3) the influence networks representing bipartite graphs connecting social network and activity networks. We formally define the three types of networks as follows.
A social graph is denoted as SG = ( U , E ), where U is the set of vertices representing the members of the collaboration network, such as customers or authors, and E is the set of edges denoting the collaborative relationships between members of the collaboration network. We use N SG to represent the size of U , i.e., N
An activity graph is defined by AG i = ( V i , S i ), where v denotes an activity vertex in the i th associated activity network AG and s  X  S i is a weighted edge representing the similarity between two activity vertices, such as functional or manufacture similarity. We denote the size of each activity vertex set as N AG i = | (a) Conference Influence Graph An influence graph is denoted as IG i = ( U , V i , S i , V and S i have the same definitions in the social graph SG and the activity graph AG i respectively. Every edge t  X  T i , denoted by ( u , v ), connecting a member vertex u  X  U to an activity vertex v  X  V a purchasing or publishing activity. Thus, IG i is a bipartite graph.
Given a social graph SG , multiple activity graphs AG i and var-ious influence graphs IG i (1  X  i  X  N ), the problem of S ocial I nfluence-based graph Cluster ing (SI-C luster ) is to partition the member vertices U into K disjoint clusters U i , where U = and U i results in densely connected groups and each has vertices with sim-ilar activity behaviors. A desired clustering result should achieve a good balance between the following two properties: (1) vertices within one cluster should have similar collaborative patterns among themselves and similar interaction patterns with activity networks; (2) vertices in di ff erent clusters should have dissimilar collabora-tive patterns and dissimilar interaction patterns with activities.
Figure 1 (a) provides an illustrating example of a heterogeneous information network extracted from the DBLP dataset. It consists of two types of entities: authors and conferences and three types of links: co-authorship, author-conference, conference similarity. In our SI-C luster framework, we reorganize a heterogeneous in-formation network into a social graph, multiple activity graphs and multiple influence graphs without loss of information. The hetero-geneous network in Figure 1 (a) is divided into three subgraphs: a social collaboration graph of authors, a conference activity graph, and an influence graph about author X  X  publishing activity in confer-ences, as shown in Figures 1 (b), (c) and (d), respectively. A red number associated with a red dashed edge quantifies the number of publications that an author published in a conference. A green number on a green edge measures the similarity score between con-ferences. For ease of presentation, we removed the conference sim-ilarities with less than 0 . 005. A number of mechanisms can be used to compute similarity of conferences. We use RankClus [24] to par-tribution and ranking in each cluster, we calculate the similarities between activities in activity graph. Black numbers in the bracket represent the total amount of publications of an author. Other black numbers on co-author edges denote the number of co-authored pa-pers. A more complex example of influence graph with 12 authors and 12 conferences (or keywords) is presented in Figure 2.
This section describes how to measure the vertex closeness in terms of self-influence and co-influence models. We first utilize heat di ff usion model to capture self-influence based similarity be-tween member vertices in the social graph. Then we use heat d-i ff usion model to construct one co-influence model for each influ-ence graph using a probabilistic classification method to compute co-influence similarities of two vertices in the social graph. Final-ly, we compute pairwise vertex similarities based on the influence similarity matrix and generate an influence-based pairwise similar-ity matrix on the social graph for each of its N influence graphs.
Heat di ff usion is a physical phenomenon that heat always flows from an object with high temperature to an object with low tem-perature. In a large social graph SG , experts with many publi-cations often influence other late authors. Consumers purchasing many products may influence other consumers with little purchas-ing. Thus the spread of influence resembles the heat di ff nomenon. Early adopters of a product with many friends or experts on a subject with many coauthors may act as heat sources, transfer their heat to others and di ff use their influence to other majority.
To e ff ectively measure vertex closeness in the social graph in terms of heat di ff usion model, we first define the non-propagating heat di ff usion kernel on social graph.
 Definition 1. [Non-propagating Heat Di ff usion Kernel on Social Graph] Let SG = ( U , E ) denote a social graph where U is the set of member vertices and E is the edge set denoting the collaborative relationships between members. Let  X  be the thermal conductivity (the heat di ff usion coe ffi cient) of SG . The heat change at vertex u  X  U between time t + X  t and time t is defined by the sum of the heat that it receives from all its neighbors, deducted by what it di ff uses. where f i ( t ) is the vertex u i  X  X  temperature at time t . p n (or n j ) denotes the amount of heat / influence that u i within the social graph, e.g., the number of authored publication-s.We express the above heat di ff usion formulation in a matrix form. where H is a N SG  X  N SG matrix, called a non-propagating heat d-i ff usion kernel on SG , as the heat di ff usion process is defined in terms of one-hop neighbors of heat source.
 where  X  i = from u i to all its neighbors.

If we use H to define self-influence similarity between vertices, then the similarity is based on one-hop or direct influence. For those authors who have no joint publications, they are considered to have zero influence on one another, which is unrealistic.
This motivates us to utilize both direct and indirect influence paths between two vertices in computing their vertex similarity. Thus, we define the self-influence similarity using the propagating heat di ff usion kernel, where the heat di ff usion process continues until vertices X  temperatures converge or the system-defined conver-gence condition is met. Concretely, by Eq.(2), we have the follow-ing di ff erential equation when  X  t  X  0. Solving this di ff erential equation, we obtain the following Eq.(5). Definition 2. [Propagating Heat Di ff usion Kernel on Social Graph] Let  X  denote the thermal conductivity, H be the non-propagating di ff usion kernel of SG and f(0) denote an initial heat (influence) column vector at time 0, which defines the initial heat distribution an exponential function with variable t for constant f(0).
We call e  X  t H as the propagating heat di ff usion kernel. It can be expanded as a Taylor series, where I is an identity matrix: where the heat di ff usion reaches convergence, i.e., thermal equilib-tionships between objects, it reflects the vertex closeness on social graph. We treat it as the self-similarity matrix W 0 , i.e., W Here, the thermal conductivity  X  is a user specific parameter. We fied similarity. Figure 3 follows the example of Figure 1. In Fig-ure 3 (a), ochre dashed lines and associated blue numbers represent the self-influence similarity by setting  X  and t equal to 1.
We have presented the use of propagating heat di ff usion kernel to measure the self-influence vertex closeness on social graph. In this section we describe how to compute pairwise co-influence similari-ty for vertices in SG based on one of N associated influence graphs.
Similarly, we first need to define the non-propagating heat kernel on an influence graph. By the definition of influence graph in Sec-tion 3, we should consider four types of one-hop influence di path in defining the non-propagating heat kernel H i .

Definition 3. [Non-propagating Heat Di ff usion Kernel on Influ-ence Graphs] We formulate H i on the influence graph IG i ed to the social graph SG and the activity graph AG i by splitting it into four blocks.
 where B = [B 1 ,  X  X  X  , B N AG the social influence of vertices in AG i on members in SG , defined by Eq.(8); C = [C 1 ,  X  X  X  , C N SG ] T is a N SG  X  N AG i the social influence of members in SG on vertices in AG i ilarities, defined by Eq.(10); and D is a N SG  X  N SG diagonal matrix. ence of v j on SG through u k and is defined by n jk normalized by influence of a conference v j on the social graph through an author, say Philip S. Yu , is defined by the number of papers he published in v normalized by the total number of papers authored by him and published in any conference of the conference graph.
 where n jk denotes the weight on edge ( u j , v k ) and C influence of u j on AG i through v k and is defined by n jk of papers u j published in v k ) normalized by the sum of the weights on ( u l , v k ) for any u l .
 where n jk represents the similarity between two activity vertices v and v k in the activity graph.  X  j = where  X  j summarizes the influence of activity vertex v j activity vertices and associated member vertices.

In the diagonal matrix D , the diagonal entry D jj in each row is equal to  X   X  j where  X  j = of member vertex u j on all activity vertices.
 Definition 4. [Propagating Heat Di ff usion Kernel on Influence Graphs] Let IG i denote the i th influence graph associated to SG and AG i ,  X  denote the thermal conductivity, H i denote the non-tribution on IG i . The vertex X  X  thermal capacity at time t is defined by an exponential function f( t ) with variable t for constant f(0). a Taylor series. where I is a ( N AG i + N SG )  X  ( N AG i + N SG ) identity matrix.
Figure 3 (b) shows the propagating heat di ff usion kernel e for the conference influence graph in our running example, where both  X  and t are set to 1. For presentation clarity, we only show the bidirectional influence flow between authors and conferences with numbers quantify the influence flows from author to conference and the influence flows from conference to author respectively.
We have defined the propagating heat di ff usion kernel e the influence graph IG i (1  X  i  X  N ). According to Eq.11, in order to conduct heat di ff usion on an influence graph and compute pairwise co-influence similarity, we need both e  X  t H i and f i (0) on IG defines the heat sources from which the propagating heat kernel starts its di ff usion process.

We observe that the co-influence between a pair of member ver-tices in the social graph can only be established through their in-teractions with activity vertices in one of the activity graphs. To make good use of the topological information of AG i , find good heat sources from AG i and reduce the commotional cost for large-scale activity graph, we propose to start by partitioning AG M i disjoint activity clusters, denoted by c i 1 , c i 2 , . . . , c with the size of ( N AG i + N SG )  X  1 is defined as follow. vertex v k in cluster c ij is chosen as an initial heat source. Note that for each activity vertex v k , there exists one and only one c cluster among the M i disjoint activity clusters, to which vertex v belongs. Thus we have p i jk = 1 in f ij (0). The last N SG f (0) represent the initial heats of member vertices in SG with al-l 0s. Thus, the initial heat distribution matrix f i (0) is defined as [f (0) = [f i1 (0) , f i2 (0) ,  X  X  X  , f iM i (0)].

We argue that two members are similar if both of them partici-pate in many activities in the same clusters. We propose a proba-bility based co-influence classification method to classify members into the activity-based clusters and generate the co-influence simi-larity between members based on the member distribution in each class. We first use f ij (0) (1  X  j  X  M i ) as the training data and erate member X  X  probability in each activity-based class. The heat distribution f i ( t ) at time t is then given as follow.
Consider conference classes DM and DB in Figure 3 (c), we have the initial conference influence distribution matrix f conf where 2 columns represent the conference classes DM and DB and 11 rows represent six conference vertices ( ICDM , KDD , SDM , SIG-MOD , VLDB and ICDE ), and five author vertices ( Philip S. Yu , Ji-awei Han , Charu C. Aggarwal , Kun-Lung Wu and Haixun Wang ). By Eq.(14) with  X  and time t set to 1, we can generate the final heat based probabilities of belonging to each of DM and DB .
We can further reduce the influence propagation matrix f i the size of ( N AG i + N SG )  X  M i to a N SG  X  M i matrix f the activity rows without loss of quality. Figure 3 (d) shows the (a) Self-influence Similarity two di ff erent conference classes. The larger the number is, the more influence author has on the conference class.

The pairwise vertex closeness is an important measure of cluster-ing quality. Let W i denote the co-influence vertex similarity matrix for influence graph IG i , M i be the number of activity classes in IG of member u j  X  U on IG i at time t , i.e., the probability of u m between members u j and u k is defined below.

The green numbers in Figure 3 (e) represents the co-influence based similarity from the conference influence graph.
The problem of integrating the influence-based similarities on both social graph and multiple influence graphs into a cohesive and unified similarity measure is quite challenging. In this paper, we propose to use a unified influence-based similarity measure togeth-er with an iterative learning algorithm to address this problem. Let W 0 denote the self-influence similarity from the social graph SG with the weight factor  X  , W i denote the co-influence similarity from the influence graph IG i (1  X  i  X  N ) with the weight unified similarity function W is defined as follow.
 where W 0 = e  X  t H ,  X  + The unified similarity between any pair of member vertices in SG is defined based on the set of N + 1 influence-based similarities.
This section presents our clustering framework, SI-C luster partitions a social graph SG based on both self-influence and co-influence similarities through a unified similarity model among SG , the activity graphs AG i , and the influence graphs IG i . SI-C follows the K-Medoids clustering method [25] by using the unified influence-based similarity with the initial weights as an input. At The weight update method computes the weighted contributions of each influence-based similarity to both clustering convergence and clustering objective, and updates N + 1 weights accordingly after each iteration. This process is repeated until convergence.
We will address two main issues in the initialization step: (1) initial weight setup and (2) cluster centroid initialization.
Choosing a weight assignment randomly often results in incor-rect clustering results. In fact, we will prove that there exists one and only one optimal weight assignment to maximize the clus-tering objective. According to Definition 7 and Theorems 4-7 in Section 5.4, we choose parameter  X  = 0 and weights  X  =  X  1 ... =  X  date scheme continuously increases weights to important influence-based similarities and decreases weights or assign zero weights to trivial influence-based similarities at each iteration.
Good initial centroids are essential for the success of partition-ing clustering algorithms. A member vertex which has a local maximum of the number of neighbors often can di ff use its heat to many vertices along multiple paths. A centroid-based cluster is thus formed when heat is di ff used to the margin of the social graph. Thus, we select such K members as the initial centroids {
With K centroids in the t th iteration, we assign each vertex u U to its closest centroid c  X  = argmax c t c vertices are assigned to some cluster, the centroid will be updated with the most centrally located vertex in each cluster. To find such a vertex, we first compute the  X  X verage point" u i of a cluster U terms of the unified similarity matrix as U . Then we find the new centroid c t + 1 i in cluster U i as
Therefore, we find the new centroid c t + 1 i in the ( t + whose unified similarity vector is the closest to the cluster average.
The objective of clustering is to maximize intra-cluster similar-ity and minimize inter-cluster similarity. We first define the inter-cluster similarity.

Definition 5. [Inter-cluster Similarity] Let SG = ( U , E ) be the cluster similarity between U p and U q is defined as follow. This inter-cluster similarity measure is designed to quantitatively measure the extent of similarity between two clusters of U .
Definition 6. [Graph Clustering Objective Function] Let SG ( U , E ) denote a social graph with the weight  X  and IG denote N influence graphs with the weights  X  1 ,..., X  N where the weight for IG i , and K be a number of clusters. The goal of SI-C luster is to find K partitions { U i } K i = 1 such that U U  X  function O ( { U l } K l = 1 , X , X  1 ,..., X  N ) is maximized. subject to  X  +
Thus the graph clustering problem can be reduced to three sub-problems: (1) cluster assignment, (2) centroid update and (3) weight adjustment, each with the goal of maximizing the objective func-tion. The first two problems are common to all partitioning clus-tering algorithms. Thus we focus on the third subproblem, weight adjustment, in the next subsection.
The objective function of our clustering algorithm is to maxi-mize intra-cluster similarity and minimize inter-cluster similarity. Theorems 1 and 2 prove that our clustering objective is equivalent to maximize a quotient of two convex functions of multiple vari-ables. It is very hard to perform function trend identification and estimation to determine the existence and uniqueness of solution-s. Therefore, we can not directly solve this sophisticated nonlinear fractional programming problem.

Definition 7. Suppose that f (  X , X  1 ,..., X  N ) =  X  g (  X , X   X  lowing optimization problem (NFPP).
 subject to  X  +
L emma 1. Let f be a function of a single variable on R . Then (1) f is concave i ff for  X  x 1 , x 2  X  R and  X   X   X  (0 , 1) we have f ((1  X   X  ) x 1 +  X  x 2 ) &gt; (1  X   X  ) f ( x 1 ) +  X  f ( x  X  ) x 1 +  X  x 2 ) 6 (1  X   X  ) f ( x 1 ) +  X  f ( x 2 ) .

Definition 8. A set S of n -vectors is convex if (1  X   X  ) x whenever x , x  X   X  S , and  X   X  [0 , 1].

L emma 2. Let f be a function of multiple variables with contin-uous partial derivatives of first and second order on the convex set S and denote the Hessian of f at the point x by  X  ( x ) . Then (1) f is concave i ff  X  ( x ) is negative semidefinite for (3) f is convex i ff  X  ( x ) is positive semidefinite for Lemmas 1, 2 and the detailed proof can be found in [26].
T heorem 1. f (  X , X  1 ,..., X  N ) is convex on the set S = { (  X , X 
Proof. We first prove that the set S is a convex set. Suppose that two arbitrary (n + 1 )-vectors x = (  X  1 , X  2 ,..., X  (  X  , X  N + 1 ,  X 
For an arbitrary  X   X  [0 , 1] , the (n + 1 )-vector (1  X   X  ((1  X   X  )  X  of each dimension for this (n + 1 )-vector is equal to (1  X   X  is still in S and S is a convex set.
 We then calculate the Hessian matrix of f as follows.
 where D i is the di ff erentiation operator with respect to the i ment.
 only one non-linear term in the Hessian matrix. We can easily prove that all of its eigenval-ues are non-negative. Thus, it is positive-semidefinite for ... ,  X  N  X  S , and f (  X , X  1 ,..., X  N ) is convex on the set S .
T heorem 2. g (  X , X  1 ,..., X  N ) is convex on S since its Hessian matrix  X  ( g ) is positive-semidefinite for  X   X  ,  X  1 , ...
The detailed proof is omitted due to space limit. This theorem can be testified by using the above-mentioned similar method.
T heorem 3. The NFPP problem is equivalent to a polynomial programming problem with polynomial constraints (PPPPC). subject to 0 6  X  6 1 / g (  X , X  1 ,..., X  N ) ,  X  +  X  &gt; 0 ,  X   X  = 1 / g (  X ,  X  / g (  X ,  X  PP, the constraints of PPPPC are satisfied by setting  X  = g (  X , X  f (  X , X  solution (  X , X  1 ,..., X  N , X  ) of PPPPC we have  X  f (  X , X  f (  X , X  =  X  f (  X ,  X 
Although PPPPC is a polynomial programming problem, the polynomial constraints make it very hard to solve. We further sim-plify it as an nonlinear parametric programming problem (NPPP). T heorem 4. A nonlinear parametric programming problem (NPP-P) is defined as z (  X  ) = Max { f (  X , X  1 ,..., X  N )  X   X  subject to  X  + NFPP problem of Eq.(23) is equivalent to this NPPP, i.e., maximum value of NFPP i ff z (  X  ) = 0 .

Proof. If (  X ,  X  1 , ...,  X  N ) is a possible solution of f (  X ,  X   X  g (  X , X  1 ,..., X  N ) 6 f (  X ,  X  1 , ...,  X  N )  X   X  g (  X ,  X  have  X  = f (  X ,  X  1 , ...,  X  N ) / g (  X ,  X  1 , ...,  X  N ) g (  X , X  (  X ,  X 
Conversely, if (  X ,  X  1 , ...,  X  N ) solves NFPP, then we have f (  X ,  X  Thus f (  X , X  1 ,..., X  N )  X   X  g (  X , X  1 ,..., X  N ) 6 f (  X ,  X   X  g (  X ,  X  1 , ...,  X  N ) = 0 . We have z (  X  ) = 0 and the maximum is taken at (  X ,  X  1 , ...,  X  N ).
 No w we have successfully transformed the original NFPP in Eq.(23) into the straightforward NPPP. This transformation can help the algorithm converge in a finite number of iterations. Although it is not clear whether the original objective is concave or convex, the objective z (  X  ) of NPPP has the following properties. T heorem 5. z (  X  ) is a convex function.

Proof: Suppose that (  X ,  X  1 , ...,  X  N ) is a possible solution of z ((1  X   X  )  X  1 +  X  X  2 ) with  X  1 ,  X  2 and 0 6  X  6 1 . z ((1  X  X   X  ( f (  X ,  X  1 , ...,  X  N )  X   X  2 g (  X ,  X  1 , ...,  X  N )) +  X  g (  X ,  X  1 , ...,  X  N )) 6  X   X  ma x ( f (  X ,  X  1 , ...,  X  + (1  X   X  )  X  ma x ( f (  X ,  X  1 , ...,  X  N )  X   X  1 g (  X ,  X  1 (1  X   X  ) z (  X  T heorem 6. z (  X  ) is a monotonic decreasing function. lution of z (  X  1 ) . Thus, z (  X  1 ) = f (  X ,  X  1 , ...,  X  &lt; f (  X ,  X  1 , ...,  X  N )  X   X  2 g (  X ,  X  1 , ...,  X  N ) 6 z T heorem 7. z (  X  ) = 0 has a unique solution.

Proof: Based on the above-mentioned theorems, we know z ( continuous as well as decreasing. In addition, lim  X   X  +  X  and lim  X   X  X  X  X  z (  X  ) =+  X  .
The procedure of solving this NPPP optimization problem in-cludes two parts: (1) find such a reasonable parameter  X  ( making NPPP equivalent to NFPP; (2) given the parameter  X  Algorithm 1 S ocial I nfluence-based Graph Cluster ing 2: Select K initial centroids with a local maximum of #neighbors; 3: Repeat until the objective function z (  X  ) converges: 5: Update the cluster centroids with the most centrally located point 6: Solve the NPPP of z (  X  ); 9: Update W; a polynomial programming problem about the original variables. Our weight adjustment mechanism is an iterative procedure to find the solution of z (  X  ) = 0 and the corresponding weights  X 
N after each iteration of the clustering process. We first generate an initial unified similarity matrix W with equal weights to initial-ize cluster centroids and partition the social graph. Since monotonic decreasing function and z (0) = Max { f (  X , X  1 is obviously non-negative, we start with an initial  X  = 0 and solve the subproblem z (0) by using existing fast polynomial program-ming model to update the weights  X  ,  X  1 , . . . ,  X  N . The updated parameter by  X  = f (  X , X  1 ,..., X  N ) / g (  X , X  1 ,..., X  N gorithm enter the next round. The algorithm repeats the above-mentioned iterative procedure until z (  X  ) converges to 0.
By assembling di ff erent pieces together, we provide the pseudo code of our clustering algorithm -SI-C luster in Algorithm 1.
T heorem 8. The objective function in Algorithm 1 converges to a local maximum in a finite number of iterations.

Proof. Existing work has studied the convergence properties of the partitioning approach to clustering, such as K-Means [27]. Our clustering follows a similar approach. So the cluster assignment and centroid update steps improve the objective function. In ad-dition, we have explained that nonlinear parametric programming optimization also fast converges a local maximum value. Therefore, the objective function keeps increasing (but z (  X  ) keeps decreasing) and converges to a local maximum in a finite number of iterations.
We have performed extensive experiments to evaluate the perfor-mance of SI-C luster on real graph datasets.
We use a full version of the DBLP bibliography data with 964 authors (dblp.xml, 836MB, 05 / 21 / 2011). We build a social graph where vertices represent authors and edges represent their collabo-ration relationships, and two associated activity graphs: conference graph and keyword graph. We make use of a multityped cluster-ing framework, RankClus [24], to partition both conferences and keywords into clusters respectively. According to the conference X  X  or keyword X  X  clustering distribution and ranking in each cluster, we calculate the similarities between conferences or keywords. The two associated influence graphs capture how authors in the social graph interact with the activity networks. We also use a smaller DBLP collaboration network with 100 , 000 highly prolific authors. The third dataset is the Amazon product co-purchasing network with 20 , 000 products. The two activity networks are product cate-gory graph and customer review graph.
We compare SI-Cluster with three recently developed represen-tative graph clustering algorithms, BAGC [21], SA-Cluster [20] and Inc-Cluster [28], and one baseline clustering algorithm, W-Cluster . The last three algorithms integrate entity, link and static attribute information into a unified model. SI-Cluster is our pro-posed algorithm which incorporates not only links, entities, static attributes but also multiple types of dynamic and inter-connected activities into a unified influence-based model. BAGC constructs a Bayesian probabilistic model to capture both structural and at-tribute aspects. Both SA-Cluster and Inc-Cluster combine both structural and attribute similarities in the clustering decisions by estimating the importance of attributes. W-Cluster combines struc-tural and attribute similarities using the equal weighting factors.
Evaluation Measures We use three measures of to evaluate the initions of the metrics are given as follows. density ( { U l } K l = 1 ) = where  X  i is the weight of influence graph IG i , entropy ( a  X   X  vertices in cluster U j which participate in the n th activity in IG entropy from all influence graphs (or attributes) over K clusters.
Davies-Bouldin Index (DBI) measures the uniqueness of clusters with respect to the unified similarity measure.
 and c j ,  X  x is the average similarity of vertices in U x Figure 4 (a) shows the density comparison on Amazon 20 , 000 Products by varying the number of clusters K = 40 , 60 , 80 The density values by SI-Cluster, BAGC, Inc-Cluster and SA-Cluster remains 0 . 89 or higher even when k is increasing. This demon-strates that these methods can find densely connected components. The density values of W-Cluster is relatively lower, in the range of 0 . 72-0 . 85 with increasing K , showing that the generated clus-ters have a very loose intra-cluster structure. Figure 4 (b) shows the entropy comparison on Amazon 20 , 000 Products with K 40 , 60 , 80 , 100. SI-Cluster has the lowest entropy, while other four algorithms have a much higher entropy than SI-Cluster, since SI-Cluster considers not only static attributes but also multiple type-s of dynamic and inter-connected activities during the clustering process. Other methods can not handle dynamic activities and on-ly treat them as static and isolated attributes. Figures 4 (c) shows the DBI comparison on Amazon 20 , 000 Products with di ff t K values. SI-Cluster has the lowest DBI of around 0 . 000008 0 . 000023, while other methods have a much higher DBI than SI-Cluster. This demonstrates that SI-Cluster can achieve both high intra-cluster similarity and low inter-cluster similarity. This is be-cause SI-Cluster integrates self-influence similarity as well as co-influence similarity with the optimal weights assignment by parameter-based optimization. It fully utilizes the connections between activi-ties and the interactions between members and activities so that the generated clusters have not only similar collaborative patterns but also similar interaction patterns with activities.
 Figures 5 (a), (b) and (c) show density, entropy and DBI on D-BLP with 100 , 000 authors when we set K = 400 , 600 , 800 These three figures have similar trends with Figures 4 (a), (b) and (c) respectively. As shown in the figures, SI-Cluster achieves high since the probabilistic clustering method partitions vertices into each possible cluster so that the density value by it often increases with K . SI-Cluster achieves a very low entropy around 2 . which is obviously better than the other methods ( &gt; 6 creases, the entropy by SI-Cluster remains stable, while the density of SI-Cluster decreases. In addition, SI-Cluster achieves the lowest DBI ( &lt; 0 . 000005) among di ff erent methods, while the DBI values by other methods are obviously larger than &gt; 0 . 000005.
Figures 6 (a), (b) and (c) show density, entropy and DBI compar-isons on DBLP with 964 , 166 authors by varying K = 4000 , 8000 , 10000. Other four methods except SI-Cluster do not work on this large dataset due to the  X  X ut of memory" problem with our 8 G main memory machine. However, SI-Cluster still shows good performance with varying K . It achieves similar high density val-(  X  0) for di ff erent K .
Figures 7 (a), (b) and (c) show the clustering time on Amazon 20,000 Products, DBLP 100 , 000 and 964 , 166 authors respective-ly. SI-Cluster outperforms all other algorithms in all experiments. When facing with an extremely large dataset, such as DBLP964 other algorithms cannot work due to the  X  X ut of memory" error, while SI-Cluster scales well with large graphs and shows good per-formance with varying K . We make the following observations on the runtime costs of di ff erent methods. First, SA-Cluster is obvi-ously worst than other methods since it needs to perform the repeat-ing random walk distance calculation during each iteration of the clustering process and the distance computation takes more than 80% of the total clustering time. Second, Inc-Cluster, an optimized version of SA-Cluster, is much slower than SI-Cluster, BAGC and W-Cluster since it still needs to incrementally calculate the ran-dom walk distance. Third, although W-Cluster compute the ran-dom walk distance only once, it still runs on a large scale matrix. (a) Amazon 20,000 Figure 8: Clustering Convergence on DBLP 964,166 Authors Fourth, the performance by BAGC is better than other approaches except SI-Cluster. Although it does not need to repeatedly compute the distance matrix, it needs to iteratively update lots of temporary matrices or interim variables and its computational cost is propor-tional to K 2 so that it may not work well when facing large K value. In comparison, SI-Cluster reorganizes a large scale heterogeneous network into multiple small scale subgraphs. It reduces the cost by partitioning activities with the topological information of the activity graph. Furthermore, SI-Cluster calculates influence-based similarity matrices only once. According to Theorems 4-7, solving z (  X  ) for a given  X  is a polynomial programming problem which can be sped up by existing fast polynomial programming model.
Figure 8 (a) shows the trend of clustering convergence in terms of the z (  X  ) value on DBLP 964 , 166 Authors. The z (  X  decreasing and has a convex curve when we iteratively perform the tasks of vertex assignment, centroid update and weight adjustment during the clustering process. z (  X  ) converges very quickly, usually in three iterations. These are consistent with Theorems 4-7.
Figure 8 (b) shows the trend of weight updates on DBLP 964 Authors with di ff erent K values: the social graph (red curve), the conference influence graph (green curve) and the keyword influence graph (blue curve). We observe that the graph weights converge as the clustering process converges. An interesting phenomenon is that both the social weight and the keyword weight are increas-ing but the conference weight is decreasing with more iterations. A reasonable explanation is that people who have many publica-tions in the same conferences may have di ff erent research topics but people who have many papers with the same keywords usually have the same research topics, and thus have a higher collaboration probability as co-authors.
We examine some details of the experiment results on DBLP 964 , 166 Authors when we set k = 100 for both conferences and keywords. Table 1 (a) shows author X  X  influence score based on the social influence propagation between authors and keyword parti-tions. We only present most prolific DBLP experts in the area of data mining or database. When social influence propagation con-verges, each row represents the influence distribution of an author in each keyword category. We can look upon this influence distri-bution as a probability based clustering result. On the other hand, each column specifies the influence distribution of di ff in the same keyword category. This influence distribution is con-sidered as a local ranking result. (b) Influence Scores Based on Selected Top Conferences
Table 1 (a) actually presents an unbalanced result since the in-fluence propagation process is based on the full DBLP dataset. We know that academic research in the area of database has a longer history and there are more academic conferences or forums focus-ing on database research. Thus, we choose the same number of top conferences for each research area to better evaluate the quality of our co-influence model. Here, we choose three top conferences from four research areas of database, data mining, information re-ence list is, DB: VLDB, SIGMOD, ICDE; DM: KDD, ICDM, SD-M; IR: SIGIR, CIKM, ECIR; AI: IJCAI, AAAI, ECAI. Table 1 (b) shows author X  X  influence score normalized by conference partitions for each author, i.e., a better probability based clustering result.
In this paper, we present a social influence based clustering frame-work for heterogeneous information networks. First, we integrate di ities from di ff erent networks into a unifying influence-based model. Second, an iterative learning algorithm is proposed to dynamical-ly refine the K clusters by continuously quantifying and adjusting the weights on multiple influence-based similarity scores toward-s the clustering convergence. Third, we transform a sophisticated nonlinear fractional programming problem of multiple weights in-to a straightforward nonlinear parametric programming problem of single variable to speed up the clustering process.
 Acknowledgement. This work is partially funded by grants from NSF CISE NetSE program and SaTC program and Intel Science and Technology Center on Cloud Computing. [1] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the [2] P. Domingos and M. Richardson. Mining the network value [3] H. Ma, H. Yang, M. R. Lyu, and I. King. Mining social [4] M. Roth, A. Ben-David, D. Deutscher, G. Flysher, I. Horn, [5] S. Myers, C. Zhu, J. Leskovec. Information Di ff usion and [6] B. Taskar, E. Segal, D. Koller. Probabilistic Classification [7] D. Cai, Z. Shao, X. He, X. Yan, and J. Han. Community [8] T. Yang, R. Jin, Y. Chi, and S. Zhu. Combining link and [9] M. Ji, J. Han, and M. Danilevsky. Ranking-based [10] X. Yu, Y. Sun, P. Zhao, and J. Han. Query-driven discovery [11] J. Shi and J. Malik. Normalized cuts and image [12] M. E. J. Newman and M. Girvan. Finding and evaluating [13] X. Xu, N. Yuruk, Z. Feng, and T. A. J. Schweiger. Scan: a [14] V. Satuluri and S. Parthasarathy. Scalable graph clustering [15] K. Macropol and A. Singh. Scalable discovery of best [16] Y. Tian, R. A. Hankins, and J. M. Patel. E ffi cient aggregation [17] N. Zhang, Y. Tian, and J. M. Patel. Discovery-driven graph [18] E. C. Kenley and Y.-R. Cho. Entropy-based graph clustering: [19] M. Shiga, I. Takigawa, H. Mamitsuka. A spectral clustering [20] Y. Zhou, H. Cheng, and J. X. Yu. Graph clustering based on [21] Z. Xu, Y. Ke, Y. Wang, H. Cheng, and J. Cheng. A [22] Y. Sun, B. Norick, J. Han, X. Yan, P. Yu, X. Yu. Integrating [23] Y. Sun, C. C. Aggarwal, and J. Han. Relation strength-aware [24] Y. Sun, J. Han, P. Zhao, Z. Yin, H. Cheng, and T. Wu. [25] L. Kaufman and P. J. Rousseeuw. Clustering by means of [26] R. T. Rockafellar. Convex Analysis . Princeton University [27] L. Botton and Y. Bengio. Convergence properties of the [28] Y. Zhou, H. Cheng, and J. X. Yu. Clustering large attributed
