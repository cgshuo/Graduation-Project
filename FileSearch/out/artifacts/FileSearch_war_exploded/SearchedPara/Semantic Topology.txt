 A reasonable requirement (among many others) for a lexical or semantic component in an information system is that it should be able to learn incrementally from the linguistic data it is exposed to, that it can distinguish between the topical impact of various terms, and that it knows if it knows stuff or not.

We work with a specific representation framework  X  se-mantic spaces  X  which well accommodates the first require-ment; we study the global qualities of semantic spaces by a topological procedure  X  mapper  X  which gives an indication of topical density of the space; we examine the local con-text of terms of interest in the semantic space using another topologically inspired approach which gives an indication of the neighbourhood of the terms of interest. Our aim is to be able to establish the qualities of the semantic space un-der consideration without resorting to inspection of the data used to build it. Distributional models , such as collocational analyses or prob-abilistic language models, are based on the analysis of ob-served item distribution and collocation in linguistic data and have a long history in linguistics. [3] Today, they pro-vide a theoretical base and profitable results for tasks such as speech recognition, language modelling and information retrieval.

In general, distributional semantic models use the notion of distance between two words to describe relation in mean-ing. This combination of distributional data with a geomet-ric interpretation is what defines semantic spaces . [15, 14] The geometric model is appealing: the notion of closeness in meaning speaks to our intuitions about how semantics work. This, however, would seem to be a somewhat false friend. Our geometric intuitions do not hold water for sev-eral thousand-dimensional spaces. Also, the metaphor of closeness does not deliver useful help if more complex se-mantic relations are considered or larger distances in the space are queried:  X  X hat is the relation between bell pepper and one-pass compiler  X ?;  X  X s cow closer to horse than cof-fee is to tea  X ?;  X  X s a bullfinch closer to bird than L A T to language  X ? Arguably those questions are meaningless for human semantics, but are handily and uselessly answered with great exactitude by geometric semantic spaces. [7] The insight that geometric models are overly specific and unwieldy, especially if built on realistic scale, is the moti-vation for e.g. dimensionality reduction approaches, various latent variable models [2, 12], graph-based models [11], and e.g. Laplacian transforms such as in self-taught hashing [18, 17, 5]. We propose here to use generalise some of those in-sights, and move from a semantic geometry to a semantic topology.

Semantic space models have no natural scale and no given base vectors. Topological models are resilient with respect to scale, rotational transformations, deformations, and co-ordinate choice, and can be constructed to focus on local structure and similarity in near relations. [1] A topologi-cal perspective of the data affords us an effective view of the structure of models, and is useful for the diagnosis and practical quality assessment of models which already have proven to be of value in real-world applications.

The basis for our experimentation are semantic spaces cre-ated using random indexing , [12] trained on various corpora of relevance for information processing tasks which require lexical semantics, e.g. ontology mapping, media monitoring, or topic tracking. We currently use such semantic spaces in practical large-scale industrial applications to find synonyms or near-synonyms of terms of interest, and to track associa-tive concepts over time, as an up-to-date lexical resource. We frequently find we need to examine the models we have trained to ascertain their qualities with respect to some topic of interest. In these following experiments we will use mod-els which are trained on traditional research corpora using the same procedure we would use on internet data for com-mercial purposes. Mapper, first introduced by Singh, M  X emoli, and Carlsson [16], is an algorithm based on topological principles to visu-alize high dimensional data. The intuition behind Mapper is to analyze the structure of the data as a whole instead of analyzing the entire dataset in detail. Mapper is intended to capture such regularities of massive data sets which are obscured by focus on geometric coordinates, by transform-ing the data set to a simplicial complex , a combinatorial and discrete data structure. If the steps of this transformation is done well, the resulting structure can be inspected to un-derstand the characteristics of the data set.

Given a dataset D = d i : d i  X  X , the Mapper procedure can be given in 4 steps: Filtering We analyze the data using a filter function f : Cover Given our dataset D , we structure it by appliying Clustering The points in each bin, meaning each subset of Graph A graph G is created with every individual cluster We illustrate the procedure first using artificially generated point cloud data, as shown in Figure 1. The data consists of 5000 points randomly generated from a Gaussian dis-tribution surrounding three centroids at [ x, y ] coordinates: [10 , 20], [  X  10 ,  X  10], [17 ,  X  10] with a standard deviation of 9. The filter function f was chosen to be Gaussian ker-nel density estimation. The coloring of the points in the graph follow the density estimation. The covering was set to 7 intervals with an overlap of 10 percent. After Mapper processing those same data can be visualized as shown in Figure 2 using a similar colouring scheme. Here, the graph shows that if the points at high density are clustered, there are three clusters; the points at low density cluster into one. Overlapping density ranges show the expected correspon-dences from high to low.

This procedure, in our application of it to semantic spaces, serves to illuminate shared structure across different dis-tance scales of the semantic space by showing if the clus-ter structure in one distance range correspond or differ from another distance range. One of the specific questions we wish to investigate is that of expertise . Given two semantic spaces, what is the extent of training in some topical domain? We will assume that ex-pertise, in the sense of being trained on a set of texts, should Figure 1: Artificially generated geometric data
Figure 2: Geometric data transformed by Mapper have effects on the topological makeup of the semantic space. We trained two semantic spaces on general English-language text 1 and then added some selected topics to each of the spaces. One semantic space was trained by including entire Wikipedia articles related to the topics; another semantic space was only given the introductory paragraphs of those same articles. Thus both semantic spaces are familiar with the foundational vocabulary of the topics, but one of them would have a passing knowledge while the other would have a more in-depth understanding of the topic.

If we now apply a filter function to the points of the se-mantic space based on relation to T , the target concept of interest, our expectation would be that in the one seman-tic space, terms for probe concepts t i known to be related to T should cluster relatively close to T ; in the other they would be more or less randomly distributed over the scale intervals. This is borne out in experiments. Figure 3 shows the difference, as measured by a filter function defined by ten probe words relative to the target topic  X  X inland X . The graph shows how the probe words cluster both better with respect to each other, and closer to the target. The second question we wish to address is that of differential qualities of terms we have observed. Some words are more topical than others, which has been observed in numerous different research traditions, but most notably in practically oriented text analysis. [9, 10, 6, 4, 13] We wish to examine
Settings: 2000 random indexing dimensions, 2 + 2 con-text, trained on the The British National Corpus. (Dis-tributed by Oxford University Computing Services at url http://www.natcorp.ox.ac.uk/). Figure 4: Local dimensionality at various angular separation for names of months the local structure of the semantic space around a term of interest.

We recently experimented using a topologically related approach to establish the density of a neighbourhood for terms in a semantic space and to thus infer the intrinsic dimensionality of the local space around the term. While we expand the radius around the spatial coordinates of a term of interest we record the rate of increase in number of neighbours within that radius. We begin by establishing how rapidly the number of neighbours of a term grows in relation to the growth of the radius of the neighbourhood. We define the rate of growth in the interval r  X  I = [ r 1 to be with n i being the number of observed term neighbours within the radius r i . We use d as an estimate of the local dimensionality around the probe term in r . Averaging the results of those computations over an entire semantic space we find that the local dimensionality was considerably lower than that of the representation itself. [8]
Here, we follow a similar approach, but instead study the particularities of individual terms, or specific categories of term. 2 In Figures 4, 5, 6, 7, we plot d at various radius
Settings: 1000 random indexing dimensions, 2 + 2 context, trained on the tasa corpus. Figure 5: Local dimensionality at various angular separation for names of months, weekdays, colours and auxiliary verbs ranges on the surface of a unit hypersphere, with the radius here graphed as the angle of separation between the probe term vector and the neighbouring vectors.

As a first illustration, Figure 4 shows the rate of neigh-bourhood growth curves for names of months, with the angle as computed from the origin on the x-axis. Note that May behaves differently from the other months, due to the poly-semy of the word. Figure 5 shows a comparison of results between names of months, weekdays, colours, and auxiliary verbs. The latter have a much more flattened distribution; the former three all have higher neighbourhood density at lower angular distances, and then the majority of neighbours at around ninety degrees, i.e. at maximum distance from the term, indicating no semantic relation. This is to be expected since months, weekdays, and colours all have fairly well de-limited semantics and thus contexts of use, whereas auxil-iaries can be expected to cooccur with numerous subjects and verbs and thus have a much more promiscuous context. This comparison would lead us to expect that content-heavy words are likely to have neighbours accrue earlier, at smaller angular distances.

A comparison between the figures in Figures 6 and 7 con-firms this. One shows the neighbourhood growth curves for the 150 most frequent words found in the corpus: the, be, to, of, and, ... . The other shows the same curves for some 300 terms which are found in Wikipedia: topic headers england, Figure 6: Local dimensionality at various angular separation for the most frequent words in the corpus Figure 7: Local dimensionality at various angular separation for the most frequent words in the corpus mississippi, instagram, socrates ... . The form of the curves are clearly different even at a cursory inspection.
To verify this observation, we performed several simple categorisation experiments, based on minimising square er-ror to the dimensionality graprh, to distinguish parts of speech and term lists of various classes of word. Table 1 shows the result of categorising the classes given in Figure 5. Similarly useful results were found between other categories of term such as various semantic categories of verbs. Semantic spaces, a useful learning framework for lexical re-sources, are typically treated as black boxes and applied us-ing geometric and linear algebraic processing tools. We have found that topological methods are useful for exploring the makeup of a semantic space. Inferred class  X  Colour Month Auxiliary Weekday Actual class  X  verb Auxiliary verbs 3% 13% 84% 0%
