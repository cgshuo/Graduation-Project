 learning from a mixture of sup ervised and unsup er vised data eg Bennett Demiriz Blum Mitc hell In this pap er w ein v estigate another source of additional information auxiliary sup ervised data dra wn from a distribution di eren t from the tar get distribution Auxiliary data are often a v ailable in mac hine learn ing application problems F or example in medical applications data ma y ha v e b een gathered in di er en t coun tries or with somewhat di eren t de nitions of the class lab els In nancial analysis data ma yha v e been gathered in earlier y ears or with sligh tly di er en t de nitions of the attributes eg the de nitions of pro ductivit y and consumer price index c hange o v er time Ac hallenge for mac hine learning is to nd w a ys of exploiting this data to impro v e p erformance on the target classi cation task The utilit y of auxiliary data can be understo o d through a biasv ariance analysis Because the real training data is scarce a learned classi er will ha v e high v ariance and therefore high error Incorp orating auxiliary data can reduce this v ariance but p ossibly increase the bias b ecause the auxiliary data is dra wn from a di eren t distribution than the real data This analysis also suggests that as the amoun t of real train ing data increases the utilit y of auxiliary data should decrease This pap er w as inspired b y an application in image classi cation for b otan y Supp ose y ou are hiking in the forest and y ou encoun ter an in teresting plan t Y ou w onder what this plan t is so y ou clip o a leaf tak eit home and scan it using y our scanner Then y ou go to a w ebbased classi cation service upload the image and the serv er classi es the leaf and then pro vides in formation ab out the plan t W ew ould lik etopro vide suc h a service for a large range of plan t sp ecies The researc h describ ed in this pap er is part of this e ort In this plan t image classi cation task the primary classi cation task is to determine the sp ecies of an isolated leaf giv en an image of that leaf T o obtain function where D h is a complexit y p enalt y to prev en t o v er tting and is an adjustable parameter that con trols the tradeo bet w een tting the data b y minimizing the loss and h yp othesis complexit y A natural approac h to exploiting auxiliary training data w ould b e to c hange the ob jectiv etoha v e a sepa rate term for tting the auxiliary data J h The parameter presumably less than con trols ho w hard w e try to t the auxiliary data Cross v alidation or holdout metho ds could b e applied to set and In man y learning algorithms the training data pla y t w o separate roles Not only do they help de ne the ob jectiv e function J h but they also help de ne the h yp othesis h In the k nearest neigh bor algorithm kNN for example h x is de ned in terms of the k training data p oin ts nearest to x The parameter k is c hosen to minimize J k where J is the lea v eoneout crossv alidation estimate of the loss In this setting w e can no w consider t w o di eren t roles for the auxiliary data First when c ho osing k w e can include the aux iliary data in the ob jectiv e function as J k Second w e can include the auxiliary data in the set of p oten tial neigh bors In other w ords the auxiliary data can be used both to ev aluate a candidate classi er using J and also to de ne the classi er T o include the auxiliary data in the set of poten tial neigh bors w e found it b est to separately compute the K p nearest primary neigh b ors and the K a nearest aux iliary neigh b ors and then tak ea w eigh ted com bination of the v otes of these neigh b ors Sp eci cally let V p c and V a c bethen um ber of v otes for class c from the primary and auxiliary nearest neigh bors Then the o v erall v ote for class c is de ned as The parameter con trols the relativ e imp ortance of the t w o t yp es of neigh bors If then only the primary nearest neigh b ors are v oting if and K p K a then equal imp ortance is giv en to primary and auxiliary neigh bors and if then only the auxiliary neigh b ors determine the classi cation The This can b e simpli ed in t w ow a ys First w e can re mo v e the auxiliary training examples from the con strain ts b y deleting the second set of constrain ts in v olving a and setting C a This giv es an LPSVM in whic h the auxiliary examples are only used as sup port v ectors This increases the expressiv e po w er of the classi er but it is still trained only to classify the primary examples correctly Alternativ ely w e can k eep the constrain ts but delete the auxiliary examples from the set of candidate sup port v ectors b y deleting all terms in v olving constrain ts and the ob jectiv e function The resulting SVM will b e de ned using only primary training exam ples as supp ort v ectors but it will ha v e b een trained to classify b oth primary and auxiliary examples w ell In the remainder of this pap er w ewill ev aluate exp er imen tally whic h of these three con gurations b oth supp ortv ectors only and constrain tsonly giv es the b est results on our isolated leaf classi cation problem Figure sho ws examples of isolated lea v es and Herbar ium sp ecimens Rather than extract feature v ectors w e compare leaf shap es to one another directly as fol lo ws First eac h image is thresholded to obtain a bi nary image for plan t pixel and otherwise Then the b oundary of eac hregionis tra v ersed and the shap e of the b oundary is con v erted in to a sequence of lo cal curv atures Let x j y j b e the co ordinates of the j th poin t on the b oundary of a region De ne angle j as the angle b et w een the line segmen ts x j y j x angles forms a lo op T o compare t w olea v es w e apply dynamic programming algorithms to align their an gle sequences and compute a distance b et w een them Similar edit distance metho ds ha v e b een applied man y times in pattern recognition and bioinformatics Durbin et al Milios P etrakis P etrakis et al W e emplo y three di eren t dynamic programming al gorithms The rst algorithm is applied to compare t w o isolated lea v es Let gle sequence of the rst leaf and b e the angle sequence of the second leaf W e will du plicate the angle sequence of the second leaf so that j angles but that the matc h ma y stop b efore it has matc hed all of the angles or it ma y wrap around and matc h some s t wice The nal matc hing score This rst dynamic programming algorithm w orks w ell for comparing isolated lea v es but it w orks v ery badly for comparing isolated lea v es to herbarium samples or herbarium samples to eac h other The problem is that a region of an herbarium sample can be v ery large and con tain m ultiple o v erlapping lea v es W e decided therefore to use our isolated training exam ples as templates to iden tify parts of the herbarium samples that are most lik ely to corresp ond to a sin gle leaf Sp eci cally w e tak e eac h isolated training example and matc hittoeac h segmen t of eac h herbar ium sample of the same sp ecies The purp ose of this matc h is to nd the longest con tiguous partial matc h of the isolated leaf against some part of the herbarium sample This partial matc hwillbe calledan herb arium se gment and it will pla y the role of the auxiliary train ing data in our exp erimen ts The pro cess is illustrated in Figure The dynamic program for extracting herbarium seg men ts w orks as follo ws Let sequence of angles extracted from the isolated leaf and from one connected region of an herbarium sample of the same sp ecies Let S be a N M matrix of costs A matc h will consist of a path that starts at an y arbi trary p oin t i s j s inthelo w er left N M matrix and terminates at some arbitrary p oin t i e j e abo v eand to the righ t S is lled according to the rule As b efore d ij i j is the cost of matc hing i to j W is the cost of a horizon tal mo v e skipping training sets of size m there are tinct training sets so w e rep ort the error rate a v eraged o v er all of these In eac h run the auxiliary data is obtained b y matc hing the isolated examples in the training set against all re gions of all herbarium samples from the same sp ecies Because the parameter is sensitiv e to tuning and the length ratio constrain t is strict only out of matc hing pro cesses pro duces a usable hebarium seg men t Th us for eac h primary training set w eha v ean auxiliary data set roughly times as large Figure sho ws the learning curv es for kNN In all cases the v alues of K p the n um b er of primary nearest neigh bors K a the n um b er of auxiliary nearest neigh b ors and the mixing co ecien t w ere set to opti mize a lexicographical ob jectiv e function consisting of four quan tities The most imp ortan tquan tit yw as the lea v eoneout n um ber of isolated examples misclassi ed Ties w ere then brok en b y considering the lea v e oneout n um ber of herbarium segmen ts misclassi ed Remaining ties w ere brok en to reduce the error mar gin n um ber of v otes for the winning class n um ber of v otes for the correct class on the isolated examples and nally to reduce the error margin on the herbar ium samples The gure sho ws that for small sam ples mixing the herbarium examples with the isolated training examples giv es better p erformance but the di erences are not statistically signi can t If w e clas sify isolated test examples using only the herbarium segmen ts the results are signi can tly w orse for small training sets Figure sho ws the v alues c hosen for the mixing pa T o apply SVMs w em ust rst con v ert the edit distance computed b y the dynamic programming algorithms in to a k ernel similarit y function W e emplo y ed the simple transformation K x i x j edit distance Ho w ev er it should be noted that this k ernel is not a Mercer k ernel First of all it is not symmetric K x i x j K x j x i b ecause the dynamic program ming algorithm do es not treat the t w o angle sequences iden tically one is required to wrap around exactly while the other is not Second w ev eri ed that some of the eigen v alues of the k ernel matrix are negativ e whic h w ould not be true for a Mercer k ernel The practical consequences of this are not clear and other authors ha v e found that empirical k ernels of this sort w ork v ery w ell Bahlmann et al Ho w ev er from a theoretical standp oin t unless a k ernel is a Mer cer k ernel there is no equiv alen t higherdimensional space in whic h the learned decision b oundary is a maxim ummargin h yp erplane Cristianini Sha w e T a ylor There are nine p ossible con gurations for our LP SVMs The constrain ts can include only isolated lea v es only herbarium segmen ts or b oth The supp ort v ectors can include only isolated lea v es only herbar ium segmen ts or b oth Figure plots learning curv es for these nine con g urations W e note that rst the o v erall b est con guration is to com bine mixed constrain ts and mixed supp ort v ectors In short the auxiliary data are use ful b oth for represen ting the classi er and for training the classi er Second for samples of size it is v ery imp ortan ttoha v e b oth mixed constrain ts and mixed supp ort v ectors This is exactly what is predicted b ya biasv ariance analysis Small samples ha v e high v ari ance so it is b etter to mix in the auxiliary data to reduce the v ariance ev en if this in tro duces some bias Third for samples of size and it is v ery im p ortan t to ha v e mixed constrain ts but it is OK to use just isolated training examples as supp ort v ectors Hence the auxiliary data is still imp ortan t One p os sible explanation is that examples p er sp ecies is still not enough data to eliminate the need for auxiliary training data This is supp orted b ythe kNN exp eri men ts where the b est v alue w as only ev en with examples p er class T o assess the statistical signi cance of the results w e applied McNemars test to p erform pairwise compar isons of v arious con gurations These comparisons con rm that the three trends men tioned ab o v e are sta tistically signi can t gorithm since our kNN algorithm handles the pri mary and auxiliary data separately F or LPSVMs histogram equalization had no statistically signi can t e ect on either the error rates or the relativ e merits of the di eren t con gurations The b est con gura tion is still the mixedconstrain tsmixedSV con gura tion W e did nd ho w ev er that histogram equaliza tion c hanged the n um ber of support v ectors found b y the LPSVM A t a sample size of histogram equal ization cuts the n um b er of herbarium supp ort v ectors b y more than half and doubles the n um b er of isolated supp ort v ectors A t a sample size of the n um ber of isolated supp ort v ectors is unc hanged but the n um ber of herbarium supp ort v ectors is reduced b y roughly an order of magnitude An explanation for this is that with the outlier herbarium segmen ts reduced b yhis togram equalization few er herbarium supp ort v ectors w ere needed to t them Ho w ev er since test set p er formance is measured strictly on isolated lea v es this reduction in herbarium supp ort v ectors has relativ ely little impact on the error rate Another e ect of histogram equalization w as to c hange the relativ e sizes of C p and C a the complexit y con trol parameters of the LPSVM Figure plots the ratio C p C a Without histogram equalization w e can see that C p w as m uc h larger than C a for sample sizes greater than so m uc hmorew eigh tw as b eing placed on tting the primary training examples than on t ting the auxiliary ones With histogram equalization the ratio sta ys closer to whic h indicates that roughly equal w eigh tw as b eing placed on primary and auxil iary training examples This pap er has describ ed a metho dology for exploit ing sources of auxiliary training data within the kNN and LPSVM learning algorithms W e ha v e sho wn that auxiliary data dra wn from a di eren t distribution than the primary training and test data can signi can tly impro v e accuracy F or the LPSVM Figure sho ws that when training on only example p er class auxiliary data reduces the error rate from to a reduction of nearly When training on examples p er class the error rate decreases from to a reduction of This pap er has also sho wn ho w SVMs can b e trained to classify ob jects based on shap e b y using b oundary curv ature edit distances as a kind of k ernel function By using separate C p and C a parameters in the SVM w e can adjust the relativ e imp ortance of tting the t w o data sources Edit distances ha v e b een used with the kNN classi er for man yy ears Our results suggest that for misclassi cation There are sev eral in teresting directions to pursue for exploiting auxiliary data in Ba y esian net w ork classi ers Bahlmann C Haasdonk B Burkhardt H 
Online handwriting recognition with supp ort v ec tor mac hinesa k ernel approac h th International
Workshop on F r ontiers in Handwriting R e c o gnition pp Los Alamitos CA IEEE Bennett K Demiriz A Semisup ervised supp ort v ector mac hines A dvanc es in Neur al Infor mation Pr o c essing Systems pp MIT Press Blum A Mitc hell T Com bining lab eled and unlab eled data with cotraining Pr o c th A nnu Conf on Comput L e arning The ory pp A CM Press New Y ork NY Clark P Mat win S Using qualitativ emod els to guide inductiv e learning Machine L e arning
Pr o c e e dings of the T enth International Confer enc e pp San F rancisco CA Morgan Kaufmann Cristianini N Sha w eT a ylor J A n in tr o duction to supp ort ve ctor machines and other kernelb ase dle arning metho ds Cam bridge Univ er sit y Press Durbin R Eddy S Krogh A Mitc hison G Biolo gic al se quenc e analysis Pr ob abilistic mo dels of pr oteins and nucleic acids Cam bridge Cam bridge Univ ersit y Press Graep el T Herbric h R Sc holk opf B Smola A Bartlett P Rob ertMuller K Ob erma y er K
Williamson B Classi cation on pro ximit y data with LPmac hines Pr o c e e dings of the Ninth
International Confer enc e on A rti cial Neur al Net works pp Mangasarian O Generalized supp ort v ector mac hines In A J Smola P L Bartlett B Sc hlk opf and D Sc h uurmans Eds A dvanc es in lar ge mar gin classi ers Cam bridge MA MIT Press Milios E P etrakis E Shap e retriev al based on dynamic programming IEEE T r ansactions on Image Pr o c essing P etrakis E Diplaros A Milios E Matc h ing and retriev al of distorted and o ccluded shap es
