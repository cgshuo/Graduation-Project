 Kernel methods provide a principled framework for nonlinear data modeling, where the inference in the input space can be transferred intactly to any feature space by simply treating the associ-ated kernel as inner products, or more generally, as nonlinear mappings on the data (Sch  X  olkopf &amp; Smola, 2002). Some well-known kernel methods include support vector machines (SVMs) (Vap-nik, 2000), kernel principal component analysis (kernel PCA) (Sch  X  olkopf et al., 1998), and kernel k -means (Shawe-Taylor &amp; Cristianini, 2004). Naturally, an important issue in kernel methods is kernel design. Indeed, the performance of a kernel method depends crucially on the kernel used, where different choices of kernels often lead to quite different results. Therefore, substantial efforts have been made to design appropriate kernels for the problems at hand. For instance, in (Chapelle &amp; Vapnik, 2000), parametric kernel functions are proposed, where the focus is on model selection (Chapelle &amp; Vapnik, 2000). The modeling capability of parametric kernels, however, is limited. A more natural idea is to learn specialized nonparametric kernels for specific problems. For instance, in cases where only inner products of the input data are involved, kernel learning is equivalent to the learning of a kernel matrix. This is the main focus of recent kernel methods.
 Currently, many different kernel learning frameworks have been proposed. These include spectral kernel learning (Li &amp; Liu, 2009), multiple kernel learning (Lanckriet et al., 2004), and the Breg-man divergence-based kernel learning (Kulis et al., 2009). Typically, a kernel learning framework consists of two main components: the problem formulation in terms of the kernel matrix, and an optimization procedure for finding the kernel matrix that has certain desirable properties. As seen in, e.g., the Maximum Variance Unfolding (MVU) method (Weinberger et al., 2004) for nonlinear dimensionality reduction (see (So, 2007) for related discussion) and Pairwise Constraint Propaga-tion (PCP) (Li et al., 2008) for constrained clustering, a nice feature of such a framework is that the problem formulation often becomes straightforward. Thus, the major challenge in optimization-based kernel learning lies in the second component, where the key is to find an efficient procedure to obtain a positive semidefinite kernel matrix that satisfies certain properties.
 Using the kernel trick, most kernel learning problems (Graepel, 2002; Weinberger et al., 2004; semidefinite programs (SDPs). Although in theory SDPs can be efficiently solved, the high computa-tional complexity has rendered the SDP approach unscalable. An effective and widely used heuristic for speedup is to perform low-rank kernel approximation and matrix factorization (Weinberger et al., 2005; Weinberger et al., 2007; Li et al., 2009). In this paper, we investigate the possibility of further speedup by studying a class of convex quadratic semidefinite programs (QSDPs). These QSDPs arise in many contexts, such as graph Laplacian regularized low-rank kernel learning in nonlinear dimensionality reduction (Sha &amp; Saul, 2005; Weinberger et al., 2007; Globerson &amp; Roweis, 2007; Song et al., 2008; Singer, 2008) and constrained clustering (Li et al., 2009). In the aforementioned papers, a QSDP is reformulated as an SDP with O ( m 2 ) variables and a linear matrix inequality of size O ( m 2 )  X  O ( m 2 ) . Such a reformulation is highly inefficient and unscalable, as it has an order of m 9 time complexity (Ben-Tal &amp; Nemirovski, 2001, Lecture 6). In this paper, we propose a novel reformulation that exploits the structure of the QSDP and leads to a semidefinite-quadratic-linear program (SQLP) that can be solved by the standard software SDPT3 (T  X  ut  X  unc  X  u et al., 2003). Such a reformulation has the advantage that it only has one positive semidefinite constraint on a matrix of size m  X  m , one second-order cone constraint of size O ( m 2 ) and a number of linear constraints on O ( m 2 ) variables. As a result, our reformulation is much easier to process numerically. Moreover, a simple complexity analysis shows that the gain in speedup over previous approaches is at least previous ones.
 The rest of the paper is organized as follows. We review related kernel learning problems in Section 2 and present our formulation in Section 3. Experiment results are reported in Section 4. Section 5 concludes the paper. In this section, we briefly review some kernel learning problems that arise in dimensionality re-duction and constrained clustering. They include MVU (Weinberger et al., 2004), Colored MVU (Song et al., 2008), (Singer, 2008), Pairwise Semidefinite Embedding (PSDE) (Globerson &amp; Roweis, 2007), and PCP (Li et al., 2008). MVU maximizes the variance of the embedding while preserving local distances of the input data. Colored MVU generalizes MVU with side information such as class labels. PSDE derives an embedding that strictly respects known similarities, in the sense that objects known to be similar are always closer in the embedding than those known to be dissimilar. PCP is designed for constrained clustering, which embeds the data on the unit hypersphere such that two objects that are known to be from the same cluster are embedded to the same point, while two objects that are known to be from different clusters are embedded orthogonally. In particular, PCP seeks the smoothest mapping for such an embedding, thereby propagating pairwise constraints. Initially, each of the above problems is formulated as an SDP, whose kernel matrix K is of size n  X  n , where n denotes the number of objects. Since such an SDP is computationally expensive, one can try to reduce the problem size by using graph Laplacian regularization. In other words, one takes K = QY Q T , where Q  X  R n  X  m consists of the smoothest m eigenvectors of the graph Laplacian ( m  X  n ), and Y is of size m  X  m (Sha &amp; Saul, 2005; Weinberger et al., 2007; Song et al., 2008; Globerson &amp; Roweis, 2007; Singer, 2008; Li et al., 2009). The learning of K is then reduced to the learning of Y , leading to a quadratic semidefinite program (QSDP) that is similar to a standard quadratic program (QP), except that the feasible set of a QSDP resides in the positive semidefinite cone as well. The intuition behind this low-rank kernel approximation is that a kernel matrix of the form K = QY Q T actually, to some degree, preserves the proximity of objects in the feature space. Detailed justification can be found in the related work mentioned above.
 Next, we use MVU and PCP as representatives to demonstrate how the SDP formulations emerge from nonlinear dimensionality reduction and constrained clustering. 2.1 MVU The SDP of MVU (Weinberger et al., 2004) is as follows: where K = ( k ij ) denotes the kernel matrix to be learned, I denotes the identity matrix, tr (  X  ) denotes the trace of a square matrix,  X  denotes the element-wise dot product between matrices, d ij denotes the Euclidean distance between the i -th and j -th objects, and N denotes the set of neighbor pairs, whose distances are to be preserved in the embedding.
 The constraint in (2) centers the embedding at the origin, thus removing the translation freedom. The constraints in (3) preserve local distances. The constraint K  X  0 in (4) specifies that K must be symmetric and positive semidefinite, which is necessary since K is taken as the inner product is characterized by V ( K ) = 1 2 n related discussion in (So, 2007), Chapter 4). Thus, the SDP in (1-4) maximizes the variance of the embedding while keeping local distances unchanged. After K is obtained, kernel PCA is applied to K to compute the low-dimensional embedding. 2.2 PCP The SDP of PCP (Li et al., 2008) is: where  X  L denotes the normalized graph Laplacian, M denotes the set of object pairs that are known to be from the same cluster, and C denotes those that are known to be from different clusters. The constraints in (6) map the objects to the unit hypersphere. The constraints in (7) map two objects that are known to be from the same cluster to the same vector. The constraints in (8) map two objects that are known to be from different clusters to vectors that are orthogonal. Let X = { x i } n i =1 be the data set, F be the feature space, and  X  : X  X  F be the associated feature map of K . Then, the degree of smoothness of  X  on the data graph can be captured by (Zhou et al., 2004): where w ij is the similarity of x i and x j , d ii = The smaller the value S (  X  ) , the smoother is the feature map  X  . Thus, the SDP in (5-9) seeks the smoothest feature map that embeds the data on the unit hypersphere and at the same time respects the pairwise constraints. After K is solved, kernel k -means is then applied to K to obtain the clusters. 2.3 Low-Rank Approximation: from SDP to QSDP The SDPs in MVU and PCP are difficult to solve efficiently because their computational complexity scales at least cubically in the size of the matrix variable and the number of constraints (Borchers, 1999). A useful heuristic is to use low-rank kernel approximation, which is motivated by the obser-vation that the degree of freedom in the data is often much smaller than the number of parameters in a fully nonparametric kernel matrix K . For instance, it may be equal to or slightly larger than the intrinsic dimension of the data manifold (for dimensionality reduction) or the number of clusters (for clustering). Another more specific observation is that it is often desirable to have nearby objects mapped to nearby points, as is done in MVU or PCP.
 Based on these observations, instead of learning a fully nonparametric K , one learns a K of the form K = QY Q T , where Q and Y are of sizes n  X  m and m  X  m , respectively, where m  X  n . The matrix Q should be smooth in the sense that nearby objects in the input space are mapped to nearby points (the i -th row of Q is taken as a new representation of x i ). Q is computed prior to the learning of K . In this way, the learning of a kernel matrix K is reduced to the learning of a much smaller Y , subject to the constraint that Y  X  0 . This idea is used in (Weinberger et al., 2007) and (Li et al., 2009) to speed up MVU and PCP, respectively, and is also adopted in Colored MVU (Song et al., 2008) and PSDE (Globerson &amp; Roweis, 2007) for efficient computation.
 The choice of Q can be different for MVU and PCP. In (Weinberger et al., 2007), Q = ( v Q = ( u 1 , . . . , u m ) , where { u i } are the eigenvectors of the normalized graph Laplacian. Since such Q  X  X  are obtained from graph Laplacians, we call the learning of K of the form K = QY Q T the Graph Laplacian Regularized Kernel Learning problem, and denote the methods in (Weinberger et al., 2007) and (Li et al., 2009) by RegMVU and RegPCP, respectively.
 With K = QY Q T , RegMVU and RegPCP become: where  X  &gt; 0 is a regularization parameter and S = { ( i, j, t ij ) | ( i, j )  X  M X  X  , or i = j, t ij = 1 if ( i, j )  X  M or i = j, t ij = 0 if ( i, j )  X  C} . Both RegMVU and RegPCP can be succinctly rewritten in the unified form: where y = vec ( Y )  X  R m 2 denotes the vector obtained by concatenating all the columns of Y , and A  X  0 (Weinberger et al., 2007; Li et al., 2009). Note that this problem is convex since both the objective function and the feasible set are convex.
 Problem (13-14) is an instance of the so-called convex quadratic semidefinite program (QSDP), where the objective is a quadratic function in the matrix variable Y . Note that similar QSDPs arise in Colored MVU, PSDE, Conformal Eigenmaps (Sha &amp; Saul, 2005), Locally Rigid Embedding (Singer, 2008), and Kernel Matrix Completion (Graepel, 2002). Before we present our approach for tackling the QSDP (13-14), let us briefly review existing approaches in the literature. 2.4 Previous Approach: from QSDP to SDP Currently, a typical approach for tackling a QSDP is to use the Schur complement (Boyd &amp; Vanden-berghe, 2004) to rewrite it as an SDP (Sha &amp; Saul, 2005; Weinberger et al., 2007; Li et al., 2009; Song et al., 2008; Globerson &amp; Roweis, 2007; Singer, 2008; Graepel, 2002), and then solve it using this approach the Schur Complement Based SDP (SCSDP) formulation. For the QSDP in (13-14), the equivalent SDP takes the form: variable serving as an upper bound of y T A y . The second semidefinite cone constraint is equivalent to ( A 1 2 y ) T ( A 1 2 y )  X   X  by the Schur complement.
 Although the SDP in (15-16) has only m ( m + 1) / 2 + 1 variables, it has two semidefinite cone constraints, of sizes m  X  m and ( m 2 +1)  X  ( m 2 +1) , respectively. Such an SDP not only scales poorly, but is also difficult to process numerically. Indeed, by considering Problem (15-16) as an SDP in the standard dual form, the number of iterations required by standard interior-point algorithms is of the order m , and the total number of arithmetic operations required is of the order m 9 (Ben-Tal &amp; Nemirovski, 2001, Lecture 6). In practice, it takes only a few seconds to solve the aforementioned SDP when m = 10 , but can take more than 1 day when m = 40 (see Section 4 for details). Thus, it is not surprising that m is set to a very small value in the related methods X  X or example, m = 10 in (Weinberger et al., 2007) and m = 15 in (Li et al., 2009). However, as noted by the authors in (Weinberger et al., 2007), a larger m does lead to better performance. In (Li et al., 2009), the authors suggest that m should be larger than the number of clusters.
 Is this formulation from QSDP to SDP the best we can have? The answer is no. In the next section, we present a novel formulation that leads to a semidefinite-quadratic-linear program (SQLP), which is much more efficient and scalable than the one above. For instance, it takes about 15 seconds when m = 30 , 2 minutes when m = 40 , and 1 hour when m = 100 , as reported in Section 4. In this section, we formulate the QSDP in (13-14) as an SQLP. Though our focus here is on the QSDP in (13-14), we should point out that our method applies to any convex QSDP.
 Recall that the size of A is m 2  X  m 2 . Let r be the rank of A . With Cholesky factorization, we can obtain an r  X  m 2 matrix B such that A = B T B , as A is symmetric positive semidefinite and of rank r (Golub &amp; Loan, 1996). Now, let z = B y . Then, the QSDP in (13-14) is equivalent to: Next, we show that the constraint in (19) is equivalent to a second-order cone constraint. Let K n be the second-order cone of dimension n , i.e., where k X k denotes the standard Euclidean norm. Let u = ( 1+  X  2 , 1  X   X  2 , z T ) T . Then, the following holds.
 Theorem 3.1. z T z  X   X  if and only if u  X  X  r +2 .
 ( e 1  X  e 2 ) T u =  X  , ( e 1 + e 2 ) T u = 1 , and z = C u . Hence, by Theorem 3.1, the problem in (17-20)
Figure 1: Swiss Roll. (a) The true manifold. (b) A set of 2000 points sampled from the manifold. is equivalent to: we have traded the semidefinite cone constraint of size ( m 2 + 1)  X  ( m 2 + 1) in (16) with one second-order cone constraint of size r + 2 and r + 1 linear constraints. As it turns out, such a formulation is much easier to process numerically and can be solved much more efficiently. Indeed, using standard interior-point algorithms, the number of iterations required is of the order Tal &amp; Nemirovski, 2001, Lecture 6), and the total number of arithmetic operations required is of the of the SCSDP approach, and our experimental results indicate that the speedup in computation is quite substantial. Moreover, in contrast with the SCSDP formulation, which does not take advantage of the low rank structure of A , our formulation does take advantage of such a structure. In this section, we perform several experiments to demonstrate the viability of our SQLP formulation and its superior computational performance. Since both the SQLP formulation and the previous SCSDP formulation can be solved by standard softwares to a satisfying gap tolerance, the focus in this comparison is not on the accuracy aspect but on the computational efficiency aspect. We set the relative gap tolerance for both algorithms to be 1e-08. We use SDPT3 (Toh et al., 2006; use CSDP 6.0.1 (Borchers, 1999) to solve the SCSDP. All experiments are conducted in Matlab 7.6.0(R2008a) on a PC with 2.5GHz CPU and 4GB RAM.
 ure 1(a)) is a standard manifold model used for manifold learning and nonlinear dimensionality reduction. In the experiments, we use the data set shown in Figure 1(b), which consists of 2000 points sampled from the Swiss Roll manifold. USPS is a handwritten digits database widely used for clustering and classification. It contains images of handwritten digits from 0 to 9 of size 16  X  16 , and has 7291 training examples and 2007 test examples. In the experiments, we use a subset of USPS with 2000 images, containing the first 200 examples of each digit from 0 -9 in the training data. The feature to represent each image is a vector formed by concatenating all the columns of the image intensities. In the sequel, we shall refer to the two subsets used in the experiments simply as Swiss Roll and USPS. The Swiss Roll (resp. USPS) is used to derive the QSDP in RegMVU (resp. RegPCP). For RegMVU, the 4 NN graph is used, i.e., w ij = 1 if x i is within the 4NN of x j or vice versa, and w ij = 0 otherwise. We verified that the 4NN graph derived from our Swiss Roll data is connected. For RegPCP, we construct the graph following the approach suggested in (Li et al., 2009). Specifically, Here,  X  is the averaged distance from each object to its 20 -th nearest neighbor. For the pairwise constraints used in RegPCP, we randomly generate 20 similarity constraints for each class, and 20 dissimilarity constraints for every two classes, yielding a total of 1100 constraints. For each data set, are formed. Each QSDP gives rise to one SQLP and one SCSDP. Therefore, for each data set, 11 SQLPs and 11 SCSDPs are derived. 4.1 The Results The computational results of the programs are shown in Tables 1 and 2. For each program, we report the total computation time, the number of iterations needed to achieve the required tolerance, and the average time per iteration in solving the program. A dash ( X ) in the box indicates that the corresponding program takes too much time to solve. We choose to stop the program if it fails to converge within 1 day. This happens for the SCSDP with m = 40 on both data sets.  X From the tables, we see that solving an SQLP is consistently much more faster than solving an SCSDP. To see the scalability, we plot the solution time (Time) against the problem size ( m ) in Figure 2. It can be seen that the solution time of the SCSDP grows much faster than that of the SQLP. This demonstrates the superiority of our proposed approach. We also note that the per-iteration computational costs of SCSDP and SQLP are drastically different. Indeed, for the same problem size m , it takes much less time per iteration for the SQLP than that for the SCSDP. This is not very surprising, as the SQLP formulation takes advantage of the low rank structure of the data matrix A . We have studied a class of convex optimization programs called convex Quadratic Semidefinite Programs (QSDPs), which arise naturally from graph Laplacian regularized kernel learning (Sha &amp; Saul, 2005; Weinberger et al., 2007; Li et al., 2009; Song et al., 2008; Globerson &amp; Roweis, 2007; Singer, 2008). A QSDP is similar to a QP, except that it is subject to a semidefinite cone constraint as well. To tackle the QSDP, one typically uses the Schur complement to rewrite it as an SDP (SCSDP), thus resulting in a large linear matrix inequality constraint. In this paper, we argue that this formulation is not computationally optimal and have proposed a novel formulation that leads to a semidefinite-quadratic-linear program (SQLP). Our formulation introduces one positive semidef-inite constraint, one second-order cone constraint and a set of linear constraints. This should be contrasted with the two large semidefinite cone constraints in the SCSDP. Our complexity analysis and experimental results have shown that the proposed SQLP formulation scales far better than the SCSDP formulation.
 The authors would like to thank Professor Kim-Chuan Toh for his valuable comments. This re-search work was supported in part by GRF grants CUHK 2150603, CUHK 414307 and CRF grant CUHK2/06C from the Research Grants Council of the Hong Kong SAR, China, as well as the NSFC-RGC joint research grant N CUHK411/07.

