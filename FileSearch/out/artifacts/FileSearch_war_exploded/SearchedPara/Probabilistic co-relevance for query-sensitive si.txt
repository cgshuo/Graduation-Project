 1. Introduction
Interdocument similarity is a major factor affecting the enhancement of cluster-based retrieval because it is one of the metric for inter-document similarity where a document is represented as terms and the similarities between documents then straight-forwardly derived by applying matching functions or retrieval models. They include the Dice coefficient and model in language modeling approaches ( Hiemstra, 1998; Lafferty &amp; Zhai, 2001; Ponte &amp; Croft, 1998 ). However, it is not known whether term-based similarity is the most suitable metric in terms of the cluster hypothesis. not specify details of the type of interdocument similarities. Therefore, we cannot assume that interdocument similarities ter satisfy the hypothesis.
The inspiration for this new method was provided by Tombros and Rijsbergen (2001, 2004) , who argued that a form of between two documents. This similarity metric makes one pair of documents more similar than others, when both are more of top-retrieved documents for each query ( Kurland &amp; Lee, 2005 ).

In this pioneering work of Tombros and Rijsbergen (2001, 2004) , query-sensitive similarity was only explored in the set-ting of a vector-space model based on cosine similarity. However, most modern retrieval methods are based on probabilistic nect the derived similarity with the cluster hypothesis in a formal manner.
 of a document is dependent on others. We then integrate the resulting estimations from both cases and decompose the co-retrieved documents without resorting to the actual relevant documents. Specifically, we applied language modeling meth-relevance feedback method employed by language modeling approaches.

The results of experiments carried out with standard TREC collections consistently show that the proposed query-specific similarity significantly outperforms the state-of-the-art method developed by Tombros and Rijsbergen (2001) in the setting an improvement over existing metrics.

This paper is organized as follows. Section 2 briefly reviews related works. Section 3 presents our proposed query-sensi-our conclusions and future work. 2. Related work Cluster-based retrieval is an approach that generates clusters from collections in order to enhance retrieval performance. have delivered inconclusive results; however, recent works based on language modeling methods indicate a significant improvement over the baseline retrieval method ( Liu &amp; Croft, 2004; Kurland &amp; Lee, 2004 ). documents in a collection rather than whole documents. However, Willett (1985)  X  X  experiments reported only a limited per-ined the use of query-specific clusters in the setting of an enhanced user interface where a user could choose the best relevant cluster from those recommended by a system. They showed that the retrieval effectiveness can be improved sub-stantially with the interface.

Tombros et al. (2002) further investigated query-specific clusters, but with hierarchical clusters based on four widely used merging methods: single linkage, complete linkage, group average, and Ward X  X  method. They concluded that query-specific clusters showed much better potential effectiveness compared to static clusters.
 2.1. Query-sensitive similarity
Query-sensitive similarity was originally motivated by query-dependency of co-relevance, the revised assumption of the which is stated as follows:  X  X  ... we do not assume that if two documents D Q , they must both be relevant or nonrelevant for query Q B
Tombros and Rijsbergen (2001) noted that this assumption concludes that optimal interdocument similarity should be defined in terms of a query-sensitive function. Based on this insight, they proposed the use of query-sensitive similarity gested that this form of query-sensitive similarity better satisfies the cluster hypothesis. were not based on a probabilistic framework because these formulations were limited to a vector space model. Therefore, it vated the major theme of our present research where we aim to incorporate the definition of query-specific similarity in a probabilistic framework.
 More recently, Fuhr, Lechtenfeld, Stein, and Gollub (2011) also reversed the cluster hypothesis and proposed the Optimal Clustering Framework (OCF), which was aimed at establishing a  X  X rinciple X  for document clustering in a similar way to the that consists of co-relevance probabilities for documents with a more general form, which they generated by summing them
In addition, it should be noted that it becomes harder to estimate the co-relevance probability when we focus on the data for a single query. The combination with term-based similarity, which was used in our method, is one way of handling the addressed estimation problem. Some new directions might possibly produce an improved similarity metric by combin-rion as a backbone term-based similarity for a query-sensitive criterion. 3. Utilizing probabilistic co-relevance for query-sensitive interdocument similarity tion. We then present an approximation for co-relevance probability using probabilistic relevance models that are specifi-cally based on language modeling approaches. 3.1. An illustrating example of a query-sensitive similarity metric
In query-sensitive similarity metric, we place more importance on terms which belong to on a query topic, i.e., we bias a ged but prefixed in a given collection, such as the inverse document frequency.

In this section, we illustrate an example that motivates the need for a query-sensitive metric. As was demonstrated in the similarity between two documents is computed by the number of matching terms common to both.
Now, suppose that examples of queries q 1 and q 2 and documents d q 1 : a q 2 : b d 1 : ab d 2 : ad d 3 : bc
Given query q 1 , d 1 and d 2 are relevant, but d 3 is not relevant. Thus, the d d and d 3 pair. However, the similarity of d 1 and d 3 is the same as that of d should be given more importance when computing the similarity between two documents. Given this query bias, d comes more similar to d 2 than d 3 , since both d 1 and d is not a query term. As a result, it is likely that both d sitive similarity, rather than using static similarity for cluster-based retrieval.

In the next subsection, we present the proposed co-relevance principle for interdocument similarity. 3.2. Co-relevance principle for interdocument similarity adopt the following assumption made by Hearst and Pedersen (1996) , which states that the co-relevance of two documents is not a static because it differs depending on the information required.

Query dependency of co-relevance ( Hearst &amp; Pedersen, 1996 ): The co-relevance of two documents depends on the query Q B .

Given the assumption of query dependency of co-relevance, as stated above, what is the best form of interdocument sim-assigned with a maximum similarity when they are co-relevant to a query, but zero otherwise. This kind of co-relevance-based optimal similarity might be obtained, provided we know the set of all documents relevant to a query. Unfortunately, evance for two documents. Without computing the exact quantity of the degree of co-relevance, we must focus on estimat-ing the probability that two documents are co-relevant to a given query, which we call co-relevance probability . Assuming the similarity metric principle .

Co-relevance principle for interdocument similarity (similarity metric principle): For a given query, the interdocument similarity between two documents that best satisfies the cluster hypothesis should be proportional to the probability that they are co-relevant to the query.

We now formally present the co-relevance principle. Let C be the set of documents in the collection. N is the total number relevant). Thus, the co-relevance principle is formally presented as follows.
This posits a form of query-sensitive similarity. 3.3. Estimation of the co-relevance probability
The estimation of Eq. (1) requires a subset of relevant documents for a given query q , if there is no simplifying approx-
Using the chain rule, P ( corel j d , d 0 , q ) is decomposed to: q . for a given query q .

As is based by the cluster hypothesis, the relevance of a document is dependent on those of its similar documents. d 0 are similar, we need to use the dependence between the relevance events of given two documents d and d 0 to decompose the cluster hypothesis.
However, the proposition that two documents d and d 0 are similar or not is not pregiven , which means, we cannot decide which we choose for further derivation, due to the independence or dependence assumption. In our estimation, a compro-mising method is used by taking both approximations derived from two different cases and integrating them into the final probability.
 is relevant to a query, given d is relevant to the query. case: where we temporarily introduce Q as a random variable to explicitly indicate that d is used as a query.
In the second case, were d and d 0 are not similar, we make an assumption of independent relevance. Given this simplifi-cation, Eq. (2) is estimated further as follows.
 for P ( corel j d , d 0 , q ), we suggest that we geometrically interpolate these estimates P mation. As a result, we obtain an estimate of P ( corel j d , d 0 , q ), as follows: venience, we introduce two notations, i.e., sim TSM ( d , d 0 ) and sim independent and dependent groups, respectively, which are defined as:
Using the notations sim TSM ( d , d 0 ) and sim QSSM ( d , d 0 , q ), Eq. (6) is now reformulated as: sim TSM ( d , d 0 ) which is known as term -based similarity , while the other is sim sitive similarity. Unless specified otherwise, query-sensitive similarity refers to either the combined metric sim general. 4 3.3.1. Estimation of the relevance probability 2003; Roelleke &amp; Wang, 2006 ). 5 As shown in Roelleke and Wang (2006) , we rewrite the relevance probability P ( r j d , q ) using Bayes X  rule as follows. which is a fraction of the probabilities of relevant and nonrelevant events. This is given as follows. p ( r j d , q ) can be represented by O ( r j d , q ) using the following relationship.

Depending on the use of query generation (language modeling approaches) or document generation (conventional probabi-found in Roelleke and Wang (2006) . 3.3.2. Language modeling approach tics of Lafferty and Zhai (2003) , as follows.
 ther derived as follows.
 scribe the generative model for relevant documents and nonrelevant documents, respectively.
As suggested by Hiemstra (1998, 2001), Miller, Leek, and Schwartz (1999), and Roelleke and Wang (2006) , we first regard P  X  w jC X  , as follows.
 2001 ), k d might be dependent or independent of d .
 follows.

Note that the derivations of Eqs. (15) and (16) employ the background collection model P  X  w jC X  to handle the relevance ment model. This duality of the collection model was also exploited by Lavrenko and Croft (2001) and Lavrenko (2010) when estimating the relevance and nonrelevance models. 6
We now complete the formulation of O ( r j d , q ). First, we decompose the odd set O ( r j d , q ) as follows. By substituting Eqs. (14) X (16) for Eq. (17) , the final form of O ( r j d , q ) is given as follows.
P ( d j r )or P  X  d j r  X  . 3.3.3. Special case: Dirichlet prior smoothing let prior smoothing, which a very popular smoothing method in language modeling because of its high effectiveness with short keyword queries ( Zhai &amp; Lafferty, 2001 ).

With Dirichlet-prior smoothing, k d is defined by l /( l + j d j ) using a smoothing parameter l . During Dirichlet-prior smoothing, the relevance score of a document d for a given query q is formulated as follows ( Zhai &amp; Lafferty, 2001 ).
Using S LM q  X  d  X  , we readily show that Eq. (18) can be rewritten as follows. This gives the following equivalent form for P ( r j d , q ) from Eq. (12) .
 cific similarity as follows:
Using the same derivation, we obtain the final form of the term-based similarity as follows. guage modeling approaches S LM q  X  d  X  . It is simple to show that this query-specific similarity is symmetric, i.e., sim ( d , d 0 )= sim ( d 0 , d ).

As a result, the query-specific similarity and the term-based similarity given by Eqs. (8) and (7) are precisely expressed using standard scoring functions in language modeling approaches S which possibly means that highly relevant documents have low relevance probabilities. To ameliorate the effect of query or queries has also used in the language modeling literature ( Kurland &amp; Lee, 2005; Lavrenko et al., 2002 ).
Formally, let the length-normalized query for q be ~ q . The term frequency for each word in query ~ q is then defined as follows.
P query, P  X  r j d ; ~ q  X  is reformulated as follows.
 which finally leads to the following length-normalized formula for sim 3.3.4. Simple setting for P(r) (6) , which leads us to make P ( r j d , q ) be proportional to O ( r j d , q ). This is referred to as the odd-based co-relevance model .
 Taking a log function of Eq. (28) provides the following variant.
 4. Experimentation
This section presents experiments that compare the proposed co-relevance-based similarity with term-based similarity. 4.1. Baseline: Tombros and Rijsbergen X  X  query-specific similarity
The original formula given by Tombros and Rijsbergen (2001) is based on a vector space model, which is simply referred abilistic models.
 score of document d to query q is then formulated in the vector space model as follows.
 as the similarity score between query q and the common document d d 0 , as follows.
By substituting Eq. (30) to Eq. (31) , the final generalized form of the TR similarity is given as follows.
The remaining problem is how to define the co-representation, i.e., the term frequency of w for the pseudo document as the geometric mean of the individual term frequencies of the word as follows. scheme, the weights of document d and query q are given as follows.
 the norms of the document and the query are given as follows. paper. 9
However, the ltc scheme has weak performance and is therefore considered to be far from a state-of-the-art vector space model. To produce a more convincing experimental result, it would be more useful to use a pivoted vector space model based 1996 ). According to Singhal (2001) , the pivoted vector space model is based on the following document and query weights. where W ( w , q ) in the pivoted vector space model is the same as Eq. (35) , j d j is defined by eter, and avgl is the average length of documents in the collection. formula of Eq. (33) .

Tombros and Rijsbergen (2004) further suggested a combined form of query-specific similarity defined above as and linear interpolation. Their experimental results showed that these variants delivered comparable performances. In this work, we use the linear interpolation, which is denoted as M3 in the work of Tombros and Rijsbergen (2004) . To complete the formalization, we define the term-based similarity sim query document d , using the same weighting function found in Eq. (30) .
The combined metric is defined as follows. where b is an interpolation parameter. 4.2. Experimental setting
In this evaluation we used two different standard TREC collections, i.e., ROBUST and WT10G. Table 1 shows the basic sta-tistics for each test collection, where NumDocs is the number of documents; NumRels is the total number of relevant docu-the range of topic numbers used for training and testing; and Qrylen is the average number of word occurrences in a query.
All experiments used the Lemur toolkit (version 4.12). We indexed English documents by performing standard prepro-cessing on queries and documents using Porter X  X  stemmer, and we removed stopwords using the standard INQUERY stoplist TR similarity defined in Section 4.1 .

The following is a summary of the baseline similarities that were compared in the experiments.  X  TSM VSM : Term-based similarity (TSM) based on a pivoted vector space model (i.e., Eq. (39) ).  X  TR-QSSM :Query-sensitive similarity (QSSM) proposed by Tombros and Rijsbergen (2001) (i.e., Eq. (32) ).  X  TR-TSM + QSSM : The combined metric of term-based similarity and query-sensitive similarity proposed by Tombros and Rijsbergen (2001) (i.e., Eq. (40) ).

The following is a summary of our proposed methods that were compared in the experiments.  X  TSM : Term-based similarity based on language modeling approaches (i.e., Eq. (27) ).  X  CoR-QSSM : The proposed (purely) query-sensitive similarity based on a co-relevance model without interpolation of term-based similarity (i.e., Eq. (26) ).  X  CoR-TSM + QSSM : The proposed combined similarity metric based on the co-relevance model (i.e., Eq. (9) with Eqs. (27) and (26) ).

For notation simplicity, we often omit the prefix  X  X  X oR X  X  when referring to our proposed method. 4.3. Evaluation measurement
Voorhees X  NNT was used as an evaluation measure. In the original definition, the NNT-based evaluation measure is de-fined as a fraction given by the number of co-relevant documents from the top five similar documents to a given relevant utilized in Tombros and Rijsbergen X  X  experiment. NNT is basically an evaluation method for a ranked list, so we used mean average precision (MAP), P5, and P10 (precision for 5 and 10 documents, respectively) for NNT, which we refer to as NNT-MAP, NNT-P5, and NNT-P10, respectively.

For all comparisons, we applied paired t -test at 0.95 confidence level to report statistical significance. 4.3.1. Two types of NNT metrics: nonnormalized and normalized by query
Note that each query has a different number of relevant documents. Thus, if we merely use all relevant documents as a single entry, the resulting NNT result values (e.g., NNT-MAP or NNT-P5) could be highly influenced by queries with a large number of relevant documents. To compensate for this effect of queries with a large number of relevant documents, we also set of relevant documents. To present the formal definition of the nonnormalized and normalized NNT metrics, we assume P5, NNT-P10, or NNT-MAP. The nonnormalized version of NNT is defined as follows.
 The normalized version of NNT is defined as follows.

Before summing them, NNT result values are first normalized by the number of relevant documents for their query. To com-pute NNT ( sim , d , q ) using the NNT-MAP, our evaluation was based on the top 1000 retrieved documents. 4.4. Parameter training iments, based on the suggestion of Lv and Zhai (2009) .
 eter was trained using other topic sets from the same test collection. For example, when Q301 X 350 in ROBUST was provided as a test set, a parameter was trained using Q351 X 450 and Q601 X 700 from the same ROBUST collection. When estimating we estimated P ( r ), let Train be the set of training queries, while R documents in the pooled documents selected by the pooling method, respectively. P ( r ) is then estimated as follows. where jR pool  X  q  X j + jNR pool  X  q  X j is the number of pooled documents for query q . 4.5. Truncation when estimating the query-sensitive similarity
In our experiments, we applied truncation to restrict the set of reference documents when computing the query-sensitive similarity and term-based similarity. With truncation, we only referred to the top-retrieved T documents in response to q when computing sim QSSM ( d , d 0 , q ) and the most similar M documents to d when computing sim Tombros and Rijsbergen (2001) . As a result, our final combined metric sim documents. T and M were fixed at 1000 in all our experiments.
 However, the resulting similarity with truncation would simply be zero when we only assign zeros to sim sim TSM ( d , d 0 ), because sim QSSM ( d , d 0 , q ) and sim not appear in the top-retrieved documents or the most similar documents. To clearly demonstrate our method, suppose that d is a given document, F X  q  X  is the set of top-retrieved T documents for query q using query-sensitive similarity sim QSSM ( d , d 0 , q ), and N X  d  X  is the set of the most similar M documents to d using term-based similarity sim are three possible cases for d 0 : (1) d 0 2F X  q  X \N X  d  X  (2) d 0 R F X  q  X  but d 0 2N X  d  X  (3) d 0 2F X  q  X  but d 0 R N X  d  X 
Case 1 is a normal case where no approximation is necessary, so standard original score functions for d 0 were used for sim TSM ( d , d 0 ) and sim QSSM ( d , d 0 , q ). In cases 2 and 3, we used the following approximating assumptions.  X  In case 2, we assume that document d 0 had no common terms with q .  X  In case 3, we assume that document d 0 had no common terms with d .

These assumptions were applied to the final computation of our proposed similarity and TR similarity. With our com-bined query-sensitive similarity, the above assumption leads to the following formulae for O  X  r j d in cases 2 and 3, respectively.
 where c ( w , d 0 ) is assumed to be 0 for the query word w in q for case 2, and for the document word w in d for case 3.
It was simpler to apply the same assumptions to TR similarity. If no common matching term exists between the document above assumption, the two similarities sim QSSM ( d , d 0 , q ) and sim 4.6. Experimental results
Tables 2 and 3 show experimental results comparing term-based similarity and query-sensitive similarity using nonnor-ting the TR metric, while rows labeled CoR indicate results with our proposed co-relevance-based similarity metric. We use
TSM and QSSM to refer to term-based and query-sensitive similarity metrics, respectively and use TSM + QSSM to indicate the combined metric of both similarities. The marks , X ,  X  , and similarities; over TR-based TSM;  X  over TR-based QSSM;  X  over for two noncombined TR-based metrics (i.e., TSM and QSSM);  X  over all three TR metrics (i.e., TSM, QSSM, and TSM + QSSM).

Both tables show that query-sensitive similarities led to significant improvements compared to term-based similarities ment of Tombros and Rijsbergen (2001) . As was shown by Tombros and Rijsbergen (2001) , we also found that query-sensi-tive similarities alone showed improvements over term-based similarity and that the improvement with the combination was greater than that with the noncombined method.

From the two query-sensitive metrics (the proposed similarity and TR similarity), the proposed similarity metric consis-or the better estimation with query-sensitive similarity, by further evaluating the TR similarity in combination using two variants. (1) Rather than the pivot vector space model, we used top-retrieved documents based on Dirichlet prior smoothing resulting TR-based query-sensitive similarity with the term-based similarity from Dirichlet prior smoothing which uses Eq. (23) .

Tables 4 and 5 show the results of the two variants of TR similarity applied with Dirichlet prior smoothing, comparing to the proposed combined similarity metric in the last row. The two variants are denoted as follows: retrieved documents are chosen based on the language modeling approaches.
  X  TR-TSM R + QSSM R : The combined similarity metric of term-based similarity with query-sensitive one proposed by Tomb-ros and Rijsbergen (2001) (i.e., Eq. (40) ), where the top-retrieved documents are chosen based on the language modeling approaches.
 The results of the first and the second variants are presented in the sub-rows named TSM the row named TR, respectively. Here, TSM R is the term-based similarity using Eq. (27) based on Dirichlet-prior smoothing which is the same as that for our proposed combined similarity in Eq. (9) . The marks , X ,  X  , and icance of improvements over TR similarities; over TR-based TSM + QSSM;  X  over TR-based TSM QSSM R ;  X  over all three TR combined metrics (i.e., TSM + QSSM, TSM improvement in comparison with the original TSM + QSSM of TR similarity, but this was still weaker than the performance of the co-relevance based metric. Thus, these results provide evidence that the further improvement with probabilistic co-relevance compared with TR similarity was not merely due to replacing the pivoted vector space model with a better retrie-a metric for defining query-sensitive similarity. 4.7. Comparison with language modeling approaches based on Tombros and Rijsbergen X  X  co-representation
The two variants of the TR similarity reported in tables still rely on the vector space model. To make a fairer comparison (2001) similarity, formulated as follows: ten form of Eq. (45) as follows:
Based on Eq. (46) , we evaluated the following similarity metrics based on the co-representation.  X  TR-QSSM-LM : Query-sensitive similarity (QSSM) presented proposed by Tombros and Rijsbergen (2001) (i.e., Eq. (46) ) applied to the language modeling approaches ( P ( r ) was fixed to 0.01).  X  TR-TSM + QSSM-LM : The combined similarity metric of term-based similarity with query-sensitive one presented pro-posed by Tombros and Rijsbergen (2001) (i.e., using Eq. (9) with Eqs. (27) and (46) )( P ( r ) was fixed to 0.01).
Tables 6 and 7 report the NNT results of similarity metrics based on co-representation relevance probability, compared with term-based similarity (Eq. (27) ) and our proposed similarity (Eq. (9) with Eqs. (27) and (26) ). The marks ,  X  , and cate statistical significance of improvements over other similarities; over TSM (using Eq. (27) );  X  over TR-QSSM-LM; three metrics (i.e., TSM, TR-QSSM-LM, and TR-TSM + QSSM-LM).

Both tables show that the combined similarity with the co-representation relevance probability (i.e., TSM + QSSM-TR) sig-not beat our decomposed style of estimation. Thus, the results again show that the high NNT values in our proposed simi-dence assumption used in our approximation shows comparative performances in this task, despite its simplification. 4.8. Effect of P(r)
The previous experiments were based on the use of a P ( r ) trained with judgment data, but we now present results with od, we found that when P ( r ) was large, such as 0.5 or 0.1, the performance was highly limited and showed no improvement method  X  X  P ( r ) = trained, X  X  but the magnitude of the change was small.
 approximately between the cases of P ( r ) = 0.1 and P ( r ) = 0.01. 4.9. Sensitivity of a cause it delivered relatively good performance. Fig. 1 shows performance curves for NNT-MAP, NNT-P5, and NNT-P10 using method always led to greater improvements when compared with both term-based and query-sensitive similarities. 4.10. Application: pseudo-relevance feedback  X  viewed as a method using the query-sensitive similarity
When setting our proposed co-relevance-based similarity, we observe an interesting equivalence between pseudo-rele-vance feedback and query-sensitive similarity-based re-ranking. In our view, pseudo-relevance feedback can be intuitively considered as a method for re-ranking document d based on the probability that d is co-relevant with the feedback documents.

To understand this equivalence, we suppose that the top-most retrieved document d pseudo-relevance feedback. Given d fb and q , all documents are re-ranked according to the score for the document d , which is defined as the co-relevance probability of d
Using our proposed estimation of Eq. (9) for P ( corel j d where const indicates a constant term that is independent of document d . To ensure that the relevance scores score
Section 3.3.4 , this leads us to obtain the following formula using Eq. (29) :
In general, with more than one document, we could also select m documents F X f d ument d is defined as the co-relevance probability of the set F and d in response to query q :
Here, we introduce the notation P  X  corel jF ; d ; q  X  as a straightforward extension of P ( corel j d posed to: in F are relevant. Note that P  X  corel jF ; q  X  is constant for document d , so we do not focus on P  X  corel jF ; q  X  . i.e., (i) the relevance of d that is dependent on F , and (ii) the relevant that is independent of F .
In the first case, we use an additional assumption, i.e., the relevance of d that is conditionally independent of F d therelevanceof d fb i .Basedonthisassumptionforeachdocument d
In the second case, the independence assumption leads to the following simplifying estimation by removing the condition part q in P  X  r j d ; F X  rel ; q  X  of Eq. (51) :
Thus, we have m + 1 estimations for P  X  r j d ; F X  rel ; q  X   X  P  X  r j d ; Q  X  d the following.
 Using c ( d fb )= (1 a ) b ( d fb ), we further derive Eq. (55) to: score FB  X  d fb 1 ; d ; q  X  ; ; score FB  X  d fb m ; d ; q  X  as follows:
In a specific case, we consider the following formula to estimate the weight c ( d only difference between Eq. (56) and the similarity scores using RM3 is that RM3 uses a smoothed document language model as a re-ranking method using pseudo-relevance feedback.
 also noted that pseudo-relevance feedback is another implementation that supports the cluster hypothesis. Indeed, one of the popular pseudo-relevance feedback methods, RM3, is a reformulation of interdocument relationships ( Cartright et al., 2010 ). Smucker and Allan (2009) also defined query-sensitive similarity as a form of pseudo-relevance feedback, although they did not begin from the co-relevance probability. 5. Conclusion
This study revisited the query-sensitive similarity metric postulated by Tombros and Rijsbergen (2001) with the aim of developing a better form of metric that might satisfy the cluster hypothesis. Based on previous work, this study addressed uments should be proportional to the probability that they are co-relevant to a given query. We then decomposed co-rele-of a widely-used standard scoring function. Experimental results showed that the proposed query-sensitive similarity sig-nificantly improved upon existing term-based similarity in terms of evaluation metrics set in the context of Voorhees X  NNT measure.

As a future work, it would be valuable to explore a discriminative model to estimate a co-relevance model based on a set of features obtained between a document and a query. This would develop a learning framework for the retrieval model, many relevant documents to be obtained, making a learnable mechanism more applicable in practice. These forms of collec-implicit or explicit feedback and improve query-sensitive similarity.
 References
