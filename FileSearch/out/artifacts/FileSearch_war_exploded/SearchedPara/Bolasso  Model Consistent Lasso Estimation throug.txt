 Regularization by the  X  est in recent years in machine learning, statistics and sign al processing. In the context of least-square linear regressi on, the problem is usually referred to as the Lasso (Tibshirani, 1994). Much of the early effort has been dedicated to al-gorithms to solve the optimization problem efficiently. In particular, the Lars algorithm of Efron et al. (2004) allows to find the entire regularization path (i.e., the set of solu-tions for all values of the regularization parameters) at th e cost of a single matrix inversion.
 Moreover, a well-known justification of the regularization by the  X  ing vectors with many zeros, and thus performs model se-2007; Zou, 2006; Wainwright, 2006) have looked precisely at the model consistency of the Lasso, i.e., if we know that the data were generated from a sparse loading vector, does the Lasso actually recover the sparsity pattern when the number of observed data points grows? In the case of a fixed number of covariates, the Lasso does recover the sparsity pattern if and only if a certain simple condition on the generating covariance matrices is verified (Yuan &amp; Lin, 2007). In particular, in low correlation settings, the Lass o is indeed consistent. However, in presence of strong corre-lations between relevant variables and irrelevant variabl es, the Lasso cannot be consistent, shedding light on potential problems of such procedures for variable selection. Adap-tive versions where data-dependent weights are added to the  X  tions (Zou, 2006).
 In this paper, we first derive a detailed asymptotic analysis of sparsity pattern selection of the Lasso estimation pro-cedure, that extends previous analysis (Zhao &amp; Yu, 2006; Yuan &amp; Lin, 2007; Zou, 2006), by focusing on a spe-cific decay of the regularization parameter. Namely, we show that when the decay is proportional to n  X  1 / 2 , where n is the number of observations, then the Lasso will se-lect all the variables that should enter the model (the rel-evant variables) with probability tending to one exponen-tially fast with n , while it selects all other variables (the irrelevant variables) with strictly positive probability. If several datasets generated from the same distribution were available, then the latter property would suggest to con-sider the intersection of the supports of the Lasso estimate s for each dataset: all relevant variables would always be se-lected for all datasets, while irrelevant variables would e n-ter the models randomly, and intersecting the supports from sufficiently many different datasets would simply eliminat e them. However, in practice, only one dataset is given; but resampling methods such as the bootstrap are exactly dedi-cated to mimic the availability of several datasets by resam -pling from the same unique dataset (Efron &amp; Tibshirani, 1998). In this paper, we show that when using the bootstrap and intersecting the supports, we actually get a consistent model estimate, without the consistency condition required by the regular Lasso. We refer to this new procedure as the Bolasso ( bo otstrap-enhanced l east a b s olute s hrinkage o perator). Finally, our Bolasso framework could be seen as a voting scheme applied to the supports of the boot-strap Lasso estimates; however, our procedure may rather be considered as a consensus combination scheme, as we keep the (largest) subset of variables on which all regres-sors agree in terms of variable selection, which is in our case provably consistent and also allows to get rid of a po-tential additional hyperparameter.
 The paper is organized as follows: in Section 2, we present the asymptotic analysis of model selection for the Lasso; in Section 3, we describe the Bolasso framework, while in Section 4, we illustrate our results on synthetic data, wher e the true sparse generating model is known, and data from the UCI machine learning repository. Sketches of proofs can be found in Appendix A.
 Notations For a vector v  X  R p , we denote k v k ( v norm and k v k sign( a ) denotes the sign of a , defined as sign( a ) = 1 a &gt; 0 ,  X  1 if a &lt; 0 , and 0 if a = 0 . For a vector of v .
 Moreover, given a vector v  X  R p and a subset I of v indexed by I . Similarly, for a matrix A  X  R p  X  p , A notes the submatrix of A composed of elements of A whose rows are in I and columns are in J . In this section, we describe existing and new asymptotic results regarding the model selection capabilities of the Lasso. 2.1. Assumptions We consider the problem of predicting a response Y  X  R from covariates X = ( X assumptions that we make on the joint distribution P ( X, Y ) are the following: ( A1 ) The cumulant generating functions E exp( s k X k ( A2 ) The joint matrix of second order moments Q = ( A3 ) E ( Y | X ) = X  X  w and var( Y | X ) =  X  2 a.s. for some We let denote J = { j, w s = sign( w ) the sign pattern of w , and  X  = Y  X  X  X  w mulant generating functions is satisfied when X and  X  have compact supports, and also when the densities of X and  X  have light tails.
 We consider independent and identically distributed (i.i.d.) data ( x the data are given in the form of matrices Y  X  R n and Note that the i.i.d. assumption, together with ( A1-3 ), are the simplest assumptions for studying the asymptotic be-havior of the Lasso; and it is of course of interest to allow more general assumptions, in particular growing number of variables p , more general random variables, etc., which are outside the scope of this paper X  X ee, e.g., Meinshausen and Yu (2008); Zhao and Yu (2006); Lounici (2008). 2.2. Lasso Estimation w  X  -norm defined as k w k at the following Lasso optimization problem (Tibshirani, 1994): where  X  w any global minimum of Eq. (1) X  X t may not be unique in general, but will with probability tending to one exponen-tially fast under assumption ( A2 ). 2.3. Model Consistency -General Results In this section, we detail the asymptotic behavior of the Lasso estimate  X  w , both in terms of the difference in norm with the population value w (i.e., regular consistency) and of the sign pattern sign(  X  w ) , for all asymptotic behaviors of the regularization parameter about the sign pattern includes information about the sup-port , i.e., the indices i for which  X  w moreover, when  X  w is consistent, consistency of the sign pattern is in fact equivalent to the consistency of the sup-port.
 We now consider five mutually exclusive possible situa-tions which explain various portions of the regularization path (we assume ( A1-3 )); many of these results appear else-where (Yuan &amp; Lin, 2007; Zhao &amp; Yu, 2006; Fu &amp; Knight, 2000; Zou, 2006; Bach, 2008; Lounici, 2008) but some of the finer results presented below are new (see Section 2.4). 1. If 2. If 3. If 4. If 5. If Among the five previous regimes, the only ones with con-sistent estimates (in norm) and a sparsity-inducing effect are 0  X  (0 ,  X  ] then we can only hope for model consistent estimates if the consistency condition in Eq. (2) is satisfied. This some-what disappointing result for the Lasso has led to various improvements on the Lasso to ensure model consistency even when Eq. (2) is not satisfied (Yuan &amp; Lin, 2007; Zou, 2006). Those are based on adaptive weights based on the non regularized least-square estimate. We propose in Sec-tion 3 an alternative way which is based on resampling. In this paper, we now consider the specific case where asymptotic results. Indeed, in this situation, we get the co r-rect signs of the relevant variables (those in J ) with proba-bility tending to one, but we also get all possible sign pat-terns consistent with this, i.e., all other variables (thos e not in
J ) may be non zero with asymptotically strictly posi-tive probability. However, if we were to repeat the Lasso estimation for many datasets obtained from the same dis-tribution, we would obtain for each ables, all of which include J with probability tending to one, but potentially containing all other subsets. By inter -secting those, we would get exactly J .
 However, this requires multiple copies of the samples, which are not usually available. Instead, we consider boot-strapped samples which exactly mimic the behavior of hav-ing multiple copies. See Section 3 for more details. 2.4. Model Consistency with Exact Root-n In this section we present detailed new results regarding the pattern consistency for Proposition 1 Assume ( A1-3 ) and 0 &gt; 0 that s  X  ( s, 0 )  X  (0 , 1) , and we have: Proposition 2 Assume ( A1-3 ) and 0 &gt; 0 s
J 6 = sign( w J ) The last two propositions state that we get all relevant vari -ables with probability tending to one exponentially fast , while we get exactly get all other patterns with probabil-ity tending to a limit strictly between zero and one. Note that the results that we give in this paper are valid for fi-nite n , i.e., we can derive actual bounds on probability of sign pattern selections with known constants that explictl y depend on w , Q and the joint distribution P Given the n i.i.d. observations ( x 1 , . . . , n , put together into matrices X  X  R n  X  p and Y  X  R n , we consider m bootstrap replications of the n data points (Efron &amp; Tibshirani, 1998); that is, for 1 , . . . , m , we consider a ghost sample ( x k i , y k i i = 1 , . . . , n , given by matrices X k  X  R n  X  p and Y The n pairs ( x k at random with replacement from the n original pairs in ( X, Y ) . The sampling of the nm pairs of observations is independent. In other words, we defined the distribution of the ghost sample ( X  X  , Y  X  ) by sampling n points with replacement from ( X, Y ) , and, given ( X, Y ) , the m ghost samples are independently sampled i.i.d. from the distribu -tion of ( X  X  , Y  X  ) .
 The asymptotic analysis from Section 2 suggests to esti-mate the supports J mates  X  w k for the bootstrap samples, k = 1 , . . . , m to intersect them to define the Bolasso model estimate of the support: J = T m timate w by the unregularized least-square fit restricted to variables in J . The detailed algorithm is given in Algo-rithm 1. The algorithm has only one extra parameter (the number of bootstrap samples m ). Following Proposition 3, log( m ) should be chosen growing with n asymptotically slower than n . In simulations, we always use m = 128 (except in Figure 3, where we study the influence of m ). Algorithm 1 Bolasso for k = 1 to m do end for
Compute J = T m Compute  X  w Note that in practice, the Bolasso estimate can be computed simultaneously for a large number of regularization param-eters because of the efficiency of the Lars algorithm (which we use in simulations), that allows to find the entire regular -ization path for the Lasso at the (empirical) cost of a single matrix inversion (Efron et al., 2004). Thus the computa-tional complexity of the Bolasso is O ( m ( p 3 + p 2 n )) The following proposition (proved in Appendix A) shows that the previous algorithm leads to consistent model selec -tion.
 Proposition 3 Assume ( A1-3 ) and 0 &gt; 0 Bolasso does not exactly select the correct model, i.e., P ( J 6 = J ) , has the following upper bound: where A Therefore, if log( m ) tends to infinity slower than n when n tends to infinity, the Bolasso asymptotically selects with overwhelming probability the correct active variable, and mate, the correct sign pattern as well. Note that the previ-ous bound is true whether the condition in Eq. (2) is sat-isfied or not, but could be improved on if we suppose that Eq. (2) is satisfied. See Section 4.1 for a detailed compari-son with the Lasso on synthetic examples. In this section, we illustrate the consistency results obta ined in this paper with a few simple simulations on synthetic examples and some medium scale datasets from the UCI machine learning repository (Asuncion &amp; Newman, 2007). 4.1. Synthetic examples For a given dimension p , we sampled X  X  R p from a nor-mal distribution with zero mean and covariance matrix gen-erated as follows: (a) sample a p  X  p matrix G with indepen-dent standard normal distributions, (b) form Q = GG  X  , (c) scale Q to unit diagonal. We then selected the first Card( J ) = r variables and sampled non zero loading vec-tors as follows: (a) sample each loading signs in { X  1 , 1 } uniformly at random and (b) rescale those by a scaling noise  X  is normally distributed with zero mean and variance  X  satisfies with probability one (with respect to the sampling of the covariance matrix) assumptions ( A1-3 ).
 In Figure 1, we sampled two distributions P 16 and r = 8 relevant variables, one for which the consis-tency condition in Eq. (2) is satisfied (left), one for which it was not satisfied (right). For a fixed number of sample n = 1000 , we generated 256 replications and computed the empirical frequencies of selecting any given variable for the Lasso as the regularization parameter varies. Those plots show the various asymptotic regimes of the Lasso de-tailed in Section 2. In particular, on the right plot, althou gh no leads to perfect selection (i.e., exactly variables with indices less than r = 8 are selected), there is a range where all relevant variables are always selected, while all other s are selected with probability within (0 , 1) .
 In Figure 2, we plot the results under the same condi-tions for the Bolasso (with a fixed number of bootstrap replications m = 128 ). We can see that in the Lasso-consistent case (left), the Bolasso widens the consistency region, while in the Lasso-inconsistent case (right), the B o-lasso  X  X reates X  a consistency region.
 In Figure 3, we selected the same two distributions and compared the probability of exactly selecting the correct support pattern, for the Lasso, and for the Bolasso with varying numbers of bootstrap replications (those probabil i-ties are computed by averaging over 256 experiments with the same distribution). In Figure 3, we can see that in the Lasso-inconsistent case (right), the Bolasso indeed allow s to fix the unability of the Lasso to find the correct pattern. Moreover, increasing m looks always beneficial; note that although it seems to contradict the asymptotic analysis in Section 3 (which imposes an upper bound for consistency), variables has very low probability and is not observed with only 256 replications.
 Finally, in Figure 4, we compare various variable selection procedures for linear regression, to the Bolasso, with two distributions where p = 64 , r = 8 and varying n . For all the methods we consider, there is a natural way to select ex-actly r variables with no free parameters (for the Bolasso, we select the most stable pattern with r elements, i.e., the pattern which corresponds to most values of ). We can see that the Bolasso outperforms all other variable selec-tion methods, even in settings where the number of samples becomes of the order of the number of variables, which re-quires additional theoretical analysis, subject of ongoin g research. Note in particular that we compare with bagging of least-square regressions (Breiman, 1996a) followed by a thresholding of the loading vector, which is another sim-ple way of using bootstrap samples: the Bolasso provides a more efficient way to use the extra information, not for usual stabilization purposes (Breiman, 1996b), but direct ly for model selection. Note finally, that the bagging of Lasso estimates requires an additional parameter and is thus not tested. 4.2. UCI datasets The previous simulations have shown that the Bolasso is succesful at performing model selection in synthetic exam-ples. We now apply it to several linear regression prob-lems and compare it to alternative methods for linear re-gression, namely, ridge regression, Lasso, bagging of Lass o estimates (Breiman, 1996a), and a soft version of the Bo-lasso (referred to as Bolasso-S), where instead of intersec t-ing the supports for each bootstrap replications, we select those which are present in at least 90% of the bootstrap replications. In Table 1, we consider data randomly gener-ated as in Section 4.1 (with p = 32 , r = 8 , n = 64 ), where the true model is known to be composed of a sparse loading vector, while in Table 2, we consider regression datasets from the UCI machine learning repository, for which we ear predictor. For all of those, we perform 10 replications of 10-fold cross validation and for all methods (which all have one free regularization parameter), we select the best regularization parameter on the 100 folds and plot the mean square prediction error and its standard deviation. Note that when the generating model is actually sparse (Ta-ble 1), the Bolasso outperforms all other models, while in other cases (Table 2) the Bolasso is sometimes too strict in intersecting models, i.e., the softened version works be t-ter and is more competitive with other methods. Studying the effects of this softened scheme (which is more simi-lar to usual voting schemes), in particular in terms of the potential trade-off between good model selection and low prediction error, and under conditions where p is large, is the subject of ongoing work. We have presented a detailed analysis of the variable se-lection properties of a boostrapped version of the Lasso. The model estimation procedure, referred to as the Bo-lasso, is provably consistent under general assumptions. This work brings to light that poor variable selection re-sults of the Lasso may be easily enhanced thanks to a simple parameter-free resampling procedure. Our contri-bution also suggests that the use of bootstrap samples by L. Breiman in Bagging/Arcing/Random Forests (Breiman, 1998) may have been so far slightly overlooked and consid-ered a minor feature, while using boostrap samples may ac-tually be a key computational feature in such algorithms for good model selection performances, and eventually good prediction performances on real datasets.
 The current work could be extended in various ways: first, we have focused on a fixed total number of variables, and allowing the numbers of variables to grow is important in theory and in practice (Meinshausen &amp; Yu, 2008). Second, the same technique can be applied to similar settings than least-square regression with the  X  ization by block  X  such as general convex classification losses. Finally, theo -retical and practical connections could be made with other work on resampling methods and boosting (B  X  uhlmann, 2006).
 In this appendix, we give sketches of proofs for the asymp-totic results presented in Section 2 and Section 3. The proofs rely on the well-known property of the Lasso op- X  0.93 1.20 1.42 1.28 Ridge 8 . 8  X  4 . 5 4 . 9  X  2 . 5 7 . 3  X  3 . 9 8 . 1  X  8 . 6 Lasso 7 . 6  X  3 . 8 4 . 4  X  2 . 3 4 . 7  X  2 . 5 5 . 1  X  6 . 5 Bolasso 5 . 4  X  3 . 0 3 . 4  X  2 . 4 3 . 4  X  1 . 7 3 . 7  X  10 . 2 Bagging 7 . 8  X  4 . 7 4 . 6  X  3 . 0 5 . 4  X  4 . 1 5 . 8  X  8 . 4 Bolasso-S 5 . 7  X  3 . 8 3 . 0  X  2 . 3 3 . 1  X  2 . 8 3 . 2  X  8 . 2 Ridge 18 . 6  X  4 . 9 7 . 7  X  4 . 8 5 . 8  X  18 . 6 28 . 0  X  5 . 9 Lasso 18 . 6  X  4 . 9 7 . 8  X  5 . 2 5 . 8  X  19 . 8 28 . 0  X  5 . 7 Bagging 18 . 6  X  5 . 0 8 . 0  X  5 . 2 6 . 0  X  18 . 9 28 . 1  X  6 . 6 timization problems, namely that if the sign pattern of the solution is known, then we can get the solution in closed form.
 A.1. Optimality Conditions We let denote  X  = Y  X  X w  X  R n , Q = X  X  X/n  X  R p  X  p Eq. (1) as: min The optimality conditions for Eq. (3) can be written in terms of the sign pattern s = s ( w ) = sign( w ) and the sparsity pattern J = J ( w ) = { j, w 2007): In this paper, we focus on regularization parameters the form is to consider that ( Q, q ) are distributed according to their limiting distributions, obtained from the law of large num-bers and the central limit theorem, i.e., Q converges to Q a.s. and n 1 / 2 q is asymptotically normally distributed with mean zero and covariance matrix  X  2 Q . When assuming this, Propositions 1 and 2 are straightforward. The main effort is to make sure that we can safely replace ( Q, q ) their limiting distributions. The following lemmas give su f-ficient conditions for correct estimation of the signs of var i-ables in J and for selecting a given pattern s (note that all constants could be expressed in terms of Q and w , details are omitted here): Lemma 1 Assume ( A2 ) and k Q  X  Q k Then sign(  X  w C 2 , where C 1 , C 2 &gt; 0 .
 Lemma 2 Assume ( A2 ) and let s  X  { X  1 , 0 , 1 } p such that s
J = sign( w J )  X  i  X  J \ J , s i Q  X  1 JJ ( q J  X  n s J ) with C sign(  X  w ) = sign( w ) .
 Those two lemmas are useful because they relate optimality of certain sign patterns to quantities from which we can derive concentration inequalities.
 A.2. Concentration Inequalities Throughout the proofs, we need to provide upper bounds on the following quantities P ( k Q  X  1 / 2 q k P ( k Q  X  Q k 2 &gt;  X  ) . We obtain, following standard argu-ments (Boucheron et al., 2004): if  X  &lt; C (where C We also consider multivariate Berry-Esseen inequalities (Bentkus, 2003); the probability P ( n 1 / 2 q  X  X  ) can be esti-mated as P ( t  X  C ) where t is normal with mean zero and C ) | is then uniformly (for all convex sets C ) upperbounded by: A.3. Proof of Proposition 1 By Lemma 2, for any A and n large enough, the probability that the sign is different from s is upperbounded by where C ( s,  X  ) is the set of t such that (a) k Q t J \ J , s i Q  X  1 JJ ( t J  X   X s J )  X  = O ((log n ) n  X  1 / 2 ) , which tends to zero, we have: P { t /  X  X  ( s, 0 (1  X   X  )) } 6 P { t /  X  X  ( s, 0 ) } + O (  X  ) terms (if A is large enough) are thus O ((log n ) n  X  1 / 2 This shows that P (sign(  X  w ) = sign( w )) &gt;  X  ( s, O ((log n ) n  X  1 / 2 ) where  X  ( s, 0 ) = P { t  X  X  ( s, (0 , 1)  X  X he probability is strictly between 0 and 1 because the set and its complement have non empty interiors and the normal distribution has a positive definite covariance matrix  X  2 Q . The other inequality can be proved similarly. Note that the constant in O ((log n ) n  X  1 / 2 ) depends on but by carefully considering this dependence on make the inequality uniform in zero or infinity at most at a logarithmic speed (i.e., it would be interesting to consider uniform bounds on por-tions of the regularization path.
 A.4. Proof of Proposition 2 From Lemma 1, the probability of not selecting any of the variables in J is upperbounded by P ( k Q  X  1 / 2 q k 2 &gt; C 1  X  n C 2 )+ P ( k Q  X  Q k 2 which is straightforwardly upper bounded (using Sec-tion A.2) by a term of the required form.
 A.5. Proof of Proposition 3 In order to simplify the proof, we made the simplifying assumption that the random variables X and  X  have com-pact supports. Extending the proofs to take into account the looser condition that k X k 2 and  X  2 have non uniformly infi-nite cumulant generating functions (i.e., assumption ( A1 )) can be done with minor changes. The probability that T of the following probabilities: (a) Probability of missing at least one variable in J in any of the m replications: by Lemma 1, the probability that for the k -th replication, one index in J is not selected, is upper bounded by P ( k Q  X  1 / 2 q  X  k 2 &gt; C 1 / 2) + P ( k Q  X  Q  X  k 2 in theoretical analysis of the bootstrap, we relate q  X  to as follows: P ( k Q  X  1 / 2 q  X  k q ) k 2 &gt; C 1 / 4)+ P ( k Q  X  1 / 2 q k 2 &gt; C 1 / 4) P ( k Q  X  Q  X  k 2 &gt;  X  min ( Q ) / 2) ). Because we have assumed that X and  X  have compact supports, the bootstrapped vari-ables have also compact support and we can use concentra-tion inequalities (given the original variables X , and also after expectation with respect to X ). Thus the probability for one bootstrap replication is upperbounded by Be  X  Cn where B and C are strictly positive constants. Thus the overall contribution of this part is less than mBe  X  Cn . (b) Probability of not selecting exactly J in all replica-tions: note that this is not tight at all since on top of the relevant variables which are selected with overwhelming probability, different additional variables may be select ed for different replications and cancel out when intersectin g. Our goal is thus to bound E P ( J  X  6 = J | X ) m . By Lemma 2, we have that P ( J  X  6 = J | X ) is upper bounded by where now, given X, Y , t  X  is normally distributed with As in (a), the first two terms and the last two ones are uni-consider the remaining term. We have C ( R Hoeffding X  X  inequality, we can replace the covariance ma-trix that depends on X and Y by  X  2 Q , at cost O ( n  X  1 / 2 We thus have to bound P ( n 1 / 2 q + y /  X  C ( normally distributed and C ( cause the set is compact, there exist constants A, B &gt; 0 such that, if k n 1 / 2 q k tion, we obtain a bound of the form: E P ( J  X  6 = J | X ) m 6 (1  X  Ae  X  B X  2 + F where we have used Hoeffding X  X  inequality to upper bound we obtain the desired inequality.
 I would like to thank Za  X   X d Harchaoui and Jean-Yves Au-work was supported by a French grant from the Agence Nationale de la Recherche (MGA Project).
 Asuncion, A., &amp; Newman, D. (2007). UCI machine learn-ing repository.
 Bach, F. R. (2008). Consistency of the group Lasso and multiple kernel learning. J. Mac. Learn. Res. , to appear. Bentkus, V. (2003). On the dependence of the Berry X 
Esseen bound on dimension. Journal of Statistical Plan-ning and Inference , 113 , 385 X 402.
 Boucheron, S., Lugosi, G., &amp; Bousquet, O. (2004). Con-centration inequalities. Advanced Lectures on Machine Learning . Springer.
 Breiman, L. (1996a). Bagging predictors. Machine Learn-ing , 24 , 123 X 140.
 Breiman, L. (1996b). Heuristics of instability and stabili za-tion in model selection. Ann. Stat. , 24 , 2350 X 2383. Breiman, L. (1998). Arcing classifier. Ann. Stat. , 26 , 801 X  849.
 B  X  uhlmann, P. (2006). Boosting for high-dimensional linear models. Ann. Stat. , 34 , 559 X 583.
 Least angle regression. Ann. Stat. , 32 , 407.
 Efron, B., &amp; Tibshirani, R. J. (1998). An introduction to the bootstrap . Chapman &amp; Hall.
 Fu, W., &amp; Knight, K. (2000). Asymptotics for Lasso-type estimators. Ann. Stat. , 28 , 1356 X 1378.
 Lounici, K. (2008). Sup-norm convergence rate and sign concentration property of Lasso and Dantzig estimators. Electronic Journal of Statistics , 2 .
 Meinshausen, N., &amp; Yu, B. (2008). Lasso-type recovery of sparse representations for high-dimensional data. Ann. Stat. , to appear.
 Tibshirani, R. (1994). Regression shrinkage and selection via the Lasso. J. Roy. Stat. Soc. B , 58 , 267 X 288. Wainwright, M. J. (2006). Sharp thresholds for noisy and high-dimensional recovery of sparsity using  X  constrained quadratic programming (Tech. report 709). Dpt. of Statistics, UC Berkeley.
 Yuan, M., &amp; Lin, Y. (2007). On the non-negative garrotte estimator. J. Roy. Stat. Soc. B , 69 , 143 X 161.
 Zhao, P., &amp; Yu, B. (2006). On model selection consistency of Lasso. J. Mac. Learn. Res. , 7 , 2541 X 2563.
 Zou, H. (2006). The adaptive Lasso and its oracle proper-
