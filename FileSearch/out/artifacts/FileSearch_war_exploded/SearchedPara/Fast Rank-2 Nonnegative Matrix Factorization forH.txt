 Nonnegative matrix factorization (NMF) has been success-fully used as a clustering method especially for flat parti-tioning of documents. In this paper, we propose an efficient hierarchical document clustering method based on a new al-gorithm for rank-2 NMF. When the two block coordinate descent framework of nonnegative least squares is applied to computing rank-2 NMF, each subproblem requires a so-lution for nonnegative least squares with only two columns in the matrix. We design the algorithm for rank-2 NMF by exploiting the fact that an exhaustive search for the opti-mal active set can be performed extremely fast when solv-ing these NNLS problems. In addition, we design a measure based on the results of rank-2 NMF for determining which leaf node should be further split. On a number of text data sets, our proposed method produces high-quality tree struc-tures in significantly less time compared to other methods such as hierarchical K-means, standard NMF, and latent Dirichlet allocation.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Clustering Algorithms, Experimentation, Performance Active-set algorithm, hierarchical document clustering, non-negative matrix factorization, rank-2 NMF
Nonnegative matrix factorization (NMF) has received wide recognition in many data mining areas such as text analysis [24]. In NMF, given a nonnegative matrix X  X  R m  X  n + and k  X  min( m,n ), X is approximated by a product of two non-negative matrices W  X  R m  X  k + and H  X  R k  X  n + . A common way to define NMF is to use the Frobenius norm to measure the difference between X and WH [11]: where k X k F denotes the Frobenius norm. The columns of X = [ x 1 ,  X  X  X  , x n ] represent n nonnegative data points in the m -dimensional space. Typically k &lt;&lt; min( m,n ), i.e. the original data points in the m -dimensional space are ap-proximated in a much lower-dimensional space of dimension k . The k columns of W are nonnegative basis vectors that span the lower-dimensional space, and the i -th column of H contains k nonnegative linear coefficients that represent x in the space spanned by the columns of W . Nonnegative data frequently occur in modern data analysis, such as text corpus when represented as a term-document matrix [19]. Because the lower rank factors W and H contain only non-negative elements which stay in the original domain of the data points, NMF often produces basis vectors that facilitate better interpretation [16].

NMF has shown excellent performances as a clustering method in numerous applications [24, 10]. When NMF is used as a clustering method, the columns of W are inter-preted as k cluster representatives, and the i -th column of H contains fractional assignment values of the i -th data point for the k clusters, which can be interpreted as soft clustering. To obtain a hard clustering result for the i -th data point, we may choose the index that corresponds to the largest element in the i -th column of H . This clustering scheme has been shown to achieve superior clustering quality, and many vari-ations such as constrained clustering and graph clustering [10, 14] have been proposed. The standard NMF (1) has been shown to perform especially well as a document clus-tering method, where the columns of W can be interpreted as k topics extracted from a corpus [24, 14]. When the j -th column of W is constrained to have unit L 1 norm, it can be interpreted as a probability distribution of terms for the j -th topic.

Most of the previous work on clustering with NMF has been focused on flat partitioning of a data set. However, hierarchical clustering often reveals additional structures in the data. For example, a tree structure often provides a more detailed taxonomy or a better description of natu-ral phenomena than a flat partitioning. In the widely-used text corpus RCV1 [17], a hierarchy of topics was defined, with 103 leaf nodes under four super categories (Corpo-rate/Industrial, Economics, Government/Social, Markets). Online retailers such as Amazon and eBay also maintain their product catalogues as a hierarchical tree structure. In this paper, we will explore hierarchical clustering with NMF and present a highly efficient algorithm with competitive clustering quality.

The lower rank k in standard NMF, which represents the number of clusters in a clustering setting, is often assumed to be given or predetermined according to prior knowledge about the data set or the embedding of the data points. Se-lecting the number of clusters k is an important and difficult issue in practice. Though model selection methods for se-lecting k have been proposed in the context of NMF [4, 8], it is expensive to compute solutions of NMF for each k in general [12]. In the NMF-based hierarchical approach we propose in this paper, a data set is recursively divided into small subsets and the number of clusters does not need to be predetermined by a user.

We will design a hierarchical clustering method based on rank-2 NMF, i.e. NMF with k = 2. The hierarchical struc-ture we will generate is a binary tree, and our method does not require any input on the level of the tree or the total number of clusters. Our motivation for hierarchical cluster-ing with binary tree structure is based on our fast algorithm for rank-2 NMF proposed in this paper. We will exploit the special properties of NMF with k = 2, and propose a very fast algorithm. We will study a particular type of existing algorithms for standard NMF, namely active-set-type algo-rithms [11, 13], and show that when k = 2, active-set-type algorithms can be reduced to a simple and efficient algorithm for rank-2 NMF, which has additional benefits when imple-mented on parallel platforms due to  X  X on-random X  memory access.

When applying rank-2 NMF to the recursive splitting of a text corpus, our empirical results reveal that if a balanced tree is constructed, its clustering quality is often worse than that of a flat partitioning. Thus we need to adaptively de-termine the next node to split. Our strategy is to compute a score for each leaf node to evaluate whether it is composed of two well-separated clusters based on the two basis vec-tors generated by rank-2 NMF before deciding which one to split. Compared to existing strategies that rely on an n  X  n document-document similarity matrix [5], our methodology never generates a large dense matrix thus is more time/space efficient. Although the rank-2 NMF computation on any leaf node in the final tree is wasted, our methodology is still very efficient overall due to the high efficiency of our rank-2 NMF algorithm.

Our contributions in this paper include:
The rest of the paper is organized as follows. We conduct detailed analysis of existing active-set-type algorithms for NMF in the special case of k = 2 in Section 2, and present our new algorithm for rank-2 NMF in Section 3. In Section 4, we describe our measure for scoring tree nodes and the hierarchical document clustering workflow. In Section 5, we show the difference between clustering approaches and topic modeling approaches when applied to flat and hierarchi-cal document clustering. In Section 6, we demonstrate the promising efficiency, clustering quality, and semantic quality of our methodology empirically on large-scale data sets. In Section 7, we summarize the advantages and shortcomings of this work. Although we focus on document clustering, the proposed hierarchical clustering method is not limited to documents.

Throughout this paper, k X k denotes the Euclidean norm, and k X k F denotes the Frobenius norm.
In this paper, we consider the algorithms for NMF that fit into the two-block coordinate descent framework [18, 11, 13, 12] due to better theoretical guarantee in convergence properties. In this framework, starting from some initial-ization, the matrices W and H are updated in an iterative manner, until some stopping criterion is satisfied. The over-all nonconvex problem (1) is thus reduced to two convex subproblems: When an optimal solution is obtained for each subproblem in each iteration, this iterative procedure is guaranteed to con-verge to a stationary point [6], which is a good convergence property for nonconvex problems such as (1). Each subprob-lem is a nonnegative least squares problem (NNLS) with multiple right-hand sides. Consider the following generic form for simplicity: be solved.

Various types of algorithms can be used to solve the NNLS problem and can be categorized into standard optimiza-tion algorithms and active-set-type algorithms. A classi-cal active-set algorithm for NNLS with a single right-hand side was introduced in [15]. In the context of NMF, Lin [18] claimed that it would be expensive to solve NNLS with mul-tiple right-hand sides using the active-set algorithm repeat-edly, and proposed a projected gradient descent (PGD) algo-rithm. However, Kim &amp; Park [11] proposed several improve-ments for the original active-set algorithm, and achieved an NMF algorithm with overall efficiency comparable to PGD. Later, a block-pivoting algorithm for NNLS [13] was pro-posed, which proved to be more efficient than the active-set algorithm when k is large. We call both active-set based and block-pivoting based algorithms for NMF as active-set-type algorithms .

In active-set-type algorithms for NNLS, we need to iden-tify a partitioning of variables in G into an active set A and a passive set P . At each step of searching for the op-timal active set, we need to solve an unconstrained least squares problem. Because the number of possible active sets is exponential, a well-guided search of the optimal active sets is important, such as presented by the active-set and block-pivoting methods. To improve the efficiency of solv-ing NNLS with multiple right-hand sides (4), the columns of G with the same active set pattern are grouped together for lower computational complexity and more cache-efficient computation [11, 22], and the grouping of columns changes when the active set is re-identified in each iteration of NNLS. Practically, the grouping step is implemented as a sorting of the columns of G , with complexity O ( n log n ) which is ex-pensive when n is large. Other steps, such as checking the optimality of the active sets, also introduces additional over-heads.

When the underlying application restricts the value of k to be 2, such as hierarchical clustering that generates a bi-nary tree, the number of possible active sets is reduced to 2 = 4 for each column of G , and it is practical to enumerate all of them. Conceptually, active-set-type algorithms search sets, and the size of the finite search space is 4 in the spe-cial case of k = 2. On the contrary, standard optimization algorithms require an indefinite number of iterations before convergence, and the actual number of iterations depends on the required accuracy. Therefore, when k = 2, stan-dard optimization algorithms such as PGD are not able to exploit the special property of NNLS, and active-set-type algorithms become the better choice. In the next section, we will propose a new algorithm and its efficient implemen-tation based on active-set-type algorithms, which will avoid all the overheads of switching between active sets.
Both flat clustering based on standard NMF and hierar-chical clustering based on rank-2 NMF can produce k non-overlapping groups of a data set. In the following, we ar-gue that the hierarchical approach is the preferred choice in terms of efficiency by conducting an analysis of computa-tional complexity for different k values, using the active-set based algorithm [11] as an exemplar algorithm. Given an m  X  n sparse data matrix X and the number of clusters k , the complexity of one NNLS run for standard NMF is upper bounded by: where N is the number of nonzero entries in X , and t is the number of search steps towards the optimal active set. In hierarchical clustering, we need to perform rank-2 NMF for k  X  1 times, and the complexity of one NNLS run summed over all the k  X  1 steps is at most: ( k  X  1)  X  [8 N + 8( m + n ) + t (8 / 3 + 8 m + 8 n )] flops. (6) The actual flops (floating point operations) in hierarchical clustering must be smaller than (6), because any splitting other than the first step is executed on a subset of the data set only. Thus, the expression (5) is superlinear with re-Table 1: Four possible active sets when B  X  R m  X  2 + . spect to k , while (6) is linear with respect to k . Assuming the number of search steps t is the same in both cases, the hierarchical approach is expected to be much less expensive. With our new algorithm specifically for rank-2 NMF in this paper, the efficiency of NMF-based hierarchical clustering will be boosted even more.
In ANLS, the problem of solving NMF is reduced to the problem of solving NNLS with multiple right-hand sides: min G  X  0 k BG  X  Y k 2 F . In the context of NMF, Y is set to be either the data matrix X , or X T . Let Y = [ y 1 ,  X  X  X  , y [ g 1 ,  X  X  X  , g n ]. We emphasize that y i  X  0 (1  X  i  X  n ) in the NNLS problem we are solving since the data is nonnegative.
Since the formulation (4) for NNLS with multiple right-hand sides can be rewritten as the solution of each column of G is independent of each other, and we obtain n NNLS problems each with a single right-hand side. We first study the solution of NNLS with a single right-hand side, and then consider the issues when combining multiple right-hand sides.
In general, when B has more than two columns, an active-set-type algorithm has to search for an optimal active set as discussed in Section 2. We denote the two parts of g that correspond to the active set and the passive set as g and g P , respectively. At each iteration of the algorithm, g
A is set to zero, and g P is solved by unconstrained least squares using a subset of columns of B corresponding to P . The optimal active set is the one where the solution of unconstrained least squares is feasible, i.e. g P  X  0, and meanwhile k B g  X  y k 2 is minimized.
 When k = 2, we have where B = [ b 1 , b 2 ]  X  R m  X  2 + , y  X  R m  X  1 + , and g = [ g R
Considering the limited number of possible active sets, our idea is to avoid the search of the optimal active set at the cost of some redundant computation. The four possibilities of the active set A is shown in Table 1. We simply enumerate all the possibilities of ( A , P ), and for each P , minimize the corresponding objective function J ( g ) in Table 1 by solving the unconstrained least squares problem. Then, of all the feasible solutions of g (i.e. g  X  0), we pick the one with the smallest J ( g ). Now we study the properties of the solutions of these unconstrained least squares problems, which will lead to an efficient algorithm to find the optimal active set. Figure 1: An illustration of one-dimensional least squares problems min k b 1 g  X  1  X  y k 2 and min k b 2 g  X  Algorithm 1 Algorithm for solving min g  X  0 k B g  X  y k 1: Solve unconstrained least squares g  X   X  min k B g  X  y k 2: if g  X   X  0 then 3: return g  X  4: else 5: g  X  1  X  ( y T b 1 ) / ( b T 1 b 1 ) 6: g  X  2  X  ( y T b 2 ) / ( b T 2 b 2 ) 7: if g  X  1 k b 1 k X  g  X  2 k b 2 k then 8: return [ g  X  1 , 0] T 9: else 10: return [0 ,g  X  2 ] T 11: end if 12: end if
First, we claim that the two unconstrained problems min k b 1 g 1  X  y k , min k b 2 g 2  X  y k always yield feasible solutions. Take min k b 1 g 1  X  y k 2 as an example. Its solution is: If b 1 6 = 0, we always have g  X  1  X  0 since y  X  0 , b 1  X  0. In the context of rank-2 NMF, the columns of W and the rows of H are usually linearly independent when nonnegative-rank( X )  X  2, thus b 1 6 = 0 holds in most cases. Geometrically (see Fig. 1), the best approximation of vector y in the one-dimensional space spanned by b 1 is the orthogonal projec-tion of y onto b 1 .
 If g  X   X  arg min k b 1 g 1 + b 2 g 2  X  y k 2 is nonnegative, then A =  X  is the optimal active set because the unconstrained solution g  X  is feasible and neither min k b 1 g 1  X  y min k b 2 g 2  X  y 2 k 2 can be smaller than J ( g  X  ). Otherwise, we only need to find the smallest objective J ( g ) among the other three cases since they all yield feasible solutions. We claim that A = { 1 , 2 } , i.e. P =  X  , can be excluded. Using g , the solution of min k b 1 g 1  X  y k 2 , we have In fact, P = { 1 } includes P =  X  as a special case when b 1  X  y .
 the illustration in Fig. 1, b 1 g  X  1  X  y  X  b 1 g  X  1 and b b g  X  2 , therefore we have for j = 1 , 2. Thus choosing the smaller objective amounts to choosing the larger value from g  X  1 k b 1 k and g  X  2
Our algorithm for NNLS with a single right-hand side is summarized in Algorithm 1. Note that B T B and B T y need only to be computed once for lines 1,5,6.
 Algorithm 2 Algorithm for solving min G  X  0 k BG  X  Y k 1: Solve unconstrained least squares G  X  = [ g  X  1 ,  X  X  X  , g 2:  X  1  X  X  b 1 k ,  X  2  X  X  b 2 k 3: u  X  ( Y T b 1 ) / X  2 1 4: v  X  ( Y T b 2 ) / X  2 2 5: for i = 1 to n do 6: if g  X  i  X  0 then 7: return g  X  i 8: else 9: if u i  X  1  X  v i  X  2 then 10: return [ u i , 0] T 11: else 12: return [0 ,v i ] T 13: end if 14: end if 15: end for
When Algorithm 1 is applied to NNLS with multiple right-hand sides, computing g  X  ,g  X  1 ,g  X  2 for different vectors y sepa-rately is not cache-efficient. In Algorithm 2, we solve NNLS with n different vectors y simultaneously, and the analy-sis in Section 3.1 becomes important. Note that the entire for-loop (lines 5-15, Algorithm 2) is embarrassingly parallel and can be vectorized. To achieve this, unconstrained so-lutions for all the three possible active sets are computed before entering the for-loop. Some computation is redun-dant, for example, the cost of solving u i and v i is wasted when g  X  i  X  0 (c.f. line 5-6, Algorithm 1). However, Algo-rithm 2 represents a non-random pattern of memory access, and we expect that it is much faster for rank-2 NMF than applying existing active-set-type algorithms directly.
Note that a na  X   X ve implementation of comparing k b 1 g y k 2 and k b 2 g  X  2  X  y k 2 for n different vectors y requires O ( mn ) complexity due to the creation of the m  X  n dense matrix BG  X  Y . In contrast, our algorithm only requires O ( m + n ) complexity at this step (line 9, Algorithm 2), because b 1 are the same across all the n right-hand sides. Assuming Y is a sparse matrix with N nonzeros, the overall complexity of Algorithm 2 is O ( N ), which is the same as the complexity of existing active-set-type algorithms when k = 2 (see Eq. 5). The dominant part comes from computing the matrix product Y T B in unconstrained least squares.
Rank-2 NMF can be recursively applied to a data set, gen-erating a hierarchical tree structure. In this section, we focus on text corpus and develop an overall efficient approach to hierarchical document clustering. In particular, we propose a strategy of choosing an existing leaf node at each splitting step. Extensive criteria for selecting the next leaf node to split were discussed in previous literature for general cluster-ing methods [5], mainly relying on cluster labels induced by the current tree structure. In the context of NMF, however, we have additional information about the clusters: Each col-umn of W is a cluster representative. In text data, a column of W is the term distribution for a topic [24], and the largest Figure 2: An illustration of a leaf node N and its two potential children L and R .
 elements in the column correspond to the top words for this topic. We will exploit this information to determine the next node to split.

In summary, our strategy is to compute a score for each leaf node by running rank-2 NMF on this node and evaluat-ing the two columns of W . Then we select the current leaf node with the highest score as the next node to split. The score for each node needs only to be computed once when the node first appears in the tree. For an illustration of a leaf node and its two potential children, see Fig. 2. We split a leaf node N if at least two well-separated topics can be discovered within the node. Thus we expect that N receives a high score if the top words for N is a well-balanced com-bination of the top words for its two potential children, L and R . We also expect that N receives a low score if the top words for L and R are almost the same.

We utilize the concept of normalized discounted cumula-tive gain (NDCG) [7] from the information retrieval com-munity. Given a perfect ranked list, NDCG measures the quality of an actual ranked list which always has value be-tween 0 and 1. A leaf node N in our tree is associated with a term distribution w N , given by a column of W from the rank-2 NMF run that generates the node N . We can ob-tain a ranked list of terms for N by sorting the elements in w
N in descending order, denoted by f N . Similarly, we can obtain ranked lists of terms for its two potential children, L and R , denoted by f L and f R . Assuming f N is a per-fect ranked list, we compute a modified NDCG (mNDCG) score for each of f L and f R . We describe our way to com-pute mNDCG in the following. Recall that m is the total number of terms in the vocabulary. Suppose the perfectly ordered terms corresponding to f N is and the shuffled orderings in f L and f R are respectively: We first define a position discount factor p ( f i ) and a gain g ( f i ) for each term f i : where l i 1 = r i 2 = i . In other words, for each term f find its positions i 1 ,i 2 in the two shuffled orderings, and place a large discount in the gain of term f i if this term is high-ranked in both shuffled orderings. The sequence of gain sequence {  X  g i } m i =1 . Then, for a shuffled ordering f or f R ), mNDCG is defined as: As we can see, mNDCG is basically computed in the same way as the standard NDCG measure, but with a modified gain function. Also note that  X  g i instead of g ( f in computing the ideal mDCG (mIDCG) so that mNDCG always has a value in the [0 , 1] interval.

Finally, the score of the leaf node N is computed as: To illustrate the effectiveness of this scoring function, let us consider some typical cases. 1. When the two potential children L , R describe well-2. When both L and R describe the same topic as that of 3. When L describes the same topic as that of N , and R
The overall hierarchical document clustering workflow is summarized in Algorithm 3, where we refer to a node and the documents associated with the node exchangably. The while-loop in this workflow (lines 8-15) defines an outlier de-tection procedure, where T trials of rank-2 NMF are allowed in order to split a leaf node M into two well-separated clus-ters. At each trial, two potential children nodes N 1 , N created, and if we believe that one of them (say, N 2 ) is com-posed of outliers, we discard N 2 from M at the next trial. If we still cannot split M into two well-separated clusters after T trials, M is marked as a permanent leaf node. Em-pirically, without the outlier detection procedure, the con-structed tree would end up with many tiny leaf nodes, which do not correspond to salient topics and degrade the cluster-ing quality. We have not specified the best moment to exit and stop the recursive splitting process, but simply set an upper limit of leaf nodes k . Other strategies can be used to determine when to exit, such as specifying a score thresh-old  X  and exiting the program when none of the leaf nodes have scores above  X  ;  X  = 0 means that the recursive split-ting process is not finished until all the leaf nodes become permanent leaf nodes.

Compared to other criteria for choosing the next node to split, such as those relying on the self-similarity of each cluster and incurring O ( n 2 ) overhead [5], our method is more efficient. In practice, the binary tree structure that results from Algorithm 3 often has meaningful hierarchies Algorithm 3 Hierarchical document clustering based on rank-2 NMF 1: Input: A term-document matrix X  X  R m  X  n + (often 2: Create a root node R , containing all the n documents 3: score( R )  X  X  X  4: repeat 5: M X  a current leaf node with the highest score 6: Trial index i  X  0 7: Outlier set Z  X  X  X  8: while i &lt; T do 9: Run rank-2 NMF on M and create two potential 10: if |N 1 | X   X  |N 2 | and score( N 2 ) is smaller than every 11: Z  X  Z  X  X  2 , M X  X  X  Z , i  X  i + 1 12: else 13: break 14: end if 15: end while 16: if i &lt; T then 17: Split M into N 1 and N 2 (hard clustering) 18: Compute score( N 1 ) and score( N 2 ) 19: else 20: M X  X  X  Z (recycle the outliers and do not split 21: score( M )  X  X  X  1 (set M as a permanent leaf node) 22: end if 23: until # leaf nodes = k 24: Output: A binary tree structure of documents, where and leaf clusters. We will evaluate its performance by clus-tering quality measures in the experiment section.
NMF can be regarded as both a clustering method and, with certain additional constraints, a probabilistic topic mod-eling method [1]. Both document clustering and topic mod-eling can be thought of as some sort of dimension reduc-tion; however, these two tasks have fundamental differences. Our paper is focused on the clustering aspect, and now we compare NMF-based clustering to a popular topic modeling method, latent Dirichlet allocation (LDA) [3].

We start with comparing flat clustering and flat topic modeling. First, LDA builds a probabilistic model that gen-erates the text corpus, and intends to predict the probability of new documents, while the goal of NMF-based clustering is to derive a partitioning that well-organizes an existing text corpus. Second, LDA models each word as a discrete ran-dom variable, thus is not compatible with tf-idf weighting when forming the term-document matrix. On the contrary, NMF-based clustering finds an algebraic latent subspace and is able to leverage the benefit of tf-idf weighting which has proved to be useful in a wide range of tasks such as infor-mation retrieval [19].

Now we discuss the difference between hierarchical clus-tering based on rank-2 NMF and hierarchical LDA (hLDA) [2]. hLDA builds a hierarchy of topics and each document is generated by sampling from the topics along a path with length L from the root to a leaf node. On the contrary, hier-archical clustering builds a hierarchy of documents, and the documents associated with each node are a mixture of two topics extracted from this node. In practice, hLDA requires all the leaf nodes be on the same level of the tree, and the depth L of the tree is chosen beforehand, while hierarchical clustering adaptively chooses a node at each splitting step.
In general, both methods have pros and cons: Topic mod-eling has a probabilistic interpretation, and clustering ap-proaches are more flexible. In the next section, we include LDA in our experiments and compare its performance with the performances of clustering based on NMF and rank-2 NMF. hLDA or other hierarchical probabilistic models are not considered in the experiments because they represent a quite different scenario of text modeling.
In this section, we describe our experimental settings and demonstrate both the efficiency and quality of our proposed algorithm. All the experiments except LDA are run in Mat-lab 7.9 (R2009b) with two Intel Xeon X5550 quad-core pro-cessors and 24GB memory.
Four text data sets with ground-truth classes are used in our experiments: 1. Reuters-21578 1 contains news ar-ticles from the Reuters newswire in 1987. We discarded documents with multiple class labels, and then selected the 20 largest classes. 2. 20 Newsgroups 2 contains articles from Usenet newsgroups and has a defined hierarchy of 3 levels. Unlike previous indexing, we observed that many ar-ticles have duplicated paragraphs due to cross-referencing. We discarded cited paragraphs and signatures. 3. Cora [20] is a collection of research papers in computer science, from which we extracted the title, abstract, and reference-contexts. Although this data set defines a topic hierarchy of 3 levels, we observed that some topics, such as  X  X I  X  NLP X  and  X  X R  X  Extraction X , are very related but reside in differ-ent subtrees. Thus we ignored the hierarchy and obtained 70 ground-truth classes as a flat partitioning. 4. RCV1 [17] is a much larger collection of news articles from Reuters. It contains over 800,000 articles in the time period of 1996-1997 and defines a sophisticated topic hierarchy with 103 labels. We discarded documents with multiple class labels, and then selected the 40 largest classes, named as RCV1-labeled . The full data set, named as RCV1-full , is also included in our experiments with no ground-truth classes.
We summarize these data sets in Table 2. All the data sets except 20 Newsgroups have very unbalanced sizes of ground-truth classes. We constructed the normalized-cut weighted version of term-document matrices as in [24].
The clustering methods in our experiments are named as follows 3 : http://www.daviddlewis.com/resources/ testcollections/reuters21578/ http://qwone.com/~jason/20Newsgroups/
We also compared our method with the off-the-shelf clus-tering software CLUTO [9]. In most cases, our method is faster than CLUTO configured by default, with comparable clustering quality. When both methods are terminated af-ter 10 iterations, our method costs 104 seconds on RCV1-full data set while CLUTO costs 398 seconds.
Hierarchical clustering and flat clustering cannot be com-pared against each other directly. We evaluate the hierarchy by taking snapshots of the tree as leaf nodes are generated, and because leaf nodes are non-overlapping, we treat all the leaf nodes in each snapshot as a flat partitioning. Thus, if the maximum number of leaf nodes is c , we produce c  X  1 flat partitionings forming a hierarchy.

For each method, we perform 20 runs on medium-scale data sets and 5 runs on large-scale data sets starting from random initializations. Average measurements are reported. Note that for flat clustering methods, each run consists of c  X  1 separate executions with the number of clusters set to 2 , 3 ,  X  X  X  ,c .

The maximum number of leaf nodes c is set to be the num-ber of ground-truth classes at the deepest level for labeled data sets (see Table 2); and we set c = 60 for RCV1-full. The hierarchical clustering workflow (Algorithm 3) runs with pa-rameters  X  = 9, T = 3 (for r2-nmf-hier and nmf-hier ) or 5 (for kmeans-hier ). The Matlab kmeans function has a batch-update phase and a more time-consuming online-update phase. We rewrote this function using BLAS3 op-erations and boosted its efficiency substantially 5 . We use http://mallet.cs.umass.edu/ http://www.cc.gatech.edu/~dkuang3/software/ kmeans3.html both phases for medium-scale data sets and only the batch-update phase for large-scale data sets. For NMF, we use the projected gradient as the stopping criterion and = 10  X  4 where a tolerance parameter is defined in [18]. All the methods are implemented with multi-threading.
Each of the six methods described above can be regarded as both a clustering method and a topic modeling method. We use the following two measures to evaluate their quality: 1. Normalized mutual information (NMI) : This is a mea-sure of the similarity between two flat partitionings. It is used to evaluate clustering quality and is only applicable to data sets with ground-truth classes. It is particularly useful when the number of generated clusters is different from that of ground-truth classes and can be used to determine the op-timal number of clusters. More details can be found in [19]. For data sets with defined hierarchy, we compute NMI be-tween a generated partitioning and the ground-truth classes at each level of the tree; if the tree has depth L , then we compute L measures corresponding to each level. For hier-archical clustering following Algorithm 3, we treat all the outliers as one separate cluster for fair evaluation. 2. Coherence : This is a measure of intra-topic similarity in topic models [21, 1]. Given the top words f 1 ,  X  X  X  ,f a topic, coherence is computed as where D ( f i ) is the document frequency of f i . D ( f i the number of documents that contain both f 1 and f 2 , and  X  is a smoothing parameter. We use  X  = 1 and K = 20 [21]. The coherence averaged over all the topics is reported. Timing results of the six methods are shown in Fig. 3. Hierarchical clustering based on rank-2 NMF is much faster than flat clustering using NMF or LDA. These results have verified our complexity analysis in Section 2, that flat clus-tering based on standard NMF exhibits a superlinear trend while hierarchical clustering based on rank-2 NMF exhibits a linear trend of running time as k increases. The first two plots correspond to medium-scale data sets, and r2-nmf-hier only requires about 1 / 3 the time needed by nmf-hier . The other three plots correspond to large-scale data sets, where we use logarithmic scale. K-means with only the batch-update phase also runs fast; however, their cluster-ing quality is not as good, which will be shown later.
The difference between our proposed algorithm and the original active-set based algorithm for rank-2 NMF is less substantial as the data size increases. The performance is mainly bounded by the computation of Y T B in Algorithm 2. Because B  X  R m  X  2 + is a very long-and-thin matrix, Y T sentially behaves like a sparse matrix-vector multiplication, which is a memory-bound operation. However, r2-nmf-hier is still much faster than all the other methods: On RCV1-full data set, r2-nmf-hier , nmf-hier , and nmf-flat cost about 7 minutes, 12 minutes, and 4.5 hours, respectively.
Clustering quality is evaluated on labeled data sets, shown in Fig. 4. The plot for the Cora data set is omitted for space reasons. For data sets with a defined hierarchy, ground-truth classes on the first 3 levels are used for evaluation, and those on deeper levels produce similar results. nmf-hier has identical results with r2-nmf-hier , thus is not shown here. r2-nmf-hier is a very competitive method in general. NMF-based methods give stably good clustering quality us-ing both the flat and hierarchical schemes. Compared to nmf-flat , we can clearly see the improved NMI values of r2-nmf-hier . Although kmeans-flat achieves comparable performances on RCV1-labeled, it performs poorly on other data sets. A general trend is that the improvement in clus-tering quality by r2-nmf-hier is more substantial when a deeper level of the defined hierarchy is used for evaluation, which correspond to more elaborated ground-truth classes.
We note that if NMI values are used for selecting the best number of clusters for a data set, r2-nmf-hier and nmf-flat frequently give different numbers (see the last two plots in ent ways. We also note that although kmeans-hier uses the same hierarchical clustering workflow as r2-nmf-hier , it performs poorly in most cases. The coherence results for all the data sets are shown in Fig. 5. None of these methods have consistent performances when the number of clusters k is small; when k is large, r2-nmf-hier gives the highest coherence value in 3 out of 5 cases. On RCV1 data set, r2-nmf-hier is a stably good method in terms of topic coherence, while nmf-flat and kmeans-hier have comparable performances sometimes but perform very poorly otherwise. More study is needed to understand the benefits of each method in terms of topic coherence.
Hierarchical document clustering has a rich history in data analysis and management [23]. In this paper, we considered the divisive approach, which splits a data set in the top-down fashion and offers a global view of the data set compared to agglomerative clustering methods. In divisive hierarchical clustering, a clustering method is needed at each splitting step. However, it is not as easy as recursively applying any flat clustering method available to generate a tree struc-ture. As can be seen in our experiments, the widely-used K-means clustering, when applied to hierarchical clustering, frequently generates very unbalanced clusters that lead to a poor organization of a corpus.

A good combination of a flat clustering method and a way to determine the next node to split is important for efficient and practical hierarchical clustering. In this paper, we pro-posed such a combination and showed its promising perfor-mance compared to other clustering methods such as NMF and LDA. For the efficiency of each splitting step, we de-signed a fast active-set-type algorithm for rank-2 NMF. Our algorithm has redundant computation but has continuous memory access, allowing better use of the cache; thus, it is faster than existing active-set-type algorithms. We also pro-posed a scoring method in the hierarchical clustering work-flow, which provides a way to evaluate the potential of each leaf node to be split into two well-separated clusters and can be used to determine when to stop splitting. Outlier detec-tion is also included in the overall workflow. Our method generated a binary tree structure of the full RCV1 data set in 7 minutes on a shared-memory machine with 2 quad-core CPUs, compared to standard NMF which costs 4.5 hours.
We conclude by listing several shortcomings of the current method for further research. First, after a node is split, each document has a hard assignment to one of the two generated leaf nodes. It would be more flexible to enable soft assign-ments. Second, the performance of our proposed algorithm for rank-2 NMF is bounded by that of sparse matrix-vector multiplication (SpMV) when the data size is very large. The efficiency of our algorithm can be further boosted by using a more efficient SpMV implementation or moving to a dis-tributed platform. Currently, our method can be used to build a hierarchical organization of documents efficiently on a single machine, possibly as part of a large machine learning infrastructure with many machines. This work was supported in part by the National Science Foundation (NSF) grants CCF-0808863 and CCF-0732318. The authors would like to thank the anonymous reviewers for their valuable comments and Daniel Lee for his careful language editing. [1] S. Arora, R. Ge, Y. Halpern, D. M. Mimno, [2] D. M. Blei, T. L. Griffiths, M. I. Jordan, and J. B. [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [4] J.-P. Brunet, P. Tamayo, T. R. Golub, and J. P. [5] C. Ding and X. He. Cluster merging and splitting in [6] L. Grippo and M. Sciandrone. On the convergence of [7] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [8] Y. Jung, H. Park, D.-Z. Du, and B. L. Drake. A [9] G. Karypis. Cluto  X  a clustering toolkit. Technical [10] H. Kim and H. Park. Sparse non-negative matrix [11] H. Kim and H. Park. Nonnegative matrix factorization [12] J. Kim, Y. He, and H. Park. Algorithms for [13] J. Kim and H. Park. Toward faster nonnegative [14] D. Kuang, C. Ding, and H. Park. Symmetric [15] C. L. Lawson and R. J. Hanson. Solving Least Squares [16] D. D. Lee and H. S. Seung. Learning the parts of [17] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1: [18] C.-J. Lin. Projected gradient methods for nonnegative [19] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [20] A. K. McCallum, K. Nigam, J. Rennie, and [21] D. Mimno, H. M. Wallach, E. Talley, M. Leenders, [22] M. H. Van Benthem and M. R. Keenan. Fast [23] P. Willett. Recent trends in hierarchic document [24] W. Xu, X. Liu, and Y. Gong. Document clustering
