 Universidade da Coru  X  na Uppsala University
Transition-based parsing is a widely used approach for dependency parsing that combines high of the known systems for projective dependency parsing can be viewed as variants of the same treebanks and find a very good fit for 2-planar trees. We end with an experimental evaluation showing that our 2-planar parser gives significant improvements in parsing accuracy over the and performs on a par with the widely used arc-eager pseudo-projective parser. 1. Introduction
Syntactic parsing using dependency-based representations has attracted considerable interest in computational linguistics in recent years, both because it appears to provide a useful interface to downstream applications of parsing and because many dependency parsers combine competitive parsing accuracy with highly efficient processing. Among the most efficient systems available are transition-based dependency parsers, which perform a greedy search through a transition system, or abstract state machines, that map sentences to dependency trees, guided by statistical models trained on treebank data (Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; Attardi 2006; Zhang and Clark 2008). Transition systems for dependency parsing come in many different varieties, and our aim in the first part of this article is to deepen our understanding of these systems by analyzing them in a uniform framework.
 literature can all be viewed as variants of a stack-based system with five elementary transitions, where different variants are obtained by composing elementary transitions into complex transitions and by adding restrictions on their applicability. We call such systems divisible transition systems and prove a number of theoretical results about their expressivity (which classes of dependency graphs they can handle) and their com-plexity (what upper bounds exist on the length of transition sequences). In particular, we show that an important subclass called efficient divisible transition systems derive planar dependency graphs in time that is linear in the length of the sentence using stan-dard inference methods for transition-based dependency parsing. Even though many of these results were already known for particular systems, the general framework allows us to derive these results from more general principles and thereby to establish connections between previously unrelated systems. We then go on to show that there explored, notably a system that is sound and complete for planar dependency trees, a mild extension to the class of projective trees that are assumed in most existing systems. departure for addressing the problem of non-projective dependency parsing. Despite the impressive results obtained with dependency parsers limited to strictly projective dependency trees X  X hat is, trees where every subtree has a contiguous yield X  X t is clear that most if not all languages have syntactic constructions whose analysis requires non-makes parsing computationally hard (McDonald and Satta 2007) and does not seem justified by the data in available treebanks (Kuhlmann and Nivre 2006; Nivre 2006a;
Havelka 2007). This suggests that we should try to find a superset of projective trees that is permissive enough to encompass constructions found in natural language yet restricted enough to permit efficient parsing. Proposals for such a set include trees with bounded arc degree (Nivre 2006a, 2007), well-nested trees with bounded gap degree (Kuhlmann and Nivre 2006; Kuhlmann and M  X  ohl 2007), as well as trees parsable by a particular transition system such as that proposed by Attardi (2006).
 generalizes the simple notion of planarity by saying that a dependency tree is k -planar if it can be decomposed into at most k planar subgraphs, a proposal that remains largely unexplored because an efficient test for k -planarity has been lacking. In this article, we construct a test for k -planarity by reducing it to a graph coloring problem. Applying this test to a wide range of dependency treebanks, we show that, although simple planarity (or 1-planarity) is clearly insufficient (Kuhlmann and Nivre 2006), the set of 2-planar the previously proposed superclasses of projective trees. We then demonstrate how the transition system for planar dependency parsing can be generalized to k -planarity by introducing additional stacks. In particular, we define a two-stack system for 2-planar dependency parsing that is provably correct and has linear complexity. Finally, we show that the 2-planar parser, when evaluated on data sets with a non-negligible proportion corresponding 1-planar and projective parsers, and provides comparable accuracy to the widely used arc-eager pseudo-projective parser. 800 concepts of dependency parsing and in particular the formalization of stack-based divisible transition systems, proves a number of theoretical results about the expres-system for 1-planar dependency parsing. Section 4 reviews the notion of multiplanarity, introduces an efficient procedure for determining the smallest k for which a dependency dependency treebanks. Section 5 shows how the divisible transition system framework additional stacks, presents proofs of correctness and complexity for the 2-planar case, and reports the results of an experimental evaluation of projective, pseudo-projective, 1-planar and 2-planar dependency parsing. Section 6 reviews related work, and Section 7 concludes and makes suggestions for future research.
 1-planar and 2-planar parsers) have been published previously by G  X  omez-Rodr  X   X guez and Nivre (2010); this article substantially revises and extends the ideas presented in that paper. The framework of divisible transition systems and all the derived theoretical results, including the properties and proofs regarding the 1-planar and 2-planar parsers, are entirely new contributions of this article. 2. Dependency Parsing
Dependency parsing is based on the idea that syntactic structure can be analyzed in terms of binary, asymmetric relations between the words of a sentence, an idea that has and Panevov  X  a 1986; Mel X   X  cuk 1988; Hudson 1990). In computational linguistics, depen-dency structures have become increasingly popular in the interface to downstream applications of parsing, such as information extraction (Culotta and Sorensen 2004;
Stevenson and Greenwood 2006; Buyko and Hahn 2010), question answering (Shen and Klakow 2006; Bikel and Castelli 2008), and machine translation (Quirk, Menezes, and Cherry 2005; Xu et al. 2009). And although dependency structures can easily be extracted from other syntactic representations, such as phrase structure trees, this has also led to an increased interest in statistical parsers that specifically produce depen-dency trees (Eisner 1996; Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; McDonald, Crammer, and Pereira 2005).
 graph-based and transition-based techniques (McDonald and Nivre 2007). Graph-based parsers parameterize the parsing problem by the structure of the dependency trees and learn models for scoring entire parse trees for a given sentence. Many of these models permit exact inference using dynamic programming (Eisner 1996; McDonald,
Crammer, and Pereira 2005; Carreras 2007; Koo and Collins 2010), but recent work has explored approximate search methods in order to widen the scope of features especially when processing non-projective trees (McDonald and Pereira 2006; Riedel and Clarke 2006; Nakagawa 2007; Smith and Eisner 2008; Martins, Smith, and Xing 2009; Koo et al. 2010; Martins et al. 2010). Transition-based parsers parameterize the parsing problem by the structure of a transition system, or abstract state machine, for mapping sentences to dependency trees and learn models for scoring individual transitions from one state to the other. Traditionally, transition-based parsers have relied on local optimization and  X  greedy, deterministic parsing (Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; Attardi 2006; Nivre 2008), but globally trained models and non-greedy parsing methods such as beam search are increasingly used (Johansson and Nugues 2006; Titov and Henderson 2007; Zhang and Clark 2008; Huang, Jiang, and Liu 2009; Huang and
Sagae 2010; Zhang and Nivre 2011). In empirical evaluations, the two main approaches to dependency parsing often achieve very similar accuracy, but transition-based parsers tend to be more efficient. In this article, we will be concerned exclusively with transition-based models.
 resentations used by dependency parsers, starting from a general characterization of dependency graphs and discussing a number of different restrictions of this class that will be relevant for the analysis later on. We then go on to review the formalization of transition systems proposed by Nivre (2008), and in particular the class of stack-based systems that provides the framework for our discussion of existing and novel transition-based models. Finally, we discuss the implementation of efficient parsers based on these transition systems. 2.1 Dependency Graphs
In dependency parsing, the syntactic structure of a sentence is modeled by a depen-dency graph , which represents each token and its syntactic dependents through labeled, directed arcs. This is exemplified in Figure 1 for a Czech sentence taken from the Prague
English sentence taken from the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993; Marcus et al. 1994). 1 In the former case, an artificial token at the beginning of the sentence, serving as the unique root of the graph and ensuring that the graph is a tree even if more than one token is independent of all other tokens.
In the latter case, no such device has been used, and we will not in general assume the existence of an artificial root node prefixed to the sentence, although all our models will be compatible with such a device. 802
Economic 1 Definition 1 1. V = { 1, ... , n } is a set of nodes, 2. A  X  V  X  V is a set of directed arcs, containing no loops (i.e., arcs of the corresponding to the linear position of a token in the sentence (where the first token may or may not be the special token ROOT ). The set A of arcs (or directed edges )isa set of pairs ( i , j ), where i and j are distinct nodes. Because arcs are used to represent dependent of i . A node with no incoming arcs is called a root .
 node k if min( i , j ) &lt; k &lt; max( i , j ).
 consisting of a head i , a label l , and a dependent j , but excluding labels for now will discuss the generalization to labeled dependency graphs whenever relevant, and the experiments reported in Section 5 all use labeled graphs.
 Definition 2
Let G = ( V , A ) be a dependency graph. 1. S INGLE -H EAD ( G )  X  every node in G has at most one incoming arc. 2. A CYCLIC ( G )  X  there are no (directed) cycles in G . 3. C ONNECTED ( G )  X  G is weakly connected. 4. T REE ( G )  X  G is a directed tree. 5. P LANAR ( G )  X  there are no crossing arcs in G . 6. N O -C OVERED -R OOTS ( G )  X  there is no root covered by an arc in G . Definition 2 lists a number of constraints that can be imposed on dependency graphs.
The most common of these is the T REE constraint, which requires that there is a root from which all other nodes are reachable by a unique directed path, and which in turn entails S INGLE -H EAD ,A CYCLIC ,andC ONNECTED . A dependency graph that satisfies the T REE constraint is called a dependency tree . we have extended them to apply to dependency graphs in general. The most common the requirement that every subtree must have a contiguous yield and rules out both crossing arcs and covered roots. By contrast, the P LANAR relaxation because there can be at most one covered root without violating the T constraint.
 Example 1
Consider the dependency graphs depicted in Figures 1 and 2, ignoring labels for the time being:
G satisfies T REE (hence also S INGLE -H EAD ,A CYCLIC ,andC C
OVERED -R OOTS but violates P LANAR (hence also P ROJECTIVE crossing arcs. By contrast, G 2 satisfies all constraints listed in Definition 2. 2.2 Transition Systems for Dependency Parsing
Transition-based dependency parsing is based on the notion of a transition system ,or abstract state machine, for mapping sentences to dependency graphs. Such systems are nondeterministic in general and usually combined with heuristic search, guided by a treebank-induced function for scoring different transitions out of a given configuration.
For the time being, we will ignore the details of the search procedure and concentrate on the underlying transition systems. We will adopt the general framework of Nivre (2008) but restricted to stack-based systems. 2 Definition 3
A transition system for dependency parsing is a quadruple S = ( C , T , c 1. C is a set of configurations , each of which contains a buffer  X  of 2. T is a set of transitions , each of which is a (partial) function t : C 3. c s is an initialization function , mapping a sentence x = w 4. C t  X  C is a set of terminal configurations . 804 where  X  is a stack of nodes,  X  is a buffer of nodes, and A is a set of dependency arcs; the initialization function is c s ( x ) = ([ ], [1, ... , n ], terminal configurations is C t = { c | c = (  X  ,[ ], A ) for any  X  , A configuration c ;weuse |  X  | and |  X  | to refer to the size of  X  and  X  (i.e., the number of nodes), and we use [ ] to denote an empty stack or buffer.
 Definition 4
Let S = ( C , T , c s , C t ) be a transition system. A transition sequence w , ... , w n in S is a sequence C 0, m = ( c 0 , c 1 , ... , c 1. c 0 = c s ( x ), 2. c m  X  C t , 3. for every i (1  X  i  X  m ), c i = t ( c i  X  1 ) for some t
The parse assigned to x by C 0, m is the dependency graph G
A manipulate  X  ,  X  ,and A until a terminal configuration is reached (  X  is empty). Be-cause the node set V is given by the input sentence itself, the set A dency arcs in the terminal configuration will determine the output dependency graph
G Definition 5
Let S = ( C , T , c s , C t ) be a transition system for dependency parsing. 1. S is sound for a class G of dependency graphs if and only if, for every 2. S is complete for a class G of dependency graphs if and only if, for every 3. S is correct for a class G of dependency graphs if and only if it is sound
As observed by Nivre (2008), soundness and completeness for transition systems are analogous to soundness and completeness for grammar parsing algorithms, according to which an algorithm is sound if it only derives parses licensed by the grammar and complete if it derives all such parses (Shieber, Schabes, and Pereira 1995). Example 2 Nivre X  X  (2008) arc-standard transition system uses three transitions: The unlabeled dependency graph in Figure 2 is derived by the transition sequence in
Figure 3. For labeled dependency parsing, the L EFT -A RC addition have a parameter for the label l of the arc being added. 2.3 Transition-Based Parsing
A transition system is an abstract machine that computes the mapping of a sentence to a dependency graph through a sequence of steps called transitions. In order to build a practical parsing system on top of this, we essentially need two additional components: a model for scoring transition sequences and an algorithm for finding the optimal transition sequence for a given sentence. Although many different scoring models are conceivable, practically all existing parsers use a linear model for scoring individual transitions whose scores are then added to get the score for an entire sequence: where f ( c i  X  1 , t i ) is a feature vector representation of transition t c 1 and w is a corresponding weight vector. Finding the highest scoring transition L EFT -A RC  X  ([], [2, ... ,9], A 1 = { (2, 1) } ) L EFT -A RC  X  ([], [3, ... ,9], A 2 = A 1  X  X  (3, 2) } ) L EFT -A RC  X  ( [3], [5, ... ,9], A 3 = A 2  X  X  (5, 4) } ) L EFT -A RC  X  ( [3, 5, 6], [8, 9], A 4 = A 3  X  X  (8, 7) } ) R IGHT -A RC  X  ( [3, 5], [6, 9], A 5 = A 4  X  X  (6, 8) } ) R IGHT -A RC  X  ( [3], [5, 9], A 6 = A 5  X  X  (5, 6) } ) R IGHT -A RC  X  ( [ ], [3, 9], A 7 = A 6  X  X  (3, 5) } ) R IGHT -A RC  X  ( [ ], [3], A 8 = A 7  X  X  (3, 9) } ) 806 sequence under this model is a hard problem in general, and transition-based parsers systems simply use greedy 1-best search (Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; Attardi 2006):
Another common approach is to use beam search with a fixed beam size (Johansson and Nugues 2006; Titov and Henderson 2007; Zhang and Clark 2008). In this case, lines 3 and 4 are replaced by an inner loop that expands all configurations in the current beam using all permissible transitions and then discards all except the k highest scoring configurations. The outer loop terminates when all configurations in the beam are terminal, and the dependency graph corresponding to the highest scoring configuration is returned. Setting the beam size to 1 makes this equivalent to greedy 1-best search. ing transition system but also on the scoring model and the search algorithm. As long as the number of configurations considered by the search algorithm is bounded by a constant k and as long as every transition can be scored and executed in constant time sequences in S (Nivre 2008). Similarly, the space complexity is given by an upper bound on the size of a configuration c  X  C , because at most k configurations need to be stored at any given time. For most of the systems considered in this article, we will see that the length of a transition sequence is O ( n ), where n is the length of the input sentence, which translates into a linear bound on parsing time for transition-based parsers using beam search (with greedy 1-best search as a special case).
 low parsing complexity in combination with very few restrictions on feature repre-optimal transition sequence is found. Recent work on tabularization for transition-based parsing has shown that it is possible to use exact dynamic programming under certain conditions, but this leads either to very inefficient parsing or to very restricted feature representations. Thus, Huang and Sagae (2010) present a dynamic programming scheme for a feature-rich arc-standard parser, but the resulting parsing complexity is O ( n 7 ) and they therefore have to resort to beam search in practical parsing experiments.
Conversely, Kuhlmann, G  X  omez-Rodr  X   X guez, and Satta (2011) show how to obtain cubic complexity for a tabularized arc-eager parser but only for very impoverished feature representations. Hence, for the remainder of this article, we will assume that transition transition-based parsers currently do. This bound holds as long as every transition can be scored and executed in constant time, which is true even when including complex features like the valency features of Zhang and Nivre (2011), which are expensive to use in dynamic programming because of the combinatorial effect they have on parsing complexity. 3. Divisible Transition Systems based transition systems, which differ from each other in the order in which they add dependency arcs as well as in the constraints that they impose on output dependency graphs. In their original definitions, these differences arise from the fact that each algo-rithm uses a distinct set of transitions. In this section, we show how these algorithms can be expressed using a common set of transitions, which we call elementary transi-tions . Under this framework, the original transitions of each algorithm are viewed as combinations of one or more elementary transitions by means of the standard function operations of composition and restriction. A direct consequence of this is that each of the parsers expressed in this framework can be viewed as a restriction of the algorithm that uses elementary transitions directly, allowing any possible concatenation of elementary transition systems .
 that can be applied to stack-based configurations:
The first three operations modify the stack and/or buffer by moving a word from the (U
NSHIFT ), or popping a word from the stack (R EDUCE ). The remaining two operations create dependency arcs involving the top of the stack and the first word in the buffer (L
EFT -A RC ,R IGHT -A RC ). We assume that L EFT -A RC configurations where the new arc is not already an element of the arc set A , an assump-sequences where the same arc is added an indefinite number of times). Note that, in the case of labeled dependency graphs, the L EFT -A RC and R have a label parameter, and this restriction should not prevent the addition of an arc with the same head and dependent as one or more existing arcs, as long as the label is different.
 transitions, which is defined as standard function composition.
 Definition 6
Let t 1 , t 2 : C  X  C be transitions. Their composition is the partial function t mapping each c  X  C to t 2 ( t 1 ( c )).

Elementary transitions are defined as partial functions t : C to the set of elementary transitions. In addition, we use function restriction to impose constraints on their domain, traditionally expressed in the literature as side conditions. 808
For this purpose, we use the standard notation by which the restriction of a function f : X  X  Y to a subset A  X  X is written as:
Transition systems that can be defined using composition of elementary transitions with restrictions are said to be divisible .
 Definition 7 tion in T is of the form t 1 s transitions can be written as a composition of restrictions of the elementary transitions S
HIFT ,U NSHIFT ,R EDUCE ,L EFT -A RC ,andR IGHT -A RC . Note that the definition allows the use of unrestricted elementary transitions in the composition, because for any transition t , we have that t C = t . 4 3.1 Examples of Divisible Transition Systems
In this section, we show that a number of transition-based parsers from the literature use divisible transition systems that can be defined using only elementary transitions. This includes the arc-eager and arc-standard projective parsers described in Nivre (2003) and Nivre (2008), the arc-eager and arc-standard parsers for directed acyclic graphs from
Sagae and Tsujii (2008), the hybrid parser of Kuhlmann, G  X  omez-Rodr  X   X guez, and Satta (2011), and the easy-first parser of Goldberg and Elhadad (2010). We also give examples of transition systems that are not divisible (Attardi 2006; Nivre 2009).

The set H  X  ( C ) is the subset of configurations where the node on top of the stack has been assigned a head in A ,and H  X  ( C ) is the subset where the top node has not been assigned a head in A . Similarly, the set H  X  ( C ) is the subset of configurations where the first node in the buffer has been assigned a head in A ,and H first node has not been assigned a head in A . Note that there are configurations that are neither in H  X  ( C )norin H  X  ( C ), namely, those where the stack is empty. There are also configurations that are neither in H  X  ( C )norin H  X  ( C ), because the buffer is empty, but these are all terminal configurations.
 Example 3 parser for projective dependency trees. Its transitions can be defined in terms of ele-mentary transitions as follows:
The S HIFT AS transition is the same as the elementary S HIFT transition composes the elementary L EFT -A RC transition with the R ensure that the left dependent of the new arc is popped from the stack and therefore cannot be assigned more than one head. The R IGHT -A RC AS four elementary transitions, where R IGHT -A RC is responsible for adding a left-headed arc, S HIFT and R EDUCE jointly remove the dependent of the new arc from the buffer, and U NSHIFT moves the head of the new arc back to the buffer so that it can find a head to the left. It is worth noting that the arc-standard system for projective trees does not make use of restrictions.
 in Nivre (2008), where arcs are created involving the topmost stack node and the first buffer node, the system has also been presented in an equivalent form with arcs built be-tween the two top nodes in the stack (Nivre 2004). This variant can also be described as a divisible transition system, with L EFT -A RC AS = U NSHIFT Example 4
Nivre X  X  (2003) arc-eager parser is a parser for projective dependency trees, which adds arcs in a strict left-to-right order using the following transitions: As in the first example, the S HIFT AE transition is equivalent to the elementary S transition, but the R IGHT -A RC AE transition differs from R 810 dependents are removed from the stack in a separate transition R equivalent to the elementary transition R EDUCE but restricted to H unattached nodes are not removed. The L EFT -A RC AE transition, finally, is the same as L system where nodes on the stack can never have a head.
 Example 5
The easy-first parser of Goldberg and Elhadad (2010) is a parser for projective trees that adds arcs in a bottom X  X p order but in a non-directional manner, trying to make the easier attachment decisions first regardless of the position of the corresponding words in the sentence. This parsing strategy corresponds to the following divisible transition system: S A A infinite set of transitions. In practice, however, only the A A
TTACH -L EFT ( i ) EF transitions such that 1  X  i  X  n  X  1 need to be considered when pars-ing a string of length n : Because the number of nodes in the buffer is bounded by n , transitions with i  X  n will always be undefined because the buffer will become empty before the first i + 1 elementary transitions can be applied. Therefore, to parse strings of length n we only need 2 n  X  1 transitions.
 ward (or leftward) arc involving the i th and ( i + 1)th words in the input string, and then remove the dependent. This means that the system is not limited to building rithm that assigns a weight to each of the A TTACH -R IGHT transitions in such a way that  X  X asier X  (more reliable) attachments are performed first.
 same as L EFT -A RC AS and R IGHT -A RC AS in the arc-standard system, but preceded by i instances of S HIFT and succeeded by i  X  1 instances of U separate S HIFT transition is needed only to reach a terminal configuration by pushing the final root(s) onto the stack. This analysis reveals that the two systems are similar in which arcs are added. It is worth pointing out that using sequences of S U
NSHIFT transitions is not the most efficient way of implementing easy-first parsing in practice.
 Example 6
The hybrid parser introduced by Kuhlmann, G  X  omez-Rodr  X   X guez, and Satta (2011) is a bottom X  X p projective transition system that builds each given dependency tree in a unique order, rather than allowing each node to collect its dependents in different orders like the arc-standard or easy-first systems. Its transitions can be defined as follows:
Note that this parser creates leftward arcs between the first node in the buffer and the top node on the stack, just like arc-standard and arc-eager. Rightward arcs, however, are created by making the topmost stack node a dependent of the second topmost stack node, and removing the former from the stack.
 Example 7
Sagae and Tsujii X  X  (2008) arc-standard DAG parser performs bottom X  X p parsing without the common assumption that syntactic structures are represented as trees, allowing nodes to have multiple heads. It uses the following transitions: standard parser, the L EFT -A TTACH DS and R IGHT -A TTACH L
EFT -R EDUCE DS and R IGHT -R EDUCE DS in that they do not remove the dependent of the new arc, thus allowing it to have additional incoming arcs. The restrictions on these transitions disallow the creation of both a left and a right arc between the same pair of nodes. Note that the class of dependency structures that can be output by this system does not exactly correspond to DAGs, however, because the system allows transition sequences that create dependency graphs with cycles. For example, starting from any rise to a cyclic structure.
 Example 8
Sagae and Tsujii X  X  (2008) arc-eager DAG parser allows nodes with multiple heads like the previous one but adds arcs in a strict left-to-right order like Nivre X  X  (2003) arc-eager parser. The transition system can be defined as follows: 812 whereas the L EFT -A RC DE and R IGHT -A RC DE transitions differ from their counterparts L
EFT -A RC AE and R IGHT -A RC AE by not removing the dependent of the new arc. Like the previous arc-standard system, this system can produce cyclic dependency graphs. For example, starting from any configuration with at least one node in the stack and two nodes in the buffer and applying R IGHT -A RC DE ,S HIFT DE and L EFT -A RC DE creates a cycle of length 3.

Before we close this section we will briefly consider two transition systems that are not divisible. Attardi X  X  (2006) non-projective parser extends the arc-standard sys-following:
Nivre X  X  (2009) non-projective parser constructs non-projective arcs in an indirect fashion by first swapping the order of two adjacent nodes on the stack:
Neither of these systems can be formalized using only elementary transitions as system. 3.2 Properties of Divisible Transition Systems
The elementary transition framework not only allows us to describe a wide range of prove formal properties of transition systems. To do so, we consider the successions of transitions allowed by these algorithms, and break their transitions up into chains of elementary transitions.
 Definition 8 tem S .The standard transition chain associated with C 0, m
T 0, m = ( t 1 , t 2 , ... , t m ) such that t i ( c i  X  1 ) = c Definition 9 under a divisible transition system S . Its associated elementary transition chain is the sequence of elementary transitions such that t 1 = t 1,1 s values of the restrictions s 1,1 , ... , s 1, p Definition 10
C 0, m = ( c 0 , c 1 , ... , c m ). Then:
After these preliminaries, we will now prove a number of results about divisible transition systems, first about the classes of dependency graphs that they can derive (Section 3.2.1) and secondly about termination and parsing complexity (Section 3.2.2). 3.2.1 Constraints on Dependency Graphs. Here we consider properties related to the graph Proposition 1
If all elementary R EDUCE transitions in the elementary transition chains under S are applied to configurations in H  X  ( C ), then no dependency graph generated by S contains covered roots.
 This property implies that algorithms where R EDUCE transitions are restricted to the set
H ( C ) always satisfy the N O -COVERED -ROOTS constraint. Note that this restriction may be expressed explicitly in the transition definitions (as in Example (4)), but it may also be implicit. For example, in Example (3), we defined L EFT -A Although we did not explicitly write L EFT -A RC ;R EDUCE tion always produces configurations that are trivially in H gives the topmost stack node a head), so the R EDUCE properties.
 Proof
To prove this proposition, we first make some simple observations about divisible transition systems that will be useful for this and subsequent proofs.
 Lemma 1
In every configuration in an (elementary) transition sequence under a divisible transi-tion system S , elements in the stack and buffer are ordered, that is, if the configuration is of the form ([ s 1 , ... , s k ], [ b 1 , ... , b l ], A ), then we know that s
This can be easily seen by induction. It holds in initial configurations, because the stack is empty and the buffer is ordered, and all of the elementary transitions preserve the order of the nodes. Note that this lemma implies that a node cannot be in both the stack and the buffer of the same configuration. 814 Lemma 2 a divisible transition system S . Then, we have that  X  ( c { (elementary) transition sequence or, in plain language, that a node that is removed from the stack and buffer can never be placed back there by elementary transitions. This can be easily seen by observing that the transitions S HIFT ,U A
RC leave the set  X  unchanged, whereas the R EDUCE transition removes one element from it by popping the stack.
 Lemma 3
Let E 0, m = ( e 0 , e 1 , ... , e q )and C 0, m = ( c 0 , c its corresponding elementary transition sequence under a divisible transition system S .
If v  X   X  ( c i ) for some v  X  [0, n ]and i  X  [0, q ], then there exists some j e = R EDUCE and c j  X  1 has v on the top of the stack. This amounts to saying that the only way an element can be removed from the set  X  in a divisible system is by a R transition, as observed earlier. Thus, whenever a token v is not present in  X  ( c given configuration c i , we can assume that it was previously popped by a R transition applied to a configuration that had v on the top of the stack. not in H  X  ( C ). Let G be a dependency graph in which the node j is a root, covered by an arc connecting the nodes i and k ( i &lt; k ). If a transition sequence C then it must apply a L EFT -A RC or R IGHT -A RC transition to a configuration having i at the top of the stack and k as the first element in the buffer, which is the only way of adding the arc involving i and k . By Lemma 1, we know that in that configuration c , j  X   X  ( c ). By Lemma 3, we know that there must thus be a previous application of a R
EDUCE transition with j on the top of the stack. Because j is a root, by definition this configuration is not in H  X  ( C ), and the proposition is proved.
 Proposition 2
If all the elementary L EFT -A RC transitions in the elementary transition chains under S are applied to configurations in H  X  ( C ), and all the R are applied to configurations in H  X  ( C ), then all the dependency graphs generated by S obey the S INGLE -H EAD constraint.
 Proof
The proof of this proposition is straightforward. Because elementary transitions it, an elementary transition sequence will generate a graph violating the S constraint if and only if it contains a L EFT -A RC or R IGHT incoming arc to a node that already has a head in the graph.
 Proposition 3
If all the elementary L EFT -A RC and R IGHT -A RC transitions in the elementary transition chains under S are applied to configurations (  X  | i , j different connected components of the undirected graph underlying A , then the un-directed graphs underlying all the dependency graphs generated by S are acyclic (i.e., the dependency graphs generated by S have no undirected cycles). Note that this in turn Proof the undirected graph underlying the generated dependency graph if an arc is added between nodes that are already connected.
 Proposition 4 All dependency graphs generated by a divisible system S are planar.
 Proof can show that there is no elementary transition chain that creates such a pair of arcs. Therefore, divisible systems can only generate planar dependency graphs.

The properties considered in this section can be used as a tool set for easily proving the soundness of transition systems with respect to different sets of dependency graphs, as well as for designing new transition systems. We exemplify the former in Example (9) and the latter in Section 3.3.
 Example 9 convenience: 816 We can easily conclude the following: 3.2.2 Termination and Complexity. In general, there are two ways in which a transition-based parser may fail to parse a given input sentence. On the one hand, it may terminate in a non-terminal configuration where no transition can be applied. On the other hand, it may fail to terminate at all, because the system allows an infinite sequence of transitions.
We say that a system is robust if it can never get stuck in a non-terminal configuration and bounded if it does not permit infinite loops.
 Definition 11 terminal configuration c  X  C \ C t , there is some transition t Definition 12 non-terminal configuration c  X  C \ C t and (non-empty) sequence of transitions t ( t  X  T ) such that t
In this section, we first provide sufficient conditions for robustness and boundedness and then go on to discuss the parsing complexity for a subset of divisible systems that are guaranteed to be robust and bounded.
 Proposition 5 Let S = ( C , T , c s , C t ) be a divisible transition system. If S Proof ration that has a non-empty buffer  X  , which by definition includes every non-terminal configuration.

Thus, in order to guarantee robustness, it is enough that a divisible transition system includes the elementary S HIFT transition. This is the case for all the divisible systems convenient to introduce three auxiliary functions that characterize the effect a transition t has on an arbitrary configuration c :
A ( t )isthe increase in size of the arc set A , which is always non-negative as there are no elementary transitions that remove arcs.  X  ( t )isthe decrease in size of the set of nodes that are on the stack  X  or in the buffer  X  , which is also non-negative as there are no elementary transitions that add new nodes.  X  ( t )isthe decrease in the size of the buffer  X  , which can be negative as well as positive (or zero).
 Proposition 6
Let S = ( C , T , c s , C t ) be a divisible transition system. If every transition t A ( t ) &gt; 0or  X  ( t ) &gt; 0or  X  ( t ) &gt; 0, then S is bounded.
 Proof
To see why the disjunctive condition excludes looping transition sequences, consider an arbitrary configuration c and an arbitrary transition t for which the condition holds. rule out the possibility of a loop. We may therefore conclude that there is no sequence Example 10 because A (U NSHIFT ) = 0,  X  (U NSHIFT ) = 0, and  X  (U NSHIFT is not bounded, because we can have an unbounded number of alternating S U NSHIFT transitions without reaching a terminal configuration.
 be seen by observing that  X  (S HIFT AE ) = 1,  X  (R EDUCE AE
A (L EFT -A RC AE ) = 1. The same reasoning can be applied to show that all the transition systems introduced in Examples (3) X (8) are bounded.

As already stated, the running time of a transition-based parser that only explores a beam-search parser with a constant-size beam) is given by an upper bound on the length of a transition sequence. To prove such bounds for divisible transition systems, we will first prove a linear bound on the number of arcs in planar graphs.
 Lemma 4
A planar dependency graph with n nodes ( n &gt; 1) has no more than 4 n 818 Proof
For n = 2, we can trivially have at most two arcs, (1, 2) and (2, 1), and thus the lemma holds because 2 = 4  X  2  X  6. For the induction step, let n &gt; 2. We will show that if the lemma holds for graphs with less than n nodes, then it also holds for graphs with n nodes.
 set  X  ( i , j ) = { min( i , j ), min( i , j ) + 1, ... , max( i , j ) ( k , l )if( i , j ) = ( k , l )andmin( i , j )  X  min( k , l ) &lt; max( k , l ) other have disjoint domains.
 be the set of arcs in A with length strictly smaller than n any arc in A with length strictly smaller than n  X  1. By definition of A
On the other hand, because G is planar, a pair of arcs in A
Furthermore, by definition of A c ,anarcin A c cannot cover another arc in A the domains of a 1 , ... , a m are disjoint subsets of { 1, ... , n
By definition of A c , every arc in A is either (i) an arc of length at least n or ( n , 1)), or (ii) an arc a i  X  A c , or (iii) an arc covered by some arc a i  X  X  1, ... , m } , the arcs of types (ii) and (iii) form a subgraph of G with ( a are at most 4( ( a i ) + 1)  X  6 arcs of this type for each value of i . Combining this with
Equations (1) and (2), we conclude that the total amount of arcs in A is bounded by
It is easy to see that the expression is maximized for m = 2, and in that case the value of the expression is bounded by 2  X  2  X  2 + 4( n  X  1) = 4 n step and thus concludes the proof of Lemma 4.
Thanks to the result in Lemma 4, we can now proceed to prove bounds on the length of transition sequences in divisible systems that are guaranteed to be robust and bounded, that is, systems that satisfy the conditions of Propositions 5 and 6. We call such systems efficient divisible transition systems.
 Definition 13 A divisible transition system S = ( C , T , c s , C t )is efficient if and only if S for every t  X  T , A ( t ) &gt; 0or  X  ( t ) &gt; 0or  X  ( t ) &gt; 0.
 systems, (ii) systems that in addition have a constant bound on the growth of the buffer, and (iii) systems that have a constant bound on the number of elementary transitions that a composite transition can contain.
 Proposition 7 transition sequence for a sentence x of length n in S is O ( n Proof
Consider an arbitrary transition sequence C 0, m = ( c s ( x ), ... , c length n and the corresponding transition chain T 1, m = ( t hold: size of the buffer.
 Proposition 8 a sentence x of length n in S is O ( n ).
 Proof together with the observation that the total number of transitions t for which A ( t ) = 0, 820
 X  ( t ) = 0, and  X  ( t ) &gt; 0 is now bounded by kn instead of n transitions that may increase the size of the buffer can only do so by at most k . Proposition 9 sition t = t 1 s elementary transition sequence for a sentence x of length n in S is O ( n ). Proof
This follows from Proposition 8 together with the constant bound on the number of elementary transitions in a composite transition.
 Example 11
All the systems defined in Section 3.1 satisfy the condition of Proposition 8 and therefore A of the sentence. Nevertheless,  X  (A TTACH -L EFT ( i ) EF all values of i ), which guarantees the linear bound on composite transitions. 3.3 Planar Dependency Parsing
So far in this section, we have shown how a number of well-known transition systems from the literature can be formulated and studied as divisible transition systems, that is, as restrictions of the same generic system based on five elementary transitions. In this section, we show how this formulation can also be used to define a novel algorithm. dependency graph (regardless of projectivity) if we use all elementary transitions except U NSHIFT directly as the transitions of the system. On top of this system, we can use S
INGLE -H EAD ,A CYCLICITY ,andN O -C OVERED -R OOTS constraints. In addition, we can use Propositions 5 X 9 to show that there is a linear bound on the length of elementary transition sequences in this system. In this way, we obtain an efficient parser for planar dependency graphs, optionally restricted to trees, which is a novel contribution in itself.
More importantly, however, we will show in Section 5 how this system can be gener-alized to a system capable of handling non-planar, hence non-projective, dependency trees using the concept of multiplanarity (to be introduced in Section 4). having the following transitions: 3.3.1 Correctness. This transition system can parse all the planar dependency graphs. To prove its correctness, we must show soundness (all the graphs produced by the system are planar) and completeness (all the planar graphs can be obtained by the system).
Soundness is trivial given Property 4, so we only need to prove completeness. To do so, we prove the stronger claim in Lemma 5.
 Lemma 5
Let G = ( V , A ) be a planar dependency graph for a sentence w transition sequence in S P ending in a terminal configuration of the form (  X  ,[ ], A ) such that all the nodes that are not covered by any dependency arc in A are in  X  . Proof
To prove this lemma, we proceed by induction on the length n of the sentence. In the case where n = 1, the only possible planar dependency graph is the graph G with a single node and no arcs. It is easy to see that the transition sequence that applies asingleS HIFT transition meets the required conditions, because it ends in a terminal configuration ([1], [ ],  X  ).
 and prove that it then also holds for sentences of length n + 1, for any n ( V n + 1 , A n + 1 ) be a planar dependency graph for a sentence w
L n + 1 the set of arcs that is, the set of incoming and outgoing arcs from the node n + 1in G denote by G n the graph that is, the graph obtained by removing the node n + 1 and all its incoming and out-going arcs from G n + 1 . By the induction hypothesis, there exists a transition sequence C whose final configuration is of the form (  X  n ,[ ], A n ), such that  X  that are not covered by any dependency arc in A n . From this transition sequence C will obtain a transition sequence C n + 1 meeting the conditions asserted by the lemma for the graph G n + 1 . To do so, we first observe that the planarity of the graph G that the left endpoints of the arcs in L n + 1 cannot be covered by any arc in A this would mean that the arc in L n + 1 and the covering arc would cross. Therefore, by the induction hypothesis, we know that all the left endpoints of the arcs in L  X  . Thus, if the left endpoints of the arcs in L n + 1 are i is ordered, by Lemma 1) is of the form
With this in mind, we can obtain the transition sequence C following extra transitions at the end of its associated transition chain: 822 where we use the notation arcs ( i ) as shorthand for:
The final configuration of the transition sequence obtained by applying these transitions at the end of C n is of the form (  X  ,  X  , A ), where:
This proves the induction step for Lemma 5, and thus correctness is proved. 3.3.2 Constraints on Planar Dependency Parsing. As we have just proved, the transition system S P is able to parse all planar dependency graphs. In many practical applications, however, it is convenient to exclude some subset of those graphs, for example, those that have cycles or more than one head per node. The results obtained in Section 3.2 can be used to easily add common constraints to the planar parser. The constraints can be added individually or jointly, so that we can obtain a variant of the planar parser combination of them.

Single-Head Constraint. To a d d t h e S INGLE -H EAD system, we restrict the L EFT -A RC P transition to H  X  ( C ), and the R to H  X  ( C ):
The soundness of this variant for the set of planar dependency graphs that meet the S INGLE -H EAD constraint is trivially given by Proposition 2. Completeness is also straightforward, because, as discussed in Proposition 2, applying a L to a configuration of H  X  ( C )oraR IGHT -A RC transition to a configuration of H always generate a graph violating the S INGLE -H EAD constraint. Therefore, any graph that meets the S INGLE -H EAD constraint and can be obtained using the S system (which has been proven complete) can also be generated by this one. Acyclicity Constraint. Analogously to the case for the S INGLE add the A CYCLICITY constraint to the S P transition system by applying Proposition 3.
To do so, we restrict the L EFT -A RC P and R IGHT -A RC P
The soundness of this variant for the set of acyclic planar dependency graphs is trivially implied by Proposition 3. This variant is not complete for acyclic planar dependency graphs, because it actually enforces a stronger variant of A only accept dependency graphs that have no undirected cycles. We can combine this acyclicity check with the S INGLE -H EAD constraint by intersecting the restrictions:
We then obtain a parser that is sound and complete for the set of planar dependency graphs that meet the S INGLE -H EAD and A CYCLICITY constraints. The reason is that, under the S INGLE -H EAD constraint, standard A CYCLICITY parser that enforces only directed A CYCLICITY but allows nodes with multiple heads, this can also be achieved. Instead of checking  X  ( i  X   X  j that the arc does not create a directed cycle (that is,  X   X  ( j  X   X  i  X  A )forR IGHT -A RC ). Although the check for undirected cycles can be im-plemented in constant time if the parser implementation keeps track of the connected component of each node in A , the check for directed cycles is more computationally costly, however. 7 No-Covered-Roots Constraint. Similarly to the other constraints, we can add the N C OVERED -R OOTS constraint to S P by applying Proposition 1. To do so, we restrict the R EDUCE P transition as follows:
The soundness of the resulting parser with respect to planar dependency graphs com-plying with the N O -COVERED -ROOTS constraint is directly given by Proposition 1. To prove completeness, we observe that the transition sequences that we build for each graph in the proof of Lemma 5 only reduce nodes that are then covered by an arc. There-fore, given a graph G that satisfies the N O -C OVERED -R the transition sequence built as in that proof will never reduce a root node. Therefore, all of its R EDUCE transitions will be applied to configurations in H same transition sequence will also parse G in this variant of the transition system, which proves completeness. The N O -C OVERED -R OOTS restriction can be combined with any C
OVERED -R OOTS restriction alone is equivalent to the arc-eager parser by Sagae and 824 Tsujii (2008). If the S INGLE -HEAD ,A CYCLICITY ,andN O are applied at the same time, together with the P LANAR constraint that is implicit in the algorithm itself, we obtain a projective parser different from the projective parsers described in Section 3.1. 3.3.3 Complexity of Planar Dependency Parsing. To study the runtime complexity of the (with or without constraints) satisfies the following: executed in constant time, because their execution only requires us to keep track of a constant number of nodes at the top of the stack and at the beginning of the buffer. The exception is the check for A CYCLICITY if this restriction is required. As explained in Section 3.3.2, however, this can be implemented in constant time if the S constraint is present by keeping track of the undirected connected component of each node in the generated dependency graph. Therefore, as explained in Section 2.3, the complexity of the planar parser with beam search is O ( n ), both for the unrestricted ver-sion and for the variant that enforces the S INGLE -H EAD 3.4 Beyond Planarity
Although the divisible transition system framework introduced in Section 3 can be used to represent and study a wide range of parsers, we have seen by Proposition 4 that it is limited to parsers that generate planar dependency graphs. As already noted, planarity is a very mild relaxation of the better known projectivity constraint, the only difference being that planarity allows graphs with covered roots (see Definition 2), and studies of natural language treebanks have shown the vast majority of non-projective structures to be non-planar as well (Kuhlmann and Nivre 2006; Havelka 2007). to parse planar dependency graphs only provides a modest improvement in practical coverage with respect to projective parsing. To increase this coverage further, we need to be able to handle dependency graphs with crossing arcs.
 proposed in the literature that introduce extra flexibility by allowing actions that fall outside the divisible transition system framework, like the systems by Attardi (2006) built to or from nodes deep in the stack in the case of Attardi (2006), adding transitions able to reorder stack nodes in the case of (Nivre 2009) X  X t seems unlikely that a simple extension of the framework can encompass all of them in a natural way. We can, how-ever, extend the framework individually for each approach by adding the respective new transitions as elementary transitions, but the details and properties of each of these extensions fall outside the scope of this article.
 generalization of the planar transition system described in Section 3.3 that can parse a large set of non-planar graphs. 4. Multiplanar Dependency Graphs
Because it has been shown that exact parsing becomes computationally intractable when arbitrary non-projective dependency graphs are allowed (McDonald and Satta 2007), a substantial amount of research in recent years has been devoted to finding a superset of projective dependency graphs that is rich enough to cover the non-projective phenomena found in natural language and restricted enough to allow for simple and efficient parsing, that is, a suitable set of mildly non-projective dependency structures .
To this end, different sets of dependency trees have been proposed, such as trees with bounded arc degree (Nivre 2006a, 2007), well-nested trees with bounded gap degree bounded gap degree (G  X  omez-Rodr  X   X guez, Weir, and Carroll 2009), or the operationally defined set of trees parsed by the transition system of Attardi (2006).
 richer sets of non-projective dependency graphs is the notion of multiplanarity ,or k -planarity, originally introduced by Yli-Jyr  X  a (2003). Quite simply, a dependency graph is said to be k -planar if it can be decomposed into k planar dependency graphs. Definition 14
G = ( V , A 1 ), ... , G k = ( V , A k ) (called planes ) such that A = A is k -planar if it is possible to assign one of k colors to each of its arcs in such a way that arcs with the same color do not cross. Note that there may be multiple ways of dividing a k -planar graph into planes, as shown in the example of Figure 4. Therefore, 826 1-planarity is equivalent to planarity, and increasing values of k yield increasingly rich sets of dependency graphs.

No algorithm was known to determine whether a given graph was k -planar, and no efficient parsing algorithm existed for k -planar dependency structures. In this article, we overcome these problems. In the remainder of this section, we present a procedure to determine the minimum value of k for which a given structure is k -planar, and we use it to show that the overwhelming majority of sentences in a number of dependency 1-planar dependency parser described in Section 3.3 can be generalized to handle k -planar dependency graphs by introducing additional stacks. In particular, we present a linear-time transition-based parser that is provably correct for 2-planar dependency trees. 9 4.1 Test for Multiplanarity practical parsing, it must provide a good balance between parsing efficiency and cover-age of non-projective phenomena present in natural language treebanks. For example,
Kuhlmann and Nivre (2006) and Havelka (2007) have shown that the vast majority gree (Bodirsky, Kuhlmann, and M  X  ohl 2005), leading to an interest in parsers for these 2009). No similar analysis has been performed for k -planar structures, however. Yli-Jyr  X  a (2003) does provide evidence that all except two structures in the Danish Dependency
Treebank (Kromann 2003) are at most 3-planar, but his analysis is based on con-he is not guaranteed to find the minimal number k for which a given structure is k -planar.
 dependency graph is k -planar and use it to show that the vast majority of sentences in a number of dependency treebanks are at most 2-planar, with a coverage comparable to that of well-nestedness. The idea is to reduce the problem of determining whether a dependency graph G = ( V , A )is k -planar, for a given value of k , to a standard graph coloring problem. To do this, we first consider the following undirected graph:
Note that we can formally say that two arcs ( i , j )and( k , l ) in a dependency graph G
These are the pairs of arcs that were forbidden in the planarity constraint introduced in Definition 2. The graph U ( G ), which we call the crossings graph of G , has one node corresponding to each arc in the dependency graph G , with an undirected edge between two nodes if they correspond to crossing arcs in G . Figure 5 shows the crossings graph of the 2-planar structure in Figure 4.
 one of k colors in such a way that two arcs that cross each other are not assigned the equivalent to saying that G is k -planar if each of the nodes of U ( G ) can be assigned one of k colors such that no two neighbors have the same color. This amounts to solving the well-known k -coloring problem for U ( G ).
 corresponds to a dependency graph being planar only if it does not have crossing arcs.
For k = 2, the problem is equivalent to determining whether the graph is bipartite, and it can be solved in time linear in the size of the graph by simple breadth-first search.
Given any undirected graph U = ( V , E ), we pick an arbitrary node v and give it one of two colors. This forces us to give the other color to all its neighbors, the first color to the neighbors X  neighbors, and so on. This process continues until we have processed obtained a 2-coloring of the connected component of U that contains v . If there are still unprocessed nodes, we repeat the process by arbitrarily selecting one of them, continue with the rest of the connected components, and in this way obtain a 2-coloring of the whole graph if it exists. Because this process can be completed by visiting each node and edge of the graph U once, its complexity is O ( V + E ). The crossings graph of a de-pendency graph with n nodes can trivially be built in time O ( n of dependency arcs to determine if they cross, and cannot contain more than n meaning that we can check if the dependency graph for a sentence of length n is 2-planar in O ( n 2 )time.
 have found this not to be a problem in practice when using it to measure multiplanarity in natural language treebanks, because the effective problem size can be reduced by noting that each connected component of the crossings graph can be treated separately, and that nodes that are not part of a cycle need not be considered. If we have a valid natural language tend to have a small proportion of non-projective arcs, the connected components of their crossings graphs tend to be very small and with few cycles, and k -colorings for them can quickly be found by brute-force search. 828 4.2 Treebank Coverage
To find out the prevalence of k -planar trees in natural language treebanks for various values of k , we applied the technique described in the previous section to all the trees in the training set for eight languages in the CoNLL-X shared task on dependency Danish (Kromann 2003), Dutch (Van der Beek et al. 2002), German (Brants et al. 2002),
Portuguese (Afonso et al. 2002), Swedish (Nilsson, Hall, and Nivre 2005), and Turkish (Atalay, Oflazer, and Say 2003; Oflazer et al. 2003). The results are shown in Table 1. to that of well-nestedness. In most of the treebanks, well over 99% of the sentences are 2-planar, and 3-planarity has almost total coverage. In comparison to well-nestedness, it is worth noting that no efficient parser has been proposed that is able to handle all well-nested dependency trees, only well-nested trees with bounded gap degree, which 2011). As will be seen in the next section, the class of 2-planar dependency trees not only has good coverage of linguistic structures in existing treebanks but is also parsable with a linear-time transition-based parser, making it a theoretically as well as practically interesting subclass of non-projective dependency trees. 5. Multiplanar Dependency Parsing
The divisible transition system framework introduced in Section 3 can be generalized to support k -planar dependency graphs by using k stacks instead of only one and applying the S HIFT and U NSHIFT elementary transitions to all of them at the same time, whereas R EDUCE ,L EFT -A RC ,andR IGHT -A RC only affect one stack at a time. The stack on which these latter transitions are applied is decided by an extra elementary transition, called S WITCH , which cycles through the k stacks selecting one of them as the active stack.
 individual stack will be planar, but pairs of arcs created in different stacks are allowed to cross. In this way, a k -stack parser will be able to build a k -planar dependency forest by using each of the stacks to construct one of its k planes.
 retical construction, we will limit ourselves in this article to the 2-planar case and show how a system built by generalizing the planar parser defined in Section 3.3 to use two stacks instead of one can yield an efficient parser for 2-planar dependency graphs, in particular 2-planar trees. As we saw in Section 4.2, this class of structures gives almost perfect coverage in existing treebanks, and we will therefore leave the exploration of k -planar dependency parsing for k higher than 2 as future work.
 using the multi-stack generalization of the divisible transition system framework, we will introduce the system directly as a generalization of the planar transition system, extended to multiple stacks (as outlined earlier) and then defining the new system on top of the extended framework. 5.1 2-Planar Dependency Parsing
The 2-planar transition system S 2 P has configurations of the form (  X  we call  X  1 the active stack and  X  2 the inactive stack . Because the system uses two stacks rather than one, it does not conform to the standard definition of a stack-based transition system given in Section 2.2, but it behaves analogously. In this case, the initialization function is c s ( w 1 , ... , w n ) = ([ ], [ ], [1, ... , n ], is C t = { c | c = (  X  1 ,  X  2 ,[ ], A ) for any  X  1 ,  X  2 following:
The S HIFT 2 P transition pops the first (leftmost) word in the buffer, and pushes it to both stacks. The L EFT -ARC 2 P transition adds an arc from the first word in the buffer to the stack to the first word in the buffer. The R EDUCE 2 P transition pops the top word from the active stack, implying that we have added all arcs to or from it on the plane tied to that stack. The S WITCH 2 P transition, finally, makes the active stack inactive and vice versa, changing the plane the parser is working with. In order to exemplify how this system can parse non-planar dependency graphs, Figure 6 shows a transition sequence for the tree in Figure 1. 5.1.1 Correctness of 2-Planar Dependency Parsing. To show that this transition system is correct for the set of 2-planar dependency graphs, we need to prove that it is sound (every graph produced by the system is 2-planar) and complete (all 2-planar graphs can be derived from the system). We do this by proving two corresponding lemmas, the second of which is a stronger claim than mere completeness.
 Lemma 6
The system S 2 P is sound for the set of 2-planar dependency graphs. 830 R
IGHT -A RC  X  ( [1, 2], [1, 2], [3, ... ,9], A 1 = { (2, 3) R
IGHT -A RC  X  ( [1], [1, 2, 3], [4, ... ,9], A 2 = A 1  X  X  (1, 4)
L EFT -A RC  X  ( [1, 4, 5], [1, ... ,5], [6, ... ,9], A 3 = A R
IGHT -A RC  X  ( [1, 4], [1, ... , 5], [6, ... ,9], A 4 = A
L EFT -A RC  X  ( [1, 2], [1, 4], [6, ... ,9], A 5 = A 4  X  X  R
IGHT -A RC  X  ( [1, 4], [1, 2, 6], [7, 8, 9], A 6 = A 5  X  X  R
IGHT -A RC  X  ( [1, 4, 7], [1, ... , 7], [8, 9], A 7 = A 6 R
IGHT -A RC  X  ( [1], [1, ... , 8], [9], A 8 = A 7  X  X  (1, 9) Proof
This lemma is proven by showing that the algorithm cannot create a pair of crossing arcs on the same stack. This is done by applying the proof of Proposition 4 separately the transition system resulting from ignoring one of the stacks in the 2-planar system 2-planar.
 Lemma 7
Let G = ( V , A ) be a 2-planar dependency graph for a sentence w and P 2 . Then there is a transition sequence in S 2 P ending in a terminal configuration of the form (  X  1 ,  X  2 ,[ ], A ) such that all the nodes that are not covered by any dependency arc in P 1 are in  X  1 , and all the nodes that are not covered by any dependency arc in P are in  X  2 . Proof
The proof is analogous to that of the planar parser, but we have to handle two stacks and two planes. As in the planar case, we proceed by induction on the length n of the sentence. In the case where n = 1, the only possible 2-planar dependency graph is the graph G 0 = ( { 1 } ,  X  ) with a single node and no arcs, and the transition sequence that applies a single S HIFT transition meets the conditions of the lemma, because it ends in a terminal configuration ([1], [1], [ ],  X  ).
 n and prove that it also holds for sentences of length n + 1, for any n
G n + 1 = ( V n + 1 , A n + 1 ) be a 2-planar dependency graph for a sentence w of arcs that is, the set of incoming and outgoing arcs from the node n + 1in G denote by G n the graph that is, the graph obtained by removing the node n + 1 and all its incoming and out-going arcs from G n + 1 . It is easy to show that the graphs P
P
P configuration is of the form (  X  1 n ,  X  2 n ,[ ], A n ), such that  X  not covered by any dependency arc in P b n ,for b = 1, 2. From this transition sequence C we will obtain a transition sequence C n + 1 meeting the conditions asserted by the lemma for the graph G n + 1 .
 that the left endpoints of the arcs in A b n + 1 cannot be covered by any arc in P by the induction hypothesis, we know that all the left endpoints of the arcs in A are in  X  b n . Thus, if the left endpoints of the arcs in A of the arcs in A 2 n + 1 are j 1 , j 2 , ... , j f ; then the stack  X  same reasoning as in Lemma 1 can be applied to the 2-planar transition system) is of the form and the stack  X  2 n is of the form 832
With this in mind, we can obtain the transition sequence C following extra transitions at the end of its associated transition chain: where we use the notation arcs ( i ) as shorthand for:
The final configuration of the transition sequence obtained by applying these transitions at the end of C n is of the form (  X  1 ,  X  2 ,  X  , A ), where: This proves the induction step for Lemma 7.
 Proposition 10 The system S 2 P is correct for the set of 2-planar dependency graphs.
 Proof
The proposition follows from Lemma 6 and Lemma 7. 5.1.2 Constraints on 2-Planar Dependency Parsing. The 2-planar parser can be restricted to graphs satisfying the S INGLE -H EAD and A CYCLICITY constraints in exactly the same way as the planar parser, and the proofs follow the same line of reasoning. Therefore, dependency forests (that is, 2-planar dependency graphs without cycles and with each node having at most one head) can be defined as follows: S HIFT 2 P =(  X  1 ,  X  2 , i | B , A )  X  (  X  1 | i ,  X  2 | i , B , A ) R EDUCE 2 P =(  X  1 | i ,  X  2 , B , A )  X  (  X  1 ,  X  2 , B , A ) L R S WITCH 2 P =(  X  1 ,  X  2 , B , A )  X  (  X  2 ,  X  1 , B , A )
Because structures in dependency treebanks are typically restricted to forests, this is the version of the 2-planar parser that we use in the experimental evaluation in Section 5.2. 2-planar parser, because in the 2-planar case a node without a head may need to be R
EDUCE transitions in the 2-planar parser to nodes with a head would also forbid some structures without covered roots. In any case, the N O -C not seem practically meaningful when we go beyond planar structures. 5.1.3 Complexity of 2-Planar Dependency Parsing. To reason about the complexity of the 2-planar parser, we first note that a naive implementation of the transition system as infinite sequence of S WITCH transitions, switching the active and inactive stacks repeat-edly and cycling between the same two configurations without making any advance. This can easily be avoided in practice by forbidding S executed if the last transition in the sequence was also a S also have incorporated this restriction into the formal system (for example, by adding a flag to configurations to indicate whether the previous transition was a S not), but this would have unnecessarily complicated the notation. Assuming that our implementation of the 2-planar parser has this restriction on S can show that the length of a transition sequence for a sentence of length n is O ( n )in the same way as for efficient divisible systems (see Section 3.2.2).
 Proposition 11 Let S 2 P be the 2-planar system restricted so that two consecutive S not permitted. Then the length of every transition sequence for a sentence x of length n in S 2 P is O ( n ).
 Proof
The proof follows the same lines as for efficient divisible transition systems. For every transition chain T 1, m = t 1 , ... , t m for x = w 1 , ... , w 834 It follows that m  X  2( n + 2 n + 8 n  X  12) + 1 and hence that m is O ( n ).
Applying the same reasoning as for the planar parser regarding constant-time execu-tion of transitions and fixed-size beam search, we conclude that the complexity of the 2-planar parser is still O ( n ), both for the unrestricted version and for the variant with the S INGLE -H EAD and A CYCLICITY constraints.
 parsers under the assumption that these parsers use deterministic search or fixed-size beam search because this is the most straightforward method to make parsing ponent of accurate transition-based parsers. The relevance of this assumption is further supported by recent results on tabularization and dynamic programming for transition-based parsing, which show that such techniques either lead to a significant increase in parsing complexity or require drastic simplifications in the feature models used. In the former case, practical parsing still has to rely on approximate inference, as in Huang and Sagae (2010). In the latter case, dynamic programming provides an exact inference method only for a very simple approximation of the original transition-based model, as in Kuhlmann, G  X  omez-Rodr  X   X guez, and Satta (2011). In general, this exemplifies the tradeoff between approximate inference with richer models (beam search) and exact inference with simpler models (dynamic programming). Thus, although the feature model used by Zhang and Nivre (2011) to achieve state-of-the-art accuracy for English makes dynamic programming very difficult due to the combinatorial effect on parsing complexity of complex valency and label set features, the feature representation of a single configuration can still be computed in constant time, which is all that is required to achieve linear-time parsing with beam search. The same is true for all the transition systems and feature models explored in this article. Nevertheless, it is an interesting theoretical question whether the novel 2-planar system allows for tabularization and what the resulting complexity would be. At present, we do not know the exact answer to this question, but a reasonable conjecture is that complexity would be exponential for the class of feature models that are relevant for transition-based parsing. 5.2 Experimental Evaluation
In this section, we present an experimental evaluation of the novel 1-planar and 2-planar transition systems in comparison to the widely used arc-eager projective sys-tem of Nivre (2003) (analyzed earlier in Example (4)). Besides being the default parsing algorithm in MaltParser (Nivre, Hall, and Nilsson 2006), this system is also the basis of the ISBN Dependency Parser (Titov and Henderson 2007) and ZPar (Zhang and
Clark 2008; Zhang and Nivre 2011). In addition to a strictly projective arc-eager parser, we also include a version that uses pseudo-projective parsing (Nivre and Nilsson projective transition-based parsing and as such a competitive baseline for the 2-planar parser.
 all four systems in the MaltParser framework and use the same type of classifiers and feature models. For the arc-eager baselines, we copy the set-up from the CoNLL-X shared task on dependency parsing, which includes the use of support vector machines with a polynomial kernel, history-based feature models tuned separately for each language, and pseudo-projective parsing with the Head encoding (Nivre et al. 2006).
For the 1-planar and 2-planar parsers, we use the same type of classifier but modify the transition systems:
We did not perform extensive feature optimization experiments for the new systems, so it is likely that there is room for further improvement. For replicability, the complete experimental settings are available at http://stp.lingfil.uu.se/
Danish, Dutch, German, Portuguese, Swedish, and Turkish. The overall accuracy metric is labeled attachment score (LAS), the percentage of tokens that are assigned both the 836 correct head and the correct label. In addition, we report labeled precision (LP-NP) and and not i  X   X  k . Precision is the percentage of non-projective arcs output by the system that are correct, and recall is the percentage of non-projective arcs in the gold standard that are output by the system. Note that, although precision is undefined for the projec-tive parser because it does not output any non-projective arcs, recall may nevertheless be greater than zero because arcs that are non-projective in the gold standard can be projective in the output of the parser. 13 with our expectations, given the substantially higher coverage of the 2-planar parser level for all languages in this group (McNemar X  X  test). For three of these languages, the 2-planar parser also outperforms the pseudo-projective parser, although the differences are not statistically significant, and only in the case of Dutch is the pseudo-projective parser significantly better. Given the relatively small difference in coverage between the projective and 1-planar parser, one would expect these systems to have very similar performance, and this is also what we find except for Portuguese where the 1-planar parser is significantly better than the projective arc-eager parser.

Swedish, Turkish), there are generally smaller differences between the parsers, and non-projective structures are rare. Interestingly, it seems that the planar parsers have an advantage over the arc-eager parsers for Arabic, where the 2-planar parser is significantly better than both the projective and pseudo-projective parsers. By contrast, the arc-eager parsers seem to have an advantage for Swedish, where the projective and pseudo-projective parsers are both significantly better than the 1-planar parser. At present, we have no explanation for this language-specific variation.
 dependency arcs, we again find that the 2-planar parser does quite well on the four languages with 19% or more non-projective trees, with precision consistently over 50% and recall in the 35 X 60% range. Again, the results are very similar to those achieved with the pseudo-projective parser, with the 2-planar parser giving higher precision for
Dutch and German and higher recall for German. For the remaining four languages, both precision and recall remains low, which probably points to a sparse data problem when learning how to switch between the two planes during parsing, but the same holds true for the pseudo-projective parser. As expected, the 1-planar parser has only marginally higher recall than the projective parser (which, as pointed out earlier, may recover non-projective dependencies by accident), but it is interesting to note that the predicts, in some cases comparable to that of the 2-planar parser.
 for languages with a sufficient proportion of non-projective trees, and that it generally performs at about the same level as the widely used arc-eager pseudo-projective parser.
We believe that it is possible to improve results even further by careful optimization of features and other parameters, but this will have to be left for future research. It would also be interesting to explore the use of global optimization and beam search, which has been shown to improve accuracy over local learning and greedy search (Titov and
Henderson 2007; Zhang and Clark 2008; Zhang and Nivre 2011). 6. Related Work
The literature on dependency parsing has grown enormously in recent years and we will not attempt a comprehensive review here but focus on previous research related to the three main themes of the article: a formal framework for analyzing and construct-ing transition systems for dependency parsing (Section 3), a procedure for classifying mildly non-projective dependency structures in terms of multiplanarity (Section 4), and a novel transition-based parser for (a subclass of) non-projective dependency structures (Section 5). 6.1 Frameworks for Dependency Parsing
Due to the growing popularity of dependency parsing, several proposals have been made that group and study different dependency parsers under common (more or less formal) frameworks. Thus, Buchholz and Marsi (2006) observed that almost all of the systems participating in the CoNLL-X shared task could be classified as belonging to one of two approaches, which they called the  X  X ll pairs X  and the  X  X tepwise X  approaches.
This was taken up by McDonald and Nivre (2007), who called the first approach global exhaustive graph-based parsing and the second approach local greedy transition-based parsing. The terms graph-based and transition-based have become well established, even though there now exist graph-based models that do not perform exhaustive search (McDonald and Pereira 2006; Koo et al. 2010) as well as transition-based models that are neither local nor greedy (Titov and Henderson 2007; Zhang and Clark 2008). parsing by means of transition systems and oracles . Two distinct types of transition sys-tems are described, differing in the data structures they use to store partially processed tokens: stack-based and list-based systems. The formalization of stack-based systems provided there has been one point of departure for the present article (see Section 2.2) but, whereas general stack-based systems allow transitions to be arbitrary partial func-where transitions are obtained by composing a small set of elementary transitions, allowing us to derive specific formal properties.
 work that can be used to describe a wide range of dependency parsers, including both graph-based and transition-based algorithms. Although the high abstraction level of this framework makes it able to describe and relate very different parsing strategies, based parsers such as their computational complexity when implemented with beam search. Kuhlmann, G  X  omez-Rodr  X   X guez, and Satta (2011) introduce a technique to obtain polynomial-time deductive parsers that simulate all the transition sequences allowed by a transition system. 838 6.2 Mildly Non-Projective Dependency Structures
Most natural language treebanks contain non-projective dependency analyses (Havelka 2007), but the general problem of parsing arbitrary non-projective dependency graphs has been shown to be computationally intractable except under strong independence assumptions (McDonald and Satta 2007). This has motivated researchers to look for sets of dependency structures that have more coverage of linguistic phenomena than pro-jective structures, while being more efficiently parsable than unrestricted non-projective graphs.
 graphs, such as arc degree (Nivre 2006a, 2007), gap degree and well-nestedness (Bodirsky, Kuhlmann, and M  X  ohl 2005; Kuhlmann and Nivre 2006; Kuhlmann and
M  X  ohl 2007), and k -ill-nestedness (Maier and Lichte 2009). Among these sets, only well-nested dependency structures with bounded gap degree have been shown to have exact polynomial-time algorithms (Kuhlmann 2010; G  X  omez-Rodr  X   X guez, Carroll, and
Weir 2011). For dependency structures with bounded arc degree, a greedy transition-based parser based on the algorithm of Covington (2001) is described in Nivre (2007). are parsable by a given algorithm. These include the graphs parsable by the transition system of Attardi (2006) or the more restrictive dynamic programming variant of Cohen, tions with the algorithm of Kuhlmann and Satta (2009), or the set of mildly ill-nested Weir 2011).

Yli-Jyr  X  a (2003), who also presents additional constraints on k -planar graphs. No algo-rithms were previously known to determine whether a given graph was k -planar or to efficiently parse k -planar dependency structures, however. 6.3 Non-Projective Transition-Based Parsing
Whereas early transition-based dependency parsers were restricted to projective de-pendency graphs (Yamada and Matsumoto 2003; Nivre 2003), several techniques have been proposed to accomodate non-projectivity within the transition-based framework.
Pseudo-projective parsing, proposed by Nivre and Nilsson (2005), is a general technique applicable to any data-driven parser. Before training the parser, dependency structures are projectivized using lifting operations (Kahane, Nasr, and Rambow 1998), and partial information about the lifting paths is encoded in augmented arc labels. After parsing, dependency structures are deprojectivized using a heuristic search procedure guided by the augmented arc labels.
 transitions to projective transition systems. Attardi (2006) parses a restricted set of non-projective trees by adding transitions that create arcs using nodes deeper than the top words, obtaining full coverage of non-projective structures in quadratic worst-case time (but achieving linear practical performance). A similar technique is used by Tratz and Hovy (2011) to develop an O ( n 2 log n ) non-projective version of the easy-first parser of Goldberg and Elhadad (2010).
 mented as a list-based transition system that in its unrestricted form is complete for
O ( n 2 ), but efficiency can be improved in practice by bounding the arc degree (Nivre 2006a, 2007). 7. Conclusion
Although data-driven dependency parsing has seen tremendous progress during the last decade in terms of empirically observed accuracy for a wide range of languages, it is probably fair to say that our theoretical understanding of the methods used is still less developed than for the more familiar paradigm of context-free grammar parsing.
In this article, we have tried to contribute to the theoretical foundations of dependency parsing in essentially two ways.
 sition systems for dependency parsing can be defined by composition and restriction of the five elementary transitions S HIFT ,U NSHIFT ,R A
RC . On the one hand, this can be used as an analytical tool to characterize existing systems for dependency parsing and prove formal properties related to expressivity and complexity. Thus, we have shown that all divisible systems, including a number of well-known systems from the literature, are sound for planar dependency graphs characterized the subclass of efficient divisible transition systems that give linear pars-ing complexity when combined with greedy inference or beam search as is customary in transition-based parsing. Even though most of these results have been established previously for particular systems, the general framework allows us to show how the results follow from more general principles. On the other hand, the framework can we have presented a system that is both sound and complete for planar dependency dependency parsing literature.
 dency parsing to multiplanar dependency graphs, an interesting hierarchy of mildly non-projective dependency structures that have remained unexplored due to the lack k -coloring problem for undirected graphs and can thereby be solved efficiently for k but in practice also for higher k due to the sparseness of non-projective dependencies in natural language. Using this procedure, we have shown that the set of 2-planar depen-dency trees have a coverage in existing treebanks that is at least as good as alternative characterizations of mildly non-projective dependency structures. In addition, we have shown how the planar dependency parser defined in the first part of the article can be generalized to the k -planar dependency graphs and in particular to the 2-planar case.
Preliminary experiments using standard methods for transition-based parsing show that this system can give significant improvements over a strictly projective system for languages with a non-negligible proportion of non-projective dependencies. of all, there are many instances of divisible transition systems that have not yet been remarked in Section 3.3.2, there is a way of restricting the 1-planar parser to projective forests, which is different from previously explored systems for projective dependency parsing. Secondly, it may be interesting to study different ways of extending divisible 840 transition systems for greater expressivity, besides introducing additional stacks. This may involve the addition of new transition types, as proposed by Attardi (2006) and
Finally, it would be interesting to see what level of accuracy can be reached for 2-planar dependency parsing with proper feature selection in combination with the latest techniques for global optimization and non-greedy search (Titov and Henderson 2007; Zhang and Clark 2008; Huang and Sagae 2010; Zhang and Nivre 2011).
 Acknowledgments References 842 844
