 Community Question Answer (cQA) archi ves are successful instances of Social Media, in which users pose and reply questions in natural languages as well as evaluate and select the best answers, e.g., Naver (www.naver.com), Yahoo! An-swer (answers.yahoo.com), Baidu Zhidao (zhidao.baidu. com) and AnswerBag (answerbag.com). The user -generated question-ans wer pairs in cQA websites have inherent advantages: humongous amounts, rapid growth rate, wide range of subject matter, and multiple languages support [2]. Nevertheless, the quality of the content is with high variance, ranging from excellent detailed answers to completely irrelevant or commercial advertisements, and even abusive language. This reduces the efficiency and user satisfaction of question-answer-pair reuse, if answers from similar questions are used to tailor users X  information needs.
The approach to resolve the uneven quality in answer content is to composite multiple answers from a question and gen erate a complete, succinct summary. The task approximates to the query-biased multi-document summarization task, in which questions are mapped into queries and answers into documents.
We present a framework for answer summarization, which employs the graph-based random walk model to weight each concept in answers and extract a set of sentences which maximize the value of the objective function by exploiting a global inference algorithm based on dynamic programming. The summaries with the maximal values are chosen as the final answers.

We focus on several essential questions to fulfill our objective: (1) What granularity of concepts distilled from answers is suitable in the graph-based random walk model for reinforcing diversity and relevance of summaries?
Generally, sentences are chosen as nodes to construct the graphs in most of literatures about document or answer summarization [8,1]. Under limitation of the amount of words in summaries, these reduplicative information between sentences might subduct diversity of summaries. In this paper, we use concepts with a finer granularity than that of se ntences to represent nodes in the graph-based random walk model. Concept-level r epresentation helps to recognize and decrease redundancy in sentences when abstracting sentences from sets of an-swers. In addition, measuring the simila rity between concepts is more accurate and easier than between sentences as fewer words are included in concepts.
We employ a topic-sensitive model to i mprove the relevance of generated summaries. The model assigns each con cept a saliency score according to its relevance to questions as well as its similarity with other concepts in answers. Hence concepts related with questions are weighed relatively high saliency scores. (2) What roles does the authority of users play in enhancing trustworthiness of answer summaries?
In most cases, the answers responded by more authoritative users are more trustworthy. Hence, the concepts from authoritative users should have greater saliency scores than those from other users. For such purpose, a two-layer link graph is constructed to redistribute concept saliency scores through incorporat-ing user social features and text content from answers. (3) How to improve the coverage and conciseness of answer summaries through global inference algorithms? With concepts weighted by the saliency scores from graph-based random walk model, summary generation is cast to the problem of finding the maximum cov-erage with knapsack constraint on all answers. This problem can be resolved by exploiting a global inference algorithm. The objective function of the algorithm is the key of boosting the coverage and conciseness of answer summaries. A good objective function should reduce redundancy and retain relevant information in summaries as much as possible.

The remainder of this paper is organized as follows: Section 2 describes how to employ graph-based random walk model to assign the saliency scores of concepts according to the information of text con tents and user social features in detail. Section 3 illustrates how to apply the global inference algorithm to generate the complete and succinct summaries. Section 4 exhibits the conduct of several experiments and discusses the results. Section 5 contains an overview about the document and answer summarization. Finally, conclusions and orientations for future work are presented. 2.1 Graph-Based Random Walk Model on Text Content A graph-based random walk model is employed to rank the answer content through assigning a saliency score to each nodes in the graph. In this paper, we suppose each answer consist of many concepts and represent each node in graphs by a concept from answers.

Given a question q and a set of concepts from its answers C = { c 1 ,c 2 , ..., c n } , we first define a graph G = &lt;V, E&gt; as an undirected graph, where V is a set of vertices representing n concepts, and E is a set of edges representing the similarity between vertices, a subset of V  X  V . The graph G is regarded as n  X  n weighted matrix A ,where a ij is a similarity score from the node i and j . Then, we normalize the matrix A into a similarity matrix S such that each element s ij
Accordingtoagivenquestion q , a relevant vector B is constructed, where b i is defined as the maximum relevant score between the concept c i from answers and the concepts from the question. We derive an normalized vector D from B such a square relevant matrix R , where each element in the i th column is assigned to d .
 We define a transition matrix M as follows: where d is a damping factor with a real value of [0, 1]. Since all rows in the matrix M have non-zero values and sum to 1, the transition matrix M is a stochastic matrix. Each element m ij in M means the transition probability from i to j in Markov chain. Thus, M has a unique stationary distribution p = M T p .The stationary distribution p can be used to rank the concepts in answers. We write the model (denoted as Rank-1) into the equation (2) in matrix notation. The simplified equation of the model Rank-1 is rewritten as follows: p ( c i | q )= d  X  where C is the set of all concepts in answers, and d is a damping factor with a real value of [0, 1] indicating the  X  X uestion bias X . The topic-sensitive saliency score question q and the similarity with other concepts in the answer set. The damping factor d is used to control which parts is mor e important in answer summaries: relevance to the question or the similarity with other concepts. sim ( c i ,c j )isthe function to measure the similarity in c i and c j . We adopt a similarity measure calculating the relevance between c i and q . The relevance of a concept c i is defined as the maximum similarity between c i and the concepts from questions.
For improving the diversity and generating compendious summaries, we em-ploy concepts with finer granularity th an sentences to represent the nodes in graphs. The concept representations are listed as following: Phrase: A phrase is a non-overlapping span in a sentence and can be obtained through partitioning a sentence based on the syntactic structure. We employ a natural language processing tool SST [4] to obtain phrases in sentences. N-gram: N-gram is a subsequence containing the continuous n words in a sen-tence. In this paper, unigram, bigram and trigram are chosen as the graph nodes, respectively.

Phrase and n-gram are two kinds of common approaches of partitioning sen-tences and widely used in natural language processing. Both phrase and n-gram contain fewer words than sentences, whi ch contributes to accurately reflect the relationship of nodes and identify redundancy information. Noted that stop words, such as articles, pronouns, prepositions, and conjunctions, are removed before calculating the similarity. 2.2 Two-Layer Link Graph for User Social Features In cQA websites, the authoritative us ers tend to provide answers with high quality. Hence, user social features can facilitate the generation of a reliable summary. A two-layer link graph is utilized to incorporate the text content and user social features in a unified framework. The two-layer graph-based random walk model is shown in Figure 1. The first layer denotes the concept relationship in the topic-sensitive random walk model . The second layer represents all users which reply questions. The connection between two layers indicates the influence for ranking concepts under condition of user authority.

Given a question q , a set of concepts from its answers C = { c 1 ,c 2 , ..., c n } ,and E  X  c represents the set of the edges between concepts, and E u  X  c denotes the set of edges between concepts and users. If the user u i is the author of the answer including the concept c j , there is an edge e ij in E u  X  c , otherwise no connection.
Taking user authority into account, the similarity matrix S is rewritten into a is a new similarity measure under the condition of u i and u j and is calculated according to the following formula: where u i and u j are the authors of the original answers including c i and c j , respectively. auth ( u i ) denotes the user authority in community. auth ( u i )iscal-culated through a HITS-based method from [6], which regarded users who pose questions as  X  X ubs X  and users who provide answers as  X  X uthorities X . An algo-rithm based on HITS is employed to compute the hub and authority value of users on the link graph according to three relationship: user-question, question-answer and answer-user. The hub and authority values of users are calculated by the following equation: where the set U A contains all the users who provide answers, and U Q includes all the users who pose questions. H ( i ) is the hub value of the user i and A ( i )is its authority value.

The final saliency score for the two-layer graph-based random walk model is denoted by: p ( c i | q )= d  X  The matrix form of the model is: p =[ d  X  R +(1  X  d )  X  S  X  ] T  X  p . The model is denoted as Rank-2.
 The model Rank-2 can provide more reliable answers than the model Rank-1. This is because the model Rank-2 tends to extract sentences from authoritative user into answer summaries compared wi th the model Rank-1 through assigning the concepts from answers of authoritative user a higher saliency score than the concepts from other users. After a sentence is represented as a set o f concepts with saliency scores deduced based on a graph-based random walk model, the aim of answer summarization is transformed to maximize the covered concepts and minimize the number of sen-tences included in the summary. Namely, we require to find a subset of sentences which satisfy two conditions: the length (the number of words or bytes) of sum-maries must be at most L (cardinality constraint), and summaries should cover as many concepts as possible. The first condition guarantees the conciseness of generated summaries, and the second one ensures the diversity and coverage. Hence, the answer summarization is mapped into a maximum coverage problem with knapsack constraint (MCKP). MCKP is an NP-hard problem [7]. For allevi-ating this problem, a global inference algorithm based on dynamic programming is employed.

We set the following objective function (denoted as Obj-1) to maximize the sum of the weights of concepts included in the generated summary: In the above program, s j is a sentence from answers and its length is l j . c i is a concept from sentences and its weight obtained from a graph-based random walk model is w i . S denotes the generated summary and is a subset of sentences included in multiple answers. The variable x i and y j indicate whether the concept c and the sentence s j occur in the summary S , respectively. If appearing, the corresponding variable x i or y j is equal to 1, otherwise 0. The variable occ ij represents whether the concept c i exists in the sentence s j . If existing, the value of occ ij is 1, otherwise 0.

The objective function accumulates the weight of concepts only once regard-less of total times of occurrence in S , which benefits to redundancy control of summaries. There are two constrains in the program: one is the length constrain, which limits the number of words in summa ries, and the other is the consistency constrain, which demonstrates that S must contain at least one sentence s j such that occ ij is equal to 1 if the concept c i occurs in the summary S .
For further eliminating redundancy, we introduce the concept groups, in which concepts are partitioned into the same group when the similarity between two concepts is above a threshold  X  . All of concepts in the set of answers are divided into concept groups under the threshold  X  . The new objective function (denoted as Obj-2) is described as follows: In the above program, s j is a sentence from answers and its length is l j . g i is a concept group and its weight w g i is defined as the maximum saliency score in the concept group. S denotes the generated summary and is a subset of sentences included in multiple answers. The variable x g i denotes whether the concept group y indicates whether s j occurs in the summary S . If appearing, the corresponding variable y j isequalto1,otherwise0.Thevariable occ g ij represents whether a mutual concept exists in both the sentence s j and g i . If existing, the value of occ g ij is 1, otherwise 0.

The objective function Obj-2 takes the similar concepts above the threshold  X  only once into account when summaries are generated. Hence, it is inclined to contain more information into answer summaries than the objective function Obj-1, thus improving in the coverage and diversity of summaries. 4.1 Experimental Setup Data Sets. We conducted experiments on the data sets from Yahoo! Answers portal, which were compiled by Tomasoni and Huang [14]. The original dataset contained 216, 563 questions and 1, 982, 006 answers created by 171, 676 users in 100 categories. A filtered version were pic ked out which reserved 89, 814 question-answer pairs with adequate statistical and linguistic characteristic. Trivial, factoid and encyclopedia-answerable que stions were eliminated. They also con-sidered some factors related to the task of summarization, such as the number of answers, length of the longest answer and length of the sum of all answers, and produced a smaller dataset for summarization. The new dataset was composed of 100 questions and 358 answers manually sel ected according to subjective and hu-man interest from 89, 814 question-answer pairs. Three human annotators were asked to generate extractive summaries of 100 questions with the limitation of 250 words.

In the following experiments, the 100 questions with 358 answers were used for evaluation of summarization. The 300 manual summaries were regarded as the gold standard for 100 questions. The data set including 89, 814 question-answer pairs was exploited to measure th e user authority which is referred to in section 2.2.
 Compared Algorithm. We compared our method wit h the topic-sensitive LexRank [13] and NegativeRank [1], which were the sentence-level methods based on random walk model. The semant ic similarity between sentences was measured by the approach proposed in [12]. We also carried out the concept-level global inference algorithm (denoted by Bigram-GIM) proposed in [5] based on a dynamic programming, which weighted concepts only by the number of bigrams appearing in documents. Bigrams were ch osen as concepts because they obtain better performance than unigrams or trigrams under a variety of ROUGE mea-sure. We treated the best answer chosen by the users as the evaluation baseline. Evaluation Metrics. We employed the standard ROUGE (version 1.5.5) [9] for evaluation. ROUGE has been widely used in automatic summarization eval-uation through counting the number of overlapping units, such as n-gram, word sequences and word pairs between the su mmaries generated by machines and the gold standard by humans. We calculated three metrics against human annotation on data set: ROUGE-1, ROUGE-2 and ROUGE-L. ROUGE-1 depends on the co-occurring unigrams between a candiat e summary and a set of reference sum-maries, ROUGE-2 depends on the co-occurring bigrams, and ROUGE-L relies on the Longest Common Subsequence (LCS). 4.2 Results Experiment 1: Performanc es under different concepts. We analyzed the impact of different concepts through t he task of answer summarization. We chose four kinds of concept representations: unigrams, bigrams, trigrams, and phrases. The graph-based random walk model only on text content (Rank-1) was employed to assign concept saliency scores. The best answers in Yahoo! Answer were regarded as the baseline. We compared with two sentence-level methods: topic-sensitive LexRank [13] and NegativeRank [1]. We also imple-mented a concept-level approach Bigram-GIM [5]. The experiment results of methods under different concepts were listed in Table 1.

All of four methods using concept-leve l representations excel the best an-swer in performance. Especially, unigr ams obtains the best performance in four concept representations .  X  X nigram+Rank-1+Obj-1 X  climbs up the performance 10 . 3% in ROUGE-1, 5 . 7% in ROUGE-2 and 9 . 5% in ROUGE-L than best answers, respectively.

Four concept-level methods also are s uperior to LexRank and NegativeRank.  X  X nigram+ Rank-1+Obj-1 X  improves the performance 3 . 2% in ROUGE-1, 6 . 9% in ROUGE-2 and 3 . 5% in ROUGE-L than LexRank, and 3 . 3% in ROUGE-1, 7 . 4% in ROUGE-2 and 3 . 8% in ROUGE-L than Negativ eRank, respectively. The increase derives from the different node representations in link graph. Concept-level representations are beneficial to more accurately capture the relations between nodes than sentence-level ones.  X  X nigram+Rank-1+Obj-1 X  also slightly outperforms  X  X igram-GIM X , improv-ing 0 . 6% in ROUGE-1, 0 . 6% in ROUGE-2 and 1 . 7% in ROUGE-L respectively. This shows that the saliency scores of concepts assigned according to the graph-based random walk model performs better than the weights from the number of concept appearance in documents.
 Experiment 2: Performances unde r the graph-based random walk model Rank-1 and Rank-2. We evaluated the effect of the user information in generating summaries. Table 2 shows the e xperiment results employing Rank-1 and Rank-2, respectively. In generally, the methods using Rank-2 obtained bet-ter performance than the ones using Rank-1 under all of concept representations.  X  X rigram + Rank 2 X  had the greatest increase in all concept representations.
The model Rank-1 is a graph-based random walk model only on document content, which prefers to pick out the se ntences containing important concepts. The model Rank-2 is a two-layer random walk model on document content and user links, which tends to extract the sentences from authoritative users with vital information. Rank-2 is based on the assumption that authoritative users tend to provide reliable answers. The experiment results in Table 2 verify this assumption is correct.
 Experiment 3: Performances under t he objective function Obj-1 and Obj-2. We also compared the objective function Obj-1 and Obj-2 to evalu-ate the influence of grouping concepts in improving diversity and coverage of answer summarization. The model Rank-2 was used to rank concepts under dif-ferent concept representations and object ive functions. The results were listed in Table 3.

Grouping concepts under a given threshold is effective for the methods em-ploying bigrams and trigrams as graph nodes.  X  X rigram+Rank-2+Obj-2 X  ob-tained the optimal result when varying concepts, ranking models and objective functions.
 The appearance of Community Question Answer portals has appealed to numer-ous researchers, and their work involves searching for similar questions already answered [15], ranking content quality usi ng social features such as the authority of users [6,2], predicting asker satisfaction [10] and other application.
A graph-based random walk model is widely applied on the document and answer summarization, most of which choo ses sentences as the nodes in graphs. Otterbacher et al. [13] introduced a topic-sensitive random walk model LexRank. Chall and Joty [3] improved LexRank through substituting the sentence simi-larity based on syntactic and semantic t ree kernels for the sentence similarity based on tf-idf. Achananuparp et al. [1] proposed the algorithm NegativeRank for increasing answer diversity through assigning negative weights to edges in the graph-based random walk model.

McDonald [11] applied the global inference model in multi-document sum-marization, which consider relevance and redundancy at the sentence level. The objective function is designed into the form of the sum of relevance scores of sentences minus the sum of the redundancy scores of each sentence pair in a summary. Gillick and Favre [5] enhanced McDonald X  X  algorithm and proposed a concept-level global inference mode l, which assumed the concepts are inde-pendent. Tomasoni and Huang [14] proposed an answer summarization on cQA, which utilized four metadata-aware me asures on the concept level to evaluate answers: Quality, Coverage, Relevance and Novelty. The final summaries about questions were generated through a global inference algorithms similar with a maximum coverage model.
 Our method has two main differences co mpared with those above algorithms. First, we adopt the graph-based link analysis to distribute the node weights under a finer level than sentences. This contributes to the analysis of dependency relationship between nodes and redundancy reduction in generated summaries. Second, we construct a two-layer link graph for strengthening the impact of the user authority in answer summarization. We propose a framework to automatically generate answer summarization for questions posed by user in cQA services . In the framework, a series of methods are employed to improve diversity, coverage, relevance, trustworthiness, and con-ciseness of answer summaries: (1) We choo se different concept representations with finer granularity than sentences on a graph-based random walk to enhance the diversity. (2) We incorporate user authority and document content from answers into a two-layer link graph to st rengthen the trustworthiness of sum-maries. (3) We cast the summary generation into a maximum coverage problem with knapsack constraint, boosting the coverage and conciseness through the optimization process. The experimental results demonstrate our approach out-performs current methods.

In future work, we plan to distill syntactic or shallow semantic structures to represent concepts. We also plan to implement the evaluations on the larger-scale data set.
 Acknowledgments. This research is supported by State Key Laboratory of Software Development Environment (under grant no. SKLSDE-2011ZX-03) and the National Natural Science Founda tion of China (unde r grant no. 61170189).
