
This paper introduces FAST, a novel two-phase sampling-based algorithm for discovering association rules in large databases. In Phase I a large initial sample of transactions is collected and used to quickly and accurately estimate the support of each individual item in the database. In Phase II these estimated supports are used to either trim "outlier" transactions or select "representative" transactions from the initial sample, thereby forming a small final sample that more accurately reflects the statistical characteristics (i.e., iternset supports) of the entire database. The expensive op-eration of discovering association rules is then performed on the final sample. In an empirical study, FAST was able to achieve 90-95% accuracy using a final sample having a size of only 15-33% of that of a comparable random sample. This efficiency gain resulted in a speedup by roughly a factor of 10 over previous algorithms that require expensive processing of the entire database --even efficient algorithms that ex-ploit sampling. Our new sampling technique can be used in conjunction with almost any standard association-rule algo-rithm, and can potentially render scalable other algorithms that mine "count" data. The volume of electronically accessible data in warehouses and on the Internet is growing faster than the speedup in processing times predicted by Moore's Law [21]. Scalability of mining algorithms is therefore a major concern. Classical mining algorithms that require one or more passes over the entire database can take hours or even days to execute, and in the future this problem will only become worse. One approach to the scalability problem is to exploit the fact that approximate answers often suffice, and execute mining algorithms over a "synopsis" or "sketch," that is, over a lossy compressed representation of the data. This approach can provide approximate answers while reducing the processing time by orders of magnitude --in the context personal or classroom use is granted without fee provided that copies are permission and/or a fee. SIGKDD '02 Edmonton, Alberta, Canada 
Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. of a rapidly-changing competitive environment, such quick approximate results can be much more useful than "exact" results that are irrelevant by the time they are computed. 
Use of a synopsis per se, however, is not guaranteed to solve the problem. The computation of many synopses proposed in the literature requires one or more expensive passes over to adequately address the scalability problem unless the cost of producing the synopsis is amortized over many queries. 
Using a sample of the data as the synopsis is a popular technique that can scale very well as the data grows. Be-sides having desirable scaling properties, sampling is also well suited to interactive exploration of massive data sets [11]. Recent work in the area of approximate aggregation processing [1, 8, 10] shows that the benefits of sampling are most fully realized when the sampling technique is tailored to the specific problem at hand. In this spirit we initiate an investigation of sampling methods that are designed to work with mining algorithms for "count" datasets, that is, datasets in which there is a base set of "items" and each data element is a vector of item counts --here "items" may correspond to physical items, responses on a survey, income levels, and so forth. As a first step, we study sampling-based algorithras for the most well-studied mining problem defined on count data: the discovery of association rules in large transaction databases. 
Agrawal, et al. [3] proposed association rules as a means of describing interesting purchasing patterns. Association rules identify relationships among sets of items and can be used to evaluate business trends, identify purchasing pat-terns, and classify customer groups. Two measures, called support and confidence, are introduced in [3] in order to quantify the significance of an association rule. The mining of association rules from a set of transactions is the process of identifying all rules having support and confidence greater than specified minimum levels; such rules are said to have "minimum confidence and support." We focus on the prob-lem of finding the "frequent" itemsets that have minimum support, because this operation is by far the most expensive phase of the mining process. We assume that the reader is familiar with t:he basic Apriori algorithm introduced in [3]. 
A variety of modifications have been proposed to reduce the computational burden---see, for example, [2, 5, 7, 14] and references therein--but with few exceptions all current al-gorithms require at least one expensive pass over the data. 
In the context of "standard" association-rule mining, use of samples can make mining studies feasible that were for-tional database do not change during the mining process. We also assume that the database is very large. 
Denote by D the database of interest, by S a simple ran-dom sample drawn without replacement from D, and I the set of all items that appear in D. Also denote by Z(D) the collection of itemsets that appear in D; a set of items A is an element of I(D) if and only if the items in A appear jointly in at least one transaction t E D. Accordingly, the collec-tion Z(S) represents itemsets in S; of course, Z(S) _C Z(D). For k &gt;_ 1 we denote by Zk(D) and I~(S) the collection of k-itemsets in D and S, respectively. Similarly, L(D) L(S) denote the frequent itemsets in D and S, and and Lu(S) the collection of frequent k-itemsets in D and S, respectively. For an itemset A C I and a set of transac-tions T, let n(A; T) be the number of transactions in T that contain A and let IT I be the total number of transactions in T. Then the support of A in D and in S is given by tively. 
Given a specified minimum support p and confidence c, the FAST-trim algorithm proceeds as follows: 1. Obtain a simple random sample S from D. 2. Compute f(A; S) for each 1-itemset A E Z1 (S). 3. Using the supports computed in Step 2, obtain a re-4. Run a standard association-rule algorithm against So 
Olken [19] provides a review of techniques that can be used in Step 1 to obtain a random sample of transaction records. In general, the cost of obtaining a sample depends upon how the data is stored. In our implementation of FAST-trim, the transaction data is stored in a fiat file and we use a sampling algorithm with a cost of O(ISI) as in Ernvall and Nevalainen [9]. The computation in Step 2 of f(A; S) for each 1-itemset A E I1 (S) is straightforward: a count is maintained for each item present in S. If each of these items and the associated counts are stored together in a hash tree, then the cost of Step 2 is at most O(Tm~  X  ISI), where Tm~x stands for the maximal transaction length. Because the cost of Step 2 is relatively low, the sample S can be relatively large, thereby helping to ensure that the estimated supports are accurate. 
The crux of the algorithm is Step 3, in which outlier trans-actions are trimmed from the sample. The following subsec-tions discuss the choice of distance functions, the trimming procedure and the stopping criteria. 
As discussed previously, we define an outlier to be a trans-action whose removal from the sample maximally reduces (or minimally increases) the discrepancy between the sup--ports of the 1-itemsets in the sample and the corresponding supports in the database D. (Since the supports of the 1-itemsets in D are unknown, we estimate them by the corre-sponding supports in S as computed in Step 2 of FAST-trim.) To make the notion of "discrepancy" between 1-itemset sup.-ports precise we define a distance function, based on the symmetric set difference, by setting for each subset So C S --in accordance with our previous notation, Li(SO) and Lz(S) denote the sets of frequent 1-itemsets in So and S. Observe that 0 &lt;_ Dist, &lt;_ 1, and that Distl is sensitive to both false frequent 1-itemsets and missed frequent 1-itemsets. Our goal is to trim away trans-actions from S so that the distance from the final sample So to the initial sample S is as small as possible. We note that other definitions of distance are possible, for example and 
Observe that Dist2, Dist3, and Dist4 correspond to LP-norm respectively. Because of the well-known equivalence between these norms, we expect algorithms based on Dist2, Dista, and Dist4 to behave similarly, and we focus on differences between Distl and Dist2. 
Suppose at first that the goal is to produce a final sam-ple So containing exactly n (_&gt; 1) transactions --note that the value of n is directly related to the time subsequently required to generate the frequent itemsets. Given an initial sample S, we therefore wish to find a solution So to the following problem: 
This combinatorial optimization problem is extremely ex-pensive to solve exactly. Indeed, it can be shown that the problem is NP-complete, by reduction from the One-In-
Three SAT problem [12]. There are, however, a variety of heuristic algorithms that yield approximate solutions to the problem in (5). A trivial algorithm is to trim "obliviously:" initially set 
So = S and then scan the transactions in So in an arbitrary order, removing each transaction in turn until IS01 = n. Al-though this procedure is inexpensive --e.g., no evaluations of Dist 0 are required --it is clear that the final sample So will not be any more representative of the database D than the initial sample S. An alternative greedy algorithm also starts by setting 
So = S, and then proceeds in stages. At each stage the algorithm finds a transaction t* E So such that Dist(S0 - X  {t*},S) _&lt; Dist(So -{t},S) for t E So and sets So = 
So -{$*}. Although this algorithm produces much better solutions than X  the "oblivious" algorithm --each transac-tion that is removed is known to be at least as much of an outlier as any other transaction currently in So --it is prohibitively expensive: when the current sample contains j (&gt; n) transactions, precisely j evaluations of Dist 0 are re-quired to remove the next transaction. The total number of 
Dist 0 evaluations required to produce the final sample So is divide So into disjoint groups of rain(k, IS01) transactions each; for each group G { compute f(A; So) for each A E Zi(S0); set So = So -{t*}, where Dist(So -{t*}, S) = divide So into disjoint groups of rain(k, IS0[) transactions each; for each group G { if (3t* such that Dist(S0 -{t*}, S) = mintEG Dist(S0 -{t}, S) and 
Dist(S0 --{t* }, ,5') &lt; Dist(S0, S)) { 
Figure i: Complete FAST-trim Algorithm 
The FAST-trim algorithm obtains the final sample So by As an alternative to the trimming procedure described in 
First consider a version of FAST-grow with a specified fi-
As with FAST-trim alternative stopping criteria are avail-
In other words, a rollback operation is applied to obtain run a standard association-rule algorithm against ure 2. Of course, we do not separately store the sets S0(0), 
S0(1), ..., So(K) --we merely keep track of the stage at which each transaction is added to So. This information is all that is needed for rollback. randomized algorithms, such as random swapping, simu-lated annealing, tabu search, and genetic algorithms. These algorithms did not perform well relative to our heuristics, and so we leave a more comprehensive investigation of ran-domized algorithms as a topic for future research. to evaluate the performance of FAST. All experiments were performed on an HP 9000 series UNIX multi-user worksta-tion with a processor speed of 132 MHz. synthetic database was generated using code from the IBM 
QUEST project [4]. The parameter settings for synthetic data generation are similar to those in [4]: the total numbers of items and transactions are set to 1000 and 100,000, the number of maximal potentially frequent itemsets is 2000, the average length of transactions is 10, and the average length of maximal potentially large itemsets is 4. We used a minimum support of 0.77%, at which level there are neither too many nor too few large itemsets, and the length of the maximal large itemsets is 6. The real~world database is a sales database from a large retailing company, and is similar to the one used in [3]. We obtained similar .results on both databases and therefore focus on the synthetic database. 
In a certain sense, the synthetic databases pose more of a challenge to the FAST algorithms than the real-world data. 
This is because the synthetic data contains relatively many frequent k-itemsets with k &gt; 1, whereas the trimming and growing heuristics are based on 1-itemset frequencies--most of the frequent itemsets in the real-world database are 1-itemsets. 
Toivonen's Algorithm, and "sl~s-Apriori", which is simple random sampling combined with the Apriori algorithm as in [22]. Distance functions Distl and Dist2 are used in FAST-trim and FAST-grow. Preliminary experiments showed that a value of k = 10 for the group--size parameter worked well in both FAST-trim and FAST-grow, and we therefore use this value throughout. We use a sampling ratio of 30% through-out to create the initial sample for FAST and its variants. We also use the Apriori algorithm to obtain frequent itemsets in the final step of the FAST algorithms --this choice per-mits fair comparisons with Toivonen's Algorithm and sRs-Apriori. we executed the algorithm 50 times, each time choosing a different simple random sample (without replacement) from the database. Thus each performance number reported be-low is an average of 50 observations. Our primary measure of accuracy is as follows: where, as before, L(D) and L(S) denote the frequent item-sets from the ,database D and the sample S. As with the distance measure, the accuracy measure in (6) is based on the symmetric set difference, and hence is sensitive to both false and missing frequent itemsets. ent algorithms in terms of accuracy and execution time. For 
FAST-trim, we consider both the "fixed-size" stopping crite-rion (no auxiliary trimming phase) and "min-distance" stop-ping criterion (which uses the auxiliary trimming phase). 
We similarly consider fixed-size and min-distance stopping criteria for FAST-grow. The results are presented using an abbreviated notation in which, for example, FAST-t-D1 de-notes the FAST-trim algorithm based on Distl and FAST-g-D2 denotes the FAST-grow algorithm based on Dist~. The final sampling ratios chosen are 5%, 7.5%, 10%, 12.5% and 15%. 
For sRs-Apriori, two additional sampling ratios of 20% and 30% are also selected. algorithms on the synthetic databases. As shown in the fig-ures, FAST-trim and FAST-grow outperform SRS-Apriori in most cases. Indeed, all fixed-size versions of FAST are more accurate than sRs-Apriori, especially those that use Dist2--with a 5% sample, these latter algorithms achieve results comparable to a 15% simple random sample. Most FAST-trim and FAST-grow algorithms missed between 3.5-6% of the frequent itemsets at a final sample size of 5%, compared  X  with 11.57% Jbr a 5% simple random sample and 6% for a 20% simple random sample. Moreover, the FAST algo-rithms typically generate 30-50% fewer false itemsets than does Sl:ts-Apriori. The metric Dist2 seems to be more ef-fective than Distx in most cases. One possible reason for this is that Distl, unlike Dist2, does not penalize for a poor approximation of a 1-itemset frequency if this frequency is less than the minimum support. Thus the entire set of 1-itemset frequencies may not be approximated as well in a global sense, and consequently the k-itemset frequencies are not approximated to sufficient accuracy. Figure 3: Accuracy vs. Sampling Ratio on Synthetic 
Data w/ Fixed-Size Stopping Criterion Figure 4: Accuracy vs. Sampling Ratio on Synthetic Data w/Min-Distance Stopping Criterion distance stopping criterion. For example, FAST-t-D1 quickly achieves a high level of accuracy --a 5% final sample size gives results comparable to a 30% simple random sample. 
In contrast, the performance of the FAST-grow algorithms degrades under the min-distance Stopping criterion. The problem appears to be that the final sample size is too small. 
For example, after FAST-g-D1 executes the rollback opera-tion, the final sample size is typically equal to about 0.2%. 
Although there are few false or missing large 1-itemsets in the small sample, a large number of false k-itemsets are gen-erated for k &gt; 1. The problems associated with using Distl are magnified in this situation, which is why tlae performance of FAST-g-D1 is particularly bad. subsequent frequent-itemset generation) of both sP.s-Apriori and various FAST algorithms on the synthetic database as a function of the sampling ratio. We consider only FAST algorithms that use the fixed-size stopping criterion --as discussed in the previous section, use of this criterion results in better performance and more stable behavior. As shown in the figure, the execution times of the FAST algorithms were all very similar to the corresponding times for SRS-
Apriori. Thus the additional processing time --relative to simple random sampling --that is required by FAST to trim or grow a sample is insignificant compared to the time required to generate the frequent itemsets. 
Figure 6, which illustrates the tradeoffs between accuracy and execution time for various FAST algorithms and for SRS-
Apriori. As shown in the figure, almost all FAST variations performed better or much better than sP.s-Apriori. Figure 5: Execution Time vs. Sampling Ratio on 
Synthetic Data w/Fixed-Size Stopping Criterion Figure 6: Accuracy vs. Execution Time on Synthetic Data w/ Fixed-Size Stopping Criterion ten seconds, FAST-g-D2 can achieve an accuracy of approxi-mately 94%, compared to an accuracy of 84% for SRS-Apriori. 
Looked at another way, FAST-g-D2 achieves an accuracy comparable to SRs-Apriori in about 35% less time. 467 We also can compare the performance of FAST with that of Toivonen's Algorithm. Within 10 seconds, FAST-g-D2 can process a 5% final sample and achieve approximately a 95% accuracy. On the contrary, it takes about 100 seconds, or about 10 times longer, for Toivonen's algorithm to finish the mining task using the same sample, whereas its resulting ac-curacy is 99.8%. The problem with Toivonen's Algorithm is that even though the generation of frequent itemsets from the sample is relatively inexpensive, the pass over the re-maining database to eliminate false itemsets can be quite expensive. The expense is especially high when the data-base contains a large number of long transactions and/or long frequent itemsets. This problem persists no matter how small the sample is --note that the smaller the sam-ple, the larger the remaining database. Also, unlike with 
FAST, the user has no real control over the tradeoff between speed and accuracy: neither the lowered minimum support value nor the size of the negative border can be manipulated by the user. The discrepancies in processing time between 
FAST and Toivonen's algorithm only increase as the data-base becomes larger. Thus if extremely high accuracy is of paramount importance and processing time is not an issue, then Toivonen's algorithm is a reasonable choice. Otherwise, 
FAST is clearly the algorithm of choice. We have introduced in this paper an efficient two-step data reduction approach based on sampling and tailored to the mining of count data, in particular, the fast discovery of association rules. In the first step, a relatively large simple random sample is obtained and used to estimate the support of each item in the database.' In the second step, a final small sample is created either by trimming ontliers or selecting representative transactions based on a distance function that incorporates the 1-itemset supports computed in the first step. An empirical study using both a real database and a syn-thetic database supports our claims of efficiency and accu-racy. FAST was able to achieve 90-95% accuracy using a final sample size 15-33% as large as the simple random sam-ple used by SRs-Apriori. This efficiency gain resulted in a speedup by roughly a factor of 10 over algorithms, even effi-cient ones such as that of Toivonen, that require one or more expensive passes over the entire database. Unlike the latter algorithms, the user of FAST has relatively fine control, by means of the adjustable algorithm parameters/c and n, over the tradeoff between speed and accuracy. As mentioned in the text, the detailed issues involved in combining the FAST sampling technique with some of the more recent association-rule algorithms, such as Apriori, 
DIC, Max-Miner, DepthMiner, or FP-tree, need to be in-vestigated. In general, we plan on exploring the efficacy of our sampling technique for other raining and statistical anal-ysis tasks for count data. It would be desirable to push the ideas developed in this paper even further, perhaps combin-ing them with online processing ideas as in [11] or [13], in order to make data mining systems even more interactive and subject to user control. We would like to thank Rakesh Agrawal, Roberto Ba-yardo, Lisa Hellerstein, Herve Bronimann, Lisa Singh, and 
R. Srikant for helpful discussions. [1] S. Acharya, P. B. Gibbons, and V. PoosMa. Congressional samples for approximate answering of group-by queries. In 
Proe. Proc. 2000 ACM SIGMOD, 2000. [2] R. C. Agarwal, C. C. Aggarwal, and V. V. V. Prasad. Depth first generation of long patterns. In Proe. Sixth ACM 
SIGKDD, 2000. [3] R. Agrawal, T. Imielinski, and A. Swami. Mining association rules between sets of items in large databases. In Proc. 1993 
ACM SIGMOD, 1993. [4] R. Agrawal and R. Srikant. Fast algorithms for mining association rnles. In Proc. gOth VLDB, 1994. [5] R. J. Bayard,). Efficiently mining large patterns from databases. In Proe. 1998 ACM SIGMOD, 1998. [6] V. Barnett and T. Lewis. Outliers in Statistical Data. John 
Wiley ~ Sons, New York, 1994. [7] S. Brin, R. Motwani, J. Ullman, and S. Tsur. Dynamic itemset counting and implication rules for market basket data. In Proc. 1997 AGM SIGMOD, 1997. [8] S. Choudhuri, M. Datar, R. Motwani, V. Narasayya. Overcoming limitations of sampling for aggregation queries. 
In Proc. 17th ICDE, 2001. [9] J. Ernvall and O. Nevalainen. An algorithm for unbiased random sampling. Comput. J., 25:45-47, 1982. [10] P. Gibbons. Distinct sampling for highly-accurate answers to distinct wdues queries and event reports. In Proc. VLDB 
Oonference., 2001. [11] P. J. Haas. Techniques for Online Exploration of Large Object-Relational Datasets. In Proc. llth Intl. Conf. Scientific and Statistical Database Management, pages 4-12. 
IEEE Press, 1999. [12] Lisa Hellerstein. Proof of the NP-completeness of FAST. 
Private Correspondence, 2000. [13] C. Hidber. Online association rule mining. In Proe. ACM 
SIGMOD, 1999. [14] Jiawei Han, Jian Pei and Yiwen Yin. Mining frequent patterns without candidate generation. In Proc. A CM 
SIGMOD, 2{}00. [15] P. J. Huber. Robust Statistics. Wiley, New York, 1981. [16] G. H. John and P. Langley. Static versus dynamic sampling for data mining. In Proc. Second Intl. Conf. Knowledge 
Discovery and Data Mining, pages 367-370. AAAI Press, 1996. [17] J. Kivinen and H. Mannila. The power of sampling in knowledge discovery. In Proc. Thirteenth A CM SIGACT-SIGMOD-SIGART Syrup. Principles of Database 
Sys., pages 77-85. ACM Press, 1994. [18] R. G. Miller. Beyond ANOVA: Basics of Applied Statistics. 
Wiley, New York, 1986. [19] F. Olken. Random Sampling from Databases. Ph.D. Dissertation, University of California, Berkeley, CA, 1993. Available as Tech. Report LBL-32883, Lawrence Berkeley 
Laboratories, Berkeley, CA. [20] H. Toivonen. Sampling large databases for association rules. In Proc. ~2nd VLDB, 1996. [21] It. Winter and K. Auerbach. The big time: 1998 Winter 
VLDB surw~y. Database Programming Design, August, 1998. [22] M. J. Zaki, S. Parthasarathy, W. Lin, and M. Ogihara. Evaluation of sampling for data mining of association rules. Technical Report 617, University of Rochester, Rochester, 
NY, 1996. 
