 Mining association patterns has a long tradition in data-mining. Most methods, however, are designed for binary or categorical attributes. The usual approach to numerical data is discretization [22]. Discretization however leads to information loss and problems such as rules being split over several intervals. Approaches al-lowing numerical attributes in rule consequent have been proposed, such as [3,25], but they do not allow undiscretized numerical attributes in rule antecedent.
Recently, progress has been reported in this area, with a number of pa-pers presenting extensions of the definiti on of support not requiring discretiza-tion [23,14,7]. Other papers provide alternative approaches which also do not require discretization [20,12,19,1,5].

This work extends those methods further, allowing for the discovery of com-plex nonlinear relationships between sets of numerical attributes without the need for discretization. The work is set in the spirit of association rule mining. First, a concept of minimum variance itemsets is introduced. Those itemsets describe functions which are always close to zero on a given dataset, and thus represent equations describing relationships in data. Based on those itemsets, rules can be derived showing relationships between disjoint sets of attributes. An Apriori style mining algorithm is also presented.

Let us now review the related work. The approach presented in [16] allows for combining attributes using arithmetic operations, but after combining them discretization is applied. Also, since only addition and subtraction are allowed, nonlinear relationships cannot be represented.

In [20,12,19] a method for finding rules of the form  X  X f a linear combination of some attributes is above a given threshold, then a linear combination of another set of attributes is above some other threshold X  is described. Rules of this type are mined using standard optimization algorithms. While the approach could be extended to nonlinear case, the method presented here is more efficient since it requires solving eigenvalue problems of limited size instead of using general optimization methods on the full dataset. Furthermore, since binary thresholds are used, the method from [20] cannot repre sent continuous relationships between groups of attributes. Our work is more in the standard association rule spirit providing both itemsets and rules, as well as an Apriori style mining algorithm.
In [1], an interesting method is presented for deriving equations describing clusters of numerical data. The authors first use a clustering algorithm to find correlation clusters in data, and then deri ve equations describ ing the linear space approximating each cluster X  X  data points based on the cluster X  X  principal com-ponents computed using eigenvectors of the correlation matrix of data in the cluster. While the use of eigenvectors to discover equations may suggest sim-ilarities, the approach presented here is quite different. We are not trying to describe previously discovered clusters, but give method of pattern discovery (defining itemsets and rules) in the spirit of association rule mining. Further we allow for arbitrarily complex nonlinear relationships to be discovered, while [1] essentially describes a cluster as a linear subspace. Third, by adding an extra constraint to the optimization, we guarantee that patterns discovered will not involve statistically independent attributes.
 There is some similarity between our approach and equation discovery [9,18]. Equation discovery algorithms are in principle capable of discovering minimum variance patterns we propose. However the discovery methodology, is quite dif-ferent in both cases. In fact our approach was more than an order of magnitude more efficient than Lagrange [9], an equation discovery system. Combining the two approaches, such as using equation discovery to give explicit formulas for minimum variance patterns is an interesting topic for future research. Let us begin by introducing the notation and some preliminary concepts.
We assume that we are dealing with a dataset D whose attributes are all numeric. Non-numerical attribut es can be trivially converted to { 0 , 1 } attributes. To avoid overflow problems while computing powers, we also assume that the attributes are scaled to the range [  X  1 , 1].

Attributes of D will be denoted with letters X with appropriate subscripts, and sets of attributes with letters I,J,K .If t  X  D is a record of D ,let t.X denote the value of attribute X in t ,and t [ I ] the projection of t on a set of attributes I . Following [15,8] we now define support of arbitrary functions. Let f be a function of an attribute set I . Support of f in D is defined as
We are now ready to describe minimum va riance itemsets, the key concept of this work. Our goal is to discover arbitrary relationships between the attributes of D . The patterns we are looking for have the general form where we expect the function f to somehow capture the relationship among the variables of I = { X 1 ,X 2 ,...,X r } .

Let us look at two examples. Suppose we have two attributes x and y ,such that x = y . The equality between them can be represented by an equation so one possible function f for this case is x  X  y . Suppose now that x, y represent random points on a circle of radius 1. The function f could now be f ( x, y )= x 2 + y  X  1 since the relationship can be described by an equation x 2 + y 2  X  1=0.Of course if noise was present the equalities would be satisfied only approximately.
The common pattern of the two above cases is, that the function f was iden-tically equal to zero for all points (reco rds) in the data. It is thus natural, for a given itemset I , to look for a function f ( I ) which minimizes We will call this quantity the variance of f around zero ,orbriefly variance , and a function minimizing it, a minimum variance itemset . This concept should not be confused with statistical notion of variance, which would be around the function X  X  mean (we consciously abuse the terminology).

This formulation has a problem. The function f ( I )  X  0 minimizes variance but does not carry any information. Also 1 2 f necessarily has lower variance than f , although it does not carry any more information. To avoid such situations, we add a normalizing condition guaranteeing that the function f is of appropriate magnitude. Several such normalizations will be presented below. 2.1 Formal Problem Statement The above discussion was in terms of arbitrary functions. In practice we have to restrict the family of functions considered. Here we choose to approximate the functions using polynomials, such that the degree of every variable does not exceed a predefined value d .Let I = { X 1 ,...,X r } be a set of attributes. Then any function f of interest to us can be represented by ficients and monomials involved in two column vectors (using the lexicographic ordering of exponents): We now have f c = c T x = x T c ,and f c 2 = c T ( xx T ) c .Noticethat xx T is a ( d +1) r  X  ( d +1) r matrix, whose entries are monomials with each variable raised to power at most 2 d . So the entry in row corresponding to (  X  1 ,..., X  r )and
We now use the trick from [15] in order to compute support of f c 2 for various values of c without accessing the data. Let t [ x ]denotethe x vector for a given where S D is a ( d +1) r  X  ( d +1) r matrix, whose entry in row corresponding to (  X  1 ,..., X  r ) and column corresponding to (  X  1 ,..., X  r ) contains the value of sary monomials, after which support of f c 2 for any coefficient vector c can be computed without accessing the data, using the quadratic form (1).

We now go back to the problem of normalizing f c such that the trivial solution f c  X  0 is avoided. We tried various normalizations: (a) require that the vector c be of unit length, || c || =1, (b) require that weighted length of c be 1,  X  w  X  c  X  2 = 1, this allows for penal-(c) require that support of f c 2 ( I ) be equal to one, under the assumption that (d) require that support of f c 2 ( I ) be equal to one, under the assumption that
When no outliers were present, all of those approaches worked reasonably well. However in the presence of outliers only approach (d) was useful. Other methods picked f c such that it was close to zero everywhere except for the few outlier points. Also, this approach guarantees that patterns involving statistically independent attributes will have high minimum variance.

We thus limit further discussion to normalization based on the requirement (d). Imagine a hypothetical database D I in which each attribute is distributed as in D but all attributes are independent. The support of f c 2 under such an independence assumption can be computed analogously to (1) as supp I ( f c 2 )= c
S I c , where an element of S I in row corresponding to (  X  1 ,..., X  r )andcolumn corresponding to (  X  1 ,..., X  r )isgivenby since variables X 1 ,...,X r are assumed to be independent.

We are now ready to formally define a minimum variance itemset for a given set attributes I : Definition 1. A real valued function f on a set of attributes I is cal led itemset on I .The variance of f is defined as A minimum variance itemset on I is a function f  X  ( I )= f c  X  ( I ) on a set of attributes I which minimizes c T S D c subject to a constraint c T S I c =1 . 2.2 Finding the Minimum Variance Itemset for a Set of Attributes To find a minimum variance itemset for a given I we use the method of Lagrange multipliers [11]. The Lagrangian is L ( c , X  )= c T S D c  X   X  c T S I c  X  1 .Using elementary matrix different ial calculus [24,13] we get  X  X   X  c =2 S D c  X  2  X  S I c ,and after equating to zero we get the necessary condition for the minimum: This is the generalized eigenvalue problem [10,24,13], well studied in computa-tional linear algebra. Routines for solving this problem are available for example in LAPACK [10]. If ( c , X  ) is a solution to (2), a candidate solution c to our opti-mization problem is obtained by scaling c to satisfy the optimization constraint: c = c  X  var( f c ) = supp D ( f c 2 )= c T S D c = The variance of c is thus equal to the corresponding eigenvalue, so the final solution c  X  is the (scaled) eigenvector corresponding to the smallest eigenvalue.
The above property can be used to speed up computations, since finding only the smallest eigenvalue can be done faster than finding all eigenvalues (routines for finding a subset of eigenvalues are also available in LAPACK).

Another important observation is that matrices S D and S I are symmetric (follows directly from their definition) and positive semi-definite (support of a square of a function cannot be negative). This again allows for more efficient computations, see [10,24] for details. 2.3 Example Calculation We will now show an example calculation on a toy example of a dataset D = { supp D ( xy 2 )=  X  32, supp D ( x 2 y 2 ) = 72. Supports under independence assump-supp D ( y ) = 24, etc. The S D and S I matrices are After solving the generalized eige nvalue problem and rescaling we get c  X  = [0 ,  X  0 . 5 ,  X  0 . 25 , 0]. The correct relationship  X  2 x  X  y = 0 has been discovered. Let us now discuss closure properties of minimum variance itemsets.
 Theorem 1. Let I  X  J be two sets of attributes, and f  X  ( I ) and g  X  ( J ) be mini-mum variance itemsets on I and J respectively. Then var( g  X  )  X  var( f  X  ) . In other words variance is upward closed, adding attributes reduces the variance. The proof is a trivial consequence of the fact that a function of I is also a function of J (constant in variables in J \ I ), so the lowest variance attainable for J is at least as low as the variance attainable for I , and may be better.

The problem is that we are interested i n itemsets with low variance, so if one is found, all its supersets are potentially interesting too. The solution is to set a minimum threshold for variance, and then find smallest (in the sense of set inclusion) sets of attributes for which the variance (of the minimum variance itemset or the itemset X  X  best equality or r egression rule) is less than the specified threshold. Similar approach has been used e.g. in [6]. The algorithm is a simple adaptation of the Apriori algorithm [2], and is omitted due to lack of space. In order to facilitate the interpretation of minimum variance itemsets two types of rules are introduced. The first kind are what we call equality rules . Definition 2. An equality rule is an expression of the form g ( I )= h ( J ) ,where I  X  J =  X  ,and g and h are real valued functions on I and J respectively. The variance of the rule is defined as var( g ( I )= h ( J )) = supp D ( g  X  h ) 2 . Thus equality rules capture relationships between disjoint groups of attributes which are usually easier to understand than the itemsets defined above.
A minimum variance equality rule g  X  ( I )= h  X  ( J ) is defined, similarly to the minimum variance itemset case above, as a pair of functions for which ( g  X  h ) 2 is equal to one, under the independence assumption. Finding mini-mum variance equality rules for given I and J can be achieved using the same approach as finding minimum variance itemsets. If we approximate both g and h with polynomials, I = { X 1 ,...,X r } and J = { X r +1 ,...,X r + s } , and denote term is omitted from c h and x h , since it is included in c g and x g .
From that point on, the derivation p roceeds exactly as in the case of min-imum variance itemsets in order to find the vector [ c g | c h ]  X  which minimizes supp D ( g + h ) 2 subject to supp I ( g + h ) 2 = 1. After finding the solution, signsofcoefficientsin c h are reversed to get from a minimum variance for g + h to the desired minimum variance for g  X  h .

Finding a minimum variance equality rule on I and J is analogous to finding a minimum variance itemset f on I  X  J subject to an additional constraint that f be a difference of functions on I and J . Thus, the minimum variance of an itemset on I  X  J is less than or equal to the minimum variance of an equality rule on I and J . If an itemset has high minimum variance, we don X  X  need to check rules which can be generated from it, sin ce their variance is necessarily high too. Another kind of rules are what we call regression rules .
 Definition 3. A regression rule is an expression of the form X = g ( I ) ,where X is an attribute, I a set of attributes, X  X  I ,and g is a function of I . It is easy to see that regression rules are equality rules with additional constraint that one side of the rule must contain a single attribute in the first power only. It is thus clear that minimum varian ce of a regression rule cannot be lower than minimum variance of a corresponding equality rule. Also, the definition of variance of a regression rule as well as discovery of minimum variance regression rules are analogous to the case of equality rules and are thus omitted.
Minimum variance regression rules correspond to standard least-squares poly-nomial regression with X being the dependent variable. Therefore minimum vari-ance equality rules can be seen as a genera lization of standard polynomial regres-sion to allow functions of dependent variables, and minimum variance itemset as a further generalization allowing for discovering patterns not involving equality. In this section we show some illustrative examples of patterns discovered, and give some suggestions on how to elicit understandable knowledge from them.
We first apply the method to a small artificial dataset. The dataset has three attributes x, y, z , and is generated as follows: ( x, y ) are randomly chosen points on a unit circle and z is set equal to x . The relationships among the attributes are therefore z = x , x 2 + y 2 =1,and z 2 + y 2 =1.

We applied the algorithm with d = 2 without any minimum variance thresh-old. Only pairs of attributes were consid ered. Generated patterns are given in the table below (terms with negligibly small coefficients are omitted)
The minimum variance itemsets for { x, y } and { y, z } do not require any com-ment. They clearly capture the correct relationship x 2 + y 2 =1.

The case for { x, z } is more interesting. Instead of the expected x  X  z =0 we obtained an equivalent, but more complicated expression ( x  X  z ) 2 =0.The reason is that the degree of the approx imating polynomial exceeds that of the true relationship. As a result, two of the eigenvalues are equal to zero, and any linear combination of their corresponding eigenvectors is also a minimum variance solution. To avoid such situat ions we recommend decreasing the value of d until a minimum value is found at which the relationship still occurs. In the currently analyzed case lowering d to 1 gives the expected  X  0 . 997 x +0 . 997 z =0. Another approach, to use regression rules, which also helps in this case.
It should be noted that the best regression rules for { x, y } and { y, z } have vari-ance of about 1, so the relationship would not have been discovered by standard regression analysis (indeed the c orrelation coefficient is about 8  X  10  X  3 ).
Let us look at another example which shows that minimum variance itemsets are able to represent patterns much reacher than those usually described using algebraic equations. Consider an artificial dataset which has two attributes x, y  X  [  X  1 , 1] and contains points randomly generated on the set where the condition x&lt; 0  X  y&lt; 0 is true. Thus no points are present in the [0 , 1]  X  [0 , 1] square. The correlation coefficient is  X  0 . 359, thus not very high. The minimum variance itemset on xy however, has small values everywhere except for the [0 , 1]  X  [0 , 1] square and the minimum variance of { x, y } is 0 . 024. The representation is of course not perfect, but tends to approximate the data quite well (Figure 1). We will see a similar pattern occurring in real life datasets ( sonar )below. Extrasolar planets data. This section shows more examples of minimum variance patterns. The dataset used is about currently known extrasolar planets, and can be downloaded from [21]. Six attributes were chosen and 197 planets selected for which all those attributes were defined. The attributes are described in the table below: Attributes were scaled to [0 , 1] range, so units are omitted. Afterwards, logarith-mic transform was applied. The advantage of the data is that there are some well established relationships which should be discovered if the method works correctly. This experiment is similar to that from [18], but uses more data and involves additional relationships.

First, semi-major axis divided by the distance of the star from Earth is equal to the tangent of the angular distance of the star from the planet. Second, by Kepler X  X  law, the square of orbital period of a planet is proportional to the cube of the semi-major axis of its orbit. If planet and star masses are known, the proportionality constant can also be determined [17]. It is possible that further relationships exist, but due to the author X  X  lack of astronomical knowledge they will not be discussed. We begin by looking at pairs of attributes. The value d =2 was used, with no minimum variance requirement.

The strongest relationship wa s discovered between planet X  X  period and its semi-major axis with minimum variance of 6 . 83  X  10  X  5 . The relationship is shown in Figure 2. The data points are marked with circles. Contour plot of the minimum variance itemset is also show n. According to Kepler X  X  law there is a linear relationship between logarithms of the two values. The minimum variance itemset is not linear (due to overfitting and ignoring the star mass) but captures the relationship well. Decreasing the degree or examining rules, reveals the linear nature. The clarity of the relationship is surprising since, planet and star masses also play a role. It turned out, that masses of most stars in the data are very close to each other, and planets X  masses are too small to distort the relationship.
To explore the relationship further we examined patterns of size 3 and 4 con-taining attributes period and semi-major axis . As expected, the most interest-ing pattern of length three added the star mass attribute (minimum variance 6 . 85  X  10  X  7 ), and by adding pl. mass , a four attribute set was obtained with variance 8 . 33  X  10  X  10  X  an almost perfect match.

The triple of attributes which had the lowest variance of 9 . 72  X  10  X  8 was semi-major axis , ang. distance and star distance . This is expected due to the deterministic relationship among t hem described above. All equality rules involving those attributes had very low variance too. Variance of regression rules was higher (in the range of 10  X  4 ).

An interesting subset of the above triple, is the pair semi-major axis and ang. distance . Its minimum variance is 0 . 027, but the variance of all rules between those attributes is much higher, about 0 . 15 in all cases. This is another example of a low variance itemset which cannot be captured by equality rules. The situation is depicted graphically in Figure 3, where data points and the contours of the itemset are shown. It can be seen that there is a clear relationship between the attributes, high values of semi-major axis correspond to low values of ang. distance and vice versa. But the relationship is not functional, and is not well described by rules. Nevertheless, the minimum variance itemset has values close to zero in the areas where there is a lot of data points. Minimum variance patterns are thus capable of discovering, and describing groupings of data points which are otherwise hard to define.
 The sonar dataset. We now turn our attention to the well known sonar dataset. Since our method is somewhat sensitiv e to outliers, we removed every record which contained a value more than 3 standard deviations from the mean for some attribute. An interesting pattern has been found between attributes 15 and 44, see Figure 4. We can see that high values of both attributes never occur together. The actual relationship is reminiscent of the second artificial dataset presented above. The correlation coefficient is only  X  0 . 114; based on it, the pattern would have most probably been missed by traditional correlation analysis. This situation is similar to  X  X oles in data X  analyzed in [4] which are well approximated in our framework. We now present performance evaluation of the minimum variance itemset mining algorithm. The default parameters were d = 2 and maximum of r = 3 attributes per itemset. We found this combination t o be flexible enough to discover complex patterns, which are still reasonably easy to interpret.

We used three datasets for testing: the extrasolar planet and sonar datasets described above, and a large Physics dataset from the KDD Cup 2004 competi-tion with 80 attributes and 50000 records.

The algorithm has been implemented in C. Figure 5 (left) shows the influ-ence of the parameter d on computation time for various minimum variance thresholds. The parameter r is kept equal to the default value of 3. Figure 5 (right) shows the influence of the r parameter ( d is kept equal to 2). Note that charts for d = 2 (left) and for r = 3 (right) in Figure 5 are identical since they correspond to the same parameter values. While performance of the algorithm is worse than for association rules in case of binary attributes (this is to be expected due to a much reacher structure of the data), the algorithm is practically applica-ble even for large datasets. It is interesting to see that, below a certain threshold, the minimum variance parameter has little influence on computation time.
We have also compared our approach with an equation discoverer Lagrange [9] (horizontal lines in Figure 5 (right)). The parameters were set such that it would discover polynomials of degree at most 2 involving at most 2 or 3 variables. Our approach was more than an order of magnitude faster than Lagrange. This is not surprising, as for every set of attributes Lagrange conducts an exhaustive search compared to a single relatively efficient eigenvalue computation in our case. A method for discovering arbitrarily complex relationships among numerical at-tributes has been presented. Its application yields itemsets and rules in the spirit of associations discovery. It has been shown experimentally that the approach does indeed produce interesting patterns, which capture various types of com-plex relationships present among the attributes. It is capable of finding patterns which would have been missed by standard polynomial regression analysis.
Future work is planned on increasing performance, e.g. by using bounds for eigenvalues to prune itemsets early.

