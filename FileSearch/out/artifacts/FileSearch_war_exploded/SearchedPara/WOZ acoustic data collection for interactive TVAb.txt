 Abstract This paper describes a multichannel acoustic data collection recorded under the European DICIT project, during Wizard of Oz (WOZ) experiments car-ried out at FAU and FBK-irst laboratories. The application of interest in DICIT is a distant-talking interface for control of interactive TV working in a typical living room, with many interfering devices. The objective of the experiments was to collect a database supporting efficient development and tuning of acoustic pro-cessing algorithms for signal enhancement. In DICIT, techniques for sound source localization, multichannel acoustic echo cancellation, blind source separation, speech activity detection, speaker identification and verification as well as beam-forming are combined to achieve a maximum possible reduction of the user speech impairments typical of distant-talking interfaces. The collected database permitted to simulate at preliminary stage a realistic scenario and to tailor the involved algorithms to the observed user behaviors. In order to match the project require-ments, the WOZ experiments were recorded in three languages: English, German and Italian. Besides the user inputs, the database also contains non-speech related acoustic events, room impulse response measurements and video data, the latter used to compute three-dimensional positions of each subject. Sessions were man-ually transcribed and segmented at word level, introducing also specific labels for acoustic events.
 Keywords Multimodal Corpus annotation Audio 1 Introduction The goal of the European project DICIT 1 (Distant-talking Interfaces for Control of Interactive TV) is the development of a user-friendly human-machine interface that enables a speech-based interaction with TV, related digital devices and infotainment services. In the foreseen scenario, the user interacts with the system in a natural and spontaneous way, without being encumbered by a head-mounted close-talk microphone. The system operates in a living room furnished with several digital devices, amongst others a TV equipped with a Set-Top Box (STB). Different users, one at a time, will have access to the system, which also offers an information service from the Electronic Program Guide (EPG). Multimodality allows the user to choose between or combine two different interaction modalities: traditional TV remote control and spoken commands.

Speech-based interaction depends on a well-suited multichannel acoustic front-end processing stage which provides the Automatic Speech Recognizer (ASR) with enhanced user speech segments. In a distant-talking scenario the quality of the acoustic signals captured by microphones is considerably deteriorated by environ-mental noise and reverberation. Multiple interfering sources and the presence of multichannel TV audio output further complicate this scenario. Apart from the design of robust dialogue strategies and the implementation of an accurate ASR engine, one of the most critical aspects of the project is the development of a reliable acoustic front-end, capable of tackling the difficulties of the given scenario by employing a suitable combination of multichannel acoustic echo cancellation, beamforming, blind source separation, acoustic event detection and classification and multiple speaker localization.

Different realizations are possible for all of the aforementioned algorithms, with performance and reliability depending on the scenario under analysis, i.e., length of the speech commands, user mobility, etc. In order to characterize the user behavior and to investigate the most crucial aspects of the project in terms of acoustic front-end processing, a set of Wizard of Oz (WOZ) experiments was conducted. In parallel, a second set of WOZ experiments tailored to support the dialogue design was conducted at other partner sites.

After a detailed description of the WOZ experiments, with particular emphasis on the multichannel setup adopted in the project, this paper gives an overview of the annotation process and describes the analysis conducted on the database at algorithmic level. Final discussions conclude the paper. 2 Description of the Wizard of Oz experiments In a Wizard of Oz experiment, a subject is requested to complete specific tasks using an artificial system, which is operated by a person not visible to the subject. The user not being aware of the operating person (the wizard), is asked to use the system intuitively. The wizard can react to user inputs in a more comprehensive way than any artificial intelligence, because not confined by computer logic. In an effort to simulate the behavior of a real system based on voice interaction as closely as possible, recognition errors are randomly simulated by the wizard. From a WOZ study, interaction patterns in terms of dialogue and acoustics can be extracted and applied to an actual prototype.

In general, the goal of a WOZ experiment is to obtain information in order to design proper dialogue strategies and language models. Conversely, our WOZ experiments focused on the need to create realistic usage scenarios for acoustic pre-interaction.

The WOZ experiments were conducted in two rooms located at two different sites. The WOZ model has been translated and the experiments have been recorded in three languages: English, Italian and German. However, as already mentioned, the data collection was meant for front-end analysis only and therefore language did not represent a crucial parameter. The data collection consists of twelve sessions as described in Table 1 .
 Three na X   X  ve users and one supervisor (co-wizard) participated in each session. Subjects were recruited from the staff and students at FAU and FBK X  X herefore, the sample was not only composed of technology professionals but also of people from other fields of work (administration, etc.). Before the beginning of the experiments, all subjects received an instruction sheet describing the tasks and the expected user behavior. Although all four participants were simultaneously present in the room, only one person at a time was allowed to interact with the system. In any case, other participants unconsciously made occasional noises that were recorded by the system. We chose to do recordings with a group of four people to simulate a typical home scenario (e.g., a family watching TV). The supervisor, a person working in the navigating through the dialogue system, to accomplish the predefined tasks. At the same time, the supervisor intentionally generated a number of acoustic events being typical for a domestic scenario. These events were a subset of the ones investigated in previous data collections conducted under the European project CHIL 2 (Temko et al. 2006 ). In our WOZ experiments the following events were considered relevant for the DICIT scenario: slamming doors, chairs being moved, ringing phones, coughing, laughing, falling objects and rustling paper. 2.1 Session structure Each session was split into two phases. At the beginning, all participants sat in front of the television and read out a set of phonetically rich sentences that include a quasi-balanced combination of all phonemes of the respective language, leaving out all combinations that are invalid for that language. Since these sentences offer a complete coverage of the phonemes, they may be used to train acoustic models for statistic-based algorithms such as speaker identification and verification (Furui 1997 ) as well as for speech recognition purposes. The duration of the phonetically rich sentences was about 15 X 20 seconds for each subject.

During the second phase, each person interacted with the system trying to accomplish a list of predefined tasks. These included the typical actions to control a traditional television (e.g., channel switching, volume control) but also more complex actions, such as teletext browsing and searching for specific news.
In order to let users get familiar with the system, the first part of the interaction was conducted using only the remote control. After this warm-up stage the users were allowed to control the system via both remote control and voice commands while sitting on their chairs. In the final part of the experiments the subjects were asked to find specific pages in the teletext using voice-commands only, while moving freely in the room. This final phase was especially intended for testing the source localization algorithms. Since our focus was on data recording for technology development rather than on dialogue modeling, we did not care about the fact that the later users gained experience by observing the behavior of the previous users (however it could be interpreted as a simulation of different levels of expertise).

Overall, each user interaction lasted about 10 minutes, which led to a total of 360 minutes of recordings. 3 Experimental setup Two office rooms (one at FAU and one at FBK) were equipped for the WOZ experiments. The objective was to simulate a typical living room, in terms of dimensions, acoustic reverberation conditions, and background noise.

Since it was necessary to hide the wizard and the real system from the users, adjacent rooms were prepared as well. 3.1 WOZ room setup The television was simulated by means of a video projector, displaying its output onto a wall, and two loudspeakers that were placed on each side of the screen.
The participants sat on four seats, positioned at a fixed distance (about 2 meters) from the screen. It was observed that, even when allowed to move, participants rarely went closer than 1 meter from the TV.

Both television broadcasting and teletext were simulated by using previously recorded TV video clips and teletext pages, provided in three languages. The use of pre-recorded TV clips and static teletext pages guarantees the repeatability of the experiments and eases the annotation and analysis process ensuring a detailed knowledge of the audio signals played back by the TV loudspeakers. Both stereo channels of the system audio output were decorrelated before playback in order to allow an effective implementation of stereo acoustic echo cancellation without impairing listening quality (Huang and Benesty 2004 ). The system was controlled by the wizard through a Windows PC station located in the adjacent room. EB GUIDE Studio, a tool suitably designed to manage the dialogue flow in WOZ experiments, was adopted to record the dialogue sessions and control the system (Goronzy and Beringer 2005 ). The tool will be described in the Sect. 3.4 . Additionally, a TV remote control was integrated into the system using an infrared (IR) receiver in the experimental room, which was connected to the serial port of the PC of the wizard. One of the WOZ rooms is depicted in Fig. 1 .
 3.2 Audio and video sensor setup A harmonic 15-electret-microphone array has been specifically developed for the project, which was mounted above the television screen. The array allows various configurations: one currently employed configuration is its subdivision into four linear sub-arrays. These sub-arrays consist of overlapping subsets of the 15 microphones: three of them consist of five equidistant sensors each and one includes seven equidistant microphones. Our beamforming algorithm uses sub-arrays with different inter-microphone distances for different frequency ranges in order to provide an almost frequency-independent mainbeam (Kellermann 1991 ). Figure 2 shows the microphone arrangement within the harmonic array.

Sessions were also acquired by a modified NIST MarkIII linear array (Brayda et al. 2005 ), placed just above the harmonic array. This was done to have the possibility to compare the different arrays. The MarkIII, depicted in Fig. 3 ,is composed of 64 uniformly-spaced electret microphones. It records synchronous data at a sampling rate of 44.1 or 22.05 kHz with a 24 bit precision. The particularities of this array are the embedded digitalization stage and the data transmission via Ethernet channel using the TCP/IP protocol.

As shown in Fig. 1 , a table microphone and two lateral microphones (located on a side wall of the room) were also used for recording. All the microphones had an omnidirectional polar pattern. The table microphone was placed one meter away from the TV and was meant to simulate the microphone of a remote control (as used in some commercial solutions). The corresponding signal could also have been used instead of the array microphone signals if the quality of the latter should have proven to be considerably lower (due to the bigger distance to the user) X  X owever, this has not been the case. As to the lateral microphones, they were exploited only for experimental analyses and are meant to support the source localization stage. Finally, participants were also recorded by close-talk microphones whose signals were used to guarantee robust segmentation and accurate transcription. In total, 24 microphone signals and additional 64 channels from the MarkIII array were recorded at FBK. At FAU a slightly different setup with two more microphones within the harmonic array was used, which is shown in Fig. 4 . The 24/26 signals were synchronously recorded at 48 kHz sampling frequency with 16 or 32 bit precision and aligned at sample level. The MarkIII array is equipped with its own acquisition board at 44.1 kHz and 24 bit precision. Figure 1 shows the positions of the acoustic sensors.

In addition to the acoustic channels, the FBK room was equipped with three video cameras as shown in Fig. 1 : two placed on the upper corners and one placed on the ceiling (not shown in the figure). Video data were used both to monitor the experiments during the annotation process and to derive 3D reference positions for each participant. Notice that video and audio signals were manually aligned taking advantage of impulsive events present in the recordings, e.g., a slamming door. 3.3 Recording hardware setup To mimic the prototype by means of the WOZ simulation and to record in parallel all acoustic data, three PCs had to be employed. Two Linux machines (PC1 and PC2) were used for data recording, while a Windows machine (PC3) was used to run the EB GUIDE Studio simulation tool. Hardware setups at FAU and FBK laboratories were similar, with minor differences only.

The FBK setup is depicted exemplarily in Fig. 5 . PC1 was equipped with a multichannel soundcard (RME HDSP 9652) connected to three RME OctamicII preamplifiers with integrated A/D-converters, using ADAT Optical Interface. Sample synchronization and alignment of all boards was guaranteed by a BNC cable connected to the word clock input. All microphone signals plus the stereo TV output were recorded by PC1.

PC2 was connected to the MarkIII array by a dedicated network interface card and a LAN crossover cable. This machine was dedicated only to record data from the MarkIII array, due to the high data rates (about 480 MB/minute),
Finally, PC3 was used by the wizard to simulate the system and is partially described in the Sect. 3.1 . EB GUIDE Studio transmitted the visual TV content through a dual-head graphic card to the projector while TV audio was connected to both the acquisition boards of PC1 and to the loudspeakers.

At FBK, the three video cameras were connected to a fourth PC, not displayed in the schema, and not strictly necessary for the audio data collection. 3.4 Software setup 3.4.1 Recording software As mentioned above, the recordings had to cover long sessions with a variety of microphone and loudspeaker signals to be acquired at high sampling rates. In order to deliver usable data for acoustic pre-processing purposes both acquisition tools, i.e., the one acquiring the MarkIII signals and the one recording all remaining channels, had to guarantee lossless and synchronized recordings of the respective signals.

A hard disk recording audio tool called Ecasound 3 was employed to synchro-nously acquire 26 channels (this refers to the FAU recording setup which differs minimally from the setup at FBK). These 26 channels were acquired via five ADAT channels of the two RME HDSP 9652 multichannel soundcards installed on PC1. The signals were recorded into a single 26-channel wav-file at a sampling rate of 48 kHz and a resolution of 32 bit (the latter dictated by the soundcards, however also allowing more flexibility than directly recording with 16 bit precision; thus, a normalization according to the actual maximum recording level followed by 32-to-16 bit conversion remains possible). The single 26-channel wav-file was subsequently separated into 26 single-channel wav-files before a 32-to-16 bit conversion was carried out.

The NIST MarkIII array is provided with utilities for hard disk recording. A command-line program listens to the network card, connects to the array and stores the incoming data stream into a single file. The file contains all 64 interleaved channels at a sampling frequency of 44.1 kHz and a resolution of 24 bit. A custom-written program was used to extract and convert the single channels to 16 bit resolution. 3.4.2 EB GUIDE studio The EB GUIDE Studio developed by Elektrobit (EB), a member of the DICIT consortium, is a tool for Human Machine Interface (HMI) development which allows the user to design multimodal User Interfaces (UIs) with graphical, haptical, and speech dialog systems.

Running on PC3, a version (tailored to the acoustic WOZ) of EB GUIDE Studio, which was provided by EB, enabled the WOZ-simulation of the DICIT TV scenario. TV programmes were available through six video clips for each language. Each clip had a length of half an hour and had been pre-recorded using a digital satellite receiver (Dreambox DM7025). Additionally, a selection of several teletext pages was available that could be displayed by the software. While TV content including overlays was transmitted to a video projector, the control interface for the wizard was shown on the monitor. TV stereo output including occasionally generated speech outputs was transmitted to the preamplifiers (splitting it up for loudspeaker-playback and recording).

The control interface allowed the wizard to react to the testing persons X  commands. Reaction, which depends on the current state of the system (e.g., initialization phase, TV transmission), included the generation of text outputs (sometimes connected to a text-to-speech engine), changing channels, volume and teletext pages. The table microphone signal, which was recorded by PC2, was also used by the wizard to monitor the user X  X  commands.
WinLIRC 4 was employed to decode and provide the remote control commands to the GUIDE software, after having been properly configured to receive codes of our remote control. WinLIRC is a free software for Windows that enables the handling of infrared signals received by an optical device. The infrared receiver was installed in the recording room and connected via a serial cable to PC3. EB GUIDE Studio then connected to WinLIRC to receive the codes of the buttons pressed on the IR remote control. 3.5 Room impulse response measurements Impulse responses inside the two WOZ rooms were measured in order to provide data which could be used later for purposes such as  X  X  X peech contamination X  X  for the development of ASR acoustic models. At FAU, measurements were taken utilizing Maximum Length Sequences (MLS). A single loudspeaker was used to play the MLS sequence while the harmonic array and five separate microphones recorded the output synchronously. The loudspeaker was moved to 12 different positions within the room so that the measurement was repeated 12 times. Figure 6 depicts the impulse measurement setup at FAU.

At FBK, impulse responses were measured in the WOZ room using a chirp sequence played by a loudspeaker positioned on the seat of each participant. 4 Data utilization 4.1 Data annotation For future front-end algorithm and speech recognition testing, the six FBK sessions (in Italian) have been manually transcribed and segmented at word level by two annotators, also introducing specific labels for acoustic events. An annotation guideline, modified from a previous work (Cristoforetti et al. 2000 ), was used in order to ensure as much consistency as possible between the annotators. The data were annotated using Transcriber 5 , a free graphic annotation tool featuring multichannel view. To better understand the interaction between users and system, stereo audio files were created, putting the signal coming from the table microphone on the left channel and the sum of the close-talk microphones on the right channel. This way, the annotators could listen selectively to the environmental audio or to the uttered commands in order to compare what was actually said by the user with what the wizard, and thus the system, would hear.

Only three speakers per session were annotated, since the last speaker was always the co-wizard. Even if the supervisors actively used the system, we decided not to annotate their speech because they didn X  X  complete all the tasks.

Annotators were provided with a preliminary automatic segmentation based on the energy of the close-talk signals. Even if not completely reliable due to cross-talk effects and non-speech human sounds, this segmentation turned out to be a very useful starting point. It was also possible to display the automatic segmentation for each speaker to help in understanding which user was speaking or producing other acoustic events. Markers were either inherited from the automatic segmentation and adjusted manually in order to have a short period of silence before and after the usable signal, or corrected in the case of faulty automatic segmentations.
Annotation information comprises the name (ID) of the speaker, the transcription of the uttered sentence and any detected noise specified in the acoustic event list. Annotators were also instructed to properly label those sentences that were personal comments between users and were not addressed to the system. Seven classes of noises were identified and annotated with square brackets (e.g., [pap] representing paper rustling). Two other classes were created to label speakers X  or unknown noises. Noises and their associated labels are listed in Table 2 . Figure 7 shows the annotation of a session, uttered speech is annotated with the speaker-ID, along with noise symbols.

Temporal extension of different noise events was identified using a particular convention to disambiguate between impulsive or prolonged events. In the lower part of Fig. 7 the activities of several speakers can be observed, i.e., speaker_1 uttering a sentence while speaker_4 is rustling with a newspaper.

As to the recorded video data, a set of 3D coordinates for the head position of each participant was created with a video tracker based on a generative approach (Lanz 2006 ). The tracker used a set of target models, which were acquired before the experiments. Although the tracker is robust to unfavorable light conditions and a limited number of cameras, the presence of both limitations made the tracking task in the WOZ experiments considerably challenging. Therefore, automatically generated 3D labels were manually double-checked and segments with imprecise tracking performance were discarded. An accurate manual correction of the labeled positions was not feasible due to low image resolution. A reference position for each user was derived out of the 3D labels for each session, which includes the ID of the active speaker, his/her coordinates and information about the presence of noises. Table 3 shows an example of a reference file. The reference files were obtained as a combination of the raw 3D labels generated by the video tracker and the manual audio annotation described above, with a rate of five labels per second (the rate is a parameter that can be adjusted). 4.2 Data exploitation/testing The data collected during the WOZ experiments have been used for a preliminary evaluation of the DICIT front-end algorithms developed at FBK.

The main goal of the evaluation was to understand the peculiarities of the DICIT scenario and to investigate their impact on our signal processing algorithms in order to appropriately design them as adequate components for the first DICIT prototype. The results proved the usefulness of a WOZ approach for acoustic data collections.
It was observed that participants tend to use very short sentences, in a command-like fashion, to control the television. As a consequence, silence is predominant and the cumulative length of all speech segments is only about 15 X 20% of the whole recordings. It was also noted that users change their positions while being silent between two consecutive commands. The observed behavior causes major problems to acoustic front-end processing, some of which are mentioned in the following.
As far as source localization algorithms are concerned, tracking is not possible while users are silent. Since in DICIT the source localization information is used to select one out of several data-independent beamformer outputs whose look  X  X  X irection X  X  corresponds to the actual user position, the localization module must react very promptly. Otherwise the speaker may be out of the beam for the time needed by the system to locate the source leading to a degradation of the overall system performance. The WOZ data collection was fundamental for implementation design and parameter selection. Moreover, we observed that side microphones did not yield any substantial performance improvement regarding the accuracy of the source localization algorithms, which lead to the decision to exclude them from the DICIT setup.

Furthermore, the performance of the voice activity detection algorithm is crucial since commands are short and the system must not even miss a short part of the speech.

The WOZ data were also used to test the speaker verification and identification system: the algorithms were applied to the signals of the close-talk microphone, to the single central microphone of the array and to a beamformer output, using matched model condition and different training material quantities. The results show that beamforming improves the system performance compared to the single microphone case. However, the results are still inferior to those obtained using the close-talk signal. In the investigated scenario, the algorithm also suffers from the short length of the spoken commands.

Finally, the WOZ data were used to test the acoustic event detection system. The test data were composed of 682 speech segments and 108 non-speech segments that were manually extracted from the continuous audio stream. The results are promising and highlight that among the investigated acoustic events the most confusable events are speech, coughing and laughing. 5 Conclusions In this paper a multichannel data collection based on WOZ experiments that were tailored to acoustic front-end development was presented.

This collection of data has been the first of its kind and is of significant benefit to the design of acoustic front-end algorithms and also dialogue strategies.

The experimental setup, the session structure of the WOZ experiments as well as the annotation of recorded data was described in detail and may serve as a guideline for setting up similar data collections.
 It was shown that from the acquired data realistic user behaviors can be studied. This gain in knowledge was supplemented by the completion of questionnaires.
The database proved extremely valuable for the development, testing and improvement of various acoustic front-end technologies that shall work robustly in realistic scenarios.
 References
