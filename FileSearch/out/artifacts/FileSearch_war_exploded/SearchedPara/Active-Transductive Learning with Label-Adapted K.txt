 This paper presents an efficient active-transductive approach for classification. A common approach of active learning al-gorithms is to focus on querying points near the class bound-ary in order to refine it. However, for certain data distri-butions, this approach has been shown to lead to uninfor-mative samples. More recent approaches consider combin-ing data exploration with traditional refinement techniques. These techniques typically require tuning sampling of un-explored regions with refinement of detected class bound-aries. They also involve significant computational costs for the exploration of informative query candidates. We present a novel iterative active learning algorithm designed to over-come these shortcomings by using a linear running-time active-transductive learning approach that naturally switches from exploration to refinement. The passive classifier employed in our algorithm builds a random-walk on the data graph based on a modified graph geometry that combines the data distribution with current label hypothesis; while the query component uses the uncertainty of the evolving hypothe-sis. Our supporting theory draws the link between the spec-tral properties of our iteration matrix and a solution to the minimal-cut problem for a fused hypothesis-data graph. Ex-periments demonstrate computational complexity that is or-ders of magnitude lower than state-of-the-art, and compet-itive results on benchmark data and real churn prediction data.
 H.2.8 [ Database Management ]: Database Applications X  Data mining classification; transductive learning; graph; active learning; uncertainty sampling  X 
Dan Kushnir is a member of technical staff at Bell Labo-ratories research arm
Active learning is concerned with the design of algorithms that efficiently select a set of labeled training samples. The goal is to query those labels from a teacher which poten-tially optimize the prediction accuracy for the remaining unlabeled set. An active learner chooses at each trial a point (or possibly a batch) to be labeled by the teacher, and then learns a classification model. This process repeats itself until a label budget is used up. It has been shown that in some cases actively querying training samples expedites the learning process over traditional semi-supervised learn-ing (e.g. [3, 15]), and is valuable especially when labels are scarce, expensive to obtain, or when unlabeled samples are abundant.

Past work on active learning can be roughly divided into two main approaches: First, an approach that focuses on refinement of the version space by actively probing the class decision boundary (e.g. [28]). This approach, typically an inductive one, often fails as it misses class boundaries that were not sampled, simply by focusing on refinement of bound-aries already discovered and avoiding  X  X utlier X  portions of the data. A second approach is a combined one, which employs both refinement of the version space and general exploration of the feature space (e.g. [3, 4, 6, 23]). These methods typi-cally use graph-based techniques, where the exploration step samples pockets of data that the learner misclassifies and re-fines their class boundaries. However, the detection of such pockets is computationally challenging.

In this paper we focus on transductive learning and in particular on the second approach which combines explo-ration with boundary refinement since it captures both com-mon types of classification failures. Our contribution is an active-transductive approach that yields competitive re-sults while its implementation and computational effort are significantly simpler and lower than state-of-the-art algo-rithms. These advantages render it useful for real large data sets. In addition, unlike the combined approach that al-ternates between boundary refinement and exploration by use of pre-calculated parameters (e.g. [3, 23]), our method demonstrates early preference for exploration and naturally switches to refinement without the need for such parameters.
Our query strategy adapts the  X  X ncertainty sampling X  cri-terion [20] to the graph setting, and shows that it can be used to both probe unexplored data pockets and refine de-cision boundaries in linear running time. It builds upon a probabilistic framework that combines the density and geometry of data with the current label hypothesis that emerges in the active learning process. Our framework uses label-adapted kernels which also improve the accuracy of the querying process when class distribution is not smooth over the data. This advantage is emphasized since many other (passive and) active-transductive approaches assume such limiting smoothness for their success (e.g. [8,15,32,35]). In this paper we introduce theoretical justification for the use of label-adapted kernels as well as a convergence analysis, demonstrating its advantages in active-transductive learn-ing. Our supporting theory relies on a careful spectral anal-ysis of the iterative technique of Jacobi [13] for a particular system of equations and its connection with spectral graph theory [10].

Aside from the observed improved accuracy, the presented classifier and the query components both have asymptotic running time that is linear in the size of the data set. In particular, and unlike other active classifiers (e.g. [3,23,35]), our algorithm does not require a separate run of several clas-sifiers for each query, and does not employ computationally demanding clustering algorithms, such as [4,22]. It exhibits orders of magnitude speedup in empirical experiments, con-firming its asymptotic complexity, while achieving superior accuracy.

The structure of the paper is as follows: in section 2 we review related work, and in section 3 we briefly introduce the problem of active learning. In section 4 we describe our approach and give a detailed description of our algorithm components and their running time. Section 5 consists of running time and accuracy experiments with benchmark and real data. We conclude in section 6.
The literature on semi-supervised learning is extensive (see [8] for an overview). Semi-supervised learning algo-rithms use unlabeled data during training to improve learn-ing performance. In the transductive setting the unlabeled set is transduced with label information from other points. Graph-based algorithms have been developed to accurately transduce labels from a few training points to the unlabeled set of points by relying on graph structure (e.g. [5,8,11,19, 32 X 34]). A basic assumption behind these methods is that class structure is smooth over the graph, or in other words, that classes separate well with respect to the data density patterns. This assumption allows to construct Markov pro-cesses whose transition probabilities rely on proximity in the data feature space (e.g. [8,11,27,34]) in order to propa-gate label information. Alternatively, others use the smooth eigenvectors of the graph Laplacian (similar to the eigen-vectors of the Markov matrix) as a basis to extend the la-beling function from training samples to the unlabeled set (e.g. [26], [5]).

Much of the research in active learning focuses on the tradeoff between refinement and exploration (e.g. [3,6,23]), which is central in motivating our approach. As an exam-ple for algorithms that conduct merely refinement, we con-sider  X  X IMPLE X  [28].  X  X IMPLE X  queries points closest to the center of the largest hypersphere that fits in the ver-sion space. Yet,  X  X IMPLE X  assumes a fairly symmetric ver-sion space and centrality of the hypersphere, which in many cases is not true. The geometry of the version space may be complex and requires exploration first, as naturally in-corporated in our approach. Moreover,  X  X IMPLE X  has a high running time, which scales between O ( N 2 )  X  O ( N as the number of support vectors is multiplied by the time to compute the kernel for each query. Other algorithms such as  X  X OMB X  [3] incorporate exploration to overcome the shortcomings of  X  X efiner X -type algorithms.  X  X OMB X  employs three active learners: the inductive  X  X IMPLE X  [28], a risk minimization active learner [24], and KFF (Kernel Farthest First) [3] which is an  X  X xplorer X -type active learner. At each stage of  X  X OMB X  a learner is chosen based on the reward util-ity assigned by each learner. While this approach, in fact, introduces a tradeoff between three different querying ap-proaches, its implementation and choice of algorithm selec-tion parameter are complex. Its computational complexity is also far from being negligible, as it uses  X  X IMPLE X . A second example is the exploration method suggested in [23]. This algorithm combines  X  X IMPLE X  with KFF. The authors use an adaptable probabilistic parameter to choose the learner. However, the implementation is still demanding in terms of running time. As a third example we consider the algorithm +EXPLORE of [4]. The authors use spectral clustering [21] to expose uncovered clusters that the learner missed while querying samples. Otherwise they use uncertainty sam-pling [20]. +EXPLORE is augmented to graph-based active-transductive algorithms such as [5,19,32,35] and introduces a remarkable improvement to these algorithms. Yet, +EX-PLORE involves a significant computational effort in order to discover data clusters at each learning step. +EXPLORE is very close to our approach as its graph construction also relies on the currently available hypothesis to change the graph similarities. Even so, it also deviates from our ap-proach in two fundamental ways: first, it uses the compu-tationally consuming spectral clustering [21] (which has to be used after each query), whereas we use a iterative linear time diffusion kernel approach. And second, +EXPLORE uses a  X  X ard X  decision criterion to connect same label nodes by an edge, while we use a  X  X oft X  local one.

The algorithms [34] and [32] are two iterative graph trans-ductive learning algorithms that inspired active learning ex-tensions such as [34], [29], and [17]. These approaches rely on the limiting smoothness assumption, whereas we introduce a new label-adapted iteration to overcome this limitation. We note [26] and [11] that use variations of function-adapted kernels in the context of passive learning. However, these ideas were not used before or extended into the active learn-ing setting.

In a recent work by [31], which focuses on active learn-ing for unsupervised spectral clustering, the authors use the eigenvalue decomposition of the data graph Laplacian to probe query points that preserve matrix structure. Our sup-porting theory also relies on the spectral properties of the data graph. Yet, for our carefully designed iteration ma-trix the query component uses a linear combination of the dominant first and second eigenfunctions which are shown to minimize a combined cost function that captures simulta-neously the labeling-function graph-cut and the data graph-cut. Finally, we consider recent work on batch-mode learn-ing (e.g. [9,18,30]). Batch-mode active learning reduces the number of classifier calls per query, yet, it involves optimiza-tion procedures that are computationally demanding (poly-nomial or exponential in number of points). The tradeoff in running time should be carefully investigated with big data sets. For small data sets such as UCI X  X  SEGMENTATION (as reported in [9]) the accuracy of our linear running time algorithm is favorable (see Sec. 5). Also, [9] reports run-ning time results on data sets of a few hundred points and the reported ones (e.g. WAVEFORM) have higher running time than those reported for our algorithm in single-query mode. [18,30] do not report running times. Our experiments on big data sets include a simplistic uncertainty-based batch selection approach, and we mark more advanced batch-mode learning as future work. We address the problem of actively learning a classifier. Without loss of generality, consider the binary case with a R D and their binary labels h  X  X  +1 ,  X  1 } . In transductive learning the learner receives X l + u = { x i } l + u i =1 training set S l = { x i ,y i } l i =1 . The learner generates a soft prediction  X   X  =( X   X  1 , ..,  X   X  l + u ), where  X   X  i  X  [+1 , puts  X  h = sign ( X   X  i ). In our active-transductive setting the l training samples are actively chosen from X u by the learner with the goal to minimize some loss-function on the remain-ing unlabeled points in X u . We also consider in our experi-ments an active learning problem in which X u = { X u 1 ,X such that the learner is allowed to query points only from X u 1 , and predicts for X u 2 .
We describe below our approach, supporting theory, and the detailed implementation of our algorithm, including its extension to the multi-class problem and its running-time complexity analysis.
We start by defining a finite weighted graph G =( V,E ) consisting of a set of vertices V ,asetof edges E  X  V  X  V , and a nonnegative weighting function W : E  X  R + . We interpret the weight W ( v,  X  v ) as a measure of similarity between the vertices v and  X  v ,when( v,  X  v )  X  E . The graph kernel is defined by where D is diagonal with D ii = d i = j W ( v i ,v j ). Thus Kf is a weighted averaging operator of some arbitrary func-tion f to be studied: f i =( d i )  X  1 ij w ij f j , with the weights measured by the similarities W ij ; it has the  X  X veraging effect X  of smoothing the function f over the graph.

In the data context, a graph G can be constructed in which the vertices of G correspond to the data points in X . W represents similarity between data points: where  X  1 ( i, j ) is a local scaling parameter, and typically m 1 ( a )= exp (  X  a )with  X  as the Euclidean distance. The actual similarities used are the local ones in order to pre-serve local geometry and reduce computation time by using sparse matrices. These local similarities are realized by com-puting the k -nearest neighbors for each point x , denoted by N k ( x ). Throughout our experiments we select  X  1 ( i, j )tobe the median distance among the k nearest neighbors of x i and x j : To facilitate notation  X  1 ( i, j ) will be typically denoted by  X  . K ( x i ,x j ) follows immediately from (1):
At the center of our algorithmic ideas lies a Markov pro-cess that is used to propagate labels from S l to X u .We start with the row-stochastic kernel defined by (1), which is viewed as the transition probabilities of a Markov ran-dom walk on the data neighborhood graph. The weights for each row i correspond exactly to the transition probabilities of a random walk starting at x i . Specifically, the one step transition probability between states x i and x k is given by wherewedenote W ( x i ,x j )by W ij to facilitate notation. We consider a random walk as means to assign a label to x i  X  X u . The predicted label of x i is associated with the probability of arriving to a labeled point of class 1 after performing a random walk starting at x i [34] (we consider the binary case w.l.g.). Marking this probability as p ( y 1 | i ), we consider the relation and associate p ( y end =1 | i ) with the probability p ( y For labeled points p ( y end =1 | i ) = 1, or 0 otherwise. De-noting 2 p ( y end =1 | i )  X  1by  X  we see that  X   X  [  X  1 , 1] and its sign can be used to generate  X  h .  X  can be partitioned as  X  =[  X  l , X  u ]. Similarly, D and W are partitioned into blocks Eq. (6) can be transformed and re-written for the unlabeled points X u and  X  in matrix form as resulting in the system for the unlabeled samples, where L = D  X  W is the graph Laplacian, and the sign of each  X  i gives the label of x i A similar system, motivated by q uadratic energy minimiza-tion, is obtained by minimizing while forcing equality on the labeled set  X  l = Y l [8]. Specif-ically, minimizing (9) with respect to  X  u leads to which is the same as (8), since L ul =  X  W ul .

The system (10) can be solved via the well-known Jacobi method [8,25]. The iterative Jacobi method solves the sys-tem Mx = b by approximating the solution at the step ( t +1) by The Jacobi iteration matrix is defined to be B J = D  X  1 ( R + L ), where D is the diagonal matrix with M ii on its i -th diagonal element, and R and L are the upper and lower triangular matrices of M . Hence, in matrix notation the iterative scheme is x For faster convergence one may consider the damped iter-ation involving a parameter  X   X  [0 , 1], where the iteration matrix is defined as B  X  =(1  X   X  ) I +  X B J ,sothat
For the system (10) we have M = L uu , x =  X  u ,and b = L ul Y l , which then yields the iteration It is clear that (14) is a label diffusion process: transduc-ing a label to x i as a weighted average of the labels of its neighbors. In a slightly different view, it is equivalent to the iterative application of the kernel (1) on the characteristic vector  X  , while restarting  X  l to Y l after each iteration. Its probabilistic justification is in the random walk framework (6), which leads to classification. The summary of the label diffusion algorithm DiffuseLabels isgiveninFig.1.
We note that this formulation is related to various label propagation algorithms suggested in [11, 32, 33] except for the normalization steps there. Also, when t  X  X  X  Diffuse-Labels is related to the harmonic classifier of [34]. Yet, the equivalence we have just drawn between our label propa-gation and the Jacobi iteration plays an important role in deriving convergence and solution properties for the follow-ing iterative process that we propose for classification and active learning. We prove bellow that our label diffusion process converges, and provide a multi-class extension to our approach.

Corollary 1. The iteration in DiffuseLabels (14) converges as t  X  X  X  .
 Proof. The proof relies on the convergence proof of the Jacobi method [25]: if M (11) is strictly diagonally dom-inant then the iteration converges. Since for L uu we have L j L uu ( x i ,x j ) , therefore M = L uu is strictly diagonally dominant. This implies that the row sums { s 1 , ..., s n iteration matrix B J are smaller than 1. Since L uu  X  = max { s 1 , ..., s n } &lt; 1 , we obtain that the spectral radius of B J is bounded by 1: max i |  X  i | X  L uu  X  &lt; 1 ,therefore B is a convergent matrix and the Jacobi iteration (14) con-verges.
 The multi-class case. In the multi-class setting we con-sider C = { c 1 , ..., c m } classes, and  X  = {  X  ( c 1 ) , ...,  X  ( c characteristic assignment vectors instead of a single one as in the binary case. Each  X  ( c j ) , 1  X  j  X  m, is a  X  X ne vs. all X  vector:  X  i ( c j )=1if x i  X  X u belongs to c j ,or  X  i ( c if x i  X  X u belongs to c k ,k = j ,otherwise  X  i ( c j )=0. The label diffusion is applied to each vector and the classification is computed for a data point x i as arg max j {  X   X  i ( c
The diffusion of labels ( DiffuseLabels ) depends only on in-formation from data feature space encoded into the weights W ij . In particular, the minimization of the quadratic energy (9) relies on the assumption that data points that belong to different classes will have a low similarity weight. It is this assumption on the smoothness and separability of the class label function which guarantees that the diffusion process will capture the class structure. This smoothness assump-tion, which is used in semi-supervised classification tasks, is often both local and global, i.e. h does not change much between nearest neighbors and on clusters of data points. However, in many real data sets this might not be the situ-ation, as h may not be smooth with respect to W .Tothis end, we propose to modify the geometry captured by W so that the geometry of  X   X  -asmoothedversionof  X  that cap-tures the current labeling hypothesis, is taken into account. To achieve this goal we consider the following label-adapted weight matrix in a second diffusion process:
Definition 1. The label-adapted weight matrix is de-fined as and its associated kernel is K  X   X  . m 1 and m 2 are de-caying exponentials,  X  1 and  X  2 are distance metrics in R D and R k (typically k =1 ),  X  1 is a data scaling parameter that is chosen as in (3),  X  2 is a scaling parameter in the label feature space (its selection is discussed below), and  X   X  is a smoothed soft label estimate for h obtained via DiffuseLabels with the kernel K .
 The construction in (15) assigns stronger affinity between points that are believed to belong to the same class, and weaker affinities between points of different classes. In par-ticular, when  X  2 &lt;&lt;  X  1 , the associated averaging kernel K will average locally, but much more along the (estimated) contours of h than across them. Thus the selection of  X  2 depends on the importance of the estimation of h by  X   X  .A median approach (3) can be used, which captures the level of uncertainty in  X   X  .Or  X  2 = 1, which captures the average distance between the two labels { -1,1 } .

We therefore consider a second label diffusion process in-volving the kernel (15), which follows a diffusion process with the kernel (4). This second diffusion process yields a new labeling estimation denoted by  X   X  , as described in the pseudo-code LAClassifier in Fig. 2. The label diffusion process involving K  X   X  will manifest fast label diffusion along the (suspected) same-class samples and slow diffusion along class boundaries. This property is advantageous to acceler-ate and improve classification, in particular when classes do not separate over the data, or in other words, the smooth-ness assumption does not hold. In this case our resulting iterant  X   X  will provide an approximation to h and not only to the data density distribution. In order to establish these claims, we first introduce Theorem 1 which shows that the Jacobi iteration with the label-adapted kernel (15) converges to an approximation of the class decision boundary indepen-dently of the smoothness assumption. Second, we show in Theorem 2 that in the modified graph geometry the energy function (9) is further minimized by  X   X  .
 Theorem 1. Let  X  x be the resulting iterant from the Jacobi iteration with the kernel (15) in LAClassifier. Then  X  x approximates a minimal graph-cut in a graph constructed from the unknown true binary label func-tion h and the data. In particular,  X  x approximates h . Proof. See appendix.

Theorem 2. Let  X   X  and  X   X  be the corresponding min-ima of the quadratic energy function (9) with W and W  X   X  . Then the label-adapted weight matrix and  X   X  fur-ther minimize (9). In particular, Proof. See appendix.
 Theorem 1 and its proof reveal the structure of  X   X  :itisdomi-nated by the first eigenvectors of the kernel W  X   X  .Thecombi-nation of the 1st and 2nd eigenvectors provides a real smooth approximation of a graph-cut of the points of the binary set h . As such it is simply an approximation of h . Most impor-tant, we show below how  X   X  is used within active learning: its transition from negative to positive values indicates the class decision boundary, and precise zero values indicate un-explored data portions. Another strength of Theorem 1 is in its independence from the smoothness assumption and class separation in data space: Popular data-constructed kernels (4) as, for example, in [5,11,32 X 34] result in a good estimate for h only if the smoothness assumption holds.

Theorem 2 establishes that the new solution  X   X  is smoother in the modified geometry as it minimizes the classification error (9) with W  X   X  .Using W  X   X  in the quadratic energy func-tion (9) is more meaningful than using W ,becauseitcon-tains a penalty that does not depend only on the data but also depends on the labels approximation  X   X  . Connecting this with Theorem 1 and its proof, the resulting iterant  X   X  that minimizes the classification error contains the smooth components which better approximate h .Asaresult,the query component which relies on the absolute values of  X   X  attains better accuracy, as shown in the following section.
To this end the label-adapted diffusion suggests two key advantages: First, it computes the soft label approximation  X   X  in linear running time, while other methods, relying on black-box clustering algorithms, run with higher complexity (e.g. [4, 22, 23]) (see Sec. 4.5). Second, it suggests a clas-sifier that is highly accurate and can be applied to classifi-cation problems where there is no class separation over the Figure 2: Label propagation with label-adapted kernels data density. Our supporting theory links  X   X  directly with h , which is key in constructing an efficient active learning al-gorithm, referred to as Label Adapted Active Learning (LAAL) . Theorem 1 and its proof establish that the output of LA-Classifier - X   X  , is an approximation to the true binary la-beling function h . As such, low  X   X  absolute values indicate either a class decision boundary or a yet unexplored region where a decision boundary could exist. With this observa-tion we choose to adapt an uncertainty sampling criterion to perform active learning in our transductive setting. In gen-eral, representative points whose approximated soft labels {  X   X  queried. Such points indicate that the labeling function has not been sampled sufficiently or that a decision boundary exists. We distinguish between the two cases: 1. Exploration : Our fundamental observation is that 2. Refinement: Our second observation is that once ex-Our query strategy is justified by the following probabilistic argument in Lemma 1: Figure 7: Running time (log-scale) vs. data size. Left to right: XOR and UCI data sets: WAVEFORM, MAGIC, MUSH, COVERTYPE, and SUSY.
We presented an efficient label diffusion method for active-transductive classification. LAAL demonstrates improved classification results over state-of-the-art active learning al-gorithms, and a significant reduction of running time. Our approach relies on the combination of classical geometric kernels with label-adapted kernels and on uncertainty sam-pling for query selection. These properties allow LAAL to efficiently overcome certain challenging situations in active learning such as non-smoothness of the class labeling func-tion over the data distribution. Future work aims at accel-erating the iteration process via low rank matrix approxi-mation, an adaptive selection of the  X  parameters to adapt to the distance from the nearest class decision boundary, imbalanced class distribution, and batch-mode learning.
We thank Dr. Ron Begleiter and Prof. Ran El-Yaniv for thecodeof X +EXPLORE X ,andKunDengforhiscontri-bution of results from [23]. We also thank our anonymous reviewers for their useful comments. [1] A. Andoni, M. Datar, N. Immorlica, and V. Mirrokni. [2] K. Bache and M. Lichman. UCI machine learning [3] Y. Baram, R. El-Yaniv, and K. Luz. Online choice of [4] R. Begleiter, R. El-Yaniv, and D. Pechyony. Repairing [5] M. Belkin, I. Matveeva, and P. Niyogi. Regularization [6] A. Bondu, V. Lemaire, and B. M. Exploration vs. [7] C. Campbell, N. Cristianini, and A. J. Smola. Query [8] O. Chapelle, B. Schlkopf, and A. Zien. Semi-Supervised [9] R. Chattopadhyay, Z. Wang, W. Fan, I. Davidson, [10] F. R. K. Chung. Spectral Graph Theory .CBMS [11] R. R. Coifman, S. Lafon, A. B. Lee, M. Maggioni, [12] I. Dagan and S. P. Engelson. Committee-based [13] G. Dahlquist and A. Bj  X  orck. Numerical Methods . [14] Churn prediction tournament , 2003. [15] E. J. Friedman. Active learning for smooth problems. [16] J. H. Friedman. Stochastic gradient boosting. Comput. [17] Q. Gu, C. Aggarwal, J. Liu, and J. Han. Selective [18] Y.Hu,D.Zhang,Z.Jin,D.Cai,andX.He.Active [19] T. Joachims. Transductive learning via spectral graph [20] D. D. Lewis and W. A. Gale. A sequential algorithm [21] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral [22] H. T. Nguyen and A. Smeulders. Active learning using [23] T. Osugi, D. Kun, and S. Scott. Balancing exploration [24] N. Roy and A. McCallum. Toward optimal active [25] Y. Saad. Iterative Methods for Sparse Linear Systems . [26] A. D. Szlam, M. Maggioni, and R. R. Coifman. [27] M. Szummer and T. Jaakkola. Partially labeled [28] S. Tong and D. Koller. Support vector machine active [29] X. Wang, R. Garnett, and J. Schneider. Active search [30] Z. Wang and J. Ye. Querying discriminative and [31] F. L. Wauthier, N. Jojic, and M. I. Jordan. Active [32] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and [33] X. Zhu and Z. Ghahramani. Learning from labeled [34] X. Zhu, Z. Ghahramani, and J. Lafferty.
 [35] X. Zhu, J. Lafferty, and Z. Ghahramani. Combining Proof of Theorem 1
Proof. We start with showing that a Jacobi iteration using an iteration matrix that is built from the true bi-class labels (using m 2 in (15)) results in an approximation to the class decision function: Definition 2. The class weight matrix is defined as W B J are the corresponding kernel and iteration matrix. Finally, the class graph Laplacian is defined as L h = D L h (and K h ) has similar eigenvectors to those of B h J .Let v , ..., v n and  X  1 , ...,  X  n be the set of eigenvectors of B dered in the decreasing order of its eigenvalues. We have that a general solution is given by where c 1 , ..., c n are coefficients that are prescribed by the initial condition  X  (0) :  X  (0) = c 1 v 1 + ... + c n v n enough t , and since  X  i |  X  i | &lt; 1, we have that the dominant components in  X  ( t +1) are the highest eigenvectors v 1 and v v 1 has a constant sign, and the structure of h :itispiece-wise constant on the indices belonging to the same class in h . A second component that may be dominant (for suffi-ciently small t and since c 2 is large because of negative and positive training samples), is v 2 . v 2 obeys v 2  X  v 1 , and thus approximates a minimal cut in h [10]: Therefore v 2 is an approximation for the binary function h , and its transition from negative to positive entries is cor-responds to the decision boundary.  X  2 corresponds to how well h can be partitioned (the algebraic connectivity of the graph) [10], in particular, if h has a clear partition then  X  is close to  X  1 . The combination of v 1 and v 2 in (16) forms an approximation to h ,since h = h itself provides the min-imum of (17). Clearly, h is partially known and we use instead  X   X  to build up a kernel and the Jacobi iteration pro-cess. Since  X   X  is approximates h from training samples and the propagated labels, the same spectral analysis applies for an iteration using  X   X  instead of h .

Now, consider the label-adapted kernel (15) in its matrix representation where W  X   X  corresponds to m 2 in (15). The associated energy function is then In this case the solution obtained by iterating with B  X   X  have components corresponding to the eigenvectors of K  X   X  a constant sign component v  X   X  1 , and a second component v 2 that reflects a cut in a fused graph constructed from a weight-by-weight multiplication of W  X   X  ij with W ij -theweight of the graph constructed from X . In particular, it will cap-ture a cut that minimizes (19). The cut reflects the data density and geometry as in (4), and, as shown above for W the currently approximated hypothesis h  X   X   X  .
 Proof of Theorem 2
Proof. Note that i does not include labeled points indices because those are always restarted in our algorithm to Y l We need to show that We distinguish between two cases 1. |  X   X  i  X   X   X  j | X | and 2. |  X   X  i  X   X   X  j | X |  X   X  i  X   X   X  j | .
