 In spite of the wide use of clustering in many practical applications, currently, there exists no princi-pled method to guide the selection of a clustering algorithm. Of course, users are aware of the costs involved in employing different clustering algorithms (software purchasing costs, running times, memory requirements, needs for data preprocessing etc.) but there is very little understanding of the differences in the outcomes that these algorithms may produce. We focus on that aspect -the input-output properties of different clustering algorithms.
 The choice of an appropriate clustering should, of course, be task dependent. A clustering that works well for one task may be unsuitable for another. Even more than for supervised learning, for clustering, the choice of an algorithm must incorporate domain knowledge. While some domain knowledge is embedded in the choice of similarity between domain elements (or the embedding of these elements into some Euclidean space), there is still a large variance in the behavior of difference clustering paradigms over a fixed similarity measure. For some clustering tasks, there is a natural clustering objective function that one may wish to op-timize (like k -means for vector quantization coding tasks), but very often the task does not readily translate into a corresponding objective function. Often users are merely searching for a meaningful clustering, without a prior preference for any specific objective function. Many (if not most) com-mon clustering paradigms do not optimize any clearly defined objective utility, either because no such objective is defined (like in the case of, say, single linkage clustering) or because optimizing the most relevant objective is computationally infeasible. To overcome computation infeasibility, the algorithms end up carrying out a heuristic whose outcome may be quite different than the actual objective-based optimum (that is the case with the k-means algorithm as well as with spectral clus-tering algorithms). What seems to be missing is a clear understanding of the differences in clustering outputs in terms of intuitive and usable properties.
 We propose a different approach to providing guidance to clustering users by identifying signif-icant properties of clustering functions that, on one hand distinguish between different clustering paradigms, and on the other hand are intended to be relevant to the domain knowledge that a user might have access to. Based on domain expertise users could then choose which properties they want an algorithm to satisfy, and determine which algorithms meet their requirements. Our vision is that ultimately, there would be a sufficiently rich set of properties that would provide a detailed, property-based, taxonomy of clustering methods, that could, in turn, be used as guidelines for a wide variety of clustering applications. This is a very ambitious enterprize, but that should not deter researchers from addressing it. This paper takes a step towards that goal by using natural properties to examine some popular clustering approaches.
 We present a taxonomy for common deterministic clustering functions with respect to the proper-ties that we propose. We also show how to extend this framework to the randomized clustering algorithms, and use these properties to distinguish between two k -means heuristics. We also study relationships between the properties, independent of any particular algorithm. In par-ticular, we strengthen Kleinberg X  X  impossibility result[8] using a relaxation of one of the properties that he proposed. 1.1 Previous work Our work follows a theoretical study of clustering that began with Kleinberg X  X  impossibility result [8], in which he proposes three candidate axioms of clustering and shows that no clustering function can simultaneously satisfy these three axioms. Ackerman and Ben-David [1] subsequently showed these axioms to be consistent in the setting of clustering quality measures. [1] also proposes to make a distinction between clustering  X  X xioms X  and clustering  X  X roperties X , where the axioms are the features that define which partitionings are worthy of the name  X  X lustering X , and the properties vary between different clustering paradigms and may be used to construct a taxonomy of clustering algorithms. We adopt that approach here.
 There are previous results that provide some property based characterizations of a specific clus-tering algorithm. In 1975, Jardine and Sibson [6] gave a characterization of single linkage. Last year, Bosagh Zadeh and Ben-David [3] characterize single-linkage within Kleinberg X  X  framework of clustering functions using a special invariance property ( X  X ath distance coherence X ). Very re-cently, Ackerman, Ben-David and Loker provided a characterization of the family of linkage-based clustering in terms of a few natural properties [2].
 Some heuristics have been proposed as a means of distinguishing between the output of clustering algorithms on specific data. These approaches require running the algorithms, and then selecting an algorithm based on the outputs that they produce. In particular, validity criteria can be used to evaluate the output of clustering algorithms. These measures can be used to select a clustering algorithm by choosing the one that yields the highest quality clustering [10]. However, the result only applies to the original data, and there are no guarantees on the quality of the output of these algorithms on any other data. Clustering is wide and heterogenous domain. For most of this paper, we focus on a basic sub-domain where the (only) input to the clustering function is a finite set of points endowed with a between-points distance (or similarity) function, and the output is a partition of that domain. A distance function is a symmetric function d : X  X  X  X  R + , such that d ( x,x ) = 0 for all x  X  X . The data sets that we consider are pairs ( X,d ) , where X is some finite domain set and d is a distance function over X . These are the inputs for clustering functions.
 A k-clustering C = { C 1 ,C 2 ,...,C k } of a data set X is a partition of X into k disjoint subsets of X (so, [ For a clustering C , let | C | denote the number of clusters in C and | C i | denote the number of points in a cluster C i . For x,y  X  X and a clustering C of X , we write x  X  C y if x and y belong to the same cluster in C and x 6 X  C y , otherwise.
 exists a bijection  X  : X  X  X 0 so that d ( x,y ) = d 0 (  X  ( x ) , X  ( y )) for all x,y  X  X . We say that two clusterings (or partitions) C = ( c 1 ,...c k ) of some domain ( X,d ) and C 0 = ( c 1 ,...c exists a bijection  X  : X  X  X 0 such that for all x,y  X  X , d ( x,y ) = d 0 (  X  ( x ) , X  ( y )) and, on top of that, x  X  C y if and only if  X  ( x )  X  C 0  X  ( y ) . Note that this notion depends on both the underlying distance functions and the clusterings.
 We consider two definitions of a clustering function.
 Definition 1 (General clustering function) . A general clustering function is a function that takes as input a pair ( X,d ) and outputs a clustering of the domain X .
 The second type are clustering functions that require that the number of clusters be provided as part of the input.
 Definition 2 (k-clustering function) . A k-clustering function is a function that takes as input a pair ( X,d ) and a parameter 1  X  k  X | X | and outputs a k-clustering of the domain X . 2.1 Properties of Clustering Functions A key component in our approach are properties of clustering functions that address the input-output behavior of these functions. The properties are formulated for k-clustering functions. However, all the properties, with the exception of locality 1 and refinement-confined, apply also for general clustering functions.
 Isomorphism invariance : The following invariance property, proposed in [2] under the name  X  X ep-resentation independence X , seems to be an essential part of our understanding of what clustering is. It requires that the output of a k-clustering function is independent of the labels of the data points. A k-clustering function F is isomorphism invariant if whenever ( X,d )  X  ( X 0 ,d 0 ) , then, for every k , F ( X,d,k ) and F ( X 0 ,d 0 ,k ) are isomorphic clusterings.
 Scale invariance : Scale invariance, proposed by Kleinberg [8], requires that the output of a clus-tering be invariant to uniform scaling of the data. A k-clustering function F is scale invariant if for any data sets ( X,d ) and ( X,d 0 ) , if there exists a real number c &gt; 0 so that for all x,y  X  X , d ( x,y ) = c  X  d 0 ( x,y ) then for every 1  X  k  X | X | , F ( X,d,k ) = F ( X,d 0 ,k ) . Order invariance : Order invariance, proposed by Jardine and Sibson[6], describes clustering func-tions that are based on the ordering of pairwise distances. A distance function d 0 of X is an order function d 0 over X is an order invariant modification of d , F ( X,d,k ) = F ( X,d 0 ,k ) for all k . Locality : Intuitively, a k-clustering function is local if its behavior on a union of clusters depends only on distances between elements of that union, and is independent of the rest of the domain set. Locality was proposed in [2]. A k-clustering function F is local if for any clustering C output by F and every subset of clusters, C 0  X  C , F ( S C 0 ,d, | C 0 | ) = C 0 .
 In other words, for every domain ( X,d ) and number of clusters, k , if X 0 is the union of k 0 clusters in F ( X,d,k ) for some k 0  X  k , then, applying F to ( X 0 ,d ) and asking for a k 0 -clustering, will yield the same clusters that we started with.
 Consistency : Consistency, proposed by Kleinberg [8], aims to formalize the preference for clusters that are dense and well-separated. This property requires that the output of a k-clustering function should remain unchanged after shrinking within-cluster distances and stretching between-cluster distances.
 Given a clustering C of some domain ( X,d ) , we say that a distance function d 0 over X , is ( C,d ) -A k-clustering function F is consistent if for every X,d,k , if d 0 is ( F ( X,d,k ) ,d ) -consistent then F ( X,d,k ) = F ( X,d 0 ,k ) .
 While this property may sound desirable and natural, it turns out that many common clustering paradigms fail to satisfy it. In a sense, this property may be viewed as the main weakness of Klein-berg X  X  impossibility result.
 The following two properties, proposed in [2], are straightforward relaxations of consistency. Inner and Outer consistency : Outer consistency represents the preference for well separated clus-ters, by requiring that the output of a k-clustering function not change if clusters are moved away from each other.
 A distance function d 0 over X is ( C,d ) -outer consistent if d 0 X ( x,y ) = d X ( x,y ) whenever x  X  C y , and d 0 X ( x,y )  X  d X ( x,y ) whenever x 6 X  C y . Outer consistency is defined in the same way consistency, except that ( C,d ) -consistent is replaced by ( C,d ) -outer consistent . Inner consistency represents the preference for placing points that are close together within the same cluster, by requiring that the output of a k-clustering function not change if elements of the same cluster are moved closer to each other.
 Inner consistency is defined in a similar manner to outer-consistency, except that d 0 is ( C,d ) -inner Clearly, consistency implies both outer-consistency and inner-consistency. Note also that if a func-tion is both inner-consistent and outer-consistent then it is consistent. k-Richness : The k-richness property requires that we be able to obtain any partition of the do-main by modifying the distances between elements. This property is based on Kleinberg X  X  [8] rich-X richness if for any sets X 1 ,X 2 ,...,X k , there exists a distance function d over X 0 = S k i =1 X i so that F ( X 0 ,d,k ) = { X 1 ,X 2 ,...,X k } .
 Outer richness : Outer richness, a natural variation on the k-richness property, was proposed in [2] under the name  X  X xtended richness. X  (we have renamed it to contrast this property with  X  X nner richness X , which we propose in Appendix A). Given k sets, a k-clustering function satisfies outer richness if there exists some way of setting the between-set distances, without modifying distances within the sets, we can get F to output each of these data sets as a cluster. This corresponds to the intuition that any groups of points, regardless of within distances, can be made into separate clusters. F ( S k i =1 X i ,  X  d,k ) = { X 1 ,X 2 ,...,X k } .
 Threshold-richness : Fundamentally, the goal of clustering is to group points that are close to each other, and to separate points that are far apart. Axioms of clustering need to represent these ob-jectives and no set of axioms of clustering can be complete without integrating such requirements. Figure 1: A taxonomy of k-clustering functions, illustrating what properties are satisfied by some common k-clustering functions. The results in the k -means row apply both when the centers are part of the data set and when the underlying space is Euclidean and the centers are arbitrary points in the space.
 Consistency is the only previous property that aims to formalize these requirements. However, con-sistency has some counterintuitive implications (see Section 3 in [1]), and is not satisfied by many common clustering functions.
 A k-clustering function F is threshold-rich if for every clustering C of X , there exist real numbers a &lt; b so that for every distance function d over X where d ( x,y )  X  a for all x  X  C y , and d ( x,y )  X  b for all x 6 X  C y , we have that F ( X,d, | C | ) = C .
 This property is based on Kleinberg X  X  [8]  X  -forcing property, and is equivalent to the requirement that for every partition  X  , there exists a &lt; b so that ( a,b ) is  X  -forcing.
 Inner richness : Complementary to outer richness, inner richness requires that there be a way of setting distances within sets, without modifying distances between the sets, so that F outputs each set as a cluster. This corresponds to the intuition that between-cluster distances cannot eliminate any partition of X . A k-clustering function F satisfies inner richness if for every data set ( X,d ) and partition { X 1 ,X 2 ,...,X k } of X , there exists a  X  d where for all a  X  X i , b  X  X j for i 6 = j ,  X  d ( a,b ) = d ( a,b ) , and F ( S k i =1 X i ,  X  d,k ) = { X 1 ,X 2 ,...,X k } .
 Refinement-confined 2 : The following formalization was proposed in [2]. A clustering C of X is a refinement of clustering C 0 of X if every cluster in C is a subset of some cluster in C 0 , or, equivalently, if every cluster of C 0 is a union of clusters of C . A k-clustering function is refinement confined if for every 1  X  k  X  k 0  X | X | , F ( X,d,k 0 ) is a refinement of F ( X,d,k ) . In this section we present a taxonomy of common k-clustering functions. The taxonomy is pre-sented in Figure 1 (definitions of the k-clustering functions are in Appendix C in the supplementary material).
 The taxonomy in Figure 1 illustrates how clustering algorithms differ from one another. For ex-ample, order-invariance and inner-consistency can be used to distinguish among the three common linkage-based algorithms. Min-sum differs from k-means and k-median in that it satisfies inner-local.
 The proofs of the claims embedded in the table appear in the supplementary material. 3.1 Axioms of clustering Our taxonomy reveals that some intuitive properties, which may be expected of all k-clustering functions, are not satisfied by some common k-clustering functions. For example, locality is not satisfied by the spectral clustering functions ratio-cut and normalized-cut. Also, most functions fail inner consistency, and therefore do not satisfy consistency, even though the latter was previously proposed as an axiom of k-clustering functions [8].
 On the other hand, isomorphism invariance, scale invariance, and all richness properties (in the set-ting where the number of clusters, k , is part of the input), are satisfied by all the clustering functions considered. Isomorphism invariance and scale-invariance make for natural axioms. Threshold rich-ness is the only one that is both satisfied by all k-clustering functions considered and reflects the main objective of clustering: to group points that are close together and to separate points that are far apart.
 It is easy to see that threshold richness implies k-richness. It can be shown that when threshold rich-ness is combined with scale invariance, it also implies outer-richness and inner-richness. Therefore, we propose that scale-invariance, isomorphism-invariance, and threshold richness can be used as clustering axioms.
 However, we emphasize that these three axioms do not make a complete set of axioms for clustering, since some functions that satisfy all three properties do not make reasonable k-clustering functions; a function that satisfies the two invariance properties can also satisfy threshold richness by behaving reasonably only on particularly well-clusterable data, while having counter-intuitive behavior on other data sets. We present a formal setting to study and analyze probabilistic k-clustering functions. A probabilistic k-clustering function F takes a data set ( X,d ) and an integer 1  X  k  X | X | and outputs F ( X,d,k ) , a probability distribution over k-clusterings of X . Let P ( F ( X,d,k ) = C ) denote the probability of clustering C in the probability distribution F ( X,d,k ) . 4.1 Properties of Probabilistic k-Clustering Functions We translate properties of different types into the probabilistic setting.
 Invariance properties : Invariance properties specify when data sets should be clustered in the same way (ex. isomorphism-invariance, scale-invariance, and order-invariance). Such properties are translated into the probabilistic setting by requiring that when data sets ( X,d ) and ( X 0 ,d 0 ) satisfy some similarity requirements, then F ( X,d,k ) = F ( X 0 ,d 0 ,k ) for all k .
 Consistency properties : Consistency properties impose conditions that should improve the quality of a clustering. Every such property has some notion of a  X  ( C,d ) -nice X  variant that specifies how the underlying distance function can be modified to better flesh out clustering C . In the probabilistic setting, such properties require that whenever d 0 is a ( C,d ) -nice variant, the k-clustering function is at least as likely to output C on d 0 as on d , P [ F ( X,d 0 , | C | ) = C ]  X  P [ F ( X,d, | C | ) = C ] . Richness properties : Richness properties require that any desired clustering can be obtained under certain constraints. In the probabilistic setting, we require that the same occurs with arbitrarily high probability. For example, the following is the probabilistic version of the k-richness property. The other variants of richness are reformulated analogously.
 Definition 3 (k-Richness) . A probabilistic k-clustering function F is k-rich if for any k-clustering C of X and any &gt; 0 , there exists a distance function d over X so that P ( F ( X,d,k ) = C )  X  1  X  . Locality : We now show how to translate locality into the probabilistic setting. We say that a clus-tering of X specifies how to cluster a subset X 0  X  X if every cluster that overlaps with X 0 is contained within X 0 . Locality requires that a k-clustering function cluster X 0 in the way specified by the superset X . Figure 2: An analysis of the k -means clustering function and k -means heuristics. The two leftmost properties distinguish the k -means clustering function, properties that are satisfied by k -means but fail for other reasonable k-clustering functions. The next three are proposed axioms of clustering, and the last two properties follow from the axioms.
 In the probabilistic setting, we require that the probability of obtaining a specific clustering of X 0  X  X is determined by the probability of obtaining that clustering as a subset of F ( X,d,k ) , given that the output of F on ( X,d ) specifies how to cluster X 0 .
 Definition 4 (Locality (probabilistic)) . A probabilistic k-clustering function F is local if for any k-clustering C 0 of X 0 , X 0  X  X , and j  X  k , where P [  X  C 1 ,...,C k s.t.  X  k i =1 C i = X 0 | F ( X,d,j ) = C ] 6 = 0 , P [ F ( X 0 ,d/X 0 , | C 0 | ) = C 0 ] = 5.1 k -means and k -means heuristics One of the most popular clustering algorithms is the Lloyd method, which aims to find clusterings with low k -means loss. Indeed, the Lloyd method is sometimes referred to as the  X  k -means algo-rithm. X  We maintain a distinction between the k -means objective function and heuristics, such as the Lloyd method, which aim to find clusterings with low k -means loss. For this section, we assume that the data lie in Euclidean space, as is often the case when the Lloyd method is applied. Definition 5 (Lloyd method) . Given a data set ( X,d ) , and a set S of points in R n , the Lloyd algorithm performs the following steps until two consecutive iterations return the same clustering. The Lloyd method is highly sensitive to the choice of initial centers. Perhaps the most common method for initializing the centers for the Lloyd method is to select k random points from the input data set, proposed by Forgy in 1965 [4]. We refer to this initialization method as Random Centroids. We propose a slight variation on a deterministic initialization method by Katsavounidis, Kuo, and Zhang [7], who propose selecting centers that are far apart. First let c 1 and c 2 be the two points furthest away from each other. Then, for all 2  X  k , let c i be the point furthest away from its closest existing center. That is, let c i be the point in X that maximizes min 1  X  j  X  i  X  1 d ( c j ,c i ) . 5.2 Distinguishing heuristics by properties An analysis of the k -means clustering functions and the two k -means heuristics discussed above is shown in Figure 2. The analysis illustrates that the k -means function differs significantly from heuristics that aim to find clusterings with low k -means objective loss. The proofs for this analysis were omitted due to a lack of space (they appear in the supplementary material).
 There are two properties that are satisfied by the k -means clustering function and fail for other rea-sonable k-clustering functions: outer-consistency and locality. Neither is satisfied by the heuristics. Note that unlike k-clustering functions that optimize common clustering objective functions, heuris-tics that aim to find clusterings with low loss for these objective functions do not necessarily make meaningful k-clustering functions. Therefore, such heuristic X  X  failure to satisfy certain properties does not preclude these properties from being axioms of clustering, but rather illustrates a weakness of the heuristic.
 It is interesting that the Lloyd method with the Furthest Centroids initialization technique satisfies our proposed axioms of clustering while Lloyd with Random Centroid fails threshold richness. This corresponds to the finding of He et. al. [5] that in practice, Furthest Centroids performs better than Randomized Centroids. In this final section, we strengthen Kleinberg X  X  famous impossibility result [8], for general clustering functions, yielding a simpler proof of the original result.
 Kleinberg impossibility theorem (Theorem 2.1, [8]) was that no general clustering function can simultaneously satisfy scale-invariance, richness, and consistency. Ackerman and Ben-David[1] later showed that consistency has some counter intuitive consequence. In Section 1, we showed that many natural clustering functions fail inner-consistency 3 , which implies that there are many general clustering functions that fail consistency.
 On the other hand, many natural algorithms satisfy outer-consistency. We strengthen Kleinberg X  X  impossibility result by relaxing consistency to outer-consistency.
 Theorem 1. No general clustering function can simultaneously satisfy outer-consistency, scale-invariance, and richness.
 Proof. Let F be any general clustering function that satisfies outer-consistency, scale-invariance and richness.
 Let X be some domain set with three or more elements. By richness, there exist distance functions d 1 and d 2 such that F ( X,d 1 ) = { X } (every domain point is a cluster on its own) and F ( X,d 2 ) is some different clustering, C = { C 1 ,...C k } of X .
 Let r = max { d 1 ( x,y ) : x,y  X  X } and let c be such that for every x 6 = y , cd 2 ( x,y )  X  r . Define  X  d ( x,y ) = c  X  d 2 ( x,y ) , for every x,y  X  X . Note that  X  d ( x,y )  X  d 1 ( x,y ) for all x,y  X  X . By is a contradiction since F ( X,d 1 ) and F ( X,d 2 ) are different clusterings.
 A similar result can be obtained, using a similar proof, with inner-consistency replacing outer con-sistency. Namely, Lemma 1. No general clustering function can simultaneously satisfy inner-consistency, scale-invariance, and richness.
 Since consistency implies both outer-consistency and inner-consistency, Kleinberg X  X  original result follows from any one of Theorem 1 or Lemma 1.
 Kleinberg X  X  impossibility result illustrates property trade-offs for general clustering functions. The good news is that these results do not apply when the number of clusters is part of the input, as is illustrated in our taxonomy; single linkage satisfies scale-invariance, consistency and richness. [1] M. Ackerman and S. Ben-David. Measures of Clustering Quality: A Working Set of Axioms [2] M. Ackerman, S. Ben-David, and D. Loker. Characterization of Linkage-based Clustering. [3] R. Bosagh Zadeh and S. Ben-David.  X  X  Uniqueness Theorem for Clustering. X  The 25th Annual [4] E. Forgy. Cluster analysis of multivariate data: efficiency vs. interpretability of classifications. [6] N. Jardine, R. Sibson, Mathematical Taxonomy Wiley, 1971. [7] I. Katsavounidis, C.-C. J. Kuo, and Z. Zhang. A new initialization technique for generalized [8] Jon Kleinberg.  X  X n Impossibility Theorem for Clustering. X  Advances in Neural Information [9] U. von Luxburg. A Tutorial on Spectral Clustering. Statistics and Computing 17(4): 395-416, [10] L. Vendramin, R.J.G.B. Campello, and E.R. Hruschka.  X  X n the comparison of relative cluster-
