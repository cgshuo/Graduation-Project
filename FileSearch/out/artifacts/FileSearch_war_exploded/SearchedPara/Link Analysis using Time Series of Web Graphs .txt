 Link analysis is a ke y technology in contemporary web search engines. Most of the previous work on link analysis only used information from one snapshot of web graph. Since commercial search engines crawl the Web periodically, they will naturally obtain time series data of web gr aphs. The historical information contained in the series of web gr aphs can be used to improve the performance of link analysis. In th is paper, we argue that page importance should be a dynamic quantity, and propose defining page importance as a function of both PageRank of the current web graph and accumulated historical page importance from previous web graphs. Specific ally, a novel algorithm named TemporalRank is designed to compute the proposed page importance. We try to use a kinetic model to interpret this page importance and show that it can be regarded as the solution to an ordinary differential equation. E xperiments on link analysis using web graph data in five snapshots show that the proposed algorithm can outperform PageRank in many measures, and can effectively filter out newly appeared link spam websites.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Performance Search engine, temporal inform ation, page importance, link analysis, PageRank The Web has grown exponentially over the past decade. However, at the same time, the number of low quality websites is also increasing rapidly which brings many problems for users and search engines. To tackle this problem, employing an effective measure of page importance becomes very critical. Much work has been done on the problem of defining and calculating page importance by PageRank [3][11], and the work in [2][6][7][9][10]. PageRank is a good measure of page importance. However, latest study [12] showed that PageRank can be easily cheated by some special techniques. Furthermore, it is not robust to link spam [4][5]. To solve the above problem, we believe that effective measure for page importance should be define d on many snapshots of Web graphs over time. Spam pages usua lly have irregular patterns of link changes which make high-quality and spam pages quite different. There has been some work on adding historical information into link analysis. Berberich et al [1] developed the T-Rank algorithm, a link analysis method that takes into account the temporal aspects: freshness a nd activity of pages and links. Yu et al [13] proposed a time-weighted PageRank, in which the in-links of a page are weighted acc ording to their timestamps. The advantage of the methods is that they extracted and utilized the temporal information, but they did not fundamentally change the framework of PageRank which uses one snapshot of web graph. In this paper, we postulate that the calculation of page importance should not be a static process. Instead, one should define a dynamic process that comput es page importance from two perspectives: the importance from the current web graph and the accumulated historical importance from previous web graphs. Therefore, conventional Page Rank is embodied in the new importance score. We then develop an efficient algorithm named TemporalRank to compute the ne w page importance, and show that it can be interpreted using a kinetic model and solved using a differential equation. Experiment s on a set of real-world web graphs show that this algorit hm can be more effective in measuring page importance than traditional methods, and can detect and filter out link spam. In this section, we propose a framework to calculate page importance based on a series of web graphs, and use a kinetic model to explain the relationship between th e so-calculated page importance and PageRank in the physical viewpoint. Based on the discussions in Section 1, we propose evaluating the page importance using both the cu rrent web graph and historical information contained in previous web graphs. We call the so-calculated page importance by TemporalRank. Denoting the TemporalRank of page i in the t -th web graph by TR t we have the following formula. Here PR k ( i ) is the PageRank of page i calculated from the current web graph G k , corresponding to the current page importance; H is the accumulated historical importance information obtained from the previous web graphs G 1 , G 2 , ... , G weighting parameter to balance how much we trust in the present and how much we trust in the history. The bigger  X  is, the more trust we have in the link information in the current snapshot. combine the PageRank scores of page i in the previous web graphs as follows. Here  X  t , t = 1,..., k -1 are decaying factors indicating how much the importance from each snapshot contributes to the overall page importance. In common, the earlier a snapshot is, the less amount of importance it s hould contribute to H k ( i ). Therefore,  X  a group of decreasing factors from t = k -1 to t = 1. Combining formulas (1) and (2), we have, From formula (2), we can see that TemporalRank can be calculated by the linear combination of the PageRank scores from a series of web graphs. How to define proper weighting factors  X  problem in (3). We will elaborate on this in the next sub-section. TemporalRank proposed in the previous sub-section, and then model for the linear combination. We suppose the page importance corresponds to the velocity of an PageRank score PR t ( i ) in the current snapshot G driving force will be added to the virtual force F That is, the current PageRank score is a positive effect for the virtual force. At the same time, the decay of the TemporalRank for this virtual force. In summary, we can use the following equation to model the above kinetic system. between the driving force and the PR t ( i ), and (0) decaying factor. Actually equation (4) is common in many kinetic systems. For example, suppose there is a car running in the highway with the velocity v , then the resisting force R it suffers is, motion equation can be written as, Denoting the mass of the car as m , according to Newton X  X  second law, we have Based on the aforementioned analogy, we can get the relationship between TemporalRank and the virtual force as below. Here m ( i ) is some intrinsic quality of web page i with similar meaning to that of mass. Combining the equations in (4) and (8), we get a first-order ordinary differential equation (ODE), This ODE can be easily solved and its general solution is, where C 0 is the integral constant. Suppose that all the pages have the same initial page importance score at the beginning ( t = 0). That is, 0 1 () TR i number of web pages in the graph. Then the solution (10) turns to be, we reduce the above solution to its discrete form as the following. Till now, we get a formula (12) to calculate the TemporalRank combination of the PageRank scores of page i in a series of web graphs. If we write the solution in another form as below, we will see that the first item on th e right-hand side of formula (13) importance; the second item is the linear combination of the PageRank scores of page i in previous web graphs, corresponding to H ( i ); and the third item contains the PageRank of page i in the current web graph. This is consiste nt with what we have discussed in Section 2.1. Comparing (13) with (3), and ignoring the constant, we can get the combination coefficients as follows. From the above derivations, we can see that the kinetic model can well interpret the novel framework for TemporalRank proposed in Section 2.1, and help us ma p the weighting coefficients  X  some more meaningful parameters  X  ,  X  , and m ( i ) . According to the discussions in the previous subsections, we summarize the proposed TemporalRank algorithm in Table 1. Input: k successive web graphs G 1 , G 2 , ... , G k Output: TemporalRank score vector TR k for the k -th web graph 1) Get the transition matrices A 1 , A 2 , ... , A k from web graphs G 2) Set the self-decay constant  X  , the enhancement constant  X  , and The dataset used in our experiments is a sub-graph sampled from a commercial search engine which contains five snapshots ( G , G 5 ) crawled in the first half of year 2006, and each snapshot consists of around 30 million we b pages and over 2000 million hyperlinks. Table 2 shows the basic information. The changes of the number of web pages between each two successive snapshots are summarized in Table 3. Table 3. The changes of the number of web pages between each To evaluate the performance of the TemporalRank algorithm, we use the following three criteria: toolbar correlation (TC), click-through correlation (CC), and spam bucket distribution. A part of users agreed to use the toolbar software offered by the commercial search engine which will log the URLs of the pages when these users surfed on the Web. There are 16,737,629 pages in the sampled sub-graphs that could be found in the toolbar log data, and we counted the click numbers of these pages as the ground truth for the evaluation. Consider the following two vectors: the ranking vector x with 16,737,629 elements outputted by the link analysis algorithm, and the toolbar click vector y which elements record the toolbar click numbers of the pages in the corresponding positions of between vector x and vector y , where M is the length of the vectors x and y . The search engine also logged the click-through data indicating which webpage was clicked after users submitting their queries to this search engine. There were 7,067,915 pages in the sub-graphs that could be found in the click-through data. Once again, their click numbers were used as the ground truth for evaluation. We use the same method with toolbar correl ation to get the click-through correlation. In common, a page will get more clic ks if it is more important. Also for link analysis methods, the higher the ranking score of a page is, the more important this page is regarded by the link analysis method. Therefore, TC and CC can be good indicators for the performance of link analysis algorithms. We also use spam bucket distributi on in our experiments to validate this. With PageRank, a list of pages in the decent order according to their scores can then be generated. We divide the list into a group of buckets, in each of which there are a certain number of web pages. Given a set of labeled link spam pa ges, we can count the number of spam pages that fall in each bucket. Here we asked five well-trained experts to label a subset of th e web graph and got 11,357 link spam pages. If a link analysis method has fewer spam pages in the top buckets than other methods, it will be regarded as a better approach. To understand how these factors in our algorithm will affect the performance, we generate the followi ng datasets as shown in Table 4 for our experiments. Table 4. The datasets containing different numbers of graphs We ran the TemporalRank algorithm with 1 m = on the five 10), and plot the corresponding performance on TC and CC in Figure 1 and Figure 2. To be noted, since S5 only contains one single snapshot of Web graph and m = 1, the performance on this dataset is exactly the same with the PageRank algorithm, which is independent of the decaying constant. From these figures, we can see that the performances of TemporalRank on datasets S1 to S4 are all better than that on S5 (corresponding to PageRank), no matter what value  X  takes. This shows the superior of our proposed method to PageRank. As mentioned above, we then check the ranking results produced by the TemporalRank method, to see how those spam pages are ranked. Specifically, we plot the spam bucket distribution in Figure 3, where each bucket contains one million web pages (only the top 20 buckets are shown) to investigate how many of the 1,502 spam dataset actually corresponds to the performance of PageRank since there is only one single web graph in this dataset. From this figure, we can see that the TemporalRank algorithm can depress the newly generated spam pages in a very effective manner as compare with PageRank. Furthermore, the more web graphs we use, the better performance we can achieve. We propose a novel framework for link analysis on a series of web graphs, in which both the link structure in the current web graph and the historical importanc e information are considered as incentives for calculating page importance. We introduced a kinetic model to interpret this framework and develop an efficient algorithm within this framework. Experiments show that by leveraging the historical informat ion, our proposed method can be much more effective than previous methods in ranking pages and removing link spam. This work was performed when the first two authors were interns at Microsoft Research Asia. This research is supported by the National Science Foundation of China, the project code: 70471064, and Research Foundation of Beijing Institute of Technology, the project code: BIT-UBF-200308G10. [1] Berberich, K., Vazirgiannis, M., and Weikum, G.T-Rank: [2] Boldi, P., Santini, M., and Vi gna, S. PageRank as a function [3] Brin, S., and Page, L. The anatomy of a large-scale [4] Gyongyi, Z., and Garcia-Molina, H. Link spam alliances. [5] Gyongyi, Z., and Garcia-Molina, H. Web spam Taxonomy. [6] Haveliwala, T. Topic-sensitive PageRank. In Proceedings of [7] Haveliwala, T., Kamvar, S., and Jeh, G. An analytical [8] Kleinberg, J. Authoritative sources in a hyperlinked [9] Langville, A., and Meyer, C. Deeper inside PageRank. [10] McSherry, F. A uniform approach to accelerated PageRank [11] Page, L., Brin, S., Motwani, R., and Winograd, T. The [12] Richardson, M., Prakash, A., an d Brill, E. Beyond PageRank: [13] Yu, P.S., Li, X., and Liu, B. Adding the Temporal 
