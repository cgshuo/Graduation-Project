 HAITONG YANG, YU ZHOU, and CHENGQING ZONG , National Laboratory of Pattern Semantic Role Labeling (SRL) is a standard task in the natural-language processing (NLP) community. Its goal is to recognize related phrases and assign a joint structure (WHO did WHAT to WHOM, WHEN, WHERE, WHY, HOW) to each predicate of a sentence [Gildea and Jurafsky 2002]. Because rich semantic information is encoded, SRL has been widely applied in many fields of NLP, such as question and answering [Narayanan and Harabagir 2004], information extraction [Surdeanu et al. 2003], and machine translation [Wu and Fung 2009a; Liu and Gildea 2010; Xiong et al. 2012; Zhai et al. 2012, 2013].

The semantic equivalence of the parallel bi-texts means that they should have the same predicate semantic structure. In bilingual or multilingual text processing, such as MT and cross-lingual information retrieval, many researchers [Wu and Fung 2009b; Liu and Gildea 2010; Xiong et al. 2012; Zhai et al. 2012, 2013] have utilized the consistency of semantic representations of bi-texts to improve the performance of their systems. Semantic structures have two major advantages over syntactic structures. First, semantic structures tend to agree better between two languages than syntactic constituents [Fung et al. 2006]. This property motivates the approach of using the consistency of semantic roles to select MT outputs [Wu and Fung 2009b]. Second, the set of semantic roles of a predicate models the skeleton of a sentence, which is crucial to the readability of MT output [Liu and Gildea 2010].

Motivated by these observations, this article focuses on bilingual SRL. A conventional way to perform bilingual SRL is using monolingual SRL systems to perform SRL on each side of bi-texts separately, as has been done by Fung et al. [2007]. However, it is very difficult to obtain consistent SRL results on both sides of bi-texts in this way. For example, it may occur that  X   X  is labeled as AM-LOC but  X  X ecently X  is labeled as AM-TMP in monolingual SRL systems. In this case, we should abandon these obvi-ously inconsistent results and search for better results for both sides. Some works have demonstrated that it is a better way to jointly infer bilingual SRL because both sides of bi-texts contain complementary language cues that can be utilized to correct the in-consistent cases. In fact, this observation has formed the basis for many cross-lingual tasks in NLP, such as syntax projection across languages [Yarowsky et al. 2001; Hwa et al. 2005; Ganchev et al. 2009], cross-lingual named entity recognition [Huang and Vogel 2002; Moore 2003] and IR [Si and Callan 2005]. In these cases, multilingual mod-els yield increased accuracy because different languages present different ambiguities and therefore offer complementary constraints on the shared underlying labels. Zhuang and Zong [2010a] first explored the problem of inferring bilingual SRL jointly. In their work, they constructed an Integer Linear Programming (ILP) framework to in-fer bilingual SRL in which some constraints are designed to enhance the consistency of bilingual SRL. They reported better performance over monolingual systems. However, there are two limits in their work. One is that, in the inference, they still adopted the argument candidates generated by monolingual systems, but in fact many arguments are discarded in the argument pruning or identification stage of monolingual systems. Instead, we believe that the complementary language cues in bi-texts could guide us to recover these missing arguments. The other limit is that the objective function of their bilingual model is very complex 1 , but the inference technique of ILP is costly in handling complex optimization problems. Moreover, if the data to be processed are very large, ILP often fails to meet the requirements of practical applications. To overcome these limits, in this article, we propose two strategies for bilingual SRL. Specifically, to recover the missing arguments, we developed a greedy algorithm called Bi-Directional Projection (BDP) to finely enlarge the set of candidates. In BDP, we first use a monolingual system to generate the initial candidates for both sides; then, with the help of word alignment information, we greedily search for the candidate with greatest match in the other side for each candidate of one side. If the resulting candidate is not in the initial candidates set, it will be added. It is noted that some noisy candidates may be added incorrectly due to incorrect word alignment. Thus, we also design some simple, but effective, rules to eliminate these noisy candidates. In the inferring phase, in contrast to Zhuang and Zong [2010a], we employed a technique called Dual Decomposition (DD) to efficiently perform the inference. In recent years, DD has been widely accepted as a simple and efficient technique to resolve combination optimization problems, such as the combination of two lexicalized parsing models [Rush et al. 2010], syntax parsing [Koo et al. 2010; Wang et al. 2013b], and bilingual named entity recognition [Wang et al. 2013a]. In this work, we prove that the technique of DD could also efficiently infer bilingual SRL.

An overview of our approach to bilingual SRL is shown in Figure 2. Our approach works as follows: first, we use monolingual systems to generate the initial candidates; second, based on the initial candidates, the proposed BDP greedily search on both sides and obtain refined argument candidates; third, the technique of Lagrange Dual Decomposition is implemented to search for the consistent SRL results for both sides of bi-texts.

We have conducted experiments on a standard parallel benchmark X  X he OntoNotes dataset. Experimental results show that our method yields significant improvements over the state-of-the-art monolingual systems. Benefiting from reliable candidates gen-erated by the BDP algorithm, our method achieved better performance than Zhuang and Zong [2010a], and our method is also much faster than theirs.

The rest of this article is organized as follows: Section 2 describes how we generate argument candidates on both sides of bi-texts using monolingual SRL systems. Section 3 presents the algorithm of BDP. Section 4 presents our joint inference model. Section 5 presents our experiments and results. Section 6 introduces related work. Our conclusions can be found in Section 7. In this section, we introduce the monolingual SRL system, which is a baseline in the experiments and also generates initial argument candidates for BDP and the inference step.

An SRL system usually takes a parse tree (either handcrafted parse trees or auto parse trees generated by an automatic parser) as the input and assigns appropriate semantic roles to the constituents in the parse tree, which are semantic arguments to the predicate in question. Most SRL systems work in stages, which consist of a prun-ing stage, an argument-identification stage, and an argument-classification stage. In the pruning stage, the goal is to eliminate the obvious nonargument candidates. The heuristic pruning method in Xue [2008] has been widely accepted by the SRL commu-nity; thus, we implemented his method in this article. The predominant approach to argument identification and argument classification is to formulate them as classifi-cation problems that can be solved with machine-learning techniques. In the stage of argument identification, all argument candidates are identified as an argument or a nonargument that is not related to the predicate in question; thus, argument identifi-cation is generally formulated as a binary classification task. Argument classification, which assigns appropriate semantic roles to candidates identified by the argument identification stage, is a natural multicategory classification problem. In the literature, many classification techniques, such as SVM [Pradhan et al. 2005], Perceptrons [Carreras et al. 2004], and Maximum Entropy [Xue and Palmer 2004; Xue 2008], have been successfully used to solve the tasks of argument identification and argument classification. In this work, we used a Maximum Entropy classifier with a tunable Gaussian prior. As a discriminative model, the Maximum Entropy classifier is scalable to handle a large set of features. Moreover, the Maximum Entropy classifier does multicategory classification naturally, thus can be straightforwardly applied in the problems here. Zhang Le X  X  MaxEnt toolkit 2 is used for the specific implementation. Features are crucial to the performance of SRL systems. Many works [Xue and Palmer 2004; Xue 2008] have studied what features are discriminative for SRL. In Xue and Palmer [2004] and Xue [2008], the author experimentally proved that the stages of ar-gument identification and classification require different features. Therefore, we follow his suggestions and utilize two groups of features: one for argument identification and the other for argument classification. The following lists describe the features in detail.
The features for argument identification:  X  X redicate lemma  X  X ath from node to predicate  X  X ead word  X  X ead word X  X  part-of-speech  X  X erb class  X  X redicate and head-word combination  X  X redicate and phrase-type combination  X  X erb class and head-word combination  X  X erb class and phrase-type combination
All of these features are also used in argument classification. Here are some other features:  X  X osition: the relative position of the candidate argument compared with the predicate  X  X ubcat frame: the syntactic rule that expands the parent of the verb  X  X he first and the last word of the candidate  X  X hrase type: the syntactic tag of the candidate argument  X  X ubcat frame+: the frame that consists of the NPs [Xue 2008]. One reason why we choose a maximum entropy classifier as our classifier is that it can easily incorporate arbitrary features. The other reason is that the maximum entropy classifier can directly output classification probabilities without any postprocessing that will be used in our joint inference stage. For clear illustration, we wrap the classification probabilities into the output of monolingual SRL systems. As shown in Figure 3, an argument output of monolingual SRL systems has three attributes: its location in the sentence loc , represented by the position of its first word and last word; its semantic role label l ; and its classification probability p . Therefore, we annotate an argument output of a monolingual SRL system as a triple ( loc , l , p ). For example, the A 0 argument in Figure 3 is ((6 , 6) , A 0 , 0 . 85). The semantic equivalence of the parallel bi-texts means that they should have the same predicate semantic structure, which implies that the aligned argument pairs in bi-texts (e.g., the pair of  X   X  and  X  X ecently X  in Figure 1) should have consistent labels. Nevertheless, the monolingual SRL systems of Chinese and English work indepen-dently. Thus, some inconsistent cases often occur on both sides of monolingual SRL systems. However, there are many complementarity clues that are beneficial to correct these mistakes. For example, in Figure 1, if  X   X  is missed or annotated with a wrong label by a Chinese SRL system, but  X  X ecently X  is labeled correctly by an English SRL system, intuitively, with the help of the English side, we can recognize the argument of  X   X  on the Chinese side and give it a correct label.

From these observations, we can see that there are mainly two types of complemen-tarity that can be utilized in the bilingual SRL system.

Location Complementarity means that, if one of an argument pair is found by the monolingual SRL system but the other is missed, we can track down the missed ar-gument in the other language according to word alignment information. It is worth noting that there are also some exceptional cases, for example, the argument ellipsis on one side. These cases have little impact on the location complementarity, however, because, in these cases, the aligned points are null in the other language.
Label Complementarity means that an aligned argument pair in a bi-text should have consistent labels. For instance, in Figure 1, the labels for  X   X  and  X  X ecently X  are AM-TMP, which are consistent. However, if the label on one side is wrong but the label on the other side is right, we obtain inconsistent results for both sides. In these cases, we should abandon this result and search for a pair of consistent results. It is worth noting that the consistent labels for the aligned argument pair do not mean that they have to have the same label. For a brief illustration, an example is given in here. In the following bi-text, the predicate pair is  X   X  and  X  X ecreased X  and has three pairs of aligned arguments. Among them, the argument pair of  X   X  X nd X  X p to February of this year X  have the same label  X  X M-TMP X , but the other two pairs have different labels because of the different usages of the predicates on both sides. Thus, it is not reasonable to force the aligned arguments to have the same label, and we need to consider the context of predicates to make a decision for bilingual SRL. (b) [up to February of this year] AM-TMP , [China X  X  general trade exports] A1 have
Here, we summarized two categories of complementarity that can be beneficial to bilingual SRL. In Sections 3 and 4, we will describe how to utilize the two categories of complementarity to improve bilingual SRL. As illustrated in Section 2, an SRL system works in a pipeline way and argument can-didates are obtained through the pruning phase and argument-identification phase. However, the pipelined way often causes error propagation because the results of the argument pruning and identification stages are not always correct. For clearer illustra-tion, an example is given in Figure 4. In the given bi-text, the predicate pair in question is  X  / allocate X . The argument candidates generated by individual monolingual SRL systems are the ones embedded in a box. We note that, on the Chinese side, only  X   X  X s identified as an argument of  X   X , while  X   X  X nd X   X  are discarded in the pruning due to parsing errors. In the SRL community, it is widely recognized that the overall performance of an SRL system is largely determined by the quality of syntactic parsers [Gildea and Palmer 2002], which is particularly notable in the pruning stage and iden-tification stage. We note that the text related to  X   X  on the Chinese side is a complex  X   X  X  phrase  X   X   X . When the text is parsed, the problem of syntactic ambiguity occurs, which causes  X   X  X nd X   X  to be discarded by the Chinese SRL system. However, we note that the translated text for  X   X  X s X  the land allocated to him by the government  X , which is a relatively simple text compared with the complex  X   X  phrase on the Chinese side and is easily parsed correctly. Therefore, on the English side, we obtain fully correct argument candidates from the English SRL system. In this case, the fully correct candidates of English side can be utilized to guide recovering the discarded candidates on the Chinese side because we can locate to the positions of the missed arguments on the Chinese side according to word alignment. We found that it is not always true that the English side obtains better candidates than the Chinese side and the opposite also occurs. Thus, an algorithm is expected to utilize the Location Complementary information of bi-texts to improve the performance of both sides simultaneously.
 Motivated by the earlier observations, in this article, we propose an algorithm called Bi-Directional Projection (BDP) to recover the argument candidates discarded in the argument pruning and identification stages. In the algorithm, a crucial step is to find the aligned candidate with equivalent semantics on the other side for a candidate on one side. For example, in Figure 4, given  X  X he land X  and  X  X he government X  on the English side, we need to find  X   X  X nd X   X  on the Chinese side. Because word alignment of bi-texts conveys semantic equivalence at the word level and provides the requisite information for our goal, word alignment is our natural choice. If the word alignment of the bi-text is known, we can find the missing candidates  X   X  X nd X   X  X fFigure4 with little effort.

The pseudo-code of BDP is shown in Algorithm 1, in which we first generate the initial candidates using monolingual SRL systems. Then, for each candidate on the English side, we search all constituents of the parse tree on the Chinese side. If the most similar candidate to the current candidate is not contained in the candidate set on the Chinese side, this candidate is added into the candidate set on the Chinese side. We call this process a projection. After the projection from English to Chinese is done, we forward a projection from Chinese to English. Because in a projection there may be new candidates generated, we iterated the BDP process until there were no more new candidates produced in both directions X  projections.

In BDP, a crucial step is to find the aligned candidate with semantic equivalence on the other side for each candidate on one side. Therefore, we need a similarity function to evaluate the similarity of a pair of candidates. Here, we adopt a simple and effective function. Specifically, we approximate the similarity of a pair of candidates by measuring the word alignment-based word overlap [Pad  X  o and Lapata 2009]. Assume that we would like to evaluate the similarity of a Chinese candidate c and an English candidate e .Let C and E denote the set of words in the yield of c and e , respectively, and TC and TE the set of aligned words for c and e in the other language, respectively. Then, the similarity of c and e is defined as: In the experiments, we use the publicly available GIZA++ [Och and Ney 2003] to automatically induce the word alignment for bi-texts. However, the results generated by GIZA++ may contain some incorrect alignments, which can cause cascading errors in BDP. Therefore, we have to take measures to filter these noisy candidates. Here, we make the noise elimination using some simple, but effective, rules: (1) Length ratio. The aligned candidates are translated text, and their length ratio (2) Identification probability. We also use the traditional module of argument identi-(3) Arguments cannot overlap with the predicate. This is a common rule in the argument (4) Candidates far from the predicate. This is a rule designed for the English side X  X  In this section, we describe how our bilingual SRL model works. Our model can be conceptually decomposed into three components: the Chinese side, the English side, and the Argument Alignment. The objective function of our bilingual model can be formulated as the sum of two subobjectives with constraints: In this equation, O c and O e represent the quality of the SRL results on the Chinese side and English side, respectively. If there are not any constraints, we can find that the results of Equation (2) are the same as the monolingual systems. Therefore, we use the third module, Argument Alignment O a , to enhance the consistency of bilingual SRL. It gives preference to the bilingual SRL results that have more consistent argument structures. In the following sections, we will introduce these three components and the inference technique optimizing Equation (2), respectively. 4.1.1. Chinese Component. For easy illustration, some notation is given here. We denote the entire semantic label set of the Chinese side as { l c 1 , l c 2 ,... l c L Chinese side, there are N c candidates denoted as cand c 1 ,..., cand c N assigning l c m to cand c i is p c im that is generated by the monolingual system. Meanwhile, an indicator variable x im is defined as: to represent whether cand c i is assigned a label l c m .
 Therefore, the objective function of the Chinese side is where the constraint L c m = 1 x im = 1 guarantees that each candidate is assigned one and only one label. 4.1.2. English Component. Similar notations are defined on the English side. The entire semantic label set of the English side is denoted as { l e 1 , l e 2 ,... l e L N e candidates on the English side and are denoted as cand e 1 variable y jn is defined as: to represent whether cand e j is assigned a label l e n . Therefore, the objective function of the English side is where the constraint L e n = 1 y jn = 1 guarantees that each candidate is assigned one and only one label. In Equation (2), we utilize the Argument Alignment module to enhance the consistency of bilingual SRL. Just as stated in Section 3, both sides of bi-texts have the same predicate argument structure, which indicates that each pair of aligned arguments has consistent labels, for example, the aligned pair of  X   X  and  X  X ecently X  in Figure 1 is consistent if labeled as AM-TMP. Moreover, the set of aligned argument pairs have been obtained through the BDP algorithm. Therefore, the goal of Argument Alignment is to determine whether a pair of aligned arguments is consistent if assigned a pair of labels. 4.2.1. Argument Alignment Model. The problem can be formulated as a binary classifi-cation task. Here, we also use a Maximum Entropy classifier to determine whether a pair of aligned candidates cand c i and cand e j are consistent when they were assigned a Argument Alignment model. Its definition is as follows:
To train the model, we need a corpus with annotations of alignment relations between arguments. Zhuang and Zong [2010a] have manually aligned the arguments in 60 files (chtb_0121.fid to chtb_0180.fid) of the OntoNotes corpus. Therefore, we use these data as our training data. Statistics about the alignment relations between arguments are shown in Table I. Each entry in Table I is the number of times for which one type of Chinese argument aligns with one type of English argument. AM* stands for all adjunct types such as AM-TMP and AM-LOC, and NUL means that the argument on the other side cannot be aligned with any argument on this side. From Table I, we can get the following observations: (1) The consistent argument pairs do not have to have the same labels. In general, an (2) The consistent argument pairs have different labels from a limited scope. For ex-4.2.2. Features for the Argument Alignment Model. We use the following features in the Argument Alignment Model.

Semantic role labels of argument pairs . From Table I, we can see that semantic role labels of argument pairs are a good indicator of whether they should align with each other. For example, a Chinese A0 argument aligns with an English A0 argument most of the time, but never aligns with an English AM* argument in Table I.
Predicate verb pair and label pair combination . Different predicate pairs have different argument alignment patterns. Let us take the Chinese predicate  X   X  X nd the English predicate  X  X row X  as an example. The argument alignment matrix for all instances of the Chinese X  X nglish predicate pair ( X   X ,  X  X row X ) in the manually aligned corpus is shown in Table II.

From Table II, we can see that all A0 arguments of  X   X  align with A1 arguments of  X  X row X , which is in contrast to the results in Table I, in which a Chinese A0 argument tends to align with an English A0 argument. This phenomenon shows that a predicate pair can determine which types of arguments should align with each other. Therefore, we use the predicate pair combined with labels as a feature in the argument alignment model.

Syntactic tags and label pair combination . Syntactic tags are a good indicator of semantic roles. Therefore, we concatenated syntactic tags with label pairs as a feature in the Argument Alignment model. We formulate our bilingual SRL model in Equation (2). In this section, we illustrate how to optimize the objective function with constraints. The notation defined in Sections 4.1 and 4.2 are still used here. In addition, the set of aligned candidates pairs is denoted as A , which is obtained from the BDP algorithm.  X  ( i , j , m , n ) is the output of the Argument Alignment module, representing whether an aligned pair cand c i and cand e j with the In other words, the constraint will be satisfied if x im = 0or y jn = 0, and it only works when ( i , j )  X  A , x im = 1and y jn = 1. Therefore, the entire objective function becomes: The Lagrange relaxation of this objective function is where u ij ( m , n ) is the Lagrange multiplier for the aligned argument pair cand c i and u , respectively.
 Then, the dual objective function is: The dual problem is to find min L ( U ). We use the subgradient method [Boyd and Mutapcic 2003] to minimize the dual objective function. Following Rush et al. [2010], we define the subgradient of L ( U )as: Then, u ij ( m , n )isadjustedasfollows: where  X &gt; 0 is a small step size and is set to 0.1 in the experiments.

Algorithm 2 presents the subgradient method to solve the dual objective. In the beginning, the algorithm initializes the Lagrange multiplier value with 0. In each iteration, the algorithm finds the best X k and Y k .If X k and Y k are consistent, then the solution is returned. Otherwise, adjust the Lagrange multipliers. It is worth noting that the max problem (Line 5) can be solved efficiently because the probability of assigning a label to each candidate has been obtained by the monolingual systems. From this optimization process, the Lagrange multiplier can be regarded as an intermediate component that generates punishments for the inconsistent results. In our experiments, we use the Xinhua News portion of Chinese and English data in LDC OntoNotes Release 3.0 (LDC2009T24). This data contains parallel proposition an-notations for 325 files (chtb_0001.fid to chtb_0325.fid) from a Chinese X  X nglish parallel treebank. The detailed description of this data can be found in Palmer et al. [2005]. The English part of this data contains proposition annotations only for verbal predicates. Therefore, we consider only verbal predicates in this work.
 We use the GIZA++ toolkit [Och and Ney 2003] to perform automatic word alignment. In additon to the parallel PropBank data, we use an extra 4,500K Chinese X  X nglish sentence pairs to induce word alignments for both directions, with the default GIZA++ settings. The alignments are symmetrized using the intersection heuristic, which is known to produce high-precision alignments.

Our monolingual SRL systems are trained separately. The Chinese SRL system is trained on 640 files (chtb_0121.fid to chtb_0931.fid) in the Chinese Propbank 1.0; the English SRL system is trained on Sections 02-21 of the English Propbank. We use Berkeley parser and Charniak parser for auto parsing in Chinese and English, respectively. The parsers are also retrained on the training sets. The max iteration number in the inference is set to 30.

In the parallel corpus, we use 80 files (chtb_0001 to chtb_0080.fid) as the test data, 40 files (chtb_0081.fid to chtb_0120.fid) as the development data, and 60 files (chtb_0121.fid to chtb_0180.fid) as the training data. The data setting is the same as in Zhuang and Zong [2010a]. Table III shows the sentence and predicate distributions in each dataset. We note that there are a small number of one-to-many Chinese X  X nglish sentence pairs except for one-to-one sentence pairs. The quality of automatic word alignment on one-to-many Chinese X  X nglish sentence pairs is usually very poor, how-ever. Therefore, we include only one-to-one Chinese X  X nglish sentence pairs in all data, and not all predicates in a sentence pair can be included. Only bilingual predicate pairs are included. A bilingual predicate pair is defined to be a pair of predicates in bi-texts, which align with each other in automatic word alignment. The setting for sentence pairs and predicate pairs is also the same as in Zhuang and Zong [2010a]. To train the Argument Alignment model, we need both gold samples and negative samples, but there are only gold samples in the manually annotated corpus. To produce suffi-cient negative samples, we fix one label on one side and randomly change the label on the other side. Finally, Zhang Le X  X  MaxEnt toolkit is adopted to train the Argument Alignment model with 300 iterations and Gaussian prior 1.0. For fair comparison of different SRL systems, we use the widely accepted criteria: Precision ( P ), Recall ( R ), and F score ( F 1 ). Following Zhang et al. [2004], we conducted statistical significance tests with 1,000 resampling size and 95% confidence interval. In the following tables, an asterisk is used to indicate that the difference between the proposed approach and the corresponding baseline system was statistically significant.
In BDP, we designed 4 heuristics rules to prevent the noisy candidates. Rules (3) and (4) can be directly applied in the argument-identification stage. But, for Rules (1) and (2), there are two hyperparameters,  X  and  X  ; we tuned them on the development set. Tables IV and V show the results of independently varying  X  and  X  on the development data, respectively. They show that the best performance is achieved when  X  = 3and  X  = 0 . 4. Therefore, we use these two thresholds in our following experiments. The main results of different approaches on the test set are shown in Table VI. We can see that both our system and Zhuang and Zong X  X  [2010a] system significantly improve the F1 scores over monolingual SRL systems, which demonstrates that jointly inferring bilingual SRL is superior to the monolingual SRL systems. On the other hand, if we compare our system with Zhuang and Zong X  X  system, our approach also outperforms theirs, which mainly benefits from the improvement of the recall score. The large improvement of the recall score indicates that BDP helps recover many candidates discarded in the argument-identification stage. Although the improvement of our method over Zhuang and Zong [2010a] is limited, our method is simple and efficient (see Section 5.5), which is particularly important if the data to be processed are very large. BDP is designed to recover the missing candidates in the argument-pruning and iden-tification stages. In this section, we investigate the effects of BDP on argument identi-fication and SRL, respectively.

The results of argument identification on the test set are shown in Table VII. In the table,  X  X i X  means the system without BDP,  X  X i-Ch X  means the system with the projection from English to Chinese,  X  X i-En X  means the system with the projection from Chinese to English, and  X  X i-All X  means the system with the complete BDP. We can see that both  X  X i-Ch X  and  X  X i-En X  improve the performance of the Chinese side and the English side, respectively, while  X  X i-All X  reaches the best performance on both sides, which indicates that Location Complementarity occurs widely on both sides of monolingual SRL systems, and BDP can recover a large number of missing candidates in the argument-identification stage.

We further investigate the effects of BDP on SRL. The results of the test set are shown in the Table VIII. In the experiments, we measure the effects of BDP on three SRL systems: monolingual SRL systems, Zhuang and Zong X  X  [2010a] system and our system. We can see that, after BDP is added, the performance of all SRL systems improved, which benefits from a large number of candidates recovered by BDP. The improvement of monolingual SRL systems is slightly lower than the others because monolingual SRL systems labeled both sides of bi-texts independently, while jointly inferring was implemented in Zhuang and Zong X  X  [2010a] system and our system. Another interesting observation is that, whatever BDP is added, our system and Zhuang and Zong X  X  [2010a] system have comparable performance. This indicates that, under the evaluation metric of F 1 , the technique of DD does not prevail over ILP in inferring bilingual SRL. The advantage of DD over ILP is its simplicity and high efficiency (described in Section 5.5). Zhuang and Zong [2010a] utilized ILP to infer bilingual SRL, but the objective func-tion in their method is very complex and the ILP technique requires costly inference. Nevertheless, from Algorithm 2, we can see that the Lagrange DD is very concise and fast. Here, we compare the time consumption of the two methods in inferring bilingual SRL; the results are shown in Figure 5. We can see that the time consumption of the DD is only one fourth of the ILP X  X  consumption under the same hardware condition. In consideration of data being produced at a vast and unprecedented scale in modern society, we think our method is significantly superior to Zhuang and Zong [2010a] in practical applications. The technique of Lagrange DD is iterative. Thus, the convergence of the algorithm is an important indicator of a good algorithm. Figure 6 shows the percentage of cases in which the exact solutions are returned (we reach an agreement between the Chinese side and the English side) versus the number of iterations of the algorithm. The algorithm produces exact solutions for over 94% of all examples if the maximum iteration number is set to 30. In addition, for over 90% of the examples, the algorithm returns an exact solution in 10 iterations or fewer. Although the dual decomposition algorithm is not guaranteed to give an exact solution, it is very successful at achieving this goal in our task. To further illustrate the effectiveness of our approach, we perform a manual error analysis on the results. Table IX shows the statistics on the errors in monolingual SRL systems and our system. We can see that our method obtained approximately 20% error reduction compared with monolingual SRL systems both on the Chinese side and the English side. The corresponding error reduction shows that our method is effective in jointly decoding bilingual SRL results. Moreover, the overall consistency of bilingual SRL results is also improved by 14%.

To better understand the advantages of jointly inferring bilingual SRL, some ex-amples are given in Table X. In Example (1), although the argument  X   X  X sa prepositional phrase (PP), it is the agent (A0) of the predicate  X   X . In the training data of the Chinese side, the arguments with a PP tag are usually labeled as AM-MNR, AM-LOC, or AM-TMP, except for A0. We found that such a case (a PP as A0) occurs less than 10 times in the training data of the Chinese side. Thus, it is not surprising that the argument  X   X  is incorrectly labeled as AM-MNR by the Chinese SRL system.
 Meanwhile, it can be seen that, although the argument  X  by Taiwan authorities  X  X nthe English side is also a PP, it is correctly labeled as A0 because a PP as A0, especially when the PP begins with  X  X y X , is very common in the training data of the English side. Given the labels AM-MNR and A0, the module of Argument Alignment successfully recognizes the inconsistency of both sides. After jointly inferring of our approach, we obtain the correct labels for both sides.

In Example (2), the argument  X  in 1995 and 1996  X  is incorrectly labeled as AM-LOC by the English SRL system, but in fact this argument indicates the time of the predicate  X  X old X . In an SRL system, AM-LOC and AM-TMP are easily confused because these two kinds of roles often occupy similar syntax contexts. However, the argument  X   X  on the Chinese side is labeled correctly as AM-TMP because the last word  X   X  (year) is a useful clue to clearly indicate the time. The argument pair with labels AM-TMP and AM-LOC is classified into the inconsistency by the module of Argument Alignment and, finally, our approach outputs the correct labels for both sides.
Although the proposed model has significantly enhanced the consistency of bi-texts, some challenges remain. We collect the classification errors for the arguments of the Argument Alignment set A in our system; the corresponding statistics are shown in Table XI. We found that the major sources of the remaining errors are monolingual SRL systems (220/320 = 68%) and the Argument Alignment module (100/320 = 32%).  X  Monolingual SRL systems. The jointly inferring in our method is based on the candidates and the classification probability generated by monolingual SRL systems.
According to the description of our method (Section 4.3), we find that the iteration process reaches the optimal through imposing punishment on the inconsistent re-sults. However, in the case that the classification probability for the gold label in monolingual SRL systems is too low, it is hard for jointly inferring to correct this kind of error. In all errors, this kind of error accounts for 68%, and is the main source for the remaining errors.
  X  The Argument Alignment module. The Argument Alignment module is designed as the constraint to enhance the consistency of bi-texts and plays a core part in our approach. Thus, it is crucial to the performance of jointly inferring bilingual SRL.
To evaluate the performance of the Argument Alignment module, we adopt a 10-fold cross-validation experiment on the training set. The average accuracy is about 85%.
Although the Argument Alignment module performs well, it is far from perfect; it causes 32% of errors. To further analyze the source of these errors, we classify these errors into two categories: one is caused by the Argument Alignment module not recognizing the inconsistent results (80/320 = 25%) and the other is caused by the
Argument Alignment module mistaking the consistent results for the inconsistent ones (20/320 = 7%). Some previous work on monolingual SRL is related to our work. Gildea and Jurafsky [2002] first presented a system based on a statistical classifier, which is trained on a hand-annotated corpora, FrameNet. In their pioneering work, they used a gold or autoparsed parse tree as the input, then extracted various lexical and syntactic fea-tures to identify the semantic roles for a given predicate. After Gildea and Jurafsky [2002], there have been a large number of works on automatic semantic role labeling. Punyakanok et al. [2004] constructed an ILP architecture in which the dependency relationships among arguments are implied in the constraints. In addition, there have been many extensions in machine-learning models [Moschitti et al. 2008], feature en-gineering [Xue and Palmer 2004], and inference procedures [Toutanova et al. 2005; Punyakanok et al. 2004; Yang and Zong 2014; Yang et al. 2015; Zhuang and Zong 2010b].

Research on SRL utilizing parallel corpus is also related to our work. Fung et al. [2007] did pioneering work on studying argument alignment on Chinese X  X nglish par-allel PropBank. They first identified and labeled the semantic structures using the Chinese and English shallow semantic parsers. Then, given the SRL results on both sides, they automatically induced the argument alignment between two sides. Pad  X  o and Lapata [2009] did research on cross-lingual annotation projection on an English X  German parallel corpus. They performed SRL only on the English side, then mapped the English SRL result to the German side. The difference between our work and these works is that we fully utilize complementary language cues in parallel bi-texts to improve the performance of both sides.

The work most related to ours is Zhuang and Zong [2010a]. They constructed an ILP framework to incorporate not only linguistic constraints on source and target sides of bi-texts, but also the bilingual argument structure consistency requirement on bi-texts. They reported substantial improvements on both sides of bi-texts compared with the state-of-the-art monolingual SRL system. However, our work is different from theirs in two ways. First, they fully accepted the candidates from monolingual SRL systems as the candidates in the inferring phase while a number of arguments were discarded in the argument-pruning and identification stage. We developed an algorithm to recover these discarded arguments. On the other hand, they used the technique of ILP to infer the bilingual SRL, but the objective function of their model is very complex and ILP usually requires costly inference. In contrast to Zhuang and Zong [2010a], we utilized Lagrange DD to efficiently infer consistent results for both sides. This article focuses on bilingual Semantic Role Labeling, whose goal is to annotate semantic roles on bi-texts. If we use monolingual SRL systems to make annota-tions on both sides of bi-texts, it is very difficult to obtain good and consistent SRL results on both sides. Moreover, we note that there is a lot of complementarity infor-mation on both sides of bi-texts. We classified the complementarity information into two categories: Location Complementarity and Label Complementarity. In this arti-cle, we describe how to utilize the two categories of complementarity information to improve bilingual SRL. We mainly make efforts on the following two points: (1) in the argument-identification stage, we propose a Bi-Directional Projection algorithm to recover candidates discarded in the argument-pruning or identification phase; (2) in the inferring stage, we adopted the technique of Lagrange Dual Decomposition to infer consistent results for bi-texts. Compared with existing methods, we obtain bet-ter results and a faster processing speed. Finally, the proposed model does not utilize language-dependent information. Therefore, although our experiments are conducted on Chinese X  X nglish language pairs, it is expected that the proposed approach can be applied to other language pairs with little adaptation effort.

In our work, we still utilize monolingual SRL systems to generate candidates for joint inferring. In the future, we will consider how to perform joint learning in both sides of bi-texts without any monolingual SRL systems.

