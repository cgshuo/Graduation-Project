 1. Introduction
Document summarization techniques are one way of helping people find information effectively and efficiently. There are two main approaches to automatic summarization: abstractive approaches and extractive approaches. Abstractive ap-proaches, which promise to produce summaries that are more like what a human might generate but are limited by the pro-gress of natural language understanding and generation and the more widely used extractive approaches which rank sentences by importance, extract salient sentences, and then compose the summary. Sentence ranking has for some time classification models which were used to identify  X  X  X ey X  sentences. Subsequent learning-to-rank approaches normally are used in classification or ranking.
 mate the importance of sentences, which can be better characterized as continuous than discrete. Another advantage of manually or automatically.

In this paper, we study how to apply regression models to the sentence ranking problem in query-focused multi-docu-ment summarization. We implement the regression models using Support Vector Regression (SVR). SVR is the regression type of Support Vector Machines ( Vapnik, 1995 ) and is capable of building state-of-the-art optimum approximation func-tions. As data for this study, we will construct  X  X  X seudo X  training data automatically from human summaries and then use these and their document sets to develop and compare several N -gram based methods that estimate  X  X  X early true X  sentence in the test data. We carry out a series of experiments to evaluate the efficiency and robustness of our approaches. proposed approach. Section 4 presents experiments, evaluations and discussions. Section 5 concludes the paper. 2. Related work The application of machine learning techniques in document summarization has a long history. Kupiec, Pedersen, and
Chen (1995) first proposed a trainable summarization approach which adopted word-based features and used a na X ve Bayes-features performed better than any other system using only one single feature. Many early studies followed this idea and used a set of documents in which key sentences had been annotated manually to train a Support Vector Machine (SVM) clas-sification model to learn how to extract important sentences. They reported that SVM outperformed other machine learning models such as decision tree or boosting methods in the Japanese Text Summarization Challenge (TSC). Zhou and Hovy (2003) proposed a Hidden Markov Model (HMM) based approach to estimate the extract desirability of an English sentence comparable to the best system in the Document Understanding Conference (DUC) 2001 generic summarization data set.
Zhao, Wu, and Huang (2005) applied the Conditional Maximum Entropy (ME) model to the DUC 2005 query-focused sum-dom Fields (CRF) based framework for generic summarization and reported that CRF performed better than many existing models, such as HMM and SVM. A common feature of all these work is that they all relied on classification models to rank sentences.

More recently, learning-to-rank models have been examined. Amini, Usunier, and Gallinari (2005) investigated how to use learning-to-rank models for query-focused single-document summarization and compared the proposed ranking algo-a perceptron-based ranking system learned from automatically constructed training data. The system ranked eighth of 34 participating systems. At DUC 2007, Toutanova (2007) proposed the PYTHY system which learned a log-linear ranking func-tion to combine more than 20 features. The PYTHY system was augmented with two sophisticated post-processing pro-cesses, i.e. heuristic sentence simplification and dynamic sentence scoring. It performed very well and ranked second of 30 DUC participating systems. Learning-to-rank models were also applied to webpage summarization tasks and compared tion models in sentence ranking.
 An important requirement of learning-based sentence ranking approaches is that one should have sufficient training data. generate the training data have been widely adopted. The most common manually-generated resources are human summa-ries which are primarily provided for automatic evaluations. The human summaries have been successfully used to judge ginal data set and thus can be used to judge sentence importance. Experiment done by Conroy, Schlesinger, and O X  X eary grams in human summaries and found that the summaries generated by directly using the  X  X  X racle X  sentence scores to ex-tract sentences are even comparable to human summaries on the DUC 2006 data set under the automatic evaluation method
ROUGE. This showed that human summaries can be used effectively in judging the importance of sentences and thus can be used to generate the training data for the learning models.

During our participation in the DUC competition, we made an initial attempt at applying regression models to the query-2006 to generate the training data and used it to learn a sentence scoring function with Support Vector Regression. The sion-style summarization framework and present a further study on how to develop effective regression-based summariza-tion approaches. First, we expand the training data construction scheme to four different methods for discovering more accurate estimation of sentence importance in order to more effectively train the regression models. Second, we adjust models. 3. Regression models for query-focused multi-document summarization
Our summarization approach is built upon the typical feature-based extractive framework, which ranks and extracts sen-ture-based approach thus search for an optimum composite scoring function with the fixed feature set. 3.1. Feature design ment summarization, including three query-dependent and four query-independent features. The features are formulated as follows. 3.1.1. Word matching feature (query-dependent)
In query-focused summarization tasks, queries directly reflect the information expected in the anticipated answers. So the words in the queries are especially informative for sentence scoring. A corresponding query-dependent word feature is then defined as the overlapping degree between the words in a sentence and the words in a query, i.e., where f is the feature value, q is the query. The function same ( w exact matches between words in sentences and queries. 3.1.2. Semantic matching feature (query-dependent) based on the semantic lexicon WordNet, as the semantic relation between the two words. 3.1.3. Named entity matching feature (query-dependent) mation contained in a sentence. The entity-based query-dependent feature is defined as the number of the matched named entities between a sentence and a query. where | entity ( s ) \ entity ( q )| is the number of named entities in both s and q . 3.1.4. Word TF-IDF feature (query-independent)
It is well known that not all words in a text are of equal importance. Many criteria have been proposed for measuring the sentences. where tfidf ( w i ) is the tfidf score of the word w i in the data set.
 3.1.5. Named entity feature (query-independent)
Similarly, we include a query-independent entity-based feature which is defined as the number of named entities in a sentence. 3.1.6. Stop-word penalty feature (query-independent) As stop-words rarely carry useful information, sentences which contain many stop-words are unlikely to be informative.
We thus define a stop-word based feature as follows. where | stopword ( s )| is the number of the stop-words in s . 3.1.7. Sentence position feature (query-independent)
On the assumption that authors introduce the main idea or briefly summarize the main content at the beginning of the text, we further assume that opening sentences are more informative and more important to the document set. We thus de-fine the position-based feature as follows: where n is the total number of the sentences, and s is the i th sentence in the document. 3.2. Regression-style sentence ranking method
A composite function uses the defined features to synthesize the effects of all the features and compute the importance ture set.

The regression model is trained from a set of known topics D which provide the importance score of every sentence. Here documents. A sentence s in D (actually in the document set of D ) is assigned to learn the optimum regression function f : F ( s ) ? R from a candidate function set { f ( x )= w x + b | w e R sion problem, linear SVR chooses the optimum function f 0 in D .

In the practical tasks, the target summary is usually limited in length to a maximum number of words. To maximize the information included in the summary, it is better to select the sentences that contain more information but fewer words. Therefore, the final scoring function is obtained with an additional normalizing process as where | s 0 | is the number of the words in s 0 . 3.3. Model comparison: classification, ranking and regression
Generally, a classification problem is depicted mathematically as follows: given the input as a vector x e R , y function is to minimize the total classification error P l function (for example, the square loss function L ( a , b )=( a b ) as SVM, decision tree, and neutral network. Here we will not look at the specific differences between the classification models.
 Classification-based summarization often categorizes sentences as either worthy of extraction into the summary or not. some way so that scores can be used to rank sentences and top-ranked sentences can be selected to form a summary.
In contrast, a learning-to-rank problem is depicted mathematically as follows: given the input as a vector x e R output pairs D  X  x 1 1 ; x 2 1 ; r 1 ; x 1 2 ; x 2 2 ; r 2 and x 2 i . The best function should minimize the total ranking error summarization, most learning-to-rank models first use real-valued scores to make a judgment between the two sentences models still rank sentences using real-valued scores.

A regression problem can be depicted mathematically as follows: given the input as a vector x e R value y e R , the target function is a function g ( x ) learned from a labeled training data set D ={( x of the predicted value and the true value P l i  X  1 L  X  g  X  x sion models attempt to directly construct a mapping function between the feature vectors and the importance scores. 3.4. Training data construction
Learning regression models requires a set of topics in which the sentence importance is known. However, there is no such precisely annotate sentence importance manually. In this paper, we adopt an alternative strategy of semi-automatically assigning  X  X  X early true X  importance scores to the sentences by using several N -gram-based methods and with reference to human summaries. The basic assumption is that if human summaries are good, the sentences in the documents which are more similar to those in human summaries are also more likely to be good would thus be likely to be assigned higher scores.

Given a document set D and a human summary set H ={ H 1 , , H a summary sentence given the human summaries.

Using the bag-of-words model, the probability of an N -gram t under the i th human summary H where freq ( t ) is the frequency of t in H i and | H i | is the number of the words in H man summaries, we consider two strategies, i.e. the Maximum strategy and the Average strategy where | H | is the total number of human summaries in H .
 can be either p max or p avg ),
Analogously, we have another two scoring methods based on N -gram statistics, i.e. and More specifically, in our experiments we use Uni-grams and Bi-grams.
 3.5. Redundancy removal
If the words in two sentences are very similar, the sentences will probably have similar feature values and consequently vey the same or quite similar information into a summary. Given the fixed length of the summary, a summary that includes tence selection. First, all the sentences are ranked in descending order according to their estimated scores. Then, the summary sentences are selected iteratively, each time with the current candidate sentence of interest being compared the summary. The iteration is repeated until the length of the summary reaches the upper limit. 4. Experiment and evaluation 4.1. Experiment set-up We conduct a series of experiments on the query-focused multi-document summarization task initiated by DUC in 2005. task has been specified as the main evaluation task over 3 years (2005 X 2007) and thus provides a good benchmark for researchers to exchange their ideas and experiences in this area. Each year, DUC assessors develop a total of about 50 DUC topics. Each topic includes 25 X 50 newswire documents and a topic description simulating a potential user X  X  query.
For each topic, four human summarizers are asked to use the related documents to write a 250-word summary that is sub-sequently used in automatic evaluation.

In all the experiments, the documents and queries are pre-processed by removing the stop-words and stemming the remaining words. Four types of named entities, including persons, organizations, locations and times, are automatically ering the focus of this study, no post-processing such as sentence compression or fusion is carried out. SVM 1999 ) is used to implement SVR and the parameters of SVM 4.2. Evaluation metrics
In DUC, the system-generated summaries are evaluated against several manual and automatic evaluation metrics ( Dang, 2005 ). In this paper, we use two of the DUC automatic evaluation criteria, namely ROUGE-2 and ROUGE-SU4, systems (implemented using the proposed approaches) with human summarizers and top-performing DUC systems. ROUGE method that makes use of N -gram comparison. It evaluates the system summaries by comparing them with human summaries. For example, ROUGE-2 evaluates a system summary by matching its Bi-grams against the human summaries, i.e., cates the Bi-grams in the summary S, Count ( t i | H j ) is the number of times the Bi-gram t
H and Count ( t i | S , H j ) is the number of times t i occurred in both S and H
ROUGE-SU4 is very similar to ROUGE-2. It matches Uni-grams and skip-Bi-grams of a summary against human summa-works well in DUC. For example, in the DUC 2005, ROUGE-2 had a Spearman correlation of 0.95 and a Pearson correlation of 0.97 compared with human evaluation. 4.3. Experiment 1: comparison of feature weights automatically learned and manually assigned on the DUC 2005 that in Section 3.4 we introduced four methods using two kinds of N -grams (including Uni-gram and Bi-gram) and two alternative scoring strategies (including Maximum and Average). In this experiment, we conduct a comparison of these based systems, we also develop several baseline systems that use linear functions with manually assigned weights to com-ment summarization) data set. The training data for the learning-based system is constructed from the DUC 2006 data set. iments unless otherwise stated.

Table 1 presents the average ROUGE-2 and ROUGE-SU4 scores and the corresponding 95% confidential intervals of the systems on the DUC 2005 data set. The systems with the same features but different composite functions performed differ-construction method was the best. 4.4. Experiment 2: different features on the DUC 2005
No matter which models are used, the features always have a direct influence on the efficiency of the composite scoring to achieve better composite functions. 4.5. Experiment 3: comparison of different learning models
The purpose of the third experiment is to compare the effectiveness of different machine learning models in sentence importance estimation, including regression models, classification models and learning-to-rank models. Here we consis-tently use the Support Vector Machine to implement all the models to make the comparison more fair, i.e., Support Vector Classification ( Vapnik, 1995 ), Support Vector Regression ( Vapnik, 1995 ) and ranking SVM ( Joachims, 2002 ). against human summaries, we normalize all the estimated scores by the maximum one among them. The training data for pairing any two sentences whose score gap is larger than 0.5. In our implementation, all the thresholds are obtained experimentally.

We first run the systems on the DUC 2005 data set. Table 3 provides the average ROUGE-2 and ROUGE-SU4 scores and the models. To further prove this result, we extend the experiment to the DUC 2006 and the 2007 data sets. When evaluated on are presented in Tables 4 and 5 respectively. The evaluations on DUC 2006 and 2007 confirm the advantages of the regres-sion-style learning approach in feature combination. Moreover, the success of utilizing more information shows that the  X  X  X seudo X  scores are reliable estimations of the sentence importance. 4.6. Experiment 4: comparison with the DUC participating systems
Next, we compare our summarization systems to the state-of-the-art systems. Tables 6 X 8 present the ROUGE results of ument for each document set. As showed in the results, our  X  X  X ni+Max X  system is able to outperform all the participating position in the DUC 2007 (30 systems in all). This shows that the proposed approaches are competitive when compared to is the reason why the rank of our system drops from 2005 to 2007.
 4.7. Experiment 5: comparison of training on the different DUC data sets
In all the preceding experiments, the models were learned on the same training data constructed from the DUC 2006 data We use the  X  X  X ni+Max X  method to generate three sets of training data from each of the DUC 2005, 2006 and 2007 data sets. cross-validations. Tables 9 and 10 present the 3 3 training-evaluation result matrices. The systems trained on the DUC training data among the known topics given the new topic to be summarized. 4.8. Discussion
Experiment 1 has shown that of the four training data construction methods, the training data generated by Uni-gram document sets, Bi-grams are spread much more sparsely than Uni-grams. For example, many more sentences that receive zero scores in Bi-gram methods (which means the Bi-grams in these sentences never appear in human summaries) than in
Uni-gram methods, at about 75% vs. 20%. In other words, the data sparseness problem is more serious in Bi-gram methods than in Uni-gram methods. This unavoidably influences the performance of machine learning approaches and accounts for construction method may also differ. The crucial factor here is how well the similarity measure based upon human summa-ries can estimate  X  X  X rue X  importance.

To sum up, a good training data set for learning regression models requires (1) a good topic set with well-written human summaries, and (2) an appropriate method for sentence importance estimation. 5. Conclusion and future work
This paper has presented our studies of how to develop a regression-style sentence ranking scheme for query-focused multi-document summarization. We examined different methods for constructing the training data based on human sum-maries. We also presented what we have learned on how to construct good training data and compared the effectiveness of classification models or learning-to-rank models for estimating the importance of the sentences. We also showed that the resulting summarization system based on the proposed ranking approach is competitive on the DUC 2005 X 2007 data sets. allow us to model documents in a more sophisticated way.
 Acknowledgements
The work described in this paper was partially supported by Hong Kong RGC Projects (PolyU5211/05E and PolyU5217/ 07E), NSFC programs (60603093 and 60875042) and 973 National Basic Research Program of China (2004CB318102). References
