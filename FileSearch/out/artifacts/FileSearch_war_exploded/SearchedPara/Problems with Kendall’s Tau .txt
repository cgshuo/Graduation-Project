 This poster describes a potential problem with a relatively well used measure in Information Retrieval research: Kendall X  X  Tau rank correlation coefficient. The coefficient is best known for its use in determining the similarity of test collections when ranking sets of retrieval runs. Threshold values for the coefficient have been defined and used in a number of published studies in information retrieval. However, this poster presents results showing that basing decisions on such thresholds is not as reliable as has been assumed. H.3.3 [ Information Search and Retrieval ]: Systems and Software ---performance evaluation. Measurement, Experimentation Kendall X  X  Tau. A great deal of information retrie val research is concerned with ranking of objects, either documents ranked by a retrieval system, sets of system runs ranked using a test collection (Voorhees, 1998), even the difficultly of topics ranked by predictive measures (Yom-Tov, et . al. 2005). A wide range of measures have been created to assess the quality of document ranking (e.g. P@10, MAP, etc), for topics ranke d for difficulty or runs ranked against each other, the de-facto standard is Kendall X  X  Tau rank correlation coefficient (  X  ). (1) Q, No. incorrectly ordered pairs number of pairs of items which are in different orders in the two rankings (Kendall, 1938) . This function is constrained such that  X  =1 if the two rankings are in the same order, and  X  =-1 if they are inverted. There are a number of  X  variations, we use the one defined in (1), which handles ties in a ranking. two test collections rank a set of runs. Voorhees was the first to use the coefficient in this way comparing two versions of TREC ad hoc collections each using relevance judgments from different sets of assessors (1998). Voorhees considered  X   X  0.9 to indicate that two test collections were equi valent. (Note that this threshold for equivalence is stricter than that of significant correlation.) In a later paper (2001) she stated. should be considered equivalent si nce it is not possible to be more precise than this. Correlations less than .8 generally reflect noticeable changes in the ranki ngs, not simply inversions among neighbors, and suggest that the eva luation schemes have different emphases . works including Sanderson &amp; Joho (2004), Carterette &amp; Allan (2005), Yilmaz &amp; Aslam (2006) and subsequent papers from Voorhees (e.g., Buckley &amp; Voor hees, 2004). Other works (e.g. Lee et al, 2002) also treat coefficients over 0.9 as important though without explicit reference to Voorhees X  X  work. However, as pointed out by Bland and Altman (1986, p.308) sample. If this is wide, the corre lation will be greater than if it is narrow . rank runs, if the range of scores assigned to each of the runs (being ranked) is wide,  X  will tend to have a higher coefficient than if the range of scores is narrow. Such qualities of correlation coefficients have long been know n about, what is less well known is how much variation will occur when using  X  to compare test collections. If only small variations in  X  are found, there may be no problem worth considering. This poster presents the design and results of an experiment that tests how much correlation varies given sets of runs with different score ranges. The implications of the results are discussed along with avenues for future work. the run data for the adhoc track of TRECs 6-8 and the web track of TREC 9 were downloaded from the TREC web site. For each of the years of TREC, automatic runs 1 were ranked against each other using full TREC relevance judgments and using judgments formed from the top 100 relevant documents retrieved for each topic in the 25% best performi ng manual runs (similar to the experiment described in Sande rson &amp; Joho, 2004). Runs were Automatic runs were those r uns where a topic was processed fully automatically by a retrieval system. ranked using Mean Average Precision (MAP). The  X  between the two ranks of runs in each year of TREC was measured. The following table shows this value in each of the four TRECs as well as the number of automa tic and manual runs used. TREC Manual runs Automatic runs  X  on full run set from the output of a few manual runs ranks automatic runs similarly to the full TREC rele vance judgments. To measure the  X  of runs over a smaller score range, the automatic runs in each year of TREC were sorted by their MAP and split in half: top 50% runs and bottom 50% runs. The table below shows  X  for each of these reduced score range sets. As can be seen,  X  is either the same or less than  X  measured on the full set and for the top 50% of runs on TREC-9,  X  is below the 0.9 threshold. TREC full run set Top 50% Bot. 50% Av. the score range: results are shown in the table below. With the exception of two values in the sixteen shown,  X  is lower again. In addition, the average  X  of the 25% sized runs is always lower than the  X  for the full runs and all four averages are under the threshold of 0.9. If one were to judge qr els based on runs with such a reduced score range instead of one conveying a fuller range, one might conclude that using a few manual runs to form relevance judgments is potentially probl ematic; the opposite conclusion drawn from the results shown in the first Table. TREC 100% Top (with the exception of data fro m TREC-7) is lower than the  X  from the bottom 25% of automatic runs. It is tempting to think that this result is showing that top performi ng automatic runs are harder to rank than bottom performing runs. However, one cannot say this with certainty as the range of sc ore values in the top 25% run set is different from the bottom 25%: at this early stage of our research we have not been able to estimate the degree of influence substantially affect  X  . definitively that score ranges are influencing  X  , as when the run sets are halved or quartered, two variables in the experiment are changed: the score range and cardinality of the sets. We eliminated the possibility that set si ze is the cause of the change in  X  with a second experiment (not s hown due to lack of space). In it, reduced run sets were formed by random selection of runs from a full set. Repeating such experiments multiple times, it was found that on average  X  measured on these sma ller run sets (which on average had the same score range of the full sets) was the same as the  X  measured on the full set. Fr om this we concluded that  X  was not affected by set size, but by the range of scores across the runs composing a set. From the results, we conclude that using thresholds for correlation coefficients when comparing test collections is potentially problematic. The variation of  X  due to changes in the range of scores in run sets can be so large that coefficients measured on sets with narrow score ranges can be substantially different from coefficients measured on sets with wider scores. It would appear that absolute thresholds used with  X  should be applied with great care, or preferably avoided. test collections with each other is work left for future consideration. In addition we be lieve that it will be worthwhile exploring in more detail the use of  X  in past information retrieval research and examining if the observed variations in  X  (shown here) have unknowingly influenced published results. Thanks to Andrew Holmes who pointed us to an important reference that led us to conduct th is investigation. This work was supported in part by the Tripod project: IST-FP6-045335. Bland, J.M., Altman, D.G. ( 1986) Statistical methods for Buckley, C. &amp; Voorhees, E.M. (2005) Retrieval evaluation with Carterette, B. &amp; Allan, J. (2005) Incremental test collections, Kendall, M. (1938) A New M easure of Rank Correlation, Lee, S., Myaeng, S.H., Kim, H., Se o, J.H., Lee, B., Cho, S. (2002) Sanderson, M. &amp; Joho, H. (2004) Fo rming test collections with no Voorhees, E.M. (1998) Variations in relevance judgments and the Voorhees, E.M. (2001) Evaluation by highly relevant documents, Yilmaz, E. &amp; Aslam, J.A. ( 2006) Estimating Average Precision Yom-Tov, E, Fine, S., Carmel, D ., Darlow, A. (2005) Learning to 
