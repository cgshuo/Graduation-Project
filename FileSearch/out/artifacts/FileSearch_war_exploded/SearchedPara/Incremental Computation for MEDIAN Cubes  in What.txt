 provide important predicting information for DSS users. What-if analysis also enables the evaluation of past performance and the estimation of the opportunity cost taken by not following alternative policies in the past. For example: company? enterprise? Q3: if the total profit need to be increased 10%, how to adjust the product structure of enterprise? 
According to the object of hypothetical update, the what-if analysis in OLAP can be classified into three categories: 
The what-if analysis based on the hypothetical schema update was studied by [1][2][3]. [4]analyzed the types of dimension updates, and the method of maintaining update[7]. It analyzes the influence of the change of aggregation value on the base data from top to bottom. 
The implementation approaches of what-if analysis(what-if query) are divided into pression is transformed into an equivalent explicit substitution, and then applying these approach stores the hypothetical update expression, but not hypothetical update data. The what-if query implemented by  X  X azy X  approach is transformed into the queries on the fact data[5]. In the  X  X ager X  approach, the hypothetical update expression is mate-rialized and stored into the system to form an assumed scenario, and then the what-if thetical update can not be presented by update expression, so it can not be adopted in some situations. 
The data cube is a convenient way to exhibit multi-dimension aggregating views in data warehouse and OLAP system. To speedup the processing of query, some cubes are materialized in data warehouse and OLAP system. As assumed changes are made to the schema, dimension or fact table, a new cube, which has same aggregate function, di-mension and measure attributes as the materialized cube, must be updated to reflect the scratch. It also can be incrementally computed by calculate the changes to the views due to the source changes. The delta cube[6] was proposed to incrementally compute the new cube. By the method, the procedure of incremental computation of a cube view is divided into two stages: propagate and refresh. In the propagate stage, a delta cube  X  function as the cube view. In the refresh stage, the cube view was refreshed by applying incremental computing, such as SUM, COUNT, etc.. The delta cube method belongs to the  X  X ager X  method. 
The aggregate functions are divided into two classes: self-maintainable and non-self-maintainable[6]. A set of aggregate functions are self-maintainable if the new function. It can not be holistic function. Therefore, the delta cube method can not be used directly to incrementally compute a cube with holistic aggregation functions such as MEDIAN and QUANTILE. In this paper, we propose a solution for the incremental computation of a cube with MEDIAN aggregation function. 
This paper is organized as following: Section 2 will discuss the related work. Section 3 introduces the background information of our study. Then, in section 4, we will describe formance experiments are given in section 5. Finally, we conclude our work in section 6. An d -dimension data cube contains 2 d cuboids, which come from the same fact table, with the same aggregate function and different group-bys. Because the computation of data cube is a time-consuming work, many research works have been done to improve the efficiency of data cube computation. In data warehouse and OLAP system, some cuboids are materialized to improve the query performance. When the data of fact table changed, the materialized cuboids must be updated to reflect the changed state of the data source. 
There are two strategies to re-compute the data cube: (1) discarding the materialized puting the new cuboids based on the original materialized cuboids. The advantage of make full use of existing resources. The re-computation of data cube consumes a lot of system resources. The advantage of second strategy is that it can make full use of ex-isting cuboids, saving system resources. However, it is fit to distributive and algebraic aggregate functions only. 
In what-if analysis, the materialized cube must be updated to reflect the hypothetical thetical dimension update is under research by [4][9][10][11]. They do not consider the maintenance of data cube under hypothetical fact table update. Both of them adopt delta cube method to incrementally compute data cube under fact table update. [8]proposes using only delta cuboids. Because the amount of delta cuboids is much smaller, the cost of computing delta cuboids is substantially reduced. The algorithm does not fit to in-crementally maintain a cube with holistic function. The work area technique is used in [12] and [13]. But, they do not discuss the incremental maintenance of cube. [15] in-troduces the methods of efficient computation of iceberg cubes with complex MEDIAN. But, it does not discuss the deletion operations. This section provides the necessary background for discussion in the rest of this paper. In section 3.1, we define some notations about what-if analysis, and discuss the storage of what-if data in section 3.2. Section 3.3 introduces the conception about data cube. 3.1 Notation Definitions In this section, we introduce some general notations about what-if analysis. Definition 1. base table: A base table T B is the object of what-if operation. In OLAP system, T B may be fact table, view or dimension table. Upd}}. Where, OP A is an atomic hypothetical update operation. It may be insert(Ins), delete(Del) or update(Upd). That is, a what-if operation is a set of atomic hypothetical update operations. In what-if analysis, there may be many what-if operations based on the same base table. What-if operation may base on tables or what-if views. Definition 3. what-if data: The data produced by what-if operation is named what-if data D wi . Definition 4. what-if view: The merging of what-if update data D wi with its base table T forms a what-if view, denoted as VI , then VI = T B  X  D wi . 3.2 Storage of What-If Data based on what-if operations does not exist. Therefore, these what-if update data should not change the real data in the data warehouse and OLAP system. Delta table is adopted to store what-if update data. That is, what-if update data are stored into one or more delta tables which are independent to real f act table. The what-if queries are processed on the union of the base table and delta table. 
This paper uses two types of delta tables to store what-if data. One is insert_delta hypothetical conditions are transformed into deletions and insertions: deleting the old data firstly, inserting the new data secondly, then storing the data of hypothetical de-letions into delete_delta table and the data of hypothetical insertions into insert_delta table. 3.3 Data Cube In data warehouse and OLAP applications, it is often necessary to analyze the data in fact table based on various combinations of dimension attributes. The DATA CUBE operator supports such analysis in data warehouse and OLAP. In a CUBE, attributes are categorized into dimension attributes, on which grouping operation may be performed, a single SELECT-FROM-WHERE-GROUP BY block, having identical aggregate functions, identical FROM and WHERE clauses, and one of the 2 d subsets of the di-mension attributes (level i indicates the level of cuboid). The what-if analysis is based on what-if view. Generally, the what-if view VI is com-putation the new view with the materialized view, T B and D wi . In this section, we dis-cuss the incremental computation of MEDIAN cube. 4.1 Work Area values of the n values is same. When inserting new tuple into the set or deleting tuple the new median value can be got from the m tuple in the work area when the number of tuples of inserting or deleting is smaller than m . puted as following when the set is updated: 1. Let t med represent the position of the median of the n tuples of S. We maintain a 2. When deleting a tuple t from the set S, we compute the new t med of S according the (1) If t &lt; t low , then t med moves 1/2 position to the left. (2) If t &gt; t high , then t med moves 1/2 position to the right. 4.2 Computing Delta Cube The paper[6] proposed delta cube method to incrementally maintain data cube. Delta cube has the same aggregation function, meas ure attributes and dimension attributes as the source data cube. For example, if the aggregation function of source data cube is SUM, then the aggregation function of the delta cube is SUM also. The new data cube can be computed by merging the source data cube and the delta cube. self-maintainable function with respect to insertions and deletions. If the MEDIAN is used to compute the delta cube, then merging the delta cube and the source data cube can not get the correct median value of the new cube. Therefore, the MEDIAN can not be used to compute the delta cube. According to the analysis of work area in 4.1, for a group(tuple or member) in a cuboid, if we have known the number of tuples which is larger than the median of the group in the source cube, and the number of tuples which median value of the group from the tuples in the work area. The tuples that is between the work area, we build a work area for each group of cuboids in the cube to store the tuples between the t low and t high . 
Therefore, the following values need to be computed when computing the delta cube: (1) NUMOFSMALL: recording the number of tuples which is smaller than the median (2) NUMOFLARGE: recording the number of tuples which is larger than the median of (3) NUMOFEQUAL: recording the number of tu ples which is equal to the median of (4) WORKAREA: storing the tuples between t low and t high of delta data. For each cuboid of the delta cube, we can compute it by the algorithm 1(as shown in Figure 2). dimension attributes; the same grouping attributes as the datagroup. cuboid . area for the group G. 4.3 Incremental Computing the MEDIAN Cube In this section, we illustrate how to compute the new MEDIAN cube using the source data cube and the delta cube. In the propagate stage, we have computed two delta cubes, one for delta insertion tuples, and the other for delta deletion tuples. Now, we compute the new MEDIAN cube using the source data cube and the two delta cubes. The new MEDIAN cube is computed from top to down. For each group in each cuboid in the source cube, we compute the new median value according to the following steps. 1. Merging the work areas: There are three work areas for each group G of cuboid, for the source data cube, delta respectively.). (1) If the number of tuples of which the value is equal to the median in W d is larger W . The two variables represent the number of tuples which are located on the left of t the deleted tuples that are equal to the median. (2) If there are some tuples of which the value is equal to the median in the W i , then inserting them into those positions which are located on the left of t med . 2. computing the new median We compute the new median accord ing to the following criteria: (1) If the old t med is deleted, then the t med moves 1/2 position to the left. (2) If NUMOFLARGE i + NUMOFSMALL d + Equal left = NUMOFLARGE d + NU-(2) If NUMOFLARGE i + NUMOFSMALL d + Equal left &lt; NUMOFLARGE d + NU-(3) If NUMOFLARGE i + NUMOFSMALL d + Equal left &gt; NUMOFLARGE d + NU-The algorithm for incremental computing a cuboid is shown in figure 3. 
A lgorithm 2 Newcuboid(delta D cuboid, delta I cuboid, s_cuboid)
Input: delta_D_cuboid: a cuboid computed from the delta delete data. delta_I_cuboid: a cuboid computed from the delta insert data. s_cuboid: a cuboid of the source data cube and its grouping attributes are same as the grouping attributes of delta_D_cuboid and delta_I_cuboid. 
Output: new_cuboid 1. Begin 2. For each group G of delta_D_cuboid and delta_I_cuboid 3. Get work area W d and W i from G d and G i respectively; 4. Get work area W s , t med , t low and t high from the corresponding 6. if t med has been deleted, then t med moves 1/2 position to 7. if t med is still in the work area and has not been deleted, 9. compute the new t med from base table and the delta table. 10. end for; 11. end. 
In the algorithm, we only re-compute the median of the affected groups. It can re-W a new position which is out of the range of W s , then it needs to be computed from the base table and the delta table. Because we incrementally compute the new cube from top to down, the sorting and grouping results of the merging result of base table and the delta table, which is completed when computing the higher level cuboid, can be used to compute the lower level cuboid. It avoids the repeated merging and sort-group opera-tions of the base table and the delta table. 4.4 Optimizing the Size of Work Area To a great extent, the size of work area affects the performance of the algorithm. If the size of work area is very small, then the probability of the new median occurring in the work area is decreased which increases the probability for re-computing the new me-effectively. But, it increases the cost for management of the work area. When the in-creased cost of management is larger, although the cost of re-computation is reduced, the performance of algorithm may decrease. 
The cuboids of a data cube implement aggregate operations based on different di-the cuboids of lower level. Therefore, they have different size requirements of the work mension attributes. The size of work area can be defined by the formula (1): value or a function. DIAN cube with fixed size of the work area(denoted as In-com-FS), and the incremental computation algorithm for the MEDIAN cube with variable size of the work area(denoted as In-com-VS). In the experiments, the number of dimension attributes of each tuple is 4, carried out on a PC with an Intel Pentium 3GHz CPU and 1GB main memory. 
We compare the performance of Re-com and In-com-FS firstly. The result is shown in figure 4. In this experiment, the number of tuples is 2,000,000, and the update ratio varies from 4% to 20%. Figure 4 shows that the time taken by the In-com-FS algorithm creasing, the gap between then In-com-FS and Re-com algorithm is becoming small. 
Next, we compare the In-com-FS and In-com-VS algorithm, In this experiment, the the In-com-FS algorithm. 
Figure 6 shows that the result of the performance evaluation when we increase the size of the base table. From the result, we can see that the In-com-VS algorithm can save about 50% in processing time. Hence, the In-com-VS algorithm can be effectively used for fast incremental maintenance of cube views. 
In all experiments, with the update ratio increasing, the gap between the incremental computation and complete re-computation becomes small. Therefore, the incremental computation fit to be adopted with lower update ratio. If update ratio is very high, the complete re-computation should be adopted. Incremental computation of data cube is an extremely important aspect of data ware-house and OLAP system under hypothetical scenarios. However, the incremental paper addresses the problem of incrementally maintaining the existing MEDIAN cube. We adopt the delta cube strategy and work area technique to incrementally compute the MEDIAN cube under the what-if operations, and we optimize the algorithm by varing the size of the work area with the number and the cardinality of dimension attributes of the cuboid. Through performance evaluation, we can draw a conclusion that the effi-ciency of the proposed algorithm is superior to the complete re-computation method, and the efficiency of the In-com-VS algorith m is superior to the In-com-FS algorithm. 
