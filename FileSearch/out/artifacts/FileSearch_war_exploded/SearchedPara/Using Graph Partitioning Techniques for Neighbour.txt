 Spectral clustering techniques have become one of the most popular clustering algorithms, mainly because of their sim-plicity and effectiveness. In this work, we make use of one of these techniques, Normalised Cut, in order to derive a cluster-based collaborative filtering algorithm which outper-forms other standard techniques in the state-of-the-art in terms of ranking precision. We frame this technique as a method for neighbour selection, and we show its effective-ness when compared with other cluster-based methods. Fur-thermore, the performance of our method could be improved if standard similarity metrics  X  such as Pearson X  X  correlation  X  are also used when predicting the user X  X  preferences. H.3.3 [ Information Search and Retrieval ]: Information Filtering, Clustering Algorithms, Experimentation, Performance Recommender systems, Collaborative Filtering, Clustering, Normalised Cut
Collaborative Filtering (CF) is a particularly successful form of personalized Information Retrieval which suggests interesting items to users based on the preferences from similar-minded people [4, 9]. In CF, the most common form of ground user preference evidence consists of ratings, which are explicit relevance values given by users to items of in-terest. CF algorithms exploit the active user X  X  ratings to make predictions, and thus it has the interesting property that no item descriptions are needed to provide recommen-dations, since it merely exploits information about past rat-ings between users and items. Moreover, it has the salient advantage that a user benefits from others X  experience, be-ing exposed to novel recommendations with respect to the latters X  personal preferences. Note that this cannot be pro-duced in general by other (content-based) approaches that tend to reproduce the user X  X  past, insofar as they examine the preferences of individual users in isolation [1].
Collaborative filtering approaches can be classified into two main categories: model-based approaches and memory-based approaches. Model-based approaches learn user/item rating patterns to build statistical models that provide rat-ing estimations. Memory-based approaches, on the other hand, compute user/item similarities based on distance and correlation metrics [3], and use these similarities to find similar-minded people of the active user. These people are usually called neighbours, and their preferences are used to predict ratings for the active user.

Memory-based CF algorithms are based on the principle that a particular user X  X  rating records are not equally use-ful to all other users as input to provide them with item suggestions [4]. Thus, central aspects in these algorithms are how to identify which neighbours form the best basis to generate item recommendations for the active user, and how to properly account for the information provided by them. Typically, neighbourhood identification is based on selecting those users who are most similar to the active user according to a certain similarity metric. In this context, the similarity of two users generally consists of finding a set of items that both users have interacted with, and examining to what de-gree the users displayed similar behaviours on these items.
Once the active user X  X  neighbours are identified, the more similar a neighbour is to the active user, the more her prefer-ences are taken into account as input to make up recommen-dations. For instance, a common memory-based approach consists of predicting the relevance of an item for the ac-tive user by a linear combination of her neighbours X  ratings, which are weighted by the similarity between each neighbour and the user. It is also a common practice to set a similarity threshold (or a maximum number of most similar users) to restrict the set of neighbours, in order to avoid the noisy disruption of long tails of dissimilar users. An instantiation of this algorithm can be formulated as follows [10]:
This method establishes that the preference of a user u for a particular unseen item i is given by a numeric rating r ( u, i ) estimated in the form of  X  r ( u, i ). To provide that estimation, the method takes into account the ratings r ( v, i ) provided by the k users v who are most similar to u , usually called neighbourhood and denoted here as N k ( u, i ). The function sim( u, v ) measures the similarity between two users u and v , and the constant C is a normalization factor. Thus, the predicted rating of user u for item i is computed over the average rating  X  r ( u ) (this step is not always required [1]), and the sum of u  X  X  similarities with her neighbours v , weighted by the deviations of v  X  X  ratings for i and average ratings  X  r ( v ). The item-based algorithm can be described analogously [11].
Different ways for building the neighbourhood N k ( u, i ) have been proposed: some authors use the concept of trust by selecting only the most trustworthy users with respect to some trust metric [9]; other authors split the set of users or items in order to improve the scalability of the recom-mender systems and their accuracy [8, 15]. Most of the latter approaches use old-fashioned clustering methods such as k-Means or hierarchical clustering. Furthermore, in some situations, external information is used for the data parti-tion, such as the content of the item (genres or tags, in the movie domain).

Cluster algorithms for neighbour selection in CF have not been widely exploited. The few existing approaches [15] pro-duced good results but at the expenses of lower coverage. In this paper, we propose a clustering method which has shown good empirical performance properties in the fields of Infor-mation Retrieval and Data Mining and apply it to Recom-mender Systems. In this process, we provide a general for-mulation for cluster-based CF methods, framed as neighbour selection methods. We report empirical results confirming where our method outperforms both standard CF methods and other cluster-based algorithms, with the additional ad-vantage that we rely exclusively on data extracted from the rating matrix, that is, no external information is used.
Spectral clustering algorithms [14] use graph spectral tech-niques to tackle the clustering problem transforming it into a graph cut problem. The dataset to be clustered is typically represented as a weighted graph, G = ( V, E, W ), where V is the set of objects to cluster, E is the set of edges between objects, and W denotes a diagonal matrix whose elements are the weights e ij between vertices v i , v j  X  V . The Nor-malised Cut ( N Cut ) value of a certain cut (a partition of V ) of a given graph was introduced by [13]. For a certain cut w = { A 1 , A 2 , ...A k } of a graph G , N Cut is defined as: where here, A 1 to A k are the connected components (ideally, the clusters) in which the graph has been divided and  X  A i are the vertices which are not included in A i .

A graph cut with a low N Cut would represent a cut of the graph in which the weights of the edges which join vertices in different connected components are as low as possible while keeping the volumes of the resulting connected components as high as possible. This last condition ensures a certain balance between the connected components, trying to avoid trivial solutions. So, a cut of G with a low N Cut would correspond to a good clustering of the data.

The minimisation of N Cut can be presented as a matrix trace minimisation problem [14]. Let H = ( h i,j ) be a n  X  k matrix which will be used to encode the membership of data points to the connected components. The j th column of H contains the membership of connected component A j (the indicator vector) encoded as follows:
Also, let D be a n  X  n diagonal matrix such that d i,i = degree( v i ) = P n j =1 w ij and let L be the Laplacian matrix of graph G (that is, L = D  X  W ). Using these matrices, it can be shown that the minimisation of NCut can be written as in Eq. (5)
It can be demonstrated that the condition of discreteness on the values of H makes the minimisation problem NP-Hard. If this discreteness of the values of H is relaxed, allowing the indicator columns composing that matrix to have any value in R n , and the substitution Y = D 1 performed, the expression in Eq. (6) is obtained:
The above equation is in the standard form of a trace min-imisation problem. Therefore, it can be demonstrated that this equation is minimised by the matrix Y which contains as columns the eigenvectors corresponding to the smallest
Due to the relaxation of the condition of discreteness of the values of H to reduce the complexity of the problem and make it computationally affordable, instead of having an in-dicator vector for each connected component, we would have a vector in R k for each datapoint (the rows of matrix Y ). Thus, we have a projection of each data instance in R k based on its similarity to the other instances. Hence, some other technique (such as k-Means) should be used to find a dis-crete segmentation of this space. Once this segmentation has been performed, we can backtrace each projected data-point to the original one, obtaining the final outcome of the clustering algorithm.
The use of N Cut in the recommendation process is pro-posed as a tool for neighbour selection. Now, we focus on the methods derived when user clusters are used, alternatively, item clusters could be used in a straightforward way and the problem would be casted as an item-based CF method. Hence, in a user-based method, the users will play the role of the nodes of the graph to cut. A good set of neighbours for each user in the collection would be obtained by finding a good N Cut of the graph. Besides, in order to perform the complete process of recommendation, we also need to establish a procedure for weighting the edges in the graph ( e ij ), i.e. the distances among users. We decided to adopt the well-known  X  and traditionally used  X  Pearson X  X  corre-lation similarity. Thus, for each cluster, we obtain a list of users belonging to such cluster. Then, we build a recom-mender which predicts the rating for user u and item i in the following way: where N C ( u ) outputs the elements who belong to the same cluster as the target user u , sim( u, e ) represents the simi-larity between the element e and the current user; finally, r ( e, i ) is the rating given by user e to item i .
Thus, we summarise our neighbour selection problem for a user-based method as follows. We use the Normalised Cut algorithm to create different user clusterings. Then, we use the information generated by this clustering for the neighbourhood formation, such that for each user u , we find the cluster c u that user belongs to, then, the rest of the users belonging to c u , are selected as the potential set of u  X  X  neighbours.
In order to empirically compare whether the clusters gen-erated by the Normalised Cut technique are able to enhance standard collaborative recommendations, we have performed the following experiments. Firstly, we have plugged those clusters into a standard user-based CF method and com-pared their performance against some well-known state-of-the-art recommenders: a user-based CF with Pearson X  X  cor-relation as similarity measure (UB [10]) and a matrix fac-torization algorithm with a latent space of dimension 50 (MF [6]). Secondly, in order to show whether the improve-ment comes from using a cluster-based method or the spe-cific technique that we propose, we compare the performance of our technique against a standard clustering method (k-Means [7, 15]).

These experiments have been carried out using the pub-licly available dataset called Movielens 100K 1 . This dataset contains 943 users, 1 , 682 items and 100 , 000 ratings. We performed a 5-fold cross validation using the splits contained in the public package, these splits retain the 80% of the data for training, and the rest for testing. The methodology used in the evaluation corresponds to the TestItems approach de-scribed in [2], although alternative methodologies (such as the one described by Koren in [6]) have also been evaluated and similar results were obtained. More specifically, the TestItems methodology generates for each user a ranking by predicting a score for every item in the test set. Then, the performance of this ranking is measured using, for instance, the trec eval program 2 . In this way, standard IR metrics such as precision, normalised Discounted Cumulative Gain (nDCG) or Mean Reciprocal Rank could be used.
In order to asses our technique X  X  ability to select good neighbours, we evaluate the NCut clustering technique (de-noted as NC+P, since Pearson X  X  correlation is used as simi-
Available at http://www.grouplens.org/node/73 Available at http://trec.nist.gov/trec eval/ Table 1: Results for cluster-based algorithm using Normalised Cut (NC+P). In brackets, the number of eigenvectors used in the Normalised Cut for each k . Statistical significant improvements according to Wilcoxon Test ( p &lt; 0 . 05 ) w.r.t. MF and UB are superscripted with m and u respectively, best values are bolded.
 larity) by the Equation 7. We compared it with the standard UB method where the neighbourhood is selected among the set of most similar users. Besides, to put our results in per-spective, we also include a well-known method which does not use any neighbour selection and which is typically among the best performing recommenders (MF method).

The results are presented in Table 1, where different values for the size of the cluster ( k ) have been evaluated against the baselines. In fact, the quality of the results of the spectral methods improve considerably if, in the projection phase, instead of taking only the first k eigenvectors, a higher num-ber of vectors ( d ) is used [5], for this reason we have taken higher number of eigenvectors (specified between brackets). Due to space constraints, only the results for a specific cut-off ( N = 5) are reported, but other cut-offs and metrics such as P@50 and nDCG@50 were also tested and a similar trend was observed. Here, we show that cluster-based meth-ods outperform baseline methods for specific values of the cluster size, namely k = 150 and 200, and these improve-ments are statistically significant. In fact only MF and in only one occasion (k=50) achieved statistical significant im-provements over our approach. Furthermore, a larger num-ber of clusters does not necessarily mean a better perfor-mance, probably because most of the clusters would contain zero or one element when this number is very large.
More importantly, a larger number of clusters tends to produce lower coverage values 3 , since the information avail-able to the recommender is not enough, which would pro-duce a well performing method, but a not very interesting one, since it would only be able to suggest items  X  for some configurations  X  to less than the 30% of the system X  X  popu-lation.
In this experiment, we analyse the recommendation per-formance variations that result when different cluster sizes
We follow the definition given in [12] of user space cover-age , that is, the number of users for which the system can recommend items. are used in a cluster-based recommender method. More specifically, we compare our NCut method with a state-of-the-art algorithm where the clusters are generated using the k-Means method [15]. Furthermore, since our method has shown good performance once it has been plugged in the standard formulation of user-based CF (previous section), now we also evaluate its performance when no information about the user-neighbour similarity is used.

Figure 1 summarises our results. We denote as NC when the cluster-based recommender with w 1 ( u, e ) = 1 and the NCut clustering technique are used, as in the previous sec-tion, when this technique is used in combination with the standard Pearson X  X  similarity coefficient, we denoted it as NC+P . Similarly, kM and kM+P denote the corresponding k-Means cluster-based methods when a constant similarity w 1 = 1 and Pearson values are used, respectively.
 Figure 1: Performance obtained (P@5) between the baseline methods (standard CF and cluster-based using k-Means alone or in combination with Pear-son X  X  similarity: kM and kM+P ), the NCut method without a similarity function ( NC ), and the combi-nation of a Pearson X  X  similarity function and NCut as the neighbour selection method ( NC+P ). Cover-age values are plotted along the secondary axis.
We can observe in the figure, first, that the coverage of both cluster-based methods is very similar, although our method obtains improvements up to 15% when less than 300 clusters are used. Interestingly enough, the performance of our technique reaches a maximum also at that point, whereas the k-Means cluster-based recommender shows a flat performance along the different number of clusters eval-uated. It should be noted also that the performance of our method when no similarity information is used (curve NC ) is comparable to that of NC+P , and thus, it becomes apparent that our method alone is improving the way the neighbours are being selected, which leads to a major performance en-hancement.
In this paper, we have presented a new approach for neigh-bour selection in Recommender Systems. N Cut is a clus-tering method based on graph partitioning which is natu-rally applied to the graph essence of the recommender sys-tems communities. We tested two approaches of exploit-ing the neighbourhoods determined by the algorithm and both of them greatly improved the performance of standard CF methods. Furthermore, when compared against other cluster-based methods, our method obtained large improve-ments in performance, while the coverage was comparable to that of those methods.

Although our approach is general enough to work for user-and item-based CF methods, we have decided to focus on user-based methods since they provided better performance in initial experiments, but we aim to further investigate in the future also item clusters along with item-based ap-proaches. We would also explore the behaviour of our pro-posal when working in larger datasets and the use of other spectral clustering techniques. Besides, we have observed that cluster-based methods tend to reduce the coverage of the system at the expense of its performance, hence, we aim to investigate alternative clustering methods which do not suffer from this behaviour.
This work is supported by the Spanish Government (TIN2011-28538-C02-01), and the Government of Madrid (S2009TIC-1542).
