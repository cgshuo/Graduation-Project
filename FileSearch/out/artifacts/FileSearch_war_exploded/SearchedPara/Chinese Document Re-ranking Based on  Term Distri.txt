 How to further improve the rankings of the relevant documents after an initial search streams: automatic query expansion and automatic document re-ranking. While the assumption behind automatic query expansion is that the high ranked documents are likely to be relevant so that the terms in these documents can be used to augment the the rankings by re-ordering the position of initial retrieved documents without doing a second search. After document re-ranking, it X  X  expected that more relevant documents appear in higher rankings, from which automatic query expansion can benefit. proposes a document re-ranking method based on document clusters [6]. They build a hierarchical cluster structure for the whole document set, and use the structure to re-rank the documents. Balinski J. et al. proposes a document re-ranking method that uses the distances between documents for modifying initial relevance weights [1]. Crouch et al. uses the un-stemmed words in queries to re-order documents [3]. Xu et al. makes use of global and local information to do local context analysis and then use the information acquired to re-rank documents [12, 13]. Qu et al. uses manually built thesaurus to re-rank retrieved documents [10], and each term in a query topic is grammars for topics to re-order documents by matching grammar rules in some segment in articles [2]. Kamps proposes a re-ranking method based on assigned controlled vocabularies [4]. Yang et al. [14, 15] use query terms which occur in both query and top N ( N &lt;=30) retrieved documents to re-rank documents. One problem in automatic document re-ranking (also for query expansion) is how which is also faced by most methods mentioned above [3, 4, 6, 7, 14, 15]. Usually, a pre-defined smaller number of the documents (say top 10 to 30) are considered. many irrelevant documents will come inside, and the noisy terms will dominate. 
Another problem is that most methods mentioned above don X  X  consider correlation between query terms. Mitra et al. [8] uses Maximal Marginal Relevance (MMR) to adjust the contribution of relevant terms. They argue that usually a document covering more aspects of a query should get higher score, which can be captured somehow by word correlation. The new score for a document is computed by summing the idf (inverse document frequency) of each query word where each word is normalized by correlation probability based on a large number of retrieved documents (say top 1000 documents). It X  X  reported that their method achieves better result in re-ranking top 50 to 100 documents. But we find that within top initially retrieved documents, some unexpectedly assigned lower scores by idf scheme. In this paper, we propose a new term weighting scheme to deal with the two problems mentioned above. First, we consider document rankings, i.e., document positions in the ranking list, in the weighting scheme of the terms. Intuitively, a term gets a lower document frequency when occurring in a lower-ranking document, and a higher document frequency when occurring in a higher-ranking document (in matter where the document is located in the list). In this way, we can randomly choose a larger number of the documents as relevance feedback, without any worry about the irrelevant documents inside. Furthermore, we don X  X  need to worry about the cases that top documents only contain very few relevant documents, since we can randomly set a larger scope as relevance feedback. 
Second, the weighting scheme incorporates both local (feedback) and global occurs in feedback documents more frequently than in the whole collection, it tends to have more contribution to document re-ranking; otherwise, it will be a noise. 
Our method doesn X  X  use word but uses the key terms extracted from queries and top retrieved documents. One motivation of this choice is that terms (including multi-word units) usually contain more complete information than individual words, and have more potential for improving the performance of information retrieval. Another where a word segmentation module is usually needed, which, however, generally requires some manual resources and suffers from the problem of portability. An automatic term extraction module could be a good alternative. 
The rest of this paper is organized as the following. In section 2, we describe key term extraction from documents. In section 3, we talk about term weighting. In section 4, we specify how to re-rank the documents based on the key terms and their weighting together with MMR based on term correlation. In section 5, we evaluate the method on NTCIR3 CLIR dataset and give some analysis. In section 6, we present the conclusion and future work. document are some word strings which are conceptually prominent in the document and play main roles in discriminating the document from other documents. We use a seeding-and-expansion mechanism to extract key terms from documents. language), seed positioning is to locate the rough position of a term in the text, while forms a term. 
To determine a seed needs to weigh individual Chinese characters to reflect their Chinese News Text) as a reference corpus. Suppose d is a document, c is an with respect to r [11], as the criteria for evaluation of seeds. corpus. definition about a key term in a document. and ii) while no other longer word strings containing it meet i) and ii). A real maximal substring meeting i) and ii) refer to a real su bstring meeting i) and ii) while no other longer real substrings containing it meet i) and ii). 
The above assumptions tell us a term is an independent maximal string which must contain a seed and occur at least 2 times in a document. For example, given a document d , suppose a Chinese character (bo3) is a seed in d , (National Palace Museum) occurs 3 times in d , (Museum) occurs 5 times in d , if we set the parameter L as 2, then both string (National Palace then (National Palace Museum) is term in d , but (Museum) is not a term in d because its independent occurrence is 2 (excluding 3 occurrences as a sub-string in (National Palace Museum)). To re-rank retrieved documents, we use the key terms in the documents, and suppose terms which also occur in the queries, which means that we don X  X  use any query term, we consider the following three factors. whole collection, the more important the term tends to be. have. Intuitively, the higher ranking a document is, the more important the terms in it tend to be. given by the following formula. document frequency weighting schemes used in our experiments. independently concepts. In other words, we need to distinguish multiple query-document matches: matches on query terms related to the same aspect, or matches on query concepts should be considered more useful than a match on two strongly related query terms. 
We use term correlation to measure the relatedness or independence of query terms and use MMR criteria to reduce redundancy of query terms while maintaining query relevance in re-ranking retrieved documents. 
To estimate the relatedness or independence of query terms, we study their co-correlated, then they are expected to occur together in many of these documents. Given the presence of one of the query terms in a document, the chance of the other occurring within the same document is likely to be relatively high. On the other hand, if two query terms deal with independent concepts, the occurrences of the query terms as P( t i | t j ): out the query terms which occur in d , then we consider the matching query terms in decreasing order of query term weight. The first matching query term contributes its how strongly this match was predicted by a previous match  X  if a matching query proportionately down-weighted. Finally, we use Weight new and the initial similarity between d and query q to calculate a new ranking score and then use the new ranking terms presented in document d (ordered by decreasing query term weight), then Weight new and Score new are given by: where Sim old is the original similarity value between document d and query q in initial retrieval, w(t i ) is the weight of query term t i . The top M retrieved documents are re-ordered by their new ranking score Score new . Figure 1 gives out the pseudo code of the procedure of document re-ordering for query q and top M retrieved documents. 
Given q is a query, K is the number of top initial retrieved documents from which to collect term correlation and query term weighting, and M (M&lt;=K) is the number of retrieved documents to be re-ordered in initial retrieval. Step 1: Acquiring query terms in q and their weights; Step 2: Acquiring query term correlation from top K retrieved documents; Step 3: Re-order top M documents; We use NTCIR3 CLIR dataset as our test dataset to re-rank top 1000 retrieved documents. The dataset contains Chinese document set CIRB011 (132,173 documents) and CIRB20 (249,508 documents). We use the officially released 42 Chinese-Chinese D-run query topics in NTCIR3 CLIR as query topics where each query is a short description of a topic by Chinese language. Chinese sentence is a contiguous Chinese character sequence without space between Chinese words. Chinese Character, bi-gram, n-gram (n&gt;2) and word are the most widely used indexing units in Chinese information retrieval. The comparison between the three kinds of indexing units (single Characters, bi-grams and short-words) is given in [5]. It shows that single character indexing is good but not good as short-word indexing in precision. [9] suggests that word indexing and bi-gram indexing can achieve comparable performance but if we consider the time and space factors, it is preferable to use words (and characters) as indexes. 
In our experiments, for initial retrieval, we use bi-gram as index unit. We use vector space model as retrieval model. The initial retrieval result is used as baseline. 
We use NTCIR3 X  X  relax relevance judgment and rigid relevance judgment to measure the precision of retrieved documents. Relax relevance judgment considers highly relevant, relevant and partially relevant documents, while rigid relevance judgment only considers highly relevant and relevant documents. We use PreAt10 and PreAt100 to represent the precision of top 10 retrieved documents and top 100 rigid relevance judgment respectively. vector space where each dimension of the vector is a bi-gram. The weight of bi-gram b in document d is given by the following tf X idf weighting scheme: number of documents in document set, D( b ) is the number of documents in document set which contain b . scheme: where T( b , q ) is the frequency of b in q . 
We X  X l do two kinds of experiments. The first focuses on the performance with various parameter settings for term extraction and various document frequency weighting schemes. The second focuses on the comparison between the performance of our method and that of other methods. 5.1 Comparison on Different Parameter Setting method, and the following is the parameter setting in our experiments: 
For document frequency weighting scheme, we test the six weighting schemes listed at Table 1. The comparison of precisions at different parame ters settings is given at Table 2-4. In Table 2-4, [PreAt10(relax)] and [PreAt 10(rigid)] represent the average precision of on PreAt100 relax and rigid relevance judgm ent respectively. Each item in table represents the precision and its improvement over the baseline [INI] with the conditions expressed by [Column] and [Row].
 INI 0.3619 0.3619 0.2595 0.2595 0.1886 0.1886 0.1279 0.1279 INI 0.3619 0.3619 0.2595 0.2595 0.1886 0.1886 0.1279 0.1279 
W4 0.4595 
W5 0.4690 
W6 0.4667 
W7 0.4595 
W8 0.4476 
W9 0.4667 INI 0.3619 0.3619 0.2595 0.2595 0.1886 0.1886 0.1279 0.1279 
W4 0.4548 
W5 0.4619 
W6 0.4524 
W7 0.4595 
W8 0.4405 
W9 0.4619 
From Table 2-4, we see that the method achieves significant improvement against [INI] in every parameter setting. 
If only considering the effectiveness of term frequency to document re-ranking, document frequency weighting schemes to document re-ranking, W5, W7 and W9 produce better results. 
If considering both term frequency and document frequency weighting, the parameter settings, our method achieves 27%-28.9% improvement for PreAt10(relax), 28.4%-32.1% improvement for PreAt10(rigid), 15.9%-18.8% improvement for PreAt100(relax) and 16.3%-21.8% improvement for PreAt100(rigid). 
To explore the co-effects of the two parameters ( L ,  X  ) on the precision, we fix one parameter and see how the precision changes with the other. finding is that the precision improves or keeps the same in most cases, while decreases in fewer cases. The reason is that the terms with lower salience seeds tend to be noises, and removing the noises leads to improvement of the precision. However, not all relevant terms do hold higher salience seeds, in addition, some documents, although containing good terms, but they are not relevant (due to different focus). However, this chance is rare, so in fewer cases, we can see that the precision decreases. Another finding is that for both PreAt100(relax) and PreAt100(rigid), most precision improves with  X  changing from 1 to 10, while for PreAt10(relax) and PreAt10(rigid), less improves. The reason may be that for top 10 documents, the terms with higher salience seeds may take decisive shares in deciding on the ranking positions of the documents, so with  X  increasing, the precision doesn X  X  improve obviously, although some noisy terms are removed. demonstrates that whether  X  =1 or 10, all the precision improves when L increases to 3 L =3, some noisy terms can be removed. That X  X  why all the precision improves. demonstrates that for top 100 documents, the precision improves universally, while for top 10 documents, it doesn X  X . The reason may be that for top 10 documents, some terms with L =3 have contribution to the document ranking, with L changing from 3 to 4, they are removed. While for top 100 documents, the terms with L =4 may have decisive shares in deciding on the document ranking positions, as L increases from 3 recede behind the top 100 documents, but more documents containing terms with ( L =4) will move forward. Regarding the effect of document ranking positions, it is noticed that with scheme W5, W7 or W9, it tends to get higher performance, while with scheme W8, it tends to get lower performance. The reason is that not all documents with top ranking are relevant in most cases. In particular, for the first retrieval, among the top 10 documents, there are only 3.6 relevant documents in average, while among the top 100 documents, there are 18.9 relevant documents in average. This means that many relevant documents are located outside the top 10, but within the top 100 in the first retrieval. With W5, W9 and W7, the terms in these documents get higher weights, and then the documents tend to move forward during the re-ranking process. On the down, and the terms in lower ranking documents get very lower weights. So the relevant documents containing the terms cannot move forward during the re-ranking. 5.2 Comparison with Other Document Re-ranking Methods We first compare our method with Mitra et al. [8] X  X  method. Mitra et al. [8] uses term correlation to re-order retrieved documents. If { w 1 , ..., w m } is the set of query words between q and d is calculated by following formula: where idf(w i ) is the inverse document frequency of word w i in retrieved documents to documents calculated by the same formula (4). 
Figure 2 and Figure 3 list the comparison of performance of Mitra X  X  document re-ranking method and our method at parameter setting (  X  =10, L =4 and f(i)=w5) at PreAt10 and PreAt100. In our experiments, we set K as 1000 and re-rank top 50, 100, 200, 300, 400, 500, 600,700, 800, 900 and top 1000 documents. For Mitra X  X  method, each query and each document is segmented into Chinese words, and Chinese words are used to re-rank documents. 
In Figure 2 and Figure 3, PreAt10 and PreAt100 represent the performance of our method, INI_PreAt10 and INI_PreAt100 refer the initial results, and Mitra_PreAt10 and Mitra_PreAt100 represents the performance of Mitra X  X  method. document number setting. For example, when reordering top 50, 100 or 1000 documents, Mitra_PreAt10(relax) is 0.4024, 0.3929 and 0.3860 respectively, while our PreAt10(relax) is 0.4571, 0.4595 and 0.4595 respectively. way as the number of documents to be re-ranked increases, while for Mitra X  X  method, the improvement generally decreases as the document number increases. For example, Mitra_PreAt10(rigid) decreases from 0.2952 to 0.2905 and 0.2786 as document number increase from 50 to 100 and 1000, while our PreAt10(rigid) keeps as 0.3333 all the way. and applicable to a larger scope of documents. make use of the information of a larger scope of the retrieved documents, while resisting the impact of noisy documents by assigning lower weights for terms in lower ranking documents. In contrast, the idf -based weighting in Mitra X  X  method assigns larger range of the documents. or 30) documents are considered as relevance feedback. The comparison of precisions our proposed document re-ranking method at K =1000, L=4 and  X  =10. Yang(N=20) 0.4405 +21.7% 0.3262 +25.7% 0.2129 +12.9% 0.141 +10.2% Yang(N=25) 0.4405 +21.7% 0.3238 +24.8% 0.2112 +12% 0.1402 +9.6% Yang(N=30) 0.4381 +21.1% 0.3238 +24.8% 0.21 +11.3% 0.139 +8.7% PreAt100 level, our method achieves more 10% improvement against Yang X  X  method. One possibility is that Yang X  X  method only uses information in top 20-30 documents while we use information in top 1000 documents. When there are fewer relevant documents falling in top 20-30 documents, their method cannot capture enough information for re-ranking. 5.3 Experiments on Okapi BM25 Model We also do experiments on OKAPI BM25 model and we use the default parameter setting of OKAPI BM25. The comparison of precisions at different parameters settings (  X  =1 or 10, L =4) is given at Table 6. From Table 6, we see that the method achieves 9.5%-13.5% improvements against [INI] in every parameter setting at PreAt10 and achieves 12.5%-15.3% improvements against [INI] in every parameter setting at PreAt100. INI 0.4190 0.4190 0.3 0.3 0.1907 0.1907 0.1293 0.1293 
W4 0.4655 
W5 0.4613 
W6 0.4651 
W7 0.4638 
W8 0.4651 
W9 0.4588 In this paper, we propose a new term weighting scheme and use it in document re-distribution in top retrieved documents and the whole document set respectively, which combines the information regarding relative document frequency, document ranking positions as well as term length. The scheme allows randomly setting a larger portion of documents as relevance feedback, and helps to improve the performance of MMR model in document re-ranking. 
Our experiments based on NTCIR3 CLIR task show that our proposed approach achieves significant improvement against the baseline by 18.4%-32.1% at top 10 documents and 12.8%-22% at top 100 docume nts in their respective precision. Compared with other two document re-ranking methods, our method also gets higher performance on NTCIR3 CLIR dataset. Furthermore, the performance of the approach generally improves or keeps as the number of the re-ranking documents increases, which shows that it is robust against the noisy documents included. 
The experimental results support our assumptions: key terms in top K retrieved documents can be used to improve precision; long key term may contain more precise information and can be used to improve precision; document frequency distribution of implies the importance of query term. for term extraction. is language independent. In the future, we X  X l do further tests on other languages. 
