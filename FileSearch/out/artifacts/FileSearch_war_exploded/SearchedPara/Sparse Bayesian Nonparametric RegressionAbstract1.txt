 Fran  X cois Caron caronfr@cs.ubc.ca Arnaud Doucet arnaud@cs.ubc.ca Consider the following linear regression model where y  X  R L is the observation,  X  = (  X  1 , . . . ,  X  K R K is the vector of unknown parameters, X is an known L  X  K matrix. We will assume that  X  follows a zero-mean normal distribution  X   X  X  I L is the identity matrix of dimension L .
 We do not impose here any restriction on L and K but we are particularly interested in the case where K &gt;&gt; L . This scenario is very common in many ap-plication domains. In such cases, we are interested in obtaining a sparse estimate of  X  ; that is an estimate b ponents b  X  k differ from zero. This might be for sake of variable selection (Tibshirani, 1996; Figueiredo, 2003; Griffin &amp; Brown, 2007) or to decompose a signal over an overcomplete basis (Lewicki &amp; Sejnowski, 2000; Chen et al., 2001).
 Numerous models and algorithms have been proposed in the machine learning and statistics literature to address this problem including Bayesian stochastic search methods based on the  X  X pike and slab X  prior (West, 2003), Lasso (Tibshirani, 1996), projection pur-suit or the Relevance Vector Machine (RVM) (Tip-ping, 2001). We follow here a Bayesian approach where we set a prior distribution on  X  and we will primarily focus on the case where b  X  is the result-ing Maximum a Posteriori (MAP) estimate or equiv-alently the Penalized Maximum Likelihood (PML) es-timate. Such MAP/PML approaches have been dis-cussed many times in the literature and include the Lasso (the corresponding prior being the Laplace dis-tribution) (Tibshirani, 1996; Lewicki &amp; Sejnowski, 2000; Girolami, 2001), the normal-Jeffreys (NJ) prior (Figueiredo, 2003) or the normal-exponential gamma prior (Griffin &amp; Brown, 2007). Asymptotic theoreti-cal properties of such PML estimates are discussed in (Fan &amp; Li, 2001).
 We propose here a class of prior distributions based on scale mixture of Gaussians for  X  . For a finite K , our prior models correspond to normal-gamma (NG) and normal-inverse Gaussian (NIG) models. This class of models includes as limiting cases both the popular Laplace and normal-Jeffreys priors but is more flex-ible. As K  X   X  , we show that the proposed pri-ors are closely related to the variance gamma and normal-inverse Gaussian processes which are L  X evy pro-cesses (Applebaum, 2004). In this respect, our mod-els are somehow complementary to two recently pro-posed Bayesian nonparametric models: the Indian buf-fet process (Ghahramani et al., 2006) and the in-finite gamma-Poisson process (Titsias, 2007). Un-der given conditions, the normal-gamma prior yields sparse MAP estimates b  X  . The log-posterior distribu-tions associated to these prior distributions are not convex but we propose an Expectation-Maximization (EM) algorithm to find modes of the posteriors and a Markov Chain Monte Carlo (MCMC) algorithm to sample from them. We demonstrate through simula-tions that these Bayesian models outperform signifi-cantly a range of established procedures on a variety of applications.
 The rest of the paper is organized as follows. In Sec-tion 2, we propose the NG and NIG models for  X  . We establish some properties of these models for K finite and in the asymptotic case where K  X   X  . We also relate our model to the Indian buffet process (Ghahra-mani et al., 2006) and the infinite gamma-Poisson pro-cess (Titsias, 2007). In Section 3, we establish con-ditions under which the MAP/PML estimate b  X  can enjoy sparsity properties. Section 4 presents an EM algorithm to find modes of the posterior distributions and a Gibbs sampling algorithm to sample from them. We demonstrate the performance of our models and algorithms in Section 5. Finally we discuss some ex-tensions in Section 6. We will consider models where the components  X  are independent and identically distributed and p (  X  k ) is a scale mixture of Gaussians; that is where N ( x ;  X ,  X  2 ) denotes the Gaussian distribution of argument x , mean  X  and variance  X  2 . We propose two conjugate distributions for  X  2 k ; namely the gamma and the inverse Gaussian distributions. The resulting marginal distribution for  X  k belongs in both cases to the class of generalized hyperbolic distributions. In the models presented here, the unknown scale pa-rameters are random and integrated out so that the marginal priors on the regression coefficients are not Gaussian. This differs from the RVM (Tipping, 2001) where these parameters are unknown and estimated through maximum likelihood. 2.1. Normal-Gamma Model 2.1.1. Definition Consider the following gamma prior distribution whose probability density function (pdf) G (  X  2 k ;  X  K , is given by Following Eq. (2), the marginal pdf of  X  k is given for  X  k 6 = 0 by p (  X  k ) = where K  X  (  X  ) is the modified Bessel function of the sec-ond kind. We have and the tails of this distribution decrease in |  X  ters  X  and  X  resp. control the shape and scale of the distribution. When  X   X  0, there is a high discrepancy between the values of  X  2 k , while when  X   X  X  X  , most of the values are equal. This class of priors includes many standard priors. In-deed, Eq. (3) reduces to the Laplace prior when  X  K = 1 and we obtain the NJ prior when  X  K  X  0 and  X   X  0 . In Figure 2 some realizations of the process are given for different values  X  = 1 , 5 , 100 and  X  2 / 2 =  X  . 2.1.2. Properties It follows from Eq. (3) that and we obtain Hence the sum of the terms remains bounded whatever being K .
 Using properties of the gamma distribution, it is possi-ble to relate  X  to a L  X evy process known as the variance gamma process as K  X   X  . First consider a finite K. random variables verifying the following (finite) stick-breaking construction  X  where B is the Beta distribution. Finally if g  X  G (  X ,  X  2 2 ) then we can check that the order statistics  X  by and therefore  X  It follows that  X  k can be written as the difference of two variables following a gamma distribution. As K  X   X  , the order statistics part of a gamma process with shape parameter  X  and scale parameter  X  2 / 2; see (Tsilevich et al., 2000) for details. In particular  X  2 = P k  X  2 ( k ) are independent and respectively distributed according to PD (  X  ) and G (  X ,  X  2 / 2) where PD (  X  ) is the Poisson-Dirichlet distribution of scale parameter  X  . It is well-known that this distribution can be re-covered by the following (infinite) stick-breaking con-struction (Tsilevich et al., 2000) as if we set for any k then the order statistics tributed from the Poisson-Dirichlet distribution. The coefficients (  X  k ) are thus nothing but the weights (jumps) of the so-called variance gamma process which is a Brownian motion evaluated at times given by a gamma process (Applebaum, 2004; Madan &amp; Seneta, 1990). 2.2. Normal-Inverse Gaussian Model 2.2.1. Definition Consider the following inverse Gaussian prior distribu-tion 1997)  X  Following Eq. (2), the marginal pdf of  X  k is given  X  X   X K and the tails of this distribution decrease in |  X  The parameters  X  and  X  resp. control the shape and scale of the distribution. When  X   X  0, there is a high discrepancy between the values of  X  2 k , while when  X   X   X  , most of the values are equal. Some realiza-tions of the model, for different values of  X  are repre-sented in Figure 3. 2.2.2. Properties The moments are given Therefore, as K  X  X  X  , the mean of sum of the absolute values is infinite while the sum of the square is  X   X  . We can also establish in this case that the coefficients (  X  k ) tend to weights (jumps) of a normal-inverse Gaus-sian process (Barndorff-Nielsen, 1997).
 2.3. Extension Consider now the case where we have N vectors of model the fact that for a given k the random variables We consider the following hierarchical model for k = 1 , . . . , K and for n = 1 , . . . , N. Some realizations of the process for different values  X  = 1 , 5 , 100 are represented in Fig-ure 4.
 In this respect, this work is complementary to two re-cently proposed Bayesian nonparametric models: the Indian buffet process (Ghahramani et al., 2006) and the infinite gamma-Poisson process (Titsias, 2007). In these two contributions, prior distributions over infi-nite matrices with integer-valued entries are defined. These models are constructed as the limits of finite-dimensional models based respectively on the beta-binomial and gamma-Poisson models. They enjoy the following property: while the number of non-zero en-tries of an (infinite) row is potentially infinite, the ex-pected number of these entries is finite. These models are also closely related to the beta and gamma pro-cesses which are L  X evy processes (Applebaum, 2004; Teh et al., 2007; Thibaux &amp; Jordan, 2007). Our mod-els could be interpreted as prior distributions over in-finite matrices with real-valued entries. In our case, the number of non-zero entries of an (infinite) row is always infinite but we can have for  X  = 1 or  X  = 2. Morever for some values of  X  K and  X  we can also ensure that for any x &gt; 0 that is there is still a non-vanishing probability of hav-ing coefficients with large values as K  X  X  X  despite Eq. (9).
 Q k =1 p (  X  1: N k ) where for the NG model and for the NIG model where Further on we will also use the following notation for any random variable u
Lasso  X  |  X  k |  X  ( N = 1) NJ N log( u k ) N/u k
NG NIG where  X   X   X  denotes equal up to an additive constant independent of u. When computing the MAP/PML estimate for N data, we select b their derivatives for different prior distributions as a function of u k and q k defined in Eq. (11). When  X /K = 1, the NG prior is equal to the Laplace prior so its penalization reduces to the ` 1 penaliza-tion used in Lasso and basis pursuit (Tibshirani, 1996; Chen et al., 2001). When  X /K  X  0 and c  X  0 the prior is the NJ prior and the penalization reduces to log( |  X  k | ) which has been used in (Figueiredo, 2003). We display in Figure 5 the contours of constant value for various prior distributions when N = 1 and K = 2 . For  X /K &lt; 1 / 2, the MAP estimate (12) does not exist as the pdf (3) is unbounded. For other values of the parameters, a mode can dominate at zero whereas we are interested in the data driven turning point/local minimum (Griffin &amp; Brown, 2007).
 Consider now the case where the matrix X is orthog-onal,  X  = 1 and N = 1. The turning point and/or MAP/PML estimate is obtained by minimizing Eq. (12) which is equivalent to minimize componentwise where z = X T y . The first derivative of (13) is Li, 2001, p. 1350), a sufficient condition for the esti-mate to be a thresholding rule is that the minimum of and the resulting thresholds corresponding to the ar-gument minimizing (13) are presented in Figure 7. It follows that the normal-gamma prior is a thresholding rule for  X /K  X  1 and yields sparse estimates. The normal-inverse Gaussian is not a thresholding rule as the derivative of the penalization is 0 when  X  k = 0 whatever being the values of the parameters. However, from Figure 7(d), it is clear that it can yield  X  X lmost sparse X  estimates; that is most components are such that 4.1. EM The log-posterior in Eq. (12) is not concave but we can use the EM algorithm to find modes of it. The EM algorithm relies on the introduction of the missing data  X  1: K = (  X  1 , ...,  X  K ). Conditional upon these missing data, the regression model is linear Gaussian and all the EM quantities can be easily computed in closed form; see for example (Figueiredo, 2003; Griffin &amp; Brown, 2007). We have at iteration i + 1 of the EM
Z After a few calculations, we obtain m r Table 1). 4.2. MCMC We can also easily sample from the posterior distribu-using the Gibbs sampler. Indeed the full conditional are available in closed-form. The distribution NG prior, we obtain which is a generalized inverse Gaussian distribution from which we can sample exactly. For the NIG distri-bution, we also obtain a generalized inverse Gaussian distribution. 5.1. Simulated Data In the following, we provide numerical comparisons between the Laplace (that is Lasso), the RVM, NJ, NG and NIG models. We simulate 100 datasets from (1) with L = 50 and  X  = 1. The correlation be- X  = (3 1 . 5 0 0 2 0 0 . . . ) T  X  R K where the remaining components of the vector are set to zero. We consider the cases where K = 20 , 60 , 100 , 200. Parameters of the Lasso, NG and NIG are estimated by 5-fold cross-validation, as described in (Tibshirani, 1996). The Lasso estimate is obtained with the Matlab implemen-tation of the interior point method downloadable at http://www.stanford.edu/  X boyd/l1 ls/. For the other priors, the estimate is obtained via 100 iterations of the EM algorithm. Box plots of the mean square er-ror (MSE) are reported in Figure 8. These plots show that the performance of the estimators based on the NG and NIG priors outperform those of classical mod-els in that case. In Figure 9 are represented the box plots of the number of estimated coefficients whose ab-solute value is below T , T = 10  X  10 (the precision tuned for the Lasso estimate) and T = 10  X  3 , for K = 200. The true number of zeros in that case is 197. The NG outperforms the other models in identifying the zeros of the model. On the contrary, as the NIG estimate is not a thresholding rule, the median number of co-model is zero. However, most of the coefficients have a very low absolute value, as the median of the coeffi-true value 197 (see Figure 9(b)). Moreover, the esti-mator obtained by thresholding the coefficients whose differences in terms of MSE. 5.2. Biscuit NIR Dataset We consider the biscuits data which have been studied in (Griffin &amp; Brown, 2007; West, 2003). The matrix X is composed of 300 (centered) NIR reflectance mea-surements from 70 biscuit dough pieces. The obser-vations y are the percentage of fat, sucrose, flour and water associated to each piece. The objective here is to predict the level of each of the ingredients from the NIR reflectance measurements. The data are divided into a training dataset (39 measurements) and a test dataset (31 measurements). The fitted coefficients of fat and flour, using 5-fold cross-validation, are repre-sented in Figure 10. The estimated spikes are consis-tent with the results obtained in (West, 2003; Griffin &amp; Brown, 2007). In particular, both models detect a spike at 1726nm, which lies in a region known for fat absorbance. The predicted observations versus the true observations are given in Figure 11 for the train-ing and test datasets. The test data are well fitted by the estimated coefficients. MSE errors for the test dataset are reported in Table 2. The proposed models show better performances for flour and similar perfor-mances for fat. We have presented some flexible priors for linear re-gression based on the NG and NIG models. The NG prior yields sparse local maxima of the poste-rior distribution whereas the NIG prior yields  X  X lmost sparse X  estimates; that is most of the coefficients are extremely close to zero. We have shown that asymp-totically these models are closely related to the vari-ance gamma process and the normal-inverse Gaus-sian process. Contrary to the NJ model or the RVM, these models require specifying two hyperparameters. However, using a simple cross-validation procedure we have demonstrated that these models can perform sig-nificantly better that well-established procedures. In particular, the experimental performance of the NIG model are surprisingly good and deserve being further studied. The NG prior has been discussed in (Griffin &amp; Brown, 2007). It was discarded because of its spike at zero and the flatness of the penalty for large values but no simulations were provided. They favour another The NG prior has nonetheless interesting asymptotic properties in terms of L  X evy processes and we have demonstrated its empirical performances. The NG, NIG and Laplace priors can also be considered as par-ticular cases of generalized hyperbolic distributions. This class of distributions has been used in (Snoussi &amp; Idier, 2006) for blind source separation.
 The extension to (probit) classification is straightfor-ward by adding latent variables corresponding to the regression function plus some normal noise. Compu-tationally it only requires adding one line in the EM algorithm and one simulation step in the Gibbs sam-pler.
 Applebaum, D. (2004). L  X evy processes and stochastic calculus . Cambridge University Press.
 Barndorff-Nielsen, O. (1997). Normal inverse Gaus-sian distributions and stochastic volatility mod-elling. Scandinavian Journal of Statistics , 24 , 1 X 13. Chen, S., Donoho, D., &amp; Saunders, M. (2001). Atomic decomposition by basis pursuit. SIAM review , 43 , 129 X 159.
 Fan, J., &amp; Li, R. (2001). Variable selection via noncon-cave penalized likelihood and its oracle properties.
Journal of the American Statistical Association , 96 , 1348 X 1360.
 Figueiredo, M. (2003). Adaptive sparseness for su-pervised learning. IEEE Transactions on Pattern Analysis and Machine Intelligence , 25 , 1150 X 1159. Ghahramani, Z., Griffiths, T., &amp; Sollich, P. (2006).
Bayesian nonparametric latent feature models. Pro-ceedings Valencia/ISBA World meeting on Bayesian statistics .
 Girolami, M. (2001). A variational method for learn-ing sparse and overcomplete representations. Neural computation , 13 , 2517 X 2532.
 Griffin, J., &amp; Brown, P. (2007). Bayesian adaptive lasso with non-convex penalization (Technical Re-port). Dept of Statistics, University of Warwick. Lewicki, M. S., &amp; Sejnowski, T. (2000). Learning over-complete representations. Neural computation , 12 , 337 X 365.
 Madan, D., &amp; Seneta, E. (1990). The variance-gamma model for share market returns. Journal of Business , 63 , 511 X 524.
 Snoussi, H., &amp; Idier, J. (2006). Bayesian blind separa-tion of generalized hyperbolic processes in noisy and underdeterminate mixtures. IEEE Transactions on Signal Processing , 54 , 3257 X 3269.
 Teh, Y., Gorur, D., &amp; Ghahramani, Z. (2007). Stick-breaking construction for the Indian buffet process.
International Conference on Artificial Intelligence and Statistics .
 Thibaux, R., &amp; Jordan, M. (2007). Hierarchical beta processes and the Indian buffet process. Inter-national Conference on Artificial Intelligence and Statistics .
 Tibshirani, R. (1996). Regression shrinkage and selec-tion via the Lasso. Journal of the Royal Statistical Society B , 58 , 267 X 288.
 Tipping, M. (2001). Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning Research , 211-244 , 211 X 244.
 Titsias, M. (2007). The infinite gamma-Poisson fea-ture model. International Conference on Neural In-formation Processing Systems .
 Tsilevich, N., Vershik, A., &amp; Yor, M. (2000). Distin-guished properties of the gamma process, and related topics (Technical Report). Laboratoire de Proba-bilit  X es et Mod`eles al  X eatoires, Paris.
 West, M. (2003). Bayesian factor regression models in the  X  X arge p, Small n X  paradigm. In J. Bernardo, M. Bayarri, J. Berger, A. Dawid, D. Heckerman,
A. Smith and M. West (Eds.), Bayesian statistics 7 ,
