 Classification [16] is one of the most popular data mining techniques [17,18], belonging to supervised learning. Super vised learning means each instance in training data with a class label. The task of classification can be divided into two parts, training and testing. Training is using training data to train a certain classifier, e.g. knearest neighbor (kNN) [19,20], support vector machine (SVM) [21] and decision tree [22], etc. Testing is using a classifier which is trained to assign a class label to a new unlabelled instance. The objective of classification is to predict the classes of unlabelled data so that users can learn which class these data belong to. The Classification results usually can support decision making and other analyses in various applications, such as disease [23], text categorization [24], search engine [25], web page [26] and image [27], etc.
However, in nowadays, abundant data grow rapidly. If we do not process these data before classification, the storage and efficiency will become a burden. Therefore, the appearance of data reduction is for this reason. Data reduction [28,29] is to extract a subset from a dataset, so the amount of data will be reduced effectively. A way to measure the level of reduction is reduction rate. The reduction rate means the percentage of instances in training data which are chosen as samples. In spite of losing some information during the reduction process, using the subset as training data is possible to maintain classification accuracy. Instance selection [30,31] is one of data reduction approaches. The task of instance selection is to choose r epresentative samples into the subset. However, most datasets contain noises, so we should ignore them during the instance selection process. If noises ca n be removed, the accuracy of using the subset for classification is possible to be higher than using the entire dataset.
Most instance selection algorithms have their own drawbacks, such as easy to preserve noises, iteratively scan a datas et and need much processing time. There-fore, the objective of this paper is to design an algorithm which can choose rep-resentative samples, ignor e noises as possible and achieve acceptable processing time.

Nearest neighbor (NN) rule [1] is a lazy learning classifier in classification because it does nothing when training. However, when testing, NN needs much computation time for computing all the distances between training data and an unlabelled instance X. Then, NN finds out the nearest neighbor of X in training data and assigns its label to X. The con cept of Reverse Nearest Neighbor (RNN) [2] is as follows: if A is the nearest neighbor of B and C, B and C will be in the RNN set of A. Therefore, the differen ce between NN and RNN is the target to be recorded. In addition, for NN, each in stance has only one nearest neighbor; for RNN, an instance is possible to have zero or more than one reverse nearest neighbor in its RNN set. As shown in the example in Figure 1, A is the nearest neighbor of B and C, and C is the nearest neighbor of A. Therefore, the RNN set of A contains B and C, and the RNN set of C contains A, but there is no instance in the RNN set of B.
 In this paper, we propose a instance selection algorithm, called the Reverse Nearest Neighbor Reduction (RNNR). RNNR utilizes the property of RNN to choose representative samples. The conception of RNNR is simple to understand and easy to implement. RNNR applies RNN to each class and selects samples which can represent other instances in thesameclass.Inourexperiments,RNNR achieves comparable accuracy and lower reduction rate than comparators. In other words, we can use a smaller subset of the training data to obtain good classification results.

The remainder of this paper is organized as follows. In Section 2, previous approaches of instance selection are described. Section 3 presents the design of RNNR. In Section 4, experiments w ill show the performance of RNNR and comparators. Section 5 shows our conclusions and future work. Most data reduction methods can be divided into two types, Instance Based Learning (IBL) algorithms [32] and Clustering Based Learning (CBL) algorithms [33]. We will focus on the former. The task of instance selection is to choose representative samples from a dataset. Many instance selection algorithms are proposed, based on NN. Some of these algorithms select samples based on two strategies, incremental and decremental. The first type of algorithms, such as Condensed NN (CNN) [6], Modified CNN (MCNN) [7], Fast NN Condensation (FCNN) [8] and Generalized CNN (GCNN) [9], select some instances as sam-ples and iteratively add instances which do not have the same class label with their nearest sample to the sample set. The second type of algorithms, such as Edited NN (ENN) [10], remove instances which do not have the same class label with their majority of kNN. These instan ces will be considered as noises. Several algorithms apply ENN to their pre-processing step, such as Iterative Case Filter-ing (ICF) [3] and Decremen tal Reduction Optimization Procedure 3 (DROP3) [4]. In addition, a case-based editing algorithm, called the RDCL case profiling technique [12] and a new graph-based representation of a training set, called Hit Miss Network (HMN) [5] are also proposed to reduce the amount of data. We briefly summarize these methods as follows. 2.1 Incremental Algorithms This type of algorithms have some charact ers, such as easy to preserve noises, iteratively scan a dataset, sensitive to the order of instances and generally achieve lower reduction rate than decremental algorithms.

CNN [6] randomly selects an instance as a sample and scans all instances of the training set. If any instance does not have the same class label with its nearest sample, it will be chosen as a sample. The process will continue until the remaining instances of the training set ar e correctly classified by samples. On the other word, each the remaining instance is absorbed by one of samples. However, CNN is possible to preserve noises and sensitive to the order of instances in a training set.
 MCNN [7] solves the latter problem of CN N to generate a consistent subset. The choosing strategy of MCNN is to sel ect one instance for each class which is the nearest point to the geometrical center of its class as samples. This step is called Centroids. If any instance is inco rrectly classified by samples, MCNN will apply the Centroids step to these misclassified instances. MCNN tends to select samples which are not close to the decision boundary.

FCNN [8] also adopts the Centroids step but makes some changes. After se-lecting the central instance for each cla ss as samples, FCNN iteratively selects the nearest neighbor with different class label for each sample as samples. How-ever, FCNN is still possible to preserve noises.

GCNN [9] like CNN adopts random selection to its initial step, but the dif-ference is GCNN randomly selects one inst ance for each class as samples. Then, GCNN sets a threshold for enhancing the absorption criterion of CNN. Although GCNN achieves higher classification a ccuracy than CNN, GCNN selects more samples than CNN. 2.2 Decremental Algorithms This type of algorithms have some characters, such as eliminating noises, needs more processing time and generally achi eve higher classifica tion accuracy than incremental algorithms.

ENN [10] removes instances which do not have the same class label with their majority of kNN. The objective of ENN is to remove noises in a training set. Therefore, using the sample set of ENN to classify probably achieves higher accuracy than using the entire training set. However, since ENN removes only noises, the reduction rate of ENN is hardly lower than other IBL algorithms.
After applying ENN to pre-processing step, DROP3 [4] removes the instance which is farthest from the nearest neighbo r with different class label in turn. But, the hypothesis is not affecting the classification of training. Therefore, DROP3 tends to preserve instances which are close to the decision boundary. ICF [3] applies ENN iteratively until it is impossible to remove any instance. Then, ICF removes each instance whose reachability set is smaller than cover-age one. Assuming an instance X, the reachability set of X contains instances which contribute to the correct classific ation of X. Besides, the coverage set of X contains instances which X contributes to the correct cla ssification of them. TheabovetwosetsisproposedbySmyth&amp;Keane[11].

The RDCL case profiling technique [12] categorizes each case by four charac-teristics, R, D, C and L. R means the reachability set. D means the dissimilarity set which is proposed by Sarah [12]. C means the coverage set. L means the liability set which is proposed by Delany &amp; Cunningham [13]. Assuming an in-stance X, the dissimilarity set of X cont ains instances which contribute to the incorrect classification of X. Besides, the liability set of X contains instances which X contributes to the incorrect classification of them. In fact, the dissimi-larity set complements the reachability set, and the liability set complements the coverage set. If a certain set of X is not emp ty, X will possess tha t characteristic. Therefore, a case could possess more than one characteristic. The best removal strategy of RDCL is removing DL and DCL cases, because these cases harm to the classification of training.

HMN [5] is a new graph-based representation of a training set. Each instance of the training set has one outgoing edge for each class. Each outgoing edge points an instance to its NN with that class. The in-degree of an instance is divided into two part, hit-degree and miss-degree. Hit-degree means the edge connects two instances with the same class labels . Miss-degree means the edge connects two instances with different class label s. HMN-E removes each instance whose number of miss-degree is equal or greater than number of hit-degree. HMN-EI iteratively applies HMN-E until the classification of training is affected by removing. A drawback of HMN-EI is that it needs much processing time to iteratively construct a n etwork for the reduced set. In this Section, we will introduce our instance selection algorithm, Reverse Nearest Neighbor Reduction (RNNR). The framework of RNNR is shown in Figure 2. At the beginning, training data are taken as the input of RNNR algorithm. Then, RNNR extracts a sample set from the training data. The generated sample set will be used to train a classifier. Finally, the classifier classifies some unlabelled data (testing data), based on the sample set. The final result shows the classification accuracy.

Since instances generally have similar features with their neighbors, we can select the most representative instance in its neighborhood as a sample. Fur-thermore, the RNN set of an instance contains instances which consider it as the nearest neighbor. Therefore, RNNR implements the above idea by applying the property of RNN. The process of RNNR algorithm is as follows. At first, RNNR applies RNN to each class. Because an instance with larger size of RNN set means it can represent more instances, RNNR will select it as a sample first. Therefore, RNNR sorts instances for each class in descending order by the size of RNN set. According to different choosin g strategy, RNNR has three versions, RNNR-AL0, RNNR-AL1 and RNNRL1. Finally, RNNR gathers samples from each class and generates a final sample set. The pseudo code of RNNR algo-rithm is shown in Figure 3. Then, we will introduce choosing strategies for three versions of RNNR. 3.1 The Choosing Strategy of R NNR-AL0 (Absorption, Larger We observed that each instance whose s ize of RNN set is zero can not represent any instance, so RNNR-AL0 will not sel ect it as a sample. Furthermore, noises tend to be closer to instances with different class labels. It is possible that no instance considers the noise as the neares t neighbor in the same class. Therefore, the size of RNN set of a noise is probably zero. If an instance belongs to the RNN set of a sample, it also will not be select ed as a sample, owing to being absorbed by the sample. In fact, the upper bound of reduction rate of RNNR-AL0 is 50% because each sample can absorb at least one instance.
 Example. The choosing strategy of RNNR-AL0 is illustrated by the training set in Figure 4. For class 1, after selecting A as a sample, B, C and D will not be selected as samples because they are in the RNN set of A. Therefore, only one sample, A, is selected for class 1. For cla ss 2, after selecting E as a sample, F and G will not be selected as samples becaus e they are in the RNN set of E. Next, the size of RNN set of H is larger than zero, so H will be selected as a sample. Because I is in the RNN set of H, I will no t be selected as a sample. Therefore, there are two samples, E and H, in class 2. For class 3, after selecting J as a sample, because K and L are in the RNN s et of J, they will not be selected as samples. Then, the size of RNN set of M is not larger than zero, so M will not be selected as a sample, too. Therefore, there is only one sample, J, in class 3. The final sample set of RNNR-AL0 is shown in Table 1. 3.2 The Choosing Strategy of R NNR-AL1 (Absorption, Larger Note that each instance whose size of RNN set is one is considered as the nearest neighbor by only one instance. For select ing samples which are more representa-tive, RNNR-AL1 will further not select each instance whose size of RNN set is one as a sample. Similar to RNNR-AL0, if an instance belongs to the RNN set of a sample, it also will not be selected asasample,owingtobeingabsorbedby the sample.
 Example. The choosing strategy of RNNR-AL1 is illustrated by the training set in Figure 4. For class 1, after selecting A as a sample, B, C and D will not be selected as samples because they are in the RNN set of A. Therefore, only one sample, A, is selected for class 1. For class 2, after selecting E as a sample, F and G will not be selected as samples because they are in the RNN set of E. Next, H and I do not contain more than one member in their RNN sets, so they will not be selected as samples, too. Ther efore, there is only one sample, E, in class 2. For class 3, after selecting J as a sample, K and L will not be selected as samples because they are in the RNN set of J. Then, the size of RNN set of M is not larger than one, so M will not be select ed as a sample, too. Therefore, there is only one sample, J, in class 3. The final sample set of RNNR-AL1 is shown in Table 1. 3.3 The Choosing Strategy of R NNR-L1 (Selecting All, Larger For observing how the absorption strategy affects the classification accuracy, RNNRL1 does not adopt the absorption strategy but selects each instance whose size of RNN set is larger than one as a sa mple. In general case, most instances contain zero or one member in their RNN sets, so the reduction rate of RNNR-L1 is still decent. Example. The choosing strategy of RNNR-L1 is illustrated by the training set in Figure 4. Since A, E, J and K contain more than one member in their RNN sets, they will be selected as samples. The final sample set of RNNR-L1 is shown in Table 1.

Note that all samples selected by RNNR -AL1 can be obtained from the sample set of RNNR-AL0 via removing each sample whose size of RNN set is one. Furthermore, because RNNR-L1 selects e ach instance whose size of RNN set is larger than one as a sample, all sample s selected by RNNR-AL1 are also selected by RNNR-L1. Therefore, RNNR-AL1 should achieve the lowest reduction rate among three versions of RNNR. A problem for RNNR-AL1 and RNNR-L1 is that they will not select any instance as a sample if the RNN set of each instance contains just one member. In fact, the opportunity of the above condition is quite low.
 To compare our algorithm RNNR with comparators, ICF [3], DROP3 [4] and HMN-EI [5], we use seven real datasets from UCI repository [14]. These datasets are summarized in Table 2. For each dataset, we perform five-fold cross validation to evaluate the performance. The implementation of comparators is obtained from Elena [5]. After sampling from training data for each algorithm, we use the IB1 algorithm (NN) of Weka [15] to test accuracy. The average accuracy and the reduction rate for each dataset are shown in Table 3 and Table 4. The reduction rate means the percentage of instances in training data which are chosen as samples, as shown in following Equation.

The experimental results, including the classification accuracy and the reduc-tion rate, are shown in Table 3 and Table 4. RNNR-AL1 achieves the highest accuracy among all comparators on three datasets, as shown in Table 3. In addi-tion, three versions of RNNR have bette r performance in accuracy than ICF and DROP3 (comparable with HMN-EI). Especially, compared with training data (TRAIN), RNNR-AL1 and RNNR-L1 improve the accuracy on four datasets. Although RNNRAL1 selects fewer samp les than RNNR-AL0, e xperimental re-sults show that RNNRAL1 generally achi eves higher accuracy than RNNR-AL0. This confirms that samples selected by R NNR-AL1 are more representative. In addition, RNNR-L1 achieves only slightly higher accuracy than RNNR-AL1 on most datasets, so adopting the absorption strategy can maintain the classifica-tion accuracy while achieving lower reduction rate.

For reduction rate, RNNR-AL1 selects the fewest samples of all comparators on five datasets, as shown in Table 4. In Section 3, we infer that RNNR-AL1 should achieve the lowest reduction rate among three versions of RNNR, because all samples selected by RNNR-AL1 w ill also be selected by RNNR-AL0 and RNNR-L1. The experimental results confirm that RNNR-AL1 indeed achieves lower reduction rate than RNNR-AL0 and RNNR-L1. Furthermore, the reduc-tion rate of each version of RNNR is consistent on all datasets. On the contrary, the reduction rate of ICF and HMN-EI are easily affected by characteristics of a dataset.

In summary, RNNR-AL1 and RNNR-L1 gen erally achieve higher accuracy than ICF and DROP3 (comparable with HMN-EI). Moreover, the reduction rate of RNNRAL1 is the lowest on most of datasets. Therefore, applying RNNR-AL1 to data reduction can more effectiv ely decrease the requirement of storage and increase the efficiency of classification, while maintaining the classification accuracy. In this paper, we proposed a new instance selection algorithm based on Reverse Nearest Neighbor (RNN), called Rever se Nearest Neighbor Reduction (RNNR). According to different choos ing strategies, we designed three versions for RNNR. Experiments indicated RNNR-AL1 and RNNR-L1 achieved comparable accu-racy with HME-EI and better than ICF and DROP3. On more than half of datasets, RNNRAL1 and RNNR-L1 even improved the accuracy of NN. For reduction rate, RNNRAL1 had the best performance among all comparators. Therefore, applying RNNRAL1 to data r eduction can more effectively decrease the requirement of storage and increase the efficiency of classification, while maintaining the classification accuracy.

Afterwards, we will extend RNN set to RkNN set, that is, find reverse k-nearest neighbors for each instance in the same class. Another idea is to apply RNN to all instances in different classes. Finally, we will observe whether the above ideas improve RNNR.

