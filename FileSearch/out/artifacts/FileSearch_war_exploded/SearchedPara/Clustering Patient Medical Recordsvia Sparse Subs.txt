
Budhaditya Saha 1 , Duc-Son Pham 2 , Dinh Phung 1 , and Svetha Venkatesh 1 , 2 Traditional methods fail to manage the scale and complexity of  X  X ig data X . The health sector is at the epicenter of this  X  X ig data X  -data on admissions, diagnosis, outcomes, spanning a bewildering and disc onnected web of images, computerized records and registries. There are no systems to manage this big data. The result is  X  X rite only data X , mostly unused. Critically it has potential to identify critical safety issues, as well as service and clin ical efficiency. This paper explores the pressing need, to construct data analytic to inform such clinical decisions. The outcomes are critically important from economic, patient safety and systems perspectives.

Historically, classical statistical methods have been used to verify stated hy-potheses. This requires a priori assumption, for example, on data distributions. As the scale, distribution and diversity of data increase, this approach leads to sub-optimal use of this information. This paper examines new ways to analyze cohorts of patients with chronic diseases, such as Diabetes mellitus (diabetes) and stroke. Chronic care is expensive to administer. One crucial problem in the management of chronic patients is to deliver care plans, such that in major-ity of cases patients can be manged in the community without hospitalization. This requires us to find sub-groups of patients with same disease characteristics, without any prior assumptions on grouping.

Considering the complexity and nature of the datasets, we propose to model the data by a union of subspaces [1] where each subspace corresponds to pa-tients with similar diagnostic conditions. This model has been used in many applications, such as lossy compression of images [2][3], motion segmentation in video sequences [4,5,6,7,8] etc. Ear ly subspace clustering methods include mixture of Gaussian, factorization, algebraic, compressed sensing/low-rank [9] methods, and examples range from K subspaces [10], mixture of probabilistic PCA [11], multi-stage learning [12] etc. These algorithms typically require prior knowledge about the subspaces -the number of subspaces or their dimensions [13]. The computation is also exponential with the number and/or dimensions. Recently, Elhamifar and Vidal [13] propo se sparse subspace clustering (SSC), in which the clustering is solved by seeking a sparse representation of data points. By computing an affinity graph on the sparse representations for all data points, SSC automatically discovers the subspaces and their dimensions. However, the previous results by SSC show that there are many instances in which the sparse coefficients corresponding to points outsid e a cluster of interest are significantly non-zero. This suggests that enforcing constraints that discourage points fur-ther apart will prevent them from entering the same cluster [14]. This was also exploited in [5], who propose a weighted version (WL-SSC).

Inspired by the related success of spars e subspace clustering in computer vi-sion, this paper proposes a novel application of this powerful approach in the context of health care data. Here, it requi res a careful modeling and interpreta-tion of subspaces in health care data as well as novel construction of weighting matrices. The weighting matrix acts as the prior knowledge on the similarity between patient records and is computed directly from the data. We explore the decomposition into union of linear subspaces (WL-SSC) and extend the model to consider decomposition of a union of affine subspaces (WA-SSC). To decide on the weighting constraints, we consider three different ways of spec-ifying proximity of points in a k -neighborhood -RBF, cosine and 0-1 matrix. We apply the models across a cohort of 1580 diabetes patients with 551 disease codes, and 1159 stroke patients with 805 codes. The data is collected over a period of 5 years, and each time the patient comes to hospital, a diagnosis code is assigned. Evaluation of such algorithms, with real-world data is notoriously hard. We propose the use of the recently introduced  X  -measure-this method allows ground-truth to be allocated based on degree of similarity between two points. Using this measure, we can compute the Rand-Index and F -measure for agiven  X  . We show that our methods outperform the unweighted version and many competitive clustering methods such as affinity propagation (AP) [15], locality-preserving pr ojection (LPP)[16] and k -means [17]. We show that further improvement can be achieved with a weighted union of affine subspace model. We also show tag clouds for clusters in the diabetes cohort and demonstrate how the sub-groups discovered are qualitatively meaningful.

The novelty in our paper is threefold: (a) it applies weighted sparse subspace clustering to a unique medical dataset p roblem to improve service efficiency, (b) it proposes a new affine, weighted subspace clustering method, and (c) uses a novel principled way to evaluate real world clustering results for which no ground-truth can be obtained.

The significance of the problem lies in the ability to save costs with efficient sub-group identification, leading to targeted care plans. Both chronic diseases chosen have reached epidemic status. For example, Diabetes mellitus (diabetes) is spreading so rapidly that recent studi es show that the total number of diabetic people across the world was 171 million i n 2001 and it is estimated to 230 million by the end of 2030 [18,19]. 2.1 Sparse Subspace Clustering (SSC) Consider a set of N data points collected in a D  X  N matrix X =[ x 1 , x 2 ,..., x N ] where x i  X  R D and D is the number of features. SSC [4] clusters the datapoints via the subspace principle. Intuitively, a linear representation of a datapoint with respect to the whole set gives more pref erences to those points that belong to the same subspace. Denote as S i the subspace (cluster) that x i belongs to. Then, the linear representation of a datapoint can be written as follows: ideal case, the coefficients in the seco nd summation of the right term are zero, not unique when the number of features D is usually much less than the number of observations N . Recent advances in sparse learning [20,21] show that it is possible to regularize the solution and at the same time achieve sparse solution, which is consistent to the ideal case, by enforcing the 1 -norm of the coefficient vector, c i 1 = | c ik | , to be small. Using this principle, SSC [4] advocates to find the solution with two variations as follows.
 Linear Sparse Subspace formulation (L-SSC). Under this formulation, we assume that data points in X are sampled from a union of linear subspaces. Then the sparse coefficients are obtained by solving following optimization problem without employing any others c onstraints on coefficient vector c i . Affine Sparse Subspace formulation (A-SSC). L-SSC can be extended to union of affine subspaces by enforcing an additional equality constraint over the sparse coefficient vector c i as follows: The coefficients are then used to compute a balanced affinity matrix for final spectral clustering:  X  C =( C + C T ) / 2. Then, the Laplacian matrix L = I  X  D  X  1 / 2  X  CD  X  1 / 2 is computed, with I being the identity matrix and D being a diagonal matrix where D ii = N j =1  X  c ij . The smallest eigenvalues of L is used to estimate number of subspaces and the corresponding data points are obtained using the k -means algorithm. 3.1 Weighted Sparse Subspace Clustering (W-SSC) from two different subspaces. However, the re are cases where they significantly deviate from zero due to numerical properties of the data matrix X [5]. To avoid undesirable sparse solutions, it has been suggested to introduce a weighting scheme in the sparse formulation [5]. Under this scheme, a weight matrix W  X  R
N  X  N is used to enforce sparse coefficients to better fall into the same subspace they deem to belong to. Such a desired solution is encouraged by minimizing the weighted 1 -norm w i c i 1 instead of c i 1 . Here, denotes element-wise product of two vectors. Inspired by this principle, we also propose to employ the weighting scheme in our method. The remaining challenge is to construct a suitable weighting matrix for the data, which we detail next. 3.2 Construction of Weighting Matrix W An optimal weighting matrix can be constructed if we have ground-truth knowl-edge of the clusters to suppress cross-cluster coefficients (by setting w ij large or small for inter-or intra-cluster coeffic ients respectively). However, as this knowledge is not available, we propose to use the information within the data to approximate the optimal weighting matrix. We rely on the principle that the weights for inter-cluster coefficients are large whilst those for intra-cluster coef-ficients are small. Denote as x i  X  X  ( x j )as x i is k -nearest neighbor of x j ,and I the indicator (0/1) function. We propose the following choices:  X  X nverseRBF : w ij = I x  X 0-1 : w ij = I x  X  X osine : w ij = I x 3.3 Weighted Formulation Extending the basic SSC algorithms, we propose to adapt to the idea in [5] and solve the following basic weighted formulation with linear subspace assumption The above basic formulation assumes noiseless data generation. Considering noise while modeling the data points sampled from the union of subspaces, we Thus, it is more realistic to extend the basic model to account for noise by considering the noise-aware version of the formulation This can be more conveniently written in a Lagrangian form Here,  X  is regularization parameter, X  X  i is X with the i th column removed, and we implicitly ignore the i th entry of c i . When considering the affine subspace modeling, the above Lagrangian formulation can be extended to account for the additional affine constraints as follows
Next, we discuss optimization algorithms to solve (7) (note that (6) can be readily solved by a slight modification of many efficient compressed sensing solver, such as reweighting the column of X  X  i by the inverse of the corresponding weights and working on the reweighted variables [5]). As they are convex prob-lems, off-the-shelf solvers, such as CVX, can be used, but we do not seek to use them because they are rather inefficient. We show that it is possible to solve (7) more efficiently with the alternative d irection method of multipliers (ADMM) [22]. For notational simplicity, we drop the subscript/superscript of c i , x i and X  X  i . Under the ADMM framework, we decouple the the quadratic terms by introducing a new variable z such that z  X  c =0and consider the augmented Lagrangian Here, y and v are the dual parameters corresponding to the inequality constraints c  X  z =0and 1 T c  X  1 = 0 respectively;  X  numerical stability (see [22] for ADMM background). By using the normalized dual variables u 1 =( y / X  1 )and u 2 =( v/ X  2 ) we derive the following ADMM updates that solve (7) Here S  X  ( c ) is the soft-thresholding shrinkage operator, defined as a vector r such that r i = sign ( c i )max( | c i | X   X  i , 0) (see [22]).

Once the coefficient vectors c i  X  X  are found, the spectral clustering part pro-ceeds in the same way as the original SSC algorithm [4]. 4.1 Datasets We validate our approach on two real-world datasets collected from patients having diabetes and heart (stroke) dise ases collected over a period of five years from 2007 to 2011 and has diagnosis records from 9878 patients. Each patient has been diagonised several times over a period of five years and assigned unique diagnosis code(s). An example of a r ecord for a patient over time might be ( E1172, I10, E1172, Z9222 ). Table 1 and 2 shows the description of some codes. Patients may be assigned similar code more than once over time.
We remove records without codes, patients diagonised less than twice and also duplicated codes. This results in 1580 diabetes patients with 551 unique codes. We construct a code-patient matrix, where codes are used as features and each patient is an observation, analogous to term-document matrix for text data analysis. In our second data set (stroke patients), there are 1159 patients with 805 diagnostic codes. 4.2 Evaluation Method As no ground-truth is available for latent groups, it is impossible to measure the clustering performance by standard evaluation metrics. Thus, we evaluate the performance using a novel  X  -measure method as follows: 2. Compute relative similarity metric s  X  (  X  x i ,  X  x j ) 3. Construct a ground-truth matrix G  X   X  R N  X  N with element g ij = I s 4. Construct a cluster membership matrix V with element v ij = I ID Next, we compute the standard Precision (P), Recall (R) and F-measure ( F ): Here, true positive (TP) is scored when two similar data points in the ground-truth are grouped together in the obtained results, a true negative (TN) is scored when two dissimilar data points are grouped separately, a false positive (FP) is scored when two dissimilar data points are grouped together and a false negative (FN) is scored when two similar data points are grouped separately. Similarly, the rand index (RI) is defined as where high RI and F indicates the better accuracy.

Algorithm 1 show the overall method of computing F -measure. Note that, we compute F measure over a matrix of N  X  N variables, instead of N number of data points. 4.3 Results and Comparisons Performance against Other Methods. We compare our proposed cluster-ing method against competitive sparse subspace clustering and baseline alter-natives, including affinity propagation (AP) [15], locality preserving projection (LPP) [16], and k -means [17]. In all experiments, we set  X  to 0.9, regularization parameter  X  to 0.001.

Table 3 presents the clustering results obtained from SSC methods for diabetes and stroke data. Clearly, our proposed method outperforms both L-SSC and A-SSC variants by obtaining larger RI and F scores. The F measure scores of WL-SSC and WA-SSC have improved over L-SSC and A-SSC by large margins of 47 %and 45 % for the diabetes data and 236 %and 257 % for the stroke data respectively. Algorithm 1. Computing F measure
Likewise, the F measure is improved by 275 %(AP), 85 % (LPP), 388 %( k -means) for diabetics datasets, whereas the betterment in RI is 87 %(AP), 14 % (LPP), 10 %( k -means) respectively. For the strokes data, F measure is improved by 173 %(AP), 54 % (LPP), 465 %( k -means) and Rand Index is 71 %(AP), 13 % (LPP), 139 %( k -means) respectively.
 Influence of Weighting Schemes. Table 4 include the performance for dif-ferent weighting schemes and it is found that the RBF choice provides better performance than the other choices.
 Discovered Clusters. The number of clusters K equals to the number of zero eigenvalues of of Laplacian matrix L . Fig. 1(c) shows the eigenvalue plot of L for the diabetes data where the number of zero eigenvalue equals to 9. Similarly, we found 12 sub-groups for stroke data.

Since  X  is the relative similarity between the two data points, which means high value of  X  denotes two observations are highly similar, we vary  X  varies from 0.1 to 1 in a separate experiment on diabetes data and plots are shown in . Figure 1(b). As expected, F -measure is high for small values of  X  and F -measure is low when  X  is increasing.

Figures 1 and 2 show the qualitative evaluation of clusters for the diabetes data. Figure 1(a) shows the affinity matrices, whilst Figure 2 shows the tag clouds of the diagnosis codes in each cluster. As anticipated the clusters are qualitatively different in terms of disease differentiation within diabetes: diabetes with heart disease, with cancer, with dialysis. Ty pe 1 and 2 are clearly differentiated. We have demonstrated a novel application of the sparse subspace clustering the-ory in solving the clustering problem of health care data. Our novel contributions includes special construction of the weighting matrices to obtain better sparse so-lution and the efficient algorithm to solve the formulation with affine constraints. To evaluate realistic health care data where no ground-truth is available, we have also suggested a novel evaluation method of clustering results. Compared with competitive alternatives in the literat ure, our proposed method achieve much better F and RI scores, and discovers meaningful patients subgroups.
