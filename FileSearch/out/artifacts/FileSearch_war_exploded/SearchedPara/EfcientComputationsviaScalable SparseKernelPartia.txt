 Kernel partial least squares (KPLS) has been kno wn as a generic kernel regression metho d and pro ven to be comp et-itiv e with other kernel regression metho ds suc h as supp ort vector mac hines for regression (SVM) and kernel ridge re-gression. Kernel boosted laten t features (KBLF) is a vari-ant of KPLS for any di eren tiable con vex loss functions. It pro vides a more exible framew ork for various predictiv e mo deling tasks suc h as classi cation with logistic loss and robust regression with L1 norm loss, etc. However, KPLS and KBLF solutions are dense and thus not suitable for large-scale computations. Sparsi cation of KPLS solutions has been studied for dual and primal forms. For dual spar-sity, it requires solving a nonlinear optimization problem at every iteration step and its computational burden limits its applicabilit y to general regression tasks. In this pap er, we prop ose simple heuristics to appro ximate sparse solutions for KPLS and the framew ork is also applied for sparsify-ing KBLF solutions. The algorithm pro vides an interesting \path" from a maxim um residual criterion based algorithm with orthogonalit y conditions to the dense KPLS/KBLF. With the orthogonalit y, it di eren tiates itself from man y existing forw ard selection-t ype algorithms. The computa-tional adv antage is illustrated by benc hmark datasets and comparison to SVM is done.
 Categories and Sub ject Descriptors: I.5.1 Pattern Recog-nition:Computing Metho dologies General Terms: Algorithms Keyw ords: Scalable and sparse kernel metho d, Partial Least Squares, Boosted Laten t Features
Partial Least Squares (PLS) [19] has been a popular re-gression metho d for chemometrics. It has been pro ven to work well when man y attributes are presen t. Generally , in suc h data, multiple collinearit y is a serious issue and classi-Cop yright 2005 ACM 1 X 59593 X 135 X  X /05/0008 ... $ 5.00. cal statistical metho ds that require hand-tuning of attribute selection may not be as e ectiv e. PLS solv es suc h a problem by extracting laten t features (LF) iterativ ely. Features are generated by maximizing covariance with the resp onse vari-able; a single feature by itself has only limited predictiv e power. By using collections of suc h features, PLS creates regularized mo dels for better generalization. The num ber of LFs is the only parameter for linear PLS and is used to con trol the mo del capacit y. In practice, the mo del selection of PLS is intuitiv e and easier than other metho ds; choice of a natural num ber in PLS is in con trast to a real num ber in ridge regression or supp ort vector mac hines (SVM) [6]. An interesting study on shrink age and regularization path has been done and regularization/shrink age in PLS is discussed in [9].

The kernel tric k [6] maps the input space into the kernel-de ned feature space and man y linear mo dels can be kernel-ized by replacing the inner pro duct by a (nonlinear) kernel function. Kernel Partial Least Squares (KPLS) [18] utilizes the kernel tric k to build nonlinear mo dels. Numerical stud-ies [18, 2] sho w comp etitiv eness of KPLS with other kernel based regression metho ds.
 KPLS is also applicable for classi cation problems [17, 3]; PLS can be seen as a regularized canonical comp onen t anal-ysis, whic h can be sho wn to be equiv alen t to (regularized) Fisher's linear discriminan t analysis. Thus the use of KPLS for classi cation is justi ed as a regularized discriminan t analysis.

One of the interpretations of PLS is to regard it as a kind of ensem ble metho d. [14] directly applied a frame-work in AnyBo ost [12] to sho w PLS can be written in terms of boosting; LFs in (K)PLS corresp ond to weak hypothe-ses and the function space is limited to a linear or kernel de ned feature space. The squared loss is emplo yed. One unique feature in PLS as a boosting is that weak hypotheses are mutually orthogonal. Thus, PLS is a kind of orthogo-nalized boosting with squared loss. With this interpreta-tion, we can easily extend the original PLS into a boosting with arbitrary loss functions. The Boosted Laten t Features (BLF) metho d [14] replaces the squared loss in PLS with any (sub)-di eren tiable loss function suc h as negativ e binomial likeliho od (logistic loss), exp onen tial loss, whic h is used in AdaBo ost and L-1 norm loss. Orthogonal loadings PLS [10], whic h is pro ven to be equiv alen t to the orthogonal laten t features PLS in univ ariate cases, utilizes least squares t at eac h step. This approac h better explains sparse PLS/BLF deriv ations, and computationally more ecien t[8].
For any kernel metho d to be applicable for a large scale problem, sparsit y needs to be sough t. Since nonlinear ker-nel metho ds use training examples (a.k.a. supp ort vectors) for predicting a point, a dense kernel mo del needs to scan through the entire training data, whic h becomes prohibitiv e as it gro ws. Sparsit y can be adv antageous for ecien t and scalable mo del training as well, esp ecially since a kernel ma-trix scales as O( m 2 ). For SVM, various metho ds have been dev elop ed by decomp osing the optimization problem [6] so that the full kernel matrix is not stored in memory . For KPLS, [15] dev elop ed a framew ork for sparse KPLS, whic h is also called -KPLS. It enforces an -insensitiv e squared loss that is conceptually similar to the linear -insensitiv e loss used in SVM to mak e the dual solutions sparse. One critical issue for this approac h is that it requires solving an optimization problem that is at least as dicult as the SVM problem at every iteration. This can be overkill since -KPLS generally needs man y iterations for better sparsit y. Thus dev eloping ecien t algorithms to solv e a -KPLS prob-lem is less attractiv e than those for SVM where, in con trast, only one optimization problem is solv ed. A more ecien t approac h has been dev elop ed for primal KPLS problem [2, 11] and applied to larger scale problems. In a primal ap-proac h, a kernel matrix is basically given as a data matrix and sparsit y is sough t in that space.

In this pap er, ecien t ways for computing the -KPLS solutions are discussed and dev elop ed. A key concept for appro ximating the solutions is to consider LFs as weak hy-potheses as done in the BLF deriv ation. These weak hy-potheses collectiv ely function and suc h collection of hypothe-ses eventually work as a single strong learner comp etitiv e with SVM or dense KPLS. Eac h weak hypothesis needs not to be strong. Tw o appro ximation approac hes are prop osed for a scalable -KPLS form ulation.

Sparse KBLF utilizes the framew ork of -KPLS. The re-sulting algorithm, -KBLF, enables us to use arbitrary loss functions and obtain sparse solutions. The supp ort vector (SV) selection is done heuristically and at the extreme case, it reduces to a coordinate descen t algorithm, with orthog-onalit y imp osed for the loadings. Orthogonalit y helps im-pro ve con vergence speed in some cases.

Section 2 reviews kernel orthogonal loadings PLS, whose form ulation can serv e as a basis for sparse KPLS. Section 3 rst review -KPLS and dev elops out-of-memory -KPLS for large-scale problems. Sparse BLF is introduced by using -KPLS in Section 4. Empirical studies are presen ted in Section 5. Section 6 concludes this pap er.

N otation : Assume we are given a training data set of size m with a single resp onse, (( x 1 ; y 1 ) ; : : : ; ( x the column vectors x i 2 R n and y i 2 R . Although PLS and KPLS can be used for multiv ariate resp onse, we focus on univ ariate resp onse cases. A T denotes a vector/matrix transp ose. The data matrix is X = [ x 1 ; : : : ; x m ] T data matrix is denoted by f X and the cen tered resp onse is ~ y . jj y jj denotes the 2-norm of y . The dot pro duct of two col-umn vectors u and v is denoted by u T v and the outer pro d-uct uv T . Iteration indices are expressed as sup erscripts. Subscripts indicate comp onen ts of a matrix or vector. A matrix T L represen ts a matrix with t i vectors as its i tors, i = 1 ; : : : ; L . diag ( y ) denotes the diagonal matrix with y as its diagonal. A vector of ones is denoted by e .
PLS creates an LF t whose covariance with the curren t residual ( u , whic h is iden tical to the cen tered resp onse ~ y in the rst iteration) is maximized. This calculation is done iterativ ely so that the mo del is gradually re ned. Another imp ortan t pro cedure is to orthogonalize the LFs at eac h step, whic h is called \de ation". PLS constructs a low-rank appro ximation of the cen tered data matrix, and also the resp onse variable.

The KPLS algorithm introduced in [18, 17] explicitly or-thogonalizes the LFs T : f K i +1 = N f K i N T , where N =
I t i t i T . Note t i is assumed to be normalized to length 1. With SIMPLS form ulation [7], the kernel de ation for multiv ariate PLS is reduced to f K i +1 = N f K i . Of course, the latter form ula is more ecien t and it is pro ven that both de ation form ulations lead to the same solution in case of single resp onse regression problems; see [7] for a pro of.
A more ecien t PLS form ulation is possible if orthogo-nalization of a loadings vector ( w : w = f X T u ) is utilized [10, 8] instead of that of LFs. This PLS varian t is called Or-thogonal Loadings PLS (OLPLS). OLPLS serv es as a basis for the dual sparse KPLS and KBLF.

In OLPLS, loadings vectors f w i g whose pro jecting direc-tion maximizes covariance with the residual at eac h itera-tion, are orthogonalized. The PLS maximization problem is simply written by: Note data and resp onse are assumed to be cen tered. The closed-form solution w = f X T u =r , where r = k f X T u k , is a loading vector and is used to extract a feature vector t = f X w that is used for tting the resp onse variable. In order to ensure orthogonalit y of the loadings, as is done in [8], one may pro ject the data onto the null dimension of w : Then an LF is computed by the loading: t i = f X i w i . The residual u i +1 in the next iteration is thus computed using the LFs obtained by the curren t iteration: u i +1 = ~ y T where c i = arg min c k u i T i c k 2 2 = T i T T i 1 T i least squares optimization ensures u i T T j = 0, ( i &gt; j ) is sat-is ed. With this prop erty, it can be sho wn that the explicit de ation (2) is not needed for dense PLS.

The kernel tric k is used for mapping the input space to the kernel-de ned feature space: x i 7! ( x i ) and the ( i; j ) elemen t of the kernel matrix is K i;j = K ( x i ; x j ) = ( x Thus, XX T 7! K . For kernel OLPLS, the loadings w can-not be expressed explicitly . Thus, the LFs are computed without w : t = f K u , where f K = I 1 m ee T K I 1 m ee [13] explains how to compute the dense KOLPLS solu-tions ecien tly in detail; the de ation can be omitted and form ula for regression coecien ts without cen tering of test kernel is given. The resulting algorithm pro vides better com-putational cost than K-SIMPLS [17].
The dual sparse KPLS, -KPLS [15] is review ed in this section. As a whole, impro ving eciency in -KPLS is a challenging problem. First of all, as seen later, the residual vector is replaced by a sparse vector that is a solution to a dual optimization problem. Orthogonalit y induced by least squares computation, u i T t j = 0 ( i &gt; j ) does not help sim-plify and impro ve eciency of the steps as done in the dense KPLS algorithm. The orthogonalit y relation between fea-tures and residuals was a key for satisfying orthogonalit y in the loadings. Thus an explicit orthogonalization of loadings is necessary . Cen tering is replaced by shifting where only a fraction of data points are used for cen tering the data. The \sparse, incomplete cen tering" does not allo w us to simply introduce a bias term to tak e care of the cen tering of data.
One goal of this section is to give form ulas for out-of-memory computations of -KPLS steps. The entire ker-nel matrix cannot be loaded in memory so that de ation and cen tering/shifting computations must be done on the y. Futhermore, a simple heuristic needs to be dev elop ed to supp ort iterations. SVM, sparsit y is induced by imp osing the -insensitiv e loss. The -PLS primal problem is form ulated as follo ws: s:t: 1 2 jj ( x i ) s u i w jj 2 i ; k w k 2 = 1 where s is a shifting vector that is optimized at the same time as the loading vector w . The introduction of the shift-ing vector is necessary since cen tering requires access the full data. -KPLS computes sparse solutions for maximiz-ing covariance and shifting of data sim ultaneously . The dual problem of Problem (3) is as follo ws: max s:t: In [15], Problem (4) is solv ed by the MA TLAB Optimiza-tion Toolb ox. Appro ximation metho ds will be discussed in Section 3.4.
Using optimal solutions , the loading and shifting, at step l , are obtained as follo ws: w l = L T ^ l ; s l = l
The LF t is de ned as t l = e l w l = ( I e ^ l T ) l w l ( I e ^ l T ) K l ^ l . The LFs are shifted by using SVs only . Since the true cen tering requires all the data points as SVs, the shifting is an appro ximation to the full cen tering. One migh t be tempted to omit shifting the LFs since shifting can be tak en care of by considering a bias term in least squares. However, the laten t features work not only as re-gressors but also as orthogonalizing loadings. Thus, the LFs need to be computed exactly at eac h step in order to ensure orthogonalit y in loadings. Since our de ation is based on orthogonalization of loadings, the data in feature space is pro jected onto the null space of the curren t loadings vector. The shifting is also done sim ultaneously with the de ation: i +1 = ~ i I w i w i T =r 2 i . Thus the kernel de ation for-mula is given as follo ws: Since the kernel matrix is not stored in memory in real situ-ations, de ation needs to be computed on the y whenev er needed. That is, elemen ts of the de ated and shifted ker-nel matrix at any iteration l have to be computed from the original kernel matrix. Hence, the de ation form ula has to be rewritten in terms of the original kernel matrix. It is con venien t to introduce a vector m l suc h that m l = K Using m l , Equation (5) is written as where we de ne m = P be calculated. Only the elemen ts necessary to construct t and m are computed by utilizing the sparsit y of the -KPLS solutions ^ and ^ . By using ^ and ^ , computations for t and m are reduced from O ( m 2 ) to O ( m j sv l j ), where sv is an index set of SVs, sv l = f i j ^ l i 6 = 0 g = f i j l -th iteration. Computing t l = I e b l T K l ^ l can also tak e adv antage of sparsit y. The calculation is divided into two steps: The rst step computes eq 1 := K l ^ l , using Equation (6), i.e., using inner pro ducts T m ^ l and t l T ^ . Note e T the de nition of ^ . The next step computes eq 2 := ^ l T Finally , we get t l = eq 1 ( eq 2) e . Since m l = K l ^ be computed at the same time as t l to avoid re-computing columns in K asso ciated with the SVs.
 As for storage requiremen ts, the t l are stored in memory . For m , the sum of m i 's: m = of inner-pro ducts ^ stored for later iteration steps.

With the extracted LFs, c and 0 are updated:
For prediction, we shift the data and multiply by the re-gression coecien t: b y = ( ( x ) S e ) T g + 0 , where g = W c . At eac h iteration, -PLS constructs W and S using the de-ated and shifted kernel matrix. W and S must be ex-pressed in terms of the original for predicting new data points with the original kernel function. Thus, we de ne matrices A and B suc h that W = T A and S = T B . The form ulas for computing A and B are given as follo ws: a = ( I see [15] for deriv ation, unfortunately space does not allo w us to give it here.

Once A and B are calculated, the regression coecien t and bias become g = W c = T A c and e T S T g = e T B T T Prediction for test data is: where e g = A c and = e T B T KA c + 0 . By using the def-inition of B , we have KB = m 1 m 2 m L . Thus, KB e = easily computed by = L m T ~ g + 0 .
The objectiv e function in (4) can be rewritten by f ( ) = T diag ( K ) 1 f Let q = diag ( K ) 1 f 2(
P written in terms of q : max T q = max Our prop osed heuristics rst computes q ( i ) using m s SVs sampled randomly from data. The SVs are assigned con-stan t weigh ts: 0 i = 1 =m s , i 2 RS , where RS is the index set of the randomly sampled points. The multipliers for all other points are set to be zero: 0 i = 0, i = 2 RS . Then we maximize the follo wing appro ximation to the original prob-lem: The solution for this appro ximation of the original problem has positiv e values for the m elemen ts i asso ciated with the top m -largest elemen ts in q ( 0 i ): i = 1 m , i 2 sv , and 0 values for the rest: i = 0, i = 2 sv . At any iteration l , Equation (11) is solv ed based on the de ated matrix K l and the residual u l using Equation (5). We call this a maxim um inner pro duct criterion (MI).

Accuracy of the appro ximation would be impro ved if the above steps were iterated. However since our goal is to de-velop ecien t ways of appro ximating the much more dicult problem (4), it is not preferable to perform an iterativ e com-putation here. Furthermore, this appro ximation may not even be sucien t for our purp ose since it involves kernel computation, whic h can be exp ensiv e dep ending on the ker-nel. Therefore we prop ose an even more ecien t approac h. Inspired by [16], we prop ose a more ecien t approac h based on the maxim um residual (MR) criterion. In this approac h, the kernel computation suc h as that in Equation (11) is omitted. Instead, the SVs are determined as those asso-ciated with m largest elemen ts of the absolute value of residual vector u . [16] chooses only a single data point that has the largest residual as an SV at eac h iteration. In con-trast to their metho ds, our approac h for -KPLS framew ork selects the m data points with the largest residuals as SVs. The follo wing summarizes the out-of-memory -KPLS: KBLF extends KPLS by interpreting PLS in terms of AnyBo ost, then replaces the least squares loss with an ar-bitrary loss function: loss ( y ; f ), where f is a vector of pre-diction/h ypothesis. In sparse KBLF, KOLPLS is used to extend the least squares loss in KPLS; replace u by a gradi-ent of a given loss function and least squares calculation of c with appropriate optimization metho d (the Newton step is used in numerical exp erimen ts in this pap er). Note that these changes do not a ect sparsi cation of KPLS. That is, -KPLS basically sparsi es solutions for maximizing the inner pro duct between f K u and . Since KBLF does not change the inner pro duct criterion for an LF creation, the -KPLS algorithm is directly applicable to -KBLF.

Thus, only a small change is required in Algorithm 1 for a -KBLF form ulation:
First, evaluation and analysis on the heuristics are needed to understand how e ectiv e they are. Therefore, in Figure 1, error curv es (EC) and fraction of SVs (FSV) are com-pared among the follo wing settings: the original problem (4), appro ximation by MI criterion (11), and MR criterion using the Boston Housing data [4]. The original problem is solv ed by MA TLAB and the results are tak en from those in [15]. For kernel, the radial basis function (RBF) is used: k ( x i ; x j ) = exp k x i x j k 2 = 2 . The kernel parameter is xed to be = 4 : 24. All the exp erimen ts are done based on 10-fold cross validation(CV). The splits of training and test sets are iden tical among di eren t exp erimen ts. The Q -statistic, whic h is the mean squared error normalized by variance, Q 2 =
At = 0 : 5, the original solution obtains the best result using as much as 86% of the data points as SVs. The frac-tion of SVs for MI beha ves similarly to that for the original problem but with better generalization. The learning speed itself slows down to some exten t due to the appro ximation, but it enlarges the set of good choices of the num ber of LFs for generalization, just like shrink age in boosting. For MR, sparsit y is inferior to other metho ds. The FSVs gro ws fastest of all. However, if is set to be small, suc h as = 0 : 1 in the gure, it sho ws good generalization with small num bers of SVs; MR needs to set small to achiev e better sparsit y for this data set.

Next, generalization abilit y with mo del selection is inves-tigated using Boston Housing and Abalone [4]. = 5 : 66 is used for all the mo dels for Abalone. For -SVM, given a value of , the mo del parameter is the trade-o constan t C . -KPLS and the appro ximations, the mo del parameter is the num ber of LFs. The mo del selection metho d utilizes a 10-fold CV error estimate (inner CV) using a training set. After the mo del parameter is selected, a nal mo del is cre-ated using the whole training set and the test set is used for evaluation. The training-test splits are based on 10-fold (outer) CV. Note that the full problem was not solv ed for in-ner cross validation runs because of the high computational cost for the full problem by the MA TLAB solv er used in [15]. Hence, it should be seen as a reference, when compared with other metho ds.

Figure 2 sho ws generalization performance with resp ect to sparsit y of mo dels: a good mo del would have less error and parsimonious. Values for parameter are also sho wn in the gure. For Boston Housing, both MI and MR mo d-els have comparable performance with -SVM with similar level of sparsit y. For Abalone, -SVM obtained the best performance with small margin to other mo dels, but could not reduce SVs with small . The -KPLS appro ximations sho w stable beha vior for small .
For -KBLF, various classi cation datasets from the UCI rep ository are used for comparing -KBLF mo dels and also C -SVM. The loss function used are squared loss, exp onen-tial loss and logistic loss. Note -KBLF with squared loss is equiv alen t to -KPLS. For all exp erimen ts of -BLF, a com bination of the two heuristics for choosing SV's is used: for the rst iteration, use MI to select SV's then use MR for later iterations. The reason to use MI in the rst iteration is man y pseudo-residuals ( u ) have same value in classi cation problems. For later iterations, MR is used for eciency .
For all exp erimen ts, the RBF kernel is used and the pa-rameter is determined by preliminary runs for eac h data set. The num ber of LFs for BLF and C for SVM are chosen by 10-fold CV inside the training set. The num ber of C 's is about 10 dep ending on complexit y of data and the maxi-mum num ber of LFs were 30-40. Further, is set to be 0 : 05 to exploit sparsit y for -BLF mo dels. Other choices may enhance the performance but at a loss of sparsit y. For all datasets, 100 rep etitions of the above exp erimen ts are per-formed. A summary of the exp erimen ts is sho wn in Table 1. The performance of -KBLF mo dels is sligh tly worse than that of SVM's though the di erences are small. However, -BLF mo dels successfully construct sparser mo dels in man y cases. The degradation of performance needs to be analyzed further. Finally , the scalabilit y of the appro ximate metho ds with -KPLS is investigated using a larger data set: DEL VE Housing8H [1] with 22,784 data points and 8 attributes. The training time is measured as the training set size is in-creased. An appropriate sized training data set is randomly selected from the whole data set and applied to all the meth-ods. Since -KPLS is an iterativ e metho d, a stopping crite-rion must be determined. Thus rst we ran SVMT orch [5] and determined what training error level would be sucien t. Note that SVMT orch utilizes -SVM and it is not iden tical to -SVM. In general, a -SVM solv er requires more time for training than an -SVM based solv er. After scaling the resp onse variable to have mean zero and standard deviation 1, the stopping criterion is set to be the mean squared error 0.45. The two appro ximation metho ds in -KPLS are im-plemen ted in MA TLAB C{API and a Pentium III 1200MHz pro cessor mac hine is emplo yed.

For MI, the kernel computation in Problem (11) is a bottle-nec k. To alleviate the exp ensiv e kernel evaluation, kernel cac hing is utilized. The cac hed SVs are the ones that are selected by earlier iterations and they are updated heuristi-cally . The cac he size for MI is set to be 200 and is 0.2 for both metho ds.
Figure 3 illustrates the results for SVMT orch, -KPLS appro ximation with MI and MR criterion. MI could not sho w a better scalabilit y than SVMT orch. Selection of SVs turned out to be too exp ensiv e for a large data set in the -KPLS framew ork. However, the MR criterion does not need to select SVs. Thus the scalabilit y of MR is much better than MI and it sho ws very similar beha vior to that of SVMT orch.

We can further enhance scalabilit y by using smaller value of although man y iterations are required. For example, if we choose a single new SV at one iteration, whic h be-comes similar to basis selection algorithms, 270 iterations were needed for 20 ; 000 points, in con trast to 27 for = 0 : 2. The result with this approac h is also sho wn as \ = min" in Figure 3. There is a trade o for reducing the num ber of LFs and iterations, but smaller can exploit sparsit y signif-ican tly, and if the data only requires sparse represen tation for mo deling, choice of small would be appropriate.
Some heuristics for appro ximately solving -KPLS were prop osed to impro ve eciency and scalabilit y of the original -KPLS algorithm. The appro ximations work well in prac-tice and computational results sho wed they are comp etitiv e with full solutions in terms of generalization abilit y.
The -KPLS framew ork involves shifting of data and max-imizing covariance with only a small fraction of training data. All the computations are now done out-of-memory so large scale problems can be dealt with. De ation and shifting are ecien tly done on the y by using the small num ber of vectors that are stored in memory . When solving large scale problems where kernel computation dominates the computation, using the very small num ber of SVs can help impro ve scalabilit y signi can tly. KPLS from dense KPLS does not a ect -KBLF form ulation so that scalable -KPLS framew ork was directly applied to -KBLF. The results sho w -KBLF can build parsimonious mo dels but more re ned analysis will be needed for further impro vemen t of the heuristics and also the algorithm, whic h is a future problem.
 Ackno wledgmen ts I thank Prof. Kristin Bennett for fruit-ful discussions and Dr. John Byrnes for pro ofreading.
