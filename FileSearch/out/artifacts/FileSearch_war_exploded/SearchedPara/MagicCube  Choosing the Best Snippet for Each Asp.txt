 Wikis are currently used in business to provide knowledge manage-ment systems, especially for individual organizations. However, building wikis manually is a laborious and time-consuming work. To assist founding wikis, we propose a methodology in this paper to automatically select the best snippets for entities as their initial explanations. Our method consists of two steps. First, we focus on extracting snippets from a given set of web pages for each en-tity. Starting from a seed sentence, a snippet grows up by adding the most relevant neighboring sentences into itself. The sentences are chosen by the Snippet Growth Model, which employs a distance function and an influence function to make decisions. Secondly, we pick out the best snippet for each aspect of an entity. The combi-nation of all the selected snippets serves as the primary description of the entity. We present three e ver-increasing methods to handle the selection process. Experimental results based on a real data set show that our proposed method works effectively in producing primary descriptions for entities such as employee names. H.3.3 [ Information System ]: Information Storage and Retrieval-Information Search and Retrieval Algorithms Snippet, Entity, MagicCube, Wiki Wikis are geting more and more popular in the web users X  life. Nowadays many organizations construct wikis as their knowledge  X 
Supported by NSFC under Grant No.60673129 and 60773162, 863 Program under Grant No.2007AA01Z154, and the 2008/2009 HP Labs Innovation Research Program  X  Corresponding author management systems. However, there are two main problems dur-ing constructing the wikis, time-consuming and laborious. To solve these two problems, we propose a method named as MagicCube which can automatically construct a lexicon just like the Wikipedia. MagicCube consists of two steps.

Snippet extraction: How to extract segments from web pages to form a snippet for an entity? We propose a model named as Snippet Growth Model to address this challenge. In this model we use WordNet [5] to expand the semantics of words. And then we present two functions which are distance function and influence function to decide whether a sentence should be included or not.
Snippet selection: How to select the best snippets to describe or summarize an entity? In our work, we classify the snippets of each entity, and choose t he top several ones in each class as the description of the entity.

We will demonstrate the effectiveness of MagicCube by experi-ments running on a real data set which was from HP Labs China. The results indicate that our method is really promising.
There are lots of studies on summarization recently [10, 11, 9, 7, 1, 6]. Our work differs from these works which focus on document classification and segment summarization. Our endeavors are to find out the snippets which are the best to describe the entity in general or in part.

Oren Etzioni et al. introduce KnowItAll in [4]. KnowItAll is based on patterns to extract fact by rules. Michael J.Cafarella et al. proposed KnowItNow [3] system to optimize the speed of extract-ing of KnowItAll. Though both the KnowItNow and our method focus on extracting the fact and information from web, the compo-nents and methods are different. Besides, we aim at automatically constructing wikis for organizations.
Let us make some explanations for Entity and Snippet. They are the key terms in our study. (1) Entity: En tities are words and com pound words that are used in specific context. These words have specific definitions within the field, which is not necessarily the same as their meanings in common use. (2) Snippet: A snippet is a small piece text of a web page, which is composed of some consequent sentences. Each snippet describes an entity partially and a snippet X  X  content can be part of the descrip-tion of the entity.

Our goal is to automatically generate primary descriptions for each entities. To achieve this we must get entities first. In this study, our data set provides the entities such as the names of employees and products.
Figure 1: The flow diagram of the Snippet Growth Model
To extract a snippet we must find out the sentences which are descriptions of entities. There are three assumptions for finding related sentences. (1) The sentence which contains the entity has the strongest abil-ity to describe the entity. We call it as the head-sentence. (2) The sentences around the head-sentence also have certain ability to describe the entity. But their abilities decrease along with the distance to head-sentence. (3) Any two sentences in the same snippet may contain some related words. The number of related words they have indicates the relevance of them. This relevance decreases as the distance between two sentences increases.
 Our model is based on these assumptions.

Firstly, we find the head-sentence according assumption 1. And then we expand the snippet around the head-sentence. It is im-portant to note that the sentences in a snippet are consequent. So we only consider the nearest neighboring sentences for each time. If either of them matches the condition it will be added into the snippet. Otherwise, if neither of them matches the condition the growing process will be ended.
 This process is very similar with the growth process of a plant. Therefore, we call this model as Snippet Growth Model. The flow diagram is shown in Figure 1.

According to assumption 3, we compare two candidate sentences with every sentence in the snippet to see if they contain the same words. It deserves noting that we only consider the nouns in sen-tences, because other parts of speech cannot indicate the relevance of two sentences.

Supposing there is a snippet S { s 1 , s 2 ,..., s n } expected to be ex-panded. n is the number of sentences in the snippet. s sentence in the snippet. We use s j to denote the sentence nearest to the snippet. The formula is as follows.

R j indicates the relevance of s j and S . s j  X  s i signifies the number of common nouns between s i and s j .If R j is larger than a threshold  X  , s j will be added into the snippet.
 The problem is, if we just take the original sentences to compute R , the result may disappoint us. Even if two sentences are quite Figure 2: The distribution diagram of distance function when l =10 interrelated, the number of common words that they contain is very low. Maybe some words are semantically related, however, it does not necessarily mean that these words are exactly the same. In order to solve this problem we use the WordNet to make semantic expansion.
WordNet is a lexical database for the English language. For each word in a sentence we find its synonyms, hypernyms and hyponyms by WordNet. As mentioned above, we only expand the nouns in the sentence. By this way the original sentence will be expanded to a new set of words. And then we conduct computation on these sets of words instead of original sentences.
 Based on Formula 4.1, we can get Formula 4.2 as follow.

W j is the set of words which is generated by expanding s Wor dN et .

There is still a problem. In this way, we do not consider the distance between two sentences. It makes no difference to the rel-evance of any two sentences regardless of the distance between them. That does not make sense. To solve this problem, we propose a distance function and add it into our formula.
According to the assumption 3, two sentences X  relevance de-creases as their distance increases. The distance of s defined as the number of sentences between s i and s j . d ij denotes the distance between sentence s i and sentence s the number of sentences that the snippet contains. Along with the increasing of d ij ,thevalueofdf( s i , s j ) attenuates. The distribution diagram of distance function is shown in Figure 2.

We t ake df ( s i , s j ) as a weight when compare two sentences. So we get Formula 4.4.

Using this method we avoid that two sentences with great dis-tance have high relevance. But this way still cannot solve the bound-ary problem. A snippet may grow as big as the whole web page, if there is a sentence s i  X  S which is close to the sentence s the df value between them is high. So we propose an influence function to handle this.
In the discussions above, we consider the sentences in the snippet equally. We compute the relevance of each pair of two sentences and add them together. Actually the sentences are not equal. Ac-cording assumption 2, the sentences X  ability to describe the entity decreases along with the distance to head-sentence. So we should introduce a weight for each sentence in the snippet. We propose an influence function. x i denotes the coordinate of s i ( s i  X  S). The head-sentece X  X  coor-dinate makes the max value of the function. We use c to denote it.

If s i is before the head-sentence, d i denotes the distance between s i and the head-sentence.
Otherwise, if s i is after the head-sentence,
The affect of Formula 4.6 and 4.7 is just fits the distance of two sentences to the coordinate system.  X  is a coefficient. It decides the size of the snippets to some extent. If  X  is too big, it may add no more sentences into the snippet besides the head-sentence. Because, the value of if attenuates too fast in this condition. In the other side, if  X  is too small, the size of a snippet will be too large.
The influence function X  X  prototype is probability density func-tion. It belongs to log-normal distribution.  X  and  X  are the mean and standard deviation of the variable X  X  natural logarithm. The in-fluence function clearly shows us the attenuation process of the sen-tences X  description ability. The if value denotes their ability. The distribution diagram of influence function is shown in Figure 3.
We note that the function attenuates faster when x i &lt; c . It makes sense. Usually when we describe a thing, we put the main content after its name. According to these we get Formula 4.8.

In this section, we gradually propose three ever-increasing meth-ods to select suitable snippets for each entity. First of all, we use LDA model [2] to handle this. We carry on the LDA to each entity X  X  snippets. The input is snippets instead of the documents in traditional LDA model. And the output is the topic-snippet distribution: p(topic|snippet) . For each topic t ,wepick
LDA and K-means method 0.5 0.1 100 500 out a snippet m , which makes the max value of p(topic t |snippet) . Supposing the number of topics is n . Through the method we can get n snippets for each entity. These snippets are the primary descriptions.
In this section we propose a new method by carrying on K-means clustering algorithm [8] to the result of the LDA method. As mentioned above, we can get p(topic|snippet) by LDA method. Based on this we build a vector:
The vector reflects the relation between a snippet and the top-ics. Using the vectors we can compute the similarity between two snippets.
All the vectors make up a matrix. We conduct K-means to the matrix. The output is clusters of snippets. We believe that each cluster describes an aspect of the entity. The center of each cluster is the most representative one. But in K-means the center does not necessarily exist. So we take the closest one as the center.
We notice that there are some noises in our data set. Such as the common word  X  X he X ,  X  X his X  and so on. To deal with this problem we conduct the tf-idf algorithm on the data set. For each snip-pet we drop the words whose tf-idf is smaller than the threshold t . And then we conduct the LDA and K-means method on the dataset which has been preprocessed. This is our third method. Our experiments are based on a real data set which is from HP Labs China. The data set consists of more than 560,000 web pages. We get the information of the employees which occur in the data set from the HP Labs China. From these names we pick out the top 1000 frequent appearan ce ones as e ntities. Using Snippet Growth Model we extract snippets for each entity. There are numerous parameters values that need to be set for the algorithm. In particular, the  X  and  X  in formula 4.5 are set at 0 and 0.75. The coefficient  X  is 0.15. And the threshold at 0.5. These parameters are all set by experience. As a result of the extracting snippets, there are 421 snippets for each entity and 5 sentences in each snippet on average.
In this section we carry on our three selection methods to the snippets.
Grade Entity snippet The parameters of the LDA are shown in Table 1. The parameter K which denotes the number of clusters in K-means algorithm is set at 10. In the experiment of The LDA and K-means with data preprocessing, for each entity X  X  snippets, we set the threshold t as a half of the average of the tf-idf values.
In this section we will evaluate the results of the three proposed methods. We judge the snippet X  X  quality by its capacity of descrip-tion for the entity.

We randomly select 100 entities and th eir final snippets which are picked out by the three models. The snippets X  qualities are judged by three volunteers. Each snippet is rated on a scale of 1-5, where 1 is the worst, and 5 is the best description of the entity. Some examples are shown in Table 2.
 For each model, we compute an average score of the snippets. The result is shown in Figure 4. Suppose we pick out n snippets for each entity. There are total n X  snippets generated by the three models. n X &lt;=n , because that there are some overlap of the three models X  result. We pick out the top n snippets from them by the grades. m is the number of snippets which contain in both the top n set and a model X  X  snippets set. We use m/n to denote the precision of each model X  X  result. The precisions for the three models are shown in Figure 5.
 Through the evaluations above, we can see that our LDA and K-means method with data preprocessing indeed outperforms the others.
In this paper, we point that building wikis manually is a laborious and time-consuming work. To assist founding wikis, we propose a methodology to automatically select the best snippet for each as-pect of an entity. Our MagicCube is consisted of two steps. In step 1, we present a Snippet Growth Model to extract snippets from web pages. In step 2, we present three methods to select some represen-tative snippets for each entity. The results of experiments show that MagicCube is highly effective.

As our future work, we will try to expand the entities to other types. We will study how to extract the entities from web pages automatically. We will also attempt to propose better method to select the snippets for each entity. [1] A. L. Berger and V. O. Mittal. Ocelot: A system for [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [3] M. J. Cafarella, D. Downey, S. Soderland, and O. Etzioni. [4] O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu, [5] C. Fellbaum, editor. WordNet An Electronic Lexical [6] J. Goldstein, M. Kantrow itz, V. Mittal, and J. Carbonell. [7] K. Knight and D. Marcu. Statistics-based summarization -[8] J. B. MacQueen. Some methods for classification and [9] X. Wan, J. Yang, and J. Xiao. Collabsum: exploiting multiple [10] X. Wan, J. Yang, and J. Xiao. Manifold-ranking based [11] X. Wan, J. Yang, and J. Xiao. Towards an iterative
