 This paper presents a boosting based algorithm for learning a bipartite ranking function (BRF) with partially labeled data. Until now different attempts had been made to build a BRF in a transductive setting, in which the test points are given to the methods in advance as unlabeled data. The proposed approach is a semi-supervised inductive ranking algorithm which, as opposed to transductive algorithms, is able to infer an ordering on new examples that were not used for its training. We evaluate our approach using the TREC-9 Ohsumed and the Reuters -21578 data collections, comparing against two semi-supervised classification algo -rithms for ROCArea ( AUC ), uninterpolated average precision ( AUP ), mean precision@50 ( mT9P ) and Precision-Recall ( PR ) curves. In the most interesting cases where there are an un-balanced number of irrelevant examples over relevant ones, we show our method to produce statistically significant im-provements with respect to these ranking measures. H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -Information Routing; H.1 [ Models and Principles ]: Miscellaneous Algorithms, Experimentation, Theory Learning to Rank with partially labeled data, Boosting, In-formation Routing
Learning with partially labeled data or semi-supervised learning has been widely studied under the classification framework [5, 13, 16, 20, 3, 11, 8]. However, there are many applications where the goal is to learn a ranking function an d for which training sets are hard to obtain mainly because the labeling of examples is time consuming, and sometimes even unrealistic. This is for example the case for domain specific search engines or routing systems [18] where, for a stable user X  X  information need there is a stream of incom-ing documents which have to be dynamically retrieved. The constitution of a labeled training set is, in this case, a diffi -cult task. But, if such a training set is available the goal of learning would be to find a bipartite ranking function (BRF) which assigns a higher score to relevant examples than to ir-relevant ones 1 .

Recently some studies have addressed the use of unlabeled data to learn a BRF in a transductive setting [1, 22, 24, 23]. In this case, one is given sample points from a labeled train-ing set and an unlabeled test set, and the goal is to build a prediction function which orders only unlabeled examples from the test set. This restriction makes the design of a ranking function that assigns scores to new examples inher-ently inconvenient. Neither can existing work on leveragin g unlabeled data for classification of pairwise examples be ap -plied directly to learn a BRF. As some pairs would have the same instances in common, this would violate the indepen-dence assumption on which classifier learning is based.
In this paper we present a new inductive ranking algo-rithm which builds a prediction function on the basis of two labeled and unlabeled training sets: one labeled and one un-labeled. In a first stage, the algorithm loops over the labele d set and assigns, for each labeled training example, the same relevance judgment to the most similar examples from the unlabeled training set. An extended version of the Rank-Boost algorithm [10] is then developed to produce a scoring function that minimizes the average number of incorrectly ordered pairs of (relevant, irrelevant) examples, over the la-beled training set and the tentatively labeled part of the unlabeled data. The novelty of the approach is that the al-gorithm optimizes an exponential upper bound of a learning criterion which combines the misordering loss for both part s of the training set.

Experiments on the TREC-9 Ohsumed and Reuters -21578 datasets, show that the proposed approach is effective on AUC , AUP , mT9P and PR ranking measures especially when there are much less relevant examples than irrelevant ones. In addition, comparisons involving two semi-supervised cl as-
These forms of ranking correspond to the bipartite feed-back case studied in [10] and we hence refer to them as the bipartite ranking problems. sification algorithms indicate that, as in the supervised ca se [7, 9], the error rate of a semi-supervised classification fu nc-tion is not necessarily a good indicator of the accuracy of the ranking function derived from it.

In the remainder of the paper, we discuss, in section 2, the problem of semi-supervised learning for bipartite rank -ing. In sections 2.1 and 2.2, we present a boosting based algorithm to find such ranking functions. In section 4, we present experimental results obtained with our approach on the TREC-9 Ohsumed and the Reuters -21578 datasets. Fi-nally, in section 5 we discuss the outcomes of this study and give some pointers to further research.
In bipartite ranking problems such as information routing [17], examples from an instance space X  X  R d are assumed to be relevant or irrelevant to a given search profile or topic . The goal of learning can then be defined as the search of a scoring function H : X  X  R which assigns higher scores to relevant instances than to irrelevant ones [10].

In this setting, the system is usually given a sample of n x  X  X   X  is associated with a relevance judgment y i  X  X  X  1 , 1 } , representing the fact that x i is relevant ( y i = +1) or not ( y i =  X  1) to that topic. The learning task reduces to min-imization of the average number of irrelevant instances in Z  X  scored better than relevant ones by H [10]. In the semi-supervised setting, we further assume that together with labeled examples in Z  X  , we also have a set of m unlabeled a ranking function on the basis of these two training sets.
In order to exploit information from X U , we assume that an unlabeled instance from X U that is similar to a labeled instance from Z  X  should have similar label. We begin by selecting examples in X U that are the most similar to a labeled example x  X  Z  X  and assign them the correspond-ing relevance judgment y . At this stage, a simple approach would consist in adding these new examples from X U to the labeled training set and then learn a ranking function as in the usual supervised case. This training scheme suf-fers from a real drawback in that as unlabeled examples are given error-prone labels, the resulting ranking would b e highly dependent on how robust the training scheme is to noisy labels. Therefore, instead of mixing Z  X  with selected examples in X U , we suggest minimizing the ranking errors on each training sets separately.

Formally, let A C be an unsupervised algorithm which spec-ifies, for each labeled data, the unlabeled instances that ar e the most similar to it. And, let Z U\ be the set of unlabeled examples obtained from A C that have been assigned labels according to similar labeled data. Our goal is thus to find a function H , which minimizes the average numbers of irrel-evant examples scored better than relevant ones in Z  X  and Z
U\ separately. We call this quantity the average ranking loss ,  X  R n + m , defined as:  X  R
Where, X + and X  X  (resp.) represent the sets of relevant and irrelevant (resp.) instances in Z  X  , while X  X  + = A and X  X   X  = A C ( X  X  ) (resp.) are the sets of unlabeled data which are similar to relevant and irrelevant (resp.) instan ces in Z  X  . P Z  X  = |X  X  ||X + | and P  X  Z number of relevant and irrelevant pairs in Z  X  and Z U\ (resp.). ( x + , x  X  ) and ( x  X  + , x  X   X  ) (resp.) denote pairs of (relevant, ir-relevant) examples in X +  X X  X  and X  X  +  X X  X   X  (resp.). Finally,  X  is a discount factor 2 and [[  X  ]] is equal to 1 if the predicate  X  holds and 0 otherwise.

Several successful Machine Learning algorithms, includin g various versions of boosting and support vector machines ar e based on replacing the loss function by a convex function [4] . This approach has important computational advantages, as the minimization of the empirical convex functional is fea-sible by, for example, gradient descent algorithms. In the following we extend the majoration of the supervised rank-ing loss proposed by [10] to the semi-supervised bipartite ranking case. Using the upper bound [[ x  X  0]]  X  e x , we get the following exponential loss: E
As for the meta-search task [21] and following [10], we de-fine the final ranking function, H = P t  X  t f t , as a weighted sum of the ranking features f t . Each ranking feature f t uniquely defined by an input feature j t  X  { 1 , ..., d } and a threshold  X  t : where,  X  j ( x ) is the j th feature characteristic of x .
The learning task is then defined as the search of the com-bination weights  X  t and the ranking features f t for which the minimum of the exponential loss E n + m ( H ) is reached.
The ranking problem presented in the previous section can be implemented efficiently by extending the RankBoost algorithm proposed in [10]. This extension iteratively mai n-tains two distributions D t and  X  D t over pairs of (relevant, irrelevant) examples in Z  X  and Z U\ .

At the beginning, all pairs are supposed to be uniformly distributed, that is,  X  ( x  X  , x + )  X  X  X   X  X + , D 1 ( x each round, D t and  X  D t are then gradually updated in order to give increasing weights to pairs that are difficult to rank correctly.

The weight for each pair is therefore increased or de-creased depending on whether f t orders that pair incorrectly, leading to the update rules: and  X 
For  X  = 0, we fall back to the situation of standard super-vised learning. where Z t = X
X such that D t +1 and  X  D t +1 remain probability distributions.
The search of the ranking feature f t and its associated weight  X  t are carried out by directly minimizing the expo-nential loss, E n + m . This optimization is performed first by noticing that the exponential loss E n + m writes: where, T is the maximum number of rounds. This rewriting results from the update rules (2-3), the use of a linear ranke r H = P T t =1  X  t f t , the exponential homomorphism property e x + y = e x e y and the fact that D t and  X  D t sum to one for every t .

The selection of ranking features is presented in section 2.2. The choice of the weight combinations,  X  t , results from the minimization of (4). At each iteration, this minimizati on is performed by rewriting the exponential loss as be upper-bounded by the following expression:
E where, r t = X
X
This follows from Jensen X  X  inequality and the convexity of e  X x , which yields e  X x  X  ( 1+ x
The right-hand side of the above inequality is minimized when: Plugging this back into the inequality yields
E The complexity of the algorithm, if implemented with dis-tributions D t and  X  D t , is O ( | X  X  || X + | + | X  X  supervised case, it is possible to reduce this complexity to O ( n + | X  X   X  | + | X  X  + | ) by setting Where  X  t and  X   X  t are two sets of weights over respectively Z and Z U\ .

Thanks to the homomorphism property of the exponen-tial, we have Z t = Z  X  t .Z + t , with Z  X  t = X and Z + t = X Algorithm 1 : Learning BRF with partially labeled data
Given : Z U\ = X  X  +  X  X  X   X  , a labeled subset of X U obtained from x  X  X  X  , for t := 1 , ..., T do end Output : The final ranking function H = a consequence, eqs. (8) and (9) are preserved by the update rules for D t and  X  D t (eqs. 2 and 3), and hold on round t + 1.
The pseudocode for this implementation is given in algo-rithm 1, where we have stated the update rules in terms of  X  and  X   X  instead of D and  X  D . At each iteration of the algo-rithm, first a ranking feature f t and its associated weight  X  are chosen in order to minimize the empirical exponential loss E n + m . Then  X  t and  X   X  t are updated, and finally the co-efficients A t and B t are estimated for the calculation of the loss (5) in the next round.
In this section, we work for a given t and therefore drop t from the notation.

As in the supervised case [10], ranking features f t can be learned efficiently in a greedy manner. Indeed, since each ranking feature is { 0 , 1 } -valued (equation 1), learning re-duces to the search in the discrete space of a feature char-acteristics j and thresholds  X  of the minimum of the upper-bound (7). This is equivalent to maximizing | Ar +  X B  X  r | .
Algorithm 2 : Semi-supervised learning of ranking fea-tures
Given :  X  1  X  ....  X   X  K , for j := 1 , ..., d do end Output : (  X   X  j ,  X   X  ) Let us first rewrite Ar +  X B  X  r in terms of  X  ,  X   X  and f :
Ar +  X B  X  r = A X As f is { 0 , 1 } -valued and  X  (resp.  X   X  ) sums to one on each subset X  X  (resp. X  X   X  ) and X + (resp. X  X  + ), this equation can be rewritten as
The search algorithm for the candidate ranking feature f  X  is described in algorithm 2. For each feature charac-teristic j  X  X  1 , ..., d } , the algorithm incrementally evaluates | Ar +  X B  X  r | on a sorted list of candidate thresholds {  X  and stores the values j  X  and  X   X  for which | Ar +  X B  X  r | is maximal. Thus a straightforward implementation of this algorithm requires O (( n + | X  X   X  | + | X  X  + | )  X  K  X  d ) time to generate a ranking feature.
We conducted a number of experiments aimed at evalu-ating how unlabeled data can help to learn an efficient bi-partite ranking function. To this end, we ran two versions of the supervised RankBoost (RB) algorithm [10]. The first one uses the labeled training set only: this provides a base-line which we hope to outperform thanks to the unlabeled data. The second uses both the labeled training set as well as the unlabeled training set with their true labels : this pro-vides an upper bound on the achievable performance, as the latter labels are not available in the semi-supervised sett ing. This comparison gives a first insight into the contribution o f unlabeled data for learning a BRF.

We also compared our algorithm with two semi-supervised algorithms proposed in the classification framework. The first one is the so-called transductive SVM (TSVM) imple-mented in SVMlight [13]. The second is an EM-like algo-rithm which was successfully applied to extractive documen t summarization [2]. It operates by first training a logistic regression (LR) classifier on the labeled training set, then outputs of the classifier are used to estimate class labels fo r unlabeled data and a new classifier is learnt on the basis of both the labeled data and these newly labeled instances. These two steps (labeling and learning) are iterated until a local maxima of the complete data likelihood is reached. We refer to this second algorithm as semi-supervised LR, or ssLR.

The unsupervised algorithm A C used for the constitution of Z U\ was the nearest neighbors ( NN ) algorithm. For each example in the labeled training set Z  X  , we assigned the same label to k of its nearest neighbors in X U . The choice of the NN algorithm for A C here is essentially motivated by its com-putational efficiency. We refer to our proposed algorithm by ssRB which stands for semi-supervised RankBoost.

Finally, each experiment is performed over 10 random splits (labeled training/unlabeled training/test) sets o f the initial collection. We conducted our experiments on TREC-9 Ohsumed and Reuters -21758 datasets. Following TREC-9 filtering tasks, the selected topic categories in Reuters -21758 served as fil-tering topics. We shall now describe the corpora and method-ology.
The Ohsumed document collection [12] is a set of 348 , 566 articles from the on-line medical information database (ME D-LINE) consisting of titles and abstracts from 270 journals over a period of 5 years (1987 to 1991). We carried out our experiments on 63 topics defined for the routing track of the TREC-9 Filtering tasks [17]. The number of rele-vant documents varies from 5 to 188 with an average of 59 . 8 relevant documents per topic. We indexed documents hav-ing an abstract (with an existing .W field -this represents 233 , 445 documents) and took terms appearing in the title, abstract as well as human assigned MeSH indexing terms. All words were converted to lowercase, digits were mapped to a single digit token and non alpha-numeric characters were suppressed. We also used a stop-list to remove very frequent words and also filtered terms occurring in less than 3 documents.
The Reuters -21578 collection contains Reuters news arti-cles from 1987 [15]. We selected documents in the collection that are assigned to at least one topic. Each document in the corpus can have multiple labels, but in practice more than 80% of articles are associated to a single topic. In ad-dition, for multiply-labeled documents, only the first topi c from the &lt; TOPIC &gt; field was retained.

We only considered documents associated with the 10 most frequent topics, which resulted in 9509 documents, each with a unique label. We carried out the same pre-Table 1: The Reuters topics used in our experiments. processing as for the Ohsumed data set. The distribution of the number of relevant documents per topic, given in table 1, varies from 1 . 67% to 41 . 77%. These relatively populous topics will allow us to test the behavior of bipartite rankin g algorithms when relevant documents are gradually removed from well-represented topics.
In order to compare the performance of the algorithms we used a set of standard ranking measures.

As the learning criterion (4) we used to train our model is related to the area under the ROC curve ( AUC ), we first compared the AUC measure of each algorithm on the test set. If a sample T contains p relevant and m irrelevant instances, the AUC of a scoring function h with respect to this sample represents the average number of relevant examples in T ranked higher than irrelevant ones [6]: In addition, we computed the mean average uninterpolated precision ( mAUP ) and the mean precision@50 (referred as mT9P [17] in the following) across topics on both datasets.
The average uninterpolated precision ( AUP ) of a given topic  X  is defined as the sum of precision value of relevant docu-ments in the r top ranked documents divided by the number of relevant documents for that topic, R (  X  ). Hence, relevant documents which do not appear in the top r ranked docu-ments receive a precision score of 0: AUP (  X  ) = We used r = 500 on the Reuters dataset, and r = 1000 on the Ohsumed collection[17].

Finally, we plot the Precision/Recall curves [19] of dif-ferent ranking algorithms for a fixed percentage of labeled-unlabeled documents in the training set. At each recall leve l, the precision score is averaged over all topics.

Each reported performance value is the average over the 10 random splits.
These experimental results test how unlabeled data affect the ranking performance of the proposed approach, vs. both of the semi-supervised classification algorithms.
We start our evaluation by analyzing the impact of unla-beled data on mean average uninterpolated precision ( mAUP ) and mean precision@50 ( mT9P ) for a fixed number of labeled and unlabeled examples in the training set of each topic in Ohsumed and Reuters datasets. In order to effectively study the role of unlabeled data on the ranking behavior we begin our experiments with very few labeled training examples. For Ohsumed , the size of the labeled training sets is hence fixed to 180 documents per topic: 3 relevant and 177 ir-relevant. For Reuters , we use 90 documents per topic: 9 relevant and 81 irrelevant documents. The remaining docu-ments from the collection are used as unlabeled data. The lower proportion of relevant documents in our Ohsumed ex-periments reflect the higher imbalance between relevant and irrelevant documents in that collection. As discussed late r in section 4.4, the value of the discount factor  X  which pro-vided the best ranking performance for these training sizes is  X  = 1. We therefore use that value in our experiments in the three coming sections.

Table 2 summarizes results obtained by RB, TSVM, ssLR and ssRB in terms of mAUP and mT9P . The RankBoost algo-rithm is trained over the labeled part of each training sets and performance of ssRB are shown for different number k of nearest neighbors of each labeled example which are as-signed the same relevance judgment. We use bold face to indicate the highest performance rates. The symbol  X  indi-cates that performance is significantly worse than the best result, according to a Wilcoxon rank sum test used at a p-value threshold of 0 . 01 [14].

Three observations can be made from these results. First, all semi-supervised algorithms perform better on both coll ec-tion, and according to both metrics, than the RB algorithm trained using only the labeled data alone. This shows empir-ically that semi-supervised algorithms are able to partial ly exploit the relevant information contained in the unlabele d examples. The second observation is that the hyperparame-ter k has a definitive (although not highly significant) impact on the performance of the ssRB algorithm. On Ohsumed , the best results for ssRB are obtained when only one unlabeled instance nearest to each labeled example is labeled ( k = 1). On Reuters , the best results are obtained for k = 2. This may be due to the fact that the NN algorithm is less effective on the Ohsumed collection, where the dimension of the doc-ument space is about 7 times higher than on the Reuters dataset. Performance of the ssRB on both datasets is lowest for k = 3, suggesting that this value of k yields too many erroneous label assignment that ssRB is unable to overcome. Finally, both TSVM and ssLR, which have been shown to be effective on classification tasks [13, 2], are less competiti ve than ssRB on both ranking measures.
Table 3 gives the AUC performance of the ranking algo-rithms on each topic of the Reuters dataset. For ssRB, the NN parameter was fixed to k = 2, in agreement with results from the previous section. The number of labeled exam-ples in the training set for each of the topics is the same as above. These results show that ssRB is significantly better than TSVM and ssLR on topics presenting a higher dispro-portion of relevant/irrelevant examples in the collection , and not significantly worse than the best on others. used as unlabeled training set.
  X  40 . 85  X  0 . 6  X  64 . 51  X  0 . 4  X   X  51 . 6  X  0 . 3  X  71 . 01  X  0 . 2  X   X  50 . 23  X  0 . 5  X  70 . 83  X  0 . 1  X 
Referring back to table 2, we can also see that on mAUP and mT9P measures, the relative margin between the perfor-mance of the best ssRB and TSVM is greater on Ohsumed than on Reuters . We recall that the proportion of relevant documents per topic is sizably higher on Reuters than on Ohsumed . For example, considering mT9P scores, the differ-ence in percentage between ssRB (for k = 1) and TSVM on the Ohsumed collection is 4% which represents 58 . 8% of the interval length between the worse and best mT9P perfor-mance [33 . 6 , 40 . 4]. While, the difference on performance be-tween the best ssRB (for k = 2) and TSVM on the Reuters dataset is 5 . 56% which represents 46 . 1% of the interval length [64 . 5 , 76 . 5].

We further investigate the effect on the AUC measure of having less and less relevant documents in a labeled train-ing pool where the number of irrelevant examples is kept fixed. Figure 1 illustrates this effect by showing the evolu-tion on AUC scores of the three semi-supervised algorithms when relevant documents are gradually removed from the labeled part of the acq training set. This topic was, with earn , one of the two topics in Reuters on which ssRB did worse than the two other semi-supervised algorithms. The initial number of relevant/irrelevant documents in the la-beled training set was set to respectively 9 and 81.
These curves show that the decrease rate of the AUC mea-sure of ssRB is lower than the two other semi-supervised classifiers. The loss in AUC for ssRB is less than 9% when Table 3: AUC measure on the 10 largest topics of the Reuters dataset for RB, TSVM, ssLR and ssRB.
 Semi-supervised algorithms use 90 labeled examples per topic. The remaining documents in the training set are all unlabeled.
 money-fx 83 . 8  X  0 . 4  X  90 . 3  X  0 . 5  X  89 . 7  X  0 . 4 crude 83 . 4  X  0 . 5  X  94 . 5  X  0 . 4 93 . 5  X  0 . 4 95.5  X  0.2 grain 84 . 5  X  0 . 4  X  91 . 1  X  0 . 6  X  92 . 4  X  0 . 3 trade 84 . 9  X  0 . 6  X  91 . 2  X  0 . 3  X  90 . 4  X  0 . 4 interest 79 . 9  X  0 . 6  X  87 . 6  X  0 . 7  X  88 . 3  X  0 . 2 money-su 80 . 2  X  0 . 3  X  86 . 2  X  0 . 3  X  87 . 3  X  0 . 1 sugar 78 . 6  X  0 . 1  X  86 . 6  X  0 . 2  X  85 . 3  X  0 . 6  X  the proportion of relevant/irrelevant documents falls fro m 1 / 9 to 1 / 27. This drop is about 16% for TSVM and ssLR.
Figure 2, top, shows precision/recall curves on Ohsumed and Reuters collections using the same number of relevant/ir-relevant documents in the respective training sets than wha t was used in previous experiments. In order to have an em-pirical upper-bound on these results we also plotted the pre -cision/recall curves of a rankboost algorithm trained over all labeled and unlabeled examples, plus their true labels, in t he different training sets. We refer to this model as RB-Fully supervised . These results confirm the previous ones as for different precision levels, the relative margin between ssR B and TSVM (or ssLR) is higher on Ohsumed than on Reuters specially for low recall rates. An explanation of these find-ings is that as ssRB learns a scoring function which output is supposed to rank relevant documents higher than irrel-evant ones in both labeled and unlabeled training sets, its performance is less affected by fewer relevant documents.
We finally report on the behavior of different ranking al-gorithms for growing number of labeled data in the train-ing sets. Figure 2 down, illustrates this behavior on the mAUP measures for both of datasets. The addition of new Figure 1: AUC on Reuters category acq with respect to the number of relevant documents in the labeled training set. The number of irrelevant documents is fixed kept to 81 . labeled data on each training sets respects the initial rele -vant/irrelevant proportion of documents on these sets. All performance curves increase monotonically with respect to the additional labeled data. The convergence rate of mAUP scores on the Reuters dataset is however faster. We expect that this is because training sets per topic on this collecti on contain more relevant information than Ohsumed topics.
Another interesting observation here is that unlabeled ex-amples become relatively less important as more labeled data are available. This observation is confirmed with re-sults on figure 3 which show that for growing number of labeled training size on the Ohsumed collection, the discount factor  X  for which a maximum is reached on mAUP moves away from 1. We recall that for  X  = 1, unlabeled data play the same role in the training of the scoring function than labeled data.
We presented a new approach to learn a boosting based, inductive ranking function in a semi-supervised setting, w hen both labeled and unlabeled data are available. We showed that unlabeled data do help provide a more efficient ranking function and that their effect depends on the initial labeled training size. In the most interesting case, when there are few labeled data, we empirically illustrated that unlabele d data have a large effect on the mAUP measure.

We evaluated and validated our approach using two test collections. We showed through different ranking measures that the new model provides results that are superior to two semi-supervised classification algorithms on both collect ions. Among other things, these results confirm theoretical and empirical studies made previously in the supervised case, showing that the classification ability of classifiers is not necessarily a good indication of their ranking performance . Figure 3: mAUP with respect to the discount factor  X  for different labeled training sizes on Ohsumed .
In future work, we will make the discount factor  X  depend on each unlabeled example in the training set using a con-tinuous function. The key of that study would be the choice of the function with necessary conditions allowing to take into consideration information contained on unlabeled dat a as best as possible. Another promising direction to explore would be the optimization of other ranking criterion than the modified AUC for learning ranking functions.
This work was supported in part by the IST Program of the European Community, under the PASCAL Network of Excellence, IST-2002-506778. This publication only reflec ts the authors X  views. [1] S. Agarwal. Ranking on Graph Data. In Proceedings of [2] M.-R. Amini and P. Gallinari. The Use of Unlabeled [3] M.-R. Amini and P. Gallinari. Semi-Supervised [4] P. L. Bartlett, M. I. Jordan and Jon D. McAuliffe. [5] A. Blum and T. Mitchell. Combining labeled and [6] A.P. Bradley. The use of the Area under the ROC [7] R. Caruana and A. Niculescu-Mizil. Data mining in [8] O. Chapelle, B. Sch  X  olkopf and A. Zien.
 [9] C. Cortes and M. Mohri. AUC optimization vs. error [10] Y. Freund, R. Iyer, R.E. Schapire and Y. Singer. An [11] E. Gaussier and C. Goutte. Learning with Partially [12] W. Hersh, C. Buckley, T. J. Leone and David Hickam. [13] T. Joachims. Transductive Inference for Text [14] E.L. Lehmann. Nonparametric Statistical Methods [15] D. D. Lewis. Reuters-21578, distribution 1.0 [16] K. Nigam, A.K. McCallum, S. Thrun and Tom [17] S. Robertson and D.A. Hull. The TREC-9 Filtering [18] I. Soboroff and S. Robertson. Building a Filtering Test [19] C. van Rijsbergen, Information Retrieval , [20] J.-N. Vittaut, M.-R. Amini and P. Gallinari, Learning [21] C.C. Vogt and G.W. Cottrell, Fusion Via a Linear [22] J. Weston, R. Kuang, C. Leslie and W.S. Noble. [23] D. Zhou, J. Weston, A. Gretton, O. Bousquet and B. [24] D. Zhou , C.J.C. Burges and T. Tao. Transductive
