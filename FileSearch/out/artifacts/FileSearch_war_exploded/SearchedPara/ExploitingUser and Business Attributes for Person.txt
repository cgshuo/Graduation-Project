 Data sparsity and cold-start are two major problems in per-sonalized recommendation. They are especially severe in business recommendation, because business transactions are usually completed offline and customers generally do not provide ratings after a transaction. Due to these two prob-lems, matrix factorization (MF) models, which are shown to be effective in many recommendation tasks, are likely to fail on business recommendation tasks, especially for new user-s and new items. In this paper, we propose an Integrated Bias and Factorization Model (IBFM), which exploits us-er and business attributes. The user attributes include de-mographic information, vote information, point-of-interests; the business attributes include check-in information, loca-tions, business names, categories, etc. To handle the cold-start problem, we employ a sampling strategy to generate the latent factor vectors for new users and new businesses based on similar users/businesses. Our methods are evalu-ated on the data set used in the RecSys 2013 Yelp business rating prediction challenge. Experimental results show that our proposed methods significantly outperform several exist-ing state-of-the-art methods. In particular, the single model IBFM performs the best in this challenge on both public and private leaderboards.
 H.3.3 [ Information Search and Retrieval ]: Information Filtering Collaborative Filtering; Data Sparsity; Cold-start; Context Information; Matrix Factorization
Collaborative Filtering (CF) based recommender systems provide an effective way to solve the information overload problem. In general, CF approaches can be divided into two categories: neighborhood models and matrix factorization (MF) models [1]. Neighborhood models capture the simi-larities between the neighbors of users (or items). However, this approach performs poorly when the data is sparse. MF maps users and items into comparable latent factors and can handle the data sparsity problem better.

There are two major problems which affect the perfor-mances of recommendation models: data sparsity and the cold-start (i.e. new user or new item ) problem. In busi-ness recommendation, these two problems are more severe. As business transactions are usually completed offline, user ratings are extremely sparse, e.g., each user has very few ratings. For example, 50% of the users have only 1 rating in the training data released in RecSys13 Yelp business rec-ommendation challenge. Traditional MF models often fail for two reasons: 1) existing user and business latent factor vectors over fit as each user/business has too few ratings in the training data; 2) predictions for new users or new busi-nesses are very unreliable and introduces much uncertainty into the latent space.

To overcome the weakness of MF, we propose an Integrat-ed Bias and Factorization Modeling approach (IBFM) which can mitigate both problems. IBFM overcomes the data spar-sity and cold-start problem simultaneously in a framework through three major components. First, IBFM incorporates multiple user and business features into an integrated frame-work, which can mitigate the data sparsity, and enable the model to learn hidden representations of users and items more reliably with augmented information. Second, IBFM utilizes a sampling strategy to generate the new uers X  (busi-nesses X ) latent factor vectors. Instead of random sampling, IBFM considers user and business attributes, and uses the trained latent vectors of a new user/business X  X  nearest exist-ing users/businesses to generate their latent vector. Third, IBFM also incorporates user X  X  potential interests in busi-nesses they are not aware of before.
Variations of MF models are becoming very popular for their good performances on standard rating prediction tasks such as Netflix challenge. An SVD model with alternating least square optimization was proposed in [5]. Stochastic gradient descent based approaches for basic MF and Regu-lar SVD were discussed [7]. SVD ++ [4] and Factorization Machine [8] which use Bayesian inference and MCMC sam-pling are now two of the state-of-the-art models in rating prediction tasks. However, these methods do not work well in the cold-start scenarios.
T o solve the cold-start problems, many methods have been tried. [11, 2, 10] asked new users for several interviews to capture their preferences, and deployed latent factor models to provide recommendations for new users. [9] and [3] uti-lized content information to solve the new item recommen-dation problem, while their methods require a long training time and are not suitable for new users.

Compared with the methods mentioned above, our ap-proaches can mitigate the data sparsity and cold-start prob-lem for both new user and new business together. They are also more efficient and scalable to large data.
To solve the data sparsity and cold-start problems simul-taneously, we propose an integrated single model, which we call Integrated Bias and Matrix Factorization Model, to in-corporate user and business X  X  various attributes together. The model contains three major components: user bias (i.e. preferences) model, business bias (i.e. preferences) model and latent factor space. We introduce each part step by step in the following sub sections.
To illustrate our idea, let X  X  start with a baseline rating bias model proposed by [4], which is represented as:  X  r ui + b u + b i . Where is the global average rating value, b is user X  X  rating bias and b i is business X  X  rating bias.
We argue that a user X  X  bias is related to his demographic information, his point-of-interests, and other users X  voting information (e.g., funny, useful, cool) for his ratings. First, users who have the same gender might have similar rating tendency. Gender bias can be seen as the shared component for all the ratings by users with the same gender. Second, we assume if users have similar vote patterns, they share similar rating bias. Thus we can employ the user X  X  vote information to infer his bias. Third, a user X  X  rating bias is also related to his point-of-interests. Therefore we try to model user X  X  bias/preference as follows: In this equation, gender ( u ) = { 0 ; 1 } , vote ( u ) = i.e., based on all the votes u gets, we choose the largest num-ber as his vote classifier (i.e. funny, useful, cool). b u;lct ( i ) We represent the business X  X  location using its zip code, thus lct ( i ) is the location (i.e. zip code) of business i . is user u  X  X  bias for location of business i (i.e. zip code) and ! u;lct ( i ) is the location weights, which is calculated as fol-lows: Where R u is u  X  X  total ratings, R u;lct ( i ) is u  X  X  rating number of the businesses whose zip code is lct ( i ). On the business side, we can also collect various features. These features can be categorized into three categories: check-in information, location information and content informa-tion.

Checkin: For the check-in information, we think that a business X  X  rating bias is related to the businesses whose check-in patterns are close to it. Thus the business rating bias can be represented as follows: where C ( i ) are the businesses with check-in patterns similar to i . We measure csim ( i  X  j ) using KL-divergence: where p i and p j denote business i and j  X  X  check-in distribu-tions in different hours of weekdays, P is the vector length, and X denotes all businesses with check-in information.
Location: Based on a business X  X  location information, we propose a location bias model that incorporates the busi-ness X  X  zip code, street, city, longitude and latitude. The model is: l In the location bias, zip code ( i ), str eet ( i ) and city ( i ) are business i  X  X  zip code, street and city information respective-ly. G ( i ) are the nearest businesses in terms of geolocation. i  X  X  bias. We use longitude and latitude to measure the sim-ilarities between two businesses: In this equation, distance ( i; j ) is calculated by the longitude and latitude of i and j , L denotes all the businesses with location information.

Name and Category: Two types of business content information are considered: name and category taxonomy. A specific company may have multiple stores in different places; e.g., in Yelp review data, there are over 30 differen-t  X  X VS pharmacy X  stores in Arizona. Assuming business-es with the same name are similar, we introduce a name based bias to capture the similarities between stores of sim-ilar names. There are over 2,000 different categories in the Yelp business data set, among which 25 are first level cate-gories. We use the first level category which each business is classified into. We also consider that the business X  X  bias is related to other businesses X  bias in the same category. Thus the content bias model is: t where CG ( i ) contains all the businesses in the same cat-egory as business i . N ame ( i ) are the set of businesses with the same name as business i . b u;rtc ( i ) is user u  X  X  bias for business i  X  X  root category (i.e. rtc ( i )).
By integrating all the aspects of business features men-tioned in Equation (2), (3) and (4), we get the business X  X  new rating bias b  X  i = b i + c i + l i + t i .

Integrating user and business attributes together, we get the full bias model as follows:
In the basic MF model, a user is modeled by a hidden vector p u and a business is modeled by a hidden vector q The prediction is calculated as the inner product of the two user and business latent factor vectors:  X  r ui = p T u q
After integrating the full bias model (Equation (5) ), we get the final Integrated Bias and Factorization Model (IBFM) as follows: where N ( u ) represents the set of all businesses user u has rated. C ( u ) and G ( u ) are the business sets user u is interest-ed in and has not rated. y j , c j and g j are parameters to be estimated and they are used to capture the user X  X  implicit feedback. C ( u ) is calculated based on the category infor-mation associated with the businesses u has rated. G ( u ) is calculated based on the zip code associated with the busi-nesses u has rated. The businesses in C ( u ) and G ( u ) are sorted by their popularity, and only the top 20 businesses are considered.

New users/businesses: Each unrated user-business pair falls into one of the following scenarios: existing user and ex-isting business, existing user and new business, new user and existing business, new user and new business. New users and new businesses pairs do not have related training data, thus their latent vectors are unknown, which introduces huge un-certainty in the prediction stage. To reduce uncertainty, we use user and business features to compensate the disadvan-tages of traditional MF models as follows: 1) New user u : we use u X  X  gender to find the top-k most popular existing users with the same gender. Then we fit a Gaussian distribution to the hidden representation of those users, and sample from this distribution to generate u  X  X  la-tent vector p u . 2) New business i : Based on i  X  X  location information, con-tent information and check-in information, we find the top-k closest existing businesses using the similarity measure de-scribed in 3.1. Then we fit a Gaussian distribution to the hidden representation of those businesses, and sample from this distribution to generate p u .
To test the performance of the proposed methods, we car-ried out extensive experiments on the Yelp business review data. RMSE is used to evaluate the models X  performance.
The Yelp business data set is a publicly available bench-mark data set shared on Kaggle 1 . The data set includes ht tp://www.kaggle.com/c/yelp-recsys-2013 Fi gure 1: The ratings' distribution in the test data rating data, business check-in data, user profile data and business profile data. There are 60,692 users and 15,040 businesses in the ratings, and the training data contains 230,000 ratings and test data has 36,404 ratings. The ob-jective is to predict user-business ratings on the test data.
This data set has several unique characteristics. First of all, rating density is only 0.03%, which means the data s-parsity issue is much more serious than that of the Yahoo! Music rating data set (0.4%) released in KDD-Cup 2011 and Netflix movie rating data set(1.1%). Second, about 49.65% users have only 1 rating in the training data, 16.15% user-s have 2 ratings, and only 8.23% users have more than 10 ratings. In addition, the cold-start problem is more serious as well. The rating distribution of the test data is shown in Figure 1. We can see only 35.36% ratings are warm ratings. In the test data set, about 52.6% users are new users and about 25.75% businesses are new businesses without train-ing data. Therefore, all the problems mentioned above cause huge difficulty to make accurate rating predictions.
Meanwhile, there is rich information in user profiles, busi-ness profiles and business check-in data. For users, their names and other users X  vote distributions for their ratings are given. Each user gender is inferred based on a male/female name dictionary. For each business, its name, category, ad-dress, city, longitude and latitude are available. The check-in data contains each business X  X  check-in times in different time periods.

We use several state-of-the-art rating prediction models as our baselines, including SVD with alternating least-square optimization (ALS-SVD) [5], basic MF [7], RSVD [7], rat-ing bias [4], SVD ++ [4] and Factorization Machine (FM) model [8]. To learn the factorization models, we random-ly generate a validation data based on the cold-start rating distributions of the test data from the original Yelp compe-tition training data, so that the validation data X  X  cold-start rating distributions are close to that of the test data. There are 32,020 ratings in the validation data, where 37.85% rat-ings are warm ratings, 37.20% ratings are new users and old businesses, 20.55% ratings are new users and new business-es, and 4.5% ratings are existing users and new businesses. In addition, there are 53.50% new users and 16.03% new businesses. The remaining data are used for training.
Without lose of generality, we set k to 100 in our ex-periments. The latent factor dimensions of all factoriza-tion models are set as 50. To learn the model parameters ( b ; y i ; c i ; g i ; p u ; q i ), we minimize the mean square error of the predicted ratings on the training data using a stochastic gradient descent method and the validation data is used for early stopping to avoid over fitting. To tune the relevant hyper-parameters, i.e., learning rate and regularization, we use Nelder-Mead simplex search algorithm [6] with the ini-tial learning rate as 0.001 and regularization value as 0.01. T able 1: Performance (RMSE) of relevant models M ethod Validation Public LB Private LB AL S-SVD 1.27046 1.31063 1.31572 BasicMF 1.26489 1.30671 1.31370 RSVD 1.21214 1.25138 1.25543 SVD++ 1.19324 1.24473 1.24971 FM 1.19298 1.24311 1.24870 RatingBias 1.19404 1.24291 1.24912 UserFeature 1.19107 1.23837 1.24012 CheckinFeature 1.19221 1.2401 1.24419 ContentFeature 1.18076 1.22701 1.23010 LocationFeature 1.17518 1.22424 1.22282 UnifiedBiasModel 1.14003 1.19171 1.19780
IBFM * 1.12691 1.17858 1.18389
The ratings of the test data are unknown, and they are randomly divided into two parts in the yelp competition. 20% ratings are used in the public leaderboard evaluation and the left 80% ratings are used in the private leaderboard evaluation. The results can be known through submitting the results in Kaggle.

The experimental results of all models are shown in Table 1. We have the following findings: 1) Both of the BasicMF and ALS-SVD perform poorly on this data set. RSVD, which incorporates user and busi-ness rating bias, greatly improve prediction accuracy com-pared with MF and ALS-SVD. RatingBias model performs much better than RSVD, and has a similar performance as SVD++ and FM. The reason is that when users have too few ratings on the training data, the user and business latent vectors have big variance. In addition, the cold-start ratings in the test data make user and business latent vectors very unreliable for predictions. 2) By incorporating user X  X  attributes (gender, vote infor-mation and point-of-interests), business X  X  features (check-in, name and category and location information), all the model-s (UserFeature, CheckinFeature, ContentFeature and Loca-tionFeature) significantly improve the recommendation per-formance. Among all the four features, the location feature is most effective. This makes sense, because locations play an important role in business recommendations. By com-bining user and business attributes together, the Unified-BiasModel further improves performance significantly. This demonstrates that the user and businesses features are com-plementary, and using appropriate feature learning method-s can effectively mitigate the data sparsity and cold-start problem. 3) Furthermore, after adding the sampling method for generating new user and new business latent vectors based on user and business attributes, IBFM shows the best per-formance on all three data sets. Compared with the state-of-the-art models (SVD++ and FM) on prediction problems, IBMF increases the prediction accuracy by 5.15% . It shows that our sampling method can effectively generate reason-able initial hidden representations for new users and new businesses.
In this paper, we propose an integrated bias and factor-ization model (IBFM) which has two advantages. First, it utilizes user X  X  demographic information, votes and point-of-interests, business check-in information, content informa-tion, and location information seamlessly. Second, it uses sampling method to generate latent factor vectors for new users and new businesses, which enables IBFM to make rea-sonable predictions in cold-start scenarios. Our experimen-tal results show that IBFM can significantly outperform the traditional state-of-the-art models on benchmark business rating prediction tasks. In addition, our single model IBFM performs the best on both the public and private leaderboard of RecSys 2013 Yelp business challenge, which demonstrate it can effectively take advantages of user and businesses at-tributes and the superiority of factorization model. This work was funded by National Science Foundation ICES-1101741 and IIS-0953908. Any opinions, findings, con-clusions or recommendations expressed in this paper are the authors, and do not necessarily reflect those of the sponsors. [1] G. Adomavicius and A. Tuzhilin. Toward the next [2] N. Golbandi, Y. Koren, and R. Lempel. Adaptive [3] A. Gunawardana and C. Meek. Tied boltzmann [4] Y. Koren. Factorization meets the neighborhood: a [5] Y. Koren, R. Bell, and C. Volinsky. Matrix [6] J. A. Nelder and R. Mead. A simplex method for [7] A. Paterek. Improving regularized singular value [8] S. Rendle. Factorization machines with libFM. ACM [9] L. Zhang and Y. Zhang. Discriminative factored prior [10] L. Zhang and Y. Zhang. Interactive retrieval based on [11] K. Zhou, S.-H. Yang, and H. Zha. Functional matrix
