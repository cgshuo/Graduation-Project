 We propose a new probabilistic approach for multi-label classification that aims to represent the class posterior dis-tribution P ( Y | X ). Our approach uses a mixture of tree-structured Bayesian networks, which can leverage the com-putational advantages of conditional tree-structured mod-els and the abilities of mixtures to compensate for tree-structured restrictions. We develop algorithms for learning the model from data and for performing multi-label pre-dictions using the learned model. Experiments on multiple datasets demonstrate that our approach outperforms several state-of-the-art multi-label classification methods. I.2.6 [ LEARNING ]: General Multi-label classification, Bayesian network, Mixture of trees
In many real-world applications, a data instance is natu-rally associated with multiple class labels. For example, a document can cover multiple topics [21, 42], an image can be annotated with multiple tags [6, 29] and a single gene may be associated with several functional classes [9, 42]. Multi-label classification (MLC) formulates such situations by assuming each data instance is associated with a subset of d labels. Al-ternatively, this problem can be defined by associating each instance with d binary class variables Y 1 , ...Y d , where Y notes whether or not the i -th label is present in the instance. The goal is to learn a function that assigns to each instance, represented by a feature vector x = ( x 1 , ..., x m ), the most probable assignment of the class variables y = ( y 1 , ..., y However, learning of such a function can be very challeng-ing because the number of possible label configurations is exponential in d .

A simple solution to the above problem is to assume that all class variables are conditionally independent of each other and learn d functions to predict each class separately [9, 6]. However, this may not suffice for many real-world prob-lems where dependences among output variables exist. To overcome this limitation, multiple machine learning meth-ods that model class relations have been proposed in recent years. These include two-layer classification models [14, 8], classifier chains [31, 41, 10], output coding methods [18, 34, 44, 45] and multi-dimensional Bayesian network classifiers [38, 5, 1].

In this work, we develop and study a new probabilis-tic approach for modeling and learning an MLC. Our ap-proach aims to represent the class posterior distribution P ( Y 1 , ..., Y d | X ) such that it captures multivariate depen-dences among features and labels. Our proposed model is defined by a mixture of Conditional Tree-structured Bayesian Networks (CTBNs) [2]. A CTBN defines P ( Y 1 , ..., Y d | X ) us-ing a directed tree structure to model the relations among the class variables conditioned on the feature variables. The main advantage of CTBN is that it allows efficient learning and inference. A mixture of CTBNs leverages the computa-tional advantages of CTBNs and the ability of a mixture to compensate for the tree-structure restriction.

Our new mixture model extends the work by [26] that models and learns the joint distribution over many variables using tree-structured distributions and their mixtures, to learn conditional distributions where the multivariate rela-tions among Y components are conditioned on inputs X . To support learning and inference in the new model, we de-velop and test new algorithms for: (1) learning the param-eters of conditional trees mixtures, (2) selecting individual tree structures and (3) inferring the maximum a posteriori (MAP) output label configurations.

An important advantage of our method compared to ex-isting MLC methods is that it gives a well-defined model of posterior class probabilities. That is, our model lets us calculate P ( Y = y | X = x ) for any ( x , y ) input-output pair. This is extremely useful not only for prediction, but also for decision making [30, 3], conditional outlier analysis [15, 16, 17], or for performing any inference over subsets of output class variables. In contrast to our approach, the majority of existing MLC methods aim to only identify the best output configuration for the given x .
In Multi-Label Classification (MLC), each instance is as-sociated with d binary class variables Y 1 , ...Y d . We are given labeled training data D = { x ( n ) , y ( n ) } N n = 1 , where x ( x 1 , ..., x ing the n -th instance (the input) and y ( n ) = ( y ( n ) is its corresponding d -dimensional class vector (the output). We want to learn a function h (from D ) that assigns to each instance, represented by its feature vector, a class vector:
One way to approach this task is to model and learn the conditional joint distribution P ( Y | X ), where Y = ( Y is a random variable for the class vector and X is a random variable for the feature vector. Assuming the 0-1 loss func-tion, the optimal classifier h  X  assigns to each instance x the maximum a posteriori (MAP) assignment of class variables:
A key challenge for modeling and learning P ( Y | X ) from data, as well as for defining the corresponding MAP classi-fier, is that the number of all possible class assignments one has to consider is 2 d . The goal of this paper is to develop a new, efficient model and methods for its learning and infer-ence that overcome this difficulty.

In this section, we briefly review the research work related to our approach and pinpoint the main differences.
MLC method based on learning independent classifiers was studied by [9, 6]. Zhang and Zhou [43] presented a multi-label k-nearest neighbor method, which learns a clas-sifier for each class by combining k-nearest neighbor with Bayesian inference. To model possible class dependences, [14, 8] proposed adding a second layer of classifiers that com-bine input features with the outputs of independent classi-fiers. The limitation of these early approaches is that class dependences are either not modeled at all, or modeled in a very limited way.

The classifier chains (CC) method [31] models the class posterior distribution P ( Y | X ) by decomposing the relations among class variables using the chain rule: Each component in the chain is a classifier that is learned separately by incorporating the predictions of preceding clas-sifiers as additional features. Zhang and Zhang [41] realized that the performance of CC is influenced by the order of classes in the chain and presented a method to learn such ordering from data. Dembczynski et al. [10] discussed the suboptimality of CC and presented probabilistic classifier chains to estimate the entire posterior distribution of classes. However, this method has to evaluate exponentially many label configurations, which greatly limits its applicability.
Another approach for modeling P ( Y | X ) relies on condi-tional random fields (CRFs) [24]. Ghamrawi and McCallum [13] presented a method called collective multi-label with features classifier (CMLF) that captures label co-occurrences conditioned on features. However, CMLF assumes a fully connected CRF structure which results in a high compu-tational cost. Later, Shahaf et al. [32] and Bradley et al. [7] proposed to learn tractable (low-treewidth) structures of class variables for CRFs using conditional mutual informa-tion. More recently, Pakdaman et al. [28] used pairwise CRFs to model the class dependences and presented L 2 -optimization-based structure and parameter learning algo-rithms. Although the later methods share similarities with our approach by modeling the conditional dependences in Y space using restricted structures, their optimization of the likelihood of data is computationally more costly. To alle-viate this, CRF-based methods often resort to optimization of a surrogate objective function (e.g., the pseudo-likelihood of data [28]) or include specific assumptions (e.g., features are assumed to be discrete [13]; relevant features for each class are assumed to be known [32, 7]), which complicate the application of the methods.

Multi-dimensional Bayesian network classifiers (MBC) [38, 5, 1] build a generative model of P ( X , Y ) using special Bayesian network structures that assume all class variables are top nodes and all feature variables are their descendants. Although our approach can be compared to MBC, there are significant differences and advantages: (1) MBC only han-dles discrete features and, thus, all features should be a pri-ori discretized; while we handle both continuous and discrete features. (2) MBC defines a joint distribution over both fea-ture and class variables and the search space of the model increases with the input dimensionality m ; while our search space does not depend on m . (3) Feature selection in MBC is done explicitly by learning the individual relationships be-tween features and class variables; while we perform feature selection by regularizing the base classifiers. (4) MBC re-quires expensive marginalization to obtain class conditional distribution P ( Y | X ); while we directly estimate P ( Y | X ).
An alternative approach for MLC is based on output cod-ing. The idea is to compress the output into a codeword, learn how to predict the codeword and then recover the cor-rect output from the noisy predictions. A variety of ap-proaches have been devised by using different compression techniques, such as compressed sensing [18], principal com-ponent analysis [34] and canonical correlation analysis [44]. The state-of-the-art in output coding utilizes a maximum margin formulation [45] that promotes both discriminative and predictable codes. The limitation of output coding methods is that they can only predict the single  X  X est X  out-put for a given input, and they cannot compute probabilities for different input-output pairs.
 Several researchers proposed using ensemble methods for MLC. Read et al. [31] presented a simple method that av-erages the predictions of multiple random classifier chains trained on a random subset of the data. Antonucci et al. [1] proposed an ensemble of multi-dimensional Bayesian net-works combined via simple averaging. These networks repre-sent different Y relations (the structures are set a priori and not learned) and all of the networks adopt the na  X   X ve Bayes assumption (the features are independent given the classes). Unlike these methods, our approach learns the structures in the mixture, its parameters and mixing coefficients from data in a principled way.
The MLC solution we propose in this work combines mul-tiple base MLC classifiers using the mixtures-of-trees (MT) [26, 39] framework, which uses a mixture of multiple trees to define a generative model of P ( Y ) for discrete multi-dimensional domains. The base classifiers we use are based on the conditional tree-structured Bayesian networks (CTBN) [2]. To begin with, we briefly review the basics of MT and CTBN.

MT consists of a set of trees that are combined using mix-ture coefficients  X  k to represent the joint distribution P ( y ). The model is defined by the following decomposition: where P ( y | T k ) are called mixture components that repre-sent the distribution of outputs defined by the k -th tree T Note that a mixture can be understood as a soft-multiplexer, where we have a hidden selector variable which takes a value k  X  { 1 , ..., K } with probability  X  k . That is, by having a con-vex combination of mutually complementary tree-structured models, MT aims at achieving a more expressive and accu-rate model.

While MT is not as computationally efficient as individual trees, it has been considered as a useful approximation at a fraction of the computational cost learning general graphical models [22]. MT has been successfully adopted in a range of applications, including modeling of handwriting patterns, medical diagnostic network, automated application screen-ing, gene classification and identification [26], face detection [20], video tracking [19], road traffic modeling [39] and cli-mate modeling [22].
 In this work, we apply the MT framework in context of MLC. In particular, we combine MT with CTBN to model individual trees. CTBN is a recently proposed probabilistic MLC method that has been shown to be competitive and efficient on a range of domains. CTBN defines P ( Y | X ) us-ing a collection of classifiers modeling relations in between features and individual labels that are tied together using a special Bayesian network structure that approximates the dependence relations among the class variables. In modeling of the dependences, it allows each class variable to have at most one other class variable as a parent (without creating a cycle) besides the feature vector X .

A CTBN T defines the joint distribution of class vector ( y 1 , ..., y d ) conditioned on feature vector x as: where  X  ( i, T ) denotes the parent class of class Y i in T (by convention,  X  ( i, T ) = {} if Y i does not have a parent class). F or example, the conditional joint distribution of class as-signment ( y 1 , y 2 , y 3 , y 4 ) given x according to the network T in Figure 1 is defined as:
Although our proposed method is motivated by MT, there are significant extensions and differences. We summarize the key distinctions below. 1. Model : Our model represents P ( Y | X ), the class poste-2. Structure learning : Our structure learning algorithm 3. Parameter learning : Not surprisingly, both our param-
In this section, we describe Mixture of Conditional Tree-structured Bayesian Networks (MC), which uses the MT framework in combination with the CTBN classifiers to im-prove the classification accuracy of MLC tasks, and develop algorithms for its learning and predictions. In section 5.1, we describe the mixture defined by the MC model. In sec-tion 5.2 through 5.4, we present the learning and prediction algorithms for the MC model.
By following the definition of MT in Equation (3), MC defines the multivariate posterior distribution of class vector y = ( y 1 , ..., y d ) as: where  X  k  X  0 ,  X  k ; and P K k =1  X  k = 1. Here each mixture component P ( y | x , T k ) is the distribution defined by CTBN T k (as in Equation (4)) and mixture coefficients are denoted by  X  k . Figure 2 depicts an example MC model, which con-sists of K CTBNs and the mixture coefficients  X  k .
In this section, we describe how to learn the parameters of MC by assuming the structures of individual CTBNs are known and fixed. The parameters of the MC model are the mixture coefficients {  X  1 , ...,  X  K } as well as the parameters of each CTBN in the mixture {  X  1 , ...,  X  K } .

Given training data D = { x ( n ) , y ( n ) } : n  X  1 , ..., N , the objective is to optimize the log-likelihood of D , which we refer to as the observed log-likelihood .
 However, this is very difficult to directly optimize because it contains the log of the sum. Hence, we cast this optimization in the expectation-maximization (EM) framework. Let us associate each instance ( x ( n ) , y ( n ) ) with a hidden variable z ( n )  X  { 1 , ..., K } indicating which CTBN it belongs. The complete log-likelihood (assuming z ( n ) are observed) is: where 1 [ z ( n ) = k ] is the indicator function, which is one if the n -th instance belongs to the k -th CTBN and zero otherwise; and  X  k is the mixture coefficient of CTBN T k which can be interpreted as its prior probability in the data.
The EM algorithm iteratively optimizes the expected com-plete log-likelihood , which is always a lower bound to the ob-served log-likelihood [27]. In the E-step , the expectation is computed with the current set of parameters; in the M-step , the parameters of the mixture (  X  k ,  X  k : k = { 1 , ..., K } ) are relearned to maximize the expected complete log-likelihood. In the following, we describe our parameter learning algo-rithm by deriving the E-step and the M-step for MC.
In the E-step, we compute the expectation of the hidden variables. Let  X  k ( n ) denote P ( z ( n ) = k | y ( n ) terior of the hidden variable z ( n ) given the observations and the current parameters. Using Bayes rule, we write: In the M-step, we learn the model parameters {  X  1 , ...,  X   X  , ...,  X  K } that maximize the expected complete log-likelihood, which is a lower bound of the observed log-likelihood. Let us first define the following two quantities:  X  k c an be interpreted as the number of observations that be-longs to the k -th CTBN (hence, P K k =1  X  k = N ), and w is the renormalized posterior  X  k ( n ), which can be interpreted as the weight of the n -th instance on the k -th CTBN.
Note that when taking the expectation of the complete log-likelihood (Equation (6)), only the indicator 1 [ z ( n ) is affected by the expectation. By using the notations intro-duced above, we rewrite the expected complete log-likelihood:
X = We wish to maximize (9) with respect to {  X  1 , ...,  X  K ,  X  subject to the constraint P K k =1  X  k = 1. Notice that (9) consists of two terms and each term has a disjoint subset of parameters  X  which allows us to maximize (9) term by term. By maximizing the first term with respect to  X  j (the mixture coefficient of T j ), we obtain: T o maximize the second term, we train  X  j (the parameters of T j ) to maximize: It turns out (10) is the instance-weighted log-likelihood, and we use instance-weighted logistic regression to optimize it. Algorithm 1 outlines our parameter learning algorithm. E-step: We compute  X  k ( n ) for each instance on every CTBN. To compute  X  k ( n ), we should estimate P ( y ( n ) which requires applying the logistic regression classifiers for each node of T k , which requires O ( md ) multiplications. Hence, the complexity of the E-step is O ( KNmd ).

M-step: The major computational cost of the M-step is to learn the instance-weighted logistic regression models for the nodes of every CTBN. Hence, the complexity is O ( Kd ) times the complexity of learning logistic regression.
In this section, we describe how to automatically learn multiple CTBN structures from data. We apply a sequential Algorithm 1 l earn-MC-parameters Input : Training data D ; base CTBNs T 1 , ..., T K Output : Model parameters {  X  1 , ...,  X  K ,  X  1 , ...,  X  1: repeat 2: E-step: 3: for k = 1 to K , n = 1 to N do 4: Compute  X  k ( n ) using Equation (8) 5: end for 6: M-step: 7: for k = 1 to K do 9: w k ( n ) =  X  k ( n ) /  X  k 10:  X  k =  X  k /N 12: end for 13: until convergence boosting-like heuristic, where in each iteration we learn th e structure that focuses on the instances that are not well predicted by the previous structures (i.e., the MC model learned so far). In the following, we first describe how to learn a single CTBN structure from instance-weighted data. After that, we describe how to re-weight the instances and present our algorithm for learning the overall MC model.
The goal here is to discover the CTBN structure that max-imizes the weighted conditional log-likelihood (WCLL) on { D,  X  } , where D = { x ( n ) , y ( n ) } N n =1 is the data and  X  = {  X  ( n ) } N n =1 is the weight for each instance. We do this by partitioning D into two parts: training data D tr and hold-out data D h . Given a CTBN structure T , we train its pa-rameters using D tr and the corresponding instance weights. On the other hand, we use WCLL of D h to score T .
In the following, we describe our algorithm for obtaining the CTBN structure that optimizes Equation (11) without having to evaluate all of the exponentially many possible tree structures.

Let us first define a weighted directed graph G = ( V, E ), which has one vertex V i for each class label Y i and a directed edge E j  X  i from each vertex V j to each vertex V i (i.e., G is complete). In addition, each vertex V i has a self-loop E The weight of edge E j  X  i , denoted as W j  X  i , is the WCLL of class Y i conditioned on X and Y j : The weight of self-loop E i  X  i , denoted as W  X   X  i , is the WCLL of class Y i conditioned only on X . Using the definition of edge weights, Equation (11) can be simplified as the sum of the edge weights:
Now we have transformed the problem of finding the op-timal tree structure into the problem of finding the tree in G that has the maximum sum of edge weights. The solution can be obtained by solving the maximum branching (ar-borescence) problem [11], which finds the maximum weight tree in a weighted directed graph.
In order to obtain multiple CTBN structures for the MC model, we apply the algorithm described above multiple times with different sets of instance weights. We assign the weights such that we give higher weights for poorly predicted instances and lower weights for well-predicted instances.
We start with assigning all instances uniform weights (i.e., all instances are equally important a priori). Using this initial set of weights, we find the initial CTBN structure T 1 (and its parameters  X  1 ) and set the current model M to be T 1 . We then estimate the prediction error margin  X  ( n ) = 1  X  P ( y ( n ) | x ( n ) , M ) for each instance and renormalize such that P N n =1  X  ( n ) = 1. We use {  X  ( n ) find the next CTBN structure T 2 . After that, we set the current model to be the MC model learned by mixing T 1 and T 2 according to Algorithm 1.

We repeat the process by incrementally adding trees to the mixture. To stop the process, we use internal validation approach. Specifically, the data used for learning are split to internal train and test sets. The structure of the trees and parameters are always learned on the internal train set. The quality of the current mixture is evaluated on the internal test set. The mixture growth stops when the log-likelihood on the internal test set for the new mixture is worse than for the previous mixture. The trees included in the previous mixture are then fixed, and the parameters of the mixture are relearned on the full training data.
In order to learn a single CTBN structure, we compute edge weights for the complete graph G , which requires esti-mating P ( Y i | X , Y j ) for all d 2 pairs of classes. Finding the maximum branching in G can be obtained in O ( d 2 ) using [35]. To learn K CTBN structures for the mixture, we re-peat these steps K times. Therefore, the overall complexity is O ( d 2 ) times the complexity of learning logistic regression.
In order to make a prediction for a new instance x , we want to find the MAP assignment of the class variables (see Equation (1)). In general, this requires to evaluate all pos-sible assignments of values to d class variables, which is ex-ponential in d .
 One important advantage of the CTBN model is that the MAP inference can be done more efficiently by avoiding blind enumeration of all possible assignments. More specifi-cally, the MAP inference on a CTBN is linear in the number of classes ( O ( d )) when implemented using a variant of the max-sum algorithm [23] on a tree structure.

However, our MC model consists of multiple CTBNs and the MAP solution may, at the end, require enumeration of exponentially many class assignments. To address this prob-lem, we rely on approximate MAP inference. Two com-monly applied MAP approximation approaches are convex programming relaxation via dual decomposition [33], and s imulated annealing using a Markov chain [40]. In this work, we use the latter approach. Briefly, we search the space of all assignments by defining a Markov chain that is induced by local changes to individual class labels. The annealed ver-sion of the exploration procedure [40] is then used to speed up the search. We initialize our MAP algorithm using the following heuristic: first, we identify the MAP assignments for each CTBN in the mixture individually, and after that, we pick the best assignment from among these candidates. We have found this (efficient) heuristic to work very well and it often results in the true MAP assignment.
We perform experiments on ten publicly available multi-label datasets. These datasets are obtained from different domains such as music recognition (emotions [36]), semantic image labeling (scene [6] and image [10]), biology (yeast [12]) and text classification (enron [4] and RCV1 [25] datasets). Table 1 summarizes the characteristics of the datasets. We show the number of instances ( N ), number of feature vari-ables ( m ) and number of class variables ( d ). In addition, we show two statistics: label cardinality (LC), which is the average number of labels per instance, and distinct label set (DLS), which is the number of all distinct configurations of classes that appear in the data. Note that, for RCV1 datasets, we have used the ten most common labels.
We compare the performance of our proposed mixture-of-CTBNs (MC) model with simple binary relevance (BR) independent classification [9, 6] as well as several state-of-the-art MLC methods. These methods include classifica-tion with heterogeneous features (CHF) [14], multi-label k-nearest neighbor (MLKNN) [43], instance-based learning by logistic regression (IBLR) [8], classifier chains (CC) [31], en-semble of classifier chains (ECC) [31], probabilistic classifier chains (PCC) [10], ensemble of probabilistic classifier chains (EPCC) [10], multi-label conditional random fields (ML-CRF) [28], and maximum margin output coding (MMOC) [45]. We also compare MC with a single CTBN (SC) [2] model without creating a mixture.

For all methods, we use the same parameter settings as suggested in their papers: For MLKNN and IBLR, which use the k-nearest neighbor (KNN) method, we use Euclidean distance to measure similarity of instances and we set the number of nearest neighbors to 10 [43, 8]; for CC, we set the we use 10 CCs in the ensemble [31, 10]; finally for MMOC, we set the decoding parameter to 1 [45]. Also note that all of t hese methods except MLKNN and MMOC are considered as meta-learners because they can work with several base classifiers. To eliminate additional effects that may bias the results, we use L 2 -penalized logistic regression for all of these methods and choose their regularization parameters by cross validation. For our MC model, we decide the number of mixture components using our stopping criterion (Section 5.3.2) and we use 150 iterations of simulated annealing for prediction.
Evaluating the performance of MLC methods is more dif-ficult than evaluating simple classification methods. The most suitable performance measure is the exact match ac-curacy (EMA), which computes the percentage of instances whose predicted label vectors are exactly the same as their true label vectors.

However, this measure could be too harsh, especially when the output dimensionality is high. Another very useful mea-sure is the conditional log-likelihood loss (CLL-loss), which computes the negative conditional log-likelihood of the test instances: CLL-loss evaluates how much probability mass is given to the true label vectors (the higher the probability, the smaller the loss).

Other evaluation measures used commonly in MLC litera-ture are based on F1 scores. Micro F1 aggregates the num-ber of true positives, false positives and false negatives for all classes and then calculates the overall F1 score. On the other hand, macro F1 computes the F1 score for each class separately and then averages these scores. Note that both measures are not the best for MLC because they do not ac-count for the correlations between classes (see [10] and [41]). However, we report them in our performance comparisons as they have been used in other MLC literature [37].
We have performed ten-fold cross validation for all of our experiments. To evaluate the statistical significance of per-formance difference, we apply paired t-tests at 0.05 signifi-cance level. We use markers  X  /  X  to indicate whether MC is significantly better/worse than the compared method.
Tables 2, 3, 4 and 5 show the performance of the meth-ods in terms of EMA, CLL-loss, micro F1 and macro F1, respectively. We only show the results of MMOC on four datasets (emotions, yeast, scene and image) because it did not finish on the remaining data (MMOC did not finish one round of the learning within a 24 hours time limit). For the same reason, we do not report the results of PCC, EPCC and MLCRF on the enron dataset. Also note that we do not report CLL-loss for MMOC, ECC and EPCC because they do not compute a probabilistic score for a given class assignment.
In terms of EMA (Table 2), MC clearly outperforms the o ther methods on most datasets. MC is significantly bet-ter than BR, CHF, MLKNN and CC on all ten datasets, significantly better than IBLR, ECC and MLCRF on nine datasets, significantly better than EPCC and SC on five datasets and significantly better than PCC on four datasets (see the last row of Table 2). Although not statistically sig-nificant, MC performs better than MMOC on all datasets MMOC is able to finish. MLKNN and IBLR perform poorly on the high-dimensional ( m &gt; 1 , 000) datasets because Eu-clidean distances between data instances become indiscernible in high dimensions.

Interestingly, MC shows significant improvements over SC (a single CTBN) on five datasets, while SC produces com-petitive results as well. We attribute the improved perfor-mance of MC to the ability of mixtures to compensate for the restricted dependences modeled by CTBNs, and that of individual CTBNs to better fit the data with different weight sets. On the contrary, ECC and EPCC do not show consis-tent improvements over their base methods (CC and PCC, respectively) and sometimes even deteriorate the accuracy. This is due to the ad-hoc nature of their ensemble learn-ing and prediction (see Section 3) that limits the potential improvement and disturbs the prediction of the ensemble classifiers.

Table 3 compares MC to other probabilistic MLC meth-ods using CLL-loss. The results show that MC outperforms all other methods. This is expected because MC is tailored to optimize the conditional log-likelihood. Among the com-pared probabilistic methods, only PCC produces compara-ble results with MC because PCC explicitly evaluates all possible class assignments to compute the entire class con-ditional distribution. On the other hand, CC greedily seeks the mode of the class conditional distribution (Equation (2)) and results in large losses. In addition, CHF and MLKNN perform very poorly because they apply ad-hoc classification heuristics without performing proper probabilistic inference. Again, MC shows consistent improvements over SC because mixing multiple CTBNs allows us to account for different patterns in the data and, hence, improves the generalization of the model.

Lastly, Tables 4 and 5 show that MC is also very com-petitive in terms of micro and macro F1 scores, although optimizing them was not our immediate objective. One noteworthy observation is that ECC and EPCC do partic-ularly well in terms of F1 scores. We consider averaging out the predictions on each class variable enhances BR-like characteristics in their ensemble decision. In the fu-ture, we will crossbreed these two different ensemble ap-proaches (e.g., MCC/MPCC by applying our mixture frame-work and algorithms to CC/PCC; ECTBN using randomly structured CTBNs and simple averaging) and compare the performances.
In the second part of our experiments, we investigate the effect of different number of mixture components in the MC model. Using three of the benchmark datasets (emotions, scene and image), we study how the performance of MC changes while we increase the number of trees in a model from 1 to 20. In particular, we use ten-fold cross validation and trace the average CLL-loss and EMA across the folds.
Figure 3 summarizes the results. Figures 3(a), 3(b) and 3(c) show how CLL-loss changes on emotions, scene and im-age, respectively. On all three datasets, adding first few trees brings the CLL-loss of a mixture model in a rapid im-provement. Then the growth becomes slower until it reaches its first peak. After it passes the first peak, CLL-loss stops improving and becomes stable.

Figures 3(d), 3(e) and 3(f) show the performance changes in EMA. Notice that EMA is closely correlated with CLL-loss on all three datasets, and our stopping criteria is useful in optimizing EMA as well as CLL-loss. That is, EMA im-proves significantly while CLL-loss increases rapidly. Once CLL-loss becomes stable, EMA also seems to be stable and does not show any signs of fluctuation or overfitting.
I n this work, we proposed a new probabilistic approach to multi-label classification based on the mixture of Condi-tional Tree-structured Bayesian Networks. We devised and presented algorithms for learning the parameters of the mix-ture, finding multiple tree structures and inferring the maxi-mum a posteriori (MAP) output label configurations for the model. Our experimental evaluation on a range of datasets shows that our approach outperforms the state-of-the-art multi-label classification methods in most cases. This work was supported by grants R01LM010019 and R01GM088224 from the NIH. Its content is solely the re-sponsibility of the authors and does not necessarily represent the official views of the NIH. [1] A. Antonucci, G. Corani, D. D. Mau  X a, and [2] I. Batal, C. Hong, and M. Hauskrecht. An efficient [3] J. Berger. Statistical decision theory and Bayesian [4] U. Berkeley. Enron email analysis. [5] C. Bielza, G. Li, and P. Larra  X naga. Multi-dimensional [6] M. R. Boutell, J. Luo, X. Shen, and C. M. Brown. [7] J. K. Bradley and C. Guestrin. Learning tree [8] W. Cheng and E. H  X  ullermeier. Combining [9] A. Clare and R. D. King. Knowledge discovery in [10] K. Dembczynski, W. Cheng, and E. H  X  ullermeier. [11] J. Edmonds. Optimum branchings. Research of the [12] A. Elisseeff and J. Weston. A kernel method for [13] N. Ghamrawi and A. McCallum. Collective multi-label [14] S. Godbole and S. Sarawagi. Discriminative methods [15] M. Hauskrecht, I. Batal, M. Valko, S. Visweswaran, [16] M. Hauskrecht, M. Valko, I. Batal, G. Clermont, [17] M. Hauskrecht, M. Valko, B. Kveton, S. Visweswaram, [18] D. Hsu, S. Kakade, J. Langford, and T. Zhang. [19] S. Ioffe and D. Forsyth. Human tracking with [20] S. Ioffe and D. A. Forsyth. Mixtures of trees for object [21] H. Kazawa, T. Izumitani, H. Taira, and E. Maeda. [22] S. Kirshner and P. Smyth. Infinite mixtures of trees. [23] D. Koller and N. Friedman. Probabilistic Graphical [24] J. D. Lafferty, A. McCallum, and F. C. N. Pereira. [25] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A [26] M. Meil  X a and M. I. Jordan. Learning with mixtures of [27] T. Moon. The expectation-maximization algorithm. [28] M. Pakdaman, I. Batal, Z. Liu, C. Hong, and [29] G.-J. Qi, X.-S. Hua, Y. Rui, J. Tang, T. Mei, and [30] H. Raiffa. Decision Analysis: Introductory Lectures on [31] J. Read, B. Pfahringer, G. Holmes, and E. Frank. [32] D. Shahaf and C. Guestrin. Learning thin junction [33] D. Sontag. Approximate Inference in Graphical Models [34] F. Tai and H.-T. Lin. Multi-label classification with [35] R. E. Tarjan. Finding optimum branchings. Networks , [36] K. Trohidis, G. Tsoumakas, G. Kalliris, and I. P. [37] G. Tsoumakas, M.-L. Zhang, and Z.-H. Zhou. Learning [38] L. C. van der Gaag and P. R. de Waal.
 [39] T.  X  Singliar and M. Hauskrecht. Modeling highway [40] C. Yuan, T.-C. Lu, and M. J. Druzdzel. Annealed [41] M.-L. Zhang and K. Zhang. Multi-label learning by [42] M.-L. Zhang and Z.-H. Zhou. Multilabel neural [43] M.-L. Zhang and Z.-H. Zhou. Ml-knn: A lazy learning [44] Y. Zhang and J. Schneider. Multi-label output codes [45] Y. Zhang and J. Schneider. Maximum margin output
