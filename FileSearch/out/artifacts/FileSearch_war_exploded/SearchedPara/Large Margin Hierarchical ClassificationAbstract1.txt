 Multiclass categorization problems are concerned with the task of assigning labels to instances where the possible la-bels come from a predefined set. It is typically assumed that the set of labels has no underlying structure and therefore different classification mistakes are of the same severity. However, in many natural machine learning problems the set of labels is structured and different types of misclas-sifications should be treated differently. In this paper we focus on a particular type of structure over labels, namely a hierarchical one. In hierarchical classification the set of labels is arranged in a predefined hierarchy which takes the form of a rooted tree. For instance, both the Yahoo! hierar-chy and the open directory project (ODP) classify Internet web-sites to categories which reside in a tree. The labels in this tree entertain a special semantic, namely, if an inter-nal vertex in the tree represents some topic then its children will correspond to refinements of this topic. As an exam-ple, the vertex in the Yahoo! hierarchy representing the topic Sports , points to numerous sub-topics such as Cricket , Curling , and Canoeing . Following the link to the sub-topic Curling we find that it points to numerous sub-sub-topics such as Equipment and Tournaments . Each of these topics, as well as most of the topics in the Yahoo! hierarchy, is as-sociated with a set of webpages. A second notable example is speech phoneme classification. Short speech utterances are typically divided into phonetic classes. Phonetic theory of spoken speech embed the set of phonemes of western languages in a phonetic hierarchy where the phonemes are leaves of the tree and broad phonetic groups, such as vow-els and consonants, are internal vertices. In this paper we conduct experiments with these two hierarchical problems. The problem of hierarchical classification, in particular hi-erarchical document classification, has been tackled by nu-merous researchers (see for instance (Koller &amp; Sahami, 1997; McCallum et al., 1998; Weigend et al., 1999; Du-mais &amp; Chen, 2000)). Most previous work on hierarchical classification decouples the problem into independent clas-sification problems by assigning and training a classifier for each internal vertex in the hierarchy. To accommodate the semantics imposed by the hierarchical structure, some re-searchers have imposed statistical similarity constraints be-tween the probabilistic models for adjacent vertices in the hierarchy (e.g. (McCallum et al., 1998)). In probabilistic settings, statistical similarities can be enforced using tech-niques such as back-off estimates (Katz, 1987) and shrink-age (McCallum et al., 1998).
 A significant amount of recent work on classification prob-lems, both binary and multiclass, has been devoted to the theory and application of large margin classifiers. See for instance the book of Vapnik (1998) and the references therein. In this paper, we describe, analyze, and apply a large margin approach to hierarchical classification which is in the spirit of statistical approaches. As in large margin methods, we associate a vector in a high dimensional space with each label in the hierarchy. We call this vector the pro-totype of the label, and classify instances according to their similarity to the various prototypes. We relax the require-ments of correct classification to large margin constraints and attempt to find prototypes that comply with these con-straints. In the spirit of Bayesian methods, we impose sim-ilarity requirements between the prototypes corresponding to adjacent labels in the hierarchy. The result is an algo-rithmic solution that may tolerate minor mistakes, such as predicting a sibling of the correct label, but avoids gross errors, such as predicting a vertex in a completely different part of the tree.
 Many hierarchical datasets contains a very large number of examples. For instance, the file containing just the ODP hierarchy itself, without the documents, is 250Mb long. To cope with large amounts of data we devise an online algo-rithm that is both memory efficient and simple to imple-ment. Our algorithmic solution builds on the pioneering work of Warmuth and colleagues. In particular, we gener-alize and fuse ideas from (Crammer et al., 2003; Herbster, 2001; Kivinen &amp; Warmuth, 1997). These papers discuss online learning of large-margin classifiers. On each round, the online hypothesis is updated such that it complies with margin constraints imposed by the example observed on this round. Along with the margin constraints, the update is required to keep the new classifier fairly close to the pre-vious one. We show that this idea can also be exploited in our setting, resulting in a simple online update which can be used in conjunction with kernel functions. Further-more, using methods for converting online to batch learn-ing (e.g. (Cesa-Bianchi et al., 2004)), we show that the on-line algorithm can be used to devise a batch algorithm with theoretical guarantees and good empirical performance. The paper is organized as follows. In Sec. 2 we formally describe the hierarchical classification problem and estab-lish our notation. Sec. 3 constitutes the algorithmic core of the paper. In this section we describe an online algo-rithm, called online Hieron , for hierarchical classification and prove a worst case bound on its performance. In Sec. 4 we describe a conversion of the online Hieron into a well performing batch algorithm, the batch Hieron . In Sec. 5 we conclude the paper with a series of experiments on syn-thetic data, a text corpus, and speech data. Let X X  R n be an instance domain and let Y be a set of labels. In the hierarchical classification setting Y plays a double role: first, as in traditional multiclass problems, it encompasses the set of labels, namely each instance in X is associated with a label v  X  X  . Second, Y defines a set of vertices arranged in a rooted tree T . We denote k = |Y| , for concreteness we assume that Y = { 0 ,...,k  X  1 } and let 0 be the root of T .
 For any pair of labels u, v  X  X  , let  X  ( u, v ) denote their dis-tance in the tree. That is,  X  ( u, v ) is defined to be the num-ber of edges along the (unique) path from u to v in T . The distance function  X  (  X  ,  X  ) is in fact a metric over Y since it is a non-negative function,  X  ( v, v )=0 ,  X  ( u, v )=  X  ( v, u ) and the triangle inequality always holds with equality. As stated above, different classification errors incur different levels of penalty, and in our model this penalty is defined by the tree distance  X  ( u, v ) . We therefore say that the tree induced error incurred by predicting the label v when the correct label is u is  X  ( u, v ) .
 We receive a training set S = { ( x i ,y i ) } m i =1 of instance-label pairs, where each x i  X  X  and each y i  X  X  . Our goal is to learn a classification function f : X X  X  which attains a small tree induced error. We focus on classifiers that are of the following form: each label v  X  X  has a matching prototype W v  X  R n , where W 0 is fixed to be the zero vector and every other prototype can be any vector in
R n . The classifier f makes its predictions according to the following rule, The task of learning f is reduced to learning For every label other than the tree root v  X  X Y\ 0 } ,we denote by A ( v ) the parent of v in the tree. Put another way, A ( v ) is the vertex adjacent to v which is closer to the tree root 0 . We also define A ( i ) ( v ) to be the i th ancestor of v (if such an ancestor exists). Formally, A ( i ) ( v ) is defined recursively as follows, For each label v  X  X  , define P ( v ) to be the set of labels along the path from 0 (the tree root) to v , For technical reasons discussed shortly, we prefer not to deal directly with the set of prototypes W 0 ,..., W k  X  1 rather with the difference between each prototype and the prototype of its parent. Formally, define w 0 to be the zero vector in R n and for each label v  X  X \ 0 , let w v = W v  X  W A ( v ) . Each prototype now decomposes to the sum The classifier f can be defined in two equivalent ways: by setting { W v } v  X  X  and using Eq. (1), or by setting { w v } v  X  X  and using Eq. (2) in conjunction with Eq. (1). Throughout this paper, we often use { w v } v  X  X  as a syn-onym for the classification function f . As a design choice, our algorithms require that adjacent vertices in the label tree have similar prototypes. The benefit of represent-ing each prototype { W v } v  X  X  as a sum of vectors from { w v } v  X  X  is that adjacent prototypes W v and W A ( v ) be kept close by simply keeping w v = W v  X  W A ( v ) small. Sec. 3 and Sec. 4 address the task of learning the set { w v } v  X  X  from labeled data. In this section we derive and analyze an efficient online learning algorithm named online Hieron for the hierarchi-cal classification problem. In online settings, learning takes place in rounds. On round i , an instance, denoted x i , is pre-sented to the learning algorithm. Hieron maintains a set of prototypes which is constantly updated in accordance with the quality of its predictions. We denote the set of proto-types used to extend the prediction on round i by { w v i Therefore, the prediction of Hieron for x i is,  X  y i = argmax Then, the correct label y i is revealed and the algorithm suf-fers an instantaneous error. The error that we employ in this paper is the tree induced error. Using the notation above, the error on round i equals  X  ( y i ,  X  y i ) .
 Our analysis, as well as the motivation for the online up-date that we derive below, assumes that there exists a set of prototypes {  X  v } v  X  X  such that for every instance-label pair ( x i ,y i ) and every r = y i it holds that, The above difference between the projection onto the proto-type corresponding to the cor-rect label and any other proto-type is a generalization of the notion of margin employed by multiclass problems (Weston &amp; Watkins, 1999). Put in-formally, we require that the margin between the correct and each of the incorrect la-bels be at least the square-root of the tree-based distance be-tween them. The goal of the Hieron algorithm is to find a set of prototypes which fulfills the margin requirement of Eq. (4) while incurring a minimal tree-induced error until such a set is found. However, the tree-induced error is a combinatorial quantity and is thus difficult to minimize di-rectly. We instead use a construction commonly used in large margin classifiers and employ the the convex hinge-loss function where [ z ] + = max { z, 0 } . In the sequel we show that ( { w v } , x ,y ) upper bounds  X  ( y,  X  y ) and use this fact to attain a bound on m i =1  X  ( y i ,  X  y i ) .
 The online Hieron algorithm belongs to the family of con-servative online algorithms, which update their classifica-tion rules only on rounds on which prediction mistakes are made. Let us therefore assume that there was a prediction mistake on round i . We would like to modify the set of vec-tors { w v i } so as to satisfy the margin constraints imposed by the i th example. One possible approach is to simply find a set of vectors that solves the constraints in Eq. (4) (Such a set must exist since we assume that there exists a set {  X  v which satisfies the margin requirements for all of the ex-amples.) There are however two caveats in such a greedy approach. The first is that by setting the new set of proto-types to be an arbitrary solution to the constraints imposed by the most recent example we are in danger of forgetting what has been learned thus far. The second, rather techni-cal, complicating factor is that there is no simple analytical solution to Eq. (4). We therefore introduce a simple con-strained optimization problem. The objective function of this optimization problem ensures that the new set { w v i +1 is kept close to the current set while the constraints ensure by the new vectors. Formally, the new set of vectors is the solution to the following problem, First, note that any vector w v corresponding to a vertex v that does not belong to neither P ( y i ) nor P ( X  y i ) does not change due to the objective function in Eq. (6), hence, i +1 = w v i . Second, note that if v  X  X  ( y i )  X  X  ( X  y i the contribution of the w v cancels out. Thus, for this case as well we get that w v i +1 = w v i . In summary, the vectors that we need to actually update correspond to the vertices in the set P ( y i ) X  P ( X  y i ) where  X  designates the symmetric difference of sets (see also Fig. 1).
 To find the solution to Eq. (6) we introduce a Lagrange multiplier  X  i , and formulate the optimization problem in Algorithm 1 Online Hieron Initialize:  X  v  X  X  : w 1 v = 0 for i =1 , 2 ,...m the form of a Lagrangian. We set the derivative of the La-grangian w.r.t. { w v } to zero and get, Since at the optimum the constraint of Eq. (6) is binding we get that, Rearranging terms in the above equation and using the def-inition of the loss from Eq. (5) we get that, Finally, noting that the cardinality of P ( y i ) X  P ( X  y to  X  ( y i ,  X  y i ) we get that, The pseudo code of the online learning algorithm is given in Algorithm 1. The following theorem implies that the cumulative loss suffered by online Hieron is bounded as long as there exists a hierarchical classifier which fulfills the margin requirements on all of the examples.
 Theorem 1. Let { ( x i ,y i ) } m i =1 be a sequence of examples where x i  X  X  X  R n and y i  X  X  . Assume there exists a set {  X  v :  X  v  X  X } that satisfies Eq. (4) for all 1  X  i  X  Then, the following bound holds, where for all i , x i  X  R and  X  ( y i ,  X  y i )  X   X  max . Proof. As a technical tool, we denote by  X   X  the concate-nation of the vectors in {  X  v } ,  X   X  =  X  0 ,...,  X  k  X  1 similarly  X  w i = w 0 i ,..., w k  X  1 i for i  X  1 . We denote by  X  the difference between the squared distance  X  w i from  X  and the squared distance of  X  w i +1 from  X   X  , We now derive upper and lower bounds on m i =1  X  i . First, note that by summing over i we obtain, Our initialization sets  X  w 1 = 0 and thus we get, This provides the upper bound on i  X  i . We next derive a lower bound on each  X  i . The minimizer of the problem defined by Eq. (6) is obtained by projecting { w v i } onto the linear constraint corresponding to our margin requirement. The result is a new set { w v i +1 } which in the above notation can be written as the vector  X  w i +1 . A well known result (see for instance (Censor &amp; Zenios, 1997), Thm. 2.4.1) states that this vector satisfies the following inequality, Hence, we get that  X  i  X   X  w i  X   X  w i +1 2 . We can now take into account that w v i is updated if and only if v  X  P ( y i ) X  P ( X  y i ) to get that, Plugging Eq. (7) into the above, we get We now use the definition of  X  i from Eq. (8) to obtain the lower bound, Using the assumptions x i  X  R and  X  ( y i ,  X  y i )  X   X  max can further bound  X  i by, Now, summing over all i and comparing the lower bound given above with the upper bound of Eq. (9) we get, Multiplying both sides of the inequality above by  X  max R gives the desired bound.
 The loss bound of Thm. 1 can be straightforwardly trans-lated into a bound on the tree-induced error as follows. Note that whenever a prediction error occurs ( y i = X  y i hinge-loss defined by Eq. (5) is greater than  X  ( y i ,  X  y Since we suffer a loss only on rounds were prediction er-rors were made, we get the following corollary.
 Corollary 1. Under the conditions of Thm. 1 the following bound on the cumulative tree-induced error holds, To conclude the algorithmic part of the paper, we note that Mercer kernels can be easily incorporated into our algo-rithm. First, rewrite the update as w v i +1 = w v i +  X  v where, Using this notation, the resulting hierarchical classifier can be rewritten as, We can replace the inner-products in Eq. (12) with a gen-eral kernel operator K (  X  ,  X  ) that satisfies Mercer X  X  condi-tions (Vapnik, 1998). It remains to show that  X  v i can be computed based on kernel operations whenever  X  v i =0 . To see this, note that we can rewrite  X  i from Eq. (8) as  X  i =[  X  i ] + / X  i where and  X  i =  X  ( y i ,  X  y i ) K ( x i , x i ) . In the previous section we presented an online algorithm for hierarchical multiclass learning. However, many com-mon hierarchical multiclass tasks fit more naturally in the batch learning setting, where the entire training set S = { ( x i ,y i ) } m i =1 is available to the learning algorithm in ad-vance. As before, the performance of a classifier f on a given example ( x ,y ) is evaluated with respect to the tree-induced error  X  ( y, f ( x )) . In contrast to online learning, where no assumptions are made on the distribution of ex-amples, we now assume that the examples are indepen-dently sampled from a distribution D over X X Y . Our goal is to use S to obtain a hierarchical classifier f which attains a low expected tree-induced error, E [  X  ( y, f ( where expectation is taken over the random selection of ex-amples from D .
 Perhaps the simplest idea is to use the online Hieron algo-rithm of Sec. 3 as a batch algorithm by applying it to the training set S in an arbitrary order and defining f to be the last classifier obtained by this process. The resulting clas-sifier is the one defined by the vector set { w v m +1 } v  X  X  practice, this idea works reasonably well, as demonstrated by our experiments (Sec. 5). However, a variation of this idea yields a significantly better classifier with an accompa-nying generalization bound. First, we slightly modify the online algorithm by selecting  X  y i to be the label which max-imizes Eq. (5) instead of selecting  X  y i according to Eq. (3). In other words, the modified algorithm predicts the label which causes it to suffer the greatest loss. This modifica-tion is possible since in the batch setting y i is available to us before  X  y i is generated. It can be easily verified that Thm. 1 and its proof still hold after this modification. S is pre-sented to the modified online algorithm, which generates the set of vectors { w v i } i,v . Now, for every v  X  X  define and let f be the multiclass classifier defined by { w v } with the standard prediction rule in Eq. (3). We have set the prototype for label v to be the average over all proto-types generated by the online algorithm for label v .We name this approach the batch Hieron algorithm. For a gen-eral discussion on taking the average online hypothesis see (Cesa-Bianchi et al., 2004).
 In our analysis below, we use Eq. (13) to define the clas-sifier generated by the batch Hieron algorithm. However, an equivalent definition can be given which is much easier to implement in practice. As stated in the previous section, each vector w v i can be represented in dual form by As a result, each of the vectors in { w v } v  X  X  can also be represented in dual form by Therefore, the output of the batch Hieron becomes Theorem 2. Let S = { ( x i ,y i ) } m i =1 be a training set sam-pled i.i.d from the distribution D . Let { w v } v  X  X  be the vec-tors obtained by applying batch Hieron to S , and let f de-note the classifier they define. Assume there exist {  X  v } that define a classifier f which attains zero loss on S . Fur-thermore, assume that R , B and  X  max are constants such that x  X  R for all x  X  X  ,  X  v  X  B for all v  X  X  ,  X  (  X  ,  X  ) is bounded by  X  max . Then with probability of at least 1  X   X  , where L = m i =1 2 ( { w v i } , x i ,y i ) and  X  = kB 2 R Proof. For any example ( x ,y ) it holds that  X  ( y, f ( x ( { w v } , x ,y ) , as discussed in the previous section. Using this fact, it suffices to prove a bound on E 2 ( { w v } , to prove the theorem. By definition, 2 ( { w v } , x ,y ) equals  X   X  loss can be rewritten as  X   X  1 m +1 where C =  X  ( y, f ( x )) . Using the convexity of the func-tion g ( a )=[ a + C ] 2 + together with Jensen X  X  inequality, we can upper bound the above by m +1 Let max ( { w v } , x ,y ) denote the maximum of Eq. (5) over all  X  y  X  X  . We now use max to bound each of the sum-mands in the expression above and obtain the bound, Taking expectations on both sides of this inequality, we get
E 2 ( { w v } , x ,y )  X  1 Recall that the modified online Hieron suffers a loss of max ( { w v i } , x i ,y i ) on round i . As a direct conse-quence of Azuma X  X  large deviation bound (see for in-stance Thm. 1 in (Cesa-Bianchi et al., 2004)), the sum i =1 E 2 max ( { w v i } , x ,y ) is bounded above with prob-ability of at least 1  X   X  by, As previously stated, Thm. 1 also holds for the modified online update. It can therefore be used to obtain the bound max ( { w v m +1 } , x ,y )  X   X  and to conclude that, Dividing both sides of the above inequality by m +1 , we have obtained an upper bound on the right hand side of Eq. (16), which gives us the desired bound on E 2 ( { w v } , x ,y ) .
 Thm. 2 is a data dependent error bound as it depends on L We would like to note in passing that a data independent bound on E [  X  ( y, f ( x ))] can also be obtained by combining Thm. 2 with Thm. 1. As stated above, Thm. 1 holds for the modified version of the online Hieron described above. The data independent bound is derived by replacing L in Thm. 2 with its upper bound given in Thm. 1. We begin this section with a comparison of the online and batch variants of Hieron with standard multiclass classifiers which are oblivious to the hierarchical structure of the la-bel set. We conducted experiments with a synthetic dataset, a dataset of web homepages taken from the Open Direc-tory Project (ODP/DMOZ), and a data set of phonemes ex-tracted from continuous natural speech. The synthetic data was generated as follows: we constructed a symmetric tri-nary tree of depth 4 and used it as the hierarchical struc-ture. This tree contains 121 vertices which are the labels of our multiclass problem. We then set w 0 ,..., w 120 to be some orthonormal set in R 121 , and defined the 121 label prototypes to be W v = u  X  X  ( v ) w u . We generated 100 train instances and 50 test instances for each label. Each example was generated by setting ( x ,y )=( W y +  X  ,y ) , where  X  is a vector of Gaussian noise generated by ran-domly drawing each of its coordinates from a Gaussian dis-tribution with expectation 0 and variance 0 . 16 . This dataset is referred to as synthetic in the figures and tables appearing in this section.
 The second dataset is a set of 8576 Internet homepages col-lected from the World Wide Web. We used the Open Di-rectory Project (ODP/DMOZ) to construct the label hier-archy. DMOZ is a comprehensive human-edited directory of the web which defines a huge hierarchical structure over thousands of webpage topics. We extracted a subset of 316 vertices from the hierarchy, arranged in a tree of maximal depth 8 . The top level topics are Arts , Shopping , Sports and Computers . An example of a typical path in our hierar-chy is: Top  X  Computers  X  Hardware  X  Peripherals  X  Printers  X  Supplies  X  Laser Toner . Most of the internal nodes and all of the leaves in the hierarchy point to WWW documents. We used a bag-of-words variant to represent each document and used 4 -fold cross validation to evaluate the performance of the algorithms on this dataset. The last dataset used in our experiments is a corpus of con-tinuous natural speech for the task of phoneme classifi-cation. It has been previously established that phonemes form an acoustic hierarchy (Deller et al., 1987; Rabiner &amp; Schafer, 1978). For instance, the phoneme /b/ (as in be ) is acoustically closer to the phoneme /d/ (as in dad ), than for instance to the phoneme /ow/ (as in oat ). In general, stop consonants are acoustically similar to each other and rather dissimilar to vowels. The data we used is a subset of the TIMIT acoustic-phonetic dataset, which is a phonet-ically transcribed corpus of high quality continuous speech spoken by North American speakers (Lemel et al., 1986). Mel-frequency cepstrum coefficients (MFCC) along with their first and the second derivatives were extracted from the speech in a standard way, based on the ETSI standard for distributed speech recognition (ETSI, 2000) and each feature vector was generated from 5 adjacent MFCC vec-tors (with overlap). The TIMIT corpus is divided into a training set and a test set in such a way that no speakers from the training set appear in the test set (speaker inde-pendent). We randomly selected 2000 training instances and 500 test instances per each of the 40 phonemes. We trained and tested the online and batch versions of Hi-eron on all three datasets. To demonstrate the benefits of exploiting the label hierarchy, we also trained and eval-uated standard multiclass predictors which ignore the hi-erarchical structure. These classifiers were trained using Hieron but with a  X  X lattened X  version of the label hierar-chy. The (normalized) cumulative tree-induced error and the percentage of multiclass errors for each experiment are summarized in Table 1 (online experiments) and Table 2 (batch experiments). Rows marked by tree refer to the per-formance of the Hieron algorithm, while rows marked by flat refer to the performance of the classifier trained with-out knowledge of the hierarchy. The results clearly indi-cate that exploiting the hierarchical structure is beneficial in achieving low tree-induced errors. In all experiments, both online and batch, Hieron achieved lower tree-induced error than its  X  X lattened X  counterpart. Furthermore, in many cases the multiclass error of Hieron is also lower than the error of the corresponding multiclass predictor, although the latter was explicitly trained to minimize the error. This behavior exemplifies that employing a hierarchical label structure may prove useful even when the goal is not nec-essarily the minimization of some tree-based error. The last examination of results further demonstrates that Hieron tends to tolerate small tree-induced errors while avoiding large ones. In Fig. 2 we depict the differences between the error rate of batch Hieron and the error rate of a standard multiclass predictor. Each bar corresponds to a of 1 and ending on the right with the largest possible value of  X  ( y,  X  y ) . It is clear from the figure that Hieron tends to make  X  X mall X  errors by predicting the parent or a sibling of the correct vertex. On the other hand Hieron seldom chooses a vertex which is in an entirely different part of the tree, thus avoiding large tree induced errors. Examining the results for the DMOZ dataset, we see that for  X  ( y,  X  y ) &lt; 6 the frequency of errors of Hieron is greater than that of the multiclass predictor while Hieron beats its multiclass coun-terpart for  X  ( y,  X  y )  X  6 . In the phoneme classification task, Hieron seldom extends a prediction  X  y such that  X  ( y,  X  y )=9 while the errors of the multiclass predictor are uniformly distributed.

DMOZ
Phonemes We conclude the experiments with a comparison of Hieron with a common construction of hierarchical classifiers (see for instance (Koller &amp; Sahami, 1997)), where separate clas-sifiers are learned and applied at each internal vertex of the hierarchy independently. To compare the two approaches, we learned a multiclass predictor at each internal vertex of the tree hierarchy. Each such classifier routes an input in-stance to one of its children. Formally, for each internal vertex v of T we trained a classifier f v using the training set S v = { ( x i ,u i ) | u i  X  X  ( y i ) ,v = A ( u i ) , ( Given a test instance x , its predicted label is the leaf  X  y such that for each u  X  X  ( X  y ) and its parent v we have f v ( x In other words, to cast a prediction we start with the root vertex and move towards one of the leaves by progress-ing from a vertex v to f v ( x ) . We refer to this hierarchi-cal classification model in Table 2 simply as greedy .In all of the experiments, batch Hieron clearly outperforms greedy. This experiment underscores the usefulness of our approach which makes global decisions in contrast to the local decisions of the greedy construction. Indeed, any sin-gle prediction error at any of the vertices along the path to the correct label will impose a global prediction error. Censor, Y., &amp; Zenios, S. (1997). Parallel optimization:
Theory, algorithms, and applications . Oxford Univer-sity Press, New York, NY, USA.
 Cesa-Bianchi, N., Conconi, A., &amp; C.Gentile (2004). On the generalization ability of on-line learning algorithms. IEEE Transactions on Information Theory . (to appear). Crammer, K., Dekel, O., Shalev-Shwartz, S., &amp; Singer, Y. (2003). Online passive aggressive algorithms. Advances in Neural Information Processing Systems 16 .
 Deller, J., Proakis, J., &amp; Hansen, J. (1987). Discrete-time processing of speech signals . Prentice-Hall.
 Dumais, S. T., &amp; Chen, H. (2000). Hierarchical classifi-cation of Web content. Proceedings of SIGIR-00 (pp. 256 X 263).
 ETSI (2000). ETSI Standard, ETSI ES 201 108.
 Herbster, M. (2001). Learning additive models online with fast evaluating kernels. Proceedings of the Fourteenth
Annual Conference on Computational Learning Theory (pp. 444 X 460).
 Katz, S. (1987). Estimation of probabilities from sparse-data for the language model component of a speech rec-ognizer. IEEE Transactions on Acoustics, Speech and Signal Processing (ASSP) , 35 , 400 X 40.
 Kivinen, J., &amp; Warmuth, M. K. (1997). Exponentiated gra-dient versus gradient descent for linear predictors. Infor-mation and Computation , 132 , 1 X 64.
 Koller, D., &amp; Sahami, M. (1997). Hierarchically classify-ing docuemnts using very few words. Machine Learn-ing: Proceedings of the Fourteenth International Con-ference (pp. 171 X 178).
 Lemel, L., Kassel, R., &amp; Seneff, S. (1986). Speech database development: Design and analysis Report no. SAIC-86/1546). Proc. DARPA Speech Recognition Workshop.
 McCallum, A. K., Rosenfeld, R., Mitchell, T. M., &amp; Ng,
A. Y. (1998). Improving text classification by shrinkage in a hierarchy of classes. Proceedings of ICML-98 (pp. 359 X 367).
 Rabiner, L. R., &amp; Schafer, R. W. (1978). Digital processing of speech signals . Prentice-Hall.
 Vapnik, V. N. (1998). Statistical learning theory . Wiley. Weigend, A. S., Wiener, E. D., &amp; Pedersen, J. O. (1999). Exploiting hierarchy in text categorization. Information Retrieval , 1 , 193 X 216.
 Weston, J., &amp; Watkins, C. (1999). Support vector machines for multi-class pattern recognition. Proceedings of the
Seventh European Symposium on Artificial Neural Net-
