 Knowledge discovery in very large data sets is so much more than a learning or clustering task. Anecdotal reports cite that approximately 80% of a KDD task is spent on collecting, parsing, extracting, checking, and cleaning the available data. For KDD to be successful in the marketplace and for the field to realize the huge potential that is expected of it, the time from problem statement to system implementation must be significantly reduced. 
The field of KDD is too immature to pose a general solution to this challenge but in the case of large transactional data streams, enough progress has been made to commence a dialog on what seems to work in solving real business problems. By  X  X ata stream X  we mean a dynamic continuous flow of data. In contrast, a  X  X ata set X  is a static collection of data. The more important distinction however lies in the time constants associated with data streams and data sets: the value of the information in a static data set persists for months 
Figure 1: Example of an information platform for a large telecommunications data stream. build powerful tools for discriminating behaviors. 
An IMP has the responsibility for providing l that the data is complete (no records lost), l that the data is correct (including parsing), l that the data is sufficiently protected (from anyone l adequate storage of the data (for sequential and l a powerful and flexible processing environment that l template interfaces that enable easy distribution of a specific implementation (see Figure 1). On a typical business day, the AT&amp;T network carries about 250M calls. At the conclusion of a telephone call, a record is generated at the originating network switch. It contains fields such as the originating phone number, the dialed phone number, the time of the call, its duration and some additional switch data related to the way the call was routed. The switches concatenate files with call records and these are sent to a central collector every few minutes. Production support is 24 x 7 so that any anomalies can be addressed immediately. 
Since this stream is the source of billing records for the corporation, it provides very high quality data as regards completeness and accuracy. most creative part of profiling. In deciding what vari-ables to choose, one has to consider the computational complexity of calculating the variables and the amount of space the  X  X ollection of profiles will require. Some variables like counts and means only require a linear scan over the data. Others like the number of simul-taneous calls or the top five callers may require more complex processing and staging of data. The amount of space required is a function of the number of entities generating the transactions, the number of variables in-cluded in the profile, the degree of quantization of these variables, and finally the desired system performance. For example, one of our applications maintains approxi-mately 15 million 512-byte profiles for a total of 7.5GB. This is larger than the available memory on the current hardware so careful tuning was required so that per-formance would not be dominated by I/O limitations. In another application we maintain approximately 450 million 8-byte profiles for a total of 3.5GB. This fits into physical memory but required that each variable in the profile be quantized to 16 levels, thereby disallowing subtle changes through time. Profiles can be stored in a variety of formats depending on the needs of the common IMP applications. The simplest form is to store profiles in flat ASCII files. This format is convenient for browsing and accommodates variable length profiles. However, ASCII files are inefficient as regards space and processing time (e.g., there is considerable overhead reading an ASCII file into a C program). A simple alternative is using fixed-length profiles and storing them in binary files. Individual profiles can be retrieved by binary search (assuming that the files are sorted by entity id-number). In cases where performance demands entity indexing, an alternative is to store profiles in B-trees [Bayer &amp; McCreight19721. This is the storage format used in our 15 million 512 byte profile example. Finally, if the application has a tight coupling with a formal database management system the profiles can be conveniently stored in a relational DBMS. 
Updating profiles. Profiles need to be updated to reflect the most recent behavior. An update of a profile requires three steps, reading the profile from disk (or from a memory location), changing the values in the profile according to some statistical blending algorithm, and writing the profile to disk (or memory location). Two processing models are common: event-driven and time-driven processing. In event-driven updating, every new record results in the associated profile being updated as the record arrives in the stream. In time-driven updating, records are staged for some time period. At the end of the time period the records are summarized and the profiles updated. 
Event-driven updating ensures that the profiles are i.e., at most 10 seconds from mouse click to full page display. 
Our experience with including tools and templates in an IMP for KDD delivery has evolved and continues to evolve as performance shortcomings are uncovered, or as applications become more ambitious. This section presents an application of the general framework and methodology to a large data stream consisting of the toll-free (800 number) traffic on the AT&amp;T network. We first briefly describe some of the characteristics of this data stream and our information platform. Then we describe the steps we took to create profiles for toll-free numbers. Finally, we illustrate the use of these profiles for a fraud-detection application. On a typical day AT&amp;T carries about 1OOM toll-free calls. These calls terminate in approximately 1.5M distinct 800 numbers. Over a year, we observe approximately 4M distinct 800 numbers. The amount of traffic into these numbers varies considerably. Most receive relatively few calls per day, while others (e.g., 800-CALL-ATT) receives several million calls per day. The source of the toll-free data stream is the subset of CDRs obtained from the streamer module (Figure 1). We used a combination of domain knowledge and visualization techniques both to gain experience with the data, and to define meaningful variables. Extensive browsing of traffic to a variety of toll-free numbers led us to the following very simple profile: l distribution of call times: as captured by 24 bins l distribution of call durations: as captured by 12 l fraction of incomplete calls: single count, l total daily volume: in minutes, l birthday: the first day that we observed a call to the l recency: the most recent day that we observed a call 
This simple approach assigns a profile of the same size and with the same fields to all 800 numbers independently of how many calls they receive. Such a standardized profile makes it easy to compare the behavior of two 800 numbers or to compare 800-profiles to a pre-specified signature (in the same format). The profiles are small enough (41 bytes each) to store them create powerful predictors. Profiles of a few suspicious 800 numbers are displayed in Figure 2. Perusing a num-ber of such profiles and by analyzing the structure of the classifiers, we were able to simply characterize this kind of fraud. Specifically, fraudulent 800-numbers tend to have extensive late night activity (hence the name night owls) and long call durations. 
The predictors are now in place and in production within the security organization. The classifiers pro-duce prioritized lists that are displayed on a web in-terface with direct (active) links into various corporate databases. We are currently evaluating the alterna-tives but initial results suggests that all perform about equally well with fraud hit rates for the top 10% of the scores in the 70-80% range. We have discussed how an IMP can provide a robust and automated KDD computing infrastructure that can serve as a vehicle for fast response to new business needs. The IMP that we described is the result of a collaborative effort involving researchers from statistics, databases, machine learning, and visualization. It continues to evolve as the applications mature and we are able to further abstract some of the core functional steps required by rapid deployment of KDD. Our work has spawned several promising research projects that should further enhance our next generation IMP: l Hancock: a high level programming language for l Dynamic Strudel: enhancements to the Strudel 
Finally we continue to explore flexible visualization tools and techniques to easily allow KDD researchers and end-users to browse and navigate through large multidimensional data. 
The concepts presented in this paper are the result of three years experience with large transactional data streams. We also drew on the collective experience of a number of colleagues that contributed extensively to the current IMP that we now enjoy. We specifically acknowledge the pioneering work of Rick Becker and 
Diane Lambert (Lucent) on event-driven updating of profiles, Allan Wilks on the streamer module to filter the stream for target applications, Rick Greer for anticipating the need for an industrial-strength database management system for such leading edge applications, Rich Drechsler and Kirsten Schultz for the visualization tools that have been invaluable, and 
