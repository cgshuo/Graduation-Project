 In the multi-class clustering problem, we are given n data points, x c . The goal is to partition the given data x are in some sense  X  X istinct X  from each other. Here x space.
 very popular spectral clustering approach [3, 10].
 spectral clustering approach in the experiments.
 to the clustering problem and good results are obtained.
 section. In the following,  X  X eighboring points X  or  X  X eighbors X  of x x according to some distance metric. 3.1 Local Learning in Supervised Learning VC dimension. 3.2 Representation of Clustering Results in [2, 10]. We also use a Partition Matrix (PM) P = [ p scheme. Namely p 1 / p |C l | . Clearly we have mapping P (  X  ) defined as R n , 1  X  l  X  c , is the l -th column of F . 3.3 Basic Idea good clustering result), we propose to solve the following optimization problem: where o l kernel learning algorithms [5], using the training data { ( x of o cluster, and the subscript i means the KM is trained with the neighbors of x the training data { ( x variables of the problem (3) X (4).
 To explain the idea behind problem (3) X (4), let us consider the following problem: Problem 1. For a data point x the proper value of f l training data { ( x that o l Therefore, a good SPM F should have the following property: For any x cluster C f should be similar to the output of the KM that is trained locally with the data { ( x different clusters.
 cluster assignment of its neighbors. 3.4 Computing o l a concrete clustering algorithm. So we consider, based on x pute o l algorithms on { ( x where K : X X X X  R is a positive definite kernel function [5], and  X  l o problem: where  X  l the regularization parameter, f l the kernel matrix over x Solving problem (6) leads to  X  l where k linear equation: where  X  It can be seen that  X  Note that f l where o l and f l are the same as in (3), while the matrix A = [ a follows:  X  x in (9), otherwise a index l .
 Substituting (10) into (3) results in a more specific optimization problem: where function (11). 3.5 Relaxation matrices [10]: have this advantage. 3.6 Discretization: Obtaining the Final Clustering Result F matrix R and the discrete PM P , we can solve the following optimization problem [10]: where 1 the approach in [10] to get the final clustering result. 3.7 Comparison with Spectral Clustering category of spectral clustering approaches.
 experiments.
 pute the matrix A in (10), which in turn requires calculating  X  this is very easy to implement and A can be computed with time complexity O ( P n In practice, just like in the spectral clustering method, the number of neighbors n fixed small value k for all x compute T explicitly.
 http://www.cis.upenn.edu/  X  jshi/software/. 4.1 Datasets The following datasets are used in the experiments. Further details of these datasets are provided in Table 1.
 n , the data dimensionality d and the number of classes c are provided. 4.2 Performance Measure rithms with the true classes by computing the following two performance measures. 4.2.1 Normalized Mutual Information For two random variable X and Y , the NMI is defined as [7]: value of NMI. Given a clustering result, the NMI in (20) is estimated as [7]: where n belonging to the h -th class ( 1  X  h  X  c ), and n tion between the cluster C 4.2.2 Clustering Error The classification error based on map (  X  ) can then be computed as: where y matching of a bipartite graph. 4.3 Parameter Selection the following two kernel functions are tried respectively in the experiments. The cosine kernel: and the Gaussian kernel: The parameter  X  in (23) is searched in:  X   X  X   X  2 where  X  as the kernel function in (5). The number of neighbors n parameter  X  (cf. (6)), which is selected from the grid:  X   X  X  0 . 1 , 1 , 1 . 5 } . smallest objective value is finally selected for LLCA.
 the number of clusters is given. 4.4 Numerical Results outperform the spectral clustering and the k-means algorithm in most cases. LLCA can work well in many cases, and clearly it still needs improvement. k-means algorithm. segmentation.

