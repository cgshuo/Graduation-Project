 Topic models are used to group words in a text dataset into a set of relevant topics. Unfortunately, when a few words frequently appear in a dataset, the topic groups identified by topic models become noisy because these frequent words repeatedly appear in  X  X rrelevant X  topic groups. This noise has not been a serious problem in a text dataset because the frequent words (e.g., the and is ) do not have much meaning and have been simply removed before a topic model analysis. However, in a social network dataset we are interested in, they correspond to popular persons (e.g., Barack Obama and Justin Bieber ) and cannot be simply removed because most people are interested in them.

To solve this  X  X opularity problem X , we explicitly model the popularity of nodes (words) in topic models. For this purpose, we first introduce a notion of a  X  X opularity compo-nent X  and propose topic model extensions that effectively ac-commodate the popularity component. We evaluate the ef-fectiveness of our models with a real-world Twitter dataset. Our proposed models achieve significantly lower perplexity (i.e., better prediction power) compared to the state-of-the-art baselines.

In addition to the popularity problem caused by the nodes with high incoming edge degree, we also investigate the ef-fect of the outgoing edge degree with another topic model extensions. We show that considering outgoing edge degree does not help much in achieving lower perplexity.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  clustering ; D.2.8 [ Software Engi-neering ]: Metrics X  performance measures Experimentation, Algorithms topic model, social-network analysis, popularity bias, han-dling popular users, Latent Dirichlet Allocation
Microblogging services such as Twitter are popular these days because they empower users to broadcast and exchange information or thoughts in realtime. Distinct from other social network services, relationships on Twitter are unidi-rectional and often interest-oriented. A user may indicate her interest in another user by  X  X ollowing X  her, and previ-ous studies [16, 23] show that users are more likely to follow people who share common interests, even though  X  X ollow-ing relationships X  among users look unorganized and hap-hazard at first glance. Thus, if we can correctly identify the shared hidden interests behind users X  following relationships, we can recommend more relevant users and group users shar-ing common interests in the social network services.
In this paper, we refine topic models to correctly identify the hidden interests behind users X  following relations (in-stead of their tweets as in [30]). A topic model is a sta-tistical model originally developed for discovering hidden topics from a collection of documents. It postulates that every document is a mixture of topics, and words in a doc-ument are attributable to these hidden topics. Here, we posit that the following relations are not random but are interest-attributable. Then, we can discover the hidden in-terest behind each following relation by regarding a user X  X  following list as a document, and each person in the user X  X  following list as a word. Now, topic models can easily help us correctly identify the hidden interests and derive a low dimensional representation of the observed following lists.
However, simply applying topic models to the follow-relation analysis may cause some problems. Our previous study [7] reported significant clustering quality degradation when the authors directly applied Latent Dirichlet Allocation (LDA) [5] to Twitter X  X  following relation dataset. As LDA is built on an assumption that every word in a document should be of roughly equal popularity, stop words like the and is must be removed in preprocessing stages. However, keeping popular persons like Barack Obama and Justin Bieber in a user X  X  following list can be beneficial for the following rea-sons: (1) These well-known users can work as an effective labels of identified topic groups. For example, when a group contains well-known politicians like Barack Obama, we may immediately identify that the group is likely to be on poli-tics. 1 (2) Many new Twitter users do not know who is on Twitter and who is not, so they often fail to follow popular users of potential interest, not knowing their presence. If the system can recommend interesting and well-known users of interest, it can significantly improve the users X  return rate and stickiness [31].

In this work, we propose refined topic models specialized in handling the quality degradation caused by a limited num-ber of popular users, which we call  X  X opularity bias X . For this, we introduce a notion of a  X  X opularity component X  and explore various ways to effectively incorporate it. We also evaluate the effectiveness of our proposed topic models with the widely used  X  X erplexity X  2 calculated over the real-world Twitter following relation dataset.

Note that the popularity bias is not limited to social net-work datasets. This bias often appears in datasets showing user X  X  preference over some popular items (or nodes) such as webpage visit logs, advertisement click logs, product pur-chase logs, etc. We believe our proposed models can effec-tively improve the quality of recommendations and cluster-ings in the web services generating such logs.

In summary, we make the following contributions in this paper:
In this section, we briefly review topic models related to social-network analysis in three categories: topic models for authorship, hypertexts, and edges.

The topic models in the first category were proposed to an-alyze documents (texts) with their authors. As these mod-els incorporate authors and their relationships in the model, they can be viewed as early forms of social-network topic models. They attempt to group documents and authors by assuming that a document is created by authors shar-ing common topics. The concept of authors (users) was ini-tially introduced by Steyvers et al. [34] in the Author-Topic (AT) model. With the additional co-author information, they could successfully extract hidden research topics and trends from CiteSeer X  X  abstract corpus. The AT model was
Existing topic models simply identify a group of words (or users) that belong to a topic group, not the semantic labels of each group.
It measures prediction power of a trained model. The def-inition is given in Section 5.2 extended to the the Community User Topic (CUT) model by Zhou et al. [41] to capture semantic communities. McCalum et al. [24] also extended the AT model and proposed the Author-Recipient-Topic (ART) model and the Role-Author-Recipient-Topic (RART) model to analyze e-mail networks. Pathak et al. [29] modified the ART model and suggested the Community-Author-Recipient-Topic (CART) model, which is similar to the RART model. In addition to these AT model family, other LDA extensions and probabilistic topic models were also proposed to analyze chat data [36], voting data [38], annotation data [22], and tagging data [17].
The topic models in the second category are more closely related to social-network analysis and analyze documents with their citations (i.e., hypertexts). Cohn et al. [10] ini-tially introduced a topic model combining PLSI [20] and PHITS [9]. Later, PLSI in this model was replaced with LDA [5] by Erosheva et al. [13]. Nallapati et al. [27] extended Erosheva X  X  model and proposed Link-PLSA-LDA model which applies PLSI and LDA to cited and citing documents, re-spectively. Chang et al. [8] also proposed the Relational Topic Model (RTM) which models a citation as a binary random variable. Dietz et al. [12] proposed a topic model to analyze topical influence of research publications. More so-phisticated models were proposed by Gruber et al. [15] and Sun et al. [35]. Hybrid approaches were also attempted. Mei at al. [25] introduced a regularized topic modeling framework incorporating a graph structure and Nallapati et al. [26] combined network flow and topic modeling.

The topic models in the last category only uses linkage (edge) information. Since they only focus on the graph structure, they can be easily applied to a variety of datasets. However, there has been relatively less research in this cate-gory. Our work belongs to this category and focuses on solv-ing the issue caused by popular nodes in the graph structure. Airoldi et al. [3] proposed the Mixed Membership Stochastic Block (MBB) model to analyze pairwise measurements such as social networks and protein interaction networks. Zhang et al. [40] and Henderson et al. [18] dealt with the issues in applying LDA to academic social networks. The former focused on the issue of converting the co-authorship infor-mation into a graph, and proposed edge weighting schemes based on collaboration frequency. The latter addressed the issue of a large number of topic clusters generated due to  X  X ow popularity X  nodes in the network. The  X  X igh popular-ity X  issue was initially addressed by Steck [32]. He defined a new metric called Popularity-Stratified Recall and sug-gested a matrix factorization method optimizing it. In our previous study [7], we investigated the issue in more detail and proposed effective solutions based on probabilistic topic models. However, our previous solutions are rather heuris-tic and more about how to tune topic models to handle the high popularity issue. In this paper, we take a more princi-pled approach to this issue and propose more effective topic models.
In this section, we explain how to apply topic models to social network analysis. After briefly explaining two proba-bilistic topic models, Probabilistic Latent Semantic Indexing (PLSI) and Latent Dirichlet Allocation (LDA), we introduce the challenge of a  X  X opularity bias X  which we address in this study.
Topic models are built on the assumption that there are latent variable(s) behind each observation in a dataset. In the case of a document corpus, the usual assumption is that there is a hidden topic behind each word. PLSI [20] intro-duced a probabilistic generative model to Latent Semantic Indexing (LSI) [11], one of the most popular topic mod-els. Equation (1) represents its document generation process based on the probabilistic generative model: p ( d,w ) denotes the probability of observing a word w in a document d and can be decomposed into two parts: p ( d ), the probability distribution of documents, and p ( w | d ), the probability distribution of words given a document. This equation describes a word selection process for a document, where an author first selects a document then a word in that document. By repeating this selection process sufficiently, we can generate a full document and eventually a whole document corpus. Based on the assumption that there is a latent topic z for each word w , the equation above can be rewritten with the multiplication of p ( w | z ), the probability distribution of words given a topic, and p ( z | d ), the probabil-ity distribution of topics given a document. In this way, an additional topic selection step is added between the docu-ment selection step and the word selection step. We sum the multiplication over a set of all independent topics Z because there exist a number of possible topics from which a word may be derived.

The goal of the topic model analysis is to accurately infer p ( w | z ) and p ( z | d ). Given the probabilistic generative model explained above, we can effectively infer p ( w | z ) and p ( z | d ) by maximizing the log-likelihood function L of observing the entire corpus as in Equation (2): where n ( d,w ) denotes the word frequency in a document. The inferred p ( w | z ) and p ( z | d ) measure the strength of as-sociation between a word w and a topic z and that be-tween a topic z and a document d , respectively. For ex-vehicle is more closely related to the topic car than the word technology , though they are all related to the topic car . In this way, PLSI and other probabilistic topic models support multiple memberships and produce more reasonable clustering results.

Although PLSI introduced a sound probabilistic genera-tive model, it showed a poor performance when predicting unobserved words and documents. To solve this  X  X verfit-ting X  problem, LDA [5] introduced Dirichlet priors  X  and  X  to PLSI, to constrain p ( z | d ) and p ( w | z ), respectively.  X  is a vector of dimension | Z | , the number of topics, and each element in  X  is a prior for a corresponding element in p ( z | d ). Thus, a higher  X  i implies that there are more frequent prior observations of topic z i in a corpus. Similarly,  X  is a vector of dimension | W | , the number of words, and each element in  X  is a prior for a corresponding element in p ( w | z ). By placing Dirichlet priors  X  and  X  on the multinomial distri-butions p ( z | d ) and p ( w | z ), these posterior distributions are smoothed by the amount of priors  X  and  X  , and the model becomes safe from PLSI X  X  overfitting problem. As a con-jugate prior for the multinomial distribution, the Dirichlet distribution also simplifies the statistical inference and en-ables the use of the collapsed Gibbs sampling [33]. It is also known that PLSI emerges as a specific instance of LDA under Dirichlet priors [14, 19].
In a social network service, a user r  X  X  following another user w can be intuitively interpreted as the user r (acting as a  X  X eader X ) expresses her interest in tweets written by the user w (acting as a  X  X riter X ). We believe this interest plays a role in the establishment of the following relation (or edge) as the topic does in the document generation process explained in Section 3.1. In this study, we assume that there exists a follow edge generative model: a reader first chooses an interest, and based on the chosen interest, the reader chooses a writer to follow. In this model, a document in a corpus becomes a reader X  X  following list, and a word becomes a writer in the list.

Analyzing Twitter follow edges using LDA delivers two interest distribution and p ( w | z ) indicates a writer w  X  X  im-portance in an interest group z . Thus, p ( w | z ) can be used in clustering Twitter users having the same interest. From Equation (1), we can easily estimate p ( w | r ), the likelihood of a reader r  X  X  following a writer w , which can be used for recommendation.

When we apply topic models to a social network dataset, we notice the following differences [7]: 1. In a document generative model, a word is sampled 2. When analyzing a textual dataset, common entries like
Because of the first difference, some probability distribu-tions in our model follow multivariate hypergeometric dis-tributions instead of multinomial distributions . This differ-ence is important because LDA benefits from Dirichlet pri-ors, which are conjugate priors of multinomial distributions. However, it is known that a multivariate hypergeometric distribution converges to a multinomial distribution as the population size grows large [1]. Since millions of users are included in our Twitter dataset, we can disregard the con-sequence caused by the sampling without replacement .
The second difference affects the quality of a topic model analysis. When celebrities are simply included without any special handling, they appear even in irrelevant topic groups and make the topic analysis severely biased to them. Such a  X  X opularity bias X  can be seen everywhere, from purchase logs to movie rating data. In the next section, we propose refined topic models which address this popularity bias.
In this section, we introduce a notion of a  X  X opularity component X  using a simple model , which acts as a base of our later models. We propose three refined models, and discuss how they may ease the popularity bias. At the end of this section, we explore various extensions to these refined models.
As described in Section 3.2, in our follow edge generative models, a reader first selects an interest (a topic) from a dis-tribution p ( z | r )(  X  ), and then selects a writer from a distri-bution p ( w | z )(  X  ). The  X  and  X  are constrained by Dirichlet priors  X  and  X  , respectively. This process is depicted in a plate notation in Figure 1(a). We formulate the probability for a reader a to follow a writer b based on an interest z (or z a,b ) as follows:
Note that this equation is equivalent to Equation (1), and we use w a,b to indicate a follow edge between a reader a to a writer b . By considering the Dirichlet priors  X  and  X  , the same probability can be represented in LDA as follows: p ( z a,b | X  )  X 
In a simple model , we incorporate a  X  X opularity compo-nent X  into LDA, as in Figure 1(b). The popularity compo-nent (in a dotted box) consists of a multinomial distribution  X  , which represents an in-corpus writer distribution, and a Dirichlet prior  X  constraining  X  . Note that  X  is a vector of length J , the number of unique writers, and each element has a value of  X  w = f w f quency of a writer w (i.e., the number of followers to the writer), and f  X  denotes a total frequency ( P w f w ). Thus, in the simple model , when a reader follows a writer, the writer selection probability  X  is multiplied by  X  so that popular writers are weighted accordingly. We formulate this change (from Equation (4)) into the following equation: p ( z a,b | X  )  X 
Although the simple model incorporates the popularity component in LDA, this incorporation is too simple. When-ever a reader follows a writer, the model favors a popu-lar writer according to her in-corpus distribution  X  . How-ever, the in-corpus writer distribution can be largely differ-ent from a in-topic writer distribution. For example, though Barack Obama is more popular than Justin Bieber in general, Justin Bieber is more popular than Barack Obama among people who like music. Thus, the writer should be picked up from the in-corpus distribution or the in-topic distribu-tion. When every topic is assumed to be equally likely (as in LDA X  X  symmetric prior assumption), the in-corpus writer distribution is the sum of per-topic (in-topic) writer distri-butions ( p ( w ) = P z p ( w | z ) p ( z )), and we may consider the former as a global writer distribution and the latter as a local writer distribution. Since a writer is picked up from her global (in-corpus) distribution or local (per-topic) distri-bution, we may represent a popularity-incorporated writer distribution as a mixture of the global distribution and the local distribution. This interpretation leads us to a polya-urn model depicted in Figure 1(c). In [2], the authors took a similar approach for a topic distribution  X  to capture global topics as well as local topics.

Figure 1(c) depicts how the global and local distribution are populated with the popularity component depicted in the dotted box. In addition to the  X  and  X  in the simple model , the popularity component in the polya-urn model has a concentration scalar  X  . Initially, the multinomial distribu-tion  X  is generated from the Dirichlet prior  X  . Then,  X  works as a Dirichlet prior for  X  , together with the concentration scalar  X  . As  X  works as a weight to the prior observation  X  ,  X  becomes similar to  X  when  X  has a high value. On the other hand,  X  deviates from  X  when  X  has a low value. Since  X  works as a base distribution and  X  deviates from  X  per topic,  X  can be considered as a global (in-corpus) writer distribution, and  X  can be considered as a local (per-topic) writer distribution.

To derive a collapsed Gibbs sampling equation for the polya-urn model , we define c k,m,j as the number of asso-ciations between a topic z k and a writer w j followed by a reader r m (or a follow edge from a reader r m to a writer w as in Equation (6) [33]: We also define c  X  ( a,b ) k,m,j as the count when we exclude the edge from a reader r a to a writer w b . Then the collapsed Gibbs sampling equation of LDA (derived from Equation (4)) becomes: where the symbol * denotes a summation over all possible subscript variables. As we select a writer from a mixture of a global and a local writer distribution, the topic assignment probability of the polya-urn model should be extended to: p ( z a,b | X  )  X 
Note that the global distribution dominates in the mixture as the concentration parameter  X  increases. On the other hand, as  X  decreases, the local distribution dominates and the whole equation becomes similar to that of LDA.
In the polya-urn model , when a reader follows a writer, she first selects a topic, and then selects a writer from the mixture of a global and a local writer distribution for the selected topic. Although the mixture explains non-topic re-lated (popularity-based) following relations as well as topic related (interest-based) ones, the initial topic selection pro-cess is common in both cases. In a two-path model , we clearly separate the non-topic related following relations from the topic related ones by assuming that there are two separate paths from which a writer can come. This separation in early stage is expected to help generate more clear topics. For this separation, we introduce a new binary latent vari-able t which indicates the path the writer comes from. t = 1 means that the writer comes from a  X  X opic path X  and t = 0 means that she comes from a  X  X opularity path X . Now, we do a  X  X ath-labeling X  as well as a  X  X opic-labeling X  for a follow edge, and our goal is to accurately infer t as well as z (when t = 1).

Figure 1(d) depicts this two-path model . The variable t follows a Bernoulli distribution  X  which is constrained by a Beta prior  X  . As a more popular writer may have a higher probability of being followed through the popularity path than a less popular writer, we pose an asymmetric prior according to the writer X  X  popularity. For example, larger portion of edges to Barack Obama will be labeled with the popularity path because he has lower  X  and  X  . We extend Equation (6) with the new path indicator variable t : Then, the path-labeling and the topic-labeling probability are derived as: where  X  is defined with a scaling constant C 1 as: The two latent variables are inferred simultaneously in every Gibbs sampling iteration. The topic-labeling process is per-formed only when t a,b = 1. When t a,b = 1 for all edges, the two-path model becomes equivalent to the standard LDA.
In the two-path model , we assumed that there are two paths from which a writer can come. Then, we introduced the path indicator t to denote the path from which the writer comes. While a writer from the topic path is assigned with a topic, a writer from the popularity path is ignored and not assigned with a topic. We generalize this binary topic indicator t to a non-negative weight (confidence) in a weight model . For example, when  X  obama = 0 . 7 in the two-path model , seven out of ten w obama observations (follow edges), likely come from the topic path and the three likely come from the popularity path. Instead of probabilistically se-lecting which w obama observation comes from which path, we may uniformly assign the  X  obama value to each w obama observation. This  X  value can be viewed as a weight or a confidence value. When we are very confident that a writer observation comes from the topic path, we may assign a value 1 to the writer observation. In the opposite case, we may assign 0 to it. If we are 70% confident, we may assign 0 . 7 to each writer observation.

Figure 1(e) depicts the newly introduced weight value  X  in the dotted box.  X  is associated with each writer observation and has a non-negative real number. If we strongly believe a writer is from the topic path, we may assign a high weight (even bigger than 1). Otherwise, we assign a value close to 0 or 0. Equation (14) is a  X  -incorporated version of Equation (6): where  X  n is defined with a scaling constant C 2 as: As in the two-path model , we believe that popular writers more likely come from the popularity path and should be assigned lower weights. When  X  n = 1 for all writers, the equation becomes equivalent to that of LDA. While the two-path model requires the additional path-labeling process, the weight model is free from it and has the same complexity as LDA. The confidence and weight approach on observations can be found in the literature on recommender systems [21, 28, 32]. Especially, Steck [32] defined a new metric called Popularity-Stratified Recall by assigning lower weights to popular items, and suggested a matrix factorization method optimizing the metric. The term weighting scheme for LDA was also proposed for cross-language retrieval [39].
We may further extend our models by considering reader-side popularity (in a sense that edges from a reader are more frequent in a dataset) as well as the writer-side popularity discussed so far. The reader-side popularity shows how  X  X c-tive X  she is because it represents the length of her following list. We expect that an active reader who follows more writ-ers can be considered less  X  X opic-sensitive X  (topic-focused) than one who follows fewer writers. If we include the reader-side popularity in the previous models, Equation (8), (10), and (15) should be accordingly extended into: where is a prior for reader X  X  path indicator distribution and defined as:
For the two-path model , we also try posing hyper priors  X  and 0 over  X  and , respectively, similar to the approach in [37]. Then, Equation (17) should be extended to: We also combine the polya-urn model with weight model . There are many possible combinations in mixing two models. We report only the meaningful results in the next section.
Before moving to the next section, we summarize the sym-bols used in this paper in Table 1.
 Table 1: Symbols used throughout this paper and their meanings
Symbol Meaning  X , X , X , X , Dirichlet (Beta) priors
In this section, we evaluate the proposed models based on the perplexity value calculated using a real-world Twitter dataset. As baselines, we use LDA and the best performer in our prior work [7] 3 . The experimental results show that our proposed models are very effective in lowering perplexity. We also discuss why and how they perform better than the baselines.
We use the Twitter dataset we used in our previous work [7]. It has 10 million follow edges from 14 , 015 reader to
As perplexity is only available for probabilistic topic mod-els, we limited our baselines to probabilistic topic models. 2 , 427 , 373 writers. The dataset is sampled to ensure that all the outgoing edges from a randomly sampled reader are pre-served. The average number of outgoing edges for a reader is 713 . 52.

Table 2 summarizes the 12 representative experimental cases we report in this section. We have two baselines: base-lda 4 , the standard LDA experiment, and base-f2step , the best performer in our previous work [7]. polya-w/r/wr denotes the polya-urn model experiment considering writer popularity, reader popularity (activeness), and both side popularity, respectively. 2path-w/r/wr are cases from the two-path model experiments, and wlda-w/r/wr are from the weight model experiments. p-w &amp; w-r denotes a combination of polya-w and wlda-r . Though we ran a lot more experi-ments than reported ones here, we only report some mean-ingful ones for clarity and to save space 5 . Other cases like the simple model , the two-path model with hyper pri-ors, and various combination models did not show much im-provements. For the weight model , we tested various weight-ing schemes based on frequency, probability, PageRank [6], pointwise mutual information (PMI) [39], etc. We only re-port the best weighting schemes in this section. The best weighting scheme might be different for a different corpus. In all of our experiments, we ran 100 Gibbs sampling iter-ations and generated 100 topic groups because they turned out reasonable in our previous experiments [7]. We also define a popular writer as a writer having more than 100 followers in our experiments 6 . We ran multiple runs to find the following optimal parameter values:  X  = 0 . 1,  X  1 = 0 . 2,  X  2 = 20 . 0, C 1 = 4 . 2, C 2 = 2 . 0, C 3 = 2 . 0, and C 4 base-f2step Two-step labeling with filtering polya-w/r/wr Polya-urn model for writer/reader/both 2path-w/r/wr Two-path model for writer/reader/both wlda-w/r/wr Weight model for writer/reader/both
We evaluate our proposed models using the widely-used perplexity metric [5, 7, 18, 19, 40] defined as: where W test denotes all the writers (edges) in a test dataset. The perplexity quantifies the prediction power of a trained model by measuring how well the model handles unobserved test data. Since the exponent part of Equation (22)is a mi-nus of the average log prediction probability over all the test edges, a lower perplexity means stronger prediction power of the model. In our previous study [7], slightly lower (3 . 27%) perplexity led to significant (64%) improvement on human-
We implemented our models based on the LDA implemen-tation in [4].
We tested simple/polya/2path/wlda-w/r/wr , p-w/r/wr &amp; w-w/r/wr , and hyperprior-w/r/wr .
We also tested 50 and 500 as the boundary value instead of 100 and the results were similar.
 perceived clustering quality 7 . Thus, we believe that we can significantly improve the clustering quality of the model by further lowering its perplexity. We calculated the perplex-ity for two separate 10% randomly held-out datasets after training a model on the remaining 80% dataset. We av-eraged results from ten runs (five runs for each held-out dataset with different random seeds). As the standard LDA ( base-lda ) is designed to minimize perplexity, it is not easy to achieve lower perplexity than base-lda .

We report the perplexity values from the 12 representative test cases in Figure 2, where we observe: 1. All our proposed models seem very effective in achiev-2. 2path-w shows the lowest perplexity. It shows 9 . 41% 3. Different from the polya-urn model and the two-path 4. The combination models do not perform better than
If we pick the best performers in each group, the per-formance order would be: two-path model &gt; weight model &gt; combination model &gt; polya-urn model &gt; base-f2step &gt; base-lda . In the two following sections, we investigate the two-path model and the weight model in detail.
To visualize the effect of the path-labeling, we compare the top-10 writers in two example topic groups related to technology from base-lda and 2path-w in Figure 3. In the left
The correlation between the perplexity and the human-perceived clustering quality was  X  0 . 806 in our previous work (excluding HLDAs). figure, we see two highlighted writers who are not related to technology : barackobama and stephenfry . These celebri-ties are included in this group because they are so popu-lar and followed by the people who are interested in tech-nology as well. The standard LDA labels the follow edges from these people as technology -related even though those edges are purely generated from the popularity path. On the other hand, all the writers in the right figure are closely related to technology . The path-labeling reduces the chance of celebrities X  appearing in irrelevant topic groups by label-ing non-topic-driven follow edges with the popularity path as explained in Section 4.3.
 Avg. num. of edges to w 387.41 71.94 5.39 Avg. num. of edges from r 509.90 846.06 0.60
To measure the effect of the path-labeling, we gathered some statistics from 2path-w and report them in Table 3. We observe that 8 . 72% of the test edges were processed by the popularity component ( group-p ) and 91 . 28% of them were processed by the topic component ( group-t ). We also calculated the average writer/reader popularity (number of incoming/outgoing edges) for the edges in both groups. The second and the third row of Table 3 tell us that each edge in group-p is from a reader following 509 . 90 writers on av-erage 8 to a writer having 387 . 41 followers on average. We observe that the edges in group-p belong to much more pop-ular writers than those in group-t . However, the edges in group-p seem to belong to less active readers. This finding contradicts our initial expectation on the  X  X ctiveness X , ex-plained in Section 4.5, that a more active reader would be less topic sensitive. To more closely investigate this contra-diction, we measured average entropy of p ( z | r ) and p ( z | w ). The intuition behind this measurement is that the higher
We excluded the readers who follow less than 10 writers from our dataset because they may contain follow edges gen-erated by pure curiosity and reciprocity. The same approach was used in [30].
 entropy (uncertainty) of p ( z | r ) or p ( z | w ) of an edge (from reader r to writer w ) may indicate that the edge has a lower probability to be labeled with a specific topic. Thus, it may have a higher chance of being labeled with the popularity path than one with a lower entropy. The fourth row of Table 3 9 reports that the edges in group-p belong to the writers having much higher topic uncertainty. On the other hand, both groups show similar topic uncertainty in terms of reader-side topic entropy 10 . Thus, the reader-side popu-larity (activeness) does not seem to be useful in the correct path/topic-labeling. This finding also explains why polya-r and 2path-r produced results with higher perplexity than their counterparts. However, we see the opposite results in the weight model experiments. The writer-side model ( wlda-w ) performs worse than the reader-side model ( wlda-r ). We discuss more about this anomaly in the next section.
To explain the reason why the anomaly explained in the
We used a reduced 1M-edge dataset to calculate entropy due to the limited memory size of our machine.
We also observe that the reader-side entropy is much higher than the writer-side entropy. previous section happens in the results from the two-path model and the weight model , we devise a very simple social network edge-labeling example. The upper section of Fig-ure 4 illustrates following edges among two readers and three writers. We assume that, among the four following edges, two of them are labeled with topic z 1 (darker one), and the other two are labeled with topic z 2 (lighter one). We also assume that edges have different weights and denote an edge with a weight 1 as a narrow edge and an edge with a weight 2 as a thick edge. Thus, the bi-partite graph in the left shows the standard LDA and the one in the center shows a wlda-r case where edges from a reader r 2 have higher weights. The bi-partite graph in the right shows a wlda-w case where edges to a writer w 2 have higher weights. Each pair of tables in the middle section show a pair of count matrices (reader-topic and topic-writer) for each bi-partite graph. Each value in a count matrix is a weight assigned to each edge. Now, let X  X  think about the case we want to label a new edge from the reader r 2 to the writer w 2 (the dotted arrow) 11 . Each bot-tom section shows the probability of labeling the new edge with topic z 1 or z 2 for each case according to Equation (7) . Unlike the standard LDA in the left, the probabilities between topic z 1 and z 2 are different in the dotted boxes in the bottom section. Interestingly, when we assign differ-ent weights to edges from different  X  X eaders X  ( wlda-r in the center) we find that the right part of the topic-labeling prob-ability, which corresponds to the  X  X riter X  distribution for a topic ( p ( w | z )), changes instead of the topic distribution for a reader ( p ( z | r )), and vice versa. In the topic-labeling for-mula given in Equation (7), the numerator in the right part is a sum of weights of the edges from many readers to a cer-tain writer. Thus, if the weights of the edges from a reader are changed, topics associated with those edges get different association probability. However, since the numerator in the left part is a sum of weights of edges from a reader, even though those weights are changed, the sum remains the same for all topics. This aspect explains why wlda-r performs bet-ter than wlda-w . While wlda-w affects the topic distribution for a reader ( p ( z | r )), wlda-r changes the writer distribution for a topic ( p ( w | z )), which is related to writer X  X  popularity distribution.

Though the weight model produces results with higher per-plexity, it has the same computational complexity as the standard LDA since it does not introduce a new hidden variable. Also, we may use various weighting schemes ac-cording to the nature of application domains. Though we only reported the result from the best weighting scheme in this paper, there were many candidate weighting schemes producing results with competitive perplexity values.
In this paper, we proposed topic models appropriate to analyze social network graphs. Different from a textual dataset, a popular user has very important meaning in a social network dataset and should be carefully handled. We started with the simple model which introduces the concept of the popularity component and explored various ways to effectively incorporate it in probabilistic topic models.
In extensive experiments with a real-world Twitter dataset, our models achieved significant improvements in terms of
We allow multiple edges in this example to make it simple.
For simplicity, we ignored  X  ( a,b ) and priors (  X  and  X  ). lowering perplexity . Particulary, our two-path model showed 9 . 41% lower perplexity than that of LDA. Given that a 3 . 27% lower perplexity led to 64% higher human-perceived clustering quality in our previous work [7], we believe that our two-path model can also significantly improve the clus-tering quality. With the two-path model , we also showed that the reader-side popularity (activeness) is not effective in judging the reader X  X  topic sensitivity. We extended the two-path model into the weight model and explained why the latter behaves differently from what we have expected. The weight model is very flexible in selecting its weighting schemes and does not increase the complexity of LDA.
Since the popularity bias is universal in various datasets including webpage visit logs, advertisement click logs, and product purchase logs, our models can effectively provide more relevant recommendations in many web services. We would like to thank Christopher Moghbel, and Sunghoon Ivan Lee for their help and feedback throughout this re-search. We are also very grateful for valuable comments from the anonymous reviewers. [1] Multinomial distribution. http://en.wikipedia.org/ [2] A. Ahmed, Y. Low, M. Aly, V. Josifovski, and A. J. [3] E. M. Airoldi, D. M. Blei, S. E. Fienberg, and E. P. [4] D. Andrzejewski and X. Zhu. Latent dirichlet [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [6] S. Brin and L. Page. The anatomy of a large-scale [7] Y. Cha and J. Cho. Social-network analsys using topic [8] J. Chang and D. Blei. Relational topic models for [9] D. Cohn and H. Chang. Learning to probabilistically [10] D. Cohn and T. Hofmann. The missing link -a [11] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. [12] L. Dietz, S. Bickel, and T. Scheffer. Unsupervised [13] E. Erosheva, S. Fienberg, and J. Lafferty. Mixed [14] M. Girolami and A. Kaban. On an equivalence [15] A. Gruber, M. Rosen-Zvi, and Y. Weiss. Latent topic [16] J. Hannon, M. Bennett, and B. Smyth.
 [17] M. Harvey, I. Ruthven, and M. J. Carman. Improving [18] K. Henderson and T. Eliassi-Rad. Applying latent [19] M. D. Hoffman, D. M. Blei, and F. Bach. Online [20] T. Hofmann. Probabilistic latent semantic indexing. In [21] Y. Hu, Y. Koren, and C. Volinsky. Collaborative [22] T. Iwata, T. Yamada, and N. Ueda. Modeling social [23] A. Java, X. Song, T. Finin, and B. L. Tseng. Why we [24] A. Mccallum, X. Wang, and A. Corrada-Emmanuel. [25] Q. Mei, D. Cai, D. Zhang, and C. Zhai. Topic [26] R. Nallapati, D. A. McFarland, and C. D. Manning. [27] R. M. Nallapati, A. Ahmed, E. P. Xing, and W. W. [28] R. Pan, Y. Zhou, B. Cao, N. N. Liu, R. Lukose, [29] N. Pathak, C. DeLong, A. Banerjee, and K. Erickson. [30] M. Pennacchiotti and S. Gurumurthy. Investigating [31] J. B. Schafer, J. Konstan, and J. Riedi. Recommender [32] H. Steck. Item popularity and recommendation [33] M. Steyvers and T. L. Griffiths. Probabilistic topic [34] M. Steyvers, P. Smyth, M. Rosen-Zvi, and [35] C. Sun, B. Gao, Z. Cao, and H. Li. Htm: a topic [36] V. Tuulos and H. Tirri. Combining topic models and [37] H. Wallach, D. Mimno, and A. McCallum. Rethinking [38] X. Wang, N. Mohanty, and A. Mccallum. Group and [39] A. T. Wilson and P. A. Chew. Term weighting [40] H. Zhang, B. Qiu, C. L. Giles, H. C. Foley, and [41] D. Zhou, E. Manavoglu, J. Li, C. L. Giles, and
