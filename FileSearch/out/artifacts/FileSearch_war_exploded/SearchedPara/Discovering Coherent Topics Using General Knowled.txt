 Topic model s have been widely used to discover latent topics in text documents. However, they may produce topics that are not interpretable for an application. Researchers have proposed to incorporate prior domain knowledge into topic model s to help produce coherent topics . The knowledge used in existi ng models is typically domain dependent and assumed to be correct. Howe v-er, one key weakness of th is knowledge -based approach is that it requires the user to know the domain very well and to be able to provide knowledge suitable for the domain, which is no t always the case because in most real -life applications , the user wants to find what they do not know . In this paper, we propose a fram e-work to leverage the general knowledge in topic models. Such knowledge is domain independen t. Specifically, w e use one form of general knowledge, i.e., lexical semantic relations of words such as synonyms , antonyms and adjective attributes , to help pro-duce more coherent topics . However, there is a major obstacle, i.e., a word can have multiple meanings /senses and each meaning often has a different set of synonyms and antonyms. N ot every meaning is suitable or correct for a domain. Wrong knowledge can result in poor quality topics . To deal with wrong knowledge , we propose a new model, called GK -LDA , which is able to effe c-tively exploit the knowledge of lexical relations in dictionaries. To the best of our knowledge, GK -LDA is the first such model that can incorporate the domain independent knowledge . Our exper i-ments using online product reviews show that GK-LDA performs significantly better than existing state -of -the -art models. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing.
 Topic Models ; General Knowledge; Lexical Relations Statistical topic models , such as pLSA [15] and LDA [5] , provide a powerful framework for extracting latent topics in text doc u-ments. However, researchers have found that these unsupervised models often produce topics that are not interpretable or meanin g-ful [30] . One key reason is that the objective functions of these models do not always correlate well with human judgments [9] . In order to deal with this problem, several knowledge -based mo d-els [1, 2, 17, 18, 31, 35] have been proposed. These models incor-porate expert domain knowledge to guide the model s. For exa m-ple, the DF -LDA model [1] takes domain knowledge in the form of must -link s and cannot -link s. A must -link states that two words should belong to the same topic, while a cannot -link states that two words should not be in the same topic. In [8, 18, 25 , 31] , sev-eral seeded models were also proposed to allow the user to pr o-vide some prior seed words in some topics. The prior knowledge used in these existing models is typically domain dependent . F or different applications, different knowledge is needed . These mo d-els also assume that the prior knowledge is correct.
 These existing knowledge -based models have a key weakness to be applied in real life data mining application s, i.e., they all as-sume that the user knows the domain very well and can provide knowledge suitable for the domain, which is not always the case because in many real life data mining applications, the user wants to discover something unknown. The knowledge provided by the user usually needs to be repeatedly tuned in order to fit the d o-main (i.e., domain dependent knowledge ) and improve the model. In this paper, we propose to take a different approach. We believe that there is a vast amount of lexical knowledge about words and their relationships available in online dictionaries or other re-sources that can be exploited in a model to generate more coherent topics. Such knowledge is domain independent and can be easily extracted automatically from online dictionaries to form a general knowledge base. This knowledge base can serve as an integral part of a topic model system as it does not change from domain to domain and can be applied to any domain without any user in-volvement . To the best of our knowledge, this approach has not been taken before.
 In this work , we u tilize a specific type of lexical knowledge, i.e., lexical semantic relations , which are relations about words. Such relations include s ynonymy , antonym, h yponymy , taxonomy, meronymy, troponymy, adjective -attribute, etc. In order not to be too ambitious, in this first study we only use synonym, antonym and adjective-attribute relations to demonstrate the benefits of these relations to topic models 1 . Here adjective-attribute relation means an adjective and its attribute nouns that the adjective d e-scribes. For example, the adjective expensive usually describes the attribute price of an object. We believe that these lexical relations are beneficial to topic model s in the sense that words in such rel a-tions are likely to belong to the same topic.
 However, there is a major challenge in using lexical relations , i.e., many such relations may not be appropriate for a particular appl i-cation because a word can have multiple meanings/senses . Each meaning/sense can have a different synonym set, a different ant o-nym set and a different adjective-attribute set. For a particular application, typically only one or two meaning s are applicable while the other senses are inappropriate. To make matters worse, even in the same sense, some words in the synonym set (also called synset ) may be incorrect for a particular domain. For exam-ple, the word picture has 10 senses as a noun in WordNet [29] . The synset for the first sense is { picture , image , icon , ikon }. In the domain of digital cameras, picture and image should belong to the same topic, but icon and ikon should not share the same topic with picture and image . In the second sense of picture , the synset is { picture , painting }. These two words are not coherently related for cameras. The situation also exists in the other two lexical rel a-tions. To deal with it , we need a model that is able to automatical-ly identify and leverage the right relations for a particular domain. Before going further, we first describe how the lexical relations are represented i n this work . W e represent them as sets, e.g., syn-onyms: {expensive, pricey}, antonyms: {expensive, cheap}, and adjective-attribute s: {expensive, price}. In our system, synonyms and antonyms are extracted from WordNet [29] . Adjective -attribute relations are obtained from the system in [11] , which identifies such relations from online dictionaries. To simplify the presentation, we call th ese sets LR -sets (for lexical relation sets) . Each LR -set indicate s one sense /meaning of the words inside it. Since the WordNet [29] and the system in [11] can provide co n-sistent sense IDs of each word, the synonyms , antonyms and a d-jective -attribute s of the same sense for each word are automatical-ly merged to form an LR -set . For example, for the word expensive in the above example, an LR -set {expensive, pricey, cheap, price } is automatically generated indicating one sense of this word . W e will see in the Section 4 that LR -sets help improve resulting topics dramatically without any user involvement . In [10] , we proposed the MDK -LDA model to exploit the knowledge from multiple domains . However, MDK -LDA did not explicitly deal with wrong knowledge , especially the unfavorable incorrect semantic relationship s in a particular domain . The knowledge used in [10] is called s -set which has the same stru c-ture as LR -set in this work. The major difference between s -set and LR -set is that s-sets were manually validated to ensure the quality of knowledge for each domain while the general knowledge in forms of LR -sets is domain independent and aut o-matically extracted from the sources in [11, 29] . Thus, LR -sets contain much more noise, demanding additional mechanism s to explicitly deal with it. This paper proposes a comprehensive framework to incorporate the general (domain independent) knowledge with a new model called GK -LDA (short for General Knowledge based LDA ). Before discussing the proposed framework , we first review the two models of MDK-LDA, i.e., MDK -LDA(b) and MDK -LDA, which are the base of the proposed framework. In MDK-LDA(b), a new latent variable  X  is added into LDA to enable the model to choose a right LR -set ([10] used s -set ) for each word. Each doc u-ment is an admixture of latent topics while each topic is a prob a-bility distribution over LR -sets. MDK-LDA(b) suffers from the issue called adverse effect of knowledge [10] , which also exists in other existing knowledge -based models. According to this issue , the frequent words may suffer from the attenuation of their proba-bilities when putting together with infrequent words as a piece of knowledge , resulting in uninterpretable output topics. To solve it, MDK-LDA employ s the generalized P X  lya urn (GPU) mo del [27] to promote LR -set as a whole. In MDK-LDA , drawing word  X  will no t only increase the probability of seeing  X  , but also i n-crease the probability of seeing word s shar ing an LR -set with  X  . However, MDK-LDA is insufficient in terms of general k nowledge represented by LR -sets due to its indiscrimination on each word in the LR -set. As mentioned before, some senses of a word may not be appropriate in a particular domain, leading to completely or partially incorrect LR -sets. Before applying LR -sets in topic models, we want to estimat e the correlation between the domain corpus and LR -sets to have some ideas of the quality of LR -sets . If the domain corpus can validate an LR -set, we then can have a higher confidence in the usefulness of this knowledge, and hence trust it more. Based on this idea, we propose a matrix called word correlation matrix which estimates the quality of an LR -set by validating the co -occurrences of the words in the LR -set in the corpus (represented by word probabilit ies under topics in LDA). In more details , the original LDA (without any knowledge) is executed on the corpus at first. The resulting word distributions under output topics of LDA are then used to estimate the correct-ness of the relationships in the LR -sets in order to reduce the u n-desirable effe cts of them . The intuition is that the words in an LR -set may be less likely to be in the same topic if they have very different probability masses (too far away from each other in the order of word probabilities) under LDA X  X  output topics. With this estim ation of LR -sets , we propose the GK -LDA model by e m-ploying a new GPU model with a new Gibbs sampler which can use the proposed word correlation matrix to discriminate the word s in each LR -set. In G K-LDA, drawing word  X  will not only promote the LR -set as a whole, but also discriminately promote each of the correlated words according to the word correlation matrix . Our experimental evaluation was conducted using online reviews from four domains. Our results show that GK -LDA outperforms several state -of -the -art baseline models by a large margin. In summary, this paper makes the following contributions: 1. It proposes the idea of exploiting the general knowledge of 2. It proposes a novel model called GK -LDA . We believe that 3. A comprehensive evaluation has been conducted to compare Topic models , such as p LSA [15] and LDA [5] , have been su c-cessfully applied to many applications, e.g. , sentiment analysis [41] and analysis of discussion threads [24] . However, these methods do not use any prior knowledge or external resources . In recent years, researcher s h ave proposed to incorporate external resources to guide the modeling process. These resources include temporal information [16] , social network [28] , citation resources [20] , search results [33] , bilingual resources [42] , etc. But these resources are mainly for some specific tasks and not widely avai l-able. Some other researchers also tried to leverage domain d e-pendent knowledge into topic models [1, 2, 8, 13, 17, 18, 25, 31, 35] , which typically assume the knowledge is correct and specific to a particular domain . The key weakness of this knowledge -based approach has been discussed in Section 1. A representative model in this regard is DF -LDA [1] , which allows the user to set must -link s and cannot -link s (see Section 1 ). DF -LDA also assumes that must -link s and cannot -link s are correct. Further, the definition of must -link is transitiv e, i.e., if A and B form a must -link, and B and C form a must -link, it implies a must -link between A and C, indi-cating A, B and C should be in the same topic. This definition can link unrelated words together due to multiple senses , giving poor results in the case of general knowledge . In our work , several LR -sets may share words but they do not have to be transitive. In [2] , the authors extended the work to allow more general knowledge in the form of first -order logic. Along the same line, several seed-ed models [8, 18, 25, 31] have been proposed. A seed set, which is a set of seed words for a topic , can also be expressed as must -links. Like D F-LDA, they also assume the user knowledge is correct . Some other related works include LDAWN [6] , which incorp o-rates WORDNET -WALK in modeling. Their synsets are similar to LR -sets . However, their task is word sense disambiguation . LDAWN assumes that a word is generated by a WordNet -Walk in WordNet [29] , while in our case, a word is gen erated by its latent topic and LR -set , where each topic is a multinomial distribution over LR -sets. One interesting result they found is that idiosyncr a-sies in the hierarchical structure of WordNet can harm perf or-mance [23] , which is consistent with our intuition that simply incorporating general lexical knowledge can degrade the perfo r-mance of the model if we do not have a mechanism to deal with wrong knowledge. [35] employ s the multi-langua ge topic sy n-chronization. The dictionary information is used to bias corr e-sponding words towards similar topics . However, it also assumes that the knowledge is correct and domain specific. It again cannot handle wrong knowledge. [4, 36] use document label information in a supervised setting . Our framework does not use supervision. In [26] , a semi -supervised model was proposed, wh ich is very different from our work as they use expert reviews to guide the analysis of user reviews. The model in [17] also enables the user to provide knowledge interactively during the modeling process. The generalized P X lya urn (GPU) model [27] was first utilized in LDA to generate coherent topics in [30] . However, [30] did not use any external resources or knowledge . In addition, its objective of applying GPU model is different in the sense that [30] use s it to smooth the probabilities of words with co-document frequencies while our objective is to promote the LR -sets and the correlated words . As we will see in Section 4 , its results are inferior to GK -LDA model . The GPU model also does not deal with wrong knowledge . Our proposed GK -LDA model, to the best of our knowledge, is the first model to exploit general knowledge (which is domain independent) and deal with wrong knowledge explicitly . In this section, we detail lexical semantic relations (LSR), and their representation s. LSRs are relations about words. There are many types of LSRs, e.g., s ynonymy , antonym, h yponymy , ta x-onomy, adjective -attribute , and others [3] . As we noted earlier, we only use s ynonymy , antonym and adjective-attribute relations in this paper to investigate the benefits of leveraging such knowledge in topic models.
 Syn onymy : Two expressions a and b of a language are synonyms iff they mean exactly or nearly the same. The notion is typically applied to lexical items, including idioms, but it can be used for larger expressions as well. In this work, we only use the word level synonyms, e.g., expensive and pric ey . Antonym : Two expressions a and b of a language are antonyms iff they have opposite meanings. Again, the notion can be words or larger expressions. In this work, we only use the word level antonyms, e.g., expensive and cheap . Adjective-attribute : An adjective is a word that modifies nouns and pronouns, primarily by describing a particular qual i-ty/attribute of the word it is modifying . Although there are some general adjectives which can describe/modify anything, e.g., good and bad , most a djectives describe some specific at-tributes or properties of nouns. For example, expensive usually describes price , and beautiful often describes appearance . As we described earlier, the LSRs are represented as LR -sets where each LR -set is automatically generated by merging syn o-nyms, antonyms and adjective-attribute s of the same word sense from the sources of [11, 29] . We first introduce the basic model MDK-LDA (b). A new latent variable  X  , which denotes the LR -set assignment to each word , is added into LDA. The generative process is given by : 1. For each topic  X   X  { 1, ... ,  X  } 2. For each document  X  X  X  {1, ... ,  X  } where  X  is the number of documents and each document  X  has  X   X  words. The vocabulary in the corpus is denoted by {1, ... ,  X  } and the number of LR -sets is  X  . We follow [10] to set the smoot h-ing h yperparameter  X  as the exponential function to control the density of Dirichlet distribution : In order to avoid the attenuation of the probability masses of the frequent words in MDK -LDA (b), MDK-LDA employ s the gene r-alized P X lya urn (GPU) model [27, 30] to promote an LR -set as a whole . In P X lya urn model s, objects of interest are represented as colored balls in an urn. In a simple P X lya urn model , w hen a ball of a particular color is drawn, that ball is replaced with two balls of the same color in the urn. In contrast, in the GPU model, when a ball is drawn, that ball is put back along with a certain number of balls of similar colors . More details can be found in [10] . In order to promote an LR -set upon observing any of its word, if a ball of color  X  is drawn,  X   X  ,  X   X  ,  X  balls of each color  X  X  X  X  {1, ... ,  X  } are put back where  X  and  X  X  share LR -set  X  .  X   X  ,  X   X  ,  X  Since our LR -sets are general knowledge from online dictio naries, some LR -sets do not make sense in a particular domain. For e x-ample, {card, bill} is a correct LR -set in the domain  X  X estau rant X , but unsuitable in the domain  X  X amera.  X  In this section, we pr o-pose the GK -LDA model to deal with wrong LR -sets, which can be applied in any a pplication domain. Note that GK -L DA still uses the graphical model of MDK -LDA. MDK-LDA(b) and MDK -LDA are able to deal with some inco r-rect LR -sets owning to their ability to choos e the LR -set with the right word sense in the modeling . However, there are two major issues that these two models are unable to deal with. 1. One issue is that there may be no correct LR -sets for a word in 2. The other issue is that an LR -set may be partially correct and To solve these two issues, we propose the GK -LDA model . First, we propose a word correlation mat rix  X  to estimate the correct-ness of the LR -sets using the given corpus (Section 3. 3.2 ). Using this matrix, for each word  X  , we relax the constraints of all wrong LR -sets (the first issue above) by adding a singleton LR -set {  X  } (Section 3. 3.3). To deal with the second issue, we scale the matrix  X  into a matrix  X   X  , which fits in the new GPU model in GK -LDA (Section 3. 3.4 ). This new matrix  X   X  is used to design a new Gibbs sampler for the GK -LDA model (Section 3.4 ). Given a piece of knowledge (LR -set) itself , we may not have any idea whether it is correct or not. However, given a corpus, it is possible to validate the LR -set through the corpus. If an LR -set has a reasonable support in the corpus, we will have some con fi-dence in its usefulness in the domain represented by the corpus, and consequently we give it a higher weight for promotion. Since the topics found by LDA are a reasonable summary of the corpus and the top words (with high probabilities) under each topic are more likely to share some semantic similarity, we use the topic -word distribution from LDA to estimate word correlations in each LR -set in a domain. The idea is that if two words in an LR -set are too far from each other (i.e., have very different probabilities) under the topics of LDA , they are more likely to have different semantic meanings, i.e., less correlated.
 Figure 1 gives a detailed algorithm for computing word correl a-tion matrix  X  . Intuitively, top words, with higher probabilities under a topi c, are more likely to represent the semantic concept of the topic while words with low probabilities contribute much less to the semantic concept . To compute the correlation of two words, we focus on the topics where the words have high probabilities. The algorithm in Figure 1 computes for all the word pairs (  X  ,  X  X  ) in each LR -set (lines 1 and 2). The word distribution under topic  X  is denoted by  X   X  in LDA. For those pairs not in any LR -set, their correlation is 0 (not shown in the algorithm) and we do not need to validate them. Lines 3 and 4 find the topics that the two words  X  and  X  X  have the maximum probabilities respectively. Lines 5 and 6 enforce that word  X  has a lower (or equal) maximum pro b-ability than  X  X  , restricting their ratio not larger than 1. Line 7 finds the topic that word  X   X  has the maximum probability, and the ratio of probabilities of both words under this topic is estimated as the corr elation (Line 8). The idea is that the ratio of word probabilities under this topic is a good indicator of the semantic correlation of the two words. Although this word correlation estimat ion may not be perfect due to the imperfect topic -word distribution s from LDA, our experiments show that it is effective in solving the two issues discussed in Section 3.3 .1. In order to solve issue 1 mentioned in Section 3.3.1 , we need to design a function to estimate the quality of LR -set  X  toward the word  X  . Since the quality of  X  depends on the correlations of words inside it with  X  , we can estimate the quality of LR -set  X  towards  X  based on the word correlation matrix  X  as follows : Basically, the quality function of LR -set  X  towards word  X  is the maximum correlation between any word  X  X  (  X  X  X  X  X  and  X  and  X  based on  X  . This quality function can give us some hints as to which LR -sets are more likely to be correct or incorrect. We set a threshold  X  (discussed in Secti on 4) such that if the quality of an LR -set towards each word inside it is less than  X  , this LR -set is estimated to be wrong (or low -quality) in the domain. Following the issue 1, if all LR -sets of word  X  are estimated to be wrong, we need to add an alternative LR -set to give the model a right LR-set relax the LR -set constraint. If  X  has any LR -set with its quality value greater than  X  , the singleton set {  X  } is not added. This pr e-processing ensures that the model can have at least one reasonable LR -set to assign to each word. However, note that, the estimated wrong LR -sets are not removed because the estimation above on topics generated by LDA may not be perfect. We now deal with issue 2 discussed in Section 3.3 .1, i.e., the pa r-tial incorrect LR -sets. In MDK-LDA , when a word is drawn, all Algorithm Computing Word Correlation Matrix  X  1 For each LR -set  X   X  { 1, ... ,  X  } 2 For each pair (  X  ,  X   X  )  X  X  X  5 If  X   X  X  X  X  (  X  ) &gt;  X   X  X  X  X  (  X   X  ) then 6 Exchange  X  and  X  X  ; 9 Return  X  ; other words inside the LR -set will be put back equally according to matrix  X  in equation 2 , which promotes the L R-set as a whole , i.e., promot ing every word inside it. Now we want to use the word correlation values of  X  to help determine the number of balls to put back which reduces the un desirable effect s of wrong relatio n-ships in an LR -set. For this purpose, we scale  X  to  X  which will be incorporated in the GK -LDA model. The coefficient  X  governs the scale of correlation corresponding to the hyperparameters  X  and  X  in the model . The value of  X  will be discussed in Section 4 . With the matrix  X   X  , we can design a new GPU model, i.e., drawing word  X  will not only increase the pro b-ability of seeing  X  , but also discriminatively increase the prob a-bility of seeing every correlated word with  X  represented by  X  Following the example of LR -set  X  = {picture, pic, flick} in Se c-tion 3.3 .1, since picture and pic are semantically related in the domain  X  X amera X  (in other words, the relationship picture -pic is correct), they tend to have reasonable high co -occurrence in the corpus and hence LDA is likely to put them together under the same topic. On the other hand, flick is semantically different from both picture and pic , and thus LDA may put flick under a different  X  LDA, seeing the word picture and pic will promote each other a lot, but promote the word flick very little, which is consistent with our a im to merge the semantically related words while separating semantically different words. In this section, we introduce a new Gibbs sampler for the GK -LDA model . The Gibbs samplers for MDK -LDA(b) and MDK -LDA can be found in [10] . In topic models, collapsed Gibbs sampling [12] , one of Markov Chain Monte Carlo (MCMC) methods [37] , is a standard proc e-dure for obtaining a Markov chain over the latent variables in the model. In GK -LDA, the latent variables (i.e., latent topic  X  and latent LR -set  X  ) are j ointly sample d, which gives us a blocked Gibbs sampler. An alternative way is to perform hierarchical sam-pling (sample  X  and then  X  ). However, [38] argues that when the latent variables are highly related, blocked samplers improve co n-vergence of the Markov chain and also reduce autoc orrelation. As in Section 3.3, the GK -LDA model employs the GPU model to promote each correlated word represented by the matrix  X  ever, t he GPU model is nonexchangeable, meanin g that the joint probability of the words in any given topic is not invariant to the permutation of those words. Inference of  X  and  X  can be comput a-tionally expensive due to the non-exchangeability of words. We take the approach of [30] which approximates the true Gibbs sa m-pling distribution by treating each w ord as if it were the last . When sampling a word  X  , we first promote the LR -set of it as a whole. Then, each word in the LR -set is promoted based on its word correlation with  X  according to  X   X  . The idea is that if a word  X   X  is more correlated with  X  , it should be promoted more when  X  is seen, pushing them into the same topic. Note that promotion in the GPU model is achieved by putting back balls of the corre-sponding colors into the urn. Denoting the random variable {  X  ,  X  ,  X  } by singular subscripts {  X   X  ,  X   X  ,  X  variable corresponding to each word in each document in the cor-pus, the conditional probability to assign a topic  X  and an LR -set  X  (containing the word  X   X  ) to the word  X   X  is given by: where  X   X  X  X  denotes the count excluding current assignment of  X  and  X   X  , i.e.,  X   X  X  X  and  X   X  X  X  .  X   X  ,  X  denotes the number of occurrences that topic  X  was assigned to word tokens in document  X  .  X  notes the count that LR -set  X  occurs under topic  X  .  X  the number of times that word  X  appears in LR -set  X  under topic  X  . In order to illustrate the differences between LDA, MDK-LDA(b), MDK-LDA, and GK -LDA , we create an artificial corpus (Figure 2) and two LR -sets: {a, b, c} and {x, y}. Each letter in the corpus represents a word token. The hyper parameters  X  and  X  are set to the value suggested in [12] . For other parameters, we empirically set  X  = 2000 ,  X  = 0.2 and  X  = 1 for the purpose of demonstra t-ing the mechanism s of our framework . We run collapsed Gibbs sampling 1000 iterations with 2 topics in total.
 Sample from Markov chain : In Figure 2 , we show the final topic assignment to each word (in superscript) based on the final Ma r-kov chain status for LDA (Figure 2(1)), MDK-LDA(b) (Figure 2(2)), MD K-LDA (Figure 2(3)), and GK -LDA (Figure 2(4)). It is clear that since LDA cannot use any external knowledge , it splits a, b and c into different topics. Additionally, it assigns topic 1 to y in line 5, which is different from other y  X  X  (and x  X  X ). On the other hand, our framework can leverage the information from LR -sets . MDK-LDA(b) drags b in line 4 to the same topic as a and c (i.e., topic 1) , as well as corrects the topic assignment of y in line 5. Both MDK -LDA and GK -LDA successfully groups all of {a, b, c} into one topic and all of {x, y} into the other topic (in red solid and blue dashed rectangles ). Topic -word distribution : Table 1 shows the top 7 words under each topic ranked by their probabilities (round to 3 decimal plac-es). The following interesting observations explain the effectiv e-ness of our framework : 1. Since LDA cannot exploit knowledge, b has high probability 2. For MDK-LDA(b), we find that a, b and c are close to each 3. MDK-LDA solves the above problem by promoting the LR -4. MDK-LDA(b) and MDK -LDA follows the knowledge assu m-The example in this section shows that our framework can effe c-tively use the knowledge. However, it is hard to show wrong knowledge with such a small corpus. O ur experiments with real -life datasets in the next section will show the benefits of GK -LDA , whic h improves MDK -LDA . We now evaluate the proposed GK -LDA model , as well as MDK-LDA(b) and MDK-LDA , and compare them with three baseline models : LDA [5] , LDA with GPU (denoted as LDA -GPU ) [30] and DF -LDA [1] . LDA is the basic knowledge -free unsupervised topic model. LDA -GPU applied GPU in LDA using co-document frequency. DF -LDA is perhaps the most well -known knowledge -based model which introduced must -links and cannot -links. It is also a natural fit for our propose d model as a must -link and an LR -set share the similar notion , i.e., they both aim at constrain ing the words in them to appear under the same topic. Note that exis t-ing models typically assume that the knowledge is correct and to our knowledge there is no prior work in topic modeling that can deal with wrong knowledg e explicitly . Our proposed GK -LDA model can deal with wrong knowledge. We will see in Sections 4.2 and 4 .3 that this capability of GK -LDA results in far better results than existing state -of -the -art models.
 In Section 4 .1, we describe the datasets and experimental set tings. In Section 4.2, we evaluate our framework objectively using the Topic Coherence metric [30] and KL -Divergence. Further , in Section 4.3.1, we report the human evaluation results by working with t wo judges who are familiar with the Amazon products and reviews. Last, we show the qualitative results with some example topics from different models in Section 4. 3.2 . Datasets : Since LR -sets and the proposed framework are domain independent mechanisms for finding topics from text collections, we use multiple datasets from different domains of online review s for our evaluation. We collected reviews from four domains from Amazon.com . Each domain collection (or corpus) contains 500 reviews . The statistics of each domain are shown in Table 2 (col-umns 2, 3, and 4 ). The four domains are  X  Camera &amp; Photo X ,  X  X ell Phones &amp; Accessories X ,  X  Gourmet Food &amp; Grocery X , and  X  Co m-puters &amp; Accessories X . For easy presentation, we simply use  X  X amera X ,  X  X ell Phone X ,  X  X ood X  , and  X  X omputer X  to denote the four domain corpora respectively. We have made the datasets publically available at the website of the first author.
 Pre-processing : W e ran the Stanford Parser 2 to perform sentence detection, lem matization , and POS tagging. Then, punctuations, stop words 3 , numbers and words appearing less than 5 times in each corpus were removed. For each domain, the domain name was also removed as it appears very frequent ly and co-occurs with most words in the corpus, leading to high similarity among topics. Our LR -sets depend on POS tags of words. In this work, we only use nouns and adjectives to produce LR -sets since they are the main parts of the topics. Verbs have a high level of noise. We plan to consider v erbs based LR -sets in our future work. The number of LR -sets (having at least two words ) and the number of estima t-ed wrong LR -sets (having at least two words) by GK -LDA (in The corpus is (1) without superscripts. Each superscript is the topic ID that is assigned to each word by each model.
 Section 3.3.3 ) in each domain are given in Table 2 (column s 5 and 6). Note that dup licate LR -sets have been removed.
 Sentences as documents : As pointed out in [41] , when standard topic models are applied to revi ew s, they tend to produce topics that correspond to global properties of product, which make topics overlapping with each other. Since applying topic models to re-views mainly aims to find different aspects or features (as topics) of products [19, 31, 41, 43] , using individual reviews for mode l-ing is not very effective [41] . Alth ough there are models dealing with sentences in complex ways [19, 41] , we t ake the approach in [7] , d ivid ing each review into sentences and treat ing each sen-tence as a document . Sentences can be used by all three baselines without any change to their models. Although the relationship between sentences of a review is lost, the data is fair to all models . Parameter settings: For all models, posterior inference was drawn after 1000 Gibbs iterations with a n initial burn -in of 100 iterations. For all models, we set  X  = 1 and  X  = 0.1 . We found that small changes of  X  and  X  did not affect the results much , which was also reported i n [19] who also used online reviews. For the number of topics, we tried different values (see Section 4.2. 1) . Note that it is difficult to know the exact number of topics. While non -parametric Bayesian approaches [40] aim to e stimate the number of topics from the corpus, they are often sensitive to hy-perparameters [14] . In this work , the heuristic value s obtained from our experi ments produced good results.
 For DF -LDA, we follow ed the definition of must -link to generate must -links from LR -sets. LR -sets don X  X  contain cannot -link knowledge. Note that the generated must -links contain wrong knowledge due to the issue of multiple senses, which degrades the performance of DF -LDA as we will see in Sections 4 .2 and 4.3. We then ran DF -LDA (implementation downloaded from its a u-thors X  website) while keeping the parameters as proposed in [1] (we also experimented with different parameter settings but they did not produce better results ). For our framework , we empirically set  X  = 2000 . For the threshold of  X  , in GK -LDA , we estimated it using some labeled LR -sets in a development corpus,  X  X atch X , which was not used in the evaluation ( differ ent from the four do-mains in Table 2). Based on the label ed data, we empirically chose the threshold  X  = 0. 07 , meaning that if the quality of LR -set is lower than this value , we will add a singleton set as d e-scribed in Section 3.3.3 . We then averaged the word correlation values (Figure 1) of word pairs in the estimated correct LR -set s to set  X  = 0.2 and  X  = 2 in equations 2 and 4. Although these three parameters come from the domain  X  Watch  X , we use them for all four domain s in Table 2 . A nother approach is to automatically determin e the threshold  X  to distinguish correct and incorrect LR -sets . W e defer this to our future work. In this section , we evaluate our framework objective ly. Topic models are often evaluated using perplexity on held-out test data. However, the perplexity measure does not reflect the semantic coherence of individual topics learned by a topic model [34] . Re-cent research has shown po tential issues with perplexity as a measure: [9] suggest ed that human judg ments can sometimes be contrary to the perplexity measure. Also , perplexity does not real-ly reflect our goal of finding coherent topics with accurate seman-tic clustering. It only p rovides a measure of how well the model fits the data. Thus, we choose two evaluation metrics, Topic C o-herence and KL -Divergence, which directly evaluate our fram e-work on topic interpretability and topic distinctiveness [21, 32] . We also report statistical significance of improvements of our framework calculated based on paired t -test.
 Topic Coherence : The T opic Coherence metric [30] (also called UMass measure [39] ) was proposed for assessing topic quality. The metric rel ies upon word co -occurrence statistics within the documents, and does not depend on external resources or human labeling . [30] show s that topic coherence is highly consistent with human labeling . Higher T opic Coherenc e score indicate s higher quality of topic , i.e. better topic interpretability . KL -Divergence : Another important metric for topic models is topic distinctive ness [21, 32] . We want to evaluate how distinctive the discovered topics are. To measure the distinctiveness, we use KL-Divergence as in [21, 32] . Since KL -Divergence is asymmet-ric, we compute its values between all pairs of topics and average them to get the average KL -Divergence. Clearly, for more distin c-tive topic discovery and better topic quality, it is desirable to hav e larger average KL -Divergence. Since the models in our experiments are all parametric topic mo d-els, we first compare the performance of each model given differ-ent number of topics. Figure 3 shows the average T opic Cohe r-ence score and KL -Divergence (over all domains ) of each model given different number of topics. We can m ake the following observations: 1. From the Topic Coherence resu lts, given different number of 2. From the KL -Divergence results, given different number of KL -Divergence 3. Although DF -LDA has larger KL -Divergence than MDK-4. LDA -GPU does not produce as good topics as other models 5. In general, with more topics, the Topic Coherence score i n-In order to see the effect s and sensitivity to general lexical knowledge, we show the detailed Topic Coherence score of  X  = 15 in Figure 4 . Again, we can find that the GK -LDA model has the highest scores across four domains , meaning that it pro-duce s the most coherent topics. We can also see that DF -LDA performs better than LDA in the domain  X  Food  X  and  X  X omputer  X  but worse in the domain  X  Camera  X  and  X  X ellphone  X . In order to fully understand it, we investigated the knowledge in each domain . We found that the knowledge is very different in the four domains: 1. In the domains  X  Food X  and  X  X omputer  X , the knowledge is 2. On the other hand, in the domains  X  Camera X  and  X  X ellphone  X , In summary, we can conclude that our proposed framework is highly effective in producing distinctive topics where each topic is highly coherent compared to the baseline models. Note that b y no means do we say that LDA -GP U and DF -LDA are not effective. We are only saying that in our problem setting of using general lexical knowledge, these mode ls do not generate as coherent to p-ics as ours because they cannot effectively deal with wrong knowledge , which is an important issue. Since our aim is to make topics more interpretable and conform to human knowledge, we worked with two judges who are familiar with Amazon products and reviews to evaluate the models subjec-tively. Since topics from topic models are rankings based on word probability and we do not know the number of correct topical words, a natural way to evaluate these rankings is to use Precision @ n (or p@n ) which was also used in [31, 43] , where n is the rank position. We give p @ n for n = 5, 10, 15 and 20. There are two steps in human evaluation : Topic labeling and Word labeling. Topic Labeling : W e followed the instructions in [30] and asked the judges to label each topic as good or bad . Each topic was pr e-sented as a list of 20 most probable words in descending order of their probabilities under that topic. The models which generated the topics for labeling were oblivious to the judges. In general, each topic was annotated as good if it had more than half of its words coherently related to each other represent ing a semantic concept together ; otherwise bad . Table 3 (column 2) reports the Cohen X  X  Kappa score for topic labeling, which is above 0.8 ind i-cating almost perfect agreements according to scale in [22] . Word Labeling : After topic labeling, we chose the topics , which were labeled as good by both judges , as good topics . Then , we asked the two judges to label each word of the top 20 words in these good topic s. Each word was annotated as correct if it was coherently related to the concept represented by the topic ; othe r-wise incorrect . Since judges already had the concept ion of each topic in mind when they were labeling topics, labeling each word was not very difficult which explains the high Kappa score s in Table 3 (column 3) . W e can see that both annotators achieve a l-most perfect agreements (Kappa &gt; 0.8) in all p @ n . Precision @ n : Figure 5 gives the average precision @ n of all good topics over all four domains. We can make the following observations: 1. GK -LDA performs the best, improving LDA by more than 11% Figure 4. Detailed Average Topic Coherence score of  X  =  X  X  X  . Table 3 . Cohen's Kappa for pairwise inter -rater agreements .
Kappa 0.915 0.844 0.881 0.916 0.895 2. MDK-LDA improves precision of MDK-LDA (b) by about 3% 3. DF -LDA performs slightly worse than LDA with less than 1% . 4. LDA -GPU does not perform well in our data due to its use of We can see that the human evaluation results are highly consistent with Topic Coherence and KL -Divergence results in Section 4.2. Upon significance testing of improvement of GK -LDA and MDK-LDA over othe r models in Figure 5 , GK -LDA and MDK-LDA improves significantly over all other models (  X  &lt; 0. 005 ). Number of Good topics : Figure 6 shows the number of good topics discovered by each model in each domain. In general, GK -LDA can generate about 2 more good topics than LDA and more than 5 additional topics compared to DF -LDA and LDA -GPU . These are very important in practice. For DF -LDA, it discover ed fewer good topics than LDA in the domains  X  X amera X  and  X  X el l-phone  X  but more good topics than LDA in the domain  X  Comput-er X , which is consistent with the analysis in Section 4 .2.2. We also found that all topics discovered by LDA , LDA -GPU and DF -LDA can be uncovered by MDK-LDA (b), MDK-LDA and GK-LDA. Thus, our framework not only produce s additional good topics but also keep s the good topics of the baseline models . This section shows some qualitative results to give an intuitive feeling of the results from different models. There are a large number of topics that GK -LDA make s major improvements. Due to space limit ations , we can only show some example s. To further foc us, we will just show some results of LDA and GK -LDA . The results from LDA -GPU and DF -LDA were inferior and even hard to match with topics found by the other models.
 Table 4 shows 6 example topics and top ranked topical words from LDA and GK -LDA . Wrong topical words are in italic and marked red (w e tried to find the best possible match for the mod-els ). We can see that GK -LDA produces much better topics. Since the labeling of topics and topical words are somewhat subjective, we do not expect everyone to agree with the labeling, but we tried our best to have the consensus with two human judges . Clearly, the results in Table 4 do not tell all the story. We also want to highlight several important points below.  X  One of the most common and important topics in online r e-views is the price of products. However, out of the four do-mains, only in the domain  X  Food  X , LDA was able to find the topic price with a reasonable precision. In other domains , t he price related topical words were mixed with all kinds of other topics by LDA . We show one example in Table 4 (in the col-umn  X  X ellphone X  ), where the best price related topic o f LDA is still poor. We believe that LDA X  X  inability is mainly due to the fact that in English, sentences like  X  The price of this p hone is expensive . X  are relatively rare. Thus, there is probably no co-occurrence of price and expensive (or other adjectives related to price , e.g., cheap ) within a sentence. Our adjective-attribute knowledge is very effective in this case, discovering the good price topic (see  X  X K -LDA  X  in the column  X  X ellphone X ).  X  L DA tends to split one topic into multiple topics , i.e. , the top i-cal words for a semantic topic appear at the top ranked pos i-tion s of several topics. GK -LDA is much better in this regard. 
The results in Table 4 also show that.  X  There are also many other examples we could not list here due to space limitations. For example, in the food domain, GK -
LDA discovered the topic H ealthy Eati ng : protein , fat , fiber , healthy, nutrient, nutrition , v itamin , magnesium . These words are all highly coherent . The best one that LDA could find were : time , snack , food , point , healthy , calorie , weight , year . In the computer domain, GK -LDA was able to find the topic of Pr o-gram Execution : game , slow , word , fast , web , star , speed , slower . Most of the words here are highly relevant except star . LDA was unable to find any topic related.
 In summary, we can s ay that GK-LDA produces much better r e-sults, both in terms of precision and the number of good topics, which indicates that our proposed framework of exploiting ge n-eral lexical knowledge is highly promising. This paper proposed a novel task of utilizing the general knowledge of lexical semantic relations in topic models in order to produce more coherent topics. In any language, there is a vast amount of such knowledge stored in dictionaries. Since such knowledge is domain independent , it should be applicable to any application do main. However, due to multiple meanings or senses of a word, some knowledge may not be suitable for a particular application domain . This paper proposed the GK -LDA model as a comprehensive framework to effectively leverage general knowledge in topic models and to also deal with the wrong knowledge . To our knowledge, this is the first work that proposes
Figure 6. Number of good topics generated by each model . a principled model to systematically incorporate the general knowledge to produc e more coherent topic s. What is even more important is that our proposed framework can automatically deal with wrong knowledge without any user input . Given the success of topic model s in many research areas, we feel that our proposed framework for encoding general knowledge presents a promising direction to advance the current state -of -the -art in the field. This work was supported in part by a grant from National Science Foundation (NSF) under grant no. IIS -1111092, and a grant from HP Labs Innovation Research Program.
