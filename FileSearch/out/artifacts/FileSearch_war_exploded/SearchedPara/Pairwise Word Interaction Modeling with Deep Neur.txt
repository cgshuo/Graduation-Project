 Given two pieces of text, measuring their seman-tic textual similarity (STS) remains a fundamental problem in language research and lies at the core of many language processing tasks, including question answering (Lin, 2007), query ranking (Burges et al., 2005), and paraphrase generation (Xu, 2014).
Traditional NLP approaches, e.g., developing hand-crafted features, suffer from sparsity because of language ambiguity and the limited amount of annotated data available. Neural networks and dis-tributed representations can alleviate such sparsity, thus neural network-based models are widely used by recent systems for the STS problem (He et al., 2015; Tai et al., 2015; Yin and Sch  X  utze, 2015).
However, most previous neural network ap-proaches are based on sentence modeling , which first maps each input sentence into a fixed-length vector and then performs comparisons on these representations. Despite its conceptual simplic-ity, researchers have raised concerns about this ap-proach (Mooney, 2014): Will fine-grained word-level information, which is crucial for similarity measurement, get lost in the coarse-grained sen-tence representations? Is it really effective to  X  X ram X  whole sentence meanings into fixed-length vectors?
In contrast, we focus on capturing fine-grained word-level information directly. Our contribution is twofold: First, instead of using sentence modeling, we propose pairwise word interaction modeling that encourages explicit word context interactions across sentences. This is inspired by our own intuitions of how people recognize textual similarity: given two sentences sent 1 and sent 2 , a careful reader might look for corresponding semantic units, which we op-erationalize in our pairwise word interaction model-ing technique (Sec. 5). Second, based on the pair-wise word interactions, we describe a novel simi-larity focus layer which helps the model selectively identify important word interactions depending on their importance for similarity measurement. Since not all words are created equal, important words that can make more contributions deserve extra  X  X ocus X  from the model (Sec. 6).

We conducted thorough evaluations on ten test sets from three SemEval STS competitions (Agirre et al., 2012; Marelli et al., 2014; Agirre et al., 2014) and two answer selection tasks (Yang et al., 2015; Wang et al., 2007). We outperform the recent multi-perspective convolutional neural networks of He et al. (2015) and demonstrate state-of-the-art accuracy on all five tasks. In addition, we conducted ablation studies and visualized our models to show the clear benefits of modeling pairwise word interactions for similarity measurement. Feature engineering was the dominant approach in most previous work; different types of sparse fea-tures were explored and found useful. For ex-ample, n -gram overlap features at the word and character levels (Madnani et al., 2012; Wan et al., 2006), syntax features (Das and Smith, 2009; Xu et al., 2014), knowledge-based features using Word-Net (Fellbaum, 1998; Fern and Stevenson, 2008) and word-alignment features (Sultan et al., 2014).
The recent shift from sparse feature engineer-ing to neural network model engineering has sig-nificantly improved accuracy on STS datasets. Most previous work use sentence modeling with a  X  X iamese X  structure (Bromley et al., 1993). For ex-ample, Hu et al. (2014) used convolutional neural networks that combine hierarchical structures with layer-by-layer composition and pooling. Tai et al. (2015) and Zhu et al. (2015) concurrently proposed tree-structured long short-term memory networks, which recursively construct sentence representations following their syntactic trees. There are many other examples of neural network-based sentence modeling approaches for the STS problem (Yin and Sch  X  utze, 2015; Huang et al., 2013; Andrew et al., 2013; Weston et al., 2011; Socher et al., 2011; Zarrella et al., 2015).
 Sentence modeling is coarse-grained by nature. Most recently, despite still using a sentence model-ing approach, He et al. (2015) moved toward fine-grained representations by exploiting multiple per-spectives of input sentences with different types of convolution filters and pooling, generating a  X  X a-trix X  representation where rows and columns cap-ture different aspects of the sentence; comparisons over local regions of the representation are then per-formed. He et al. (2015) achieves highly competitive accuracy, suggesting the usefulness of fine-grained information. However, these multiple perspectives are obtained at the cost of increased model complex-ity, resulting in slow model training. In this work, we take a different approach by focusing directly on pairwise word interaction modeling.
 Figure 1: Our end-to-end neural network model, consisting of four major components. Figure 1 shows our end-to-end model with four ma-jor components: 1. Bidirectional Long Short-Term Memory Net-works (Bi-LSTMs) (Graves et al., 2005; Graves et al., 2006) are used for context modeling of input sentences, which serves as the basis for all follow-ing components (Sec. 4). 2. A novel pairwise word interaction modeling tech-nique encourages direct comparisons between word contexts across sentences (Sec. 5). 3. A novel similarity focus layer helps the model identify important pairwise word interactions across sentences (Sec. 6). 4. A 19 -layer deep convolutional neural network (ConvNet) converts the similarity measurement problem into a pattern recognition problem for fi-nal classification (Sec. 7).

To our best knowledge this is the first neural net-work model, a novel hybrid architecture combining Bi-LSTMs and a deep ConvNet, that uses a simi-larity focus mechanism with selective attention to important pairwise word interactions for the STS problem. Our approach only uses pretrained word embeddings, and unlike several previous neural net-work models (Yin and Sch  X  utze, 2015; Tai et al., 2015), we do not use sparse features, unsupervised model pretraining, syntactic parsers, or external re-sources like WordNet. We describe details of each component in the following sections. Different words occurring in similar semantic con-texts of respective sentences have a higher chance to contribute to the similarity measurement. We there-fore need word context modeling, which serves as a basis for all following components of this work.
LSTM (Hochreiter and Schmidhuber, 1997) is a special variant of Recurrent Neural Net-works (Williams and Zipser, 1989). It can cap-ture long-range dependencies and nonlinear dynam-ics between words, and has been successfully ap-plied to many NLP tasks (Sutskever et al., 2014; Fil-ippova et al., 2015). LSTM has a memory cell that can store information over a long history, as well as three gates that control the flow of information into and out of the memory cell. At time step t , given an input x t , previous output h t  X  1 , input gate i t , output gate o t and forget gate f t , LSTM ( x t ,h t  X  1 ) outputs the hidden state h t based on the equations below: where  X  is the logistic sigmoid activation, W  X  , LSTMs are better than RNNs for context modeling, in that their memory cells and gating mechanisms handle the vanishing gradients problem in training.
We use bidirectional LSTMs (Bi-LSTMs) for context modeling in this work. Bi-LSTMs consist of two LSTMs that run in parallel in opposite direc-tions: one (forward LSTM f ) on the input sequence and the other (backward LSTM b ) on the reverse of the sequence. At time step t , the Bi-LSTMs hidden state h bi t is a concatenation of the hidden state h for representing the neighbor contexts of input x t in the sequence. We define the unpack operation below:
Context modeling with Bi-LSTMs allows all the following components to be built over word con-texts, rather than over individual words. From our own intuitions, given two sentences in a STS task, a careful human reader might compare words and phrases across the sentences to establish semantic correspondences and from these infer sim-ilarity. Our pairwise word interaction model is in-spired by such behavior: whenever the next word of a sentence is read, the model would compare it and its context against all words and their contexts in the other sentence. Figure 2 illustrates this model.
We first define a comparison unit for comparing two hidden states
Cosine distance ( cos ) measures the distance of two vectors by the angle between them, while L 2 Euclidean distance ( L 2 Euclid ) and dot-product distance ( DotProduct ) measure magnitude differ-ences. We use three similarity functions for richer measurement.

Algorithm 1 provides details of the modeling pro-cess. Given the input x a t  X  sent a at time step t where a  X  { 1 , 2 } , its Bi-LSTMs hidden state h bi at is the concatenation of the forward state h for at and the Figure 2: Pairwise word interaction modeling. Sen-tences are encoded by weight-shared Bi-LSTMs. We construct pairwise word interactions for context comparisons across sentences.
 Algorithm 1 Pairwise Word Interaction Modeling backward state h back at . Algorithm 1 proceeds as fol-lows: it enumerates all word pairs ( s,t ) across both sentences, then perform comparisons using the coU and h bi 2 s ; 2) forward hidden states h for the addition of forward and backward hidden states 1 t and h add 2 s . The output of Algorithm 1 is a where | sent  X  | is the number of words in the sentence sent  X  . The 13 values collected from each word pair ( s,t ) are: the 12 similarity distances, plus one extra dimension for the padding indicator. Note that all word interactions are modeled over word contexts in Algorithm 1, rather than individual words.
Our pairwise word interaction model shares sim-ilarities with recent popular neural attention mod-els (Bahdanau et al., 2014; Rush et al., 2015). How-ever, there are important differences: For example, we do not use attention weight vectors or weighted Figure 3: The similarity focus layer helps identify important pairwise word interactions (in black dots) depending on their importance for similarity mea-surement. representations, which are the core of attention mod-els. The other difference is that attention weights are typically interpreted as soft degrees with which the model attends to particular words; in contrast, our word interaction model directly utilizes multiple similarity metrics, and thus is more explicit. Since not all words are created equal, important pairwise word interactions between the sentences (Sec. 5) that can better contribute to the similarity measurement deserve more model focus. We there-fore develop a similarity focus layer which can iden-tify important word interactions and increase their model weights correspondingly. This similarity fo-cus layer is directly incorporated into our end-to-end model and is placed on top of the pairwise word in-teraction model, as in Figure 1.

Figure 3 shows one example where each cell of the matrix represents a pairwise word interaction. The similarity focus layer introduces re-weightings to word interactions depending on their importance for similarity measurement. The ones tagged with black dots are considered important, and are given higher weights than those without.

Algorithm 2 shows the forward pass of the sim-ilarity focus layer. Its input is the similarity cube simCube (Section 5). Algorithm 2 is designed to incorporate two different aspects of similarity based on cosine (angular) and L 2 (magnitude) sim-ilarity, thus it has two symmetric components: the first one is based on cosine similarity (Line 5 to Line 13); and the second one is based on L 2 sim-ilarity (Line 15 to Line 23). We also aim for the goal that similarity values of all found important word in-Algorithm 2 Forward Pass: Similarity Focus Layer teractions should be maximized. To achieve this, we sort the similarity values in descending order (Line 5 for cosine , Line 15 for L 2 ). Note channels 10 and 11 of the simCube contain cosine and L 2 values, respectively; the padding indicator is in Line 24.
We start with the cosine part first, then L 2 . For each, we check word interaction candidates mov-ing down the sorted list. Function calcPos is used to calculate relative sentence positions pos s  X  in the simCube given one interaction pair. We follow the constraint that no word in both sentences should be tagged to be important more than once. We in-crease weights of important word interactions to 1 (in Line 11 based on cosine and Line 21 based on L 2 ), while unimportant word interactions receive weights of 0 . 1 (in Line 2).

We use a mask matrix, mask , to hold the weights of each. The final output of the similarity focus layer is a focus-weighted similarity cube focusCube , which is the element-wise multiplication (Line 25) of the matrix mask and the input simCube .

The similarity focus layer is based on the follow-ing intuition: given each word in one sentence, we look for its semantically similar twin in the other sentence; if found then this word is considered im-portant, otherwise it contributes to a semantic dif-ference. Though technically different, this process shares conceptual similarity with finding translation equivalences in statistical machine translation (Al-onaizan et al., 1999).

The backward pass of the similarity focus layer is straightforward: we reuse the mask matrix as gener-ated in the forward pass and apply the element-wise multiplication of mask and inflow gradients, then propagate the resulting gradients backward. The focusCube contains focus-weighted fine-grained similarity information. In the final model component we use the focusCube to compute the final similarity score. If we treat the focusCube as an  X  X mage X  with 13 channels, then semantic simi-larity measurement can be converted into a pattern recognition (image processing) problem, where we are looking for patterns of strong pairwise word in-teractions in the  X  X mage X . The stronger the overall pairwise word interactions are, the higher similarity the sentence pair will have.
 Recent advances from successful systems at ImageNet competitions (Simonyan and Zisserman, 2014; Szegedy et al., 2015) show that the depth of a neural network is a critical component for achieving competitive performance. We therefore use a deep homogeneous architecture which has repetitive con-volution and pooling layers.

Our network architecture (Table 1) is composed of spatial max pooling layers, spatial convolutional layers (Conv) with a small filter size of 3  X  3 plus stride 1 and padding 1 . We adopt this filter size because it is the smallest one to capture the space of left/right, up/down, and center; the padding and stride is used to preserve the spatial input resolution. We then use fully-connected layers followed by the final softmax layer for the output. After each spatial convolutional layer, a rectified linear units (ReLU) non-linearity layer (Krizhevsky et al., 2012) is used.
The input to this deep ConvNet is the focusCube , which does not always have the same size because Table 1: Deep ConvNet architecture given two padding size configurations for final classification. the lengths of input sentences vary. To address this, we use zero padding. For computational reasons we provide two configurations in Table 1, for length padding up to either 32  X  32 or 48  X  48 . The only difference between the two configurations is the last pooling layer. If sentences are longer than the padding length limit we only use the number of words up to the limit. In our experiments we found the 48  X  48 padding limit to be acceptable since most sentences in our datasets are only 1  X  30 words long. Datasets. We conducted five separate experiments on ten different datasets: three recent SemEval com-petitions and two answer selection tasks. Note that the answer selection task, which is to rank candi-date answer sentences based on their similarity to the questions, is essentially the similarity measure-ment problem. The five experiments are as follows: 1. Sentences Involving Compositional Knowledge (SICK) is from Task 1 of the 2014 SemEval com-petition (Marelli et al., 2014) and consists of 9,927 annotated sentence pairs, with 4,500 for training, 500 as a development set, and 4,927 for testing. Each pair has a relatedness score  X  [1 , 5] which increases with similarity. 2. Microsoft Video Paraphrase Corpus (MSRVID) is from Task 6 of the 2012 SemEval competi-tion (Agirre et al., 2012) and consists of 1,500 an-notated pairs of video descriptions, with half for training and the other half for testing. Each sen-tence pair has a relatedness score  X  [0 , 5] which increases with similarity. 3. Task 10 of the 2014 SemEval competition on Se-mantic Textual Similarity (STS2014) (Agirre et al., 2014) provided six different test sets from dif-ferent domains. Each pair has a similarity score  X  [0 , 5] which increases with similarity. Follow-ing the competition rules, our training data is only drawn from previous STS competitions in 2012 and 2013. We excluded training sentences with lengths longer than the 48 word padding limit, resulting in 7,382 training pairs out of a total of 7,592. Table 2 provides a brief description of the test sets. 4. The open domain question-answering WikiQA data is from Bing query logs by Yang et al. (2015). We followed the same pre-processing steps as Yang et al. (2015), where questions with no correct candidate answer sentences are ex-cluded and answer sentences are truncated to 40 tokens. The resulting dataset consists of 873 questions with 8,672 question-answer pairs in the training set, 126 questions with 1,130 pairs in the development set, and 243 questions with 2,351 pairs in the test set. 5. The TrecQA dataset (Wang et al., 2007) from the Text Retrieval Conferences has been widely used for the answer selection task during the past decade. To enable direct comparison with pre-vious work, we used the same training, develop-ment, and test sets as released by Yao et al. (2013).
The TrecQA data consists of 1,229 questions with 53,417 question-answer pairs in the TRAIN-ALL training set, 82 questions with 1,148 pairs in the development set, and 100 questions with 1,517 pairs in the test set.
 Training. For experiments on SICK, MSRVID, and STS2014, the training objective is to minimize the KL-divergence loss: where f is the ground truth, b f  X  is the predicted dis-tribution with model weights  X  , and n is the number of training examples.

We used a hinge loss for the answer selection task on WikiQA and TrecQA data. The training objec-tive is to minimize the following loss, summed over examples  X  x,y gold  X  : loss (  X ,x,y gold ) = where y gold is the ground truth label, input x is the pair of sentences x = { S 1 ,S 2 } ,  X  is the model weight vector, and the function f  X  ( x,y 0 ) is the out-put of our model.
 In all cases, we performed optimization using RMSProp (Tieleman and Hinton, 2012) with back-propagation (Bottou, 1998), with a learning rate Settings. For the SICK and MSRVID experi-ments, we used 300-dimension GloVe word embed-dings (Pennington et al., 2014). For the STS2014, WikiQA, and TrecQA experiments, we used 300-dimension PARAGRAM -SL 999 embeddings from Wieting et al. (2015) and the PARAGRAM -PHRASE embeddings from Wieting et al. (2016), trained on word pairs from the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). We did not up-date word embeddings in all experiments.

We used the SICK development set for tuning and then applied exactly the same hyperparameters to all ten test sets. For the answer selection task (Wiki-QA and TrecQA), we used the official trec eval scorer to compute the metrics Mean Average Preci-sion (MAP) and Mean Reciprocal Rank (MRR) and Table 3: Test results on SICK grouped as: (1) RNN variants; (2) SemEval 2014 systems; (3) Sequential LSTM variants; (4) Dependency and constituency tree LSTMs. Evaluation metrics are Pearson X  X  r , Spearman X  X   X  , and mean squared error (MSE). Rows in grey are neural network models. selected the best development model based on MRR for final testing. Our timing experiments were con-ducted on an Intel Xeon E5-2680 CPU.

Due to sentence length variations, for the SICK and MSRVID data we padded the sentences to 32 words; for the STS2014, WikiQA, and TrecQA data, we padded the sentences to 48 words. SICK Results (Table 3). Our model outperforms previous neural network models, most of which are based on sentence modeling. The ConvNet work (He et al., 2015) and TreeLSTM work (Tai et al., 2015) achieve comparable accuracy; for exam-ple, their difference in Pearson X  X  r is only 0.1%. In comparison, our model outperforms both by 1% in Pearson X  X  r , over 1.1% in Spearman X  X   X  , and 2-3% in MSE. Note that we used the same word embed-dings, sparse distribution targets, and loss function as in He et al. (2015) and Tai et al. (2015), thereby representing comparable experimental conditions. MSRVID Results (Table 4). Our model outper-forms the work of He et al. (2015), which already reports a Pearson X  X  r score of over 0 . 9 , STS2014 Results (Table 5). Systems in the com-petition are ranked by the weighted mean (the of-Table 5: Test results on all six test sets in STS2014. We show results of the top three participating sys-tems at the competition in Pearson X  X  r scores. ficial measure) of Pearson X  X  r scores calculated based on the number of sentence pairs in each test set. We show the 1st ranked (Sultan et al., 2014), 2nd (Kashyap et al., 2014), 3rd (Lynum et al., 2014) systems in the STS2014 competition, all of which are based on heavy feature engineering. Our model does not use any sparse features, WordNet, or parse trees, but still performs favorably compared to the STS2014 winning system (Sultan et al., 2014). WikiQA Results (Table 6). We compared our model to competitive baselines prepared by Yang et al. (2015) and also evaluated He et al. (2015) X  X  multi-perspective ConvNet on the same data. The neural network models in the table, paragraph vec-tor (PV) (Le and Mikolov, 2014), CNN (Yu et al., 2014), and PV-Cnt/CNN-Cnt with word matching features (Yang et al., 2015), are mostly based on sen-tence modeling. Our model outperforms them all. TrecQA Results (Table 7). This is the largest dataset in our experiments, with over 55,000 question-answer pairs. Only recently have neural network approaches (Yu et al., 2014) started to show promising results on this decade-old dataset. Pre-vious approaches with probabilistic tree-edit tech-niques or tree kernels (Wang and Manning, 2010; Heilman and Smith, 2010; Yao et al., 2013) have been successful since tree structure information per-mits a fine-grained focus on important words for similarity comparison purposes. Our approach es-sentially follows this intuition, but in a neural net-work setting with the use of our similarity focus layer. Our model outperforms previous work. Ablation Studies. Table 8 shows the results of abla-tion studies on SICK and WikiQA data. We removed or replaced one component at a time from the full system and performed re-training and re-testing. We found large drops when removing the context mod-eling component, indicating that the context infor-mation provided by the Bi-LSTMs is crucial for the following components (e.g., interaction modeling). The use of our similarity focus layer is also ben-eficial, especially on the WikiQA data. When we replaced the entire similarity focus layer with a ran-dom dropout layer ( p = 0 . 3 ), the dropout layer hurts accuracy; this shows the importance of directing the model to focus on important pairwise word interac-tions, to better capture similarity.
 Model Efficiency and Storage. He et al. (2015) X  X  Table 8: Ablation studies on SICK and WikiQA data, removing each component separately.
 Table 9: Comparison of training efficiency and num-ber of tunable model parameters on SICK data. Tim-ing is the average epoch time in seconds for training on a single CPU thread.
 ConvNet model uses multiple types of convolution and pooling for sentence modeling. This results in a wide architecture with around 10 million tunable pa-rameters. Our approach only models pairwise word interactions and does not require such a complicated architecture. Compared to that previous work, Ta-ble 9 shows that our model is 3.4  X  faster in training and has 83% fewer tunable parameters.
 Visualization. Table 10 visualizes the cosine value channel of the focusCube for pairwise word inter-actions given two sentence pairs in the SICK test set. Note for easier visualization, the values are multi-plied by 10 . Darker red areas indicate stronger pair-wise word interactions. From these visualizations, we see that our model is able to identify important word pairs (in dark red) and tag them with proper similarity values, which are significantly higher than the ones of their neighboring unimportant pairs. This shows that our model is able to recognize im-portant fine-grained word-level information for bet-ter similarity measurement, suggesting the reason why our model performs well. In summary, we developed a novel neural net-work model based on a hybrid of ConvNet and Bi-Table 10: Visualization of cosine values (multiplied by 10) in the focusCube given two sentence pairs in the SICK test set.
 LSTMs for the semantic textual similarity measure-ment problem. Our pairwise word interaction model and the similarity focus layer can better capture fine-grained semantic information, compared to previ-ous sentence modeling approaches that attempt to  X  X ram X  all sentence information into a fixed-length vector. We demonstrated the state-of-the-art accu-racy of our approach on data from three SemEval competitions and two answer selection tasks. This work was supported by the U.S. National Sci-ence Foundation under awards IIS-1218043 and CNS-1405688. Any opinions, findings, conclusions, or recommendations expressed are those of the au-thors and do not necessarily reflect the views of the sponsor. We would like to thank Kevin Gimpel, John Wieting and TS for all the support.

