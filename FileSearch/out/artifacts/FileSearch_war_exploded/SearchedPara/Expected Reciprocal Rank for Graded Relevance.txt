 While numerous metrics for information retrieval are avail -able in the case of binary relevance, there is only one com-monly used metric for graded relevance, namely the Dis-counted Cumulative Gain (DCG). A drawback of DCG is its additive nature and the underlying independence assump-tion: a document in a given position has always the same gain and discount independently of the documents shown above it. Inspired by the  X  X ascade X  user model, we present a new editorial metric for graded relevance which overcomes this difficulty and implicitly discounts documents which are shown below very relevant documents. More precisely, this new metric is defined as the expected reciprocal length of time that the user will take to find a relevant document. This can be seen as an extension of the classical recipro-cal rank to the graded relevance case and we call this metric Expected Reciprocal Rank (ERR). We conduct an extensive evaluation on the query logs of a commercial search engine and show that ERR correlates better with clicks metrics than other editorial metrics.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Performance evaluation (efficiency and ef-fectiveness) Experimentation, Measurement evaluation, non-binary relevance, web search, user model
Evaluation has gained a great deal of attention in the field of information retrieval recently, primarily due to the rap idly changing landscape of information retrieval systems. The validity of the assumptions underlying Cranfield and TREC-style evaluation have been questioned, especially in light of growing test collections, new types of information needs, a nd the availability of new sources of relevance data, such as cl ick logs and crowdsourcing.

How to properly evaluate web search engines continues to be a challenging, open research problem. Most web search evaluations in the information retrieval literature make u se of cumulative gain-based metrics, such as Discounted Cumu-lative Gain (DCG) [15]. These metrics are popular because they support graded relevance, which is often used when judging the relevance of web documents. Although support for graded relevance is important, there are other importan t factors that should be considered when evaluating metrics.
One important factor that DCG does not account for is how the user actually interacts with the ranked list. The metric assumes that users will browse to some position in the ranked list according to some probability that only de-pends on the position. However, in reality, the probability that a user browses to some position in the ranked list de-pends on many other factors other than the position alone. One serious issue with DCG is the assumption that the use-fulness of a document at rank i is independent of the useful-ness of the documents at rank less than i . Recent research on modeling user click behavior has demonstrated that the position-based browsing assumption that underlies DCG is invalid [12, 8]. Instead, these studies have shown that the likelihood a user examines the document at rank i is depen-dent on how satisfied the user was with previously observed documents in the ranked list. This type of user model is known as the cascade model.

To make things more concrete, let us consider a simple example. Suppose that we are evaluating two ranked lists, where judgments are on a 5 point scale ( perfect , excellent , good , fair , and bad ). Suppose that the first ranked list con-sists of 20 good documents and the second list has 1 perfect document followed by 19 bad documents. Which ranked list is better? Under most settings, DCG would indicate that the first list is better. However, the single perfect document in the second ranked list completely satisfies the user X  X  in-formation need, 1 and thus the user would observe the first document, be satisfied, stop browsing, and never see any of the bad documents. On the other hand, with the first ranked
In our guidelines a perfect grade is typically only given to the destination page of a navigational query. list, the user would have to expend much more effort to sat-isfy his information need, because each good document only partially satisfies the need. Thus, users would actually pre -fer the second ranked list over the first, because it satisfies their information need with the least amount of effort.
The rank biased precision (RBP) metric recently proposed by Moffat and Zobel [17] incorporates a simple user model, but does not directly address the issue described here. The metric models the user X  X  persistence in finding relevant docu-ments. Less persistent users are only likely to look at a smal l number of results, while very persistent users will look dee p in the ranked list. The primary issue with this simple user model is that real user browsing behavior is unlikely to be determined based entirely on user persistence, but also on the quality of the results in the ranked list, as illustrated in the example above. Indeed, Moffat and Zobel acknowledge this and explain that a more complex user model could be used instead, but leave this direction as future work.
The primary goal of this paper is to devise a metric that infuses RBP with a more accurate user model, thereby pro-viding a better alternative to DCG and RBP. To achieve this, we propose a novel metric called expected reciprocal rank (ERR). The metric supports graded relevance judg-ments and assumes a cascade browsing model. In this way, the metric quantifies the usefulness of a document at rank i conditioned on the degree of relevance of the items at ranks less than i . We argue, and show empirically, that our metric models user satisfaction better than DCG and is therefore a more appropriate metric for evaluating retrieval systems , especially those with graded judgments, such as web search engines.

There are two primary contributions of this work. First, we propose a novel metric based on a cascade-style user browsing model. We will demonstrate that most previous metrics have assumed position-based browsing, which has been shown empirically to be a poor assumption, and that the cascade model captures real user browsing behavior bet-ter. Second, our experimental results, which are carried ou t in the context of a very large test collection from a commer-cial search engine, show that ERR correlates with a range of click-based metrics much better than other editorial metri cs such as DCG.
 The remainder of this paper is laid out as follows. First, Section 2 provides some background on various aspects of information retrieval evaluation. Section 3 then describe s re-cent research on user browsing models. Our proposed ERR metric is explained in Section 4. Section 5 explores how ERR is related to several existing retrieval metrics. Our rigor ous, comprehensive experimental evaluation is described in Sec -tion 6. Finally, in Section 7 we discuss possible extensions of the model and conclude the paper in Section 8.
Evaluation plays a critical role in the field of information retrieval. Retrieval systems are often evaluated in terms o f their effectiveness and efficiency . Effectiveness evaluations quantify how good the search system is at satisfying users X  search needs, while efficiency evaluations measure the speed of the system. Since our focus here is effectiveness, we will provide a brief overview of the various effectiveness measur es that have been proposed for information retrieval.
Most, if not all, information retrieval effectiveness mea-sures depend on the ill-defined notion of relevance . This is largely due to the prevalence and popularity of Cranfield and TREC-style evaluations [10, 25]. These evaluations are based on a fixed set of queries, a fixed set of documents, and a fixed set of relevance judgments . Relevance judgments are collected by asking human editors to assess the relevance of a document to a given query. In this way, relevance judgments capture the notion of user relevance . Retrieval metrics are then computed by comparing the output of the retrieval sys-tem, typically in the form of ranked lists over the queries in the evaluation set, and the relevance judgments.

Although relevance judgments and metrics can be decou-pled, they are closely related. For this reason, whenever a new method for collecting relevance judgments is proposed, it typically comes along with a new retrieval metric. How-ever, the other direction is not as common, as newly pro-posed metrics do not always require new relevance judgment criteria.
 Relevance judgments come in many different flavors. The Cranfield experiments, and many subsequent evaluations, make a certain set of assumptions about the judgments. For example, it is assumed that relevance is topical (relevant documents are on the same topic as the query), that judg-ments are binary (relevant / not relevant), independent (rel-evance of document A does not depend on relevance of docu-ment B), stable (judgments do not change over time), consis-tent (judgments are consistent across editors), and complete (there are no missing judgments) [23]. These assumptions underlie most of the classical information retrieval metri cs, such as precision, average precision, and recall.
While these assumptions simplify the editorial process to some extent, many are unrealistic and not well aligned with user relevance. For this reason, researchers have looked at various relaxations of the assumptions that have subse-quently led to new retrieval metrics. We now highlight sev-graded relevance judgments and proposed the DCG metric that can exploit such judgments. Second, the TREC Nov-elty track investigated dependent relevance assessments [ 14]. The TREC Interactive Track evaluations had editors assign subtopics to each query. Editors were then asked to judge each document with respect to the subtopics [18]. This has led to various subtopic retrieval and diversity metrics [27 , 9, 1]. Finally, various researchers have showed how the com-pleteness assumption could be relaxed by inferring the rele -vance of missing judgments in various ways [3, 5, 7], ignorin g unjudged documents [21], or intelligently choosing which u n-judged documents to obtain judgments for [6].

Evaluation metrics themselves tend to be much simpler and have fewer issues compared to relevance judgments. Evaluation metrics are computed given the output of a re-trieval system and the relevance judgments. Most measures make various assumptions about what makes for a  X  X ood X  ranked list based on the existing editorial judgments and op -erationalizes that using some easy-to-compute mathematic al formulation. For example, the DCG at rank K for a given query is computed as [4]: where g i is the relevance grade of the document at rank i . The numerator of the metric rewards documents with large relevance grades, while the denominator discounts the gain s at lower ranks. This simple metric operationalizes the no-tion that systems that rank highly relevant documents high in the ranked list are better than systems which rank highly relevant documents deep in the ranking. This general idea forms the basis for most precision-based metrics, includin g average precision, and the RBP metric described in the in-troduction, which is computed as follows: where g i indicates the degree of relevance of the document at rank i and p is a parameter that models how persistent a user is while looking through the ranked list. This measure makes similar assumptions to DCG, except the persistence parameter p models some notion of user browsing behavior, which is absent in DCG. We will return to this important fact, and how it relates to our proposed metric, later in this paper.

We now explain where our proposed metric fits into the vast evaluation research landscape. Our metric is similar i n spirit to DCG, in that we assume that relevance judgments are graded, independent, and complete. However, as with DCG, it is important to note that our metric can easily be extended to handle incompleteness [7], but for simplicity w e assume completeness. Our metric is different from DCG, however, in that it incorporates a user model that acts as a proxy for dependent relevance judgments. DCG only dis-counts based on the rank of the document, but does not consider any of the documents previously seen by the user. Our metric, however, implicitly discounts documents based on the relevance of previously seen documents. The dis-counting that we use corresponds to a user browsing model.
An accurate user model, which closely reflects users X  inter-actions with the retrieval system, is essential for develop ing a good relevance metric. Before we go into the details of our proposed metric, it is worthwhile to first review exist-ing user models for retrieval system. In general, there are two main types of user models: position models and cascade models. Both types of models attempt to capture the posi-tion bias of search result presentation. While the position al models assume independence among documents in different positions and model the examination probability as a func-tion of the position, cascade models simultaneously model the relevance and examination probability of documents in the entire result set.

Position-based models [12, 20] are a popular class of meth-ods for dealing with the presentation bias problem inherent in ranked retrieval systems. Among them, the examination model explicitly predicts the probability of examination a t various positions. It relies on the assumption that the user clicks on a link if and only if the following two conditions ar e met: the user examined the URL and found it relevant; in addition, the probability of examination depends only on th e position. The probability of click on the URL u in position p is thus modeled as [12, eq (3)]: where a u is the attractiveness of the URL u  X  or the propen-Figure 1: Comparison of the discounting function in DCG and RBP with the probability of examination at a given position (estimated from click logs). sity of users to click on that URL, independent of the posi-tion  X  and b p is the probability of examination in position p , which depends only on that position.

Both the DCG metrics and RBP metrics adopt the po-sition model as their underlying user browsing model and apply a position-based discounting function to progressiv ely reduce the contribution of a document as its rank increases. Figure 1 compares the discounting functions of DCG and RBP with the probability of examination b p , which was es-timated from the click logs of a commercial search engine us-ing the position model described above. As the graph shows, RBP with p = 0 . 7 closely approximates the estimated prob-ability of examination. However, as we show next, assuming that the probability of examination only depends on positio n has some serious fundamental drawbacks.

Take for instance a relevant document in position 3; if both documents in position 1 and 2 are very relevant, it is likely that this document will be examined less and hence have very few clicks. On the other hand, if the two top documents are non-relevant, it is more likely to be examined and receive many clicks. A click model depending only on the position will not be able to model these two cases  X  the same document at the same position has a different click through rate (CTR) when the documents ranked above are different. A real example of this phenomenon, taken from the click logs of a commercial search engine, is shown in Figure 2. In this example, we observe that the CTR of the URL www.myspace.com in position 1 is 9 times larger than the CTR of the same URL when it is shown in position 2. On average, however, the CTR of position 1 is roughly twice the CTR of position 2. The difference is so large here because the URL shown in position 1 is an excellent match, and hence the user rarely even browses to position 2. Position models, assuming the independence among URLs, fail to explain such a drastic CTR difference.

The examples above showed that in order to accurately model clicks and probabilities of examination, position is not sufficient and the relevance of documents above the docu-ment of interest has to be considered. Cascade models differ from the position models in that they consider the depen-uk.myspace.com 0.97 www.myspace.com 0.97 www.myspace.com 0.11 Figure 2: Illustration of the problem with position-based models. The query is myspace in the UK mar-ket. See text for discussion. dency among URLs on a search results page. In its generic form, the cascade model assumes that the user views search results from top to bottom and at each position, the user has a certain probability of being satisfied. Let R i be this probability at position i . 2 Once the user is satisfied with a document, he/she terminates the search and documents below this result are not examined regardless of their posi-tion. It is of course natural to expect R i to be an increasing function of the relevance grade, and indeed in what follows we will assimilate it to the often loosely-defined notion of  X  X elevance X . This generic version of the cascade model is summarized in Algorithm 1.
 Algorithm 1 The cascade user model Require: R 1 , . . . , R 10 the relevance of the 10 URLs on the 1: i = 1 2: User examines position i . 3: if random(0,1)  X  R i then 4: User is satisfied with the document in position i and 5: else 6: i  X  i + 1; go to 2 7: end if
Two instantiations of this model have been presented in [12, 8]. In the former, R i is the same as the attractiveness defined above for position-based models: it measures a prob-ability of click which can be interpreted as the relevance of the snippet. In that model, it is assumed that the user is al-ways satisfied after clicking. It can however be the case that the snippet looks attractive, but that the user does not find any relevant information on the corresponding landing page . This is the reason why an extended cascade model has been proposed in [8, Section 5], in which the user might not be satisfied after clicking. More precisely, there is a probabi lity, depending on the landing page, that the user will go back to the search result list after clicking. The R i in Algorithm 1 have now to be understood as the relevance probability of the landing page.

In both models a document satisfies the user with prob-ability R i . The values R i can be estimated by maximum likelihood on the click logs. Alternatively, as we will do in the next section, the R i values can be set as a function of the editorial grade of the URL. For a given set of R i , the likeli-hood of a session for which the user is satisfied and stops at position r is:
The probability is in fact a function of the i -th document d ( i ). However, for simplicity we shorten R d ( i ) to R which is simply the probability the the user is not satisfied with the first r  X  1 results and is satisfied with the r -th one.
We now introduce our proposed metric based on the cas-cade model described in the previous section. A key step is the definition of the probability that a user finds a doc-ument relevant as a function of the editorial grade of that document. Let g i be the grade of the i -th document, then: where R is a mapping from relevance grades to probability of relevance. R can be chosen in different ways; in accordance with the gain function for DCG used in [4], we might take it to be: When the document is non-relevant ( g = 0), the probability that the user finds it relevant is 0, while when the document is extremely relevant ( g = 4 if a 5 point scale is used), then the probability of relevance is near 1.

We first define the metric in a more general way by con-sidering a utility function  X  of the position. This function typically satisfies  X  (1) = 1 and  X  ( r )  X  0 as r goes to +  X  .
Definition 1 (Cascade based metric). Given a util-ity function  X  , a cascade based metric is the expectation of  X  ( r ) , where r is the rank where the user finds the document he was looking for. The underlying user model is the cascade model (2) , where the R i are given by (3) .

In the rest of this paper we will be considering the special case  X  ( r ) = 1 /r , but there is nothing particular about that choice and, for instance, we could have instead picked  X  ( r ) = Definition 2 (Expected Reciprocal Rank).
 The Expected Reciprocal Rank is a cascade based metric with  X  ( r ) = 1 /r .

It may not seem straightforward to compute ERR from the previous definition because there is an expectation. How -ever it can easily be computed as follows: where n is the number of documents in the ranking. The probability that the user stops at position r is given by the definition of the cascade model (2). Plugging that value into the above equation, we finally obtain: A na  X   X ve computation using the above requires O ( n 2 ) oper-ations. But as shown in Algorithm 2, ERR can easily be computed in O ( n ) time.
 Compared to position-based metrics such as DCG and RBP for which the discount depends only the position, the discount in ERR depends on the relevance of documents Algorithm 2 Algorithm to compute the ERR metric (5) in linear time.
 Require: Relevance grades g i , 1  X  i  X  n , and mapping function R such as the one defined in (4). p  X  1, ERR  X  0. for r = 1 to n do end for return ERR shown above it. The  X  X ffective X  discount in ERR of docu-ment at position r is indeed: Thus the more relevant the previous documents are, the more discounted the other documents are. This diminish-ing return property is desirable because it reflects real use r behavior.
 Figure 3 summarizes our discussion up until this point. The figure shows the connection between user models and metrics. As the figure shows, most traditional measures, such as DCG and RBP assume a position-based user brows-ing model. As we have discussed, these models have been shown to be poor approximations of actual user behavior. The cascade-based user model, which is a more accurate user model, forms the basis for our proposed ERR metric. Figure 3: Links between user models and metrics: DCG and RBP are instances of metrics that can be motivated using a position-based model. But the cascade user model is a more accurate model and the ERR metric derived from it correlates better with user satisfaction.
As we discussed in Section 2, our metric is similar to DCG to a certain extent. The metric also shares commonalities with several other metrics, which we will describe in this section.

First, our metric is related to the expected search length (ESL) metric, which was proposed by Cooper in 1968 [11]. The metric, which is defined over a weak ordering of doc-uments, quantifies the amount of user effort necessary to find K relevant documents. The measure computes the ex-pected number of non-relevant documents that the user will see, by sequentially browsing the search result list, befor e finding the K th relevant document. In the simplest case, the weak ordering is taken as the ranked output of the sys-tem, in which the ESL can be computed by simply counting the number of non-relevant documents that occur before the K th relevant document. The measure has been shown to be useful for measuring the effectiveness of web search en-gines [24]. Our metric differs from ESL in that we explicitly support graded judgments and also take into account a user browsing model that is absent from the ESL model. One of the primary problems with ESL is that it requires knowing the appropriate value of K for each query. Rather than as-suming the user wants to find K relevant documents, our metric measures the (inverse) expected user effort required to be satisfied.

Second, ERR is closely related to Moffat and Zobel X  X  RBP metric [17]. Our metric can be thought of as an extension and generalization of RBP that makes use of the cascade model as a user browsing model. We note that Moffat and Zobel discuss the possibility of incorporating a user model into RBP by making p dependent on the previously seen documents, the authors left this direction open as future work. The combination of the cascade model and RBP is natural and provides a number of benefits, including no need to set p a priori and the possibility of seemlessly combining human judgments and clicks in a single unified framework, as will be discussed in Section 7.3.

Third, suppose that all of the R i values are either 0 or 1, which corresponds to the binary relevance setting. In this scenario it is easy to see that: which is exactly the reciprocal rank (RR) metric [26]. Thus, under binary relevance, ERR simplifies to RR.
 Fourth, ERR can be seen as a special case of Normalized Cumulative Utility (NCU) [22], which is defined as where p ( r ) is the probability that the user stops at position r and NU ( r ) is a utility , defined as a combination of benefit and effort for the user to have examined all the documents from position 1 to r . In the case of ERR, p ( r ) is given by the cascade model (2), while NU ( r ) = 1 /r . But we could have considered other choices for the utility function NU , such as precision or a normalized cumulative gain as discussed in [22]. The important common point between ERR and NCU is the separation of the stopping point probability from utility as discussed in [22, section 3.1].

Finally, one can recover the case of additive metrics such as DCG in the limit where the R i are infinitesimally small. In this case, Q r  X  1 i =1 R i  X  1 and equation (5) is approxima-tively equal to: It course does not make sense from a user model point of view to consider infinitesimally small R i , but the point here is that when the R i are far away from 1, ERR turns out to be more similar to DCG. This happens in particular for difficult queries where there are only marginally relevant documents . This behavior has also been empirically observed as we shall see at the end of section 6.2.
The evaluation of new metrics is challenging because there is no ground truth to compare with. Because of that, most papers that propose new metrics do not have direct evalu-ations. For instance in [16, 17] it is shown that the newly introduced metrics correlate well with other standard met-rics. However, that does not imply that these metrics are  X  X etter X  in the sense of user satisfaction.

We attempt to narrow this gap in this paper by consider-ing click metrics. Even though clicks constitute indirect a nd noisy user feedback, they still contain some valuable infor -mation about user preferences. In fact, it has been shown that the quality of a retrieval system can be rather well es-timated with clickthrough data [19]. In this evaluation, we will compute correlations between various click metrics an d editorial metrics.
We collected clickthrough data from a major commercial search engine in two different markets. The first step in-volves constructing session data. A session can be defined in various ways, but for this evaluation, it is defined as fol-lows. A session always has a unique user and unique query. It starts when a user issues a query and ends with 60 min-utes idle time on the user side. For each session, we get the query, list of URLs from the result sets, and a list of clicked URLs. Simple normalization is applied to queries and URLs. We restrict ourselves to either the top 5 or top 10 URLs of the first page along with the clicks on these URLs. In par-ticular, for the top 5, the clicks on URLs after position 5 are ignored. We then intersect this data with editorial judg -ments and keep only the sessions for which we have editorial judgements for all the URLs. This is the reason why we con-sider sessions with depth 5: the intersection with editoria l judgments is larger than with depth 10 and a larger number of sessions are retained. The editorial judgments are on a 5 grades scale from the set: Because of variations in the search engine, the result set fo r a given query may vary. We call a configuration a query with a given set of ordered results. The statistics of the data we collected in this way are summarized in Table 1.
Using the datasets presented in the previous section, we compute a weighted correlation over the configurations be-tween an editorial and a click metric. More precisely, sup-pose that there are N configurations (remember that a con-figuration is a query and an ordered set of results). For the i -th configuration, let x i be the value of some editorial met-ric, y i the value of the click metric, and n i the number of times this configuration is present in the dataset. Then, the weighted correlation is computed as follows: C ( x, y, n ) = where m x and m y are the weighted averages: The reason for introducing weights in the correlation com-putation is to reproduce the click logs distribution and giv e more weight to frequent queries.

The five editorial metrics that were compared in this eval-uation are: DCG Discounted Cumulative Gain (1), truncated at 5 or NDCG Normalized (with respect to the ideal DCG) DCG. AP Average Precision, where the grades perfect and excel-RR Reciprocal Rank. The grades are mapped to binary ERR The metric proposed in this paper.
 Regarding AP and RR, it may seem surprising that we mapped the good grade to non-relevant. However, as we will see later, this results in a higher correlation than mapping it to relevant.

Let us now discuss the click metrics. All of them are de-fined as averages over all the sessions belonging to a given configuration. Leveraging the conclusions from a previous study [19, table 2], we experimented with the most dis-criminative metrics according to the findings of that paper, namely: QCTR Number of clicks in a session.
 UCTR Binary variable indicating whether there was a click Max, Mean, Min RR Respectively maximum, mean and
Of these 4 metrics, we found that QCTR was slightly negatively correlated with editorial metrics and we decide d not to include it in the following tables. The problem with QCTR is that there are two conflicting interpretations: on the one hand, more relevant results should lead to higher number of clicks; but on the other hand, worse results can also lead to higher number of clicks because the user is strug -gling to find the information he is searching for. This is par-ticularly true for navigational queries if the expected URL is not at the top of the ranking.

There are two other click metrics that we considered, which have not been published previously, but that we be-lieve are good measurements of user satisfaction: SS Search Success is similar to UCTR except that clicks on PLC Precision at Lowest Click is defined as the number Table 3: Correlations (market 1, depth 5) with AP and RR for two different mappings from graded to binary relevance: in the first one (G  X  NR), a good document is considered non-relevant, while in the second one (G  X  R), it is considered relevant.

The correlations between the editorial and click metrics described above are listed in Table 2. It is remarkable that for all click metrics and all datasets, ERR correlates bette r than any other editorial metric. This is an indication that ERR captures user satisfaction better than other metrics. It is worth noting that the correlation of NDCG is overall a bit inferior to DCG. A similar observation was made in [2]. As for the click metrics, the highest correlation seems to be with Min RR, which is the reciprocal rank of the lowest click. This can be explained by the fact that, in general, users are satisfied by the last clicked document, and if click s are done from top to bottom, this also corresponds to the lowest clicked document.

For AP and RR as editorial metrics, the graded relevance judgements had to be converted to binary relevance. As indi-cated earlier, perfect and excellent were converted to relevant while good , fair and bad were converted to non-relevant. We have also tried to consider good documents as relevant, but as shown in Table 3, this resulted in much lower correlations .
In order to have a finer analysis of why ERR correlates better with click metrics, we now compute correlations con-ditioned on several properties of the query. Note that be-cause of the amalgamation paradox [13] (similar to Simpson X  X  paradox), these conditional correlations can be lower than the aggregated correlations reported in Table 2.

The queries have been split on 3 different axes: query length, query frequency, navigational vs non-navigationa l. For each split, we compute the correlation between the click and editorial metrics as was done in the previous section. For the sake of brevity, we restrict ourselves to market 1 with depth 5 and consider only mean reciprocal rank of the clicks as the click metric. This metric showed the largest overall correlation with editorial metrics in Table 2. Table 4: Correlation with mean RR as a function of the query length. Table 5: Correlation with mean RR on navigational queries (35% of queries are in that category) and non-navigational queries.

The correlations as a function of the number of query terms and of the query type (navigational or not) are shown in tables 4 and 5 respectively. As for query frequency, we sorted queries by increasing frequency and put them in 10 different bins of equal size. The correlation for each decile is plotted in Figure 4. The same conclusion emerges from these three studies: the difference between DCG and ERR is not so large for difficult and tail queries, but it is more pronounced for easy and head queries. This behavior is not unexpected. As noted earlier, one drawback of DCG is that it does not enough discount documents following a perfect result (and to a lesser extent excellent . However, ERR does that implicitly by assuming that most users will not examine results after a perfect , and perfect results are more frequent for easy / head / navigational queries. We had also noted at the end of Section 5 that ERR tends to behave as an additive metric like DCG for difficult queries. The results presented here confirm that argument.
There is a potential pitfall in the evaluation from the previous section: the correlation coefficient implicitly co m-pares metrics across configurations and queries which, as suggested for example by [25], has little significance; the only important aspect of these metrics X  behavior is with re-spect to changes in the search results configuration for a given query.

We thus devise a mechanism to measure the correlations between differences of click and editorial metrics. From Ta-ble 1, it appears that there are on average 10 different con-figurations per query. We emulate two ranking systems by assigning, at random, two of these configurations to two vir-tual ranking systems for each query. Ideally, the sign of column: depth 10; right column: depth 5.
 UCTR 0.131 0.108 0.225 0.238 0.307 max RR 0.197 0.199 0.348 0.366 0.449 mean RR 0.220 0.217 0.376 0.395 0.480 min RR 0.235 0.230 0.394 0.414 0.500 PLC 0.210 0.209 0.364 0.382 0.466 SS 0.357 0.361 0.402 0.415 0.482 UCTR 0.298 0.215 0.335 0.356 0.435 max RR 0.319 0.253 0.369 0.407 0.493 mean RR 0.347 0.268 0.395 0.434 0.515 min RR 0.362 0.275 0.408 0.447 0.524 PLC 0.334 0.261 0.386 0.424 0.507 SS 0.427 0.378 0.373 0.402 0.465 Figure 4: Correlation with mean RR as a function of the query decile (1 = rarest; 10 = most frequent). difference between the editorial metric for these two sys-tems should be the same as the sign of the difference of the respective click metric. To measure this, we repeat this procedure 1000 times and compute the correlation of the differences. The methodology is summarized in Algorithm 3 and the correlations are reported in Table 6.

In this evaluation ERR shows again the largest correlation with all click metrics reflecting that it is suitable to predi ct accurately, via the clicks, the difference in user satisfact ion between two ranking functions. Therefore, ERR is not only effective for evaluating individual systems, but also quite effective at identifying differences between systems, as wel l. This section discusses a number of possible extensions of ERR that may be useful in a variety of evaluation scenarios.
If L is the number of relevance levels, then there are L adjustable parameters in our metric corresponding to the values R ( g ) for each relevance level. Instead of fixing these values as in equation (4), we could optimize them by either maximizing the correlation with clicks or the agreement rat e with side-by-side editorial tests. The latter was done for Algorithm 3 Methodology used to simulate two ranking functions and see whether the difference in the editorial met-ric correlates with the difference in click metric. 1: for i = 1 to 1000 do 2: for q = 1 to Q do 3: C q := { set of configurations for query q } 4: if | C q | X  2 then 5: c 1 , c 2  X  two random elements of C q . 6: Assign c 1 to engine A and c 2 to engine B. 7: end if 8: end for 9: Compute the average editorial metric (over the 10: Let  X  E i be the difference between these two numbers. 11: Similarly, let  X  C i be the difference between the aver-12: end for 13: Compute the correlation between  X  E and  X  C . Table 6: Correlation between the differences of click and editorial metrics for two virtual ranking func-tions. See Algorithm 3 for details. The dataset is from market 1 with depth 10 (1st table) and 5 (2nd table).
 DCG in [28]. In addition to learning to the mapping function R , it is also possible to learn the utility function  X  (see definition 1), that is, instead of using  X  ( r ) = 1 /r , learn the values  X  (1) , . . . ,  X  ( K ), where K is the number of positions.
The original cascade model of [12] has later been extended in [8] to include an abandonment probability: if the user is not satisfied at a given position, he will examine the next url with probability  X  , but has a probability 1  X   X  of aban-doning. In that model, the probability of the user stopping at position r is: which is the same as (2) but multiplied by  X  r  X  1 . With the possibility of the user giving up, it can make sense to define a simpler utility function: 0 if the user abandoned, 1 other-wise; that is  X  ( r ) = 1 in definition 1. The resulting metric is then defined as: which is very similar to the ERR formulation (5), the only difference being that the 1 /r decay is replaced by a geometric decay  X  r  X  1 .
The mapping function from relevance grade to probability of relevance is currently chosen to match the gain function for DCG. An alternative way to define the mapping function is to learn it directly from click logs. For instance R ( g ) can be the average relevance estimated from click logs of all the URLs having grade g .

Our proposed metric can also be easily extended to accom-modate relevance judgments in the form of combined edito-rial data and click data. So far most previous metrics have been based entirely on one type of relevance judgment and do not easily extend to use more than one. Since our metric is based on a click model, clicks can be combined with rele-vance judgments seemlessly within the metric. When there is a document with missing editorial judgement, we can use its predicted probability of relevance by fitting the cascad e model with click logs. This could help the missing judgment issue. On the other hand, we can also largely rely on clicks for evaluation and only actively collect editorial judgmen t when we could not get a confident prediction for the proba-bility of relevance from click logs. The latter one would be a more cost effective way of evaluating search engines.
Research on metrics that incorporate the notion of diver-sity has recently gained interest [1, 9]. The measure pre-sented in this paper and the underlying cascade model can easily be extended to handle this notion.

Let P ( t | q ) be the distribution of topics 3 for a given query q . Each document is now judged with respect to the possible topics, where g t i denotes the grade of the document in posi-tion i for topic t . The associated probability of relevance is
Instead of topics, [1] refers to classes and [9] to nuggets. then R t i := R ( g t i ). As in the standard cascade model, a user interested by topic t will stop at rank r with probability: Marginalizing over the topics, the probability that a user stops at rank r is: And the diversity extension of ERR can be written as:
Interestingly a similar equation to (6) has been derived in [1], but for the purpose of finding a diverse set of results, not for evaluation (their evaluation is based on an expected DCG which, in our opinion, does not give enough weight to minor intents). To be precise, the objective function of [1] is the probability that user finds a relevant result and not the expected reciprocal rank. But both are similar. Also [1] notes in conclusion that in future work, it would be better to optimize for  X  X he expected rank at which the user will find useful information X  instead of the probability that he will find something: that is what (6) achieves.
In this paper, we proposed a novel evaluation metric for in-formation retrieval called expected reciprocal rank, or ER R. The metric, which was inspired by the cascade user browsing model, measures the (inverse) expected effort required for a user to satisfy their information need. The metric differs from average precision, rank-biased precision, and DCG in that it heavily discounts the contributions of documents th at appear after highly relevant documents. This intuition, bo r-rowed from the cascade model, assumes that a user is more likely to stop browsing if they have already seen one or more highly relevant documents. ERR supports graded relevance judgments and simplifies to reciprocal rank in the case of binary relevance judgments.

A rigorous set of empirical evaluations were carried out on a data set from a commercial search engine. The re-sults showed that ERR consistently correlates better with a wide range of click-based metrics compared to DCG and other editorial metrics. The difference in correlation was particularly pronounced for navigational, short, and head queries, where ERR was much more highly correlated than DCG. Our experimental results suggest that ERR reflects real user browsing behavior better and quantifies user satis -faction more accurately than DCG.

Finally, we proposed several possible extensions to ERR that make the metric even more robust and attractive. These extensions include a method for automatically estimating the metric parameters, the ability to use both human edito-rial judgments and click data with the metric, and a simple way to incorporate the notion of diversity into the metric.
Thus, we argue that ERR should replace DCG as the de facto evaluation measure for web search engines, since it is inspired from an accurate user browsing model, correlates much better with a wide range of click-based metrics, and has a number of highly practical and useful extensions. [1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. [2] A. Al-Maskari, M. Sanderson, and P. Clough. The [3] J. A. Aslam and E. Yilmaz. Inferring document [4] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [5] S. B  X  uttcher, C. L. A. Clarke, P. C. K. Yeung, and [6] B. Carterette, J. Allan, and R. Sitaraman. Minimal [7] B. Carterette and R. Jones. Evaluating search engines [8] O. Chapelle and Y. Zhang. A dynamic bayesian [9] C. L. Clarke, M. Kolla, G. V. Cormack, [10] C. W. Cleverdon. Report on the Testing and Analysis [11] W. Cooper. Expected search length: A single measure [12] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An [13] I. Good and Y. Mittal. The amalgamation and [14] D. Harman. Overview of the TREC 2002 novelty [15] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [16] J. Kek  X  al  X  ainen. Binary and graded relevance in IR [17] A. Moffat and J. Zobel. Rank-biased precision for [18] P. Over. TREC-7 interactive track report. In Proc. 7th [19] F. Radlinski, M. Kurup, and T. Joachims. How does [20] M. Richardson, E. Dominowska, and R. Ragno.
 [21] T. Sakai. Alternatives to bpref. In Proc. 30th Ann. [22] T. Sakai and S. Robertson. Modelling a user [23] T. Saracevic. Relevance: A review of the literature and [24] M.-C. Tang and Y. Sun. Evaluation of web-based [25] E. M. Voorhees. The philosophy of information [26] E. M. Voorhees and D. M. Tice. The TREC-8 [27] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond [28] K. Zhou, H. Zha, G. Xue, and Y. Yu. Learning the
