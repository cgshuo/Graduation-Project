 Most traditional text clustering methods are based on  X  X ag of words X  ( BOW ) representation based on frequency statistics in a set of documents. BOW , however, ignores the important information on the semantic relationships between key terms. To overcome this problem, several me thods have been proposed to enrich text representation with external resource in the past, such as WordNet. However, many of these approaches suffer from some limitations: 1) WordNet has limited coverage and has a lack of effective word-sense disa mbiguation ability; 2) Most of the text representation enrichment strategies, which append or replace document terms with their hypernym and synonym, are overly simple. In this paper, to overcome these deficiencies, we first propose a way to build a concept thesaurus based on the semantic relations (synonym, hypernym, and associative relation) extracted from Wikipedia. Th en, we develop a unified framework to leverage these semantic relations in order to enhance traditional content similarity measure for text clustering. The experimental results on Reuters and OHSUMED datasets show that with the help of Wi kipedia thesaurus, the clustering performance of our method is improved as compared to previous methods. In addition, with the optimized weights for hypernym, synonym, and associative concepts that are tuned with the help of a few labeled data users provided, the clustering performance can be further improved. I.5.2 [ Pattern Recognition ]: Design Methodology-Clustering design and evaluation .
 Algorithms, Experimentation, Performance, Human Factors Text Clustering, Wikipedia, Th esaurus, Similarity Measure The exponential growth of online document in World Wide Web has raised an urgent demand for efficient, high-quality text clustering algorithms for fast na vigation and browsing of users based on better document organizat ions. However, the traditional document clustering algorithms have been based on a variation of the  X  X ag of words X  ( BOW ) approach, which represents the documents with features as wei ghted occurrence frequencies of individual terms. 
The BOW representation is limited as it only counts in the term frequency statistics in the docu ments and ignores the important information of the semantic relationships between key terms. Thus, the distance measure of text clustering based on  X  BOW  X  cannot reflect the actual dist ance of two documents. As the clustering performance is heavily relied on the distance measure of document pairs, finding an a ccurate distance measure which can break the limitation of  X  BOW  X  is crucial for improving text clustering performance. Several wo rks have been done to exploit external resource to enrich text document representation. [1][16][17][24] utilize term ontology structured from WordNet [19] to improve the BOW text representation. Among them, Hotho et al. [1] adopts various strategies to enrich text documents representation with synonym and hypernym from WordNet, and experimental resu lts showed some improvements in clustering performance. Othe r research works explored the usage of world knowledge bases in the Web such as Open Directory Project (ODP) [20] and Wikipedia to enrich text document representation. Gabrilovich et al. [2][3] try to apply feature generation techniques on ODP and Wikipedia to create new features that augment the ba g of words. Their application on text classification confirmed that background-knowledge-based features generated from ODP or Wikipedia can help text categorization and Wikipedia is less noise than ODP when used as knowledge base. However, these approaches have a number of limitations: First, WordNet has limited coverage  X  WordNet focuses on the usages of common words which are rarely to be the representative words of a document, and is lack of an effective word sense disambiguation method -the descrip tion for different senses of a word is quite short. Second, ODP and Wikipedia themselves are not structured thesaurus as WordNet. While enriching documents with features generated by ODP or Wikipedia, they are not as easy as WordNet to handle the problems of synonymy and polysemy, which are two fundamental problems in text clustering. Meanwhile, The structur al relations in Wikipedia is representation enrichment strategies of these approaches, which append or replace document te rms with their hypernym and synonym are overly simple -hypernym and synonym should have different importance as compared to the original document content when computing similarity of document pairs in different datasets. 
In this paper, we show that by fully leveraging the structural relationship information in Wi kipedia, we can enhance the clustering result by obtaining a more accurate distance measure. In particular, we first build an informative and easy-to-use thesaurus from Wikipedia, which explicitly deri ves the concept relationships based on the struct ural knowledge in Wikipedia, including synonymy, polysemy, hypernymy and associative relation. The generated thesaurus serves as a control vocabulary that bridges the variety of idiole cts and terminologies present in the document corpus. The thesaurus facilitates the integration of the rich encyclopedia knowledge of Wikipedia into text documents, because it resolves synonyms and introduces more general and associative concepts which may help identify related topics between text documents. Also, the coverage of the thesaurus is much larger than manually constructed thesaurus like WordNet, and it provides a richer context for polysemy concept sense disambiguation. We then propose a novel framework to leverage the hierarchical, synonymy and associative semantic relations from a Wikipedia thesaurus that we generated, where we treat the different relations in the thesaurus according to their different importance, in order to enhance traditional content similarity measure for text clustering. To evaluate the performance of the proposed method, we have performed an empirical evaluation on two real datasets  X  Reuters and OHSUMED. The experimental results show that with the help of the Wikipedia thesauru s, the clustering performance based on our proposed framework is improved over the previous methods. Moreover, with the optimized weights for hypernym, synonym, and associative concepts tuned with a few labeled data users provided, the clustering performance can be further improved. 
The rest of our paper is organized as follows: Section 2 describes the related works. In Section 3, our method of building thesaurus from Wikipedia is disc ussed. We outline the algorithm that utilizes Wikipedia thesaurus to improve text clustering in Section 4 before introducing our data set and evaluating our algorithm X  X  performance in Section 5. 
To date, the work on integrating semantic background knowledge into text clustering (c lassification) or other related tasks is quite few and the results are not good enough. Buenaga Rodriguez et al [16] and Urena Loez et al [24] successfully integrated the WordNet resource for a document categorization task. They improved classification results of Rocchio and Widrow-Hoff algorithms on Reuters corpus. In contrast to our approach, [16] utilize WordNet in a supervised scenario without employing WordNet relations such as hypernyms and associative relations. Meanwhile, they built the term vectors manually. Dave et al. [17] has utilized WordNet synsets as features for document representation and subsequent cl ustering. He did not perform word sense disambiguation and found that WordNet synsets decreased clustering performanc e in his experiments. Hotho et al. [1] integrated WordNet knowledge into text clustering, and investigated word sense disamb iguation strategies and feature weighting schema through considering the hypernym relations from WordNet. The experimental results on Reuters corpus show improvements compared with the best baseline. However, considering the few word usage contexts provided by WordNet, the word sense disambiguation effect is quite limited. Meanwhile, the enrichment strategy which appends or replaces document terms with their hypernym and synonym is overly simple. 
Gabrilovich et al. [2][3] proposed and evaluate a method to render text classification system s with encyclopedic knowledge  X  Wikipedia and ODP. They first build an auxiliary text classifier that can match documents with the most relevant articles of Wikipedia, and then augment the conventional BOW representation with new features which are the concepts (mainly the titles) represented by the relevant Wikipedia articles. Empirical results show that this representation improve text categorization performance across a diverse collection of datasets. However, they do not make full use of the rich relations in Wikipedia such as hyponym, synon yms and associated terms. In addition, they also employ similar document enrichment strategy Wikipedia and utilize the extracted relations to improve text classification. However, they also treat the hyponym and associative concepts equal with terms in document. 
Wikipedia is a dynamic and fast growing resource  X  articles about newsworthy events are often added within few days of their occurrence [23]. Each article in Wikipedia describes a single topic; its title is a succinct, well-formed phrase that resembles a term in a conventional thesaurus [5]. Meanwhile, each article must belong to at least one category of Wikipedia. Hyperlinks between articles keep many of the same semantic relations as defined in internati onal standard for thesauri [1], such as equivalence relation (synonymy), hierarchical relation (hypernym) and associative relation. However, as an open resource, it inevitable includes mu ch noise. To make it a clean and easy-to-use as a thesaurus, we first preprocess the Wikipedia data to collect Wikipedia concepts, and then explicitly derive relationships between Wikipedia based on the structural knowledge of Wikipedia. 
Each title of Wikipedia articl es describes a topic, and we denote it as a concept. However, some of the titles are meaningless  X  it is only used for Wikipedia ma nagement and administration, such as  X 1980s X ,  X  X ist of newspapers X , .etc. Hence, we first filter Wikipedia titles according to the rules describing below (titles satisfy one of below will be filtered): 
Wikipedia guarantees that there is only one article for each concept by using  X  X edirect X  hyperlink to group equivalent concepts to the preferred one. The  X  X edirect X  link copes with capitalization and spelling variat ions, abbreviations, synonyms, and colloquialisms. Synonymy in Wikipedia mainly comes from these redirect links. As an instance in [6], an example entry with a considerably higher number of redi rect pages is  X  X nited States X . Its redirect pages correspond to acronyms (U.S.A., U.S., USA, US), Spanish translations (Los Estados Unidos, Estados Unidos), misspellings (Untied States) or synonyms (Yankee land). 
In addition, Wikipedi a articles often mention other concepts, which already have corresponding articles in Wikipedia. The anchor text on each hyperlink may be different with the title of the linked article. Thus, anchor texts can be used as another source of synonymy. 
Wikipedia contains a lot of di sambiguation pages, which are created for ambiguous term s, i.e. terms that denote two or more entities. For example, the term  X  X uma X  may refer to either a kind of animal or a kind of racing car or a famous sportswear brand. Therefore, Wikipedia provides di sambiguation pages that present various possible meanings from whic h users could select articles corresponding to their intended concepts. For example, the disambiguation page for the te rm  X  X uma X  lists 22 associated concepts, from persons, ve hicles to sport clubs. 
In Wikipedia, both articles a nd categories can belong to more than one category, i.e. the ar ticle of  X  X uma X  belongs to two categories:  X  X at stubs X  and  X  X elines X . These categories can be further categorized by associating them with one or more parent categories. The category structur e of Wikipedia does not form a simple tree-structured taxonomy but a directed acyclic graph, in which multiple categorization schemes co-exist simultaneously [5]. Thus, Wikipedia category system is not taxonomy with a fully-fledged subsumption hierar chy, but only a thematically organized thesaurus. To extract the real  X  X s a X  relations from Wikipedia categories, we utilize the methods proposed in [18] to derive generic  X  X s a X  relation from category links. Thus, we can get hypernym for each Wikipedia concepts. 
Each Wikipedia article contains a lot of hyperlinks, which express relatedness between them. As Milne et al. [5] mentioned that links often occur between articles that are only tenuously related. For example, comparing the following two links: one from the article  X  X ougar X  to the article  X  X outh America X , the that the former two articles are not as related as the later pair. So, how to measure the relatedness of hyperlinks within articles in Wikipedia is an important issue. Here, we introduce two kinds of measurements to rank links in an article of Wikipedia. Content based measure 
The cosine similarity of article pairs in Wikipedia may reflect the relatedness between the two c oncepts. However the drawback of this measurement is the same as that of BOW approach, since it only considers terms appeared in text documents. We need synthesize other measur ements with this one. Out-linked category based measure Another method to measure the relatedness between a pair of Wikipedia articles is to compare the similarity between out-linked categories of the two ar ticles. Through observation, we found that if two articles share some out-linked categories, the concepts described in these two articles are most likely related. For example, Table 1 shows part of the common out-linked categories shared by  X  X ata mining X ,  X  X achine learning X  and  X  X omputer Network X . Oblivious ly, the category distribution between  X  X ata mining X  and  X  X achine learning X  is more similar than that between  X  X ata mining  X  and  X  X omputer Network X . In Wikipedia, each article has a lot of hyperlinks which point to other related articles in Wikipedia, and each Wikipedia article belongs to at least one category. Thus, for each Wikipedia article article of i c ,and () k cate c denotes the categories Then, we measure the relatedness of articles i c and cosine similarity of i f and j f after weighted with TFIDF. Table 1: Out-link Categories of the article  X  X ata mining X  and Combination of the two measures 
To get an overall relatedness of two Wikipedia concepts, we combine the above two measure using follow equation: Where, content based similarity measure based measure Then, we ranked all the out-linked concepts for each Wikipedia concept using the above equation, and we denote the out-linked concepts with relatedn ess above certain threshold (in our experiments, it is set to 0. 2) as associative ones for each concept. 
In this section, we first describe the traditional text document similarity measure based on  X  BOW  X , and previous text representation enrichment stra tegies. Then, we introduce our framework which integrates hierarchical, synonym and associative relations from our bu ilt Wikipedia thesaurus with traditional text similarity measure. 
Intuitively, if two articles address similar topics, it is highly possible that they share lots of substantive terms, while two irrelevant articles are most likely using different vocabulary therefore seldom share any terms. Thus, the text document can be represented as weighted bag of words. After remove the stopwords and stemmed by stemmer such as Porter stemmer [8], the stemmed terms construct a vector representation text document. Then, TFIDF weighs each term in a document. Finally, we compute semantic re latedness of a pair of text fragments as the cosine similari ty of their corresponding vectors which is defined as 
As introduced in the related works, to break the bottleneck of traditional  X  X OW X  representation, previous approaches enriched text representation with external resources such as WordNet and ODP. We summarize their processes as below: 
First, they generate new features dnew t  X  for each document in the dataset. The features can be synonym or hypernym for document terms as in [1][16][24], or expanded features for terms, sentences and documents as in [2][3]. 
Second, the generated new feat ures replace or append to original document representation d t , and construct new vector representation dext t  X  with TFIDF, the similarity measure of document pairs is defined as 
In this section, we will intr oduce our framework of leveraging the semantic relations in our built Wikipedia thesaurus to enhance traditional content similarity measure for text clustering. 
To use Wikipedia thesaurus to enhance clustering, one of the key issues is how to map terms in text documents to Wikipedia concepts. Considering frequently occurred synonymy, polysemy and hypernymy in text documents, accurate allocation of terms in Wikipedia is really critical in the whole clustering process. 
To facilitate the mapping proces s of phrases in text document to Wikipedia concepts, we build a phrase index which includes the phrases of Wikipedia concep ts , their synonym, and polysemy in Wikipedia thesaurus. Base d on the generated Wikipedia phrases index, all candidate phrases can be recognized in the web page. We use the Forward Maximum Matching algorithm [25] to search candidate phrases, which is a dictionary-based word segmentation approach. It is necessary to do word sense disambiguation to find its most proper meaning mentioned in documents, if a candidate concept is a polysem. Silviu [12] proposed a disambiguation method which augments the Wikipedia category information with Wikipedia pages content, and the implemented system shows high disambiguation accuracy. We adopt Silviu X  X  method to do word sense disambiguation for the polysem concepts in the document. In Wikipedia, each concept belongs to one or more categories, while these categories are furthe r belonged to more higher level categories, forming an acyclic category graph. The set of categories contained in the categ ory graph of a given concept c is category may have several different paths link to a concept. We calculate the distance (, ) i dis c cate by the length of the shortest path from the concept c to the category i cate . 
Sharing a common category indicates that two articles are somehow related. However, it may take several steps to let two articles find their commonly belonged category. Intuitively, those high level categories have less influence than those low level categories since low level categories are more specific and therefore can depict the articles more accurate. We represent the influence of categories on th  X  layer on concept c as levels of categories. Therefore, we have 1 lnf ( ) lnf ( ) cc = . As each Wikipedia concepts has more than one category, and each category has mo re than one parent categories, a big  X  will introduce too many categories. Therefore, we set  X   X  in our experiments. Thus, for each concept c we can build a category vector category ci cate on concept c . For the collection C which contains all the concepts in document d , the corresponding category vector can be calculated as Cc similarity measure using cate gory vectors is defined as: Considering the original document content, the similarity measure can be represented as: where  X  is used to control the importance of cate combined measure. Specifically, when use the category decay factor  X  , Eq. 5 can be rewritten as 
To better relieve BOW shortcomings, synonym and associative relations in Wikipedia can be used to include more related concepts in the similarity measure. For each concept rela c c w c w c w = are selected from its synonym and associative concepts, in which i r c is the i concepts of i c and i r w is the relatedness between c (For synonym 1 measured by Equation 1). Given two articles, we use the two weighted sets of concepts to me asure their similarity. Consider two sets of concepts represent as (0 ) ckn &lt;  X  and (0 ) j b cjm &lt;  X  is Wikipedia concept in the two articles, and , kj ab f f are their corresponding weights. We expand b c with all the related co ncepts of its elements are contained in a c . The expanded weighted concept set is defined as: where c w is calculated by summing up all weighted occurrence of corresponding related concepts. We append the expanded concept set ext C to b C and get the extended b C as: 
Given two concepts sets, we al ways choose the smaller one as C since the expanding procedure always makes the set bigger. And we define the similarity as: sets a C={(CS,1), (ML,1)} and b C ={(DM,1),(DB,1)} (CS  X  Computer Science, ML  X  Machin e Learning, DM  X  Data Mining, DB -Database). We gave the similarity measure of the four concepts in Table 3. We can get the extended set C= {(CS,0.3+0.3),(ML,0.7+0.1)}={(CS,0.6),(ML,0.8)} and C ={(DM,1),(DB,1),(CS,0.6),(ML,0.8)} . The similarity between a C and b C is 0 since they share no common concepts. However, using b -ext C , we get 0.57 simila rity which indicates there is correlation between a C and b C . Data Mining 0.3 0.7 Database 0.3 0.1 
Considering the original document content, the combined similarity measure is defined as: where  X  is used to control the importance of asso S in the combined measure. In the previous sections, we describe the methods to combine Wikipedia hierarchical relation, synonym and associative relation with traditional text document similarity measure. In this section, we incorporate both of them into the similarity measure using a linear combination and it is defined as: 
SSSS  X   X   X  X  X  = X  X  X  + + (11) where  X  and  X  weight the importance of hierarchical relation, synonym &amp; associative relation in the similarity measure, respectively. As text clusteri ng is an unsupervised method , where we do not have labele d data, we cannot tune the parameters with validation data. Thus, in these cases, we can set equal weights 1/3  X   X  == . If users can provide some prior-knowledge or validation data which specifies that some documents be clustered together, the weights for  X  ,  X  can be optimized based on such data.
Our incorporation of Wikipedia background knowledge is independent of the concrete clustering methods. The only requirement we have is that the algorithm could achieve good clustering results in an efficient. K-Means [21] is a widely-used clustering algorithm with good accuracy and efficiency. In our experiments, we use K-Means cl ustering algorithm to evaluate our proposed methods, and we will try to use other clustering algorithms in our future work. 
As an open source project, Wikipedia content is easily http://download.wikipedia.org. It is available in the form of database dumps that are releas ed periodically. The version we used in our experiments was re leased on Sep. 9, 2007. We identified over four million distinct entities (articles and redirections) that constitute th e vocabulary of thesaurus. These were organized into 127,325 categories with an average of two subcategories and 26 articles each. The articles themselves are highly inter-linked; each links to an average of 25 others. After filtering Wikipedia concepts as described in Sec 3.1, we got 1,614,132 concepts. 
Reuters-21578[10] is a news corpus containing 11,367 manually labeled documen ts classified into 82 clusters, with 9494 documents uniquely labeled. We filter those clusters with less than 15 documents or more than 200 documents, leaving 30 clusters comprising of 1,658 documents. 
OHSUMED[11] is a subset of MEDLINE containing 348,566 medical documents, about two-thirds of which (233,445) also have an abstract. We choose a s ubset of 18,302 abstracts which are classified into 23 categories, with each category contains from 56 to 2,876 abstracts. 
The purity measure based on prec ision measure in information retrieval field is applied to ev aluate the performance of our strategies. Both purity and inverse purity are calculated to collaboratively depict the accuracy of the clustering result. Given a data set D , 1 2, ... ,, n M MM M = represent the n manually generated using our algorithm. defined as: 
They collaboratively measure the accuracy of the clustering result. Though purity and invers e purity are often positively correlated, they do not always get their peak at the same point. In this case, we choose the point with the highest sum of purity and inverse purity as the global optimal. 
In our experiments, each eval uation result described in the following denotes an average from 10 test runs performed on given corpus for a given combination of parameters with randomly chosen initial values for K-Means. We also applied t-tests to check for significance with a confidence of 99%. We used three baselines for comparing: The first one is K-Means clustering with traditi onal text document similarity measure  X  we denote it as BASE1 ; The second one is K-Means clustering with document representation improved with Gabrilovich X  X  feature generation technique on Wikipedia [2]  X  we denote it as BASE2 ; The third one is K-Means clustering with Hotho X  X  text document representation enrichment with WordNet [1]  X  we denote it as BASE3 . In this section, we first introduce the overall experimental performance. 
We conducted severa l experiments: BASE1, BASE2, BASE3, transitional text similarity measure with hierarchical relations ( HR ), transitional text similarity measure relations with synonym and associative relations ( SAR ), and the combination of HR and SAR ( COB ). For BASE1, BASE2 and BASE3, We select parameters according to their best performance setting in our experiments. For HR, SAR, and COB , we fixed several parameters for the rest experiment s. i.e. associative concepts ranking of Wikipedia thesaurus, and  X  = . As it is suppose to be no validation data for document clustering, we use the average weight for  X  and  X  on both Reuters and OHSUMED datasets. The experimental performance of these six methods is summarized in Table 3. Purity and Inverse in the table represent the purity and inverse purity measures for each method and Impr represents the average improvement of purity and impur ity measures compared with BASE1 . Table 3: Baseline,HR, SAR and COB performance results 
From the performance results in Table 3, we can see that within the three baseline algorit hms, both Gabrilovich X  X  feature generation and Hotho X  X  WordNet enrichment can improve clustering performance compared with traditional BOW representation. However, Gabr ilovich X  X  method can only get 0.53% and 3.17% improvement on Reuters and OHSUMED compared with BASE1 , which is less than Hotho X  X  BASE3 ( 1.43% and 4.72% ) improvement and far less than our COB method ( 9.28% and 9.77% ) improvement. As we use the same weights for hierarchical relation and synonym &amp; associative relation, the result indicates that Wikipedia thesaurus can improve text clustering performance. 
Comparing our proposed HR, SAR, and COB experiment results, we find that hierarchical relation, synonym &amp; associative relation and their combination improve clustering performance on both Reuters and OHSUMED data sets -( 0.36%, 8.57%, 9.28%) vs (12%, 5.23%, 9.77%) improvement. Reuters benefits from combing both relations  X  9.28% improvements in COB , in which synonym &amp; associative relation plays a more importance role than hierarchical relations (HR only gets 0.36% improvement). However, for OH SUMED, hierarchical category relations has already impr oved the performance a lot ( 12% improvements) so that adding sy nonym and associative relations do not offer more help in the COB experiment  X  the improvement is even decreased from 12% to 9.77%. More specifically, we can see that adding synonym and associative relations contributes more in the clustering of the Reuters data set while adding hierarchical relations are more helpful in the OHSUMED data. This can been explained from the fact that OHSUMED are professional medicine articles that the implicit hyponymy can be extracted only through hierarchical category relations of Wikipedia, while Reuters are more general news articles often varying words and phrases therefore adding related concepts is more helpful in this case. It also indicates that for different kinds of datasets, the effect of the hierarchical relation and synonym &amp; associative re lation is various, and their combination can take advantages their both generally. 
In previous experiments, we gi ve the same fixed values for and  X  , which means we treat hyponym and associative concepts are equal important. However, as shown in Table 3, for different datasets, when computing the sim ilarity between document pairs, the original document content, hyponym and associative concepts should have differ ent importance. As we know, clustering is unsupervised, and it does not need labeled training data. But it does not mean that users should not provide prior-knowledge or validation data which specifies that some documents be clustered together . Although it is laborious for users to label a large amount of data, it is worthwhile if the clustering performance can be greatly improved when users can provide some labeled data fo r optimizing some important parameters. In the following experiments, we will evaluate the clustering performance when usi ng different amount of labeled data to optimize parameters  X  and  X  . 
Figure 1 and 2 show the purity and inverse purity curve of our algorithm under different number of labeled validation data on Reuters and OHSUMED. The Axis-x of the two figures denotes the percentage of documents with label in Reuters or OHSUMED used for optimizing parameters  X  and  X  . From the two figures, we can see that, as we expected, with the increase of labeled tuning documents, the purity and inverse purity increase gradually. Especially when the number of labeled tuning data increases from 0 to 5%, the clustering performance improved a lot  X  On OHSUMED, purity (inve rse purity) increases from 0.449 to 0.467 (0.381 to 0.392), which means the clustering performance can be gr eatly improved if users can provide a small amount of labeled data for tuning parameters  X  and  X  .When the tuning data increases from 10% to 20%, the curve of purity and inverse purity still goes up, but not as quick as the one from 0 to 5%. If we still increase the labeling data (from 20% to 50%), the curve becomes quite stable, wh ich indicates that 20% of data is sufficient for tuning optimal weights for hierarchical and associative relations. Table 4 summarizes the best results we get on Reuters and OHSUMED after us ing optimized weights. As shown in Table 4, based on the optimal weights, we can get 0.697 purity and 0.636 inverse purity on Reuters (16.2% improvement compared with BASE1 ), and 0.485 purity 0.414 inverse purity on OHSUMED ( 18.8% improvement compared with BASE1 ). 
Figure 1: Impact of tuning document number in Reuters for Figure 2: Impact of tuning document number in OHSUMED Table 4: Clustering performance using optimized weights 
To better understand the reasons why our approach works better than Gabrilovich X  X  fe ature generation and Hotho X  X  WordNet enrichment, we analyze the generated features of the three methods for the Reuters document #15264, and summarize part of their generated features in Table 5. The left part of  X -&gt; X  is the original word (term or phrase) in the document used for generating new features, and the righ t part of it is the generated new features ( S denotes Synonym, H denotes Hypernym, and A denotes associative terms). Compar ing the generated features of Hotho X  X  WordNet enrichment and our Wikipedia based approach in Table 5, we find that words used for generating features in Hotho X  X  method are single terms and most of them are polysemy; While our approach generates fe atures not only for single terms but also for phrases, and most of them are name entities not covered by WordNet. Due to rich context provided by Wikipedia, our disambiguation module can fi nd the proper meaning for most of polysemy in document  X  TECK can be mapped to Teck Cominco -a mining company in Canada, which is directly related to the topic of the document (mining). Although Gabrilovich also utilizes Wikipe dia to generate features, the classification-based feature genera tion approach brings a lot of noise features, and it also does n X  X  contain hypernym features which are proven quite useful in finding sharing topics between document pairs. 
Table 5: Generated features of Hotho X  X , Gabrilovich X  X  and 
As mentioned in Sec. 3.5, we adopt two ways to measure the associative relatedness between Wikipedia concepts. Here we introduce the method to tune 10 Wikipedia concepts randomly, and then extract all the out-linked concepts in the Wikipedia articles corresponding to the 10 concepts. To obtain high quality ground truth for tuning, we asked three assessors to manually label all the linked concepts in the 10 articles to three relevance levels (relevant -3, neutral -2, and not relevant -1). The labeling process was carried out independently among assessors w ho are graduate students and have good command of the English. No one among the three assessors could access the labeling results of others. After labeling, each out-linked concept in the 10 articles is labeled with 3 relevance tags, and we use average value as the final relatedness value. Based on the labeled data, we calculate TFIDF similarity and out-linked catego ry similarity between the 10 concepts and their out-linked concepts. We tune the value of  X  , with which the result of linear combination matches the user evaluation result best. From experiments, 
Other parameter that we pay special attention to category decay factor  X  we used in similarity measure combining hierarchical relations. We con duct extensive experiments on different parameter settings and 0.5  X  = always show the best results. Therefore, we set 0.5  X  = in our experiments. 
Wikipedia is a huge resource of encyclopedia knowledge which contains a lot of name entities that are widely used in our daily life. But it is not structured as WordNet and cannot be used for other application directly. Th erefore, we first proposed a way to mine synonym, hypernym and associative relations explicitly for each concept through analyzing the rich links in Wikipedia, and build it as an easy-to-use thesaurus. Then, we introduce a framework to integrate the hierarchical, synonym, and associative relations in built Wikipedia thesaurus to traditional text similarity measure to facilitate document clustering. The text clustering experiments on two datasets indicate that with the help of our built Wikipedia thesaurus, the clustering performance of our method is improved compared with previous methods. Meanwhile, with the optimized parameters based on a few labeled data users provide, the clustering performance can be further improved -16.2% and 18.8 improvement compared with the baseline on Reuters and OHSUMED, respectively. 
For the future work, as Wikipedia provide a flexible, detailed, dynamic-updated category system, it provides a good way to organize the tremendous Web cont ent. Also, the Wikipedia is multilingual encyclopedia; it provides more than 20 languages with rich inter-links betwee n them. Thus, we can use the multilingual relations to explore the application in Cross-language Information Retrieval and Cross-language Text Categorization, which are our next step works. Qiang Yang would like to thank the support of Hong Kong RGC Project 621307. We thank the anonymous reviewers for their useful comments [1] A. Hotho, S. Staab and G. Stumme. Wordnet improves text [2] E. Gabrilovich and S. Mar kovitch. Feature Generation for [3] E. Gabrilovich and S. Markovitch. Overcoming the [4] E. Gabrilovich and S. Mar kovitch. Computing Semantic [5] D. Milne, O. Medelyan and I. H. Witten. Mining Domain-[6] R. Bunescu and M. Pasca. Using encyclopedic knowledge [7] M. Strube and S. P. Ponz etto. WikiRelate! Computing [8] M. F. Porter. An algorithm for suffix stripping. Program, [9] E. Agirre and G. Rigau. A Proposal for Word Sense [10] Reuters-21578 text categorization test collection, [11] W. Hersh, C. Buckley, T. Leone and D. Hickam. [12] K. Lang. Newsweeder: Learning to filter netnews. In [13] T. Joachims. Text categoriz ation with support vector [14] F. Sebastiani. Machine learning in automated text [15] Y. Yang and X. Liu. A re-examin ation of text categorization [16] M. de Buenaga Rodr  X  guez, J. M. G. Hidalgo, and B. D  X  az-[17] D. M. P. Kushal Dave, Steve Lawrence. Mining the peanut [18] Ponzetto, Simone Paolo; Str ube, Michael (2007). Deriving a [19] Miller, G. (1995). WordNet: A lexical database for english. [20] Open Directory Proj ect, http://dmoz.org [21] Steinbach, M., Karypis, G. , &amp; Kumar, V. (2000). A [22] Pu Wang, Jian Hu, .etc. Impr oving text categorization by [23] Wikipedia, http://en.wikipedia.org/wiki/Wikipedia:About [24] Ure  X na L  X oez, M., &amp; Hidalgo, J. M. G. (2001). Integrating [25] Wong, P. and Chan, C., Chinese word segmentation based 
