 While it seems intuitive to treat certain sequences of tokens as single terms, there is still consider-able controversy about the definition of what ex-actly such a multiword expression (MWE) con-stitutes. Sag et al. (2001) pinpoint the need of treating MWEs correctly and classify a range of syntactic formations that could form MWEs and define MWEs as being non-compositional with re-spect to the meaning of their parts. While the exact requirements on MWEs is bound to specific tasks (such as parsing, keyword extraction, etc.), we op-erationalize the notion of non-compositionality by using distributional semantics and introduce a new measure that works well for a range of task-based MWE definitions.

Most previous MWE ranking approaches use the following mechanisms to determine multiwordness: part-of-speech (POS) tags, word/multiword frequency and significance of co-occurrence of the parts. In this paper we do not want to introduce  X  X et another ranking function X  but rather an additional mechanism, which performs ranking based on distributional semantics.

Distributional semantics has already been used for MWE identification, but mainly to discriminate between compositional and non-compositional MWEs (Schone and Jurafsky, 2001; Salehi et al., 2014; Hermann and Blunsom, 2014). Here we introduce a new concept to de-scribe the multiwordness of a term by its unique-ness . Using the uniqueness score we measure how likely a term in context can be replaced by a single word. This measure is motivated by the semiotic consideration that due to parsimony con-cepts are often expressed as single words. Further-more, we implement a context-aware punishment, called incompleteness , which degrades the score of terms that seem incomplete regarding their con-texts. Both concepts are combined into a single score we call DRUID , which is calculated based on a distributional thesaurus. In the following, we show the impact of that new method for French and English and also examine the effect of cor-pus size on MWE extraction. Additionally, we report on results without using any linguistic pre-processing except tokenization. The generation of MWE dictionaries has drawn much attention in the field of Natural Lan-guage Processing (NLP). Early computational approaches (e.g. Justeson and Katz (1995)) use POS sequences as MWE extractors. Other ap-proaches, relying on word frequency, statistically verify the hypothesis whether the parts of the MWE occur more often together than would be expected by chance (Manning and Sch  X  utze, 1999; Evert, 2005; Ramisch, 2012). One of the first measures that consider context information (co-occurrences) are the C-value and the NC-value in-troduced by Frantzi et al. (1998). These meth-ods first extract candidates using POS information and then compute scores based on the frequency of the MWE and the frequency of nested MWE candidates. The method described by Wermter and Hahn (2005) computes a score by multiplying the frequency of a candidate when placing wild-cards for each word. A newer method is intro-duced in Lossio-Ventura et al. (2014), which re-ranks scores based on an extension of the C-value, which uses a POS-based probability and an inverse document frequency. Using different measures and learning a classifier that predicts the multi-wordness was first proposed by Pecina (2010), who, however, restricts his experiments to two-word MWEs for the Czech language only. Ko-rkontzelos (2010) comparatively evaluates several MWE ranking measures. The best MWE extrac-tor reported in his work is the scorer by (Naka-gawa and Mori, 2002; Nakagawa and Mori, 2003), who use the un-nested frequency (called marginal frequency) of each candidate and multiply these by the geometric mean of the distinct neighbor of each word within the candidate.

Distributional semantics is mostly used to de-tect compositionality of MWEs (Salehi et al., 2014; Katz and Giesbrecht, 2006). Most ap-proaches therefore compare the context vector of a MWE with the combined vectors based on the constituent words of the MWE. The similarity be-tween the vectors is then used as degree for com-positionality. In machine translation, words are sometimes considered as multiwords if they can be translated as single term (cf. (Bouamor et al., 2012; Anastasiou, 2010)). Whereas this follows the same intuition as our uniqueness measure, we do not require any bilingual corpora.

Regarding the evaluation, mostly precision at k ( P @ k ) and recall at k ( R @ k ) are applied (e.g. (Ev-ert, 2005; Frantzi et al., 1998; Lossio-Ventura et al., 2014)). Another general approach is using the average precision (AP), which is also used in In-formation Retrieval (IR) (Thater et al., 2009) and has also been applied by Ramisch et al. (2012). We will evaluate our method by comparing our MWE ranking to multiword lists that have been annotated in corpora. Here, we introduce an up-per bound and two baseline methods and give a brief description of the competitive methods used in this paper. Most of these methods require a list of candidate terms T , usually extracted with POS sequences (see Section 5). 3.1 Upper Bound We use a perfect ranking as upper bound, where we rank all positive candidates before all negative ones. 3.2 Lower Baseline and Frequency Baseline The ratio between true candidates and all candi-dates serves as lower baseline, which is also called baseline precision (Evert, 2008). The second base-line is the frequency baseline, which ranks can-didate terms t  X  T according to their frequency f req ( t ) . 3.3 C-value/NC-value The commonly used C-value (see Eq. 1) was de-veloped by Frantzi et al. (1998). The first fac-tor, logarithm of the term length in words, favors longer MWEs. The second factor is the frequency of the term reduced by the average frequency of all candidate terms T , which nest the term t , i.e. t is a substring of the terms we denote as T t . c( t ) = log 2 ( | t | )(freq( t )  X  An extension of the C-value was proposed by Frantzi et al. (1998) as well and is named NC-value. It takes advantage of context words C t by assigning weights to them. As context words only nouns , adjectives and verbs are considered 1 . Con-text words are weighted with Equation 2, where k denotes the number of times the context word c  X  C t occurs with any of the candidate terms. This number is normalized by the number of can-didate terms. The NC-value is a weighted sum of the C-value and the product of the term t occurring with each context c which form the term t c : nc( t ) = 0 . 8  X  c( t ) + 0 . 2 3.4 t-test The t-test (see e.g. (Manning and Sch  X  utze, 1999, p.163)) is a statistical test for the significance of co-occurrence of two words. It relies on the proba-bilities of the term and its single words. The prob-ability of a word p ( w ) is defined as the frequency of the term divided by the total number of terms of the same length. The t-test statistic is computed using Equation 4 with f req ( . ) being the total fre-quency of unigrams. t ( w 1 . . . w n )  X  We then use this score to rank the candidate terms. 3.5 FGM Score Another method inspired by the C/NC-value is proposed in (Nakagawa and Mori, 2002; Naka-gawa and Mori, 2003). The method was developed on a Japanese dataset and outperformed a modi-of two scoring mechanisms for the candidate term t as shown in Equation 5.
 The first term in the equation is a geometric mean GM ( . ) of the number of distinct direct left l ( . ) and right r ( . ) neighboring words for each single word t within t .
 The neighboring words are extracted directly from the corpus; the method does neither rely on can-didate lists nor POS tags. To the contrary, the marginal frequency M F ( t ) relies on the candi-date list and the underlying corpus. This frequency counts how often the candidate term occurs within the corpus and is not a subset of a candidate. In Korkontzelos (2010) it was shown that while scor-ing according to Equation 5 leads to comparatively good results, it is consistently outperformed by MF only. We present two new mechanisms relying on a Dis-tributional Thesaurus (DT), which we use to rank terms regarding their multiwordness: A score for the uniqueness of a term and a punishing score that conveys the incompleteness . 4.1 Similarity Computation The DT is computed based on Biemann and Riedl (2013). First we extract n-grams from text and consider the left and the right neighbor of each n-gram as context feature. Then, we calculate the Lexicographer X  X  Mutual Information (LMI) sig-nificance score (Bordag, 2008) between n-grams and features and remove all context features, which co-occur with more than 1000 terms, as these features tend to be to general. In the next step we keep for each n-gram only the 1000 con-text features, with the highest LMI score. The similarity score is then computed based on the overlap of features between two terms. Due to pruning this overlap-based significance measure is proportional to the Jaccard similarity measure, al-beit we do not consider any normalization. After computing the feature overlap between two terms, we keep for each n-gram the 200 most similar n-grams. An example for the most similar n-grams to the terms red blood cell and red blood including their feature overlap are shown in Table 1. 4.2 Uniqness Computation The first mechanism of our MWE ranking method is based on the following hypothesis: n-grams, which are MWE, could be substituted by sin-gle words, thus they have many single words amongst their most similar terms. This is moti-vated by semiotic considerations: Because of par-simony, concepts are usually expressed in single words. When a semantically non-compositional word combination is added to the vocabulary, it expresses a concept that is necessarily similar to other concepts. Hence, if a candidate multiword is similar to many single word terms, this indicates multiwordness.

To compute the uniqueness score (uq) of an n-gram t , we first extract the n-grams it is simi-lar to using the DT as described in Section 4.1. The function similarities ( t ) returns the 200 most similar n-grams to the given n-gram t . We then compute the ratio between unigrams and all simi-lar n-grams considered using the formula: We illustrate the computation of our measure based on the MWE red blood cell and the non-MWE red blood . When considering only the ten most similar entries for both n-grams as illustrated in Figure 1, we observe an uniqueness score of 7 / 10 = 0 . 7 for both n-grams. If considering the Table 1: We show the ten most similar entries for the term red blood cell (left) and red blood (right). Here, seven out of ten terms are single words. top 200 similar n-grams, which are also used in our experiments we will obtain 135 unigrams for the candidate red blood cell and 100 unigrams for the n-gram red blood . We will use these counts for showing the workings of the method in the re-mainder. 4.3 Incompleteness Computation Similar to the C/NC-value method, we also as-sign a context weighting function that punishes incomplete terms, which we call incompleteness (ic) . For this function we extract the 1000 most significant context features using the function context ( t ) , which yields tuples of left and right contexts. These context features are the same that are used for the similarity computation in Section 4.1 and have been ranked according to the LMI measure. For the example term red blood , some of the contexts are  X  extravasated, cells  X  ,  X  uninfected, cells  X  ,  X  nucleated, corpuscles  X  . In the next step we split each tuple to its left and right word in-cluding its relative position (left/right) to the can-didate term. Using the first context feature results in:  X  extravasated, left  X  ,  X  cells, right  X  . Then, we sum up the occurrences of for each single context, as shown in Table 2 for the two terms.

We subsequently select the maximal count and normalize it by the counts of features | context ( t ) | considered, which is 1000. This results into the incompleteness measure ic ( t ) . For our example terms we achieve the values ic ( red blood ) = 557 / 1000 and ic ( red blood cell ) = 48 / 1000 . Whereas the uniqueness scores for the most simi-lar entries were equal, we now have a measure that indicates the incompleteness of an n-gram, with higher scores indicating more incomplete terms. Table 2: Top three most frequent context words for the term red blood cell and red blood in the Medline corpus. 4.4 Combining Both Measures As shown in the previous two sections, a high uniqueness score indicates the multiwordness and a high incompleteness score should decrease the overall score. In experiments, we found the best score from the uniqueness score. This mechanism is inspired by the C-value and motivated as terms that are often preceded/followed by the same word do not cover the full multiword expression and need to be downranked. This leads to Equation 8, which we call D ist R ibutional U niqueness and I ncompleteness D egree: Applying the DRUID score to our example terms (considering the 200 most similar terms) we will achieve the scores DRUID ( red blood cell ) = 135 / 200  X  48 / 1000 = 0 . 627 and DRUID ( red blood ) = 100 / 200  X  557 / 1000 =  X  0 . 057 . As a higher DRUID score indicates the multiwordness of an n-gram, we can summarize that the n-gram red blood cell is a better MWE than the n-gram red blood . We examine two experimental settings: First, we compute all measures on a small corpus that has been annotated for MWEs, which serves as the gold standard. In the second setting we compute the measures on a larger in-domain corpus. The evaluation is again performed for the same candi-date terms as given by the gold standard. Results for the top k ranked entries are reported using the precision at k ( P @ k = 1 1 if the i -th ranked candidate is annotated as MWE and 0 otherwise). For an overall performance we use the average precision (AP) as defined in Thater et al. (2009): AP = 1 T mwe beeing the set of positive MWE. When fac-ing tied scores we mix false and true candidates randomly cf. Cabanac et al. (2010). 5.1 Corpora For the experiments we consider two annotated (small) corpora and two unannotated (large) cor-pora. 5.1.1 GENIA corpus and SPMRL 2013: In the first experiments we use two small anno-tated corpora that serve the gold standard MWEs. We use the medical GENIA corpus (Kim et al., This corpus has annotations regarding important and biomedical terms. Also single terms are anno-tated in this data set, which we ignore.
 The second small corpus is based on the French Treebank (Abeill  X  e and Barrier, 2004), which was extended for the SPMRL task (Seddah et al., 2013). This version of the corpus also contains compounds annotated as MWEs. In our experi-ments we use the training data, which covers 0.4 million words.

Whereas the GENIA MWEs target term match-ing and medical information retrieval, the SPMRL MWEs mainly focus on improving parsing through compound recognition. 5.1.2 Medline Corpus and Est R  X  epublican In a second experiment the scalability to larger corpora is tested. For this, we make use of the en-billion words. The Est R  X  epublican Corpus (ERC) It consists of local French news from the east-ern part of France and comprises of 150 million words. 5.2 Candidate Selection In the first two experiments, we use POS filters to select candidates. We concentrate on filters that extract noun MWEs and avoid further pre-processing like lemmatization. We use the filter English medical datasets. Considering only terms that appear more than ten times leads to 1,340 can-didates for the GENIA dataset and 29,790 can-didates for the Medline dataset. According to Table 3 we observe that most candidates are bi-grams. Whereas for both corpora still around 20% of trigrams are contained, the number of 4-grams is only marginally represented. For the French datasets we apply the filter proposed by Daille nal MWEs. Applying the same filtering as for the medical corpora leads to 330 candidate terms for the SPMRL and 7,365 candidate terms for the ERC. Here the ratio between bi-and trigrams is more balanced but again the number of 4-grams constitutes the smallest class.
 Table 3: Number of candidates after filtering for the expected POS-tag and their distribution over n-grams with n  X  X  1 , 2 , 3 , 4 } .

In comparison to the Medline dataset, the ratio of multiwords extracted by the POS filter on the French corpus is much lower. The reason for that property is that in the French data, many adverbial, prepositional MWEs are annotated, which are not covered by the POS filter.

The third experiment shows the performance of the method in absence of language-specific pre-processing. Thus, we only filter the candidates by frequency and do not make use of POS filtering. As most previous methods rely on POS-filtered data we cannot make use of them in this setting.
For the evaluation, we compute the scores of the competitive methods in two settings: First, we compute the scores based on the full candidate list without any frequency filter and prune low-frequent candidates only for the evaluation (post-prune). In the second setting we filter candidates according to their frequency before the computa-tion of scores (pre-prune). This leads to differ-ences for context-aware measures, since in the pre-pruned case, a lower number of less noisier con-texts is used. 6.1 Small Corpora Results First we show the results based on the GENIA cor-pus (see Table 4). Almost all competitive methods beat the lower baseline. The C/NC-value perform best when the pruning is done after a frequency filter. In line with the findings of Korkontzelos (2010) and in contrast to Frantzi et al. (1998) the AP of the C-value is slightly higher than for the NC-value. All the FGM based methods except the GM measure alone outperform the C-value. The results in Table 4 indicate that the best compet-itive system is the post-pruned FGM system as it has much higher average precision scores and misses only 50 MWEs in the first 500 entries. A slightly different picture is presented in Figure 1 where the P @ k scores against the number of can-didates are plotted. Here DRUID performs well for the top-k list for small k, i.e. finds many good MWEs with high confidence thus combines well with MF, which extends to larger k, but places too much importance of frequency when used alone. Common errors are frequent chunks such as  X  X n patience X , see Table 9 in Section 7. Whereas for the post-pruned case FGM scores higher than MF, the inverse is observed when pre-pruning. Us-ing our DRUID methods can surmount the FGM method only for the first 300 ranked terms (see Figure 1 and Table 4). Multiplying the logarith-mic frequency to the DRUID , the results improve slightly and the best P @100 with 0.97 is achieved. All FGM results are outperformed when combin-ing the post-pruned FGM scores with our measure. According to Figure 1 this combination achieves high precision for the first ranked candidates and still exploits the good performance of the post-pruned FGM based method for the middle-ranked candidates.

Different results are achieved for the SPMRL dataset as can be seen in Table 5. Whereas the pre-pruned C-value again receives better results than frequency, it scores below the lower base-line. Also the post-pruned FGM and MF method
Table 5: Results for the French SPMRL dataset do not exceed the lower baseline. Data analysis revealed that for the French dataset only ten out of the 330 candidate terms are nested within any of the candidates. This is much lower than the 637 terms nested in the 1340 candidate terms for the GENIA dataset. As both the FGM-based methods and the C/NC-value heavily rely on nested can-didates, they cannot profit from the candidates of this dataset and achieve similar scores as ordering candidates according to frequency. Comparing the baselines to our scoring method, this time we ob-tain the best result for DRUID without additional factors. However, multiplying DRUID with MF or log(frequency) still outperforms the other methods and the baselines. 6.2 Large Corpora Results Most MWE evaluations have been performed on rather small corpora. Here we want to inspect the performance of the measures for large corpora, so as to realistically simulate a situation where the MWEs should be found automatically for an entire domain.

Using the Medline corpus, all methods except the GM score outperform the lower baseline and the frequency baseline (see Table 6). Regarding Table 6: Results computed on the Medline corpus. the AP the best results are obtained when combin-ing our DRUID method with the MF, whereas for P @100 and P @500 the log-frequency weighted DRUID scores best. Using solely the DRUID method or the combined variation with the log-frequency lead to the best ranking for the first 1000 ranked candidates and is then outperformed by the MF based DRUID variations. In this exper-iment the C-value achieves the best performance from the competitive methods for the P @100 and P @500 , followed by the t-test. But the highest AP is reached with the post-pruned MF method, which also outperforms the sole DRUID slightly. Contrary to the GENIA results, the MF scores are consistently higher than the FGM scores.

When using the French ERC we figured out that no nested terms are found within the candidates. Thus, the post-and pre-pruned settings are equiv-alent and thus MF equals frequency. The best re-sults are again obtained with our method with and without the logarithmic frequency weighting (see Table 7). Again the AP of the C-value and most
Table 7: Results computed based on the ERC. of the FGM-based methods are inferior to the fre-quency scoring. Only the t-test and the MF are trast to the results based on the smaller SPMRL dataset, the MF, FGM and C-value can outperform the lower baseline. In comparison to the smaller corpora, the performance for the larger corpora is much lower. Especially low-frequent terms in the small corpora that have high frequencies in the larger corpora have not been annotated as MWEs. 6.3 Results without POS Filtering In the last experiment, we apply our method to candidates without any POS filtering and report results using a frequency threshold of ten. As the competitive methods from the previous section rely on POS tags, we use the t-test for compari-son. Analysis revealed that the top-scored candi-Table 8: Results without linguistic pre-processing. dates according to the t-test begin with stop words. As an additional heuristic for the t-test, we shift MWEs, which start or end with one of the ten most frequent words, to the last ranks. For the smaller dataset the best results are achieved with the sole DRUID (see Table 8) and the frequency weight-ing does not seem to be beneficial, as highly fre-quent n-grams ending with stopwords are ranked higher in absence of POS filtering. This, how-ever, is not observed for larger corpora. Here the best results for Medline are achieved with the frequency weighted DRUID . Whereas for French, the sole DRUID method performs best, the differ-ence between the DRUID and the log-frequency-weighted DRUID is rather small. The low APs throughout can be explained by the large number of considered candidates. The second best scores are achieved with stop word based t-test (t-test + sw). C-value performs en par with frequency. 6.4 Components of DRUID Here, we show different parameters for DRUID , relying on the English GENIA dataset without POS filtering of MWE candidates and by consid-ering only terms with a frequency of 10 or more. Inspecting the two different components of the DRUID measure (see upper graph in Figure 2), we observe that the uniqueness measure contributes most to the DRUID score. The main effect of the incompleteness component is the downranking of a rather small number of terms with high unique-ness scores, which improves the overall ranking. We can also see that for the top ranked terms the negative incompleteness score does not improve over the frequency baseline but outperforms the frequency in the middle ranked candidates. Used in DRUID we observe a slight improvement for the complete ranking. We achieve a P@500 of 0.474 for the uniqueness scoring and 0.498 for the DRUID score.

When filtering similar entries, used for the uq scoring, by their similarity score (see bottom graph in Figure 2), we observe that the amount of similar n-grams considered seems to be more im-portant then the quality of the similar entries: With the increasing filtering also the quality of extracted candidate MWEs diminishes. The experiments confirm that our DRUID mea-sure, either weighted with the MF or alone, works best across two languages and across different cor-Figure 2: Results for the components of the DRUID measure (top) and for different filtering thresholds of the similar entries considered for the uniqueness scoring (bottom). pus sizes. It also achieves the best results in ab-sence of POS filtering for candidate term extrac-tion. The optimal weighting of DRUID depends on the nestedness of the MWEs: Using DRUID with the MF should be used when there are more than 20% of nested candidates and using the log-frequency or no frequency weighting when there are almost no nested candidate terms.

We show the best-ranked candidates obtained with our method and with the best competitive method in terms of P @100 for the two smaller corpora. Using the GENIA dataset, our log-frequency based DRUID (see left column in Table 9) ranks only true MWE within the 15 top-scored candidates.

The right-hand side shows results extracted with the pre-pruned MF method that yields three non-MWE terms. Whereas that could be a POS error, Table 9: Top ranked candidates from the GENIA dataset using our method (left) and the competitive method (right). Each term is marked, whether the term is an MWE (1) or not (0). the MF and also the C-value are not capable to re-move terms starting with stop words. The DRUID score alleviates this problem with the uniqueness factor. For the French dataset our method ranks only one false candidate whereas the MF (post-pruned) ranks eight non-annotated candidates in the top 15 list (see Table 10).
 Table 10: Top ranked candidates from the SPMRL dataset for the best DRUID method (left) and the best competitive method (right). Each term is marked, if it is an MWE (1) or not (0).

Whereas the unweighted DRUID method scores better than its competitors on the large corpora, the best results are achieved when using DRUID with frequency-based weights on the smaller cor-pora. For a direct comparison we evaluated the small and large corpora using an equal candidate set. We observed that all methods computed on the large corpora achieve slightly inferior results than when computing them using the small cor-pora. Data analysis revealed that we would con-sider many of the high ranked  X  X alse X  candidates as MWE.

Therefore we extracted the top ten ranked terms, which are not annotated as MWE from the meth-ods with the best P @100 performance, resulting to the log(freq) DRUID and the pre-pruned C-value methods.

First, we observed that the first  X  X alse X  candidate for our method appears at rank 26 and at rank 1 for the C-value. Additionally, only ten out of the top 74 candidates are not annotated as MWEs for our method and 48 for the competitor. When search-find seven terms ranked from our method and two for the competitive method. Uniqueness is a new mechanism in MWE model-ing. Whereas frequency and co-occurrence have been captured in many previous approaches (see Manning and Sch  X  utze (1999), Ramisch et al. (2012) and Korkontzelos (2010) for a survey), we boost multiword candidates t by their grade of dis-tributional similarity with single word terms. We implement such contextual substitutability with a model where the term t can consist of multiword tokens and similarity is measured based on the right and neighboring word between all (single and multiword) terms. Since it is the default to ex-press concepts with single words, a high unique-ness score is given to multiwords that belong to a category just as single words would. E.g. for an English open-domain corpus hot dog is most similar to the terms: food , burger , hamburger , sausage and roadside . Candidates with a low number of single word similarities also serve the same function, but more frequently we observe single n-grams with function words or modifying adjectives concatenated with content words, i.e. small dog is most similar to  X  various cat  X ,  X  large amount of  X ,  X  large dog  X ,  X  certain dog  X ,  X  dog  X . To be able to kick in, the measure requires a certain minimum frequency for candidates in order to find enough contextual overlap with other terms. Ad-ditionally, we also demonstrate effective perfor-mance on larger corpora and show its applicability when used in a complete unsupervised evaluation setting. This work has been supported by the German Fed-eral Ministry of Education and Research (BMBF) within the context of the Software Campus project LiCoRes under grant No. 01IS12054 and the Deutsche Forschungsgemeinschaft (DFG) within the SeMSch project.

