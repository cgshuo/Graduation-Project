 In this paper, we apply met hods from educational testing to measure the reliability of an IR collection. H.3.4 [ Information Storage and Retrieval ]: Systems and Software  X  Performance evaluation (efficiency and effectiveness) Measurement, Performance, Experimentation, Standardization Evaluation, Generalizability Theory, Test Theory, Test Collection In IR and other engineering fields, we subject algorithms to standard test cases. This allows direct comparison between methods. But the question arises , how do we know that these particular test results are a fair basis for drawing general conclusions about the algorithms X  performance? A similar question arises in the educationa l context, where students (rather than algorithms) are evaluated on the basis of standardized tests such as college entrance exams or ordinary class exams. The field of educational testing has devel oped measures of test reliability. Even better, the methods of Generalizability Theory can be used in advance, to construct a more reliable test. We propose that IR and other engineering communities can benefit from using those methods to evaluate and plan our test collections. The most widespread approach to addressing the question involves hypothesis testing. The que stion is framed in terms of a difference of means: What is the probability ( p -value) that the observed performance differen ce between two systems (or conditions, etc.) reflects a difference in their true means? Individual researchers employ this method when reporting individual results, and testbed desi gners may use this approach to investigate the testbeds X  general reliability. Voorhees [4][5] has used a data-driven version of th is approach, which empirically measures the sensitivity of perfo rmance rankings to the choice of queries, number of queries, and ot her features of the test. The weakness of such a data-driven experiment is that it lacks theoretical meaning, and so cannot be interpreted or generalized. Its strength is that it makes no distributional assumptions. Test Theory is a well-founded statistical theory that makes no distributional assumptions, and so it arguably has the best of both worlds. In Test Theory, there is no concept of an invisible mean and no concept of hypothesis tes ting. Whereas hypothesis testing concerns itself with p-values of the hypothesis test, Test Theory concerns itself with the R-squa re. It answers this question: How much of observed test score variance is due to differences between subjects, and how much to other sources of variance, such as differences betw een judges, queries, etc.? Classical Test Theory [1] is a simple version of Test Theory. It defines a subject X  X  (in our case, an algorithm X  X ) true score not as an invisible parameter but as the average observed score over all admissible testing conditions, such as acceptable occasions, queries, judges, etc. A test X  X  reliability the proportion of variance in exam inees' scores that is due to differences in true scores, as opposed to random error. One analytical result has made CTT widely used: Using Cronbach X  X  alpha, ' XX  X  can be estimated from a single administration of a test. The calculation of Cronbach's alpha is:  X  where k is the number of items on the exam, 2  X  i  X  is the estimated variance for item i , and estimated variance of the total scores. The formula provides direct practical advice: The key to a high reliability is to have individual items with (1) high variances, a nd (2) high correlations. Questions that have negative correlations w ith the total, detract from the test X  X  reliability. They may be measuring something, but not what the rest of the test is measur ing. One should consider excluding Generalizability Theory (GT) [2][3] is a much richer version of Test Theory. It conceives of a testing situation as comprised of many facets  X  the examinees, the assessors, the questions, etc. A given test administration is viewed as having samples from each of these facets  X  perhaps a sample of 50 questions assessed by 2 assessors each. GT uses analysis of variance to distinguish exactly how much noise each facet contributes. This is done on a small sample in advance. The main idea  X  this is simplified --is that to maximize a test X  X  reliability, the sample size from each facet should be proportional to the size of its error variance. For example, if a question X  X  difficulty accounts for twice as much variance as an assessor X  X  judgmen ts, then having twice as many questions as assessors will maximi ze test reliability. In this way, test designers can decide how to direct their limited budget. Although GT is possible under other circumstances, in the ideal case we have a sample of data in which all the factors are  X  X rossed X , that is, multiple judge s, each one of whom assesses multiple queries on many documents. We demonstrate using data from [4], a subset of which has the property that 2 judges are crossed with 5 queries. An analysis of variance yields the figures of table 1, which should be interp reted as the percentage of error variance caused by a single participant, query, assessor, etc. participant  X  2 p 0.055 0.0029 11.4 query  X  2 q 0.633 0.009 35.3 assessor  X  2 a 0.015 0.00001 0.05 p-q Interaction  X  2 pq 0.026 0.01275 49.9 p-a Interaction  X  2 pa 0.0004 0 0 q-a Interaction  X  2 qa 0.013 0.00038 1.5 p-q-a Interaction (residual)  X  2 pqa The main participant effect is 11.4%, perhaps disappointing for a test whose purpose is to assess par ticipants. But the key to GT is that these numbers are not used in hindsight to measure the design of a test collection. Specifically, it helps us decide how many assessors and queries to have in the full-scale test we are planning, as follows: For any proposed design, the expected error from queries is  X  2 q (as found in the pilot) divided by the proposed number of queries; the expected error from assessors is  X  divided by the proposed number of assessors, etc. For example, for a proposed test using 20 queries crossed with 3 assessors that are from the same universe as the ones in our pilot study, the expected absolute error is:  X  2  X  = 0.009/20 + 0.00001/3 + 0.01275/20 + 0/3 + 0.00035/(20*3) + 0.0005/(20*3) = 0.001106. If we are primarily interested in relative performance among participants, we can calculate the expected relative error, which represents noise when comparing one participant to another. This includes only error terms i nvolving interactions with p : 0.01275/20 + 0/3 + 0.0005/(20*3) = 0.000646 in our example. We proceed in this manner to calculate the errors associated with various proposed numbers of que ries and assessors, or other facets. Ultimately, we are interested in choosing designs that, for a given cost, maximize these two coefficients: error, and regarding relative error. In this particular example, because error due to queries and participant-query interaction is so much greater than error due to assessors or assessor-participant in teraction, it is wise to spend resources increasing the number of queries, not assessors. In fact, regarding absolute error the idea ratio of queries to assessors is (35.3 + 49.9 + 1.5 + 2.0)/(.05 + 0 + 1.5 + 2.0); for relative error, it is (49.9 +2.0 / 0 + 2.0). However, more interesting results would be obtained by accounting also for the document facet, in addition to the query and assessors facets as we did here. Including a separate document facet in the analysis means that  X  X ssessor variance X  will measure the variance contributed by the choice of assessor (or query) on a single random document , rather than averaged over all documents, as was the case here. The assessor effect would increase enormously with that more complete analysis. This is left for ongoing work. In addition to allowing consider ation of different numbers for each facet, GT has machinery for considering different test structures. For example, a test design may  X  X ross X  assessors with queries, in which case each of a assessors contributes relevance judgments about each of q queries; in an alternative  X  X ested X  design, a different set of assessors is responsible for contributing relevance judgments for each query. Thus, on the basis of a single small pilot, GT allows direct comparison of designs that vary in the number of elements in each facet and in how these facets are structured together. This is a lo t of power for a small amount of work. There are even analytical results that require no data. For example, assuming more than one assessor per query, relative reliability is always maximized if different assessors are assigned to each query, rather than the same assessors for all. Using the well-developed theory provides pr actical tools and theoretical interpretations of test reliability. [1] Linda Crocker a nd James Algina. Introduction to Classical [2] R Robert L. Brennan. Generalizability Theory . Springer-[3] Richard J. Shavelson, Noreen M. Webb. Generalizability [4] Ellen M. Voorhees. Variations in relevance judgments and [5] Ellen M. Voorhees, Chris Buckley. The effect of topic set 
