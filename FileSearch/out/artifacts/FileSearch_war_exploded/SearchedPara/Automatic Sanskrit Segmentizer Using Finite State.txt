 Sanskrit has a rich tradition of oral transmission of texts and this process causes the text to un-dergo euphonic changes at the word boundaries. In oral transmission, the text is predominantly spo-ken as a continuous speech. However, continuous speech makes the text ambiguous. To overcome this problem, there is also a tradition of reciting the pada-p  X  at . ha (recitation of words) in addition to the recitation of a sam . hit  X  a (a continuous sandhied text). In the written form, because of the domi-nance of oral transmission, the text is written as a continuous string of letters rather than a sequence of words. Thus, the Sanskrit texts consist of a very long sequence of phonemes, with the word bound-aries having undergone euphonic changes. This makes it difficult to split a continuous string into words and process the text automatically.

Sanskrit words are mostly analyzed by build-ing a finite state transducer (Beesley, 1998). In the first approach, this transducer was modified by linking the final states to appropriate interme-diate states incorporating the sandhi rules. This approach then allows one to traverse the string from left to right and generate all and only possible splits that are morphologically valid. The second approach is very closely based on the Optimality Theory (Prince and Smolensky, 1993) where we generate all the possible splits for a word and vali-date each using a morphological analyzer. We use one of the fastest morphological analyzers avail-able viz. the one developed by Apertium group 2 . The splits that are not validated are pruned out. Based on the number of times the first answer is correct, we achieved an accuracy of around 92% using the second approach while the first approach performed with around 71% accuracy. The segmentizer is an important component of an NLP system. Especially, languages such as Chinese (Badino, 2004), Japanese, Thai (Haruechaiyasak, 2008) or Vietnamese (Thang et al. , 2008) which do not mark word bound-aries explicitly or highly agglutinative languages like Turkish need segmentizers. In all these lan-guages, there are no explicit delimiters to spec-ify the word boundaries. In Thai, each syllable is transcribed using several characters and there is no space in the text between syllables. So the problem of segmentation is basically twofold: (1) syllable segmentation followed by (2) word seg-mentation itself. A sentence in these languages is segmented by predicting the word boundaries, where euphonic changes do not occur across the word boundaries and it is more like mere concate-nation of words. So the task here is just to choose between various combinations of the words in a sentence.

However, in Sanskrit, euphonic changes occur across word boundaries leading to addition and deletion of some original part of the combining words. These euphonic changes in Sanskrit intro-duce non-determinism in the segmentation. This makes the segmentation process in Sanskrit more complex than in Chinese or Japanese. In case of highly agglutinative languages like Turkish, the components are related to each other semantically involving dependency analysis. Whereas in Sanskrit, only the compounds involve a certain level of dependency analysis, while sandhi is just gluing of words together, without the need for words to be related semantically. For example, consider the following part of a verse, San: n  X  aradam paripapraccha v  X  alm  X   X kirmunipu  X  ngavam gloss: to the Narada asked Valmiki-to the wisest among sages Eng: Valmiki asked the Narada, the wisest among the sages.

In the above verse, the words v  X  alm  X   X kih . and mu-nipu  X  ngavam (wisest among the sages -an adjec-tive of Narada) are not related semantically, but still undergo euphonic change and are glued to-gether as v  X  alm  X   X kirmunipu  X  ngavam .
Further, the split need not be unique. Here is may be decomposed in two different ways after undergoing euphonic changes across word bound-aries.
There are special cases where the sandhied forms are not necessarily written together. In such cases, the white space that physically marks the boundary of the words, logically refers to a single sandhied form. Thus, the white space is deceptive, and if treated as a word boundary, the morphological analyzer fails to recognize the word. For example, consider  X  srutv  X  a ca n  X  arado vacah . .

In this example, the space between  X  srutv  X  a and ca represent a proper word boundary and the word  X  srutv  X  a is recognized by the morphological analyzer whereas the space between n  X  arado and vacah . does not mark the word boundary making it deceptive. Because of the word vacah . , n  X  aradah . has undergone a phonetic change and is rendered as n  X  arado . In unsandhied form, it would be written as, San:  X  srutv  X  a ca n  X  aradah . vacah . . gloss: after listening and Narada X  X  speech Eng: And after listening to Narada X  X  speech
The third factor aggravating Sanskrit segmen-tation is productive compound formation. Unlike English, where either the components of a com-pound are written as distinct words or are sepa-rated by a hyphen, the components of compounds in Sanskrit are always written together. Moreover, before these components are joined, they undergo the euphonic changes. The components of a com-pound typically do not carry inflection or in other words they are the bound morphemes used only in compounds. This forces a need of a special mod-ule to recognize compounds.

Assuming that a sandhi handler to handle the sandhi involving spaces is available and a bound morpheme recognizer is available, we discuss the development of sandhi splitter or a segmentizer that splits a continuous string of letters into meaningful words. To illustrate this point, we give an example.
 Consider the text,
We assume that the sandhi handler handling the sandhi involving spaces is available and it splits the above string as, vacah . .

The sandhi splitter or segmentizer is supposed to split this into  X  srutv  X  a ca etat triloka-j  X  nah . v  X  alm  X   X keh vacah . .

This presupposes the availability of rules corre-sponding to euphonic changes and a good cover-age morphological analyzer that can also analyze the bound morphemes in compounds.

A segmentizer for Sanskrit developed by Huet (Huet, 2009), decorates the final states of its fi-nite state transducer handling Sanskrit morphol-ogy with the possible sandhi rules. However, it is still not clear how one can prioritize various splits with this approach. Further, this system in current state demands some more work before the sandhi splitter of this system can be used as a stan-dalone system allowing plugging in of different morphological analyzers. With a variety of mor-phological analyzers being developed by various ties, it would be worth to experiment with vari-ous morphological analyzers for splitting a sand-hied text. Hence, we thought of exploring other alternatives and present two approaches, both of which assume the existence of a good coverage morphological analyzer. Before we describe our approaches, we first define the scoring matrix used to prioritize various analyses followed by the base-line system. Just as in the case of any NLP systems, with the sandhi splitter being no exception, it is always de-sirable to produce the most likely output when a machine produces multiple outputs. To ensure that the correct output is not deeply buried down the pile of incorrect answers, it is natural to prioritize solutions based on some frequencies. A Parallel corpus of Sanskrit text in sandhied and sandhi split form is being developed as a part of the Consor-tium project in India. The corpus contains texts from various fields ranging from children stories, dramas to Ayurveda texts. Around 100K words of such a parallel corpus is available from which around 25,000 parallel strings of unsandhied and corresponding sandhied texts were extracted. The same corpus was also used to extract a total of 2650 sandhi rules including the cases of mere con-catenation, and the frequency distribution of these sandhi rules. Each sandhi rule is a triple ( x, y, z ) where y is the last letter of the first primitive, z is the first letter of the second primitive, and x is the letter sequence created by euphonic combination. We define the estimated probability of the occur-rence of a sandhi rule as follows:
Let R i denote the i th rule with f R quency of occurrence in the manually split parallel text. The probability of rule R i is: where n denotes the total number of sandhi rules found in the corpus.

Let a word be split into a candidate S j with k constituents as &lt; c 1 , c 2 , ..c k &gt; by applying k  X  1 sandhi rules &lt; R 1 , R 2 , ..R k  X  1 &gt; in between the constituents. It should be noted here that the rules R 1 , ..R k  X  1 and the constituents c 1 , ..c k are inter-dependent since a different rule sequence will re-sult in a different constituents sequence. Also, ex-cept c 1 and c k , all intermediate constituents take part in two segmentations, one as the right word and one as the left.
 The weight of the split S j is defined as: where P c word c x in the corpus. The factor of k was intro-duced to give more preference to the split with less number of segments than the one with more seg-ments. We define our own baseline system which assumes that each Sanskrit word can be segmented only in two constituents. A word is traversed from left to right and is segmented by applying the first appli-cable rule provided both the constituents are valid morphs. Using the 2,650 rules, on a test data of 2,510 words parallel corpus, the baseline perfor-mance of the system was around 52.7% where the first answer was correct. We now present the two approaches we explored for sandhi splitting. 5.1 Augmenting FST with Sandhi rules In this approach, we build an FST, using Open-Fst (Allauzen et al., 2007) toolkit, incorporating sandhi rules in the FST itself and traverse it to find the sandhi splittings.

We illustrate the augmentation of a sandhi rule with an example. Let the two strings be xaXi (dadhi) 4 and awra (atra). The initial FST without considering any sandhi rules is shown in Figure 1. Figure 1: Initial FST accepting only two words xaXi and awra .

As the figure depicts, 0 is the start state and 4 is the final state. Each transition is a 4-tuple &lt; c, n, i, o &gt; where c is current state, n is the next state, i is the input symbol and o is the output. The FST marks word boundaries by flushing out cer-tain features about the words whenever it encoun-ters a valid word. Multiple features are separated by a  X  |  X . E.g., the output for xaXi is lc,s | vc,s and for awra it is vc,s where lc,s stands for locative, singular and vc,s is vocative, singular . The FST in Figure 1 recognize exactly two words xaXi and awra .

One of the sandhi rule states that i+a  X  ya which will be represented as a triple ( ya, i, a ) . Ap-plying the sandhi rule, we get: xaX i + a wra  X  xaX ya wra . After adding this sandhi rule to the FST, we get the modified FST that is represented in Figure 2.
 Figure 2: Modified FST after inserting the rule.  X  X  X  X  indicates the newly added transition.

Here, a transition arc is added depicting the rule which says that on receiving an input symbol ya at state 3, go to state 5 with an output i+a  X  ya . Thus the new FST accepts xaXyawra in addition to xaXi and awra .

Thus, we see that the original transducer gets modified with all possible transitions at the end of a final phoneme, and hence, also explodes the number of transitions leading to a complex trans-ducer.

The basic outline of the algorithm to split the given string into sub-strings is: Algorithm 1 To split a string into sub-strings 1: Let the FST for morphology be f . 2: Add sandhi rules to the final states of f1 link-3: Traverse f  X  to find all possible splits for a 4: Calculate the weights of the possible outputs
The pseudo-code of the algorithm used to insert sandhi rules in the FST is illustrated here: Algorithm 2 To insert sandhi rules in the FST 1: I = Input Symbol; X = last character of the 2: for each transition in the FST transition table 3: if next state is a final state then 4: for all rules where I is the last character 5: S = next state from the start state on 6: Y = first character of the result of the 7: transition T = current state, S, Y, rule; 8: Add T into the FST; 9: end for 10: end if 11: end for
The main problem with this approach is that ev-ery finite state can have as many transitions as the number of euphonic rules resulting in phoneme change. This increases the size of the FST con-siderably. It should be noted that, we have not in-cluded the cases, where there is just a concatena-tion. In such cases, if the input string is not ex-hausted, but the current state is a final state, we go back to the start state with the remaining string as the input. 5.1.1 Results The performance of this system measured in terms of the number of times the highest ranked segmen-tation is correct, with around 500 sandhi rules, and only noun morphology tested on the same test data used for testing baseline system gave the following rank-wise distribution presented in Table 1. Table 1: Rank-wise Distribution for Approach-1.
The system was slow consuming, on an average,
With the increase in the sandhi rules, though system X  X  performance was better, it slowed down the system further. Moreover, this was tested only with the inflection morphology of nouns. The verb inflection morphology and the derivational mor-phology were not used at all. Since, the system is supposed to be part of a real time application viz. machine translation, we decided to explore other possibilities. 5.2 Approach based on Optimality Theory Our second approach follows optimality the-ory(OT) which proposes that the observed forms of a language are a result of the interaction be-tween the conflicting constraints. The three basic components of the theory are: 1. GEN -generates all possible outputs, or can-2. CON -provides the criteria and the con-3. EVAL -chooses the optimal candidate based
OT assumes that these components are univer-sal and the grammars differ in the way they rank the universal constraint set, CON. The grammar of each language ranks the constraints in some dom-inance order in such a way that every constraint must have outperformed every lower ranked con-straint. Thus a candidate A is optimal if it per-forms better than some other candidate B on a higher ranking constraint even if A has more vi-olations of a lower ranked constraint than B.
The GEN function produces every possible seg-mentation by applying the rules wherever appli-cable. The rules tokenize the input surface form into individual constituents. This might contain some insignificant words that will be eventually pruned out using the morphological analyser in the EVAL function thus leaving the winning can-didate. Therefore, the approach followed is very closely based on optimality theory. The morph analyser has no role in the generation of the can-didates but only during their validation thus com-posing the back-end of the segmentizer. In orig-inal OT, the winning candidate need not satisfy all the constraints but it must outperform all the other candidates on some higher ranked constraint. While in our scenario, the winning candidate must satisfy all the constraints and therefore there could be more than one winning candidates.
 Currently we are applying only two constraints. We are planning to introduce some more con-straints. The constraints applied are: The basic outline of the algorithm is: 1: Recursively break a word at every possible po-2: Pass the constituents of all the candidates 3: Declare the candidate as a valid candidate, if 4: Assign weights to the accepted candidates and 5: The optimal solution will be the one with the 5.2.1 Results The current morphological analyzer can recognize around 140 million words. Using the 2650 rules and the same test data used for previous approach, we obtained the following results: The complete rank wise distribution is given in Ta-ble 2. We presented two methods to automatically seg-ment a Sanskrit word into its morphologically valid constituents. Though both the approaches outperformed the baseline system, the approach that is close to optimality theory gives better re-sults both in terms of time consumption and seg-mentations. The results are encouraging. But the real test of this system will be when it is inte-grated with some real application such as a ma-chine translation system. This sandhi splitter be-ing modular, wherein one can plug in different morphological analyzer and different set of sandhi rules, the splitter can also be used for segmentiza-tion of other languages.

Future Work The major task would be to ex-plore ways to shift rank 2 and rank 3 segmenta-tions more towards rank 1. We are also explor-ing the possibility of including some semantic in-formation about the words while defining weights. The sandhi with white spaces also needs to be han-dled.
 I would like to express my gratitude to Amba Kulkarni and Rajeev Sangal for their guidance and support.

