 The focus of research on query performance prediction is to predict the effectiveness of a query given a search system and a collection of documents. If the performance of queries can be estimated in advance of, or during the retrieval stage , specific measures can be taken to improve the overall per-formance of the system. In particular, pre-retrieval predi c-tors predict the query performance before the retrieval ste p and are thus independent of the ranked list of results; such predictors base their predictions solely on query terms, th e collection statistics and possibly external sources such a s WordNet. In this poster, 22 pre-retrieval predictors are ca t-egorized and assessed on three different TREC test collec-tions.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Performance query performance prediction
Research on query performance prediction deals with the task of predicting how well or poorly a query will perform given a search system and a collection of documents. If the performance of queries can be estimated in advance of or during the retrieval stage, specific steps can be taken to improve retrieval.

Over the last years, a number of query performance pre-diction algorithms have been proposed that broadly fall int o two categories: pre-retrieval and post-retrieval approac hes. Pre-retrieval predictors estimate the performance of a que ry before the retrieval stage is reached and are, thus, inde-pendent of the ranked list of results; essentially, they are search-independent. They base their predictions on query term characteristics, collection statistics and possibly exter-nal sources such as WordNet[1], which provides information on the terms X  semantic relationships. Since pre-retrieval pre-dictors rely on information that is available at indexing ti me, they usually can be calculated more efficiently, causing less overhead to the search system. On the other hand, post-retrieval predictors such as Query Clarity[3] base their pr e-dictions on the ranked list of results, which provides more information to the predictor, making accurate predictions easier to achieve. This benefit is offset however, by the added requirement of a second retrieval stage, as, given un-satisfactory predictor scores, another retrieval stage mi ght be necessary. In this poster, we concentrate on the evalu-ation of existing pre-retrieval predictors. In the followi ng section, a categorization of pre-retrieval predictors is p re-sented. Subsequently, an overview of the predictors X  perfo r-mances across different test collections is given, followed by a brief discussion of the results.
Pre-retrieval predictors can be divided into four different groups according to the heuristic they exploit: specificity , ambiguity (AMBI), term relatedness (REL) and ranking sen-sitivity (RNK). The specificity based predictors predict a query to perform better with increased specificity. How the specificity is determined, further divides these predic -tors into collection statistics based and query based. The former are based on the inverse term/document frequency of the query terms (e.g. AvIDF , AvICT F , SCS ) while the predictor in the latter case ( AvQL ) relies on the length of the query terms alone. A different variety of predictors exploit s the query terms X  ambiguity . In such a scheme, if a term always appears in the same or similar contexts, the term is considered to be unambiguous. Low ambiguity indicates an easy query. AvQC and AvQCG for instance depend on document clustering to determine a term X  X  ambiguity. An alternative to collection based ambiguity is the utilizati on of WordNet[1]. AvP exploits the number of noun senses in WordNet for prediction, whereas AvP relies on the num-ber of senses across all word types. Here, the higher the sense count, the higher the term X  X  ambiguity. Term relat-edness based predictors consider the relationship between query terms. A strong relationship between query terms sug-gests a well performing query. The maximum and average of the pointwise mutual information over all query term pairs ( MaxP MI , AvP MI ) are two collection based predictors in this category. The semantic similarity between the query terms (their relatedness) can also be derived from Word-Net. The predictors AvLesk , AvV P and AvP ath belong to this group. Finally, ranking sensitivity based predictors predict a query to be difficult if the retrieval algorithm can-not distinguish the documents containing the query terms from each other. Specifically, the predictors in [11] ( AvV AR , MaxV AR , SumV AR ) rely on the distribution of TF.IDF weights. Note that the last three predictors and the predic-tors based on document clustering ( AvQC , AvQCG ) require a significant amount of preprocessing, compared to predic-tors exploiting the inverse term/document frequencies alo ne. 22 pre-retrieval predictors were assessed on three differ-ent TREC collections: TREC Volumes 4+5 (minus CR) with title topics 301-450, WT10g with title topics 451-550 and GOV2 with title topics 701-850. The collections were Krovetz stemmed and stopwords were removed. As retrieval approach, Language Modeling with Dirichlet smoothing [10] was chosen. Three different levels of smoothing were se-lected: low (  X  = 100), medium (  X  = 1500) and high (  X  = 5000). As most predictors exploit the inverse term/documen t frequencies in some way, it is hypothesized that the amount of smoothing influences the quality of the predictors. For the purpose of evaluating the predictors, we report the line ar correlation coefficient (Table 1) and Kendall X  X  tau (Table 2) . Results marked with a star are statistically significant; th e best performing predictor for each collection and level of smoothing is shown in italics.
 Table 1: Results of the predictor evaluations given by the linear correlation coefficient.
 Table 2: Results of the predictor evaluations given by Kendall X  X  tau.
The predictor performances depend on the particular test collection as well as the particular retrieval approach. In -creasing the amount of smoothing tends to increase the pre-dictor performance of those collection based predictors th at already perform well on low smoothing. The WordNet based measures are somewhat effective on TREC Volumes 4+5, but fail completely on WT10g and GOV2. Overall, predic-tion for the title topics of WT10g has shown to be the most difficult. Among the assessed predictors, there is not a sin-gle predictor that outperforms all others across all settin gs evaluated. Although MaxV AR achieves the highest linear correlation coefficient and Kendall X  X  tau for a number of set-tings, the difference in correlation with respect to MaxIDF , a predictor that results in slightly lower correlation coeffi -cients, is not statistically significant. This result sugge sts, and will be studied in more depth in future work, that the comparison of query performance predictors according to th e absolute values of their correlation coefficients alone may n ot be sufficient when the coefficients are similar. [1] WordNet -An Electronic Lexical Database . The MIT [2] S. Banerjee and T. Pedersen. Extended gloss overlaps [3] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. [4] B. He and I. Ounis. Inferring query performance using [5] J. He, M. Larson, and M. de Rijke. Using [6] J. Mothe and L. Tanguy. Linguistic features to predict [7] S. Patwardhan and T. Pedersen. Using wordnet based [8] R. Rada, H. Mili, E. Bicknell, and M. Blettner. [9] F. Scholer, H. Williams, and A. Turpin. Query [10] C. Zhai and J. Lafferty. A study of smoothing methods [11] Y. Zhao, F. Scholer, and Y. Tsegay. Effective
