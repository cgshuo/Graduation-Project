 Text Categorization (TC) is one of the most important task in the information system. Sebastiani believes that TC is the meeting point of machine learning and information retrieval (IR), the  X  X other X  of all disciplines concerned with automated content-based document management [1]. TC can be defined as the task of determining an assignment of a tag from of documents. In traditional supervised learning framework, all the users share one training data, and then, use the learned model to classify a test data set. 
Short message (SMS) filtering is to distinguish between spam and ham for any individual user. Different user may have different definition of spam messages, which implies that some users X  spam message may be classified as ham by others. For example, although some group-sending-SMS, such as weather forecast, finan-cial information, news report, etc., may be regarded as spam by user A, but on the contrary user B may have an urgent need for them. It is necessary to train a specific filter for each user. 
Personalized SMS filtering task is similar to TREC Spam Track [15]: there is a set filter, which yields a binary judgment (spam or ham); the user will immediately feed-back a gold standard, and the filter should be updated at once. 
Adaptive filtering has been widely discussed. In this paper, we face a new chal-lenge that the filter is expected to converge much faster than ever before, e.g. within 10 labeled SMSs or less. It seems impossible for filter, but it is reasonable for users. Users hardly have patience of affording enough labeled data and waiting for filter X  X  learning. 
The dimension of the feature space can be defined as the number of different words (based on our statistics, there are 27,054 different words in 200,000 pieces of SMS) in more than 20,000 parameters to be estimated, which is difficult for training sample size to meet this requirement. According to the statistical learning theory [6], dimen-this paper, latent topics are employed as feature space to minimize dimensions. We focus on utilizing the topic models to achieve knowledge extraction, knowledge rep-resentation and knowledge utilization. 
The rest of the paper is organized as follows. In Section 2, we give a brief review introduced. In Section 4, dual-PLSA is presented to solve the problem of insufficient labeled samples. In section 5, supervised dual-PLSA is proposed to deal with practical problems. In section 6, experiments results and the analysis are shown. A recent trend in dimensionality reduction is using the probabilistic models. These models, which include generative topological mapping, factor analysis, independent component analysis and probabilistic latent semantic analysis(PLSA), are generally specified in terms of an underlying independence assumption or low-rank assumption. The models are generally fit with maximum likelihood, although Bayesian methods are sometimes used [9] .

In LSA [2], the singular value decomposition (SVD) is used to extract the latent se-mantic features, which form a low dimensional vector space. The shortcoming of LSA is the lack of statistical interpretation. By using the conditional probability to interpret the semantic feature (latent topic) z i , Hofmann presented PLSA (Fig. 1) [3]. Blei added a Dirichlet prior assumption and obtained Latent Dirichlet Allocation (LDA) genera-that models each data point as a collection of draws from a mixture model in which each mixture component is known as a topic [5]. Typical LSA, PLSA and LDA are unsupervised methods, in which the documents' categories are not taken into account. Those methods can be employed in document classification. Blei obtained a generative model for classification by using one LDA module for each class [5]. And he believed the LDA also can be applied in the discriminative framework [5]. 
Currently, many supervised topic models have been presented. Blei added LDA a response variable to get supervised latent Dirichlet allocation (sLDA), which was used to predict movie ratings and web page popularity (Fig. 3) [8]. A discriminative variation on LDA named DiscLDA employs a class-dependent linear transformation on the topic mixture proportions (Fig. 4) [9]. And Xue proposed topic-bridged PLSA for cross-domain text classification [7]. Some other methods also make use of cate-models is that a topic often belongs to only one category, which means the conditional servable, which means no supervised information about topics is employed. Some-times topics are observable or semi-observable, and information from these observed topics can greatly improve the performance of filters. For example, [14] presented a semi-supervised topic model, which takes advantage of the high readability of the expert review to structure the unorganized ordinary opinions. In this paper, we find that there are only two key conditional probabilities for TC: model is first proposed to extract topics with sufficient labeled SMSs. In c -w PLSA, C parameters. Unfortunately, we can X  X  afford enough training data. A reasonable as-sumption is that more observed data is helpful for learning latent topic. Therefore, two generative models, c-w PLSA and typical PLSA, are combined to share observable variables in order to utilize other observed data. This is the dual-PLSA, which has an additional observable variable D . Furthermore, observable information about Z is employed to acquire more useful topic model. Since Z is no longer latent, this model is called supervised dual-PLSA. Assume that category set is C ={ c 1 =spam, c 2 =ham}, and word set is W ={ w 1 , ..., w M }. Each SMS is represented as a  X  X ag-of-words X  vector. The posterior probability of an input SMS d l is: From (1) and (2) we can get: (3) means three kinds of information are needed to filter d l : what words are contained fore, we present c -w PLSA model shown in expression (4) and figure 5. C and W are observable variables, and Z is latent. Labeled data is needed to estimate P ( W | Z ). Un-fortunately, we can X  X  afford enough training data, and then the dual-PLSA model is presented. The c -w PLSA is similar with the typical PLSA (shown in figure 1), and they share the same conditional probability, P ( w j | z i ). However, the c -w PLSA needs plenty of labeled data. In our problem, labeled data denoted as D l is deficient, while unlabeled data de-noted as D u is abundant. Because typical PLSA model is suitable for unsupervised learning, we employ a PLSA model, named d -w PLSA, to extract latent topic from are statistically independent on condition that z i is given, the dual-PLSA model can be equivalent to Figure 6(b). However, whether dual-PLSA is a generative model is not observed data to estimate this parameter. ( c tags of all the SMSs in D u . EM algorithm is employed to resolve this problem. P combined with a changeable radio to acquire a more accurate P ( w j | z i ). reflect the category information; while c -w PLSA can X  X  acquire enough labeled data to should be setting approximated to 1. With the iteration time going up, a should n ( c k , w j ) is counted from D u and D l . In the above mentioned models, supervised information about topics is still not em-ployed. Actually supervised information is indispensable to make the filter converged as quickly as possible. To acquire supervised information, some SMS taxonomies with labeled data can be utilized. For example, from a taxonomy we pick out catego-ries which are related with spam filtering, and consider them as topics. Then the when the following two conditions in formula (2) are satisfied: tained from D t via (11) before the EM iteration of dual-PLSA model. 
Algorithm 1 can X  X  guarantee to be processed in real-time, then we simplify it and P ( z i | c k ) before the EM iteration. 6.1 Datasets which contains 715,589 SMSs. Both D l-train is the set labeled with  X  X pam X  and D l-test is D others). D t has 91,403 samples. 6.2 Algorithms Explanation Two different types of algorithms are employed for comparison with proposed c -w PLSA model and adaptive supervised dual-PLSA. The first one is Na X ve Bayesian classifier (NBC). Before tested with D l-test , no training is given to NBC, which implies method is LDA discriminative approach with five topics, which is trained with D u . w PLSA model is trained with D l-train . Adaptive supervised dual-PLSA is trained with D to obtain P ( w j | z i ), and it estimate P ( z i | c k ) by the adaptive learning in table 2. 6.3 Evaluation Metric There exist many evaluation metrics for measuring the classification performance. In comparing different algorithms. The error rate is defined as: E =|{ d | d  X  D l-test  X  label c = L ( d ) by the filter. The cumulative error rate is a function of processed mes-sages number. Assume n is the processed messages number, and D l-test ( n ) is the proc- X  which is the neighborhood size of the n th SMS. 6.4 Overall Performance and Conclusion The experimental results are listed in table 5~7 and figure 7~9. To our surprise, nei-10.53%, though trained previously with 310,000 ham/spam labeled samples. By adap-tive learning with only 2,000 gold standard feedback, other filters X  cumulative error rates become lower than c -w PLSA. The reason maybe lies in table 5. From this table spam and ham. By checking P ( W | z 4 ), we find topic 4 includes many high-frequency words of D l-train . Most of those words co-occur in spam and ham, yet a relatively con-siderable quantity of discriminable words lie either in spam or in ham. Then these 9.83%.The key reason of bad performance from c -w PLSA and LDA is that: topics in them are served to generate documents other than distinguish categories. Therefore, supervised information about topics is indispensable. 
Table 7 and figure 8 show that dual-PLSA converges very fast. Within 100 gold standard feedback, the cumulative error rate drops to 9%. Its total error rate is 6.94%, which is the lowest. However, NBC needs 4,000 feedback to reduce the cumulative error rate to below 9%. And its total error rate is 7.94%. The cumulative error rate of NBC keeps slow descending in the whole test process. 
The dual-PLSA X  X  outstanding performance is due to supervised information about topics. Though it only has 5 topics, the meanings of those topics are determined by labeled data other than by unsupervised methods like LDA. And the real value of accurately and easily. Acknowledgments. This work was supported by the national High-tech Research and Development Plan of China under grant No. 2007AA01Z417 and the 111 Project of China under grant No. B08004. It was also supported by Nokia Research Center ( China). We thank the anonymous reviewers for their useful comments. 
