 Machine reading systems (Poon et al., 2010; Richardson et al., 2013) can be tested on their ability to answer queries about contents of doc-uments that they read, thus a central problem is how the information of documents should be orga-nized in the system and retrieved by the queries. Recently, large scale datasets of document-query-answer triples have been constructed from online newspaper articles and their summaries (Hermann et al., 2015), by replacing named entities in the summaries with placeholders to form Cloze (Tay-lor, 1953) style questions (Figure 1). These datasets have enabled training and testing of complicated neural network models of hypothesized machine readers (Hermann et al., 2015; Hill et al., 2015). be accumulated by its subsequent occurrence, such as  X  Downey recently presented a robotic arm . . .  X . Thus, named entities basically serve as anchors to link multiple pieces of information encoded in dif-ferent sentences. This insight has been reflected by the anonymization process in construction of the dataset, in which coreferent entities (e.g.  X  Robert Downey Jr.  X  and  X  Downey  X ) are replaced by ran-domly permuted abstract entity markers (e.g.  X  @en-tity0  X ), in order to prevent additional world knowl-edge from being attached to the surface form of the entities (Hermann et al., 2015). We, however, take it as a strong motivation to implement a reader that dy-namically builds meaning representations for each entity, by gathering and accumulating information on that entity as it reads a document (Section 2).
Evaluation of our model, DER Network , exhibits better results than previous research (Section 3). In particular, we find that max-pooling of entity rep-resentations, which is intended to model the accu-mulation of information on entities, can drastically improve performance. Further analysis suggests that max-pooling can help our model draw multiple pieces of information from different sentences. Following Hermann et al. (2015), our model esti-mates the conditional probability p ( e | D,q ) , where q is a query and D is a document. A candidate answer for the query is denoted by e , which in this paper is any named entity. Our model can be factorized as: in which u ( q ) is the learned meaning for the query and v ( e ; D,q ) the dynamically constructed mean-ing for an entity, depending on the document D and the query q . We note that (1) is in contrast to the factorization used by Hermann et al. (2015): in which a vector u ( D,q ) is learned to represent the status of a reader after reading a document and a query, and this vector is used to retrieve an answer by coupling with the answer vector v ( a ) . 1 Figure 2: Dynamic entity representa-tion d e,c encodes LSTM outputs, mod-eling surrounding context. v ( e ; D,q ) for each entity as a weighted sum 2 : in which s e,c ( q ) is calculated by the attention mech-anism (Bahdanau et al., 2015), modeling the degree to which our reader should attend to a particular oc-currence of an entity, given the query q . More pre-cisely, s e,c ( q ) is defined as the following: where s e,c ( q ) is calculated by taking the softmax of representation d e,c 0 and the query vector q . The vec-tor m , matrix W dm , and the bias b s in (7) are learned parameters in the attention mechanism. Vector m is used here to map a vector value to a scalar.
The query vector 3 u ( q ) is constructed similarly as dynamic entity representations, using bidirectional output vectors. More precisely, if we denote the length of the query as T and the index of the place-holder as  X  , the query vector is calculated as: Then, v ( e ; D,q ) and u ( q ) are used in (1) to calcu-late probability p ( e | D,q ) . We use the CNN-QA dataset (Hermann et al., 2015) for evaluating our model X  X  ability to answer ques-tions about named entities. The dataset consists of ( D,q,e ) -triples, where the document D is taken from online news articles, and the query q is formed by hiding a named entity e in a summarizing bullet point of the document (Figure 1). The training set has 90k articles and 380k queries, and both valida-tion and test sets have 1k articles and 3k queries. An average article has about 25 entities and 700 word tokens. One trains a machine reading system on the data by maximizing likelihood of correct answers. We use Chainer 5 (Tokui et al., 2015) to implement Experimental Settings Named entities in CNN-QA are already recognized. For preprocessing, we segment sentences at punctuation marks  X . X ,  X ! X , and ablation test on several techniques that improve our basic model.
 Results As shown in Table 1, Max-pooling de-scribed in Section 2.2 drastically improves perfor-mance, showing the effect of accumulating informa-tion on entities. Another technique, called  X  X yway X , is based on the observation that the attention mech-anism (5) must always promote some entity occur-rences (since all weights sum to 1 ), which could be difficult if the entity does not answer the query. To counter this, we make an artificial occurrence for each entity with no contexts, which serves as a by-way to attend when no other occurrences can be rea-sonably related to the query. This simple trick shows Analysis In the example shown in Figure 4, our basic model missed by paying little attention to the second and third sentences, probably because it does not mention @entity0 ( Downey ). In contrast, max-pooling of @entity2 ( Iron Man ) draws attention to the second and third sentences because Iron Man is said related to Downey in the first sentence. This helps Iron Man surpass @entity26 ( Transformers ), which is the name of a different movie series in which robots appear but Downey doesn X  X . Quanti-tatively, in the 479 samples in test set correctly an-swered by max-pooling but missed by basic model, the average occurrences of answer entities (8.0) is higher than the one (7.2) in the 1782 samples cor-rectly answered by both models. This suggests that max-pooling especially helps samples with more en-tity mentions. It is actually a surprise for us that deep learning mod-els, despite their vast amount of parameters, seem able to learn as intended by the designers. This also indicates a potential that additional linguistic intu-itions modeled by deep learning methods can im-prove performances, as in the other work using max-pooling (LeCun et al., 1998; Socher et al., 2011; Le et al., 2012; Collobert et al., 2011; Kalchbrenner et al., 2014), attention (Bahdanau et al., 2015; Luong et al., 2015; Xu et al., 2015; Rush et al., 2015), etc. In this work, we have focused on modeling a reader that dynamically builds meanings for entities. We believe the methodology can be inspiring to other problems as well.
 This work was supported by CREST, JST and JSPS KAKENHI Grant Number 15H01702 and 15H05318. We would like to thank members of Pre-ferred Infrastructure, Inc. and Preferred Networks, Inc. for useful discussions. We also thank the anony-mous reviewers for comments on earlier version of this paper.

