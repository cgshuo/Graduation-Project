 ORIGINAL PAPER Suryaprakash Kompalli  X  Srirangaraj Setlur  X  Venu Govindaraju Abstract This paper describes a novel recognition driven segmentation methodology for Devanagari Optical Charac-ter Recognition. Prior approaches have used sequential rules to segment characters followed by template matching for classification. Our method uses a graph representation to segment characters. This method allows us to segment hori-zontally or vertically overlapping characters as well as those connected along non-linear boundaries into finer primitive components. The components are then processed by a clas-sifier and the classifier score is used to determine if the com-ponents need to be further segmented. Multiple hypotheses are obtained for each composite character by considering all possible combinations of the classifier results for the primi-tive components. Word recognition is performed by design-ing a stochastic finite state automaton (SFSA) that takes into account both classifier scores as well as character frequen-cies. A novel feature of our approach is that we use sub-char-acter primitive components in the classification stage in order to reduce the number of classes whereas we use an n-gram language model based on the linguistic character units for word recognition. 1 Introduction Optical Character Recognition (OCR), the process of auto-mated reading of text from scanned images, is a vital tool for applications such as the creation of digital libraries. Latin and CJK (Chinese, Japanese, Korean) OCRs report high accura-cies on machine-printed documents and have been integrated into several applications [ 12 , 23 , 32 , 43 ]. The term OCR is sometimes loosely used to include pre-processing steps such as binarization and noise removal, skew correction, and text block segmentation prior to actual recognition. The pre-processing methods employed by us have been dealt with in a previous article [ 17 ]. Several OCR challenges remain even after employing disciplined scanning methods and cut-ting-edge pre-processing techniques. Most Devanagari OCR methods segment a word into smaller units [ 2 , 9 , 30 ] for clas-sification. Classifiers are designed to assign class labels to thesegmentedunits.Post-processingusingdictionarylookup and other techniques are used to improve recognition perfor-mance at the word level.
 In this work, we focus on the recognition of printed Devanagari text after pre-processing, and propose a graph-based representation that facilitates a recognition-driven segmentation approach, and a stochastic finite state autom-aton for word recognition. Experimental results have been reported on the Indian Language Technologies (ILT) data set [ 20 , 26 ]. The EMILLE Hindi text data corpus [ 1 ] has been used for generating the probabilistic n-gram language model.
Devanagari isasyllabic-alphabeticscript[ 11 ]withasetof basic symbols X  X onsonants, half-consonants, vowels, vowel modifiers, and special diacritic marks, which we will refer to as characters (Table 2 ). Composite characters are formed by combining one or more of these characters. We will refer to these as composites . The formation of composites from characters can be represented as a regular expression shown in Eq.( 1 ); a similar form has been used previously in [ 2 ]. A horizontal header line ( Shirorekha ) runs across the top of the characters in a word, and the characters span three dis-tinct zones (Fig. 1 ); an ascender zone above the Shirorekha , the core zone just below the Shirorekha , and a descender zone below the baseline of the core zone. Symbols written above or below the core will be referred to as ascender or descender components, respectively. A composite character formed by one or more half consonants followed by a con-sonant and a vowel modifier will be referred to as a con-junct character or conjunct. Further, we use the following nomenclature for composite characters to indicate modifier location:Consonant X  X owel(independentconsonantorvowel with no ascenders or descenders), consonant X  X scender (con-sonantwithascender),consonant X  X escender(consonantwith descender), conjunct X  X ore (conjunct with no ascenders or descenders), conjunct X  X scender, and conjunct X  X escender. Composite character  X  V | ( c  X  C [  X  ][ s ] ) Devanagari word  X  ( Composite character ) + 1.1 Prior work in segmentation Removal of the shirorekha is a frequently used technique to split words into composites [ 9 , 38 ]. Classification at the com-posite character level is very challenging as the number of valid classes exceeds 5,000. The ILT image data set [ 20 , 26 ] has 10,606 word images containing 973 composite character classes, and the EMILLE text data set [ 1 ] has 5,573 compos-ite character classes. Some sample images from the ILT data set are shown in Fig. 3 . Typically, the difficulty of designing a classifier increases with the size of the class space, and find-ing sufficient samples of all classes for training becomes a critical problem. For example, 537 composite character clas-ses from the ILT data set occur less than 25 times each. Since the number of such poorly represented classes is very large, OCR results are adversely affected.

An alternative approach [ 2 , 17 , 30 ] is to further segment the composite characters into smaller parts, which we will refer to as components. The class space of components is significantly smaller than that of composite characters. For example, the 11 composite characters in the set combining 8 characters: , and these in turn are formed from the following 5 components: and location of components from one font to another, the basic shape and position of components remains fairly con-sistent.e.g:Thevowelmodifier hasanascenderpart that always occurs above the shirorekha, and a part that occurs in the core region ; However, minor variations in these components can be seen in the examples in Fig. 2 (b).
The position of components may be estimated using structural attributes such as location and number of vertical strokes, average size, and aspect ratio. Structural information can be extracted from a paragraph or line of text, or based on knowledge of the font. However, font variations and poor print quality cause structural features to vary across words in the same line [ 24 ] (Fig. 2 a, b). We show that it is possi-ble to use a graph representation of a word image to capture the basic positions of components and also enable segmenta-tion of composites. Structural information from paragraphs, lines, or a priori font data is not required to compute the graph representation.

Our work is partly inspired by the paradigm of recogni-tion-driven OCR, originally proposed by Casey et al. [ 8 ]. Devanagari OCR methods reported in the literature [ 4 ]fol-low a dissection technique; a word is segmented into smaller parts, and classifiers are employed to assign class labels to the segmented image pieces. There is no feedback loop to correct erroneous segmentation. We design a stochastic framework that is used to prune multiple segmentation hypotheses.
Sliding window with forepart prediction has been exten-sively used in multi-font Latin OCR to separate fused characters [ 41 ]. A linear segmentation window is employed. Classifiers are used to map the two segmented parts to Latin characters. If the segmented parts produce classifier results with confidence less than a threshold, that segmentation hypothesis is rejected and the next hypothesis is considered. Linear windows sliding in only one direction do not work well with Devanagari script where composites need to be segmented along non-linear boundaries in two directions: left to right and top to bottom (Fig. 2 c).

Lee et al. [ 29 ] outline a technique wherein they construct a graph by treating pixels of a gray-scale image as nodes that can be traversed to obtain segmentation hypotheses. While this method can trace a non-linear boundary, it leads to a large number of segmentation hypotheses. 1.2 Prior work in language modeling Devanagari character formation can be said to be governed by a set of script composition rules [ 37 ]: e.g. Vowel modifiers should be associated with a consonant, only one vowel modifier must be present in a character. OCRs can take advantage of these rules to improve performance [ 2 , 30 ]. Dictionary lookup has been used as a post-processing step by Bansal et al. [ 3 ] to prune word recognition results. Use of probabilistic models which can enhance recognition results before using dictionary lookup have not been reported in the Devanagari OCR literature. Examples of such probabilistic models include alphabet n-grams which have been used in Latin OCR [ 7 , 19 ], and word n-grams and trigger pairs which have been used in speech recognition systems [ 35 ]. Most of these probabilistic language models have used characters or words as units of the language model. At the sub-word level, attempts have been made to design models based on mor-phemes, or suffix-prefix relations [ 18 , 39 ]. Readers should refer to Kukich [ 27 ] for an excellent survey of some of these techniques.

Sections 2.1 and 2.2 describe our recognition driven appr-oach for segmentation and generation of multiple component hypotheses. Section 2.3 describes the design of a Stochas-tic Finite Automata (SFSA) that outputs word recognition results based on the component hypotheses and n-gram sta-tistics. Section 2.4 describes the design of a language model based on the linguistic composite character units. Section 3 presents our experimental results with analysis. Section 4 outlines future work. 2 Proposed segmentation methodology Letusdefineagraphrepresentation G ofawordoracharacter image I ,derivedfromatopologicalrepresentation[ 42 ](Eq. 2 ).
I  X  I
Each graph G consists of a set of nodes N and a set of edges E . Each image I is divided into a number of horizon-tal runs(Hruns). Each of these runs are classified as merging , splitting or continuing runs based on the number of Hruns in the row immediately above it ( abv ) or in the row immedi-ately below ( bel ) it. In splitting Hruns, ab v  X  1 and bel and in merging Hruns, ab v&gt; 1 and bel  X  1. The remain-ing Hruns are classified as continuing . Adjacent runs which are classified as continuing are merged into a single block. A splitting Hrun is added to the block that occurs above it and a merging Hrun is added to the block that occurs below it. A graph can be constructed by connecting the centroids of the neighboring blocks. A group of the continuing runs is defined as an image segment I i or block. A node N i of the graph G represents the center of mass of the image segment I i .We define an edge E ij when the corresponding nodes N i , N j connected.

Variants of such a graph representation have been used in forms processing applications for form frame detection [ 46 ] and to extract features like loops, cusps, joints for classifica-tion in Chinese and Latin script recognition [ 34 , 45 , 47 ].
Figure 4 illustrates the process of generating a block adjacency graph (BAG) from a character image, and Fig. 5 a shows the BAG generated from a word image.

AftertheBAGisconstructedfromawordimage,baselines are computed using linear regression [ 40 ]. Blocks that lie parallel to the baseline and have a large aspect ratio and area greater than three times the average stroke width are identi-fied as the shirorekha s . Blocks above the shirorekha corre-spond to ascenders a , and the remaining blocks correspond to core components c . Since the shirorekha is very prominent, gross labeling of s , a , c is trivial even in noisy Devanagari words. The core components can be isolated characters that do not require further segmentation or conjunct and fused characters that may or may not have descenders. A recog-nition driven segmentation paradigm using graph features and classifiers is used to segment the conjuncts and fused characters.
 2.1 Segmentation using graphs We perform segmentation by selecting subgraphs from the BAG representation of word or character images. A sub-graph selected is a group of nodes from the BAG G and the ideal selection of sub-graphs will segment the image into its constituent Devanagari components. A naive approach is to exhaustively segment sub-graphs from the BAG. Such a scheme is computationally inefficient, since the number of possible hypotheses grows exponentially. The search space of hypotheses is pruned using the following constraints: (i) The primitives in the class space are assumed to be joined from left to right or top to bottom, and (ii) all of the primitives are assumed to consist of a single connected component. The equivalent conditions for sub-graph selection are: (i) A node cannot be selected if it is to the right (bottom for descen-der characters) of a node that has not been selected; and (ii) sub-graphs cannot have disconnected nodes. Using the graph representation, we can segment a composite character along curves, splits, and merges in the word image. The method generates fewer segmentation hypotheses than the window based approaches reported in literature [ 29 , 41 ]. Once the subgraphs have been extracted, they can be mapped back to the associated image segments thereby generating segmen-tation hypotheses. Images corresponding to the subgraphs are then input to a classifier (Sect. 2.2 ). Figure 5 c shows an example of how different subgraphs can be selected from G to obtain segmentation hypotheses. Using the constraints described, the number of hypotheses can be further reduced. 2.2 Recognition driven segmentation The use of classifiers to limit segmentation results has been used in Latin OCR [ 8 , 28 ]. We adopt a similar strategy for Devanagari. Table 1 shows the 71 component shapes that form our class space. There are 6 ascender shapes ( a ), 5 descender shapes ( d ) and 60 other core component shapes ( c ). For each subgraph extracted from the BAG of the word or character image, we input the corresponding subgraph image to a classifier. A score from the classifier below the threshold is taken as an indication that the input segmentation hypothesis does not correspond to one of the known classes. Procedure 1 outlines the specific steps used in our approach. Procedure 1 Segment( G ) 2.2.1 Classifiers Classifiers are used to assign a class label from the class space for each input image and several classifier designs have emerged over the years. We have implemented previ-ously known feature extraction techniques and applied them to Devanagari component classification. The features used in the experiments are gradient, structural and concavity fea-tures that are scale invariant and the classifiers are based on neural networks, or K-nearest neighbor. The classifiers can be considered as a function score ( I , C ) where I is the input image, and C is the class space. The result S is of the output class and s h i  X  R represents the class score or class-conditional probability obtained from the classifier. 1. Neural network classifier using Gradient features This classifier has 72 input nodes, 48 hidden nodes and 40 output nodes and uses the gradient sub-set of the GSC fea-ture set. The GSC feature set was originally introduced for Latin handwriting recognition in [ 14 ]. The top-choice Procedure 2 Recognize( h ) classifier accuracy on isolated consonant images is 94.36% 2. Neural network classifier using GSC features This clas-sifier has 512 input nodes, 128 hidden nodes and 40 output nodes and uses the GSC feature set. The top-choice classifier accuracy on isolated consonant images is 95.25%. 3. K-nearest neighbor classifier using GSC features This is a K-nearest neighbor classifier that uses the GSC feature set and the Kulzinsky metric. This classifier has been previously used for handwriting recognition for Latin script [ 44 ]. We examined different values of K before selecting K =3, which gave a top choice accuracy of 94.27% on isolated consonant images.

Both the neural networks are fully connected, and use logistic activation function. The neural networks have been trained using standard back-propagation. We retain the top 3 results returned by each classifier. The output scores for both the neural network as well as the K-nearest neighbor classifier range from 0.0 to 1.0. Receiver operator charac-teristics are analyzed and the equal error rate is selected as the threshold score for each class. Low thresholds increase the number of false positives (Conjuncts and descenders are accepted and under-segmentation occurs) and high thresh-olds increase false rejects (Consonants/vowels are rejected and over-segmentation occurs). The neural network classi-fier with GSC features provides the best result amongst the three methods.

The character recognition results from this stage are of the form: O h ={ o h 1 s h 1 , o h 2 s h 2 , ..., o h k s represent the output class label and the associated confidence score from the classifier. In cases where each hypothesis h represents a core component fused with another component (e.g. a conjunct character), the output is returned in the form the fused component and h i 2 is the second part. In case of ascenders, only one hypothesis is obtained. For example, the results from the image shown in Fig. 5 d can be written as follows:
A probabilistic finite state automata is used to process these core, descender and ascender component hypotheses and generate word-level results. 2.3 Finite state automata to generate words TherecognitiondrivensegmentationbasedontheBAGstruc-ture generates multiple hypotheses for each composite or fused character. While many words may be formed by com-bining these component results, not all of them are equally likely or valid. Script composition rules and corpus-based probabilisticlanguagemodelscanbeusedtoeliminateinvalid choices and rank the word results. We design a Stochastic Finite State Automata (SFSA) to combine the rule based script composition validity checking and a probabilistic n-gram language model into a single framework. We com-pare the use of various linguistic modeling units in selecting the n-grams.
 The general idea in designing a SFSA is to create a Finite-State Machine (FSM) that accepts strings from a particular language, and assigning data-driven probabilities to transi-tions between states. If a candidate word is accepted, a score or probability is obtained and if rejected, corrections can be made using the minimum set of transitions that could not be traversed [ 21 ]. SFSA has been previously used to generate predictive text for hand-held devices [ 15 ], and to model error correctingparsers[ 22 ].Inbothapplications,theSFSAisbuilt by studying n-grams using suffix/prefix relations in English text. SFSA has also been used in handwriting recognition to model the relation between image features like loops, cusps, or lines and the underlying alphabets and words [ 45 ]. In our methodology, the SFSA is modeled using script composition rules, and the transition probabilities are modeled on charac-ter or composite character n-grams. Some of the rules encode positional properties of components. For instance, descen-der components are placed below a core component (e.g. the modifier is written below to form: ), or the vowel modifier is made of the component written on top of . The edges E of our graph representation (Eq. 2 ) inher-ently encode the connectivity between components. The rec-ognition driven segmentation outlined in Procedure 1 trans-lates this information into a sequence of class labels; For instance the class label sequence for and are: and respectively. Due to this inherent ordering pres-ent in class labels, positional information is not specified explicitly in our implementation of script composition rules. Equation 1 represents a context sensitive grammar that describes the formation of valid Devanagari characters, and Table 2 describes the decomposition of characters into their sub-character primitives (components).

The SFSA is defined as follows:  X  = ( Q , O , I , F , A , X ,  X ) , where  X  Q : states  X  O : observations (72 component classes, Table 1 )  X  I : initial states, which indicate start of a character  X  F : final states, which indicate end of a character  X  A = a i , j ( o ) : set of transitions where a i , j ( o  X   X  : emission symbols (characters from Table 2 , and the  X   X  : A X  X  , a mapping from transitions to the emissions.
When a sequence of observations are accepted by the FSA, emissions generated by the corresponding transitions give the word result for this sequence. For example, the following transitions can accept the classifier labels given in Fig. 5 d and emit the correct word where S  X  I and T c , T d , T v  X  F and NT m represents a non-terminal state. Emissions on the third and fourth transi-tions allow us to represent the combination of components and to form . The sequence of emissions for this example would be: . The four transitions in this example represent the following script writing grammar rules: (i) The modifier is made of the ascender connected to a con-sonant and a vertical bar, and (ii) Only one vowel modifier can be attached to a consonant. The resulting FSA is shown in Fig. 6 .

In some SFSA models [ 45 ] the relation between states and observations is not apparent, and techniques like the Baum-Welch algorithm are used to learn transition probabilities. In our case, the observations are classifier symbols and the states are well-defined linguistic units indicating characters or composite characters. Using script composition rules, we parse the Devanagari text to obtain sequences of states for each word and the corresponding classifier symbols. Transi-tion probabilities for the SFSA are obtained from text corpus using the formula: a ( o ) = Count of observing o going from state i to j
Given a sequence of observation symbols, the best word hypothesis can be generated using the Viterbi algorithm. The Viterbi probability  X  i ( t ) , which is the highest probability of being in state i at time t is given by Eq. 4 . The classifier score is also incorporated into this process (Eq. 5 ).
  X  ( t ) = 1  X  ( t ) = 1
Tracing the transitions which generate  X  j ( T ) , we obtain the best word result corresponding to the matrix of compo-nent sequences ( O 1 ,..., O 4 ). 2.4 Hindi n-gram models We now describe language specific inputs to the SFSA model to obtain multiple word hypotheses. To enhance the recognition results provided by the SFSA, we studied n-gram frequencies from the EMILLE Hindi corpus [ 1 ]. After removing punctuation, digits, and non-Devanagari letters the corpus has 11,381,720 words and 59,524,848 letters. One-third of the corpus was set aside for testing and the remaining two-thirds was used for training. An n-gram model specifies the conditional probability of each unit, or token of text c with respect to its history c 1 ,..., c n  X  1 [ 10 ]: P (
We designed different n-gram models and report perplexity values over the test set ( Pp ): where q represents the probability assigned by an n-gram model. A perplexity of k indicates that while predicting a token, the language model is as surprised as a uniform, inde-pendent selection from k tokens [ 10 ]. While language models with low perplexity are desirable, perplexity measures have to be examined in the context of the number of classes in the model. This is because perplexity decreases with decreas-ing number of classes. Consider a hypothetical model which has a single class  X  X evanagari character X  and places all the tokens in our text into this category. Since this model places every token in the correct class (the class of  X  X evanagari character X ), it will have a perplexity of zero. However, plac-ing all tokens in a single group will not help an OCR. The token classes employed in an n-gram model are significant to model design and behavior. If two models have the same perplexity, it is likely that better modeling is being performed by the one having larger number of token classes. We exper-imented with models having different number of classes and studied the resulting variation in perplexity. Each model has a different binning approach where tokens of a specific type are included in a common class [ 10 ]. We have devised eight binning techniques for Devanagari and extract trigram and bigram frequencies. Some of our binning methods put char-acters having similar phonetic properties (Table 3 )intoasin-gle bin. Motivation for such grouping comes from linguistic studies which state that consonants having similar phonetic properties are likely to exhibit similar co-occurrence behav-iors [ 33 ]. In addition, each token c i can be a character giving us letter n-grams, or tokens can be composite characters, giv-ing us composite character n-grams. 1. Characters with no classing No classing is performed, 2. Phonetic grouping of vowels and consonants Tokens are 3. Aspirated plosive consonants in separate classes Similar 4. APCs in one class Similar to case 2, except that all APCs 5. Nasals in one class and APCs in separate classes 6. NCs in separate classes and APCs in a single class 7. NCs and APCs in separate classes Similar to case 2, 8. Composite characters with no classing Tokens corre-
We observe that with 5,538 classes, the composite charac-ter trigram and bigram perplexities are better than the char-acter language model which has only 126 classes (Table 4 ). We have incorporated both character and composite charac-ter n-gram frequencies into the SFSA word generation model  X  = ( Q , O , I , F , A , X , X ) . 2.4.1 Composite character n-grams in SFSA The transition and Viterbi probabilities in Eqs. 3 and 4 do not capture character or composite character frequencies. While Eqs. 3 and 4 are defined with respect to each state Q of the SFSA, the terminal and start states respectively indicate which character or composite has been emitted. For exam-ple, while accepting classifier symbols for the word (Fig. 6 ), the state sequences are: S 1  X  T c 2  X  T d 3  X  S 4  X  T c 5  X  S 6  X  T c 7  X  NT m 8  X  T v 9  X  S 10. In this example, S 1, S 4, S 6, and S 10 are start states, T T 5 , T c 7 , T v 9 areterminalstates,and NT m 8isanon-terminal state. To reach the terminal T c 2, the FSA emits , indicating that the character is the recognition result. At the transition to T d 3, it emits the character . The combination of these two characters gives the composite . This composite is the emission between states S 1 and S 4. Similarly, and are emissions between S4 X  X 6 and S6 X  X 10. Transitions between the terminal and start states can capture character or com-posite n-gram statistics, but are not reflected in Eqs. 4 and 5 . Therefore, we redefine the estimates as:
To obtain the top-n results from our SFSA, we perform a recursive search through the lattice of component results and get all possible word recognition results ranked by  X   X  j (Procedure 3 ). The time complexity of the search is expo-nential and would therefore be expensive for a generic graph. In practice, words have limited number of conjunct and descender hypotheses and an exhaustive search through the lattice of component results can be performed in real time. Dictionary lookup has been implemented using a lexicon of 24,391 words taken from the ILT data set. The lookup is performed using Levenshtein string edit distance with unit weights for substitution, deletion, and addition. The dictio-nary entry which gives minimum Levenshtien edit distance with SFSA output is taken as the dictionary corrected result. Procedure 3 GetWords( O ,  X  ) 3 Experimental results and analysis Our OCR was tested on 10,606 words from the University at Buffalo ILT data set. The data set contains images scanned at 300dpi from varied sources such as magazines, newspapers, contemporary, and period articles. After binarization, line, and word separation, a test set of composite characters and components was extracted from word images using a semi-automated character segmentation technique [ 17 ]. The test set contains 8,209 samples of isolated consonants and vowels belonging to 40 classes, 686 samples of conjuncts belonging Procedure 4 Parse( V , O , t , S , E , pos ,  X  ) Procedure 5 Backtrack( V , O , t , S , E , pos ,  X  ) to 97 classes, 479 samples of consonant X  X escenders belong-ing to 56 classes, and approximately 400 samples each of isolated ascenders and descenders. While many more valid conjunct and consonant X  X escender character classes can be obtained by combining the 38 consonants and 5 descenders, clean samples corresponding to only 153 (97+56) classes could be obtained from the data set. 3.1 Recognition of composite characters The accuracy of recognition of composite characters is ana-lyzed with reference to the three procedural stages of our system: (i) accepting and recognizing an independent con-sonant/vowel and rejecting a conjunct/consonant X  X escender using the component classifier as indicated in Procedure 1 , (ii) identifying the direction of segmentation for the rejected images as listed in Procedure 1 , and (iii) using the graph to segment a composite from left to right or top to bottom and recognizing the constituent components as per Procedure 2 .
In order to test stage (i), samples of consonants/vowels, conjunct and descender characters were input to the classifiers. False Accept Rate (FAR X  X  conjunct/descender is wrongly accepted), and False Reject Rates (FRR X  X  con-sonant/vowel is wrongly rejected) for different classifiers is reported in Table 5 . The SFSA model is designed to use all classifier results and handle over-segmentation. False rejects can therefore be tolerated by the system better than false accepts. In our case, the best FAR and FRR rates are obtained using the GSC neural network classifier: 5.83% FRR and 6.88% FAR. Once a consonant/vowel sample is identified, it is classified into one of the consonant/vowel classes. The average accuracy of recognizing such consonants/vowels is in the range of 94 X 97% depending on the features used and the classification scheme (Fig. 7 ).

A majority of the composites that contribute to FAR are conjuncts that are formed by adding small modifiers to the consonant, for example, are formed by adding a small stroke (called rakaar) to the consonants . The clas-sifier confuses such rakaar-characters with the closest con-sonant. The most frequent rakaar character in our data set is , and is frequently recognized as with a high con-fidence. One solution to this problem is to include in the class space of isolated core components and re-train the classifier. However, this still results in significant confusion between the two classes. This problem could also be poten-tially overcome during post-processing using a suitable lan-guage model.

In stage (ii), the system uses structural features from the graph to label the characters as either conjunct/consonant X  descenders or conjuncts. This stage has been tested using a data set containing 846 words which have at least one conjunct consonant, and 727 words having at least one con-junct X  X escender or consonant X  X escender. The operations are performed at the word level in order to estimate the baseline. Baselines are calculated for each word, and the characters are classified as conjuncts or conjunct/consonant X  X escend-ers. The typical errors in this step are due to the mislabeling of consonant X  X escenders like as vertically joined conjunct characters.

Results for stage (iii) are shown in Table 6 . This stage has been tested using 686 conjunct character images and 479 conjunct/consonant X  X escender images. Top choice accuracy in recognizing these composites ranges from 26 to 38%, whereas the top-5 choice result ranges from 91 to 98%. Empirical analysis shows that errors are due to confusion between similar consonants/vowels; e.g. being recog-nized as ,or . Higher error rates are also seen in composites which contain three or more components, e.g. Devanagari text (0.47% in CEDAR-ILT, and 0.64% in EMILLE). Better modeling is required to overcome these issues at the character recognition level. 3.2 Word recognition Word recognition performance was measured using four dif-ferent stochastic finite state automaton (SFSA) models: (i) Script writing grammar, with no n-gram statistics, (ii) SFSA with character bigrams, (iii) SFSA with composite character bigrams, and (iv) (SFSA) with composite character trigrams. A neural network classifier using GSC features was used for the experiments.

Table 6 summarizes the word recognition performance on the test set using the four models. Although the exhaustive search through the lattice (Procedures 3 , 4 , 5 ) is exponential in time complexity, it becomes tractable given the small num-ber of conjunct and conjunct/consonant X  X escender hypoth-eses (less than ten). On average, one character generates 7 consonant X  X escender or conjunct hypotheses using our BAG approach. We achieved word level accuracy of 74.96% for the top choice using composite character bigram model with dictionary lookup. Examples of BAG generation and recog-nition on words of different fonts are shown in Fig. 8 . 3.3 Comparison with prior methods Our recognition driven segmentation paradigm is able to over-segment characters along non-linear boundaries when needed using the BAG representation. Classifier confidence and language models are used to select the correct segmenta-tion hypothesis. Previous techniques for conjunct character segmentation have used linear segmentation boundaries [ 5 , 16 ]. Previous techniques for classification have used a picture language description (PLANG) of features [ 2 , 36 ] and also decision trees [ 9 ]. These techniques use structural features specific to the Devanagari script such as vertical bars both during segmentation as well as subsequent classification. For comparison purposes, we used a recognizer developed in our lab using similar structural features [ 17 , 24 , 25 ].
The top-choice accuracy of the character classifier using structural features on the ILT test set is 74.53%, which is significantly higher than the top-choice accuracy of 26 X 38% obtained using the proposed method (Table 6 ). However, our method produces several segmentation hypotheses leading to a top-5 accuracy of 85 X 98%. We use linguistic information to reject the incorrect segmentation hypotheses. Given the minor variations in character shapes between many classes, visual features alone are not sufficient. A language model is used to provide additional discriminating power. The results with and without dictionary lookup for the  X  X op-1, No n-gram X  run are 32.99 and 13.25%, respectively (Table 7 ). Corresponding values for  X  X op-20, No n-gram X  are 87.32 and 78.17% , and for  X  X op-1 with character bigram X  the values are 74.71 and 70.17%. Figure 9 shows accuracy across differ-ent documents. The top-5 accuracy of the recognition driven BAG segmentation ranges from 72 to 90%, and the top-1 choice accuracy ranges from 62 to 85%. In comparison, top-1 accuracy of the segmentation driven approach ranges from 39 to 75%. Further, the segmentation driven approach does not provide multiple word results. Table 8 shows a comparison of results of Devanagari OCR systems reported in the literature. The recognition-driven OCR method shows significant gains over a purely segmentation-driven approach when measured on the same data set. The character level accuracy of the cur-rent method is also higher than other results reported in the literature and comparable to results reported at the word level on other closed data sets. It has to be borne in mind that these numbers have been reported on different varied closed data sets. 4 Summary and future work This paper presents a novel recognition driven segmenta-tion methodology (Fig. 10 ) for Devanagari script OCR using the hypothesize and test paradigm. Composite characters are segmented along non-linear boundaries using the block adja-cency graph representation, thus accommodating the natu-ral breaks and joints in a character. The methodology used can readily accommodate commonly used feature extraction and classification techniques [ 6 , 13 , 31 ]. A stochastic model for word recognition has been presented. It combines classi-fier scores, script composition rules, and character n-gram statistics. Post-processing tools such as word n-grams or sentence-level grammar models are applied to prune the top-n choice results.

We have not considered special diacritic marks like avag-raha, udatta, anudatta , special consonants such as , punctuation and numerals. Symbols such as anusvara, visa-rga and the reph character often tend to be classified as noise. A Hindi corpus has been used to design the language model. Corpora from other languages that use the Devanagari script remain to be investigated.
 References
