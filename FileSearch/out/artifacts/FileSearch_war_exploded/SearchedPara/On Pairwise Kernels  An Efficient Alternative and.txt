 Most phenomena in the world can be represented by sets of entities, and sets of static and dynamic relationships among the entitie s. Such relationships include friendships among people, actions such as someone clicking an on-line advertisement, and phys-ical interactions among proteins. Supervised pairwise prediction aims to predict such pairwise relationships based on known rela tionships. It has many applications includ-ing network prediction, entity resolution, and collaborative filtering. Models for pair-wise prediction should take a pair of instances as its input, and output the relationship between the two instances. In this paper, we f ocus on pairwise classification problem, where the task is to predict whether or not a relation exists between given two nodes, and we apply kernel methods [1] to this problem. To apply kernel methods to pairwise classification, we need to define a kernel func tion between two pairs of instances. Inter-estingly, three research groups have independently proposed an exactly same pairwise kernel by combining two instance-wise kernel functions [2,3,4]. The proposed pairwise kernel matrix is considered as a Kronecker product of two instance-wise kernel matri-ces. However, the pairwise kernel is signifi cantly time-and-space-c onsuming since the pairwise kernel matrix is huge. For this reason, only sampled training data have been used in most of its applications.

In this paper, we propose a new pairwise kernel called Cartesian kernel as a more efficient alternative to the existing pairwise kernel (which we refer to as Kronecker ker-nel ). The proposed kernel is defined as a Kronecker sum of two instance-wise kernel matrices, and therefore more computational-and space-efficient th an the existing pair-wise kernel. The experimental results using numbers of real network data show that the proposed pairwise kernel is much faster than the existing pairwise kernel, and at the same time, competitive with the existing pairwise kernel in predictive performance. Fi-nally, we give the generalization bounds of the two pairwise kernels by using eigenvalue analysis of the kernel matrices [5,6]. In this section, we introduce the definition of t he (binary) pairwise classification prob-lem and review the existing pairwise kernel independently proposed by three research f : V  X  X  +1 ,  X  1 } ,where V indicates the set of all possible instances. On the f : V (1)  X  V (2)  X  X  +1 ,  X  1 } ,where V (1) and V (2) are two sets of all possible in-stances. Let us assume that we are given a | V (1) | X | V (2) | class matrix F whose ele-ments have one of +1 (positive class),  X  1 (negative class), and 0 (unknown). Our task is to fill in the unknown parts of the class matrix which have 0 value. In the context of link prediction, the F can be regarded as the adjacency matrix for a network including V (1) and V (2) as its nodes. The [ F ] i v [
F ] matrix, we have two kernel matrices K (1) and K (2) for V (1) and V (2) , respectively. In exchangeable cases, K (1) = K (2) := K . Note that those kernel matrices are positive semi-definite.

Since we are interested in classification of pairs of instances, we need a kernel func-tion between two pairs of instances if we apply kernel methods [1] to this problem. In many case, it is rather easy to design kernels for two basic instances, so we con-struct pairwise kernels by using these instance-wise kernels as building blocks. As-( v v 1 are similar, and at the same time, define the pairwise similarity as the pr oduct of two instance-wise similarities as
Since products of Mercer kernels are also Mercer kernels [1], the above similarity is symmetrized as eters of the kernel machine. In excha ngeable and symmetric cases, it becomes [
F ] for ( v j 1 ,v j 2 ) such that j 1  X  j 2 is not used because of symmetry. The kernel matrix v be interpreted as a weighted adjacency matr ix of the Kronecker product graph [8] of the two graphs whose weighted adjacency matrices are the instance-wise kernel matrices. Therefore, we refer to this pairwise kernel as Kronecker kernel to distinguish from the one we will propose in the next section. In this section, we propose a more efficient pairwise kernel. At the end of the previous section, we mentioned the relationship between the existing pairwise kernel and a Kro-necker product graph. So, it is natural to imagine that we can design another pairwise kernel based on another kind of product gra ph. In this paper, we adopt another kind of product graph called Cartesian product graph [8]. Assume that we have two graphs G (1) and G (2) whose sets of nodes are V (1) and V (2) , respectively. The product graph of
G (1) and G (2) has nodes V (1)  X  V (2) , each of whose nodes is defined as a pair of product graph. In Kronecker product graphs, a link between these two pairs exists if v We can notice that the condition for a link existing in Cartesian product graphs is more strict than that for Kronecker product graphs.

Inspired by the definition of the Cartesian product graph, we define the Cartesian where  X  is an indicator function, which returns 1 when its argument is true and 0 other-wise. Since  X  is considered as an identity kernel, the above similarity measure is also a Mercer kernel if the element-wise kernels are Mercer kernels. In exchangeable and symmetric cases, the kernel between ( v i 1 ,v i 2 ) and ( v j 1 ,v j 2 ) is symmetrized as
The kernel matrix of the Cartesian kernel is equivalently written as the Kronecker sum [7] of two instance-wise kernel matrices as K  X  = K (2)  X  K (1) , where the Kronecker sum operation is defined as K (2)  X  K (1) = K (2)  X  I + I  X  K (1) . At the first sight, the size of the Cartesian kernel matrix is the s ame as that of the Kronecker kernel. But, the number of the non-zero elements in the kernel matrix is much smaller, since the Cartesian kernel is based on the Kronecker products of an element-wise kernel matrix and an identity matrix.

Finally, we mention computational efficiency of the Cartesian kernel. While the Kro-necker kernel can give a score greater than 0 between arbitrary two pairs, the Cartesian kernel can give a non-zero value only to the pairs which share one of their instances. This fact indicates that the Cartesian kernel is much faster than the Kronecker kernel. In this section, we show several experimental results in which we compare the Kro-necker kernel and the Cartesian kernel.

We used three data sets for network prediction. Two of them are biological net-works, and one of them is a social network. All data are symmetric networks. The first data set [9] contains the metabolic pathway network of the yeast S. Cerevisiae in the KEGG/PATHWAY database [10]. Proteins are represented as nodes, and a symmetric edge indicates that the two proteins are enz ymes that catalyze successive reactions. The number of nodes in the network is 618, and the number of links is 2,782. In this data set, four element-wise kernel matrices ba sed on gene expressions, chemical informa-tion, localization sites, and phylogenetic profiles are given. We used them as the kernel network constructed by von Mering et al. [11]. We followed Tsuda and Noble [12], and used the medium confidence network. This network contains 2 , 617 nodes and 11 , 855 symmetric links. Each protein is given a 76 -dimensional binary vector, each of whose dimensions indicates whether or not the protein is related to a particular function. We used the inner products of the vectors as the element-wise kernel matrix 2 . The third data set is a social network representing the co-authorships in the NIPS conferences, containing 2 , 865 nodes and 4 , 733 links. Authors correspond to nodes, and a symmet-ric link between two nodes indicates that there is at least one co-authored paper by the corresponding authors. Each author is give n a feature vector, each of whose dimensions corresponds to occurrences of a particular word in the author X  X  papers. We used the in-ner product of the vectors as the element-wise kernel matrix 3 . All of the element-wise kernel matrices are normalized so that all of their diagonals are 1 . The models were trained by using PUMMA [13], an on-line learning algorithm whose solutions asymp-totically converge to those by the support vector machine with squared hinge loss. The hyperparameter for regularization was set to C =1 . All of the training data were pro-cessed thee times in the training phase. The results were evaluated in AUC by 5 -fold cross validation with 20% of all of the pairwise relationships as training data.
Now, we show the predictive performances and execution times by the two pairwise kernels. In Fig. 1, the gray bars indicate t he AUCs by the Kronecker kernel, and the black bars represent the AUCs by the Cartesi an kernel. The error bars indicate the stan-dard deviations of the AUC values. In the upper figure of Fig. 1, each of the pairs of AUC bars indicates the results when we used gene expressions, chemical information, phylogenetic profiles, or localization sites for element-wise kernel matrices. In the lower figures, the left pair of the AUC bars is for the protein-protein interaction networks, and the right one is for the co-authoring network.

We can observe that the predictive perform ance of the Cartesian kernel is competi-tive with that of the Kronecker kernel excep t for the co-authoring network. The reason for the degraded performance by the Cartesi an kernel in the co-authoring network is not clear, and we could not find the reason in the theoretical analysis in the following sec-tion. But it might be relaled to network sparsity (the co-authoring network is the most sparse), or differences between natures of biological networks and social networks. Figure 2 shows the average training time for each data set in log scale. We can see that the Cartesian kernel is at most 16 times fast er than the Kronecker kernel. The differ-ences are remarkable when the network size is large. Based on the above results, we conclude that the Cartesian kernel is a pro mising alternative to the Kronecker kernel especially for large data sets. In this section, we discuss generalization bounds for the Kronecker kernel and the Carte-sian kernel. It is known that the generalization bound for a kernel machine such as a sup-port vector machine is given by using the distribution of the eigenvalues of the kernel matrix [5,6]. To compute the generalization bounds, we need to compute the eigenval-ues of the Kronecker product K  X  or the Kronecker sum K  X  of the two instance-wise kernel matrices. It is difficult to directly compute the eigenvalues for the Kronecker product or the Kronecker sum, since their size are very huge.However, we can compute them from the eigenvalues of the instance-wi se kernel matrices by using the following theorem [7].
 and K (2) , respectively. The set of eigenvalues of the Kronecker product K (2)  X  K (1) is {  X  Figure 3 (left) shows the eigenvalues of the Kronecker kernel matrix and the Cartesian kernel matrix derived from phylogenetic profiles for the KEGG metabolic network. Also, Figure 3 (right) shows the eigenvalues for the co-authoring network. The two pairwise kernels, the Kronecker kernel and t he Cartesian kernel, yield very different eigenvalue distributions. Although we do not show the eigenvalues for the other net-works, the general trend is that the high-rank ed eigenvalues of the Kronecker kernel are larger than those of the Cartesian kernel, wh ile the low-ranked eigenvalues of the Carte-of the Kronecker product decay faster than those of the Kronecker sum.

In the following analysis, we assume that the set of all possible data points X = { x 1 ,x 2 ,...,x M } ( X = V (1)  X  V (2) in our case of pairwise classification) are known in advance of the training phase. Let Y = { 1 ,  X  1 } be the set of labels. We also assume that ( x, y )  X  Z = X  X  Y follows a certain distribution P ( x, y ) .The expected risk of a hypothesis h  X  X  is given by R ( h )= ( x,y )  X  Z  X  ( h ( x ) y  X  0) P ( x, y ) . Given a set of labeled samples { ( x i ,y i ): i  X  X  1 ,...,m }} from Z with size m&lt;M ,the empirical margin risk for a certain margin  X  is defined by the rate of the samples with h ( x i ) y i &lt; X  : R  X  s ( h )= 1 gives an upper bound for the expected risk.
 Theorem 2. Let  X  1  X   X  2  X  X  X  X  X  X   X  M be the set of eigenvalues of sider the hypothesis class F ( c ) B = { w, x + b : w  X  c, | b | X  B } .Then P where The generalization bounds computed for the metabolic network and the co-authoring network are shown in Fig. 4. The bounds for the Kronecker kernels are smaller than the bounds for the Cartesian kernels. Those theoretical results are consistent with the experimental results in the previous section, where the Kronecker kernels were slightly superior to the Cartesian kernels, although there were a few exceptions and the differ-ences were subtle in the most of the cases. As mentioned in the preceding work [5], the bounds are not very tight. Also, the theoretical result gives no explanation of the large performance difference in the co-authoring network. In future work, we will investigate tighter bounds including the several possib ilities for impr ovements mentioned in the preceding work [5].

