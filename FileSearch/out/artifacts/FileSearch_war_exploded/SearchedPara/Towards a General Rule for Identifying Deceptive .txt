 Consumers increasingly rely on user-generated online reviews when making purchase deci-sion (Cone, 2011; Ipsos, 2012). Unfortunately, the ease of posting content to the Web, poten-tially anonymously, creates opportunities and in-centives for unscrupulous businesses to post de-ceptive opinion spam  X  X ictitious reviews that are deliberately written to sound authentic, in order to to be widespread and growing concern among both businesses and the public about this poten-tial abuse (Meyer, 2009; Miller, 2009; Streitfeld, 2012; Topping, 2010; Ott, 2013).

Existing approaches for spam detection are usu-ally focused on developing supervised learning-based algorithms to help users identify decep-tive opinion spam, which are highly dependent upon high-quality gold-standard labeled data (Jin-dal and Liu, 2008; Jindal et al., 2010; Lim et al., 2010; Wang et al., 2011; Wu et al., 2010). Stud-ies in the literature rely on a couple of approaches for obtaining labeled data, which usually fall into two categories. The first relies on the judge-ments of human annotators (Jindal et al., 2010; Mukherjee et al., 2012). However, recent stud-ies show that deceptive opinion spam is not eas-ily identified by human readers (Ott et al., 2011). An alternative approach, as introduced by Ott et al. (2011), crowdsourced deceptive reviews using works have been introduced based on Ott et al. X  X  dataset, including estimating prevalence of decep-tion in online reviews (Ott et al., 2012), identifica-tion of negative deceptive opinion spam (Ott et al., 2013), and identifying manipulated offerings (Li et al., 2013b).

Despite the advantages of soliciting deceptive gold-standard material from Turkers (it is easy, large-scale, and affordable), it is unclear whether Turkers are representative of the general popula-tion that generate fake reviews, or in other words, Ott et al. X  X  data set may correspond to only one type of online deceptive opinion spam  X  fake re-views generated by people who have never been to offerings or experienced the entities. Specifi-cally, according to their findings (Ott et al., 2011; Li et al., 2013a), truthful hotel reviews encode more spatial details, characterized by terms such as  X  X athroom X  and  X  X ocation X , while deceptive re-views talk about general concepts such as why or with whom they went to the hotel. However, a hotel can instead solicit fake reviews from their employees or customers who possess substantial domain knowledge to write fake reviews and en-code more spatial details in their lies. Indeed, cases have been reported where hotel owners bribe guests in return for good reviews on TripAdvi-they were satisfied customers and write glowing The domain knowledge possessed by domain ex-perts enables them to craft reviews that are much more difficult for classifiers to detect, compared to the crowdsourced fake reviews.

Additionally, existing supervised algorithms in the literature are usually narrowed to one spe-cific domain and heavily rely on domain-specific vocabulary. For example, classifiers assign high weights to domain-specific terms such as  X  X otel X ,  X  X ooms X , or even the name of the hotels such as  X  X ilton X  when trained on reviews on hotels. It is unclear whether these classifiers will perform well at detecting deception in other domains, e.g., Restaurant or Doctor reviews. Even in a single do-main, e.g., Hotel, classifiers trained from reviews of one city (e.g., Chicago) may not be effective if directly applied to reviews from other cities (e.g., New York City) (Li et al., 2013b). In the exam-ples in Table 1, we trained a linear SVM clas-sifier on Ott X  X  Chicago-hotel dataset on unigram features and tested it on a couple of different do-mains (the details of data acquisition are illustrated in Section 3). Good performance is obtained on Chicago-hotel reviews (Ott et al., 2011), but not as good on New York City ones. The performance is reasonable in Restaurant reviews due to the many shared properties among restaurants and hotels, but suffers in Doctor settings.

In this paper, we try to obtain a deeper under-standing of the general nature of deceptive opin-ion spam. One contribution of the work presented here is the creation of the cross-domain (i.e., Ho-tel, Restaurant and Doctor) gold-standard dataset. Table 1: SVM performance on datasets for a clas-sifier trained on Chicago hotel review based on Unigram feature.
 In contrast to existing work (Ott et al., 2011; Li et al., 2013b), our new gold standard includes three types of reviews: domain expert deceptive opinion spam ( Employee ), crowdsourced deceptive opin-ion spam ( Turker ), and truthful Customer reviews ( Customer ). In addition, some of domains contain both positive ( P ) and negative ( N ) reviews. 6
To explore the general rule of deceptive opinion spam, we extended SAGE Model (Eisenstein et al., 2011), a bayesian generative approach that can capture the multiple generative facets (i.e., decep-tive vs truthful, positive vs negative, experienced vs non-experienced, hotel vs restaurant vs doctor) in the text collection. We find that more general features, such as LIWC and POS, are more robust when modeled using SAGE, compared with just bag-of-words.

We additionally make theoretical contributions that may shed light on a longstanding debate in the literature about deception. For example, in con-trast to existing findings that highlight the lack of spatial detail in deceptive reviews (Ott et al., 2011; Li et al., 2013b), we find that a lack of spatial de-tail may not be a universal cue to deception, since it does not apply to fake reviews written by domain experts. Instead, our finding suggest that other lin-guistic features may offer more robust cues to de-ceptive opinion spam, such as overly highlighted sentiment in the review or the overuse of first-person singular pronouns.
 The rest of this paper is organized as follows. In Section 2, we briefly go over related work. We describe the creation of our data set in Section 3 and present our model in Section 4. Experimental results are shown in Section 5. We present anal-ysis of general cues to deception in Section 6 and conclude this paper in Section 7. Spam has been historically studied in the contexts of Web text (Gy  X  ongyi et al., 2004; Ntoulas et al., 2006) or email (Drucker et al., 1999). Recently there has been increasing concern about deceptive opinion spam (Jindal and Liu, 2008; Ott et al., 2011; Wu et al., 2010; Mukherjee et al., 2013b; Wang et al., 2012).

Jindal and Liu (2008) first studied the deceptive opinion problem and trained models using features based on the review text, reviewer, and product to identify duplicate opinions, i.e., opinions that appear more than once in the corpus with simi-lar contexts. Wu et al. (2010) propose an alter-native strategy to detect deceptive opinion spam in the absence of a gold standard. Yoo and Gret-zel (2009) gathered 40 truthful and 42 deceptive hotel reviews and manually compare the linguis-tic differences between them. Ott et al. created a gold-standard collection by employing Turkers to write fake reviews, and follow-up research was based on their data (Ott et al., 2012; Ott et al., 2013; Li et al., 2013b; Feng and Hirst, 2013). For example, Song et al. (2012) looked into syntactic features from Context Free Grammar parse trees to improve the classifier performance. A step fur-ther, Feng and Hirst (2013) make use of degree of compatibility between the personal experiment and a collection of reference reviews about the same product rather than simple textual features.
In addition to exploring text or linguistic fea-tures in deception, some existing work looks into customers X  behavior to identify deception (Mukherjee et al., 2013a). For example, Mukher-jee et al. (2011; 2012) delved into group behavior to identify group of reviewers who work collabo-ratively to write fake reviews. Qian and Liu (2013) identified multiple user IDs that are generated by the same author, as these authors are more likely to generate deceptive reviews.

In the psychological literature, researchers have looked into possible linguistic cues to deception (Newman et al., 2003), such as decreased spatial detail , which is consistent with theories of reality monitoring (Johnson and Raye, 1981), increased negative emotion terms (Newman et al., 2003), or the writing style difference between informative (truthful) and imaginative (deceptive) writings in (Rayson et al., 2001). The former typically con-sists of more nouns, adjectives, prepositions, de-terminers, and coordinating conjunctions, while the latter consists of more verbs, adverbs, pro-nouns, and pre-determiners.
 SAGE (Sparse Additive Generative Model): SAGE is an generative bayesian approach in-troduced by Eisenstein et al. (2011), which can be viewed as an combination of topic mod-els (Blei et al., 2003) and generalized additive models (Hastie and Tibshirani, 1990). Unlike other derivatives of topic models, SAGE drops the Dirichlet-multinomial assumption and adopts a Laplacian prior, triggering sparsity in topic-word distribution. The reason why SAGE is tailored for our task is that SAGE constructs multi-faceted la-tent variable models by simply adding together the component vectors rather than incorporating mul-tiple switching latent variables in multiple facets. In this section, we report our efforts to gather gold-standard opinion spam datasets. Our datasets con-tain the following domains, namely Hotel, Restau-rant, and Doctor. 3.1 Turker set, using Mechanical Turk Crowdsourcing services such as AMT greatly fa-cilitate large-scale data annotation and collection efforts. Anyone with basic programming skills can create Human Intelligence Tasks (HITs) and ac-cess a marketplace of anonymous online workers (Turkers) willing to complete the tasks. We bor-rowed some rules used by Ott et al. to create their dataset, such as restricting task to Turkers located in the United States, and who maintain an approval rating of at least 90% .
 Hotel-Turker : We directly borrowed datasets Restaurant-Turker : We gathered 20 positive (P) deceptive reviews for each of 10 of the most popular restaurants in Chicago, for a total of 200 positive deceptive restaurant reviews.
 Doctor-Turker : We gathered a total number of 200 positive reviews from Turkers. 3.2 Employee set, by domain experts We seek deceptive opinion spam written by people with expert-level domain knowledge. It is not ap-propriate to use crowdsourcing to obtain this data, so instead we solicit reviews written by employees in each domain.
 Hotel-Employee: We asked two hotel employ-ees from each of seven hotels (14 employees to-tal) each to write 10 deceptive positive-sentiment reviews of their own hotel, and 10 deceptive negative-sentiment reviews of their biggest local competitor X  X  hotel. In total, we obtained 280 de-ceptive reviews of 14 hotels, including a balanced mix of positive-and negative-sentiment reviews. Restaurant-Employee: We asked employees from selected restaurants (a waiter/waitress or cook) to each write positive-sentiment reviews of their restaurant.
 Doctor-Employee: We asked real doctors to write positive fake reviews about themselves. In total we obtained 32 reviews from 15 doctors. 3.3 Customer set from Actual Customers Hotel-Customer: We borrowed from Ott et al. X  X  dataset.
 Restaurant/Doctor-Customer: We solicited data by matching a set of truthful reviews as Ott et al. did in collecting truthful hotel reviews. 3.4 Summary for Data Creation Statistics for our data set is presented in Table 2. Due to the difficulty in obtaining gold-standard data in the literature, there is no doubt that our data set is not perfect. Some parts are missing, some are unbalanced, participants in the survey may not be representative of the general population. How-ever, as far as we know, this is the most compre-hensive dataset for deceptive opinion spam so far, and may to some extent shed insights on the nature of online deception. In this section, we briefly describe our model. Since mathematics are not the main theme of this paper, we omit the exact details for inference, which can be found in (Eisenstein et al., 2011).
Before describing the model in detail, we note the following advantages of the SAGE model, and our reasons for using it in this paper: 1. the  X  X dditive X  nature of SAGE allows a better 2. For cross-domain classification task, standard 4.1 Model In SAGE, each term w is drawn from a distribution  X  term frequency,  X  y frequency deviation representing topic z n , facet y , and the second-order interaction part respec-tively. Superscripts T , A and I respectively denote the index of the topic, facet, and second-order in-teraction. In our task, we adapt the SAGE model as follows:
Y = { y Sentiment  X  X  positive, negative } , We model three  X   X  X , one for each type of y . Let i,j,k denote the index of the different types of y , so that each term w is drawn as follows: where the higher order parts denote the interac-tions between different facets.

In our approach each document-level feature f is drawn from the following distribution: value of feature f . For each review d , the proba-bility that it is drawn from facets with index i,j,k is as follows: In the training process, parameters  X  ( w ) y and  X  ( f ) are to be learned by maximizing the posterior distribution following the original SAGE training procedure. For prediction, we estimate y Source for each document given all or part of  X  ( w ) y and  X  ( f ) y as follows: given for each document d . Note that we as-sume conditional independence between features and words given y , similar to other topic mod-els (Blei et al., 2003). Notably, our revised SAGE model degenerates into a model similar to Gen-eralized Additive Model (Hastie and Tibshirani, 1990) when word features are not considered. In this section, we report our experimental results. We first restrict experiments to the within-domain task and see what features most characterize the deceptive reviews, and how. We later extend it to cross domains to explore a more general classifier of deceptive opinion spam. 5.1 Intra-Domain Classification We explore the effect of both domain experts and crowdsourcing workers on intra-domain de-ception. Specifically, we reframe it as a intra-domain multi-class classification task , where given the labeled training data from one domain, we learn a classifier to classify reviews accord-ing to their source, i.e., Employee , Turker and Customer . Since the machine learning classi-fier is trained and tested within the same domain,  X 
We use a One-Versus-Rest (OvR) scheme, in which we train m classifiers using SAGE, such that each classifier f i , for i  X  [1 ,m ] , is trained to distinguish between class i on the one hand, and all classes except i on the other. To make an m -way decision, we then choose the class c with the most confident prediction. OvR approaches have been shown to produce state-of-art performance compared to other multi-class approaches such as Multinomial Naive Bayes or One-Versus-One clas-sification scheme. We train the OvR classifier on three sets of features, LIWC , Unigram , and POS . 9
Multi-class classification results are given at Ta-ble 3. We report both OvR performance and the performance of three One-versus-One binary clas-sifiers, trained to distinguish between each pair of classes. In particular, the three-class classifier is around 65% accurate at distinguishing between Employee , Customer , and Turker for each of the domains using Unigram, significantly higher than random guess. We also observe that each of the three One-versus-One binary classifications per-forms significantly better than chance, suggesting that Employee , Customer , and Turker are in fact three different classes. In particular, the two-class classifier is around 0.76 accurate in distinguish-ing between Turker and Employee reviews, de-spite both kinds of reviews being deceptive opin-ion spam.

Best performance is achieved on Unigram fea-tures, constantly outperforming LIWC and POS features in both three-class and two-class settings in the hotel domain. Similar results are observed for restaurant and doctor domains and details are excluded for brevity. This suggests that a universal set of keyword-based deception cues (e.g., LIWC) is not the best approach for Intra-Domain Classifi-cation. Similar results were also reported in previ-ous work (Ott et al., 2012; Ott, 2013). 5.2 Cross-domain Classification In this subsection, we frame our problem as a domain adaptation task (Pan and Yang, 2010). Again, we explore 3 feature sets: LIWC , Uni-gram and POS . We train a classifier on hotel re-views, and evaluate the performance on other do-mains. For simplicity, we focus on truthful ( Cus-tomer ) versus deceptive ( Turker ) binary classifi-cation rather than a multi-class classification. ble 4. We first observe that classifiers trained on hotel reviews apply well in the restaurant domain, which is reasonable due to the many shared prop-erties among restaurants and hotels. Among three types of features, Unigram still performs best. POS and LIWC features are also robust across do-mains.

In the doctor domain, we observe that models trained on Unigram features from the hotels do-main do not generalize well to doctor reviews, and the performance is a little bit better than random guess with only 0.55 accuracy. For SVM, models trained on POS and LIWC features achieve even lower accuracy than Unigram. POS and LIWC features obtain around 0.5 precision and 1.0 re-call, indicating that all doctor reviews are classi-fied as deceptive by the classifier. One plausible explanation could be doctor reviews generally en-code some type of positive-weighted (deceptive) features more than hotel reviews and these types of features dominate the decision making proce-dures, leading all reviews to be classified as de-ceptive.
 Tables 5 and 6 give the top weighted LIWC and POS features. We observe that many features are indeed shared among doctor and hotel domains. Notably, POS features are more robust than LIWC as more shared features are observed. As domain specific properties will be considered in the in-Table 5: Top weighted LIWC features for Turker vs Customer in Doctor and Hotel reviews. Blue denotes shared positive (deceptive) features and red denotes negative (truthful) features. tive model, SAGE achieve much better results than SVM, and is around 0.65 accurate in the cross-domain task. In this section, we examine a number of general POS and LIWC features that may shed light on a general rule for identifying deceptive opinion NORMALIZED from the log-frequency function. Table 6: Top weighted POS features for Turker vs Customer in Doctor and Hotel reviews. Blue de-notes shared positive (deceptive) features and red denotes negative (truthful) features. spam. Our modified SAGE model provides us with a tailored tool for this analysis. Specifically, each feature f is associated with a background value m f . For each facet A ,  X  f specific preference value for feature f . Note that sentiments are separated into positive and negative dimensions, which is necessary because hotel em-ployee authors wrote positive-sentiment reviews when reviewing their own hotels, and negative-sentiment reviews when reviewing their competi-tors X  hotels. 6.1 POS features Early findings in the literature (Rayson et al., 2001; Buller and Burgoon, 1996; Biber et al., 1999) found that informative (truthful) writings typically consist of more nouns, adjectives, prepo-sitions, determiners, and coordinating conjunc-tions, while imaginative (deceptive) writing con-sist of more verbs, adverbs, pronouns, and pre-determiners (with a few exceptions). Our find-ings with POS features are largely in agreement with these findings when distinguishing between Turker and Customer reviews, but are violated in the Employee set.
 We present the eight types of POS features in Figure 1, namely, N (Noun), JJ (Adjective), IN (Preposition or subordinating conjunction) and DT (Determiner), V (Verb), RB (Adverb), PRP (Pro-nouns, both personal and possessive) and PDT (Pre-Determiner).

From Figures 1(a)(b)(e)(f), we observe that with the exception of PDT, the word frequency of which is too small to draw a conclusion, Turker and Customer reviews exhibit linguistic patterns in agreement with previous findings in the literature, where truthful reviews ( Customer ) tend to include more N, JJ, IN and DT, while deceptive writings tend to encode more V, RB and PRP.

However, in the case of the Employee-Positive dataset, which is equally deceptive, most of these rules are violated. Notably, reviews from the Employee-Positive set did not encode fewer N, JJ and DT terms, as expected (see Figures 1(a)(c)). Instead, they encode even more N, JJ and DT vocabularies than truthful reviews from the Cus-tomer reviews. Also, fewer V and RB are found in Employee-Positive reviews compared with Cus-tomer reviews (see Figures 1(e)(g)).

One explanation for these observations is that informative (truthful) writing tends to be more in-troductory and descriptive, encoding more con-crete details, when compared with imaginary writ-ings. As domain experts possess considerable knowledge of their own offerings, they highlight normalized from the log-frequency function. the details and their lies may be even more in-formative and descriptive than those generated by real customers! This explains why Employee-Positive contains more N, IN and DT. Meanwhile, as domain experts are engaged more in talking about the details, they inevitably overlook other information, possibly leading to fewer V and RB.
For Employee-Positive reviews, shown in Fig-ures 1(d)(h), it turns out that domain experts do not compensate for their lack of prior experience when writing negative reviews for competitors X  of-ferings, as we will see again with LIWC features in the next subsection. 6.2 LIWC features We explore 3 LIWC categories (from left to right in subfigures of Figure 2): sentiment ( neg emo and pos emo ), spatial detail ( space ), and first-person singular pronouns ( first-person ).
 Space: Note that spatial details are more spe-cific in the Hotel and Restaurant domains, which is reflected in the high positive value of  X  domain-specific details can be predictive of decep-tive text. Similarly predictive LIWC features are home for the Hotel domain, ingest for the Restau-rant domain, and health and body for the Doctor domain.

In Figure 2(i)(j)(k)(l), we can easily see that both actual customers and domain experts encode more spatial details in their reviews (positive value of  X  ), which is in agreement with our expectation. This further demonstrates that a lack of spatial de-tails would not be a general cue for deception. Moreover, it appears that general domain expertise does not compensate for the lack of prior experi-ence when writing deceptive negative reviews for competitors X  hotels, as demonstrated by the lack of spatial details in the negative-sentiment reviews by employees shown in Figure 2(k).
 Sentiment: According to our findings, the pres-ence of sentiment is a general cue to deceptive opinion spam, as observed when comparing Fig-ure 2(b) to Figure 2(c) and (d). Participants, both Employees and Turkers, tend to exaggerate senti-ment, and include more sentiment-related vocabu-laries in their lies. In other words, positive decep-tive reviews were generally more positive and neg-ative deceptive reviews were more negative in sen-timent when compared with the truthful reviews generated by actual customers. A similar pattern can also be observed when comparing Figure 2(i) to Figure 2(j). First-Person Singular Pronouns: The litera-ture also associates deception with decreased us-age of first-person singular pronouns, an effect at-tributed to psychological distancing, whereby de-ceivers talk less about themselves due either to a lack of personal experience, or to detach them-selves from the lie (Newman et al., 2003; Zhou et al., 2004; Buller et al., 1996; Knapp and Co-maden, 1979). However, according to our find-ings, we find the opposite to hold. Increased first person singular is an apparent indicator of decep-tion, when comparing Figure 2(b) to 2(c) and 2(e). We suspect that this relates to an effect observed in previous studies of deception, where liars inad-vertently undermine their lies by overemphasizing aspects of their deception that they believe reflect credibility (Bond and DePaulo, 2006; DePaulo et al., 2003). One interpretation for this phenomenon would be that deceivers try to overemphasize their physical presence because they believe that this in-creases their credibility. In this work, we have developed a multi-domain large-scale dataset containing gold-standard de-ceptive opinion spam. It includes reviews of Ho-tels, Restaurants and Doctors, generated through crowdsourcing and domain experts. We study this data using SAGE, which enables us to make ob-servations about the respects in which truthful and deceptive text differs. Our model includes sev-eral domain-independent features that shed light on these differences, which further allows us to formulate some general rules for recognizing de-ceptive opinion spam.

We also acknowledge several important caveats to this work. By soliciting fake reviews from par-ticipants, including crowd workers and domain experts, we have found that is possible to de-tect fake reviews with above-chance accuracy, and have used our models to explore several psycho-logical theories of deception. However, it is still very difficult to estimate the practical impact of such methods, as it is very challenging to obtain gold-standard data in the real world. Moreover, by soliciting deceptive opinion spam in an arti-ficial environment, we are endorsing the decep-tion, which may influence the cues that we ob-serve (Feeley and others, 1998; Frank and Ekman, 1997; Newman et al., 2003; Ott, 2013). Finally, it may be possible to train people to tell more con-vincing lies. Many of the characteristics regard-ing fake review generation might be overcome by well-trained fake review writers, which would re-sults in opinion spam that is harder for detect. Fu-ture work may wish to consider some of these ad-ditional challenges. We thank Wenjie Li and Xun Wang for useful dis-cussions and suggestions. This work was sup-ported in part by National Science Foundation Grant BCS-0904822, a DARPA Deft grant, as well as a gift from Google. We also thank the ACL re-viewers for their helpful comments and advice.
