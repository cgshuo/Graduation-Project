 Craig Macdonald  X  Iadh Ounis Abstract In an expert search task, the users X  need is to identify people who have relevant expertise to a topic of interest. An expert search system predicts and ranks the expertise of a set of candidate persons with respect to the users X  query. In this paper, we propose a novel approach for predicting and ranking candidate expertise with respect to a query, called the Voting Model for Expert Search. In the Voting Model, we see the problem of ranking experts as a voting problem. We model the voting problem using 12 various voting techniques, which are inspired from the data fusion field. We investigate the effectiveness of the Voting Model and the associated voting techniques across a range of document weighting models, in the context of the TREC 2005 and TREC 2006 Enterprise tracks. The evaluation results show that thevotingparadigmisveryeffective,withoutusinganyqueryorcollection-specificheuristics. Moreover, we show that improving the quality of the underlying document representation can significantly improve the retrieval performance of the voting techniques on an expert search task. In particular, we demonstrate that applying field-based weighting models improves the ranking of candidates. Finally, we demonstrate that the relative performance of the voting techniques for the proposed approach is stable on a given task regardless of the used weighting models, suggesting that some of the proposed voting techniques will always perform better than other voting techniques.
 Keywords Voting  X  Expert finding  X  Expertise modelling  X  Expert search  X  Information retrieval  X  Ranking  X  Data fusion 1 Introduction With the advent of the vast pools of information and documents in large Enterprise orga-nisations, collaborative users regularly have the need to find not only documents, but also people with whom they share common interests, or who have specific knowledge in a required area.

Hertzum and Pejtersen [ 14 ] found that engineers in product-development organisations often intertwine looking for informative documents with looking for informed people. People are a critical source of information because they can explain and provide arguments about why specific decisions were made.

Yimam-Seid and Kobsa [ 45 ] identified five scenarios when people may seek an expert as a source of information to complement other sources: 1. Access to non-documented information  X  X .g., in an organisation where not all relevant 2. Specification need  X  X he user is unable to formulate a plan to solve a problem, and resorts 3. Leveraging on another X  X  expertise (group efficiency)  X  X .g., finding a piece of information 4. Interpretation need  X  X .g., deriving the implications of, or understanding, a piece of 5. Socialisation need  X  X he user may prefer that the human dimension be involved, as op-
An expert search 1 system is an Information Retrieval (IR) system that can aid users with their  X  X xpertise need X  in the above scenarios. In contrast with classical document retrieval wheredocumentsareretrieved,anexpertsearchsystemsupportsusersinidentifyinginformed people:Theuserformulatesaquerytorepresenttheirtopicofinteresttothesystem;thesystem then ranks candidate persons with respect to their predicted expertise about the query, using available documentary evidence.

Expert search systems make use of evidence of expertise to rank candidates. Predomi-nantly, these systems work by using a profile of textual evidence for each candidate. The profiles represent the system X  X  knowledge of the expertise of each candidate, and they are ranked in response to a user query [ 9 , 11 , 20 , 42 ].

There are two requirements for an expert search system: a list of candidate persons that can be retrieved by the system, and some textual evidence of the expertise of each candidate to the candidate persons that can be retrieved by the system. Candidate profiles can be created either explicitly or implicitly: candidates may explicitly update their profile with an abstract or list of their skills and expertise [ 11 ]. However, this process is cumbersome, and may not reflect a rich vocabulary of their expertise, nor the changing interests of people. Alternatively, the expert search system can implicitly and automatically generate each profile from a corpus of documents. There are several strategies for associating documents to candidates, to generate a profile of their expertise:  X  Documents containing the candidate X  X  name: exact or partial match [ 9 ].  X  Emails sent or received by the candidate [ 3 , 6 , 10 ].  X  The candidate X  X  homepage on the Internet or intranet and their Curriculum Vitae [ 26 ].  X  Documents written by the candidate [ 26 ].  X  Team, group or department-level evidence [ 27 ].  X  Web pages visited by the candidate [ 44 ].

In the Voting Model, each candidate X  X  profile is modelled as a set of documents, which is built automatically from a corpus of documents (described in Sect. 4 ). Then we consider a ranking of documents with respect to the expert search query. We see each document retrieved as an implicit vote for the candidate whose profile contains that document. Therefore, expert search can be modelled as a voting problem, where the votes from documents to candidates have to be combined. We propose many techniques, called voting techniques, to aggregate the document votes into a ranking of candidates. The voting techniques are inspired by techniques from the data fusion field.

The retrieval performance of an expert search system is an important issue: If an expert search system suggests incorrect experts, then this could lead the user to contacting these people inappropriately. An expert search system should aim to rank candidate experts while maximising the traditional evaluation measures in IR: precision , the accuracy of suggested candidates expertise; and recall , the number of candidates with relevant expertise retrieved. Expert search has been a retrieval task in the Enterprise tracks of the Text REtrieval Conferences (TREC) since 2005 [ 8 ], aiming to evaluate expert search approaches. The TREC forum provides IR researchers with a means to evaluate their retrieval systems on a shared test collection . Generally speaking, a test collection consists of a common corpus of documents, a series of queries (known as topics), and a corresponding set of relevance assessments. For the expert search task, the test collection consisted of a corpus, a list of experts identified in the corpus, and a list of expertise topics with corresponding relevance assessments. The retrieval performances of participating systems are evaluated by using relevance assessments to calculate precision and recall-based measures, such as Precision @ 10 (P@10), and Mean Average Precision (MAP).

To assess the usefulness of the proposed voting techniques from the Voting Model, we use the TREC W3C test collection and the TREC 2005 and 2006 Enterprise track expert search tasks. Note that the voting techniques rely on the weighting model used to generate the underlying ranking of documents. Hence, we experiment with a range of probabilistic weighting models, to determine if the performances of the voting techniques are stable across the range of weighting models experimented with. The obtained results show that the Voting Model for Expert Search is very effective compared with the results of TREC participating systems, while making no use of query or collection-specific heuristics.

The success of the Voting Model depends on the quality of the underlying ranking of documents X  X n particular, how high and frequently documents from relevant candidates are ranked. In order to improve the underlying ranking of documents, we further refine the representation of documents used by the retrieval system, to take the structure of documents into account. In particular, the structure of each document can be represented as separate fields during indexing and retrieval. We use content, title and anchor text of incoming hyperlinks as separate document fields during retrieval. Each document is represented by these fields. We demonstrate that applying a weighting model that uses these fields improves the performance of the voting techniques.

The structure of this paper is as follows: We review related work in Sect. 2 . In Sect. 3 , we describe how expert search can be modelled as a voting problem. To aggregate the votes from the documents to candidates and produce a single ranking of candidates, we propose various voting techniques. We describe our experimental setup in Sect. 4 , and evaluate the proposed voting approach across a selection of document weighting models in Sect. 5 .In Sect. 6 , we use field-based weighting models in an expert search context, and show how this improves the performance of the voting techniques adapted for the approach. In Sect. 7 ,we demonstrate that the performance of the voting techniques is stable across various weighting models and settings. Finally, we provide concluding remarks and suggestions for future work in Sect. 8 . 2 Related work There is some previous work on expert search models where candidate profiles consist of a set of documents. Having defined profiles of expertise for each candidate, an expert search system needs to accurately rank the candidate profiles in response to a user query. Craswell et al. [ 9 ] proposed concatenating the terms of all documents in each profile into  X  X irtual documents X  (i.e., one large virtual document for each candidate, containing all the expertise evidence for that candidate), and ranking these using a traditional IR system. However, this approach lacks granularity, as the contribution of each document in a candidate X  X  profile is not measured individually, making this approach less effective than other approaches described below.

Liu et al. [ 20 ] addressed the expert search problem in the context of a community-based question-answering service. They applied three different language models, and experimented with varying the size of the candidate profiles. They concluded that retrieval performance can be enhanced by including more evidence in the profiles (in this case questions or answers written by the candidate).

Social network analysis also features in some related work to expert search. Graph-based techniques are used to infer connections between candidates, and are particularly useful on corpora of email communications [ 10 , 26 , 42 ]. Two approaches make use of the HITS algorithm [ 17 ] to calculate  X  X epute X  and  X  X esourcefulness X  scores for each candidate [ 6 , 44 ]. McLean et al. [ 27 ] used a graph structure to propagate expertise evidence between members of a project team.

The advent of the expert search task in the recent TREC 2005 and 2006 Enterprise tracks has stimulated research interest in expert search [ 8 , 43 ]. From this, there have been three main approaches for expert search: Balog et al. [ 4 ] proposed the use of language models in expert search based on two formal models. Their first model is based on Craswell et al virtual docu-ment approach described above. The second has similarities to the proposed Voting Model, in that evidence from distinct documents in the candidate profiles are combined. Similarly, Fang and Zhai [ 12 ] applied relevance language models to the expert search task. In contrast, the probabilistic approach proposed by Cao et al. [ 7 ] and the hierarchical language models proposed by Petkova and Croft [ 34 ] do not consider expertise evidence on a document level, but instead work on a more fine-grained approach using windowing. In all these approaches, the relevance computation of documents can only be computed using the language modelling approach.

In this work, we consider a different and novel approach to ranking expertise. In particular, we consider expert search as a voting problem. Using the ranked list of retrieved documents for the expert search query (generated using any IR approach or document search engine), we propose that the ranking of candidates can be modelled as a voting process using the retrieved document ranking and the set of documents in each candidate profile. The problem is how to aggregate the votes for each candidate so as to produce the final ranking of experts. In Sect. 3 , we show how existing data fusion techniques can be appropriately adapted to voting techniques, to combine votes for candidates. Our approach contrasts from the three main related approaches, as we are not restricted to using any one technique for creating the initial underlyingrankingofdocuments:Anystandardretrievaltechnique,suchasprobabilistic[ 39 ], language modelling [ 15 ] of Divergence from Randomness [ 1 ] can be used to generate the document ranking. 3 Expert search as a voting problem Datafusiontechniques X  X lsoknownasmetasearchtechniques X  X reusedtocombineseparate rankings of documents into a single ranking, with the aim of improving over the performance of any constituent ranking. Each time a document is retrieved by a ranking, an implicit vote has been made for that document to be included higher in the combined ranking. Fox and Shaw [ 13 ] defined several data fusion techniques (CombSUM, CombMNZ, etc.), and these have been the object of much research since (for examples, see [ 18 , 29 , 41 ]).
Two main classes of data fusion techniques exist: those that combine rankings using the ranks of the retrieved documents, and those that combine rankings using the scores of the retrieved documents.

As introduced in Sect. 1 , we see expert search as a voting problem: In this work, the profile of each candidate is a set of documents associated to them to represent their expertise. We then consider a ranking of documents by an IR system with respect to the query. Each document retrieved by the IR system that is associated with the profile of a candidate, can be seen as an implicit vote for that candidate to have relevant expertise to the query. The ranking of the candidate profiles can then be determined from the votes. In this work, we introduce twelve voting techniques to aggregate the votes for candidates by the retrieved documents.
Let R ( Q ) be the set of documents retrieved for query Q , and the set of documents belon-ging to the profile of candidate C be denoted pr o f ile ( C ) . In expert search, we need to find a ranking of candidates, given R ( Q ) . Consider the simple example in Fig. 1 . The ranking of D d in that order. Using the candidate profiles, candidate C 1 has then accumulated 2 votes, C 2 2votes, C 3 3 votes and C 4 no votes. Hence, if all votes are counted as equal, and each document in a candidate X  X  profile is equally weighted, a possible ranking of candidates to this query could be C 3 &gt; rank C 1 &gt; rank C 2 . By using appropriate vote aggregation techniques (voting techniques), we can have different rankings of candidates. In the remainder of this paper, we introduce twelve voting techniques which are inspired by the data fusion field, and evaluate them to establish how well they model the proposed voting paradigm for expert search.

We determine the score of the candidate with respect to the query, denoted score _ cand (
C , Q ) , as the aggregation of votes of all documents d that are retrieved, but which also belong to the profile of the candidate (i.e., d  X  R ( Q )  X  pr o f ile ( C ) ). We have two central intuitions about expert search: firstly, a candidate that has written prolifically about a topic of interest (ie has many on-topic documents in their profile) is likely to have relevant expertise; and secondly, the more about a topic the documents in their profile are, the stronger is the likelihood of relevant expertise. We consider three forms of evidence when aggregating the votes to each candidate, based on these intuitions: A The number of retrieved documents voting for each candidate B The scores of the retrieved documents voting for each candidate C The ranks of the retrieved documents voting for each candidate We examine and evaluate 12 voting techniques based on known data fusion techniques, which aggregate the votes from the single ranking of documents into a single ranking of candidates, using appropriate forms of evidence.

It is of note that the proposed voting techniques are not straightforward uses of existing data fusion techniques, because in the normal application of data fusion techniques, several rankings of documents are combined into a single ranking of documents. In contrast, our novel approach aggregates votes from a single ranking of documents into a single ranking of candidates, using the document-to-candidate associations of the candidate profiles. 3.1 Voting techniques We now show how some established data fusion techniques can be adapted for expert search. Firstly, we examine the Reciprocal Rank (RR) data fusion technique [ 47 ] for expert search. In this data fusion technique, the rank of a document in the combined ranking is determined by the sum of the reciprocal rank received by the document in each of the individual rankings. Adapting the Reciprocal Rank technique to our approach, we define the score of a candidate X  X  expertise as where rank ( d , Q ) is the rank of document d in the document ranking R ( Q ) . Intuitively, from an expert search perspective, RR will highly rank candidates that have many of their profile documents rank near the top of the document ranking R ( Q ) . RR uses forms of evidence A &amp; C, but with more focus on C.

In CombSUM [ 13 ] X  X  score aggregation technique X  X he score of a document is the sum of the normalised scores received by the document in each individual ranking. CombSUM can also be used in expert search. In this case, the score of a candidate X  X  expertise is where score ( d , Q ) is the score of the document d in the document ranking R ( Q ) ,asdefined by a suitable document weighting model. Intuitively, a candidate X  X  expertise with respect to a query is scored as the sum of the relevance score of all the documents in R ( Q ) that are voting for that candidate. CombSUM uses forms of evidence A &amp; B. Similarly, CombMNZ [ 13 ] can be adapted for expert search: score _ cand CombMN Z ( C , Q ) = R ( Q )  X  pr o f ile ( C )  X  where R ( Q )  X  pr o f ile ( C ) is the number of documents from the profile of candidate C that are in the ranking R ( Q ) . This has a similar intuition to CombSUM (Eq. ( 2 )), except that candidate with a larger number of votes are scored higher. Again, CombMNZ uses forms of evidence A &amp; B, but places more emphasis on A than CombSUM.

Normally, in the CombSUM and CombMNZ data fusion techniques, it is necessary to normalise the scores of documents across all input rankings [ 29 ]. However, in Eqs. ( 2 )and ( 3 ), no score normalisation is necessary: Indeed, in our case, as stressed above, only one input ranking of documents is involved, and hence the scores are all comparable. Table 1 summarises all the voting techniques that we use and evaluate in this work. In addition to the three techniques described above, we also use: a technique that we call Votes, which simply counts the number of retrieved documents of each candidate profile; a technique inspired by BordaFuse [ 5 ], which works by linearly summing the ranks of the retrieved documents; and several other score aggregation techniques first defined by Fox and Shaw [ 13 ]. The final three voting techniques listed in the table, namely expComb-SUM, expCombANZ and expCombMNZ, are slight variants of CombSUM, CombANZ and CombMNZ, respectively. In these variants, the score of each document is transformed by applying the exponential function ( e score ), as suggested by Ogilvie and Callan [ 31 ]. Applying the exponential function boosts the scores of highly ranked documents, which, in turn, boosts the retrieval scores of candidates associated to highly ranked documents (by emphasising the most highly scored documents in evidence form B).

Other data fusion techniques could also have been considered in this work for adaptation to the Voting Model, including one based on Condorcet voting-theory [ 30 ], a technique that models score distributions [ 25 ], and a logical regression model [ 40 ]. However, in this work, due to the more complex nature of these techniques and due to the fact that their adaption to the Voting Model would be non-trivial, we focus the evaluation of our proposed voting approach on the techniques in Table 1 .

It is of note that the CombSUM voting technique is very similar to the 2nd formal model proposed by Balog et al. [ 4 ]. In particular, the Voting Model can emulate Model 2 from [ 4 ]if Hiemstra X  X  language modelling was used to generate the initial ranking R ( Q ) . 2 However, the Voting Model is more general in two senses: the document ranking R ( Q ) can be generated using any approach including but not restricted to language modelling; and secondly there are more diverse methods of aggregating the votes than just the simple sum of scores. Indeed, in the following experiments, we will show that the voting techniques from the Voting Model can be applied using a selection of weighting models, and that various voting techniques can be successfully applied to the expert search task. 4 Experimental setting In the following, we aim to demonstrate that voting is an effective approach for expert search and that the voting techniques applied are suitable to implement the proposed approach. As discussed above, in the Voting Model, any technique can be used to generate the underlying document ranking R ( Q ) . In our experiments, we hypothesise that the choice of an effective weightingmodelwillhavelittleeffectontherelativerankingofvotingtechniques.Inthiscase, while the weighting model can have an effect on the magnitude of the retrieval performance, the main parameter in the Voting Model is the choice of voting technique, and how well it covers the three forms of evidence described in Sect. 3 . Hence we experiment using three different statistical document weighting models to generate R ( Q ) .

To evaluate our approach, we use the Expert Search tasks of the TREC 2005 and TREC 2006 Enterprise tracks. The TREC Enterprise test collection consists of 331,037 documents collected from the World Wide Web Consortium (W3C) website in 2005 [ 8 ]. For research purposes, the W3C is a useful example of an Enterprise organisation, as it operates almost entirely over the Internet. Moreover, its documents are freely available online. This allows research on an Enterprise-level corpus, without the intellectual property issues normally associated with obtaining such a corpus. The corpus is also wide-ranging, containing the main W3C Web presence (www), personal homepages (people), official standards and re-commendation documents, email discussion list archives (lists), a wiki (esw), and a source code repository (dev).

The W3C test collection includes a list of 1,092 candidate experts. We assess the retrieval accuracy of our expert search approach using the 50 topics of the TREC 2005 Expert Search task (EX1-EX50), and the 49 topics of the TREC 2006 task (EX52-EX104). The retrieval performance is evaluated using Mean Average Precision (MAP) X  X o assess the overall qua-lity of the ranking X  X nd Precision @ 10 (P@10), to assess the accuracy of the top-ranked candidates retrieved by the system.

As the expert search task in TREC 2005 was a pilot task, the candidate rankings for each query submitted by participating groups were not assessed as such, but instead were evalua-ted on their accuracy to a ground truth -in this case the W3C working group memberships. For TREC 2006, the submitted rankings from participating groups were evaluated by manual relevance assessing of each candidate for each query. For this to be achievable, systems submitted supporting documents for each candidate, which assessors used to judge each candidate (For example, systems could provide top-scoring documents from each suggested candidate X  X  profile to justify that candidate X  X  relevance). Therefore, because the relevance as-sessments for the TREC 2006 task were more  X  X omplete X , the retrieval accuracies achievable by expert search systems are far higher than for the TREC 2005 pilot task.

To generate the profile for each candidate, we use the Unix grep command to identify documents from the collection which contain an exact match of the candidate X  X  full name. Note that this article improves on the CIKM version of this work by applying a candidate profile set that performs robustly on both TREC tasks [ 19 ]. This improved candidate profile set contains on average more than 12 times as many documents per candidate than that applied in the CIKM version. The set of documents identified for each candidate C form their pr o f ile ( C ) .Table 2 shows a break down of the TREC W3C collection by subsection of the collection, showing the number of documents in each subsection of the collection, and how well the subsection is represented in the candidate profiles. Overall 41% of the documents in the collection are included in one or more candidate profiles. Moreover, it appears that most candidates are only represented in the corpus by the emails they have sent, because the lists subsection forms the largest part of the expertise evidence in the profiles.
In this work, we index the W3C collection and carry out all experiments using the Terrier platform [ 32 , 33 ]. During indexing, each document is represented by its textual content and the anchor text of its incoming hyperlinks. Stopwords are removed, and as we would like to favour high precision, we use a weak stemming algorithm, which only applies the first two steps of Porter X  X  stemming algorithm.

We test the proposed Voting Model using the twelve voting techniques listed in Table 1 with three statistically different document weighting models to generate the underlying ran-king R ( Q ) . The first of these weighting models is the well-established probabilistic Okapi BM25 [ 39 ], where the relevance score of a document d for a query Q is given by where qt f is the frequency of the query term t in the query; k 1 and k 3 are parameters, for which the default setting is k 1 = 1 . 2and k 3 = 1000 [ 38 ]; w ( 1 ) is the id f factor, which is given by N is the number of documents in the whole collection. N t is the document frequency of term t . The normalised term frequency tfn is given by where tf is the term frequency of the term t in document d . b is the term frequency norma-lisation hyper-parameter, for which the default setting is b = 0 . 75 [ 38 ]. l is the document length in tokens and a v g _ l is the average document length in the collection.
The remaining two weighting models tested are from the Divergence from Randomness (DFR) framework [ 1 ]. The first of these, PL2, is robust and performs particularly well for tasks requiring high early-precision [ 35 ]. For the PL2 model, the relevance score of a document d for a query Q is given by score ( d , Q ) = where  X  is the mean and variance of a Poisson distribution. It is given by  X  = F / N . F is the frequency of the query term in the collection and N is the number of documents in the whole collection. The query term weight qt w is given by qt f / qt f max . qt f is the query term frequency. qt f max is the maximum query term frequency among the query terms.
 The normalised term frequency tfn is given by the so-called Normalisation 2 from the DFR framework: where tf is the term frequency of the term t in document d and l is the length of the document. a v g _ l is the average document length in the whole collection. c is the hyper-parameter that controls the normalisation applied to the term frequency with respect to the document length. The default value is c = 1 . 0[ 1 ].

The DLH13 document weighting model is a generalisation of the parameter-free hyper-geometric DFR model in a binomial case [ 2 , 22 ]. The hypergeometric model assumes that the document is a sample, and the population is from the collection. For the DLH13 document weighting model, the relevance score of a document d for a query Q is given by: score ( d , Q ) =
Note that the DLH13 weighting model has no term frequency normalisation component, as this is assumed to be inherent to the model. Hence, DLH13 has no term frequency normali-sation hyper-parameters that require tuning. Indeed, all variables are automatically computed from the collection and query statistics.

The BM25 and PL2 document weighting models include hyper-parameters, which can be tuned using relevance assessments to improve retrieval performance. In our experiments, we assess the performance of the voting techniques, both using the default parameter settings for each weighting model, and when, for each voting technique, the parameters of the weighting model have been empirically set to maximise MAP on the TREC 2005 task. This allows assessment of the maximum potential of the proposed approach for the TREC 2005 task, and how well the approach performs on the TREC 2006 task given realistic training data. Note again that the DLH13 model has no parameters that need to be tuned, and therefore is deployed directly in the expert search task. 5 Experimental results Table 3 shows the retrieval performance of the proposed voting approach, using the twelve voting techniques, across three weighting models, namely BM25, PL2, and DLH13 for both the TREC 2005 and TREC 2006 expert search tasks. In these experiments, the default setting is used for BM25 and PL2 (see Sect. 4 ). Table 4 shows the retrieval performance on both the TREC 2005 and TREC 2006 when the term frequency hyper-parameters of the weighting models are empirically set for TREC 2005. This enables us to determine the maximum potential of each technique and weighting model on the TREC 2005 task. Moreover, it allows to determine the extent that the setting for TREC 2006 can be determine using the TREC 2005 task.

Firstly, it is noticeable that the retrieval performance of the voting techniques differs between the TREC 2005 and TREC 2006 tasks. This is expected, as the relevance assessments for the TREC 2006 task are more complete, and the easier nature of this task is reflected by the increased median MAP achieved by the participating groups for TREC 2006 (TREC 2005 median MAP 0.1402; TREC 2006 median MAP 0.3412). Tables 3 and 4 also present the statistical significance of results, when compared to the median run of all participants of each TREC year, using the Wilcoxon Matched-Pairs Signed-Rank test. 3
In their default settings, the relative retrieval performance of the voting techniques is overall consistent across the three weighting models and two tasks. Moreover, as expected, retrieval accuracy is generally enhanced in Table 4 over Table 3 . In particular for TREC 2005, all voting techniques benefit when the weighting model is tuned. However, this tuning does not always results in an increase in performance for the TREC 2006 task. In terms of weighting models, BM25 outperforms PL2 and DLH13 on the TREC 2005 task, while DLH13 and BM25 are roughly comparable for TREC 2006, even when BM25 has been tuned as per Sect. 4 .

Most of the voting techniques lead to a clear increase in performance over the median runs. In particular, applying either CombSUM, CombMAX, CombMNZ or the exponential variants always results in a statistically significant increase in MAP from the baseline (except CombMNZ using DLH13 for TREC 2005). However, by examining Tables 3 and 4 in detail, we can make a number of observations concerning the voting techniques. Firstly, the Votes technique, which simply counts the number of document votes for each candidate, shows good performance. The rank-based techniques, RR and BordaFuse, both perform well across the three weighting models. Note the good performance of RR on P@10 for all weighting models and tasks. RR highly scores candidate profiles that have documents occurring at the very top of the ranking, suggesting that the highly ranked documents contribute more to the expertise of a candidate, and should be considered as stronger votes (evidence form C). In contrast, the BordaFuse technique assigns linearly scaled votes across the document ranking to candidates, without emphasising the strength of votes by top ranked documents, which, as expected slightly hinders the retrieval performance compared to the high-rank focused RR.

On the other hand, the score-based voting techniques have varying effectiveness, de-pending on the exact combination of evidence applied. CombMAX works extremely well for TREC 2005, but is not as effective as other voting techniques on the TREC 2006 task. CombMAX ranks candidates by their most highest ranked profile documents, without taking into account the number of votes for a candidate profile. Its relatively strong performance demonstrates that the most highly ranked document for each candidate is a good indicator of its expertise, without taking into account any additional votes from R ( Q ) .

The reasonably good effectiveness of CombSUM and CombMNZ mirrors previous studies of their use in classical data fusion [ 28 , 29 ]. In particular, for expert search, both take into account the strength of the document votes, i.e., the magnitude of the score for each retrieved document of the candidate X  X  profile. Moreover, CombMNZ adds a second component, the number of votes for each candidate, explaining its slight overall performance edge over CombSUM.
 The high performance on both tasks of the exponential variants of CombSUM and Comb-MNZ, expCombSUM and expCombMNZ, is expected, and can be explained in that the exponential function increases the scores of the highly-scored documents more than the low-scored documents, increasing the strength of their votes. Hence a candidate with many weak votes will be lower ranked, while a candidate with fewer stronger votes will be higher ranked. In terms of MAP, expCombSUM and expCombMNZ outperform all other techniques across all weighting models for TREC 2006, and are only beaten by CombMAX for BM25 and DLH13 on the TREC 2005 task.

The CombANZ, CombMIN, and expCombANZ techniques do not perform well on either task, because they focus too much on the low scoring documents of each profile, which, intuitively, are not good indicators of expertise. Interestingly, taking the median of the sco-red documents in a profile (CombMED) outperforms taking the average (CombANZ). This finding is inconsistent with previous experiments using these techniques for classical data fusion [ 13 ]. A possible interpretation is that the denominator component of CombANZ im-pairs the evidence in the distribution of the candidate scores.

From the results, we can surmise that good indicators of expertise of a candidate seem to be the number of documents in the candidate X  X  profile retrieved for a query (number of votes, evidence form A), and the relative magnitude of the retrieval scores in the candidate X  X  profile (strong votes, evidence form B). The techniques Votes and CombMAX exemplify each of these indicators respectively. Moreover, the robustly performing CombMNZ and expCombMNZ techniques combines both these indicators. The rank-based voting technique RR also combines votes with a focus on highly-scored documents (in this case manifested by high-ranks, evidence form C).

Overall, we have shown that the proposed Voting Model, using voting techniques inspi-red by data fusion techniques, can be effectively applied to expert search. Indeed, the best performing runs in Table 3 would rank as high as the top three participants in TREC 2005 Enterprise track runs, and as high as the top two participants in the TREC 2006 Enterprise track runs, without using any collection-specific heuristics, nor any parameter tuning. The training for Table 4 , as expected, enhances the retrieval performance for the TREC 2005 task, however it does not enhance the performance of all techniques on the TREC 2006 task. As discussed in Sect. 4 , this is probably due to the fact that the TREC 2005 task (a pilot study) was evaluated in a different manner to the TREC 2006 task, and hence is not a representative training set for TREC 2006.
 The proposed voting techniques are low-cost, and are easy to deploy in an operational Enterprise setting. The proposed voting techniques perform robustly using a selection of statistically different document weighting models to generate R ( Q ) . Moreover, the results of the experiments show that the voting techniques are overall consistent across the different weighting models, showing that the main parameter of the Voting Model is not the weighting model used to generate R ( Q ) , but the voting technique applied.

In the next section, we will show that we can significantly improve on the performance of the proposed approach by improving the quality of the underlying document ranking. 6 Use of document structure The quality of the underlying document ranking returned by the IR system in response to the expert search query is important to the success of the proposed voting approach. If the quality of the document ranking is improved, then we naturally hypothesise that the ranking of candidates will also likely improve.

Experiments in the Web and Enterprise tracks have shown that when the structure of documents (fields) is taken into account by a retrieval system, then the retrieval performance can be improved [ 21 , 46 ]. For example, a Web document can be represented by three fields: the body, the title, and the anchor text of its incoming hyperlinks. Robertson et al. [ 37 ]and Plachouras and Ounis [ 36 ] then showed improved retrieval effectiveness in Web tasks when the contribution of each field to the document ranking was controlled by the use of weights. We hypothesise that the retrieval performance of our proposed voting approach for expert search could be improved if the quality of the underlying document ranking is increased. In this section, we use field-based document weighting models that account for the document structure to improve the quality of document ranking, and assess the effect on the proposed voting approach for expert search.

In the remainder of this section, we use variations of the BM25 and PL2 models that take fields into account. Next, we apply these fields-based document weighting models in our voting approach for expert search. 6.1 Field-based document weighting models The BM25 weighting model can be extended into a field-based document weighting model, BM25F [ 46 ], by replacing Eq. ( 6 ) with where tf f is the term frequency of term t in field f of document d , l f is the length of field f in d ,and a v g _ l f is the average length of documents in f . The normalisation applied to terms from field f can be controlled by the field hyper-parameter, b f , while the contribution of the field is controlled by the weight w f .
 Similarly,wecanextendthePL2documentweightingmodeltohandlefields.Theso-called Normalisation 2 (Eq. ( 8 )) is replaced with Normalisation 2F [ 21 ], so that the normalised term frequency tfn corresponds to the weighted sum of the normalised term frequencies tf f for each used field f : where c f is a hyper-parameter for each field controlling the term frequency normalisation, and the contribution of the field is controlled by the weight w f . Having defined Normalisation 2F, the PL2 model (Eq. ( 7 )) can be extended to PL2F by using Normalisation 2F.
In the following experiments, we index the body, anchor text and titles of documents as separate fields using Terrier. Table 5 shows the breakdown of the statistics of each field on the W3C collection. As in Sect. 4 , we remove stopwords and apply the first two steps of Porter X  X  stemming algorithm. We again train the parameter settings using the 50 expert search task topics from TREC 2005 Enterprise track, and report the obtained retrieval effectiveness on the TREC 2005 and TREC 2006 Expert Search tasks. Similar to Sect. 5 , this allows us to assess the maximum contributions a field-based model can have using the TREC 2005 task, and how well it can work in a realistic train/test setting using the TREC 2006 task.
We follow [ 46 ] to optimise the involved hyper-parameter values and the field weights as follows. Firstly, for each voting technique, the hyper-parameter for each field ( c f or b f ) is tuned using a simulated annealing. During this, the w f of that field is set to 1, and the weights of the other fields are set to 0. Once good hyper-parameter values have been found, a three-dimensional simulated annealing is used to find the optimal w f values. Contrary to Zaragoza et al. [ 46 ], who assumed that the body field should have a weight of 1, we do not assume any constraints on the weights of any fields. Moreover, as the training is done for each voting technique individually, there is a total of 144 parameters. Hence, for reasons of brevity, we have chosen not to provide the settings obtained. 6.2 Experiments and results Table 6 shows the retrieval performance of the proposed voting approach using the twelve voting techniques, and the BM25F and PL2F field-based weighting models. Retrieval per-formance is shown on both the TREC 2005 and TREC 2006 Expert Search tasks. From the obtained results, we can see that the use of fields has led to an overall improvement in effectiveness compared to Table 4 , for both MAP and P@10. In particular, for a large number of cases on the TREC 2005 task, there is a statistically significant improvement over the corresponding entry in Table 4 . For example, expCombMNZ shows a MAP of 0.2740 for BM25F, compared to only 0.2078 for BM25 X  X  statistically significant improvement of 31%. The relative performance of the voting techniques remains mostly consistent with the previous experiments.

By introducing fields into the underlying document ranking technique, we are able to rank highly more documents that are good indicators of expertise for the query. In general, this increased quality of the document ranking leads to an increased performance by the proposed voting approach on all voting techniques. In particular, performance is increased for all voting techniques on the TREC 2005 task. This infers that when a document ranking can be appropriately fine-tuned to rank higher more documents associated with relevant candidates, then the voting techniques can take this into account, enhancing retrieval effectiveness.
For the TREC 2006 task, as noted in Sect. 5 and expected, training using the TREC 2005 topics did not provide the same increases in retrieval performance. While some of the mediocre or poor techniques can be improved (eg CombANZ or CombMED), it appears that training using the TREC 2005 data does not give the correct evidence about the relative importance of the underlying document structure for the high performing voting techniques. This is perhaps due to the different schemes for relevance assessing between the two tasks, as discussed in Sect. 4 . Among the well-performing techniques on the TREC 2006 task, RR shows the most improvement for BM25F (+3% MAP and +6% P@10), while expCombMNZ shows the same margin of improvements for PL2F. This underlines the stability of these techniques, which always perform extremely well across all weighting models and tasks, because they use appropriate mixes of the forms of evidence defined in Sect. 3 . The obtained results are in the top two most effective techniques reported for both the TREC 2005 and TREC 2006 Expert Search tasks [ 8 , 43 ], while not using any collection-dependent means, such as focusing on the pages containing many of the answers. This is very encouraging, as our approach could be extended to include other factors, such as document andcandidatepriors,degreeofassociationbetweendocumentsandprofiles,andtheproximity of query terms to candidate names etc. Moreover, it can been shown that query expansion can be applied, in either a document-centric or a candidate-centric manner, to improve the quality of R ( Q ) and hence improve the accuracy of generated candidate ranking [ 23 ]. Our voting approach is general, and can easily be applied in an Enterprise setting independent of the collection and its structure. 7 Stability of voting techniques As mentioned in the hypothesis in Sect. 4 , our voting approach relies on the voting techniques to provide a suitable aggregation of the votes by documents for candidates. In this section, we test the robustness and stability of the voting techniques used in the proposed approach, across the seven weighting schemes and settings applied in Tables 3 , 4 and 6 .UsingMAPas the evaluation measure, for each weighting model, we order the adapted voting techniques from the best to the worst performing. For example, from the TREC 2005 task in Table 6 , the ordering for BM25F has CombMAX as the best and expCombANZ as the worst voting technique.

Figures 2 and 3 show the retrieval performance in terms of MAP of each voting technique across all weighting schemes and settings, for the TREC 2005 and TREC 2006 tasks, respec-tively. From Fig. 2 , we can see that for the TREC 2005 task, there are three distinct groups of fusion techniques X  X he best (MAP  X  0.20); the middle range (0.17 X 0.20 MAP), and the rest (MAP 0.16 and below). For the easier TREC 2006 task (Fig. 3 ), we see that more of the techniques give roughly comparable performance (MAP  X  0.50). Moreover, from Fig. 2 , we can observe that the lines joining the performance of each fusion technique on the diffe-rent weighting models are mostly parallel. This suggests that the relative performance of the voting techniques is stable on this task, since their ordering remains unchanged regardless of the used weighting model. In Fig. 3 , more techniques have comparable performance, and there are more swaps between their relative performances. However, across the two figures, it is apparent that the choice of weighting model is not as important as the choice of voting technique.
To check that the voting techniques are indeed stable across different document weighting models, we can use a statistical concordance measure. Kendall X  X  [ 16 ] W of concordance measures the concordance of n items over a set of m rankings. W is in the range W  X  [0,1], where W = 1 are identical rankings, and W = 0 are completely disagreeing rankings. We use Kendall X  X  W to measure the concordance of the seven weighting models and the twelve voting techniques for each TREC task. Moreover, using Table 6 in [ 16 ], we can calculate the significance of such concordance. For Fig. 2 (TREC 2005), the concordance is W = 0 . 82, while for Fig. 3 (TREC 2006), the concordance is W = 0 . 36 (both values are significant at p  X  0 . 01).

Hence for the TREC 2005 task, we can see that there is a statistically significant concor-dance between the rankings of the voting techniques, showing that the relative performance of the various techniques are indeed very stable, regardless of the weighting model used. For the TREC 2006 task, the concordance is weaker (but still significant), due to the similarity of many of the techniques in the MAP  X  0.50 range, and hence the increased number of swaps compared to the TREC 2005 rankings. We can conclude that although we cannot predict the absolute performance of each voting technique on an arbitrary weighting model, we can conclude that some techniques are always more likely to perform better than others. These are the ones which model the important sources of expertise evidence, A, B &amp; C, introduced in Sect. 3 . 8 Conclusions and future work In this paper, we proposed that expert search can be seen as a voting problem, where documents from an initial ranking R ( Q ) vote for the candidates with relevant expertise. We call this the Voting Model for Expert Search, and proposed twelve voting techniques for our proposed approach, inspired by the data fusion field. Three statistically different docu-ment weighting models were tested, to assess the effectiveness and stability of the data fusion techniques in our approach. The evaluation was conducted in the context of the expert search tasks of the TREC 2005 and TREC 2006 Enterprise tracks.

The results in Sect. 5 show that our proposed approach is effective when using appropriate adapted voting techniques that use the forms of evidence (A, B or C) introduced in Sect. 3 , namely the number of votes, the scores of associated documents, and the ranks of associated documents, respectively. While the techniques have varying degrees of performance, some of them consistently outperform others, regardless of the applied document weighting model. The most successful techniques usually integrate the most highly ranked or scored documents of the profile (forms of evidence B or C X  X trong votes), and the number of retrieved docu-ments from the profile (evidence form A X  X umber of votes). Our experimental results also suggest that the quality of the underlying ranking of documents is important in enhancing the retrieval performance of the expert search system. Indeed, we showed that a recent Web IR technique X  X .e., the use of fields to represent the document structure X  X ery often leads to marked performance improvements, which are sometimes significant (see Sect. 6 ).
We also demonstrate that the relative performance of the voting techniques is stable across the various weighting models and settings applied. Indeed, when the voting techniques are compared across various weighting models, the concordance of their relative performance rankings shows that some of the data fusion techniques are always more likely to outperform others.

The approach proposed in this paper is general in the sense that it is not dependent on heuristics from the used Enterprise collection, and can be easily operationally deployed with little computational overhead. Moreover, we have successfully deployed an expert search system based on these techniques [ 22 ].

Comparing the Voting Model with other existing techniques, we note that in certain cir-cumstances, the Voting Model is similar to the language modelling formal model of Balog et al. [ 4 ]. In particular, the Voting Model can emulate the language modelling approach Model 2 of Balog et al. if CombSUM is used to combine the scores of documents ranked by the language models of Hiemstra [ 15 ]. In contrast, the voting approach is more general, as any technique can be used to rank the documents in R ( Q )  X  X t is not restricted to language modelling approach. Moreover, as we have shown, other more effective techniques exist for combining the votes of the documents in R ( Q ) (eg expCombSUM or expCombMNZ).

This work can be naturally extended to integrate prior knowledge. For example, we believe that not all documents are likely to be good indicators of expertise, and furthermore that not all candidates are likely to be experts. Designing and integrating document and candidate priors within our approach could increase the retrieval effectiveness of the expert search system. Moreover, we are keen to evaluate our proposed approach on another expert search test collection. Finally, as we work towards our overall objective of having a better understanding of expert search task, we would like to investigate in more detail how the document ranking R (
Q ) and the final ranking of candidates correlate, and how the ranking R ( Q ) can be directly evaluated so that we have a better understanding of the connection between the performance of R ( Q ) and the effectiveness of the expert search system.
 References Authors Biography
