 The emphasis in database privacy should fall on a balance between confidentiality, integrity and availability of personal data, rather than on confidentiality alone. This balance should not necessarily be a trade-off, but should take into account the sensitive nature of the data being stored and attempt to increase all three dimensions to the highest level possible.
 To achieve such a balance, technological means should be developed.
 The paper illustrates some of the inherent problems in data-base privacy that should be addressed by technical solutions. It next demonstrates that the notion of privacy is complex; this complexity is likely to impede development of technical solutions.
 Finally, the paper finally uses the notion of informed consent to illustrate how the privacy problem can be viewed from multiple angles to flesh out the underlying problems that may be addressed by technical solutions.
 Security Database privacy, personal data, confidentiality, integrity, availability, dataveillance For many people, when writing about privacy and comput-ing, the temptation exists to focus only on the negative. It is simple to list many examples of cases where the use of computers were instrumental in some breach of an individ-ual X  X  privacy. From such examples and the sheer size of the global database collecting information about each of us, it is often concluded that the age of an Orwellian Big Brother has now arrived and personal privacy has forever been lost. On the other side of the coin are the proponents of tech-nology who enthuse about new technology, citing the many obvious advantages it has. However, in many of these cases they are either oblivious of the debate about privacy or see those who are concerned about privacy as uninformed scare-mongers.
 As is often the case when looking at such problems, the truth lies somewhere between the extremes. While it is not possi-ble to pinpoint the appropriate spot between the extremes (since this will be influenced by time, location, culture and other factors) debate is required to establish some range of acceptable use of private information.
 This is a well-known phenomenon in security: security is widely regarded as a balance between confidentiality, in-tegrity and availability. Without the need for availability, the confidentiality problem is trivially solved by  X  X nplug-ging X  the database. In the privacy sphere, the availability dimension has mostly been ignored when technical solutions to the privacy problem has been sought. In this mode of thought, anonymity presents itself as the natural solution: if private data cannot be collected, it cannot be misused. While this is clearly an important aspect of privacy, it is (also clearly) not applicable to a major class of privacy prob-lems: For many (most?) transactions that have an effect in real life, details of the transaction and transacting parties need to be recorded  X  hence availability of personal infor-mation becomes an issue. And, once personal information has been recorded, the confidentiality problem changes from one of ensuring that private information is not disclosed for recording to one that ensures that private information is not improperly disclosed from where it has been recorded in a database.
 The challenge of database privacy is therefore to enable the storage of personal information in databases in a manner that balances society X  X  needs with those of the individual, with particular emphasis on the vulnerability of the individ-ual.
 Here  X  X ociety X  X  needs X  should be understood in an inclusive manner. It includes society X  X  need for accountability (as ex-emplified by unique number or licence plates on motor cars), the need for information for civil interaction (as exemplified by  X  X haring X  one X  X  credit card number with a merchant from whom one buys goods) and even the needs of the individual (as exemplified by the individual who wants to be unique rather than blend in with the crowd).
 This paper argues that the concept of privacy is often more complex than realised. The next section argues that privacy mechanisms should extend security because the problems have fundamental differences. Section 3 mentions a couple of historical milestones to show that database privacy has received attention in the past  X  before communications pri-vacy started to become the major focus in the area. I next use a personal example to highlight a number of relevant SIGKDD Explorations. Volume 4, Issue 2 -page 20 issues  X  in particular the need for availability. Section 5 shows that the concept of privacy is problematic, while sec-tion 6 attempts to cast some light on how the underlying concepts of privacy can be identified. Section 7 concludes the paper by summarising the major challenges to be ad-dressed in the field. This paper deals with database privacy. Database privacy concerns the protection of information about individuals that is stored in a database. Database privacy can (and should) use solutions developed for database security. Both database security and database privacy are based on a bal-ance of confidentiality, integrity and availability. However, database privacy differs inherently from database security in some key respects. To mention just three of these differ-ences, consider the questions 1. What is (was) the intended use of stored data? 2. Who carries the risk if data is disclosed to an unau-3. Why does someone need to know a specific piece of The first question is typically irrelevant in the case of data-base security. Information is considered an asset that gives an organisation a competitive advantage over its competi-tors. The fact that data is not only to be used for the intended reason why it was collected, is supported by the emergence of technologies such as data mining and OLAP (on-line analytical processing) . Security is typically con-cerned about keeping sensitive information out of the hands of competitors and and ensuring that data is not modified in some manner that will cause the organisation to lose money or suffer embarrassment. In contrast to this, the intended use of personal information is a cornerstone of privacy. In the ideal case, individuals are informed exactly what the information collected about them will be used for and the information is subsequently used solely for that purpose. The relevance of the second question can be demonstrated with an example: Suppose an organisation accidentally dis-closes some sensitive medical diagnosis about someone in their database. The organisation may be liable and may be forced to pay damages to the individual. In many cases the organisation will be ensured against such mishaps and the only real effect may be some embarrassment to the organi-sation. In contrast, this may have a profound effect on the individual concerned, who may be ostracised from his/her community and for whom reparation in the form of damages or other remedy may be of little value. We have, after all, only one life.
 The third question alludes to the standard need-to-know principle employed in security: tight security mandates that someone should only be given access to information he/she needs to know to do his/her job. The problem appears when an employee may need to access an individual X  X  information. This may happen when the individual phones to discuss an item on his/her account or other record, when the employee happens to process an order, account or other item related to the individual and in many simple scenarios. Making it difficult for the employee to access personal information of, say, customers may hamper efficiency (and even the individ-ual X  X  perception of the organisation because they seem not have the information readily available to provide proper cus-tomer service). The problem with giving people access to all individuals X  information is that it becomes relatively hard to distinguish between work and mere browsing of individual information.
 What these questions have illustrated are the facts that da-tabase privacy needs to consider the purpose(s) for which data was collected, verification of protection mechanisms by those who would stand to lose most if private information is disclosed or modified and protection of each specific indi-vidual X  X  information. Note that these and similar issues are not solved by simplistic solutions, such as banning the col-lection of personal information in databases: not only is the recording of some such information required by the society we live in at the beginning of the twenty-first century, but recording of such information is, in many cases, beneficial to the individual. Both of these aspects will be considered in more detail below. Note that many very useful approaches (such as P3P [21], onion-routing [12], Crowds [22], LPWA [9] and others) exist to prevent recording of information in a database in cases where such recording can be avoided. Preventing recording, however, is not our concern here, but rather dealing with cases where recording in unavoidable (or has already occurred). Database privacy has received significant interest over the past three decades (even though the term database privacy was not used in most cases). It is only with the emergence of the World-Wide Web in the 1990s that preservation of pri-vacy during communications has begun to overshadow pri-vacy protection of stored data. In some cases  X  notably P3P  X  the intention of communications privacy is to only com-municate data with a server that, according to the server X  X  privacy policy will treat the private data in a manner one is comfortable with. In many other cases, the goal of commu-nications privacy is to ensure anonymity and/or ensure that communications cannot be tapped.
 Computer databases have forever changed the landscape of protection of private information. Where previously infor-mation may have been recorded in a manual file system that made it labour-intensive to locate records and costly to store and copy them, data is now stored in a manner that is cheap, easy to make perfect copies and that can be searched quickly using very sophisticated queries.
 This section lists some of the major milestones in database privacy  X  ie in the protection of stored personal data. Much of what has happened in the field rests on legal and societal norms that address misuse of personal information. While such restrictions are important, they are insufficient on their own and need to be augmented by technical controls. Already in 1973 the Code of Fair Information Practices es-tablished some of the fundamental principles of database privacy used to this day, including the requirements that the existence of no database containing personal information should be secret, that a person should be able to determine, correct and/or amend information stored about him/her, and that precautions against misuse of data should be taken by those who work with such data [10, p.7].
 One of the next major milestones was the publication of pri-SIGKDD Explorations. Volume 4, Issue 2 -page 21 vacy principles by the OECD (Organisation for Economic Co-operation and Development) with the intention to  X  X ar-monize national privacy legislation and, while upholding hu-man rights, [to] at the same time prevent interruptions in international flows of data X  [4, p.74].
 The European Data Protection Directive limits the transfer of information across national boundaries to countries that will impose similar restrictions on use of the data as Euro-pean countries do. In some countries, recording of specific data, such as ethnic origin, religious or life convictions and health information is prohibited (unless intended for a few specified cases) [24].
 One of the major threats to consider is the aggregation of personal data from multiple sources. Bennett (as quoted by Whitaker [26, pp.125, 138]) uses the term dataveillance to refer to the extent that  X  X he surveillance practices that the massive collection and storage of vast quantities of personal data have facilitated. X  The potential to invade privacy by combining different data sources is also recognised by the US Computer Matching and Privacy Protection Act of 1988 (5 U.S.C. 552a(o) et seq ) that restricts federal agencies X  ability to match data collected from different sources.
 One of the best-known privacy cases, in fact, involved the collection and planned sale of information about the buy-ing patterns of millions of Americans by Lotus Development Corporation and Equifax [3, p.17][10, p.9][4, p.57]. These plans were cancelled after about 30 000 letters were sent to Lotus and Equifax to protest against the sale of this infor-mation.
 Many of the more recent developments in this area have not concentrated primarily on placing more restrictions on what data should be recorded, how it should (not) be used and when collection should be prohibited. To the contrary, such legislation has often placed a duty on communications and other providers to collect information about subscribers X  use of their systems so that law enforcement agencies can get access to such data when required. While writing this paper, we were planning a trip to the Netherlands. Immediately after receiving the last of our required documentation we duly applied for the required visas, which we were told should take a day. With more than a week remaining, this seemed like ample time. However, the embassy phoned my wife that same afternoon and told her that her application would take several days to several weeks to complete because she is a medical doctor. Apparently, they are afraid that a visiting doctor (with a tourist visa) would work as a doctor while there and take work away from a registered doctor. 1 The fact that my wife would only be in the country for 3.5 days (Saturday afternoon to Wednesday morning) made no difference to the bureaucracy. Doctors need special permission. Since we did not want to wager the significant costs we have already incurred for travel and accommodation on the possibility that it would take several days rather than several weeks, she cancelled her visa application and we amended our travel plans to spend more time in a country that do not consider doctors as a  X  X hreat. X  Obviously the application fee (equivalent to about 19 Big Mac TM burgers, to express costs in terms of
For the record, she planned to accompany me solely to take a break from her hectic work schedule in South Africa. international purchasing power) was not refunded. This example illustrates three major points 2 : Firstly, any personal attribute can be the basis for  X  X pecial X  treatment: in this case, special skills held undesirable possibilities for the authorities. 3 This is in stark contrast to traditional privacy wisdom that usually holds that information about an individual (or per-sonal attributes) can be categorised into categories that are more or less sensitive, with one X  X  own medical diagnoses of-ten more sensitive than, say, one X  X  surname. 4 The second major point illustrated by this example is some-thing that we will return to below: the notion of informed consent. On a visa application one is indeed warned that your personal information may be communicated to other countries. No indication is given that specific jobs will be treated different from others. If one argues that it is im-plied by the fact that your trade or profession is asked, it probably follows that marital status, age, the names of your spouse and parents, country of birth and similar informa-tion can all be used in non-obvious ways. In fact, when I went to the Embassy to try and get access to the direc-tive or instructions that earmark doctors for nonstandard treatment, I was told that the document was confidential. When I tried to get a reference or other number or name for that document (so that I could try to get hold of it via the Dutch Freedom of Information Act 5 , I was told that no
The visa example also raises a fourth point. Why is it necessary that a South African medical doctor X  X  visit needs to be preapproved via a longer  X  and costly  X  process while this is, presumably, not required in the case of a doc-tor from, say, the United States who wishes to visit the Netherlands? One can only assume that the huge disparity between salaries earned in the first and third world makes it more lucrative for a doctor from a third world country to quickly earn a few Euros. While it is true that living costs are lower in third world countries, medical equipment, books and other necessities are actually much more expen-sive in third world countries. This inevitably makes one think of Chomsky X  X  insight (expressed by Fox [8]) that  X  X n the ill-balanced scales of global business, the favoured Eu-roamerican  X elites must inevitably grow richer, while the rest of the world could revert to the conditions of Blake X  X   X  X ark Satanic Mills X . X  While this has profound privacy implica-tions  X  because being African is a personal attribute and because the greater the disparities between first and third world countries become, the greater the  X  X hreat X  that citi-zens from third world countries will hold for the first world will become  X  this point will not be discussed further in the current paper.
This reminds me of an anecdote I heard about someone who, when the previous South African Government was still in control, decided to study Russian simply because he was interested in languages. He soon realised that, presumably because the African National Council had ties with Russia, learning Russian was an undesirable skill in South Africa then, that brought his activities under close scrutiny from government.
However, a surname often has religious, nationality, eth-nic and other connotations. My surname, for example, is associated with the French Huguenots who fled France in the late 1500s and early 1600s when Protestants were per-secuted by the French state. A surname with a religious (or other) connotation become sensitive in a privacy sense when it is associated with a group from which  X  rightly or wrongly  X  a current national or other threat is perceived. The connotation holds whether one is a member of the spe-cific  X  X hreatening X  group or not. Wet Openbaarheid van Bestuur, Wet van 31 oktober 1991 SIGKDD Explorations. Volume 4, Issue 2 -page 22 such identifying number existed.
 The third  X  and perhaps most significant  X  point illus-trated by this example, concerns controlled use of informa-tion one wants to share. If I used this example in years gone by, it would probably have been published in a paper-based form and distributed to technically inclined people. However, this paper is to be published on the Web, where it is accessible to the world. Conventional privacy wis-dom holds that one should look at the privacy policy of whomever one gives information to and rather withhold it if one sees possible harm coming from the sharing. The  X  X rivacy policy X  for this paper is wide, public dissemination. The fact that I use an example that expresses an opinion that the procedure used by the Dutch government seems, at the least, silly, is a personal opinion that may be used against me later  X  I have no idea how willing national gov-ernments are to let foreigners who have criticised them in public visit their countries. Conventional privacy principles therefore dictate that I should rather not use this particu-lar example. By implication, therefore traditional privacy limits freedom of speech. And this is a true challenge for database privacy: how can one enhance the goals of pri-vacy with the least impact on freedom of speech? A con-crete example may illustrate this point better. People rou-tinely publish their e-mail addresses on websites with the intention that individuals should be able to contact them; however, those addresses are (usually) not intended for col-lection by spiders or bots to compile address lists that are subsequently used for spamming. Over the years, people have employed various mechanisms to be still able to pub-lish e-mail addresses but withhold them from automated collectors. Initially, mailto links were dropped, because they unambiguously identify a piece of text as an e-mail address. Others are publishing their addresses with a por-tion that should be removed by a human before using it, eg jsoap@somedomain.removethisbeforesending.edu . Others are publishing the same address as jsoap AT somedomain.edu or, for lists of users, a webpage might say that the all e-mail addresses on the page should get an @somedomain.edu as a suffix and then list J Soap X  X  e-mail address next to his name simply as jsoap . Yet others are using a (graphical) image to convey their e-mail addresses to humans in a manner that is hard to collect by bots. To take protection of e-mail ad-dresses one step further, the standard format for articles in this publication includes an e-mail field; since I know the paper will be (widely) published I can opt-in or opt-out to include my e-mail address. In the first case, I lose control over how my personal information (e-mail address) is pub-lished; in the second case I make it hard for academics who would want to discuss any aspect of the topic with me (and the ease with which an author can be contacted probably has some impact  X  albeit small  X  on the number of times an author X  X  work is cited, which again impacts on promo-tion, funding and other similar aspects of academic life). In the case of this particular article, I addressed the problem by giving a URL for a webpage from which my address can be obtained, and where I have control over the format in which the address is shared.
 Note that the solutions given in the previous paragraph are limited because of the focus on one specific problem that is not indicative of the range of privacy problems to be ad-dressed. Further, the solutions are all ad hoc meaning that do not generalise well to other privacy problems. Some more general approaches will be mentioned in section 7 below. However, before considering technical solutions that will help balance confidentiality, integrity and availability of pri-vate data, the notion of privacy needs further attention. It will be clear that privacy as a concept is highly problem-atic and it is not possible to establish the required balance without fleshing the concept out somewhat. The Right to Privacy is permeated with problems, such as the exact definition of privacy , whether it constitutes a fun-damental right and whether people are and/or should be concerned about it.
 Garfinkel [10, p.4] says privacy is  X  X bout self-possession, au-tonomy and integrity. X  According to Margalit [17, p.211]  X  X elf-respect and humiliation are based on a private space whose invasion is a symbolic act interpreted as humiliation, in the sense of lack of consideration for the victim X  X  vital interests. X  Rosenberg [23, p.76] defines privacy as the  X  X re-vention of others from securing information about us that is immediately embarrassing (and so causes us pain) or of strategic value to others in their integration with us (and so imposes on us other material costs). X  From these (and many other similar) statements it is clear that two concepts are central to the kind of privacy we are interested in in this article: autonomy and (the implied possibility of) harm : The right to privacy (if such a right exists) claims that indi-viduals should have the greatest possible autonomy over in-formation about themselves to avoid the harm that could be done if information about themselves were to become avail-able to parties to whom they would not willingly give access to the information. Usually missing from definitions of pri-vacy is a third aspect: the benefits that storage of personal data may hold for the individual, such as the possibility of improved customer service mentioned earlier.
 However, beyond these two constants found in most defi-nitions, confusion reigns. Rosenberg [23, p.76] argues that privacy may not be a right after all but a taste:  X  X f privacy is in the end a matter of individual taste, then seeking a moral foundation for it  X  beyond its role in making social institutions possible that we happen to prize  X  will be no more fruitful than seeking a moral foundation for the taste for truffles. X  In 1758 Hume still thought it possible to estab-lish a standard of taste [13]. Hume X  X  argument is based on the existence of critics who are able to judge art and thereby establish this standard. Subsequent philosophers have de-nied the existence of such a standard, but have accepted the universal appeal that  X sthetic judgment has: When one judges art as beautiful, Kant says, one judges as if others ought to also judge it as beautiful [14]. If privacy is indeed a matter of taste, rather than a right, it would explain so-ciety X  X  difficulties to agree on exactly what privacy is. In that case much of what is commonly seen in the field of computing (including the last part of this paper) operates in the Humean fashion where  X  X xperts X  (critics) are trying to establish a standard that does not exist. And the attempts only seem realistic due to the universal nature of judgement of taste. Even though this is a core problem of privacy that should affect the manner in which it is dealt with in com-puting, it is not considered further in this paper. If privacy is indeed a right, one should consider whether it is a fundamental human right (or a moral right), a property SIGKDD Explorations. Volume 4, Issue 2 -page 23 right or some other right.
 John Rawls X  X  [20] theory of justice considers the just soci-ety and therefore can be used to think about fundamental human rights. The essence of his theory is that whatever people under a  X  X eil of ignorance X  (in the so-called original position) would choose, would be just. In other words, if people, who do not know whether they will turn out to be the privileged or underprivileged in society, were to choose for or against privacy, what would they choose? That can then be considered just. Elsewhere [19] I have argued that his theory supports privacy. (This brief argument was pre-sented to argue that a decrease in privacy levels for all to identify those who should be suspected of unlawful activi-ties would be unjust.) In a similar vein, Garret [11] men-tions some specific restrictions (or guidelines) that should be applied when collecting information about individuals and notes that such restrictions  X  X ould be endorsed by rep-resentative persons in the original position and included in the understanding of the Equal Liberties Principle that they would adopt to govern a just social order. X  Rosenberg [23, pp.84 X 90], in apparent contrast, tacitly in-vokes Rawls to argue that a just society would not choose (absolute) privacy for medical information and credit re-porting, since society has more to gain by forfeiting some privacy: By requiring medical information from applicants, medical insurance can be made affordable for people with av-erage health; without it, medical insurance and hence med-ical treatment becomes prohibitively expensive for almost everyone. By collecting information about people who de-fault on their loan repayments on blacklists, credit becomes possible in society; without such lists credit becomes pro-hibitively expensive with the implication that most people will not be able to buy expensive goods such as motorcars and houses.
 This contrast makes is necessary to reconsider whether pri-vacy is a fundamental human right. Note that privacy is indeed included in the bills of rights in a number of coun-tries. The constitution of South Africa [1], as one example, entrenches this right. However, it is often difficult to relate such rights to database privacy. To illustrate, the clauses of the privacy right in the South African Constitution that may apply directly to database privacy are those that grant ev-eryone not to have  X  X heir person . . . searched X  (  X  2(14)(a)),  X  X heir property searched X  (  X  2(14)(b)) and  X  X he privacy of their communications infringed X  (  X  2(14)(d)). The first two will be considered again below, but are indeed problematic to apply in database privacy. The third clause does seem to be applicable: if one communicates private information with a second party (who stores it in a database) and the second party subsequently shares that information with a third party without one X  X  permission, privacy has arguably been violated. The third clause does not, however, necessar-ily exclude violations of privacy by the second party itself (without involving any third parties).
 The first two clauses quoted above tie in with the notion of privacy rights as property rights. One often sees claims that you own your name and other private information and therefore controls it. Branscomb [3], however, asks whether you own your name, address, telephone number, medical history and a list of related personal attributes. 6 In each
While her legal arguments are from the US context, many of the arguments could also apply in other contexts. case she comes to the conclusion that the information (for example, your name) is considered public knowledge, or that someone else (the post office, the doctor, etc) actually owns the information. You are only a stakeholder. And the fact that conferences can be held on the topic  X  X ho owns our genes? X  [18] clearly illustrates that it is far from clear that one can claim bodily integrity to protect personal attributes such as your name.
 Moreover the fact that it is hard to treat privacy as an ab-solute right in any of the senses above, is painfully obvi-ous. According to Rosenberg [23], if the right to privacy only becomes important once the relative value of private information compared to the costs to obtain it rises beyond some level, the right to privacy is a prudential right and not a moral right. Brin [4, p.14] supports the view that privacy is not an absolute right:  X  X merican judicial rulings tend to treat privacy as a highly subjective and contingent commodity, a matter of trade-offs and balanced interests, whereas freedom of speech and freedom of the press are de-fended with sweeping judgments of broad generality. X  If privacy is not an absolute or fundamental right, one may ask whether it is a privileged right  X  one that takes prece-dence over many other rights. However, many authors are sceptical about the privileged status of a right to privacy. Brin [4, pp.14 X 15] argues that too much privacy will actually undermine privacy. His argument is that, while most people prefer not to be stared at by strangers while eating, they nev-ertheless go to restaurants to eat amongst strangers. Now suppose a restauranteur improves the privacy of all guests by erecting thin screens around all the tables so that guests are protected from the gazes of strangers. Brin argues that the voyeur in their midsts will jump at the opportunity that his or her newly established privacy offers, to find a means to look at the other guests  X  by, for example, making a small hole in the screen through which other people can be watched, but through which the other guests cannot see the voyeur. In other words, while the visibility of the voyeur in the open system was checked by the possibility of being seen staring at others, this protective mechanism falls away in the more private setting.
 Brin uses this example to consider more serious implications of too much privacy. Taking his cue from Popper X  X  notion of an open society he argues that transparency in society is of the utmost importance. Where Popper X  X  open society is a society that is open to criticism, Brin X  X  transparent society is one where actions of the  X  X atchers X  can be  X  X atched X  so that criticism can effectively function. It is not that privacy should be totally abandoned as bad, but  X  X ransparency is underrepresented in today X  X  fervid discussions about privacy and freedom in the information age X  [4, p.18] .
 Etzioni [6, p.5] uses a communitarian view to argue in favour of a  X  X uch needed social correction  X  [the] balancing of rights with a fresh emphasis on responsibilities  X  [that] has yet to be brought to bear on privacy issues. X  I suggest that much of the confusion arises from an oversim-plification of the concept of privacy (while acknowledging that I have not done justice to many of the more sophisti-cated views on privacy developed by some of the work cited above). One way to gain insight into the nature of privacy (or those aspects of it that can possibly be meaningfully pro-tected) is to consider informed consent. Informed consent already plays a role in database privacy. However, informed consent also plays a significant role in medicine  X  in partic-SIGKDD Explorations. Volume 4, Issue 2 -page 24 ular in medical research. Like privacy, medicine also has to balance wellbeing with possible harm. And, based on a num-ber of clear iniquities (such as the Tuskagee case) widespread debate has ensued over many years and these debates have culminated in ethical guidelines that address informed con-sent in medical research. While these guidelines explicitly consider informed consent, they implicitly guide the research process as such. Since informed consent can also be used in database privacy, guidelines for informed consent, in a sim-ilar manner, say something about the (practical) nature of privacy.
 The next section first briefly considers some of the issues surrounding informed consent  X  including in the manner in which it is used in privacy. Next, some of the best-known guidelines for informed consent are reinterpreted to see what light they may shed on privacy. The first objection against informed consent is that  X  for privacy purposes  X  it is usually a binary decision: if one accepts the terms of the privacy policy, one can go ahead and use the offered service; if not, one has to look elsewhere. This is, amongst others, the way that the procedure has been automated for P3P [21]. The debate about whether allowing people to  X  X pt-in X  or  X  X pt-out X  of services is an example of this binary mode of thinking.
 Often this (binary) choice that one purportedly has, is illu-sionary. As Etzioni [7] points out in many cases one has no alternative, but to use the service, and is therefore forced to give consent for using one X  X  personal in manners one would have preferred not to.
 Where one has a choice, consent is often not informed con-sent, due to the legalese used on consent forms, the time allowed for thinking about giving consent and the unpre-dictability of what records, that one is willing to share now, will contain in future. In fact, consent is often  X  X anufac-tured X  throughout society: when the content of the media is driven by the needs of the state and big business, the pub-lic  X  X ill accept the meaningless and subordinate lives that are appropriate for them and they X  X l forget subversive ideas about taking control of their own lives X  [27, p.85]. Finally, people  X  X ften do not read consent forms carefully because they assume that someone else has scrutinised the risks and benefits on their behalf X  [28]. This will be of par-ticular importance when considering my recommendations. Rather than using this state of affairs to argue that consent is of little use to protect privacy  X   X  X s a limited, secondary source for protection of privacy X  [7]  X  I suggest that consent itself needs to be rethought.
 Since informed consent forms a crucial part of medical re-search, it has seen much debate and current thinking is cap-tured in ethics guidelines. I suggest that the following as-pects hold the most potential for application in database privacy. Note that they are intended as points of dicus-sion both for informed consent when collecting information about individuals and as technical requirements of database privacy. To illustrate, the first point states that the individ-ual should be given an explanation of the purpose and that purpose should form an inherent part of the materialised database. 1. An explanation of the purpose for which data is being 2. Confirmation that individuals have the right to access 3. A  X  X escription of any reasonable foreseeable risks or 4. Disclosure of alternatives that exist for the individual 5. It should be stated what mechanisms are in place to 6. Ways in which an individual will be compensated if 7. What secondary uses the data may be used for [5, 8. Contact details for questions about the database and/or 9. It should be made clear when recording in a database 10. The approximate number of individuals whose infor-SIGKDD Explorations. Volume 4, Issue 2 -page 25 11. How long the information will remain in the database Having reconsidered informed consent from medical research in the light of privacy, the question should be asked: If pri-vacy depended on an individual X  X  informed consent for use of personal data, would it contribute to an increase in pri-vacy? Clearly, informed consent (and its implications for database privacy) can become so complex that it would be of little use to the average individual. I contend, however, that technology can be used to help solve this problem. The next section makes some suggestions in this direction. The picture of privacy that emerges from the preceding dis-cussion is indeed a complex one. The question that this brings to mind is: Is it worthwhile to implement technical means to protect privacy, if it is so hard to capture privacy itself? Above I have tried to flesh out some of the details (that are obviously open to alternatives, criticism and de-bate). It is possible to use some such details to construct systems that enhance privacy, without the need of a precise definition of privacy.
 The first challenge is to develop systems that can find a proper balance for confidentiality, integrity and availability of private information. Two approaches (at least) seem to have potential. The first is to encapsulate private informa-tion in some container or envelope. Whenever the private information is to be used the container verifies that the in-tended use is legitimate [25]. In this manner private infor-mation is made available, with the necessary confidential-ity built-in. Implementation of such systems, however, still presents major obstacles.
 Another approach is to distribute private information over a number of repositories and supply whoever has a legitimate need for such information with a pointer to the information [16]. Access to the information is controlled with the use of tickets or some other form of access control. Again the infor-mation is available to whoever has a legitimate need for the information. Moreover, when such data is properly main-tained in such repositories rather than in the databases of those who need the data, integrity can be enhanced, since the repository can be updated once with all users imme-diately directed to the updated information, improving in-tegrity.
 The second challenge is to develop systems that can deal with the inherent complexities inherent in privacy. One pos-sibility is the development of ontologies that express some of the underlying aspects of privacy, so that an individual X  X  privacy policy may be better compared with that of an-other party (using automated means). Additionally, given the complexity for the average individual of such a view of privacy, mechanisms should be established to deal with the complexity. Whether these mechanisms should operate analogous to trade unions, activists, to some other existing societal structure or should use a new form of cooperation is not clear. However, to be effective, it is clear that this process should be implemented using technology  X  because only if technology is used can it be employed wide enough and can it be used in a proactive manner, rather than only once privacy has been violated.
 This paper has intentionally steered clear of attempting to give specific technical solutions. Its intention was to high-light some of the issues that need to be considered to address database privacy. Issues will inspire solutions and solutions will suggest further issues. Such an iterative, balanced ap-proach is required for a humane and prosperous future. [1] Constitution of the Republic of South Africa, 1996. Act [2] Code of Federal Regulations, title 45, Public Welfare, [3] A. W. Branscomb. Who Owns Information? From Pri-[4] D. Brin. The Transparent Society  X  Will Technol-[5] Council for International Organizations of Medical Sci-[6] A. Etzioni. The Limits of Privacy . Basic Books, New [7] A. Etzioni. Medical records  X  enhancing privacy. pre-[8] J. Fox. Chomsky and Globalisation . Icon Books, Cam-[9] E. Gabber, P. B. Gibbons, D. M. Kristol, Y. Matias, [10] S. Garfinkel. Database Nation  X  The Death of Privacy [11] J. Garret. John rawls on moral principles for individu-[12] D. Goldschlag, M. Reed, and P. Syverson. Onion rout-[13] D. Hume. Of the standard of taste. In I. Aalen, editor, [14] I. Kant. Critique of Judgment . Hackett, 1987. Originally [15] F. A. Lategan and M. S. Olivier. Enforcing privacy by [16] F. A. Lategan and M. S. Olivier. PrivGuard: A model SIGKDD Explorations. Volume 4, Issue 2 -page 26 [17] A. Margalit. The Decent Society . Harvard University [18] Nordic Committee on Bioethics. Who owns our genes? , [19] M. S. Olivier. Position for panel on privacy. In Sixteenth [20] J. Rawls. A Theory of Justice . Harvard University [21] J. Reagle and L. F. Cranor. The platform for privacy [22] M. K. Reiter and A. D. Rubin. Anonymous web trans-[23] A. Rosenberg. Privacy as a matter of taste and right. In [24] L. B. Sauerwein and J. J. Linnemann. Handleiding voor [25] R.-C. Serban. The Private Cyberspace: Modeling Elec-[26] R. Whitaker. The End of Privacy  X  How Total Surveil-[27] M. Winston. On Chomsky . Wadsworth, Bellmont, CA, [28] J. Wise. Patients do not read consent forms. BMJ ,
