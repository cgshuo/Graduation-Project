 It is more and more difficult to extract information from medic al texts manually, be-cause of the increasing amount of the biomedical corpora. Autom atically extracting useful information from these resources will be helpful to doct ors and patients. For example, doctors can give patients the methods of treatment bas ed on the relationship between drugs and diseases. Currently, the development of named entity recognition technique for recognizing the dr ug, protein, disease, gene and treatment has reached a high level and the accuracy gives us an opportunity to extract relations from sentences with the named entities. There are three paradigms for the task of relation extraction below. 
In supervised approaches, resear chers take the task of relation extraction as a classi-fication task. The early supervised approaches include kernel m ethods [1] and feature based methods [2]. Recently, some deep learning methods (such a s the approach pro-posed by Miwa [3]) also belong to supervised approaches. The fe ature based methods classify. The kernel methods extract relations from sentences t hrough computing the work model used for relation extraction from the train corpora. Supervised paradigms suffer from two problems including expensive labeled corpora an d the feature selection. Different domain texts need different feature selections. 
The main idea of semi-supervised approaches is that if two enti ties are co-occurrence in many sentences, there may be a relation between them. Early, Snowball [4] used escu and Mooney used kernel method to extract relation from web [5]. However, the kernel method to extract relation, we need design new kernels a ccording to different domain texts. 
Another approach is unsupervised methods. The approaches [6-8] are assumed that the context between two entities maybe describe relation. There fore, these methods ex-ever, it will produce many string relations which need being ma pped to particular rela-tion base. 
We propose a new method of relation extraction which combines t he advantages of three approaches. We use the bootstrapping method to extract an d discover relation. At each iteration, we chose the universal lexical feature to form word vectors and generate domain texts. We can also extract relations from large amounts of corpora because only small amounts of data for parameters training are used without labeled corpora. Mean-while, we can use the method to discover new relations between entities which is not defined before in the domain corpora. The new relation which ot her supervised and semi-supervised methods can X  X  ex tract maybe also contain import ant information. This is useful to construct a m edical knowledge graph. Relation extraction is one of im portant tasks in NLP. Researche rs have explored nu-merous approaches to extract information from texts. tree kernel methods and the theory of co-occurrence analysis [1 1] proposed by DIPRE system [10]. Subsequence kernel approach for extracting protein relation uses sparse subsequences as feature and computes dot product between two ve ctors by the Support Vector Machines (SVM). Compared t o subsequence kernel, Qian pro posed an im-proved tree kernel method [12] which apply semantic information to classify protein interaction. Mintz proposed a distant supervised method [13] in 2008. They used Free-base [14], a large semantic database, instead of hand-labeled d ata to train. However, the method is not applied to biomedical corpora. Stanford NLP a lso studied the relation system. researchers can apply neural network to train model for relatio n extraction. Some re-achieved effective results. Firstly, researchers selected diffe rent features to form the vectors according to different corpora. Then, they trained neur al network through ad-justing parameters in the experiments. Finally, they would get a model to classify the relation in test collection. Sahu [16] proposed a method based on CNN in 2016. They used pre-trained word vectors obtained after running word2vec [ 17] on Pubmed arti-cles. These approaches what they proposed can X  X  discover the ne w relation in texts. 3.1 Relation Extraction and Discovery Algorithm bootstrapping algorithm. The main idea of method is that words around two entity men-tions in sentence maybe describe the relation. We can restrict the number o f the words around two entities [18]. There are similar structure of senten ces and semantic key-words in same relation description. Therefore, we can extract a nd discover relations from medical texts based on the basic idea. Here are the examples [s1], [s2], [s3] below:  X  [s2]: Tuberculosis can be cured with rifampicin in clinical trials.  X  [s3]: Tuberculosis maybe turn into lung cancer when people take rifampicin . gestive heart failure, Lexix &gt; which is new relation. 
We can take use of the rule to extract and discover relation fr om medical texts. The extracted relation will be helpful to doctors and patients. The input of our algorithm rithm of architecture is shown in Table 1. 3.2 Corpora Procession The preprocessing task is to identify entities in corpora, reco gnize POS, discard punc-tuation, select seed tuples and segment sentences. If a sentenc e contains more than two entities, we segment it into several sentences, each only conta ining two entities. For example, if a sentence contains n entities, we replicate it to n -1 sentences with two on the rule. Fig 1 shows the solution of segmentation. In order to select adapt seeds , we can combine the manual w ith automatic way. After tagging the entities, we use program to count the frequency of entity pairs and select good seeds from the candidate seed tuples according to the sent ences after analysis. 3.3 Tuple Generation It is an important step to generate the 5-tuples through the se ed tuples. We can use the seed tuples to extract the 5-tupl es from sentences with tagged entities. entity tags, and prefix, middl e, and suffix are context. We can adjust the words X  number that is no more than three in t he prefix and suffix. Middle consists of the words be tween two entities. For example, we can extract 5-tuple, tioned before. The number of prefix and suffix words can be lim ited by parameters of  X  X  X   X  X  X  and  X  X  X   X  X  X  in 5-tuple. Here, we limit the number of prefix words to 2. Nu mber of suffix words is the same as the prefix. Therefore, prefix fe ature vector can be gener-ated from  X  X e X  and  X  X now X  word v ectors. Suffix feature vector c onsists of  X  X n X  and  X  X linical X  word vectors. Middle f eature vector consists of  X  X as  X ,  X  X een X ,  X  X ured X  and  X  X ith X  word vectors. After we ex tract the 5-tuple, we combine t he prefix, middle and suffix feature vector i nto a 5-tuple vector. 3.4 Feature Selection We propose the way shown in this section to select features and construct 5-tuple vec-tors. Each word in every sentence is designed into four discret e features which are word ( POS ) respectively. We take the sentence [s2] as an example and Tab le 2 show the result of feature selection. The detail of feature space of eac h word is demonstrated below: 1. W : word vector prese nting sense of itself in each sentence. 2.  X  X  X  X   X  : represent the distance betwee n word and first entity [19]. 3.  X  X  X  X   X  : represent the distance betw een word and second entity. 4. POS : represent the part-of-speech o f the word. We can get the tag after running the Stanford NLP 2 . There are four dimensions in th e feature space of each word, wh ere  X  X  X   X   X  X   X   X   X  ically, if  X 1 X  ,  X   X  represents the W matrix which can be obtained after running the corpora on word2vec software. If  X 2 X  ,  X   X  can be generated randomly. The n in the huge, the others are not. We demonstrate the form of word vecto rs and 5-tuple vectors in Definition 2 and Definition 3. 
Definition 2: There is a 5-tuple,  X  X  X   X   X   X   X ...  X  , which consists of n serial words and contains m different words (  X  X  X  ). If  X 1 X  , each word in sentence (word is rep-from each word is below: Here  X   X   X  represents connection operatio n. The dimensions of vector of each word is 65. 
Definition 3: The construction of 5-tuple vector is shown in eq uation 2, excluding the entities X  vectors: 3.5 Pattern Extraction Cluster Vectors and Gene rate Candidate Pattern. Every seed tuple will extract many 5-tuple vectors from corpora. In order to generate pattern s, we should cluster the vectors and take the center vectors as candidate patterns.
 where  X   X   X   X   X   X   X ,  X   X ,  X   X ,  X   X   X ,  X   X  , and  X   X   X   X   X   X   X ,  X   X ,  X   X ,  X   X   X ,  X   X  . we use the equation 3 defined above to compute the similarity o f the 5-tuple vectors. If the value of similarity reach es the minimum threshold  X   X  X  X  X  , we cluster them into same algorithm to finish the clustering task. This kind of clusterin g method can be used in large amounts of corpora. 
At each iteration, there will be s everal classes after clusteri ng. We will select the centroid which can be generated through computing the center of vectors as candidate have identical values for e1 and e2. The definition 5 shows the equation 4 to compute the centroid. center vector. We select the 5-tuple which 5-tuple vector  X   X  that has the maximum value of Select Patterns. At each iteration, we will use equation 3 to compute confidenc e be-tween the candidate pattern vect ors and the pattern vectors tha t were generated in last discarded. Extract New Relation Tuples. After generating the new patterns and pattern vectors, we extract relation tuples from the 5-tuples when the similarit y between the 5-tuple vectors and pattern vectors is greater than threshold  X   X  X  X  X  X  X  X  . To evaluate our method, we experiment on two validated datasets which have been and F1-score to combine our method with others. This is shown i n equation 5. relation collection extracted from dataset and TRUE is all correct relation collection defined in the dataset. 4.1 SemEval-2010 Task 8 dataset We use the SemEval-2010 Task 8 dataset [20] which is freely ava ilable to evaluate our method. The dataset is composed of 10,717 tagged sentences, inc luding 8,000 training sentences and 2,717 testing sentences. There are nine relation classes and an Other class. The nine relations comprise Cause-effect, Component-Whol e, Content-Con-tainer, Entity-Destination, Entity-Origin, Instrument-Agency, M ember-Collection, Message-Topic and Product-Producer. The following example of se ntence [s4] demon-the Message-Topic(e2,e1) is incorrect. The directionality is ta ken into account for the official definition of relation. in small pharmacy chains. 
The method of official evaluation is based on the F1-scores for nine relations. Com-pared to previous approaches using the F1-score, we divide the corpus into five parts and select three seed tuples in each relation of every parts. T hen, we can generate 27 seed tuples as input. 
In contrast, we evaluate the exp eriment result via five-fold cr oss-validation. The Ta-ble 3 show the optimal p arameters of each fold. Because our method is different f rom the supervised method, we can extract the relation between entities without considering the directionality. For a fair comparison, the rela-Hendrickx. These methods used the traditional features and empl oyed SVM as the clas-sifier. Socher [21] proposed the RNN method described in the fo urth competitor to two entities to learn feature vectors. In the same article, the MVRNN method proposed by Socher was based on the RNN. This model assigned a matrix to every word and modified the meaning of other words. The fifth competitor based on CDNN was pro-and sentence level features. The seventh method named Proposed is our method. Classifier Feature Set F1-score SVM POS, stemming, syntactic patterns 60.1 SVM POS, stemming, syntactic patterns, WordNet 74.8 
SVM POS, prefixes, morphological, WordNet, depend-RNN POS, NER, WordNET 77.6 MVRNN POS, NER, WordNet 82.4 CDNN word pair, words around word pair, WordNet 82.7 Proposed W ,  X  X  X  X   X  ,  X  X  X  X   X  , POS 89.77 We can get the following conclusions from the Table 4. 2. The deep learning method can make a better performance through learning. 3. Our method uses the least features and outperforms the others i n the Table 4. Com-pared to the other methods in the Table 4, the universal featur es in our method can be token in other corpora and our bootstrapping methods can be used in large amounts of corpora. 4.2 i2b2-2010 dataset in the data comprise problems, t reatment and test. The relation s were defined TrCP, TrAP, TrWP, TrIP, TrNAP, TeRP, TeCP, PIP. To compare to the SVM method, we select the five relation types a nd Other class. We discard the sentences which contains one entity in the corpora and replicate the sentences with more than two entities. After that, the dataset is shown in the Table 5. We perform five-fold cross-valid ation on above dataset. The way of selecting seeds is the same as the SemEval-2010 Task 8 dataset. Table 6 shows the optimal parameters of each fold and Table 7 shows the result of different methods in the dataset. The first three competitors are linear SVM classifier with diff erent cost parameter C. The features set used by the SVM includes: any word between rel ation arguments, any POS tags between relation arguments, any bigram between relatio n arguments, word preceding first and second argum ent, any three words succeeding the first and second arguments, sequence of chunk tags between relation arguments, s tring of words be-tween relation arguments, first and second argument type (probl em, treatment, test), order of argument type appeared in sentence, distance between t wo arguments in terms of number of word and presence of only punctuation sign between arguments. The fourth method named Proposed is our method. We can get the foll owing conclusions from the Table 7. 1. If the cost parameter C in SVM is 0.1, the performance is the b est. 2. If the cost parameter C is 0.01 and 1, our precision and F1-sco re outperform the 
SVM. If the cost parameter C is 0.1, our precision is 0.02% low er than the SVM, but our F1-score is 0.02% higher than the SVM. Compared to the SVM in the Table 7, our method uses the less features than SVM and get good perform ance in the dataset. In this paper, we present a bootstrapping method to extract rel ation from medical texts. The approach shows better performance than several state-of-the -art methods in the experiment. Meanwhile, we can use the method to discover new re lation which is not be defined in the corpora. In the second dataset named i2b2, ou r precision is 0.02% lower than the SVM which cost parameter c is 0.01. Because the new discovered rela-tion is added to the EXP . Our method can avoid the disadvantages of lacking of labeled corpora and use the universal features to extract relations fro m different domain large amounts of texts. Moreover, the method has a good migration whi ch has been proved by the good performance in different corpora. Even, we can use our method to help people to label the new relations in large amounts of corpora. For the future work, we would like to improve the method of clustering and try to use the CNN to train the relation. Then, we plan to construct a knowledge graph system to help doctors and patients make decisions. This work is supported by National Natural Science Foundation o f China (Grand No. 61502022) and State Key Laboratory of Software Development Envi ronment (Grand No. SKLSDE-2015ZX-22). 
