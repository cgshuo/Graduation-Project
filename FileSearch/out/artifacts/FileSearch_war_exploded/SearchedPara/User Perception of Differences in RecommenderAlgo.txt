 Recent developments in user evaluation of recommender systems havebroughtforthpowerfulnewtoolsforunderstandingwhatmakes recommendations effective and useful. We apply these methods to understandhowusersevaluaterecommendationlistsforthepurpose of selecting an algorithm for finding movies. This paper reports on an experiment in which we asked users to compare lists produced by three common collaborative filtering algorithms on the dimensions of novelty, diversity, accuracy, satisfaction, and degree of personal-ization, and to select a recommender that they would like to use in the future. We find that satisfaction is negatively dependent on nov-elty and positively dependent on diversity in this setting, and that satisfaction predicts the user X  X  final selection. We also compare users X  subjective perceptions of recommendation properties with objective measures of those same characteristics. To our knowl-edge, this is the first study that applies modern survey design and analysis techniques to a within-subjects, direct comparison study of recommender algorithms.
 H.1.2[ User/machinesystems ]: Humanfactors; H.3.3[ Information storage and retrieval ]: Retrieval models recommender systems; user study
The ability of a recommender system to meet the needs of its users depends on many aspects of the recommender X  X  behavior, the application domain, and the user X  X  information needs. We want to understand the relevant properties of each of these entities (rec-ommenders, domains, and needs) and how they interact to form a compelling recommendation experience in a robust and systematic fashion. This leads to a key question: how do users perceive the outputs from various recommender algorithms to be different, and how those differences affect their choice of algorithm?
The research community has long known that there are subjective differences in the output of recommender algorithms, even among algorithms with comparable accuracy [21, 12]. To map out some of those differences in the movie recommendation domain, we present a user study aimed at understanding the subjective differences users perceivebetweendifferentcollaborativefilteringalgorithmsandhow those differences affect their choice of recommender system.
We are taking advantage of a unique opportunity: we have a large base of experienced recommender system users (the Movie-Lens user community) and a software toolkit capable of supporting a wide array of recommender algorithms (LensKit [5]). Further, we are preparing the general release of a new version of the MovieLens platform, providing an opportunity to conduct an experiment in a context where the question of user preference among recommender algorithms has real meaning. Finally, new insights from user evalu-ation studies allow us to measure the subjective aspects that explain why particular recommender algorithms are preferred [11].
This paper is one installment in a series of work on understand-ing how recommenders can best meet users X  information needs. It builds on extensive work on offline recommender evaluation and previous results identifying factors that influence user preference among recommendations and recommendation lists, such as diver-sity and novelty. Follow-up work will need to examine a greater range of algorithms, contexts of use, and properties that mediate a recommender X  X  ability to satisfy its users needs. Critically, it will also need to look at users X  long-term satisfaction with their recom-mender choices.
 In the present work we seek to answer the following questions: RQ1 How are users X  overall preferences for recommendation lists RQ2 What differences do users perceive between the lists of rec-RQ3 How do objective algorithm performance metrics relate to
To that end, we present the results of a user study we conducted users of the MovieLens recommender system, asking them to com-pare recommendation lists produced by popular recommender algo-rithms. We specifically explore item-item, user-user, and SVD al-gorithms, looking at user perceptions of accuracy, personalization, diversity, novelty, and overall satisfaction. Each user provided a first-impression preference between a pair of algorithms, subjective comparisons of the algorithms X  output on our dimensions of inter-est, and selected an algorithm for future use. We build a model that predicts both the user X  X  initial preference and their final choice of algorithm the subjective perceptions and objective measures of the recommender algorithms and their output.

While this experiment focuses on one recommendation domain thatisadmittedlywell-studied, ituncoverssubjectivecharacteristics of recommender behavior that explain users X  selections in a manner thatprovidesagoodbasisforgeneralization, replication, andfurther validation. We report specific relationships that can be tested for validity in additional contexts, providing much greater insight into what aspects of algorithm suitability for movie recommendation are task-specific and what are more general behaviors.

In addition to answering our immediate questions, the data col-lected in this survey should be a useful ground truth for calibrating new offline measures of recommender behavior to more accurately estimate how algorithms will be experienced by their users.
Many researchers have acknowledged the role of factors beyond accuracy  X  either predictive or retrieval accuracy  X  in evaluating recommender systems [13]. This has resulted in the development of offline evaluation protocols that incorporate metrics beyond accu-racy [8, 20], user-based research on recommender perception [11, 16], and a number of workshops and tutorials on recommender sys-tem evaluation. Industrial applications often evaluate recommender approaches by measuring lift, click-through rates, and other observ-able user behaviors that affect the core business goals the recom-mender is intended to serve; these behaviors arise from the holistic impact of the recommender on the user X  X  actions.

User studies are widely used to evaluate the usefulness of particu-lar recommender applications [4] and to answer scientific questions about user interaction with recommender systems [1]. The design and execution of user studies has improved over time; historically, many studies involved relatively simple user questionaires (a prac-tice that continues today), but recent years have seen increasing de-velopment and use of more sophisticated study designs and analysis techniques [11].

One such technique, structural equation modeling [10, 11], is a powerful tool for investigating the perceived factors that influence user satisfaction and choices. It allows us to not only measure what algorithms or items the user ultimately prefers, but also assess how specific aspects of the recommendations (such as novelty and diver-sity) influence their preferences and behavior. A user may prefer algorithm A over B because it is diverse and therefore more appro-priate to meeting their needs, and SEM allows us to quantify and test these kinds of relationships.

The particular factors that we consider are motivated by a long line of work in human-recommender interaction and recommender user experience [14, 11, 16]. Novelty [25, 22] and diversity [27, 22, 26, 24] are both widely regarded as an important factor in recom-mender system perception and acceptance.
To assess the differences among various algorithms for recom-mending movies with explicit user ratings, we conducted an exper-iment in which users reviewed two lists of recommendations and took a survey comparing them. Figure 1 shows a screenshot of the experimental interface.
We conducted our experiment on users of MovieLens, a movie recommendation service. The survey was integrated into a beta launch of a new version of MovieLens; we invited active users to preview the beta with an on-site banner and required them to partic-ipate in the experiment prior to using MovieLens Beta. 1052 users attempted the survey, of which 582 completed it. Since we limited recruiting to active users, all users had at least 15 ratings (the me-dian rating count was 473).
For this experiment, we tested three widely-used collaborative fil-tering algorithms as implemented in LensKit version 2.1-M2 [5]. To tune the algorithm parameters, we used the item-item CF con-figuration in the MovieLens production environment and values re-portedinthepublishedliterature[5, 6]asastartingpointandrefined the configurations with 5-fold cross-validation over the MovieLens database (using RMSE and prediction nDCG as our metrics to opti-mize) and manual inspection of recommender output. This resulted in the following algorithm configurations:
For each user, we randomly selected two of the algorithms. For each algorithm, we computed a recommendation list containing the 10 movies with the highest predicted rating among those the user had not rated, sorted by predicted rating. We presented these lists as  X  X ist A X  and  X  X ist B X  (the ordering of algorithms was randomized).
In internal pre-testing, the user-user and SVD algorithms often recommended very obscure movies. This created a significant risk that users would be entirely unfamiliar with the recommendations of these algorithms. While we want to measure novelty, users are limited in their ability to judge completely unfamiliar lists. To test the algorithms in something close to their pure form, while increas-ing the likelihood that users would have at least heard of some of the movies and therefore be able to provide meaningful feedback, we limited each algorithm to recommending from the 2500 most-rated movies in MovieLens (about 10% of MovieLens X  X  entire col-lection). This adjustment may limit effect sizes (e.g. decreasing perceived novelty of an algorithm X  X  recommendations), but should allow each algorithm to still demonstrate its general behaviors rela-tive to the others.

Not all algorithms could produce 10 recommendations for all users. If a user could not receive 10 recommendations from each algorithm, we exclude them from the analysis.

Most studies of recommender user experience employ a between-subjects design in which the users only see one condition (i.e. one algorithm at the time). Such between-subject designs are more real-istic of real world experiences. However, in our present experiment we are primarily interested in detecting differences between algo-rithms, some of which may be quite subtle. If users evaluated each algorithm X  X  output separately, their experience with that algorithm would not be related to another; this is problematic as evaluation is a naturally relative activity: absolute judgments are much more dif-ficult than relative judgments and less sensitive to small differences [9]. Therefore we chose to evaluate these algorithms with a simulta-neous within-subjects design in which our participants jointly eval-uate two out of three algorithms side-by-side.
Algorithms will not necessarily generate scores on the same por-tions of the rating scale. For example, one algorithm may tend to predict 4.5 X 5 stars, while another algorithm may be more conserva-tive and predict 3.5 X 4.5 stars. While we want users to evaluate the recommendation lists, not the predictions, this could have a con-founding affect if the predicted rating affects the user X  X  perception of the movie lists. To control for this, we assigned each user ran-domly to one of the following prediction conditions:
If predicted ratings do not affect the user X  X  perception of the rec-ommendation lists, then there should be no difference in response between these conditions and we can average across them in the final analysis.
Our survey consists of four parts. The first question, visible in fig. 1, asks users which list of recommendations they prefer, based on their initial impression. 5 options are available, with the ex-tremes labeled  X  X uch more A than B X  and  X  X uch more B than A X .
Following initial preference are 22 questions about various as-pects of the lists, designed to measure the user X  X  perception of the recommendation lists across five factors: Acc Accuracy  X  the recommender X  X  ability to find  X  X ood X  movies. Sat Satisfaction  X  the user X  X  overall satisfaction with the recom-Und Perceived personalization ( X  X nderstands Me X )  X  the user X  X  Nov Novelty X  X hepropensityoftherecommendertosuggestitems Div Diversity  X  the diversity of the recommended items.

For this portion of the survey, we started with questions that have worked well in previous experiments [11], adapted them to be com-parative instead of absolute, and wrote a small number of new ques-tions. The full list of questions is in table 1a.

After the main body of questions, we ask users which algorithm they would like to use by default once MovieLens gains the ability to support multiple recommender algorithms in parallel (a feature we are planning to develop in the coming months). This question is forced-choice, requiring users to pick one of the two algorithms. It also carries some consequence for users: while they will be able to switch algorithms in their user settings page without much diffi-culty, the algorithm they select will be providing their default rec-ommendations in the future.
In addition to soliciting users X  subjective perceptions of the rec-ommendations, we computed objective measures of the algorithms X  behavior with respect to accuracy, novelty, and diversity. We estimate the accuracy of each algorithm by computing the RMSE of using it to predict each user X  X  last 5 ratings prior to taking the survey, averaging the errors per user. To estimate novelty, we take the simple approach of computing the mean popularity rank of the items recommended to the user (fig. 3b); this creates an  X  X b-scurity X  metric, where high values correspond to lists with more obscure items.

We compute diversity with intra-list similarity [27] using cosine between tag genome vectors[23] as the itemwisesimilarity function and normalizing the final metric so that a list of completely similar items has a score of 1; we exclude items for which tag genome data is not available (no list required us to exclude more than 2 items); fig. 3c shows these values.

To convert the metrics into comparative measures, we take the log ratio of the objective metric values for the two recommendation lists presented to a user 1 . This produces a single value for a pair of algorithms or recommendation lists that we can attempt to correlate with the users X  subjective comparative judgements. 582 users completed the study over 81 days. Table 2 shows how many participated in each algorithm condition, along with their fi-nal choice of algorithm. Users generally selected both item-item and SVD over user-user (  X  &lt; 0.0001 ), but there was no statistically significant difference in the proportion of users choosing between item-item vs. SVD. Table 1b summarizes the responses to each of our questions by algorithm condition, and fig. 3 shows the objective measures of each algorithm X  X  output.

We observed no significant effect of either the ordering of algo-rithms or of the prediction condition 2 , so we exclude those from the remainder of the analysis.
To answer our more detailed research questions about the fac-tors at play in users X  choice of algorithms, we subjected the survey results to confirmatory factor analysis (CFA) and structure equa-tion modeling (SEM). We used Lavaan [18] for R [17] to compute the CFA and SEM, treating all question responses as ordinal vari-ables. Each question is mapped to the factor it was designed to target. Table 1c shows the question/factor loadings from both the initial CFA and a simplified SEM derived from it 3 . In the full CFA, there are several questions that have very low explanatory power (such as  X  X hich recommender more represents mainstream tastes? X  with  X  2 = 0.006 ); in addition, the Accuracy , Satisfaction , and Un-derstands Me factors are very highly correlated (corellation coeffi-centsinexcessof 0.9 ), sowecannotlegitimatelyconsiderthemtobe measuring different constructs in this experiment. We simplify the model by removing the Accuracy and Understands Me factors (we retain Satisfaction because it has the highest explanatory power, as
W e also experimented with computing raw differences, but gener-ally found the log ratio to be a better predictor.
There are at least 3 possible causes of this non-effect: recom-menders all predicted in the same range, prediction had no effect on perception, or our questions successfully guided users to evaluate the lists independent of prediction. In any case, it did not confound our results.
Full Lavaan output for the SEM is included in the thesis form of this work [3]. measured by the Average Variance Extracted, and all 5 of its ques-tions load strongly), and removing poorly-loading questions from Novelty . We then expand the simplified CFA into an SEM, which we call the Overall SEM , by adding structural relationships between factors, regressing themagainsttheexperimental conditionsandob-jective metrics, and regressing the user X  X  first impression and final selection against the experimental factors.

Figure 2 and table 1c show the structure and question/factor load-229.5 ,  X  &lt; 0.001 , CFI = 0.999, TLI = 0.998, RMSEA = 0.033). The model uses standardized factor scores, so a coefficient for the effect on or of a factor measures the effect in standard deviations of the factor. We use item-item vs. SVD as the baseline condition, en-coding the item-item/user-user and SVD/user-user conditions with dummy condition variables.
To address RQ1, we consider the impact of the factors ( Nov , Div , and Sat ) on the user X  X  first impression of the recommendation lists and on their final choice of algorithm (see fig. 2). Most users who preferredonealgorithmovertheotherattheirfirstimpressionpicked that algorithm in the final forced-choice question.

The only significant predictor (besides first impression) of the user X  X  final choice of algorithm was their relative satisfaction with the two recommendation lists. Users tended to pick the algorithm with which they expressed more satisfaction.

Satisfaction in turn is influenced by the novelty (negatively) and diversity (positively) of the recommended items. Novelty also has a small positive impact on diversity, suggesting that there is an upside to novelty (as it correlating with more diverse lists, which correlates positively with satisfaction) but a strong downside (users don X  X  like recommendation lists full of unfamiliar items).

In addition to its indirect effect through satisfaction, novelty had an additional negative influence on the user X  X  first-impression pref-erence. This means that novelty has a strong initial impact on user preference. However, after the user has made their first judgement, answered the more in-depth questions, and finally selected an algo-rithm, the direct impact of novelty went away and their final choice depended primarily on satisfaction. Novelty is still a significant negative influence, but it is mediated through satisfaction.
In RQ2, we want to understand how the algorithms themselves compare on relative satisfaction, diversity, novelty, and user prefer-ence as exhibited in their choice of algorithm. Table 2 summarizes the final choice performance of the three algorithms: as measured by users picking an algorithm for use, user-user clearly loses, and item-item and SVD are effectively tied.

Table 1b provides some insight into users X  perception of the rel-ative charateristics of the algorithms. Across most questions, item-item and SVD are indistinguishable (user responses are symmet-rically distributed about the neutral response). Item-item shows slightlymorediversitythanSVD.Theotheralgorithmpairingsshow more differences across the board, with the exception of item-item and user-user being indistinguishable on diversity.

Our overall SEM (fig. 2) and related factor analysis incorporate the experimental condition, but its impact is difficult to interpret due to the comparative nature of the experiment. To better under-stand each pair of algorithm X  X  relative performance, we reinterpret our experiment as three pseudo-experiments. Each of these pseudo-experiments uses one of the algorithms as a baseline and compares the other two algorithms on their performance and behavior relative to the baseline in a between-subjects design; the experimental treat-T able2: Finalalgorithmselectionbycondition.  X  -valuesarefor two-sided proportion tests,  X  0  X   X / X  = 0.5 . ment is the choice of algorithm to compare against the baseline. We will refer to this algorithm as the tested algorithm.

Randomization ensures that the behavior characteristics of the baseline algorithm are likely to be evenly distributed between the two sets of users encountering that algorithm, so we can (with some limitations)interpretrelativemeasurementsofonealgorithm X  X com-parison with the baseline as absolute measurements of that algo-rithm X  X  behavior for the purposes of comparing with measurements of another algorithm against the same baseline.

Table3showsthepseudo-experiments,theirconditions,anduser X  X  selections under this interpretation. The first pair of rows describes one of the three pseudo-experiments. Examining all users assigned to one of the two conditions involving item-item CF, we use item-item as the baseline and ask how often users picked user-user or SVD over the baseline. We can apply this interpretation to all ques-tions and factors, not just selection. This allows us to make cleaner inferences at the expense of some statistical power.

For each experiment, we re-analyzed the data using SEM and basic regressions to predict the user X  X  relative preference and final T able 3: Split experiment summary.  X  -values are testing the null hypothesis that the user picked the tested algorithm over the baseline the same proportion of the time. choice. Each SEM reused the factor loadings from the overall SEM but re-learned the relationships between factors, choice, and con-dition. We also omitted the objective metrics from these SEMs in order to focus on the subjective differences between the algorithms. The model structure for each experiment is a simplification of fig. 2.
In addition to the condition, we also consider the number of rat-ings in the user X  X  history prior to joining the experiment as a proxy for their level of experience. It is possible for algorithms to perform differently for different users, or for more experienced users to judge recommendation lists differently. We computed the median number of ratings for the users participating in the experiment and set a con-dition variable indicating whether a particular had  X  X any X  or  X  X ew X  ratings relative to the median. (b) Popularity rank of recommended items (lower is more popular).
Users perceived user-user X  X  recommendations to be more novel than SVD X  X  (coef. 0.953,  X  &lt; 0.001 ). They also reported user-user to be producing more diverse recommendation lists (coef. 0.312,  X  &lt; 0.001 ). The effect on novelty was substantially stronger; com-bined with novelty X  X  strong negative influence on preference, im-pression, and choice, users generally found SVD X  X  recommenda-tions more satisfactory and desirable than user-users. The effect of novelty on diversity was not present in this model; novelty only affected satisfaction directly.

As explained above, these results are from comparative judge-mentsbetween the output of the tested algorithm (SVDor user-user) and the baseline algorithm (item-item). However, due to random-ization, we assume that there are no important differences in item-item X  X  output between the users comparing it against SVD and those comparing it against user-user. Therefore, we can reasonably make inferences about the relative behavior of SVD and user-user. These results are consistent with the raw survey response data for direct comparisons between SVD and user-user (table 1b), providing fur-ther support for their validity. They are also consistent with the objective measures of obscurity and diversity (figs. 3b and 3c).
Users selected SVD significantly more often than user-user (ta-ble 3), consistent with the results from users directly comparing SVD and user-user (table 2).
Wefoundno significantdifference indiversitybetweenitem-item and user-user CF; this is consistent with the raw results of direct comparison of these two algorithms in table 1b.

User-user produced more novel recommendation lists than item-item (coef.  X 1.563 ,  X  &lt; 0.001 ). This effect interacted with user experience (rating count); for high-rating users, user-user X  X  recom-mendationswerenotasnovelastheywereforlow-ratingusers. This moderating effect was small, however, and user-user was signifi-cantly more novel than item-item even for high-rating users. There was no significant difference in item-item X  X  novelty performance between low-and high-rating users. Item-item produced slightly more diverse recommendations than SVD (coef.  X 0.26 ,  X  &lt; 0.001 ); this is consistent with the response distributions in table 1b as well as the difference in intra-list similar-ity (fig. 3c). However, diversity did not have a significant influence on satisfaction in this pseudo-experiment: the only significant pre-dictor of satisfaction was novelty.

The number of ratings the user had in their history prior to the experiment had a significant effect on the algorithm: for high-rating users, both algorithms were more novel than user-user. Since item-item and SVD did not have significantly different perceived novelty, this effect is reflecting user-user X  X  decreased novelty for high-rating users. Whether there is an additional increase the novelty of item-item and SVD for high-rating users, or just a decrease in user-user X  X  novelty, is beyond this experiment X  X  capability to measure.
To address RQ3, we consider in more detail the relationships be-tween the objective metrics and subjective factors. Figure 3 shows the distributions of all objective metrics we computed.

The raw distributions of novelty and diversity measurements are consistentwiththeusersurveyresults. User-userproduceslistswith less popular (and therefore likely more novel) items than SVD or item-item. SVD tends to produce somewhat less diverse recom-mendation lists. All three algorithms had comparable retrospective accuracy , with SVD having a slight edge. Popularity/obscurity was the only objective metric that we found to significantly differ be-tween conditions in the overall model.

Each objective metric was a statistically significant predictor of itscorrespondingsubjectivefactor(fig.2)andnootherfactor. There-fore, there is good correspondence between the subjective and ob-jective measures of these three concepts, and the effect of the objec-tivemeasureson finalchoiceiscompletelymediatedbytheirimpact on the subjective measures. All indirect effects of objective mea-sures on final choice are significant.

This means that predictive accuracy, for example, does affect the user X  X  final choice, but only through the increased satisfaction that it produces. Further, the impact of novelty and diversity on satisfac-tion means that after controlling for predictive accuracy, diversity and novelty still have significant impacts on user satisfaction.
The direct effects of condition on novelty, in addition to the effect mediated through objective obscurity, suggest that user-user is pro-ducing lists that users perceive to be more novel beyond the sense of novelty that our objective metric can capture.
We set out to measure user perception of various interesting prop-erties of the output of different recommender systems in a widely-studied domain. Our experiment uncovered mediation effects of novelty, diversity, satisfaction on users X  choice of recommender al-gorithms. In this section, we highlight some of the key findings.
One of the most striking things we found is that the novelty of recommended items has a signficiant negative impact on users X  per-ception of a recommender X  X  ability to satisfactorially meet their in-formation needs. This effect was particularly strong in its impact on the user X  X  first impression of an algorithm, and was present even though we restricted the depth of the long tail into which our algo-rithms could reach.

This suggests that recommender system designers should care-fully watch the novelty of their system X  X  recommendations, partic-ularly for new users. Too many unfamiliar recommendations may give users a poor impression of a particular recommender, poten-tially driving them to use other systems instead. Increasing the nov-elty of recommendations as the user gains more experience with the system and has had more time to consider its ability to meet their needs may provide benefit, but our results cannot confirm or deny this. The users in our study are experienced with movie recommen-dation in general and MovieLens in particular (the median user has rated 473 movies), and their first impressions were still heavily in-fluenced by novelty.

Our results complement the notion that that trust-building is an important goal of a recommender in the early stage of its relation-ship with its users [14]. They are also consistent with previous re-sultsfindingthatnoveltyisnotnecessarilypositivelycorrelatedwith user satisfaction or adoption of recommendations [2].
We have also demonstrated that the diversity of recommenda-tions has a positive influence on user choice of systems for general-purpose movie recommendation. Diversity is often framed as being intensionwithaccuracy, sothataccuracymustbesacrificedinorder to obtain diverse recommendation lists [27, 25, 26], and many di-versification techniques do result in reduced accuracy by traditional objective measures. The strong correlation of perceived accuracy and satisfaction in our results provide evidence that there may not be such a tradeoff when considering user perception instead of tra-ditional accuracy metrics.

The influence of novelty and diversity on satisfaction even after controlling for predictive accuracy provides direct, quantitative evi-dence for subjective but observable characteristics of recommenda-tion lists affect user satisfaction and choice. Further, accuracy alone does not all aspects of satisfaction.
When it comes to comparing the particular algorithms that we tested, item-item and SVD performed very similar, with users pre-ferring them in roughly equal measure. We do not yet have insight into whether there are identifiable circumstances in which one is preferable over the other. It may be that one works better for some users than others; it may also be that their performance is roughly equivalent, and one does not work significantly better. The differ-ence in the diversity of SVD and item-item, however, provides evi-dence for some kind of interesting difference between them.
User-user is the clear loser in our tests. Its prective accuracy was comparable to that of the other algorithms, but it had a signifi-cant propensity for novel recommendations that hurt both users X  ex-pressedsatisfactionwithitsoutputandtheirinterestinusingitinthe future. The lack of a significant independent effect of user-user con-ditiononsatisfactionorselectionsuggeststhattheincreasednovelty is the primary cause of user-user X  X  poor subjective performance.
Finally, all three algorithms had similar predictive accuracy, but users still had strong preferences between some pairings. However, users selected item-item and SVD in almost equal numbers even though SVD had slightly higher predictive accuracy. This provides additional evidence that, at least beyond a certain point, offline met-rics fail to capture much of what will impact the user X  X  experience with a recommender system.
We have reported the results of a user study to compare the out-put of three common collaborative filtering algorithms and identify subjective, user-perceptible differences between them. This work directly advances our understanding of the role of diversity and nov-elty in how users evaluate recommender systems for potential use. We hope that the collected data will also be useful for developing and refining additional measures of recommender behavior, allow-ing for high-throughput offline evaluation to more accurately esti-mate the user experience with recommendations.
 In the future, we plan at least two direct extensions of this work. First, we will examine users X  long-term use of the algorithm switch-ing feature we are developing for MovieLens; in particular, we want to see if users X  expressed preference in our study corresponds to their long-term stable choice of algorithm. Second, we want to see if there are aspects of a user X  X  profile beyond their experience level that predict their algorithm preference. Even though user-user did poorly overall, about 25% of users preferred it: who are these users, and why does it work for them?
We also want to explore additional objective measures that may predict the subjective characteristics we describe here.
Our results are currently limited to a single domain and task, al-though they are consistent with results elsewhere [24]. They are alsolimitedtosingleconfigurationsofeachofthetestedalgorithms; alternative tunings may result in very different performance. The structural model and mediating factors we have described, however, provide a valuable starting point for understanding exactly how our results do or do not generalize. Further experiments in other do-mains can examine the subjective characteristics we have studied to see whether their relationships hold across recommendation tasks.
U nderstandinghowusersperceiveandinteractwithrecommenders is critical to building better tools for meeting users X  information needs. This work provides new insights into the factors at work in the usefulness of movie recommendations. We look forward to much more work, from ourselves and others, to build out a more systematic understanding of how to produce effective, useful, and even delightful recommendations in a broad range of applications. We thank our colleagues in GroupLens Research for their support and assistance. This work has been funded by the National Science Foundation under grants IIS 08-08692 and 10-17697.
 [1] D. Bollen, B. Knijnenburg, M. Willemsen, and M. Graus. [2]  X . Celma and P. Herrera. A New Approach to Evaluating [3] M. D. Ekstrand. Towards Recommender Engineering: Tools [4] M. Ekstrand, P. Kannan, J. Stemper, J. Butler, J. A. Konstan, [5] M. Ekstrand, M. Ludwig, J. A. Konstan, and J. Riedl. [6] S. Funk. Netflix Update: Try This at Home. The Evolution [7] J. Herlocker, J. A. Konstan, and J. Riedl. An Empirical [8] J. Herlocker, J. A. Konstan, L. Terveen, and J. Riedl. [9] C. K. Hsee and J. Zhang. General Evaluability Theory . [10] R. B. Kline. Principles and Practice of Structural Equation [11] B. Knijnenburg, M. Willemsen, Z. Gantner, H. Soncu, and [12] S. McNee, N. Kapoor, and J. A. Konstan. Don X  X  Look [13] S. McNee, J. Riedl, and J. A. Konstan. Being accurate is not [14] S. McNee, J. Riedl, and J. A. Konstan. Making [15] A. Paterek. Improving regularized singular value [16] P. Pu, L. Chen, and R. Hu. A user-centric evaluation [17] R Core Team. R: A Language and Environment for [18] Y. Rosseel. lavaan: An R Package for Structural Equation [19] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl. Item-based [20] G. Shani and A. Gunawardana. Evaluating [21] R. Torres, S. McNee, M. Abel, J. A. Konstan, and J. Riedl. [22] S. Vargas and P. Castells. Rank and Relevance in Novelty [23] J. Vig, S. Sen, and J. Riedl. The Tag Genome: Encoding [24] M. C. Willemsen, M. P. Graus, and B. P. Knijnenburg. [25] M. Zhang and N. Hurley. Avoiding Monotony: Improving [26] T. Zhou, Z. Kuscsik, J.-G. Liu, M. Medo, J. R. Wakeling, [27] C.-N. Ziegler, S. McNee, J. A. Konstan, and G. Lausen.
