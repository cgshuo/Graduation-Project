 To discover the natural group structure of objects represented in numerical or categorical attributes [1], clustering analysis has been widely applied to a va-riety of scientific areas. Traditionally, c lustering analysis mostly concentrates on purely numerical data only. The typical clustering algorithms include the k-means [2] and EM algorithm [3]. Since the objective functions of these two algorithms are both numerically defined, they are not essentially applicable to the data sets with categorical attributes. Under the circumstances, a straight-forward way to overcome this problem is to transform the categorical values into numerical ones, e.g. the binary strings, and then apply the aforementioned numerical-value based clustering methods. Nevertheless, such a method has ig-nored the similarity information embedded in the categorical values and cannot faithfully reveal the similarity structure of the data sets [4]. Hence, it is desirable to solve this problem by finding a unified similarity metric for categorical and numerical attributes such that the met ric gap between numerical and categori-cal data can be eliminated. Subsequently, a general clustering algorithm which is applicable to various data types can be presented based on this unified metric.
In this paper, we will propose a unified clustering approach for both categorical and numeric data sets. Firstly, we prese nt a general clustering framework based on the concept of object-clust er similarity. Then, a new metric for both of numer-ical and categorical attributes is proposed. Under this metric, the object-cluster similarity for either categorical or numerical attributes has a uniform criterion. Hence, transformation and parameter adjustment between categorical and nu-merical values in data clustering are ci rcumvented. Subsequently, analogous to the framework of k-means, an iterative algorithm is introduced to implement the data clustering. This algorithm conducts an efficient clustering analysis without manually adjusting parameters and is applicable to the three types of data: nu-merical, categorical, or mixed data, i.e. the data with both of numerical and categorical attributes. Empirical studies have shown the promising results. Roughly, the existing clustering approaches dealing with data sets which con-tain categorical attributes can be summarized into the four categories [5]. The first category of the methods is based on the perspective of similarity. For ex-ample, based on Goodall similarity metric [6] that assigns a greater weight to uncommon feature value matching in similarity computations without assuming the underlying distributions of the feature values, paper [7] presents the Sim-ilarity Based Agglomerative Clustering (SBAC) algorithm. This method has a good capability of dealing with the mixed numeric and categorical attributes, but its computation is quite laborious. Beside the similarity concepts, the sec-ond category is based on graph partitioning. A typical example is the CLICKS algorithm [8], which mines subspace clusters for categorical data sets. This novel method encodes a data set into a weighted g raph structure, where each weighted vertex stands for an attribute value and two nodes are connected if there is a sample in which the corresponding attribute values co-occur. It is experimen-tally demonstrated that CLICKS outperforms ROCK algorithm [9] and scales better for high-dimensional data sets. However, this algorithm is not applicable to data mixed with categorical and numerical attributes and its performance also depends upon a set of parameters whose tuning is quite difficult from the practical viewpoint. The third category is entropy-based methods. For example, the COOLCAT algorithm [10] utilizes the information entropy to measure the closeness between objects and presents aschemetofindaclusteringstructure via minimizing the expected entropy of c lusters. The perfo rmance of this al-gorithm is stable for different data sizes and parameter settings. Nevertheless, this method can only be applied to purely categorical data and cannot handle numerical attributes. The last category of approaches attempts to give a dis-tance metric between categorical values so that the distance-based clustering algorithms (e.g. the k-means) can be directly adopted. Along this line, the most cost-effective one may be the k-prototype algorithm proposed by Huang [11]. In this method, the distance between two categorical values is defined as 0 if they are the same, and 1 otherwise while the d istance between numerical values is quantified with Euclidean distance. Subsequently, the k-means paradigm is uti-lized for clustering. However, since diff erent metrics are adopted for numerical and categorical attributes, a user-de fined parameter is utilized to control the proportions of numerical distance and categorical distance. Nevertheless, vari-ous settings of this parameter will lead to a totally different clustering result. A simplified version of k-prototype algorithm namely k-modes [12, 13], which is applicable for purely categorical data clustering, has also been widely utilized due to its satisfactory efficiency, and diff erent improvement strategies have been explored on this method [14 X 16]. The general task of clustering is to classi fy the given objects into several clusters such that the similarities between objects in the same group are high while the similarities between objects in different g roups are low [17]. Therefore, clustering ... , C k , can be formulated to find the optimal Q  X  via the following objective function: is an N  X  k partition matrix satisfying with Evidently, the desired clusters can be o btained by (1) as long as the metric of object-cluster similarity is determined. In the following sub-sections, we shall therefore study the similarity metric. 3.1 Similarity Metric for Mixed Data This sub-section will study the object-cluster similarity metric for mixed data. Suppose the mixed data x i with d different attributes consists of d c categorical at-and dom ( A r ) contains all possible values that can be chosen by attribute A r .For categorical attributes, the value domains are finite and unordered, dom ( A r ) with m r elements can be therefore represented with dom ( A r )=
Firstly, we focus on the difference betw een categorical attr ibutes and numeri-cal attributes. For categorical attributes, each attribute can usually represent an important feature of the given object. Therefore, when we conduct classification or clustering analysis, we often investigate the categorical attributes one by one such as Decision Tree method. By contrast, the numerical attributes are often treated as a vector and handled together in clustering analysis. Based on these observations, for the mixed data x i ,the d u numerical attributes can be treated as a whole but the d c categorical attributes should be investigated individually. be the average of the similarity calculated based on each attribute, we will have That is, the similarity between each numerical attribute and the cluster C j is replaced with the similarity between the c luster and the whole numerical vector x get Then, (4) can be further rewritten as s ( x i ,C j )= is the similarity on numerical attributes. Subsequently, the object-cluster simi-Similarity Metric for Categorical Attributes. In (5), we have assumed that each categorical attribute has the same contribution to the calculation of similarity on categorical part. However, from the practical viewpoint, due to the different distributions of attribute values, categorical attributes each often have unequal importance for clustering analysis. In light of this characteristic, (5) should be further modified with where w r is the weight of categorical attribute A r satisfying 0  X  w r  X  1and weighted summation of the similarity between the cluster and each attribute value. Weight factor w r describes the importance of each categorical attribute and is utilized to control the contribution o f attribute-cluster similarity to object-cluster similarity.
 Definition 1. The similarity between a categorical attribute value x c ir and clus-where NULL refers to empty, and  X  A r = x c called instances hereinafter) that have the value x c ir for attribute A r in cluster C .
 From Definition 1 , we can find that this metric of attribute-cluster similarity has the following properties: (1) 0  X  s ( x c ir ,C j )  X  1; According to (7) and (8), the object-cluster similarity for categorical part can be therefore calculated by where i  X  X  1 , 2 ,...,N } ,and j  X  X  1 , 2 ,...,k } .
 Remark 1. Since 0  X  s ( x c ir ,C j )  X  1and and will fall into the interval [0 , 1].
 Next, we discuss how to estimate the importance of each categorical attribute. From the view point of information theory, the significance of an attribute can be regarded as the inhomogeneity degree of the data set with respect to this attribute. Furthermore, it is described in [18] that if the information content of an attribute is high, the inhomogeneity of the data set is also high for this attribute. Hence, the importance of any categorical attribute A r ( r  X  X  1 , 2 ,...,d c } )can be calculated by with the total number of values that can be chosen by A r and X is the whole data set. Furthermore, according to (10), the more different values an attribute has, the higher its significance is. However, in practice, an attribute with too many different values may have little contribution to clustering. For example, the ID number of instances is unique for each instance, but this information is useless for clustering analysis. Hence, (10) can be further modified with That is, the importance of an attribute is quantified by its average entropy over each attribute value. The weight of each attribute is then computed as Subsequently, the object-cluster similarity on categorical part can be given by In practice, for an attribute A r , if all the instances to be classified have the same value a , it can be obtained from (12) and (11) that the importance of this attribute will be 0 as p ( a )=1and log (1) = 0. Then, the corresponding attribute weight will also be zero and this attribute will have no contribution to the whole clustering learning.
 Similarity Metric for Numerical Attributes. Since the distance between each vector x u i can be numerically calculated, the similarity metric for numerical attributes can be defined based on the m easure of distance. According to [19] and [20], it is a universal law that the distance and perceived similarity between numerical vectors are related via an exponential function as follows: where Dis stands for a distance measure. Moreover, it can be observed that the magnitudes of distances between instances from variant data sets may have a significant difference in practice. To avoid the potential influence of this scenario, we can further use proportional distance instead of absolute distance to estimate the similarity between numerical vectors.
 Definition 2. The object-cluster similarity between numerical vector x u i and cluster C j , i  X  X  1 , 2 ,...,N } , j  X  X  1 , 2 ,...,k } , is given by where c j is the center of all numerical vectors in cluster C j .
 It can be seen from Definition 2 that the values of this similarity metric also fall into the interval [0, 1]. In practice, different distance metrics can be utilized to calculate Dis ( x u i , c j ). For example, if the Minkowski distance is adopted, we shall have: where p&gt; 0 is a constant which characterizes the distance function. A typically special case of (17) is the Euclidean distance with p =2.

Finally, according to (6), (14), and (16), the object-cluster similarity metric for mixed data is defined as s ( x i ,C j )= where i =1 , 2 ,...,N , j =1 , 2 ,...,k . It can be seen that the defined similarities for categorical and numerical attributes in (18) are in the same scale. Hence, unlike k -prototype method, there is no need any more to manually adjust the parameter to control the proportions of numerical and categorical distances for different data sets. This paper concentrates on hard partition only, i.e., q ij  X  X  0 , 1 } , although it can be easily extended to the soft partition in terms of posterior probability. Under the circumstances, given a set of N objects, the optimal Q  X  = { q  X  ij } in (1) can be given by Therefore, similar to the learning procedure of k-means, an iterative algorithm, denoted as OCIL, can be conducted to implement the clustering analysis as shown in Algorithm 1.

The first step in OCIL algorithm, i.e. Step 1, is a procedure for the calcu-lation of object-cluster similarity. Thus, we can find that the iterative steps of Algorithm 1 Iterative clustering learning bas ed on object-clu ster similarity metric (OCIL) OCIL algorithm is the same as the k-means algorithm and the only difference is the measurement of similarity between ob ject and clusters. Therefore, the effec-tiveness of the proposed similarity metric can be easily evaluated by comparing OCIL with other similar algorithms, such as k-means and k-prototype. Next, we further give the time complexity analysis of OCIL algorithm. It can be ob-served that the computation cost of Step 1 is O ( mNd c ), where m is the average number of different values that can be chosen by each categorical attribute. For each iteration, the cost of the  X  for  X  statement is O ( mNkd c + Nkd u ). Hence, the total time cost of this algorithm is O ( t ( mNkd c + Nkd u )), where t stands for the number of iterations. From the practical viewpoint, k , m and t can be regarded as a constant in most cases. Th erefore, the time complexity of this algorithm approaches to O ( dN ). Hence, the proposed algorithm is efficient for data clustering, particularly for a large data set. This section is to investigate the effectiv eness of the proposed approach to data clustering. We applied it to various categorical and mixed data sets obtained from UCI Machine Learning Data Repository 1 and compared its performance with the existing counterparts. Since the proposed method on numerical data degenerates to the k-means algorithm, the effectiven ess of OCIL algorithm on numerical data set is transparent. Hence, there is n o need to investigate it any more. Each algorithm was coded with MATLAB and all experiments were implemented by a desktop PC computer with Intel(R) Core(TM)2QuadCPU,2.40GHzmain frequency, and 4GB DDR2 667 RAM.

Moreover, in our experiments, the clust ering accuracy [21] for measuring the clustering performance was estimated by where N is the number of instances in the data set, c i stands for the provided label, map ( r i ) is a mapping function which maps the obtained cluster label r i to the equivalent label from the data corpus by using the Kuhn-Munkres algorithm, Correspondingly, the clustering error rate is computed as e =1  X  ACC . 5.1 Performance on Mixed Data Sets In the following experiments, we will investigate the performance of the proposed algorithm on real data sets in comparison with the existing counterparts. Firstly, experiments were conducted on mixed data and the information of selected data sets is shown in Table 1. The performance of the proposed method has been compared with the k-prototype algorithm [11] and k-means algorithm, whose time complexity are also O ( Nd ). In k-prototype method, the distance regulation parameter  X  was set at 0 . 5  X  [11], where  X  is the average standard deviation of numerical attributes. When utilizing k-means, the categorical values were transformed into integers in our experime nts. Moreover, the Euclidean distance has been adopted as the distance metric of numerical vectors for consistency. Each algorithm has been run 100 times on each data set and the clustering results are summarized in Table 2.
 It can be seen that, both with random initializations, the proposed algorithm OCIL has an obvious superiority in ter ms of clustering accuracy over the k-prototype and k-means methods. This result shows that, in comparison with numerically representing th e distance between categorical values, the proposed similarity metric in this paper is a more reasonable measurement for clustering analysis on mixed data. Moreover, comparing the average running time of OCIL and k-prototype algorithms listed in Table 3, we can find that, although OCIL needs additional time to calculate the weight of each categorical attribute, its total running time is no more than k-prototype X  X  one. A plausible reason can be found from Table 3 is that the convergence speed of OCIL is usually faster than k-prototype in most cases we have tried so far. Therefore, the proposed similarity metric is efficient for mixed data clustering. 5.2 Performance on Categorical Data Sets We further investigated the performance of the proposed algorithm on purely categorical data. The information of four different benchmark data sets we uti-lized has been summarized in Table 4. To conduct comparative studies, we have also implemented the other two existing categorical data clustering algorithms: original k-modes (H X  X  k-modes) [12] and k-modes with Ng X  X  dissimilarity met-ric (N X  X  k-modes) [16]. In this experiment, each algorithm was conducted with random initializations. Table 5 lists the average value and standard deviation in error obtained by OCIL and the other tw o algorithms, respectively. It can be seen that the proposed clustering metho d has competitive advantage in terms of clustering accuracy and robustness compared with the other two methods. In this paper, we have proposed a general clustering framework based on object-cluster similarity, through which a unified similarity metric for both categorical and numerical attributes has been presented. Under this new metric, the object-cluster similarity for categorical and numerical attributes are with the same scale, which is beneficial to clustering analysis on various data types. Subsequently, analogous to k-means method, an iterative algorithm has been introduced to implement the data clustering. The advantages of the proposed method have been experimentally demons trated in comparison with the existing counterparts. Acknowledgment. The work described in this paper was supported by the Faculty Research Grant of Hong Kong Baptist University with the Project Code: FRG2/11-12/067, and the NSFC under grant 61272366.

