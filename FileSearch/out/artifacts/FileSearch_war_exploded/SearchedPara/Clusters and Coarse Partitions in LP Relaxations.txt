 dsontag@csail.mit.edu A common inference task in graphical models is finding the most likely setting of the values of the variables (the MAP assignment). Indeed, many important practical problems can be formulated as MAP problems (e.g., protein-design problems [9]). The complexity of the MAP problem depends on the structure of the dependencies between the variables (i.e. the graph structure) and is known to be NP-hard in general. Specifically, for problems such as protein-design, the underlying interaction graphs are dense, rendering standard exact inference algorithms useless.
 A great deal of effort has been spent recently on developing approximate algorithms for the MAP problem. One promising approach is based on linear programming relaxations, solved via message passing algorithms akin to belief propagation [2, 3]. In this case, the MAP problem is first cast as an integer linear program, and then is relaxed to a linear program by removing the integer constraints and adding new constraints on the continuous variables. Whenever the relaxed solution is integral, it is guaranteed to be the optimal solution. However, this happens only if the relaxation is sufficiently  X  X ight X  (with respect to a particular objective function).
 Relaxations can be made increasingly tight by introducing LP variables that correspond to clusters of variables in the original model. In fact, in recent work [6] we have shown that by adding a set of clusters over three variables, complex problems such as protein-design and stereo-vision may be solved exactly. The problem with adding clusters over variables is that computational cost scales exponentially with the cluster size. Consider, for example, a problem where each variable has 100 states (cf. protein-design). Using clusters of s variables means adding 100 s LP variables, which is computationally demanding even for clusters of size three.
 Our goal in the current paper is to design methods that introduce constraints over clusters at a reduced computational cost. We achieve this by representing clusters at a coarser level of granularity. The key observation is that it may not be necessary to represent all the possible joint states of a cluster of variables. Instead, we partition the cluster X  X  assignments at a coarser level, and enforce consistency only across such partitions. This removes the number of states per variable from consideration, and instead focuses on resolving currently ambiguous settings of the variables. Following the approach of [2], we formulate a dual LP for the partition-based LP relaxations and derive a message passing algorithm for optimizing the dual LP based on block coordinate descent. Unlike standard message passing algorithms, the algorithm we derive involves passing messages between coarse and fine representations of the same set of variables.
 MAP and its LP relaxation. We consider discrete pairwise Markov random fields on a graph G = ( V,E ) , defined as the following exponential family distribution 1 Here  X  is a parameter vector specifying how pairs of variables in E interact. The MAP problem we consider here is to find the most likely assignment of the variables under p ( x ;  X  ) (we assume that the evidence has already been incorporated into the model). This is equivalent to finding the assignment x M that maximizes the function f ( x ;  X  ) = P ij  X  E  X  ij ( x i ,x j ) .
 The resulting discrete optimization problem may also be cast as a linear program. Define  X  to be a vector of marginal probabilities associated with the interacting pairs of variables (edges) {  X  ij ( x i ,x j ) } ij  X  E as well as {  X  i ( x i ) } i  X  V for the nodes. The set of  X   X  X  that could arise from some joint distribution on G is known as the marginal polytope M ( G ) [7]. The MAP problem is then equivalent to the following linear program: are integral and correspond one-to-one with assignments x . Thus, there always exists a maximiz-ing  X  that is integral and corresponds to x M . Although the number of variables in this LP is only O ( | E | + | V | ) , the difficulty comes from an exponential number of linear inequalities typically re-quired to describe the marginal polytope M ( G ) .
 LP relaxations replace the difficult global constraint that the marginals in  X  must arise from some common joint distribution by ensuring only that the marginals are locally consistent with one an-other. The most common such relaxation, pairwise consistency , enforces that the edge marginals are consistent with the node marginals, {  X  | P x points of this local marginal polytope also correspond to assignments. If a solution is obtained at one such extreme point, it is provably the MAP assignment. However, the local marginal polytope also contains fractional extreme points, and, as a relaxation, will in general not be tight. We are therefore interested in tightening the relaxation. There are many known ways to do so, in-cluding cycle inequalities [5] and semi-definite constraints [8]. However, perhaps the most straight-forward approach corresponds to lifting the relaxation by adding marginals over clusters of nodes to the model (cf. generalized belief propagation [10]) and constraining them to be consistent with the edge marginals. However, each cluster comes with a computational cost that grows as k s , where s is the number of variables in the cluster and k is the number of states for each variable. We seek to offset this exponential cost by introducing coarsened clusters, as we show next. We begin with an illustrative example. Suppose we have a graphical model that is a triangle with each variable taking k states. We can recover the exact marginal polytope in this case by forcing the when k is large, introducing the corresponding k 3 variables to our LP may be too costly and perhaps unnecessary, if a weaker consistency constraint would already lead to an integral extreme point. To this end, we will use a coarse-grained version of  X  123 where the joint states are partitioned into larger collections, and consistency is enforced over the partitions. Figure 1: A graphical illustration of the consistency constraint between the original (fine granularity) For example, the shaded area in all three figures represents the same probability mass.
 The simplest partitioning scheme builds on coarse-grained versions of each variable X i . Let Z i denote a disjoint collection of sets covering the possible values of X i . For example, if variable X i has five states, Z i might be defined as { 1 , 2 } , { 3 , 5 } , { 4 } . Given such a partitioning scheme, we can introduce a distribution over coarsened variables  X  123 ( z 1 ,z 2 ,z 3 ) and constrain it to agree graphically in Fig. 1. In the case when Z i individuates each state, i.e., { 1 } , { 2 } , { 3 } , { 4 } , we recover the usual cluster consistency constraint.
 We use the above idea to construct tighter outer bounds on the marginal polytope and incorporate them into the MAP-LP relaxation. We assume that we are given a set of clusters C . For each cluster c  X  C and variable i  X  c we also have a partition Z c i as in the above example 2 (the choice of clusters and partitions will be discussed later). We introduce marginals over the coarsened clusters  X  ( z c ) and constrain them to agree with the edge variables  X  ij ( x i ,x j ) for all edges ij  X  c : The key idea is that the coarsened cluster represents higher-order marginals albeit at a lower res-olution, whereas the edge variables represent lower-order marginals but at a finer resolution. The constraint in Eq. 3 implies that these two representations should agree.
 We can now state the LP that we set out to solve. Our LP optimizes over the following marginal coarse-grained clusters. We would like to constrain these variables to belong to the following outer bound on the marginal polytope: Note that P z ation is then: This LP could in principle be solved using generic LP optimization tools. However, a more efficient and scalable approach is to solve it via message passing in the dual LP, which we show how to do in the next section. In addition, for this method to be successful, it is critical that we choose good coarsenings, meaning that it should have few partitions per variable, yet still sufficiently tightens the relaxation. Our approach for choosing the coarsenings is to iteratively solve the LP using an initial relaxation (beginning with the pairwise consistency constraints), then to introduce additional cluster constraints, letting the current solution guide how to coarsen the variables. As we showed in earlier work [6], solving with the dual LP gives us a simple method for  X  X arm starting X  the new LP (the tighter relaxation) using the previous solution, and also results in an algorithm for which every step monotonically decreases an upper bound on the MAP assignment. We will give further details of the coarsening scheme in Section 4. In this section we give the dual of the partition-based LP from Eq. 5, and use it to obtain a message passing algorithm to efficiently optimize this relaxation. Our approach extends earlier work by Globerson and Jaakkola [2] who gave the generalized max-product linear programming (MPLP) algorithm to solve the usual (non-coarsened) cluster LP relaxation in the dual.
 The dual formulation in [2] was derived by adding auxiliary variables to the primal. We fol-lowed a similar approach to obtain the LP dual of Eq. 5. The dual variables are as follows:  X  ened cluster c and edge ij  X  c . As in [2], we define the following functions of  X  : As we show below, the variables  X  correspond to the messages sent in the message passing algorithm that we use for optimizing the dual. Thus  X  ij  X  i ( x i ) should be read as the message sent from edge edges. Finally,  X  ij  X  ij ( x i ,x j ) is the message sent from an edge to itself. The dual of Eq. 5 is the following constrained minimization problem: By convex duality, the dual objective evaluated at a dual feasible point upper bounds the primal LP optimum, which in turn upper bounds the value of the MAP assignment. It is illustrative to compare this dual LP with [2] where the cluster dual variables were  X  c  X  ij ( x c ) . Our dual corresponds to The advantage of the above dual is that it can be optimized via a simple message passing algorithm that corresponds to block coordinate descent. The key idea is that it is possible to fix the values of the  X  variables corresponding to all clusters except one, and to find a closed form solution for the non-fixed  X  s. It then turns out that one does not need to work with  X  variables directly, but can keep only the  X  message variables. Fig. 2 provides the form of the updates for all three message types. S ( c ) is the set of edges in cluster c (e.g. ij,jk,ik ). Importantly, all messages outgoing from a cluster or edge must be sent simultaneously.
 Here we derive the cluster to edge updates, which differ from [2]. Assume that all values of  X  are depends on  X  c  X  ij ( z c i ,z c j ) can be written equivalently as can be easily shown (using an equalization argument as in [2]) that the  X  c  X  ij ( z c ) that satisfy the constraint and minimize the objective are given by The message update given in Fig. 2 follows from the definition of  X  c  X  ij . Note that none of the cluster messages involve the original cluster variables x c , but rather only z c . Thus, we have achieved the goal of both representing higher-order clusters and doing so at a reduced computational cost.  X  Edge to Node : For every edge ij  X  E and node i (or j ) in the edge:  X   X  Edge to Edge : For every edge ij  X  E :  X  Cluster to Edge : First define The algorithm in Fig. 2 solves the dual for a given choice of coarsened clusters. As mentioned in Sec. 2, we would like to add such clusters gradually, as in [6]. Our overall algorithm is thus similar in structure to [6] and proceeds as follows (we denote the message passing algorithm from Fig. 2 by MPLP): 1. Run MPLP until convergence using the pairwise relaxation, 2. Find an integral dual objective given in Eq. 8 is sufficiently close to the primal objective f ( x ;  X  ) , terminate, 4. Add a new coarsened cluster c using the strategy given in Sec. 4, 5. Initialize messages going out of the new cluster c to zero, and keep all the previous message values (this will not change the bound value), 6. Run MPLP for N iterations, then return to 2 . Until now we have not discussed how to choose the clusters to add and their partitionings. Our strategy for doing so closely follows that of our earlier work [6]. Given a set C of candidate clusters to add (e.g., the set of all triplets in the graph as in [6]), we would like to add a cluster that would result in the maximum decrease of the dual bound on the MAP. In principle such a cluster could be found by optimizing the dual for each candidate cluster, then choosing the best one. However, this is computationally costly, so in [6] we instead use the bound decrease resulting from just once sending messages from the candidate cluster to its intersection edges.
 If we were to add the full (un-coarsened) cluster, this bound decrease would be: Our strategy now is as follows: we add the cluster c that maximizes d ( c ) , and then choose a parti-be achieved by using the trivial partition Z c i = X i (which achieves d ( c ) ). However, in many cases it is also possible to achieve it while using much coarser partitionings.
 The set of all possible partitionings Z c i is too large to optimize over. Instead, we consider just | X i | candidate partitions that are generated based on the beliefs b i ( x i ) . Intuitively, the states with lower belief values b i ( x i ) are less likely to influence the MAP, and can thus be bundled together. We will therefore consider partitions where the k states with lowest belief values are put into the same  X  X atch-all X  coarse state s c i , and all other states of x i get their own coarse state. Formally, a partition Z i is characterized by a value  X  i such that s is how big we can make the catch-all state without sacrificing the bound decrease.
 We employ a greedy scheme whereby each i  X  c (in arbitrary order) is partitioned separately, while the other partitions are kept fixed. The process starts with Z c i = X i for all i  X  c . We would like to choose s c i such that it is sufficiently separated from the state that achieves d ( c ) . Formally, given a margin parameter  X  we choose  X  i to be as large as possible such that the following constraint still holds 3 : by starting with  X  i =  X  X  X  and increasing it until the constraint is violated. Since each subsequent value of s c i differs by one additional state x i , we can re-use the maximizations over z c \ i for the previous value of s c i in evaluating the constraint for the current s c i .
 It can be shown by induction that this results in a coarsening that has a guaranteed bound decrease of at least d ( c ) + min(0 , X  ) . Setting  X  &lt; 0 would give a partitioning with fewer coarse states at the cost of a smaller guaranteed bound decrease. On the other hand, setting  X  &gt; 0 results in a margin between the value of the dual objective (after sending the coarsened cluster message) and its value if we were to fix x i in the max terms of Eq. 11 to a value in s c i . This makes it less likely that a state in s c i will become important again in subsequent message passing iterations. For the experiments in this paper we use  X  = 3 d ( c ) , scaling  X  with the value of the guaranteed bound decrease for the full cluster. Note that this greedy algorithm does not necessarily find the partitioning with the fewest number of coarse states that achieves the bound decrease. We report results on the protein design problem, originally described in [9]. The protein design problem is the inverse of the protein folding problem. Given a desired backbone structure for the protein, the goal is to construct the sequence of amino-acids that results in a low energy, and thus stable, configuration. We can use an approximate energy function to guide us towards finding a set of amino-acids and rotamer configurations with minimal energy. In [9] the design problem was posed as finding a MAP configuration in a pairwise MRF. The models used there (which are also available online) have a number of states per variable that is between 2 and 158, and contain up to 180 variables per model. The models are also quite dense so that exact calculation is not feasible. Recently we showed [6] that all but one of the problems described in [9] can be solved exactly by using a LP relaxation with clusters on three variables. However, since each individual state has roughly 100 possible values, processing triplets required 10 6 operations, making the optimization costly. In what follows we describe two sets of experiments that show that, by coarsening, we can both significantly reduce the computation time and achieve similar performance as if we had used un-coarsened triplets [6]. The experiments differ in the strategy for adding triplets, and illustrate two performance regimes. In both experimental setups we first run the standard edge-based message passing algorithm for 1000 iterations.
 In the first experiment, we add all triplets that correspond to variables whose single node beliefs are tied (within 10  X  5 ) at the maximum after running the edge-based algorithm. Since tied beliefs cor-respond to fractional LP solutions, it is natural to consider these in tighter relaxations. The triplets correspond to partitioned variables, as explained in Sec. 2. The partitioning is guided by the ties in the single node beliefs. Specifically, for each variable X i we find states whose single node beliefs are tied at the maximum. Denote the number of states maximizing the belief by r . Then, we partition Figure 3: Comparison with algorithm from [6] for the protein  X 1aac X , after the first 1000 iterations. Left : Dual objective as a function of time. Right : The cost per one iteration over the entire graph. the states into r subsets, each containing a different maximizing state. The other (non-maximizing) states are split randomly among the r subsets. The triplets are then constructed over the coarsened variables Z c i and the message passing algorithm of Sec. 3 is applied to the resulting structure. After convergence of the algorithm, we recalculate the single node beliefs. These may result in a different partition scheme, and hence new variables Z c i . We add new triplets corresponding to the new vari-ables and re-run. We repeat until the dual-LP bound is sufficiently close to the value of the integral assignment obtained from the messages (note that these values would not coincide if the relaxation were not tight; in these experiments they do, so the final relaxation is tight).
 We applied the above scheme to the ten smallest proteins in the dataset used in [6] (for the larger proteins we used a different strategy described next). We were able to solve all ten exactly, as in [6]. The mean running time was six minutes. The gain in computational efficiency as a result of using coarsened-triplets was considerable: The average state space size for coarsened triplets was on average 3000 times smaller than that of the original triplet state space, resulting in a factor 3000 speed gain over a scheme that uses the complete (un-coarsened) triplets. 4 This big factor comes about because a very small number of states are tied per variable, thus increasing the efficiency of our method where the number of partitions is equal to the number of tied states. While running on full triplets was completely impractical, the coarsened message passing algorithm is very practical and achieves the exact MAP assignments.
 Our second set of experiments follows the setup of [6] (see Sec. 3), alternating between adding 5 triplets to the relaxation and running MPLP for 20 more iterations. The only difference is that, after deciding to add a cluster, we use the algorithm from Sec. 4 to partition the variables. We tried various settings of  X  , including  X  = 0 and . 01 , and found that  X  = 3 d ( c ) gave the best overall runtimes. We applied this second scheme to the 15 largest proteins in the dataset. 5 Of these, we found the exact MAP in 47% of the cases (according to the criterion used in [6]), and in the rest of the cases were within 10  X  2 of the known optimal value. For the cases that were solved exactly, the mean running time was 1 . 5 hours, and on average the proteins were solved 8 . 1 times faster than with [6]. 6 To compare the running times on all 15 proteins, we checked how long it took for the difference between the dual and primal objectives to be less than . 01 f ( x M ;  X  ) , where x M is the MAP assignment. This revealed that our method is faster by an average factor of 4 . 3 . The reason why these factors are less than the 3000 in the previous setup is that, for the larger proteins, the number of tied states is typically much higher than that for the small ones.
 Results for one of the proteins that we solved exactly are shown in Fig. 3. The cost per iteration increases very little after adding each triplet, showing that our algorithm significantly coarsened the clusters. The total number of iterations and number of triplets added were roughly the same. Two triplet clusters were added twice using different coarsenings, but otherwise each triplet only needed to be added once, demonstrating that our algorithm chose the right coarsenings. We presented an algorithm that enforces higher-order consistency constraints on LP relaxations, but at a reduced computational cost. Our technique further explores the trade-offs of representing complex constraints on the marginal polytope while keeping the optimization tractable. In applying the method, we chose to cluster variables X  states based a bound minimization criterion after solving using a looser constraint on the polytope.
 A class of approaches related to ours are the  X  X oarse-to-fine X  applications of belief propagation [1, 4]. In those, one solves low-resolution versions of an MRF, and uses the resulting beliefs to initialize finer resolution versions. Although they share the element of coarsening with our approach, the goal of coarse-to-fine approaches is very different from our objective. Specifically, the low-resolution MRFs only serve to speed-up convergence of the full resolution MRF via better initialization. Thus, one typically should not expect it to perform better than the finest granularity MRF. In contrast, our approach is designed to strictly improve the performance of the original MRF by introducing setting of coarse and fine variables are refined iteratively whereas in [1], once a coarse MRF has been solved, it is not revisited.
 There are a number of interesting directions to explore. Using the same ideas as in this paper, one can introduce coarsened pairwise consistency constraints in addition the full pairwise consistency constraints. Although this would not tighten the relaxation, by passing messages more frequently in the coarsened space, and only occasionally revisiting the full edges, this could give significant computational benefits when the nodes have large numbers of states. This would be much more similar to the coarse-to-fine approach described above.
 With the coarsening strategy used here, the number of variables still grows exponentially with the states of a cluster into a fixed number of states (e.g., two), and then constrain such partitions to be consistent with each other. Such a process may be repeated recursively, generating a hierarchy of coarsened variables. The key advantage in this approach is that it represents progressively larger clusters, but with no exponential growth. An interesting open question is to understand how these hierarchies should be constructed.
 Our techniques may also be helpful for finding the MAP assignment in MRFs with structured poten-tials, such as context-specific Bayesian networks. Finally, these constraints can also be used when calculating marginals.

