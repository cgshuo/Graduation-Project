 News reporting has seen a shift toward fast-paced online re-porting in new sources such as social media. Web Search engines that support a news vertical have historically re-lied upon articles published by major newswire providers when serving news-related queries. In this paper, we in-vestigate to what extent real-time content from newswire, blogs, Twitter and Wikipedia sources are useful to return to the user in the current fast-paced news search setting. In particular, we perform a detailed user study using the emerging medium of crowdsourcing to determine when and where integrating news-related content from these various sources can better serve the user X  X  news need. We sampled approximately 300 news-related search queries using Google Trends and Bitly data in real-time for two time periods. For these queries, we have crowdsourced workers compare Web search rankings for each, with similar rankings integrating real-time news content from sources such as Twitter or the blogosphere. Our results show that users exhibited a prefer-ence for rankings integrating newswire articles for only half of our queries, indicating that relying solely on newswire providers for news-related content is now insufficient. More-over, our results show that users preferred rankings that in-tegrate tweets more often than those that integrate newswire articles, showing the potential of using social media to better serve news queries.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval General Terms: Experimentation, Performance Keywords: News Vertical, Web Search, User-generated Content
Major universal Web search engines serve around a billion of user queries each day [25]. It has been reported that up to 11% of these Web search queries are related to current news [8]. We refer to these queries as news-related queries . When a news-related query is submitted to a Web search Figure 1: Example of vertical search result integra-t ion for the query  X  X gypt clashes X  on the 25/01/2013. engine, a news-vertical is used to identify relevant news-related content from one or more providers, traditionally e-newspapers. If relevant content is found, then this content can be integrated into the Web search ranking, normally within a vertical result block, as illustrated in Figure 1.
The online news landscape has been greatly affected by the emergence of user-generated content sources such as Twit-ter. In particular, the role of the public in the online news space has shifted from static consumers to real-time reporters and commentators. Indeed, current events are now sum-marised and discussed in real-time [7, 27] using a variety of diverse media [21], driven by user interaction and user-generated content, e.g. news reporting in Twitter [26].
This shift has had important consequences for Web search engines when tackling news-related queries. In particular, users are now searching for information about events mere seconds after those events have occurred [14]. Moreover, new and potentially valuable content is being produced out-side of the normal newswire providers that are used by news verticals. Indeed, we argue that relying solely on newswire providers is no longer sufficient to satisfy news-related queries.
In this paper, we investigate three questions with regard to real-time news vertical search, namely: Is newswire article integration still sufficient given the increasing pace of news-reporting? To what extent can content from user-generated sources also satisfy the end-user? How does the age of an event affect the types of content that users prefer? To an-swer these questions, we perform a large-scale user study. In particular, we first develop an evaluation interface for performing preference assessment between pairs of rankings. We then sample almost 300 queries relating to recent news events in real-time using the Google Trends and Bitly APIs. For each query and time, we also collect individual rankings o f Web pages, newswire articles, blogs, tweets and Wikipedia pages for them. Using our proposed interface in conjunction with the crowdsourcing marketplace Amazon X  X  Mechanical Turk, we employ workers (acting as surrogates for end-users) to compare Web page rankings with the same rankings en-hanced with additional news content from newswire, the bl-ogosphere, Twitter or Wikipedia. In this way, we evaluate the extent to which different sources can be used to better satisfy news-related queries. Indeed, this is the first user study examining the integration of different content types for news vertical search.
 The remainder of this paper is structured as follows. In Section 2, we provide a background into prior works that have examined news vertical search and crowdsourcing. Sec-tion 3 describes our methodology, including the design of the crowdsourcing interface we use in our user study. In Section 4, we detail our experimental setup in terms of topic development and crowdsourcing/worker statistics. Section 5 describes the results of our crowdsourced user study with re-spect to our three research questions. Concluding remarks are provided in Section 6.
This paper builds upon two areas related to the field of information retrieval (IR), namely news vertical search and crowdsourcing. We discuss prior works that examined news vertical search in Section 2.1, while we provide a background on crowdsourcing for IR in Section 2.2.
The field of news vertical search has seen little investi-gation to date, with prior works focusing on how to predict when a user will click on a news-related document added into the Web search ranking. In particular, Diaz [12] proposed a machine learning approach for click prediction on newswire articles. This approach trains an initial model using features extracted from the news articles along with additional query features from past Web and news vertical query-logs. The approach then incorporates click-feedback, in that it allows a user X  X  subsequent clicks to enhance the model over time. Arguello et al. [4] expanded upon the approach by Diaz for multiple verticals, extracting features from each vertical considered to build a classification model. Later, K  X  onig et al. [20] also examined a learned approach for click prediction for news-related queries. They propose the use of additional features from the query, in addition to features describing the distribution of the query terms in blog, newswire and Wikipedia corpora, with the aim of better estimating the proportion of users that will click on a newswire article if dis-played. Importantly, these works only examine the integra-tion of newswire articles into the Web search results, not the integration of user-generated content (in K  X  onig X  X  case blogs and Wikipedia are used to better determine when newswire articles should be displayed, not as a source of content). In contrast, in this paper, we examine the integration of blogs, tweets and Wikipedia articles in addition to newswire arti-cles. Moreover, rather than predicting document clicks to estimate when to integrate content (which requires access to large proprietary query logs), we perform a user study to ascertain from a user perspective when integrating ad-ditional content is useful. Furthermore, an added value of this work is that the assessments produced as part of our user study could be used to train machine learned models for news vertical search, like those described above.
Relatedly, in the context of search over multiple verticals (including the news vertical), Zhou et al. [31], proposed a general evaluation framework for aggregate search. How-ever, their framework pre-supposes that relevance assess-ments for each document are available, which is not the case in our scenario. Hence, we take an alternative evaluation approach in this paper. Arguello et al. [5] performed a user study using 29 people from a nearby town to examine the re-lationship between task complexity and the use of integrated search results from multiple. However, their study focused on the user cognitive process for general tasks, rather than news-specific search tasks. Indeed, to the best of our knowl-edge, our user study is the first that examines news vertical search to date.
Crowdsourcing in general is the act of outsourcing tasks, traditionally performed by a specialist person or group, to a large undefined group of people or community (referred to as the  X  X rowd X ), through an open call [16]. There are many motivations for crowdsourcing tasks. For example, simple tasks can be completed at a relatively small cost, and often very quickly [3]. Moreover, by employing a crowd of  X  X sers X  to perform assessments as opposed to a few  X  X xperts X , a wider range of talent can be accessed and expert bias avoided [15]. However, crowdsourcing has also been the subject of much controversy as to its effectiveness. In particular, the work produced by crowdsourced workers is known to sometimes be of low quality [6] and results can be affected by random or malicious work [13].

In an IR setting, crowdsourcing has been used as an al-ternative method for evaluating search system performance. Alonso et al. [3] first suggested crowdsourcing as a cheap, fast, effective and flexible alternative to using specialist as-sessors when creating relevance assessments for ad-hoc test collections. Indeed, crowdsourcing was later used to gen-erate relevance assessments for larger collections [2], in ad-support other tracks [24] at the Text REtrieval Conference (TREC). Relatedly, Zhu and Carterette [32] also used crowd-sourcing to perform a successful pilot study examining the integration of images into the Web search results.
In this paper, we use crowdsourced workers from the Ama-zon X  X  Mechanical Turk marketplace to compare Web search rankings to those enhanced with additional news-related con-tent for news queries. Indeed, crowdsourcing is particularly suitable to use in our user study for three reasons. First, workers can act as surrogates of end-users on the assumption that those same workers also use Web search engines. Sec-ond, the worker-base is large and diverse [17] meaning that our results are less likely to suffer from demographic bias, in contrast to using students or co-workers. Third, assessments
Prior works in the field of crowdsourcing have identified effective validation approaches for the identification of poor quality work, with the aim of increased accuracy. Snow et h ttps://sites.google.com/site/treccrowd/ face and access to the dataset described later in Section 4.1, which is available at http://terrierteam.dcs.gla.ac.uk/ vertical/ . a l. [29] and Callison-Burch [10] investigated the accuracy of crowdsourced labels generated for natural language pro-cessing tasks, concluding that  X  X xpert X  quality labels can be achieved by having three or five workers complete each crowdsourcing task and taking the majority (most commonly selected) label. Kittur et al. [19] proposed the introduction of questions with verifiable, quantitative answers, often re-ferred to as  X  X old judgements X  or a  X  X oney-pot X  to identify poorly performing workers. However, subsequent research by Ipeirotis [18] has indicated that within a Web page clas-sification context, gold judgements are unnecessary when larger numbers of assessors work on each task. In partic-ular, their results indicate that when 10 assessors work on each task and a majority result is taken, then no apprecia-ble gains in accuracy are observed when further adding a gold judgement validation, i.e. crowdsourced work can be validated through redundancy. Following this prior work, as part of our user study, we use 10 redundant workers to compare each of our rankings. The majority vote is used to in Section 3.1) also contains an in-built form of validation, to identify bots and users that randomly click on answers. In the next section, we describe our experimental method-ology, including how we designed the interface we show to our crowdsourced workers.
In an IR setting, rankings are usually evaluated using in-complete Cranfield-style relevance assessments [11] pooled from multiple systems [30]. However, such an approach is not suitable for evaluation of vertical search rankings be-cause documents within a ranking cannot be judged inde-pendently of one another. In particular, whether the news-related results added by the news vertical improve the ini-tial Web search ranking depends both upon the documents added and the extent to which the Web search ranking al-ready satisfied the user X  X  news need. Moreover, each ranking contains multiple types of documents, which from a pair-wise perspective may not be directly comparable. For in-stance, how can the value of a tweet be directly compared to a newswire article? Furthermore, the real-time nature of news-related queries mean that the value that each news-related document brings to the ranking is dependant upon both its relevance and also its recency/timeliness with re-gard to the event underlying the query.

Instead, inspired by prior works by Thomas and Hawk-ing [28] and Carterette [9], we propose a comparative eval-uation approach, where the top ten documents from each ranking are considered as a single evaluation unit (rather than evaluating the individual documents). For this type of (10 in our case) workers. evaluation, multiple different rankings are displayed to the user, who then selects the one that he/she prefers. In our scenario, for a news-related query, the user is shown both the unmodified Web search ranking and a ranking with content from either newswire, the blogosphere, Twitter or Wikipedia integrated. In particular, for each news-related query, we have users compare the Web search ranking to rankings en-hanced with content from each of the aforementioned sources in turn, such that they can identify those cases where inte-grating additional content is useful. The advantage of this approach is that it avoids the issues of pair-wise document comparability and redundancy by having end-users compare both document rankings as single units.

Formally, our approach takes as input a Web search rank-ing W and a set of news and user-generated content rank-ings e  X  E for a news-related query Q and a given point in time t . For our experiments, E is comprised of four rank-ings, namely rankings of newswire articles, blogs, tweets and Wikipedia pages. Each ranking is ranked by relevance to Q and with respect to the query time t , i.e. only documents from before t are ranked. For each query, the Web search ranking W is combined with each of the four news and user-generated content rankings in E to form four enhanced rank-ings . We denote an enhanced ranking as W + e , e.g. W + Blogs .
Our strategy for generating the enhanced rankings is as the top three documents ranked for Q and add them to the top of W within a special result box. This simulates the look and feel of current news vertical search results, like include only the top ranked Wikipedia page for the query, based on the observation that Web search engines typically only return one page from Wikipedia in their top 10 results.
Finally, we combine the Web search ranking W with its four enhanced rankings to form four document ranking pairs, e.g. &lt;W , W + Blogs &gt; . During our user study, for each query, a user is shown each of the four ranking pairs for that query in turn, selecting the one that they would prefer to see for Q and time t in each case. Figure 2 illustrates the workflow for a single assignment. As described later in Section 4.1, we evaluate using two datasets. For the second dataset, after the user selects a ranking, we also give the user a multiple-choice questionnaire about why they selected each ranking.
If the majority of users that compare a ranking pair for a query Q select the enhanced ranking W + e , we say that W + better satisfies Q (the query topic) for time t . In Section 3.1, we describe the interface we show to the users, while Sec-tion 3.2 describes of the questionnaire that we use.
To facilitate our comparative evaluation approach, we de-velop an assessment interface to perform the presentation of document ranking pairs to the user, and record the results. Figure 3 provides an illustration of the assessment interface for the query  X  X ariangela melato X  made on January 12th 2013. We enlarge the enhanced ranking for easy viewing in Figure 3. At the top of the interface, the title of the task is displayed along with a button that reveals the instruc-tions for the task (hidden in Figure 3). The instructions are divided into two separate components, namely: the main in-struction block that describes what the user is being asked to do; and the guidelines block that provides additional clar-ifications about the task. When a user first views the task (before accepting it) the instructions are displayed to them. When actively attempting the task, the instructions are au-tomatically closed by default to save screen real-estate.
Below the instructions, the news-related user query is dis-played along with a short topic description. For the example in Figure 3, the user was searching for information about the death of Italian cinema and theatre actress Mariangela Melato. Below the query Q and topic description, the doc-ument rankings W and W + e from the current pair are ren-dered ( W + Blogs in Figure 3). Users are instructed to select the ranking that best satisfies the query, while the guide-lines instruct the user to focus on whether the additional results added to one of the rankings make it better or worse. To enable users to state their preference for one of the dis-played rankings, each ranking is clickable. Clicking a ranking records the user X  X  preference for that ranking. Recall that the rankings are produced with respect to the query time t , i.e. only documents published before the time of the query are ranked. Both the timestamp of the query and the pub-lication time of each post (if available) are rendered as part of the interface. Users are instructed to consider recency as well as relevance when assessing using these timestamps.
The four ranking pairs for each query topic are assessed by a single user in sequence to increase efficiency [23]. After a user clicks on a ranking, if there are any ranking pairs re-maining for the current query topic (of the four total), then the next ranking pair is rendered for assessment, else a link is displayed to enable the user to submit their assessments. If any of the news and/or user-generated content sources did not return documents for the query, e.g. because no rel-evant documents had yet been published, then both of the rankings in the associated pair would be identical. In these cases, the interface automatically skips to the next pair to be assessed.

As described in Section 2.2, one of the criticisms of crowd-sourcing is a susceptibility to poor quality work. In addition to having 10 unique users assess each ranking pair, our as-sessment interface also integrates one method for validating user work. In particular, the interface renders the Web re-sult ranking twice, creating three rankings. One of these rankings has a red border (as shown in Figure 3). The users are instructed never to select the red bordered ranking. We use this ranking to identify bots or malicious users that are randomly selecting rankings. Any set of assessments where a red-bordered ranking has been clicked is automatically re-jected without payment. The three rankings are randomly ordered to avoid click bias for any one position.
When assessing the topics in our second dataset (see Sec-tion 4.1), we show a questionnaire to the user after they se-lect each ranking. Depending upon which ranking the user selected, a different questionnaire is displayed. In particu-lar, for the enhanced ranking, the user is asked to select one or more statements that apply to the content added (either newswire articles, blogs, tweets or Wikipedia pages). The six statements are listed below:
In contrast, if the user selected the Web search ranking, they are asked to select one of the following reasons why the additional content did not better satisfy the query:
In this way, we first have users decide whether a ranking is improved by the integration of news or user-generated con-tent and then have them elaborate as to why this is the case. Table 1: Statistics for the two datasets used in this p aper and the number of query topics belonging to each temporal class within each. We use the interface and questionnaire described above in c onjunction with Amazon X  X  Mechanical Turk to perform our user study. In the next section, we describe our experimental setup for the user study.
Due to a lack of publicly available datasets upon which we could evaluate ([12] and [20] described in Section 2.1 used private Yahoo! and Microsoft data for instance) we develop two new datasets for evaluation. In Section 4.1, we describe the creation of two new datasets that we use for evaluation. Section 4.2 summaries the structure of our user study. In Section 4.3, we detail the statistics of our crowdsourcing experiments, while Section 4.4 provides information about our MTurk workers.
We develop two new datasets to facilitate our evaluation using publicly available resources. Our first dataset, referred to as 2012 Apr , spans the period of the 11th to the 23rd of April 2012. The second dataset, referred to as 2013 Jan spans the period of the 10th to the 16th of January 2013. Each dataset contains a set of &lt; query,timestamp &gt; pairs and rankings of Web pages, newswire articles, blogs, tweets and Wikipedia pages for each.
 To generate our news-related queries, we use the Google 2012 Apr dataset, we sampled the queries that were reported trending by Google Trends on an hourly basis for that pe-riod. At the end of each day, we manually identified those queries that matched a reported news event, while all other queries were discarded. This left a set of 199 queries known to be news-related over the 12 day period. For the 2013 Jan dataset, we instead crawled the  X  X ursting Phrases X  service of Bitly X  X  Data API at 30 minute intervals, since Bitly updates its bursting phrases more frequently than Google Trends. The bursting phrases service returns phrases that were re-ceiving a consistently high volume of click traffic at the time. From this set of bursty phrases returned, we manually identi-fied 98 that referred to news events, using these as our news-related queries for the second dataset. For both datasets, the timestamp of the query is the time when it was crawled using either API. Hence, this timestamp corresponds to a period when the query/phrase was either receiving high traffic as recorded by Google or Bitly. We also manually created topic descriptions for each query with reference to the main news stories around the time of each query.

To collect rankings for each topic, we again used publicly available API X  X . For the time of each query as recorded by h ttp://www.google.co.uk/trends/hottrends http://dev.bitly.com/ Table 2: Types of data collected from the two crowd-sourcing tasks.
 that query X  X  timestamp, we downloaded Bing Web search rankings are real-time in nature, i.e. they contain only doc-uments published before the query timestamp t . The doc-uments are ranked by relevance with respect to the query Q . Recall from Section 3 that the top three newswire arti-cles, blog posts and tweets are selected for integration into the Web search ranking, while for Wikipedia, only the top ranked Wikipedia page is integrated.

To aid our later analysis, we also divide our queries into four classes, representing the time elapsed between the event underpinning each query and that query X  X  timestamp:
Table 1 summarises the statistics of our two datasets and the number of query topics belonging to each query class within them.
Following best practices in crowdsourcing [1], we adopt an iterative design methodology. In particular, we sepa-rate our crowdsourced user study into two separate crowd-sourcing experiments, one for each of the two datasets de-scribed in the previous section. Based upon worker feed-back from the first experiment (2012 Apr ), we improved the assessment interface, user behaviour logging and the pro-vided instructions before crowdsourcing the second exper-iment (2013 Jan ). Most notably, the questionnaires were added to enable users to better articulate why one rank-ing might be better than another, while IP-address logging was enabled to estimate the location of workers. Table 2 summaries the data that was collected from the two crowd-sourcing experiments.
We use the crowdsourcing marketplace Amazon X  X  Mechan-ical Turk (MTurk) to recruit workers for our evaluation. Fol-lowing [18], as described in Section 2.2, we have ten individ-ual workers assess each document ranking pair. Each MTurk h ttp://www.bing.com/developers/ http://blog.blekko.com/2012/10/15/ powering-web-apps-with-the-blekko-api/ https://dev.Twitter.com/ http://www.mediawiki.org/wiki/API:Main_page assignment involves the assessment of 4 document pairs (the Web search ranking in comparison to 4 different enhanced rankings). The second experiment also added the question-naires after each assessment was made. We paid US $0.05 for each set of 4 document pairs assessed, since assessment requires that the user browses the two rankings and makes a single click. For the second experiment, an additional $0.05 was paid for completion of the questionnaires. We use the in-built worker validation described in Section 3.1 to detect bots and spammers. Any assignment where a red-bordered ranking was selected was rejected without payment and re-submitted for another user to complete. The total cost of running the two experiments was US $217.25, in-cluding MTurk X  X  10% fee.

In terms of the two experiments, the 199 topics from the first dataset translate into 1990 assignments (199x10), while the 98 topics from the second dataset translate into 980 as-signments (98x10). Both experiments were completed in-dividually in under 48 hours. Automatic validation (based on clicking of red-bordered rankings) resulted in the rejec-tion of 9.5% and 3.05% of assignments for each of the two datasets respectively. Inter-worker agreement was 0.33 us-ing Cohen  X  for the first experiment and 0.23 using Cohen  X  for the second experiment. Note that a low agreement is ex-pected in this case due to both the large number of workers attempting each task and a greater scope for disagreement between workers due to individual preferences.
In total, 98 unique workers completed one or more as-signments over our two experiments, 57 workers during the first experiment and 41 during the second experiment. Fig-and Web search rankings ( W ) selected by workers. As is typical for crowdsourced tasks [2, 24], we see that the com-pleted work follows a long-tail distribution, with a few work-ers completing many assignments and a long tail of workers that completed only a few assignments. Furthermore, we Table 3: Work statistics broken down by country for the second dataset.
 also observe that the majority of workers show no clear bias t oward selecting either type of ranking. Note that because 10 individual workers assess each topic, the impact that a single worker can have on the final result is limited.
The time taken by the workers to complete each assign-ment varies. Figures 5 (a) and (b) show the amount of time that assignments (4 assessments) took, separated into 20/30 second bins (for readabilty). From Figure 5, we observe that for the first experiment (2012 Apr ), to assess four rank-ing pairs took the majority of the workers (57%) under 20 seconds, with a further 23% of assignments taking between 21 and 40 seconds. In contrast, for the second experiment (2013 Jan ), assessment took longer on average, with 33% of workers taking between 31 and 60 seconds and a further 20% taking between 61 and 90 seconds. This is because the second experiment also required workers to fill in the ques-tionnaire after each of the four ranking pairs per assignment.
Finally, for the second experiment (2013 Jan ), we also logged the IP-addresses of the workers. From this information, we identified the county of origin that each worker comes from. Table 3 reports the number of page requests, unique IP-addresses and their proportions for each country made dur-ing the second experiment. From Table 3, we observe that 3 9% of IP-addresses and hence workers came from India, fol-lowed by 38% from the U.S. However, while the number of unique workers is similar between these two countries, the volume of work done is much higher for Indian workers. This indicates that Indian workers completed more assignments on average than US workers.
In our subsequent experiments, we report the majority preference over the ten workers that compared each ranking pair. Recall from Section 3.1 that not all sources return content for all of the topics. In these cases, both rankings in a pair would be identical, hence we remove these cases from our dataset. For this reason, the click counts can be lower than the total number of assignments.
In this section, we investigate to what extent integrating newswire articles, blogs, tweets and Wikipedia pages into the search results for news-related queries can better satisfy the user. In particular, we examine the following three research questions, each in a separate subsection:
We begin by examining our first research question, i.e. is news article integration still sufficient to satisfy news-related queries in real-time. We report on the number of users that clicked (stated a preference for) rankings enhanced with newswire articles ( W + News ) in comparison to the Web search ranking unaltered ( W ). If the users prefer the newswire-enhanced rankings, then this would indicate that newswire articles are still sufficient. On the other hand, if there are many topics where users do not prefer the newswire-enhanced ranking, then this would indicate that we need to look to other sources of content to satisfy the user for these queries.
Table 4 reports the raw number of users that clicked on the newswire-enhanced or Web search ranking, i.e. when &lt;W , W + News &gt; pairs were compared. Recall that we have ten users (workers) assess each ranking pair to counteract any noise introduced by poorly performing workers. Table 4 also reports the majority preference over the ten workers, i.e. the number of real-time news topics where the majority preferred either ranking. From Table 4, we observe the fol-lowing. First, comparing the raw click counts for each of the newswire-enhanced and Web search rankings (rows 3 and 5), we observe that for 28.9% of topics the users preferred the Web search ranking, while 48.5% preferred the newswire en-hanced ranking (aggregating both datasets). Second, when taking the majority vote (rows 4 and 6), we similarly see that the Web search ranking was preferred by the majority of users for 17.8% of topics, while for 44.4% of the topics, the news-enhanced rankings were preferred by the major-ity of users. These results show that only around half of Table 4: Number of topics for which the majority of end-users preferred either the Web search rank-ing or the rankings enhanced with user-generated content.
 our real-time news-related topics could be better satisfied b y integrating newswire articles.

To investigate why newswire article integration was not able to satisfy many of our real-time news-related topics, we examine the reasons selected by our users in the ques-tionnaire from our second experiment (see Section 3.2). Of the 418 Web search rankings clicked by our users in the sec-ond experiment, Figure 6 reports the percentage of those clicks that were attributed to the added documents in the newswire-enhanced ranking being either partially relevant, out-of-date, redundant, unrelated or spam. From Figure 6, we see that the main reason that users gave was that the results added were partially relevant, but did not make the enhanced ranking better. This result is interesting, since it indicates that the newswire sources were providing related content to the query, but that this content was not useful to the majority of our users. One example where the majority of users selected the Web search ranking was the query  X  X ame industry X , with the topic description  X  X ews/Politics: Biden meeting with representatives from the video game industry X . For this query, the top two added newswire articles discussed violence relating to the video game industry, but did not specifically mention Joe Biden or the topic. The third inte-grated article was directly relevant to the topic. However, the original Web search ranking already contained two rel-evant Associated Press articles about the topic. Hence, the Joe Biden article was redundant, but the two other added articles were deemed by our users to be useful  X  even though they do not mention the topic directly.

Next, we examine the 442 instances from the second ex-periment where integrating newswire articles did improve the Web search ranking. Figure 7 illustrates the percentage of newswire-enhanced search rankings selected because they were more relevant, recent, reported breaking news, better satisfied the query, were particularly informative or provided ure 7, we observe the following. First, as expected, we see that over 77.2% of the enhanced rankings were selected be-cause they contained more relevant documents than the Web search ranking. We also see that over 63.3% of the enhanced rankings were selected because the added documents related to the recent event described in the topic description. Note the distinction between documents being relevant and doc-uments relating to the event referred to by the query, i.e. some users may find that adding background articles for longer-running events is enough to improve over the Web search ranking. On the other hand, a much lower percentage of users (28.2%) indicated that they selected the enhanced ranking because the added documents were about breaking news. Also of note is that only 41.2% of users indicated that the introduction of newswire articles better satisfy the query single ranking, the percentages do not sum to one. Figure 6: The percentage of Web search rankings c licked that were attributed to the added documents in the newswire-enhanced ranking being either par-tially relevant, out-of-date, redundant, unrelated or spam. by being included, even though they stated a preference for the enhanced ranking. This indicates that either our users disagreed upon the meaning of  X  X atisfy X  in the questionnaire, or that users can prefer the enhanced ranking for a topic, even if it does not better satisfy that topic. Indeed, almost all users selected the  X  X elevant X  but not the  X  X etter satisfied X  choice for one or more topics.

To answer our first research question, newswire articles were not sufficient to satisfy the majority of topics in our two datasets, primarily because while some relevant content is added for most queries, users did not find that it improved the ranking. For the assignments where users preferred the newswire-enhanced ranking, users indicated that they pre-ferred them because the added content was about the event underlying the query or was simply relevant to the query topic. In the next section, we investigate whether the inte-gration of user-generated content can better satisfy users.
Next, we examine our second research question, i.e. to what extent can content from user-generated sources also satisfy the end-user? To this end, we begin by reporting on the number of users that clicked (stated a preference for) rankings enhanced with either blog posts, tweets or Wikipedia pages in comparison to the Web search rankings unaltered. If the number of topics where the majority of users selected the user-generated content-enhanced rankings is greater than for the number of topics that were similarly enhanced by integrating newswire articles (132 topics  X  see Table 4 row 6) then we can conclude that user-generated content sources may be more effective for news vertical in-tegration than the traditional newswire sources used today.
Table 5 row 3 reports the number of topics where the majority of users preferred rankings enhanced with either blogs, tweets or Wikipedia pages to the Web search ranking unaltered. We report both the number of topics for each of the two datasets (2012 Apr and 2013 Jan ), as well as the summation over the two. From Table 5 row 3, we observe that in total, the blog-enhanced rankings were preferred for 56.9% of the topics over the unaltered Web search rankings, while tweet-enhanced rankings were preferred for 71.2% of topics and Wikipedia pages for 52.1% of topics. Moreover, these proportions are consistent across both datasets. This result shows that all three of the user-generated content sources tested can enhance the Web search ranking for a sub-stantial proportion of the 297 (see Table 1) real-time news-related topics tested. Most notably, the rankings including tweets were particularly favoured by our users, indicating Figure 7: The percentage of newswire-enhanced s earch rankings selected because they were more rel-evant, recent, report breaking news, better satisfy the query, were particularly informative or provide general (background) information. that Twitter is a good source of real-time news content to integrate. Indeed, integrating tweets satisfied more query topics (168  X  see Table 5 row 3, column 7) than integrating newswire articles (132 topics  X  see Table 4 row 6), indicating that Twitter is a better overall news source than traditional news providers.

However, if at the time t of each topic there exists relevant newswire articles, then user-generated content may not be required to satisfy the user, i.e. the extra content may be redundant. Indeed, in the previous section, we showed that approximately half of our topics could be better satisfied by integrating newswire articles than returning the Web search ranking unaltered. To examine this, we contrast the query topics that benefited from the integration of news articles to those that benefited by the integration of user-generated content. For a given query, if relevant user-generated content was found, integrated and the ranking was preferred by the majority of users, then user-generated content can be said to better satisfy that query than when integrating nothing. However, that same query may be equally or better satisfied by the integration of newswire articles. On the other hand, if there are news-related queries that user-generated content can satisfy, but newswire articles cannot (at the time those queries were made), then we can show that user-generated content is necessary to satisfy those queries.

Table 5 rows 4-6 report the number of topics for which integrating user-generated content aided but that integrat-ing newswire did not (# Topics Better than Newswire). For comparison, we also report the reverse case (where integrat-ing newswire articles aided but integrating user-generated content did not) and where integrating either newswire ar-ticles or user-generated content was equivalent (either both better satisfied the user or neither did). The total number of topics where integrating user-generated content aided but integrating newswire did not is highlighted in bold. From Table 5, we see the following two points of interest. First, each of our three user-generated content sources were able to satisfy topics which were not already satisfied by newswire articles. For instance, 22.2% of all topics could be better satisfied by integrating blogs but not newswire articles (row 4, column 4). Second, of the three user-generated content sources tested, we again see that Twitter is the most ef-fective, improving 84 topics (34%), which were not better satisfied by integrating newswire articles. The query  X  X ionel richie X  (issued on the 13th of April 2012) was one exam-ple where integrating tweets better satisfied our users but integrating newswire articles did not. For this query, the Twitter stream returned tweets reporting that music star Lionel Richie was facing a large tax bill over allegations he o wed more than $1 million to the U.S. government. On the other hand, our newswire stream returns no relevant articles for that query and time.

Having shown that integrating user-generated content can be more effective than integrating newswire articles, we now investigate whether the three user-generated content sources predominantly aid for the same or different topics. Indeed, even if Twitter is the most effective of the three user-generated sources overall, it may be that blogs and Wikipedia can aid for other topics. Table 5 rows 7-15 report the number of topics for which the rankings enhanced with content from each user-generated source were preferred over the rank-ings enhanced with another user-generated source. From Table 5, we observe the following. First, the three user-generated content sources appear to aid for different topics. For example, blog-enhanced rankings were preferred for 46 topics where rankings integrating tweets were not. How-ever, there were similarly 99 topics for which the Twitter-enhanced rankings were preferred, where the blog-enhanced rankings were not. Second, the proportion of topics where integrating two different user-generated content sources were equivalent (both aided or neither did) is quite small, e.g. 22.5% of topics when considering tweets and blog posts. This indicates that user-generated content sources are useful for tackling different types of queries and that by effectively using multiple types of content, a larger proportion of news-related queries might better be satisfied. For instance, the topic  X  X ames holmes X  about a plea extension in the Aurora movie theatre shooting court case was only better satisfied by integrating tweets, while the topic  X  X aby lion X  about a dog being mistaken for baby lion in Virginia was better covered by local newswire outlets.

To answer our second research question, integrating user-generated content into the Web search results can better satisfy many of our news-related topics. Moreover, many of these topics could not be similarly satisfied by integrating newswire articles published at the time. The most effective user-generated content source tested was Twitter, highlight-ing its usefulness for real-time news vertical search.
In this section, we investigate our final research question, i.e. how does the age of a event affect the types of content that users prefer. To this end, we examine the query top-ics for which the integration of user-generated content aided when we break down our topics by the time between the orig-inating event and the topic time. The aim is to determine whether different sources are better to integrate at different points during an event X  X  life cycle.

Figures 8 (a) and (b) report the proportion of query topics where users preferred the enhanced ranking in comparison to the Web search ranking divided into three time classes (see Section 4.1) for each dataset, respectively. From Fig-ure 8 (a) and (b) we observe the following. First, the pro-portion of topics improved by integrating newswire arti-cles shows a downward trend as the time elapsed from the event increases. This indicates that newswire articles are less useful to return for long-running events, possibly be-cause newswire providers tend to publish a burst of articles around the time of the event, with little content published later. Secondly, in contrast, we see that integrating blogs satisfied a larger proportion of long-running queries than breaking or recent queries. This supports prior observations that for some types of events bloggers tend to lag behind newswire when publishing [22]. Next, we see that integrat-ing Tweets was most effective for satisfying queries relating to breaking events. Indeed, on the first dataset, 52% of the breaking news queries could be better satisfied by inte-grating tweets into the Web search ranking. For example, one such query was  X  X evon helm X , referring to American rock multi-instrumentalist and actor who was fighting cancer at the time. When this query was issued on the 19th of April 2012, Twitter returned tweets reporting his death, while our newswire stream only returned articles reporting that he was in the final stages of cancer. This illustrates the value that Twitter can bring from a content integration perspective for breaking news queries where no newswire articles have yet been published.

To answer our third research question, we find that users prefer rankings that integrate tweets or newswire articles soon after an event breaks. As an event matures, tweets and newswire articles become less effective, while integrating blogs becomes more effective.
In this paper, we examined to what extent integrating news and user-generated content for real-time news-related queries could better satisfy end-users. We performed a novel user study using the emerging medium of crowdsourcing, where we had ten different users compare Web search rank-ings for 297 news-related queries against rankings that in-tegrate newswire articles, blog posts, tweets and Wikipedia pages. From this user study, we showed that newswire ar-ticles were not sufficient to satisfy the majority of news-related queries tested, highlighting the need to examine new class (the amount of time between the originating event and topic time). sources of content. We also showed that integrating blog posts, tweets and Wikipedia pages instead can satisfy many of the queries where integrating newswire articles did not. Finally, our results indicate that users tend to prefer rank-ings that integrate tweets or newswire articles soon after an event breaks, with blogs becoming more useful over time.
From the results of this study, we believe that new ap-proaches that effectively integrate both newswire articles and user-generated content based upon a combination of re-latedness, novelty and timeliness will be able to better serve news-related queries in real-time than using newswire arti-cles alone. For future work, we aim to investigate machine learned approaches to automatically select user-generated content to integrate for news-related queries, as well as fur-ther examining what documents are useful to return for such real-time news-related information needs.
