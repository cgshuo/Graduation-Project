 1. Introduction
Automatic face recognition has received an increasing interest for security applications in the last two decades. Face recognition by machine appears to be difficult, while it is done effortlessly by human beings. The main reason for this difficulty is that it is difficult to articulate the mechanism humans use. Face recognition can be categorized into face identification and verification. The objective of a face identification system is to determine the identity of a test subject from the set of reference subjects. The performance of the face identification system is quantified in terms of identification rate or recognition rate (RR). On the other hand, a face verification system should accept or reject the identity claim of a subject, and the performance is measured in terms of equal error rate (EER). Person verification systems make use of one or more biometric modalities such as speech, face, fingerprint, signature, iris and hand geometry to accept or reject the identity claim of an individual. In this paper, face and mouth modalities are used for person identification and verification. The terms facial and mouth features refer to the features extracted from the face and mouth image of the person, respectively. 1.1. Related work
Several techniques have been proposed in the literature for still-image-based face recognition such as principal component or eigenface analysis (PCA) ( Turk and Pentland, 1991 ), linear discri-minant analysis (LDA) ( Swets and Weng, 1996; Belhumeur et al., 1997 ), independent component analysis (ICA) ( Bartlett et al., 1998 ), elastic graph matching ( Lades et al., 1993; Wiskott et al., 1997 ), line edge map ( Gao and Leung, 2002 ), support vector machine (SVM) ( Heisele et al., 2002; Kotropoulos and Pitas, 2001 ) and correlation filter ( Savvides et al., 2002 ). Most of the video-based face recognition methods apply still-image-based recognition to selected frames ( Zhao et al., 2000 ). The radial basis function neural network (RBFNN) ( Howell and Buxton, 1996 ), probabilistic modeling ( Zhou et al., 2003 ) and hidden Markov model (HMM) ( Liu and Chen, 2003 ) are also used for video-based face recognition. A comprehensive survey of automated face recognition techniques can be found in Zhao et al. (2003) . The problem of face recognition over changes in illumination is widely recognized to be difficult for humans ( OToole and Phillips, 2007 ). A fully automatic face recognition algorithm and its performance on the FRGC v2.0 data were presented in Mian et al. (2007) . The computational tools and a hardware prototype for 3D face recognition were presented in Kakadiaris et al. (2007) . The mouth features such as discrete cosine transform (DCT) of the lip region 2003 ) are commonly used to represent the mouth image. The video-based face recognition system called PersonSpotter described in Steffens et al. (1998) used elastic graph matching technique. A recognition rate of about 90.0% was reported. A method proposed in Zhou et al. (2003) used probabilistic modeling of intensity values of the images, and a recognition performance of about 98.0% was reported using the MoBo database ( Gross and Shi, 2001 ) having 25 subjects. The method described in Liu and Chen (2003) used PCA and HMM, and it reported 98.8% recognition rate for the MoBo database. Facial image representation based on local binary pattern (LBP) texture features is given in Timo Ahonen and Inen (2006) . The logarithmic total variation (LTV) model for face recognition under varying illumination, including natural lighting conditions was described in Chen et al. (2006) . The average face recognition rate of 99.65% was reported for Carnegie Mellon University (CMU) Pose,
Illumination, and Expression (PIE) (CMU-PIE) database ( Sim et al., 2002 ). In Xiang et al. (2006) , recursive Fisher linear discriminant (RFLD) is used for face recognition and a recognition rate of 98.72% was reported for Yale database. 1.2. Contributions and outline of work
Most of the existing person recognition methods use raw pixel values of the face image for identification. Features usually encode knowledge about domain and it is difficult to learn from raw pixel values. In this work, local minima (maxima) features are extracted from the facial (mouth) regions (fovea regions) and the extracted features are compared to form the fovea intensity comparison code (FICC). The facial features such as hair, face outline, eyebrows, eyes and mouth play an important role in perceiving and remembering faces. A cartoonist extracts the required information from these features and represent in terms of lines and arcs. These lines and arcs correspond to the gradient or local extrema (minima and maxima) in an image. The local maxima and minima are the largest and smallest intensity values of an image within some local neighborhood, respectively. The flowchart of person identification and verification system is shown in Fig. 1 .

The automatic person identification and verification system are described in this paper consists four modules: face detection, facial and mouth feature extraction, fovea intensity comparison code and matching. Face detection is described in Section 2. Facial and mouth feature extraction are described in Sections 3 and 4, respectively. Section 5 describes fovea intensity comparison code and matching. Experimental results are given in Section 6.
Section 7 concludes the paper. 2. Face detection
Detecting faces automatically from the intensity or color image is an essential task for many applications like person verification and video indexing. Number of techniques have been proposed for face tracking and localization ( Yang et al., 2002; Fleuret and
Geman, 2001; Rowley et al., 1998 ), we used the method proposed in Viola and Jones (2001) for determining the face region in the video. The difference between face and non-face is more obvious to human vision perception than face identification and verifica-tion. In a machine vision system, when scaling a face image into a small size, the face image resembles a horizontal dark bar across the two eyes and a vertical light column across the nose. By sensing the bar and the column, plausible faces can be detected in images, and non-face images are rejected. The final classifier is to distinguish face and non-face. In the first step, simple rectangle features as shown in Fig. 2 are used. These features are reminiscent of Haar basis functions. The rectangle features are computed quickly using an intermediate representation by the sum of the pixels above and to the left of x , y inclusive: II  X  x , y  X  X 
The set of rectangle features used to provide rich image representation which supports effective learning. In the second step, Adaboost learning algorithm is used to select a small set of features and to train the classifier.

About 180,000 rectangle features are associated with each image sub-window. Out of this large number of features, a very small number of features are combined to form an effective classifier. In the third step, a cascade of classifiers as shown in
Fig. 3 is constructed which increases the detection performance thereby reducing the computation time.

A positive result from the first classifier triggers the evaluation of the second classifier which triggers the third classifier and so on. A negative result from any stage is rejected. The stages in the cascade are constructed using Adaboost algorithm and the threshold is adjusted to minimize false negatives. Stages are added until the target for false positive and detection rate is achieved. A rectangle is fitted over the detected face region. The detected face region is shown in Fig. 4 . 3. Facial feature extraction
One of the main issue in constructing an automatic person recognition system is to extract the facial or mouth features that are invariant to the size of the face. The face tracking/localization method gives only the upright rectangular bounding box for the face region, and hence the size of the face cannot be determined from the bounding box. Size of the face can be determined if the locations of two or more facial features are identified. Among the facial features, eyes and mouth are the most prominent features used for determining the size and pose of the face ( Nikolaidis and Pitas, 2000; Hsu et al., 2002 ). In this section, a method is proposed for extracting the facial features from the face. The facial features are extracted relative to the locations of the eyes. 3.1. Determination of eye location
Several techniques have been proposed in the literature for determining the locations of the eyes. Template-based approach is commonly used for locating the eyes ( Lam and Yan, 1996; Smeralsi et al., 2000 ), and the methods given in Nikolaidis and Pitas (2000) and Hsu et al. (2002) use the gray-scale morpholo-gical operations dilation and erosion ( Jackway and Deriche, 1996 ). In Nikolaidis and Pitas (2000) the morphological operations are applied on the image to enhance the dark regions, and in Hsu et al. (2002) morphological operations are used to emphasize brighter and darker pixels in the luminance ( Y ) component around the eye regions. In addition to luminance component the blue and red chrominance ( C b and C r ) information is also used in Hsu et al. (2002) . The YC b C r color space is obtained from RGB color space using Y  X  0 : 299 R  X  0 : 587 G  X  0 : 114 B C b  X  B Y
C r  X  R Y 8 &gt; &lt; &gt; : where R , G and B are the red, green and blue components of the color image, respectively. The RGB and YC b C r representation of the face region is shown in Fig. 5 .
 The Y , C b and C r values are normalized to the range [0, 255]. The eye regions have low intensity ( Y ), high blue chrominance ( C and low red chrominance ( C r ) when compared to the forehead region of the face. Using this fact, the face region is thresholded to obtain the thresholded face image U , given by U  X  i , j  X  X  where l 1 , l 2 and l 3 are the average Y , C b and C r values of the pixels in the forehead region, respectively. The forehead region is determined from w 1 and ( c x , c y ). Fig. 6 shows the construction of the thresholded face image. The white blobs in Figs. 6 (a) X (c) are, respectively, the low intensity, high blue chrominance and low red chrominance regions when compared to the forehead region of the face. The threshold face image is shown in Fig. 6 (d). Morphological closing operation is applied to the thresholded face image, and the centroids of the blobs are determined.

The relative positions of the centroids with respect to the rectangular bounding box enclosing the face region and the center (average) of the eyebrow pixel coordinates are used to determine the locations of the eyes. The eyebrow ( E ) pixel coordinates are obtained based on change in gray values in the eyebrow region.
E  X  i , j  X  X 
Fig. 7 (a) shows the centroids of the white blobs in the thresholded face image, and Fig. 7 (b) shows the locations of the eyes. 3.2. Facial feature extraction Facial feature extraction is an interesting and challenging task.
The key facial features such as hair, eyebrows, eyes, nostrils and end points of lips are associated with local minima, and shape of the lip contour and nose tip corresponds to local maxima. The local maxima and minima can be extracted using the gray scale morphological operations dilation and erosion, respectively ( Jackway and Deriche, 1996 ). The morphological dynamic link architecture (MDLA) method for face recognition described in
Kotropoulos et al. (2000) uses multiscale morphological dilation and erosion under the elastic graph matching frame work. In our method, an elliptical rigid grid is placed over the face region, and the multiscale morphological erosion is used for feature extraction. Most of the key facial features are associated with the local minima, and hence we use only the erosion operation for facial feature extraction. The elliptical grid is used instead of a rectangular grid ( Kotropoulos et al., 2000 ) in order to extract features only from the face region. In the elliptical grid, the grid points (nodes) lie within an elliptical region. The grid points are selected in order to extract local features within some local neighborhood of each grid point. The grid points are placed non-uniformly over the face image based on experimental studies. The face outline or contour can be captured using a rectangular grid which assumes that the training and testing images have same background. The performance of the person recognition technique must be invariant to the position of the face in the image, and hence we use an elliptical grid instead of a rectangular grid. The length and the slope of the line connecting the eyes are used to determine the size and orientation of the grid, respectively. The elliptical grid consists of 73 nodes, and the positions of these nodes are determined relative to the locations of the eyes. The multiscale morphological erosion operation is applied at each grid node for extracting the facial features as described below.
The multiscale morphological erosion operation is based on the gray scale morphology, erosion. Let Z denote the set of integer numbers. Given an image I : D D Z 2 ! Z and a structuring function G s : G s D Z 2 ! Z at scale s , the erosion of the image I by the structuring function G s is denoted as  X  I G s  X  , and it is defined by  X  I G s  X  X  i , j  X  X  min x , y f I  X  i  X  x , j  X  y  X  G s  X  x , y  X g X  5  X  where m a r x , y r m b , with 1 r i r w ,1 r j r h . The size of the structuring function is decided by the parameters m a and m is given by ( m a + m b +1) ( m a + m b +1). The structuring functions such as flat, hemisphere, paraboloid are commonly used in morphological operations ( Jackway and Deriche, 1996 ). The flat structuring function G s  X  x , y  X  X  0 is used in this paper. For a flat structuring function the expression for erosion reduces to  X  I G s  X  X  i , j  X  X  min x , y f I  X  i  X  x , j  X  y  X g X  6  X  where m a r x , y r m b . The erosion operation (6) is applied at each grid node for s  X  1 , 2 , ... , p to obtain p facial feature vectors from the face image. The distance between the eyes ( d e )is used to determine the parameters m a , m b and p . The value m been used in our experiments. These parameters are chosen in such a way that m a + m b +1 for s  X  p is less than or equal to the minimal distance between two nodes of the grid which depends on the number of nodes in the grid. Fig. 8 (a) shows the eroded images for s  X  1 , 2 and 3. Fig. 8 (b) shows the facial regions used for extracting the feature vectors for s  X  1 , 2 and 3. 4. Mouth feature extraction
One of the main issue in constructing an automatic person verification system is to extract the mouth features that are invariant to the size of the face and mouth. In this section, a method is proposed for extracting mouth features from the mouth image. The mouth features are extracted relative to the locations of the eyes and mouth. 4.1. Determination of mouth center
The mouth or lip image analysis has received considerable attention in the area of speech recognition and person recogni-tion. Mouth image segmentation is a necessary step for real time mouth feature extraction. Recent methods ( Hsu et al., 2002; Chen, 2001; Leung et al., 2004; Wang et al., 2004 ) use color information to distinguish the lip and nonlip regions in the face. The term lip image or lip region refers to the lips, teeth, mustache and the interior of the mouth. For face images with weak color contrast, accurate and automatic extraction of inner and outer lip boundary remains a challenging task. Different types of facial hair in the mouth region complicate the lip contour extraction or the lip contour itself may not be visible. The mouth region is determined from the locations of the eyes and the center of the mouth. The center of mouth ( m cx , m cy ) is calculated using m cx  X  e cx  X  1 : 1 d cos  X  y  X  1 : 57  X  m cy  X  e cy  X  1 : 1 d sin  X  y  X  1 : 57  X  e  X   X  e lx  X  e rx  X  2 e  X   X  e ly  X  e ry  X  2 d  X  y  X  tan 1 e ry e ly e where m cx and m cy are x and y -coordinates of the mouth center, respectively. e cx and e cy are the average x and y -coordinates of the locations of the eyes, respectively. e lx and e ly are x and y -coordinates of the location of left eye. e rx and e ry y -coordinates of the location of right eye. d is the distance between the eyes and y is the slope of the line connecting the locations of the eyes with respect to x -axis. The orientation of face model with the coordinates of locations of eyes and mouth is and right ( y  X  45 3 ) orientations of the face model. The center of the mouth is shown in Fig. 10 . 4.2. Mouth feature extraction
The static nature of the mouth or appearance of the mouth image over a period of time characterizes an individual to some extent. The shape of the lip contour and shape of the mustache are the dominant mouth features in the mouth region. These features are associated with local maxima because the lip, mustache and the interior of the mouth have low luminance ( Y ) than the nonlip region. The local maxima can be extracted using the morphological dilation ( Jackway and Deriche, 1996 ). For real time mouth feature extraction from the mouth image, a rectangular grid consisting of 25 nodes is placed over the mouth region. The positions of these nodes are determined relative to the locations of the eyes and mouth. The features are extracted at each grid node using the multiscale morphological dilation operation as described below. Given an image I :D D Z 2 ! Z and a structuring function
G : G s D Z 2 ! Z at scale s , the dilation of the image I by the structuring function G s is denoted as  X  I G s  X  , and it is defined by  X  I G s  X  X  i , j  X  X  max x , y f I  X  i x , j y  X  X  G s  X  x , y  X g X  8  X  ing function the dilation can be expressed as  X  I G s  X  X  i , j  X  X  max x , y f I  X  i x , j y  X g X  9  X  where m a r x , y r m b . The dilation operation (9) is applied at each grid node for s  X  1 , 2 , ... , p to obtain p mouth feature vectors from the mouth image. The distance between the eyes ( d e used to determine the parameters m a , m b and p . The value m been used in our experiments. Fig. 11 (a) shows the dilated images for s  X  1 , 2 and 3. Fig. 11 (b) shows the visual regions used for extracting the feature vectors for s  X  1 , 2 and 3. 5. Fovea intensity comparison code and matching 5.1. Fovea intensity comparison code
The fovea intensity comparison code (FICC) is obtained from the m -dimensional feature vector ( x ) extracted from the face (mouth) image using y  X  m  X  i 1  X  X  j 0 : 5 i  X  i  X  1  X  X  1 r i r m , 1 r j r m , and i o j  X  10  X  y is the d -dimensional fovea intensity comparison code, where d  X  ( m 2 m )/2. In (10), intensity of each fovea region is compared with the intensity of other fovea regions to form the fovea intensity comparison code.

The 2628 dimensional face fovea intensity comparison code is obtained from the 73 dimensional feature vector extracted from the face image using (10). Similarly, 300 dimensional mouth fovea intensity comparison code is obtained from the 25 dimensional feature vector extracted from the mouth image. 5.2. Matching of fovea intensity comparison codes
The similarity between fovea intensity comparison codes is calculated using exclusive-OR operation. The d -dimensional FICC for a test face (mouth) image is compared with the FICC of training face (mouth) images to obtain the error for each subject e ( k ), using e  X  k  X  X  1 d where n is the number of subjects, s ( k ) is the error for k th subject, y test and y train are the feature vectors extracted from test image and train image, respectively, and is an exclusive-OR operation.
The 2628 dimensional FICC for a test face image is compared with the FICC of training face images to obtain the error for each subject using (11). Similarly, 300 dimensional FICC for a test mouth image is compared with the FICC of training mouth images to obtain the error for each subject. The identity of the test image is decided based on the lowest error. 5.3. Projected fovea intensity comparison codes and matching
Principal component analysis (PCA) ( Turk and Pentland, 1991 ) is a multivariate statistical technique for reducing the dimension of a vector ( d ) to a lower dimension ( p ) using orthogonal factor space. Then, by minimum distance matching, the projected test vector ( t ) can be assigned to the class ( c ) corresponding to the projected training vector g i , where c  X  arg min where J J represents the Euclidean distance in R p .

The 2628 dimensional FICC of face images are projected and reduced into 30 dimensional vectors using PCA. The 30 dimen-sional projected FICC of a test face image is compared with the projected FICC of training face images to obtain the identity of the test face image using (12). Similarly, 300 dimensional FICC of mouth images are projected and reduced into 20 dimensional vectors. The 20 dimensional projected FICC of a test mouth image is compared with the projected FICC of training mouth images to obtain the identity of the test mouth image. Projected face feature vectors for male and female subjects are shown in Figs. 12 and 13, respectively. Projected mouth feature vectors for male and female subjects are shown in Figs. 14 and 15, respectively. 6. Experimental results
The performance of person recognition using fovea intensity comparison code is evaluated using XM2VTS database and in the laboratory environment for 50 subjects using a camera with a resolution of 160 120. Two hundred and forty-eight subjects of the XM2VTS audio-visual database ( Messer et al., 1999 ) are used for our experiments. The XM2VTS database consists of video data recorded from 295 subjects in four sessions, spaced monthly. The first recording per session of the phonetically balanced sentence ( X  X oe took father X  X  green shoe bench out X ) is used. The original frame resolution of 720 576 is downsampled to 320 240, and is used. For person recognition, we used the method described in
Viola and Jones (2001) for determining the face region as described in Section 2. The locations of the eyes and mouth are determined as described in Sections 3 and 4, respectively.
The method can detect the locations of the eyes in the presence of eye glasses as long as the eye regions are visible. The experimental results show that the method correctly detects locations of eyes about 95% of the frames in the video. The facial and mouth features are extracted only if all the 73 nodes lie with in the face/head region. Fig. 16 shows the face region, locations of the eyes and mouth in real time for a few subjects.

For enrolling a subject, facial and mouth features are extracted in real time as described in Sections 3 and 4 for 50 face (mouth) images with variations in size, orientation, appearance, expres-sions and pose of the face. The morphological erosion (dilation) is applied on the face (mouth) image for three different scales ( p  X  3). The distance between the eyes varied from 24 to 33 pixels and hence the value of p  X  3 is used in our experiments.
In training phase, 73 dimensional feature vector is extracted from face image and is converted into 2628 dimensional FICC as described in Section 5 for three different scales. For training, 7500 face feature vectors are extracted from 50 subjects (150 feature vectors per subject). Similarly, 25 dimensional feature vector is extracted from mouth image and is converted into 300 dimen-sional FICC as described in Section 5 for three different scales. For training, 7500 feature vectors are extracted from 50 subjects.
For identification, 73 dimensional face feature vector is extracted in real time, and is converted into 2628 dimensional FICC as described in Section 5 for three different scales. The 2628 dimensional FICC for a test face image is compared with the FICC of training face images to obtain the error for each subject using (11) for three different scales. Similarly, 25 dimensional mouth feature vector is extracted in real time, and is converted into 300 dimensional FICC as described in Section 5 for three different scales. The 300 dimensional FICC for a test mouth image is compared with the FICC of training mouth images to obtain the error for each subject using (11) for three different scales. The identity of the test image is decided based on the lowest error. The identification performance is measured in terms of recognition rate (RR).

For verification, 73 dimensional face feature vector is extracted in real time, and is converted into 2628 dimensional FICC as described in Section 5 for three different scales. The 2628 dimensional FICC for a test face image is compared with the FICC of training face images of the respective subject using (11) for three different scales. Similarly, 25 dimensional mouth feature vector is extracted in real time, and is converted into 300 dimensional FICC as described in Section 5 for three different scales. The 300 dimensional FICC for a test mouth image is compared with the FICC of training mouth images of the respective subject using (11) for three different scales. The claim is accepted if the error is less than a threshold, otherwise the claim is rejected. The identification and verification experiments are repeated for one more time for each subject. Similar experiments are carried out using projected face and mouth feature vectors for identification and verification.

In the database of 50 subjects, there are 50 authentic claims and 49 50 impostor claims. The verification performance is measured in terms of equal error rate (EER), where the false acceptance rate (FAR) and false rejection rate (FRR) are equal. The EER can be found for each subject (person-specific threshold) or considering all the subjects together (person-independent thresh-old). In our experiments, the EER is obtained by employing person-independent thresholds.

Similar experiments are carried out for the XM2VTS database ( Messer et al., 1999 ). The identification and verification perfor-mance of the system is evaluated for the single and combined modalities in real time and XM2VTS database using the proposed method and the existing methods PCA, FLD and RFLD. The performances of proposed and existing methods of person recognition are given in Table 1 . Most of the existing person recognition methods assume the availability of the cropped database such as M2VTS, XM2VTS and hence it is not possible to use it in real time applications. The proposed can be used for real time applications.

The output from face and mouth modalities are combined using (13) to obtain the multimodal or combined error ( e e  X  k  X  X  w f e f  X  k  X  X  w m e m  X  k  X  , 1 r k r n  X  13  X  where e f ( k ) and e m ( k ) are the facial and mouth error for k th subject, respectively. The weight for each of the modality is decided by the parameters w f and w m . Eq. (13) is applied for each frame in the test video and average error is used for identification and verification. In our experiments the modalities are combined using w f  X  0.7 and w m  X  0.3. The values of the parameters w w m are chosen such that the system gives optimal performance in terms of EER for the combined modality. The comparative recognition rate chart for performance of different models of person identification is shown in Fig. 17 .

The comparative EER chart for performance of different models of person identification is shown in Fig. 18 .

Figs. 19 and 20 show the real time facial and mouth feature extraction for varying size, orientation and background and feature extraction with variations in mouth appearance and expressions are shown in Fig. 21. Figs. 22 and 23 show the facial and mouth feature extraction using XM2VTS database. Fig. 24 shows the snapshot of the video based person identification and verification system using FICC. The person recognition system tracks the face, determines the locations of the eyes, extracts mouth features and calculates the output in real time at about 12 frames/s on a PC with 3.06GHz CPU. The lighting conditions in the laboratory are not controlled, and hence there is a slight increase in the EER. The performance of the recognition system must be invariant to size of the mouth, background, orientation and pose of the face, and lighting conditions, in order to use it for commercial applications. The proposed method can be extended and used for applications such as Internet access and secured ATM transactions in controlled environment. The proposed method is not sensitive to the size of the mouth, its position in the image and its background, and orientation of the face. It is also not sensitive to the pose of the face as long as the eye regions are visible. The proposed method is less sensitive to variation in the image brightness. However, the proposed method is sensitive to shadows, variation in lighting conditions and profile view of the face. Person recognition in uncontrolled lighting conditions and real time applications is still a challenging task. 7. Conclusion
In this paper, a method was proposed for automatic person identification and verification using fovea intensity comparison code. The proposed method uses the method proposed in Viola and Jones (2001) to detect the face region, and the face region is processed in YC b C r color space to determine the locations of the eyes. The center of the mouth is determined relative to the locations of the eyes. The facial features are extracted relative to the locations of the eyes, and mouth features are extracted relative to the locations of the eyes and mouth using multiscale morphological operations. Fovea intensity comparison code and exclusive-OR operation for matching are used to recognize a person in video sequences using face and mouth modalities. The performance of the system using FICC is evaluated in real time in the laboratory environment, and the system achieves a recogni-tion rate of 97.0% and an equal error rate (EER) of about 0.89% for 50 subjects. The performance of the system is also evaluated for XM2VTS database, and the system achieves a recognition rate of 100% and an equal error rate (EER) of about 0.27%. Projected fovea intensity comparison code (PFICC) and Euclidean distance for matching are also used to recognize a person in video sequences using face and mouth modalities. The performance of the system using PFICC is evaluated in real time in the laboratory environ-ment, and the system achieves a recognition rate of 99.0% and an equal error rate (EER) of about 0.84% for 50 subjects. The performance of the system using PFICC is also evaluated for XM2VTS database, and the system achieves a recognition rate of 100% and an equal error rate (EER) of about 0.23%. The method is invariant to size of the mouth, its position in the image and its background. The proposed method can be extended and used for applications such as Internet access and secured ATM transactions in controlled environment. The feature extraction techniques are computationally efficient, and the system recognize a subject within a reasonable time. Person identification and verification in uncontrolled lighting conditions and real time applications is still a challenging task.
 References
