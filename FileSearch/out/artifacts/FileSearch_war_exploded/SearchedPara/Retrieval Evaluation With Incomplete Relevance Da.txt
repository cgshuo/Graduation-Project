 H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Performance evaluation Measurement, Experimentation Cranfield evaluation model, TREC, incomplete judgments, retrieval effectiveness, rank effectiveness
This poster investigates tw o relatively new measures of retrieval effectiveness in relation to the problem of incom-plete relevance data. The measures, bpref -10 and RankEff , which do not take into account documents that have not been relevance judged, are compared theoretically and ex-perimentally to the more used MAP. The results indicate that RankEff is the most stable of the three measures when the amount of relevance data is reduced, with respect to system ranking and absolute values.
Our experiments used the test collections from TREC-8 (ad hoc task), TREC-10 (Web track) and TREC-12 (robust track). They contain 50 topics each except for TREC-12 with 100 topics. Table 1 shows that more than 500 in aver-Table 1: Overview of system top 1000 outputs TREC Runs Top 1000 Averages TREC-8 122 997.5 53.2 555.3 TREC-10 76 989.6 41.8 562.4
TREC-12 71 999.5 33.2 558.2 age among the top-1000 retrieved documents are unjudged. One concern with unjudged documents is that runs, which did not contribute to the pools, may retrieve unjudged rele-vant documents, and thereby be treated unfairly during eval-uation. Although it has been argued on empirical grounds that the TREC collections are not biased against such runs [3], we believe that large shares of unjudged documents, es-pecially at low ranking positions, is a sufficient motivation for the construction of measures that do not take unjudged documents into account. Buckley and Voorhees [1] pro-posed a measure of retrieval effectiveness, intended to be used when some retrieved documents have not been judged for relevance. For the system M and the topic t , define: where r is the number of relevant, n the total number of relevance judged documents for t , I 10+ r ( d i )isthenumber of documents d among the 10 + r most highly ranked irrele-vant documents such that Rank ( d, t, M ) &lt;Rank ( d i ,t,M ). Gr  X  onqvist [2] suggested a measure RankEff , inspired by bpref -10: I ( d i ) is the number of irrelevant documents d such that d and Rank ( d, t, M ) &gt;Rank ( d i ,t,M ). It uses more informa-tion, since each relevance judged document is taken into consideration. Both measure are composed as sums over the set of relevant documents. The RankEff measure gives the mean number of known irrelevant documents that are ranked lower than a known relevant document, in relation to the number of known irrelevant documents. RankEff handles topics with a small or extremely large number of relevant documents better than bpref -10. RankEff (and AP which MAP is based on) also have the positive property that if a relevant document swaps position with a higher ranked irrelvant, the score always rises, which is not always the case for bpref -10.
We are principally concerned with comparing RankEff and bpref -10. However, we compare the two measures to MAP, heavily used in TREC.
One way to empirically study if two evaluation measures measure the same thing is to compute the correlation be-tween two system rankings. Table 2 gives, for each of the three TRECs, correlation data for bpref -10, RankEff and MAP. Correlations are measured by Kendall X  X   X  . The Kendall correlation values between bpref -10 and RankEff are fairly weak over all TRECs. bpref -10 has higher correlations with MAP than with RankEff . One of the correlation values be-tween bpref -10 and MAP is (slightly) less than, while the remaining two values are greater than 0.9, which have been used as a cut-off for equivalent rankings [1].
 Table 2: Kendall correlations: System rankings Measures TREC-8 TREC-10 TREC-12 MAP X  X pref-10 0.932 0.892 0.939
MAP X  X ankEff 0.856 0.760 0.770 bpref-10 vs.

RankEff
We started with the full pools and gradually reduced them by removing randomly selected documents. The share of relevant documents were held as constant as possible. Note that 100% means the full TREC-pools, but as seen in Ta-ble 1 most of the documents retrieved by the participating systems has not been relevance judged.
Figure 1 shows the effects of gradually reducing the pools of judged documents on consistency of absolute average val-ues. For each measure, a plotted value is the average over all topics and all runs, in relation to a certain level of relevance data incompleteness. RankEff remains almost constant at the same value during the reduction down to 10% while the other measures are far from constant. Figure 1: Average values in TREC-12 when rele-vance data are reduced
Figure 2 shows the Kendall correlation between the full pool system rankings and each degree of reduction from 100% down to 1%. RankEff has clearly the best correlation between full and reduced pools, but note also that bpref -10 shows a much better behavior than MAP. Figure 2: Kendall correlations for system rankings between full and reduced pools for TREC-12
The experimental results indicate that RankEff may be a better alternative to MAP than bpref -10 when the rele-vance data are incomplete (as for TREC). System ranking stability, the ability of a measure to rank systems in ap-proximately the same relative order under different levels of relevance data incompleteness, is a very important variable. The major objection against RankEff is the low correlation to MAP. However, Figure 2 shows that MAP indeed has problem with incomplete relevance data. The issue of what aspect of retrieval effectiveness RankEff is actually measur-ing should therefore be further addressed. [1] C. Buckley and E. M. Voorhees. Retrieval evaluation [2] L. Gr  X  onqvist. Evaluating latent semantic vector models [3] E. M. Voorhees. The philosophy of information
