 Link-based document ranking algorithms concern about link mining techniques that can identify  X  X uthoritative X  or  X  X restigious X  documents from set of hyper-linked webpages or from other citation da ta within a certain structure. These methods always try to find out relationships among documents or authors and documents. There exist a variety of algorithms that rank documents in a social network according to criteria that reflect structural properti es of the network. Madadhain et al. [14] grouped these algorithms in three major paradigms: be-tweenness centrality [3], eigenvector centr ality [20] and voltage-based rankers [22]. A comprehensive overview on link analysis can be found in [10] and [2], which made a taxonomy of common link mining task. Link analysis encompasses a wide range of tasks and we only focus on the core issues addressed by a majority of ongoing research in the field of link-based document ranking.

PageRank [17] and HITS [13] algorithms are the most famous approaches in link-based document ranking. Both of them are eigenvector methods for identify-ing  X  X uthoritative X  or  X  X nfluential X  articles, given hyperlink or citation informa-tion. There are many extensions on these methods. Bharat and Henzinger [1] and Chakrabarti et al. [4] made modifications to HITS to exploit webpage content for weighing pages and links based on relevance. Cohn and Chang [6] introduced PHITS as a probabilistic analogue of the HITS algorithm, attempting to explain the link structure in terms of a set of latent factors. Ding et al. [8] introduced a unified framework encompassing both PageRank and HITS and presented several new ranking algorithms. Cohn and Hofmann [7] constructed Link-PLSI, a joint latent probabilistic model to integrate content and connectivity together while Doan et al. [9] presented probabilistic models inspired by HITS and PageRank, which incorporate both content and link structure. Xu [23] proposed a ranking algorithm trying to introduce the content information into link-based methods as implicit links. Matthew Richardson et al. [19] achieved a novel ranking algo-rithm using features that are independent of the link structure of the Web. Ng et al. [16] analyzed the stability of PageRank and HITS to small perturbations in the link structure and presented modifications to HITS that yield more stable rankings. O X  X adadhain and Smyth [14] and O X  X adadhain et al. [15] proposed a framework for ranking algorithms that r espect event sequences and provided a natural way of tracking changes in ranking over time.

The dramatic growth and changes of link information, however, exhibit dy-namic patterns of the importance of lin ked documents, as references and links can be changed or become inaccessible. As the amount of data grows and the number of sources expands, techniques which can track the changes in documents rank over time become a hot issue. As a naive approach to catch the update of links, static ranking methods can be applied to data streams over various time intervals, which means to re-run the batch algorithm from scratch on all existing data each time a new data comes in. But it is computationally expensive for use during link development. Another obviou s weakness is that changes to the links themselves can not be auto matically updated with the content of stable ranking results maintained after re-running of the batch algorithm.

In this paper, we develop a novel incremental probabilistic model, which mod-els hyperlinked data by sequentially folding in the corresponding new documents and citations. Initially, IPHITS uses the traditional PHITS technique to recog-nize links and stores them into the PHITS model. For the arriving of new linked data and removal of out of date data, IPHITS analyzes the changes and the new structure of linking repository and then updates the corresponding links. Since the method does not need to recalculate the probabilities completely whenever it receives new linked data, it is very fast computationally.

The key contributions of this paper include a novel link updating technique using our novel incremental PHITS algorithm (IPHITS) that can cope with link changes. Importantly, with the great reduction in time complexity, the model allows latent topics of citations to be automatically maintained. Before we de-scribe our new model, we summarize the main notations used in this paper in Table 1.

The rest of this paper is organized as follows: In Section 2, we introduce the PHITS model and its principles. In Section 3, we give detailed information on our proposed IPHITS algorithm. Section 4 considers our test corpora, the performance measures, together with th e experiment results. We conclude and discuss future work in Section 5.

Note that in the rest of the paper, we use the terms  X  X itation X  and  X  X yperlink X  interchangeably. Likewise, the term  X  X iting X  is synonymous to  X  X inking X  and so is  X  X ited X  to  X  X inked X . PHITS [6] model is based on a two way factor analysis that is in most respects identical to the topic model used by Ho fmann [11]. The model attempts to ex-plain two sets of observables (citing documents and cited documents) in terms of a small number of common but unobserved variables. Under the PHITS as-sumptions, a document is modeled as a mixture of latent topics that generates citations. A representation of the mode l in terms of graphical model is depicted in Fig 1. We can see that PHITS introduces a hidden variable z k ( k  X  1 , ... ,K ) in the generative process of each cited document c r ( r  X  1 , ... ,R )inadocument d ( i  X  1 , ... ,N ). Given the unobservable variable z k ,each c r is independent of the document d i it comes from and the latent topic can be identified within individual research communities.
Essentially, we can describe the model as a generative process: a document d i is generated with some probability P ( d i ), a latent topic z k associated with doc-uments and citations is chosen probabilistically so that their association can be for predicting citations in documents is defined as: where c r and d i both refer to document in the document set and they may be identical. They are kept sep arate notationally to reinforce different roles they play in the model, c r is conveyed by being cited and d i is conveyed by citing [11].

An EM algorithm is used to compute the parameters P ( c r | z k )and P ( z k | d i ) through maximizing the following log-likelihood function of the observed data: The steps of the EM algorithm are described as follows:
E-step: The conditional distribution P ( z k | d i ,c r ) are computed from the pre-vious estimate value of the parameters P ( c r | z k )and P ( z k | d i ): M-step: The parameters P ( c r | z k )and P ( z k | d i ) are updated with the new ex-pected values and P ( z k | d i ,c r ): In order to make full use of existing information, an incremental learning algo-rithm is presented that can efficiently model the incoming information. Instead of modeling documents all at once, we start with a given document set and add new documents or delete documents at each stage while preserving document co-herence. The basic idea of our updating algorithm is straightforward: the PHITS algorithm is performed once on the initial linked-documents at the beginning. When a set of new documents introduce new citations, a cycle should be created for folding documents and citations and the model is then updated during the cycle.

Chou et al. [5] proposed an Incremental PLSI (IPLSI), aiming to address the problem of online event detection, this model captures the basic concept of in-cremental learning for PLSI and offers an excellent foundation on which to build our model. Our technique is computationally similar to the IPLSI procedure. However, what we are trying to do is, co nceptually speaki ng, very different. In this section, for the update of links when new links are added or deleted, we develop an incremental approach to PHITS technique. The PHITS algorithm is executed once on the initial documents. Then, for each new adding or deleting of linked data, we need to adjust link-topic probabilities at the lowest cost. On one hand, we need to update citation-topic probabilities for out of date links; on the other, new citation-topic a nd new document-topic probabilities should be adjusted corresponding. We take advantage of folding-in method, a partial version of EM algorithm, to update the unknown parameters with the known parameters kept fixed so as to maximize the likelihood with respect to the previously trained parameters.
 Fig. 2 is an illustration of sequences for updating related information of IPHITS, where d and c indicate new documents and new citations respectively. As the figure shows, new documents should first be folded in with old links fixed, and then P ( d | z k ) are calculated which sets a foundation for folding in new links. final EM algorithm and this guarantees a faster convergence. (Note that d all is a final document in the entire document set, and so is c all ). Algorithm 1 gives a detailed illustration of Incremental PHITS algorithm.

In Step 2, preprocessing is done as the first phase for the incremental learning, involving elimination of out-of-date documents and hyperlinks. Documents not used anymore should be discarded and hyperlinks related to these documents should be removed as well. The model can not be augmented directly, as the basic principle of probability that the total probability will be equal to one should be observed, the remaining parameters need to be renormalized proportionally. Note that P 0 ( c r | z k ) stand for the probabilities of the remaining terms and citations.
In Step 4, we fold in documents with old links kept fixed since old links are well trained and the arriving documents contain old links. In contrast, old documents convey no corresponding information to aid the folding in of hyperlinks. It is a pity that P ( c new | z ) can not be folded in using P ( z | d new ) directly. That is because the sum of all probabilities of citations in old citation sets Algorithm 1. Incremental PHITS under z already equals to one, which means P ( c r | z k ) have been well trained and normalized. If we randomize and normalize all P ( c new | z k ) when new documents arrive, the sum of the probabilities of all citations under z will be larger than one. This restriction makes it inapplicable to update new terms and citations directly. To avoid this, we first derive P ( d new | z k )inStep6.

In Step 8, we develop a mechanism for new citations update, which can satisfy the basic of requirement of probabilities of latent topics under new citations equal to one. P ( z k | c new ) are randomly initialized and normalized. We then update P ( z k | c new )with P ( d new | z k )fixed.
 In Step 10, our method deals with issu es of how to get the final normalized are calculated according to Step 8 while for old terms and citations, we use Eq.(4) to get P ( z k | d i ,c r ).
 the original EM algorithm for updating the model. As new documents arrive and old documents disappear, the above algorithm can preserve the probability and continuity of the latent parameters during each revision of the model in a fast way. This section reports on the empirical evaluation on the performance and use-fulness of our approach. We design two experiments to test the viability of the model: time expenditure and analysis of the stability of the algorithm. 4.1 Data Description and Baseline Representation The performance of our model is evalua ted using two different types of linked data: scientific liter ature from Citeseer which is conn ected with citations, Wikipe-dia Webpages dataset containing hyperlinks. We adjust the link structure to in-clude the incoming links and outgoing links only within each corpus, and then take advantage of these dataset for our model construction with adding new documents and citations and deleting out of date information.

The Citeseer data can be obtained from Citeseer collection that was made pub-licly available by Lise Getoor X  X  research group at University of Maryland [21]. There are altogether 3312 documents using abstract, title and citation informa-tion in the corpus. The Citeseer dataset only includes articles that cite or are cited by at least two other documents. Thereafter the corpus size is limited to 1168 documents, of which only 168 documents have both incoming and outgoing links.

The dataset of Wikipedia Webpages is downloaded from Wikipedia by crawl-ing within the Wikipedia domain, starting from the  X  X rtificial Intelligence X  Wikipedia page and the dataset is composed of 6324 documents and 16387 links.
We compare our IPHITS with the following methods:  X  Naive IPHITS. For every advance of the new documents, the EM algorithm uses new random initial setting to re-estimate all the parameters of the PHITS algorithm.  X  PageRank. The outgoing and incoming links are counted and the implemen-tation of PageRank used in the experiments comes from [17].  X  HITS. A n -by-n adjacency matrix is co nstructed, whose ( i, j )-entry is 1 if there is a link from i to j , and 0 otherwise. An iterative process is used to identify the principal eigenvector and principal community of the matrix. 4.2 Experiment on Time Cost We perform this experiment aiming at evaluating time efficiency of IPHITS in comparison with Naive IPHITS. In order to observe the correlation between the number of topics k and the time saved by our model, we examine the impact of different numbers of latent variables and run IPHITS on the two datasets with different k .Foreach k , we run these two algorithms on the subset of each database consisting of 90% of the entire documents respectively. We then ran-domly delete 10% subset of the documents and add the same amount of data. Table 2 gives a detailed illustration on the total time and the number of itera-tions required to achieve convergence (The total time of IPHITS is divided into two parts: Link-PLSI time and folding time).
 As seen in Table 2, the IPHITS meth odcansavealargeamountoftime.
 In general, the computation time of the Naive IPHITS approach is 10 times longer than that of the our algorithm. With k =30 on Wiki dataset, IPHITS can reduce the time cost by 14 times. The reason is that the Naive IPHITS approach uses new random initial settings to re-es timate all relevant parameters of EM algorithm each time and requires a large number of iterations to converge to a different local optimum while IPHITS has preserved a better starting point and can therefore converge much faster. The larger the dataset is, the more time our model can save. Furthermore, when k increases, time cost in creases as well, these results are consistent with our intuition. 4.3 Stability of the Algorithm This experiment aims to analysis the sensitivity of the rank orderings of different algorithms using different datasets. We believe that small changes in the dataset should have a small effect on the output of a stable algorithm.

Specifically, we run the Naive IPHITS and IPHITS on the subset of the Wiki database consisting of 514 Artificial Intelligence webpages, and examined the list of papers that they considered  X  X nfluential X . To evaluate the stability of these two algorithms, we also constructed 20 perturbed versions of the datasets (which are denoted as wiki-1, wiki-2, etc), each of which containing a randomly deleted 10% subset of the documents and added of the same amount of webpages. Note that we keep the top ranking 10 documents in each dataset, assuming that a robust and stable model is not sensitive to the perturbation of the link structure. Likewise, we develop a simila r experiment on Citeseer dataset. In this experiment, the number of latent topics is set to 5.

Due to the limitation of the paper, we only present the most authoritive webpages generated by IPHITS and Naive IPHITS under the topic  X  X rtificial intelligence X  along with its probability in Table 3 and Table 4 respectively. The leftmost column is the authority ranking obtained by analyzing the initial set of Wiki dataset, the 5 rightmost columns demonstrate the ranks on some perturbed datasets. We can see substantial variation across the different datasets.
We found that results obtained from Naive IPHITS are not consistent to perturbation of these datasets. And under this perturbation it sometimes ignores some webpages indeed  X  X nfluential X  and returns different webpages as top-ranked documents while our method almost always returns similar results as its pervious outcome. This qualitative evaluation reveals the stable property of our model to perturbation.

In terms of quantitive evaluation, the difference between two rank orderings is defined as the mean absolute difference in rank ordering between pair of indi-vidual datasets, borrowed from [14]: where M j and M k represent two different models on different dataset, and O M j ( d ) denotes the index of the rank assigned to d .

For a given document set D , d ( M j ,M k ) obtains its maximum value when M j produces a reserve ordering of M k : Table 5 shows the results of cross-comparison of four ranking models over vari-ation on different datasets. We can see that IPHITS and PageRank are much more stable than that of the Naive IPHITS and HITS on different datasets.
Similar with [16], we count the number of top 10 pages under each topic which drop or rise in ranking drastically in each dataset and made 5 trials on each dataset. Since we perturb the dataset by deletions and insertions, large rises in ranking the algorithms and large drops in ranking are all interesting to observe. Table 6 shows the average percentage of different algorithm that suffer rank drops(rises). The Percentage of rank drop(rise) is defined as the percentage of the number of top 10 pages that drop(rise) below(above) rank 20 and the total number of times of trails. It is clear that our model rank drop(rise) only changes slightly compared with Naive IPHITS. The reason is that the latent variables generated by the Naive IPHITS are discontinuous, whereas our algo-rithm maintains good continuity in the content of latent variables. The ranking for our algorithm also appears less stable on Wiki than on Citeseer, which in-dicates the different characteristic of cited papers and hyperlinked webpages. In the Citeseer experiments, removing a document from the dataset does not remove any of the papers cited by or citing while in the Wiki case, each deleted webpage also removes its surrounding link structure. So do new adding of papers or webpages. Many emerging applications require linked documents to be repeatedly updated. Such documents include cited papers, webpages, and shared community re-sources such as Wikipedia. In order to accommodate dynamically changing top-ics, efficient incremental algorithms need to be developed. In this paper, we have developed an incremental technique to effectively update the hyperlinked infor-mation dynamically. The novelty of our model is the ability to identify hidden topics while reducing the amount of computations and maintaining the latent topic from one time period to another incrementally. When tested on a corpus of Wikipedia articles and Citeseer papers, our model performs much faster than the batched methods. Extending the model to the unified probabilistic model of the content and connections of linked documents is our future work. Acknowledgments. This work is supported by the National Basic Research Priorities Programme (No. 2007CB311004), 863 National High-Tech Program (No.2007AA01Z132)and the National Sc ience Foundation of China (60775035).
