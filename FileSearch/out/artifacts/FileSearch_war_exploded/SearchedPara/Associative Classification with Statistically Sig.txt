 Rule-based classifier has shown its popularity in building many decision support systems such as medical diagnosis and financial fraud detection. One major advantage is that the models are human understandable and can be edited. Associative classifiers, as an extension of rule-based classi-fiers, use association rules to associate attributes with class labels. A delicate issue of associative classifiers is the need for subtle thresholds: minimum support and minimum con-fidence. Without prior knowledge, it could be difficult to choose the proper thresholds, and the discovered rules within the support-confidence framework are not statistically sig-nificant, i.e., inclusion of noisy rules and exclusion of valu-able rules. Besides, most associative classifiers proposed so far, are built with only positive association rules. Negative rules, however, are also able to provide valuable information to discriminate between classes. To solve the above men-tioned problems, we propose a novel associative classifier which is built upon both positive and negative classification association rules that show statistically significant depen-dencies. Experimental results on real-world datasets show that our method achieves competitive or even better perfor-mance than well-known rule-based and associative classifiers in terms of both classification accuracy and computational efficiency.
 D.2.8 [ DATABASE MANAGEMENT ]: Database Ap-plications X  Data Mining Algorithms Associative Classification; Negative Rules; Statistical Signif-icance c  X 
The task of mining association rules over market basket analysis was introduced in [3] which aims to find associa-tions between items or itemsets in a transaction database. The data is typically retail sales in the form of customer transactions, but can be any data if it is transformed to transactions like image data or text data. The problem is formally defined as follows: assume a transaction dataset D consists of a set of items I = { i 1 ,i 2 ,...,i m } , each transaction T is a set of items such that T  X  X  and is associated with a unique identifier. A transaction T is said to contain X , i.e., a set of items in I , if X  X  T . Then an association rule is an implication of the form  X  X  X  Y  X , where X  X  X  , Y  X  X  , and X  X  Y =  X  . The rule X  X  Y has a support s in the transaction set D if the percentage of transactions in D that contain X and Y is s . It is also said that the rule X  X  Y holds in the transaction dataset D with confidence c if the percentage of transactions in D that contain X also contain Y is c . The problem of discovering all association rules from a set of transactions D consists of generating association rules that have a support and confidence greater than given thresholds. These discovered rules are called strong rules .
Classification is another common task in data mining and machine learning [15, 19, 26, 27]. A classifier is a system that is able to assign an unlabeled object to one or more pre-defined classes. Usually, it is created by building a learning model on the training data whose class labels are known in advance, then its ability to discriminate between classes is evaluated on the test data. Associative classification [25] is a classification method that integrates association rule mining and classification together. To build an associative classifier, classification association rules (CARs) with consequent as class labels are first mined by association rule mining tech-niques. Afterwards, some noisy CARs are pruned to improve the prediction performance, the remaining CARs form the actual associative classifier and these CARs are able to pre-dict classes for unlabeled test data.

Existing associative classification methods mine the train-ing dataset mostly in an Apriori-like way [4] or through a FP-growth approach [22], both of which are based on the support and confidence paradigm. However, appropriate support and confidence thresholds are not easy to determine. Besides, according to Webb [36, 37], traditional association rule mining methods have the risk of false discoveries such that the antecedent part and consequent part of some rules are not strongly correlated. In the worst cases, we may find all spurious rules while miss all rules with strong correlation. On the other hand, most associative classifiers proposed so far use only positive CARs (rules of the form X  X  C ) in the classification process. In addition to positive CARs, nega-tive CARs are also able to provide valuable information to discriminate between classes. A negative CAR is in one of the following form: X  X   X  C or  X  X  X  C (where X and  X  X indicate the presence and absence of itemset X , respec-tively). Let us consider the following example:
Example 1. For a binary classification problem, assume we discover some positive and negative CARs, such as: X  X  c , Y  X  c 2 , XY  X   X  c 1 . Now we have a new unlabeled in-stance XY , how to classify it? When only positive CARs X  X  c 1 and Y  X  c 2 are considered, both c 1 and c 2 are pos-sible as there is a matching rule for either c 1 or c 2 . But the classification task is easier since we have another negative CAR XY  X   X  c 1 which reinforces the decision in favor of class c 2 .

In this paper, we propose a novel associative classifier to tackle the above mentioned issues. Following traditional associative classification methods, the proposed associative classifier consists of three steps: rule generation, rule prun-ing and rule classification.

The main contributions of this paper are as follows: 1. A novel associative classifier is proposed, it achieves a 2. By extending the Kingfisher algorithm in [20], we are 3. We develop a novel rule pruning strategy to prune 4. We present and compare different rule classification
The reminder of the paper is organized as follows. The overview of related work on associative classifier and nega-tive rule mining is given in Section 2. Section 3 describes the proposed associative classifier in three steps. Section 4 con-tains the experimental results and the statistical analysis. Section 5 concludes the paper and discusses some possible future work.
In this section, we review some related work on associative classification and negative rule mining.
The first reference to use association rules as CARs is credited to [12], while the first associative classifier, CBA, was introduced by Liu et al. [25]. The main steps in building an associative classifier are as follows:
CBA [25] mines the complete set of CARs through an apriori-like approach, in addition, it ignores rules by  X  X es-simistic error rate X  as C4.5 [26]. In the rule pruning phase, CBA adopts a strategy called  X  X atabase coverage X . Database coverage consists of going over all the rules ranked by their confidence values and evaluating them against the training instances. Whenever a rule applies correctly on some in-stances, the rule is marked and the instances are eliminated until all training instances are covered. Finally, unmarked rules are simply pruned. New instances are classified by a matching rule with the highest ranking.

Motivated by the idea of CBA, many improvements have been proposed to build more accurate associative classifiers. CMAR [24] maintains a CR-tree to compactly store and retrieve rules, the CARs are discovered by a FP-growth approach. In addition to the database coverage method, CMAR also prunes lower ranked and more specific rules. The rule R 1 : P  X  C with confidence conf 1 is a lower ranked and more specific rule w.r.t rule R 2 : P 0  X  C with confidence conf 2 if P 0 ( P and conf 1  X  conf 2 . For a new unlabeled in-stance, CMAR makes a prediction based on multiple match-ing rules with a weighted chi-square measure.

In the classification phase, ARC [6] takes all rules that apply within a confidence range, but instead, calculates the average confidence for each set of rules grouped by class labels in the consequent, and selects the class label of the group with the highest confidence average.
 There are some other variants of associative classifiers: Harmony [35] is an example which directly mines CARs. It adopts an instance-centric approach to find the highest con-fidence rule for each training instance and builds the classi-fication model from the union of these rules. It shows to be more effective and scalable than other associative classifiers. 2SARC [9] is a two-stage classification model that is able to automatically learn to select rules for classification. First, an associative classifier is learned by standard techniques. Second, multiple predefined features are computed on the associative classifier, then they act as input to a neural net-work to achieve a more accurate classification model.
CCCS [10] uses a new measure,  X  X omplement Class Sup-port X  (CCS) to mine positively correlated CARs for the im-balanced classification problem. It forces the CCS measure to be monotonic, thus the complete set of CARs are discov-ered by a row enumeration algorithm. An associative clas-sifier is then built upon these positively correlated CARs.
SPAR-CCC [33] is another associative classifier designed for imbalanced data. It also integrates a new measure,  X  X lass Correlation Ratio X  (CCR) into the rule mining phase, the classifier works comparably on balanced datasets and out-performs other associative classifiers on imbalanced datasets.
ARC-PAN [7] is the first associative classifier that uses both positive and negative CARs. It proposes to add Pear-son X  X  correlation coefficient on the basis of support and con-fidence framework to mine positively and negatively corre-lated CARs. The ability of negative CARs is demonstrated by their usage in the classification phase. A negative association between two positive itemsets X , Y are rules of the following forms:  X  X  X  Y and X  X  X  Y , where  X  X and  X  Y indicate the absence of itemsets X and Y in the transaction dataset D , respectively. Mining as-sociation rules from a transaction dataset that contains in-formation about both present and absent itemsets is com-putationally expensive, traditional association rule mining algorithms cannot be directly applied. This is the reason why new algorithms are needed to efficiently mine associ-ation rules with negative itemsets. Here we survey algo-rithms that efficiently mine some variety of negative associ-ation rules from data.

Brin et al. [13] mention for the first time the notion of negative relationships in the literature. They propose to use chi-square test between two itemsets. The statistical test verifies the independence between two itemsets. To deter-mine the nature (positive or negative) of the relationship, a correlation metric is used.

Aggarwal and Yu [1, 2] introduce a new method for find-ing interesting itemsets in data. Their method is based on mining strongly collective itemsets. The collective strength v ( I ) is the violation rate of an itemset I , i.e., the fraction of violations over the entire set of transactions and E [ v ( i )] is its expected value. An itemset I is in a violation of a transaction if only a subset of its itemsets appears in that transaction. The collective strength ranges from 0 to  X  , where a value of 0 means that the items are perfectly nega-tively correlated and a value of  X  means that the items are perfectly positively correlated.

In [29], the authors present a new idea to mine strong neg-ative rules. They combine positive frequent itemsets with domain knowledge in the form of taxonomy to mine nega-tive associations. The idea is to reduce the search space by constraining the search to positive patterns that pass the minimum support threshold. When all the positive itemsets are discovered, candidate negative itemsets are considered based on the used taxonomy.

Wu et al. [38] derive another algorithm for generating both positive and negative association rules. The negative asso-ciations discovered are of the following forms:  X  X  X  Y , X  X  X  Y and  X  X  X  X  Y . They add another measure called  X  X ininterest X  on top of the support-confidence framework for a better pruning of the frequent itemsets, which is also used to assess the dependency between two itemsets.
The SRM algorithm [30, 31], discovers a subset of negative associations. The authors develop an algorithm to discover negative associations of the type X  X   X  Y . These associa-tion rules can be used to discover which items are substitutes for others in market basket analysis.

Antonie and Za  X   X ane [8] propose an algorithm to mine strong positive and negative association rules based on the Pear-son X  X  correlation coefficient. In their algorithm, itemset and rule generation are combined and the relevant rules are gen-erated on-the-fly while analyzing the correlations within each candidate itemset.

In [32], the authors extend an existing algorithm for asso-ciation rule mining, i.e., GRD (generalized rule discovery), to include negative items in the discovered rules. The algo-rithm discovers top-K positive and negative rules.
Cornelis et al. [16] propose a new Apriori-based algorithm (PNAR) that exploits the upward closure property of neg-ative association rules. With this upward closure property, valid positive and negative association rules can be discov-ered efficiently. Wang et al. [34] give a more intuitive way to express the validity of both positive and negative association rules, the mining process is very similar to PNAR.

MINR [23] is a method that uses Fisher X  X  exact test to identify itemsets that do not occur together by chance, i.e., with a statistically significant probability. An itemset with a support greater than the positive chance threshold is con-sidered for positive rule generation, while an itemset with a support less than the negative chance threshold is considered for negative rule generation.

Kingfisher [20, 21] is developed to discover both positive and negative dependency rules. The dependency rule can be formulated on the basis of association rule and the statistical dependency of a rule can be calculated by Fisher X  X  exact test. In order to reduce the search space, the author introduces a branch-and-bound search method with three lower bounds for the measure of p F -value. Another two pruning strategies (pruning by minimality and pruning by principles of Lapis philosophorum) are also included to speed up the search.
A more detailed review of negative rule mining can be referred to [5].
In this section, we introduce details about the proposed as-sociative classifier in three steps: rule generation, rule prun-ing and rule classification. Before talking about the detailed steps, we introduce some notations and definitions used in this paper.
Assume D is a transformed transaction dataset with a set of items I = { i 1 ,i 2 ,...,i m } and a set of class labels C = { c 1 ,c 2 ,...,c n } . Each transaction T is associated with a set of items X and a class label c k , where X  X  I and c k  X  C . A CAR is in the form of X  X  c k or X  X   X  c and it is considered dependent if P ( X,c k ) 6 = P ( X ) P ( c tistically significant positive and negative CARs, we take Fisher X  X  exact test [20, 23, 33] as a significance measure. The dependency of the CAR X  X  c k or X  X   X  c k is con-sidered statistically significant at level  X  , if the probability of observing equal or stronger dependency in the dataset under a null hypothesis model, is not greater than  X  . X and c k (or  X  c k ) are independent in the null hypothesis. The probability p , i.e., p -value is: p ( X  X  c k ) = p ( X  X  X  c k ) = , where the consequent class label c k can either be present or absent.  X  (  X  ) denotes the support count of  X  ,  X  can be the conjunction of any itemsets, either being present or absent, for example, it can be X , Xc k ,  X  Xc k ,  X  X  X  c k , etc. The significance level  X  is usually set to be 0.05.
 The dependency of the positive CAR X  X  c k or negative CAR X  X  X  c k is statistically significant, if p F ( X  X  c or p F ( X  X  X  c k )  X   X  . In the field of rule mining, an impor-tant task is to mine non-redundant rules. Rules are consid-ered redundant when they do not add new information to the remaining rules. Without the non-redundancy property taken into consideration, the number of discovered rules is usually too large for people to read and interpret. In order to reduce the number of rules and to make the classification model more readable, only non-redundant CARs are con-sidered. Following [20], we define non-redundant CARs and minimal CARs as follows: Definition 1. Non-redundant CARs The CAR X  X  c k or X  X   X  c k is considered as non-redundant, if there does not exist any CARs in the form of Y  X  c k or Y  X  X  c k such that: or where Y ( X .
 Definition 2. Minimal CARs The CAR X  X  c k or X  X   X  c k is considered as minimal, if and only if X  X  c k or X  X   X  c k is non-redundant, and, there does not exist any CARs in the form of Z  X  c Z  X  X  c k such that: or where X ( Z .
In traditional association rule mining algorithms, the mea-sure of support is usually used to prune non-frequent pat-terns or rules since it has a downward closure property, but it is not the case for the p -value, making it impossible to be used as a monotonic property for some effective pruning. Recently, Kingfisher [20] was proposed to find the complete set of positive and negative rules that show statistically sig-nificant dependencies. However, it was designed for the dis-covery of general rules, not specifically for CARs. Therefore, adaption of the Kingfisher algorithm to enable the discov-ery of only CARs is necessary as it can reduce the number of discovered rules. To find statistically significant positive and negative CARs, we extend the Kingfisher algorithm by pushing the rule constraint in the rule generation phase. First, two theorems in [20] are given:
Theorem 1. In a transaction database D , assume R is the set of all items, for any item A  X  R and X  X  R\ A , it has p if  X  ( A )  X  |D| 2 , then for any B  X  R , X  X  R\{ A,B } , it when  X  ( A ) &lt;  X  |D| , the item A cannot appear in any statis-tically significant rules.

Theorem 2. In a transaction database D , assume R is the set of all items, for any item A  X  R , X  X  R\ A and Q  X  R\{ X,A } , if  X  ( X )  X   X  ( A ) or  X  ( X )  X   X  (  X  A ) holds, then it
Given these two theorems, we derive three corollaries to generate positive and negative statistically significant CARs.
Corollary 1. There exists a threshold  X   X  0 . 5 such that the item I  X  X  is impossible to be in any statistically signif-icant CARs if its support is smaller than  X  |D| .
 Proof. Corollary 1 is a special case of Theorem 1 when I  X  I . First we assume that I can be in the consequent part of the rule, then according to Theorem 1, we can find a threshold  X   X  0 . 5 such that when  X  ( I ) &lt;  X  |D| , I cannot appear in any statistically significant rules. Since we only intend to find CARs where item I can only be in the an-tecedent part, if the condition  X  ( I ) &lt;  X  |D| holds, item I can cannot appear in any statistically significant CARs.
Some impossible items are pruned before further analy-sis by Corollary 1. It is assumed that s items ( s  X  m ) are left. The remaining s items are reordered and renamed in an ascending order by their support count, i.e., I rest { i ,i 2 ,...,i s } , where  X  ( i 1 )  X   X  ( i 2 )  X  ...  X   X  ( i der to traverse the whole search space, an enumeration tree is built over I rest . For each node in the tree, the antecedent part is a combination of items in the power set of I rest consequent part is 2 n possible class labels (either positive or negative). Therefore, for each node in the enumeration tree, we have to check all 2 n possible CARs to see if they are statistically significant.

Corollary 2. For any X  X  I rest , Q  X  ( I rest \ X ) , if  X  ( X )  X   X  ( c k ) or  X  ( X )  X   X  (  X  c k ) holds, we can get p c respectively.

Proof. Corollary 2 can be considered as a special case of Theorem 2 when either c k or  X  c k is the consequent part of a rule.

According to Corollary 2, the lowest value of p F ( XQ  X  c ) and p F ( XQ  X  X  c k ) provide the lower bounds for p F ( X  X  c ) and p F ( X  X   X  c k ), respectively. Therefore, if the lower bound exceeds  X  , the corresponding CAR X  X  c k or X  X   X  c k is not statistically significant and can be directly pruned. Otherwise, the CAR X  X  c k or X  X   X  c k is considered as PSS , i.e.,  X  X otentially Statistically Significant X . Definition 3. The CAR X  X  c k or X  X  X  c k is defined as PSS , i.e.,  X  X otentially Statistically Significant X , if it meets either of the following conditions: (1)  X  ( X )  X   X  ( c  X  ( X )  X   X  (  X  c k ) holds, and the lower bound  X  (  X  X )!  X  ( c tively; (2)  X  ( X ) &gt;  X  ( c k ) or  X  ( X ) &gt;  X  (  X  c
If a CAR is PSS , we need to calculate the exact p -value to see if it is indeed statistically significant.

Corollary 3. If CAR X  X  c k or X  X   X  c k is PSS , then any of its parent rule Y  X  c k or Y  X   X  c k is also PSS , where Y ( X and | Y | = | X | X  1 .
 Proof. There are two situations making X  X  c k being PSS . The first situation is when  X  ( X ) &gt;  X  ( c k ), since Y parent rule Y  X  c k is also PSS . The second situation is when X  X  c k but lowerbound ( p F ( XQ  X  c k )) &lt;  X  , where Q  X  ( I rest \ X ). Now let XQ = Y ( X \ Y ) Q = Y R , because ( X \ Y )  X  ( I rest \ Y ) and Q  X  ( I rest \ X )  X  ( I rest R = ( X \ Y ) Q  X  ( I rest \ Y ) and therefore, there must exists rule Y  X  c k is PSS . The proof is similar for the negative CAR X  X  X  c k .

With these three corollaries, the whole search problem can be summarized as follows: We first use Corollary 1 to prune impossible items, sort and rename the remaining items in an ascending order by their support. Next, all candidate CARs with only one antecedent item are listed. We then use Corol-lary 2 to check if they are PSS , non-PSS candidate CARs can be pruned directly without further analysis. PSS CARs are further checked to see if they are indeed statistically sig-nificant. From PSS 1-itemset CARs, we generate candidate PSS 2-itemset CARs by Corollary 3. The process repeats until no PSS CARs are generated at a certain level. It also needs to be mentioned that in the searching process, the minimality of the CARs is considered, if the CAR is marked as minimal, we stop the expansion from this CAR because all of its children CARs are impossible to get a lower p -value. In fact, checking minimality for a CAR is a hard task, because we have to consider its whole subtree. We use a well-proven result from [20] that if P ( c k | X ) = 1 or P (  X  c k | X ) = 1, the corresponding CAR X  X  c k or X  X   X  c k is minimal. In other words, the property of minimality can be detected by calculating the conditional probability of c k or  X  c k given X . Therefore, for a certain CAR, we do not need to check all its children CARs in its subtree to see if it is minimal anymore. The rule generation process is presented in Algorithm 1.
In the rule generation phase, redundant (lower ranked and more specific) CARs have been removed, but the number of discovered statistically significant CARs could still be very large. The disadvantages of a large number of CARs are two folds: first, noisy CARs may be included, they may jeopar-dize the classification performance; second, a classifier with a small number of rules is important since it allows domain experts to tune a classifier by editing rules if necessary.
The most widely used rule pruning strategy is database coverage [25], however, the database coverage heuristic can only be applied to positive CARs and negative CARs in the form of  X  X  X  c k . Through our rule generation phase, Data : Transaction Dataset D , set of antecedent items Result : Statistically significant positive and negative Prune impossible antecedent items I with Corollary 1; I rest : the reoredered and renamed antecedent item set; Create root node and level-1 nodes;
Set l = 1; while l  X |I rest | do end
Algorithm 1: Statistically significant positive and nega-tive CARs generation. the discovered CARs are all in the form of X  X   X  c reduce the number of negative CARs in this type, we propose a novel rule pruning strategy to prune noisy positive and negative CARs simultaneously.
 We first scan through the set of discovered negative CARs. For each negative CAR X  X  X  c k , if it misclassifies at least one training instance, in other words, if we find an instance t in the training dataset such that X  X  t.antecedent and c k t.class , the negative CAR X  X  X  c k is pruned, otherwise, it is kept for the following classification phase.

For the positive CARs X  X  c k , we first rank them by their confidence values, then use the database coverage method to select a subset of high quality.

Here a problem arises, in some datasets, the number of re-maining negative CARs may be much larger than the num-ber of remaining positive CARs. In the extreme case if only negative CARs X  X   X  c k are left, it is still hard to make a prediction for a new instance, for example, for instance XY , the only information obtained is that class label c correct. Therefore, we still wish the positive CARs domi-nate the classification decision phase, while taking negative CARs as a complement to help positive CARs. Considering this, we adjust the number of negative CARs, make it at most as large as the number of positive CARs. To be more specific, let n neg and n pos denote the number of pruned pos-itive and negative CARs, respectively, if n neg &gt; n pos the first n pos negative CARs and all positive CARs are cho-
Data : Set of positive and negative CARs R pos , R neg Result : Pruned CARs set R newpos and R newneg .
 Ranking R pos and R neg according to confidence values; // 1. Negative CARs pruning for each CAR r in R neg do end // 2. Positive CARs pruning for each CAR r in R positive do end // 3. Negative CARs set adjustment end
Algorithm 2: Positive and negative CARs pruning. sen as the actual classifier. The whole process is illustrated in Algorithm 2. The set of statistically significant positive and negative CARs left from the previous rule pruning phase represents the actual associative classifier. Given a new unlabeled ob-ject, the classification process searches for the set of CARs that are relevant to this object, and makes the prediction ac-cording to the label information of all these relevant rules. Here we discuss how to make predictions for new objects based on the set of rules in the classifier. There are two types of CARs in our classifier: positive CARs in the form of X  X  c k and negative CARs in the form of X  X  X  c k . These two types of CARs are both considered in our classification phase. A simple way to classify a new object is to select the matching rule with the highest confidence value and assign its label to the new object. This is the strategy adopted in CBA [25]. But in this way, the negative rule X  X  X  c k does not make any sense if it has the highest confidence since it does not allow the labeling. In this way, only positive CARs influence the classification decision. Therefore, we propose to divide all matching rules into groups according to their class labels. The groups are ordered either by average con-fidence values or sum of confidence values. Then the group with the highest average or sum of confidence values will be assigned to the new object. These three possible clas-sification methods are denoted as BEST, AVE and SUM, representing classifying by the best matching rule, by the average and by the sum of confidence values, respectively. Data : A new instance o to be classified. Set of positive Result : Class label of the new instance o .
 T pos =  X  ; // set of positive rules matching o
T neg =  X  ; // set of negative rules matching o for each CAR r in R newpos do end for each CAR r in R newneg do end
Divide T into n subsets by class labels: T 1 ,T 2 ,...,T n // 1. Classification method BEST for each subset T 1 ,T 2 ,...,T n do end
Assign its class label to o ; // 2. Classification method AVE for each subset T 1 ,T 2 ,...,T n do end
Assign the class with the highest average of confidence value to o ; // 3. Classification method SUM for each subset T 1 ,T 2 ,...,T n do end
Assign the class with the highest sum of confidence value to o ; Algorithm 3: Three methods to classify a new instance. The detailed descriptions of these classification methods are presented in Algorithm 3.
 It is obvious that the confidence values of positive CARs X  X  c k are added to the class c i in the classification phase. However, the negative CARs X  X  X  c k is treated differently, we choose to subtract its confidence value from the total confidence of the corresponding class c k .
In this section, we evaluate the proposed associative clas-sifier on 20 datasets from the UCI Machine Learning Repos-itory [11]. In these datasets, the numerical attributes have been discretized by the author of [14], the discretization strategy is different from [24, 25], thus the classification performance may be slightly different from the results re-ported before. All the experimental results are reported as an average over 10-fold cross validation. To have a fair com-parison with some other methods, we also list the classi-fication performance of two rule-based classifier: C4.5 [26] and FOIL [27]; two associative classifier on positive CARs: CBA [25] and CMAR [24]; an associative classifier on both positive and negative CARs: ARC-PAN [7]; a hybrid of rule-based and associative classifier: CPAR [39]; Na  X   X ve Bayes [28] and SVM [17]. The parameters of all these classifiers follow the default settings as the original papers.
First, we show the classification accuracy by different meth-ods on 20 UCI datasets. The experimental results are shown in Table 1. Columns 2-9 list the classification accuracies of these compared methods. In Column 10 , classification ac-curacy is determined by the best matching rule. Columns 11-12 show the performance with only positive CARs on two classification methods, SUM and AVE, while Columns 13-14 list the classification results when negative CARs are also considered.

As can be observed, rules+-with SUM gets the best over-all classification performance (82.7%) and wins 4 out of 20 datasets, followed by rules+ with SUM (82.1%). Both of their average classification accuracy outperform the others. Then we compare three different classification strategies, SUM, BEST as well as AVE. The classification method SUM is better than the other two classification methods BEST and AVE by winning around 2%-4% average classification accuracy, no matter with only positive CARs or with both positive and negative CARs. When the other two classi-fication methods BEST and AVE are used, the associative classifier still performs comparably to other well-known rule-based and associative classifiers on average of 20 datasets.
To validate the effect of negative CARs in the associative classifier, we compare rules+-with SUM and AVE to their corresponding alternatives: rules+ with SUM and AVE. The average classification accuracy is higher when negative CARs are included. We also compare the count of wins and losses of rules+-when it is measured against rules+ on SUM and AVE classification methods, when the negative CARs are integrated, both of them win their positive alternatives by 12 times and only loses 2 and 3 times, respectively. It demonstrates the power of negative CARs. They indeed help us get more reliable and more accurate classification results on most datasets.
In the rule pruning phase, due to the absence of negative rule pruning strategies in the literature, we propose a novel rule pruning method to prune positive and negative CARs simultaneously. We compare the classification performance of three different scenarios: prune both positive and negative CARs, prune only positive CARs and without rule pruning. The comparison is performed on SUM classification meth-ods. In Table 2, Columns 2-4 show the classification re-sults of these three scenarios on SUM classification method. The average accuracy of Column 2 (prune both positive and negative CARs) is the highest and it wins 18 out of 20 datasets. Therefore, the proposed rule pruning method not only reduces the number of CARs in the classifier, but also improves the classification performance compared to the associative classifier pruning only positive CARs and the as-sociative classifier without rule pruning phase.
From Table 1, we can conclude that the associative classi-fier built with positive and negative CARs is an as good or even better associative classifier compared with other well-known classifiers and the associative classifier with only pos-itive CARs; the SUM classification method is better than BEST and AVE. In Table 2, we show the proposed rule Table 2: Comparison of rule pruning strategies.
 pruning strategy is effective. These conclusions are obtained mainly by measuring average classification accuracies and winning times. Although they give us some intuition about the lead of a certain classifier, a certain rule pruning or a classification strategy, the conclusion is not forceful since the dominance is unsurpassed over all 20 datasets.

To better validate the conclusions we get, we use Dem-sar X  X  [18] method, conducting a set of non-parametric sta-tistical tests to compare different classifiers over multiple datasets. In the first step, Friedman test is applied to mea-sure if there is a significant difference between different clas-sification models on Table 1. We first rank different classi-fiers on each dataset separately, r j i denotes the rank of the j -th of k classifiers on i -th of N datasets. Then the average rank of j -th classifier is computed as: In the null hypothesis, the average ranks of different classi-fiers are equivalent, and the Friedman statistic is: with k  X  1 degrees of freedom, when N &gt; 10 and k &gt; 5. If the Friedman statistic exceeds a critical value, the null hypothesis is rejected and we conduct post-hoc tests to make pairwise comparisons between different classifiers, otherwise, there is no statistically significant differences among the k classifiers over these N datasets.

The Friedman statistics of 13 classifiers from Table 1 ex-ceeds the critical value, so we continue to use Wilcoxon signed-ranks test to compare the differences between dif-ferent classifiers pairwisely. In Wilcoxon signed-ranks test, suppose d i denotes the classification accuracy difference on the i -th of N datasets. We then rank the difference d i cording to their absolute values, if ties occur, average ranks are assigned. Next, the sum of ranks R + , R  X  are calculated on datasets in which the second classifier outperforms the first classifier and the first classifier outperforms the second classifier, respectively: Let T be the smaller value of these two sums, when N  X  20, Wilcoxon W statistic tends to form a normal distribution, then we can use z -value to evaluate the null hypothesis that there is no statistical difference between these two classifiers. The z -score is: If z &lt;  X  1 . 96 then the corresponding p -value is smaller than 0.05, therefore, the null hypothesis is rejected.
 A series of Wilcoxon signed-ranks test from Table 1 and Table 2 are listed in Table 3. It shows the count of wins, losses, ties and corresponding p -value for pairwise post-hoc comparisons. Rows 2-9 show the comparisons of our asso-ciative classifier rules+-(SUM) with the other 8 well estab-lished classifiers. Our associative classifier always wins more than half of the 20 datasets, but the only strong conclusion we draw is that our method is significantly better than C4.5 and ARC-PAN. ARC-PAN is an associative classifier most similar to our method which also uses the negative CARs, however, it fails to consider the statistical dependency of the discovered CARs. The statistically significant difference between our method and ARC-PAN is very appealing. It shows the power of introducing statistical dependency in the associative classification problem. Rows 10-11 show SUM method is significantly better than BEST and AVE. Through Rows 12-13 , we can find that when the negative CARs are included, the associative classifier is significantly better than that with only positive CARs. Rows 14-16 in-dicate the effect of the proposed rule pruning strategy, the difference between pruning only positive CARs and with-out pruning is not statistically significant although pruning only positive rules wins 14 times. But when we also prune negative CARs, the classification performance is greatly im-proved, the p -value is very small. Therefore, by applying the proposed rule pruning strategy, we get a much better classifier with higher accuracy and fewer rules.
To test the efficiency of the proposed associative classi-fier, we compare the running time of rules+-(SUM) with three associative classifiers, CBA, CMAR and ARC-PAN. We only report the results on 6 datasets due to space limit. The results are shown in Table 4. As can be seen from the table, the proposed classifier is faster than the three con-tender associative classifiers in many cases.
In this paper, we introduce a novel associative classifier which is built on statistically significant positive and neg-ative CARs. The proposed associative classifier consists of three steps: rule generation, rule pruning and rule classifica-tion. In the first phase, we extend the Kingfisher algorithm by pushing the rule constraint in the rule generation phase to enable the discovery of statistically significant positive and negative CARs. After the rule generation step, there are still many noisy CARs which may jeopardize the classi-fication phase or overfit the model, therefore we propose a novel rule pruning strategy to prune both positive and neg-ative CARs simultaneously. At the last step, we present and compare different rule classification methods to ensure the correct prediction of unlabeled data.

The experimental results are very encouraging. Our asso-ciative classifier achieves a comparably good or even better classification result when measured against other classifiers. Meanwhile, it is also computational efficient. By integrat-ing negative CARs in the classifier, the classification per-formance indeed improves compared to the classifiers built with only positive CARs. We also propose a novel rule pruning strategy to prune positive and negative CARs si-multaneously. The pruning not only reduces the number of CARs, but also greatly improves the performance of the classification. Three different rule classification methods are presented and compared, the SUM method works best, it in-dicates that to classify a new instance, we should sum up the confidence values of all matching rules and make a class label prediction based on the summations from different classes. A two step statistical test are used to validate all these con-clusions.

In the future, we aim to further improve the Kingfisher algorithm to enable the discovery of positive and negative CARs more efficiently. Another challenging task is to find more effective rule pruning strategies post rule generation to further improve the classification performance. [1] C. C. Aggarwal and P. S. Yu. A new framework for [2] C. C. Aggarwal and P. S. Yu. Mining associations [3] R. Agrawal, T. Imieli  X nski, and A. Swami. Mining [4] R. Agrawal and R. Srikant. Fast algorithms for mining [5] L. Antonie, J. Li, and O. Zaiane. Negative association [6] M.-L. Antonie and O. R. Za  X   X ane. Text document [7] M.-L. Antonie and O. R. Za  X   X ane. An associative [8] M.-L. Antonie and O. R. Za  X   X ane. Mining positive and [9] M.-L. Antonie, O. R. Za  X   X ane, and R. C. Holte. [10] B. Arunasalam and S. Chawla. Cccs: a top-down [11] K. Bache and M. Lichman. UCI machine learning [12] R. J. Bayardo Jr. Brute-force mining of [13] S. Brin, R. Motwani, and C. Silverstein. Beyond [14] F. Coenen. The LUCS-KDD software library. [15] W. W. Cohen. Fast effective rule induction. In Proc. [16] C. Cornelis, P. Yan, X. Zhang, and G. Chen. Mining [17] C. Cortes and V. Vapnik. Support-vector networks. [18] J. Dem X sar. Statistical comparisons of classifiers over [19] R. O. Duda, P. E. Hart, et al. Pattern classification [20] W. H  X  am  X  al  X  ainen. Efficient discovery of the top-k [22] J. Han, J. Pei, and Y. Yin. Mining frequent patterns [23] Y. S. Koh and R. Pears. Efficiently finding negative [24] W. Li, J. Han, and J. Pei. Cmar: Accurate and [25] B. Liu, W. Hsu, and Y. Ma. Integrating classification [26] J. R. Quinlan. C4. 5: programs for machine learning , [27] J. R. Quinlan and R. M. Cameron-Jones. Foil: A [28] S. Russell and P. Norvig. Artificial intelligence: a [29] A. Savasere, E. Omiecinski, and S. Navathe. Mining [30] W.-G. Teng, M.-J. Hsieh, and M.-S. Chen. On the [31] W.-G. Teng, M.-J. Hsieh, and M.-S. Chen. A [32] D. R. Thiruvady and G. I. Webb. Mining negative [33] F. Verhein and S. Chawla. Using significant, positively [34] H. Wang, X. Zhang, and G. Chen. Mining a complete [35] J. Wang and G. Karypis. Harmony: Efficiently mining [36] G. I. Webb. Discovering significant rules. In Proc. of [37] G. I. Webb. Discovering significant patterns. Machine [38] X. Wu, C. Zhang, and S. Zhang. Efficient mining of [39] X. Yin and J. Han. Cpar: Classification based on
