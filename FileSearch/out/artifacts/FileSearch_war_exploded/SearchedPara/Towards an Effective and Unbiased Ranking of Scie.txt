 It is important to help researchers find valuable scientific papers from a large literature collection containing information of au-thors, papers and venues. Graph-based algorithms have been pro-posed to rank papers based on networks formed by citation and co-author relationships. This pa per proposes a new graph-based ranking framework MutualRank that integrates mutual reinforce-ment relationships among networks of papers, researchers and venues to achieve a more synthe tic, accurate and fair ranking result than previous graph-based methods. MutualRank leverages the network structure information among papers, authors, and their venues available from a literature collection dataset and sets up a unified mutual reinforcement model that involves both intra-and inter-network information for ranking papers, authors and venues simultaneously. To evaluate, we collect a set of recom-mended papers from websites of graduate-level computational linguistics courses of 15 top universities as the benchmark and apply different methods to estimate paper importance. The results show that MutualRank greatly outperforms the competitors in-cluding PageRank, HITS and CoRank in ranking papers as well as researchers. The experimental results also demonstrate that ven-ues ranked by MutualRank are reasonable. H.3.3 [ Information Search and Retrieval ]: Retrieval models. Algorithms, Experimentation. Mutual reinforcement, iterative ranking, time distortion Researchers and students new in a particular area often feel at a loss when facing an ocean of papers published year after year. Researchers often need to answer the following questions.  X  X hich classical papers in a particular area are the most influen-tial and valuable to read for building consolidated background knowledge? Which papers and researchers are worth being paid attention to so as to catch up recent advance? And which venue is the most suitable for one to publish so as to maximize personal capacity? X  Of all the three questions, the former two are more difficult to answer because of the huge numbers of papers and researchers. For example, many digital libraries like DBLP CiteSeer 2 contain paper metadata such as authors and venues, but it is hard for people to manually ex tract a series of important pa-pers or authors for a given area. To rank authors, papers and jour nals, citation count information has long been used, e.g., [6] [9] and [15] for journal ranking, h-index [7] for researcher ranking and citation count for paper rank-ing. But only using citation count based metrics for evaluation is still a controversial issue as most of these methods do not consid-er the network structure of literature information available. Re-cently, graph-based methods have been developed for literature ranking [3] [5] [13] [19] [23-24] [27]. They model a literature collection as a network and apply iterative computation over the adjacency matrix of the network to achieve a converged ranking vector for objects, just as PageRank over Web pages [2]. Most of these works focus on only one type of network, which limits their effectiveness in ranking objects. Recent works begin to consider multiple networks for ranking [4] [18] [21] [25-26]. For example, one of the most relevant research CoRank combines the citation network and the corresponding co-authorship network to achieve better ranking results for both authors and documents [26]. However, current research still does not fully leverage the availa-ble information of a general literature dataset which contains pa-pers, authors and venues. All of the previous works on ranking papers and authors only utilize th e paper and/or author metadata. But venue information is also important for ranking because we often assign reasonable importance values to new papers in pres-tigious venues with few citations and to young scientists with few collaborators. Besides, previous works assign only one type of importance values to each type of objects. But in many cases, different types of objects may, in nature, play different roles in a network and thus have different ki nds of importance values. In the meanwhile, how to evaluate different ranking methods is still a problem. There lacks a common standard for ranking performance evaluation. Citation counts and fu ture downloads are two predom-inant metrics widely adopted in previous research. However, we argue that it is researcher who is the most appropriate for judging the relevance of papers and importance of authors. Thus any rea-sonable ranking algorithm should re turn many papers that are highly recommended by domain experts when user needs are taken into consideration.
 To overcome these limitations, we propose a new ranking frame-work, MutualRank, which employs mutual reinforcement rela-tionships across networks of papers, researchers and venues to achieve a more synthetic and reasonable ranking of papers, au-thors and venues simultaneously. The intuition of mutual rein-forcement is that a higher value of x will lead to a higher value of y and vice versa. For instance, HITS [10] can be viewed as a mu-tual reinforcement process betw een authority and hub values of nodes. Mutual reinforcement is a ubiquitous phenomenon in real-world networks and has already been successfully studied and used in several works, e.g., the preferential attaching model de-picts the features of complex networks in physical world [1]. It is also widely found that high degree nodes in social networks tend to link to other high degree networks [16]. In real applications, mutual reinforcement can be used for finding specific patterns. Jensen et al. use reinforcement relationships to mine significant places: places visited by authoritative users are significant and users visiting significant places gain authorities [8]. Compared to viewing ranking itera tion as a random walk process, it is more instructive to take it as a reinforcement process. In Mu-tualRank, three basic networks are constructed to capture the intra-network influences betw een papers, authors and venues respectively. Moreover, mutual reinforcement information across these networks are defined as follows: a paper which is written by an important researcher and published in a prestigious venue should be ranked high; a researcher who publishes highly ranked papers on prestigious venues gains personal importance; and a venue where important researchers publish highly ranked papers will be prestigious. Based on inter-network mutual reinforcement relationships, we integrate all these networks into one unified ranking framework to find importa nt papers, authors and venues simultaneously. A synthetic transition matrix is then built for the iterative computation of ranks of different objects. This paper uses the ACL (Association of Computational Linguis-tics) Anthology Network (AAN) 3 [20] dataset for evaluating rank-ing algorithms (the latest version till March 2011). AAN consists of all the metadata of ACL Anthology, a collection of papers published in journals and conferences hosted by the ACL. The dataset contains 18041 papers published in 273 venues and au-thored by 14386 researchers. From the AAN dataset we can ob-tain the authors, publication venue and reference list of a paper. For joint conferences such as ACL-COLING 2006, we treat the corresponding published papers as being published in more than one venue. For example, the paper  X  X 06-1001 X  belongs to both ACL X 2006 and COLING X 2006. If a conference accepts papers other than long, regular or full papers, we construct a new venue X-Companion for those papers in this conference, where X stands for the conference name. For example, there are two venues for the annual conference of ACL: ACL for full papers and ACL-Companion for short and student papers. For ranking papers. To evaluate the results of paper ranking, we collect papers from the reading lists of graduate-level courses in natural language processing or computational linguistics of top universities, and record the number of times each paper is rec-ommended as the basic benchmark dataset BenchP . We discard those reading lists focusing only on a limited number of subareas of computational linguistics and only include those with an exten-sive coverage of the whole area. If a paper in the AAN dataset is on k universities X  reading lists, i.e., receives k recommendations, its importance is k . If a paper receives recommendations from more universities, it is regarded as more relevant and more im-portant in the area of computational linguistics. In 218 papers collected from 15 universities, 181 are recommend-ed once, 28 are recommended twice, 6 papers receive 3 recom-mendations, and the rest 3 papers occur 4, 7 and 8 times respec-tively on the reading lists. We believe the established benchmark well reflects the consensus of the research community because: (1) the benchmark papers are all recommended by prestigious re-searchers from world famous uni versities including MIT, CMU, Stanford, Cornell etc.; (2) although most papers has only one recommendation, the fact of winning recommendation is still an indicator of influence to a certain degree; (3) the benchmark set is consistent with the reference section of many survey papers and standard text books such as  X  X peech and Language Processing X . For ranking researchers. We construct two different benchmark collections for researchers. The first benchmark BenchR1 is con-structed from BenchP . If an author has at least 2 papers in BenchP , the author is put in BenchR1 . There are in total 46 researchers in BenchR1 . Four researchers are each recommended 16, 13, 8 and 7 times. Five occur on the reading lis ts 6 times. Six researchers win 5 and another six researchers get 4 recommendations. Each of the remaining 25 researchers gets 3 recommendations. The second benchmark collection for researchers BenchR2 consists of the first 100 top-cited authors of AAN. 4 We build three basic types of networks from the AAN dataset: Paper Influence Network (PIN), Research Influence Network (RIN) and Venue Influence Netw ork (VIN). Figure 1 gives an example of the network construction. Network of papers. We construct the PIN based on the citation relationship. For each paper p i in AAN, we add a corresponding node i to PIN. If paper p i cites another paper p ( i , j ) to PIN. PIN is un-weighted. Network of researchers. We use citation relationships to build the weighted directed RIN. For each researcher r i in AAN, we add a node i to RIN. If r i has a paper that cites another paper authored assigned as follows. Let and P ( r i ) be the set of papers written by researcher r of edge ( i , j ) in RIN is set as The other type of researcher network is the Researcher Collabora-tion Network (RCN). RCN is a we ighted undirected network. If two researchers r i and r j have coauthored at least one paper, there is an edge ( i , j ) in RCN. The weight of edge ( i , j ) in RCN is set as Different from CoRank, we use RIN rather than RCN for ranking researchers. The motivation behi nd RCN is that the collaboration with an important researcher on some papers will raise the im-portance of researcher him/herself. However, we argue that if papers of a researcher are highly cited, we are confident that he/she is influential because his/her ideas may inspire a lot of other researchers in their own work. In Section 4, we will show that performances are slightly improved by substituting RIN for RCN. It should be noted that our framework is flexible enough for incorporating different ways of modeling researcher network. Network of venues. We construct the VIN to incorporate venue information. VIN is a weighted directed network such that if a paper p k published on venue v i has another paper p Let P ( v i ) be the set of papers published in v edge ( i , j ) in VIN is set as follows. Let P ( v published in v i . Then the weight of edge ( i , j ) in VIN is set as Before delving into the details of MutualRank, we first make a preliminary study of the limitati ons of two classic ranking algo-rithms, PageRank and HITS, on ranking papers and discuss the causes of these limitations. Typically, similar phenomena appear in simply applying these algorithms on RIN (resp. VIN) for rank-ing researchers (resp. venues). The results obtained in this section well motivate the proposed MutualRank method. Table 1 lists the PageRank results that match BenchP . We only show the results from the top 100 answers returned by each algo-rithm. The matched ranking result of a paper is given in the for-mat  X  o p : pname [ c p ] X , where o p is the ranked order, c ommendation count in BenchP , and pname is the paper id used in ACL anthology. For example,  X 4:J90-2002[1] X  means that the paper J90-2002 is ranked 4th by PageRank and is recommended once in BenchP .  X 7:J93-2003[7] X  is ranked 7th by PageRank and is recommended by 7 times in BenchP . Relevant papers published in the 1990s and 1980s are represented in boldface and italic re-spectively in Table 1 The Problem of Time Distortion. In PageRank, paper ranks are transferred from one paper to the other through the citing relation-ships. Thus, papers published in earlier years will inevitably gain higher ranks than new papers. This phenomenon is clearly reflect-ed by Table 1. In all the 19 results, only 3 are published after year 2000 (P02-1040, A00-2018 and J02-3001). This contradicts to our experience that researchers not only read old classic papers but also pay much attention to recent papers to keep up with the fast-changing research fronts. We call this problem as time distortion . 4:J90-2002[1] 29:P02-1040[2] 70:J93-2006[1] 7:J93-2003[7] 33:P97-1003[3] 75:J02-3001[2] 8:J86-3001[1] 40:A00-2018[1] 84:P98-2127[1] 12:J96-1002[2] 49:J95-4004[ 1] 88:P97-1023[1] 13:J92-4003[1] 54:J96-2004[1] 96:P02-1053[1] 18:J88-1003[1] 60:P95-1026[4] 25:W96-0213[2] 65:C96-2141[1] We use a randomized version of HI TS (RHITS) from [17] in this study because RHITS typically returns the same results with the HITS by Kleinberg [10] and is more easy to formulate and ana-lyze in our context. RHITS can be formalized as follows. Let P be the adjacency matrix corresponding to PIN and  X  P is the corre-sponding transition matrix obtained by normalizing each row of P . Similar to PageRank, random jump (aka. teleportation) is incorpo-rated by rewriting the transition matrix  X  P into P as follows: where n P is the number of papers in PIN, d r is an n vector indicating which row of  X  P is zero, e is an n identity vector and (1- X  ) is the dangling factor controlling the probability in which the rank of a paper is evenly distributed onto every paper in the network. Similarly, P T is rewritten into T P . Let paut and phub be the authority and hub vectors of papers in PIN. We formulate RHITS using a similar way as SALSA [12] which models the ranking process as two independent random walks as in Eq. (6  X  7). Shown in Table 2, RHITS overcom es the problem of PageRank to some extent. (1) RHITS returns more relevant papers than Pag-eRank. (2) 20 out of total 27 returned papers are in year after 2000. The second point is due to the mutual reinforcement nature of HITS. Unlike PageRank where ranks flow uni-directionally from new papers to old papers (forward flow), RHITS has the mechanism to let the ranks of old papers flow back to new papers (backward flow). Therefore, new papers have much higher proba-bilities for being authoritative in RHITS than in PageRank. But Table 2 also shows that RHITS overestimate the values of new papers. Firstly, no papers in the 1980s are returned by HITS alt-hough many pioneering work in computational linguistics are done during this period. Secondly, it misses many important pa-pers in the 1990s, the decade of statistical natural language pro-cessing which greatly boost the prosperity of the area and incu-bates many applications and new research problems in the new century. However, RHITS only returns 7 papers in this period. In conclusion, RHITS performs better than PageRank in ranking on PIN because RHITS assigns two different kinds of paper ranks and the two types of ranks mutually reinforce each other to allevi-ate time distortion. Thus, mutual reinforcement may play an im-portant role in improving ranking precision. However, the fact that RHITS may overestimate new papers is in fact another type of time distortion. It means that using PIN only for ranking papers may inevitably suffer from time di stortion. All these findings lead us to develop a new ranking method. 1:P02-1040[2] 29:A00-2018[1] 58:W96-0213[2] 4:J93-2003[7] 30:P03-1054[2] 59:P06-1096[1] 7:P05-1033[1] 33:P07-1019[1] 60:P05-1012[1] 9:J04-4002[2] 38:N04-1021[1] 75:D07-1090[2] 14:C96-2141[1] 40:P05-1074[2] 77:E06-1032[1] 24:J96-1002[2] 49:P05-1074[1] 79:P01-1030[2] 26:J90-2002[1] 51:D07-1091[1] 85:P06-1055[3] 27:N06-1014[1] 52:P05-1022[2] 89:J92-4003[1] 28:W06-1606[1] 57:P97-1003[3] 97:J04-2003[1] We develop the MutualRank framework for ranking papers, re-searchers and venues simultaneously. The MutalRank algorithm employs mutual reinforcement relationships between papers, re-searchers and venues to improve the precision of recommending valuable papers. In MutualRank, the importance measures of dif-ferent objects are as follows: Authority paut ( p i ) depicts whether this paper is of high influence and promotes research advancements in its corresponding area. Soundness psnd ( p i ) describes whether this paper grounds itself on a thorough study of major previ ous works. Authority and sound-ness correspond to the authority and hub values in the traditional HITS algorithm. We use soundness instead of hub to make the semantics of this importance measure clearer in our context. Low authority implies that this paper is somewhat less important or inspiring. High soundness indicates that this paper might be a good reference for background knowledge of a particular area. For each paper p i published in venue v k and each researcher r authoring p i , the mutual reinforcement relationships are as below: (1) If a researcher r j is important, a paper p high authority and high soundness, i.e., paut ( p i )  X  rimp ( r P ( r j ), and if a paper p j is of high authority and high soundness, the researchers r i authoring p j receives high importance, i.e., rimp ( r  X  paut ( p i ) and rimp ( r j )  X  psnd ( p i ) if p i  X  P ( r (2) For a paper p i published in venue v k , p i is of high rank if v is prestigious and v k gains more prestige if p i is ranked high, i.e., paut ( p i )  X  vprs ( v k ), psnd ( p i )  X  vprs ( v k vprs ( v k )  X  psnd ( p i ). (3) Similarly, for a researcher r j authoring some papers in ven-ue v k , we have rimp ( r j )  X  vprs ( v k ) and vprs ( v
To compute the ranks of objects we need to consider both intra-network relationships and inter-network relationships. Intra-network ranking iteration is modeled by using PIN, RIN and VIN respectively. The classical PageRank is applied to RIN and VIN. To compensate the biases of PageRank and RHITS towards old and new papers respectively and improve ranking on PIN, we derive the First-Order HITS as follows. and psnd ( t +1) only relies on paut ( t ) . We call this Zero-Order HITS , which means paut (resp. psnd ) does not inherit its histori-cal values directly. The name is borrowed from notations of Mar-kovian processes. However, in First-Order HITS , paut (resp. (8)  X  (9)). More specifically, paut ( t +1) inherits  X  paut nearest historical value, while the remaining comes from psnd from  X  psnd ( t ) , while the remaining is reinforced by paut  X  Ppaut . Eq. (8) and (9) can be rewritten into the following matrix equation, where r = [ paut T , psnd T ] T and M matrix. M is a transition matrix corres ponding to a Markovian process. M is irreducible because there is only one communicating class aperiodic because there exists self-loops in the transition matrix. And M 1 is clearly also a stochastic matrix. So according to Fro-benius theorem [11], r calculated in Eq. (10) converges to the principle eigenvector of M 1 with the principle eigenvalue being 1. For incorporating the mutual reinforcement relationships between different networks, 3 undirected networks are constructed: Paper-Researcher Network (PRN), Pa per-Venue Network (PVN) and Researcher-Venue Network (RVN). They are constructed as fol-lows: If a researcher r i has written a paper p j published in venue v ( i , k ) to RVN. Then, we obtain two transition matrices for each of the above three networks as follows. Let PR (resp. RP ) be the adjacency matrix from PIN (resp. RIN) to RIN (resp. PIN). We have Let PV and VP be the adjacency matrices coupling PIN and VIN together. We have Finally, RV and VR are the adjacency matrices between RIN and VIN and we have Now we have totally 9 adjacency matrices from the 6 networks which capture intra-network ranking iteration and inter-network mutual reinforcement. Similar to Eq. (5), we create the corre-sponding intra-network transition matrices P , R , V and inter-network transition matrices PR , RP , PV , VP , RV and 
VR . Thus, we can couple the First-Order HITS on PIN and Pag-eRank on RIN and VIN into a unifi ed framework as follows (see Figure 2 for illustration). Taking initial vectors of paut, psnd, rimp and vprs as inputs, the trilateral mutual reinforcement pro-cess updates these vectors at each iteration as follows. 1. Value of paut at time t reinforces values of paut, psnd, rimp 2. Value of psnd at time t reinforces values of paut , psnd , rimp 3. For rimp at time t to reinforce paut , psnd , rimp and vprs at To summarize, MutualRank is formulated in Eq. (14)  X  (17). + (1  X  )(1  X  ) paut paut P psnd psnd P paut psnd rimp PR paut PR psnd vprs PV paut PV psnd Eq. (14)  X  (17) can be rephrased as follows: where r = [ paut T , psnd T , rimp T , vprs T ] T , and  X  1 is a diagonal matrix with each  X  i , i = 1 ( i = 1, ..., n tion matrix corresponding to a Markovian process. Similar to Eq. stochastic matrix. So r converges to the principle eigenvector of M with principle eigenvalue of 1. Given initial values of r , Mutu-alRank iteratively updates r until the convergence criterion  X  r  X   X  is met for some small  X  . The initial values of paut , psnd, rimp and vprs are set to e p /(2 n P + n R + n V ), e p n ), and e v /(2 n P + n R + n V ) respectively, where e p n -and n V -dimensional unit vectors respectively. In evaluation, we do not use the traditional IR metric Precision-at-k [14] because there exists a partial order between paper and researcher pairs. What X  X  more, each benchmark paper has a rec-ommendation count and our design of evaluation metric should Figure 2. Mutual reinforcement relationships between paut, psnd, rimp and vprs. reflect this information which cannot be reflected by Precision-at-k . Meanwhile, it is also hard to employ Normalized Discounted Cumulative Gain (NDCG@ k ) in our context although NDCG@ k is almost a de facto metric for ranking evaluation beyond binary relevance. This is because we are estimating the importance of papers in the whole computational linguistics area, and even for human, it is very difficult to judge the levels of relevance. The performance metric we use is called Recommendation Intensity . the recommendation intensity of p at k , denoted as RI ( p )@ k , is defined as where c p is the number of recommendations of p in BenchP and o is the ranked order of p in P . Eq. (20) means that if a paper p has larger recommendation counts (with larger c p ) and is ranked high-er (with smaller o p ), then its recommendation intensity RI ( p )@ k is higher. log( c p ) is a controlling factor for preventing a few papers with extremely high recommendati on counts from dominating the value of recommendation intensity of the whole result set. (1  X  o is to reflect the fact that a returned relevant result p (i.e. p is also cause users tend to look at only the first several items in the re-turned result list. Thus, the higher the rank of a relevant paper is, the more chances it has to be read by the user. Recommendation intensity of P at k , denoted as RI ( P )@ k , is defined as The bigger RI ( P )@ k is, the better a ranking method is. RI ( P )@ k is an extension to Precision -at -k . In fact, if we ignore recommenda-tion counts, RI ( P ) @k degenerates to Precision -at -k . Recommen-dation intensity is also used for ranking researchers. Based on mendation intensity of r at k is defined as where c r is the recommendation count of r in BenchR1 and o the order of r in R . The recommendation intensity of R at k is For evaluation using BenchR2 , if a researcher r is the o the top-k result list R and the g r -th in BenchR2, then and RI 2 ( R )@ k is defined as to RI 1 ( R ). We compare MutualRank with PageRank, RHITS and CoRank using the above benchmark datasets and metrics. CoRank works on both PIN and RCN. CoRank is rephrased as follows, where r = [ prnk T , rimp T ] T , prnk T and rimp T for papers and researchers respectively. The superscripts l is the number of times a random surfer does inter-network walk, and m and n denote the number of intra-network random walks. We first demonstrate how MutualRank compensate the problem of time distortion compared to PageRank and RHITS. Table 3 lists the relevant returned results of MutualRank under parameter settings that  X  = 0.5,  X  = 0.5,  X  = 0.85 and  X  = 1e-5. Out of the top-100 results returned by MutualRank, 36 are verified as relevant according to BenchP . (1) Out of the 36 relevant results, 16 papers (in boldface) are published in the 1990s and 17 papers (in normal font) come from the 2000s whic h demonstrates that MutualRank not only accurately identifies the contributions of the last decade of the 20th century to the development of statistical natural lan-guage processing, but also assigns enough importance to the new century where many practical applications flourish, many new research problems emerge and the progresses in NLP are ever faster than before, armed with the promising tools developed and together with the boom of web science. To better catch up this, readers can refer to Table 5 and verify that all the top-15 papers published in the 1990s are indeed amongst the most influential papers through the years. (2) MutualRank returns extra papers of high recommendations that are lost by RHITS, e.g. P95-1026[4] and W02-1011[3] etc. 
Table 3. Relevant Papers in top-100 MutualRank Results 2:J93-2003[7] 24:P95-1026[4] 63:J01-4004[1] 3:J86-3001 [ 1 ] 25:P03-1054[2] 66:J88-1003 [ 1 ] 4:P02-1040[2] 26:P97-1003[3] 68:J00-3003[1] 5:J96-1002[2] 27:J95-2003[1] 72:H94-1028[1] 8:J90-2002[1] 30:W02-1011[3] 73:P87-1022 [ 1 ] 13:J92-4003[1] 33:W02-1001[2] 76:C96-2141[1] 15:J96-2004[1] 38:P02-1053[1] 79:P06-1055[3] 17:A00-2018[1] 43:P05-1022[2] 80:J01-2001[3] 18:J95-4004[1] 45:N03-1028[1] 82:P05-1012[1] 19:J02-3001[2] 47:J98-1004[1] 86:J93-2006[1] 21:P98-2127[1] 52:P05-1033[1] 89:J94-2001[2] 23:W96-0213[2] 58:J04-4002[2] 95:P02-1035[1] Figure 3 shows the results of ranking papers. For Figure 3(a), we compare the results of each competitor algorithm with BenchP . For Figure 3(b), we use a subset of papers in BenchP with at least two recommendations. To obtain the results in Figure 3, we set the parameters as follows, (i) for MutualRank  X  = 0.5, (ii) for Figure 3. Recommendation intensity curves of different meth-ods on ranking papers. CoRank, m = 2, n = 2, l = 1 (parameters that obtain the best per-formances in the original paper [26]), and (iii) for both  X  = 0.5. Results show that MutualRank has the best performance. Taking MutualRank and RHITS for example, there are 20 MutualRank-only relevant papers (those benchmark papers that are only rec-ommended by MutualRank) on the top-100 result list and among them 5 are recommended more than twice. There are only 11 RHITS-only papers and only 2 of them are recommended more than twice. As MutualRank applies HITS-style ranking on PIN, the results verify that using different types of importance values for certain objects does improve ranking performance. An interesting observation is that, although RHITS uses PIN only, it sometimes performs even better than CoRank, e.g. when k  X  40 in Figure 3(a) and k  X  15 in Figure 3(b). After investigating the results of both algorithms we find that RHITS returns more rele-vant results than CoRank and some RHITS-only papers have high recommendation counts in BenchP . A possible explanation is that CoRank uses PageRank for ranking in PIN which fails to reflect the soundness of the paper and thus still biases towards old papers to some extent. The time distortion of CoRank can also be seen from the CR column of Table 5. High rank papers are still mostly old papers and ranks of papers after 2000 are relatively lower. An extreme example is P07-2045 (shadowed in Table 5). Although it is ranked 10th by MutualRank and receives 325 citations in less than 4 years, it is only ranked 102th and 195th by CoRank and PageRank respectively. To understand why MutualRank wins, we take a deep look at the relevant results. We see from Table 4 that, besides more relevant papers by MutualRank, most common (highly recommended) relevant results are ranked higher in MutualRank than in CoRank except for the papers in italic (Notice that these papers are pub-lished in earlier years). There are also 15 MutualRank-only results where 6 papers are recommended more than 2 times in BenchP . Table 4. Shared Relevant Papers of MutualRank and CoRank Paper id #rec Rank Paper id #rec Rank MR CR MR CR J93-2003 7 2 6 W96-0213 2 23 15 J86-3001 1 3 7 P95-1026 4 24 49 P02-1040 2 4 14 P97-1003 3 26 23 J96-1002 2 5 8 W02-1011 3 30 75 J90-2002 1 8 3 W02-1001 2 33 77 J92-4003 1 13 9 P02-1053 1 38 61 J96-2004 1 15 30 J88-1003 1 66 19 A00-2018 1 17 25 H94-1028 1 72 50 J95-4004 1 18 32 C96-2141 1 76 64 J02-3001 2 19 62 J93-2006 1 86 67 P98-2127 1 21 68 Figure 4 shows the results of Mu tualRank under different differ-ent parameter settings and network models. We include CoRank for comparison with  X  varying from 0 to 1 with parameter step of 0.02 (see Eq. (25)). To see how researcher and venue information helps improve ranking performance, we fix  X  in MutualRank to 0.5 and vary  X  from 0 to 1 with parameter step of 0.02. For a fair-er comparison, we do not use venue information. MutualRank thus degenerates to BiRank in Figure 4(a). In both MutualRank and CoRank, parameter  X  governs how much information outside the PIN is incorporated for improving ranking. The larger  X  is, the more researcher information is used. When  X  = 0 CoRank degen-erates to PageRank and MutualRank degenerates to First-Order Figure 4. Parameters and network models on ranking perfor-mance (BiRank: MutualRank without venue information; TriRank: MutualRank with venue information). HITS.  X  = 1 means that, loosely speaking, paper rank in both Mu-tualRank and CoRank is the aggregated importance of its authors. As is expected, when  X  increases from 0 to 1, ranking performances of both MutualRank and CoRank increase. This means adding metadata information such as research (or venue) influence net-work indeed improves ranking of papers. This performance gain peaks at some points (around  X  = 0.46 for MutualRank and  X  = 0.20 for CoRank) and turns to decrease when  X  becomes too large. The results in Figure 4(a) show that using First-Order HITS in-stead of PageRank on PIN helps improve ranking performance a lot. Figure 4(b) shows the influence of network model on recom-mendation intensity, where both  X  and  X  are fixed to 0.5. We compare two versions of Mutual Rank and two versions CoRank. On the whole, performances using RIN is slightly better RCN. To get an intuitive understanding of MutualRank, we list the top-15 papers returned by MutualRank and its competitors in Table 5. We include the ranks by CoRank, PageRank, RHITS and #cite (citation count) for comparison. It is clear that the ranks given by MutualRank are quite reasonable. We also see how severely Pag-eRank and RHITS bias towards old and new papers respectively (see the boldfaced items). The results of CoRank are in a sense close to PageRank. The IDs of papers are given so that interested readers can find them on the AAN website for more information. For ranking researchers, we compare MutualRank to CoRank. The recommendation intensity curves in Figure 5 show that Mu-tualRank outperforms CoRank by at most 53.8% under BenchR1 and 19.7% under BenchR2 . Taking BenchR2 for example, in the top-10 results returned by MutualRank, two matches BenchR2 . They are r (5) =  X  X ella Pietra, Vincent J. X  and r (10) =  X  X anning, Christopher D. X , but none of CoRank X  X  top-10 results matches BenchR2 . Moreover, compared to CoRank results, most MutualRank ranks are closer to their benchmark orders. For example, the top 4 researchers in BenchR2  X  X ch, Franz Josef X ,  X  X ey, Hermann X  ,  X  X oehn, Philipp X  and  X  X arcu, Daniel X  are ranked 24th, 19th, 50th and 16th by MutualRank and 45th, 11th, 65th, and 39th by CoRank respectively. Table 6 shows the top-15 researchers by MutualRank with their benchmark and CoRank ranks. The columns  X  X ank in BenchR1  X  and  X  X ank in BenchR2  X  are in the form of o r (# rec ) and r (# cite ), respectively, where o r is the order of r in the benchmark, # rec and # cite are the number of recommendations in BenchR1 and number of incoming citations in BenchR2 respectively. Two lines in bold-face are exact match of BenchR2 by MutualRank. Exact match by BenchR1 is italicized. The symbol  X --(--) X  means that the returned researcher is not in BenchR1 . Although it is difficult to give a consolidated quantitative metric for verifying venue ranking, the results show that the top venues returned by MutualRank conform to our common knowledge on conference and journal reputations. Among all the 273 venues, the top 10 are v (1) = ACL, v (2) = COLING, v (3) = CL, v v = HLT, v (6) = NAACL, v (7) = wDSANL, v (8) CoNLL, and v (10) = EACL. wDSANL is a shorthand for  X  X ork-shop on Speech and Natural Language X  and CL stands for  X  X our-nal of Computational Linguistics X . Following are some other re-puted conferences including v (11) = LREC, v (14) IJCNLP, v (18) = NLG, v (19) = wSMT, v (20) = VLC. MU is short for the  X  X essage Understanding Conference X , NLG stands for  X  X nter-national Conference on Natural Language Generation X , wSMT stands for  X  X orkshop on Statistical Machine Translation X  and VLC abbreviates  X  X orkshop on Very Large Corpora X . Figure 5. Recommendation intensity curves of different meth-ods on ranking researchers.
 To make a qualitative investigation, we compare our results with the 2008 conference ranks given by ArnetMiner 5 [22] and 2 hu-man evaluations (H1 and H2 in Table 7) of 8 well-known venues, and analyze the correlations between these four rankings. As Ta-ble 7 shows, the four rankings are highly correlated with each other. The only significant differ ence lies between MR and H1, where the correlation coefficient of 0.64 is not high enough to be consistent. With in-depth investigation of H1 and MR, we found the problem lies in the following points. Firstly, in MutualRank, a joint conference such as HLT-NAACL is split into two different venues. Such a way assigns extra prestige to less-prestigious con-ferences, that is HLT in this context. We are unable to distinguish between jointly held conferences using AAN metadata only. Sec-ondly, the Journal of Computati onal Linguistics is by all means the most prestigious venue to NLP researchers. However, in Mu-tualRank it is ranked lower than ACL and COLING because the latter two conferences have far more papers published every year and so their aggregated prestige is higher. Very interestingly in MutualRank, ACL-Companion (12th), COL-ING-Companion (13th), HLT-Companion (15th) and NAACL-Companion (17th) all have high prestige too. This phenomenon reveals that, on one hand, researchers often focus on the most influential conferences and thus even the short or demo papers published in these conferences gain many citations, and on the other hand, it is more attractive to important researchers for pub-lishing their short or demo papers on these prestigious confer-ences. Moreover, MutualRank returns some well-known work-shops too, e.g., v (21) = wDAD (SIGDIAL Workshop on Discourse And Dialogue), v (23) = BioNLP (a series of workshops on Natural Language Processing in Biology and Biomedicine). Finally, we study the time distributions of returned papers of dif-ferent ranking algorithms. Figure 6 shows the yearly distributions of the relevant papers in the top-100 papers returned by different algorithms. (1) From the chart we can see that two peaks around year 1996 and 2002 can be identified by PageRank, CoRank and MutualRank but the RHITS algorithm has an obvious bias to-wards new papers after 2000 year. Bias here means favoring cer-tain years or year ranges. PageRank has a strong bias towards older papers and older papers receive citations more easily than new papers. CoRank and MutualRank are less biased in this sense. (2) Around each of these two peaks, MutualRank has a slowly-increasing and slowly-decreasing slope, which is much close to the real development process of a booming area while PageRank and CoRank has sharper busts at the two peaks. From the above, we argue that MutualRank not only captures the booming of sta-tistical natural language in the 1990s but also shows enough re-spect of the flourishing devel opment in the new century. To make a deeper investigation of the problem of time distortion identified in Section3.1.1, we define a new metric recommenda-tion sensitivity for ranking papers. Given a year range Y = [ y and a paper list P returned by some ranking algorithm, for each year y  X  Y , let pub ( y )@ k and pub ( Y )@ k be the percentages of spectively. Let ()@ ()@ | | mendation sensitivity of P at k , denoted as RS ( P )@ k , is defined as where ()@ ()@ ()@ dev y k pub y k pub y k = X  . If dev ( y )@ k = 0, the corresponding log -element in Eq. (7) is set to 0. Recommen-dation sensitivity measures how biased the yearly distribution of returned papers during a year range is. Big recommendation sen-sitivity means that the yearly distribution of the returned papers has very sharp peaks, i.e. biasi ng towards certain periods. What X  X  more, we can see how recommendation sensitivity changes during different periods by choosing different start years of the year ranges with fixed end year and drawing the recommendation sen-sitivity curves corresponding to the year ranges. Figure 7 shows the recommendation sensitivity curves (RSC) of different meth-ods, where we fix the end year as 2010 and vary the start year from 1981 to 2006 with step = 5. Figure 7(a) shows the RSCs of relevant papers, i.e. the top-100 papers that match BenchP , while Figure 7(b) illustrates the RSCs of the top-100 papers returned by different algorithms. Parameters of different ranking algorithms are just set as in Figure 3. An unbiased and fair ranking algorithm should have both small recommendation sensitivity values in different year ranges and a flat RSC in the whole time period. From Figure 7(a), we see that RHITS is very time-sensitive. The sharp slope reflects the fact that RHITS heavily biases towards new papers. By contrast, PageRank and CoRank are relatively not that sensitive. However, the sh arp slope from year 2001 to 2006 reveals that PageRank and CoRank both bias towards old papers. On the contrary, the smaller values and flatter slop of MutualRank RSC from year 1986 to 2010 demonstrate that MutualRank is not only less sensitive to changes in year range but also less biased Figure 7. Recommendation sensitivity curves of different towards papers of certain time periods. There are two more points about MuturalRank RSC worth note. (1) The sharp slope from 1981 to 1986 shows that MutualRank returns few papers which are too old. (2) The flat slope from year 2001 to 2006 reveals the fact that MutualRank does not omit some important recent papers as PageRank and CoRank do. To sum up, MutualRank is the most unbiased method for ranking papers. Similar observations can be found in Figure 7(b). The point reserving attention is that, unlike in Figure 7(a), PageRank and CoRank also have sharp slopes from year 1981 to 1986 and from year 1986 to 1991. This is un-derstandable because in the early days fewer papers are published every year, so all the methods have fewer returned results in these years (Note that these returned papers are not necessarily relevant with respect to BenchP ). This paper proposes a new framework MutualRank towards an unbiased ranking of papers. MutualRank employs the mutual reinforcement relationships between papers, researchers and pub-lication venues to fairly rank papers of certain time periods, as well as researchers and venues. It models the intra-and inter-network rankings in a unified way and computes the ranking vec-tors of papers, researchers and venues in an iterative fashion. Using the manually collected benchmark datasets for papers and researchers, we show through experiments that MutualRank out-performs state-of-the-art comp etitors including PageRank, Ran-domized HITS and CoRank in the following aspects: (1) Mutu-alRank returns more relevant highly-ranked papers and research-ers, and (2) results of MutualRank are more unbiased. The Mutu-alRank rankings of venues are verified to be reasonable. We also present a detailed discussion on the problem of time distortion in ranking papers and propose methods for judging biases of ranking. In the future work, we will study the convergence properties of the ranking algorithms, establish a better benchmark, and consider more indicators for evaluation such as awards for ranking papers and researchers and social factors. The idea of MutualRank can be also applied to other network resource ranking scenarios where multiple heterogeneous sub-networks are interwoven with differ-ent node types and edge semantics. For example, in a heterogene-ous social network where friend network, email network, collabo-ration network and affiliation netw ork co-exist, mutual reinforce-ment information can be modeled to help ranking nodes in a syn-thetic way. This work was partially supported by National Science Founda-tion of China (No.61075074 and No.61070183), Natural Science Foundation of Chongqing (No.cstc2012jjB40012), and the Key Discipline Fund of National 211 Project (Southwest University: NSKD11013). We thank Dr. Yang Liu for annotating venue rank-ings. Thanks also go to Dr. Jianmin Yao for venue ranking anno-tation and helpful discussions and comments. [1] Barab X si, A.-L. and Albert, R. 1999. Emergence of Scaling in Ran-[2] Brin, S., and Page, L. 1998. The anatomy of a large-scale hypertex-[3] Chen, P., Xie, H., Maslov, S., and Redner, S. 2007. Finding scienti fi c [4] Das, S., Mitra, P., and Lee Giles, C. 2011. Ranking Authors in Digi-[5] Ding, Y., Yan, E., Frazho, R., and Caverlee, J. 2009. PageRank for [6] Gar fi eld, E. 1972. Citation analysis as a tool in journal evaluation. [7] Hirsch, J. E. 2005. An index to quantify an individual X  X  scientific [8] Jensen, C. S., Cao, X., and Cong, G. 2010. Mining Significant Se-[9] Katerattanakul, P., Han, B., an d Hong, S. 2003. Objective quality [10] Kleinberg, J. M. 1999. Authoritative Sources in a Hyperlinked Envi-[11] Lefebvre, M. 2006. Applied Stochastic Processes . Springer. [12] Lempel, R., and Moran, S. 2001. SALSA: The Stochastic Approach [13] Li, X., Liu, B., and Yu, P. 2008. Time Sensitive Ranking with Appli-[14] Manning, C. D., Raghavan, R., and Sch X tze, H. 2008. Introduction to [15] Nerur, S., Sikora, R., Mangalaraj, G., and Balijepally, V. 2005. As-[16] Newman, M. E. J. 2002. Assortative Mixing in Networks. Phys. Rev. [17] Ng, A. Y., Zheng, A. X., and Jord an, M. I. 2001. Stable Algorithms [18] Ng, M. K., Li, X., and Ye, Y. 2011. MultiRank: co-ranking for ob-[19] Radicchi, F., Fortunato, S., Markines, B., and Vespignani, A. 2009. [20] Radev, D. R., Muthukrishnan, P., and Qazvinian, V. 2009. The ACL [21] Sayyadi, H., and Getoor, L. 2009. FutureRank: Ranking Scientific [22] Tang, J., Zhang, J., Yao, L., Li, J., Zhang, L., and Su, Z. 2008. [23] Walker, D., Xie, H., Yan, K.-K ., and Maslov, S. 2007. Ranking [24] Yan, E., and Ding, Y. 2009. Applying centrality measures to impact [25] Yan, S., and Lee, D.-W. 2007. Toward Alternative Measures for [26] Zhou, D., Orshanskiy, S. A., Zha, H., and Lee Giles, C. 2007. Co-[27] Zhuge, H., and Zhang, J. 2010. Topological Centrality and Its e-
