
Liang Huai Yang 1 , Mingchao Liu 1 , Yifan Pan 1 , Weihua Gong 1 , and Simon Stannus 2 Join is an expensive core operation in DBMSs and its implementation dictates the overall performance of a DBMS. Hash join is popular and shown to outperform sort-merge join in many situations. With the emergence of flash memory, two trends of flash usage in DBMS are currently observed, i.e., flash-only or hybrid flash-HDD systems. Flash-based DBMSs became a hot research topic. A concise review on effec-tive use of SSDs in DBMSs was given in other works [1,7]. All works generally leve-rage the fast random reads of flash SSDs and avoid random writes. Several flash-aware join algorithms for flash-only storage have been proposed. RARE-join [5] uses PAX page layout to reduce I/Os by only fetching the join col-umns and PAX mini-pages while taking advantage of SSDs X  low random read laten-cy. However, authors only address this idea theoretically. DigestJoin [1] is similar to RARE-join except that it uses the traditional row-oriented page layout. FlashJoin[6] is a pipelined multi-table join algorithm optimized for SSDs by using join indexes and late materialization to reduce memory usage and data overflow. Hash join perfor-mance with SSDs in commercial DBMSs was assessed in [4] and the superiority of SSDs was shown. 
The most related work to ours is Seq+[3], which reduced random HDD I/Os in the hash join by using batch-write and write-and read-group techniques, and its effec-tiveness was proved by theoretical analysis. As SSDs fill the latency gap between RAM and HDDs, we believe that it is worthwhile to revisit the issue of reducing ran-dom I/Os by using an SSD as the cache. 
In this paper, we propose a new algorithm called CGHJ (Cached Grace Hash Join) for hybrid drives, which reduces hard disk random I/Os that occur in the partitioning phase of the traditional GHJ by caching the segments of buckets in the SSD and then migrating all these segments of each partition contiguously to the HDD. 
The rest of this paper is organized as follows: Section 2 details the CGHJ algorithm by using an SSD as the cache between the RAM and HDD. Experiments are con-ducted in Section 3 and we conclude in Section 4. The outstanding I/O throughput gap between HDDs and SSDs makes SSDs ideal for caching data. Based on this, we propose a hash join algorithm called Cached Grace Hash Join (CGHJ for short) for hybrid drives, which reduces hard disk random I/Os in the partitioning phase of the traditional GHJ algorithm by caching the segments of buckets in an SSD and migrating these segments of each partition contiguously to the HDD later. In the partitioning phase, when a bucket buffer is full, instead of writing to the hard disk, CGHJ buffers the content to a temporary file on the SSD. Since random writes may cause unnecessary erase operations or even  X  X rite amplification X  on SSDs, and thus degrade SSD performance, CGHJ adopts the append-only method to avoid random writes on the SSD. When the SSD buffer is full, the segments, which belong to each partition, will be migrated to the corresponding bucket file on the hard SSD buffer is emptied and the partitioning process continues until no tuples are left in relations. The partitioning process is shown in Fig. 1. Using this scheme, CGHJ uses the SSD as a cache to reduce time-consuming random accesses to the HDD and take advantage of the low access latency and high random read throughput of SSDs. The join phase of CGHJ remains the same as that in the GHJ algorithm. 
Here we borrow some I/O analysis results from [3]. Given two relations R and S to be joined, we assume that the smaller one R is the inner relation, and the other S is the outer relation. Let M be the total size of the memory buffer, M IB , the input buffer size, M number of partitions, so the buffer size for each bucket is M B / N . The cost of one ran-HDD until the SSD buffer runs out of space. Therefore, the effect of such a scheme is phase. Hence, the I/O cost for reading R and S in the partitioning phase becomes: The cost for writing back all the buckets to the bucket files on the HDD is: C denotes the cost of migration the cached bucket data from SSD to HDD. buffer is larger than memory while smaller than the inner relation. Given that SSD buffer is much larger than M B and M IB , from Formulae 1, 2 and 3 we can compute the cost saved by CGHJ in partitioning phase as follows: After some calculations, it arrives at: Formula 4 implies that the cost saving of CGHJ comes from 3 factors: HDD characte-ristics, SSD buffer size, relation sizes. In terms of the characteristics of the HDD, the larger the cost difference between random access and sequential access ( A ran -A seq ) is, the faster CGHJ will be. However, this factor depends solely on the characteristics of the HDD. The cost saving also depends on the difference between the inverse of the input buffer size and the inverse of the SSD buffer size, i.e., 11 () will contribute more to the cost saving of CGHJ. 
On the other hand, from Formula 4, we can infer that the sequential access cost of two relations in the HDD has been deducted from the final cost and the remainder is the I/O cost saved by CGHJ. CGHJ needs one extra pass of sequential write and ran-dom read on the SSD compared to GHJ; however, considering that the bandwidth of SSDs is much higher than that of HDDs, and that this paper assumes that the working space is still limited and thus will result in a large number of random I/Os, the extra I/O cost on the SSD will be counteracted by the saved cost of CGHJ through reducing the random I/Os on the HDD. This will be validated in the experiment part. Generally speaking, a larger SSD buffer will lead to higher cost savings. The GHJ and CGHJ algorithms were implemented in C++ with no special optimiza-tions incorporated. Our experiments were conducted under the Windows 7 with an Intel Core 2 Quad Core Q8200 2.33GHz processor, 4GB main memory, Seagate 7200RPM 500GB HDD, and two SSDs of different brands: a Samsung 830 Series 128GB SSD and an Apacer 32GB SSD. BIOS AHCI option is enabled to achieve highest performance of SSDs. Table 1 and Table 3 illustrate the I/O performance of our Seagate HDD as tested by the hard disk utility HD Tune . The I/O performance measurements of the SSD are referred to in Table 4, as measured by AS SSD Benchmark 1.6 . CGHJ uses a migration buffer of size 256KB as the default except where specified otherwise. 
In our experiments, the relations to be joined are generated by the data generator of the TPC-H benchmark with different scales from 1 to 30. Hash join is performed on ORDERS and CUSTOMER relations, with ORDERS as the outer relation and CUSTOMER as the inner relation. The join attributes of two relations are the primary key of CUSTOMER and the foreign key of ORDERS respectively. The sizes of CUSTOMER and ORDERS under 4 different scales are shown in Table 2. 3.1 Effects of Dataset Scales on CGHJ The first set of experiments examined the impact of 4 dataset scales on CGHJ and the conventional GHJ algorithm by fixing the memory buffer size at 3MB under two different configurations of the hybrid storage system: Apacer SSD + HDD and Sam-sung SSD + HDD. The effects of various migration buffer sizes on performance will be evaluated in another section. 
From Table 1, we find that the random read/write performance of the HDD is ac-ceptable when the data I/O block size is between 64KB to 1024KB. So to be realistic, each partition cached in the SSD should be at least multiples of this block size. Also, to be fair in comparing the performance of CGHJ and GHJ among different dataset scales, we decide to allocate the SSD buffer in size proportional to the dataset scale, i.e., the SSD buffer size is allocated to be N*512KB, where the number of partitions is N. The experiment results are shown in Fig. 2. 
The Y-axis shows the total join time of CGHJ and GHJ. The result shows that the larger the input relation is, the higher cost saving CGHJ can achieve. When the mem-ory buffer size is fixed, a larger relation results in more random I/Os in the partition-ing phase for the GHJ algorithm and creates more opportunities for CGHJ to reduce random I/Os. Also, it can be seen that the performance for CGHJ on the two different configurations is different. Due to the higher random read/write throughput of the Samsung SSD, CGHJ on the Samsung SSD outperforms that on the Apacer SSD. For the Samsung SSD + HDD, CGHJ is 1 to 4 times faster than GHJ; for the Apacer SSD exception at scale 1. For the dataset at scale 1, the relations are very small and CGHJ will not benefit from the low-end SSD + HDD combination. With this memory buffer size, each bucket buffer (3MB/11  X  279KB) approaches an ideal I/O block size for random writing on the HDD and therefore there is not so much room left for CGHJ to reduce random I/Os. Moreover, since the dataset at scale 1 can be wholly held in the SSD there is no need to handle it this way, so we did not evaluate the experiments on this dataset any further. 3.2 Effects of Various Memory Sizes on CGHJ The experiments of this section explore the effects of various memory sizes on the performance of CGHJ. The results of scales 10, 20, and 30 are shown in Fig. 3, Fig. 4, and Fig. 5 respectively. It can be seen that the performances of all algorithms improve with the increase in buffer size. However, wh en the buffer increases to a certain limit, the performance of CGHJ reaches a plateau. The reason is that as the size of the buf-fer increases, the size of each bucket X  X  buffe r increases as well, and this will decrease the number of random I/Os in the partitioning phase. This paper explored the possibility of using an SSD to reduce the random I/Os in hash joins as a part of a hybrid storage system. Our proposed scheme CGHJ reduces ran-dom I/Os on the HDD by using the SSD as cache between the RAM and HDD. CGHJ works well in situations where GHJ would face a large number of random I/Os due to limited memory or relations are large. And this idea could be extended to hybrid hash joins as well. Acknowledgements. This work was supported by National Natural Science Foundation of China (Grant No. 61070042) and Zhejiang Provincial Natural Science Foundation (Grant No. Y1090096 and Y13F020114). 
