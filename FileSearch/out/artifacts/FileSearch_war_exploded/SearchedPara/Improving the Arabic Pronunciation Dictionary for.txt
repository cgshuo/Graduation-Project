 The correspondence between orthography and pro-nunciation in Modern Standard Arabic (MSA) falls somewhere between that of languages such as Span-ish and Finnish, which have an almost one-to-one mapping between letters and sounds, and languages such as English and French, which exhibit a more complex letter-to-sound mapping (El-Imam, 2004). The more complex this mapping is, the more diffi-cult the language is for Automatic Speech Recogni-tion (ASR).

An essential component of an ASR system is its pronunciation dictionary (lexicon), which maps the orthographic representation of words to their pho-netic or phonemic pronunciation variants. For lan-guages with complex letter-to-sound mappings, such dictionaries are typically written by hand. However, pronunciation dictionaries are difficult to create by hand, because of the large number of word forms, each of which has a large number of possible pro-nunciations. Fortunately, the relationship between orthography and pronunciation is relatively regu-lar and well understood for MSA. Moreover, re-cent automatic techniques for morphological anal-ysis and disambiguation (MADA) can also be useful in automating part of the dictionary creation process (Habash and Rambow, 2005; Habash and Rambow, 2007) Nonetheless, most documented Arabic ASR systems appear to handle only a subset of Arabic phonetic phenomena; very few use morphological disambiguation tools.

In Section 2, we briefly describe related work, in-cluding the baseline system we use. In Section 3, we outline the linguistic phenomena we believe are crit-ical to improving MSA pronunciation dictionaries. In Section 4, we describe the pronunciation rules we have developed based upon these linguistic phenom-ena. In Section 5, we describe how these rules are used, together with MADA, to build our pronuncia-tion dictionaries for training and decoding automat-ically. In Section 6, we present results of our eval-uations of our phone-and word-recognition systems (XPR and XWR) on MSA comparing these systems to two baseline systems, B ASE PR and B ASE WR. We conclude in Section 7 and identify directions for future research. Most recent work on ASR for MSA uses a sin-gle pronunciation dictionary constructed by map-ping every undiacritized word in the training cor-pus to all of the diacritized Buckwalter analyses and the diacritized versions of this word in the Arabic Treebank (Maamouri et al., 2003; Afify et al., 2005; Messaoudi et al., 2006; Soltau et al., 2007). In these papers, each diacritized word is converted to a sin-gle pronunciation with a one-to-one mapping using  X  X ery few X  unspecified rules. None of these systems use morphological disambiguation to determine the most likely pronunciation of the word given its con-text. Vergyri et al. (2008) do use morphological in-formation to predict word pronunciation. They se-lect the top choice from the MADA (Morphological Analysis and Disambiguation for Arabic) system for each word to train their acoustic models. For the test lexicon they used the undiacritized orthography, as well as all diacritizations found for each word in the training data as possible pronunciation variants. We use this system as our baseline for comparison. MSA is written in a morpho-phonemic orthographic representation using the Arabic script , an alphabet 34 phonemes (28 consonants, 3 long vowels and 3 short vowels). The Arabic script has 36 basic let-ters (ignoring ligatures) and 9 diacritics. Most Ara-bic letters have a one-to-one mapping to an MSA phoneme; however, there are a small number of common exceptions (Habash et al., 2007; El-Imam, 2004) which we summarize next. 3.1 Optional Diacritics Arabic script commonly uses nine optional diacrit-ics: (a) three short-vowel diacritics representing the vowels /a/, /u/ and /i/; (b) one long-vowel diacritic (Dagger Alif  X  ) representing the long vowel /A/ in a small number of words; (c) three nunation diacrit-ics ( F /an/, N /un/, K /in/) representing a combina-tion of a short vowel and the nominal indefiniteness marker /n/ in MSA; (d) one consonant lengthening diacritic (called Shadda  X  ) which repeats/elongates the previous consonant (e.g., kat  X  ab is pronounced /kattab/); and (e) one diacritic for marking when there is no diacritic (called Sukun o ).

Arabic diacritics can only appear after a let-ter. Word-initial diacritics (in practice, only short vowels) are handled by adding an extra Alif (also called Hamzat-Wasl) at the beginning of the word. Sentence/utterance initial Hamzat-Wasl is pronounced like a glottal stop preceding the short vowel; however, the sentence medial Hamzat-Wasl is silent except for the short vowel. For exam-ple, Ainkataba kitAbN is /Ginkataba kitAbun/ but kitAbN Ainkataba is /kitAbun inkataba/. A  X  X eal X  Hamza (glottal stop) is always pronounced as a glot-tal stop. The Hamzat-Wasl appears most commonly as the Alif of the definite article Al . It also appears in specific words and word classes such as relative pronouns (e.g., Aly  X  X ho X  and verbs in pattern VII (Ain1a2a3).

Arabic short vowel diacritics are used together with the glide consonant letters w and y to denote the long vowels /U/ (as uw ) and /I/ ( iy ). This makes these two letters ambiguous.

Diacritics are largely restricted to religious texts and Arabic language school textbooks. In other texts, fewer than 1.5% of words contain a diacritic. Some diacritics are lexical (where word meaning varies) and others are inflectional (where nominal case or verbal mood varies). Inflectional diacritics are typically word final. Since nominal case, verbal mood and nunation have all disappeared in spoken dialectal Arabic, Arabic speakers do not always pro-duce these inflections correctly or at all.

Much work has been done on automatic Arabic diacritization (Vergyri and Kirchhoff, 2004; Anan-thakrishnan et al., 2005; Zitouni et al., 2006; Habash and Rambow, 2007). In this paper, we use the MADA (Morphological Analysis and Disambigua-tion for Arabic) system to diacritize Arabic (Habash and Rambow, 2005; Habash and Rambow, 2007). MADA, which uses the Buckwalter Arabic mor-phological Analyzer databases (Buckwalter, 2004), provides the necessary information to determine Hamzat-Wasl through morphologically tagging the definite article; in most other cases it outputs the spe-cial symbol  X  {  X  for Hamzat-Wasl. 3.2 Hamza Spelling The consonant Hamza (glottal stop /G/) has multi-ple forms in Arabic script:  X  , &gt; , &lt; , &amp; , } , | . There are complex rules for Hamza spelling that primarily depend on its vocalic context. For ex-ample, preceded or followed by an /i/ vowel. Similarly, the Hamza form | is used when the Hamza is followed by the long vowel /A/.

Hamza spelling is further complicated by the fact that Arabic writers often replace hamzated letters with the un-hamzated form ( &gt;  X  two-letter spelling, e.g. this variation, the un-hamzated forms (particularly for &gt; and &lt; ) are ignored in Arabic ASR evalua-tion. The MADA system regularizes most of these spelling variations as part of its analysis. 3.3 Morpho-phonemic Spelling Arabic script includes a small number of mor-phemic/lexical phenomena, some very common:  X  Ta-Marbuta The Ta-Marbuta ( p ) is typically a  X  Alif-Maqsura The Alif-Maqsura ( Y ) is a silent  X  Definite Article The Arabic definite article is  X  Silent Letters A silent Alif appears in the mor-As noted in Section 3, diacritization alone does not predict actual pronunciation in MSA. In this section we describe a set of rules based on MSA phonol-ogy which will extend a diacritized word to a set of possible pronunciations. It should be noted that even MSA-trained speakers, such as broadcast news anchors, may not follow the  X  X roper X  pronunciation according to Arabic syntax and phonology. So we attempt to accommodate these pronunciation vari-ants in our pronunciation dictionary.

The following rules are applied on each dia-categories: (I) a shared set of rules used in all systems compared (B ASE PR, B ASE WR, XPR B
ASE WR which we modified for XPR and XWR; (III) a first set of new rules devised for our systems XPR and XWR; and (IV) a second set of new rules that generate additional pronunciation variants. Below we indicate, for each rule, how many words in the training corpus (335,324 words) had their pronunciation affected by the rule.
 I. Shared Pronunciation Rules 1. Dagger Alif:  X   X  /A/ 2. Madda: | X  /G A/ 3. Nunation: AF  X  /a n/, F  X  /a n/, /K/  X  /i n/, 4. Hamza: All Hamza forms:  X  , } , &amp; , &lt;, &gt;  X  /G/ 5. Ta-Marbuta: p  X  /t/
II. Modified Pronunciation Rules 1. Alif-Maqsura: Y  X  /a/ 2. Shadda: Shadda is always removed 3. U and I: uwo  X  /U/, iyo  X  /I/
III. New Pronunciation Rules 1. Waw Al-jamaa: suffixes uwoA  X  /U/ 2. Definite Article: Al  X  /a l/ (if tagged as Al+ 3. Hamzat-Wasl: { is always removed. 4.  X  X l X  in relative pronouns: Al  X  /a l/ 5. Sun letters: if the definite article (Al) is fol-
IV. New Pronunciation Rules Generating Addi-tional Variants  X  Ta-Marbuta: if a word ends with Ta-Marbuta  X  Case ending: if a word ends with a short vowel
As a post-processing step in all systems, we re-move the Sukun diacritic and convert every letter X to phoneme /X/. In XPR and XWR, we also remove short vowels that precede or succeed a long vowel. As noted above, pronunciation dictionaries map words to one or more phonetically expressed pro-nunciation variants. These dictionaries are used for training and decoding in ASR systems. Typi-cally, most data available to train large vocabulary ASR systems is orthographically (not phonetically) transcribed. There are two well-known alternatives for training acoustic models in ASR: (1) bootstrap training, when some phonetically annotated data is available, and (2) flat-start, when such data is not available (Young et al., 2006). In flat-start training, for example, the pronunciation dictionary is used to map the orthographic transcription of the train-ing data to a sequence of phonetic labels to train the initial monophone models. Next, the dictionary is employed again to produce networks of possible pronunciations which can be used in forced align-ment to obtain the most likely phone sequence that matches the acoustic data. Finally, the monophone acoustic models are re-estimated. In our work, we refer to this dictionary as the training pronuncia-tion dictionary . The second usage of the pronun-ciation dictionary is to generate the pronunciation models while decoding. We refer to this dictionary as the decoding pronunciation dictionary .

For languages like English, no distinction be-tween decoding and training pronunciation dictio-naries is necessary. However, as noted in Section 3, short vowels and other diacritic markers are typi-cally not orthographically represented in MSA texts. Thus ASR systems typically do not output fully di-acritized transcripts. Diacritization is generally not necessary to make the transcript readable by Arabic-literate readers. Therefore, entries in the decod-ing pronunciation dictionary consist of undiacritized words that are mapped to a set of phonetically-represented diacritizations. However, every entry in the training pronunciation dictionary is a fully dia-critized word mapped to a set of possible context-dependent pronunciations. Particularly in the train-ing step, contextual information for each word is available from the transcript, so, for our work, we can use the MADA morphological tagger to obtain the most likely diacritics. As a result, the speech signal is mapped to a more accurate representation of the training transcript, which we hypothesize will lead to a better estimation of the acoustic models.
As noted in Section 1, pronunciation dictionaries for ASR systems are usually written by hand. How-ever, Arabic X  X  morphological richness makes it dif-ficult to create a pronunciation dictionary by hand since there are a very large number of word forms, each of which has a large number of possible pro-nunciations. The relatively regular relationship be-tween orthography and pronunciation and tools for morphological analysis and disambiguation such as MADA, however, make it possible to create such 5.1 Training Pronunciation Dictionary In this section, we describe an automatic approach to building a pronunciation dictionary for MSA that covers all words in the orthographic transcripts of the training data. First, for each word in each ut-terance, we run MADA to disambiguate the word based on its context in the transcript. MADA outputs all possible fully-diacritized morphological analy-ses, ranked by their likelihood, the MADA confi-thographic transcription for training. Second, we map the highest-ranked diacritization of each word to a set of pronunciations, which we obtain from the pronunciation rules described in Section 4. Since MADA may not always rank the best analysis as its top choice, we also run the pronunciation rules on the second best choice returned by MADA, when the difference between the top two choices is less than a threshold determined empirically (in our im-plementation we chose 0.2). In Figure 1, the training pronunciation dictionary maps the 2 nd column (the entry keys) to the 3 rd column.

We generate the baseline training pronunciation dictionary using only the baseline rules from Section 4. This dictionary also makes use of MADA, but it maps the MADA-diacritized word to only one pro-nunciation. The baseline training dictionary maps the 2 nd column (the entry keys) to only one pronun-ciation in the 3 rd column in Figure 1. 5.2 Decoding Pronunciation Dictionary The decoding pronunciation dictionary is used in ASR to build the pronunciation models while decod-ing. Since, as noted above, it is standard to produce unvocalized transcripts when recognizing MSA, we must map word pronunciations to unvocalized ortho-graphic output. Therefore, for each diacritized word in our training pronunciation dictionary, we remove diacritic markers and replace Hamzat-Wasl ({), &lt;, and &gt; by the letter  X  X  X , and then map the modified word to the set of pronunciations for that word. For example, in Figure 1 the undiacritized word mdrsp in the 1 st column is mapped to the pronunciations in the 3 rd column. The baseline decoding pronun-ciation dictionary is constructed similarly from the baseline training pronunciation dictionary. To determine whether our pronunciation rules are useful in speech processing applications, we eval-uated their impact on two tasks, automatic phone recognition and ASR. For our experiments, we used the broadcast news TDT4 corpus (Arabic Set 1), di-vided into 47.61 hours of speech (89 news shows) for training and 5.18 hours (11 shows); test and training shows were selected at random. Both train-ing and test data were segmented based on silence and non-speech segments and down-sampled to segments for our training data and 2,255 segments for testing. 6.1 Acoustic Models Our monophone acoustic models are built using 3-state continuous HMMs without state-skipping with a mixture of 12 Gaussians per state. We extract standard MFCC (Mel Frequency Cepstral Coeffi-cients) features from 25 ms frames, with a frame shift of 10 ms. Each feature vector is 39D: 13 fea-tures (12 cepstral features plus energy), 13 deltas, and 13 double-deltas. The features are normalized using cepstral mean normalization. For our ASR experiments, tied context-dependent cross-word tri-phone HMMs are created with the same settings as monophones. The acoustic models are speaker-and gender-independent, trained using ML (maximum using the HMM Toolkit (HTK) (Young et al., 2006). 6.2 Phone Recognition Evaluation We hypothesize that improved pronunciation rules will have a profound impact on phone recognition accuracy. To compare our phone recognition (XPR) system with the baseline (B ASE PR), we train two phone recognizers using HTK. The B ASE PR rec-ognizer uses the training-pronunciation dictionary generated using the baseline rules; the XPR sys-tem uses a pronunciation dictionary generated using these rules plus our modified and new rules (cf. Sec-tion 5). The two systems are identical except for their pronunciation dictionaries.

We evaluate the two systems under two condi-tions: (1) phone recognition with a bigram phone with an open-loop phone recognizer, such that any phoneme can follow any other phoneme with a uni-form distribution. Results of this evaluation are pre-sented in Table 1.

Ideally, we would like to compare the perfor-mance of these systems against a common MSA phonetically-transcribed gold standard. Unfortu-nately, to our knowledge, such a data set does not exist. So we approximate such a gold standard on a blind test set through forced alignment, us-ing the trained acoustic models and pronunciation dictionaries. Since our choice of acoustic model (of B ASE PR or XPR) and pronunciation dictionary (again of B ASE PR or XPR) can bias our results, we consider four gold variants (GV) with differ-ent combinations of acoustic model and pronunci-ation dictionary, to set expected lower and upper bounds. These combinations are represented in Ta-ble 1 as GV1 X 4, where the source of acoustic mod-els is B ASE PR or XPR and source of pronuncia-tion rules are B ASE PR, XPR or XPR and B ASE PR combined. These GV are described in more detail below, as we describe our results.

Since B ASE PR system uses a pronunciation dic-tionary with a one-to-one mapping of orthography to phones, the GV1 phone sequence for any test utterance X  X  orthographical transcript according to B
ASE PR can be obtained directly from the ortho-graphic transcript. Note that if, in fact, GV1 does represent the true gold standard (i.e., the correct phone sequence for the test utterances) then if XPR obtains a lower phone error rate using this gold stan-dard than B ASE PR does, we can conclude that in fact XPR X  X  acoustic models are better estimated. This is in fact the case. In Table 1, first line, we see that XPR under both conditions (open-loop and bigram LM) significantly (p-value &lt; 2 . 2 e  X  16 ) out-performs the corresponding B ASE PR phone recog-
If GV1 does not accurately represent the phone sequences of the test data, then there must be some phones in the GV1 sequences that should be deleted, inserted, or substituted. On the hypothesis that our training-pronunciation dictionary might improve the B
ASE PR assignments, we enrich the baseline pro-nunciation dictionary with XPR X  X  dictionary. Now, we force-align the orthographic transcript using this extended pronunciation dictionary, still using B
ASE PR X  X  acoustic models, with the acoustic sig-nal. We denote the output phone sequences as GV2. If a pronunciation generated using the B ASE PR dic-tionary was already correct (in GV1) according to the acoustic signal, this forced alignment process still has the option of choosing it. We hypothesize that the result, GV2, is a more accurate represen-tation of the true phone sequences in the test data, since it should be able to model the acoustic sig-nal more accurately. On GV2, as on GV1, we see that XPR, under both conditions, significantly (p-value &lt; 2 . 2 e  X  16 ) outperforms the corresponding B
ASE PR phone recognizers (see Table 1, second line).

We also compared the performance of the two systems using upper bound variants. For GV3 we used the forced alignment of the orthographic tran-scription using only XPR X  X  pronuncation dictionary with XPR X  X  acoustic models. In GV4 we combine the pronunciation dictionary of XPR with B ASE PR dictionary and use XPR X  X  acoustic models. Unsur-prisingly, we find that the XPR recognizer signifi-cantly (p-value &lt; 2 . 2 e  X  16 ) outperforms B ASE PR when using these two variants under both conditions (see Table 1, third and fourth lines).

The results presented in Table 1 compare the ro-bustness of the acoustic models as well as the pro-nunciation components of the two systems. We also want to evaluate the accuracy of our pronunciation predictions in representing the actual acoustic sig-nal. One way to do this is to see how often the forced alignment process choose phone sequences using the B ASE PR pronunciation dictionary as opposed to XPR X  X . We forced aligned the test transcript  X  using the XPR acoustic models and only the XPR pronunciation dictionary  X  with the acoustic sig-nal. We then compare the output sequences to the output of the forced alignment process where the combined pronunciations from B ASE PR+XPR and the XPR acoustic models were used. We find that the difference between the two is only 1.03% (with 246,376 phones, 557 deletions, 1696 substitutions, and 277 insertions). Thus, adding the B ASE PR rules to XPR does not appear to contribute a great deal to the representation chosen by forced alignment. In a similar experiment, we use the B ASE PR acous-tic models instead of the XPR models and compare the results of using B ASE PR-pronunciation dictio-nary with the combination of XPR+B ASE PR X  X  dic-tionaries for forced alignment. Interestingly, in this experiment we do find a significantly larger differ-ence between the two outputs 17.04% (with 233,787 phones, 1404 deletions, 14013 substitutions, and 27040 insertions). We can hypothesize from these experiments that the baseline pronunciation dictio-nary alone is not sufficient to represent the acoustic signal accurately, since large numbers of phonemes are edited when adding the XPR pronunciations. In contrast, adding the B ASE PR X  X  pronunciation dic-tionary to XPR X  X  shows a relatively small percent-age of edits, which suggests that the XPR pronun-ciation dictionary extends and covers more accu-rately the pronunciations already contained in the B
ASE PR dictionary. 6.3 Speech Recognition Evaluation We have also conducted an ASR experiment to eval-uate the usefulness of our pronunciation rules for nunciation rules to generate the baseline training and decoding pronunciation dictionaries. Using these dictionaries, we build the baseline ASR sys-tem (B ASE WR). Using our extended pronunciation rules, we generate our dictionaries and train our ASR system (XWR). Both systems have the same model settings, as described in Section 6.1. They also share the same language model (LM), a trigram LM trained on the undiacritized transcripts of the training data and a subset of Arabic gigawords (ap-proximately 281 million words, in total), using the SRILM toolkit (Stolcke, 2002).

Table 2 presents the comparison of B ASE WR with the XWR system. In Section 5.1, we noted that the top two choices from MADA may be included in the XWR pronunciation dictionary when the differ-ence in MADA confidence scores for these two is less than a given threshold. So we analyze the im-pact of including this second MADA option in both the training and decoding dictionaries on ASR re-sults. In all cases, whether the second MADA choice is included or not, XWR significantly (p-values &lt; 8.1e-15) outperforms B ASE WR. Our best results are obtained when we include the top first and second MADA option in the decoding pronunciation dictio-nary but only the top MADA choice in the training pronunciation dictionary. The difference between this version of XWR and an XWR version which includes the top second MADA choice in the train-ing dictionary is significant (p-value = 0 . 017 ).
To evaluate the impact of the set of rules that gen-erate additional pronunciation variants (described in Section 4 -IV) on word recognition, we built a system, denoted as XWR_I-III, that uses only the first three sets of rules (I X  X II) and compared its per-formance to that of both B ASE WR and the corre-sponding XWR system. As shown in Table 2, we observe that XWR_I-III significantly outperforms B
ASE WR in 2.27 (p-value &lt; 2.2e-16). Also, the corresponding XWR that uses all the rules (includ-ing IV set) significantly outperforms XWR_I-III in 1.24 (p-value &lt; 2.2e-16).

The undiacritized vocabulary size used in our ex-periment was 34,511. We observe that 6.38% of the words in the test data were out of vocabulary (OOV), which may partly explain our low absolute recognition accuracy. The dictionary size statistics (for entries generated from the training data only) used in these experiments are shown in Table 3. We have done some error analysis to understand the rea-son behind high absolute error rate for both systems. We observe that many of the test utterances are very noisy. We wanted to see whether XWR still out-performs B ASE WR if we remove these utterances. Removing all utterances for which B ASE WR ob-tains an accuracy of less than 25%, we are left with 1720/2255 utterances. On these remaining utter-ances, the B ASE WR accuracy is 64.4% and XWR X  X  accuracy is 67.23%  X  a significant difference de-spite the bias in favor of B ASE WR. In this paper we have shown that the use of more linguistically motivated pronunciation rules can im-prove phone recognition and word recognition re-sults for MSA. We have described some of the pho-netic, phonological, and morphological features of MSA that are rarely modeled in ASR systems and have developed a set of pronunciation rules that en-capsulate these features. We have demonstrated how the use of these rules can significantly improve both MSA phone recognition and MSA word recognition accuracy by a series of experiments comparing our XPR and XWR systems to the corresponding base-line systems B ASE PR and B ASE WR. We obtain an improvement in absolute accuracy in phone recogni-tion of 3.77% X 7.29% and a significant improvement of 4.1% in absolute accuracy in ASR.

In future work, we will address several issues which appear to hurt our recognition accuracy, such as handling the words that MADA fails to analyze. We also will develop a similar approach to handling dialectical Arabic speech using the MAGEAD mor-phological analyzer (Habash and Rambow, 2006). A larger goal is to employ the MSA and dialectical phone recognizers to aid in spoken Arabic dialect identification using phonotactic modeling (see (Bi-adsy et al., 2009)).

