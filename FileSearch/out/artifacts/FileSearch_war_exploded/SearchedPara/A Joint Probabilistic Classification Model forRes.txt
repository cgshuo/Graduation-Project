 Resource selection is an important task in Federated Search to select a small number of most relevant information sources. Current resource selection algorithms such as GlOSS, CORI, ReDDE, Geometric Average and the recent classification-based method focus on the evidence of individual informa-tion sources to determine the relevance of available sources. Current algorithms do not model the important relationship information among individual sources. For example, an in-formation source tends to be relevant to a user query if it is similar to another source with high probability of being rele-vant. This paper proposes a joint probabilistic classification model for resource selection. The model estimates the prob-ability of relevance of information sources in a joint man-ner by considering both the evidence of individual sources and their relationship. An extensive set of experiments have been conducted on several datasets to demonstrate the ad-vantage of the proposed model.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Design, Performance Federated Search, Resource Selection, Joint Classification
Federated text search provides a unified search interface for multiple search engines of distributed text information sources. There are three major research problems in fed-erated search as resource representation, resource selection  X  Vietnam Education Foundation Fellow and results merging. This paper focuses on resource selec-tion, which selects a small number of most relevant informa-tion sources to search for any particular user query.
Resource selection for federated search has been a popu-lar research topic in the last two decades. Many methods treat each information source as a single big document and rank available sources either by using statistics from the sample documents (CORI [6]), or by building a language model for each source (Xu and Croft [27], Si and Callan [22]). Other methods such as GlOSS [12], Geometric Av-erage [17], ReDDE [20], CRCS [18] and SUSHI [24] look further inside an information source by estimating the rele-vance of each document and calculate the source X  X  score as an aggregate function of the documents that the source con-tains. More recent methods such as the classification-based method [1] and the work in [2] treat resource selection as a classification problem and build probabilistic models by combining multiple types of evidence of individual sources.
Existing resource selection methods judge an information source by its own characteristics, but miss an important piece of evidence, which is the relationship between avail-able sources. In practice, we notice that relationship can be meaningful and indicative. An information source that is  X  X imilar X  to another highly relevant source has a better chance of being relevant. The evidence of source relation-ship can be very valuable for real world federated search so-lutions. In particular, the resource representation (e.g., sam-ple documents) of each information source is often limited and prevents resource selection algorithms from identifying relevant sources, while the relationship between sources can help to alleviate the problem by providing more evidence from similar sources. Our study of a real world federated search application with digital libraries also suggests that it is di ffi cult to obtain thorough resource representation from many sources (i.e., digital libraries). For example, some sources may only provide the abstracts of its documents in-stead of the full texts.

This paper proposes a novel probabilistic discriminative model for resource selection that explicitly models the rela-tionship between information sources. In particular, the new research combines both the evidence of individual sources and the relationship evidence between sources into a sin-gle probabilistic model for estimating the joint probability of relevance of a set of sources. Di ff erent similarity met-rics have been studied to explore the relationship between information sources. An extensive set of experiments have been conducted on two TREC testbeds for federated search research and one real world application for searching digi-tal libraries. The experiment results demonstrate the e ff ec-tiveness and robustness of the proposed resource selection algorithm with the joint classification model.

The rest of this paper is organized as follows: the next sec-tion discusses the related research work. Section 3 presents the classification model for resource selection. Section 4 proposes the joint classification model. Section 5 discusses experimental methodology. Section 6 presents experimen-tal results and related discussions. Section 7 concludes and points out some future research work.
There has been considerable research on all of the three subtasks of federated search as resource representation, re-source selection and results merging. Since this paper fo-cuses on the resource selection task, we mainly survey most related prior research work in resource selection and briefly talk about resource representation and results merging.
The first step in federated search is to obtain representa-tive resource descriptions from available information sources. The START protocol [11] provides accurate information in collaborative federated search environments, but it does not work for uncooperative environments. On other side, the query-based sampling technique [4] has been widely used in federated search to obtain sample documents from each source by issuing randomly generated queries. In particu-lar, the query-based sampling approach is used in this work to acquire sample documents from available sources. After that, all sample documents are merged together as a cen-tralized sample database .

Resource selection selects a small set of most relevant sources for each user query [3][8][13]. Most early resource selection algorithms treat each individual source as a sin-gle big document which they extract summary statistics from. Those big document methods such as KL [27], CORI [6], and CVV [28] utilize di ff erent types of summary statis-tics of sources and finally rank available sources by match-ing the statistics with the user X  X  query. These methods ig-nore the boundaries of individual documents within indi-vidual sources, which limits their performance of identifying sources with a large number of relevant documents.
Some recent resource selection algorithms such as ReDDE [20], DTF [9][10], CRCS [18] and SUSHI [24] step away from treating each source as a single big document. Those algo-rithms often analyze individual sample documents within re-source representation for ranking sources. For example, the ReDDE selection algorithm estimates the distribution of rel-evant documents by treating top-ranked sample documents as a representative subset of relevant documents in avail-able sources. Related algorithms such as UUM [22], RUM [23] and CRCS [18] have been proposed, which use di ff erent methods to weight top-ranked documents and estimate the probability of relevance.

More recent resource selection algorithms such as the clas-sification-based resource selection in federated search [1] or vertical search [2] treat resource selection as a classification problem. A classification model can be learned from a set of training queries and is used to predict the relevance of a source for test queries. It has been shown [1] that the clas-sification approach can outperform state-of-the-art resource selection algorithms like ReDDE.

Existing resource selection methods utilize evidence within individual sources to judge their relevance but ignore the ev-idence of the relationship between available sources. How-ever, the relationship evidence is a valuable piece of infor-mation, which promises to improve the accuracy of resource selection.

Two other related research work in [25][7] learn from the results of past queries for resource selection. However, these two methods do not model the relationship between infor-mation sources and do not use formal models based on clas-sification.

The last step of federated search is results merging, which merges returned documents from selected sources into a sin-gle list. The most e ff ective method is to download and recal-culate scores for all returned documents within a centralized retrieval model, but this is often ine ffi cient. More e ffi cient methods such as the CORI merging formula, the SSL [21] and the SAFE merging algorithms [19] try to approximate the results of centralized retrieval in di ff erent ways.
Many resource selection algorithms are unsupervised and provide one source of evidence. To combine di ff erent evi-dence in a unified framework, one needs a training dataset, usually in the form of binary judgments on sources. Specif-ically, given a set of sources C and a set of training queries Q , the objective is to find a mapping F of the form where +1 indicates the relevance between the query and the source, and  X  1 indicates irrelevance.

Arguello et al.[1] have proposed a method to construct those judgments. Each query q  X  Q will be issued to a full-dataset index for searching. A source C i  X  C is considered to be relevant with q if more than  X  documents from C i are present in top T of the full-dataset result. Otherwise, it is marked as irrelevant.

While this method can produce a rank list that mimics the rank list produced by a full-dataset retrieval, it is di ffi cult to apply in a real world environment because of the absence of a full-dataset. We propose an alternative method that could be more feasible. A query q is now issued to each remote source C i and we only count their returned documents that are relevant. Top T documents from each source will be inspected, then a source is marked as relevant if it has more than  X  relevant documents presenting in that list. In our work, we set T = 100. For dataset with a large average number of relevant documents per query (over 100), we set  X  = 3; otherwise  X  is equal to 1.
This section presents di ff erent types of evidence of indi-vidual sources for building our classification model.
Big Document (BIGDOC) approach treats each informa-tion source as a big document that contains all of its sample documents. A query is then issued to an index which con-tains a set of big documents, each representing one source. Sources are then ranked by how their merged sample docu-ments match the query. The disadvantage of this method is that it does not take into account the variation of sources X  sizes. Assuming that the sampling process is uniform, for a very big source, the sampling process only covers a small fraction of its documents. Therefore, it may present fewer relevant documents in the centralized sample database than a much smaller one, although the absolute number of rele-vant documents in the big source is higher. Without consid-ering the sources X  sizes, it would be misleading to conclude that the small source is the better choice. Nevertheless, when combined with other features, BIGDOC could have a good contribution, especially in the case that many sources contain roughly the same number of documents. While CORI (discussed in the next part) also treats each source as one document, BIGDOC approach is more flexible since it can be used with di ff erent retrieval algorithms. In our experiments, the algorithm is Indri [14]. For each pair of a query and a source, one BIGDOC feature is built from the sample documents. The CORI resource selection algorithm [6] uses Bayesian Inference Network model to rank sources. The belief P ( q | C that a source C i satisfies query q is the combination of mul-tiple P ( r j | C i ), the belief corresponding to each term r query q . CORI applies a variant of tf.idf formula to deter-mine each P ( r j | C i ) and combine them together to calculate the final belief score of each source. CORI was proven to have robust performance for resource selection. In our ex-periments, one CORI feature is used for each pair of a query and a source.
In this method, a query is first issued to a centralized sample database, which was mentioned in section 2. Then, each source C i is scored according to the geometric average query likelihood of its top K sample documents [17], where d ij is the j -th sample document in the rank list of source C i . If C i presents less than K documents in the rank list, the product above is padded with the minimum query likelihood score. Recall that ReDDE score [20] is calculated according to : ReDDE q ( C i )= where R samp N is the top N documents returned from search-ing the centralized sample database. N est i is the estimated size of source C i , N samp i is the sample size of C i , and I ( . ) is the indicator function. The number of top returned docu-ments, N , is equal to  X   X  N est all , where N est all is the estimated total number of documents of all sources and  X  is a constant, which is usually in the range 0.002-0.005.

ReDDE uses a step function to estimate P q ( rel | d ), the probability that the document d is relevant to query q .For all top N documents, that probability is equal to a constant. In our experiment, we use a modified version of ReDDE, which replaces P q ( rel | d )by P ( q | d ), the retrieval score of document d with respect to query q . The Indri retrieval algorithm [14] is used for searching the centralized sample database. The modified ReDDE feature has been shown empirically better than the original ReDDE feature. There is one modified ReDDE feature for each pair of a query and a source.
 ReDDE.top [1] is another variant of ReDDE. Unlike ReDDE, ReDDE.top set a specific number to N . In our experiment, we add another two ReDDE.top features with N = 100 and N = 1000 respectively.
We propose a novel joint probabilistic model for the re-source selection task. First of all, a logistic model is built to combine all the features of individual sources. We refer to this model as the independent model (Ind).

Let v = { v 1 ,...,v n } be the relevance vector. v i = 1 indi-cates that the i -th source is relevant, otherwise v i = 0. The relevance probability of a source c i given its feature vector f ( c i ) is calculated as: where  X  denotes the combination weight vector. For simplic-ity, the vector f ( c i ) contains the bias feature (which is 1 for every pair of a query and a source) and the weight vector  X  contains the bias element  X  0 . The conditional probability of v given n sources is:
P ( v | c )= 1 Z exp where Z is the normalizing constant.

Our joint classification model (Jnt) expands the above formula with a new term to model the relationship between sources. The conditional probability of v given n sources is now: P ( v | c )= 1 Z exp which can be rewritten as: P ( v | c )= 1 Z exp where sim ( c i ,c j ) denotes the similarity between two sources c and c j , and Z is another normalizing constant.

The parameter  X  controls the influence of similarity. If |  X  | is high, the model tends to promote only similar (or dissim-ilar) sources. When  X  = 0, we get back to the independent model.

In the learning step, we learn the feature weight vector  X  from the independent model by using logistic regression. This vector is then used in the joint model. Learning  X  , however, is generally intractable. One can see that the space of vector v is 2 n , and so inferencing and estimation become impossible when n is large. We resolve this issue by first ranking the sources using the independent model, and then apply the joint classification model only to the top K =10 sources. This is equivalent to reranking the top K sources.
From the set of training queries, we use maximum log-likelihood estimation to learn the parameter  X  . Because there is no closed-form solution for the maximum of this log-likelihood function, gradient search method is used instead.
In the prediction step, for a test query, the score of each source c i is assigned by its probability of being relevant:
R ( c i )= P ( v i =1 | c )= where v \ v i denotes the set of variables in v with variable v omitted. In practice, the summation is taken over K  X  1 variables and so is feasible when K is small . After that, the top K sources will be reranked according to the new score.
Given a set of training queries, the similarity between two sources can be measured by looking at the set of queries for wich they are both relevant. The bigger that set is, the more related they are. Specifically, we apply a cross-product formula to measure this metric: where Q is the set of training queries, rel ( c i ,q ) is equal to 1 if source c i is relevant to query q based on the classification approach described above, otherwise it is 0. This method is called Similarity Metric based-on Evaluation (SME).
One issue with the SME is that it is independent of the query. A source may be highly related with another source with respect to a query but unrelated with that source with respect to another query. Therefore, it is better to incorpo-rate the similarity between queries into this formula. By ex-tending the above SME, we derive another metric called Sim-ilarity Metric based on Query-specific Evaluation (SMQE). where sim ( q, q ) denotes the correlation (or similarity) be-tween the test query q and a training query q . There are many studies that explore the topicality or classification of queries, however, in this paper, we choose one simple ap-proach. A query in consideration is issued to the central-ized sample database, and the number of documents from each source that appear in top M documents of the result is recorded. In our work, M is equal to 100. The correlation between two queries is derived by a cosine-like formula: sim ( q, q )= i where numdoc ( q, c i ) is the number of documents of source c that appear in the top M documents returned from query q .

Both the SME and SMQE metrics can be modified in many ways. First of all, the term rel ( i, q ) can be repre-sented either by a binary number or the absolute number of relevant documents. Or we can set di ff erent thresholds to the searching on the centralized sample database. Another choice is to normalize the relevance vector. However, in our experiments, those changes do not have much e ff ect on the results. In fact, SMQE provides the best result, proving that it better reflects the relationship between sources.
This method tries to reveal the similarity between sources by looking at their own vocabularies. Specifically, a language model [16] is built for each sample source. Then we calculate the Kullback-Leibler divergence between those two language models. Recall that the Kullback-Leibler divergence is actu-ally the distance between two probabilistic models, which is the inverse of their similarity. However, because our model can adapt this change by inferring a negative similarity co-e ffi cient  X  , we keep the KL-value as it is. This metric is referred to as SMKL.
We evaluate our proposed algorithms on 3 datasets. The first two datasets are well-known TREC testbeds, the last one comes from a real world application.
We make the dataset available as feature file at http://www.cs.purdue.edu/homes/dthong/ Table 1: Summary Statistics of TREC123 and TREC4 Testbed (GB) Documents (x1000) TREC123 3.2 0.7 10.8 39.7 28 32 42 TREC4 2.0 5.6 5.6 5.6 4 20 138 Table 2: Statistical Information about DIGLIB: Number of Sources Corresponding to their Avail-able Information Fields
A note on resource specific retrieval algorithm: DIGLIB is a real world application of digital libraries, each of its sources implements a di ff erent retrieval algorithm, which is not known. We can only access those sources through a unified interface. For TREC123 and TREC4, we assign one retrieval algorithm to each source in a round-robin manner. The set of assigned algorithms is Inquery, Language Model and Vector Space (tf.idf). These algorithms influence the query-based sampling process, as well as the classification process. A less e ff ective retrieval algorithm like Vector Space model may reduce a source X  X  chance of being marked as rel-evant.

For each testbed, we repeat every experiment 5 times. In each trial, we randomly select 50% of the queries as training set, and test on the other 50%. All the results shown in the next section are averaged over 5 trials.

Each source is sampled with 300 documents. We also compare the main results with 100 sample documents. The experiments are measured on several levels:
On TREC123 and TREC4, all tests at di ff erent levels are presented. On DIGLIB dataset, we only report the results at source level because the document judgments are di ffi cult to make as many sources do not provide their full text infor-mation. In most of the experiments, SMQE is used as our default similarity metric. However, in section 6.4, we also discuss the experimental results with di ff erent other metrics.
In all of our experiments, we use paired t-test on queries to check significance. A  X  denotes a significance on p&lt; 0 . 1 level;  X  corresponds to p&lt; 0 . 05 level and  X  corresponds to p&lt; 0 . 01 level.
First of all, we compare the joint classification model with the independent model on the two TREC testbeds. Table 3 represents the source level results in accuracy on TREC123 and TREC4. The second column of each dataset is the joint classification model. Numbers in parentheses show the rela-tive improvement of the joint classification model (denoted as  X  X nt X ) over the independent model (denoted as  X  X nd X ).
Table 4 shows the R-metric comparison between the inde-pendent model and joint classification model. Table 5 shows the high precision at document level. We also report the full centralized retrieval, which includes all sources. This is de-noted as the  X  X ull X  column in the table.

It can be seen that joint classification model always leads to better results than independent model, as it shows in all three tables. Both models have the same source level accuracy and R-metric values at top 10 because of the fact that we rerank the top 10 sources. The results are more statistically significant on TREC123 than on TREC4. This can be explained as in TREC123, we have trained on 50 queries; whereas in TREC4, we use only 25 queries out of 50 for training.
The result at source level of Digital Library is reported in Table 6. In this real world dataset, the joint classifica-tion model significantly outperforms the independent model. This accounts to the fact that many sources only provide Table 3: Source Level Results in Accuracy on TREC123 &amp; TREC4 with 300 Sample Documents
Src Rank TREC123 TREC4 Ind Jnt Ind Jnt @1 0.512 0.524(2.3 % ) 0.480 0.536(11.7 % ) @3 0.456 0.499(9.4 % )  X  0.451 0.475(5.3 % ) @5 0.451 0.484(7.3 % )  X  0.430 0.446(3.7 % ) @10 0.439 0.439(0%) 0.414 0.414(0%) Table 4: Source Level Results in R-metric on TREC123 &amp; TREC4 with 300 Sample Documents
Src Rank TREC123 TREC4 Ind Jnt Ind Jnt @1 0.262 0.319(21.8 % )  X  0.287 0.309(7.7 % ) @3 0.309 0.364(17.8 % )  X  0.324 0.340(4.9 % ) @5 0.354 0.400(13.0 % )  X  0.343 0.355(3.5 % ) @10 0.426 0.426(0%) 0.414 0.414(0%) partial information about themselves. This also shows that the joint classification model can alleviate the problem of missing information.
We conduct experiments on three datasets with only 100 documents sampled from each source. This test is to show the robustness of the model, as well as the e ff ect of sam-pling size on the results. The results of source level (both in accuracy and R-metric) and document level are reported for TREC123 and TREC4 (Table 7, Table 8 and Table 9 re-spectively), while only source level is reported for DIGLIB (Table 10).

The sample size clearly a ff ects TREC123. Its performance of the independent model drops significantly. However, this also leaves room for joint classification model to show its e ff ectiveness: the accuracy on source level is statistically more significant. On document level, the improvement is a bit weaker. This can be explained as the initial choice of top 10 sources from the independent model is less precise, so is the joint classification model, which uses the initial ranking list directly.

Most results on TREC4 from Table 7 to Table 9 indicate the advantage of the joint classification model against inde-pendent model with a small number of sample documents, although the di ff erence is smaller than TREC123 due to the limited amount of training information.
 The results on DIGLIB (Table 10) are also consistent. The performances of both resource selection algorithms drop with 100 sample documents. However, the results of the joint classification method are still significantly better than those of the independent method.
We conduct tests on three testbeds with di ff erent simi-larity metrics discussed in Section 4.2. Figure 1 shows the Table 5: Document Level Results in High Precision on TREC123 &amp; TREC4 with 300 Sample Documents Table 6: Source Level Results in Accuracy on DIGLIB with 300 Sample Documents Table 7: Source Level Results in Accuracy on TREC123 &amp; TREC4 with 100 Sample Documents
Src Rank TREC123 TREC4 Ind Jnt Ind Jnt @1 0.320 0.380(18.8 % )  X  0.496 0.480(-3.2%) @3 0.299 0.373(24.7 % )  X  0.405 0.411(1.5 % ) @5 0.318 0.357(12.3 % )  X  0.379 0.403(6.3 % ) @10 0.319 0.319(0%) 0.367 0.367(0%) Table 8: Source Level Results in R-metric on TREC123 &amp; TREC4 with 100 Sample Documents
Src Rank TREC123 TREC4 Ind Jnt Ind Jnt @1 0.183 0.233(27.3 % )  X  0.278 0.317(14 % ) @3 0.214 0.262(22.4 % )  X  0.264 0.293(11 % ) @5 0.244 0.279(14.3 % )  X  0.293 0.311(6.1 % ) @10 0.311 0.311(0%) 0.341 0.341(0%) Table 9: Document Level Results in High Precision on TREC123 &amp; TREC4 with 100 Sample Documents
Docs Rank TREC123 TREC4 Ind Jnt Ind Jnt @5 0.328 0.329(0.3 % ) 0.283 0.301(6.4 % ) @10 0.302 0.316(4.6 % ) 0.243 0.254(4.5 % ) @15 0.288 0.306(6.2 % )  X  0.223 0.227(1.8 % ) @20 0.277 0.296(6.9 % )  X  0.195 0.204(4.6 % ) @30 0.253 0.268(5.9 % )  X  0.165 0.166(0.6 % ) Table 10: Source Level Results in Accuracy of DIGLIB with 100 Sample Documents results of TREC123 and TREC4 at document level. From this figure, we notice that the SMQE method outperforms all other metrics, due to the fact the it considers the similar-ity between queries. The SME produces a quite close-to-best result, but the SMKL tends not to be a good choice for the joint classification model. On TREC4, SMKL is compara-ble with independent model, but it is worse than SMQE and SME.

Figure 2 shows the results of DIGLIB at source level. In this case, both SMQE and SME are comparable, except for the precision at top 1. Again SMKL is not a good choice.
This paper proposes a novel joint probabilistic classifica-tion model for the resource selection task in federated text search. Existing resource selection algorithms only utilize evidence of individual information sources to select relevant sources, but they do not model the valuable relationship information between the sources. The proposed algorithm estimates the probability of relevance of information sources in a joint manner by combining both the evidence of individ-ual sources and the relationship between the sources. The importance of di ff erent types of evidence is determined in a discriminative manner for maximizing the accuracy of re-source selection with some training queries. Di ff erent types of similarity metrics have been explored to model source similarity based on the performance of available sources on training queries and the Kullback-Leibler divergence on the contents of the sources. A set of experiments were conducted with two TREC datasets and one real world application with digital libraries. The empirical results in di ff erent configu-rations have demonstrated the e ff ectiveness of the proposed joint classification model.

There are several directions to extend the research work in the paper. First, one advantage of the proposed joint prob-abilistic model is to integrate di ff erent types of evidence of Figure 1: Document Level High Precision on TREC123 &amp; TREC4 with Di ff erent Similarity Met-rics Figure 2: Source Level Accuracy on DIGLIB with Di ff erent Similarity Metrics individual sources and their relationship. We plan to explore more features for improving the performance of resource selection. For example, we can combine multiple types of similarity evidence in a single framework (with di ff erent  X  weights), which may better model sources X  relationship for more accurate resource selection. Second, the joint model in this paper utilizes a reranking approach in resource se-lection with a small set of information sources (e.g., top 10) to avoid large computational complexity. It is possible to break this limit by utilizing some other approximate infer-ence algorithms (e.g., the pseudo likelihood approach [15]) or making further assumptions on the sources X  relationship. For example, one strategy is to first divide available sources into groups of closely related sources. Inference can be con-ducted by building a small model in each group and assum-ing independence of sources between di ff erent groups.
This research was partially supported by the Vietnam Ed-ucation Foundation (VEF) and the NSF grant IIS-0749462. The opinions, findings, and conclusions stated herein are those of the authors and do not necessarily reflect those of the sponsors. [1] J. Arguello, J. Callan, and F. Diaz.
 [2] J. Arguello, F. D  X  X az, J. Callan, and J. Crespo. Sources [3] J. Callan. Distributed information retrieval. Advances [4] J. Callan and M. Connell. Query-based sampling of [5] J. Callan, W. B. Croft, and S. M. Harding. The [6] J. Callan, Z. Lu, and W. B. Croft. Searching [7] S. Cetintas, L. Si, and H. Yuan. Learning from past [8] N. Craswell, P. Bailey, and D. Hawking. Server [9] N. Fuhr. A decision-theoretic approach to database [10] N. Fuhr. Resource discovery in distributed digital [11] L. Gravano, K. Chang, C-C., H. Garc  X  X a-Molina, and [12] L. Gravano, H. Garc  X  X a-Molina, and A. Tomasic. Gloss: [13] W. Meng, C. Yu, and K. Liu. Building e ffi cient and [14] D. Metzler and W. Croft. Combining the language [15] S. Parise and M. Welling. Learning in markov random [16] J. M. Ponte and W. B. Croft. A language modeling [17] J. Seo and W. B. Croft. Blog site search using [18] M. Shokouhi. Central-rank-based collection selection [19] M. Shokouhi and J. Zobel. Robust result merging [20] L. Si and J. Callan. Relevant document distribution [21] L. Si and J. Callan. A semi-supervised learning [22] L. Si and J. Callan. Unified utility maximization [23] L. Si and J. Callan. Modeling search engine [24] P. Thomas and M. Shokouhi. Sushi: scoring scaled [25] E. Voorhees, N. K. Gupta, and B. Johnson-Laird. [26] J. Xu and J. Callan. E ff ective retrieval with [27] J. Xu and W. B. Croft. Cluster-based language models [28] B. Yuwono and D. L. Lee. Server ranking for
