 The unprecedented growth of available data nowadays has stimulated the development of new methods for organizing and extracting useful knowledge from this immense amount of data. Automatic Document Classification (ADC) is one of such methods, that uses machine learning techniques to build models capable of automatically associating documents to well-defined semantic classes. ADC is the basis of many important applications such as language identification, senti-ment analysis, recommender systems, spam filtering, among others. Recently, the use of meta-features has been shown to substantially improve the effectiveness of ADC algorithms. In particular, the use of meta-features that make a com-bined use of local information (through kNN-based features) and global information (through category centroids) has pro-duced promising results. However, the generation of these meta-features is very costly in terms of both, memory con-sumption and runtime since there is the need to constantly call the kNN algorithm. We take advantage of the current manycore GPU architecture and present a massively paral-lel version of the kNN algorithm for highly dimensional and sparse datasets (which is the case for ADC). Our experi-mental results show that we can obtain speedup gains of up to 15x while reducing memory consumption in more than 5000x when compared to a state-of-the-art parallel base-line. This opens up the possibility of applying meta-features based classification in large collections of documents, that would otherwise take too much time or require the use of an expensive computational platform.
 Document Classification; Meta-features; Parallelism
The processing of large amounts of data efficiently is crit-ical for Information Retrieval (IR) and, as this amount of data grows, storing, indexing and searching costs rise up al-together with penalties in response time. Parallel computing may represent an efficient solution for enhancing modern IR systems but the requirements of designing new parallel al-gorithms for modern platforms such as manycore Graphical Processing Units (GPUs) have hampered the exploitation of this opportunity by the IR community.

In particular, similarity search is at the heart of many IR systems. It scores queries against documents, presenting the highest scoring documents to the user in ranked order. The kNN algorithm is commonly used for this function, retriev-ing the most similar k documents for each query. Another very common application domain for kNN is Automatic Doc-ument Classification (ADC). In this case, the kNN algorithm is used to automatically map (classify) a new document d to a set of predefined classes given a set of labeled (training) documents, based on the similarities between d and each of the training docoments. kNN has been shown to produce competitive results in several datasets [17]. The speedup of this type of algorithm, using some efficient parallel strate-gies, mainly in applications in which it is repetitively applied as the core of other applications, opens huge opportunities.
One recent application which uses kNN intensively is the generation of meta-level features (or simply meta-features ) for ADC [24]. Such meta-features capture local and global information about the likelihood of a document to belong to a class, which can then be exploited by a different clas-sifier (e.g. SVM). More specifically, meta-features capture: (i) the similarity value between a test example and the near-est neighbor in each considered class, and (2) the similarity value of the test example with the classes X  centroids 1 . As shown in [24] and in our experiments, the use of meta-features can substantially improve the effectiveness of ADC algorithms.

However, the generation of meta-features is very costly in terms of both, memory consumption and runtime. In order to generate them, there is the need to constantly call the kNN algorithm. kNN, however, is known to produce poor performance (execution time) in classification tasks in com-parison to other (non-lazy) supervised methods, normally being not the best choice to deal with on-the-fly classifica-
Notice that this also has to be applied to all training doc-uments in an offline procedure. tions. Classification using kNN-based meta-features inherits this poor performance, since we need to generate meta-level features for both, all training, and each new test sample be-fore actual classification. For textual datasets (with large vocabularies), the performance problem is hardened, since the kNN algorithm will have to run on high dimensional data. In this scenario, kNN often requires large portions of memory to represent all the training data and intensive computation to calculate the similarity between points. In fact, as we shall see the generation of meta-features is not feasible using previous meta-feature generators [7, 24] for the larger datasets we experimented with.

In this paper we present a new GPU-based implemen-tation of kNN specially designed for high-dimensional and sparse datasets (which is the case for ADC), allowing a very fast and much more scalable meta-feature generation which allows one to apply this technique in large collections much faster. Some of the most interesting characteristics of our ap-proach, compared to other state-of-the-art GPU-based pro-posals, include:
Our experimental results show that we can obtain speedup gains of up to 140 x and 15 x while reducing memory con-sumption in more than 8000 x and 5000 x when compared to a standard sequential implementation and to a state-of-the-art parallel baseline, respectively.

This paper is organized as follows. Section 2 covers re-lated work. Section 3 introduces the use of meta-feature in ADC. Section 4 provides a brief introduction to paral-lelism in GPU. Section 5 describes our GPU-based imple-mentation, specially designed for highly dimensional, sparse data. Section 6 presents an analysis of the complexity of the proposed solution. Section 7 presents our experimental evaluation while Section 8 concludes the paper.
Several meta-features have been proposed to improve the effectiveness of machine learning methods. They can be based on ensemble of classifiers [3, 22], derived from clus-tering methods [11, 16] or from the instance-based kNN method [7, 24].

Meta-features derived from ensembles exploit the proba-bility distribution over all classes generated by each of the individual classifiers composing the ensemble [22]. In [3] other ensemble-based meta-features were also used, includ-ing: the entropies of the class probability distributions and the maximum probability returned by each classifier. This scheme was found to perform better than using only proba-bility distributions.

Clustering techniques may also be used to derive meta-features. In this case, the feature space is augmented using clusters derived from a previous clustering step considering both the labeled and unlabeled data [16, 11]. The idea is that clusters represent higher level  X  X oncepts X  in the feature space, and the features derived from the clusters indicate the similarity of each example to these concepts. In [16] the largest n clusters are chosen as representatives of the major concepts. Each cluster c contributes with a set o meta-features like, for instance, binary feature indicating if c is the closest of the n clusters to the example, the similar-ity of the example to the cluster X  X  centroid, among others. In [11] the number of clusters is chosen to be equal to the predefined number of classes and each cluster corresponds to an additional meta-feature.

Recently, [7] reported good results by designing meta-features that make a combined use of local information (through kNN-based features) and global information (through cate-gory centroids) in the training set. Despite the fact that these meta-features are not created based on an ensemble of classifiers, they differ from the previously presented meta-features derived from clusters because they explicitly cap-ture information from the labeled set.

Although the kNN algorithm can be applied broadly, it has some shortcomings. For large datasets ( n ) and high dimensional space ( d ), its complexity O ( nd ) can easily be-come prohibitive. Moreover, if m successive queries are to be performed, the complexity further increases to O ( mnd ). Recently, some proposals have been presented to accelerate the kNN algorithm via a highly mutithreaded GPU-based approach. The first, and most cited, GPU-based kNN im-plementation was proposed by Garcia et al. [5]. They used the brute force approach and reported speedups of up to two orders of magnitude when compared to a brute force CPU-based implementation. Their implementation assumes that multiple queries are performed and computes and stores a complete distance matrix, what makes it impracticable for large data (over 65,536 documents).

Following Garcia X  X  et al. work, Kuang and Zhao [10] im-plemented their own optimized matrix operations for calcu-lating the distances, and used radix sort to find the top-k elements. Liang et al. [14] took advantage of CUDA Streams to overlap computation and communication (CPU/GPU) when dealing with several queries, and thus decrease the GPU memory requirements. The distances were computed in blocks and later merged first locally and then globally to find the top-k elements. However, such works can still be considered brute-force. Sismanis et al. [20] concentrated on the sorting phase of the brute-force kNN and provided an extensive comparison among parallel truncated sorts. They conclude that the truncated biotonic sort (TBiS) produces the best results.

Our proposal differs from the above mentioned work in many aspects. First, it exploits a very efficient GPU imple-mentation of inverted indexes which supports an exact kNN solution without relying in brute-force. This also allows our solution to save a lot of memory space since the inverted index corresponds to a sparse representation of the data. In the distance calculation step, we resort to a smart load bal-ancing among threads to increase the parallelism. And in the sorting step, we exploit a GPU-based sorting procedure, which was shown to be superior to other partial sorting al-gorithms[20], in combination with a CPU merge operation based on a priority queue.
In here, we formally introduce the meta-features whose kNN-based calculation we intend to speed up and scale with our proposed massively parallel approach.

Let X and C denote the input (feature) and output (class) spaces, respectively. Let D train = { ( x i ,c i )  X  X  X C}| the training set. Recall that the main goal of supervised classification is to learn a mapping function h : X 7 X  X  which is general enough to accurately classify examples x 0 6 X  D train
The kNN-based meta-level features proposed in [7], are designed to replace the original input space X with a new informative and compact input space M . Therefore, each vector of meta-features m f  X  X  is expressed as the concate-nation of the sub-vectors below, which are defined for each example x f  X  X  and category c j  X  X  for j = 1 , 2 ,..., |C| as:
Considering k neighbors, the number of features in vector x f is (3 k + 2) per category, and the total of (3 k + 2) |C| for all categories. The size of this meta-level feature set is much smaller than that typically found in ADC tasks, while explicitly capturing class discriminative information from the labeled set.
In the last few years, the focus on processor architectures has moved from increasing clock rate to increasing paral-lelism. Rather than increasing the speed of its individual processor cores, traditional CPUs are now virtually all multi-core processors. In a similar fashion, manycore architectures like GPUs have concentrated on using simpler and slower cores, but in much larger counts, in the order of thousands of cores. The general perception is that processors are not getting faster, but instead are getting wider, with an ever in-creasing number of cores. This has forced a renewed interest in parallelism as the only way to increase performance.
The high computational power and affordability of GPUs has led to a growing number of researchers making use of GPUs to handle massive amounts of data. While multicore CPUs are optimized for single-threaded performance, GPUs are optimized for throughput and a massive multi-threaded parallelism. As a result, the GPUs deliver much better en-ergy efficiency and achieves higher peak performance for throughput workloads. However, GPUs have a different ar-chitecture and memory organization and to fully exploit its capabilities it is necessary considerable parallelism (tens of thousands of threads) and an adequate use of its hardware resources. This imposes some constraints in terms of de-signing appropriate algorithms, requiring the design of novel solutions and new implementation approaches. However, a few research groups and companies have faced this challenge with promising results in Database Scalability, Document Clustering, Learning to Rank, Big Data Analytics and In-teractive Visualization [1, 21, 19, 15, 8].

The GPU consists of a M-SIMD machine, that is, a Mul-tiple SIMD (Single Instruction Multiple Data) processor. Each SIMD unit is known as a streaming multiprocessor (SM) and contains streaming processor (SP) cores. At any given clock cycle, each SP executes the same instruction, but operates on different data. The GPU supports thousands of light-weight concurrent threads and, unlike the CPU threads, the overhead of creation and switching is negligible. The threads on each SM are organized into thread groups that share computation resources such as registers. A thread group is divided into multiple schedule units, called warps, that are dynamically scheduled on the SM. Because of the SIMD nature of the SP X  X  execution units, if threads in a schedule unit must perform different operations, such as go-ing through branches, these operations will be executed seri-ally as opposed to in parallel. Additionally, if a thread stalls on a memory operation, the entire warp will be stalled until the memory access is done. In this case the SM selects an-other ready warp and switches to that one. The GPU global memory is typically measured in gigabytes of capacity. It is an off-chip memory and has both a high bandwidth and a high access latency. To hide the high latency of this mem-ory, it is important to have more threads than the number of SPs and to have threads in a warp accessing consecutive memory addresses that can be easily coalesced. The GPU also provides a fast on-chip shared memory which is acces-sible by all SPs of a SM. The size of this memory is small but it has a low latency and it can be used as a software-controlled cache. Moving data from the CPU to the GPU and vice versa is done through a PCIExpress connection.
The GPU programming model requires that part of the application runs on the CPU while the computationally-intensive part is accelerated by the GPU. The programmer has to modify his application to take the compute-intensive kernels and map them to the GPU. A GPU program exposes parallelism through a data-parallel SPMD (Single Program Multiple Data) kernel function. During implementation, the programmer can configure the number of threads to be used. Threads execute data parallel computations of the kernel and are organized in groups called thread blocks, which in turn are organized into a grid structure. When a kernel is launched, the blocks within a grid are distributed on idle SMs. Threads of a block are divided into warps, the sched-ule unit used by the SMs, leaving for the GPU to decide in which order and when to execute each warp. Threads that belong to different blocks cannot communicate explic-itly and have to rely on the global memory to share their results. Threads within a thread block are executed by the SPs of a single SM and can communicate through the SM shared memory. Furthermore, each thread inside a block has its own registers and private local memory and uses a global thread block index, and a local thread index within a thread block, to uniquely identify its data. The proposed parallel implementation, called GPU-based Textual kNN (GT-kNN), greatly improves the k nearest neighbors search in textual datasets. The solution efficiently implements an inverted index in the GPU, by using a paral-lel counting operation followed by a parallel prefix-sum cal-culation, taking advantage of Zipf X  X  law, which states that in a textual corpus, few terms are common, while many of them are rare. This makes the inverted index a good choice for saving space and avoiding unnecessary calculations. At query time, this inverted index is used to quickly find the documents sharing terms with the query document. This is made by constructing a query index which is used for a load balancing strategy to evenly distribute the distance cal-culations among the GPU X  X  threads. Finally, the k nearest neighbors are determined through the use of a truncated bitonic sort to avoid sorting all computed distances. Next we present a detailed description of these steps.
The inverted index is created in the GPU memory, assum-ing the training dataset fits in memory and is static. Let V be the vocabulary of the training dataset, that is the set of distinct terms of the training set. The input data is the set E of distinct term-documents ( t,d ), pairs occurring in the original training dataset, with t  X  V and d  X  D train . Each pair ( t,d )  X  E is initially associated with a term frequency tf , which is the number of times the term t occurs in the document d . An array of size |E| is used to store the inverted index. Once the set E has been moved to the GPU memory, each pair in it is examined in parallel, so that each time a term is visited the number of documents where it appears (document frequency -df ) is incremented and stored in the array df of size |V| . A parallel prefix-sum is executed, using the CUDPP library [18], on the df array by mapping each el-ement to the sum of all terms before it and storing the results in the index array. Thus, each element of the index array points to the position of the corresponding first element in the invertedIndex , where all ( t,d ) pairs will be stored or-dered by term. Finally, the pairs ( t,d ) are processed in paral-lel and the frequency-inverse document frequency tf -idf ( t,d ) for each pair is computed and included together with the documents identification in the invertedIndex array, using the pointers provided by the index array. Also during this parallel processing, the value of the norm for each training document, which is used in the calculus of the cosine or Eu-clidean distance, is computed and stored in the norms array. Algorithm 1 depicts the inverted index creation process. Figure 1 illustrates each step of the inverted index creation
Algorithm 1: CreateInvetedIndex ( E ) for a five documents collection where only five terms are used. If we take t 2 as an example, the index array indicates that its inverted document list ( d 2 ,d 4 ) starts at position 3 of the invertedindex array and finishes at position 4 (5 minus 1).

Once the inverted index has been created, it is now pos-sible to calculate the distances between a given query doc-ument q and the documents in D train . The distances com-putation can take advantage of the inverted index model, because only the distances between query q and those doc-uments in D train that have terms in common with q have to be computed. These documents correspond to the elements of the invertedIndex pointed to by the entries of the index array corresponding to the terms occurring in the query q .
The obvious solution to compute the distances is to dis-tribute the terms of query q evenly among the processors and let each processor p access the inverted lists corresponding to terms allocated to it. However, the distribution of terms in documents of text collections is known to follow approx-imately the Zipf Law. This means that few terms occur in large amount of documents and most of terms occur in only few documents. Consequently, the sizes of the inverted list also vary according to te Zipf Law, thus distributing the work load according to the terms of q could cause a great imbalance of the work among the processors.

In this paper besides using an inverted index to boost the computation of the distances, we also propose a load balance method to distribute the documents evenly among the pro-cessors so that each processor computes approximately the same number of distances. In order to facilitate the expla-nation of this method, suppose that we concatenate all the inverted lists corresponding to terms in q in a logical vector E q = [ 0 .. | E q | X  1 ], where | E q | is the sum of the sizes of all inverted lists of terms in q . Considering the example in Fig. 1 and supposing that q is composed by the terms t 1 , t 3 t , the logical vector E q would be formed by the following pairs of the inverted index: E q = [( t 1 ,d 1 ) , ( t 1 ,d ( t ,d 1 ) , ( t 3 ,d 5 ) , ( t 4 ,d 1 )] and | E q | equals to six.
Given a set of processors P = { p 0 ,  X  X  X  p |P| X  1 } , the load balance method should allocate elements of E q in inter-vals of approximately the same size, that is, each proces-sor p i  X  P should process elements of E q in the interval [ i d ample stated above, and suppose that the set of processors is P = { p 0 ,p 1 ,p 2 } . Thus elements of E q with indices in the interval [0 , 1] would be assigned to p 0 , indices in [2 , 3] would be processed by p 1 and indices in [4 , 5] would be processed by p 2 .

Since each processor knows the interval of the indices of the logical vector E q it has to process, all that is necessary to execute the load balancing is a mapping of the logical indices of E q to the appropriate indices in the inverted index (ar-ray invertedIndex ). In the case of the example associated to Fig. 1, the following mappings between logical indices and indices of the invertedIndex array must be performed: 0  X  0, 1  X  1, 2  X  2, 3  X  5, 4  X  6 and 5  X  7. Each processor executes the mapping for the indices in the inter-val corresponding to it and finds the corresponding elements in the invertedIndex array for which it has to compute the distances to the query.

Let V q  X  V be the vocabulary of the query document d . The mapping proposed in this paper uses three aux-iliary arrays: df q [ 0 .. |V q |  X  1 ], start q [ 0 .. |V index q [ 0 .. |V q | X  1 ]. The arrays df q and start q are obtained together by copying in parallel df [ t i ] to df q [ t i ] and index [ t to start q [ t i ], respectively, for each term t i in the query q . Once the df q is obtained, an inclusive parallel prefix sum on df q is performed and the results are stored in index q .
Algorithm 2 shows the pseudo-code for the parallel com-putation of the distances between documents in the training set and the query document. In lines 4-7 the arrays df q start q are obtained. In line 8 the array index q is obtained by applying a parallel prefix sum on array df q . Next, each processor executes a mapping of each position x in the in-terval of indices of E q associated to it to the appropriate position of the invertedIndex. This mapping is described in lines 10-17 of the algorithm. Then, the mapped entries of the inverted index are used to compute the distances be-tween each document associated with these entries and the query.
 Figure 2 illustrates each step of Algorithm 2 for a query
Algorithm 2: DistanceCalculation ( invertedIndex,q ) Figure 2: Example of the execution of Algorithm 2 for a query with three terms. containing three terms, t 1 , t 3 and t 4 , using the same col-lection presented in the example of Figure 1. Initially, the arrays df q and start q are obtained by copying in parallel en-tries respectively from arrays df and index , corresponding to the three query terms. Next a parallel prefix sum is applied to array df q and the index q array is obtained. Finally the Figure shows the mapping of each position of the logical ar-ray E q into the corresponding positions of the invertedIndex array.
With the distances computed, it is necessary to obtain the k closest documents. This can be accomplished by mak-ing use of a partial sorting algorithm on the array contain-ing the distances, which is of size | D train | . For this, we im-plemented a parallel version of the Truncated Bitonic Sort (TBiS), which was shown to be superior to other partial sorting algorithms in this context [20]. One advantage of the parallel TBiS is data independence. At each step, the algo-rithm distributes elements equally among the GPU X  X  threads avoiding synchronizations as well as memory access conflicts. Although the partial bitonic sort is O ( | D train | log 2 than the best known algorithm which is O ( | D train | log k ), for a small k the ratio of log k becomes almost negligible. In the case of ADC using kNN, the value of k is usually not greater than 50. Our parallel TBiS implementation also uses a re-duction strategy, allowing each GPU block to act indepen-dently from each other on a partition of array containing the computed distances. Results are then merged in the CPU using a priority queue.
In this section we analyze the amount of time and mem-ory used to construct the index and to compute the k near-est neighbors for a given query document q . The first step of the construction of the inverted index is to obtain the df array (line 5 of the Algorithm 1). During this step, the set of input pairs E is read in parallel by all proces-sors in P and for each term the corresponding document counter is incremented. This takes time O ( | E | |P| ). The par-allel prefix sum algorithm applied to array df to obtain the index takes time O ( |V| |P| log |V| ) [18]. Next, the computa-tion of tf -idf , the computation of accumulated square of tf -idf (to compose the norms of the documents), and the insertion of pairs in the invertedIndex (lines 9-11) are done by accessing elements of E in parallel, thus taking time O ( | E | |P| ). Finally the square roots of the norms are computed in time O ( | D train | |P| ). The total time of the index construction time complexity of the index construction is O ( | E | |P|
The computation of the distances between each training document and the query document q starts by obtaining the arrays df q and start q (lines 4-7 of Algorithm 2). This step is executed in parallel by all processors in P . Thus the two arrays are computed in time O ( |V q | |P| ). The computation of array index q is the result of the use of the parallel prefix sum on array df q , thus it is done in time O ( |V q | |P|
Each processor p i  X  P executes the mapping of | E q | |P| sitions of the logical array E q . It is possible to estimate the value of | E q | in terms of the sizes of V and V member that the logical array E q represents the concate-nation of all inverted lists of terms in the query document q , that is, |V q | inverted lists. Considering that the train-ing collection follows the Zipf Law, we have that the prob-ability of the occurrence of a term t with rank k is given terizing the distribution. The term with greatest probabil-ity is the term with rank 1. Thus the expected document frequency of this term is given by | E | classic version of Zipf X  X  law, the exponent s is 1, then the document frequency of this term is | E | value represents an upper bound for lengths of the docu-ment frequency of each term in q . Thus, in the worst case, we have that | E q | X  X V q | | E | ln |V| . According to Heaps Law, the size of vocabulary is |V| = k | W |  X  , where k is a constant, usually in the range 10-100, W is the set formed by all oc-currences of all terms in the collection, and  X  is another constant in the range 0 . 4-0 . 6. The size of W can be taken as an upper bound for the size of the input pairs E . Thus | E We conclude that and each processor p i executes the map-ping of O ( |V q | |P| ) positions.

Now we analyze the time to compute the mapping of a sin-gle position (lines 11-17 of Algorithm 2). The computing of variable pos in line 11 can be performed in time O (log |V because values in array df q are disposed in ascending or-der and a binary search can be used to find the minimum index required. All the remaining operations (lines 12-18) are computed in constant time ( O (1)). The processing of each mapped pair of the inverted index, as part of the com-putation of the distance between q and the corresponding document in the pair, is also done in constant time. Thus, the execution time of one iteration of inner loop (lines 10-18) is O (log |V q | ) + O (1) = O (log |V q | ). Finally the partial sort of the distances is computed in time O ( | D train | log k ). Con-sequently, the overall execution time of Algorithm 2 corre-O ( | D train | log k ) = O ( |V q | |P| )( O (log |V q | ) + O ( |
The work of Garcia et al.[5] processes many query docu-ments in parallel, however, each query q is compared to every document in the training set. Besides, the query and each document are represented as arrays of sizes |V| . Thus, the processing time of query q is O ( |V|| D train | )+ O ( | When Comparing the speedup of our solution over the Garcia X  X  algorithm we do not take into consideration the time to sort the array of distances, since this task ads the same computing time in both solution. As consequence the per bound for |V q | , we have that the speedup obtained is:
If we consider that number of processors is constant (in one GPU), and that, according to Heaps Law the number of new words in vocabulary V does not grow much as the collection size increases, we have that, the speedup increases proportionally to the number of documents in the collection.
Considering memory space requirements, the proposed so-lution consumes 2 |E| units of memory to store arrays E and invertedIndex , consumes 2 |V| units to store arrays df and index , consumes O ( |V q | ) space to store the related-to-query arrays ( df q , start q , and index q ) and O ( | D train store the array containing the norms of the documents and the array containing the distances. Thus, the space complex-ity of the solution is O ( |E| )+ O ( |V| )+ O ( |V q | )+ O ( | O ( |E| ) + O ( | D train | ).

The solution presented by Garcia et al. [5] uses a matrix with dimensions |V|| D train | to store the training set and an array of size |V| to store the query q . Thus the space com-plexity of their solution is O ( |V|| D train | ) + O ( | ratio between the space used by Garcia X  X  solution and ours sparsity of the matrix storing the training set in Garcia X  X  so-lution.
In order to evaluate the meta-feature strategies, we con-sider six real-world textual datasets, namely, 20 Newsgroups, Four Universities, Reuters, ACM Digital Library, MEDLINE and RCV1 datasets. For all datasets, we performed a tradi-tional preprocessing task: we removed stopwords, using the standard SMART list, and applied a simple feature selection by removing terms with low  X  X ocument frequency (DF) X  2 . Regarding term weighting, we used TFIDF for both, SVM and kNN. All datasets are single-label. In particular, in the case of RCV1, the original dataset is multi-label with the multi-label cases needing special treatment, such as score thresholding, etc. (see [13] for details). As our current focus is on single-label tasks, to allow a fair comparison among the other datasets (which are also single-label) and all baselines (which also focus on single-label tasks), we decided to trans-form all multi-label cases into single-label ones. In order to do this fairly, we randomly selected, among all documents with more than one label, a single label to be attached to that document. This procedure was applied in about 20% of the documents of RCV1 which happened to be multi-label. More details about the datasets are shown in Table 1.
All experiments were run on a Intel R  X  i7-870, running at 2 . 93GHz, with 16Gb RAM. The GPU experiments were run on a NVIDIA Tesla K40, with 12Gb RAM. In order to con-sider the costs of all data transfers in our efficiency experi-ments, we report the wall times on a dedicated machine so as to rule out external factors, like high load caused by other processes. To compare the average results on our cross-validation experiments, we assess the statistical significance of our results with a paired t-test with 95% confidence and Bonferroni correction to account for multiple tests. This test assures that the best results, marked in bold , are statisti-cally superior to others.

We compare the computation time to generate meta-features using three different algorithms: (1) GT k NN , our GPU-based implementation of kNN 3 ; (2) BF-CUDA , a brute force kNN implementation using CUDA proposed by Gar-cia et al. [5]; and (3) ANN , a C++ library that supports
We removed all terms that occur in less than six documents (i.e., DF &lt; 6).
Source code is available under GNU Public License (GPL) at http://purl.oclc.org/NET/gtknn/ . exact and approximate nearest neighbor searching 4 . We use the ANN exact version, since it was used in the previous meta-feature works [7, 24]. We chose BF-CUDA because it is the main representative of the GPU-based brute force ap-proach. However, the other implementations mentioned in Section 2 (some not available for download) also work with a complete distance matrix and would produce similar results.
We also conducted controlled experiments to evaluate the effectiveness of classifiers learned with three different sets of features. The names and the descriptions of each set of fea-tures are given as follows: (1) Bag of Words , a set contain-ing only the original features, i.e, TF-IDF weights of the doc-ument X  X  terms; (2) Meta-features , a set of meta-features proposed recently in literature (state-of-art meta-features) and described in Section 3; and (3) Bag + Meta-features , the combination of the above sets. The effectiveness of the features were compared using two standard text categoriza-tion measures: micro averaged F 1 (MicroF 1 ) and macro av-eraged F 1 (MacroF 1 ), which capture distinct aspects of the ADC task [13, 23]. To evaluate the performance of dif-ferent groups of features, we adopted the LIBLINEAR [4] implementation of the SVM classifier, which received as in-put either the original space of features (Bag of Words), the Meta-features we generate to represent the original space, or the combination Bag + Meta-features. The regularization parameter was chosen by using 5-fold cross-validation in the training set. For the size of neighborhood used for generat-ing the kNN-based meta-features, we adopted k = 30 in all experiments, since it was empirically demonstrated as the best parameter for text classification [2, 9, 23].

We would like to point out that some of the results ob-tained in some datasets with and without the meta-features may differ from the ones reported in other works for the same datasets (e.g., [6, 12]). Such discrepancies may be due to several factors such as differences in dataset preparation the use of different splits of the datasets (e.g., some datasets have  X  X efault splits X  such as REUT and RCV1 6 ), the appli-cation of some score thresholding, such as SCUT and PCUT, which, besides being an important step for multi-label prob-lems, also affects classification performance by minimizing class imbalance effects, among other factors. We would like to stress that we ran all alternatives under the same conditions in all datasets, using the best traditional fea-ture weighting scheme, using standardized and well-accepted cross-validation procedures that optimize parameters for each of alternatives, and applying the proper statistical tools for the analysis of the results. Our datasets are available for result replication and testing of new configurations.
We start by demonstrating the effectiveness of the meta-features. As shown in Table 2, in most datasets, the use of the traditional bag-of-words was statistically worse than meta-features, justifying meta-features as a replacement for http://www.cs.umd.edu/~mount/ANN/
For instance, some works do exploit complex feature weighting schemes or feature selection mechanisms that do favor some algorithms in detriment to others.
We believe that running experiments only in the default splits is not the best experimental procedure as it does not allow a proper statistical treatment of the results. the original high dimensional feature space, as demonstrated in previous work. Most of the results of SVM on the original space (bag-of-words) are superior or tied with kNN.
The only datasets in which the effectiveness of meta-features was not better than that of bag-of-words were MED and RCV1Uni. We hypothesize that it is due to the fact that these datasets have a large training dataset, which allows the classification method (SVM) to deal better with the highly dimensional data, since there is enough training examples to learn discriminative patterns from more dimensions. Although bag-of-words achieved good results in MED and RCV1Uni, the combination Bag + Meta-features proposed in this work achieved the best results all datasets but REUT90, as shown in Table 3. This demonstrates the complementar-ity between the Bag and Meta-features in these datasets. REUT90 was the only case in which bag + meta-features was worse than the meta-features alone. Since this dataset has only a few training examples per class, the inclusion of more noisy features (from the bag-of-words) only makes it more difficult to find an effective SVM model.
Table 4 shows the average time to generate meta-features for a batch of test examples using ours and the baseline X  X  kNN implementations. Since we use a 5-fold cross validation, we measure the time to generate meta-features for all the examples in the batch of examples in each test fold. Notice that the time to generate meta-features is practically the average time to classify a fold, since the time to classify the test examples with the SVM implementation is negligible.
As can be seen in Table 4, the generation of meta-features using GT k NN shows significant speedups in relation to the other kNN implementations. In particular, the speedups for the small datasets range from 4.8 to 141.3 in relation to the ANN implementation, used in previous works to generate meta-features. This high speedup was somewhat expected, since the ANN do not explore parallelism to compute the distances. However, even when compared to the parallel BF-CUDA implementation[5], GT k NN was able to achieve speedups ranging from 3.6 to 15.7. This was possible mainly because BF-CUDA does not optimize the distance calcula-tions to deal with the low density of terms present in textual documents nor tries to balance the load among threads. GT k NN produced the best speedups in 4UNI, 20NG and ACM, but it obtained a lower speedup in REUT90. This may be due to the fact that REUT90 has a large number of classes and only a few documents in each class. Since the meta-features are generated one class at a time, we could not explore the parallelism in its full potential. For example, if there are only 10 training documents in a class, we can only perform at most 10 simultaneous distance calculations, leaving most CUDA cores idle.

GT k NN was the only implementation able to generate meta-features for the larger datasets: MED and RCV1Uni. In fact, ANN and BF-CUDA are extremely slow in this con-text. Figure 3 shows the time to generate meta-features for a single test example considering a sample size of 100 up to 2000 documents in MED. Since this dataset has more than 800,000 features, even a small fraction of it (e.g. 2000 documents -0.3%) requires a significant time (2.5 and 1.5 seconds for ANN and BF-CUDA respectively) to generate meta-features. On the contrary, GT k NN takes no more than 0.005 seconds to generate the meta-features. This discrep-ancy is expected since the calculations involved to deal with such high dimensions (greater than 800,000) are huge. The sequential nature of ANN and the lack of support for data sparsity make both ANN and BF-CUDA less competitive in this situation.

The average time to generate meta-features for a single test example is presented in Table 5. In this experiment, we use a leave-one-out methodology, where only one example is chosen as a test example, leaving all others as training examples. GT k NN never takes more than 0.2 seconds to generate meta-features to a test example. It makes possible to classify examples on-the-fly even using very big training datasets. The most costly examples are in REUT90 and RCV1Uni due the fact that GT k NN cannot fully explore the parallelism with multiple classes on the current imple-mentation, as described before.

Although the ANN speedups are similar in Tables 5 and 4, we notice that the BF-CUDA speedups in Table 5 are much higher than the ones previously presented in Table 4. This happens because the BF-CUDA implementation is optimized to perform multiple queries in parallel, which matches the needs for generating meta-features to multiple examples. However, when a single test example has to be processed, the meta-feature generation using the BF-CUDA implementation is considerably slowed down.
For textual datasets, the traditional data representation using a D x V matrix with D training documents and V features (the vocabulary of the collection) is not a good choice as the density of words in each document is very low. The proposed GT k NN approach represents textual data in a very compact way by exploiting an efficient in-memory GPU-based implementation of an inverted index, allowing us to store only the word statistics of each document.
Table 6 shows the memory consumption figures required to generate meta-features using the three kNN implementa-tions. The numbers correspond to the peak memory con-sumption during the meta-feature generation process. An estimate of memory consumption, based on the allocation of data structures, was made for the ANN and BF-CUDA im-plementations with MED and RCV1Uni, since these imple-mentations are not capable of processing these large datasets.
Our GT k NN implementation stands out way ahead of the competition when it comes to memory usage. The most impressive memory reduction occurs in MED. Although the second largest dataset, MED has only a few classes. Since our meta-feature generation strategy is performed one class at a time, a great number of documents has to be processed for each class. Thus MED consumes the largest amount of memory (339 MB) for our implementation, but the baselines ANN and BF-CUDA consume more than 5,000 x and 8,000 x this space respectively for the same dataset. We can also obtain a large memory demand reduction in RCV1, but not as expressive as in MED due to its large number of classes. In general, for all kNN implementations, the peak memory usage depends on the class with most training examples.
The use of meta-features in automatic document classi-fication has permitted important improvements in the ef-ficiency of classification algorithms. One of these meta-feature approaches is based on intensive use of the kNN al-gorithm, in order to exploit local information regarding the neighborhood of training documents. However, intensive use of kNN, combined with the high dimensionality and spar-sity of textual data, make this a challenging computational task. We have presented a very fast and scalable GPU-based approach for computing kNN-based meta-feature document classification. Different from other GPU-based kNN imple-mentations, we avoid comparing the query document with all training documents. Instead we build an inverted index in the GPU that is used to quickly find the documents shar-ing terms with the query document. Although the index does not allow a regular and predictable access to the data, we use a load balancing strategy to evenly distributed the computation among thousand threads in the GPU. After calculating the distances, we again use a massive number of threads to select the k smallest distance by implementing a truncated bitonic sort in the GPU followed by a merge operation in the CPU. We tested our approach in a very memory-demanding and time consuming task which requires intensive and recurrent execution of kNN. Our results show very significant gains in speedup and memory consumption when compared to our baselines. In fact, running our base-lines in the largest datasets demonstrated to be unfeasible, stressing the value of our contribution. And even in scenar-ios where the dataset is too big for our implementation (a single GPU), our approach can easily be extended by spiting the dataset, executing the kNN in each part, and merging the partial results. Thus, meta-feature based classification can be applied in huge collections of documents, taking a rea-sonable time and without requiring expensive machines. As future work, we intend to approach other tasks and exploit Multi-GPU platforms to improve even further our solutions. This work was partially supported by CNPq, CAPES, FAPEMIG, INWEB and FAPEG. We thank NVIDIA Cor-poration for equipment donations. [1] Y.-S. Chang, R.-K. Sheu, S.-M. Yuan, and J.-J. Hsu. [2] H. Chen and T. K. Ho. Evaluation of decision forests [3] S. Dzeroski and B. Zenko. Is combining classifiers with [4] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, [5] V. Garcia, E. Debreuve, and M. Barlaud. Fast k [6] S. Godbole and S. Sarawagi. Discriminative methods [7] S. Gopal and Y. Yang. Multilabel classification with [8] T. Graham. A gpu database for real-time big data [9] T. Joachims. Text categorization with suport vector [10] Q. Kuang and L. Zhao. L.: A practical gpu based knn [11] A. Kyriakopoulou and T. Kalamboukis. Using [12] M. Lan, C.-L. Tan, and H.-B. Low. Proposing a new [13] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A [14] S. Liang, C. Wang, Y. Liu, and L. Jian. Cuknn: A [15] O. Netzer. Getting big data done on a gpu-based [16] B. Raskutti, H. L. Ferr  X a, and A. Kowalczyk. Using [17] F. Sebastiani. Machine learning in automated text [18] S. Sengupta, M. Harris, M. Garland, and J. D. Owens. [19] A. Shchekalev. Using gpus to accelerate learning to [20] N. Sismanis, N. Pitsianis, and X. Sun. Parallel search [21] B. E. Teitler, J. Sankaranarayanan, H. Samet, and [22] K. M. Ting and I. H. Witten. Issues in stacked [23] Y. Yang. An evaluation of statistical approaches to [24] Y. Yang and S. Gopal. Multilabel classification with
