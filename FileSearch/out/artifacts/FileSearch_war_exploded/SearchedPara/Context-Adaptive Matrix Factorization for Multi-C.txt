 { Data sparsity is a long-standing challenge for recommender sys-tems based on collaborative filtering. A promising solution for this problem is multi-context recommendation, i.e., leveraging users X  explicit or implicit feedback from multiple contexts. In multi-context recommendation, various types of interactions between entities (users and items) are combined to alleviate data sparsity of a single con-text in a collective manner. Two issues are crucial for multi-context recommendation: (1) How to differentiate context-specific factors from entity-intrinsic factors shared across contexts? (2) How to capture the salient phenomenon that some entities are insensitive to contexts while others are remarkably context-dependent? Previous methods either do not consider context-specific factors, or assume that a context imposes equal influence on different entities, limiting their capability of combating data sparsity problem by taking full advantage of multiple contexts.

In this paper, we propose a context-adaptive matrix factorization method for multi-context recommendation by simultaneously mod-eling context-specific factors and entity-intrinsic factors in a unified model. We learn an entity-intrinsic latent factor for every entity, and a context-specific latent factor for every entity in each context. Meanwhile, using a context-entity mixture parameter matrix we ex-plicitly model the extent to which each context imposes influence on each entity. Experiments on two real scenarios demonstrate that our method consistently outperforms previous multi-context rec-ommendation methods on all different sparsity levels. Such a con-sistent performance promotion forms the unique superiority of our method, enabling it to be a reliable model for multi-context recom-mendation.
 H.2.8.d [ Information Technology and Systems ]: Database Appli-cations -Data Mining This work is done when Junming Huang as a PhD candidate in Institute of Computing Technology, Chinese Academy of Sciences. c  X  Algorithms, Measurement, Experimentation Recommender System, Collaborative Filtering, Multi-context Rec-ommendation, Data Sparsity
With the increasingly growing amount of information available online, recommender system becomes a necessary tool to help users efficiently find their desired items, e.g., movies [1], music [2], news [3], books [4], online friends [5] and travel packages [6]. Personalized recommender systems gain great success in industry by collecting and analyzing users X  explicit or implicit feedback, generally repre-sented as interactions between users and items . For example, users offer explicit scores to movies they watched or books they read; users listen to their favorite songs multiple times; users frequently browse the products they potentially want to buy. Based on these observed interactions, collaborative filtering is widely used to in-fer users X  preference and to predict items they are likely to inter-act with. Existing collaborative filtering methods could be classi-fied into neighborhood-based methods [7] and model-based meth-ods [8, 9]. Among those methods, matrix factorization [1] is the mainstream technique in personalized recommender system.

Matrix factorization (MF) formalizes collaborative filtering into a matrix completion problem, recovering an incomplete user-item preference matrix according to observed interactions or ratings be-tween users and items [1]. MF factorizes the preference matrix into two low-rank matrices that represent latent factors for users and items respectively. With the two latent factor matrices, rec-ommendation problem is addressed simply by their inner product. Recommendation accuracy of matrix factorization heavily depends on observed preference matrix. Unfortunately, preference matrix is often highly sparse in practical scenarios, forming a hard barrier for most real-world recommender systems [10]. Moreover, pref-erence matrix is unevenly distributed , i.e., a majority of inactive users express preference on a small number of items and a ma-jority of unpopular items get few feedbacks. Such a data sparsity problem is particularly severe for newly-joining users and newly-arriving items, forming the so-called cold-start problem. As the number of users and items rapidly grows, the sparsity problem and the cold-start problem become the bottleneck of modern recom-mender systems in business.

Many efforts have been made to combat the long-standing data sparsity problem. Existing methods could be roughly classified into two paradigms. The first focuses on improving recommendation methods or models, dealing with data sparsity problem by introduc-ing appropriate priors or regularization [11]. Although some im-provements have been achieved, these methods quickly hit the limit caused by data sparsity of single context. Therefore researchers turn to multi-context recommendation, leveraging feedbacks or rat-ings from multiple contexts to improve the recommendation accu-racy in a particular context [12, 13, 14]. This is motivated by the fact that entities (users or items) are always involved in multiple contexts. For example, one user may express his/her preferences on music at Last.fm and preferences on videos at YouTube; one movie could receive ratings in different movie-rating websites, such as Netflix and MovieLens. Moreover, the degree of data sparsity for a user/item could be quite different across contexts. Figure 1 illus-trates the distribution of rating counts for 140 randomly-sampled movies occurring in two different datasets  X  Netflix and Movie-Lens. Although the distribution of rating counts in the two datasets as a whole is linearly correlated, several movies have remarkably different rating counts in the two datasets. For example, the French action thriller film  X  Le Professionnel  X  has only a few ratings in Net-flix but gets a lot of ratings in MovieLens, and the western adven-ture film  X  Last of the Dogmen  X  is popular in Netflix but cold in MovieLens. For multi-context recommendation, it is expected that users/items that have no or little data in the context of interest may have data in other contexts that could be leveraged to improve the recommendation in the target context.

Collective matrix factorization (CMF) was first proposed to ad-dress the multi-context recommendation problem [12]. CMF as-sumes that one entity (user or item) shares the same latent fac-tor across different contexts . The global factor for each entity is learned based on the observed interaction data from all the contexts in which this entity is involved. CMF simply takes the observed in-teractions in all contexts as being from a single one. Consequently, in CMF, there is a danger that the context with high number of observed interactions could dominate other contexts with few ob-servations. This is problematic for multi-context recommendation because the volume of interactions in different contexts could be remarkably different in real scenario. To address this problem, lo-calized matrix factorization (LMF) [13] and HeteroMF [14] were proposed. In the two models, each entity has two latent factors, i.e., a global latent factor shared across different contexts and a lo-cal latent factor specific to each context. Context-specific factors are viewed as being generated from the global one by multiplying it with a context-specific transfer matrix. By modeling context-(a) 20,000 Leagues Under the
Sea Figure 2: Rating distribution of two movies under two different contexts, i.e., MovieLens and Netflix. specific factors and context-independent factors separately, these two models outperform CMF for entities that have spare obser-vations in one context but lots of observations in other contexts. However, their performance is limited by their underlying assump-tion: the same context exerts the same influence to all entities in-discriminately, reflected by the transfer matrix specific to each con-text. Indeed, the influence of the same context to different entities could be quite different. Figure 2 depicts the rating distribution of two movies under two contexts. For movie  X  20,000 Leagues Under the Sea  X , the rating distribution is almost the same in these two contexts, indicating that it is insusceptible to contexts. For movie  X  Mad Dog Time  X , on the other hand, its rating distribution is highly affected by contexts. Neglecting the difference at the degree to which contexts influence different entities, existing models fail to fully, even improperly, capture the knowledge offered by multi-context observations.

In this paper, we address two critical issues in multi-context recommendation: (1) How to differentiate context-specific factors from entity-intrinsic factors shared across contexts? (2) How to capture such a salient phenomenon: some entities are insensitive to contexts while others are remarkably context-dependent? We pro-pose a context-adaptive matrix factorization (AdaMF) method, ad-dressing the multi-context recommendation problem via simultane-ously modeling context-specific factors and entity-intrinsic factors in a unified model. We learn for every entity an entity-intrinsic la-tent factor shared cross different contexts and a latent factor specific to each context. Meanwhile, using a context-entity mixture param-eter matrix we explicitly model the degree to which each context imposes influence on each entity. We develop a Gaussian mixture model to describe the interaction between users X  intrinsic factors and users X  context-specific factors. Experiments on two real sce-narios demonstrate that our method consistently outperforms pre-vious methods for multi-context recommendation on all different sparsity levels.

Our main contributions are summarized as follows:
This paper is organized as follows: we first give some prelim-inaries about multi-context recommendation. Next, related works are described in Section 3. In Section 4, we propose our model and design an EM framework to train the model. In Section 5, we present our experiments on two multi-context datasets, namely an item-aligned context and a user-aligned context. Section 6 con-cludes this paper.
Let us consider L different contexts, each of which could repre-sent a recommendation application. We use special indexing letters to distinguish users from items: a user set U with M users and an item set I with N items. These users and items are embedded in the L contexts. We use R ( l ) ij to denote the rating that user i gives to item j in context l . All the rating information in context l is stored in a context-specific rating matrix R ( l ) . Typically, R ( l ) We denote all rating matrices as R = { R (1) ,R (2) ,...,R the multi-context recommendation problem could be formalized as:
Multi-Context Recommendation: Observing the rating infor-mation R from L different contexts, the user set U and item set I , for a user-item pair ( i,j ) and a context l such that R ( l )
In real application, there are two typical scenarios of multi-context recommendation, i.e., user-aligned scenario and item-aligned sce-nario. For user-aligned scenario, users are involved in multiple con-texts. For example, a user may register in multiple online social networks, such as Facebook and Twitter. Even in one network, the user could be involved in multiple contexts, reflecting their differ-ent actions or feedbacks. For example, in Twitter, a user involves himself in multiple contexts by following other users or forwarding tweets. The other scenario is item-aligned. Taking movie recom-mendation as an example, there are many movie rating resources, such as Netflix, MovieLens, and IMDB. With movies X  names, re-lease date, actors, directors, and other information, we can identify movies that appear in multiple contexts.
 Context is a multi-faceted concept across different research disci-plines. For context-aware recommender systems (CARS) [15, 16], the term  X  X ontext X  refers to the contextual information when a user takes an action on an item, e.g., rating a movie. Contextual in-formation can be explicit, like time, location, and mood. CARS aims to incorporate these explicit information into recommenda-tion models.

The context considered in this paper refers to the data source of users X  (items X ) information. In the real world, users X  and items X  in-formation is decentralized into multiple data sources, which could be considered as a context for the users and items involved. Some related works considered cross-domain recommendation [17, 18], a special scenario for multi-context problem where users are aligned across different contexts and items are heterogenous in different contexts. The definition of multi-context recommendation is more general, since items could be homogenous across different con-texts. To summarize, the key point of multi-context recommen-dation is the method of combining the information from different contexts, where items from different contexts could be heteroge-nous or homogeneous.
 Matrix factorization (MF) is one of the state-of-the-art techniques in collaborative filtering. MF was proposed to make prediction for a single user-item rating matrix under single-context scenario. In MF, each user/item is associated with a low-dimensional latent fac-tor. Users X  latent factor vectors are stored in matrix U , where latent factor of user i is the i th column of matrix U , denoted by U latent factor V j of item j is the j th column of the item factor matrix V . The rating of user-item pair ( i,j ) is modeled by a probabilistic model with Gaussian observation noise. The conditional distribu-tion over observed rating matrix R as where N ( x |  X , X  2 ) is the probability density function of the Gaus-sian distribution with mean  X  and variance  X  2 , and I ij cator function that reflects whether user i rated item j . Generally, spherical Gaussian priors are placed on user and item factor vec-tors: Figure 3(a) shows the graphical model of matrix factorization. The latent factors of users and items are inferred in the learning phase using the observed rating matrix. Typically, stochastic gradient de-scent learning method is used. To predict the rating of user i on item j , the inner product of latent factor vectors U i and V puted. Our AdaMF model is built on matrix factorization.
Former researchers tried to solve data sparsity problem and cold-start problem by involving new information sources such as so-cial relationship and taxonomy information. For example, Ma [19] and Liu [20] included social network information to improve the accuracy of recommender systems. In these works, social infor-mation is incorporated into recommendation models as regulariza-tion terms, constraining taste difference between a user and his/her friends. Noam [2] and Weng [21] used taxonomy information on the item side. Noam extended the matrix factorization model by incorporating a rich bias model with terms that capture informa-tion from the taxonomy of items. Weng modified the similarity measurement with the item taxonomy information, improving the traditional neighborhood-based recommender system.

Recently, some general frameworks are proposed to incorporate multiple kinds of information across multiple contexts. Berkovsky extended the neighborhood-based models into the multi-context sce-nario [22]. Similarity between users and items are calculated by assembling all the information across all the contexts. Since this work is only an simple extension of neighborhood-based models, the improvement is limited. Chen proposed SVDFeature, a feature-based matrix factorization framework [23]. The feature-based set-ting allows to build factorization models, incorporating side infor-mation like temporal dynamics, neighborhood relationship, and hi-erarchical information. Xiao solved the recommendation problem in a heterogeneous information network [24]. Using the concept of meta-path to construct many different user-item preference ma-trices, they built recommendation algorithm based on these matri-ces separately. The final results are formulated by combining all these preference matrices using a linear model. However, side-information, both from user-side and item-side, is hard to get in many real world applications.
Multi-context recommendation problem could also be done by transfer learning , which transfers the knowledge from some auxil-iary data source to a target data source [25]. Pan considered a col-lective factorization model to transfer rating knowledge from some auxiliary data source in binary form to a target numerical rating matrix [26]. Li considered a Codebook transfer (CBT) algorithm under the scenario to transfer rating information across different domains [17]. Auxiliary rating matrix is first compressed into an informative and yet compact cluster-level rating pattern represen-tation referred to as a codebook, and then the target rating matrix is reconstructed by expanding the codebook. However, the hard clustering constraint greatly reduces the expressive power of CBT. A generative model is introduced in [27], relaxing the constraints in CBT from hard clustering to soft clustering using a probabilis-tic graphical model. In our model, all rating matrices in different contexts are factorized simultaneously, without the need to explic-itly distinguish these matrices into auxiliary and target ones. The knowledge is transferred on two directions for all these matrices.
As discussed before, matrix factorization is proposed for sin-gle context, where only one rating matrix is considered. In multi-context recommender system, users (items) will be involved in in-teractions across multiple contexts. Factorizing the rating matrix in each context separately would not take advantage of any correla-tion between contexts. We use MF as one of our baseline model in our experiments.

There are some related works of multi-context recommendation based on direct extension of matrix factorization model. Collective matrix factorization (CMF) is proposed by Singh and Gordon [12] to deal with multi-context data. CMF decomposes the rating matrix in each context into a product of two latent factor matrices, repre-senting users X  and items X  latent interest space respectively. When-ever one entity participates into more than one context, the latent factor for the entity is shared across all contexts. Fig 3(b) shows the probabilistic graphical model of CMF. In this model, latent factors of users and items are shared in all different contexts.

In CMF, the latent factor for an entity is learned based on the observed ratings from the contexts in which the entity participates. However, there is one issue with CMF that objects share the same latent factor across different contexts. This is claimed to be prob-lematic in [13, 14] in two aspects. Firstly, latent factors for objects that are cold in a context will be learned mainly based on the data from other contexts where it is not cold. Consequently, latent fac-tors for the cold objects are not properly learned. Secondly, the latent factors for objects participating in multi-context are learned mainly based on the dominating context and the dominated context has little effect on the learned latent factors. LMF and HeteroMF models are proposed to tackle these problems. In both of these two models, each entity has one global latent factor. For each context, there is a transfer matrix to transfer the global latent factor into the context-specific latent factor. The probabilistic graphical model of HeteroMF is shown in Fig 3(c). Just as the probabilistic graphical model of CMF, there are L contexts, rating for user-item pair ( i,j ) in the l th context is labeled by R ( l ) ij . For the i th user, the global factor is U i , and for each item there is a global factor V context and the entity (namely, user and item) type, there is a trans-fer matrix to model the influence of context. The local factor is the result of the interaction between the transfer matrix and the global factor. Taking user-side as an example, the local factor for the i th user in the l th context is generated by the product of the global factor and a transfer matrix M l u . Following the notation in the pre-liminary, the probability distribution of the global and local factors of the i th user are:
The definition of the global factor and local factor of the items are just the same as the users. The underlying assumption of mod-eling the context as a transfer matrix is that the effect of one context to all objects in the context is the same. We relax this assumption by directly modeling the context-specific factor of one entity in a given context. The final local factor for one entity is the result of a mixture interaction between the context-specific factor and the global factor. The parameters of the mixture model are adaptively learned from the data.
In this section we propose AdaMF, a context-adaptive matrix factorization model, to address multi-context recommendation. Con-sider M users and N items involved in L different contexts. Each context l is associated with a preference matrix R ( l ) , whose ele-ment R ( l ) ij describes the preference (usually a rating) of user i on item j . Our task is to predict the missing value R ( l ) As shown in Figure 3(d), AdaMF predicts a missing preference R ij based on latent factors representing user i and item j . Dif-ferent from a standard matrix factorization model that uses one la-tent factor for each user/item, a sharing-user version of AdaMF uses multiple factors to describe each user, representing her intrin-sic preference and context-specific interests respectively, in a sce-nario where different contexts share an identical set of users. A sharing-item version of AdaMF uses multiple factors to describe each item, representing its intrinsic quality and context-specific quality respectively. Here, as an example, we discuss the sharing-item version of AdaMF without loss of generalization. Specifically, each user i is assigned with a latent factor U i while each item j is assigned with multiple latent factors, including an entity-intrinsic factor V j related to its inherit property, and a context-specific fac-tor V ( l ) j associated with each context l . A rating R from a mixture of two Gaussian distributions, with mean values of U intrinsic and context-specific part of item j respectively. Those two distributions are mixed with an item-context mixture param-eter  X  ( l ) j , written as
We assume zero-mean spherical Gaussian priors for all latent factors where I denotes an identity matrix.

We introduce a latent Bernoulli variable Z ( l ) ij  X  Bern (  X  Z ij = 0 indicates that the rating R tion based on the entity-intrinsic factor, while Z ( l ) ij that the context-specific factor is used.
 The joint distribution of R ( l ) ij and Z ( l ) ij is as follows,
Accordingly, the joint likelihood of observations and latent vari-ables of the whole dataset could be written as P ( R,Z,U,V, [ where R = { R ( l ) ij } , Z = { Z ( l ) ij } , U = { U i j } , and  X  = {  X  u , X  v , S l  X  v,l } is the variance parameter set of the model,  X  = {  X  ( l ) } is a mixture parameter matrix. Here,  X  is the mixture parameter vector of items in context l .
We use an expectation maximization (EM) algorithm to infer model parameters and latent factors by maximizing the logarithmic likelihood L of the whole dataset, L = X  X  1 where I l i,j = 1 if user i has ratings of item j in context l ; I otherwise. || X || 2 F denotes the Frobenius norm. In the E-step, we compute the expected logarithmic likelihood with respect to the mixture parameters of items and latent variables. In the M-step, we maximize that expected likelihood of the whole dataset. The two steps are repeated alternately until convergence.
In the E-step, we use the current mixture parameter values  X  and latent factors to find the posterior distribution of the latent variables Z . We then use this posterior distribution to find the expectation of the complete-data logarithmic likelihood evaluated according to the mixture parameters and latent factors.

The expected values (responsibility) of Z ( l ) ij is evaluated as fol-low, = = 1 The expected log-likelihood is computed as follows, E Z [ L ] = X  X  1  X  1  X  1
In the M-step, we maximize the expected log-likelihood of the complete dataset. Note that the expected log-likelihood could be divided into two parts. The first part only involves the mixture parameters  X  ( l ) j , which could be estimated by
The second part of the objective function with respect to the la-tent variables is maximized by the stochastic gradient descent strat-egy (SGD) with a tiny update step  X  . Just as what was done in [9], maximizing the second part of the objective function is equivalent to minimizing the sum-of-squared-errors objective function with quadratic regularization terms, written as
For simplicity, we set all the  X  l and  X  v,l as the same for all con-texts: so  X  u =  X  2 l / X  2 u ,  X  v =  X  2 l / X  2 v ,  X  v,l  X  = {  X  u , X  v , S l  X  v,l } as the regularization parameters set, and C is the constant term.

For each rating in one context R ( l ) ij , we update for U leveraging gradients,
V j  X  V
The framework of the whole algorithm is formalized in Algo-rithm 1.

After inference, we predict an unobserved rating of user i on item j in context l with its expectation, Algorithm 1 AdaMF [ U,V, S l V ( l ) , X  ] = AdaMF ( R, X , X  ) E-Step : M-Step : factors and parameters U i , V j , V ( l ) j by Eq. (11)
Repeated until convergence.
AdaMF is evaluated with two real-world datasets, compared with mainstream baselines. The MovieLens-Netflix dataset contains users and aligned movies in two contexts corresponding to two public benchmark datasets from Netflix Prize 1 and MovieLens project 5 , 871 movies are aligned by IMDB meta information after data cleaning. Since there are much fewer MovieLens users than Net-flix users, we sampled 70 , 132 users from Netflix for balancing two contexts. The Douban dataset contains items and aligned users in three contexts, namely books, movies, and music. It is crawled from an online social network Douban 3 where users rate books, movies, and music [28]. We remove inactive users with less than 5 items, and obtain 10 , 000 users and their ratings. Statistics of the two datasets are shown in Table 1 and Table 2.
 Each dataset is split into a 80% training set and a 20% testing set. The dimension of the latent factors are chosen as 20, 50 and 100. In the training phase, we set all regularization parameters as 0.01, although later we empirically find that our model is insensitive to those parameters. The training data is split into 5 parts to do 5-fold cross validation, determining when we stop the learning phase. In the testing phase, we evaluate prediction performance of AdaMF using root mean squared error (RMSE) as, where R ij denotes the actual rating that user i posts to item j , the predicted rating, and R testing denotes the ratings in the testing set.

Besides the rating-based metric RMSE, we also considered a ranking-based metric, mean average precision (MAP). Given all the candidate items, we generate a recommendation list L for each user by ranking all the items in a descending order according to the predicted rating score. Then, the top-K average precision (AP@K) http://www.netflixprize.com/ http://www.movielens.org/ http://www.douban.com/ is defined as P ( j ) is the precision of the recommendation list at position j , I ( L j ) is the function to indicate if the j th item in the list is pre-ferred by the user. In our experiments, we set the highest rated item as the preferred items for one user. K is set as 5 in our experiments. We evaluate recommendation method using the mean AP over all users, i.e., where M is the number of users.

Three baseline methods are:
We first compare our algorithm with baseline methods on the whole dataset. Table 3 and table 4 show the performance of all these methods on the movie-aligned dataset. On the two different movie contexts, our method consistently outperforms all the base-lines on all the tested number of dimensions. As the dimension of latent factors increases, the improvement becomes more signifi-cant.

Table 5 and table 6 show the performance of all the methods on the user-aligned dataset. As there are three domains of items in Douban dataset, we conduct experiments on three scenarios: movie-book, movie-music, and book-music respectively. The re-sults show that our AdaMF model outperforms other baselines on all the three scenarios. However, the improvement of AdaMF is not always significant. For example, the improvement of music recom-mendation in book-music scenario is not as significant as that in movie-music scenario. This phenomenon may be related to corre-lation between these domains.
Due to the imbalance problem in observations, users/items with extremely few ratings suffer most severely from data sparsity. The AdaMF model is particularly effective on those sparsity entities. To show this, we categorize movies in the MovieLens-Netflix dataset into three sparsity levels:  X  X old X  movies with 100 or less ratings in a context,  X  X ormal X  movies with 100  X  1 , 000 ratings, and  X  X arm X  movies with more than 1 , 000 ratings. In this way, a movie can be warm in MovieLens and normal in Netflix. In total, 7 label con-figurations are observed, leaving two empty zones, since no warm movies in MovieLens are observed cold in Netflix and vice versa. Figure 4(a) reports the RMSE of prediction on MovieLens (target context), leveraging ratings in Netflix (auxiliary context). In most configurations, standard matrix factorization performs the worst. CMF slightly reduces RMSE on movies that are cold (bottom row) and normal (middle row), but performs even worse on movies la-beled warm (top row). That might be due to that it does no good to incorporate external ratings for items with sufficient ratings in the target context. HeteroMF outperforms MF and CMF on movies la-beled  X  X ormal X  and  X  X arm X  in the target context (middle and right column), but shows modest advantage on movies labeled  X  X old X  in the target context (left column), implying that a fixed form of trans-fer matrix sharing mechanism as done in HeteroMF does harm to items sparse in the target context. In contrast, AdaMF consistently shows significant advantage on every configuration, demonstrating that AdaMF works well in any sparse level. Figure 4(b) reports prediction RMSE on Netflix, exhibiting similar results.
To offer some intuition about why adaMF works well, we present three typical cases in Figure 5. The first movie is  X  Love is all there is  X  that receives sparse ratings in both contexts forming a cold-cold case.  X  Hellfighter  X  is sparse in MovieLens but warm in Netflix, forming a cold-warm case.  X  Titanic  X  receives plenty of ratings in both contexts forming a warm-warm case. From the perspective of model complexity, a simple model works well when few ratings are available, and a complex model captures more detailed information with a large dataset. Therefore, as the simplest model, CMF uses an identical latent factor to represent all occurrences of a movie on different contexts, and not surprisingly outperforms MF and Het-eroMF when predicting the first movie and the sparse side of the second movie. On the other hand, HeteroMF builds a complex model to describe specific behaviors of a movie in multiple con-texts, and significantly outperforms CMF for the third movie and the rich side of the second movie. Due to the different applicability of simple and complex models, no baseline consistently perform well in all levels of sparsity. Noticeably, with a flexible parameter to adjust the weight of a global latent factor and a context-specific one, our model actually gains the ability to adaptively fit different levels of sparsity, and as a result outperforms baselines on all three typical cases.
In AdaMF model, context-entity mixture parameter reflects the balance between entity-intrinsic factor and context-specific factor. One interesting question is: what is the distribution of the values of this mixture parameter over all users/items? As shown in Figure 6, for most movies the mixture parameters locate within [0 . 4 , 0 . 6] , indicating that an entity X  X  intrinsic attributes play almost the same important role in producing preference with the context-specific at-tributes.

Table 7 shows the mixture parameters for the three movies, stud-ied in the aforementioned case analysis. For movie  X  Love is all there is  X , mixture parameters are small in both contexts. In this case, entity-intrinsic factors are more critical than context-specific factors. For movie  X  Hellfighter  X , the mixture parameter is small in the  X  X old X  context but large in the  X  X arm X  context, indicating that the AdaMF indeed adaptively capture the degree to which context influences rating. For movie  X  Titanic  X , a  X  X arm-warm X  case, the mixture parameter in both contexts is larger than 0 . 5 , indicating that context matters much more. As above, the diversity of mix-ture parameters reveal the necessity to develop an adaptive method rather than a fixed form as done in HeteroMF.
We now discuss the issue of model complexity. To this end, we list the number of effective parameters for all the multi-context rec-ommendation models discussed in this paper. We assume that there are M users, N items, and l contexts. We denote with d the dimen-sion of latent factors. For CMF, the effective number is ( M + N )  X  d . For HeteroMF, the effective number is ( M + N )  X  d + d  X  d  X  l . For our AdaMF, the effective number is ( M + N )  X  d + ( M + N )  X  d  X  l . We can see that HeteroMF and AdaMF are more complex than CMF since the two models attempt to model context-specific factors in different schemas. HeteroMF simply assumes that each context indiscriminately affects all entities, reflected by a transfer matrix. AdaMF aims to adaptively capture the influence of context through a context-entity mixture matrix.

Note that the main superiority of AdaMF over other competing models is its flexibility at adaptively capturing the degree that each context exerts on each entity. As such, AdaMF achieves consistent (a) RMSE in MovieLens. X-axis represents the sparsity level in MovieLens, Y-axis represents the sparsity level in Netflix (b) RMSE in Netflix. X-axis represents the sparsity level in Netflix, Y-axis represents the sparsity level in MovieLens Figure 4: Rating prediction accuracy of movies with different sparse levels. improvements for entities at all sparsity levels, i.e., cold, normal, and warm (Figure 4). This consistent improvement is particularly important for a real recommender system, since such a system ex-hibits reliable performance than systems that work better on some cases but worse on others. Perhaps, some people may argue that the improvement of the AdaMF is marginal, compared with Het-eroMF. Indeed, the main point of this paper is that AdaMF could Figure 5: Performance in different sparse levels (a) Cold-cold case: movie  X  Love is all there is  X  has 23 ratings in MovieLens dataset and 32 ratings in Netflix dataset; (b) Cold-warm case: movie  X  Hellfighter  X  has 16 ratings in MovieLens dataset and 455 ratings in Netflix dataset; (c) Warm-warm case: movie  X  Titanic  X  has 27550 ratings in MovieLens dataset and 21234 ratings in Netflix dataset. consistently improve multi-context recommendation by adaptively balancing the entity-intrinsic factor and context-specific factor. Its gain deserves its cost. Data sparsity is one of the challenges for recommender system. Lack of relevant information for users and items is the key issue for data sparsity problem. In the real world, users (items) are involved in multiple contexts but not isolated. In this paper, we propose a context-adaptive matrix factorization method for multi-context rec-ommendation problem via simultaneously modeling context-specific factors and entity-intrinsic factors in a unified model. We learn for every entity an entity-intrinsic latent factor and a context-specific latent factor in each context. Meanwhile, using a context-entity mixture parameter we explicitly model the degree to which each context imposes influence on each entity. We performed experi-ments on two real scenarios, an item-aligned movie dataset and a user-aligned online social network, demonstrating that our model is better than the baseline models on all sparsity level. As future work, we will extend the AdaMF model to heterogenous networks containing contexts with various types.
 This work was funded by the National Basic Research Program of China (the 973 program) under grant numbers 2014CB340401 and 2012CB316303, and the National Natural Science Foundation of China with Nos. 61232010, 61472400, 61202215, 61433014. This work is also funded by the National Information Security Program of China (the 242 program) with grant number 2014A137.
