 We propose a new class of spatio-temporal cluster detection meth-ods designed for the rapid detection of emer ging space-time clus-ters. We focus on the moti vating application of prospecti ve dis-ease surv eillance: detecting space-time clusters of disease cases resulting from an emer ging disease outbreak. Automatic, real-time detection of outbreaks can enable rapid epidemiological response, potentially reducing rates of morbidity and mortality . Building on the prior work on spatial and space-time scan statistics, our meth-ods combine time series analysis (to determine how man y cases we expect to observ e for a given spatial region in a given time in-terv al) with new  X emer ging cluster X  space-time scan statistics (to decide whether an observ ed increase in cases in a region is signif-icant), enabling fast and accurate detection of emer ging outbreaks. We evaluate these methods on two types of simulated outbreaks: aerosol release of inhalational anthrax (e.g. from a bioterrorist at-tack) and FLOO ( X Fictional Linear Onset Outbreak X ), injected into actual baseline data (Emer genc y Department records and over-the-counter drug sales data from Alle ghen y County). We demonstrate that our methods are successful in rapidly detecting both outbreak types while keeping the number of false positi ves low, and sho w that our new  X emer ging cluster X  scan statistics consistently outper -form the standard  X persistent cluster X  scan statistics approach. H.2.8 [ Database Management ]: Database Apps-Data Mining Algorithms Cluster detection, space-time scan statistics, biosurv eillance
In man y data mining applications, we are faced with the task of detecting cluster s : regions of space where some quantity is signi-Cop yright 2005 ACM 1 X 59593 X 135 X  X /05/0008 ... $ 5.00. cantly higher than expected. For example, our goal may be to detect clusters of disease cases, which may be indicati ve of a naturally oc-curring disease epidemic (e.g. inuenza), a bioterrorist attack (e.g. anthrax release), or an environmental hazard (e.g. radiation leak). In medical imaging, we may attempt to detect tumors or other haz-ardous gro wths; in neuroscience, we may be interested in detect-ing spatial patterns of brain acti vity (measured by fMRI acti vation) that correspond to various cogniti ve tasks. [11] discusses man y other applications of cluster detection, including mining astronom-ical data (identifying clusters of stars or galaxies) and military re-connaissance (monitoring strength and acti vity of enemy forces). In all of these applications, we have two main goals: to pinpoint the location, shape, and size of each potential cluster , and to de-termine (by statistical signicance testing) whether each potential cluster is lik ely to be a  X true X  cluster or simply a chance occurrence.
While most of the prior work on cluster detection is purely spa-tial in nature (e.g. [1, 10, 6]), it is clear from the abo ve list of appli-cations that time is an essential component of most cluster detection problems. We are often interested in clusters which are emer ging in time: for example, a gro wing tumor , an outbreak of disease, or an increase in troop acti vity . In some applications, the time dimen-sion can be dealt with easily , either by applying some purely spatial cluster detection method at each time step, or by treating time as an-other spatial dimension and thus applying spatial cluster detection in a d + 1 dimensional space ( d spatial dimensions, plus time). The disadv antage of the rst approach is that by only examining one day of data at a time, we may fail to detect more slo wly emer ging clusters. The disadv antage of the second approach is that we may detect less rele vant clusters: those clusters that have persisted for a long time, rather than those that are newly emer ging.

To impro ve on these methods, it is helpful to consider the guid-ing question,  X Ho w is time, as a dimension, dif ferent from space? X  We argue that there are three important distinctions which require us to treat spatio-temporal cluster detection dif ferently from spa-tial cluster detection. First, time (unlik e space) has an important point of reference: the present. We often care only about those space-time clusters that are still  X acti ve X  at the present time, and in these cases we should use a prospecti ve method (searching for clusters which end at the present time) rather than a retrospecti ve method (searching for clusters which end at or before the present time). Second, in the spatial cluster detection frame work, we typi-cally assume that we have some baseline denominator data such as a census population (for epidemiology), and that the expected count (e.g. number of disease cases) is proportional to this baseline. In the spatio-temporal frame work, we are generally not pro vided with explicit denominator data; instead, we infer the expected values of the most recent days' counts from the time series of past counts. Finally , and most interestingly , time has an explicit direction or  X ar -row, X  proceeding from the past, through the present, to the future. We are generally interested in clusters which emer ge over time: for example, a disease epidemic may start with only a few reported cases, then increase in magnitude either gradually or rapidly . One major focus of this paper is developing statistical methods which are more appropriate for detecting such emer ging clusters.
We focus here on the moti vating application of prospective dis-ease surveillance : detecting space-time clusters of disease cases resulting from an emer ging disease outbreak. In this application, we perform surv eillance on a daily (or even hourly) basis, with the goal of nding emer ging epidemics as quickly as possible. By detecting epidemics rapidly and automatically , we hope to allo w more rapid epidemiological response (e.g. distrib ution of vaccines, public health warnings), potentially reducing the rates of mortal-ity and morbidity from an outbreak. In this application, we are given the number of disease cases of some given type (e.g. respi-ratory) in each spatial location (e.g. zip code) on each day . More precisely , since we cannot measure the actual number of cases, we instead rely on related observ able quantities such as the number of respiratory Emer genc y Department visits, or sales of over-the-counter cough and cold medication, in a given spatial location on a given day . We must then detect those increases that are indica-tive of emer ging outbreaks, as close to the start of the outbreak as possible, while keeping the number of false positi ves low.
In the general case, we have data collected at a set of discrete time steps t = 1 : : : T (where time T represents the present) at a set of discrete spatial locations s i . For each s i at each time step t , we are given a count c t i , and our goal is to nd if there is any region S (set of locations s i ) and time interv al ( t = t min : : : the counts are signicantly higher than expected. Thus we must rst decide on the set of spatial regions S , and the time interv als t min : : : t max , that we are interested in searching. In the scan statis-tics frame work discussed belo w, we typically search over the set of all spatial regions of some given shape, and variable size. For sim-plicity , we assume here (as in [18]) that the spatial locations s aggre gated to a uniform, two-dimensional, N N grid G , and we search over the set of all axis-aligned rectangular regions S G . This allo ws us to detect both compact and elong ated clusters, which is important since disease clusters may be elong ated due to disper -sal of pathogens by wind, water , or other factors. For prospecti ve surv eillance, as is our focus here, we care only about those clusters which are still present at the current time T , and thus we search over time interv als with t max = T ; if we were performing a retrospecti ve analysis, on the other hand, we would search over all t max must also choose the size of the  X temporal windo w X  W : we assume that we are only interested in detecting clusters that have emer ged within the last W days (and are still present), and thus we search over time interv als t min : : : T for all T W &lt; t min
In the disease detection frame work, we assume that the count (number of cases) in each spatial region s i on each day t is Poisson distrib uted, c t i Po ( l t i ) with some unkno wn parameter l our method consists of two parts: time series analysis for calcu-lating the expected number of cases (or  X baseline X ) b t i each spatial region on each day , and space-time scan statistics for determining whether the actual numbers of cases c t i in some region S are signicantly higher than expected (gi ven b t i ) in the last W days. The choice of temporal windo w size W impacts both parts of our method: we calculate the baselines b t i for the  X current X  days
Non-axis-aligned rectangles can be detected by examining multi-ple rotations of the data, as in [18].
 T W &lt; t T by time series analysis, based on the  X past X  days 1 t T W , and then determine whether there are any emer ging space-time clusters in the last W days. In addition to the tempo-ral windo w size, three other considerations may impact the perfor -mance of our method: the type of space-time scan statistic used, the level on which the data is aggre gated, and the method of time series analysis. We discuss these considerations in detail belo w.
One of the most important statistical tools for cluster detection is the spatial scan statistic [15, 10, 11]. This method searches over a given set of spatial regions, nding those regions which maximize a lik elihood ratio statistic and thus are most lik ely to be generated under the alternati ve hypothesis of clustering rather than under the null hypothesis of no clustering. Randomization testing is used to compute the p -value of each detected region, correctly adjust-ing for multiple hypothesis testing, and thus we can both identify potential clusters and determine whether the y are signicant. The standard spatial scan algorithm [11] has two primary dra wbacks: it is extremely computationally intensi ve, making it infeasible to use for massi ve real-w orld datasets, and only compact (circular) clus-ters are detected. In prior work, we have addressed both of these problems by proposing the  X fast spatial scan X  algorithm [18, 19], which can rapidly search for elong ated clusters (hyper -rectangles) in lar ge multi-dimensional datasets. As noted abo ve, we choose here to search over rectangular regions, using a space-time variant of the fast spatial scan as necessary to speed up our search.
In its original formulation [15, 10], the spatial scan statistic does not tak e time into account. Instead, it assumes a single count c number of disease cases) for each spatial location s i , as well as a given baseline b i (e.g. at-risk population). Then the goal of the scan statistic is to nd regions where the rate (or expected ratio of count to baseline) is higher inside the region than outside. The statistic used for this is the lik elihood ratio D ( S ) = Pr ( Data the null hypothesis H 0 represents no clustering, and each alterna-tive hypothesis H 1 ( S ) represents clustering in some region S . More precisely , under H 0 we assume a uniform disease rate q all c i Po ( q all b i ) for all locations s i . Under H 1 ( S c i Po ( q in b i ) for all locations s i 2 S , and c i Po ( cations s i 2 G S , for some constants q in &gt; q out . From this, we can deri ve an expression for D ( S ) using the maximum lik elihood esti-mates of q in , q out , and q all : D ( S ) = C in B if are the sums of counts and baselines for S , G S , and G respec-tively . Then the most signicant spatial region S is the one with the highest score D ( S ) ; we denote this region by S , and its score by D . Once we have found this region by searching over the space of possible regions S , we must still determine its statistical signif-icance, i.e. whether S is a signicant spatial cluster . To adjust correctly for multiple hypothesis testing, we nd the region' s p -value by randomization: we randomly create a lar ge number R of replica grids under the null hypothesis c i Po ( q all b i highest scoring region and its score for each replica grid. Then the p -value can be computed as R bea t + 1 R replica grids with D higher than the original grid. If this p -value is less than some constant a (here a = : 05), we can conclude that the disco vered region is unlik ely to have occurred by chance, and is thus a signicant spatial cluster; we can then search for secondary clusters. Otherwise, no signicant clusters exist.

The formulation of the scan statistic that we use here is some-what dif ferent, because we are interested not in detecting regions with higher rates inside than outside, but regions with higher counts than expected . Let us assume that baselines b i represent the ex-pected values of each count c i ; we discuss how to obtain these baselines belo w. Then we wish to test the null hypothesis H counts c i are generated by c i Po ( b i ) , against the set of alterna-tive hypotheses H 1 ( S ) : for spatial locations s i 2 S , all counts c generated by c i Po ( qb i ) , for some constant q &gt; 1, and for all other spatial locations s i 2 G S , all counts c i Po ( b compute the lik elihood ratio: Using the maximum lik elihood estimate q = max 1 ; C in B tain D ( S ) = C in B As before, we search over all spatial regions S to nd the highest scoring region S . Then the statistical signicance ( p -value) of S can be found by randomization testing as before, where the replica grids are generated under the null hypothesis c i Po ( b i
To extend this spatial scan statistic to the prospecti ve space-time case, the simplest method is to use a 1-day temporal windo w ( W 1), searching for clusters on only the present day t = T . Thus we wish to kno w whether there is any spatial region S with higher than expected counts on day T , given the actual counts c T i and expected counts b T i for each spatial location s i . To do so, we compare the hypotheses H 1 ( S ) : c T i Po ( qb T i ) for all s i 2 S , for some constant q &gt; 1, and c T i Po ( b T i ) else where. Thus the statistic tak es the same form as the purely spatial scan statistic, and we obtain: D and B =  X  s S on time step T . Ag ain, we search over all spatial regions S to nd the highest scoring region S and its score D . To compute the p -value, we perform randomization testing as before, where each replica grid has counts c T i generated from Po ( b T counts c t i ( t 6 = T ) copied from the original grid.
While the 1-day prospecti ve space-time scan statistic is very use-ful for detecting rapidly gro wing outbreaks, it may have dif culty detecting more slo wly gro wing outbreaks, as noted abo ve. For the multi-day prospecti ve space-time scan statistics, we have some temporal windo w W &gt; 1, and must determine whether any out-breaks have emer ged within the most recent W days (and are still present). In other words, we wish to nd whether there is any spa-tial region S with higher than expected counts on days t min for some T W &lt; t min T . To do so, we rst compute the ex-pected counts b t i and the actual counts c t i for each spatial location s on each day T W &lt; t T ; we discuss how the baselines b are calculated in the follo wing section. We then search over all spatial regions S G , and all allo wable values of t min , nding the highest value of the spatio-temporal score function D ( S calculation of this function depends on whether we are searching for  X persistent X  or  X emer ging X  clusters, as we discuss belo w. In any case, once we have found the highest scoring region ( and its score D , we can compute the p -value of this region by per -forming randomization testing as before, where each replica grid has counts c t i generated from Po ( b t i ) for T W &lt; other counts c t i copied from the original grid.

No w we must consider how to compute the function D ( S ; The standard method for computing the space-time scan statistic, proposed for the retrospecti ve case by [13] and for the prospecti ve case by [12], builds on the Kulldorf f spatial scan statistic [10] given abo ve. As in the purely spatial scan, this method assumes that base-lines b t i are given in adv ance (e.g. population in each location for each time interv al), and that counts c t i are generated from Poisson distrib utions with means proportional to b t i . Then the goal is to nd space-time clusters ( S ; t min ) where the rate (ratio of count to base-line) is signicantly higher inside the region than outside. As in the purely spatial case, this can be adapted to our frame work, in which the goal is to nd space-time clusters where the observ ed counts c are higher than the expected counts b t i . For the  X persistent clus-ter X  case, we maintain the other major assumption of the standard model: that the multiplicati ve increase in counts ( X relati ve risk X ) in an affected region remains constant through the temporal duration of the cluster . For the  X emer ging cluster X  case, we instead mak e the assumption that the relati ve risk increases monotonically through the cluster' s duration. It is also possible to assume a parametric form for the increase in relati ve risk over time (e.g. exponential or linear increase), and we consider such statistics in [17].
The test for persistent clusters assumes that the relati ve risk of a cluster remains constant over time; as a result, the score function is very similar to the 1-day statistic, with sums tak en over the entire duration of a cluster rather than only a single day .

As noted abo ve, we must search over all spatial regions S and all values of t min (where T W &lt; t min T ), nding the maximum score D ( S ; t min ) . For a given region S and value t the null hypothesis H 0 : c t i Po ( b t i ) for all spatial locations s all T W &lt; t T , to the alternati ve hypothesis H 1 ( S Po ( qb t i ) for s i 2 S and t = t min : : : T , for some constant q c i Po ( b D (
S ; t min ) = where the products are tak en over s i 2 S and t min t T . This simplies to max q 1 q C e qB e B , where C and B are the total count  X  Finally , using the maximum lik elihood estimate q = max 1 we obtain D ( S ; t min ) = C B C e B C if C &gt; B , and D
While the space-time scan statistic for persistent clusters assumes that relati ve risk of a cluster remains constant through its duration, this is typically not true in disease surv eillance. When a disease outbreak occurs, the disease rate will typically rise continually over the duration of the outbreak until the outbreak reaches its peak, at which point it will level off or decrease. Our main goal in the epi-demiological domain is to detect emer ging outbreaks (i.e. those that have not yet reached their peak), so we focus on nding clus-ters where the relati ve risk is monotonically increasing over the duration of the cluster . Ag ain, we must search over all spatial re-gions S and all values of t min (where T W &lt; t min T ), nding the maximum score D ( S ; t min ) . For a given region S and value t compare the null hypothesis H 0 : c t i Po ( b t i ) for all spatial locations s and all T W &lt; t T , to the alternati ve hypothesis H 1 c i Po ( q t b cally increasing sequence of constants 1 q t c i Po ( b where the products are tak en over s i 2 S and t min t T . This count  X  s total baseline  X  s
No w, we must maximize the numerator subject to the constraints on the q t . To do so, let E = E 1 : : : E p be a partitioning of t into sets of consecuti ve inte gers, such that for all t 1 q = Q j , and for all E j 1 and E j 2 , where j 1 &lt; j 2 , Q words, the E j dene a partitioning of t min : : : T into time periods where the relati ve risk is constant. Note that the q t are uniquely dened by the partitions E j and the rates Q j . We can then write: where B j =  X  s
In [17], we pro ve that this expression is maximized when Q for all j . This allo ws us to simplify the expression to:
Then the question is how to choose the optimal partitioning E f
E j g , and in [17] we present the follo wing algorithm. This method uses a stack data structure, where each element of the stack repre-sents a partition E j by a 5-tuple t start ; t end ; C j ; rithm starts by pushing T ; T ; C T ; B T ; max 1 ; C T B Then for each t , from T 1 down to t min , we do the follo wing:
As we pro ve in [17], this  X step method X  produces the unique optimal partitioning E and rates Q , and thus the values of q maximize the score subject to the monotonicity constraints abo ve.
In order to infer the baselines b t i for the  X current X  days T W t T , we must consider two distinct questions: on what level to aggregate the data for time series analysis, and what method of time series analysis to use. We consider three dif ferent levels of spatial aggre gation, which we term  X building-aggre gated time se-ries X  (BATS),  X cell-aggre gated time series X  (CA TS), and  X re gion-aggre gated time series X  (RA TS) respecti vely . For the BATS method, we consider the time series for each spatial location independently; for example, we may have a separate time series for each store or hospital, or counts may be already aggre gated at some level (e.g. zip code). For each of these locations s i , we independently compute the baselines b t i ( T W &lt; t T ) from the past counts c (1 t T W ), using one of the time series analysis methods be-low. Then whene ver we calculate D ( S ; t min ) for a region, we use the baselines b t i and counts c t i for each location in the region. The CA TS method rst computes the aggre gate count c t i for each cell of the grid s i 2 G on each day t , by summing counts of all spatial loca-tions in that cell. Then the baselines b t i are computed independently for each grid cell s i 2 G , and whene ver we calculate D region, it is the cell counts and baselines that we use to compute the score. Finally , the RA TS method, whene ver it searches a region S , aggre gates the time series of counts C t ( S )  X on the y X  by summing counts of all spatial locations in that region, computes baselines B (
S ) for the  X current X  days T W &lt; t T , and applies the score function D ( S ; t min ) to the resulting counts and baselines.
Randomization testing must also be performed dif ferently for each of the three levels of aggre gation. To generate a replica grid for BATS, we independently dra w a count for each spatial location s for each current day t , using its baseline b t i . To generate a replica grid for CA TS, we independently dra w a count for each cell of the grid s i 2 G for each current day t , using the cell baseline b nally , randomization testing for RA TS is some what more dif cult than for the other methods, since we must produce cell counts from a correlated distrib ution. Various sampling methods can be used to do this, but this mak es randomization extremely computationally expensi ve; see [17] for more details.
For a given location, cell, or region s i , our goal is to estimate the expected values of the  X current X  counts, b t i = E [ c t i from the time series of  X past X  counts c t i , 1 t T W . A variety of methods are possible, depending on how we wish to deal with three questions: day of week effects, seasonal trends, and bias. Man y epidemiological quantities (for example, OTC drug sales) exhibit strong day of week and seasonal trends. Here we consider three methods of dealing with day of week effects: we can ignore them, str atify by day of week (i.e. perform a separate time series calcula-tion for each day of the week), or adjust for day of week. To adjust for day of week, we assume that the observ ed count on a given day is the product of an  X actual X  count and a constant dependent on the day of week. Thus we compute the proportion of counts b i day of the week ( i = 1 : : : 7). Then we transform each past day' s ob-serv ed count by dividing by 7 b i , do a single time series calculation on the transformed past counts to predict the transformed current counts, and nally multiply by 7 b i to obtain the predicted count for each current day . By adjusting instead of stratifying, more data is used to predict each day' s count (potentially reducing the variance of our estimates), but the success of this approach depends on the assumption of a constant and multiplicati ve day-of-week effect.
We also consider three methods of adjusting for seasonal trends: to use only the most recent counts (e.g. the past four weeks) for prediction, to use all counts but weight the most recent counts more (as is done in our exponentially weighted mo ving average and ex-ponentially weighted linear regression methods), and to use regres-sion techniques to extrapolate seasonal trends to the current data. Finally , we consider both methods which attempt to give an unbi-ased estimate of the current count (e.g. mean of past counts), and methods which attempt to give a positi vely biased estimate of the current count (e.g. maximum of past counts). As we sho w, the unbiased methods typically have better detection power , but the bi-ased methods have the adv antage of reducing the number of false positi ves to a more manageable level (see Section 7.5).

Here we consider a total of 10 time series analysis methods, in-cluding  X all max X  ( b t i = maximum count of last 28 days),  X all mean X  ( b i = mean count of last 28 days),  X strat max X  ( b count of same day of week, 1-4 weeks ago),  X strat mean X  ( b mean count of same day of week, 1-4 weeks ago), two exponen-tially weighted mo ving average methods ( X strat EWMA  X  stratied by day of week,  X adj EWMA  X  adjusted for day of week), and two exponentially weighted linear regression methods ( X strat EWLR X  stratied by day of week,  X adj EWLR X  adjusted for day of week). Our nal two methods are inspired by the recent work of Kulldorf f et al. [14] on the  X space-time permutation scan statistic,  X  so we call them  X strat Kull X  (stratied by day of week) and  X all Kull X  (ig-noring day of week effects). In this frame work, the baseline b pendent, so the expected fraction of all cases occurring in location s on day t can be computed as the product of the fraction of all cases occurring in location s i and the fraction of all cases occurring on day t . The problem with this method is that the current day' s counts are used for prediction of the current day' s expected counts. As a result, if there is a cluster on the current day , the baselines for the current day will also be higher , reducing our power to detect the cluster . Ne vertheless, the strat Kull and all Kull methods do ex-tremely well when detecting localized clusters (where the increase in counts is noticeable for a small region, but the region is small enough that the total count for the day is essentially unaf fected).
We also note an interesting interaction between the level of ag-gre gation and the method of time series analysis. If the expected counts b t i ( T W &lt; t T ) are calculated as a linear combination of past counts c t i (1 t T W ), and the weights for each past day t are constant from location to location, then we will calculate the same baselines (and thus, the same scores) regardless of whether we aggre gate on the building, cell, or region level. This turns our to be true for most of the methods we investig ate: all mean, strat mean, strat EWMA, strat EWLR, all Kull, and strat Kull. On the other hand, if we choose dif ferent weights for each location (as is the case when we adjust for day of week, as in adj EWMA and adj EWLR), we will calculate dif ferent baselines (and thus, dif-ferent scores) depending on our level of aggre gation. Finally , we have very dif ferent results for the  X max X  methods (strat max and all max) depending on the level of aggre gation, because the maxi-mum is not a linear operator . Since the sum of the maximum counts of each location (  X  s sum (max t  X  s baselines, and RA TS to predict the lowest baselines. For the re-sults given belo w, we only distinguish between BATS, CA TS, and RA TS aggre gation for those methods where the distinction is rele-vant (all max, strat max, adj EWMA, and adj EWLR).
In general, spatio-temporal methods can be divided into three classes: spatial modeling techniques such as  X disease mapping,  X  where observ ed values are spatially smoothed to infer the distrib u-tion of values in space-time [4, 3]; tests for a general tendenc y of the data to cluster [9, 16]; and tests which attempt to infer the loca-tion of clusters [13, 12, 14]. We focus on the latter class of meth-ods, since these are the only methods which allo w us to both an-swer whether any signicant clusters exist, and if so, identify these clusters. Three spatio-temporal cluster detection approaches have been proposed by Kulldorf f et al.: the retrospecti ve and prospecti ve space-time scan statistics [13, 12], and the space-time permutation scan statistic [14]. The rst two approaches attempt to detect per -sistent clusters, assuming that baselines are given based on census population estimates. The retrospecti ve statistic searches over all space-time interv als, while the prospecti ve statistic searches over those interv als ending at the present time. As noted abo ve, these formulations mak e sense for the case of explicitly given denomina-tor data, and counts proportional to these baselines (e.g. we expect a population of 10000 to have twice as man y cases as a population of 5000, but do not kno w how man y cases we expect to see). The y are not appropriate for the case where we infer the expected values of counts from the time series of past counts (e.g. based on past data, we expect to see 40 cases in the rst population and 15 cases in the second). Ev en if accurate denominator data is pro vided, the retrospecti ve and prospecti ve statistics may pick up purely spatial clusters resulting from spatial variation in the underlying rate (e.g. dif ferent parts of the country have dif ferent disease rates), or purely temporal clusters based on temporal uctuations in rate (seasonal effects or long-term trends), and thus the detected clusters tend to be less useful for prospecti ve detection of emer ging outbreaks.
The recently proposed  X space-time permutation scan statistic X  [14] attempts to remedy these problems; lik e the present work, it allo ws baseline data to be inferred from the time series of past counts. As noted abo ve, baselines are calculated by assuming that cases are in-dependently distrib uted in space and time, and a variant of the test for persistent clusters is used (searching for regions with higher rate inside than outside). Then randomization testing is done by permuting the dates and locations of cases. This method focuses on detecting space-time inter action , and explicitly avoids detect-ing purely spatial or purely temporal clusters. The disadv antages of this are twofold. First, it loses power to detect spatially lar ge clusters, because (as noted abo ve) the current day' s counts are used to estimate what the current day' s counts should be. In the most extreme case, a spatially uniform multiplicati ve increase in disease rate over the entire search area would be completely ignored by this method, and thus it is unsafe to use for surv eillance except in combination with other methods. The second disadv antage is that if the count decreases in one spatial region and remains constant else where, this is detected as a spatio-temporal cluster . This results in false positi ves in cases where stores in one area are closed and stores in a dif ferent area remain open: the open stores are agged as a cluster even if their counts have actually decreased.
Several other spatio-temporal cluster detection methods have also been proposed. Iyeng ar [8] searches over  X truncated rectangular pyramid X  shapes in space-time, thus allo wing detection of clus-ters which mo ve and gro w or shrink over time; the disadv antage is that this much lar ger set of possible space-time regions can only be searched approximately . Assuncao et al [2] assume a spatio-temporal Poisson point process: the exact location of each point in time and space is given, rather than aggre gating points to discrete locations and interv als. A test statistic similar to the space-time permutation scan statistic is deri ved, assuming a Poisson intensity function that is separable in space and time.
We begin by making two important observ ations. First, for any of the time series analysis methods given abo ve, the baselines b ( T W &lt; t T ) can be inferred from the past counts c t t T W ) in O ( T ) . Second, we can compute the score function D (
S ; t min ) , for a given spatial region S and for all T W in total time O ( W ) , regardless of whether the persistent or emer g-ing scan statistic is used. This is obvious for the persistent statistic since we can simply proceed backw ard in time, adding the cumula-tive count C t and cumulati ve baseline B t for each day t , and recom-puting the score. (W e can accumulate these counts and baselines in O (
W ) by using the  X cumulati ve counts X  trick discussed in [18] for each of the W current days.) The O ( W ) comple xity is less obvious for the emer ging statistic, since adding any new day t may result in up to O ( W ) pops from the stack. But each day is pushed onto the stack at most once, and thus the total number of pops for the W days is at most W , giving total comple xity O ( W ) , not O
For the BATS method, our computation may be divided into three steps: rst, we compute baselines for each spatial location, requiring total time O ( N s T ) , where N s is the number of locations. Second, we aggre gate  X current X  store baselines and counts to the grid, requiring time O ( N 2 W ) where N is the grid size. Third, we search over all spatio-temporal regions ( S ; t min ): for each such re-gion, we must compute the aggre gate counts and baselines, and apply the score function D . As noted abo ve, we can do this in O (
W ) per region, but since a nai ve search requires us to exam-ine all O ( N 4 ) gridded rectangular regions, the total search time is O (
N 4 W ) , bringing the total comple xity to O ( N s T + N CA TS, we rst aggre gate all store baselines and counts to the grid, requiring time O ( N s T + N 2 T ) . Then we calculate baselines for each of the N 2 grid cells, requiring total time O ( N 2 T search over all spatio-temporal regions; as in BATS, this requires time O ( N 4 W ) , bringing the total comple xity to O ( N
W ) . For RA TS, we rst aggre gate all store baselines and counts to the grid (as in CA TS), requiring time O ( N s T + N 2 each of the N 4 regions we search, we must calculate the baselines for  X current X  days on the y, requiring time O ( T ) , and compute the score function using the counts and baselines for current days, requiring time O ( W ) . Thus the total comple xity is O (
For lar ge grid sizes N , the O ( N 4 ) comple xity of searching over all spatial regions mak es a nai ve search over all such regions com-putationally infeasible. Ho we ver, we can apply the fast spatial scan of [18, 19], allo wing us to nd the highest scoring region and its p -value while searching only a small fraction of possible regions. In the purely spatial case, the fast spatial scan works by using a multi-resolution, branch-and-bound search to prune sets of regions that can be pro ven to have lower scores than the best region score found so far. We can easily extend this method to the space-time case: given a spatial region S , we must upper bound the scores D for all regions S 0 S and T W &lt; t min T . The simplest way of doing so is to compute separate bounds on baselines and counts of S 0 for each time step t , using the methods given in [18], then use these bounds to compute an upper bound on the score. It might also be possible to achie ve tighter bounds (and thus, better prun-ing) by enforcing consistency constraints across multiple days, i.e. ensuring that S 0 has the same spatial dimensions on each time step.
We evaluated our methods on two types of simulated outbreaks, injected into real Emer genc y Department and over-the-counter drug sale data for Alle ghen y County , PA. 2 First, we considered aerosol releases of inhalational anthrax (e.g. from a bioterrorist attack), produced by the BARD ( X Bayesian Aerosol Release Detector X ) simulator of Hog an et al. [7]. The BARD simulator tak es in a  X baseline dataset X  consisting of one year' s worth of Emer genc y Department records, and the quantity of anthrax released. It then produces multiple simulated attacks, each with a random attack lo-cation and environmental conditions (e.g. wind direction), and uses a Bayesian netw ork model to determine the number of spores in-haled by members of the affected population, the resulting number and severity of anthrax cases, and the resulting number of respi-ratory Emer genc y Department cases on each day of the outbreak in each affected zip code. Each simulated outbreak can then be injected into the baseline ED dataset, and our methods' detection performance can be evaluated using the testing frame work belo w.
All data was aggre gated to the zip code level to ensure anon ymity , giving 88 distinct spatial locations (zip code centroids). The ED data contained an average of 40 respiratory cases/day , while the OTC data averaged 4000 sales of cough and cold medication/day .
Second, we considered a  X Fictional Linear Onset Outbreak X  (or  X FLOO X ), with a linear increase in cases over the duration of the outbreak. A FLOO outbreak is a simple simulated outbreak dened by a set of zip codes, a duration T f loo , and a value D . The FLOO simulator then produces an outbreak lasting T f loo days, with t D res-piratory cases in each of the zip codes on day t , 0 &lt; and T f loo D = 2 cases on day t , T f loo = 2 t &lt; T f loo an outbreak where the number of cases ramps up linearly for some period of time, then levels off. While this is clearly a less realistic model than the BARD-simulated anthrax attack, it does have sev-eral adv antages. It allo ws us to precisely control the parameters of the outbreak curv e (number of cases on each day), allo wing us to test the effects of these parameters on our methods' detection performance. Also, it allo ws us to perform experiments using over-the-counter drug sale data as well as Emer genc y Department data, while the BARD simulator only simulates ED cases.

We now discuss our basic semi-synthetic testing frame work, fol-lowed by a discussion of the performance of our methods on each of the three main experiments (anthrax outbreaks in ED data, FLOO outbreaks in ED data, and FLOO outbreaks in OTC data).
Our basic goal in the semi-synthetic testing frame work is to eval-uate detection performance: what proportion of outbreaks a method can detect, and how long it tak es to detect these outbreaks. Clearly these numbers are dependent on how often the method is allo wed to  X sound the alarm,  X  and thus we have a tradeof f between sensiti v-ity (i.e. ability to detect true outbreaks) and detection time on the one hand, and specicity (i.e. frequenc y of false positi ves) on the other . More precisely , our semi-synthetic frame work consists of the follo wing components. First, given one year of baseline data (as-sumed to contain no outbreaks), we run the space-time scan statis-tic for each day of the last nine months of the year (the rst three months are used to pro vide baseline data only; no outbreaks in this time are considered). We thus obtain the highest scoring region S , and its score D = D ( S ) , for each of these days. Then for each  X at-tack X  that we wish to test, we do the follo wing. First, we inject that outbreak into the data, incrementing the number of cases as abo ve. Then for each day of the attack, we compute the highest scoring rel-evant region S and its score D , where a rele vant region is dened as one which contains the centroid of all the cases injected that day . The reason that we only allo w the algorithm to search over rele vant regions is because we do not want to reward it for triggering an alarm and pinpointing a region which has nothing to do with the outbreak. We then compute, for each day t = 0 : : : T outbreak T outbreak is the length of the attack), the fraction of baseline days (excluding the attack ed interv al) with scores higher than the max-imum score of all rele vant regions on days 0 to t . This is the pro-portion of false positi ves we would have to accept in order to have detected that outbreak by day t . By repeating this procedure on a number of outbreaks, we can obtain summary statistics about the detection performance of each method: we compute its averaged AMOC curv e [5] (average proportion of false positi ves needed for detection on day t of an outbreak), and for a x ed level of false positi ves (e.g. 1 false positi ve/month), we compute the proportion of outbreaks detected and the average number of days to detection.
Note that this basic frame work does not perform randomiza-tion testing, but only compares scor es of attack and baseline days. There are several disadv antages to this method: rst, since the base-lines b t i for each day are dif ferent, the distrib ution of scores for each day' s replica grids will be dif ferent, and thus the highest scoring re-gions may not correspond exactly to those with the lowest p -values. A second disadv antage is that it does not tell us how to perform cal-ibr ation : setting threshold p -values in order to obtain a x ed false positi ve rate in real data. This is discussed in more detail belo w.
We tested a total of 150 methods: each combination of the three aggre gation levels (BATS, CA TS, RA TS), ve space-time scan statis-tics (1-day , 3-day emer ging, 3-day persistent, 7-day emer ging, 7-day persistent) and the ten methods of time series analysis listed abo ve. We compared these methods against two simple  X stra w men X : a purely spatial scan statistic (assuming uniform underlying at-risk population, and thus setting the baseline of a region propor -tional to its area), and a purely temporal scan statistic (analyzing the single time series formed by aggre gating together all spatial loca-tions, using 1-day all mean). Since both the ED and OTC datasets were relati vely small in spatial extent (containing only records from Alle ghen y County), we used a small grid ( N = 16, maximum clus-ter size = 8), and thus it was not necessary to use the fast spatial scan. For lar ger datasets, such as nationwide OTC data, a much lar ger grid size (e.g. N = 256) is necessary to achie ve adequate spatial resolution, and thus the fast spatial scan will be an impor -tant component of our nationwide disease surv eillance system.
For each outbreak type, we compared the detection performance of our methods to the two stra w men, and also determined which of our methods was most successful (Table 1). Performance was eval-uated based on detection rate (proportion of outbreaks detected) at 1 false positi ve/month, with ties brok en based on average num-ber of days to detect; we list both the performance of our  X best X  spatio-temporal method according to this criterion, as well as a representati ve  X median X  method (i.e. the 75th best method out of 150). We compare the methods in more detail in Table 2, giv-ing each method' s average number of days to detection at 1 false positi ve/month, assuming that undetected outbreaks were detected on day T outbreak . For each of the ve scan statistics, we report performance assuming its best combination of time series analy-sis method and aggre gation level; for each of the ten time series analysis methods, we report performance assuming its best scan statistic. Le vel of aggre gation only made a signicant dif ference for the all max and strat max methods, so we report these results separately for BATS, CA TS, and RA TS. For each outbreak, we also construct AMOC curv es of the  X best,  X   X median,  X  purely temporal, and purely spatial methods; we present three of these curv es (one for each outbreak type) in Figure 1. We also discuss each outbreak type in more detail belo w.
For the anthrax outbreaks, we began with real baseline data for respiratory Emer genc y Department visits in Alle ghen y County in 2002. We used this data to simulate epidemics using BARD at two dif ferent levels of anthrax release: 0.125 (high) and 0.015625 (lo w). For each release amount, 60 simulated epidemics were cre-ated. Separately for the high and low levels, we tested all methods, forming an average AMOC curv e for each over all simulated epi-demics, and measuring detection rate and average days to detect.
For the high release dataset, all of the methods tested were able to rapidly detect all 60 outbreaks. For a x ed false positi ve rate of 1/month, every method detected all outbreaks (100% detection rate), with average time to detection ranging from 1.6 to 2.067 days. The top method (1.6 days to detect) was the 1-day statistic using all mean, and half of all methods detected in 1.8 days or fewer . Since the average delay from release to the rst reported case was 1.18 days, these times were close to ideal detection performance. All methods except all max outperformed the purely temporal scan statistic (100% detection rate, 1.9 days to detect), and all methods outperformed the purely spatial scan statistic (100% detection rate, 2.317 days to detect). For this dataset, there was very little dif fer -ence between the best and worst performing methods, and thus it is hard to dra w deniti ve conclusions. Ne vertheless, we observ ed that shorter temporal windo ws performed better (1-day was best, 7-day was worst), and there were no signicant dif ferences between emer ging and persistent scan statistics. Looking at the outbreak curv e for this epidemic, it is clear wh y this is the case: all out-breaks have huge spik es in the number of cases starting on day 1 or 2, so there is no adv antage to having a longer windo w; and since there is essentially no  X ramp-up X  in the number of cases (just the lar ge spik e, at which point the outbreak is obvious to any method) there is no adv antage to the emer ging over persistent statistics. For time series analysis, the all mean method performed best, follo wed by adj EWMA. This result is some what surprising, suggesting that the ED baseline data has very little day of week or seasonal trends.
Results on the low release dataset were similar , except for two dif ferences resulting from the amount of release. First, 7 of the 60 outbreaks were missed by all of our methods; these outbreaks consisted of a very small number of cases (less than 5 in total), and as a result there was essentially no signal to detect. The other 53 outbreaks typically produced a lar ge and obvious spik e in the number of cases (ag ain, with very little ramp-up prior to the spik e), though the delay between release and spik e was longer on average (2.6 days from release to rst reported case). Ag ain, the 1-day win-dow was best, though the 3-day statistics performed almost as well, and again all mean and adj EWMA were the top two methods. Our spatio-temporal methods again outperformed the stra w men, requir -ing 3.679 days to detect (best) and 3.906 days to detect (median) at 1 false positi ve/month. This was substantially better than the purely temporal and purely spatial methods, which required 4.250 days and 5.094 days respecti vely .
For the FLOO ED outbreaks, we again began with the 2002 Al-leghen y County ED dataset. We injected three types of FLOO attacks, assuming that only zip code 15213 (Pittsb urgh) was af-fected: ( D = 4 ; T f loo = 14 ) , ( D = 2 ; T f loo = 20 20 ) . Thus the rst attack has the fastest-gro wing outbreak curv e (4 t cases on day t ), and the third has the slo west-gro wing out-break curv e ( t cases on day t ). For each outbreak type, we simu-lated outbreaks for all possible start dates in April-December 2002, and computed each method' s average performance over all such outbreaks. All the spatio-temporal methods were able to detect all injected outbreaks at a rate of 1 false positi ve/month; not sur -prisingly , median number of days to detect increased from 2.076 for the fastest gro wing outbreak, to 5.066 for the slo west gro w-ing outbreak. All of these detection times were more than one full day faster than the purely spatial and purely temporal methods, with one exception (0.22 days faster than purely spatial for D Ag ain, the all mean method performed well (1-day all mean was the winner for D = 4, with a detection time of 1.748 days), as did adj EWMA and strat EWMA (3-day emer ging strat EWMA was the winner for D = 2 and D = 1, with detection times of 2.898 and 4.484 days respecti vely). Our most interesting result was the effect of the temporal windo w size W : for the fastest gro wing outbreak, the 1-day method detected outbreaks 0.2 days faster than the 3-day and 7-day methods, but for the slo west gro wing outbreak, both 3-day and 7-day methods detected outbreaks a full day faster than the 1-day method. Emer ging methods outperformed persistent meth-ods for approximately 80% of our trials, though the dif ference in detection time was typically fairly small (0.02-0.10 days, depend-ing on the time series analysis method). We also observ ed that higher aggre gation typically performed better for the all max and strat max methods (i.e. RA TS performed best, and BATS worst). perf ormance.

For the FLOO OTC outbreaks, we began with one year' s worth of data for retail sales of over-the-counter cough and cold med-ication in Alle ghen y County , collected from 2/13/04-2/12/05. We injected three types of FLOO attacks: for the rst two, we again as-sumed that only zip code 15213 was affected, but (since the overall numbers of OTC sales were much higher than the overall numbers of ED visits) we injected lar ger numbers of counts, ( D = 14 ) and ( D = 20 ; T f loo = 20 ) . For the third attack, we assumed that all zip codes in Alle ghen y County were affected, using 1 ;
T f loo = 14 ) for each. For each outbreak type, we simulated out-breaks for all possible start dates over the last nine months of our data, and computed each method' s average performance over all such outbreaks. Our rst observ ation was that these attacks were substantially harder to detect than in the ED data: for the two local-ized attacks, our median methods only detected 98.1% and 59.5% of outbreaks for the faster -gro wing ( D = 40) and slo wer -gro wing ( D = 20) outbreaks respecti vely . It appears that the main reason for this was the dif culty in accurately predicting the OTC counts for the baseline days, as we observ ed huge dif ferences in perfor -mance between the various time series analysis methods. The data contained signicant seasonal and day of week trends, as well as other irre gularities (e.g. lar ge spik es in sales in single stores, prob-ably resulting from promotions), and most of our methods were not entirely successful in accounting for these; nevertheless, the y performed much better than the purely spatial and purely tempo-ral methods, which only detected 23-32% of these outbreaks. Our second observ ation was that the strat Kull method performed re-markably well in predicting the localized outbreaks, detecting with 100% accurac y in 2.32 and 3.89 days for D = 40 and D = 20 re-specti vely; strat Kull and all Kull detected the D = 20 outbreaks over two days faster than any other methods. This suggests that those methods were able to predict baselines for the non-attack days much more accurately than any of the other time series anal-ysis methods: using the current day' s counts to predict the current day' s baselines allo ws accurate adjustment for seasonal trends, and if the attac k is suf ciently localized , only slightly reduces detec-tion power . Clearly it would be better to have a method which correctly predicts the trends without using the current day' s counts, but none of the methods discussed here were able to do this. For the non-localized attack (cases added to every zip code), the power of strat Kull was substantially reduced, and it was only able to detect 36% of outbreaks, while our best-performing method (strat EWLR) detected 48%. And this is far from the worst case for strat Kull: since dif ferent zip codes have dif ferent average sales, adding the same number of counts to each creates a lar ge amount of space-time interaction. If we had instead multiplied counts in each zip code by the same factor , strat Kull would have no power to detect this. We also note that the 1-day statistics performed best for all three outbreak types on the OTC data, though the 3-day emer g-ing statistics performed almost as well. Ag ain, emer ging methods consistently outperformed persistent methods, and the dif ference in detection time was lar ger than on the ED data (typically 0.05-0.20 days). Finally , we note that the lower levels of aggre gation (BATS and CA TS) outperformed RA TS for the  X max X  methods; this is the opposite result from what we observ ed on the ED data.

Based on these conicting results, it is dif cult to recommend a single method for use on all datasets and outbreak types. As sho wn abo ve, the optimal temporal windo w size depends on how fast the number of cases increases, with longer temporal windo ws appro-priate for more slo wly gro wing outbreaks. The optimal temporal windo w is also affected by our desired tradeof f between number of false positi ves and detection time: a lower acceptable false positi ve rate (and thus, longer acceptable detection time) increases the opti-mal windo w size. For example, for the FLOO ED (1,20) outbreak, the 3-day emer ging statistic has the fastest time to detection at a rate of 1 false positi ve/month, while the 7-day emer ging statistic has the fastest time to detection at a rate of 1 false positi ve/year . As noted abo ve, the emer ging statistics consistently outperform the corre-sponding persistent statistics, and while the amount of dif ference is not that lar ge (0.02-0.20 days across all outbreaks and methods), even slightly earlier detection may mak e a substantial dif ference in the efcac y of outbreak response. It appears that the 3-day emer g-ing statistic is a reasonable compromise solution, at least for the set of outbreaks tested. It may also be a good idea to run emer g-ing statistics with dif ferent windo w sizes in parallel, for better de-tection of both fast-gro wing and slo w-gro wing outbreaks; optimal combination of detectors is an interesting and open research ques-tion. It is clear that the best time series analysis method depends on the characteristics of the dataset, as well as whether the out-break is spatially localized or occupies a lar ge spatial region: the strat Kull method is excellent for localized outbreaks, but should be used only in parallel with another method that can detect lar ge-scale outbreaks. For datasets with little seasonal trend, such as the ED data used here, very simple mean and mo ving average methods are suf cient, but it is still an open question to nd a method which can accurately predict baseline counts for OTC data without using the current day' s counts to predict the current day' s expectations.
As noted abo ve, our testing frame work simply compares scores of the highest scoring regions on each day , and computes AMOC curv es; no randomization testing is done, and thus we do not ac-tually compute the p -value of disco vered regions. Because our detection performance is high, it is clear that the attack ed regions would have lower p -values than the highest scoring regions on non-attack ed days. But this does not answer the question of calibration: at what threshold p -value should we trigger an alarm? If non-attack ed days were actually generated under the null hypothesis, we could choose some level a and be guaranteed that we will only trigger false alarms that proportion of the time (e.g. once every 20 days for a = : 05). Ho we ver, our null hypothesis, that each count c is generated by a Poisson distrib ution with mean b t i , is clearly false, since b t i is only an estimate of what we expect c t i to be, assuming that no outbreak is present. If this estimate were unbiased and ex-actly precise (zero variance), then we would achie ve a false positi ve rate of a . In practice, howe ver, this estimate can be both biased and highly imprecise. For any method of calculating baselines that is approximately unbiased, but has non-zero variance (i.e. all of our time series analysis methods except all max and strat max), we ex-pect the proportion of false positi ves to be greater than a , since the scan statistic picks out any regions where b t i is an underestimate of c . The all max and strat max methods, on the other hand, are con-serv atively biased (predicting values of b t i which overestimate c on average) but also have non-zero variances; thus the y may result in proportions of false positi ves either higher or lower than a . To examine the calibration of our methods, we calculated the p -value for each day in both the ED and OTC datasets (with no injected attacks). We used a 3-day emer ging scan statistic, BATS aggre ga-tion, with four dif ferent time series analysis methods: two unbiased methods (adj EWLR and all mean) and two conserv ative methods (all max and strat max). R = 100 randomizations were performed, and we counted the proportion of false positi ves at a = 0 a = 0 : 05 for each method on each dataset. See Table 3 for results.
As expected, we observ e a lar ge number of false positi ves in both datasets for the unbiased methods. For the OTC dataset, we also have high false positi ve rates even for the conserv ative methods. What conclusions can we dra w from this? Because of the variance in our predictions, the baseline data, especially the OTC data, is not t well by the null hypothesis. Ne vertheless, the lik elihood ratio statistic (which serv es as a sort of distance away from the null hypothesis) is very successful at distinguishing between attacks and non-attack ed days. So how can we calibrate the statistic? One option would be to use an unbiased method with a much lower threshold a , but the problem with this is that it would require a huge number of randomizations to determine whether the p -value is less than a . Another option would be to use a conserv ative method, but the problem is that these methods not only record fewer false positi ves, but also are less able to detect a true positi ve. In fact, as our results abo ve demonstrate, the conserv ative methods typically have much less power to distinguish attacks from non-attack ed days for a given level of false positi ves, so this is clearly not a good idea. A better option is to trigger alarms for a given threshold on the scor e rather than on the p -value, with that threshold learned from pre vious data (e.g. the year of ED and OTC data used here). An even better solution might be to account for the uncertainty of our baseline estimates b t i , as discussed belo w, and thus mak e our null hypothesis more accurately describe the real data.
We have presented a new class of space-time scan statistics de-signed for the rapid detection of emer ging clusters, and demon-strated that these methods are highly successful on the task of rapidly and accurately detecting emer ging disease epidemics. We are cur -rently working to extend this frame work in a number of ways. Per -haps the most important of these extensions is to account for the imprecision in our baseline estimates b t i , using methods of time series analysis which not only predict the expected values of the  X current X  counts but also estimate the variance in these estimates. Our current dif culty is that we are testing the null hypothesis that all counts c t i are generated from the estimated values b these values are only estimates, the null hypothesis is clearly false. As a result, as we demonstrated in the pre vious section, the stan-dard randomization testing frame work results in lar ge numbers of false positi ves, i.e. on most non-attack days we still observ e a p -value less than 0.05. The combination of time series methods which account for imprecision of estimates, and scan statistics which use distrib utions that can account for mean and variance separately (e.g. Gaussian or negative binomial distrib utions) should allo w us to cor -rect these problems. This will also mak e the distinction between building-aggre gated, cell-aggre gated, and region-aggre gated time series methods more rele vant, as the variance computations will be very dif ferent depending on the level of aggre gation. A second (and related) extension is accounting for factors such as overdispersion and spatial correlation between neighboring counts. Our current methods assume that each spatial location, cell, or region has an independent time series of counts, and thus infer baselines inde-pendently for each such time series. When we extend the model to distrib utions that model mean and variance separately , we should be able to calculate correlations between time series of neighbor -ing spatial locations, and adjust for these correlations.
Finally , we are in the process of applying our spatio-temporal scan statistics to nationwide over-the-counter drug sales, searching for emer ging disease outbreaks on a daily basis. Scaling up the sys-tem to national data creates both computational issues (the use of the fast spatial scan is essential for searching lar ge grids) as well as statistical issues (dealing with irre gularities in the data, such as missing data, and increased sales resulting from product promo-tions). We are currently working with state and local public health ofcials to ensure that the clusters we report correspond to rele vant potential outbreaks, thus rapidly and accurately identifying emer g-ing outbreaks while keeping the number of false positi ves low.
