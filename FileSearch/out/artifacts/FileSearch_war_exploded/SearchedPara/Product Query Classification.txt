 Web query classification is an effective way to understand Web user intents, which can further improve Web search and online advertising relevance. However, Web queries are usually very short which cannot fully reflect their meanings. What is more, it is quite hard to obtain enough training data for training accurate classifiers. Therefore, previous work on query classification has focused on two issues. One is how to represent Web queries through query expansion. The other is how to increase the amount of training data. In this paper, we took product query classification as an example, which is to classify Web queries into a predefined product taxonomy, and systematically studied the impact of query expansion and the size of training data. We proposed two methods of enriching Web queries and three approaches of collecting training data. Thereafter, we conducted a series of experiments to compare the classification performance of using different combinations of training data and query representations over a real data set. The data set consists of hundreds of thousands queries collected from a popular commercial search engine. From the experiments, we found some interesting observations, which were not discussed be-fore. Finally, we proposed an effective and efficient product query classification method based on our observations. H.3.m [ Information Storage and Retrieval ]: Miscella-neous Algorithms, Experimentation Product Query Classification, Query Enrichment
More and more Web users take Web search engines as an indispensable tool for obtaining the desired information. In order to return the correct information to end users, in terms of either Web pages, or advertisements, search engines have to accurately understand the users X  intents. Clearly, there are many aspects which can impact users X  intents, including users X  demographics, users X  locations and so on. Among all of these aspects, Web queries issued by the users explicitly and directly reflect the users X  intents. Therefore, many ef-forts have been conducted to estimate users X  intents through Web queries [3, 12, 6, 21]. In this paper, we will focus on Web query classification, which can be used to infer users X  intents by classifying the Web queries into some predefined taxonomies. For example, once we know the queries  X  X d450 X  and  X  X enovo x61 X  belong to  X  X amera X  and  X  X omputer X  re-spectively, we can recommend some relevant information to users even if we do not have the exact matched information about  X  X d450 X  and  X  X enovo x61 X .

However, as noted in previous work [3, 12, 23], Web query classification is quite challenging due to two factors. One is that Web queries are very short, containing less than 3 terms in most cases, which cannot provide enough clues for classification. Another is that it is hard to collect enough training data since the queries may be too short for the labelers to understand without extra efforts such as resorting to search engines. To solve the first problem, researchers have exploited ways to enrich Web queries. For example, [23] uses Web search results to represent Web queries, which can help understand Web queries. For the second problem, some work has tried to automatically expand a small training data set using click-through log data [12], or take labeled Web pages as the training data [23].

Limited by the availability of large scale data sets, previ-ous work did not exploit several important aspects of Web query classification. For example, some work adopts existing Web pages as training data to train classifiers for Web query classification. However, are the classifiers trained over Web pages applicable to Web query classification? Some work uses Web search snippets returned for queries to represent Web queries. Is this an effective way? Do we still need to enrich queries when we have enough training data? If query enrichment through Web search is helpful for query classification, can we find an approach which can effectively and efficiently classify Web queries without relying on Web search too much?
In this paper we take product query classification as an example and try to answer the above questions through a comprehensive study over a large data set collected from a commercial search engine. In this paper, we exploit two ways to enrich Web queries. One is the well studied method which enriches Web queries by search snippets. Another one is to expand queries by their similar queries which can be au-tomatically discovered from click-through log data. Experi-mental results show that both methods can improve the clas-sification accuracy significantly while the former performs better in most cases.

On the training data side, besides using labeled queries (as well as their enriched representations) to train classifiers, we also build classifiers using labeled product names. Prod-uct names are effective representation of products, based on which we can build accurate product classifiers. How-ever, due to the difference of languages for Web queries and product names, classifiers trained over product names are much worse than those trained over queries. To remove the gap between product names and Web queries, we put for-ward a solution to translate product names to Web queries (which we call pseudo queries to differentiate them from real queries). Then we can train classifier using the pseudo queries. Although these classifiers cannot beat those trained over labeled queries, they are clearly better than the clas-sifiers trained over product names directly. To study the impact of training data size, we vary the size of the training data and compare the different query representation meth-ods. We found that the smaller the training data set, the larger the difference between different query representation methods. Using search snippets to represent Web queries achieves the best results compared to other methods. How-ever, retrieving search snippets is time consuming and it may be infeasible for a large scale query set. Therefore, we try to minimize the dependency over search snippets. We build a classifier using original queries and classify Web queries di-rectly. Only when we cannot classify the queries with high confidence, we use snippets. With this solution, we need to retrieve snippets for less than 30% queries and maintain the accuracy as using search snippets for all Web queries.
The rest of the paper is organized as follows. We discuss the related work in Section 2. Section 3 presents the query representation methods and training data selection. Section 4 describes the data sets used for experiments, evaluation methodology, experiments and results. Section 5 gives con-clusion of the paper and some possible future research issues.
Web queries are currently the major bridge between Web users and search engines. Accurately understanding Web queries is of great importance. Web query classification pro-vides such a way. Generally speaking, query classification can be split into two groups: (1) classification according to queries X  types, such as informational or navigational or transactional [2, 9, 10]; (2) classification according to the queries X  topic, such as  X  X omputers/hardware X  or  X  X omput-ers/software X  [4, 23].

For query type classification, Broder classifies Web queries according to their intents into three types: informational, navigational and transactional in [2]. Although Broder does not provide a way to distinguish different types of queries, he makes an informative survey which shows that the naviga-tional queries takes up 24.5% while the informational queries and transactional queries take up 39% and 36% approxi-mately. Following Broder X  X  work, Lee et al. further study whether the types of a query is predictable and how to pre-dict it [10]. They propose two types of features for the query type classification: user-click behavior and anchor-link dis-tribution, which are proved to be quite effective empirically.
Besides working with the taxonomy defined by Broder, people conduct query type classification in other directions. For example, Gravano et al. discuss how to categorize queries according to their geographical locality [7]. Similar work on locality of Web queries is conducted in [16, 25]. Another work related to query type classification is presented in [6]. The authors focus on capturing commercial intention from search queries and Web pages.

In query topic classification, queries are classified into some predefined categories according to queries X  topics or subjects. At the very beginning, some researchers manually classified Web queries for query analysis, especially on the query topic distribution in query logs [24]. Recently, au-tomatic topic classification techniques have been exploited and they have been used for building query filters, study of user interests and enriching Web taxonomies [19].
In [19] and [4], the authors work on query topic classifi-cation independently with the similar techniques. Both of them use real-world search engines to obtain highly ranked Web documents based on each unknown query. These doc-uments are used to extract cooccurring terms and to create a feature set. They can then use these features to judge the most appropriate categories of a query. Similarly, both [23] and [3] rely on Web search results to enrich Web queries for classification, except that [3] focuses on rare queries while [23] takes an existing taxonomy as the bridge to connect Web queries and the target categories. Instead of relying on Web search results, Beitzel et al. study the topic query classifica-tion in a different direction [1]. Given a set of labeled queries and unlabeled queries, Beitzel et al. exploit several classifi-cation approaches including exact-match using labeled data, N-Gram match using labeled data, classifiers based on per-ceptron and an approach adapted from computational lin-guistics named selectional preferences [13]. Different from previous work which emphasizes on feature representation of Web queries, Li et al. study how to drastically increase the amount of training data by semi-supervised learning with click-though log data [12]. They come to the conclusion that automatically increasing the training data can lead to significant improvement in classification performance.
In this paper, we will focus on query topic classification and take product query classification as an example. In pre-vious work about query topic classification, as we mentioned above, researchers have focused on either how to represent Web queries (such as by using search results), or how to get more training data. Little work has been done to ex-ploit the impact of Web query representation when varying the amount of training data. What is more, the data sets used in previous studies are relative small. In this paper, we are going to employ hundreds of thousands Web queries col-lected from a commercial search engines for comprehensive comparisons among different classification methods.
Query topic classification is to classify Web queries into a predefined taxonomy based on the queries X  topic, or about-ness. It is closely related to conventional text classification, such as document or Web page classification. However, in query topic classification, the objects are Web queries, which are much shorter than documents/Web pages in text classifi-cation and thus become more ambiguous. For example,  X  X p-ple X  can mean either a kind of fruit, or an IT company. This makes the query topic classification problem even harder.
We highlight  X  X opic classification X  since we want to differ-entiate this problem from query type or functionality clas-sification. Query type classification is to group the queries according to their functionality. For example, when peo-ple submit a query  X  X merican airlines home X , they probably have a particular site in mind and want to reach it directly through this query. However, for queries like  X  X ars X , people may want to find more information about  X  X ars X . Therefore, the former group of queries are called X  X avigational queries X , while the latter is referred by  X  X nformational queries X  [2]. It is clear that we need to consider different features for dif-ferent classification problems (topic classification, or type classification). In this paper, we will focus on the topic clas-sification problem.

More specifically, we will work in a special domain, that is, product query classification. The goal is to classify a query into an existing product taxonomy, which includes categories like  X  X amera &amp; Optics X ,  X  X omputing X , and so on. The taxonomy may have a hierarchical structure, as it is in most cases. Figure 1 shows part of taxonomy used in this paper. We choose this domain because product queries are very important for both Web users and commercial search engines, especially when more and more people tend to pur-chase what they need through Internet or at least conduct some online research before the actual deals happen. An-other reason is that we have plenty of data in this domain, over which we can conduct solid experiments and provide insights for classification problem in other domains.
As we mentioned above, Web queries are usually very short, consisting of less than 3 terms on average [23, 8]. Such short text may not be enough to reflect the aboutness of the queries well, which results in bad classification performance. Therefore, people have tried to enrich the Web queries as the first step. In the following, we will introduce two ways for query enrichment. One is using Web search, which is widely adopted in previous work [23, 3]; another one is to use similar queries where the similarity is estimated from click-through log data.
Given a query, a straightforward way to catch its meaning is through its contexts. For example, even we do not know what  X  X D450 X  is, but we can make a safe prediction based on the text surrounding it, such as  X ...Canon PowerShot SD450 digital camera specifications...  X . We can use many approaches to obtain such contexts. One convenient and popular one is using search engines, which can provide a list of Web pages given a certain query. For most popular search engines, the returned Web pages are sorted according to their relevance to the query, along with the URL, title, a piece of text, which is often called  X  X nippet X . The snippet is a kind of query-dependent summary, which provide the con-text surrounding the query. Previous studies have validated the effectiveness of using Web search results to represent Web queries and found that titles and snippets of the re-turned pages are good to represent Web queries, while the text of the whole Web page may introduce noise. Therefore, in this paper, we will follow them and use titles and snippets to represent Web queries.

One common debate about using search snippets is that we expect search engines to return more relevant Web pages based on the query classification results, then how we can rely on search results for query classification. As we stated above, what we need is a method to get the contextual infor-mation around the queries. Web search engines provide such a way. Even if the search results are not strongly related to the Web queries, we can still use it to get the contextual information. Actually, the same idea has been exploited in pseudo relevance feedback, which can greatly improve search performance [15].
Instead of using the search snippets returned by search en-gines, we can use the similar queries to enrich a target query. For example, even if we know nothing about X  X d450 X , we can infer its meaning from its similar queries, such as X  X owershot sd450 X ,  X  X anon elph X  and so on. Similarity among queries can be automatically estimated from the click-through log data collected by commercial search engines. As we know, after search engines return the pages according to a user X  X  query, the user may click some pages. The user, the sub-mitted query, as well as the clicked pages constitute the click-through data. Intuitively, the queries leading to the same clicked pages are similar in terms of their aboutness. Based on this observation, which is widely used in previous study, we put forward a way to calculate query similarities and further enrich queries based on the calculated similarity.
Specifically, we construct a bipartite graph over queries and URLs [17]. If a user clicks a URL given a query, we add a link from the query to the URL. The link is further weighted by the click number. With this graph, a query X  X  similar queries can be found by a random walk starting from the given query. The random walk repeatedly travels from queries to URLs and then from URLs to queries. The tran-sition probabilities are proportional to the weights. Finally, the similarity between two queries is then measured by the hitting times, which are the expected number of the random walk from one query to another.
The quantity and quality of training data is critical for most classification problems. As we discussed above, it is hard to collect large scale manually labeled Web queries. Therefore, some previous work takes labeled Web pages as the training data [23], while others starts from a small la-beled query set and automatically discover more queries for training [12]. Since we are working on product query clas-sification, we take labeled products as training data accord-ingly. Considering the difference between the languages of queries and products, we propose to translate the labeled products to labeled queries, which can be used as training data. Finally, we can also obtain training data from the click-through log data with some heuristic rules.
With the development of online shopping, more and more products are available online with labels, as those in MSN Shopping (http://shopping.msn.com), Amazon (http://www. amazon.com) and eBay (http://www.ebay.com). In this pa-per, we collect the products from MSN Shopping. Most of the products have product names, product description and other attributes like color, size and weight. Clearly, product names and product descriptions are more proper to repre-sent the products in the context of Web query classification since the queries usually do not contain information about detailed attributes. We further compared product names and product description empirically and found that classi-fiers trained over product names are much better than those trained over product descriptions. Therefore, we finally use the product names (associated with labels) as the training data in this paper.
As we will discuss in Section 4, the performance of the classifiers trained over product data is not very promising. One of the possible reasons is that the language of product names is different from that of product queries. In other word, the feature space (details about feature generation are presented in Section 4.2) of the training data is different from the feature space of test data. To overcome this problem, we put forward a method to translate the labeled product names to labeled queries. We call the translated queries as  X  X seudo Queries X  to distinguish them from the real queries.
Specifically, we generate pseudo queries based on the idea and method of statistical machine translation. We treat product names as sentences in the source language, and their corresponding query forms as those in the target language. Given a set of training sentence pairs (which will be dis-cussed shortly), we learn a translation model based on tuple ngrams [5], while other translation models can also applied to our problem. At the highest level, a tuple-ngram-based translation model aims to learn the joint probability of a sen-tence pair, which is modeled by ngrams of phrase pairs. At translation time, the model finds the target sentence (query form) that maximizes the joint probability given a source sentence (product name). To deal with words/phrases that never occurred in training, we use the backoff strategy that a word/phrase can always be translated into itself or to NULL ( i.e. , deletion). The technical details of this approach can be found in [11].

To obtain the training sentence pairs, we leverage a large amount of click-through data in the product search domain. In product search, each search result corresponds to a dis-tinct product item in the data feed. Whenever a user issues a query and clicks on a product item, we create an associa-tion of that query and the corresponding product name as a sentence pair, and we select the most frequent such pairs to form our training data.
With the labeled product data, we can obtain the labels of some queries from click-through log data with heuris-tic rules. Intuitively, among the returned search results from a commercial search engine for a query q , if most of the users click on the products with the same label l , we can claim that we should label q by l . Formally speaking, give a query q , we aggregate its associated clicked prod-ucts according to their categories and get a set click ( q ) = { ( l 1 , n 1 ) , . . . , ( l i , n i ) , . . . , ( l m , n m ) } , where l label), n i is the number of clicked products belonging to l m is the number of categories. Then we can label q by l k if n k meets the following condition. Note that a query may have more than one label as labeled in this way.
This method is kind of voting from the Web users, which reflects the views of the majority of the users. However, we have to admit that this method may introduce some wrong labels due to noisy clicks. Therefore, we will use a large t to reduce the wrong labels. After collecting the labeled queries using the above method, we can adopt the two query enrichment methods to expand the queries and thus enrich the representation of the training data.

Three natural questions arise here: Firstly, how consis-tent is it between the automatically generated query labels and the manually assigned query labels? Secondly, if we can automatically label the queries using the click-through log data, why do we have to build classifiers? Thirdly, does the classifier trained over the automatically collected data work well for the queries which cannot be automatically labeled using click-through data? For the first question, we empiri-cally compared the automatically generated labels with the labels provided by three human labelers over 6,000 randomly sampled queries. We can see the consistency is very high, which ranges from 94.9% to 98.3%. For the second one, the answer is clear since not all the queries have enough appearances in the click-through log, especially for the new emerging queries, and thus require a classifier to do the clas-sification. For the third one, we randomly selected 2,000 queries, which cannot be automatically labeled using the click-through data, and invited three human labelers to la-bel them. The experiments show that the trained classifiers work pretty well on these queries.
The taxonomy we use in this paper is from MSN Shopping, which consists of thousands of categories spanning over sev-eral levels. For simplicity, we consider two flat classification tasks, one is to classify the queries into the direct children of the root node, and another is to classify queries into the direct children of  X  X omputing X  (See Figure 1). We refer to the former as X  X evel-1 X  X lassification and the latter as X  X om-puting X  classification. For  X  X evel-1 X , we have 26 categories including X  X omputing X  X nd for X  X omputing X  X e have 11 cat-egories. We choose two classification tasks because we want to demonstrate the contribution of different training data and query enrichment methods for classification tasks with different granularity.

Since it is hard and expensive to collect a large manually labeled query data set, we use the method introduced in Section 3.3.3 to automatically obtain the labeled data for experiments. We randomly sampled 710,670 queries which lead to 40,676,169 clicks from the click-through log collected from MSN Shopping. Clearly, when the threshold t in the inequality (1) is small, a query may have more than one labels. The smaller t , the more labels a query may have. As shown in table 1, when the threshold is 0.0, where we count all the categories associated with the clicked products for a query, the average number of labels assigned to the queries is 1.388. By increasing the threshold, we see that both the number of labeled queries and the average number of labels per query become smaller. The average number of labels for the product queries is quite different from that of the general Web queries as presented in [23], which means that the product queries are not so ambiguous. This is one of the reason why the classification performance reported in this paper is much higher than that in [23]. In this paper, in order to obtain high quality labeled data, we set the threshold as 0.5, which results in 662,692 queries. Similarly, we collect 24,056 labeled queries for  X  X omputing X .

To clarify the first question raised in Section 3.3.3, we randomly selected 6,000 queries from the 662,692 queries. Then we invited 3 human labelers to labels the queries man-ually. For simplicity, we let them label the queries with the 26 categories in  X  X evel-1 X  classification task. Actually, we can do the same evaluation over the  X  X omputing X  classifi-cation task. Table 2 shows the consistency of the labeling results among human labelers and the automatic rule based method. Consistency between two labelers (including the rule-based method) is measured by the percentage of queries which are assigned the same labels. The column  X  X ajority X  means that we combine the labeling results from the three labelers by voting when the labelers disagree with each other for a certain query. For a few queries, the labelers come to three different labels, then we invite a fourth labeler to make the decision. From this table we can see that the consistency between the rule based method and the human labelers is quite high, ranging from 0.949 to 0.983, which is 0.963 on average. The consistency between the rule-based method and the combination of the three labelers is 0.971, which assures us to conduct experiments over the automatically labeled data. Related to the observation of smaller average number of labels a product query may have, we find that the consistency among human labelers in this paper is much higher than that in [23].

To train the translation model as described in 3.3.2, we extracted 2M click events from a the product search query log in Live Search. We selected 500K unique (query, prod-uct) pairs that occurred more than once in the click data, which served as training sentence pairs for our translation model.

To calculate the query similarity as presented in 3.2.2, we use a query log from Live Search, which contains hundreds of millions queries and URLs. Adult queries are removed from the query log. We also removed the query and URL pair if the click number is below than a specific threshold (We set the threshold as 10 empirically in this paper).
There are several state-of-the-art classification models in-cluding Support Vector Machines (SVM) , Logistic Regres-sion (LR) [18] and so on. In this paper we take LR in that it is easy to implement, fast in training and achieves com-parable classification performance in most cases [18]. LR is to model the conditional probability of labels given a query, Table 1: Statistics of Labeled Data when Changing Threshold t Table 2: Consistency among Human Labelers and Automatic Rule based Method Rule Based 0.949 0.956 0.983 0.971 Labeler1 0.950 0.952 0.964 Labeler2 0.973 0.964
Labeler3 0.988 as shown below: where q denotes a query and l denotes the possible labels.  X  ( q, l ) represent the features extracted for the pair q and l .  X  j is the parameter corresponding to  X  j ( q, l ) which indi-cates the importance or contribution of the feature  X  j ( q, l ) in determining the probability of l given q . It is clear that two components are important for LR. One is how to define the features and the other is how to estimate the parame-ters. Several methods have been exploited in the literatures for estimating the parameters using a maximum likelihood objectives [18, 22]. In this paper, we adopt L-BFGS, which is shown to converge significantly faster than others [22].
Feature extraction is very important for classification prob-lems and the optimal feature spaces vary a lot for different tasks. In this paper, we use unigram and bigram features as well as several regular expressions to generalize terms with certain patterns. Unigram and bigram are frequently used in language models [14], which refer to the single terms and two consecutive terms in a query respectively. Note that for bigrams, we treat sentence start  X  &lt; s &gt;  X  and sen-tence end  X  &lt; /s &gt;  X  as two special terms. Before we generate the unigrams and bigrams, we do some conventional prepro-cessing work including converting the queries to lower cases, removing stopwords and stemming. We adopt some regu-lar expressions to generate features since we want to lever-age some informative patterns which are prevail in product queries. For example, products with the same label from the same producer may have the same patterns in their models, such as  X  X D430 X  and  X  X D450 X . Several examples of the pat-terns used in this paper include  X  &lt; NUM &gt;  X (such as  X 2007 X ),  X  &lt; CHAR &gt;&lt; NUM &gt;  X  (such as  X  X d450 X ) and so on.
We employ the standard measures to evaluate the perfor-mance of query classification, i.e. precision, recall and F1-measure [20]. To evaluate the average performance across multiple categories, there are two conventional methods: Table 3: Comparison between Products and Pseudo Queries Product 0.591 0.549 0.539 0.682
PseudoQry 0.654 0.615 0.554 0.700 micro-average and macro-average. Micro-average gives equal weight to every query, while macro-average gives equal weight to every category, regardless of its frequency.

For some experiments in this paper, we allow the classifier to predict the label of a query only when the confidence of the classifier is beyond a threshold. In these scenarios, we introduce two more measurements, accuracy and coverage. Coverage means the percentage of the queries for which the classifier X  X  confidence is beyond a predefined threshold. Ac-curacy means the percentage of queries among the covered queries which are correctly classified. Accuracy is actually micro-precision when each query has only one label.
In our experiments, we split the labeled queries into train-ing and test data. In order to reduce the uncertainty of data split, a 3-fold cross validation procedure is applied . That is, we randomly split the queries into 3 folds and pick up one fold as the test data and the other two folds as the training data each time. Then we report the average results of the three runs. For some experiments, such as using product and pseudo queries as training data, we do not use queries as training data, but we still run the test over each fold and report the average results.

In the discussions of the experiments, when we claim one classifier is better than another, we use a pair-difference t-test over the results of the cross-validation to verify the sig-nificance of the difference. By convention, we say that a difference between means at the 95% level is  X  X ignificant X ; a difference at 99% level is  X  X ighly significant X .
We first compare two kinds of training data, one is prod-uct data as described in 3.3.1 and the other is  X  X seudo Queries X  (denoted by PseudoQry ) given in 3.3.2. We ran-domly sampled 1,500,000 and 500,000 product names from the labeled product data in MSN Shopping, for the  X  X evel-1 X  and  X  X omputing X  tasks, respectively. For each of these product names, we can get a set of pseudo queries using the translation model and keep those whose confidence is larger than 0.15 (detailed study of the threshold is given in Section 4.4.6). As we can see from Table 3, classifiers trained over pseudo queries can improve the classification performance, especially for  X  X omputing X  classification, where the relative improvement is more than 11%. Using the t-test, we find that the improvement is  X  X ighly significant X .
In this section we study the effect of enriching queries through their similar queries as introduced in Section 3.2.2 when changing the size of the training data. For each query, we collect all the queries with which the similarity is larger than 0.001. Detailed study of the threshold is given in Section 4.4.6. Here, we study four combinations, (1) us-ing original queries for training and test (denoted by Qry-Trn.QryTst); (2) using original queries for training and en-riched queries for test (denoted by QryTrn.QrySimTst); (3) using enriched queries for training and original queries for test (denoted by QrySimTrn.QryTst); (4) using enriched queries for training and test. Figures 2(a) and 2(b) shows the performance of the four combinations for  X  X omputing X  clas-sification and  X  X evel-1 X  classification respectively. In Figure 2(a) and 2(b), the vertical axis is micro-F1 while the hori-zontal axis is the percent of training data we randomly sam-pled from the two folders used for training in each run (refer to Section 4.3 for more information about splitting training and test data). As shown in the figures, the performance of all the combinations becomes better when we increase the size of training data, which becomes relatively stable when we use 70% training data or more. Another observation is that for both  X  X omputing X  and  X  X evel-1 X , we can get the order of the four combinations based on their performance as follows: QryTrn.QrySimTst &gt; QrySimTrn.QrySimTst &gt; QryTrn.QryTst &gt; QrySimTrn.QryTst. Using the t-test, we find that the difference between each pair of combina-tions is  X  X ighly significant X . We can also find that the fewer the training data, the larger the gap between these com-binations. Specifically, the relative improvement of  X  X ry-Trn.QrySimTst X  against  X  X ryTrn.QryTst X  is 3.9% and 3.5% for X  X omputing X  X nd X  X evel-1 X , respectively, when we use all the training data. These numbers become 9.2% and 5.8% when we use 10% of the training data. We draw the figures in terms of macro-F1 too and come to the same conclusions, so we did not show those figures for simplicity.

Queries enriched through similar queries have a much richer representation, which can reduce the data sparseness and in-crease the classification performance. Therefore, it is easy to understand why X  X rySimTrn.QrySimTst X  X utperforms X  X ry-Trn.QryTst X . Similarly, we can explain why X  X ryTrn.QrySim-Tst X  X s better than X  X ryTrn.QryTst X . However, why X  X rySim-Trn.QryTst X  performs worst is not straightforward. One explanation is that the classifier trained over the enriched queries lies in a larger feature space, which cannot distin-guish the queries represented by their original feature space.
Similar to Section 4.4.2, we study the effect of query en-richment through search snippets. Here, for each query, we enrich it by the top 10 returned snippets. The impact of the number of returned snippets is studied in Section 4.4.6. Figures 3(a) and 3(b) show the results for  X  X omputing X  and  X  X evel-1 X  respectively. All the notations have the same meanings as in Section 4.4.2, except that  X  X rySnpt X  refers to the enriched queries through search snippets.

Similar to the figures as shown in Section 4.4.2, we or-der the four combinations based on their performance as follows: QrySnptTrn.QrySnptTst &gt; QryTrn.QrySnptTst &gt; QryTrn.QryTst &gt; QrySnptTrn.QryTst. Using the t-test, we find that the difference between each pair of combinations (except the pair of QryTrn.QryTst and QrySnptTrn.QryTst for  X  X omputing X ) is at least  X  X ignificant X . We can see that the gaps between these combinations (except QrySnpTrn.Qry-Tst) become larger when we use fewer training data. Specifi-cally, the relative improvement of X  X rySnptTrn.QrySnptTst X  against  X  X ryTrn.QryTst X  is 6.6% and 7.5% for  X  X omput-Figure 2: Comparison between Original Queries and Query Expansion through Similar Queries ing X  X nd X  X evel-1 X , respectively, when we use all the training data. These numbers become 19.4% and 13.9% when we use 10% of the training data.

With the similar reasons as given in Section 4.4.2, we can explain why QryTrn.QrySnptTst and QrySnptTrn.QrySnptTst are better than X  X ryTrn.QryTst X  X nd why X  X rySnptTrn.QryTst X  performs worst. One obvious observation from Figure 3(b) is that  X  X rySnptTrn.QryTst X  becomes significantly worse when we increase the size of training data. This is because the feature space of snippets-enriched queries may become more different from the feature space of the original queries when we use more training data. Therefore, the classifier trained over the enriched queries are tuned to fit the en-riched feature space, which cannot accurately classify the queries represented in the original feature space. Compar-ing with the performance of  X  X rySimTrn.QryTst X  in Section 4.4.2, we may see that the web search snippet based query enrichment is more ambitious, which can cause relatively large drift from the original queries.

As indicated in Figures 2(a) and 2(a), for both  X  X omput-ing X  X nd X  X evel-1 X  X asks, X  X ryTrn.QrySimTst X  X erforms bet-ter than  X  X rySimTrn.QrySimTst X . Contradictorily , in Fig-ure 3(a) and 3(b), we find that  X  X rySnptTrn.QrySnptTst X  performs better than  X  X ryTrn.QrySnptTst X  for both  X  X om-puting X  and  X  X evel-1 X . Thus, we cannot simply conclude which classifiers ( those trained from the original queries, or those trained over enriched queries ) are better when they are used to classify enriched queries. The reason we Figure 3: Comparison between Original Queries and Query Expansion through Search Snippets mentioned above, that is, search-snippet based query en-richment method is more ambitious than the similar-query based method, can partially explain the difference. However, the deep reason is under investigation and we will leave it to our future work. But whatever the reason is, we can clearly see that properly using query enrichment can significantly improve the classification performance, especially when we do not have enough training data.
In Sections 4.4.1, 4.4.2 and 4.4.3, we have studied the ef-fect of different kinds of training data and query enrichment methods. We can see that although using pseudo queries as the training data can improve the classification performance compared to using product names, it is not as good as using automatically labeled queries, either in their original form, or in their enriched form. We also find that using the en-riched queries in a proper way can significantly improve the classification performance, especially when the size of train-ing data is small. In this section, we put all the possible combinations of training data and query enrichment meth-ods together and further compare them in detail. In order to see a clear picture of the advantages of each combination, we randomly select 50% training data to train the classifier when the training data is the original query, and their enrich-ment, either by through similar queries, or search snippets.
Tables 4 and 5 show the comparison results over  X  X om-puting X  and  X  X evel-1 X  respectively, which include the aver-age micro-F1 and macro-F1 over three folds, as well as their standard deviation values. The rows in these tables indi-cate which kind of training data is used to training classi-fiers. The columns indicate which method is used to enrich the queries for test. Besides the observations we discussed in previous sections, we can see several more interesting ones. Firstly, the classifiers trained from product names achieve much better performance when classifying enriched queries compared to the original queries. They work espe-cially well for the queries enriched through search snippets. Secondly, classifiers trained from pseudo queries outperform those trained from product names when classifying both the original queries, and those enriched by similar queries. However, their performance drops significantly when they are used to classify the queries enriched by search snippets. Thirdly, the classifiers trained over the enriched queries by similar queries can achieve better performance when they are used to classify the enriched queries through search snip-pets. However, on the other side, we see dramatic drops in performance when we use the classifiers trained over search-snippet based enriched queries to classify the queries which are enriched by similar queries. Notice that both similar-query based enrichment and pseudo queries share similar natures of the original queries, while they are all quite dif-ferent from the search-snippet based enrichment, it is easy to explain the above observations using the reasons presented in Sections 4.4.1, 4.4.2 and 4.4.3.
As discussed in Section 4.4.4,  X  X rySnptTrn.QrySnptTst X  and X  X ryTrn.QrySnptTst X  X erform better than X  X ryTrn.QryTst X  for both  X  X omputing X  and  X  X evel-1 X . However, for  X  X ryS-nptTrn.QrySnptTst X  and  X  X ryTrn.QrySnptTst X , we have to collect search snippets for all the queries as a first step for the classification, which requires lots of resources and take a long time. Therefore, we want to find a hybrid classifier, which can reduce the efforts of collecting search snippets while maintain high performance. Recall the Equation (2), given a query q , a LR classifier can calculate the probability of each possible label and then assign the label with highest probability to q . The probability can also be called as con-fidence of the classifier to make the classification decision. Intuitively, the classification accuracy is higher when the confidence is larger. However, if the classifier only classify the queries with high confidence, the number of the queries which can be classified (or called coverage as defined in Sec-tion 4.3) becomes smaller.

In this section, we first train a LR classifier using the orig-inal queries, and then use it to classify the original queries, or enriched queries by search snippets (top 10 search snip-pets are used as in the above experiments). The top four rows (corresponding to QryTst and QrySnptTst) in Tables 6 and 7 illustrate the trade-off between accuracy and coverage when we change the confidence threshold from 0.95 to 0.10. Compared to QrySnptTst, the classifier can achieve much higher accuracy while lower coverage on QryTst when the confidence threshold is higher. By decreasing the thresh-old, we can see that the accuracy on QryTst drops quickly while the drop on the QrySnptTst is relatively slow. There-fore, we should trust the classification results of using the original queries more when the confidence is high. Thus, a straightforward method to get a hybrid classifier is to find a threshold so that we output the classification results of using the original queries when the classifier X  X  confidence is larger than that threshold; otherwise, we output the classi-fication results of using the enriched queries through search snippets. With this method, it is clear that we can assign labels to all the queries, while we just need to retrieve the search snippets for some of the queries. The last two rows in Tables 6 and 7 validate the proposed hybrid classifier. We can see that by setting the threshold as 0.70, we can classify more than 70% queries directly without retrieving the search snippets and the final accuracy is even higher than adopting QrySnptTst, for which we have to collect the search snippets for all test queries.
In the above experiments, we have empirically set the val-ues of several parameters. In this section, we will study the impact of the parameters on the classification performance.
One parameter is for enriching queries through their simi-lar queries. Clearly, we have to use a similarity threshold to control what queries can be used to enrich a target queries. If the threshold is too small, we may include some non-similar queries. On the other side, if the threshold is too large, the queries we can get for a query may be too few. Both cases will result in bad classification performance. For simplicity, we only study the impact of the threshold for the  X  X rySimTrn.QrySimTst X  and  X  X ryTrn.QrySimTst X  on the  X  X evel-1 X  task where we use 50% training data. The re-sults are shown in Table 8. The first row shows the average number of queries we can get for each query given a certain threshold. It is easy to see that the larger the threshold, the smaller the average number. We can also find that when the threshold changes from 0.001 to 0.01, the performance changes slightly. After that, the performance drops quickly. In the above experiments, we set the threshold as 0.001.
Another parameter is for query enrichment through Web snippets, which decides how many returned snippets we use to enrich a query. Here, we show the impact of the threshold on X  X rySnptTrn.QrySnptTst X  X nd X  X ryTrn.QrySnptTst X  X or the X  X omputing X  X ask, where we use 50% training data. The results are shown in Table 9. From the table, we can see that the classification performance will change but not too much by varying the number of snippets. Although using 40 or 50 snippets will lead to the best performance, we use only 10 snippets in the above experiments to speed them up.
The final parameter we want to study is the threshold used to generate pseudo queries. As shown in Section 3.3.2, given a product a name, we can generate a set of pseudo queries together with the confidences. We can use a threshold to keep the queries with high confidence and remove the noise. We conduct a group of experiments for  X  X omputing X  and  X  X evel-1 X  by varying the threshold from 0.05 to 0.30. How-ever, due to the space limit, we do not show the detailed results. From the experiments, we find that the classifi-cation performance becomes better first and then worse in most cases. Therefore, in the above experiments,we choose a middle threshold, which is 0.15.
All the above experiments are conducted over the auto-matically labeled queries. Although we have shown in 4.1 that the automatically obtained labels are quite consistent with manually generated labels, we still want to see how 10 20 30 40 50 Labeler1 0.926 0.943 0.952 Labeler2 0.965 0.961 Labeler3 0.973
Predicted 0.935 0.939 0.956 0.948 the classifiers trained from the automatically labeled queries work over the queries which cannot be automatically labeled using the method given in Section 3.3.3. Therefore, we ran-domly select 2,000 queries which cannot be automatically labeled and let the three labelers label them manually with the 26 categories in the X  X evel-1 X  X ask. After that, we use the labeled queries to evaluate the hybrid classifier discussed in Section 4.4.5. Note that here, we use all the automatically labeled queries to train the hybrid classifier. The evalua-tion results are given in Table 10. Table 10 also shows the consistency among the three labelers and their consistency with the voted labels ( X  X ajority X  as defined in Section 4.1). We can see the consistency among the labelers for the 2,000 queries is a little lower than their consistency on the 6,000 queries introduced in Section 4.1. One of the reasons for the difference is that the 2,000 may be very new, or not so frequent, or more ambiguous compared to the 6,000 queries, which is also the reason why they cannot be labeled auto-matically. The performance of the classifier(as denoted by  X  X redicted X ) over the 2,000 queries is pretty good, which is even better than those shown in Table 7. One of the rea-sons is that the hybrid classifier used in this experiments is trained over more training data. Another reason is that the labels provided by the labelers are a little more accurate compared to the automatically generated labels, which will not misjudge the classifier X  X  correct predictions.
This paper presented a comprehensive study of query clas-sification by taking product query classification as an ex-ample. In this paper, we proposed to use labeled product data (corresponding to Web pages in general query classi-fication) to train classifiers and then further improve the classifier by introducing pseudo queries, which are trans-lated from labeled product names. We also put forward two query enrichment methods to better represent the queries, either through similar queries estimated from click-though log data, or through Web search snippets. After that, we systematically compared different combinations of training data and query representation methods over a large query data set and found some interesting observations. Most of these observations are not investigated before due to the limit of large scale data sets. Based on these observations, we worked out a hybrid classifier, which can achieve high classification accuracy without collecting search snippets for most of the test queries.

In the future, we will manually label more test data to fur-ther validate the hybrid classifier. We will also build some hierarchical classifiers and study the contributions of differ-ent training data and different query enrichment methods. Finally, we will investigate how the product query classifi-cation methods can improve the product search relevance.
