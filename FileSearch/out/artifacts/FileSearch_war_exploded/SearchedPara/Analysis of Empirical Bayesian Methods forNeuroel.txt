 Magnetoencephalography (MEG) and electroencephalography (EEG) use an array of sensors to take EM field measurements from on or near the scalp surface with excellent temporal resolution. In both cases, the observed field is generated by the same synchronous, compact current sources located within the brain. Because the mapping from source activity configuration to sensor measurement is many to one, accurately determining the spatial locations of these unknown sources is extremely difficult. The relevant localization problem can be posed as follows: The measured EM signal is d obtained by segmenting a structural MR scan of a human subject and tesselating the gray matter surface with a set of vertices. B and S are related by the generative model where L is the so-called lead-field matrix, the i -th column of which represents the signal vector each directional component). Multiple methods based on the physical properties of the brain and Maxwell X  X  equations are available for this computation. Finally, E is a noise term with columns drawn independently from N (0 ,  X  ) .
 much larger than the number of sensors ( d reconstruction are heavily dependent on prior assumptions, which in a Bayesian framework are em-case of minimum ` and sLORETA [10]. Alternatively, a number of empirical Bayesian approaches have been proposed Examples include variational Bayesian methods [14, 15], hierarchial covariance component models [4, 8, 11], and automatic relevance determination (ARD) [7, 9, 12, 13, 17]. While seemingly quite different in some respects, we present a generalized framework that encompasses many of these methods and points to connections between algorithms. We also analyze several theoretical proper-ties of this framework related to computational/convergence issues, local minima, and localization bias. Overall, we envision that by providing a unifying perspective on these approaches, neuroelec-In this section, we present a general-purpose Bayesian framework for source localization. In doing so, we focus on the common ground between many of the methods discussed above. While de-rived using different assumptions and methodology, they can be related via the notion of automatic relevance determination [9] and evidence maximization [7].
 While the unknown noise covariance can also be parameterized and estimated from the data, for simplicity we assume that  X  is known and fixed. Next we adopt the following source prior for S : [  X  of each covariance basis matrix C giving where  X  is then maximized with respect to the unknown hyperparameters, a process referred to as type-II maximum likelihood or evidence maximization [7, 9] or restricted maximum likelihood [4]. Thus the optimization problem shifts from finding the maximum a posteriori sources given a fixed prior to finding the optimal hyperparameters of a parameterized prior. Once these estimates are obtained exists in closed form, where  X   X  unknown sources can be obtained by evaluating the posterior mean The specific choice of the C component model per se, that primarily differentiates the many different empirical Bayesian ap-proaches and points to novel algorithms for future study. The optimization strategy adopted for factors.
 In the simplest case, use of the single component  X  ` -norm solution. More interesting covariance component terms have been used to effect spatial to the latter, it has been suggested that prior information about a source location can be codified by including a C of probable source activity, perhaps based on fMRI data [11]. An associated hyperparameter  X  is then estimated to determine the appropriate contribution of this component to the overall prior where activity is occurring with both high spatial and temporal resolution. Therefore, we cannot reliably known how to choose an appropriate location-prior term in many situations. The empirical Bayesian solution to this dilemma, which amounts to a form of model selection, is to try out many different (or even all possible) combinations of location priors, and determine which one has the highest Bayesian evidence, i.e., maximizes p ( B ;  X  lead-field grid, then we may choose  X  of zeros with a  X 1 X  for the i -th element (and so C of an unknown hyperparameter for every candidate source location which, on casual analysis may priors). However, the process of marginalization, or the integrating out of the unknown sources S , provides an extremely powerful regularizing effect, driving most of the unknown  X  problem and effectively reduces the space of possible active source locations by choosing a small mean providing a good estimate of source activity. Such a procedure has been empirically successful In contrast, to model sources with some spatial extent, we can choose C  X  location and activity extent [13]. In this scenario, the number of hyperparameters satisfies d where v is the number of scales we wish to examine in a multi-resolution decomposition, and can be quite large ( d mixture components, or the number of nonzero  X  ARD, that marginalization and subsequent evidence maximization leads to a pruning of unsupported hypotheses, remains unchanged.
 computing the desired source posterior. This motivates the inclusion of a variational approximation resulting cost function is exactly equivalent to standard ARD assuming  X  and so d hyperprior on  X  , as is also commonly done with ARD methods [1]. Optimization is then performed using simple EM update rules.
 of [15] are all identical with respect to their ARD-based cost functions; they differ only in which covariance components (and possibly hyperpriors) are used and in how optimization is performed as will be discussed below. In contrast, the variational model from [14] introduces an additional hierarchy to the ARD framework to explicitly model temporal correlations between sources which may be spatially separated. 2 Here it is assumed that S can be decomposed with respect to d sources via direct application of ARD would involve integration over W and Z to find the hyperparameters  X  that maximize p ( B ;  X  to explore the characteristics of this method were we able to perform the necessary computation. This allows us to relate the full model of [14] to standard ARD.
 standard ARD prior (2) are equivalent (up to a constant factor), although higher-order moments will be different. However, as the number of pre-sources d limit-theorem arguments can be used to explicitly show that the distribution of S converges to an objective were it feasible, approaches regular ARD when the number of pre-sources grows large. In approximate posterior factorization for handling the decomposition (6). This produces approximate posteriors on W and Z but the result cannot be integrated to form the posterior on S . However, the substantially improve beamforming results that were errantly based on uncorrelated source models. Note however that this procedure implicitly uses the somewhat peculiar criteria of combining the posterior mean of W with the prior on Z to form an estimate of the distribution of S . 2.1 Computational Issues The primary objective of ARD is to maximize the evidence p ( B ;  X  lently, to minimize In [4], a restricted maximum likelihood (ReML) approach is proposed for this optimization, which utilizes what amounts to EM-based updates. This method typically requires a nonlinear search for each M-step and does not guarantee that the estimated covariance is positive definite. While shown problematic when very large numbers of hyperparameters are present. For example, in several toy problems (with d negative-valued, inconsistent with our initial premise.
 12, 15, 17] to the arbitrary covariance model discussed above and guarantee that  X  Because of the flexibility this allows in constructing  X  is required to proceed. A new decomposition of  X  where trace operator, L (  X  ) only depends on the data B through the d Therefore, to reduce the computational burden, we replace B with a matrix that without altering that actual cost function.
 form of  X  different effective computation for the M-step. By casting the update rules in this way and noting most O d 2 where different pseudo lead-field components, e.g., some in common. This situation occurs if we desire to use the geodesic basis functions with flexible dependence on d large numbers of hyperparameters and covariance components.
 The problem then with (9) is not the per-iteration complexity but the convergence rate, which we and large numbers of hyperparameters. The only reported localization results using this type of EM algorithm are from [15], where a relatively low resolution lead-field matrix is used in conjunction with a simplifying heuristic that constrains some of the hyperparameter values. However, to avoid these types of constraints, which can potentially degrade the quality of source estimates, a faster update rule is needed. To this end, we modified the procedure of [7], which involves taking the gradient of L (  X  ) with respect to  X  , rearranging terms, and forming the fixed-point update
The complexity of each iteration is the same as before, only now the convergence rate can be orders of magnitude faster. For example, given d a pseudo lead-field with 120,000 unique columns and an equal number of hyperparameters, requires approximately 5-10 mins. runtime using Matlab code on a PC to completely converge. The EM update does not converge after 24 hours. Example localization results using (10) demonstrate the ability to recover very complex source configurations with variable spatial extent [13]. descent function, although we have never observed it to increase (7) in practice. While we can show provable convergence is still suspect. However, a similar update rule can be derived that is both cost function, this update is given by Details of the derivation can be found in [20].
 Finally, the correlated source method from [14] can be incorporated into the general ARD framework by this method, the iterations now scale as ( P can be prohibitive in applications with large numbers of covariance components. 2.2 Relationship with Other Bayesian Methods As a point of comparison, we now describe how ARD can be related to alternative Bayesian-inspired algorithm [5]. The connection is most transparent when we substitute the prior covariance  X  P  X  i =  X  where  X  , diag[  X  ] , ` hyperparameters produced after a single iteration of ARD are equivalent to computing the sLORETA [10, 12]. So ARD can be interpreted as a recursive refinement of what amounts to the non-adaptive, linear sLORETA estimate.
 nearly the same as the FOCUSS iterations modified to simultaneously handle multiple observation this can be offset by an appropriate rescaling of the FOCUSS  X  trade-off parameter (analogous to  X  ). Therefore, ARD can be viewed in some sense as taking the recursive FOCUSS update rules and including the sLORETA normalization that, among other things, allows for depth bias compensation. Thus far, we have focused on similarities in update rules between the ARD formulation (restricted to the case where  X  general ARD cost function relates to that of FOCUSS and MCE and suggests a useful generalization of both approaches. Recall that the evidence maximization procedure upon which ARD is based involves integrating out the unknown sources before optimizing the hyperparameters  X  . However, if out the hyperparameters and then maximize S directly, thus solving the MAP estimation problem max where each A log x and (13) becomes a generalized form of the FOCUSS cost function (and reduces to the exact FOCUSS cost when A and we obtain a generalized version of MCE. In both cases, multiple simultaneous constraints (e.g., MAP methods. Additionally, as with ARD, source components that are not sufficiently important in representing the observed data are pruned; however, the undesirable discontinuities in standard FOCUSS or MCE source estimates across time, which previously have required smoothing using heuristic measures [6], do not occur when using (13). This is because sparsity is only encouraged between components due to the concavity of g (  X  ) , but not within components where the Frobenius norm operator promotes smooth solutions [2]. All of these issues, as well as efficient ARD-like update rules for optimizing (13), are discussed in [20]. ARD methods maintain several attributes that make them desirable candidates for source localiza-tion. For example, unlike most MAP procedures, the ARD cost function is often invariant to lead-the selection of the C every normalization scheme. As such, ARD is considerably more robust to the particular heuristic used for this task and can readily handle deep current sources.
 Previously, we have claimed that the ARD process naturally forces excessive/irrelevant hyperpa-rameters to converge to zero, thereby reducing model complexity. While this observation has been multimodal, non-convex ARD cost function. As such, we provide the following result: Result 1. Every local minimum of the generalized ARD cost function (7) is achieved at a solution with at most rank( B ) d practice, for any reasonable value of  X  , the number of nonzero hyperparameters is typically much smaller than d hyperparameter pruning, and therefore covariance component pruning, is built into the ARD frame-work irrespective of the noise-based regularization. Moreover, the number of nonzero hyperparam-eters decreases monotonically to zero as  X  is increased. And so there is always some  X  =  X  0 sonable confident that the pruning mechanism of ARD is not merely an empirical phenomena. Nor is it dependent on a particular sparse hyperprior, since the ARD cost from (7) implicitly assumes a flat (uniform) hyperprior.
 The number of observation vectors n also plays an important role in shaping ARD solutions. In-to getting stuck in a suboptimal extrema) and (ii), it improves the quality of this minimum by mit-estimation problem reduces to an equivalent problem with n = 1 , so the local minima profile of the cost function does not improve with increasing n . Of course standard ARD can still be very effec-tive in this scenario [13]. In contrast, geometric arguments can be made to show that uncorrelated sources with large n offer the best opportunity for local minima avoidance. However, when strong to model correlations) could offer a worthwhile alternative, albeit at a high computational cost. Further theoretical support for ARD is possible in the context of localization bias assuming simple source configurations. For example, substantial import has been devoted to quantifying localization bias when estimating a single dipolar source. Recently it has been shown, both empirically [10] Viewed then as an iterative enhancement of sLORETA as described in Section 2.2, the question naturally arises whether ARD methods retain this desirable property. In fact, it can be shown that column. Unbiasedness can also be shown in the continuous case for both sLORETA and ARD, but the discrete scenario is more straightforward and of course more relevant to any practical task. Result 2. Assume that  X  e e T i . Then in the absence of noise (high SNR), ARD has provably zero localization bias when estimating a single dipolar source, regardless of the value of n .
 uncorrelated (orthogonal) across time and assuming some mild technical conditions [20]. This re-sult also formalizes the notion, mentioned above, that ARD performs best with uncorrelated sources. Turning to the more realistic scenario where noise is present gives the following: Result 3. Let  X  For most reasonable lead-fields and covariance components, this global minimum will be unique, covariance component estimates. While details have been deferred to [20], the basic idea is that if the outerproduct BB T can be expressed as some non-negative linear combination of the avail-able covariance components, then the ARD cost function is unimodal and  X  minimizing solution. This  X  While theoretical results of this kind are admittedly limited, other iterative Bayesian schemes in fact fail to exhibit similar performance. For example, all of the MAP-based focal algorithms we are aware of, including FOCUSS and MCE methods, provably maintain a localization bias in the general result.) When we move to more complex source configurations with possible correlations and noise, For example, given a 275  X  40 , 000 lead-field matrix constructed from an MR scan and assuming fixed orientation constraints and a spherical head model, ARD using  X  bias when estimating up to 15-20 dipoles, while sLORETA starts to show a bias with only a few. The efficacy of modern empirical Bayesian techniques and variational approximations make them ods relate nor which should be expected to perform best in various situations. By developing a general framework around the notion of ARD, deriving several theoretical properties, and showing connections between algorithms, we hope to bring an insightful perspective to these techniques. [1] C. M. Bishop and M. E. Tipping,  X  X ariational relevance vector machines, X  Proc. 16th Conf. [2] S.F. Cotter, B.D. Rao, K. Engan, and K. Kreutz-Delgado,  X  X parse solutions to linear inverse [3] M.A.T. Figueiredo,  X  X daptive sparseness using Jeffreys prior, X  Advances in Neural Information [5] I.F. Gorodnitsky, J.S. George, and B.D. Rao,  X  X euromagnetic source imaging with FOCUSS: [6] M. Huang, A. Dale, T. Song, E. Halgren, D. Harrington, I. Podgorny, J. Canive, S. Lewis, and [7] D.J.C. MacKay,  X  X ayesian interpolation, X  Neural Computation , vol. 4, no. 3, 1992. [8] J. Mattout, C. Phillips, W.D. Penny, M.D. Rugg, and K.J. Friston,  X  X EG source localization [9] R.M. Neal, Bayesian Learning for Neural Networks , Springer-Verlag, New York, 1996. [10] R.D. Pascual-Marqui,  X  X tandardized low resolution brain electromagnetic tomography [12] R.R. Ram  X   X rez, Neuromagnetic Source Imaging of Spontaneous and Evoked Human Brain [13] R.R. Ram  X   X rez and S. Makeig,  X  X euroelectromagnetic source imaging using multiscale [14] M. Sahani and S.S. Nagarajan,  X  X econstructing MEG sources with unknown correlations, X  [15] M. Sato, T. Yoshioka, S. Kajihara, K. Toyama, N. Goda, K. Doya, and M. Kawato,  X  X ierar-[19] D.P. Wipf and B.D. Rao,  X  X parse Bayesian learning for basis selection, X  IEEE Trans. Sig. [20] D.P. Wipf and R.R. Ram  X   X rez and J.A. Palmer and S. Makeig and B.D. Rao, Automatic Rel-
