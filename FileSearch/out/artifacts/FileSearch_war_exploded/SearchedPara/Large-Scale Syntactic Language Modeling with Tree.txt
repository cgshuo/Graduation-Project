 N -gram language models are a central component of all speech recognition and machine translation systems, and a great deal of research centers around refining models (Chen and Goodman, 1998), ef-ficient storage (Pauls and Klein, 2011; Heafield, 2011), and integration into decoders (Koehn, 2004; Chiang, 2005). At the same time, because n -gram language models only condition on a local window of linear word-level context, they are poor models of long-range syntactic dependencies. Although sev-eral lines of work have proposed generative syntac-tic language models that improve on n -gram mod-els for moderate amounts of data (Chelba, 1997; Xu et al., 2002; Charniak, 2001; Hall, 2004; Roark, 2004), these models have only recently been scaled to the impressive amounts of data routinely used by n -gram language models (Tan et al., 2011).

In this paper, we describe a generative, syntac-tic language model that conditions on local con-text treelets 1 in a parse tree, backing off to smaller treelets as necessary. Our model can be trained sim-ply by collecting counts and using the same smooth-ing techniques normally applied to n -gram mod-els (Kneser and Ney, 1995), enabling us to apply techniques developed for scaling n -gram models out of the box (Brants et al., 2007; Pauls and Klein, 2011). The simplicity of our training procedure al-lows us to train a model on a billion tokens of data in a matter of hours on a single machine, which com-pares favorably to the more involved training algo-rithm of Tan et al. (2011), who use a two-pass EM training algorithm that takes several days on several hundred CPUs using similar amounts of data.

The simplicity of our approach also contrasts with recent work on language modeling with tree sub-stitution grammars (Post and Gildea, 2009), where larger treelet contexts are incorporated by using so-phisticated priors to learn a segmentation of parse trees. Such an approach implicitly assumes that a  X  X orrect X  segmentation exists, but it is not clear that this is true in practice. Instead, we build upon the success of n -gram language models, which do not assume a segmentation and instead score all over-lapping contexts.

We evaluate our model in terms of perplexity, and show that we achieve the same performance as a state-of-the-art n -gram model. We also evaluate our model on several grammaticality tasks proposed in rule context would need its own state in the gram-mar), and extensive pruning would be in order.
In practice, however, language models are nor-mally integrated into a decoder, a non-trivial task that is highly problem-dependent and beyond the scope of this paper. However, we note that for machine translation, a model that builds target-side constituency parses, such as that of Galley et al. (2006), combined with an efficient pruning strategy like cube pruning (Chiang, 2005), should be able to integrate our model without much difficulty.
That said, for evaluation purposes, whenever we need to query our model, we use the simple strategy of parsing a sentence using a black box parser, and summing over our model X  X  probabilities of the 1000-best parses. 4 Note that the bottleneck in this case is the parser, so our model can essentially score a sentence at the speed of a parser. We evaluate our model along several dimensions. We first show some sample sentences generated by our model in Section 5.1. We report perplexity re-sults in Section 5.2. In Section 5.3, we measure its ability to distinguish between grammatical En-glish and various types of automatically generated, or pseudo-negative , 5 English. We report machine translation reranking results in Section 5.4. 5.1 Generating Samples Because our model is generative, we can qualita-tively assess it by generating samples and verifying that they are more syntactically coherent than other approaches. In Table 1, we show the first four sam-ples of length between 15 and 20 generated from both model and a 5-gram model trained on the Penn Treebank. 5.2 Perplexity Perplexity is the standard intrinsic evaluation metric for language models. It measures the inverse of the per-word probability a model assigns to some held-out set of grammatical English (so lower is better). For training data, we constructed a large treebank by concatenating the Penn Treebank, the Brown Cor-pus, the 50K BLLIP training sentences from Post (2011), and the AFP and APW portions of English the literature (Okanohara and Tsujii, 2007; Foster et al., 2008; Cherry and Quirk, 2008) and show that it consistently outperforms an n -gram model as well as other head-driven and tree-driven generative base-lines. Our model even competes with state-of-the-art discriminative classifiers specifically designed for each task, despite being estimated on positive data alone. We also show fluency improvements in a pre-liminary machine translation reranking experiment. The common denominator of most n -gram language models is that they assign probabilities roughly ac-cording to empirical frequencies for observed n -grams, but fall back to distributions conditioned on smaller contexts for unobserved n -grams, as shown in Figure 1(a). This type of smoothing is both highly robust and easy to implement, requiring only the col-lection of counts from data.

We would like to apply the same smoothing tech-niques to distributions over rule yields in a con-stituency tree, conditioned on contexts consisting of previously generated treelets (rules, nodes, etc.). Formally, let T be a constituency tree consisting of context-free rules of the form r = P  X  C where P is the parent symbol of rule r and C d C bilities to trees 2 where the conditioning context h is some portion of the already-generated parts of the tree. In this paper, we assume that the children of a rule are expanded from left to right, so that when generating the yield C available. Note that a raw PCFG would condition only on P , i.e. h = P .

As in the n -gram case, we would like to pick h to be large enough to capture relevant dependencies, but small enough that we can obtain meaningful es-timates from data. We start with a straightforward choice of context: we condition on P , as well as the rule r 0 that generated P , as shown in in Figure 1(b).
Conditioning on the parent rule r 0 allows us to capture several important dependencies. First, it captures both P and its parent P 0 , which predicts the distribution over child symbols far better than just P (Johnson, 1998). Second, it captures posi-tional effects. For example, subject and object noun phrases (NPs) have different distributions (Klein and Manning, 2003), and the position of an NP relative to a verb is a good indicator of this distinction. Fi-nally, the generation of words at preterminals can condition on siblings, allowing the model to capture, for example, verb subcategorization frames.

We should be clear that we are not the first to use back-off-based smoothing for syntactic lan-guage modeling  X  such techniques have been ap-plied to models that condition on head-word con-texts (Charniak, 2001; Roark, 2004; Zhang, 2009). Parent rule context has also been employed in trans-lation (Vaswani et al., 2011). However, to our knowledge, we are the first to apply these techniques for language modeling on large amounts of data. 2.1 Lexical context Although it is tempting to think that we can replace the left-to-right generation of n -gram models with the purely top-down generation of typical PCFGs, in practice, words are often highly predictive of the words that follow them  X  indeed, n -gram models would be terrible language models if this were not the case. To capture linear effects, we extend the context for terminal (lexical) productions to include the previous two words w  X  2 and w  X  1 in the sentence allows us to capture collocations and other lexical correlations. 2.2 Backing off As with n -gram models, counts for rule yields con-propriate back-off strategy. We handle terminal and non-terminal productions slightly differently.
For non-terminal productions, we back off from r 0 to P and its parent P 0 , and then to just P . That is, we back off from a rule-annotated gram-mar p ( C d mar (Johnson, 1998) p ( C d PCFG p ( C d rule yields C d sic PCFG probability p ( C d 4-gram model over symbols C conditioned on P , interpolated with an unconditional 4-gram model PCFG to where  X  = 0 . 9 is an interpolation constant.
For terminal (i.e lexical) productions, we first remove lexical context, backing off from p ( w | P,R,r 0 ,w  X  1 ,w  X  2 ) to p ( w | P,R,r 0 ,w  X  1 then p ( w | P,R,r 0 ) . From there, we back off to p ( w | P,R ) where R is the sibling immediately to the right of P , then to a raw PCFG p ( w | P ) , and finally to a unigram distribution. We chose this scheme be-cause p ( w | P,R ) allows, for example, a verb to be generated conditioned on the non-terminal category of the argument it takes (since arguments usually im-mediately follow verbs). We depict these two back-off schemes pictorially in Figure 1(b) and (c). 2.3 Estimation Estimating the probabilities in our model can be done very simply using the same techniques (in fact, the same code) used to estimate n -gram language models. Our model requires estimates of four distri-butions: p ( C d we require empirical counts of treelet tuples in the same way that we require counts of word tuples for estimating n -gram language models.

There is one additional hurdle in the estimation of our model: while there exist corpora with human-annotated constituency parses like the Penn Tree-bank (Marcus et al., 1993), these corpora are quite small  X  on the order of millions of tokens  X  and we cannot gather nearly as many counts as we can for n -grams, for which billions or even trillions (Brants et al., 2007) of tokens are available on the Web. How-ever, we can use one of several high-quality con-stituency parsers (Collins, 1997; Charniak, 2000; Petrov et al., 2006) to automatically generate parses. These parses may contain errors, but not all parsing errors are problematic for our model, since we only care about the sentences generated by our model and not the parses themselves. We show in our experi-ments that the addition of data with automatic parses does improve the performance of our language mod-els across a range of tasks. In the previous section, we described how to condi-tion on rich parse context to better capture the dis-tribution of English trees. While such context al-lows our model to capture many interesting depen-dencies, several important dependencies require ad-ditional attention. In this section, we describe a number of transformations of Treebank constituency parses that allow us to capture such dependencies. We list the annotations and deletions in the order in which they are performed. A sample transformed tree is shown in Figure 2.
 Computing the probability of a sentence w ` our model requires summing over all possible parses of w ` straightforward PCFG, allowing O ( ` 3 ) computation of this sum, the grammar constant for this PCFG would be unmanageably large (since every parent rule context would need its own state in the gram-mar), and extensive pruning would be in order.
In practice, however, language models are nor-mally integrated into a decoder, a non-trivial task that is highly problem-dependent and beyond the scope of this paper. For machine translation, a model that builds target-side constituency parses, such as that of Galley et al. (2006), combined with an ef-ficient pruning strategy like cube pruning (Chiang, 2005), should be able to integrate our model without much difficulty.

That said, for evaluation purposes, whenever we need to query our model, we use the simple strategy of parsing a sentence using a black box parser, and summing over our model X  X  probabilities of the 1000-best parses. 4 Note that the bottleneck in this case is the parser, so our model can essentially score a sentence at the speed of a parser. We evaluate our model along several dimensions. We first show some sample generated sentences in Section 5.1. We report perplexity results in Sec-tion 5.2. In Section 5.3, we measure its ability to distinguish between grammatical English and var-ious types of automatically generated, or pseudo-negative , 5 English. We report machine translation reranking results in Section 5.4. 5.1 Generating Samples Because our model is generative, we can qualita-tively assess it by generating samples and verifying that they are more syntactically coherent than other approaches. In Table 1, we show the first four sam-ples of length between 15 and 20 generated from our model and a 5-gram model trained on the Penn Tree-bank. 5.2 Perplexity Perplexity is the standard intrinsic evaluation metric for language models. It measures the inverse of the per-word probability a model assigns to some held-out set of grammatical English (so lower is better). For training data, we constructed a large treebank by concatenating the WSJ and Brown portions of the Penn Treebank, the 50K BLLIP training sentences from Post (2011), and the AFP and APW portions of English Gigaword version 3 (Graff, 2003), total-ing about 1.3 billion tokens. We used the human-annotated parses for the sentences in the Penn Tree-bank, but parsed the Gigaword and BLLIP sentences with the Berkeley Parser. Hereafter, we refer to this training data as our 1B corpus. We used Section 0 of the WSJ as our test corpus. Results are shown in Table 2. In addition to our T REELET model, we also show results for the following baselines:
We used the Berkeley LM toolkit (Pauls and Klein, 2011), which implements Kneser-Ney smoothing, to estimate all back-off models for both n -gram and treelet models. To deal with unknown words, we use the following strategy: after the first 10000 sentences, whenever we see a new word in our training data, we replace it with a signature 10% of the time.

Our model outperforms all other generative mod-els, though the improvement over the n -gram model is not statistically significant. Note that because we use a k -best approximation for the sum over trees, all perplexities (except for P CFG -LA and 5-GRAM ) are pessimistic bounds. 5.3 Classification of Pseudo-Negative Sentences We make use of three kinds of automatically gener-ated pseudo-negative sentences previously proposed in the literature: Okanohara and Tsujii (2007) pro-posed generating pseudo-negative examples from a trigram language model; Foster et al. (2008) create  X  X oisy X  sentences by automatically inserting a sin-gle error into grammatical sentences with a script that randomly deletes, inserts, or misspells a word; and Och et al. (2004) and Cherry and Quirk (2008) both use the 1-best output of a machine translation system. Examples of these three types of pseudo-negative data are shown in Table 3. We evaluate our model X  X  ability to distinguish positive from pseudo-negative data, and compare against generative base-lines and state-of-the-art discriminative methods.
We would like to use our model to make grammat-icality judgements, but as a generative model it can only provide us with probabilities. Simply thresh-olding generative probabilities, even with a separate threshold for each length, has been shown to be very ineffective for grammaticality judgements, both for n -gram and syntactic language models (Cherry and Quirk, 2008; Post, 2011). We used a simple measure for isolating the syntactic likelihood of a sentence: we take the log-probability under our model and subtract the log-probability under a unigram model, then normalize by the length of the sentence. 8 This measure, which we call the syntactic log-odds ratio (SLR), is a crude way of  X  X ubtracting out X  the se-mantic component of the generative probability, so that sentences that use rare words are not penalized for doing so. 5.3.1 Trigram Classification
To facilitate comparison with previous work, we used the same negative corpora as Post (2011) for trigram classification. They randomly selected 50K train, 3K development, and 3K positive test sen-tences from the BLLIP corpus, then trained a tri-gram model on 450K BLLIP sentences and gener-ated 50K train, 3K development, and 3K negative sentences. We parsed the 50K positive training ex-amples of Post (2011) with the Berkeley Parser and used the resulting treebank to train a treelet language model. We set an SLR threshold for each model on the 6K positive and negative development sentences.
Results are shown in Table 4. In addition to our generative baselines, we show results for the dis-criminative models reported in Cherry and Quirk (2008) and Post (2011). The former train a latent PCFG support vector machine for binary classifica-tion (L SVM ). The latter report results for two bi-nary classifiers: R ERANK uses the reranking fea-tures of Charniak and Johnson (2005), and T SG uses indicator features extracted from a tree substitution grammar derivation of each sentence.

Our T REELET model performs nearly as well as the T SG method, and substantially outperforms the L
SVM method, though the latter was not tested on the same random split. Interestingly, the T REELET -R
ULE baseline, which removes lexical context from our model, outperforms the full model. This is likely because the negative data is largely coherent at the trigram level (because it was generated from a tri-gram model), and the full model is much more sen-sitive to trigram coherence than the T REELET -R ULE model. This also explains the poor performance of the 5-GRAM model.

We emphasize that the discriminative baselines are specifically trained to separate trigram text from natural English, while our model is trained on pos-itive examples alone. Indeed, the methods in Post (2011) are simple binary classifiers, and it is not clear that these models would be properly calibrated for any other task, such as integration in a decoder.
One of the design goals of our system was that it be scalable. Unlike some of the discriminative baselines, which require expensive operations 9 on each training sentence, we can very easily scale our model to much larger amounts of data. In Ta-ble 4, we also show the performance of the gener-ative models trained on our 1B corpus. All gener-ative models improve, but T REELET -R ULE remains the best, now outperforming the R ERANK system, though of course it is likely that R ERANK would im-prove if it could be scaled up to more training data. 5.3.2  X  X oisy X  Classification
We also evaluate the performance of our model on the task of distinguishing the noisy WSJ sen-tences of Foster et al. (2008) from their original versions. We use the noisy versions of Section 0 and 23 produced by their error-generating proce-dure. Because they only report classification re-sults on Section 0, we used Section 23 to tune an SLR threshold, and tested our model on Section 0. We show the results of both independent and pair-wise classification for the WSJ and 1B training sets in Table 5. Note that independent classification is much more difficult than for the trigram data, be-cause sentences contain at most one change, which may not even result in an ungrammaticality. Again, our model outperforms the n -gram model for both types of classification, and achieves the same per-formance as the discriminative system of Foster et al. (2008), which is state-of-the-art for this data set. The T REELET -R ULE system again slightly outper-forms the full T REELET model at independent clas-sification, though not at pairwise classification. This probably reflects the fact that semantic coherence can still influence the SLR score, despite our efforts to subtract it out. Because the T REELET model in-cludes lexical context, it is more sensitive to seman-tic coherence and thus more likely to misclassify semantically coherent but ungrammatical sentences. For pairwise comparisons, where semantic coher-ence is effectively held constant, such sentences are not problematic. 5.3.3 Machine Translation Classification
We follow Och et al. (2004) and Cherry and Quirk (2008) in evaluating our language models on their ability to distinguish the 1-best output of a machine translation system from a reference translation in a pairwise fashion. Unfortunately, we do not have access to the data used in those papers, so a di-rect comparison is not possible. Instead, we col-lected the English output of Moses (Hoang et al., 2007), using both French and German as source lan-guage, trained on the Europarl corpus used by WMT 2009. 10 We also collected the output of Joshua (Li et al., 2009) trained on 500K sentences of GALE Chinese-English parallel newswire. We trained both our T REELET model and a 5-GRAM model on the union of our 1B corpus and the English sides of our parallel corpora.

In Table 6, we show the pairwise comparison ac-curacy (using SLR) on these three corpora. We see that our system prefers the reference much more of-ten than the 5-GRAM language model. 11 However, we also note that the easiness of the task is corre-lated with the quality of translations (as measured in BLEU score). This is not surprising  X  high-quality translations are often grammatical and even a per-fect language model might not be able to differenti-ate such translations from their references. 5.4 Machine Translation Fluency We also carried out reranking experiments on 1000-best lists from Moses using our syntactic language model as a feature. We did not find that the use of our syntactic language model made any statis-tically significant increases in BLEU score. How-ever, we noticed in general that the translations fa-vored by our model were more fluent, a useful im-provement to which BLEU is often insensitive. To confirm this, we carried out an Amazon Mechan-ical Turk experiment where users from the United States were asked to compare translations using our T
REELET language model as the language model feature to those using the 5-GRAM model. 12 We had 1000 such translation pairs rated by 4 separate Turk-ers each. Although these two hypothesis sets had the same BLEU score (up to statistical significance), the Turkers preferred the output obtained using our syntactic language model 59% of the time, indicat-ing that our model had managed to pick out more fluent hypotheses that nonetheless were of the same BLEU score. This result was statistically significant with p &lt; 0 . 001 using bootstrap resampling. We have presented a simple syntactic language model that can be estimated using standard n -gram smoothing techniques on large amounts of data. Our model outperforms generative baselines on several evaluation metrics and achieves the same perfor-mance as state-of-the-art discriminative classifiers specifically trained on several types of negative data.
