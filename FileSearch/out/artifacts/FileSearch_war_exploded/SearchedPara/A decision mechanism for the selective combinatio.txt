 Vassilis Plachouras  X  Fidel Cacheda  X  Iadh Ounis
Abstract The combination of evidence can increase retrieval effectiveness. In this paper, we investigate the effectiveness of a decision mechanism for the selective combination of evidence for Web Information Retrieval and particularly for topic distillation. We introduce two measures of a query X  X  broadness and use them to select an appropriate combination of evidence for each query. The results from our experiments show that there is a statistically significant association between the output of the decision mechanism and the relative effec-tiveness of the different combinations of evidence. Moreover, we show that the proposed methodology can be applied in an operational setting, where relevance information is not available, by setting the decision mechanism X  X  thresholds automatically.
 Keywords Web information retrieval . Topic distillation .
 combination of evidence . Query scope . Aggregates 1. Introduction
It has been recognised that the combination of different sources of evidence can improve the effectiveness of Information Retrieval (IR) systems (Croft, 2000). In the context of Web IR, and more specifically topic distillation, in addition to the textual content of documents, there is another source of evidence, namely the hyperlink structure, which can be employed to re-fine the content-based retrieval and increase the precision among the top ranked documents.
However, the weaker evidence provided from the hyperlink structure and the different types of queries, i.e. specific or broad queries (Kleinberg 1998), or navigational versus informational queries (Broder, 2002), suggest that combining content and hyperlink analysis in a uniform way, independently from the queries, does not necessarily lead to optimal retrieval effective-ness (Plachouras et al. 2003; Plachouras and Ounis 2004).

We propose a decision mechanism for the selective combination of evidence. This decision mechanism is based on the query scope , which estimates two important statistical aspects of the set of retrieved documents. The first aspect, query extent , estimates how broad a query is, by counting the number of documents that contain all the query terms. Query topics that correspond to high query extent are well covered in the documents collection. Therefore, more evidence from hyperlink analysis can be used to detect documents of high quality.
The second aspect, result extent , counts the number of domains or directories that contain a high number of retrieved documents. This aspect of the set of retrieved documents indicates whether there are whole or large parts of sites about the query topic. If this is the case, then we can employ evidence from the hyperlink or the document structure and retrieve the entry points of these sites.

The decision mechanism uses the query scope aspects to select an appropriate combination of evidence for each query. For the evaluation of the decision mechanism, we experiment with the .GOV TREC Web test collection and the associated topics from the topic distillation tasks of TREC11 (Craswell and Hawking. 2002) and TREC12 (Craswell et al., 2003), respectively. The evaluation of the decision mechanism takes place in two steps.
 and result extent independently, by testing a range of different threshold values. We show that improvements are obtained over the uniform combination of evidence for all queries.
Moreover, we find that there is a statistically significant association between the values of the query scope aspects and the relative effectiveness of the combinations of evidence. For example, we show that content retrieval is more effective for the queries that correspond to a low query or result extent values. For the second step of the evaluation, we consider a more realistic setting, where relevance information is not readily available for tuning the thresholds of the decision mechanism. We sample terms from the document collection and use them as single-term queries, an approach similar to that proposed by Cronen-Townsend et al. (2002). After computing the values of query extent and result extent for the engineered queries, we set the thresholds of the decision mechanism, so that a given percent of each aspect X  X  values are below the corresponding threshold. The results show that we can still obtain improvements in retrieval effectiveness by setting the thresholds automatically.
The remainder of the paper is organised as follows. In Section 2, we look at the potential uniform combination of evidence. Section 3 contains the description of the proposed decision mechanism for selecting the most appropriate combination of evidence for each query. In
Sections 4 and 5, we evaluate the decision mechanism in different settings, using relevance information. Next, in Section 6, we set the decision mechanism X  X  thresholds automatically, without relevance information. We continue in Section 7 with a discussion of interesting points from the experiments. Section 8 contains a brief overview of the related work, and we close with concluding remarks in Section 9. 2. Combination of evidence for topic distillation of evidence for topic distillation. We evaluate different uniform combinations of evidence, where the same sources of evidence are used for all queries. Following, we assume that there exists a decision mechanism, which selects the most appropriate combination of evidence for each query, and we show that there is room for improvement in retrieval effectiveness over the uniform combination of evidence.
 We experiment with the TREC. GOV collection, a crawl of approximately 1.25 million
Web documents from the .gov domain. For indexing the collection, we removed stop-words and applied Porter X  X  stemming algorithm. We use the topics information from the topic distillation tasks of TREC11 (Craswell and Hawking, 2002) (49 topics 2 ) and TREC12 (Craswell et al., 2003) (50 topics).

Both used tasks involve finding useful entry points to sites that are relevant to the query topics. However, a difference between the two tasks is that the relevant documents for the
TREC12 topics were restricted to be homepages of relevant sites. This resulted in a lower number of relevant documents, less than 10 relevant documents for many topics. Thus, it would not be theoretically possible to obtain 100% precision at 10 documents, which was the evaluation measure for TREC11. For this reason, the TREC Web track organisers chose the R-Precision (precision after R documents have been retrieved, where R is the number of relevant documents for the topic) as the official evaluation measure for the TREC12 topic distillation task. We will use precision at 10 for both TREC11 and TREC12 tasks, and we will also report R-Precision for the TREC12 experiments.

There are various sources of evidence that can be used for Web IR, in addition to the textual content of documents. In this paper, we focus on three different sources of evidence. The first is the textual content of documents, denoted by C. The second is the anchor text associated with the incoming links of a document, denoted by A. It has been shown that the anchor text is an effective source of evidence for topic distillation (Craswell et al., 2003), as well as for other tasks, such as finding homepages (Craswell et al., 2001). The third source of evidence is the document X  X  URL (Kraaij et al., 2002; Upstill et al., 2003), and more specifically the length in characters of its path, since it is likely that homepages of sites will have URLs with shorter paths. We denote this last source of evidence as U.
 From all possible combinations of the sources of evidence mentioned above (C, A, U,
CA, CU, AU, CAU), we will restrict our analysis to three of them, which have proved to be effective for the TREC tasks we experiment with. More specifically, for the first combination we use only the textual content of documents C. For the second combination of evidence, we extend each document with the anchor text of its incoming links (CA). For the third combination of evidence, we combine CA with the length of the document X  X  URL (CAU), as follows: score i = where sc i is the content analysis score for document d i documents, we apply this approach only to the top 1000 retrieved documents, where content and anchor text is used for retrieval.

For the content analysis, we consider three diverse and statistically independent weighting schemes. The first one is the well-known BM25 weighting scheme (Robertson and Walker, 1994), with b set empirically to 0.72. The other two are from Amati and Van Rijsbergen X  X  (2002) Divergence From Randomness (DFR) probabilistic framework, namely PL2 and
I( n e )C2. The weight of a query term t that occurs in a document is given by the follow-ing formulae: w ei ght PL2 ( t ) = tfn 1  X  log 2 w where: tfn 1 = term freq  X  log 2 1 + c  X  tfn 2 = term freq  X  ln 1 + c  X  n = N  X  1  X  1
N is the size of the collection, col freq is the within-collection term-frequency, term freq is the within-document term-frequency, doc freq is the document-frequency of the term and  X  = col freq / N , which is the mean and variance of a Poisson distribution, with col freq N .

The only parameter of the system is automatically set to c posed by He and Ounis (2003). The final score of a document d corresponds to the sum of the weights of all terms that occur in the query q and in the document: weight x ( d | q ) = where x corresponds to BM25, PL2, or I( n e )C2.

In Table 1, we present the evaluation of the three selected combinations of evidence, when applied for all queries uniformly. We can see that for TREC11, using the textual content of documents is sufficiently effective and any of the two other combinations yields lower precision at 10. The results for TREC12 show that when the relevant documents are only homepages of sites, then the most effective combination of evidence uses all three sources of evidence. For both TREC11 and TREC12, employing content and anchor text (CA) is the second most effective combination of evidence. Before continuing, it is worth noting that the best official run submitted for the TREC11 topic distillation task achieved 0.2510 precision at 10 (Craswell and Hawking, 2002). The highest precision at 10 and R-Precision, achieved by the official runs submitted for the TREC12 topic distillation task, were 0.1280 and 0.1636 respectively (these two numbers correspond to two different runs) (Craswell et al., 2003).
In what follows, we investigate whether the selective application of a combination of ev-idence on a per-query basis can lead to improvements in retrieval effectiveness. We assume that there is a decision mechanism MAX(X, Y, ... ) that will select on a per-query basis the most effective combination of evidence from X, Y, ... , where each of X, Y, to one of the combinations of evidence C, CA, or CAU. In this way, we would obtain the highest possible retrieval effectiveness on average with the given setting. Table 2 contains the results of the decision mechanism MAX(X, Y, ... ) for different weighting schemes and all possible pairs of the three combinations of evidence C, CA and CAU. For example, we can see that MAX(C PL2 ,CA PL2 ), which selects between C and CA, using the weighting scheme PL2, results in 0.2898 average precision at 10 for TREC11. In addition, we can see that C PL2 performs better for 9 queries, while CA PL2 queries in total. From the same table, we can see that for TREC11, the decision mecha-nism MAX is most effective, when we use C PL2 and CAU PL2 precision at 10. Similarly for TREC12, the decision mechanism MAX is most effective when we employ CA PL2 and CAU PL2 , achieving 0.1680 average precision at 10 and 0.1880 average R-Precision. For the two other weighting schemes, namely BM25 and I( n observe that improvements are also obtained over the uniform combination of evidence for all queries.

For a more realistic decision mechanism, we would have to choose the combinations of evidence to use among all the possible ones. We could employ two approaches to perform this selection. First, we could use the average retrieval effectiveness of the decision mechanism effectiveness. As we can see from Table 2, for the TREC11 topic distillation task, MAX(C CAU
PL2 ) outperforms MAX(C PL2 ,CA PL2 ), even though CAU PL2 (0.1367 with respect to 0.2551 average precision at 10, from Table 1). This suggests that employing the average retrieval effectiveness of MAX for selecting the combinations of evidence to use is prone to over-fitting. The second approach is based on selecting those combinations of evidence that result in the highest average retrieval effectiveness. For the remainder of the paper, we will employ the second approach and select the two combinations of evidence with the highest average precision at 10 retrieved documents, as shown in Table 1. Therefore, for TREC11, we will employ C PL2 and CA PL2 , while for TREC12, we will use CA
So far, we have looked at the potential improvements from the decision mechanism MAX, where the combinations of evidence use the same weighting scheme. In Table 3, we consider some cases, where a different weighting scheme is used for each of the two combinations of evidence. For example, MAX(C PL2 ,CA I( n e )C2 ) corresponds to the case, where the decision mechanism MAX selects either content-only retrieval with PL2, or content and anchor text retrieval with I( n e )C2. Among the results shown in Table 3, the most effective pair of combina-tions of evidence for TREC11 is C PL2 and CA BM25 , which results in 0.3041 average precision at 10. For TREC12, the highest R-Precision is obtained when CA
By comparing the results in Tables 1 and 2, we can see that there is room for improvement between the uniform combination of evidence and an appropriate combination of sources of evidence on a per-query basis. Based on this conclusion, we will introduce in the next section a simple decision mechanism for selecting an appropriate combination of evidence on a per-query basis. In our experiments, we will employ the weighting scheme PL2, which has been shown to be effective for both used TREC topic distillation tasks. We will also report results from experiments with different weighting schemes. 3. Decision mechanism
We have shown in the previous section that not all queries benefit equally from the same combination of evidence. In this section, we introduce a decision mechanism that aims to apply an appropriate combination of evidence for each query. For example, we employ content-only retrieval for specific queries, while we use evidence from the hyperlinks, or the
URLs, for more generic queries. The decision mechanism is based on the query scope, which addresses two important statistical aspects of the set of retrieved documents.
The first aspect, query extent , is related to the number of retrieved documents. We assume that for the more generic queries, there will be many documents that contain all the query terms. In these cases, the queries address a topic that is widely covered in the collection.
Therefore, evidence from hyperlink analysis may be more useful in detecting high quality documents, or homepages of relevant sites. The query extent is the number of retrieved documents { d i } that contain all the query terms, normalised between 0 and 1 by dividing with a given percentage  X  of the total number of documents in the test collection: query extent = min where N is the number of documents in the collection ( N = tion). The query extent does not depend on the relevance scores assigned to documents. The normalisation is introduced as most of the queries retrieve only a small fraction of documents from the collection and therefore, dividing by  X  leads to a better distribution of the query scope values. The parameter  X  can take values from the range (0,1). Because for each query from both TREC11 and TREC12 topic distillation tasks, the retrieved documents that contain all the query terms are on average fewer than 1% of the collection size, we will only test values less or equal to 0.01. For the remainder of the paper, the query extent will be writen as query extent  X  , in order to denote the used value of  X  clearly. A similar measure, without the normalisation, has been introduced by Amitay et al. (2003), simultaneously to that defined by Plachouras et al. (2003).

For the second query scope aspect, result extent , we take different perspective and con-sider additional structural information. Indeed, hypertext and the Web encourage authors to organise documents in several different ways. First, documents are grouped in sites, where most of the documents cover either a specific topic, or a series of related topics. Within sites, documents are usually organised in a hierarchical directory structure. In addition, a document may correspond to more than one Web page. This is different from classical IR document collections, where each physical document constitutes a logical document (Eiron and McCurley, 2003a). There have been different efforts towards the automatic identification of aggregates of hypertext, or Web pages. Botafogo and Shneiderman (1991) have employed a graph theoretic approach in order to identify aggregates. Differently, Eiron and McCurley (2003a), and Li et al. (2000), define heuristics based on observations of the structure of sites.
In the context of TREC experiments, grouping documents according to their domain has been employed in order to limit the redundancy of retrieving many documents from a given site (Kwok et al., 2002).

We define result extent , so that it indicates whether there are aggregates of documents, devoted to the query X  X  topic. If there exist aggregates with a high number of documents containing all the query terms, we expect that their entry points will be more useful than other documents. We denote by size j the number of documents from the aggregate a addition, let  X  size and  X  size be the average and the standard deviation respectively of size j  X  [1 , n ], where n is the number of aggregates formed from the set of retrieved documents that contain all query terms. We define the result extent as the number of aggregates a which size j is higher than  X  size + 2  X   X  size : result extent =|{ a j | size j &gt; X  size + 2  X   X  size }|
The result extent does not depend on the relevance scores assigned to documents. We form the aggregates in two different ways. First, a coarse-grained approach is adopted, where an aggregate is formed by grouping all the retrieved documents that belong to the same domain.
In this case, result extent is denoted as result extent ( domains ). Alternatively, we take a more granular approach and define aggregates as groups of documents from the same directory. In this case, result extent is denoted as result extent ( directories ).

After defining query extent and result extent , we introduce a simple decision mechanism, which uses either of the two query scope aspects, to select an appropriate combination of evidence on a perquery basis. In order to evaluate the effectiveness of each measure separately, we employ a simple approach, where the value of one of the measures, computed for the retrieved documents, is compared to a threshold. According to the result of this comparison, we assign one of the available combinations of evidence to the query. In this work, we consider two possible combinations of evidence, the selection of which is initially based on thresholds obtained by using relevance information. It should be noted that this selection mechanism can be extended, in order to use more query scope aspects and combinations of evidence.
For example, we can select from more than two combinations of evidence, or employ both query scope aspects in the selection process. The selection mechanism is shown in Table 4.
For each query, we compute the value of the query scope aspect qscope , given the set of retrieved documents and then, we compare its value to the threshold t .If qscope ( q ) then we select E 1 as the most appropriate combination of evidence for query q , otherwise we select E 2. Note that the order in which specific combinations are assigned to E 1 and
E 2 may significantly affect the effectiveness of the selection mechanism. This order should be consistent with the basic assumptions underlying the employed query scope aspect and each of the combinations of evidence. If there is an inconsistency, then we should expect a detrimental effect in the retrieval effectiveness.

In the remainder of the paper, we experiment and evaluate the effectiveness of the decision mechanism S ELECT C OMBINATION O F E VIDENCE , or SCE for brevity, in two steps. For the first step, Sections 4 and 5 contain the results from experiments, where we test query extent and result extent for different threshold values, under the assumption that there exists relevance information. The second step involves a method for setting the thresholds automatically, without relevance information (Section 6).
 4. Evaluation of the decision mechanism
In this section, we aim to evaluate how effective the decision mechanism and each of query extent and result extent are in identifying appropriate combinations of evidence for each query. For each TREC topic distillation task, we use the two most effective combi-nations of evidence, according to the evaluation presented in Table 1. For TREC11, we employ C PL2 and CA PL2 , while for TREC12, we use CA PL2 threshold values, from each measure X  X  minimum value, to its maximum value, as computed for the queries. 3 Because the values of the different query scope aspects are not in the same range, the threshold values in subsequent figures will be normalised between 0 and 1. This linear transformation of the threshold values does not affect the results. The normalised values are also reported in all corresponding tables, in order to facilitate reading the figures. The results from the experiments on TREC 11 data are shown in Fig. 1 and Table 5.
Both measures based on the result extent are more effective, with result extent ( domains ) achieving 0.2796 average precision at 10, when the threshold values are in the range [20.04, 23.88]. The threshold values in this range, which result in the highest average precision at 10, correspond to the 11% of the tested threshold values, showing that result extent ( domains ) can give stable improvements. According to Fisher X  X  exact test, which determines if there is any non-random association between categorical variables, when the threshold t is in the range [20.04, 23.88], the decision mechanism that uses result extent ( domains ), selects the most appropriate combination of evidence for a statistically significant number of queries ( p = 0 . 0294), for which there is a difference in C and CA X  X  performance. This means that the queries for which C PL2 is more effective than CA PL2 select an appropriate combination of evidence. In Fig. 1, and in subsequent figures, the range of threshold values, where there is such a significant association, is marked with a bold line.
From Fig. 1 and Table 5, we can also see that the decision mechanism, which uses query extent 0 . 01 , performs as well as C PL2 only when the threshold t this is due to an inconsistency between the assumption underlying query extent and the two combinations of evidence, C PL2 and CA PL2 . If we swap the combinations of evidence in the de-cision mechanism, so that CA PL2 is applied when qscope ( q ) are able to improve precision at 10 over that of C PL2 , as discussed by Plachouras et al. (2004).
We obtain similar improvements, when we experiment with the topic distillation data from TREC12 (see Fig. 2 and Table 6). More specifically, query extent baseline CAU and results in 0.1460 average precision at 10 for the threshold range [0.097, 0.137] (5% of the tested threshold values). The corresponding decision mechanism selects the most effective combination of evidence for a statistically significant number of queries, for the threshold ranges [0.097, 0.137] ( p = 0 . 0472), [0.438, 0.498] ( p 0.639] ( p = 0 . 0140), as we can see from the bold lines in Fig. 2, even though the last two ranges of thresholds do not correspond to the highest precision at 10. In addition, when we use result extent ( domains ), the highest average precision at 10 is 0.1480 for the threshold ranges [8.17, 9.61] and [12.48, 13.91]. These ranges correspond to the 6% of the tested threshold values. From Fig. 2, Fisher X  X  exact test shows that for the threshold range [7.45, 13.91] the output of the decision mechanism is statistically significant, with p
This range of threshold values includes values, which do not result in the highest precision at 10. When result extent ( directories ) is used, the highest average precision at 10 is 0.1400, which is equal to that of CAU.

For the TREC12 experiments, we also consider R-Precision as the evaluation mea-sure (Fig. 3 and Table 7). All three query scope aspects result in improvements over the baseline CAU for a wide range of threshold values. The highest average R-Precision is 0.1701, obtained with query extent 0 . 01 for the threshold range [0.378, 0.388]. Following, result extent ( domains ) results in 0.1612 R-Precision for the threshold range [8.17, 8.89]. In both cases, the association between the relative performance of CA and CAU, and the output of the decision mechanism that uses query extent 0 . 01 or result extent ( domains ), is non-random, according to Fisher X  X  exact test, with p equal to 0.0414 and 0.0403 respectively. These results were obtained for 2% of the tested threshold values (Fig. 3). When result extent ( directories ) was employed, we obtained 0.1539 R-Precision for only 1% of the tested threshold values.
In this section, we have investigated the effectiveness of both query extent and result extent for the TREC11 and TREC12 topic distillation tasks. We have shown that improvements are obtained for wide ranges of thresholds. Moreover, according to Fisher X  X  exact test, we have found that there is a statistically significant association between the output of the decision mechanism, which employs either query extent or result extent , and the relative effectiveness of the employed combinations of evidence. Thus, the two query scope aspects are successful in capturing how broad a query is, and the decision mechanism can effectively identify an appropriate combination of evidence for each query. 5. Additional experiments with the decision mechanism
So far, we have shown that the decision mechanism that employs either query extent or result extent improves the retrieval effectiveness. In this section, we focus our analysis on how different parameters of the decision mechanism, such as the normalisation of the query extent , or employing different weighting schemes, affect its effectiveness. The remainder of this section is organised as follows. Section 5.1 contains experiments, where we examine the effect of using different  X  values for normalising query extent . In Section 5.2, we test the effect of using different weighting schemes for each combination of evidence. Section 5.3 contains experiments, where we show how the query scope aspect values change when we use both content and anchor text for their calculation. 5.1. Normalisation of query extent
In this section, we focus on query extent and the effect of the normalising parameter we have set  X  = 0 . 01. In the following experiments, we set addition to 0.01, and observe the effect on the performance of our decision mechanism. Fig. 4 and Table 8 show the results for the TREC11 topics. Moreover, Fig. 5 and Table 9 contain the results for the TREC12 topics, when precision at 10 is employed for the evaluation, and
Fig. 6 and Table 10 contain the results for the TREC12 topics, considering R-Precision as the evaluation measure.

For the TREC11 experiments (Table 8), we can see that, for all the tested values of the highest obtained average precision at 10 is 0.2694 for the threshold t the effect of changing the value of  X  is demonstrated in Fig. 4 by the shifting of the curves corresponding to different  X  values. More specifically, when we decrease the value of more sensitive to specific queries.

This is more evident in the TREC12 experiments, for both precision at 10 (Fig. 5 and Table 9) and R-Precision (Fig. 6 and Table 10). We observe that the shift of the optimal thresholds is approximately reciprocal to the ratio between the different see from Table 5 that when  X  = 0 . 01 , 0 . 005 and 0.002 respectively, the middle values of the threshold ranges are 0.117, 0.233 and 0.585, from which we have: 0 . 01 0 . 005
The shifting is also reflected on the threshold ranges, for which there is a statistically significant association between the relative effectiveness of CA and CAU, and the output of
SCE( q ,CA PL2 , query extent  X , t ), as shown by the bold parts of the lines in Fig. 5. This is expected because  X  is in the denominator of the ratio |{ d Similar results are obtained when we use R-Precision for the evaluation (Fig. 6 and Table 10). Additionally, we can see the effect of selecting unsuitable parameter values. As  X  decreases, the highest precision decreases as well, because query extent becomes more sensitive to specific queries, and the decision mechanism applies CAU even though CA PL2 may be more effective.

Overall, we have seen that the normalisation of query extent by dividing with the decision mechanism X  X  effectiveness. Employing an inappropriate value may lead to a decision mechanism with reduced potential for improvement. Therefore, in an operational environment, it is important to select an appropriate value for retrieval effectiveness is obtained within the threshold range and the decision mechanism is effective for a wider range of threshold values. We believe that a reasonable value for the normalising parameter should not be significantly higher than the average number of documents containing all query terms.
 5.2. Employing different weighting schemes
In our experiments, we have used the same weighting schemes for the different combinations of evidence. However, in Section 2, we saw that using different weighting schemes for each combination of evidence may result in even higher retrieval effectiveness (see Table 3).
In the following experiments, we employ different weighting schemes for both TREC11 and TREC12. More specifically, we show that if we use C sult extent ( domains ) for TREC11, we get improvements over the highest average precision obtained when we employ only PL2 for both C and CA (0.2837, as shown in Table 11, with respect to 0.2796 average precision at 10, as shown in Table 5). Both query extent and result extent ( directories ) are also more effective than when they are used with C CA
PL2 . When we employed other combinations of weighting schemes, for both tested tasks, the correspondence between the query scope aspect values and the relative effectiveness of the combinations of evidence was weak, and the decision mechanism did not perform better than the baselines.

These experiments, where we employed different weighting schemes, show that signifi-cant improvements can be obtained when we use the weighting schemes PL2 and I( n
Further experiments are required in order to establish a relation between the independence of weighting schemes and their effectiveness in the context of the decision mechanism. 5.3. Using content and anchor text for computing the query scope The values of the query scope aspects depend only on the retrieved documents for each query.
For our last experiments, we investigate how the values of the query scope aspects change if we use a different document representation, where the document is extended by adding the anchor text of its incoming hyperlinks. Indeed, employing the anchor text is an effective retrieval approach for finding entry points of sites (Craswell et al., 2001, 2003).
In Table 12, we show that the mean values of the query scope aspects for both TREC11 and TREC12 tasks, computed using either document representations, are similar. According to the p values of Wilcoxon X  X  signed rank test for paired samples, which determines if the medians of two distributions are different, there is no significant change in the values of result extent ( domains ) for the two tested document representations. On the other hand, the values of query extent 0 . 01 are significantly different when anchor text is used, in addition to the content of documents. Moreover, the values of result extent ( directories ) for the two document representations are significantly different only for the TREC12 topic distillation queries. These differences are due to the fact that when we employ content and anchor text, the number of documents with all query terms can only increase.

We compare the effect of the different document representations on the decision mecha-nism. The setting of the decision mechanism is the same as in Section 4. Moreover, we test the same threshold values for the query scope aspects, for both document representations.
We have found that the highest obtained retrieval effectiveness remains the same, indepen-dently of the used document representations (the highest obtained values are the same as trieval effectiveness of the decision mechanism is the same when we use query extent or result extent ( domains ) for TREC11, and it changes for 3% of the tested thresholds for
TREC12. When we use result extent ( directories ) for TREC11, there is a difference for 1% of the tested thresholds. For TREC12, the average precision at 10 of the decision mechanism with result extent ( directories ) changes for 6% of the tested thresholds. If we consider R-Precision for TREC12, there is a change in the average effectiveness of the decision mechanism for 5% of the tested thresholds.

These small changes occur because there are only few documents for which the query hyperlinks. For the used .GOV collection, one possible explanation is that this collection contains high quality documents from controlled sites. If we use a less controlled Web test collection, where there is greater variability in the anchor text, we may find that using the content of documents and their anchor text for computing query extent and result extent might have an impact on their values. 6. Setting thresholds without relevance information
In the first step of the decision mechanism X  X  evaluation, we have employed relevance infor-mation, in order to choose the two most effective combinations of evidence for each task, and to select the threshold ranges for which we obtain improvements in effectiveness. However, in an operational environment, where relevance information is not readily available, we should be able to select the combinations of evidence E 1 and E 2 used in the decision mechanism of
Table 4, as well as automatically set the thresholds to values, which would result in improved retrieval effectiveness.

For selecting the most effective combinations of evidence to use in the decision mechanism, it is reasonable to assume that we can perform experiments with a set of training queries.
From the results, we could obtain the more appropriate combinations of evidence for the queries that users submit to our retrieval system.

In order to predict appropriate threshold values, we employ an approach, based on the sampling method introduced by Cronen-Townsend et al. (2002). We approximate the process of querying, by sampling terms from the vocabulary of the test collection, and submitting them to our retrieval system as single-term queries. The intuition for sampling single-terms to set the decision mechanism X  X  threshold for multi-term queries is that in both cases we use the same vocabulary for querying and therefore, we expect to obtain a similar distribution of query scope aspect values. From the set of retrieved documents, we compute the values of query extent and result extent . In our experiments, we have sampled the terms with doc-ument frequency in the range [500, 20000], in order to obtain approximately the number of retrieved documents with all query terms for the TREC11 and TREC12 topic distillation queries.

After sampling the terms from the vocabulary, we estimate the probability densi-ties for the number of documents containing a term, result extent ( domains ) and re-sult extent ( directories ). We employ kernel density estimation with Gaussian kernels and auto-matic setting of the bandwidth (Scott, 1992). The estimated probability densities for the num-ber of documents containing all the query terms (in this case, this corresponds to the probabil-are shown in Fig. 8. Because query extent , as defined in Eq. (3), is not a continuous function of the number of documents that contain all the query terms, the estimation of its probability density would not be precise for values close to 1. Therefore, we have estimated the proba-bility density for the document frequency and we compute the values of query extent from the number of documents containing a term by using Eq. (3). We set significantly higher than the average percentage of documents containing all the query terms for the TREC11 and TREC12 topic distillation queries.

From the estimated probability densities, we compute the threshold values at the points where n percent of the values are lower than the threshold, for n values for the estimates query extent 0 . 01 , and result extent for both domains and directories are shown in Table 13.

Next, we employ the estimated threshold values in the decision mechanism. Table 14
TREC12 topics, using either precision at 10 or R-Precision for the evaluation. We can see that improvements over the baselines (C for TREC11 and CAU for TREC12) are obtained, when the thresholds are set automatically. Furthermore, the entry in bold shows that the association between the relative effectiveness of the different combinations of evidence and the output of the decision mechanism for the corresponding thresholds is statistically significant.
More specifically, we can see from Table 14 that improvements are obtained for all mains ) and result extent ( directories ). According to Fisher X  X  exact test, the decision mecha-selects the most effective combination of evidence for a statistically significant number of queries ( p = 0 . 0329).

The results for the TREC12 queries are similar. As shown in Table 15, we obtain im-provements in precision at 10 over the baseline CAU for both query extent either query extent 0 . 01 or result extent ( domains ) with the 50% threshold, selects the most effective combination of evidence for a statistically significant number of queries ( p and p = 0 . 0086 respectively). We obtain the highest R-Precision when we use result extent ( domains ) and the 50% threshold. Thus, result extent ( domains ) proves to be more effective and stable than query extent or result extent ( directories ).

Overall, we can say that the sampling of terms from the collection X  X  vocabulary, in order to set the decision mechanism X  X  thresholds automatically, without relevance information, is an effective approach. These results show that the selective combination of evidence is a method applicable in an operational environment. From all three query scope aspects we tested, we found that result extent ( domains ) is the most effective one in the context of the decision mechanism with the automatically set thresholds.

For the experiments in this section, we have considered single terms from the document collection as queries. Even though we set the thresholds to a value, so that 75% of the estimated values are lower, this threshold value is quite low. Especially in the case of query extent the estimated threshold values are lower than the ones resulting in optimal precision (Table 7 and Table 13). We believe that sampling sets of related terms from the collection X  X  vocabulary may result in a better approximation of the querying process. Such an approach would require measuring the association between terms (Silverstein et al., 1999), or using the anchor text of documents (Eiron and McCurley, 2003b). 7. Discussion
We have shown that the decision mechanism from Section 3, when used with the proposed es-timates of how specific or broad a query is, increases retrieval effectiveness for both TREC11 and TREC12 topic distillation tasks.

Comparing the effectiveness of result extent ( domains ) with result extent ( directories ), we can see that the decision mechanism is more effective for both TREC tasks, when domains are used for forming the aggregates. A reason for this is that there may exist many directories that contain few documents, thus biasing the average directory size towards lower values, and resulting in a relatively high value for result extent ( directories ). Further research is needed in order to employ more granular approaches for detecting aggregates of documents (Eiron and McCurley, 2003a; Li et al. 2000).

We have found that result extent ( domains ) has other useful properties. First, its values do not change significantly when they are computed using a different document represen-tation, such as content and anchor text. In addition, the average retrieval effectiveness of the decision mechanism changes only for a small percentage of thresholds when we use result extent ( domains ) with either document representations. When we employ it in the de-cision mechanism, we obtain the highest precision for a wide range of consecutive thresholds.
In addition, when the decision mechanism X  X  thresholds are set automatically, it is effective for both tested TREC tasks. Therefore, we believe that result extent ( domains ) is a robust query scope aspect, which can be used in a realistic setting.

An interesting generalisation of our methodology would be to investigate the automatic selection of specific combinations of evidence to use with each query scope aspect. Suppose that there is no relevance information available. Instead of comparing the relative retrieval effectiveness between two different combinations of evidence, we could compute a measure of distance between the corresponding rankings of documents. If the distance between the rankings is correlated with the difference between the query scope aspect value for a query, and the threshold value of the decision mechanism, then we would expect that this particular setting of the decision mechanism can be used effectively. In this way, we would obtain an estimation of how effectively we can use a query scope aspect and two different combinations of evidence, in the context of the decision mechanism. 8. Related work
The combination of evidence from different sources has been employed in various ways in order to increase retrieval precision (Croft, 2000). Belkin et al. (1993) used different query representations, while (Turtle and Croft, 1991) proposed a Bayesian inference network in order to combine different query and document representations. Similarly, Ribeiro-Neto and
Muntz (1996) proposed a belief network model for the combination of different sources of evidence, including the hyperlink structure.

From a different perspective, Bartell et al. (1994) investigated the automatic combination of multiple retrieval approaches. They model the combination of evidence from individual  X  X xperts X  as the linear combination of the individuals X  estimates. Instead of combining the scores of different retrieval approaches linearly, Aslam and Montague (2001) proposed a method for fusing ranked lists of documents, an approach with applications in meta-search engines, where the scores of documents are not available. From the perspective of model selection, He and Ounis (2004) consider applying a different weighting model per query, by clustering a set of training queries and applying the same weighting model for all queries in the same cluster.

In the context of Web IR, recent research has focused on detecting when to employ addi-tional evidence to the textual content of documents. Ogilvie and Callan (2003), and Fagin et al. (2003), investigate a variety of combinations of evidence in different settings. Other ap-proaches focus on the relation between the effectiveness of hyperlink analysis and the density of hyperlinks in a test collection (Gurrin and Smeaton, 2003; Fisher and Everson 2003).
There have also been proposed approaches, where the weight of additional evidence from either hyperlink analysis algorithms HITS or SALSA (Amitay et al., 2002), or other sources of evidence such as the anchor text and the number of incoming hyperlinks (Amitay et al., 2003), is set on a per-query basis, according to characteristics of the set of retrieved documents. Differently, our decision mechanism enables or disables a source of evidence on a per-query basis. This methodology can be seen as a linear combination of multiple retrieval approaches, where only one coefficient is non-zero. As a result, the query scope aspect values and the relative effectiveness of the combinations of evidence do not need to be strongly correlated. A second difference is that we employ evidence from the distribution of aggregates of documents, in order to model how broad or specific the queries are.
In addition, Kang and Kim (2003) propose a query type classification method. The query type corresponds to the retrieval task, and a different combination of evidence is applied for each query type. On the other hand, our methodology does not consider the query type explicitly, but aims to apply an appropriate combination of evidence on a per-query basis. 9. Conclusions
In this paper, we have presented a decision mechanism that enables the selective combination of evidence on a per-query basis, for Web IR, and more specifically, for topic distillation. The decision mechanism is based on two query scope aspects. The first one is the query extent , which corresponds to the number of documents in the collection that contain all the query aggregates in the set of retrieved documents. An aggregate of documents is a group of related documents, belonging to the same domain ( result extent ( domains )), or stored in the same directory ( result extent ( directories )).

We have shown that the decision mechanism and the query scope aspects can increase the retrieval effectiveness, compared to the uniform combination of evidence, irrespectively of the queries. Improvements over the baselines are obtained for the TREC11 topic dis-well as for the TREC12 topic distillation task, when we employ CA and CAU with ei-ther query extent or result extent ( domains ). We have found that using domain-based ag-gregates is more effective than directory-based aggregates. The improvements in retrieval effectiveness correspond to the cases where there is a statistically significant association between the relative effectiveness of the different combinations of evidence and the out-put of the decision mechanism that uses the query scope aspects. Following, we have evance information. Overall the most effective query scope aspect was found to be re-sult extent ( domains ), and improvements over the baselines were obtained, proving that our approach can be applied in an operational setting, where relevance information is not readily available.

In the future, we would like to experiment with larger test collections, such as the .GOV2, or with other, less controlled test collections, as they become available. Additionally, we would like to generalise our approach, by introducing a measure that estimates the effectiveness of the decision mechanism without relevance information. Experimentation in the context of different retrieval tasks, such as the filtering task, where relevance information becomes incrementally available, is another possible application of our approach.
 References
