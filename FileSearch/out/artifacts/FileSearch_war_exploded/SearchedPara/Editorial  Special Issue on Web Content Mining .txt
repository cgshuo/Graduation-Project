 With the phenomenal growth of the Web, there is an ever-increasing volume of data and in formation published in numerous Web pages. The research in We b mining aims to develop new techniques to effectively extract and mine useful knowledge or information from these Web pages [8]. Due to the heterogeneity and lack of structure of Web data, automated discovery of targeted or unexpected knowledge/information is a challenging task. It calls for novel methods th at draw from a wide range of fields spanning data mining, m achine learning, natural language processing, statistics, databases, and information retrieval. In the past few years, there was a rapi d expansion of activities in the Web mining field, which consis ts of Web usage mining, Web structure mining, and Web cont ent mining. Web usage mining refers to the discovery of user access patterns from Web usage logs. Web structure mining trie s to discover useful knowledge from the structure of hyperlinks. Web content mining aims to extract/mine useful information or knowledge from Web page contents. For this special issue, we focus on Web content mining . The objectives of this special issue are two-fold: 1. To bring together and to present some of the latest research 2. To encourage more research activities in the field. With the In this Editorial, we begin by reviewing some of the important problems in Web content mining. We then introduce the papers published in this issue. It is often said that the Web offers an unprecedented opportunity and challenge for data mining. We believe that this is so due to the following characteristics of the Web: 1. The amount of data/information on the Web is huge and still 2. The coverage of Web informati on is wide and diverse. One 3. Data of all types exist on the Web, e.g., structured tables, 4. Information on the Web is heterogeneous. Multiple Web 5. Much of the Web information is semi-structured due to the 6. Much of the Web information is linked. There are links 7. Much of the Web information is redundant. The same piece of 8. The Web is noisy. A Web page typically contains a mixture of 9. The Web consists of surface Web and deep Web. Surface 10. The Web is also about servi ces. Many Web sites and pages 11. Above all, the Web is a virtual society. It is not only about 12. The Web is dynamic. Information on the Web changes We can see why the Web is such a fascinating place and why it offers so many opportunities for da ta mining. Below, we give an introduction to some of the current Web content mining tasks. We will not discuss traditional mining tasks that are directly applied to the Web data, e.g., Web page classification and Web page clustering. This is perhaps the most widely studied research topic of Web content mining. One of the r easons for its importance and popularity is that structured data on the Web are often very important as they represent their host pages X  essential information, e.g., lists of produc ts and services. Extracting such data allows one to provide value added services, e.g., comparative shopping, and meta-search. Structured data is also easier to extract compared to unstructured texts. This problem has been studied by researchers in AI, da tabase and data mining, and Web communities. There are several approaches to structured data extraction, which is also calle d wrapper generation. The first approach is to manually write an extraction program for each Web site based on observed format patterns of the site. This approach is very labor intensive and time consuming. It thus does not scale to a large number of sites. The second approach is wrapper induction or wrapper l earning, which is the main technique currently. Wrapper learning works as follows: The user first manually labels a set of tr ained pages. A learning system then generates rules from the tr aining pages. The resulting rules are then applied to extract target items from Web pages. Example wrapper induction systems include WIEN [27], Stalker [39], BWI [18], WL 2 [13], etc. The third approach is the automatic approach. Since structured data objects on the Web are normally database records retrieved from underlying da tabases and displayed in Web pages with some fixed templates. Automatic methods aim to find patterns/grammars from the Web pages and then use them to extract data. Examples of automatic systems include IEPAD [9], MDR [33], RoadRunner [15], EXALG [3], [19], [31], [42], etc. Most Web pages can be seen as text documents. Extracting information from Web documents ha s also been studied by many researchers. The research is closely related to text mining, information retrieval and natura l language processing. Current techniques are mainly based on machine learning and natural language processing to learn extr action rules from manual labeled examples [6, 7, 14, 24, 29, 46]. Recently, a number of researchers also make use of common langua ge patterns (common sentence structures used to express certain facts or relations) and redundancy of information on the We b to find concepts, relations among concepts and named entities [12, 20, 32]. The patterns can be automatically learnt or supplied by human users. Another direction of research in this area is Web question-answering. Although question-answering was first studied in information retrieval literature, it becomes very important on the Web as Web offers the largest source of in formation and the objectives of many Web search queries are to obtain answers to some simple questions. [28, 43] extend question-answering to the Web by query transformation, query expans ion, and then selection. Due to the sheer scale of the Web and diverse authorships, various Web sites may use different syntaxes to express similar or related information. In order to make us e of or to extract information from multiple sites to provide value added services, e.g., metasearch, deep Web search, etc, one needs to semantically integrate information from multip le sources. Recently, several researchers attempted this task. Two popular problems related to the Web are (1) Web query interface integration, to enable querying multiple Web databases (which are hidden in the deep Web) [21, 22, 23, 57, 50], a nd (2) schema matching, e.g., integrating Yahoo and Google X  X  dir ectories to match concepts in the hierarchies [2, 17]. The ability to query multiple deep Web databases is attractive and interesting because the deep Web contains a huge amount of informati on or data that is not indexed by general search engines [5, 10]. Because of the huge size of the Web, organization of information is obviously an important issue. Although it is hard to organize the whole Web, it is feasible to organize Web search results of a given query. A linear list of ranked pages produced by search engines is insufficient for many applications. The standard method for information organization is concept hierarchy and/or categorization. The popular technique for hierarchy construction is text clustering, which groups sim ilar search results together in a hierarchical fashion. Several researchers have attempted the task using clustering [11, 26, 30, 36, 55, 56]. In [32], a different approach is proposed which does not use clustering. Instead, it exploits existing organizational structures in the original Web documents, emphasizing tags and language patterns to perform data mining to find important c oncepts, sub-concepts and their hierarchical relationships. In order words, it makes use of the information redundancy property and semi-structure nature of the Web to find what concepts are important and what their relationships might be. This work aims to compile a survey article or a book on the Web automatically. A typical Web page consists of ma ny blocks or areas, e.g., main content areas, navigation areas, advertisements, etc. It is useful to separate these areas automatically for several practical applications. For example, in Web data mining, e.g., classification and clustering, identifying main content areas or removing noisy blocks (e.g., advertisements, navi gation panels, etc) enables one to produce much better results. It was shown in [51, 52] that the information contained in noisy blocks can seriously harm Web data mining. Another applicati on is Web browsing using a small screen device, such as a PDA. Id entifying different content blocks allows one to re-arrange the layout of the page so that the main contents can be seen easily wit hout losing any other information from the page. In this past two y ears, several papers have been published on this topic [51, 52, 53, 47, 44]. This research also includes detecting common layout and templates of Web pages [4, 25, 45]. Consumer opinions used to be ve ry difficult to obtain before the Web was available. Companies us ually conduct consumer surveys or engage external consultants to find such opinions about their products and those of their competitors. Now much of the information is publicly availabl e on the Web. There are numerous Web sites and pages containing c onsumer opinions, e.g., customer reviews of products, forums, discussion groups, and blogs. This online word-of-mouth behavior re presents new and measurable sources of information for marketing intelligence. Techniques are now being developed to exploit these sources to help companies and individuals to gain such info rmation effectively and easily [1, 16, 24, 38, 40, 41, 48, 49]. For instance, [24] proposes a feature based summarization method to au tomatically analyze consumer opinions in customer reviews from online merchant sites and dedicated review sites. The result of such a summary is useful to both potential customers and product manufacturers. This special issue of SIGKDD Explor ations brings together some of the latest research results in the field of Web content mining. It presents eight papers, which deal with a wide range of problems. All the papers propose some novel a nd/or principled techniques to solve these problems. The first paper by Zhang, Laks hmanan, and Zamar studies the problem of extracting data record s from a large set of Web pages. What is interesting about this paper is that it proposes a novel method to estimate the current c overage of the results of the system, based on capture-recapture models with unequal capture probabilities. Techniques are also proposed to estimate the error rate of the extracted information. To evaluate the method and ideas proposed in this paper, a large number of experiments have been conducted to demonstrate their effectiveness. The second paper by Song et al proposes a new method to segment a Web page into blocks of different importance using machine learning methods. Speci fically, it proposes a set of features for learning a block impor tance model for Web pages. As mentioned earlier, page segmentation is important because in many applications, only the main content blocks are useful. Other noisy blocks should be removed. Ea rlier work in this area requires multiple pages from a site to detect templates [52]. The technique in this paper works on a single page, which is a main advantage. The third paper by Cimiano and Staab proposes a methodology for extracting knowledge from the Web in knowledge acquisition. Specifically, it reports a system, called PANKOW, which classifies concepts into a given ontology. The method first generates a set of language patte rns from the query concept and then uses a search engine (it us es Google) to collect statistical information about each pattern on the Web. The knowledge engineer then makes the decision regarding the classification. The basic idea is that certain lexico-syntactic patterns matched in texts convey a specific semantic relation. The fourth paper by Zhang et al explores the problem of correlated summarization of a pair of online news articles. The algorithm aligns (sub)topics of the two news articles and summarizes their correlation by sentence extraction. They model a pair of news articles with a we ighted bipartite graph. A mutual reinforcement principle is then applied to identify a dense sub-graph of the weighted bipartite graph. Sentences corresponding to the sub-graph are correlated well in textual contents and convey the dominant shared topic of the articles. Their experiment results show that the technique works well. The fifth paper by Gruhl et al studies the diffusion or the dynamics of information in the blogspace. They show how by using macro (topical) and micro (individual) models, various structures and behaviors can be understood, ranging from the strong driving effect of outside world events on what is being discussed to the applicability of traditional sociological models of influence to bloggers. The discovered characterizations of information propagation allow prac tical applications to take advantage of these emerging Web phenomena. The sixth paper by Dong, Ma dhavan and Halevy presents techniques for searching and matching Web services by exploiting statistics in a large co rpus of structures (e.g., WSDL files). Web services are loosel y coupled software components, published, located, and invoked across the Web with SOAP. A Web service typically comprise s of several operations with parameters. This paper reviews and compares two recent works: Searching for Web services and schema matching, both of which leverage corpora of structures to bridge semantic heterogeneity. The seventh paper by Sarawagi and Vydiswaran addresses the problem of finding the paths leading to specific  X  X oal pages X  on a large Web site. It addresses this problem as sequential labeling with Conditional Random Fields. Thus, unlike prior  X  X ocused crawling X  works, this paper pr oposes to capture the dependency or correlation between classifica tions of sequential pages on a path. The specific focus of  X  X rawling in a Web site X  also distinguishes the work from general-purpose focused crawling. The eighth paper by Chang, He, and Zhang studies dynamic  X  X n-the-fly X  semantics discovery for large scale integration on the  X  X eep Web, X  and proposes  X  X olis tic mining X  as a conceptual framework unifying initial works and a general approach for such large-scale integration. It reports three sample tasks as evidences: interface extraction, schema matching, and query translation. To generalize, it then proposes holistic mining as a unified insight to observe and leverage  X  X idden re gularities X  across holistic sources for large scale integration, a nd outlines future challenges. In summary, the eight papers repr esent some of the latest and most promising research results in this new and exciting field, which continues to make significant impact on real-world applications. We are confident that this special issue will stimulate further research in this area. [1] Agrawal, R., Rajagopalan, S., Srikant, R., Xu, Y. Mining [2] Agrawal, R., Srikant, R. On Integrating Catalogs. WWW-01 , [3] Arasu, A. and Garcia-Molina, H. Extracting Structured Data [4] Bar-Yossef, Z. and Rajagopalan, S. Template Detection via [5] Bergman, M. K. The Deep Web: Surfacing Hidden Value. [6] Bunescu, R., Mooney, R. Collective Information Extraction [7] Califf, M and Mooney, R. Bo ttom-up Relational Learning of [8] Chakrabarti, S . Mining the Web: Discovering Knowledge [9] Chang, C-H., Lui, S-L. IEPAD: Information Extraction [10] Chang, K. C.-C., He, B., Li, C., Patel, M., Zhang, Z. [11] Chuang, S.-L. and Chien, L. -F., A Practical Web-based [12] Cimiano, P., Handschuh, S., Staab, S. Towards the Self-[13] Cohen, W., Hurst, M., and Je nsen, L. A Flexible Learning [14] Cohen, W., Sarawagi, S. Expl oiting Dictionaries in Named [15] Crescenzi, V., Mecca, G. a nd Merialdo, P. ROADRUNNER: [16] Dave, K., Lawrence, S., and Pe nnock, D. Mining the Peanut [17] Doan, A. Madhavan, J. Do mingos, P., and Halevy, A. [18] Freitag, D and McCallum, A. Information Extraction with [19] Embley, D., Jiang, Y and Ng, Y. Record-boundary discovery [20] Etzioni, O, Cafarella, M, Downey , D., Kok, S. Popescu, A. [21] He, B., Chang, K. C.-C. Sta tistical Schema Matching across [22] He, B., Chang, K. C.-C., Han J. Discovering Complex [23] He, H, Meng, W., Yu, C. Wu, Z. WISE-Integrator: An [24] Hu, M and Liu, B. Mining and Summarizing Customer [25] Kao, J., Lin, S. Ho, J. Chen, M. Entropy-based Link Analysis [26] Kummamuru, K., Lotlikar, R., Roy, S., Singal, K. and [27] Kushmerick, N. Wrapper Induction: Efficiency and [28] Kwok, C., Etzioni, O., Weld, D. Scaling Question Answering [29] Lafferty, J., McCallum, A. Pe reira, F. Conditional Random [30] Lawrie, D.J. and Croft, W. B., Generating Hierarchical [31] Lerman, K., Getoor L., Minton, S. and Knoblock, C. Using [32] Liu, B., Chin, C., Ng, H-T. Mining Topic-Specific Concepts [33] Liu, B., Grossman, R. and Zhai, Y. Mining Data Records in [34] Liu, B., Zhao, K., and Yi, L. Visualizing Web site [35] Liu, B. Ma, Y., Yu, P. Disc overing Unexpected Information [36] Maedche, A. and Staab, A. Mining Ontologies from Text. [37] McCallum, A and Li, W. Early Results for Named Entity [38] Morinaga, S., Yamanishi, K., Ta teishi, K, and Fukushima, T. [39] Muslea, I., Minton, S. and K noblock, C. A Hierarchical [40] Nigam, K. and Hurst, M. Towards a Robust Metric of [41] Pang, B., Lee, L., and Vaithyanathan, S., Thumbs up? [42] Pinto, D., McCallum, A., Wei, X. and Bruce, W. Table [43] Radev, D., Fan, W., Qi, H., Wu , H., Grewal, A. Probabilistic [44] Ramaswamy, L, Ivengar, A, Liu, L, and Douglis, F. [45] Reis, D. Golgher, P, Silva, A. Laender, A. Automatic Web [46] Sarawagi, S., Cohen, W. Semi-Markov Conditional Random [47] Song, R., Liu, H., Wen, J.-R, W.-Y, Ma. Learning Block [48] Turney, P. Thumbs Up or Thumbs Down? Semantic [49] Wilson, T, Wiebe, J, &amp; Hwa, R. Just How Mad are You? [50] Wu, W., Yu, C., Doan, A and Meng, W. An Interactive [51] Yi, L. and Liu, B. Eliminating Noisy Information in Web [52] Yi, L., and Liu, B. Web Pa ge Cleaning for Web Mining [53] Yin, X. and Lee, W. S., Us ing Link Analysis to Improve [54] Yu, S., Cai, D., Wen, J.-R. and Ma, W.-Y., Improving [55] Zamir, O. and Etzioni, O., Grouper: A Dynamic Clustering [56] Zeng, H. He, Q, Chen, Z., Ma, W. and Ma, J. Learning to [57] Zhang, Z., He, B., Chang, K. C.-C. Understanding Web 
