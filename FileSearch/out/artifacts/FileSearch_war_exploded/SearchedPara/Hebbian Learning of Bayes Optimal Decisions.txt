 in Bayesian networks has been developed.
 algorithm. The approach of [5] interprets the weight w representing the random variables x these weights.
 among inputs.
 We consider the model of a linear threshold neuron with outpu t y neuron is firing and y is given by a linear decision function  X  y current firing states of all presynaptic neurons and w We propose the following learning rule, which we call the Bay esian Hebb rule: postsynaptic neuron y the Bayesian Hebb rule learns log-probability ratios of the postsynaptic firing state y on a corresponding presynaptic firing state y Section 4). We will prove that the rule converges globally to the target weight value w  X  We first show that the expected update E[ X  w Since the above is a chain of equivalence transformations, t his proves that w  X  changes of the Bayesian Hebb rule (1) in the n -dimensional weight-space R n . of The first factor in (4) is always non-negative, hence  X  &lt; 0 implies E [ X  w Already after having seen a finite set of examples h y frequentist X  X  approach would use counters a estimate every w  X  A Bayesian approach would model p ( y counters a both approaches a new example with y where N Furthermore,  X  w new for each synapse.
  X  w  X   X  distribution even for a sufficiently small constant learnin g rate. Learning rate adaptation  X  produces larger fluctuations once the steady state is reache d. (6) and (7) suggest a decaying learning rate  X  ( N i ) examples with y implausible counters, and is robust enough to deal even with non-stationary distributions. over longer time periods with a non-vanishing learning rate will resemble a Beta ( a transformed to the log-odd domain. The parameters a this statistical model of w bution of w synaptic weight can therefore recover estimates of the virt ual sample sizes a weights and the squares of weights in  X  w application is the Naive Bayesian classifier, where a binary target variable x from a vector of multinomial variables x = h x conditionally independent given x probability theory the posterior probability ratio for x where m function I is defined as I ( true ) = 1 and I ( f alse ) = 0 . Let the m input variables x variable x the m The binary target variable x Substituting the state variables y Hence the optimal decision under the Naive Bayes assumption is The optimal weights w  X  simply learned as an unconditional log-odd). 3.1 Learning Bayesian decisions for arbitrary distributio ns x the node to be learned is x described by m + 1 (possibly empty) parent sets defined by The joint probability distribution on the variables x ated for x p ( x 0 = 1 , x ) p ( x 0 = 0 , x ) resulting preprocessing circuit as generalized preproces sing (GP). log-odd can be expressed as a sum of conditional log-odds onl y: log We now develop a suitable sparse encoding of of x spond to conditional log-odds of y the previously introduced population coding operator  X  is generalized such that  X  ( x creates a vector of length Q l identifies by its position in the vector the present assignme nt of the input variables x sum of log-odds of the target variable: Every synaptic weight w modification that the update is not only triggered by y tioning scenario, where the learner receives at each trial a n input x = h x one can rewrite this log-odd as y preprocessed input vector y can then be written as a linear sum where the optimal weights are w  X  Hebb rule (1): space, independently of the exploration policy (see [9]). 5.1 Results for prediction tasks measured the percentage of correct predictions after every update step. biological systems. We modified the weight update  X  w the interval  X  w approximation. 5.2 Results for action selection tasks tation of the Bayesian Hebb rule (noisy updates, uniformly d istributed in  X  w the same learning performance. w but it converges to the optimal policy in the long run. 5.3 A model for the experiment of Yang and Shadlen qualitatively similar to the policies adopted by monkeys H and J in [1] after learning. with regard to resulting behaviors.
 adapting probabilistic inference machine.
 Acknowledgments trian Science Fund FWF, project # P17229-N04, project # S9102-N04, and project # FP6-015879 (FACETS) as well as # FP7-216593 (SECO) of the European Union. References
