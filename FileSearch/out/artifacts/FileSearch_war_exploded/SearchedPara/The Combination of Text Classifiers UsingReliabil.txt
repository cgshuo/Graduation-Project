 1. Introduction Researchers have long pursued the promise of harnessing multiple text classifiers to syn-thesize a more accurate classification procedure via some combination of the outputs of the contributing classifiers. Studies of classifier combination have been motivated primarily by could leverage the distinct strengths of each method.

Classifiers can be combined in a variety of ways. In one approach, a text classifier is output scores or some combination of output scores and features considered in the analysis. Other procedures for combining classifiers consider inputs generated by the contributing classifiers. For example, in a voting analysis, a combination function considers the final best classification. In a finer-grained approach to combining multiple classifiers, the scores 68 BENNETT, DUMAIS AND HORVITZ generated by the contributing classifiers are taken as inputs to the combination function. Whichever approach to combination is employed, the creation of enhanced metaclassifiers perform in different informational contexts.

We have pursued the development of probabilistic combination procedures that hinge than rely solely on output scores or on the set of domain-level features employed in text-reliability. We borrow the reliability-indicator methodology from work presented initially by Toyama and Horvitz (2000) in the context of automated vision. They introduced the reliability-indicator learning and inference framework and showed how the approach could accuracy composite visual analysis. We have found that the reliability-indicator method-ology is useful in text classification for providing context-sensitive signals about accuracy that can be used to weave together multiple classifiers in a coherent probabilistic manner to boost overall accuracy.
 We will first review related work on the combination of text-classification procedures. Then, we introduce the use of reliability indicators in text classification, and show how we unigram, support vector machine (SVM), and decision-tree classifiers. We describe how we integrate the indicator variables with base-level features and scores output by classifiers to b uild metaclassifiers that improve text classification performance. We highlight our meth-odology and results by reviewing several sets of experiments. Finally, we summarize our contributions and discuss future directions. 2. Related work Appropriately combining information sources to form a more effective output than any of the individual sources is a problem that has been investigated in many fields. The challenges of integrating information have gone under the labels of diagnosis (Horvitz et al. 1988), pattern recognition (Duda et al. 2001), sensor fusion (Klein 1999), distributed data mining (Kargupta and Chan 2000), and a variety of ensemble methods (Dietterich 2000). Diagnosis centers on identifying disorders from multiple pieces of evidence, such as reasoning about probability distributions over a patient X  X  diseases from a set of symptoms and test results. P attern recognition and sensor fusion typically address challenges with integrating infor-mation from multiple modalities (e.g., auditory and visual) while distributed data mining addresses how results retrieved from distinct training data sets can be unified to provide one coherent view to the user. Ensemble methods first solve a classification or regression problem by creating multiple learners that each attempt to solve the task independently, then use the procedure specified by the particular ensemble method for selecting or weighting the individual learners. Ensemble methods include such techniques as Bayesian averaging, bagging, boosting, stacking, cascade generalization, hierarchical mixture of experts, and the work presented in this paper.
 COMBINATION OF TEXT CLASSIFIERS 69 labels from a set of predefined content-based categories. These categories may be primary etc. (Kessler et al. 1997)), or a variety of other distinctions (e.g.., normal e-mail vs. junk e-mail ,a s used by Sahami et al. (1998), or urgent e-mail vs. non-urgent e-mail ,a se xplored by Horvitz et al. (1999)). Text classification methods can thus provide a backend for many see Sebastiani (2002) for a broad survey of recent applications of machine learning to text classification.

The overlaying of multiple methodologies or representations has been employed in sev-eral areas of information retrieval. For example, previous research in information retrieval has demonstrated that retrieval effectiveness can be improved by using multiple, distinct representations (Bartell et al. 1994, Katzer et al. 1982, Rajashekar and Croft 1995), or by using multiple queries or search strategies (Belkin et al. 1993, Shaw and Fox 1995). In the realm of text classification, several researchers have achieved improvements in classifica-tion accuracy by combining different classifiers (Al-Kofahi et al. 2001, Hull et al. 1996, Larkey and Croft 1996, Li and Jain 1998, Yang et al. 2000). Similarly, several investigators have enhanced classification performance by applying many instances of the same classifier, such as boosting procedures (Schapire and Singer 2000, Weiss et al. 1999).
 basic policies for selecting the best classifier or for combining the output of multiple clas-sifiers. As some examples, Larkey and Croft (1996) used weighted linear combinations of system ranks or scores; Hull et al. (1996) used linear combinations of probabilities or log odds scores; Yang et al. (2000) used a linear combination of normalized scores; Li and Jain (1998) used voting and classifier selection techniques; and Lam and Lai (2001) used category-averaged features to pick a (potentially different) classifier to use for each category.

Larkey and Croft (1996) used rank-based measures of performance because they were interested in interactive systems in which a rank list of codes for each document would be displayed to users. Many other applications such as automatic routing or tagging require We focus on classifier combination to enhance such classification decisions. This goal is more challenging than the use of classifiers for document ranking. As an example, Hull et al. (1996) found that, although combination techniques were able to improve document classification decisions.
 our work centers on the use of a richer probabilistic combination of inputs, using com-bination functions learned with Bayesian and SVM learning methods. In this respect, our approach is similar to work by Ting and Witten (1999) in stacked generalization and Gama (1998a, 1998b) in cascade generalization, although they did not apply their approach to text problems. We also report baseline comparisons with voting and classifier-selection techniques. 70 BENNETT, DUMAIS AND HORVITZ 3. Problem approach Our work is distinguished from earlier combination approaches for text classification by (1) the use of expressive probabilistic dependency models to combine lower-level classifiers, leveraging special signaling variables, referred to as reliability indicators, and (2) a focus on measures of classification performance rather than the more common consideration of ranking. 3.1. Reliability indicators Previous approaches to classifier combination have typically limited the information con-sidered at the metalevel to the output of the classifiers (Ting and Witten 1999) and/or the a whole domain, an intuitive alternative is to identify the document-specific context that differentiates between regions where a base classifier has higher or lower reliability.
Figure 1 shows an example using four base classifiers: decision tree, SVM, na  X   X ve Bayes, and unigram. When given a test document as input, each of the four base classifiers outputs a probability distribution over possible class labels (depicted graphically as a histogram in the figure). The metaclassifier uses this information along with document context (to be described in more detail) to produce a final classification of the document.

We address the challenge of learning about the reliability of different classifiers in dif-ferent neighborhoods of the classification domain by introducing variables referred to as r eliability indicators which represent the analytic  X  X ontext X  of a specific document. A re-regions of a classification problem where a classifier performs relatively strongly or poorly. COMBINATION OF TEXT CLASSIFIERS 71
The reliability-indicator methodology was introduced by Toyama and Horvitz (2000) lighting, color, and the overall configuration of the visual scene. The authors introduced each of the modalities. To learn probabilistic models for combining the multiple modal-Bayesian network model with the ability to appropriately integrate the outputs from each of the visual modalities in real time, providing an overall higher-accuracy composite visual analysis.

The value of the indicator-variable methodology in machine vision stimulated us to e xplore the approach for representing and learning about reliability-dependent contexts in text classification problems. For the task of combining classifiers, we formulate and include sets of variables that hold promise as being related to the performance of the underlying and, thus, bypass the need to make ad hoc modifications to the base classifiers. This allows the metaclassifier to harness the reliability variables if they contain useful discriminatory information, and, if they do not, to fall back in a graceful manner to using the output of the base classifiers.

As an example, consider three types of documents where: (1) the words in the document are either uninformative or strongly associated with one class; (2) the words in the docu-ment are weakly associated with several disjoint classes; or (3) the words in the document are strongly associated with several disjoint classes. Classifiers (e.g., a unigram model) will sometimes demonstrate different patterns of error on these different document types. types, then we can assign the appropriate weight to the classifier X  X  output for this kind of document. We have pursued the formulation of reliability indicators that capture dif-ferent association patterns among words in documents and the structure of classes under consideration. We seek indicator variables that would allow us to learn context-sensitive reliabilities of classifiers, conditioned on the observed states of the variable in different settings.

To highlight the approach with a concrete example, figure 2 shows a portion of the type of combination function we can capture with the reliability-indicator methodology. The nodes on different branches of a decision tree include the values output by base classi-fiers, as well as the values of reliability indicators for the document being classified. The decision tree provides a probabilistic, context-sensitive combination rule indicated by the particular relevant branching of values of classifier scores and indicator variables. In this case, the portion of the tree displayed shows a classifier-combination function that considers thresholds on scores provided by a base-level linear SVM ( OutputOfSmox ) classifier and a base-level unigram classifier ( OutputOfUnigram ), and then uses the context established by 72 BENNETT, DUMAIS AND HORVITZ reliability-indicator variables ( UnigramVariance and %FavoringInClassAfterFS )to make a final decision about a classification. The annotations in the figure show the threshold the test, and a graphical representation of the probability distribution at the leaves. The likelihood of class membership is indicated by the length of the bars at the leaves of the tree.

The variable UnigramVariance represents the variance of unigram weights for words present in the current document. The intuition behind the formulation of this reliability-indicator variable is that the unigram classifier would show a tendency toward higher ac-curacies when there is low variance in weights. The variable %FavoringInClassAfterFS is the percentage of words (after feature selection) that occur more often in documents within a target class than in other classes. Classifiers that weight positive and negative evidence differently should be distinguished by this variable. Appendix A gives further details about the reliability indicators used in these experiments.

The indicator variables used in our studies represent an attempt to formulate states that capture influential contexts. We constructed variables to represent a variety of contexts that held promise as being predictive of accuracy. These include such variables as the number of across the positive vs. negative classes, and the mean and variance of classifier-specific weights.

We can broadly group reliability-indicator variables into one of four types, including v ariables that measure (1) the amount of information present in the original document, (2) the information loss or mismatch between the representation used by a classifier and the original document, (3) the sensitivity of the decision to evidence shift, and (4) some basic v oting statistics.

DocumentLength is an example of a reliability-indicator variable of type 1. The perfor-mance of classifiers is sometimes correlated with document length, because longer docu-ments give more information to use in making a classification. DocumentLength can also COMBINATION OF TEXT CLASSIFIERS 73 be informative because some classifiers will perform poorly over longer documents as they do not model the influence of document length on classification performance (e.g., they double count evidence and longer documents are more likely to deviate from a correct determination).

Pe r centRemoved serves as an example of type 2. This variable represents the percent of features removed in the process of feature selection. If most of the document was not represented by the feature set employed by a classifier, then some classifiers may be unre-liable. Others classifiers (e.g., decision trees that model missing attributes) may continue to be reliable. When the base classifiers are allowed to use different representations, type 2 features can play an even more important role.

An example of type 3 is the UnigramVariance v ariable. Low variance means the decision of the classifier is unlikely to change with a small change in the document content; high v ariance increases the chances that the decision would change with only a small change in the document.

Finally, NumVotingForClass or Pe r centAgreement are examples of type 4 reliability in-dicators. These simple voting statistics improve the metaclassifier search space (since the metaclassifier is given the base classifier decisions as input as well). For a two-class case the Pe r centAgreement v ariable may provide little extra information but for greater number a small number of classes or across a wide array. We found all four types of reliability indicators to be useful in the final combination scheme, and preliminary analyses did not indicate that any one type dominates in the combination models.

Beyond the key difference in the semantics of their usage, reliability-indicator variables the examples better than random. We also do not assume that classification confidence shows monotonicity trends as in classifiers. 3.2. STRIVE: Metaclassifier with reliability indicators We refer to our classifier combination learning and inference framework as STRIVE for St acked R eliability I ndicator V ariable E nsemble. We select this name because the approach can be viewed as essentially extending the stacking framework by introducing reliability indicators at the metalevel. The STRIVE architecture is depicted graphically in figure 4.
Our methodology maps the original classification task into a new learning problem. In the original learning problem (figure 3), the base classifiers simply predict the class from a w ord-based representation of the document, or more generally, each base classifier outputs a distribution (possibly unnormalized) over class labels. STRIVE adds another layer of learning to the base problem. A set of reliability-indicator functions use the words in the document and the classifier outputs to generate the reliability indicator values, r i , for a particular document. This process can be viewed as yielding a new representation of the document that consists of the values of the reliability indicators, as well as the outputs classification. This enables the metaclassifier to employ a model that uses the output of the 74 BENNETT, DUMAIS AND HORVITZ classification.

We require the outputs of the base classifiers to train the metaclassifier. Thus, we perform cross-validation over the training data and use the resulting base classifier predictions, ob-tained when an example serves as a validation item, as training inputs for the metaclassifier. function over the original data, then the resulting scheme can be viewed as a variant of cascade generalization (Gama 1998a). 4. Experimental analysis We performed a large number of experiments to test the value of probabilistic classifier combination with reliability-indicator variables. We now describe the corpora, methodology, and results.
 COMBINATION OF TEXT CLASSIFIERS 75 4.1. Data We e xamined several corpora, including the MSN Web Directory , Reuters , and TREC-AP. 4.1.1. MSN Web Directory. The MSN Web Directory is a large collection of heterogeneous web pages (from a May 1999 web snapshot) that have been hierarchically classified. We used the same train/test split of 50078/10024 documents as that reported by Dumais and Chen (2000).
 set, they range from 1 . 14 to 21 . 54%. The classes are general subject categories such as zero or more categories.

Fo r the experiments below, we used only the top 1000 words with highest mutual informa-tion for each class; approximately 195 K words appear in at least three training documents. 4.1.2. Reuters. The Reuters 21578 corpus (Lewis 1997) contains Reuters news articles from 1987. For this data set, we used the ModApte standard train/test split of 9603/3299 documents (8676 unused documents). The classes are economic subjects (e.g.,  X  X cq X  for acquisitions,  X  X arn X  for earnings, etc.) that human taggers applied to the document; a docu-ment may have multiple subjects. There are actually 135 classes in this domain (only 90 of which occur in the training and testing set); however, we only examined the ten most fre-quent classes since small numbers of testing e xamples makes estimating some performance measures unreliable due to high variance. Limiting to the ten largest classes allows us to compare our results to previously published results (Zhang and Oles 2001, Dumais et al. 1998, Joachims 1998, McCallum and Nigam 1998, Platt 1999b). The class proportions in
Fo r the experiments below we used only the top 300 words with highest mutual informa-tion for each class; approximately 15 K words appear in at least three training documents. 4.1.3. TREC-AP. The TREC-AP corpus is a collection of AP news stories from 1988 to 1990. We used the same train/test split of 142791/66992 documents that was used by Lewis et al. (1996). As described by Lewis and Gale (1994) (see also Lewis (1995)), the categories are defined by keywords in a keyword field. The title and body fields are used in the experiments below. There are twenty categories in total.

The frequencies of the twenty classes are the same as those reported by Lewis et al. they range from 0 . 03 to 4 . 32%.

Fo r the experiments described below, we use only the top 1000 words with the highest mutual information for each class; approximately 123 K words appear in at least 3 training documents. 76 BENNETT, DUMAIS AND HORVITZ 4.2. Classifiers We employed several base-level classifiers and classifier combination methods in our com-parative studies. We review the classifiers and combination methods here. 4.2.1. Base classifiers. In an attempt to isolate the benefits gained from the probabilistic combination of classifiers with reliability indicators, we worked to keep the representa-representations) would only improve the performance as this would likely decorrelate the performance of the base classifiers. We selected four classifiers that have been used tra-ditionally for text classification: decision trees, linear SVMs, na  X   X ve Bayes, and a unigram classifier.

Fo r the decision-tree implementation, we employed the WinMine decision networks toolkit and refer to this as Dnet below (WinMine Toolkit v1.0, 2001). Dnet b uilds decision trees using a Bayesian machine learning algorithm (Chickering et al. 1997, Heckerman et estimates, we found that Dnet models usually perform acceptably for the goal of minimizing error rate. However, we found that the performance of Dnet with regard to other measures is sometimes poor.
 Fo r linear SVMs, we used the Smox toolkit which is based on Platt X  X  S equential M inimal O ptimization algorithm (Platt 1999a). We have experimented with both binary and contin-uous feature representations, and both perform at approximately the same level of accuracy. In order to keep the base representations used by each classifier as similar as possible, we used a continuous model.

The na  X   X ve Bayes classifier has also been referred to as a multivariate Bernoulli model. In using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.

The unigram classifier uses probability estimates from a unigram language model. This estimates are smoothed in a similar fashion to smoothing in the na  X   X ve Bayes classifier. 4.2.2. Basic combination methods. We performed experiments to explore a variety of classifier-combination methods. We considered several different combination procedures. problem, based on the one that performed best for a validation set. We refer to this method as the Best By Class method.
 Another combination method centers on taking a majority vote of the base classifiers. This approach is perhaps the most popular methodology used for the combination of text classifiers. When performing a majority vote, ties can be broken in a variety of ways (e.g., breaking ties by always voting for in class ). We experimented with several variants of this method, but we only present results here for the method which relies on breaking ties by v oting with the Best By Class classifier as this procedure nearly always outperformed the other majority vote methods. We refer to this method as Majority BBC .
 COMBINATION OF TEXT CLASSIFIERS 77 4.2.3. Hierarchical combination methods Stacking. Finally, we investigate several variants of the hierarchical models described ear-lier. As mentioned above, omitting the reliability-indicator variables transforms STRIVE to a stacking methodology (Ting and Witten 1999, Wolpert 1992). We refer to these classifiers below as Stack-X where X is replaced by the first letter of the classifier that is performing the metaclassification. Therefore, Stack-D uses a decision tree as the metaclassifier, and Stack-S uses a linear SVM as the metaclassifier. We note that Stack-S is also a weighted linear combination method since it is based on a linear SVM and uses only the classifier outputs.

We found it was challenging to learn the weights for an SVM when the inputs have v astly different scales. At times, it is not possible to identify good weights. To address the problem of handling inputs with greatly varying scales, we use an input normalization procedure: We normalize the inputs to the metaclassifiers to have zero mean and unit standard deviation. In order to perform consistent comparisons, we perform the same alteration for the decision-tree learners is relatively minimal (and has both positive and negative influences). To denote the metaclassifiers whose inputs have been normalized in this manner, we append  X ( norm ) X  to their names.
 STRIVE. Similar to the notation described above, we add a letter to STRIVE to denote the particular metaclassifier method being used. So, STRIVE-D is the STRIVE framework using Dnet as a metaclassifier. For comparison to the stacking methods, we evaluate STRIVE-D and STRIVE-S . Normalization, as above, is again noted by appending  X ( norm ) X  to the system names.

The experiments reported here use a total of 49 reliability indicators (including those specific examples given in Section 3.1). The full list of reliability indicators is described in detail in Appendix A. These reliability indicators were formulated by hand as an initial pass at representing potentially valuable contexts. We are currently taking a closer look at fundamental informational properties of different reliability indicators and have examined procedures for identifying new reliability indicators. We delve more deeply into the nature and authoring of reliability indicators in forthcoming work. 4.2.4. BestSelect classifier. To study the effectiveness of the STRIVE methodology, we formulated a simple optimal combination approach as a point of reference. Such an upper bound can be useful as a benchmark in experiments with classifier combination procedures. This bound follows quite naturally, when classifier combination is formulated as the process of selecting the best base classifier, on a per-example basis.

To classify a given document, if any of the classifiers correctly predict that document X  X  class, the best combination would select any of the correct classifiers. Thus, such a clas-sification combination errs only when all of the base classifiers are incorrect. We refer to dom, the BestSelect is the theoretical upper-bound on performance when combining a set of classifiers in a selection framework . 78 BENNETT, DUMAIS AND HORVITZ
We note that we are not using a pure selection approach, as our framework allows the classifiers are not better than random (or are logically dependent), such an upper bound may be uninformatively loose. Even though we are not working in a pure selection framework, we found it is rarely the case the metaclassifier outputs a prediction which none of the base classifiers made. Therefore, we have employed this BestSelect bound to assist with understanding the performance of STRIVE . 4.3. Performance measures To compare the performance of the classification methods we look at a set of standard performance measures. The F1 measure (van Rijsbergen 1979, Yang and Liu 1999) is the
Actual Positives .F or F1, we can either macro-average or micro-average. In macro-averaging, the score is computed separately for each class and then arithmetically averaged; this tends to weight rare classes more heavily. Micro-averaged values are computed directly from the binary decisions over all classes; this places more weight on the common classes. We ev aluated the systems with both macro and micro averaged F1.
 We can often assess a cost function in classification settings that can be described as tive classification and FN is the cost of a false negative classification. The most commonly importance of varying cost functions has been recognized by many researchers because ap-plications rarely have equal costs for different types of errors (Provost and Fawcett 2001). In order to assess how sensitive performance is to the utility measure, we considered results for C (10 , 1) and C (1 , 10).

In addition, we computed and displayed a receiver-operating characteristic (ROC) curve, which represents the performance of a classifier under any linear utility function (Provost and Fawcett 2001). We report results on the area under the ROC curve as an attempt to summarize the linear utility space of functions. 4.4. Experimental methodology As the categories under consideration in the experiments are not mutually exclusive, the clas-sification was carried out by training n binary classifiers, where n is the number of classes. Decision thresholds for each classifier were set by optimizing them for each performance of the separate performance measures (and for each class). This ensures that the base classi-fiers are as competitive as possible across the various measures. For the micro performance measures, obtaining truly optimal performance requires optimizing all the thresholds in a corpus in conjunction; we have taken the more computationally efficient approach of using the macro-optimized thresholds (i.e., each class X  X  threshold is set independently from the thresholds for the other classes).
 COMBINATION OF TEXT CLASSIFIERS 79
To generate the data for training the metaclassifier (i.e., reliability indicators, classifier each of the corpora. The data set obtained through this process was then used to train the metaclassifiers. Finally, the resulting metaclassifiers were applied to the separate testing data described above. 4.5. Results T ables 1, 2 and 3, present the main performance results over the three corpora. In terms of the various performance measures, better performance is indicated by larger F1 or ROC area values or by smaller C ( FP , FN )v alues. The best performance (ignoring BestSelect )in each column is given in bold.

To determine statistical significance for the macro-averaged measures, a one-sided macro sign test and macro t -test were performed (Yang and Liu 1999). For micro-F1, a one-sided micro sign test was performed (Yang and Liu 1999). Differences with a p -level above 0.05 were not considered statistically significant.

We do not explicitly report significance results for the t -test comparisons over the main performance results; instead, our analysis follows the macro and micro sign test which yield more conservative comparisons (i.e., the t -test primarily increased the number of differences found to be significant in the tables).

The classifier combinations are annotated to indicate the results of the macro sign test and, for micro F1, the micro sign test. A  X  indicates the method significantly outperforms 80 BENNETT, DUMAIS AND HORVITZ COMBINATION OF TEXT CLASSIFIERS 81 a  X  indicates that the method outperforms the basic combination methods. Results for the remaining sign test comparisons are omitted for brevity.

T ables 4, 5 and 6, present the performance results for additional experiments (described in Section 4.6.3) we conducted to determine how much could be gained by incorporating the 82 BENNETT, DUMAIS AND HORVITZ classifier outputs, as in STRIVE. Thus, this ablation experiment aims to empirically examine how much first-order information the reliability indicators have versus their information when conditioned on the other classifier outputs. The best performance in each column is given in bold.

The significance tests for this ablation experiment are summarized as an ordering over the systems in Table 7. For each performance measure, the systems are ordered from best (left) to worst (right). When all systems above a performance level significantly outper-according to the macro sign test, a  X   X  X  s used to separate the columns; when significant according to the macro t -test a,  X   X  X  s used as the column-separator; when significant ac-cording to both tests, a  X   X  X  s used. For micro F1, which uses only a micro sign test, a  X   X  X s used. 4.6. Discussion First, we note that the base classifiers are competitive and consistent with the previously reported results over these corpora (Zhang and Oles 2001, Dumais and Chen 2000, Du-mais et al. 1998, Joachims 1998, Lewis 1995, Lewis and Gale 1994, McCallum and Nigam 1998). 1 Furthermore, the fact that the linear SVM Smox tends to be the best base clas-sifier is consistent with the literature (Dumais et al. 1998, Joachims 1998, Yang and Liu 1999). 4.6.1. MSN Web Directory. Examining the main results for the MSN Web Directory corpus in Table 1 highlights several points. First, the basic combiners have only one significant win, C(1,10) for the Majority BBC approach. The results directly support the idea that the performance of a very good learner ( Smox ) tends to be diminished when combined via a majority vote scheme with weak learners; in addition, the win most likely results from the When false negatives are weighed more heavily, the shift toward predicting positive helps reduce the number of false negatives.

Next, we see that Stacking posts several significant wins and appears to have some advan-tages over the base classifiers. However, the Stacking combination shows little significant improvement over the basic combination methods.

STRIVE-D and STRIVE-S ( norm ) show advantages that are robust across a variety of performance measures. Each shows a small (about 5% error reduction) but consis-tent improvement across a variety of performance measures. When compared to the best theoretical performance that could be achieved by a per-example selection model using vided by the STRIVE combination methods is an even greater portion of the total possible reduction.

As can be inferred from the sign tests, these results are very consistent across classes. For e xample, by the ROC area measure of performance, STRIVE-D beats the base classifiers and basic combiners on 13/13 classes, and it beats the stacking methods on 12/13 classes. COMBINATION OF TEXT CLASSIFIERS 83 84 BENNETT, DUMAIS AND HORVITZ The notable exception is the performance of STRIVE-S ( norm )o nR OC area; graphical inspection of the ROC curves suggests this result arises because too much weight is being placed on the strong classifier for a curve.

Often, there is a crossover in the ROC curve between two of the base classifiers further out on the false-positives axis. Most utility measures in practice correspond to the early part of the curve (this depends on the particular features of the given curve). The Smox metaclassifier sometimes seems to lock onto the classifier that is strong in the early portion of the curve and loses out on the later part of the curve. Since this portion of the curve rarely matters, one could consider using an abbreviated version of curve area to assess systems.
 In figure 5, we can see that the two STRIVE v ariants dominate the four base classifiers. In fact, STRIVE-D dominates (i.e., its quality is greater than any other curve at every point) most of the MSN Web Directory corpus. We also can see (note the truncated scale) the base classifiers catching up with STRIVE-S ( norm )o n the right side of the curve. The base classifiers, in fact, do surpass STRIVE-S ( norm ). As a result, STRIVE-D is usually a more than false positives.

In some cases, we can develop an understanding of why the decision tree is more ap-tree establishes a score region for Smox and a score region for Dnet where the reliability COMBINATION OF TEXT CLASSIFIERS 85 indicators give further information about how to classify an example. Since a linear SVM is a weighted sum over the inputs, it cannot represent crossovers that are dependent on tion present in other variables to try to distinguish these regions. Higher-order polynomial k ernels are one way to allow an SVM to represent this type of information.

Finally, we performed an ablation experiment to determine how STRIVE w ould be-have without the presence of an extremely strong base classifier. Since Smox frequently outperforms the other base classifiers, we investigated the level of performance STRIVE could obtain if the output of Smox wa s omitted from the inputs given to the metaclassifier, STRIVE-D ( norm , omit Smox ). The results show that when we omit the base classifier Smox , the resulting combination improves by a large margin over the remaining base methods; This suggests that there are not enough indicator variables tied to Smox  X  X  behavior, or al-complementary fashion. 4.6.2. Reuters and TREC-AP. The results for Reuters and TREC-AP in Tables 2 and 3 are consistent with the above analysis. We note that the level of improvement tends to be less pronounced for these corpora. Since it is common in the literature to show results for each class of the top ten largest classes in Reuters, we provide a detailed listing in Appendix B. derstand to what extent their information can be directly used to improve classification these issues, we performed an experiment where we added all of the reliability indica-tors to the standard document representation and built a model using Dnet . The result-almost all of the reliability indicators 2 and the standard document representation (denoted Dnet + RIV X  ). We compare these systems to their most similar counterparts presented in the earlier results: the base classifier ( Dnet ); the stacking method using the decision STRIVE-D .
 The expectation is that Dnet will be outperformed by all other systems, and Dnet + RIV X  will be outperformed by all others except Dnet .A quick examination of the re-sults indicates that this is generally the case according to nearly all of the performance measures. The amount of improvement of Dnet + RIV X  over Dnet indicates the extent to which the reliability indicators give information that can be used directly to improve classification X  X hich is quite large when compared to Dnet .H ow ev er, the remaining methods still often show significant improvement beyond this when all the base classi-86 BENNETT, DUMAIS AND HORVITZ function which places a heavy penalty on false positives, i.e., emphasizing extremely high precision.
 representational inhomogeneity: the majority of words that define support vectors are not present in any given document, but all the reliability indicators have values for documents. Thus, the reliability indicators dominate the influence of words by their sheer magnitude and not necessarily by the reliability information they carry. This highlights the value of hierarchical modeling for classifier combination; hierarchical modeling allows variables of the same type to be modeled in the same layers. This highlights that one major advantage to hierarchically combining systems is that variables of the same type can be modeled in the same layer. 5. Future work We are excited about the opportunities for probabilistic combination of multiple classifiers with reliability indicators. We are pursuing several research directions. Foremost, we believe that a functional search that generates and tests a larger number of reliability indicators could provide valuable sets of informative reliability indicators. Also, we are interested in e xploring the value of introducing more flexibility into the set of base classifiers that are being used. In the experiments we described, the classifiers were purposely held constant we will allow representations to vary to induce more variety among the base classifiers metaclassifiers. The metaclassifier should be a classifier that handles correlated input well (e.g., use of maximum entropy (Nigam et al. 1999)) as classifiers performing better than random will be necessarily correlated.

In addition, we believe the STRIVE framework can be used to elucidate deeper, information-theoretic foundations of how classifiers leverage information in text classi-fication. Currently, we build one metaclassifier for each binary topic problem in a corpus (e.g., Acquisitions vs. not Acquisitions ). However, a promising abstraction of the STRIVE framework enables us to use model structure or parameters learned using data from one tested by coalescing metalevel training data from different binary discrimination tasks and b uilding one, more general, metaclassifier. With this approach, we treat the metaclassifier (e.g., Acquisitions vs. not Acquisitions )t o the problem of discriminating topic membership used as the representation of topic-specific knowledge, while the metaclassifier provides metaclassifier still improves performance of base classifiers, then the reliability indicators will have inductively defined informational properties for classifier combination across all of the text problems considered. Such an extension is only possible if we generalize the COMBINATION OF TEXT CLASSIFIERS 87 when  X  X hares X  occurs in a document in the Acquisitions discrimination task and  X  X orn X  occurs in a document in the Corn Futures discrimination task. One task-invariant repre-sentation of context at the metalevel might transform both of these to: Is the word with maximum mutual information for the current task present in this document ? This represen-tation enables the metaclassifier to use information about how document-specific context influences topic discrimination across a wide variety of text classification tasks. Recent e xperiments have demonstrated that this methodology can be valuable in enhancing the performance of classifiers (Bennett et al. 2003). We are continuing to pursue this promising e xtension. 6. Summary and conclusions We reviewed a methodology for building a metaclassifier for text documents that centers leverages reliability-indicator variables. Reliability indicators provide information about the context-sensitive nature of classifier reliability, informing a metaclassifier about the duced the STRIVE methodology that uses reliability indicators in a hierarchical combina-tion model and reviewed comparative studies comparing STRIVE with other combination mechanisms.
 We conducted experimental evaluations over three text-classification corpora (MSN W eb, Reuters 21578, and TREC-AP) with a variety of performance measures. These measures were selected to determine the robustness of the classification procedures un-der different misclassification penalties. The empirical evaluations support the conclusion can weaken the best classifier X  X  performance. In contrast, in all of these corpora across all measures, the STRIVE methodology was competitive, failing to produce the top per-former in only two instances (the two skewed linear utility measures in the TREC-AP cor-pus). Furthermore, on a class-by-class basis, the STRIVE methodology produced receiver-operating characteristic curves that dominated the other classifiers in nearly every class of the MSN Web corpus X  X emonstrating that it provides the best choice for any possi-ble linear utility function in this corpus. In conclusion, the experiments show that stack-ing and STRIVE provide robust combination schemes across a variety of performance measures.
 A ppendix A: Detailed descriptions of inputs to STRIVE This appendix gives details for all of the inputs to STRIVE X  X ncluding the base classifier outputs in addition to all 49 reliability-indicator variables. 88 BENNETT, DUMAIS AND HORVITZ A.1. Outputs of base classifiers We considered the outputs of four base classifiers as inputs to STRIVE .  X  OutputOfDnet This is the output of the decision tree built using the Dnet classifier (available as the
W inMine toolkit (WinMine Toolkit v1.0 2001)). Its value is the estimated probability at the leaf node of belonging to the class.  X  OutputOfSmox
This is the output of the Linear SVM model built using the Smox toolkit. It is a probability estimate of class membership obtained via a monotonic transformation using a fitted sigmoid (Platt 1999) of the raw score of the SVM.  X  OutputOfNa  X   X vebayes
This is the output of the na  X   X ve Bayes model built using a multivariate Bernoulli repre-sentation (i.e. only feature presence/absence in an example is modeled) (McCallum and
Nigam 1998). It is the log-odds (or logistic) of the model X  X  probability estimate of class enough machine floating-point precision is not available to preserve the ranking induced by this model (they cluster too closely to zero and one).  X  OutputOfUnigram
This is the output of the unigram model (also referred to as a multinomial model (McCallum and Nigam 1998). It is the log-odds (or logistic) of the model X  X  proba-is not used because typically enough machine floating-point precision is not available one).
 A.2. Reliability indicator variables Indicator variables are currently broken roughly into one of four types:  X 
Amount of information present in the original document (15/15);  X 
Information loss or mismatch between representations (12/12);  X 
Sensitivity of the decision to evidence shift (20/6);  X  Basic voting statistics (2/2).

The first number in parentheses is the number of variables of this type that were present in the experimental evaluation. The second number is the number of those whose effect on the metaclassifier model appears to be non-negligible. We group the reliability indicators into We note that this is only a soft clustering; some reliability indicators may provide context information in more than one way.

Several of the variables listed below have an instantiation for each class in a learning problem (the variable counts we report tallies each instance separately). For these variables COMBINATION OF TEXT CLASSIFIERS 89 classifier for each topic, then our experiments have a P ositive and a Negative class version. In a two-class problem, the values of the two instantiations may be redundant. We have, however, retained each since in polyclass (3 or more classes) discrimination they are more distinct.
 is 49.
 A.2.1. Type 1: Amount of information present in the original document. There are 15 counting instantiations for each class).  X  (1) DocumentLength
The number of words in a document before feature selection. Presumably longer doc-uments provide more information to base a decision upon. Therefore, longer docu-ments will lead to more reliable decisions (when DocumentLength is correctly modeled).
Alternatively, models that do not correctly normalize for document length may be less reliable for extreme lengths (short or long) of documents.  X  (1) EffectiveDocumentLength
DocumentLength minus the number of out-of-vocabulary words in the document. Since a model cannot generalize strongly (other than smoothing) for features that were not seen in the training set, this variable may be a better indicator of information present in the document than DocumentLength .  X  (1) NumUniqueWords
Number of distinct tokens in a document, i.e. |{ w | w  X  document }| (as opposed to length which counts repeats of a token in a document). The motivation is similar to Document-
Length ,b ut here the variable is only counting each new word as an indicator of new information.  X  (1) EffectiveUniqueWords
NumUniqueWords minus the number of unique out-of-vocabulary words. This is the analogue of EffectiveDocumentLength and is included for similar reasons.  X  (1) Pe r centUnique
UniqueWords / DocumentLength . This can also be seen as 1/average number of times a w ord is repeated in a document. Close to 1 means very few words (if any) are repeated in the document; close to 0 means the documents consists of very few unique words (possibly repeated many times). This is essentially a normalized version of NumUnique-
Wo rd s ;h ow ev er this variable will show high variance for short documents. The intuition here is that more complex documents, while providing more information, also might be more difficult to classify (since they may have many features each carrying some small weight). 90 BENNETT, DUMAIS AND HORVITZ  X  (1) Pe r centOOV
The percentage of the words in a document which weren X  X  seen in the training set. It is equal to the number of out-of-vocabulary words divided by DocumentLength . Similar to Pe r centUnique , this variable can show high variance for short values. The intuition here is that the more novel words a document contains the more likely a classifier is to incorrectly classify the document into the a priori prevalent class (typically unseen w ords slightly favor minority classes since we have less samples from them). This is av ariable that essentially allows a global smoothing model to be induced. Its range is [0 , 1]. Therefore, as it approaches 1, we would expect minority classes to be more likely than our base models might estimate.  X  (1) Pe r centUniqueOOV
The percentage of the words (not counting duplicates) in a document which weren X  X  seen in the training set. This is the distinct token analogue for Pe r centOOV .A gain, the motivations are similar to just using a different information model.  X  (2) Pe r centIn { Class } BeforeFS
Of all words occurring in the training set (i.e. out-of-vocabulary words are ignored), longing to the class. It is equal to the number of words that occurred in the class be-fore feature selection divided by EffectiveDocumentLength . Similar to Pe r centOOV , liable. For the binary case with a negative class that effectively groups many classes together, this isn X  X  quite expected with respect to Pe r centInNegBeforeFS (since pre-dictions of  X  X egative X  would almost always expected to be more reliable under that assumption).  X  (2) UpercentIn { Class } BeforeFS
Of all words occurring in the training set (i.e. out-of-vocabulary words are ignored), the percentage of unique words in a document that occurred at least once in examples belonging to the class. This is the analogue to Pe r centIn { Class } BeforeFS using unique tokens as the basis for the information model.  X  (2) %Favoring { Class } BeforeFS
Of all words occurring in the training set, the percentage of words in a document that occurred more times in examples belonging to the class than in examples not belonging to the class. This is essentially a rough statistic for an unnormalized unigram model (tied slightly into the smoothing related variables discussed above) that gives a very rough sense of the evidential weight of the original document.  X  (2) U%Favoring { Class } BeforeFS
Of all words occurring in the training set, the percentage of unique words in a docu-ment that occurred more times in examples belonging to the class than in examples not belonging to the class. This is the analogue to %Favoring { Class } BeforeFS . A2.2. Type 2: Information loss or mismatch between representations. There are 12 COMBINATION OF TEXT CLASSIFIERS 91 loss.  X  (1) DocumentLengthAfterFS
The number of words in a document after out-of-vocabulary words have been removed and feature selection was performed. Similar to DocumentLength , this is the measure of information that the classifier actually sees with respect to this document. Currently, it X  X  assumed the metaclassifier can combine this with DocumentLength to infer information loss.  X  (1) UniqueAfterFS
The number of unique words remaining in a document after out-of-vocabulary words have been removed and feature selection was performed. This is the distinct token ana-logue of DocumentLengthAfterFS and is similarly expected to be used in conjunction with NumUniqueWords as a gauge of information loss.  X  (1) Pe r centRemoved
The percentage of a document that was discarded because it was out-of-vocabulary or removed by feature selection. It can have high variance for short documents. The
Removed .  X  (1) UniquePercentRemoved
The percentage of unique words in a document that were discarded because they were out-of-vocabulary or removed by feature selection. The distinct token analogue of
Pe r centRemoved where the information model is unique words.  X  (2) Pe r centIn { Class } AfterFS
Of all words occurring in the training set, the percentage of words remaining in a document after feature selection that occurred at least once in examples in the class. Together with
Pe r centIn { Class } BeforeFS , allows the model to inductively model shift in information content because of feature selection.  X  (2) UpercentIn { Class } AfterFS
Of all words occurring in the training set, the percentage of unique words remaining in a document (after feature selection) that occurred at least once in the class. Again, this is expected to be used in conjunction with UpercentIn { Class } BeforeFS to model information loss.  X  (2) %Favoring { Class } AfterFS
Of all words occurring in the training set, the percentage of words remaining in a document (after feature selection) that occurred more times in examples in the class than in exam-ples not in the class. Like its BeforeFS counterpart, it is essentially like an unnormalized unigram model. We expect that it can be used in conjunction with %Favoring { Class } -
BeforeFS to measure how a feature selection method may have biased the information for a given document toward a particular class.  X  (2) U%Favoring { Class } AfterFS
Of all words occurring in the training set, the percentage of distinct words remaining in a document after feature selection that occurred more times in examples in the class than e xamples not in the class. 92 BENNETT, DUMAIS AND HORVITZ v ariables that fall into this class. Ten of them are based on a unigram model (where only the multivariate Bernoulli model (where a document is modeled in terms of the probabilities of each word X  X  presence/absence). 3  X  (2) UnigramVariance , Na  X   X veBayesVariance the feature values (word occurrences or presence/absence) in a specific document. If the v ariance is close to zero, that means all of the words tended to point toward one class.
As the variance increases, this means there was a large skew in the amount of evidence present by the various words (possibly strong words pulling toward two classes). The this variable increases. To apply this to polyclass learning problems, there should be one second being preferred.  X  (4) UnigramVarianceOfLogOfWordGiven { Class } , Na  X   X veBayesVarianceOfLogOfWord -Given { Class }
These variables represent the variance of the log of the conditional probabilities of a word given a class. For example UnigramVarianceOfLogOfWordGivenPositive is the variance of log P ( w | Class = P ositive ) ove r the words in the document.  X  (4) UnigramMeanOfWordGiven { Class } , Na  X   X veBayesMeanOfWordGiven { Class }
These variables represent the mean of the conditional probabilities of a word given a class. For example UnigramMean OfWordGivenPositive is the mean of P ( w | Class = result.  X  (4) UnigramMeanOfLogOfWordGiven { Class } , Na  X   X veBayesMeanOfLogOfWordGiven-{ Class } These variables represent the mean of the conditional probabilities of a word given a class. Fo re xample UnigramMeanOfLogOfWordGivenPositive is the mean of log P ( w | Class =
P ositive ) ove r the words in the document. This essentially provides a log scaling of the v ariables ( UnigramMeanOfWordGiven { Class } , Na  X   X veBayesMeanOfWordGiven { Class } ) described above and is only a factor if the metaclassifier is sensitive to the scaling. None of these variables had much impact on the metaclassifier model and have been eliminated from future study as a result.  X  (4) UnigramVarianceOfWordGiven { Class } , Na  X   X veBayesVarianceOfWordGiven { Class }
These variables represent the variance of the log of the conditional probabilities of a w ord given a class. For example UnigramVarianceOfWordGivenPositive is the variance of P ( w | Class = P ositive ) ove r the words in the document. This essentially provides a COMBINATION OF TEXT CLASSIFIERS 93 log scaling of the variables ( UnigramVarianceOfLogOfWordGiven { Class } , Na  X   X veBayes-
V arianceOfLogOfWordGiven { Class } ) described in the Type 3 section and is only a factor if the metaclassifier is sensitive to the scaling. We have found that the metaclassifiers have not been sensitive to this scaling, and these variables have been eliminated from future study while their counterparts are being retained.  X  (2) UnigramMeanOfLogOfRatioOfWordGivenClass , Na  X   X veBayesMeanOfLogOfRatioOf-Wo r dGivenClass the word gives evidence to the positive class ( c ), and if it is less than zero, the word these weights for the feature values (word occurrences or presence/absence) in a spe-cific document. These variables are being eliminated from future study because their information is highly correlated with the output scores of the unigram and na  X   X ve Bayes classifiers.
 A.2.4. Type 4: Basic voting statistics. There are 2 reliability indicator variables whose primary type is considered to be this type. Both of these were introduced mainly to reduce the data required to learn m -of-n rules in the decision tree metaclassifier.  X  (1) Pe r centPredictingPositive
We refer to this in the main text as NumVotingForClass . This variable is the percentage of base classifiers (out of all base classifiers) that vote for membership in the class. In our experimental evaluation, we only used one instantiation of this variable. This was added to help the search space since learning this m -of-n type of feature can require significant data for a decision tree learning algorithm (unless it is specifically altered for this).  X  (1) Pe r centAgreeWBest This variable is referred to as Pe r centAgreement in the main text. For polyclass problems,
Pe r centAgreement can be used to indicate how many classes the classifiers fracture their v otes among. Since there are only two classes here, we altered it to indicate the percent agreement with the best base classifier (the classifier that performed best over the training data).
 A ppendix B: Detailed results for reuters Since it is common in the literature to provide detailed performance results for the top ten most frequent classes in Reuters 21578, we present a breakdown by class here. For the reader X  X  benefit, we also give after the class name the number of training documents and testing documents with membership in the class ( Tr ain/Test ). The best result per class per performance measure (excluding BestSelect )i si n bold. 94 BENNETT, DUMAIS AND HORVITZ COMBINATION OF TEXT CLASSIFIERS 95 96 BENNETT, DUMAIS AND HORVITZ COMBINATION OF TEXT CLASSIFIERS 97 Acknowledgments We thank Max Chickering and Robert Rounthwaite for their special support of the WinMine toolkit, and John Platt for advice and code support for the linear SVM classifier. We would also like to thank the anonymous reviewers for the useful suggestions they provided. Notes 98 BENNETT, DUMAIS AND HORVITZ References COMBINATION OF TEXT CLASSIFIERS 99 100 BENNETT, DUMAIS AND HORVITZ
