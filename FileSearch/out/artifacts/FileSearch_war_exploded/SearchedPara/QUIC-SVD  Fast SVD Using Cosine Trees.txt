 The Singular Value Decomposition (SVD) is a fundamental linear algebraic operation whose abun-dant useful properties have placed it at the computational center of many methods in machine learn-ing and related fields. Principal component analysis (PCA) and its kernel and nonlinear variants are prominent examples, and countless other instances are found in manifold and metric learning, clustering, natural language processing/search, collaborative filtering, bioinformatics and more. Notwithstanding the utility of the SVD, it is critically bottlenecked by a computational complexity that renders it impractical on massive datasets. Yet massive datasets are increasingly common in applications, many of which require real-time responsiveness. Such applications could use SVD-based methods more liberally if the SVD were not so slow to compute. We present a new method, QUIC-SVD, for fast, sample-based SVD approximation with automatic relative error control. This algorithm is based on a new type of data partitioning tree, the cosine tree, that shows excellent ability to home in on the subspaces needed for good SVD approximation. We demonstrate several-order-of-magnitude speedups on medium-sized datasets, and verify that approximation error is properly controlled. Based on these results, QUIC-SVD seems able to help address the scale of modern problems and datasets, with the potential to benefit a wide array of methods and applications. For A  X  R m  X  n , we write A ( i ) for the i th row of A and A ( j ) for the j th column. We use O m  X  n to represent the subset of R m  X  n whose columns are orthonormal. Since the columns of V  X  O m  X  n are an orthonormal basis, we sometimes use expressions such as  X  X he subspace V  X  to refer to the subspace spanned by the columns of V . Throughout this paper we assume m  X  n , such that sampling rows gives bigger speedup than sampling columns. This is no loss of generality, since whenever m &lt; n we can perform SVD on the transpose, then swap U and V to get the SVD of the original matrix. Alternatively, row-sampling-based methods have analogous column-sampling versions that can be used in place of transposition; we leave this implicit and develop only the row-sampling version of our algorithm. Algorithm 1 Optimal approximate SVD within a row subspace b V .
 The singular value decomposition is defined as follows: Definition 1. Let A be an m  X  n real matrix of rank  X  . Then there exists a factorization of the form where U and V each have orthonormal columns and are of size m  X   X  and n  X   X  , respectively, and  X  is diagonal with entries  X  1  X   X  2  X  ...  X   X   X  &gt; 0 .
 Equivalently, we can write the SVD as a weighted sum of rank-one outer products: A = P are referred to as the left and right singular vectors, while the weights  X  i are the singular values. Though it is sometimes overkill, the SVD can be used to solve essentially any problem in numerical linear algebra. Instances of such problems abound in machine learning.
 Given m  X  n , the exact SVD has O ( mn 2 ) runtime ( O ( n 3 ) for square matrices). This is highly unscalable, rendering exact SVD impractical for large datasets. However, it is often the case that good approximations can be found using subsets of the rows or columns. Of significant interest are low-rank approximations to a matrix. The optimal k -rank approximation, in the sense of minimizing the squared error || A  X  b A || 2 F , is the k -rank truncation of the SVD: A k is the projection of A  X  X  rows onto the subspace spanned by the top k right singular vectors, i.e., A k = AV k V T k . The optimality of A k implies that the columns of V k span the subspace of dimension at most k in which the squared error of A  X  X  row-wise projection is minimized. This leads us to a formulation of SVD approximation in which we seek to find a subspace in which A  X  X  projection has sufficiently low error, then perform the SVD of A in that subspace. If the subspace is substantially lower in rank/dimension than A , the SVD of the projection can be computed significantly faster than the SVD of the original A (quadratically so, as we will have decreased the n in O ( mn 2 ) ). An important procedure we will require is the extraction of the best approximate SVD within a given subspace b V . Algorithm 1 describes this process; portions of this idea appeared in [1] and [2], but without enumeration of its properties. We state some of the key properties as a lemma.
 Lemma 1. Given a target matrix A and a row subspace basis stored in the columns of b V , E
XTRACT SVD has the following properties: We omit the fairly straightforward proof. The runtime of the procedure is O ( kmn ) , where k is the rank of b V . As this SVD extraction will constitute the last and most expensive step of our algorithm, we therefore require a subspace discovery method that finds a subspace of sufficient quality with as low a rank k as possible. This motivates the essential idea of our approach, which is to leverage the Whole-Matrix SVD Approximation Low-Rank Matrix Approximation True SVD: U ,  X  , and V b A or unaligned b V &amp; b  X  only Addresses full-rank matrix Fixed low-rank k Full-rank relative error bound k -rank error bound, additive or relative Table 2: Distinctions between subspace construction in QUIC-SVD and previous LRMA methods. QUIC-SVD Previous LRMA Methods Iterative buildup, fast empirical error control One-off computation, loose error bound Adaptive sample size minimization Fixed a priori sample size (loose)
Cosine tree sampling Various sampling schemes geometric structure of a matrix to efficiently derive compact (i.e., minimal-rank) subspaces in which to carry out the approximate SVD.
 Previous Work . A recent vein of work in the theory and algorithms community has focused on using sampling to solve the problem of low-rank matrix approximation (LRMA). The user specifies a desired low rank k , and the algorithms try to output something close to the optimal k -rank approxi-mation. This problem is different from the whole-matrix SVD approximation we address, but a close relationship allow us to draw on some of the LRMA ideas. Table 1 highlights the distinctions be-tween whole-matrix SVD approximation and LRMA. Table 2 summarizes the differences between our algorithmic approach and the more theoretically-oriented approaches taken in the LRMA work. Each LRMA algorithm has a way of sampling to build up a subspace in which the matrix projection has bounded error. Our SVD also samples to build a subspace, so the LRMA sampling methods are directly comparable to our tree-based approach. Three main LRMA sampling techniques have emerged, 1 and we will discuss each from the perspective of iteratively sampling a row, updating a subspace so it spans the new row, and continuing until the subspace captures the input matrix to within a desired error threshold. This is how our method works, and it is similar to the framework used by Friedland et al. [1]. The key to efficiency (i.e., rank-compactness) is for each sampled row to represent well the rows that are not yet well represented in the subspace.
 Length-squared (LS) sampling . Rows are sampled with probability proportional to their squared lengths: p i = || A ( i ) || 2 F / || A || 2 F . LS sampling was used in the seminal work of Frieze, Kannan, and Vempala [3], and in much of the follow-on work [4, 5]. It is essentially an importance sampling scheme for the squared error objective. However, it has two important weaknesses. First, a row can have high norm while not being representative of other rows. Second, the distribution is non-adaptive, in that a point is equally likely to be drawn whether or not it is already well represented in the subspace. Both of these lead to wasted samples and needless inflation of the subspace rank. Residual length-squared (RLS) sampling . Introduced by Deshpande and Vempala [2], RLS modi- X 
V ( A ) || 2 F , where  X  V represents projection onto the current subspace V . By adapting the LS distri-bution to be over residuals, this method avoids drawing samples that are already well represented in the subspace. Unfortunately, there is still nothing to enforce that any sample will be representative of other high-residual samples. Further, updating residuals requires an expensive s passes through the matrix for every s samples that are added, which significantly limits practical utility. Random projections (RP) . Introduced by Sarl  X  os [6], the idea is to sample linear combinations of rows, with random combination coefficients drawn from a Gaussian. This method is strong where LS and RLS are weak  X  because all rows influence every sample, each sample is likely to represent a sizeable number of rows. Unfortunately the combination coefficients are not informed by importance (squared length), and the sampling distribution is non-adaptive. Further, each linear combination requires a full matrix pass, again limiting practicality.
 Also deserving mention is the randomized sparsification used by Achlioptas et al. [7]. Each of the LRMA sampling methods has strengths we can draw on and weaknesses we can improve upon. In particular, our cosine tree sampling method can be viewed as combining the representativeness of RP sampling with the adaptivity of RLS, which explains its empirically dominant rank efficiency. Algorithm 2 Cosine tree construction.
 Rather than a fixed low-rank matrix approximation, our objective is to approximate the whole-matrix SVD with as high a rank as is required to obtain the following whole-matrix relative error bound: where b A = U  X  V T is the matrix reconstructed by our SVD approximation. In contrast to the error bounds of previous methods, which are stated in terms of the unknown low-rank A k , our error bound is in terms of the known A . This enables us to use a fast, empirical Monte Carlo technique to determine with high confidence when we have achieved the error target, and therefore to terminate with as few samples and as compact a subspace as possible. Minimizing subspace rank is crucial for speed, as the final SVD extraction is greatly slowed by excess rank when the input matrix is large. We use an iterative subspace buildup as described in the previous section, with sampling governed by a new spatial partitioning structure we call the cosine tree . Cosine trees are designed to leverage the geometrical structure of a matrix and a partial subspace in order to quickly home in on good rep-resentative samples from the regions least well represented. Key to the efficiency of our algorithm is an efficient error checking scheme, which we accomplish by Monte Carlo error estimation at judi-ciously chosen stages. Such a combination of spatial partitioning trees and Monte Carlo estimation has been used before to good effect [8], and we find it to be a successful pairing here as well. Cosine Trees for Efficient Subspace Discovery . The ideal subspace discovery algorithm would oracularly choose as samples the singular vectors v i . Each v i is precisely the direction that, added to the subspace spanned by the previous singular vectors, will maximally decrease residual error over all rows of the matrix. This intuition is the guiding idea for cosine trees.
 A cosine tree is constructed as follows. Starting with a root node, which contains all points (rows), we take its centroid as a representative to include in our subspace span, and randomly sample a point to serve as the pivot for splitting. We sample the pivot from the basic LS distribution, that being the cheapest source of information as to sample importance. The remaining points are sorted by their absolute cosines relative to the pivot point, then split according to whether they are closer to the high or low end of the cosines. The two groups are assigned to two child nodes, which are placed in a Algorithm 3 Monte Carlo estimation of the squared error of a matrix projection onto a subspace. Algorithm 4 QUIC-SVD: fast whole-matrix approximate SVD with relative error control.
 queue prioritized by the residual error of each node. The process is then repeated according to the priority order of the queue. Algorithm 2 defines the splitting process.
 Why do cosine trees improve sampling efficiency? By prioritizing expansion by the residual error of the frontier nodes, sampling is always focused on the areas with maximum potential for error reduction. Since cosine-based splitting guides the nodes toward groupings with higher parallelism, the residual magnitude of each node is increasingly likely to be well captured along the direction of the node centroid. Expanding the subspace in the direction of the highest-priority node centroid is therefore a good guess as to the direction that will maximally reduce residual error. Thus, cosine tree sampling approximates the ideal of oracularly sampling the true singular vectors. 3.1 QUIC-SVD Strong error control . Algorithm 4, QUIC-SVD (QUantized Iterative Cosine tree) 2 , specifies a way to leverage cosine trees in the construction of an approximate SVD while providing a strong probabilistic error guarantee. The algorithm builds a subspace by expanding a cosine tree as de-scribed above, checking residual error after each expansion. Once the residual error is sufficiently low, we return the SVD of the projection into the subspace. Note that exact error checking would require an expensive O ( k 2 mn ) total cost, where k is the final subspace rank, so we instead use a Monte Carlo error estimate as specified in Algorithm 3. We also employ Algorithm 3 for the error estimates used in node prioritization. With Monte Carlo instead of exact error computations, the total cost for error checking decreases to O ( k 2 n log m ) , a significant practical reduction. The other main contributions to runtime are: 1) k cosine tree node splits for a total of O ( kmn ) , 2) O ( k ) single-vector Gram-Schmidt orthonormalizations at O ( km ) each for a total of O ( k 2 m ) , and 3) final SVD extraction at O ( kmn ) . Total runtime is therefore O ( kmn ) , with the final projection onto the subspace being the costliest step since the O ( kmn ) from node splitting is a very loose worst-case bound. We now state the QUIC-SVD error guarantee.
 Theorem 1. Given a matrix A  X  R m  X  n and , X   X  [0 , 1] , the algorithm QUIC-SVD returns an SVD U,  X  ,V such that b A = U  X  V T satisfies || A  X  b A || 2 F  X  || A || 2 F with probability at least 1  X   X  . Proof sketch. The algorithm terminates after mcSqErr  X  || A || 2 F with a call to E XTRACT SVD. From Lemma 1 we know that E XTRACT SVD returns an SVD that reconstructs to A  X  X  projection onto V (i.e., b A = AV V T ). Thus, we have only to show that mcSqErr in the terminal iteration is an upper bound on the error || A  X  b A || 2 F with probability at least 1  X   X  . Note that intermediate error checks do not affect the success probability, since they only ever tell us to continue expand-ing the subspace, which is never a failure. From the Pythagorean theorem, || A  X  AV V T || 2 F = call to MCS Q E RROR (step 3(e)) performs a Monte Carlo estimate of || AV || 2 F in order to esti-mate || A || 2 F  X  X | AV || 2 F . It is easily verified that the length-squared-weighted sample mean used by MCS Q E RROR produces an unbiased estimate of || AV || 2 F . By using a valid confidence interval to generate a 1  X   X  lower bound on || AV || 2 F from the sample mean and variance (e.g., Theorem 1 of [9] or similar), MCS Q E RROR is guaranteed to return an upper bound on || A || 2 F  X  X | AV || 2 F with probability at least 1  X   X  , which establishes the theorem.
 Relaxed error control . Though the QUIC-SVD procedure specified in Algorithm 4 provides a strong error guarantee, in practice its error checking routine is overconservative and is invoked more frequently than necessary. For practical usage, we therefore approximate the strict error checking of Algorithm 4 by making three modifications: Although these modifications forfeit the strict guarantee of Theorem 1, they are principled approx-imations that more aggressively accelerate the computation while still keeping error well under control (this will be demonstrated empirically). Changes 1 and 2 are based on the fact that, because mcSqErr is an unbiased estimate generated by a sample mean, it obeys the Central Limit Theorem and thus approaches a normal distribution centered on the true squared error. Under such a sym-metric distribution, the probability that a single evaluation of mcSqErr will exceed the true error is 0.5. The probability that, in a series of x evaluations, at least one of them will exceed the true error is approximately 1  X  0 . 5 x (1 minus the probability that they all come in below the true error). The probability that at least one of our mcSqErr evaluations results in an upper bound on the true error (i.e., the probability that our error check is correct) thus goes quickly to 1. In our experiments, we use x = 3 , corresponding to a success probability of approximately 0.9 (i.e.,  X   X  0 . 1 ). Change 3 exploits that fact that the rate at which error decreases is typically monotonically non-increasing. Thus, extrapolating the rate of error decrease from past error evaluations yields a con-servative estimate of the number of splits required to achieve the error target. Naturally, we have to impose limits to guard against outlier cases where the estimated number is unreasonably high. Our experiments limit the size of the split jumps to be no more than 100. We report the results of two sets of experiments, one comparing the sample efficiency of cosine trees to previous LRMA sampling methods, and the other evaluating the composite speed and error performance of QUIC-SVD. Due to space considerations we give results for only two datasets, and Figure 1: Relative squared error vs. subspace rank for various subspace discovery methods. LS is length-squared, RLS is residual length-squared, RP is random projection, and CT is cosine tree. due to the need to compute the exact SVD as a baseline we limit ourselves to medium-sized matrices. Nonetheless, these results are illustrative of the more general performance of the algorithm. Sample efficiency . Because the runtime of our algorithm is O ( kmn ) , where k is the final dimension of the projection subspace, it is critical that we use a sampling method that achieves the error target with the minimum possible subspace rank k . We therefore compare our cosine tree sampling method to the previous sampling methods proposed in the LRMA literature. Figure 1 shows results for the various sampling methods on two matrices, one a 2000  X  2000 Gaussian kernel matrix produced by the Madelon dataset from the NIPS 2003 Workshop on Feature Extraction (madelon kernel), and the other a 4656  X  3923 scan of the US Declaration of Independence (declaration). Plotted is the relative squared error of the input matrix X  X  projection onto the subspaces generated by each method at each subspace rank. Also shown is the optimal error produced by the exact SVD at each rank. Both graphs show cosine trees dominating the other methods in terms of rank efficiency. This dominance has been confirmed by many other empirical results we lack space to report here. It is particularly interesting how closely the cosine tree error can track that of the exact SVD. This would seem to give some justification to the principle of grouping points according to their degree of mutual parallelism, and validates our use of cosine trees as the sampling mechanism for QUIC-SVD. Speedup and error. In the second set of experiments we evaluate the runtime and error performance of QUIC-SVD. Figure 2 shows results for the madelon kernel and declaration matrices. On the top row we show how speedup over exact SVD varies with the target error . Speedups range from 831 at = 0 . 0025 to over 3,600 at = 0 . 023 for madelon kernel, and from 118 at = 0 . 01 to nearly 20,000 at = 0 . 03 for declaration. On the bottom row we show the actual error of the algorithm in comparison to the target error. While the actual error is most often slightly above the target, it nevertheless hugs the target line quite closely, never exceeding the target by more than 10%. Overall, the several-order-of-magnitude speedups and controlled error shown by QUIC-SVD would seem to make it an attractive option for any algorithm computing costly SVDs. We have presented a fast approximate SVD algorithm, QUIC-SVD, and demonstrated several-order-of-magnitude speedups with controlled error on medium-sized datasets. This algorithm dif-fers from previous related work in that it addresses the whole-matrix SVD, not low-rank matrix approximation, it uses a new efficient sampling procedure based on cosine trees, and it uses em-pirical Monte Carlo error estimates to adaptively minimize needed sample sizes, rather than fixing a loose sample size a priori . In addition to theoretical justifications, the empirical performance of QUIC-SVD argues for its effectiveness and utility. We note that a refined version of QUIC-SVD is forthcoming. The new version is greatly simplified, and features even greater speed with a determin-istic error guarantee. More work is needed to explore the SVD-using methods to which QUIC-SVD can be applied, particularly with an eye to how the introduction of controlled error in the SVD will Figure 2: Speedup and actual relative error vs. for QUIC-SVD on madelon kernel and declaration. affect the quality of the methods using it. We expect there will be many opportunities to enable new applications through the scalability of this approximation.

