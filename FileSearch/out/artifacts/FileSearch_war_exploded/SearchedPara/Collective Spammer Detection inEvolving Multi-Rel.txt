 Detecting unsolicited content and the spammers who create it is a long-standing challenge that affects all of us on a daily basis. The recent growth of richly-structured social net-works has provided new challenges and opportunities in the spam detection landscape. Motivated by the Tagged.com 1 social network, we develop methods to identify spammers in evolving multi-relational social networks. We model a so-cial network as a time-stamped multi-relational graph where vertices represent users, and edges represent different ac-tivities between them. To identify spammer accounts, our approach makes use of structural features, sequence mod-elling, and collective reasoning. We leverage relational se-quence information using k -gram features and probabilistic modelling with a mixture of Markov models. Furthermore, in order to perform collective reasoning and improve the predictive power of a noisy abuse reporting system, we de-velop a statistical relational model using hinge-loss Markov random fields (HL-MRFs), a class of probabilistic graphical models which are highly scalable. We use Graphlab Cre-ate TM and Probabilistic Soft Logic (PSL) 2 to prototype and experimentally evaluate our solutions on internet-scale data from Tagged.com . Our experiments demonstrate the effec-tiveness of our approach, and show that models which in-corporate the multi-relational nature of the social network significantly gain predictive performance over those that do not.  X 
Contribution partly performed while under internship at if(we) Inc., formerly Tagged Inc.  X 
Currently with Niara, Inc., Sunnyvale, CA.
Tagged.com was founded in 2004, has over 300 million reg-istered members, and is aimed towards fostering new con-nections between people. http://psl.umiacs.umd.edu DOI: http://dx.doi.org/10.1145/2783258.2788606 .
 Social Networks, Spam, Social Spam, Collective Classifica-tion, Graph Mining, Multi-relational Networks, Heteroge-neous Networks, Sequence Mining, Tree-Augmented Naive Bayes, k-grams, Hinge-loss Markov Random Fields (HL-MRFs), Probabilistic Soft Logic (PSL), Graphlab.
Unsolicited or inappropriate messages sent to a large num-ber of recipients, known as  X  X pam X , can be used for various malicious purposes, including phishing and virus attacks, marketing of objectionable materials and services, and com-promising the reputation of a system. From printed ad-vertisements to unsolicited phone calls, spam has been a perennial problem in modern human communication. With the emergence of the Internet, spammers have found a cost-effective medium to reach a broader audience than was pre-viously possible. Email spam is almost as old as the Internet itself. The first email spam was sent in 1978 to all several hundred users of ARPANET [ 1].

More recently, social media has given spammers a new and effective medium to spread their content. Using social media platforms, spammers can disguise themselves as le-gitimate users and engage in realistic looking interactions. They can use these platforms to send messages to users, leave spam comments on popular pages, and reply to legiti-mate comments using spam content. Such diversity of choice has often increased spammers X  ability to conceal their inten-tions from traditional spam filters. According to a study by Nexgate [2 ], social spam grew by more than 355% between January to July of 2013, one in 200 social messages contain spam, and 5% of all social apps are spammy.

While content-based approaches have been shown to be effective in stopping spam in email and the web, they can be manipulated by sophisticated spammers via incorporat-ing content randomness. Unlike in email and the web, social media enables spammers to split their content across multi-ple messages in order to bypass spam filters. Link-based approaches that leverage the connectivity of the entities, have been combined with content-based methods to build more effective methods. While it is easier to pass tradi-tional content-based filters, behavioral patterns and graph properties of the users X  interactions are harder to manipu-Figure 1: A time-stamped multi-relation social network with legitimate users and spammers. Each link  X  v 1 ,v 2 network represents an action (e.g. profile view, message, or poke) performed by v 1 towards v 2 at specific time t . late. Furthermore, many social networks can not monitor all the generated contents due to privacy and resources con-cerns. Content-independent frameworks, such as the one proposed in this paper, can be applied to systems that pro-vide maximum user privacy with end-to-end encryption.
Perhaps the most important difference between social net-works and email or web graphs is that social networks have a multi-relational nature, where users have relationships of different types with other users and entities in the networks. For example, they can send messages to each other, add each other as friends,  X  X ike X  each other X  X  posts, and send non-verbal signals such as  X  X inks X  or  X  X okes. X  Figure 1 shows a representation of a social network as a time-stamped multi-relation graph. The multi-relational nature provides more choices for spammers, but it also empowers detection sys-tems to monitor patterns across activity types, and time. In this paper, we propose a content-independent framework which is based on the multi-relational graph structure of different activities between users, and their sequences.
Our proposed framework is motivated by Tagged.com , a social network for meeting new people which was founded in 2004 and has over 300 million registered members. More generally, the framework is applicable to any multi-relational social network. Our goal is to identify sophisticated spam-mers that require manual or semi-automated intervention by the administrative security team. These spammers have already passed initial classifiers and know how to manipu-late their accounts and contents to avoid being caught by automatic filters. We show that our framework significantly reduces the need for manual administration to control spam.
Our framework consists of three components. First, we extract graph structure features for each of the relations and show that considering the multi-relational nature of the graphs improves the performance. Second, we consider the activity sequence of each user across these relations and ex-tract k -gram features and employ mixtures of Markov mod-els to label spammers. Third, we propose a statistical re-lational model based on hinge-loss Markov random fields to perform collective reasoning using signals from an abuse re-porting system in the social network.

The following sections formally define the problem and our solution framework along with an experimental validation of our approach on internet-scale data from Tagged.com .
We represent a social network as a directed time-stamped dynamic multi-relational graph G =  X  X  , E X  , where V is the set of vertices of the form v =  X  f 1 ,...,f n  X  representing users and their demographic features f i , and E is the set of directed edges of the form e =  X  v src ,v dst ,r i ,t i ing their interactions, relation type r i , and a discrete time-stamp t i . The social spam detection problem is to predict whether v i with an unobserved label is a spammer or not, based on the given network G and a set of observed labels for already identified spammers. Since the deployed security system could employ different measures based on the clas-sification confidence, we are interested in (un-normalized) probabilities or ranking scores of the likelihood that each user is a spammer. In other words, the problem is assign-ment of a score (e.g., a probability) to user accounts to rank them from the most to the least probable spammer in the system: c : v i  X  [0 , 1].
In our framework, we focus on three different mechanisms to identify spammers and malicious activities. We first cre-ate networks from the user interactions and compute net-work structure features from them. As these are evolving networks, each user generates a sequence of actions with the passage of time. Mining these sequences can provide valu-able insights into the intentions of the user. We use two methods to study these sequences and extract features from them. We use the output of these methods as features to classify spammers. We then employ a collective model to identify spammer accounts only based on the signals from the abuse reporting system ( G report ) as a secondary source to reassure predictions. The following sections discuss our framework and extracted features in more details.
We create a directed graph G r =  X  X  , E r  X  for each relation r in the social network, where vertices V consist of users, and edges E r represent interactions of type r between users, e.g. if user 1 sends a message to user 2 then G message will contain v and v 2 representing the two users, and e 1 , 2 representing the relation between them. We have ten different graphs each containing the same users as vertices but different actions as edges.

We use Graphlab Create TM 3 to generate features based on each of these graphs for each user. We use six graph analytics methods m i to compute the features. Using each m i we create a set of features for each relation graph G r following: where m i is one of the graph analytics methods described below, and r i is one of the relationships considered in the study.

We then use these features together to get a complete multi-relational graph feature-set, as the following:
The graph analytics methods m i we use to extract the features from each relation network are described in the fol-http://dato.com/products/create lowing section. Each of these algorithms provieds differ-ent perspectives on the local connectivity of the graph and neighborhood characteristics of each user. Our goal is to capture the structural differences between spammers X  and legitimate users X  multi-relational neighborhood graph. PageRank [3 ], is a well known ranking algorithm proposed for ranking websites, and computes a score for each node by considering the number and quality of links to a node. The algorithm is based on the underlying assumption that important nodes receive more links from other nodes. We compute the total degree, in-degree, and out-degree of each node for each relation, which correspond to the total number of activities a user has been involved in, the num-ber of communications (or actions) a user received, and the number of actions the user performed. k -core [ 4] is a centrality measure that is based on the graph decomposition via a recursive pruning of the least connected vertices. The value each vertex receives depends on the step in which the vertex is eliminated from the graph. e.g, ver-tices removed on the third iteration receive the value three. Graph coloring [ 5] is an assignment of colors to elements (here vertices) of a graph, such that no two adjacent vertices share the same color. Using a greedy implementation, we obtain the color identifier of each vertex as a feature. A connected component [6 ] is a group of vertices with a path between each vertex and all other vertices in the com-ponent. A weakly connected component is a maximal set of vertices such that there is an undirected path between any two vertices in the set. We compute the weakly connected component on each graph and extract the component iden-tifier and size of the component that the vertex participates in as features.
 The triangle count [ 7] of a vertex is the number of triangles (a complete subgraph of three vertices) in the graph the vertex participates in. Such number is an indication of the connectivity of the graph around that vertex.
Sequence classification is used in many domains, includ-ing biology and health-informatics, anomaly detection, and information retrieval [8 ]. In dynamically evolving multi-relational social networks, each user v i generates a sequence of edges via their actions as the following:
Spammers typically pursue specific purposes in the net-work and it is likely that their sequence of actions diverge from the norm. In this section we study these sequences and provide two different solutions for classifying users based on their activity sequences. It is important to note that such an approach would not be possible if the network were not multi-relational.
 The simplest way to represent a sequence with features is to treat each element in the sequence as a feature indepen-dently. However, the order of the sequence cannot be cap-tured with this approach. Furthermore, in our scenario the values of these features will be the same as the out-degree for each vertex, which we previously computed in the graph-based features. To address this, a short sequence segment of k consecutive actions, called a k -gram, can be used to cap-ture the order of events [8 ]. The sequence can be represented as a vector of the frequencies of the k -grams. To keep the feature space computationally manageable we chose bigram sequence features where k = 2. For example, the number of times a user v i sent a message after performing a profile The bigram feature set for the sequence S will be the fol-lowing: where r i is one of the relationships considered in the study, X times user v i performed an action of type r q consecutively after performing r p .
 While k -gram features capture some aspects of the order of elements in the sequence, they may miss patterns in longer sequences. Increasing k will rapidly increase the feature space, introducing computational barriers and estimation challenges due to feature sparsity. Instead, to capture the salient information from longer sequence chains, and to study the predictive power of this information, we construct a sim-ple generative model for sequence data. The model is equiv-alent to the chain-augmented naive Bayes model of [ 9], a special case of the tree-augmented naive Bayes model [10 ] which has been shown to be effective in language modelling. The model posits that each user X  X  actions are generated via a mixture of Markov models. In more detail, each class (spam-mer or not spammer) is associated with a mixture compo-nent y . Conditional on the class (mixture component) y for a user, that user X  X  sequence of actions are assumed to be generated from a Markov chain specific to that class. The joint probability for a user X  X  class y and action sequence x ,...,x n is given by which we summarize with a directed graphical model dia-gram in Figure 2. We place symmetric Dirichlet priors on the parameters of the discrete distributions P ( y ), P ( x 1 P ( x i | x i  X  1 ,y ), and compute maximum a posteriori (MAP) estimates of them, which are readily obtained as the propor-tion of each outcome in the training data, with the counts first adjusted by adding the Dirichlet smoothing parame-ter  X  = 1. Finally, at test time we compute the posterior probability of the user X  X  class label given the observed action
There are multiple methods to incorporate the predictions from this model into our framework. We simply use the Figure 2: The directed graphical model for the mixture of Markov models / chain-augmented naive Bayes model, for one user. In the diagram, y indicates the label (spammer or not) and x i represents the i th action performed by the user. ratio of posterior probabilities and their logarithmic forms as a small feature-set ( X S M ) for our classifier.
Most websites that enable users to publish content also provide an abuse reporting mechanism for other users to bring malicious behavior to the system X  X  attention. How-ever, these systems do not necessary offer clean signals. Spa-mmers themselves often randomly report other users (spam-mers and legitimate users) to increase the noise, legitimate users often have different standards for malicious behaviors, and users may report others for personal gains such as cen-sorship or blocking an opponent in a (social) game from accessing the system. A model that can extract sufficient information from the relational report feature, can enhance the administrative team X  X  performance by focusing their at-tention, and can also provide an additional feature or parallel mechanism for spam classification.

We propose a model based on hinge-loss Markov random fields [ 11 ] to collectively classify spammers within the re-ported users, and assign credibility scores to the users of-fering feedback via the reporting system. Using this model a better ranking of the reported users based on their prob-ability of being spammers can be provided to the security administration team. The hinge-loss formulation has the ad-vantage of admitting highly scalable inference, regardless of the structure of the network.
 Hinge-loss Markov random fields (HL-MRFs) are a general class of conditional, continuous probabilistic models [ 11, 12]. HL-MRFs are log-linear models whose features are hinge-loss functions of the variable states. Through constructions based on soft logic , hinge-loss potentials can be used to model generalizations of logical conjunction and implication. A hinge-loss Markov random field P over random variables Y and conditioned on random variables X defines a condi-tional probability density function as the following: where Z is the normalization constant of the form In the above,  X  is a set of m continuous potential of the form where ` is a linear function of Y , and X and p j  X  X  1 , 2 } .
Probabilistic Soft Logic (PSL) [12] uses a first-order log-ical syntax as a templating language for HL-MRFs. HL-MRFs have achieved state-of-the-art performance in many domains including knowledge graph identification [ 13 ], un-derstanding engagements in MOOCs [14 ], biomedicine and multi-relational link prediction [15 , 16 ], and modelling social trust [17 ]. A typical example of a PSL rule is where P , Q , and R are predicates , a and b are variables , and  X  is the weight associated with the rule, indicating its importance. For instance, P ( a,b ) can represent a relational edge in the graph such as Reported ( a,b ), and Q ( a ) could represent a value for a vertex such as Credible ( b ). Each grounding forms a ground atom, or logical fact, that has a soft-truth value in the range [0 , 1]. The rules can en-code domain knowledge about dependencies between these predicates. PSL uses the Lukasiewicz norms to provide re-laxations of the binary connectives to soft-truth values. A ground instance of a rule r ( r body  X  X  X  r head ) is satisfied when the value of r body is not greater than the value of r head defined to capture the distance to satisfaction for rules: The goal of this model is to use reports to predict spammers. We study three HL-MRFs models to incorporate the report-ing users X  credibility into the reporting system and improve the predictability of the reports. We show that collective reasoning over credibility of the reporting user and the prob-ability of the reported user being an spammer, increases the classification performance of the system.

Our collective HL-MRFs model uses the report relation graph ( G report ), and is based on the intuition that the credi-bility of a user X  X  abuse reporting should increase when they report users that are more likely to be spammers. Hence, if a user reports other users whom there are other evidence sup-porting them being spammers, the credibility of that person should increase. On the other hand, if the user reports an-other user that is unlikely to be a spammer, the credibility of the reporting user should decrease.
  X  Spammer ( v 2 )  X  Reported ( v 1 ,v 2 )  X  X  Credible ( v Figure 3: Collective HL-MRFs model to predict spammers based on the reports from other users.

We propose the model shown in Figure 3 to capture the collective intuition. We incorporate prior credibility of the reporting users based on the past reporting behavior into the model. The negative prior on SPAMMER is included in the model to complement the first rule that increases the score of users being spammers. To study the effect of each part of the model, we experimentally compare the proposed collective model with two simpler HL-MRFs models that do not contain the collective reasoning and credibility priors in section 5.4 .
The dataset 4 was collected from the Tagged.com social network website, which is a network for meeting new peo-ple, and has multiple methods for users to make new con-nections. Tagged has various methods to deal with spam. It uses several registration and activation filters to identify and block spam accounts based on traditional methods such as content and registration information and patterns. Tagged also employs a reporting mechanism that users can report spammers to the system. An administrative security team also monitors the network for malicious behaviors and man-ually blocks spammers. Our goal in this study is to identify sophisticated spammers that require manual intervention by the security team. These spammers have already passed ini-tial classifiers and know how to manipulate their content to avoid being caught by automatic filters.

The purpose of the social network affects its susceptibility to spam. A social network which is designed for connecting the users who already know each other, can control spam by limiting the communications between users who are not already connected in the network. However, a social network that promotes finding new connections may like to impose minimum limitations on how users interact. Tagged , which is a social network for meeting new people, has multiple venues for users to communicate without much restriction.
Another challenge with identifying spammers in multi-purpose social networks such as Tagged is that users join the network for different reasons. For example, users may come to Tagged to play social games such as Pets and MeetMe , to find romantic relationships, or simply to spend time with virtual connections. Not only they will generate different behavioral patterns, they will use security measures such as abuse reporting mechanism differently and introduce noise to it.

In our experiments, all the users who had at least one activity in the sampling time frame were included in the sample dataset. More formally our initial sample included the following elements:
V = { v |  X  e =  X  v,v  X  ,r  X  ,t k  X  X  X E all  X  t b  X  t k  X  t
E = { e =  X  v i ,v j ,r  X  ,t k  X  |  X  v i ,v j  X  X   X  t s  X  t where v  X  indicates any user in the network, r  X  indicates any type of action in the study, E all indicates all the edges in the Tagged network, and t b and t e indicate the time of the beginning and the end of the sampling period.

To perform a retrospective study, we chose t b and t e such that enough time had passed since the sampling period by the time we accessed the data ( t access ), so that most of the spam accounts were identified and labeled. We then removed the users who had deactivated their accounts themselves by t access , because we could not determine their labels. The remaining users were labeled as spam if their accounts has
An anonymized sample of the multi-relational part of the dataset along with our code for the experiments can be found here: http://github.com/shobeir/fakhraei_kdd2015 . been manually canceled by a security team by t access . Al-though the security team cancels accounts for multiple rea-sons, not just spam, most of the canceled accounts are due to malicious activities. For simplicity, we labeled all the can-celed accounts as spammers. Ten different activities on the website were selected during the sampling time frame. The activities included in the study are: viewing another user X  X  profile, sending friend requests, sending messages, sending luv , sending winks , buying or wishing others in the Pets game, clicking yes or no in the MeetMe game, and reporting other users for abuse.

There are more effective ways to sample the network in or-der to conserve its characteristics [18 , 19 , 20]. However, for practical reasons and ease of deployment, we have chosen the simple time-based sampling method. Further perfor-mance improvements may be achieved via better sampling employments. The spammer accounts that were selected for this study could initially bypass Tagged deployed preventa-tive measures and successfully perform at least one action in the network. Although they could be identified within a short period of time after their activity, their identifica-tion required a manual or semi-automated procedure by the members of the security team. Not only are these spammers harder to identify, they are also very rare in the dataset, causing a huge class imbalance.
 Table 1 shows some statistics from the sample we used. These numbers do not represent the statistics of the Tagged social network, as they have been altered by limiting the number of action types in the study as well as eliminating users with deactivated accounts at t access (which is later than the sample period). Furthermore, only the users who per-formed an action in the sampling period were included in the dataset.

Entity Count |V| (total users) 5,607,454 |E| (total actions) 912,280,409 max( |E r | ) (number of actions that are most frequent action type) min( |E r | ) (number of actions that are least frequent action type) total users labeled as spammers (%3.9) 221,305
All of our experiments are based on the relational data in the following form: where t i is the time stamp, v src is the user who initiated the action, v dst is the user the action was towards, and r categorizes the type of action.
We performed four sets of experiments to evaluate the proposed methods. First we study the graph structure prop-erties and compare the multi-relational approach with only considering a single relation. We also study using one graph analytics algorithm as a feature, comparing to having fea-tures from multiple methods. We then study the effective-ness of sequence mining features and combine them with graph-based methods to measure the overall performance enhancements. We then include only three demographics features for each user to measure their influence on the performance. Finally we perform collective reasoning over abuse reports and measure the improvement of the predic-tions with this method.
 For our experiments we used Graphlab Create TM and the Java-based open-source Probabilistic Soft Logic (PSL) , a single Ubuntu machine with 32GB RAM and 3.2GHz CPU (4 cores). For classification, we used Gradient-Boosted De-cision Trees which is a collection of decision trees combined through a technique called gradient boosting [ 21].
The deployment options of the framework and what ac-tions are planned to be taken on the identified spammer accounts determine which performance metrics are more ap-propriate for this task. High precision lets the spam accounts be blocked without manual intervention, and without con-cerns of the system harming legitimate users. High recall allows the system to identify the legitimate users with more confidence and clear the environment via deploying measures such as CAPTCHA and additional account verifications for the users with suspicious status. Hence, the appropriate metric to measure the performance of this system is the Precision-Recall curve. The ROC curve could also be use-ful, however, due to the high class-imbalance, it would not provide much insight, and unless properly adjusted, it would result in over-optimistic estimates. We report the area under Precision-Recall curve (AUPR) and the area under the ROC curve (AUROC) for the experiments. We used 10-fold cross-validation to estimate the performance of each method and feature-set. Unless stated otherwise, the reported numbers represent mean and standard deviation over 10-fold cross-validation.
Table 2 shows the average results of classification via graph-based features. The first row indicated the best results from using a single relation with features from all the graph-based algorithms. The second row shows the best graph-based fea-ture with all the relations. Comparing the results from the two rows suggests that combining different relations is more effective than combining features from different algorithms on a single relation. Using all algorithms to compute fea-tures on all relation graphs results in the best performance for graph-based methods.

Experiment AUPR AUROC h h h
Next, we experimentally evaluated the sequence-based fea-tures. First, we study their effectiveness independently, and then we measure their performance in combination with the graph-based features. To compute the bigram features, we first sorted all of the activities in our dataset based on user http://psl.umiacs.umd.edu IDs and timestamps via the standard external sort function in Linux. We did a single pass on the sorted file to compute the bigram features.

Experiment AUPR AUROC h h
Table 3 shows the results of classification using the bigram features. The second row suggests that a model that uses both graph-based and k -gram features outperforms the ones that use them independently. Precision-Recall and ROC curves from graph-based and k -gram features are shown in Figure 4.

We further study the sequence-based classification with the Mixture of Markov Models (MMM) approach. We did a single pass on the sorted file we already generated for the bigram features to compute the probabilities for this model. We then used the probabilities generated from this model in logarithmic and ratio forms as features for classification. Table 4: Classification with mixture of Markov models.
Experiment AUPR AUROC h h h
The results from Table 4 shows the classification perfor-mance with these features, which suggests minimal improve-ment employing longer sequence models. This may suggest that the bigram features can incorporate enough signal to capture spam activity in a multi-relational network. How-ever, computing the Mixture of Markov Models does not im-pose much overhead when extracting bigram features, and can be done within the same process.
Many people use Tagged to find new relationships. We anticipate that in such environment users behave differently based on their demographics. To capture this point, we added three features ( X D ) to our model: age, gender, and time since registration. Age and gender highly improved the classification results as they tend to be most discriminative of behavioral patterns. Another feature that we included in our model is the time past since registration. As mentioned earlier we labeled all the cancelled accounts for malicious activities as spammers. However, these users have different behavioral patterns, where spammers who mainly mass ad-vertise, may use much newer accounts, in contrast to users who have been blocked due to misbehaviors, and have been active in the system much longer.

Table 5 shows the significant improvements of the results when including these features in different models. Figure 4 demographics significantly improve the results. shows the Precision-Recall and ROC curves of the complete framework.
 Table 5: Classification when including user demographics information.

Experiment AUPR AUROC h h h h
The reporting system can have useful information to de-tect spammers. We studied the effectiveness of our proposed collective model (in Figure 3) to extract useful signals from this relation. We first designed a baseline model shown in Figure 5a to only use the reports to detect spammers. This model gives similar results to assigning total count of the reports for each user as their score of being a spammer. We then designed the model shown in Figure 5b to use the re-ports and prior credibility of the reporting user to detect spammers. This model gives similar results to assigning to-tal weighted count of the reports for each user as their score of being a spammer, where reports are weighted by the cred-ibility of the reporting users.

To perform the experiments we have only used G report which is a sparse graph. Our collective model is aimed to propagate information between the reported users X  likeli-hood of being spammer, through the credibility of the re-porting users. In order for information to propagate in the model, each reporting user should at least have reported two other users. Hence, we removed the vertices with out-degree less than two. We then performed 10-fold cross validation to compare the three models and study the effectiveness of the collective model. We used the ratio of the correctly reported spammers from the training data as a simple prior on cred-ibility for each user. Potentially more effective priors could incorporate the the count and the frequency of the reports as well.
 Figure 5: Simple HL-MRFs models to compare with the collective model shown in Figure 3.

Table 6 shows the results from our three experiments. Us-ing the collective model significantly increases the perfor-mance of the reports in detecting spammers. These predic-tions can be added to the overall classification framework as a feature. However, since the report graph was sparse relative to the other relation graphs in our dataset, many users could not be classified with this model. Hence, we did not include these predictions as a feature in our framework. This model can be deployed independently to improve the signal from the reports.
 Table 6: Classification with collective HL-MRFs model. Experiment AUPR AUROC Reports (Figure 5a ) 0.674  X  0.008 0.611  X  0.007
Reports &amp; Credibility (Fig-ure 5b )
Reports &amp; Credibility &amp; Col-lective Reasoning (Figure 3)
Spam detection in email [ 22 ] and the web [ 23] have been extensively studied, and various methods and features have been proposed for them. Network-based approaches are more closely related to our proposed framework. These methods can be generally categorized based on feature con-struction and label propagation. Shrivastava et al. [ 24 ] gen-eralized the network-based spam detection to random link attacks and showed that the problem is NP-complete. Tseng and Chen [25 ] used network features to identify email spam-mers, and incrementally updated the SVM classifier to cap-ture the changes in spam patterns. Oscar and Roychowd-bury [26 ] used a network representation of the emails where nodes were email addresses and links between them indi-cated a sender-receiver relationship. They used clustering properties of the network to build white and black lists of email addresses and identify spammers. Becchetti et al. [27 ] proposed a link-based classification for web spam detection, and later combined it with content-based features and used graph topology to improve performance [ 28 ]. Since spam-mers tend to form clusters on the web (unlike in social net-works), the authors leveraged clustering and label propaga-tion, to further improve their predictions.

A group of methods are based on label propagation and influenced by PageRank. TrustRank [29 ] for example, used reputable sites as seeds and propagated reputations through the network. There are multiple variations which prop-agate dis-trust. Similar to this work, Chirita et al. [30 ] proposed MailRank which ranked the trustworthiness of a sender based on the network representation of the mail en-vironment. Abernethy et al. [31 ] proposed a method based on graph regularization and used regularizers that is based on the intuition that linked pages are somewhat similar.
The research focus on spam detection in social networks is relatively more recent. Heymann et al. [32 ] surveyed dif-ferent countermeasures to address the spam issue in social networks, and categorized them into methods based on de-tection, demotion, and prevention. Hu et al. [33 ] combined information from email, text messages (SMS), and web with Twitter content to detect spammers, and showed improve-ments in results. Tan et al. [ 34 ] proposed an unsupervised spam detection method that focused on identifying a white list of non-spammers from the social network. They argued that legitimate users show more stable patterns in social blogs.
 Stein et al. [35 ] described the spam filtering system in Facebook. They highlighted that attacks on social media use multiple channels, and an effective systems must share feedback and feature data across channels. Gao et al. [ 36] studied messages between users in Facebook, and used clus-tering to detect spam campaigns. They identify multiple clusters associated with several campaigns.

Markines et al. [ 37 ] studied multiple features and classi-fiers to detect spam in social tagging systems. Benevenuto et al. [38 ] used content such as presence or absence of a URL in the post, and user social behaviors such as number of posts to detect spam on Twitter. Lee et al. [39 ] used hon-eypots in Twitter and MySpace to harvest deceptive spam profiles. They then used content, posting rate, number of friends, and user demographics such as age and gender as features in their classifier.

Zhu et al. [40 ] reported that unlike email and web, in social networks, spammers do not form clusters with other spammers, and their neighbors are mostly non-spammers. They use matrix factorization on user activity matrix of data extracted from Renren 6 and use the latent factors as features for classification.

Evolving social networks are of high interest to researchers and have been studied for different purposes [41 ]. Jin et al. [42] modeled a social network as a time-stamped heteroge-neous network and used a clustering method to identify spa-mmers. They also used active learning to refine their model. Zhang et al. [43 ] identified spam campaigns on Twitter by linking accounts with similar malicious URLs in their posts.
Laorden et al. [44 ] used collective classification to filter spam messages based on their text, to reduce the number of necessary labeled messages. They used implementations in WEKA for collective classification in their evaluation. Geng et al. [45 ] used a semi-supervised learning algorithm to re-duce the labeled training data requirement for web spam detection. Torkamani and Lowd [46 ] proposed a method to robustly perform collective classification against malicious adversaries that change their behavior in the system.
We have studied the characteristics of time-stamped multi-relational social networks that can be leveraged to detect spammers. We showed that by considering action or relation types and incorporating graph-based features from different relations, one can improve the spammer classification per-formance. We then showed two sequence mining techniques and their effectiveness to model sequences extracted from time-stamped multi-relational network for spam detection. We also proposed a collective model to refine and improve the signals from the abuse report graph.

Depending on the precision of the results from the model, the security system could either automatically flag a user as spammer and deactivate the account or block its activities in the system, or ask for more verification. Our experimen-tal results show that our model can detect over 65% of the manually detected spammers with higher than 85% preci-sion. These sophisticated spammers had passed the already deployed security measures and performed some activity in the network. Inspecting some of the false positives with the highest spammer probability, we found unlabeled and aban-doned spammer accounts, which suggests the real precision
A social network in China: http://renren.com of the proposed framework might actually be higher than reported. These results can significantly reduce the manual overhead of the administrative security team. Furthermore, our results show that the precision at 80% recall, is above 50%, suggesting this portion of users can be asked for addi-tional verification (e.g., CAPTCHA) without affecting many legitimate users.

This model can be deployed as an iterative batch module to complement real-time filters. Except for some parameters such as the user credibility prior for the report system that should be set and adjusted globally for the user, the model has to be re-trained from a fresh sample of the network to ad-just to the adversarial changes in patterns. Computing the parameters of the models on a relatively low-powered single machine for our experiments suggests that the framework could be run on very short intervals depending on the train-ing size and computational power. The features can also be computed in parallel. Using Graphlab Create TM , comput-ing the features is highly efficient. To provide an example, for a graph with 5.6 million vertices and 350 million edges computing PageRank on our experiment machine took ap-proximately 6.25 minutes, triangle counting 17.98 minutes, k-core 14.3 minutes, and graph coloring 143 minutes.
To optimize the model for production, it is possible to perform feature selection and reduce the necessary features. Feature selection [47 ] may also improve the performance of the model. Our method for collectively refining the signals from the report graph can be used independently or as a feature in the framework. Improved precision of the pre-dictions via reports enables the system to take actions with more confidence, and reduces the manual overhead.
 Our method should be retrained with every new sample. An online learning method that can incorporate the changes in the dynamic network can effectively improve the usability of our framework. Another approach that could improve the prediction results significantly could be incorporating this framework with content-based models. Furthermore, spam accounts often do not act independently and are part of spam campaigns. Their targets may often not be at random as well. They may use a white list of legitimate users to tar-get. Our initial observations show that spammers make re-lations with legitimate users disproportionally to the overall population ratios. A multi-relational model that can classify spammer accounts based on their target accounts, and iden-tify campaigns based on their relational information could potentially improve the results.
 Part of this work was performed during the first author X  X  internship at if(we) formerly Tagged Inc . We are highly grateful to Johann Schleier-Smith and Karl Dawson for their extensive help and support. We also thank Dai Li, Stuart Robinson, Vinit Garg, and Simon Hill for constructive dis-cussions, and Jay Pujara and Arti Ramesh for their helpful suggestions and feedback. We also like to thank the Dato (Graphlab) team for their insightful guidance and help with using Graphlab Create TM for this project, especially Danny Bickson, Brian Kent, Srikrishna Sridhar, Rajat Arya, Shawn Scully, and Alice Zheng. This work is partially supported by the National Science Foundation (NSF) under contract number IIS0746930. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the supporting institutions.

