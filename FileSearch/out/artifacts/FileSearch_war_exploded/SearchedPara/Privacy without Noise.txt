 This paper presents several results on statistical databas e privacy. We first point out a serious vulnerability in a widel y-accepted approach which perturbs query results with addi-tive noise. We then show that for sum queries which aggre-gate across all records, when the dataset is sufficiently larg e, the inherent uncertainty associated with unknown quantiti es is enough to provide similar perturbation and the same pri-vacy can be obtained without external noise. Sum query is a surprisingly general primitive supporting a large number o f data mining algorithms such as SVD, PCA, k -means, ID3, SVM, EM, and all the algorithms in the statistical query model. We derive privacy conditions for sum queries and provide the first mathematical proof for the intuition that aggregates across a large number of individuals is private using a widely accepted notion of privacy. We also show how the results can be used to construct simulatable query auditing algorithms with stronger privacy.
 Categories and Subject Descriptors: H.2.0 [ Information Systems ]: Database Management  X  security, integrity and protection ; G.3 [ Mathematics of Computing ]: Probabil-ity and Statistics  X  statistical computing General Terms: Algorithms, Security and Theory Keywords: differential privacy, sum queries, simulatable query auditing
We consider privacy issues in statistical database and data mining where queries are executed on data collected from a large number of individuals. The goal is to discover sta-tistical patterns and a major challenge is to release such  X  Part of the work was performed when the author was a Ph.D. student at Computer Science Division, University of California, Berkeley.  X 
A full version of this paper, with detailed dis-cussion and complete proofs, is available at http://bid.berkeley.edu/projects/p4p/papers/pwn-ful l.pdf. information while preserving the privacy of the individual s.
Statistical database privacy has been extensively studied since the 1970 X  X . The early results were mixed and not rig-orous, mostly due to the lack of a general notion of privacy. A newly emerging line of work [7, 1, 11, 10, 9, 15] strive to provide privacy definition and protections as rigorous as those in cryptography. Current results state that query re-sults need to be perturbed by random noise with sufficient variance in order to maintain privacy.

In this paper we show that noise is not essential for pri-vacy. On one hand, the additive noise approach has a vulner-ability that allows for easy exploitation in a real-world de -ployment. On the other hand, for some types of queries, the inherent randomness associated with unknown quantities is enough to provide similar perturbation and the same notion of privacy can be obtained without external noise. One ex-ample is sum queries that aggregate across many records. It is widely believed in traditional research that aggregates are privacy-preserving since they reveal only  X  X lobal X  inform a-tion [15]. Such belief is in effect in many practical systems i n the real-world (e.g., recommendation and voting systems). However, there is currently no formal substantiation of suc h an assessment. Ours is the first to provide a rigorous proof for this conjecture using a widely accepted notion of privac y. Besides the theoretical significance for furthering our und er-standing on privacy, our results also have practical implic a-tions: As collecting and processing large amount of data is a reality now, the asymptotic state is not so far away, the conditions derived in this paper can be applied.

Our results are also relevant in distributed data mining and secure multiparty computation. By showing that, when some conditions are met, releasing certain information ac-curately does not cause privacy breach, one could afford to reveal some intermediate results. This could significantly reduce the complexity of the protocol and a solution that is practical at realistically large scale can be obtained. In p ar-ticular, our results close a gap in a previously proposed pra c-tical data analysis framework that used vector addition to compute many useful algorithms [4, 8]. They all treated the intermediate sums as public. This leaves open the question whether these protocols provide enough privacy protection . Our results validate this approach.
Let D be an arbitrary domain. A statistical database is modeled as a vector d  X  D n . We use d i to denote the i th entry (also called row or record) of d . We assume the data records are drawn i.i.d. from D according to some distribu-tion D . Each row contains information about an individual. The hamming distance H ( d, d  X  ) between two databases d, d is the number of entries on which they differ.
We consider queries in the form f ( d ) = P n i =1 g ( d i g ( d i ) = [ g 1 ( d i ) , g 2 ( d i ) , . . . , g m ( d f is the sum of n m -vectors, one computed for each record. g is also called the j th query and m is the maximum number of queries allowed for the lifetime of the database.
It has been shown that this simple form is a surprisingly powerful tool for computing a large number of popular sta-tistical analysis algorithms [6, 4, 8]. Examples include EM algorithms, Naive Bayes, SVD, PCA, k -means, etc., and all the algorithms in the statistical query model. Please see [6 , 4, 8] for more details.
There are several formalizations of privacy for private dat a analysis but so far the strongest achievable privacy is differ-ential privacy , introduced in [11], further refined by [10, 9], and adopted by many latest works such as [3, 15, 13, 2]. In-tuitively, a mechanism is private if it ensures that the risk to one X  X  privacy should not substantially increase as a result of participating in a statistical database. Differential priv acy captures this intuition and is defined as
Definition 1 (Differential Privacy [11, 10]).  X   X ,  X   X  0 , an algorithm A f gives (  X ,  X  ) -differential privacy with re-spect to a query function f if for all S  X  Range ( A f ) , for all d, d  X   X  D n such that H ( d, d  X  ) = 1 Several solutions achieve differential privacy (e.g., [10, 3, 15, 13, 2]). Most of them are based on perturbing the response with additive noise. The initial work [11] used Laplace nois e and [10] extended the results to gaussian and binomial. [2, 13, 5] do not use additive noise directly on the results but still perturb the computation (e.g., the objective functio n [5]) with external randomness in some way.
We model the adversary X  X  knowledge about the data as follows:
A DVERSARY M ODEL Given a database d with n records drawn i.i.d. from some distribution D , there are sufficiently large number of records about which the adversary possesses no additional information besides D .

This model is only slightly weaker than some existing schemes and equivalent to many others. This can be seen from several perspectives: (1) We make no assumption about distribution D other than that it is public, which is consis-tent with many other works such as [14, 12, 2], and satisfies some mild requirements which are made precise in theorem 1. (2) The assumption that a substantial number of records are concealed from the adversary is realistic in many applic a-tions. Any large data sets, if collected and stored in a secur e manner, satisfy this condition (e.g., voting and census dat a that only publish aggregate statistics). In fact, works in query auditing that consider probabilistic compromise [14 , 12] are all in this model as well since they all treat the data as random variables. (3) The model allows other records to be compromised in an arbitrary way. The privacy depends solely on the unknown quantities.
The idea of achieving privacy by adding noise is natural and intuitive. However, noise is neither effective nor nec-essary for privacy. There is a serious vulnerability with al l response perturbation solutions: namely limiting the num-ber of queries, which is a crucial means for them to prevent leakage (as the noise is zero-mean and independently gen-erated for each query, its effectiveness can be canceled by posing related queries), cannot be enforced when there are shared items between multiple databases each administrate d by an autonomous entity. An adversary could query these databases about the shared records. It is impossible for one database to adjust the noise level and the query bound to account for possible leakage by other databases. By pooling all the queries from different databases, the number could exceed the threshold and allow the adversary to get substan-tial information about the record. And such a threat is real: It is very common nowadays for a user to have profiles in multiple online vendors. Relying on noise alone is not effec-tive in protecting privacy. We will show in section 6 that our condition is actually stronger than additive noise thus is more secure against this attack.

On the other hand, there are  X  X afe X  functions that are insensitive to small perturbation to the input in the same spirit as differential privacy and can be published accurate ly without sacrificing privacy. Under certain conditions, sum is one such function.
We first introduce a lemma for achieving differential pri-vacy in terms of perturbation to each individual record:
Lemma 1. A mechanism is (  X ,  X  ) -private if  X  d  X  D n ,  X  { 1 , . . . , n } , each element of g  X  i is independently perturbed with additive random noise following gaussian distributio n N ( ,  X  2 ) with  X  2  X  2 m 2 log(2 m/ X  ) / X  2 .
 This lemma states that, differential privacy can be achieved if each element of g ( d  X  i ) is perturbed with independent gaus-sian random noise with sufficiently large variance. Previous works achieve this via external noise. In the case of sum queries, by multidimensional central limit theorem (CLT), when n is large, the quantity  X   X  i = P n i =1 ,i 6 =  X  i in distribution to a multivariate gaussian distribution wh ich potentially could provide adequate protection. The difficul ty is that the limit distribution may have a non-diagonal co-variance matrix. In other words, the perturbations to each element of the g j ( d  X  i ) may not be independent.
However, it is still possible that this type of perturbation could guarantee at least the same privacy. Let I m be the m  X  m identity matrix. The intuition is, in the case of inde-pendent perturbation such as [1, 11, 7] etc., the noise added to the vector g ( d  X  i ) corresponds to an m -dimensional mul-tivariate gaussian random variable with covariance matrix  X 
I m . The surfaces of equal probability are m -dimensional hyperspheres. This means the amount of noise is the same in all directions. In our case, the covariance matrix of  X  may not even be diagonal. The surfaces of equal proba-bility are m -dimensional ellipsoids whose axes of symmetry are given by the principal components (the eigenvectors) of Figure 1: (a) Independent and (b) non-independent gaussian perturbations in 2-dimensional case. (b) has variance  X  2 along its minor axis. Note how the perturbation in (b)  X  X nvelops X  that in (a). the covariance matrix. The length of the ellipsoid along the i th axis is proportional to the eigenvalue associated with the corresponding eigenvector. The perturbation to differ-ent directions are asymmetric. However, it is reasonable to speculate that, if the noise in the direction of the eigenvec tor associated with the smallest eigenvalue, which corresponds to the smallest variance, is greater than the required thres h-old, then perhaps this perturbation, although maybe non-independent for each query, can also provide the same level of privacy, because the perturbations in the other directio ns are all greater. The idea is illustrated in figure 1. This intuition is indeed correct and we can prove:
Theorem 1 (Main). Let a i = g ( d i )  X  [0 , 1] m . Assum-ing a 1 , . . . , a n are i.i.d with E [ a i ] =  X  and E [ a V &lt;  X  , the summation is (  X ,  X  ) -private if n is sufficiently large and where  X  min ( V ) is the smallest eigenvalue of matrix V .
Or equivalently, let A = [ a 1 , . . . , a n ] be the query answer matrix and X = A ( I n  X  1 n e n e T n ) where e n is the column vector of n 1s. The summation is (  X ,  X  ) -private if where  X  m ( X ) is the m -th largest singular value of X .
Our results can be used in query auditing which is another approach to statistical database privacy. Instead of pertu rb-ing the responses, it restricts queries that can cause priva cy breach. Since they return accurate answers to other queries , the notion of privacy is different from that used by the per-turbation approach. Latest works regarding sum queries [12, 14] consider probabilistic compromise for bounded range data where a significant change in the adversary X  X  confi-dence about the range of a data record is considered privacy breach. We are not aware of any query auditing work that maintains differential privacy.

Query auditing can operate in online mode where an au-ditor must decide if answering a new query will infringe pri-vacy. Online auditing is tricky because the auditor X  X  denia l may leak information [12]. To handle this, [12] introduced the notion of simulatable auditing . The idea is that an on-line auditor X  X  decision should be based on information that is available to the adversary who can then simulate the de-cision process thus no information is leaked by the denials. [14, 12] achieve simulatability by basing the decision not o n the data but a sample of the underlying distribution.
Using our privacy conditions, we can handle online and offline auditing in a uniform way. Our scheme has a few contributions: (1) The privacy obtained is differential pri-vacy . Not only is this result stronger, it also bridges the two lines of work: We use auditing to verify the privacy condi-tions that stem from perturbation results. (2) We introduce a relaxed definition of simulatability that still guarantee s privacy but allows the auditor to use the data in its deci-sion making process. (3) Our method is agnostic to query language and supports more general queries that allow for arbitrary functions on each record.
Online auditing requires quick response. We now derive a simple method for detecting unsafe queries quickly. Suppos e we have answered k queries which are all deemed safe. Let X | k be the matrix of X restricted to the first k rows. Given the ( k + 1)-th query, the condition becomes Adding a new row to X | k can be modeled as x k +1 is a 1  X  n vector whose i th element is the current query evaluated on the i -th record and shifted by its mean. Such perturbation will cause changes to the singular values of ma -trix X . Using the results from matrix perturbation theory, in particular Weyl theorem [16], we can derive a simple nec-essary condition for the privacy of this ( k + 1)-th query:
Theorem 2. The ( k + 1) -th query is unsafe if This is a simple condition. k x k +1 k 2 can be easily computed as a by-product of the query. This result also has an intuitiv e interpretation. Note that k x k +1 k 2 is the standard deviation. The theorem states if the query results have small variance over the users, the aggregate may cause privacy breach. The well-known phenomena that aggregates of very sparse data are not private is a special case of such breach.
To ensure no leakage is caused by the denials, the defi-nition of simulatability in [12] prohibits the query audito r from using the data when making a decision. This is too restrictive. The essence of simulatability is ensuring tha t the adversary, who has no access to the data, can derive the same information as it could from seeing the auditor X  X  response. Forbidding the auditor from using the data in its decision making process is sufficient but not necessary. A very similar notion has been used extensively in cryptogra-phy (e.g., in defining zero-knowledge), yet it is rarely the case in cryptography that the information to be protected is not allowed to be used. In fact in the construction of zero-knowledge proofs, the secret must be used otherwise by definition the proof will not be accepted. The point is, as long as the adversary could simulate the output using only public information, it does not reveal more. We thus modify the definition in keeping with the cryptography approach:
Definition 2 ((  X ,  X  )-Simulatablity). Let d be drawn from distribution D n . Let Q k be the set of k queries and A k  X  1 the answers to the first ( k  X  1) ones. A safety checking mechanism Safe is a polynomial time algorithm that takes as input d , Q k , A k  X  1 and D . Safe is simulatable if, for any polynomial time (in n and k ) simulator Sim such that, for sufficiently large n The probability is taken over the randomness in the distri-bution D and the coin tosses of Safe and the simulator.
A query auditing algorithm A Safe that uses Safe  X  X  output to make decisions about whether to deny a query is simulatable if Safe is simulatable.
A simulatable query auditing algorithm with differential privacy is presented in figure 2. For simplicity, we only used the original condition in equation 3 which is equivalent to the one in theorem 1. It is straightforward to use theorem 2 to filter out unsafe queries quickly.
 Figure 2: Safety Check and Query Auditing Algorithm.
Theorem 3. The online auditing scheme is simulatable.
While our scheme provides accurate responses, it is more secure than the response perturbation solutions in the con-text of multiple databases. First, our scheme incorporates the idea from query auditing and handles repeated or related queries by denying them. The simple attack via repeated or related queries to reduce the variance of the perturbation does not work. Second, we show in the following that from querying a single database that implements our access mech-anism, the adversary could obtain less accurate estimate of a record. It may appear counterintuitive that an access mech-anism that provides noisy responses is less private than one that returns accurate results. This is not the case because i n addition to limiting the number of queries, our method also enforces conditions on the covariance matrix of the result vector which is more stringent.

Wlog, suppose d 1  X  R is a record shared among database d and others and the adversary is trying to obtain information on d 1 . The adversary could obtain m queries containing d 1 from one database, represented using the m -vector s = d a +  X  where a = [1 , 1 , . . . , 1] T and  X   X  N (( n  X  1) ,  X ). Let  X  2 be the lower bound on the safe variance (as defined in theorem 1), it must be that  X  min ( X )  X   X  2 . By averaging, the adversary can obtain where  X  0  X  N (0 ,  X ). The adversary could reduce the per-turbation on d 1 by computing the linear combinations of the elements of . Let u  X  R m and u T a =  X  (i.e. the sum of u  X  X  elements is  X  ), then u T = d 1  X  + u T  X  0 and an estimate Consider the variance (note that the r.v. is a scalar) Since  X  is a scalar, we have  X  2 =  X  X  T = u T aa T u = u T where A = aa T is a m  X  m matrix with all entries being 1s. The above becomes m is the case when the perturbations are all independent. It is interesting to notice that independent perturbation, as is done in the output perturbation solutions, allows the most effective noise reduction thus they are most vulnerable to the multiple databases attack. The equality holds iff a is an eigenvector of  X . In all other situations, our scheme is strictly better.
