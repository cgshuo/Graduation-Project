 Weighted Association Rule Mining has been proposed as a method of generating a com-pact rule base whose rules contain items that are of most interest to the user [2,8,10,11]. Items are typically weighted based on background domain knowledge. For example, items in a market basket dataset may be wei ghted based on the profit they generate. However, in many applications pre-assignment of weights is not practical. In high di-mensional datasets containing thousands of different items it may not be feasible to gather domain specific information on every single item, especially in a dynamically changing environment. In such situations it is more practical to exploit domain infor-mation to set weights for only a small subset of items (which we refer to as landmark items) and to then estimate the weights of the rest through the use of a suitable inter-polation mechanism. This research addresses the issue of constructing a suitable model which will facilitate the estimation of unknown weights in terms of a given small subset of items with known weights.

Another key issue that needs to be addressed is the validity of assigning item weights based on domain specific input alone. Typically, items are supplied weights based on their perceived importance, for example the profit that they generate. However, such weight assessments are made in isolation to other items and thus do not account for the indirect profit that an item generates by promoting the sale of other items which may be of high profit. For example, retailers often reduce their profit margin on items that already have relatively low profit and ma rket them as a package d eal involving high profit items. A concrete example is a disc ount on a mobile handset that is conditional on the customer signing a long term contract with the phone company involved. In such situations, the  X  X ow profit X  item (mobile handset) is used as an incentive to entice cus-tomers into buying the high profit items (calling plan contract). Clearly, in such contexts the actual profit margin of the low profit ite m does not accurately reflect its importance. Thus one of the premises of this research is that domain input on item weighting even when available may not be sufficient by itself in characterizing the importance of an item. Transactional linkages between items add value to domain sp ecific information and when these two inputs are combined in a suitable manner a more accurate assess-ment can be made on an item X  X  importance.

Two major contributions of this research are the development of a model that ex-presses the unknown weights of items in terms of known weights (landmark weights) and an interpolation mechanism that estimates weights by taking into account linkages between items that occur together. The rest of the paper is organized as follows. In the next section, we examine previous work in the area of weighted association rule mining. In Section 3 we give a formal definition of the weighted estimation problem. Section 4 presents our model for weight estimation. Our experimental results are presented in Section 5. Finally we summarize our research contributions in Section 6. In the context of weighted association rule mining a number of different schemes have been proposed for item weighting. Most of the schemes propose that domain informa-tion be utilized for setting weights for items. Tao et al. [9], Cai et al. [2] and Sanjay et al. [6] propose that item profit be utilized for assigning weight s in retail environ-ments for items while Yan et al. [11] use page dwelling time to assign weights in a web click stream data environment. More recent work reported in [8,3,4] took a different approach to the weight assignment probl em. Sun and Bai introduced the concept of w -support which assigns weights to items based on the properties of transactions in a given dataset thus removing the need for domain specific input. The dataset was first converted into a bipartite graph, with one set of nodes representing the items and the other set of nodes representing the transactions. The w -support was then calculated by counting the links between items and the transactions that the items appeared in. Koh et al. [3] proposed a Valency model where the weight of an item was defined as a linear combination of its purity and its connectivity . Purity takes into account the number of items that a given item interacts with, while Connectivity accounted for the degree of interaction between an item and its neighboring items. Pears et al. [4] used a weight in-ference mechanism based on Eigenvectors der ived from a transactional dataset. Given a dataset D , the covariance of every pair of items that occur in transactions across D was expressed in terms of its covariance matrix M . The first Eigenvector derived from the covariance matrix M was used to assign weights to items.

None of the work done so far in weight inference directly addresses the issue of weight estimation from a set of known landmark weights. A simple extension such as restricting the set of items input to only include the unknown items will not suffice as the weights will be computed only on the basis of the interactions between the set of unknown items and the interactions with the landmark items will be neglected. As such, none of the above work can directly be utilized in their entirety. The weight fitting problem that we frame is to estimate the overall weight of items and not simply the domain specific weights for items that are unspecified ( i.e. , item not in the landmark set, L ). As stated in the introduction, certain items that are perceived to be of low importance on the basis of domain knowledge may actually assume a higher importance than their perceived rating due to strong interactions with items that are of high importance. In our problem setting we associate with each item a domain weight and an interaction weight. Domain weights dw are only available for the set of landmark items, L , whereas interaction weights iw areavailablefor all items as these can be deduced from the co-occurrences of items given a transaction database.
Given a set of items I , a subset L of landmark items where L  X  I , and a transaction database D , the acquired weight w i of a given item i is determined by: where N represents the set of neighbors of item i , dw i  X  0 when i  X  L and dw i = 0 ,otherwise . Thus an item i acquires a weight from its interactions with its neighbors who transmit their own weights in a quantity proportional to the degree of interaction, iw . Neighbors that are landmarks transmit their domain weights as well as their ac-quired weights while neighbors in the set M of items that are not landmarks only trans-mit their acquired weights. In the context of this research a neighbor of a given item i is taken to be any item j that co-occurs with item i when taken across the database D .In effect, an item that is a landmark item contributes both its own domain weight and the weight acquired from its neighbors, while non landmark items simply transmit their ac-quired weight which in turn was obtained from their own interactions with neighboring items, which could include landmark items. Henceforth we shall abbreviate the term acquired weight simply by the term weight , except when it is necessary to emphasize the composite nature of the weight assignment.

The accuracy of the weight estimation m echanism expressed by Equation 1 above is dependent on how the interaction component is modeled. This specification is a model-ing issue which does not impact on the general definition of the problem and so further discussion of this component is deferred to Section 4 which deals with the model de-veloped to solve the problem.

For a given set of landmark items L the problem can now be stated formally as follows: return all items i  X  H where
H = { i | i  X  I is in the top p % of items when ranked on acquired weight from Eq (1) } (2) where p is a user-supplied threshold that determines the minimum overall weight to be returned to the user for use in a subsequent weighted association rule mining phase. In this section we present a model that we use as the basis of the solution to the weight estimation problem. We use a graph structure ( N,E ) where nodes are represented by items and edges by interactions between pairs of items. Each node i is associated with the weight w i of the item, while an edge between items i and j is represented by G ( i, j ) where G is the Gini information index [5]. The Gini information index G ( i, j ) measures the degree of dependence of item j on item i . A high value of G ( i, j ) indicates that item j occurs with a high degree of probability whenever item i occurs, likewise, the non occurrence of item i leads to the non occurrence of item j with a high degree of probability. Thus the G value captures the degree of dependence between pairs of items. The higher the dependence of item j on item i , the higher the proportion of weight transmission from item i to item j , and as such the Gini index can be used to express the interaction weight component of the model.

As an illustrative example, consider two different scenarios with four items whereby we have item k with unknown domain weight and three other items i 1 , i 2 ,and i 3 with known domain weights. In the firs t case, (Figure 1 (a)) each of i 1 , i 2 and i 3 have domain specified weights of 0.8 and each interacts with item k with a G value of 0.9. The WeightTransmitter model that we propose ret urns a weight value of 2.4 for each of the items, which when normalized yields a value of 0.89. With the same set of items but with domain specific weights set to 0.1 (Figure 1(b)) all weights for the four items end up with the same value of 0.3, which when normalized yields a value of 0.11. This example illustrates the importance of n eighborhood in the weight estimation process; an item which is strongly connected through high G values to high weight items will acquire a high weight, whereas the same item when connected to low weight items will acquire a low weight, regardless of the strength of the connections.

We now present the WeightTransmitter model by expressing the weight of a given item k in terms of the weights of its neighbors as: where S 1 represents the set of neighbors of item i whose domain supplied weight dw i components are known in advance and S 2 is the set of neighbors of item i whose domain weights are unknown. Now i  X  S 1 G ( k,i )+ i  X  S 2 G ( k,i ) represents a known quan-tity c 1 k , since all G index values can be calculated fro m the transaction database. The dw i terms in set S 1 also represent known quantities. We denote j  X  S 1 G ( k,i ) .dw i by c 2 k . Substituting the known constants c 1 k , c 2 k in the above equation and re-arranging the terms gives: where S = S 1  X  S 2 represents the complete neighborhood of item k. The above now represents a system of k linear simultaneous equations in k unknowns which has an exact solution with the Gaussian elimination method which we employ. The algorithm below illustrates how the WeightTransmitter model fits in with the traditional weighted association rule mining algorithm.
 Our evaluation is divided into three sections: weight estimation evaluation, rule evalua-tion, and runtime evaluation. In the next section we describe the datasets that were used in these evaluations. 5.1 Datasets Our experiments were conducted on five real-world datasets which are described below.  X  Retail dataset. We used a retail market basket dataset supplied by a supermarket  X  Nasa weblog datasets. We also used two different web log files from the NASA  X  Computer Science Lab datasets. Finally, we used two datasets containing web 5.2 Weight Estimation Evaluation This evaluation was designed with three key objectives in mind. Firstly, to establish the degree of sampling required in order to achie ve convergence between estimated and ac-tual weight on the composite weight measure. Ideally, convergence should be achieved at a low level of sampling for the weight estimation process to be effective. Secondly, to identify items that were flagged as being lo w weight according to domain information but were assigned high weight values by the WeightTransmitter model. These items are potentially interesting as they highlight situations where domain knowledge is inade-quate to characterize the true importance of such items. Thirdly, we wanted to assess the level of accuracy achieved by the weight estimation process at the point of conver-gence. Since we had access to the domain wei ghts for the complete set of items we were able to establish the ground truth in terms of the composite weights by simply running WeightTransmitter with a landmark sampling level at 100%.

To evaluate the accuracy of the weights p roduced by the WeightTransmitter model we varied the percentage of landmark weights in the range 10% to 90% and tracked the overall accuracy and precision across the high weight items. We start with the accuracy evaluation. At each of the sampling levels 30 different runs were used that chose dif-ferent sets of landmark items at random. The accuracy measures presented represent an average of the measure taken across the 30 different trials.
 Weight Convergence and Accuracy Analysis. Accuracy was tracked using three dif-ferent measures: Correlation, Precision on high weight items, and Target Lift [7]. Target Lift is a measure commonly used to measure the lift in response rate that occurs when marketing offers are sent to a small set of customers who are likely to respond (identified through some prediction method) rather than a mass marketing campaign that targets the entire population. In the context of weight estimation the set of items returned by WeightTransmitter which it regards as hig h weight corresponds to the set of probable customers and the universe of items represents the entire customer population.
In the first analysis, we ran WeightTransmitter with the set of landmark weights as input and collected the results into the set S l . We then re-ran WeightTransmitter with the complete set of known weights as input and collected the results into the set S c .We then plotted the Pearson correlation between the two result sets against the sampling percentages that we used for the landmark weights. Figure 2 shows that there is a sta-bilization of correlation around the 30% mark; the average correlation value is 89%, with a standard deviation of 0.07. As expected, as the percentage of landmark items increases the greater is the degree of conve rgence between estimated weight and actual values on the composite weight value. Figure 2 shows that reasonable convergence of weights is achieved around the 30% mark.

For the second analysis each of the sets S l and S c were divided into two parts (bins): low and high . For each of the two sets, the top 10 percentile of items in terms of weight were allocated to high bin, and all other items to the low bin. Using the bins based on the set S c ( i.e. , by running WeightTransmitter at 100% level of sampling) to establish a benchmark we were able to compute precision on the high weight category (bin). Figure 3 shows the precision in the high weight weight category as a function of the sampling percentage. At 30% the average precision value for the high weight items is 80%, with a standard deviation of 0.06.

In the third analysis we calculated the target lift. Table 1 shows that the lift in the true positive rate at a 30% sampling rate is much greater than 1 across all datasets, thus demonstrating the effectiveness of Weight Transmitter over a random weight assignment scheme in identifying high weight items.
 Profit Analysis. Our weight accuracy analysis in the previous section establishes the effectiveness of our model in accurately estimating composite weight. However, we were also interested in tracking our other r esearch premise which was the effect of the weighting scheme on items that interacted strongly with items that were known to have high weight. In particular, we were int erested in tracking the set of items ( H )where H = { i | i  X  I where i is in the top p percentile on the basis of composite weight but not on the basis of domain weight } . We were able to compute the set H as we had access to the weights of all items. For all items belonging to H we defined a profit measure ( P ) that took into account the amount of indirect profit that such items generated. The profit measure for a given item i  X  H was computed by taking the total profit ( P 1 ) over all transactions ( T 1 ) in which item i occurs and then subtracting from this value the total profit ( P 2 ) over all transactions ( T 2 ) in which item i does not occur.
In order to isolate the confounding effect of transactions in T 2 having more items than T 1 we restricted each of the transactions involved in T 2 to only have the neighbors of the item i under consideration. Furthermore, we also compensated for the differences in the sizes of T 1 and T 2 by scaling P 1 with the factor | T 2 | | T 1 | . where w ( k ) represents the weight of a high weight item k that is connected to item i and T is the set of all transactions in the trans action database. Equation 5 as defined above captures the indirect profit due to item i without the effects of the confounding factors just mentioned. However, the profit measure P by itself has little meaning unless it is compared with the profit generated by the set of items NH that remain low in weight without making the transition to the high wei ght category. For our premise that domain input on item weighting may not be sufficient by itself in characterizing the importance of an item the P values of items in the set H needs to be substantially higher than the profit values in the set NH . Table 2 shows that this is indeed the case as the values in the H column contains much higher values than the NH column for all of the datasets that we tracked. Table 2 contains the following columns; the percentage of items which have transited to the high weight category when transactional linkages are accounted for, average profit of items rated high by WeightTransmitter but not by domain weighting ( i.e. ,theset H ), average profit of items rated high by domain weighting ( i.e. ,theset H ), and average profit of items that were not rated high by WeightTransmitter ( i.e. , the set NH ).
 Sensitivity Analysis. Given that the WeightTransmitter model achieved a high level of precision at the relatively small sampling level of 30% we were interested in inves-tigating how robust the scheme was to changes in the data distribution. In particular, we were interested in tracking the sensitivity of Precision to the degree of variance in the data. Due to the fact that WeightTransmitter uses a sample defined over the set of landmark items, the question that arises is whether the error caused by the sampling re-mains stable or changes substantially when the underlying data distribution changes. To investigate this issue we used the Retail, Nasa (June), and Computer Science 1 datasets. Each weight value w in each of the selected datasets was perturbed by adding white Gaussian noise to it. Each weight value w for a given set was transformed into a weight value, w p = w + N (0 ,w/d ) ,where d is a parameter that controls the level of variance injected into the dataset. W e experimented with different values of d so as to obtain 3 levels of drift in variance from the baseline. The drift levels we used were 25%, 50% and 90% where drift level for a transformed dataset D is defined by: dard deviations across the baseline and perturbed datasets respectively. Figure 4 is the histogram of support distribution for the Computer Science dataset. The other datasets follow a similar distribution. The baseline represents the situation when the complete ground truth is known, i.e. , the domain weights for all items are known, thus enabling the composite weight to be calculated exactly with no error involved. As mentioned before we had access to the complete set of domain weights for each of the datasets that we experimented with, thus enabling us to measure the true deviation in precision with the degree of drift.

Table 3 shows that for the Retail dataset the deviation in Precision from the baseline ranged from 0 to 10.59%. In general as the l evel of sampling increased the error de-creased. The deviation showed some sensitivity to the degree of variance in the data; as the drift level increased the deviation tended to increase. However, even at the extreme drift level of 90%, the deviation was no more than 10%. A similar pattern was observed for the Nasa and Computer Science datasets although the extent of the decrease in pre-cision at the higher degrees of drift was on a smaller scale than with the Retail dataset. These results demonstrate that WeightTransmitter was not overly dependent on which items were chosen as landmarks, even with data that had a very high degree of variabil-ity. This is a very desirable feature of a we ight estimation mechan ism in general and in terms of WeightTransmitter it inspires m ore confidence that the good performance at the 30% sampling level will generalize to a wide variety of different datasets. 5.3 Rule Evaluation One of the major premises behind this research was that the true weight of an item is dependent not just on its individual importance, but also by its interaction with other items that it co-occurs with. For our premise to be true the rule base should contain rules of the form X  X  Y where X is a low weight item based on domain knowledge whereas Y is a highly rated item on the basis of domain knowledge. If such patterns occur then they signify that the set X of items appearing in rule antecedents should be weighted much more heavily than what is suggested on the basis of domain knowledge alone as such items co-occur strongly with highly weighted items.
 The rule base was generated by inputing the top p % of items produced by Weight-Transmitter to a standard rule generator. The rules generated for each dataset were sub-jected to a minimum support threshold of 0 . 03 , confidence threshold of 0.75 and a lift threshold of 1.0. We computed rule interest measures such as Coherence and All Con-fidence and ranked the rule bases by the Cohe rence measure. We then analyzed the rule base to look for patterns of the form X  X  Y as described above that either support or refute this premise. The top p parameter was set at 20% for the Retail dataset and at 40% for the rest of the datasets. Figure 5 shows a small sample of 4 rules produced on the Retail dataset that exhibit this pattern. The presence of such rules validates one of the major premises behind this research. The rule bases produced from the other 3 datasets also exhibited such patterns but could not be presented due to the limitations of space. In terms of the Retail environment the practical value of such rules is that although items such as 541145000000 and 210000 are low profit items they are nevertheless im portant as the purchase of these items leads to the purchase of the high profit item, 250000. It is also important to note that the rules above would not have been generated if the items were weighted merely on the basis of their domain weights (i.e profit margins) alone as they would have not met the top p % threshold and would thus not have participated in the rule generation phase. As such, this represents one of the key contributions of this research.

Rules 1 and 2 in Figure 5 reveal the existence of a clique of 3 items: 541145000000, 210000, and 250000 that interact with each other strongly as shown in Figure 6. In the WeightTransmitter model item 250000 tr ansmits its high domain weight to both items 541145000000 and 210000 in proportions G (3 , 1) and G (3 , 2) respectively, thus increasing the domain weights of item 1 ( 541145000000) and item 2 (210000). This results in transforming these two items into high weight items. At the same time each of items 1 and 2 transmit their respective domain weights to item 3 in proportion to G (1 , 3) and G (2 , 3) thus increasing the weight of item 3. This transmission of weights, although increasing the weight of item 3 does not have a significant effect as item 3 is already of high weight. 5.4 Runtime Evaluation As shown in the previous section the WeightTransmitter model leads to the discovery of valuable knowledge in the form of patterns that can be exploited in a useful manner. However, the model does introduce run time overheads in solving a system of linear equations. As such, our final experiment wa s to quantify what these overheads were and to ascertain whether the rule generation run time remained within reasonable bounds. Table 4 shows the runtime (measured in seconds) for our experiments with 30%, 60%, and 90% of items used as landmarks, along with the time taken to generate a rule base on the basis of domain knowledge alone, w ithout the use of the WeightTransmitter model. In generating the latter rule base we used exactly the same constraints on min-imum support, Confidence and Lift (with the same top p value) in order to keep the comparison fair. Table 4 reveals that the ru n time overhead introduced by WeightTrans-mitter does remain within reasonable bounds and that such overhead tends to decrease as a higher rate of landmark sampling is used. The decrease in run time at higher sam-pling levels is caused by the reduced number of operations required to transform the initial matrix into row echelon form due to the presence of more known values in the form of domain weights. The only result that goes against the above trend was with the Computer Science 2 dataset where the run time actually increased for the generation of the rule base built with the use of domain knowledge only. This was due to the larger number of items being returned in the top p list when compared to the list generated by WeightTransmitter. This resulted in a larger number of itemsets being generated which in turn resulted in a larger rule base, thus contributing to the increase in run time. This research has revealed that weight estimation based on a small set of landmark weights can be performed accurately through the use of the novel WeightTransmitter model that we introduced. Furthermore, we showed through a Profit Analysis conducted on ground truth data that a substantial percentage of items switched status from the low or moderate weight categories to the high weight category, thus supporting our premise that weight assessments on an item should not be made in isolation to other items.
The use of other methods other than simple random sampling to identify landmark items will be explored in future work. As alternatives to simple random sampling, we plan to investigate the use of stratified random sampling as well as entropy based meth-ods to identify influential items that will act as landmarks.

