 We propose a new scalable algorithm for the facility-location problem. We study the graph setting , where the cost of serv-ing a client from a facility is represented by the shortest-path distance on a graph. This setting is applicable to various problems arising in the Web and social media, and allows to leverage the inherent sparsity of such graphs.

To obtain truly scalable performance, we design a parallel algorithm that operates on clusters of shared-nothing ma-chines. In particular, we target modern Pregel-like architec-tures, and we implement our algorithm on Apache Giraph.
Our work builds upon previous results: a facility location algorithm for the PRAM model, a recent distance-sketching method for massive graphs, and a parallel algorithm to find-ing maximal independent sets. The main challenge is to adapt those building blocks to the distributed graph setting, while maintaining the approximation guarantee and limiting the amount of distributed communication. Extensive exper-imental results show that our algorithm scales gracefully to graphs with billions of edges, while, in terms of quality, being competitive with state-of-the-art sequential algorithms.
Facility location is a classic combinatorial-optimization problem. It has been widely studied in operations research [31, 39] and theoretical computer science [1, 11, 30, 48], and it has many different applications, e.g., in data compres-sion [9], grammar inference [23], information retrieval [49], and design of communication networks [43]. In the most ba-sic setting of the problem, we are given a set of facilities F , a set of clients C , and costs c ( f ) for opening a facility f  X  F and d ( c,f ) for serving a client c  X  C with a facility f  X  F . The goal is to select a subset of facilities to open so that all clients are served by an open facility and the total cost of opening the facilities plus serving the clients is minimized.
This work was supported by a Yahoo! Faculty Research and Engagement Program (FREP) award. This work was partially funded by a Google Faculty Award. ACM. ACM 978-1-4503-3794-6/15/10 $15.00 DOI: http://dx.doi. org/10.1145/2806416.2806508 .

Modern applications related to web graphs, large social networks, and other such massive-scale datasets, whose size may far exceed the memory of a single machine, could also benefit from facility location as a general-purpose optimiza-tion mechanism. Examples of such applications include plac-ing caches for content delivery on the Internet [24], place-ment of shopping centers on a road network, finding central nodes in a social network [7]. Similar objective functions arise in outbreak detection in networks [34].

We study the facility location problem in the graph set-ting, where the input consists of a graph with clients and fa-cilities being represented by vertices while the cost of serving a client from a facility is represented by their shortest-path distance on the graph. Our goal is to develop efficient dis-tributed algorithms for this problem setting.

Several sequential approximation algorithms [1, 11, 30, 48], as well as parallel/distributed algorithms [13, 4] have been devised for the facility location problem. Alas, adapt-ing existing methods to the distributed graph setting is chal-lenging, as the known algorithms present at least one of the following shortcomings: ( i ) they assume the input to be the full distance matrix between facilities and clients, which re-quires  X ( n 2 ) space and time to be materialized (where n is the number of vertices in the graph); ( ii ) they are sequential and assume that the input data reside in main memory.
In this paper, we present the first algorithm for the facility-location problem that addresses both issues outlined above. Our algorithm is designed for the Pregel model of computa-tion [38]. In particular, we implement our algorithm on Gi-raph [12], thus adding facility location to the toolbox of opti-mization problems that can be solved for very large datasets on modern computer clusters.
 Graph setting. Most works in the the area of theoretical computer science focus on the classical formulation of the facility-location problem, where the input consists of the full | F | X | C | set of distances [46]. Unfortunately, in this setting even algorithms with linear running time (in the size of input) are not practical when both | F | and | C | are large.
In many real-world problems the input for facility location can be represented as a graph G = ( V,E ), where F and C are vertices of G . Typically, the number of edges m = | E | is much smaller than | F || C | , i.e., the graph is sparse. A client c  X  C can be served by a facility f  X  F even if ( c,f ) 6 X  E , provided that there is a path in the graph from c to f , and the cost d ( c,f ) is the induced shortest-path distance . Hence, in this so-called graph-setting , we do not need to represent the full | F | X | C | set of distances. This setting poses non-trivial challenges, as such distances have to be computed throughout the execution of the algorithm. However, it is crucial to leverage the sparsity of a massive input graph.
Our approach is designed for this graph-based setting. We require that the time and space requirements of our algo-rithm be quasilinear functions of | E | . If the graph is sparse, as most real-world datasets are, this leads to a significantly more scalable algorithm. Thorup [48] has proposed the only previous (sequential) algorithm for the graph setting. Pregel model. To cope with large problem sizes mod-ern applications take advantage of distributed systems, such as MapReduce [19] and Hadoop, or of variants targeted to graph data, such as Pregel [38] and its open-source clones, Giraph [12] and GraphLab [36]. Such systems offer several advantages, among which, high scalability and a simple pro-gramming interface.

Our algorithm targets a parallel shared-nothing comput-ing environment. While other parallel algorithms have been proposed in the literature, our approach is the first one to target modern clusters. In particular, Blelloch and Tang-wongsan [4] proposed a parallel facility-location approxima-tion algorithm for the PRAM model. Our work extends this parallel algorithm to the more scalable Pregel model. Algorithm summary. Our approach has three phases: cility selection . All three phases are fully implemented in Giraph, and the code is open-source. 1
The first phase builds an all-distances sketch (ADS) that estimates the neighborhood function of each vertex. This sketch is used in the second phase to decide when to open a facility. Our sketch relies on the historic inverse probability (HIP) estimator, recently proposed by Cohen [15].

The facility-opening phase expands balls around facilities in parallel. It decides which facilities to open depending on the number of clients that reside within the facility-centered balls. To estimate the number of clients inside the balls, we use the sketch created in the previous phase.

Finally, the facility-selection phase removes duplicate as-signments of a client to more than one facility that might have been created due to the parallel nature of the algo-rithm. To accomplish this task, we need to compute a max-imal independent set (MIS) on the 2-hop graph of the open facilities. For this sub-problem, we design a distributed ver-sion of a recent greedy approximation algorithm [5].
Our main challenge is to adapt and combine previous re-sults to the distributed graph setting, while maintaining the approximation guarantee of the algorithm, and limiting the amount of distributed communication. Concretely, the con-tributions of this paper are as follows:  X  we provide the first Pregel solution for the facility-location  X  at the same time, we do not compromise on accuracy  X   X  our solution uses the sparse graph-based representation  X  our algorithm employs fundamental subproblems, all-https://github.com/gvrkiran/giraph-facility-location  X  we provide an extensive experimental evaluation that
The rest of the paper is organized as follows. In Section 2 we formally define our problem, and discuss the background techniques needed for our approach. Our algorithm, com-prising of the three phases outlined above, is detailed in Section 3, while an experimental evaluation of the different components of our method is presented in Section 4. In Sec-tion 5 we place our work in the context of relevant research, while Section 6 is a short conclusion. Problem definition. In the metric uncapacitated facility-location problem , we are given a set of facilities F and a set of clients C . For each facility f  X  F and client c  X  C , there is a cost c ( f ) for opening the facility f , and a cost d ( c,f ) for serving client c with facility f . The objective is to select a set of facilities S  X  F to open in order to minimize the objective function where d ( c,S ) is the distance of client c  X  C to its closest open facility, i.e., d ( c,S ) = min f  X  S d ( c,f ).
In this paper, we are interested in the graph setting of the facility-location problem, where we are also given a weighted graph G = ( V,E,w ), with w : E  X  R + a weight function on the edges of the graph. The sets of facilities and clients are subsets of the graph vertices ( F,C  X  V ). The distance between clients and facilities is given by the shortest-path distance on the weighted graph. We assume that facility costs and edge weights be polynomial in | V | .

We focus on the graph setting of the facility-location prob-lem so as to leverage the sparsity of real-world graphs, such as web graphs and social networks. This allows us to develop practical and scalable algorithms, with running time being quasilinear in the size of the input graph. Previous algo-rithms for our problem, are sequential or require all pairwise vertex distances which is not practical for large graphs.
We consider both directed and undirected graphs. For the case of undirected graphs our algorithm offers a provable ap-proximation guarantee, while for the case of directed graphs the algorithm provides a practical heuristic. We focus on the case when F = C = V , although all our claims hold for the more general case when F,C  X  V .
 The Giraph platform. The algorithms presented in this paper are designed for the Giraph platform [12], an Apache implementation of the Pregel computational paradigm. Pregel is based on the Bulk Synchronous Parallel (BSP) computa-tion model, and can be summarized by the motto  X  X hink like a vertex X  [38]. At the beginning of the computation, the vertices of the graph are distributed across worker tasks running on different machines on a cluster. Computation proceeds as a sequence of iterations called supersteps. Al-gorithms are expressed in a vertex-centric fashion inside a vertex.compute() function, which gets called on each ver-tex exactly once in every superstep. The computation in-volves three activities: receiving messages from the previous superstep, updating the local value of the vertex, and send-ing messages to other vertices. Algorithm 1: Build ADS sequentially Input : Graph G ( V,E )
Output : ADS of G for v  X  V do 2 ADS( v ) =  X  3 BKMH( v ) =  X  // Bottom-k min-hash for v  X  V and u  X  X  V sorted by d ( v ) } do 5 if r ( u ) &lt; max r (BKMH( v )) then 6 ADS( v )  X  ADS( v )  X  ( u,d ( v,u )) 7 BKMH( v )  X  bottomK(BKMH( v )  X  u ) return ADS
Pregel also provides aggregators , a mechanism used for co-ordination and monitoring, as well as for computing global statistics. Each vertex can write a value to an aggregator in superstep t , the system combines those values via a reduc-tion operator, and the resulting value is made available to all vertices in superstep t + 1.

Giraph adds an optional master.compute() to the Pregel model. This function performs centralized computation, and is executed by a single master task before each superstep. Aggregators written by workers are read by the master in the following superstep, while aggregators written by the master are read by workers in the same superstep. We employ this feature in our implementation (see Section 3.5).
 Approximate neighborhoods (ADS). The all-distances sketch (ADS) is a probabilistic data structure for approxi-mating the neighborhood function of a graph [15]. ADS aims to answer the query  X  X ow many vertices are within distance d from vertex v ? X  . ADS maintains a logarithmic-size sketch for each vertex. In the sequential computational model, the total time to build the ADS is quasilinear in the number of graph edges. ADS-based techniques have been used to esti-mate efficiently graph properties, such as the distance dis-tribution, effective diameter, and vertex similarities [6, 16].
The ADS of a vertex v consists of a random sample of vertices. The probability that a vertex u is included in the sketch of vertex v decreases with the distance d ( u,v ). The sketch contains not only the vertex u but also the distance d ( u,v ). The ADS can be thought as an extension of the simpler min-hash sketch [8, 14], which has been used for approximate distinct counting [14, 20, 21], and for similarity estimation [8, 14]. The ADS of v is simply the union of the min-hash sketches of all the sets of the ` closest vertices to v , for each possible value of ` . Min-hash sketches have a parameter k that controls the trade-off between size and accuracy: a larger k entails a better approximation at the expense of a larger sketch. The size of the ADS is O ( k log n ).
Our algorithm relies heavily on a recently-proposed ADS structure, the historic inverse probability (HIP) estimator [15], which extends significantly previous variants and offers novel estimation capabilities. In particular, HIP can be used to answer neighborhood queries for both unweighted and weighted graphs. It can also be used to answer predicated neighborhood queries, that is, to approximate the number of vertices in a neighborhood that satisfy a certain predicate on vertex attributes. We use this latter feature in to exclude already served clients from the estimation of the number of clients within a ball (see Section 3).
 Algorithm 2: ADS in Giraph. Vertex.Compute() Input : vertex value v , edge values E , messages M Output : updated vertex value v 0
Data : ADS =  X  ; BKMH =  X  // state variables are stored in the vertex v
OutMsgs =  X  for m  X  M do 3 for ( u,d )  X  m. getEntries () do 4 if r ( u ) &lt; max r ( BKMH ) then 5 ADS  X  ADS  X  ( u,d ) 6 CleanUp(ADS( u ) ,d ) // for each 7 BKMH  X  bottomK(BKMH  X  u ) 8 OutMsgs  X  OutMsgs  X  ( u,d + e ( u,v )) for e  X  E do 10 sendMsgTo( e, OutMsgs)
The sequential version of ADS is presented in Algorithm 1, and our adaptation for Giraph is shown in Algorithm 2. The algorithm works for both weighted and unweighted graphs. A cleanup operation is performed in line 6, which removes those entries for which the hash is not in the bottom-k min-hashes for a given distance (this may happen only for weighted graphs, when the vertices processed by ADS are discovered by a BFS and are not sorted by their distance from a given vertex). While the cleanup step can be time-consuming, we do not need to perform it in each superstep, but only periodically when the size of the ADS becomes too large. For unweighted graphs the cleanup is not needed.
As discussed earlier, our algorithm consists of three phases: cility selection . This section presents the main body of the algorithm: phases ( ii ) and ( iii ). In the pseudocode pre-sented henceforward, for and while loops are meant to be parallel i.e., executed by all vertices in parallel. Our method is inspired by the algorithm of Blelloch and Tangwongsan [4], which is developed for the PRAM model. We adapt this algorithm to a Pregel-like platform, and also extend it to the graph setting, as discussed in Section 2.
The algorithm operates in two phases: facility opening, and facility selection. It starts with all facilities being un-opened and all clients being unfrozen .

The algorithm maintains a graph H that represents the connections between clients and open facilities. Initially, H has F and C as vertices and an empty set of edges. During the execution of the algorithm, if a client c is to be served by a facility f , the edge ( c,f ) is added in H . During the facility-opening phase, it is possible for a client to be connected to more than one facility. However, in the facility-selection phase, redundant facilities are closed and it is ensured that each client is connected to exactly one facility.

In the facility-opening phase, each client tries to reach a facility by expanding a ball with radius  X  , in parallel. The expansion phase is iterative, and in each iteration the radius grows by a factor of (1 + ), where is a parameter that provides an accuracy-efficiency trade-off.

The radius of the ball of a client c is denoted by  X  ( c ). If a client c is unfrozen, the radius of its corresponding ball is set to the current global value  X  , while if a client c gets frozen it does not increase the radius of its ball anymore. When a facility f is reached by a sufficiently large number of clients it is declared open . In particular, a facility f is opened when
For a newly opened facility f , all clients c within radius  X  from f are frozen, and the edges ( c,f ) are added in H . The facility-opening phase continues as long as there is at least one unopened facility and at least one unfrozen client.
At this point, as mentioned earlier, a client may be served by more than one facility. In the facility-selection phase, the algorithm closes the facilities that are not necessary, as their clients can be served by other nearby facilities. This step relies on computing a maximal independent set (MIS) in an appropriately-defined graph H : the open facilities are the vertices of H and there is an edge between two facilities f , f b if and only if there is a client c that is connected to both f a and f b . It is easy to see that a maximal independent set S in H has the property that each client c is connected to exactly one facility. Clients whose facility is not in S are assigned to the nearest open facility.

To complete the description of the algorithm, the initial ball radius is set to  X  0 =  X  m 2 (1 + ), where m = | F || C | and  X  is defined as follows. For each client c  X  C we set and then  X  = max c  X  C  X  c . Blelloch and Tangwongsan [4] prove the following theorem on the quality of approximation. Theorem 1 ([4]) . For any &gt; 0 , the algorithm of Blelloch and Tangwongsan has an approximation guarantee of 3 + ,
We now discuss how to adapt the algorithm of Blelloch and Tangwongsan to the graph setting discussed in Sec-tion 2, as well as in a Pregel-like platform, such as Apache Giraph, discussed in Section 2. The main challenges we need to tackle are the following:  X  leverage the sparsity of the graph G = ( V,E ) to avoid  X  compute efficiently, in a distributed manner, a maximal
The first challenge, i.e., exploiting the sparsity of the graph, boils down to being able to check whether Equation (1) is satisfied so as to decide when to open a facility.

To this end, we rearrange the left-hand side of Equa-tion (1), so as to be able to evaluate it by means of the ADS algorithm discussed in Section 2. Observe that the  X   X  X  take values in the range R = {  X  0 , (1 + )  X  0 , (1 + ) 2
For every facility f , let N ( f,d ) be the number of clients within distance d from f , while let n ( f,d ) be the number of clients whose distance from f is in the range ( d/ (1 + ) ,d ]. Suppose that all clients within distance  X   X  R from facility f are unfrozen. In this case, we know that for all these unfrozen clients  X  ( c ) =  X  , so we can rewrite the left-hand side of Equation (1) as follows: where we replace  X  ( c ) X  X  with  X  and rearrange the terms of the summation by grouping terms with the same value.
If some clients within distance  X  from f are frozen, the former claim might not hold anymore, and we need a more sophisticated solution. Our goal is then to maintain an ap-proximation of the the left-hand side of Equation (1) incre-mentally. Let q ( f ) denote the current approximation com-puted by our algorithm. Also, for each facility f , let  X  be the number of unfrozen clients within distance d from f , while let  X  n ( f,d ) be the number of unfrozen clients at dis-tance in the range ( d/ (1 + ) ,d ]. At each iteration of the ball-expansion phase, we add a term t ( f, X  ) to q ( f ). This term accounts for the increase in contribution to q ( f ) due to the newly-reached unfrozen clients, while subtracting excess contribution due to previous iterations.
 The increase in contribution t ( f, X  ) is defined as if  X  =  X  0 (no excess contribution to be subtracted), and otherwise. The term t ( f, X  ) is added to q ( f ) in each iteration of the algorithm for the current value of radius  X  .
The terms  X  N ( f,d ) can be computed efficiently in a dis-tributed fashion by employing the ADS. Given that  X  n ( f,d ) =  X  N ( f,d )  X   X  N ( f,d/ (1 + )), it follows that also the left-hand side of Equation (1) can be computed efficiently in a dis-tributed fashion. To show the validity of our approximation we need the following definition.
 Definition 2. Given real numbers a,b, &gt; 0 , we say that a approximates b with accuracy , and write a  X  b , if a  X  [(1 + )  X  1 b, (1 + ) b ] .

Our approximation is quantified with the following Lemma, whose proof is omitted from due to space limitations. Lemma 3. Given &gt; 0 , consider the quantity q ( f ) com-puted as described above, for f  X  F . Let  X  be the ball radius at the current step of the algorithm. The following holds:
Algorithm 3: Pregel-like algorithm for facility location (graph setting) Input : Graph G = ( V,E,d ), facilities F  X  V , clients
Output : Subset of opened facilities S
O  X  X  X  // opened facilities
U  X  C // Unfrozen clients  X   X   X  0  X   X  m 2 (1 + ) // Initial ball radius q ( f )  X  0 for each f  X  F // next one is a sequential while while ( O 6 = F ) and ( U 6 =  X  ) do 6  X   X   X  (1 + ) // Increase ball radius 7  X  ( c )  X   X  for each c  X  U 8 O  X  OpenFacilities( G,F \ O,U,c (  X  ) , X , X  (  X  ) ,q (  X  )) 9 for f  X   X  O do 10 send( f, X ,  X  X reezeClient X ) // f sends a 11 for c  X  U do 12 if c receives a  X  X reezeClient X  message then 13 U  X  U \{ c } 14 O  X  O  X  O if O = F and U 6 =  X  then 16 for c  X  U do 17  X  ( c )  X  arg min f d ( c,f )
S = MIS H( G,O,C, X  (  X  )) // Computing a MIS of H return S
Pseudocode for our method is shown in Algorithm 3. It consists of two building blocks: an algorithm for deciding which facilities to open (Algorithm 4), and an algorithm for computing a maximal independent set of the graph H without explicitly building such a graph (Algorithm 5). The pseudocode for distributing messages in the graph (denoted by the send procedure) is omitted for brevity.

During the execution of the algorithm, for each open faci-lity f we let  X  ( f ) be the value of  X  when f is opened. Ob-serve that there is an edge ( c,f ) in H only if (1)  X  ( c ) =  X  ( f ), (2) c is within distance (1 + )  X  ( c ) from f , and (3) f is open. Therefore, storing the values for  X  ( f ) and  X  ( c ) allows us not to materialize H , which might be very costly.
Salihoglu and Widom [45] recently proposed an implemen-tation of the classic Luby X  X  algorithm [37] for computing the MIS in a Pregel-like system such as Giraph. In our ap-proach, we need to compute a MIS of H which is essentially the graph H 2 after removing all unopened facilities (and their edges) from H 2 . As we do not materialize H nor H , even computing the degree of a vertex in H (which is needed in Luby X  X  algorithm) might require to exchange a large num-ber of messages. Therefore, we resort to another algorithm developed by Blelloch et al. [5] and which works as follows.
Initially, all vertices are active and a unique ID is assigned randomly to each of them. Let  X  ( f ) be the ID for facility f . Then, in parallel, each active vertex v checks whether its ID is the minimum among its neighbors. If this is the case, v is included in the maximal independent set and all its neigh-Algorithm 4: OpenFacilities( G,D,U,c (  X  ) , X , X  (  X  ) ,q (  X  ))
Input : Graph G = ( V,E ), unopened facilities D ,
Output : Newly opened facilities O for f  X  D // For each unopened facility do 3 Compute  X  n ( d,f ) for each f  X  F and d  X  R 4 if  X  =  X  0 then 5 Compute t ( f, X  ) as in Equation (2) 6 else 7 Compute t ( f, X  ) as in Equation (3) 8 q ( f )  X  q ( f ) + t ( f, X  ) 9 if q ( f )  X  c ( f ) then 10 add f in O 11  X  ( f )  X   X  return O bors become inactive . This process is iterated O (log times. It can be shown that, with high probability, the se-lected vertices induce a maximal independent set.

As we do not materialize H nor H , we need to slightly modify the algorithm by Blelloch et al. [5]. Recall that there is an edge ( c,f ) in E ( H ) only if (1)  X  ( c ) =  X  ( f ), (2) c is within distance (1 + )  X  ( c ) from f , and (3) f is open. More-over, there is an edge ( f a ,f b ) in E ( H ) if there exist c  X  C such that ( c,f a ) and ( c,f b )  X  E ( H ). After determining its ID  X  ( f ), each facility f sends a message (  X  ( f ) , X  ( f )) to all vertices within distance (1 + )  X  ( f ) from f . Each client c collects all messages (  X  ( f ) , X  ( f )), and retains only the pairs (  X  ( f ) , X  ( f )) corresponding to the facilities f that c is con-nected. Then, each client computes the minimum ID  X  min among all the facilities it is connected to, and sends back a message containing  X  min to all such facilities. Each facility f is included in the maximal independent set if an only if  X  min =  X  ( f ), in which case it sends  X  min to all neighbor-ing facilities (in H ) so that they are removed from the set of active vertices. The last step is performed by letting each fa-cility f send  X  min to all clients c within distance (1 + )  X  ( f ), which in turn deliver such message to all facilities within distance (1 + )  X  ( c ). For pseudocode see Algorithm 5.
In Section 4, we evaluate the proposed algorithms against the algorithm proposed by Salihoglu and Widom [45].
Combining Theorem 1, Lemma 3, and the fact that ADS provides an approximation to the values of  X  n ( f,d ), we can show an approximation guarantee for our algorithm.
 Theorem 4. For any &gt; 0 and any integer k  X  1 , Al-gorithm 3 has an approximation guarantee of 3 + o (1) + . The total number of parallel iterations is O (  X  log 2 ( n )) , while the total number of messages exchanged by vertices is O ( m ) , with each message requiring O ( k log n ) bits.

The parameter k is related to the bottom-k in ADS. Ob-serve that we are able to derive the same approximation guarantees of Thorup [48], but in a distributed setting.
Algorithm 5: MIS H( G,O,C, X  (  X  ))
S  X  X  X  ,A  X  O for f  X  A do 3  X  ( f )  X  RAND ([1 ,n 3 ]) // next one is a sequential for for i = 1 ,..., d log 2 n e do 5 for f  X  A do 6 send( f, (1 + )  X  ( f ) , (  X  ( f ) , X  ( f ))) 7 for c  X  C do 9 for f  X  A do 10 if  X  min =  X  ( f ) then 11 S  X  S  X  X  f } 12 A  X  A \{ f } 13 send( f, (1 + )  X  ( f ) , X  min ) 14 for c  X  C do 15 if c receives  X  min , send( c, (1 + )  X  ( c ) , X  min ) 16 for f  X  A do 17 if  X  min &lt;  X  ( f ), A  X  A \{ f } return S
Theorem 4 holds only for undirected graphs. For directed graphs our guarantee does not hold, even though our algo-rithm can be adapted in a straightforward manner. In our experiments we have used directed graphs as well, and the performance of the algorithm is equally good.

With respect to the running time, the overall number of supersteps required by the algorithm is proportional to the diameter  X  of the graph. This follows from the hop-by-hop communication between clients and facilities. Thus, as typ-ically real-world graphs have small diameter, we expect our algorithm to terminate in a small number of supersteps.
The facility-opening phase consists of two subroutines, ball expansion and client freezing , which are implemented by the vertices and masters compute functions. Initially, the algorithm expands the balls around the potential facili-ties in parallel. When one of the balls encompasses a large enough number of clients, the facility at the center of the ball opens. At this point, all clients within the ball freeze, via the FreezeClients subroutine. The algorithm then re-sumes expanding the balls in parallel until another facility opens. This phase terminates when no unfrozen client re-mains, condition monitored by the master via a sum aggre-gator. By the end of the algorithm, vertices are either open facilities, or frozen clients with at least one facility serving them. Clients may have multiple facilities serving them as a result of concurrent openings or intersecting balls.
We now describe in detail the implementation of this phase of the algorithm in Giraph. The communication and coor-dination between the two main subroutines is particularly interesting. In Giraph, the communication between master and workers happens via aggregators.
 How to  X  X all a subroutine X ? While expanding the balls, we use a boolean aggregator called SwitchState to monitor if any facility was opened in the current superstep. Every vertex can write a boolean value to this aggregator, and the master can read the boolean and of all the values in its next superstep. The value of the aggregator is computed effi-ciently in parallel via a tree-like reduction. If SwitchState is true, the master writes to another aggregator State that represents the current function being computed. By set-ting the value of State to FreezeClients , the master can communicate to the vertices to switch their computation, effectively mimicking a subroutine call.
 How does FreezeClients work? The vertices execute different subroutines by switching on the State aggregator. When FreezeClients is executed, each facility opened in the last superstep sends a  X  X reezeClient X  message to all the clients within the current radius of the ball. This message contains the ID of the facility, and the distance it needs to reach, i.e., the radius. Each client that receives this mes-sage gets activated modifies its state to frozen by the faci-lity whose ID is in the message, and propagates the message to its own neighbors, as explained next. When a vertex deactivates, it writes true on the SwitchState aggregator. When all the vertices terminate and deactivate, the master X  X  SwitchState aggregator (which is a boolean and ) becomes true. The master can then resume the OpenFacilities rou-tine by writing on the State aggregator.
 How to send a message to all vertices at distance d? In Giraph, messages are usually propagated along the graph, hop-by-hop. A vertex v that wants to send a message M to all veritces within distance d , sends to each neighbor u a message containing M , as well as, the remaining distance d  X  d ( v,u ), if such a distance is larger than zero. The mes-sage is then in turn propagated by u to its neighbors if the remaining distance is larger than zero. If a vertex receives multiple copies of the same message M it propagates only the one with maximum remaining distance. This subroutine takes several supersteps to complete, proportional to the dis-tance to reach, and sends a number of messages proportional to the number of edges within distance d .
 How to estimate the number of unfrozen clients? We employ ADS to estimate N ( f i ,d ), i.e., the number of unfrozen clients within distance d from f i . We achieve this by using the predicated query feature of ADS. Given that ADS is composed by a sample of the vertices in a graph (for each possible distance), we can obtain an unbiased sample of a subset of the vertices that satisfy a predicate simply by filtering the ADS with such predicate. That is, we can apply the condition a posteriori , after having built the ADS. However, there is another issue to solve in our setting. The predicate we want to compute (unfrozen) is dynamic, as clients are frozen continuously while the algorithm is run-ning. Therefore, we implement this predicate by maintain-ing explicitly the set of frozen clients. Whenever a client is frozen, it writes its own ID to a custom aggregator which computes the set union of all the values written in it. At the next superstep, each facility has access to this set, and can use it to filter the ADS for the following query. Notice that, even though this set can grow quite large, it can be ap-proximated by using a Bloom filter at the cost of decreased accuracy in the estimate. Our experiments are not affected by this issue, so for simplicity we do not explore the use of Bloom filters, and defer its study to a later work. We test our approach using several datasets on a shared Giraph cluster containing up to 500 machines. We design our experiments to answer the following questions: Q1: What is the performance of ADS and how does it affect Q2: How does our algorithm compare with state-of-the-art Q3: What is the scalability of our approach in terms of time Q4: How do the two implementations of MIS compare with Parameters. There are two parameters of interest in our approach: the parameter k of the bottom-k min-hash, which regulates the space-accuracy trade-off of ADS, and the pa-rameter , which regulates the time-accuracy trade-off in the facility-location algorithm.
 Datasets. Table 1 summarizes the datasets used in our ex-periments. We use both synthetic and real-world datasets. We use two types of synthetic datasets and create instances with exponentially increasing sizes to test the scalability of our approach. We choose graph-generation models that re-semble real-world graphs. The first type of synthetic graphs is generated using the Forest Fire (FF) model [33], where the forward burning probability is set equal to 0 . 3 and the backward equal to 0 . 4. The second type of synthetic graphs uses the recursive matrix model (RMAT) [10] with param-eters a = 0 . 45, b = 0 . 15, c = 0 . 15, and d = 0 . 25. model can only generate graphs with a number of vertices that is a power of 2. For weighted graphs, we assign weights between 1 and 100, uniformly at random.
We perform experiments to assess the quality of the ADS estimates. To the best of our knowledge, we are the first to implement and test ADS on Giraph on a large scale. First, we evaluate the quality of ADS approximation by comparing against exact neighborhood sizes. Then, we experiment with the time taken for computing the ADS as a function of k . Accuracy vs. k : To evaluate the accuracy of the estimates produced by ADS, we need to compute exact neighborhood sizes. Since such computation is infeasible for large graphs, we compute exact neighborhood sizes on a sample of ver-tices. For each neighborhood distance (from 1 to 20 for unweighted graphs, and from 100 to 2000, at increments of
For both models, FF and RMAT, we use the default pa-rameters that the data generators come with.
F igure 1: ADS relative error vs. k (unweighted graphs).
F igure 2: ADS relative error vs. k (weighted graphs). 100, for weighted graphs), we sample 100 random vertices and compute their exact neighborhood sizes. For each sam-pled vertex and each distance, we compute the relative error as | S E  X  S ADS | /S E , where S E is the exact neighborhood size and S ADS the ADS estimate. Relative error averages and variances across the 2000 samples are reported in Figure 1 (unweighted) and Figure 2 (weighted).

We can see that the estimates are of high quality. In most cases, even for small values of k , the average relative error is less than 50%. The variance is also small.
 Time vs. k : Next, we measure the time taken to compute the ADS as a function of k , as shown in Figure 3. We clearly see that even for graphs with 1 million vertices and k as large as 500, the algorithm finishes in less than 800 seconds. Space requirements : Since increasing the value of k does not increase the time taken by the algoritom, one would as-sume that we could use a very high value of k in order to improve the quality of approximation of ADS. The bottle-neck, though, is the size of the ADS, which is proportional to k . In our setting, we have a limitation of 3.5 GB of mem-ory per machine, which makes it infeasible to store an ADS for large graphs with very large values of k (say, n &gt; 10m and k &gt; 200). Recall, however, that the size of the ADS is proportional to nk log n , thus, the value of k can increase linearly with the number of available machines. Figure 3: ADS time taken vs. k (unweighted graphs). Table 2: Relative cost of the Giraph algorithm against the sequential one ( k = 200).

We evaluate the quality of the solutions produced by our distributed algorithm by comparing against a simple sequen-tial baseline. We evaluate the performance and the running time of the algorithm as a function of .
 Comparison with sequential algorithm : As a baseline we use the sequential approximation algorithm by Charikar and Guha [11], which achieves an approximation ratio of (2 . 414 + ) and has running time of  X  O ( n 2 / ). The sequential algorithm assumes the availability of all-pairs shortest path distances, which is very expensive to compute, even for small graphs. Therefore, we perform our evaluation with graphs consisting of no more than 10 K vertices.

Table 2 shows the results of the comparison in terms of relative cost , which is defined as the cost of the sequential algorithm divided by the cost of our algorithm, for different values of . A smaller value means that our algorithm is competitive with the baseline. We can see that our algo-rithm performs quite well, even for large values of . Cost vs. accuracy ( ) : Table 2 shows the relative cost of our algorithm (compared again to the sequential algorithm) with respect to the accuracy. As expected, we get better solutions for smaller values of , but the solution does not get much worse even for large values of .
 Running time vs. accuracy ( ) : Figure 4 shows the time taken by our algorithm as a function of . We see that, as expected, our algorithm scales linearly with respect to the size of the graph, 3 and is faster for larger values of . A linear fit gives R 2 values between 0 . 8 and 0 . 9. Figure 4: Time taken by the algorithm for different values of on several graphs.

Figure 5: Time taken by each phase of the algorithm.
Next, we examine the scalability of the different phases of our algorithm. Recall that the three phases of our algorithm are ( i ) ADS computation (pre-processing), ( ii ) facility-location algorithm, and ( iii ) MIS computation (post-processing). Fig-ure 5 presents the time taken, for different datasets, broken down by phase. The total running time, for various graph sizes, is also shown in Figure 6.

For the results shown in Figure 5, we used k = 20, = 0 . 1, and 200 machines. Since the running time on a distributed cluster depends on various factors, such as the current load of the machines, we repeat all experiments three times and report the median running time.

Next, we vary the size of the graph and the number of machines, keeping the number of vertices per machine con-stant and test the time taken by our algorithm. The results are presented in Figure 7. We can clearly see that the time taken almost remains constant, indicating the scalability of our approach to large graphs.
As discussed above, we implement two methods for finding the maximal independent set (MIS): Luby X  X  [37], which was also implemented recently by Salihoglu and Widom [45], and a recent algorithm by Blelloch et al. [5]. We compare the two methods in terms of total time taken and number of supersteps needed to converge. Table 3 shows the results. We see that our parallel MIS algorithm implementation is at least 3 to 5 times faster than Luby X  X  algorithm. Figure 7: Total time taken by our algorithm for different number of machines for different sized graphs. Facility location. Facility location is a classic optimiza-tion problem. The traditional formulation (metric uncapac-itated facility location) is NP -hard, and so are many of its variants. Existing algorithms rely on techniques such as LP rounding, local search, primal dual, and greedy. The greedy heuristic obtains a solution with an approximation guaran-tee of (1 + log |D| ) [29], while constant-factor approximation algorithms have also been introduced [1, 11]. The approxi-mation algorithm with the best factor so far (1 . 488) is very close to the approximability lower bound (1 . 463) [35].
Differently from most previous work, the input to our al-gorithm is a sparse graph representing potential facilities and clients and their distances, rather than the full bipar-tite graph of distances between facilities and clients. Note that building the full bipartite graph requires computing all-pairs of distances and implies an  X ( n 2 ) algorithm. Thorup [48] considers a setting similar to ours, and provides a fast sequential algorithm  X  O ( n + m ).

Blelloch and Tangwongsan [4] propose a parallel approxi-mation algorithm for facility location in the PRAM model. In this work, we extend the former algorithm to work in a more realistic shared-nothing Pregel-like model. Other par-allel algorithms have also been proposed [25, 41, 42]. Applications. Facility location is a flexible model that has been applied successfully in many domains, such as city planning, telecommunications, electronics, and others. For an overview of applications, please refer to the textbook of Hamacher and Drezner [28].
 Large-scale graph processing. MapReduce [19] is one of the most popular paradigms used for mining massive datasets. Many algorithms have been proposed for vari-ous graph problems, such as counting triangles [47], match-ing [18, 32, 40], building similarity graphs [3, 17], and finding Table 3: Comparison of two implementations of MIS, in terms of supersteps and time taken (median over three runs). densest subgraphs [2].

However, given the iterative nature of most graph algo-rithms, MapReduce is often not the most efficient solution. Pregel [38] is large-scale graph processing platform that sup-ports a vertex-centric programming paradigm and uses the bulk synchronous parallel (BSP) model of computation. Gi-raph [12] is an open-source clone of Pregel. It is the plat-form that we use in this work. Other distributed systems for graph processing have recently been proposed, for in-stance, GraphLab [36], PowerGraph [26], GPS [44], and GraphX [27]. Most of the APIs of these system follow the gather-apply-scatter (GAS) paradigm, which can be readily used to express our algorithm. However, the BSP model is still used due to its simplicity and ease of use.
 Algorithms. Our work takes advantage of a number of successful algorithmic techniques. We use the all-distance-sketches (ADS) and the historic inverse probability (HIP) estimator by Cohen [15] to estimate the number of vertices within certain distance from a given vertex. HIP is a cardi-nality estimator similar to HyperLogLog counters [22] and Flajolet-Martin counters [21]. HyperANF [6] is a related algorithm that approximates the global neighborhood func-tion of the graph by using HyperLogLog counters, but it is not directly usable in our case as we need separate neigh-borhood functions for each vertex.
We have shown how to tackle the facility-location prob-lem at scale by using Pregel-like systems. In particular, we addressed the graph setting of the problem, which allows to represent the input in sparse format as a graph. We lever-aged graph sparsity to tackle problem instances whose size is much larger than previously possible.

Our algorithm is composed by three phases: ( i ) neigh-borhood sketching , ( ii ) facility opening , and ( iii ) facility se-lection . We implemented all three phases in Giraph, and published the code as open-source software. For the first phase, we showed how to use ADS with HIP, a recent graph-sketching technique. We adapted an existing PRAM algo-rithm with approximation guarantees for the second phase. Finally, for the third phase we proposed a new Giraph algo-rithm for the maximal independent set (MIS), which is much faster than the previous state-of-the-art. Our approach was able to scale to graphs with millions of vertices and billions of edges, thus adding facility location to the tool set of al-gorithms available for large-scale problems.

This work opens up several new research questions. From the point of view of the practitioner, this algorithm enables to solve large-scale facility-location problems, thus is a can-didate for real-world applications in Web and social-network analysis. A more general question is whether better algo-rithms exist for the setting we consider. Also, it would be interesting to know whether there are any primitives that the system could offer to develop better algorithms.
