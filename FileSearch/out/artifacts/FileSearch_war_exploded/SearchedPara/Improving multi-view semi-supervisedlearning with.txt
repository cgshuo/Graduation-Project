
School of Information Technology and Engineering, University of Ottawa, Ottawa, ON, Canada Institute for Computer Science, Polish Academy of Sciences, Warsaw, Poland 1. Introduction
In some real world applications, it is usually dif fi cult or expensive to obtain enough labeled data to train a strong model. On the other hand, usually a large amount of unlabeled data are available. Con-sequently, leveraging both labeled and unlabeled data to build a strong model is an important research topic.

Semi-supervised learning algorithms were proposed to use a limited amount of labeled instances and a large amount of unlabeled instances to build such strong models. Some of the most commonly used algorithms include self-training, co-training [6], semi-supervised EM (Expectation Maximization) [14], co-EM [13] and graph-based methods [4]. These algorithms usually use at least one set of features for learning. Each such set of features is called a view. Sometimes, we may have more than one view for learning. For example, web pages can be classi fi ed by their content or the content of the pages that point to them. An object can be categorized based on its color or shape. From this perspective, semi-supervised learning algorithms can be categori zed into two groups: single view lear ning and multiple view learning.
Single view semi-supervised learning algorithms include self-training, EM, and graph-based meth-ods [4]. Self-training is the most straightforward single view learning method. It fi rst uses a small amount of labeled data to train a classi fi er. This classi fi er is then used to classify the unlabeled instances. Some re-trained and the above process is repeated. In the self-training process the classi fi er gradually improves its classi fi cation performance by self-teaching.

Semi-supervised EM [14] is another single view semi-supervised learning algorithm. It applies a prob-abilistic learning algorithm to a small set of labeled examples and a large set of unlabeled ones. Semi-supervised EM fi rst trains an initial classi fi er h based solely on the labeled examples. Then it repeatedly performs a two-step procedure: fi rst, h is used to probabilistically label all unlabeled examples; then, a new maximum a posterior (MAP) hypothesis h is learned based on the examples labeled in the previous step. EM tries to fi nd the most likely hypothesis that could generate the distribution of the unlabeled data.

Graph-based semi-supervised methods de fi ne a graph where the nodes are labeled and unlabeled ex-amples in the dataset, and edges re fl ect the similarity of examples. These methods usually assume label smoothness over the graph. Graph-based methods are nonparametric, discriminative, and transitive in nature. Many graph-based methods can be viewed as estimating a function on the graph. The func-tion should be close to the given labels on the labeled nodes, and it should be smooth on the whole graph. Graph-based methods include mincut [5], Discrete Markov Random Fields [17], Gaussian Ran-dom Fields and Harmonic Functions [24]. These methods primarily differ in the particular choice of the loss function and the regularizer.

A large body of research has been devoted to multi-vi ew learning [1,8,9,11, 18]. The commonly used multi-view semi-supervised learning algorithms include co-training [6] and co-EM. Co-training, a well known algorithm in the fi eld, expands self-training by using two classi fi ers that learn from each other. This requires the existence of two different views. One classi fi er is learned for each view and it is used to label unlabeled instances for the other view classi fi er. Iteration of these steps results in expansion of the labeled set.

Co-EM [13] is another multi-view algorithm that uses the classi fi er learned on one view to probabilis-tically label the examples for the other view classi fi er. Intuitively, Co-EM runs EM on each view and, before each new EM iteration, inte r-changes the proba bilistic la bels generated by each view.
Blum and Mitchell [6] showed that when the two views are independent and each one is suf fi cient for learning, co-training can effectively build an ef fi cient model. However, when the two views are not inde-pendent or not suf fi cient for learning, [13] showed that co-training and co-EM performance are greatly degraded. In many co-training applications, such as natural language processing [7,15,16], two inde-pendent and suf fi cient views often do not exist. As a result, real world co-training applications usually cannot achieve the theoretical optimal performance shown in [6]. Therefore, fi nding other means of im-proving the classi fi cation performance of co-training and co-EM when the independence and suf fi ciency requirements can not be met is an important research question. In our view an alternative sampling strategy is one such possibility.

In active learning, which is also used when there is a limited amount of labeled data, some most in-formative unlabeled instances are selected and labeled by a domain expert. Therefore sampling the most informative unlabeled instances from a large unlabeled instance pool is a key issue for active learning. Signi fi cant amount of research in active learning has focused on designing new sampling strategies [10, 19,21]. Previous research shows that in active learning random sampling usually leads to a poor per-formance. The original co-training algorithm [6] uses random sampling to select a subset of unlabeled data to replenish a much smaller working set of unlabeled data. As a result, different set of unlabeled instances are used in different co-training runs. Since in active learning random sampling usually cannot achieve the best performance, naturally we want to know if we can replace the random sampling strat-egy of co-training to obtain better results. To our knowledge, no previous work has demonstrated that sampling is also an important factor that in fl uences co-training performance.
 In this paper, we propose a new sampling method, agreement-based sampling, to improve co-training. Although co-EM does not use a sampling process, we believe suitable sampling may also improve co-EM X  X  performance. Thus we also modi fi ed the co-EM method to use our new sampling method. In the following sections, we fi rst give a detailed description of the original random sampling in co-training and the new sampling methods. Next we perform experiments to evaluate the performance of co-training and co-EM using our new sampling methods. We then provide an in-depth discussion to explore why the new sampling method can improve performance. Finally we present our conclusions and discuss the future work. 2. Sampling in co-training
Blum and Mitchell [6] proposed the co-training algorithm for semi-supervised learning with redun-dant views. As introduced in the previous section, co-training fi rst uses the labeled dataset to train two classi fi ers on two views. Then it repeatedly extracts some unlabeled instances to assign labels and add them to the labeled dataset. In an earlier implementation the candidate unlabeled examples were chosen form the complete unlabeled dataset. However, they found that better results can be obtained if a smaller working set is used as follows. First, a small unlabeled set U is randomly sampled from unlabeled dataset U . Then during the co-training process, two classi fi ers h 1 and h 2 whicharetrainedonthetwo views V 1 and V 2 of labeled dataset L are used to label all unlabeled instances in U . h 1 and h 2 each selects p positive and n negative most con fi dently predicted instances from U and add them to L .The unlabeled set U is then replenished by random sampling from U . This process is repeated many times until the co-training is fi nished. The co-training algorithm is shown in Fig. 1.

The original co-training algorithm uses random sampling to select unlabeled instances from U to replenish U . Since random sampling usually cannot achieve the best performance, we explore whether we can choose other sampling methods to improve the co-training performance. Since sampling is a key issue in active learning, we fi rst review some research on sampling in active learning.

Tong and Koller [19] used a sampling strategy by minimizing the version space to improve active learning for support vector m achine. Freund and Seung [10] used a committee of classi fi ers to sample unlabeled instances. This method selects unlabeled instances that the committee members disagree the most on their class. Muslea and Minton [12] proposed an active sampling method which is called co-are used to classify all unlabeled instances. Some of the unlabeled instances for which the two view classi fi ers disagree the most are sampled and presented to a domain experts to assign labels. Those newly labeled instances are then added to the labeled instance set. Wang and Zhou [21] theoretically analyzed the sample complexity of multi-view active learning under the data distribution assumption mentioned by Balcan and Blum [2]. They proved that the sample complexity of multi-view active learning can be exponentially improved. They also studied the resu lting performance for combining multi-view active learning with semi-supervised learning. They showed that the combination is more ef fi cient than pure multi-view active learning.
Intuitively, one may say that we can directly apply the sampling methods used in the active learning to the co-training process. Unfortunately, this is not feasible as co-training is a passive learning process. The sampling process in active learning selects the most informative unlabeled instances to be labeled by a domain expert. Those most informative unlabeled instances are usually least con fi dently predicted by the classi fi ers used in co-training. If those unlabeled instances are sampled in co-training, they are very likely to be mislabeled by co-training classi fi ers. This will degrade the co-training performance. Therefore the sampling strategies adopted by active learning cannot be directly applied in co-training. Consequently, we have to design a new sampling method for co-training. 3. Agreement-based sampling strategy
We propose a novel sampling method to replace the random sampling used in co-training. This new sampling strategy is called agreement-based sampling. Agreement-based sampling is a simple sampling method which is motivated by the idea that in co-training the unlabeled instances that two view classi fi ers agree in classi fi cation are more reliably labeled, thus they are more likely to improve the performance of the trained model. We also apply the agreement-based sampling strategy to co-EM and multi-view semi-supervised learning method. 3.1. Agreement-based sampling for co-training
Our sampling method is motivated by co-testing [12] algorithms. Similar to co-training, co-testing uses redundant views to expand labeled dataset to build strong learning models. Unlike co-training which uses labeled instances generated by the view classi fi ers in the next iteration, co-testing assumes there is an expert to label the sample of unlabeled instances suggested by the algorithm. Co-testing uses the following sampling method. It fi rst trains two view classi fi ers using two views of the labeled dataset. It then uses these view classi fi ers to classify all unlabeled instances.Those unlabeled instances that the two view classi fi ers disagree the most on their classi fi cation are then sampled.

In our method we also return a sample of the instances that are labeled by the two view classi fi ers. The difference between our sampling method and co-testing is that we always sample unlabeled instances classify all unlabeled instances. We then replenish U by sampling those most agreed instances from U . We use a ranking function to rank all unlabeled instances according to the predictions of the two view classi fi ers. The ranking score function for an unlabeled instance x i is de fi ned as where p ( x i ) and p 2 ( x i ) are predicted probabilities for the positive class by the two view classi fi ers respec-tively.

This functions assigns higher scores; i.e. ranking, to instances that the two view classi fi ers label the same with high con fi dence. The fi rst term I ( x i ) guarantees that the unlabeled instances for which two view classi fi ers produce the same labels are always ranked higher than those given different labels. predicted probabilities for the positive and negative classes by the view classi fi ers. We plot this ranking score function in Fig. 2. From this fi gure we can see that when p 1 = p 2 =1 or p 1 = p 2 =0 the ranking score function s achieves the maximum value of 2. In these cases the two view classi fi ers assign the same label with the highest classi fi cation con fi dence. When p 1 =1 ,p 2 =0 or p 1 =0 ,p 2 =1 , ranking score function s achieves the minimum value of 0.5. In these cases the two view classi fi ers assign different labels with the lowest classi fi cation con fi dence for both positive and negative classes. The new co-training algorithm that uses agreement-based sampling is shown in Fig. 3.

Figure 4 demonstrates the difference between ra ndom, co-testing and agreement-based sampling method. For a given labeled dataset with two views V 1 and V 2 , two view classi fi ers h 1 and h 2 are trained on the two views respectively. The two view classi fi ers are then used to classify all unlabeled instances. Figure 4(a) shows the distribution of the classi fi ed unlabeled instances. Here the horizontal V 1 are classi fi ed as positive by h 1 ; while those below the line V 1 are classi fi ed as negative. The vertical line V 2 represents the classi fi cation boundary of the view classi fi er h 2 . Similarly, instances to the left of classi fi cation for an instance is proportional to its distance to the boundary. Instances that are farther form the boundary are assigned higher classi fi cation con fi dence. In Fig. 4(a) the instances which are in the upper left and lower right regions are assigned the same labels by the two view classi fi ers. We use  X  +  X  X nd X   X   X  to represent the unlabeled instances that are classi fi ed as positive and negative by both h 1 and h 2 . The instances in the other two regions are those that the two view classi fi ers disagree in their classi fi cation. We use  X . X  to represent these instances. Figure 4(b) shows how random sampling method works. It randomly selects some instances from all unlabeled instances. We use circles to rep-resent the sampled instances. Figure 4(c) shows the sampling method of co-testing. Co-testing samples the instances that the two view classi fi ers label differently and that are farthest from both classi fi cation boundaries. Figure 4(d) shows our new sampling method. It selects instances that the two view classi fi ers label the same and that are also farthest from the boundaries. 3.2. Agreement-based sampling for co-EM
In the previous section we discussed agreement-based sampling strategy which we propose as a re-placement for the random sampling used by co-training. In this section we will apply this technique to the co-EM algorithm. As introduced in Section 1, co-EM is actually a multi-view semi-supervised learning algorithm which applies EM to the two views. Initially, a classi fi er is built from the labeled data on one view. Then this classi fi er is used to produce the pr obabilitie s of each class for al l instance, both labeled and unlabeled.

In the steps that follows, the instances that are probabilistically labeled by one view classi fi er is used for training the other view classi fi er, which then relabels all the instances, with associated probabilities, for retraining the fi rst classi fi er, etc. This process is repeated many times until co-EM converges. The pseudo code of co-EM is shown in Fig. 5.

Since co-testing labels and uses all the available instances for traing, we need to modify the algorithm to incorporate sampling. To do this we use the current two view classi fi ers to assign probabilistic labels to all unlabeled instances and sample the top p% instances that view classi fi ers agree the most on. Those sampled instances with the original labeled instances are used to train the view classi fi ers in the subsequent iterations.
Note that there are at least two methods to set the sampling rate p. One method is to use a constant value through the whole process. The other method is to set a small p initially and increase it gradually in subsequent iteration. We call the second approach co-EM with dynamic agreement-based sampling. The pseudo code for this method is shown in Fig. 6. In each iteration the algorithm runs co-EM with gradually larger labeled set L and U , a sample of unlabeled data U . At the end of co-EM step we create the L dataset which is a union of the L set that was passed to co-EM and the result of labeling the instances in U . To classify these unlabeled instances we combine the predictions of the h 1 and h 2 classi fi ers that are produced at the end of the co-EM step. In the experiments reported in this paper we used the average probability of positive classi fi cation generated by these two classi fi ers to produce the fi nal label of the instances in U . To run the algorithm with fi xed sampling ratio we simply set parameter delta to 0. 3.3. Agreement-based sampling for semi-supervised ensemble
The agreement based sampling method discussed so far is based on two views. In this section we extend the idea to multi-view setting. Classi fi er ensembles are used for semi-supervised learning. Zhou and Li [23] proposed a method which is called tri-training. It used three classi fi ers for semi-supervised learning. In their method, initially three classi fi ers are built from labeled data. When two classi fi ers agree in labeling an unlabeled instance, this instance is labeled and added to the third classi fi er X  X  training dataset. [20] extended tri-training method to ensembles with more classi fi ers. Initially an ensemble with N classi fi ers are built from the given labeled dataset. They build the ensemble members using multiple views. These views are randomly selected subsets of attributes used in the original dataset.
Each member classi fi er is then built from a subset of attributes. When N  X  1 classi fi ers agree in classifying one unlabeled instance, this unlabeled instance is labeled and added to the training dataset for the other classi fi er. This process is repeated many times. Although [20] showed that the ensemble method performs better than co-training, one potential problem is that this method is too strict in choosing unlabeled instances to expand training datasets for classi fi ers. Only those instances for which N  X  1 classi fi ers agree in classi fi cation can be used to expand labeled dataset. As a result, unlabeled instances are only partialy used for model building.

To apply agreement-based sampling to a multi-vi ew ensemble setting we should modify the scoring function shown in Formula 1. For each classi fi er this function should take into account the classi fi cations made by other classi fi ers. In formula 1, I ( x i ) represents whether two classi fi ers agree in classifying instance x i . The second term is the larger of the average positive and negative probabilities predicted by the two classi fi ers. When we apply a similar function for a classi fi er C in an ensemble, I ( x i ) can be replaced with the number of other classi fi ers which agree in classifying x i . The second term can be de fi ned as the average of the probabilities generated by the majority classi fi ers. Thus the ranking score function for a classi fi er C is where I ( x i ) is the size of majority in classifying x i and p k x i as generated by the m majority classi fi ers.

For an ensemble with N classi fi ers, and a binary classi fi cation task, when all other N  X  1 classi fi ers agree in classifying instance x i and all predicted probabilities are 1, ranking function 2 achieves its maximum value of N . When only half of the other classi fi ers agree in classifying instance x i and all the predicted probabilities are 0 . 5 , the ranking function 2 achieves the minimum value of N/ 2 .
The above formula can be normalized as When applying this ranking function to semi-supervised ensemble, we can set a threshold  X  for s C . Only those unlabeled instances with ranking scores greater than  X  are selected and added to training data. 4. Experiments
We chose 16 UCI datasets [3] to evaluate the performance of agreement-based sampling for co-training, co-EM and multi-view ensemble. The characteristics of the datasets and the values of param-eters used for co-training are shown in Table 1. For co-training and co-EM we generate the two views by splitting the attribute set into two subsets. This is a practical approach when prede fi ned views do not exist. To better account for the effect of views ideally we should perform experiments on all possible views. But unfortunately, since the number of pairs of views is exponential to the number of attributes, this exhaustive approach is not practical for datasets with large number of attributes. Instead, in our ex-periments we randomly generate these pairs of views. For all datasets other than balance-scale we have randomly generated between 150 and 200 view pairs. The numbers in the last column of Table 1 show the number of view pairs that we have experimented with and the total number of potential views.
The multi-class datasets balance-scale, primary-tumor, splice, soybean, segment, vowel are converted to binary by grouping some classes as positive and the rest to negative. Table 2 gives the conversion de-tails for those datasets. The new binary balance-scale, splice, soybean, and segment datasets are roughly balanced.

Our fi rst set of experiments compares the performance of original co-training and co-training with agreement-based sampling (cotrain-AS). We used naive Bayes and decision tree learning algorithms to train the models. In the case of naive Bayes, numeric attributes of all datasets are discretized using the ten-bin unsupervised discretization method of Weka [22]. For a given pair of views, we randomly partition the dataset into 5 equal-sized non-overlapping subsets. Each subset in turn is used as a testing set and the remaining 4 subsets are fu rther randomly split into the initial L , U and U sets. This allows us to run 5 co-training sessions. We repeat this process 10 times. In other words for each view pair we perform 50 co-training sessions. The number of iterations for each co-training session is 20; i.e. k =20 .
Semi-supervised learning can be viewed from two perspectives. First, it is used to build a strong model from labeled and unlabeled instances. Second, it is used to expand the current limited labeled dataset in order to train a classi fi er for which certain requirements are speci fi ed. In many real world applications, the choice of the required model is constrained. For instance one may need an explainable model which eliminates the use of black box algorithms such as SVMs. We call this classi fi er a modeling classi fi er . The question then is, How can we use an unlabeled dataset to train the desired modeling classi fi er? The obvious answer is to somehow obtain the label for these unlabeled instances and train the modeling classi fi er. Keep in mind that this does not require that the label generation process to use similar learning algorithm to the one that will be used to generate the fi nal modeling classi fi er. We believe co-training, viewed as a method to automatically expand an initial small labeled set, could be a cost effective answer to this question. Consequently, the quality of the expanded labeled set generated by co-training is of particular interest to us. We will return to this perspective in Section 5.

We train a modeling classi fi er to evaluate co-training performance. The modeling classi fi er is trained using all attributes of the expanded labeled set in the fi nal round of co-training. We have used the same learning algorithm for the two view classi fi ers and the modeling classi fi er (as stated above, this does not need to be the case).

We use the classi fi cation error rate of the modeling classi fi ers on test sets for evaluating the algo-rithms.The results are summarized in Table 3. We perform a paired t-test with 95% con fi dence level on the error rates to show whether one model is signi fi cantly different from the other. Our results show that when using naive Bayes, cotrain-AS is signi fi cantly better than the original co-training in 7 out of 16 datasets and, with the exception of diabetes, ties with it in the case of remaining datasets. These results indicate that cotrain-AS can indeed make improvements over the original co-training. The detailed ex-perimental results on diabetes show that the two view classi fi ers perform quite differently in many view splits. In many cases they give totally different labels for unlabeled instances. This may have affected our sampling method adversely as it values the agreement among view classi fi ers The results for using decision tree learning algorithm are also shown in Table 3. In this case, cotrain-AS is signi fi cantly better than the original co-training in 4 out of the 16 datasets and ties with it for the remaining datasets, with the exception of diabetes. The better results obtained for naive Bayes could be a consequence of poorer proba bility estimates generated by the decision tree algorithm. The better such estimates are, the fi ner and more accurate the ranking generated by the agreement-based sampling will be.

We also compared the performance of co-EM and co-EM using agreement-based sampling. We call co-EM using agreement-based sampling with a fi xed sampling rate co-EM-AS. We call co-EM using agreement-based sampling with a gradually increasing sampling rate co-EM-DAS (co-EM with Dy-namic Agreement Sampling).
 In co-EM-AS, we use p% to represent the percentage of sampling unlabeled instances in each iteration. In our experiments we set this parameter to 10%, 25%, 50% and 75%. For co-EM-DAS, we initially use the sampling rate of 10%. Then in each iteration, we increase the sampling rate value by 1.5%. Again, the same algorithm was used for learning modeling and view classi fi er. We report the classi fi cation error rates of modeling classi fi ers in Tables 4 and 5 for naive Bayes and decision tree, respectively.
When using naive Bayes, co-EM-AS is signi fi cantly worse than co-EM in 5 and 2 datasets for sam-pling ratios of 10% and 25%, respectively. However, when sampling rates of 50% and 75% are used, co-EM-AS performs better than co-EM. For 50% sampling rate co-EM-AS is signi fi cantly better than original co-EM in 5 out of the 16 datasets. When sampling rate is set to 75% co-EM-AS is signi fi cantly better than the original co-EM in 3 out of the 16 datasets. Similar patterns emerge when comparing results for decision tree learning.

On the other hand, co-EM-DAS performs signi fi cantly better than co-EM in 8 out of the 16 datasets when using naive Bayes. It is signi fi cantly better than co-EM in 6 out of the 16 datasets when using decision tree learning. These results indicate that applying agreement-based sampling with a dynamic sampling rate to co-EM can make improvements over the original co-EM.

Finally we performed experiments to investigate whether agreement-based sampling can improve the performance of a semi-supervised ensemble. We compared the performance of the semi-supervised en-semble used in [20] with our approach which incorporates agreement-based sampling as discussed in Section 3.3. We refer to these two methods as semi-ens and semi-ens-AS, respectively. In constructing ensembles, we in itially build each member by usin g the same labeled dataset on a subset of its attributes which is randomly selected; i.e. a view. Each member of the ensemble uses its own view and expanding labeled set through the process. Each ensemble has 20 members. The experimental results for both naive Bayes and decision tree learning are shown in Table 6. When using naive Bayes learning algorithm, in 5 out of the 16 datasets, semi-ens-AS performs better than the original semi-supervised ensemble of [20]. When using decision tree learning algorithm, semi-ens-AS performs signi fi cantly better than the original semi-ens in 4 out of the 16 datasets. For the remaining datasets these methods tie. These exper-imental results indicate that agreement-based sampling method could potentially improve the results of semi-supervised ensemble. 5. Discussion
In this section we investigate why agreement-based sampling method can improve the performance of co-training, co-EM and multi-view ensemble. As shown in Section 2, agreement-based sampling selects the unlabeled instances for which the two view classi fi ers agree the most on their label. When both view classi fi ers are suf fi cient and independent, they classify the unlabeled instances more reliably. Thus selecting those instances that both view classi fi ers agree on is less likely to introduce errors in the expanded labeled set. This is because there is more evidence about the correctness of assigned label. One way to investigate why our new sampling method works is to explore the labeling errors introduced during the semi-supervised learning process. We are able to do this because the datasets used in our experiments are labeled.

We compared the error rates of co-training versus cotrain-AS and co-EM versus co-EM-DAS for the experiments conducted in Section 4. A paired t-test with 95% con fi dence level was used when comparing the labeling error rates. Table 7 shows the results for co-training and cotrain-AS . The corresponding classi fi cation error rates were shown in Table 3. In the case of naive Bayes learning algorithm, cotrain-AS is signi fi cantly better than the original co-training in 7 out of the 16 datasets and ties with it for the other datasets except diabetes. In the case of decision tree learning algorithm, cotrain-AS is signi fi cantly better than the original co-training in 5 out of the 16 datasets and ties with it for the other datasets.
Labeling error rates for co-EM and co-EM-DAS are shown in Table 8. We see that co-EM-DAS is signi fi cantly better than co-EM in 7 and 5 out of the 16 datasets for naive Bayes and decision tree learning, respectively. The two methods tie in the case of remaining datasets.

Comparing Tables 3 and 7 shows that for some datasets co-train-AS is better than co-training both in terms of classi fi cation error rates and labeling error rates. Similar observations can be made for co-EM-DAS and co-EM. This points to a potential correlation between these two error measures. A strong correlation could explain why co-train-AS and co-EM-DAS outperform co-training and co-EM, respec-tively We use the following method to explore whether classi fi cation and labeling error rates are correlated. As mentioned earlier, for a given learning algorithm such as naive Bayes or decision tree learning and a view split, we generate 5  X  10 = 50 fi nal modeling-classi fi ers. This process also results in 50 ex-panded labeled set that these models were trained on. We have calculated both labeling error on the fi nal expanded set and classi fi cation error on the test set for each modeling classi fi er. When comparing classi fi cation error rates of two semi-supervised methods A 1 and A 2 , for a given view split, we perform a paired t-test on these 50 pairs. If method A 1 is signi fi cantly better than A 2 we output a value of 1, otherwise we output 0. Repeating this process for all the view splits that we have experimented with will generate a list of t-test signi fi cance results for methods A 1 and A 2 . We generate a similar list for labeling error rates of methods A 1 and A 2 . Finally we calculate the correlation between these two lists 1, the Spearman X  X  footrule is calculated as  X  =1  X  n i =1 ( a i  X  b i ) 2 /n .
Following the above procedure, we calculated the Spearman X  X  correlation coef fi cient for classi fi cation errors and labeling errors of cotrain-AS and original co-training, and co-EM-DAS and co-EM. The results are shown in Tables 9 and 10.

Table 9 shows that for all the datasets and both learning methods the correlation coef fi cient is grater than 0.6. In the case of naive Bayes 12 out of 16 values are above 0.85, while this is true for 10 out of the 16 datasets for decision tree learning. When comparing results for co-EM and co-EM-DAS in Table 10 we see that 9 and 8 out of the 16 correlation coef fi cients are above 0.85 for naive Bayes and decision tree learning algorithms.

These results show the strong correlation that exists between better labeling sets produced by semi-supervised learning methods and classi fi cation error rates of modeling classi fi ers. We believe better results achieved by cotrain-AS and co-EM-DAS when compared to the original cotraining and co-EM can be explained by the quality of labeled sets produced by these algorithms. 6. Conclusions and future work
In this paper we proposed a new sampling method to improve the classi fi cation performance of multi-view semi-supervised learning methods. The basic idea behind this sampling method is to select unla-beled instances such that two (or more) view classi fi ers agree the most on their label. The criterion used in this sampling method is opposite of the one used in co-testing. We de fi ned a ranking score function which includes prediction con fi dence assigned to each classi fi cation to better capture this bias toward strong agreement between the view classi fi ers. We also proposed a multi-vi ew version of this ranking score function. We de fi ned alternative versions of co-training, co-EM and semi-supervised ensemble which incorporate this agreement-based sampling method. Our experiments show that our sampling method can indeed make improvements over the original algorithms. Finally we empirically investi-gated the reason why agreement-based sampling method works. We showed that the major reason for this is the lower labeling error in expanded labeled sets generated by our method. In future work, we will investigate the following outstanding research questions that this paper posits: what other sampling strategies could perform well? What are the effects of the size of sample, and of the choice of view clas-si fi er algorithm, on the overall performance of agreement-based co-training? We will also investigate the use of algorithms that produce better proba bility estimates for c lass memberships.
 Acknowledgments The research described here has been supported by grants from the Natural Sciences and Engineering Research Council of Canada, as well as from the Ontario Centres of Excellence.
 References
