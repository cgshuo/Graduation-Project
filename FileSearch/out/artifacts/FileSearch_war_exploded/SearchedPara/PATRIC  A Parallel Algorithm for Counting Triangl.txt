 Massive networks arising in numerous application areas poses significant challenges for network analysts as these networks grow to billions of nodes and are prohibitively large to fit in the main memory. Finding the number of triangles in a network is an important problem in the analysis of complex networks. Several interesting graph mining applications de-pend on the number of triangles in the graph. In this paper, we present an efficient MPI-based distributed memory paral-lel algorithm, called PATRIC, for counting triangles in mas-sive networks. PATRIC scales well to networks with billions of nodes and can compute the exact number of triangles in a network with one billion nodes and 10 billion edges in 16 minutes. Balancing computational loads among processors for a graph problem like counting triangles is a challenging issue. We present and analyze several schemes for balancing load among processors for the triangle counting problem. These schemes achieve very good load balancing. We also show how our parallel algorithm can adapt an existing edge sparsification technique to approximate the number of tri-angles with very high accuracy. This modification allows us to count triangles in even larger networks.
 G.2.2 [ Discrete Mathematics ]: Graph Theory X  Graph Algorithms ; D.1.3 [ Programming Techniques ]: Concur-rent Programming X  Parallel Programming ; H.2.8 [ Database Management ]: Database Applications X  Data Mining triangle-counting; big data; clustering-coefficient; massive networks; parallel algorithms; social networks; graph mining and anonymous reviewers for their suggestions and com-ments. This work has been partially supported by DTRA CNIMS Contract HDTRA1-11-D-0016-0001, DTRA Grant HDTRA1-11-1-0016, DTRA NSF NetSE Grant CNS-1011769 and NSF SDCI Grant OCI-1032677.

Today, data from diverse fields are modeled as graphs because of their convenience in representing underlying re-lations and structures [11]. Some significant examples are the Web, various social networks, e.g., Facebook, Twitter [17], collaboration and co-authorship networks [21], infras-tructure networks (e.g., transportation networks) and many forms of biological networks [15]. Due to the advancement in technology, we are deluged with data from a wide range of areas such as business and finance [3], computational bi-ology [9] and social science. As Google reported in 2008, the Web graph has over 1 trillion webpages. Most of the social networks, such as, Twitter, Facebook, and MSN, have mil-lions to billions of users [10]. To analyze these huge data rep-resented by massive networks, parallel algorithms are nec-essary. Furthermore, such massive networks pose another challenge of a large memory requirement. These graphs do not fit in the main memory of a single processing unit, and the algorithms must be able to work on a small part of the graph at a time.

Here, we study the problem of counting triangles in mas-sive networks that do not fit in the main memory of a sin-gle computing node. We present an MPI-based distributed memory parallel algorithm for this problem, which scales well to networks with billions of nodes and edges. Counting triangles in a network is an important algorithmic problem arising in the study of complex networks. Efficient solution to the trainagle counting problem can also lead to efficient solutions for many other graph theoretic problems, e.g. com-putation of clustering coefficient, transitivity, and triangular connectivity [10, 11, 20]. Existence of triangles and the re-sulting high clustering coefficient in a social network reflect the common theory of social science where people who have common friends tend to be friends themselves [19]. Also, tri-angle counting has important applications in graph mining. Recently, it has been used to detect spamming activity and assess content quality in social networks [7], to uncover the-matic structure of the web [14], query planning optimization in databases [4], etc.

The problem of counting triangles, and almost equiva-lently computing the clustering coefficients of the nodes in a graph, has a rich history [2,18,22 X 25,28]. Both exact and approximate algorithms for this problem can be found in the literature. Much of the earlier work are mainly based on matrix multiplication and adjacency matrix representation of the network. These matrix based algorithms [2, 12] are not useful in the analysis of social networks as adjacency matrix representation of network requires O ( n 2 ) memory space, where n is th e number of nodes in the network  X  even for medium sized networks, say a graph with a few hundred thousand nodes, such an amount of space can be prohibitively large. As a result, in the last decade the focus has been shifted to algorithms that use adjacency list rep-resentation [18,22 X 25,28], which takes O ( m ) memory. In a real-world network, m can be much smaller than n 2 as the average degree of a node can be significantly smaller than n , although few of the nodes can have very large degrees.
Although substantial research has been done on the tri-angle counting problem, much less attention was given, un-til recently, to the problems associated with massive net-works that do not fit in the main memory of a single pro-cessor. Several techniques can be employed to deal with such massive networks: streaming algorithms, sparsifica-tion based algorithms, external-memory algorithms, and dis-tributed memory parallel algorithms. The streaming and sparsification based techniques provide approximation algo-rithms whereas external-memory and parallel algorithms can be used to find exact solutions.

Streaming algorithms [4, 7, 16] perform a constant num-ber of passes, in some cases O (log n ) passes, over the data streams and make an estimation of the number of trian-gles. Recently, two elegant algorithms based on sparsifica-tion were presented in [28] and [22]. These algorithms store only a randomly chosen subset of the edges in the memory. Then the number of triangles in the original network is es-timated based on the number of triangles in this sparsed graph. Although the estimation for the total triangle counts can be reasonably good for some applications, the accuracy for the local triangle counts and clustering coefficients of the individual nodes can have larger errors.

To the best of our knowledge, very few papers have ad-dressed the problems associated with massive networks that do not fit in the main memory and provide an exact solution. A recent paper [10] presents an external-memory algorithm to find the exact number of triangles in a network. Although this algorithm provides an impressive solution to working with massive networks, external-memory algorithms can be very I/O intensive leading to a large running time. Only a parallel algorithm can solve the problem of such a large running time by distributing computing tasks to multiple processors.

Another recent paper [27] presents a parallel algorithm for exact triangle counts using MapReduce framework [13]. Our parallel algorithm improves the performance, both in time and space, over [27] significantly. A detailed comparison with this algorithm is given in Section 4. Our contributions include:
The rest of the paper is organized as follows. The prelim-inary concepts, notations and datasets are briefly described in Section 2. In Section 3, we discuss sequential algorithms for counting triangles. We present our parallel algorithm PATRIC and the load balancing schemes in Section 4. The parallelization of the sparsification technique is given in Sec-tion 5.
The given network is denoted by G ( V, E ), where V and E are the sets of vertices (nodes) and edges, respectively, with 1. We use the words node and vertex interchangeably. We assume that the input graph is undirected. If ( u, v )  X  E , we say u and v are neighbors of each other. The set of all neighbors of v  X  V is denoted by N v , i.e., N v = { u  X  V | ( u, v )  X  E } . The degree of v is d v = | N v | .
A triangle is a set of three nodes u, v, w  X  V such that there is an edge between each pair of these three nodes, i.e., ( u, v ) , ( v, w ) , ( w, u )  X  E . The number of triangles containing node v (in other words, triangles incident on v ) is denoted by T v . Notice that the number of triangles containing node v is as same as the number of edges among the neighbors of v , i.e., T v = |{ ( u, w )  X  E | u, w  X  N v }| . We use K, M and B to denote thousands, millions and billions, respectively; e.g., 1B stands for one billion.

Datasets. We used both real world networks and artifi-cially generated networks. A summary of all the networks is provided in Table 1. Twitter [17], web-BerkStan, LiveJour-nal and Email-Enron [26] are real-world networks. Miami is a synthetic, but realistic, social contact network [6] for Mi-ami city: each node is a person from Miami city, and there is an edge between two persons if they  X  X hysically X  interact with each other within a 24 hour period. Network Gnp( n, d ) is generated using the Erd  X os-R  X eyni random graph model [8], also known as G ( n, p ) model, with n nodes and edge proba-bility p = d n  X  1 so t hat the expected degree of each node is d . Network PA( n, d ) is generated using preferential attachment (PA) model [5] with n nodes and average degree d . PA( n, d ) has power-law degree distribution, which is a very skewed distribution.

Computation Model. We develop parallel algorithms for message passing interface (MPI) based distributed-memory parallel systems, where each processor has its own local memory. The processors do not have any shared memory, one processor cannot directly access the local memory of another processor, and the processors communicate via ex-changing messages using MPI.

Experimental Setup. We perform our experiments us-ing a computing cluster (Dell C6100) with 30 computing nodes, 12 processors (Intel Xeon X5670, 2.93GHz) per node, Network Nodes Edges Source
Email-Enron 37K 0 . 36M SNAP [26] web-BerkStan 0 . 69M 13M SNAP [26] Miami 2 . 1M 100M [6] LiveJournal 4 . 8M 86M SNAP [26] Twitter 42M 2 . 4B [17] Gnp( n, d ) n 1 2 nd Erd  X os-R  X eyni
PA( n, d ) n 1 2 nd Pref. Attachment Figure 1: Algorithm NodeIterator++, where  X  is th e degree based ordering of the nodes defined in Equation 1. memory 4GB/processor, and operating system SLES 11.1. We evaluate the performance of our algorithms using both real-world and artificial networks listed in Table 1. The number of nodes in the real-world networks ranges from 37K to 42M and the number of edges from 0.36M to 2.4B. Note that web-BerkStan, LiveJournal and Twitter networks have a very skewed degree distribution. For experiments on larger networks, we rely on random PA( n, d ) networks, with vary-ing n and d , which allow us to experiment with varying net-work sizes. Since PA( n, d ) networks have extremely skewed degree distribution, they make load balancing a challenging task and give us a chance to measure the performance of our algorithms in some of the worst case scenarios.
In this section, we discuss sequential algorithms for count-ing triangles using adjacency list representation and show that a simple modification to a state-of-the-art algorithm im-proves both time and space complexity. Although the mod-ification seems quite simple, and others might have used it previously, our theoretical and experimental analyses of this modification are new. To the best of our knowledge, our analysis is the first to show that such simple modification improves the performance significantly. This modification is also used in our parallel algorithm PATRIC.

A simple but efficient algorithm [23, 27] for counting tri-angles is: for each node v  X  V , find the number of edges among its neighbors, i.e., the number of pairs of neighbors that complete a triangle with vertex v . In this method, each triangle ( u, v, w ) is counted six times  X  all six permutations of u, v , and w . Many algorithms exist [10,18,23,24,27], which provide significant improvement over the above method. A very comprehensive survey of the sequential algorithms can be found in [18,23]. One of the state of the art algorithms, known as NodeIterator++, as identified in two very recent papers [10,27], is shown in Figure 1. Both [10] and [27] use this algorithm as a basis of their external-memory algorithm and parallel algorithm, respectively. Figure 2: Algorithm NodeIteratorN, a modification of Nod eIterator++.

This algorithm uses a total ordering  X  of the nodes to avoid duplicate counts of the same triangle. Any arbitrary ordering of the nodes, e.g., ordering the nodes based on their IDs, makes sure each triangle is counted exactly once  X  counts only one among the six possible permutations. How-ever, the algorithm NodeIterator++ incorporates an inter-esting node ordering based on the degrees of the nodes, with ties broken by node IDs, as defined below: This degree based ordering can improve the running time. Let  X  d v be the number of neighbors u of v such that v  X  u . We call  X  d v the effective degree of v . Assuming N v s, for all v , are sorted and a binary search is used to check ( u, w )  X  E , a running time O P v (  X  d v d v +  X  d 2 v log d max ) can be shown, where d max = max v d v . This running time is minimized when  X  d v values of the nodes are as close to each other as possible, although, for any ordering of the nodes, P v  X  d m is invariant. Notice that in the degree-based ordering, diversity of the  X  d v values are reduced significantly.
We also observe that for the same reason, degree-based ordering of the nodes helps keep the loads among the pro-cessors balanced, to some extent, in a parallel algorithm. We use this degree-based ordering in our parallel algorithm PATRIC and discuss this issue in detail in Section 4.
A simple modification of NodeIterator++ is as follows: perform comparison u  X  v for each edge ( u, v )  X  E in a preprocessing step rather than doing it while counting the triangles. This preprocessing step reduces the total number of  X  comparisons to O ( m ) and allows us to use an efficient set intersection operation. For each node v , set of neighbors N v is maintained in the memory; however, for each edge ( v, u ), u is stored in N v if and only if v  X  u . The modified algorithm NodeIteratorN is presented in Figure 2. All trian-gles containing node v and any u  X  N v can be found by set intersection N u  X  N v (Line 10 in Figure 2). The correctness of NodeIteratorN is proven in Theorem 1.

Theorem 1. Algorithm NodeIteratorN counts each tri-angle in G once and only once.

Proof. Consider a triangle ( x 1 , x 2 , x 3 ) in G , and with-out the loss of generality, assume that x 1  X  x 2  X  x 3 . By the constructions of N x in the preprocessing step, we have x , x 3  X  N x 1 and x 3  X  N x 2 . When the loops in Line 8-9 begin with v = x 1 and u = x 2 , node x 3 appears in S (Line 10-11), and the triangle ( x 1 , x 2 , x 3 ) is counted once. But this triangle cannot be counted for any other values of v and u (in Line 8-9) because x 1 /  X  N x 2 and x 1 , x 2 /  X  N x In NodeIteratorN, | N v | =  X  d v , the effective degree of v . When N v and N u are sorted, N u  X  N v can be computed in O (  X  d u +  X  d v ) time. Then we have O P v  X  V d v  X  d v time com-plexity for NodeIteratorN as shown in Theorem 2, in con-trast to O P v (  X  d v d v +  X  d 2 v log d max ) for NodeIterator++.
Theorem 2. The time complexity of algorithm NodeIter-
Proof. Time for the construction of N v for all v is O P v = O ( m ), and sorting these N v requires O P v  X  d v log time. Now, computing intersection N v  X  N u takes O (  X  d  X  d ) time. Thus, the time complexity of NodeIteratorN is
The second last step follows from the fact that for each v  X  V , term  X  d v appears d v times in this expression.
Notice that set intersection operation can also be used with N odeIterator++ by replacing Line 4-6 of NodeItera-tor++ in Figure 1 with the following three lines as shown in [10] (Page 674):
However, with this set intersection operation, the run-time o f NodeIterator++ is O P v d 2 v since | N v | = d v NodeIterator++, and computing N v  X  N u takes O ( d u + d time. Further, the memory requirement for NodeIteratorN is half of that for NodeIterator++. NodeIteratorN stores P P v d v = 2 m elements. Here we would like to note that two algorithms presented in [18, 24] take the same asymptotic time complexity as NodeIteratorN. However, the algorithm in [24] requires three times more memory than NodeItera-torN. The algorithm in [18] requires more than twice the memory as NodeIteratorN, maintains a list of indices for all nodes, and the hidden constant in the runtime can be much larger.

We also experimentally compare the performance of NodeIt-eratorN and NodeIterator++ using both real-world and ar-tificial networks. NodeIteratorN is significantly faster than NodeIterator++ for these networks as shown in Table 2.
In this section, we present our parallel algorithm PATRIC for counting triangles in massive networks.
We assume that the network is massive and does not fit in the local memory of a single computing node. Locally each processor stores only a part of the network in its memory. Let P be the number of processors used in the computa-tion . The network is partitioned into P partitions, and each processor is assigned one such partition G i ( V i , E i ) (formally defined below). Processor i performs computation on its partition G i . The network data is given as input in a single disk file. Each processor, in parallel, reads its own part of the network (the necessary data to construct its own parti-tion G i ) in its local memory. The main steps of PATRIC are given in Figure 3. In the following subsections, we de-scribe the details of these steps and several load balancing schemes.
The memory restriction poses a difficulty where the graph must be partitioned in such a way that the memory required to store a partition is minimized and at the same time the partition contains sufficient information to minimize com-munications among processors. For the input graph G ( V, E ), processor i works on G i ( V i , E i ), which is a subgraph of G induced by V i . The subgraph G i is constructed as follows: First, set of nodes V is partitioned into P disjoint subsets V , V c 1 , . . . , V c p  X  1 , such that, for any j and k , V and S k V c k = V . Second, set V i is constructed containing all edges ( u, v ) such that u, v  X  V i and ( u, v )  X  E .
Each processor i is responsible for counting triangles in-cident on the nodes in V c i . We call any node v  X  V c i core node of processor i . Each v  X  V is a core node in exactly one partition. How the nodes in V are distributed among the core sets V c i for all processors i crucially and significantly affect the load balancing and performance of the algorithm. Later in Section 4.4, we present several load balancing schemes and the details of how sets V c i are con-structed.
 Now, processor i stores neighbor set N v of all v  X  V i . Notice that for a node w  X  ( V i  X  V c i ), neighbor set N contain some nodes that are not in V i . Such neighbors of w , which are not in V i , can be safely removed from N w the number of triangles incident on all v  X  V c i can still be computed correctly. But, the presence of these nodes in
Figure 4: Memory usage with optimized and non-optimized data storing. do  X  N N w does not affect the correctness of the algorithm either. However, we do not store such nodes in N w to optimize memory usage. Figure 4 shows the differences in memory usage with and without this optimization for two networks: Miami and LiveJournal. As the experimental results show, this optimization saves about 50% of memory space. Figure 4 also demonstrates the memory-scalability of PATRIC: as the more processors are used, each processor consumes less memory space.
Once each processor i has its partition G i ( V i , E i ), it uses the modified sequential algorithm NodeIteratorN presented in Section 3 to count triangles in G i for each core node v  X  V c i . Neighbor sets N w for the nodes w  X  V i  X  V c help in finding the edges among the neighbors of the core nodes. To be able to use an efficient intersection operation, N v for all v  X  V i are sorted. The code executed by processor i is given in Figure 5.

Once all processors complete their counting steps, the counts from all processors are aggregated into a single count by an MPI reduce function, which takes O (log P ) time. Or-dering of the nodes, construction of N v , and disjoint node partitions V c i make sure that each triangle in the network appears exactly in one partition G i . Thus, the correctness of the sequential algorithm NodeIteratorN shown in Section 3 ensures that each triangle is counted exactly once.
A parallel algorithm is completed when all of the pro-cessors complete their tasks. Thus, to reduce the running time of a parallel algorithm, it is desirable that no proces-sor remains idle and all processors complete their executions almost at the same time. Furthermore, to deal with a mas-sive network, it is also desirable that all partitions G i require almost the same amount of memory space.

In Section 3, we discussed how degree based ordering of the nodes can reduce the running time of the sequential al-gorithm, and hence it reduces the running time of the local computation in each processor i . We observe that, inter-estingly, this degree-based ordering also provides load bal-ancing to some extent, both in terms of running time and space, at no additional cost. Consider the example network shown in Figure 6. With an arbitrary ordering of the nodes, | N v 0 | can be as much as n  X  1, and a single processor which contains v 0 as a core node is responsible for counting all tri-angles incident on v 0 . Then the running time of the parallel algorithm can essentially be as same as that of a sequen-tial algorithm. With the degree-based ordering, we have | N v 0 | = 0 and | N v i | X  3 for all i . Now if the core nodes are equally distributed among the processors, both space and computation time are almost balanced.

Although degree-based ordering helps mitigate the effect of skewness in degree distribution and balance load to some extent, working with more complex networks and highly skewed degree distribution reveals that distributing core nodes equally among processors does not make the load well-balanced in many cases. Figure 7 shows speedup of the parallel al-gorithm with an equal number of core nodes assigned to each processor. The speedup factor due to a parallelization is defined as t s /t p , where t s and t p are computation time required by a sequential and the parallel algorithm, respec-tively. As shown in Figure 7, LiveJournal networks show poor speedup, whereas the Miami network shows a rela-tively better speedup. This poor speedup for LiveJournal network is a consequence of highly imbalanced computation load across the processors as shown in Figure 8. Although most of the processors complete their tasks in less than a sec-ond, very few of them take an unusually longer time leading to poor speedup. Unlike Miami network, LiveJournal net-work has a very skewed degree distribution. In the next section, we present several load balancing schemes that im-prove the performance of our algorithm significantly. The load balancing schemes we propose require some pre-computation before executing the main steps for counting the triangles. Thus, our parallel algorithm PATRIC works in two phases, as shown below. 1. Computing balanced load: This phase computes par-2. Counting triangles: This phase counts the triangles
Computational cost for phase 1 is referred to as load-balancing cost , for phase 2 as counting cost , and the total cost for these two phases as total computational cost . In order to be able to distribute load evenly among the proces-sors, we need an estimation of computation load for com-puting triangles. For this purpose, we define a cost function f : V  X  R , such that f ( v ) is the computational cost for counting triangle incident on node v (Lines 4-7 in Figure
Figure 7: Speedup with equal number of cor e nodes in all processors. 5). Then, the total cost incurred to processor i is given by P v  X  V c i f ( v ). To achieve a good load balancing, P v  X  V should be almost equal for all i . Thus, the computation of balanced load consists of the following: 1. Computing f : Compute f ( v ) for each v  X  V 2. Computing partitions: Determine P disjoint parti-T he above computation must also be done in parallel. Otherwise, this computation takes at least  X ( n ) time, which can wipe out the benefit gained from balancing load com-pletely or even have a negative effect on the performance. Parallelizing the above computation, especially computing partitions step, is a non-trivial problem. Next, we describe parallel algorithm to perform the above computation. Computing f : It might not be possible to exactly compute the value of f ( v ) before the actual execution of counting triangles takes place. Fortunately, Theorem 2 provides a mathematical for-mulation of counting cost in terms of the number of vertices, edges, original degree d and effective degree  X  d . Guided by Theorem 2, we have come up with several approximate cost function f ( v ) which are listed in Table 3. Each function corresponds to one load balancing scheme. The rightmost column of the table contains short notations used to identify the individual schemes.
 Table 3: Cost functions f ( . ) for load balancing schemes
The input network is given in a file in adjacency list for-mat : adjacency list of the first node followed by that of the second node, and so on. The input file is considered divided by size (number of bytes) into P chunks. Initially, processor i reads in i th chunk from the file in parallel. Thus processor i contains the adjacency lists N v for the nodes v that are in the i th chunk. If a chunk boundary falls in the middle of an adjacency list, the boundary is shifted so that the entire adjacency list is in only one chunk. (Note that the nodes in i th chunk do not necessarily constitute the core nodes V processor i . These chunks are used only for the purpose of computing balanced load and finding the actual partitions V .) Thus, the degree d v of each node v in i th chunk is known to processor i . Every processor i computes f ( v ) for all nodes v in the i th chunk in parallel. Computation of f ( v ) for different schemes are given below. Computing partitions: Given that each processor i knows f ( v ) for all v in i th chunk as described above, our goal is to partition V into P disjoint subsets V c i such that P v  X  V c ing the nodes in V are labeled as 0 , 1 , 2 , . . . , n  X  1 in this order, first the cumulative sum g ( v ) = P v k =0 f ( k ) for each v  X  V is computed using a parallel algorithm given in [1] and summarized below: ii. The processors compute cumulative local sums R i as iii. Each processor i computes cumulative sum g ( v ) as fol-
Next we show how the partitions V c i can be computed from the cumulative sums g ( v ) for all v  X  V . Notice that R
P  X  1 = P v  X  V f ( v ). Processor ( P  X  1) computes  X  =
P v  X  V f ( v ) = 1 P R P  X  1 and broadcast  X  to all other pro-cessors. Each processors i finds the boundary nodes x j in its chunk: node x j is the j th boundary node if and only if g ( x j  X  1) &lt; j X   X  g ( x j ). Processor i can find the bound-ary nodes in the i th chunk by simply scanning the cumu-lative sum g ( v ) for the nodes in the i th chunk. Notice that some chunks may have multiple boundary nodes and some chunks may not have any. For each boundary node x j found in the i th chunk, processor i sends messages con-taining x j  X  1 and x j to processor j  X  1 and j , respectively. Each processor j receives exactly two messages containing x j and x j +1  X  1. Then partition V c j is the set of nodes { x j , x j + 1 , . . . , x j +1  X  1 } .

Since scheme DPD requires two levels of communication for computing f ( . ), it has the largest load balancing cost among all schemes . Computing f ( . ) for DPD requires O ( m P log P ) time. Computing partitions has a runtime complex-ity of O ( m P + P ). T herefore, the load balancing cost of DPD is given by O ( m P + P log P ). Figure 9 shows an experimen-tal result of the load balancing cost for different schemes on the LiveJournal network. Scheme N has the lowest cost and DPD the highest. Schemes DH , DH 2 , and DDH have a quite similar load balancing cost.
In this section, we present the experimental results eval-uating the performance of PATRIC and the load balancing schemes given in Section 4.4. We also compare the perfor-mance of PATRIC with the only other known distributed-memory parallel algorithm [27] for counting triangles.
Strong scaling of a parallel algorithm shows how much speedup a parallel algorithm gains as the number of proces-sors increases. Figure 10 shows strong scaling of PATRIC on LiveJournal, Miami and Twitter networks with differ-ent load balancing schemes. The speedup factors of these schemes are almost equal on Miami network. Schemes N and D have a little better speedup than the others. On the contrary, for LiveJournal and Twitter networks, speedup factors for different load balancing schemes vary quite signif-icantly. Schemes DPD and DH 2 achieve better speedup than the other schemes for these networks. As discussed before, Miami is a network with an almost even degree distribution. Thus, all load balancing schemes, even simpler schemes like N and D , distribute loads equally among processors (Fig-ure 11). This produces an almost same speedup on Miami network with all load balancing schemes. A lower load bal-ancing cost of schemes N and D (as shown in Figure 9) yields a little higher speedup. Unlike Miami network, LiveJournal and Twitter have a very skewed degree distribution. As a result, partitioning the network based on number of nodes ( N ) or degree ( D ) do not provide good load balancing. The other schemes, especially DPD , capture the computational load more precisely and produce a very even load distribu-tion among processors (Figure 11). In fact, scheme DPD provides the best speedup for LiveJournal and Twitter net-works. Our subsequent results will be based on the scheme DPD since it performs better than other schemes on real world networks with skewed degree distribution. The load-balancing cost of our algorithm, as shown in Section 4.4, is O ( m/P + P log P ). For the algorithm given in Figure 5, the counting cost is O ( P v  X  V c Thus, the total computational cost of our algorithm is, F ( P ) = O ( m/P + P log P + max where c 1 , c 2 , and c 3 are constants. Now, quantity denoting computation cost, ( c 1 m/P + c 3 P v  X  V c decreases with the increase of P , but communication cost P log P increases with P . Thus, initially when P increases, the overall runtime decreases (hence the speedup increases). But, for some large value of P , the term P log P becomes dominating, and the overall runtime increases with the ad-dition of further processors, as seen for P A (5 M, 50) network in Figure 12.

Notice that communication cost P log P is independent of network size. Therefore, when networks grow larger, com-putation cost increases, and hence they scale to a higher number of processors, as shown in Figure 12. This is, in fact, a highly desirable behavior of our parallel algorithm which is designed for real world massive networks. We need large number of processors when the network size is large and computation time is high.

Consequently, there is an optimal value of P , P opt , for which the total time F ( P ) drops to its minimum and the speedup reaches its maximum. To have an estimation of P opt , we replace d and  X  d with average degree  X  d and  X  spectively, and have F ( P )  X  c 1 n  X  d/P + c 2 P log P + c At the minimum point, d d P F ( P ) = 0, which gives the following relationship of P opt , n and  X  d : P 2 (1 + log P ) = with average degree  X  d  X  experimentally shows an optimal P of P  X  then another network with n nodes and an average degree  X  reflected in the result presented in Figure 12, where, for ex-ample, the network P A (25 M, 50) has P opt  X  250 which is approximately Thus, if we compute P  X  opt experimentally by trial and error
Figure 12: Improved scalability with in-crea sed network size. for a smaller network, we can estimate P opt for all other networks.
Weak scaling of a parallel algorithm shows the ability of the algorithm to maintain constant computation time when the problem size grows proportionally with the increasing number of processors. We use PA( n, m ) networks for this experiment, and for x processors, we use network PA( x/ 10  X  1 M, 50). The weak scaling of PATRIC is shown in Figure 13. Triangle counting cost remains almost constant (blue line). Since the load-balancing step has a communication overhead of O ( P log P ), load-balancing cost increases gradually with the increase of processors. It causes the total computation time to grow slowly with the addition of processors (red line). Since the growth is very slow and the runtime remains almost constant, the weak scaling of PATRIC is very good.
The runtime of PATRIC on the mentioned networks are shown in Table 4. We compare the running of PATRIC with a very recent distributed-memory parallel algorithm for counting triangles given in [27]. We select three of the five networks used in [27]. Twitter and LiveJournal are the two largest among the networks used in [27]. We also use web-BerkStan which has a very skewed degree distribution. No artificial network is used in [27]. For all of these three networks, PATRIC is more than 45 times faster than the algorithm in [27]. The improvement over [27] is due to the fac t that their algorithm generates a huge volume of inter-mediate data, which are all possible 2-paths centered at each node. The amount of such intermediate data can be signif-icantly larger than the original network. For example, for the Twitter network, 300B 2-paths are generated while there are only 2.4B edges in the network. The algorithm in [27] shuffles and regroups these 2-paths, which take significantly larger time and also memory. To deal with the memory is-sue, they proposed a partitioning scheme that improves the memory requirement to an extent; however, it does not im-prove the running time (see [27] for details).
In this section, we integrate a sparsification technique, called DOULION, proposed in [28] with our parallel algo-rithm. Our adapted version of DOULION provides more accuracy than DOULION. Sparsification of a network is a sampling technique where some randomly chosen edges are retained and the rest are deleted, and then computation is performed in the sparsified network. Sparsification of a net-work saves both computation time and memory space and provides an approximate result.

Let G ( V, E ) and G  X  ( V, E  X   X  E ) be the networks before and after sparsification, respectively. Network G  X  ( V, E  X  ) is obtained from G ( V, E ) by retaining each edge, independently, with probability p and removing with probability 1  X  p . Now any algorithm can be used to find the exact number of tri-angles in G  X  . Let T ( G  X  ) be the number of triangles in G  X  . The estimated number of triangles in G is given by 1 p 3 T ( G  X  ), wh ich is an unbiased estimation since E h 1 p 3 T ( G  X  )
As shown in [28], the variance of the estimated number of triangles is w here k is the number of pairs of triangles in G with an overlapping edge (see Figure 14).

In our parallel algorithm, sparsification is done as follows: each processor i independently performs sparsification on its partition G i ( V i , E i ). While loading partition G i into its local memory, it retains each edge ( u, v )  X  E i with probability p and discards it with probability 1  X  p as shown Figure 15. If T  X  is the number of triangles obtained after sparsification, T  X  is th e estimated number of triangles in G .

Notice that the sparsification of our algorithm is not ex-actly the same as that of DOULION. Consider two triangles ( v, u, w ) and ( v  X  , u, w ) with an overlapping edge ( u, w ) as Table 4: Runtime Performance of PATRIC using 200 pro-cessors and the algorithm in [27].

Figure 15: Triangle counting with parallel sparsification sho wn in Fig. 14. In DOULION, if edge ( u, w ) is not re-tained, none of the two triangles survive, and as a result, sur-vivals of ( v, u, w ) and ( v  X  , u, w ) are not independent events. Now, in our case, if v and v  X  are core nodes in two differ-ent partitions G i and G j , processor i may retain edge ( u, w ) while processor j discards ( u, w ), and vice versa. As proces-sor i and j perform sparsification independently, survivals of triangles ( v, u, w ) and ( v  X  , u, w ) are independent events.
However, our estimation is also unbiased, and in fact, this difference (with DOULION) improves the accuracy of the estimation by our parallel algorithm. Since the probabil-ity of survival of any triangle is still exactly 1 p 3 , we have E let k  X  i be the number of pairs of triangles with an overlap-ping edge such that both triangles are in partition G i , and k  X  = P i k  X  i . Let k  X  X  be the number of pairs of triangles ( v, u, w ) and ( v  X  , u, w ) with an overlapping edge ( u, w ) (as shown in Fig. 14) and v and v  X  are core nodes in two differ-ent partitions. Then clearly, k  X  + k  X  X  = k and k  X   X  k . Now following the same steps as in [28], one can show that the variance of our estimation is
C omparing Eqn. 3 and 4, if k  X  X  &gt; 0, we have k  X  &lt; k and reduced variance leading to improved accuracy. This observation is verified by experimental results on two real-world networks (Table 5). It also suggests that accuracy can be improved with a larger number of processors.

In [28], it was shown that due to sparsification with pa-rameter p , the computation can be faster as much as 1 /p times. However, in practice the speed up is typically smaller than 1 /p 2 but larger than 1 /p . Table 6 shows the accuracy and speedup factor with varying p for the LiveJournal net-work. The speedup factor, due to sparsification, of our algo-rithm is better than that of DOULION. For the LiveJournal network, DOULION shows a speedup of 31 with p = 0 . 1, while our algorithm has a speedup of 58. Sparsification also reduces memory requirement since only a subset of the edges are stored in the main memory. As a result, adaptation of sparsification allows our parallel algorithm to work with even larger networks. With sampling probability p (the probabil-ity of retaining an edge), the expected number of edges to be stored in the main memory is p | E | . Thus, we can expect that the use of sparsification with PATRIC will allow us to work with a network 1 /p times larger, a network with few hundreds billion edges. [1] M. Alam and M. Khan. Efficient algorithms for [2] N. Alon, R. Yuster, and U. Zwick. Finding and [3] C. Apte, B. Liu, E. Pednault, and P. Smyth. Business [4] Z. Bar-Yosseff, R. Kumar, and D. Sivakumar.
 [5] A. Barabasi and R. Albert. Emergence of scaling in [6] C. Barrett, R. Beckman, et al. Generation and [7] L. Becchetti, P. Boldi, C. Castillo, and A. Gionis. [8] B. Bollobas. Random Graphs . Cambridge Univ. Press, [9] J. Chen and S. Lonardi. Biological Data Mining . [10] S. Chu and J. Cheng. Triangle listing in massive [11] F. Chung and L. Lu. Complex Graphs and Networks . [12] D. Coppersmith and S. Winograd. Matrix [13] J. Dean and S. Ghemawat. Mapreduce: Simplified [14] J. Eckmann and E. Moses. Curvature of co-links [15] M. Girvan and M. Newman. Community structure in [16] M. Jha, C. Seshadhri, and A. Pinar. A space efficient [17] H. Kwak, C. Lee, et al. What is twitter, a social [18] M. Latapy. Main-memory triangle computations for [19] M. McPherson, L. Smith-Lovin, and J. Cook. Birds of [20] R. Milo, S. Shen-Orr, et al. Network motifs: simple [21] M. Newman. Coauthorship networks and patterns of [22] R. Pagh and C. Tsourakakis. Colorful triangle [23] T. Schank. Algorithmic Aspects of Triangle-Based [24] T. Schank and D. Wagner. Finding, counting and [25] C. Seshadhri, A. Pinar, and T. Kolda. Triadic [26] SNAP. Stanford network analysis project. [27] S. Suri and S. Vassilvitskii. Counting triangles and the [28] C. Tsourakakis, U. Kang, G. Miller, and C. Faloutsos.
