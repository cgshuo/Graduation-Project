 User generated content is the basic element of social media websites. Relatively few studies have systematically ana-lyzed the motivation to create and share content, especially from the perspective of a common user. In this paper, we perform a comprehensive analysis of user posting behavior on a popular social media website, Twitter. Specifically, we assume that user behavior is mainly influenced by three factors: breaking news, posts from social friends and user X  X  intrinsic interest, and propose a mixture latent topic model to combine all these factors. We evaluated our model on a large-scale Twitter dataset from three different perspec-tives: the perplexity of held-out content, the performance of predicting retweets and the quality of generated laten-t topics. The results were encouraging, our model clearly outperformed its competitors.
 H.4 [ Information Systems Applications ]: Miscellaneous; H.3.3 [ Information Search and Retrieval ]: Information filtering X  performance measures Algorithms, Experimentation Twitter, user modeling, user behavior, topic model
With the rising popularity of social media, better under-standing of user posting behavior has become crucial for many personalization and information filtering application-s, as well as for better site design and advertising policies. Towards this goal, existing works have examined the work-loads of various social media websites [7, 13, 21], aimed at providing a global picture of user activity patterns on these websites. There are also studies focused more on individual user behavior, by analyzing the content users have created [30, 37, 41] or inferring from their social friends [9, 14, 40], to help users find interesting information or people.
While previous works on individual user behavior [9, 14, 30, 37, 40, 41] have simply assumed that users tend to pub-lish content they are interested in or make friends with sim-ilar interest, however, reality is much more complicated due to different usage patterns and user intentions. For exam-ple, it is reported that users are easily attracted by breaking news [1, 27], and are likely to create conversation with their intimate friends [15, 20]. On the other side, friendship on social media does not necessarily indicate similar interest, since it may arise from different sources such as influence, homophily, environment and reciprocity [4]. All these prob-lems require a more comprehensive model of user behavior on social media, which is the task we deal with in this paper.
Inspired by those early works [1, 15, 20, 27], we believe that when a user publishes a post, he is probably influenced by three factors: breaking news happens at that moment, posts published by his friends recently and his intrinsic inter-est. In light of this, we equally divide our experimental time period into time intervals, and within each time interval, we compute the distribution of breaking news and friends X  timeline for each user, which are assumed as two external factors that might affect his posting behavior in the same time interval. Subsequently, by modeling user interest as a distribution over latent topics, we use a mixture latent topic model to represent user posting behavior, and present the inference of our model based on collapsed Gibbs sampling.
Our experiment is based on Twitter, a popular social me-dia website. Since Twitter has attracted thousands of indi-viduals and organizations with business intents (e.g., news channels, online brands and social spammers), we first built a dataset of 11,358 common users, and then collected all tweets published by those users and samples of their social friends during a 70-days experimental time period. We test-ed our model on this dataset and showed its superiority over the competitors. Although our work has been done in the context of Twitter, we expect the same results would hold for many other similar applications, such as Facebook up-dates and Google Buzz.

The main contributions of our work include: 1. We build a large and reasonable dataset for analyzing user posting behavior on Twitter. 2. A simple but effective method is used to recognize breaking news from Twitter streams in a certain time period. 3. We analyze the influence of different social relationships on user posting behavior, and quantitatively measure the influence between users. 4. We propose a mixture topic model to analyze user posting behavior, and demonstrate the superiority of our model from three different tasks.

The remainder of this paper is organized as follows: sec-tion 2 provides a brief review of related work, section 3 describes the way to build our dataset, section 4 formally presents our mixture model, followed by the results of our experiments in section 5. Finally we conclude in section 6. Social media has become indispensable to users recently. A rich set of studies has been conducted on various forms of social media, such as blogs, photo and video sharing com-munities, question/answering portals and social bookmark-ing sites, focused on different properties and applications of them. For example, Gruhl et al. studied the dynamics of information propagation in blogspace [12], Leskovec et al. analyzed the network structure and evolution on different information networks [24, 25], Agichtein et al. introduced a classification framework to extract high quality content on question/answering portals [3] and Benevenuto et al. tried to discover spammers on a video sharing community [6].
Among the various successful social media websites, Twit-ter, a microblogging service, has attracted considerable at-tention from research area recently. With a limit of 140 characters for each message, Twitter enables an even faster mode of communication and information propagation. Ear-ly works [15, 20] examined the usage patterns and network properties of Twitter, and revealed that Twitter was mainly used in two different ways: as an information platform or as a social network. Subsequently, to better leverage its great wealth of both textual and social information, researchers have used Twitter to discover breaking news [28, 36], detect natural disasters [35, 39], improve realtime web search [11], characterize media events [10] and identify influential users [41] or interesting content [9].
The massive amount of data generated by social media users has provided researchers with insights into user be-havior. For instance, by analyzing workloads from three information networks, Guo et al. showed that users X  posting behavior exhibited strong daily and weekly patterns. They also pointed out that different types of content would have different characteristics [13]. Benevenuto et al. used click-stream data from a social network aggregator to compare us-er behavior across different online social networks, and they further investigated social interactions on those networks [7]. These macroscopic analysis of user behavior provided inter-esting observations about general usage patterns on social media websites, but they might lack interpretations at an in-dividual level. To reach a better understanding of individual user behavior, work [32] investigated the causality between individual behavior and social influence by observing the in-formation diffusion among users, work [27] predicted a user X  X  news interest from the user activities and the news trend-s, work [37] proposed a user interest model based on tags generated by users and their social friends, and the SVM classification framework was leveraged in [5, 6, 23] to detect spammers and content promoters on social media.

Within the research area of Twitter, few have been done to systematically analyze individual user behavior. Previ-ous efforts about user modeling on Twitter simply built a  X  X ag-of-words X  profile for each user based on his tweets, and extracted key words [9], entities [2], categories [30] or latent topics [17, 41] for that user. Although existing works can to some extent help recognize important information about users, however, they failed to capture the real motivation of users to publish content, as user behavior can easily be affected by some external factors other than user interest. To reach a comprehensive model of user behavior, we pro-pose a mixture model which incorporates three importan-t factors that might trigger user posting behavior, namely breaking news, friends X  timeline and user interest. Our mod-el is under the framework of latent topic models, since the entity-based and category-based user modeling frameworks would require external knowledge bases such as Wikipedia and AlchemyAPI 1 , which are time and resources consum-ing. Inspired by previous works on multiple text streams modeling [8, 18, 33], we present the inference of our model based on collapsed Gibbs sampling, and further test it on a large-scale Twitter dataset from three different tasks.
We started by using Twitter X  X  streaming API 2 to collect a random sample of the public tweets from March 10, 2011 to May 19, 2011 (the streaming API would return about 1% of all tweets each day, and is widely used for analyzing news on Twitter). After removing non-English tweets, the stream dataset contained 56,415,430 tweets publishe d by 9,292,345 users, with an average of 805,935 tweets each day. This stream dataset was used to extract breaking news for each time interval on Twitter. Since Twitter imposes a rate limit on crawling posts of a specific user, it is difficult for us to analyze large amount of users. Thus we would like to build a relatively small dataset of common active users.
Specifically, we assumed that a user was common and ac-tive if he had (100-3000) friends/followers, (10-200) tweet-s per week and has been listed (1-50) times. Most com-mon and active Twitter users were believed to fall into this category. We randomly picked 11,358 ordinary users as our experimental users, and crawled all their tweets dur-ing the 70-days experimental time period, yielding a dataset of 7,843,190 tweets. For each user, we crawled his entire social graph, including his followees, listers (people in the user-generated lists) and listfollowees (people in the user-followed lists). As it was difficult for us to collect tweets from all those friends, we created a sample of friends for each user, including all friends that have been retweeted or mentioned more than 4 times by him (on average 30 friends were chosen for each user), and 5 randomly picked followees, listers and listfollowees respectively. Finally the entire dataset of social friends contained 179,456 unique users, and we crawled all their posts during the experimental time period, yielding a dataset of 86,815,267 tweets.

Admittedly, our samples of social friends only contain s-mall proportion of users X  friends. However, as presented http://www.alchemyapi.com/ https://dev.twitter.com/docs/streaming-api/ later in this paper, users X  top retweeted/mentioned friends are much more influential than other social friends. Thus we believe that our dataset (which includes users X  top retweet-ed/mentioned friends) is still to some extent reasonable for analyzing the influence of social friends.
Imagine the situation when a common user publishes a post about iphone, the reason behind this behavior might be: (1) he is a fan of smartphones and has a long time fo-cus on iphone (2) he is reminded by some big events about iphone, such as the release of Iphone 4S (3) he is attracted by a discussion about iphone raised by his close social friends. In light of this, user posting behavior can be represented as a mixture model of three different factors: breaking news, posts from social friends and the user X  X  intrinsic interest. Specifically, given a user a in time interval T , the likelihood for him to generate a word w is regarded as a sample of the following mixture model (based on the bag-of-words as-sumption).
In the formula above,  X  B is the background smoothing model using word frequency from the entire dataset, and  X  is the mixing weight of the background model  X  B . p T ( w denotes the distribution of breaking news in time interval T ,and p T ( w |  X  aF ) is the distribution of friends X  timeline for user a in time interval T . All breaking news and friends X  posts in the same time interval are assumed to have influence on user posting behavior, since it will be computationally expensive to consider what has happened before each user behavior. p ( w |  X  aI ) means the distribution of a  X  X  interest, and is represented by a distribution over latent topics in this paper.  X  a 2 ,  X  a 3 and  X  a 0 are the mixing weights of breaking news, friends X  timeline and user interest for a respectively.
Notice that, for each user, the model uses different mixing weights, considering the difference between users in usage patterns. For instance, some users regard Twitter as an instant messaging tool and use it to communicate with their friends (where  X  a 3 should be big). On the other hand, some people consider Twitter to be an information platform and use it to release or seek information they are interested in (where  X  a 0 should be big). As presented later in this section, all mixing weights can be automatically learned during the training process.

In the following of this section, we first separately analyze the influence of breaking news and friends X  timeline on user posting behavior, and compute their corresponding distri-butions. Then we view user interest as a distribution over latent topics, and use a latent topic model framework to represent our mixture model of user posting behavior.
To identify emerging news on Twitter, we borrow the idea from TwitterMonitor [28], where news is represented by a group of bursty keywords that suddenly appear in tweets at an unusually high rate. For each time interval T ,asetof bursty keywords is extracted using equation (2):
Time interval Bursty Words where NT ( w ) represents the number of tweets containing word w in time interval T , NT is the number of tweets in T , N ( w ) is the number of tweets containing word w in the entire dataset, and N is the number of tweets in the entire dataset. NT ( w ) is used to promote the scores of high frequency words and punish low frequency words. We set a threshold S for score ( w ) (300 is chosen when T is 24 hours), and discard words below the threshold. It is worth to mention that we also try to set different threshold S ,and find that there are no obvious change for the experimental results when S is larger than 50.

Table 1 gives the top 10 bursty words in the 2nd, 50th and 53th time intervals when T issettobe24hours,well represent 3 real world events: japan earthquake, wedding of prince William and the death of Osama bin Laden. Figure 1 shows user participation in the 3 events. Among the 11358 experimental users, 46.5% of users published tweets about japan earthquake in the 2nd time interval, 50.9% of users talked about the wedding of prince William in the 50th time interval and the death of Osama bin Laden attracted 52.4% of users in the 53th time interval, which demonstrate that breaking news has great impact on user posting behavior.
For each time interval T , we model the distribution of breaking news according to equation (3), where w stands for any word that meets score ( w )  X  S .
Twitter introduces a directed social relationship named  X  X ollow X , which enables users to follow others to receive their tweets. To further help users organize their followees and fil-ter incoming tweets, Twitter has launched another feature retweeted proportion Figure 2: Influence of Friends on Retweet Behavior. named  X  X ist X  since November 2, 2009, which can group sets of users into categories. Users can create their own lists, add and delete list members, or just follow other users X  list-s. Besides these two explicit relationships, there are two implicit relationships indicated by tweets, namely  X  X etweet X  and X  X ention X . The retweet operates as a citation of another user X  X  tweet, with the form  X  X T @username X , while mention acts as a response to another user X  X  tweet, with the form  X  X username X . Both retweet and mention are strong signals of social influence [22].

In our dataset, on average, each user has 644 followees. 39.8% of users have created at least 1 list, with an average of 169 people in each list, 37.2% of users have followed at least 1 list, with an average of 577 people in each list. 44.7% of users do not use list, which means list has not been used as widely as follow yet. Figure 2 analyzes the influence of followees, listers and listfollowees on user retweet behavior. During the 70-days experimental time period, on average, 21% of users X  followees have been retweeted by them, 28.5% of listers have been retweeted and only 10% of users X  listfol-lowees have been retweeted by them. The influence on user mention behavior is similar, as reported in figure 3, 28.8% of followees, 37.6% of listers and 13.3% of listfollowees have been mentioned respectively. The results show that lister-s have a little greater impact on users than followees, but listfollowees are far less important. However, most of users X  social friends have not been retweeted or mentioned, which means that the explicit relationships on Twitter do not nec-essarily indicate strong influence [19]. As influence mainly exsits in the form of retweet and mention on Twitter, we as-sume that for each user, the more times a friend is retweeted or mentioned by him, the more influence that friend has on the user. To approximately verify this assumption, we build a  X  X ag-of-words X  profile for each user based on his tweets, and use TF-IDF algorithm to determine the word weight. Only the top 200 words are selected. For each experimental user, we compute the cosine similarities with his top retweet-ed friends, top mentioned friends, the random sample of his friends in section 3 (i.e., 5 listers, 5 followees and 5 listfol-lowees) and 5 random users that are not directly connected with him. As demonstrated in Figure 4, on average, the similarities with top retweeted and top mentioned friends are clearly higher than other friends, which proves that our assumption is reasonable. On the other side, the similarities with random listers, followees and listfollowees are almost the same as random users, which is consistent with our pre-vious conclusion that explicit relationships on Twitter are mentioned proportion Figure 3: Influence of Friends on Mention Behavior. not strong symbols of influence. Based on the assumption, we use equation (4) to measure the influence of friend j on user i : Where X j,i is as: Here NR ( j, i )isthenumberoftimesfriend j is retweeted by user i , NM ( j, i )isthenumberoftimesfriend j is mentioned by user i and N ( j ) is the total number of tweets posted by user j . Due to the similar performance in figure 4, we view retweet and mention the same in equation (5).

Our measure of social influence is computationally effi-cient, and generally captures the strength of communica-tions between friends, which is shown to accurately reflect the strength of relationship between friends [26, 40]. Admit-tedly, there are many other works on measuring influence in social networks. Since we mainly focus on modeling user posting behavior rather than computing social influence be-tween friends, we leave it as future work to compare different measures of social influence.

For each user i in time interval T , we compute the distri-bution of his friends X  timeline as: In the equation above, N jT ( w ) means the number of times word w is tweeted by friend j during the time interval T . If word w has never been tweeted by any friends during T , equation (6) is set to be 0.
We use a latent topic model framework to represent our mixture model of user posting behavior, where user interest is represented as a random mixture over latent topics, and can be automatically inferred during the training process.
Figure 5 shows the Bayesian graphical framework of the proposedmodel. Themodelcanbeviewedasanextension of author-topic model [34], a widely used variation of La-tent Dirichlet Allocation (LDA) [29] to integrate authorship information of documents into topic modeling. The author-topic model assumes that each author in the document col-lection is represented by a distribution over topics, and each word is associated with two latent variables: an author and a topic. To generate each document from a document col-lection, it first chooses an author from a document X  X  author list, samples a topic from topic distribution associated with the selected author, and then picks a word from the topic specific word distribution. As each tweet has only 1 author, the author-topic model here acts as to collect a document for each user based on all his tweets, and uses LDA to extract the topic distribution of this document.

The proposed model has a similar general structure to the author-topic model, but with additional machinery to handle the distribution of breaking news, friends X  timeline and background words respectively. In particular, a latent random variable x is associated with each word, acts as a switch to determine whether the word is generated from the distribution of background model, breaking news, posts from social friends or user X  X  intrinsic interest. x is sampled from a user-specific multinomial distribution  X  a ,whichinturn has a symmetric Dirichlet prior,  X  . A indicates the set of authors, T 1 is the set of latent topics, N d means the length of tweet d and D is the set of tweets. The generative process of this model is as follows: 1. For each topic k ,draw  X  k from Dir(  X  ) 2. For each author a ,draw  X  a from Dir(  X  ) 3. For each author a ,draw  X  a from Dir(  X  ) 4. For the ith word w i posted by a during T
Our inference of the latent variable x is inspired by pre-vious works on multiple text streams modeling [8, 18, 33], where a word is  X  X plit X  into different streams, and a latent variable is sampled to indicate which stream the word be-longs to. For each word in a document, the assignment of the latent variable is decided by two factors: the distribution of streams in the document, and the importance of the word in each stream. Based on this idea, we view the distribution of background model, user interest, breaking news and friends X  timeline as four different streams, and apply collapsed Gibbs Figure 5: Bayesian Graphical Framework of the Mixture Model. sampling using the following updating rules: where e a,j,  X  i and e a,  X  i are computed as: Here c a,j,  X  i is the number of words written by a assigned to stream j (excluding the ith word), |  X  j | is the size of stream j , which means the number of non-zero words in the distri-bution of stream j (for the stream of user interest, we use c 0 ,  X  i as an approximation). While previous works in [8, 18, 33] simply use c a,j,  X  i to denote the importance of stream j , in our work we smooth it with the size of the corresponding stream, since different streams have different size. m a,t, is the number of words posted by a assigned to topic t (ex-cluding the ith word) and n t,w,  X  i is the number of times word w assigned to topic t (excluding the current one). m denotes the total number of words posted by a and n t is the total number of words under topic t . K means the number of latent topics and W is the number of words.

The other parameters can be estimated as follows: perplexity Hyper-parameters like  X  ,  X  and  X  can be estimated using standard methods introduced in [31]. It is worth to men-tion that we also try to compute the weight of background model automatically using formula similar to (15), however the result is not as good as to set a reasonable weight before training starts (but it still outperforms our baseline).
We examine the proposed model from three different per-spectives: the perplexity of held-out content, the perfor-mance of predicting retweets and the quality of generated latent topics.
The perplexity in this study means the performance of prediction for new tweets, which is a widely used method to judge the performance of a topic model. We compare the perplexities of two topic models: our model and the author-topic model. We randomly split the tweets of each user into 90% training tweets and 10% test tweets, and compute the perplexity of all test tweets according to: where the predictive probability of a test word is denoted by p ( w test ), and is computed by equation (1) in our model. A lower perplexity indicates better performance. We run each model five times and the perplexity of each model is average value. Figure 6 shows the results for the proposed model and author-topic model (AT model) with different number of topics. T is set to be 24 hours and  X  B is set to be 0.3 in the mixture model. As a result, the proposed model outperforms the author-topic model. We also add tweets published by users X  close friends (top retweeted/mentioned friends) into the training data of author-topic model, however, as denoted by AT1 model, directly use posts from social friends even lower the performance. Notice that, the perplexities of both models do not change apparently when the number of topics is greater than 50. precision
The main purpose of user modeling is to help users find interesting information from the overwhelming information streams. While in the context of Twitter, retweet is the most important signal of user interest, as users are prone to broadcast their favorite tweets to their followers. Thus, the performance of predicting retweets is a good standard to judge the performance of a user model.

Specifically, for each user in every time interval T ,we randomly select a tweet that is retweeted by him (if exists), and mix it with 10 other tweets that are not retweeted by him. All the 11 tweets are published by his top retweet-ed/mentioned friends in the same time interval. This exper-iment can be seen as a real information filtering application: when a user is viewing his Twitter stream consists of 11 new tweets, he might find a tweet interesting and retweet it to his followers, on the other side, the other 10 tweets are not important to him relatively (since all the 11 tweets are published in the same time interval by his close friends, it is reasonable to assume that the user can see all of them at once). The task of a user model is to accurately predic-t which tweet can attract the user X  X  attention and will be retweeted by him.
We first compare the performance of our model with three user models: the author-topic model in [41], the TF-IDF al-gorithm used in [9] and the entity-based user profile in [2] (The AlchemyAPI is used to extract entities). The predic-tive probability of each tweet d is computed according to equation (18) in our model, and the one with the highest probability is predicted as the retweet. T is set to be 24 hours and  X  B is set to be 0.3 again. For the three com-petitors, tweets are ranked based on their cosine similarities with user profiles. We repeat the experiment five times on different random sample sets and the results are the average value. Figure 7 shows the predictive precision of our model and all the other competitors.
As a result, about 17.2% of retweets are correctly predict-ed in our model, which is clearly better than our competi-
Figure 8: Recall of Predicting Retweets (5.2.1). tors. The precision of TF-IDF algorithm, entity-based user profile and author-topic model are 10.4%, 9.8% and 9.2% respectively, which are only little better than random se-lection (9.1%). However, we must point out that the task is difficult since all candidate tweets are published by user-s X  closest friends. Furthermore, all the four methods tend to reflect the similarities of tweets with users X  general post-ing behavior, which are not especially intended for retweet prediction settings. Thus the relatively low precision of all models are reasonable. Nevertheless, the result still shows that our model can reach a better understanding of user posting behavior than the three competitors.

To avoid missing too much retweets, we should provide more candidates to increase the recall of all models. Assume that each time we provide the top n results returned by all methods, figure 8 gives the recall of retweets with different n . The number of topics is set to be 50 in our model. As demonstrated, our model outperforms the three competitors for all different n .
We compare the performance of our model with a retweet prediction model. While retweet is recognized as the key mechanism for information diffusion on Twitter, a rich set of studies has been conducted to predict retweets [16, 38], mainly based on classification frameworks which incorporate different features related to tweets or authors.

We use logistic regression to build a retweet prediction model, leveraging 16 different features that are found to be important in previous retweet prediction models [16, 38], including author-based features (i.e., # of followers, # of followees, # of times listed, is he a verified user, his account age, # of tweets published totally and # of tweets published per day), tweet-based features (i.e., # of urls, # of hashtags, # of users mentioned, # of words, is the tweet a reply, is the tweet a retweet itself) and content-based features (i.e., the TF-IDF, entity and latent topic similarities of tweets with users X  past tweets, which are the three competitors in 5.2.1).
We use the same test set in 5.2.1, and build another dif-ferent training dataset based on the same method: for each group of 11 tweets, the retweet is labeled as positive example and the other 10 tweets are viewed as negative examples. To
Figure 9: Recall of Predicting Retweets (5.2.2). predict retweets, tweets are ranked based on their retweet probabilities returned by logistic regression.

Figure 9 gives the recall of predicting retweets with dif-ferent n . The performance of our model remains unchanged to figure 8 because of the same test set. As shown in figure 9, the retweet prediction model (denoted by 16 features) in-deed has large superiority over our model, since the proposed mixture model is not quite intended for retweet prediction tasks (our model only tries to reflect the likelihood of users to generate content, where other important factors associ-ated with retweets are not considered, such as the global influence of the author and the syntactic features of tweets).
However, our mixture model can still provide an impor-tant feature for retweet models. As denoted by 17 features, the performance of the retweet model is improved after us-ing the predictive probability of our model as a new fea-ture (e.g., the recall of retweets is improved from 37.8% to 41.2% for top 1 result), which means that our model is of great importance for retweet prediction models. Notice that, in the 17-features retweet model, if we remove 64% tweets from a user X  X  Twitter stream, we can still reach a recall of retweets over 80%, which is quite meaningful for social me-dia websites like Twitter, where information overwhelming has already become a serious problem.
We investigate the impact of  X  B and T on the model per-formance. Figure 10 shows the results of different  X  B when T is set to be 24 hours. The vertical axis on the left side of the graph is the perplexity of held-out content and the one on the right side means the precision of predicting retweets. As shown in figure 10, the model performance is generally similar when  X  B is small, and drops dramatically when  X  B is greater than 0.5. The best value for  X  B is between 0.2 and 0.3. Even if we set  X  B to be zero, the result is still sat-isfactory, which means the background model is not quite important to our method. We further fix  X  B to be 0.3 and analyze the influence of different time interval T in figure 11. When T is 24 hours, the model performance is the best. With T get longer, the model becomes coarse-grained and the performance drops. On the other side, the performance will also drop when T is shorter than 24 hours. According to our observation, it might probably due to the data sparsity problem in each short time interval.

We further analyze the importance of breaking news and friends X  timeline in our model respectively. As shown in fig-ure 12, when removing breaking news from our model (de-noted by RN), the performance remains almost the same, and when removing friends X  timeline from our model (de-noted by RF), the performance drops dramatically. This in-dicates that the distribution of friends X  timeline contributes a lot to the mixture model, but the distribution of breaking news is of little importance. The little impact of breaking news might due to two reasons: 1. the distribution of break-ing news is really sparse 2. it is quite possible that users X  friends will publish posts about breaking news, which might also lower the importance of breaking news. However, the existence of breaking news is still meaningful, for example, as denoted by D50, the performance of our model is clearly better than average on the 50th day, when the wedding of prince William happened.
Another typical method to judge the performance of topic models is to print top words for the latent topics and judge them by experience. We design an experiment to compare the latent topics generated by author-topic model and the proposed model. Specifically, we set the number of topics to be 50 and manually extract the same salient topics for both models. As a result, 8 latent topics are extracted, and the rest of latent topics are either meaningless topics or differen-Figure 12: Importance of News and Friends X  Posts. t topics between two models. We present the top 50 words of each latent topic to three labelers and ask them to label which one can better represent a topic. Due to the limit of space, table 2 only displays the top 10 words for each salient topic, and figure 13 gives the labeled result. On average, 71% of topics generated by the proposed model are labeled better, which shows that our model can reach a better un-derstanding of latent topics behind Twitter streams.
The results of our experiments prove that the proposed model clearly outperforms other user models. By comparing the perplexity of held-out content and the quality of gener-ated latent topics, our mixture model is shown to be better than the traditional author-topic model, and within the task of predicting retweets, its superiority over other competitors is still obvious. Although our model is not comparable to retweet models in predicting retweets, however, it can still provide an important feature for them.

To reach a microscopic understanding of our mixture mod-el, we empirically analyze how different factors work in our model based on a random sample of 30 users. First, we find that words associated with breaking news are generally well recognized. Second, for users with obvious interest (11 users are found to often publish tweets about some areas), ourmodelcandiscoverwordsrelatedtouserinterestvery well. Take a technology fan as an example, words such as  X  X pad X  and  X  X ndroid X  are easily assigned to his interest by our model. On the other side, if a user does not show strong interest in any area, words assigned to user interest will be less reasonable, since it is difficult to model his topics of in-terest accurately. Third, the factor of social influence mainly captures words that are recently published by users X  friends (and are not quite related to users X  interest). Most words in-spired by friends can be captured successfully, despite there are also many noise words included. Finally, some words out of all three factors can be handled by the background model, most of which are high-frequency words appear in daily life posts and conversations.

What is the potential value of our model for some real tweets recommendation systems? Admittedly, as demon-strated in the low accuracy of predicting retweets, it might still not be accurate enough to recommend tweets only based on our model. We believe that a good tweets recommenda-tion system should based on classification frameworks which incorporate various of features (just like the retweet model in 5.2.2), and our model can be used as one feature to reflect the similarities of tweets with users X  general posting behav-ior, as well as other user modeling frameworks. Further-more, there are also many other important factors should be considered, such as the geographic information of users and their entire social graph, which we leave as future work due to the restriction of our current dataset.

Finally, we must point out that it is quite a difficult task to model user posting behavior on Twitter, since users can easily generate content with any intentions at different time and place. On account of this, our model is still a macro-level modeling of user behavior, as only three factors are included in it. Thus, the interpretation of our model at micro-level might not be good enough sometimes, especially for users with no obvious interest. Nevertheless, as shown in the results of our experiments, our model can still reach a better understanding of user posting behavior in most cases, and lay out a foundation to personalization and information filtering applications on Twitter.
In this paper, we propose a mixture model to analyze user posting behavior on social media. By assuming that user be-havior is mainly influenced by three factors: breaking news, posts from social friends and user X  X  intrinsic interest, our method is able to reach a more comprehensive model of user posting behavior on social media. We demonstrate the su-periority of the proposed model on a popular social media website, Twitter, from three different tasks: the perplexity of held-out content, the performance of predicting retweets and the quality of generated latent topics. The results are satisfactory and our model clearly outperforms other tradi-tional methods.

Our future work lies in several areas. First, our basic idea is not limited to latent topic models, and it will be an interesting direction to test it under other framework-s, such as the entity-based or category-based user modeling frameworks. Second, while it is widely agreed that user in-terest will change with time, our method does not model the change of user interest explicitly since the experimental time period is relatively short. However, to achieve a long time understanding of user posting behavior, it is necessary to incorporate time factors into user interest model. Third, there might be some special terms besides words in tweets, such as URLs, hashtags and usernames. We tend to investi-gate whether the presence of those terms can help improve our model performance. Finally, the distribution of break-ing news and friends X  timeline computed in our model are simple, and more accurate methods are worth further ex-ploration. For example, to compute breaking news around users X  geographic location, or try different measures of social influence in computing the distribution of friends X  timeline. [1] F. Abel, Q. Gao, G.-J. Houben, and K. Tao.
 [2] F. Abel, Q. Gao, G.-J. Houben, and K. Tao.
 [3] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and [4] A. Anagnostopoulos, R. Kumar, and M. Mahdian. [5] F. Benevenuto, G. Magno, T. Rodrigues, and [6] F. Benevenuto, T. Rodrigues, and V. Almeida.
 [7] F. Benevenuto, T. Rodrigues, M. Cha, and [8] C. Chemudugunta, P. Smyth, and M. Steyvers.
 [9] J. Chen, R. Nairn, L. Nelson, M. Bernstein, and E. H. [10] N. A. Diakopoulos and D. A. Shamma. Characterizing [11] A. Dong, R. Zhang, P. Kolari, J. Bai, F. Diaz, [12] D. Gruhl, R.Guha, D. Liben-Nowell, and A. Tomkins. [13] L. Guo, E. Tan, S. Chen, X. Zhang, and Y. Zhao. [14] J. Hannon, M. Bennett, and B. Smyth. Recommending [15] C. Honeycutt and S. C.Herring. Beyond [16] L. Hong, O. Dan, and B. D. Davison. Predicting [17] L. Hong and B. D. Davison. Empirical study of topic [18] L. Hong, B. Dom, S. Gurumurthy, and [19] B. A. Huberman, D. M. Romero, and F. Wu. Social [20] A. Java, X. Song, T. Finin, and B. Tseng. Why we [21] H. Kwak, C. Lee, H. Park, and S. Moon. What is [22] A. Leavitt, E. Burchard, D. Fisher, and S. Gilbert. [23] K. Lee, J. Caverlee, and S. Webb. Uncovering social [24] J. Leskovec, L. Backstrom, R. Kumar, and [25] J. Leskovec, K. J.Lang, A. Dasgupta, and [26] C.-Y. Lin, K. Ehrlich, V. GriffithsFisher, and [27] J. Liu, P. Dolan, and E. R. Pedersen. Personalized [28] M. Mathioudakis and N. Koudas. Twittermonitor: [29] D. M.Blei, A. Y.Ng, and M. I.Jordan. Latent dirichlet [30] M. Michelson and S. A. Macskassy. Discovering users X  [31] T. P. Minka. Estimating a dirichlet distribution. 2009. [32] M. Papagelis, V. Murdock, and R. van Zwol.
 [33] M. Paul and R. Girju. Cross-cultural analysis of blogs [34] M. Rosen-Zvi, T. Griffiths, M. Steyvers, and [35] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake [36] J. Sankaranarayanan, H. Samet, B. E. Teitler, [37] J. Stoyanovich, S. Amer-Yahia, C. Marlow, and C. Yu. [38] B. Suh, L. Hong, P. Pirolli, and E. H. Chi. Want to be [39] S. Vieweg, A. L. Hughes, K. Starbird, and L. Palen. [40] Z. Wen and C.-Y. Lin. On the quality of inferring [41] J. Weng, E.-P. Lim, J. Jiang, and Q. He. Twitterrank:
