 Answers on mobile search result pages have become a common way to attempt to satisfy users without them needing to click on search results. Many different types of answers exist, such as weather, flight and currency answers. Understanding the effect that these different answer types have on mobile user behavior and how they contribute to satisfaction is important for search engine evaluation. We study these two aspects by analyzing the logs of a commercial search engine and through a user study. Our results show that user click, abandonment and engagement behavior differs depending on the answer types present on a page. Furthermore, we find that sat-isfaction rates differ in the presence of different answer types with simple answer types, such as time zone answers, leading to more satisfaction than more complex answers, such as news answers. Our findings have implications for the study and application of user satisfaction for search systems.
Mobile search has seen explosive growth in recent years. For instance, in 2013 it was estimated that 63% of Americans used their mobile phones to go online in comparison to only 31% in 2009 [5]. With this growth in mobile use, search engines have had to adapt to better suit user needs and behavior, which have been shown to be different on mobile devices [8, 10]. For instance, previous research has shown that mobile users may formulate queries in such a way so as to increase the likelihood of them being directly satisfied by the SERP [10] and that mobile queries differ from traditional desktop queries in being shorter and in their intents [8].

Search engines have had to adapt in order to accommodate these differences and one way they have done this is by showing answers on the Search Engine Results Page (SERP), such as answers about weather, the time, and sports scores. These answers typically ap-pear in small boxes that appear on the SERP and contain factual information. For instance, Figure 1 shows a mobile answer for the Furthermore, previous research has also shown that factual answers on a SERP were responsible for 56% of the satisfaction from aban-doned queries in mobile search [12]. Thus, it is clear that a rela-tionship exists between mobile answers, abandonment, and satis-faction. However, to date, most research investigating answers on mobile devices have not differentiated among answer types and in-vestigated how the answer type affects abandonment behavior. We hypothesize that it may be useful to take the answer type into con-sideration when developing a metric to assess abandonment in the presence of answers on a mobile SERP based on the belief that not all answer types contribute equally to good abandonment. Thus, in this study, we choose to empirically investigate the relationship between answer types and abandonment behavior. In doing so, we seek to answer the following two research questions:
To answer our research questions, we conduct a large scale anal-ysis of the logs of a commercial search engine and also analyze sat-isfaction ratings gathered in a controlled user study. To our knowl-edge, this is the first work to investigate the relationship between answer types and good abandonment in mobile search.
This paper extends a long line of work investigating satisfaction in search [1, 3, 7, 11]. Previous work has shown how gesture fea-tures can be used to differentiate between good and bad abandon-ment in mobile search [12]. However, while the cited work showed that answers were a large driver of good abandonment, it did not investigate the effect that different answers types have on satisfac-tion as we do in this study. Chilton and Teevan [2] show how users interact with different types of answers in desktop search, where interaction specifically focused on click behavior. It was found that, as expected, answers reduced interaction with the rest of the SERP, which the authors refer to as cannibalization . Our work is similar to the cited work in that it seeks to understand the relation-ship between answers and user interactions. However, whereas the cited work focuses on the desktop, we focus on mobile search. Fur-thermore, we go further than the cited work in that we empirically evaluate the relationship between answer types and satisfaction. In [10] it was shown how mobile users often construct queries in such a way so as to be directly satisfied by the SERP. Furthermore, in [4] it was shown that high quality SERPs lead to increased satisfaction and good abandonment and answers are one way of potentially in-creasing the quality of SERPs and thus good abandonment. Lagun et al. [9] use viewport and eye-tracking to measure user engagement in mobile search and studied the effect of having relevant/irrelevant answers on a mobile SERP. Our work is similar in studying the effect of answers on a mobile SERP; however, it differs in that we attempt to understand how different answer types affect satisfaction and abandonment behavior.
As previously mentioned, we hypothesize that the answer type, such as weather, time, etc., has an effect on abandonment and sat-isfaction. Thus, we begin by describing the answer types that we investigate in this study and also describe the metrics that we use. where T otalClicks ( s ) is the number of clicks on SERP s . We also calculate the average number of direct clicks that answer type a receives on the answer itself as: where T otalAnswerClicks ( s ) is the number of clicks on the click-able components of the answer box in SERP s . We then define the ER for answer type a as:
Thus the engagement rate captures the extent to which page clicks are engagements with the answer.
The previous section described the answer types that we con-sider in this study when evaluating abandonment and also defined the metrics that we consider. In this section, we use these metrics to assess how user behavior in terms of clicks, abandonment, and engagement differs in the presence of different answer types.
We perform our analysis on a large scale sample of mobile search logs from a commercial search engine. We sample over 20 million mobile impressions from a week during June 2015. These impres-sions come from about 9 million sessions and 1 million users. Each impression is associated with anonymized information about the query and session, information about the SERP elements that were visible, such as answers and organic search results, and information about the click behavior of the user. We use this dataset to perform a large-scale analysis of behavior for different answer types.
Figure 2 shows the click rate (CR) and abandonment rate (AR) for different types of answers on the mobile SERP. In the figure we observe different click and abandonment behavior depending on the answer type. For instance, we notice from Figure 2 that pages that contain answers, such as package tracking , phonebook , news , math , and show times have relatively high click rates ranging from 59% to 93%, meaning that people often click on pages containing these types of answers. This is perhaps not surprising since infor-mation needs related to news, package tracking, and show times, often require the user to perform additional navigation in order to fully satisfy their information need. By contrast, it can also be seen from Figure 2 that pages containing answers related to currency , finance , dictionary , and time zones experience relatively low click rates ranging from 14% to 32% implying high abandonment rates from 68% to 86%. As was the case with answer types that led to high click rates, the fact that pages with these types of answers ex-perience relatively high abandonment rates is not surprising since the answers satisfy simple and straightforward information needs. The evidence here suggests that, when evaluating abandonment, it is useful to take into consideration the type of information present on the SERP. For instance, when abandonment happens on a SERP that usually has a high click rate, that may suggest that the user was not satisfied. By contrast, abandonment on a SERP that usually has a high abandonment rate may not indicate dissatisfaction.
Engagement rates for different answer types are shown in Figure 3. As can be seen from the figure, some answer types dominate the SERP page clicks or, to use the language of [2], the answers Figure 4: SAT and DSAT rating associated with the different answer types gathered in the user study. 25.5 (  X  5 . 4 ) years and the user study included 5 information seek-ing tasks, which were designed to increase the likelihood of good abandonment [12]. At the end of the tasks, the users were asked to provide a satisfaction ratings. The dataset contains a total of 607 queries of which 576 were classified as abandoned since they re-ceived no clicks. Since this dataset involved a controlled user study experiment, not all of the answer types described in Section 3.1 were present. Thus, we only focus on the following subset of an-swer types: news, reference, time zone, sports, math with frequen-cies of 62, 5, 5, 14, and 19, respectively. After filtering out queries that did not trigger these answer types, we retained 105 queries.
For each answer type that appeared for the queries in the data described above, we measure the SAT and DSAT rates. As can be seen in Figure 4, we observe different SAT and DSAT rates for dif-ferent answer types. For instance, the presence of time zone, sports , and math answer types are all associated with relatively high SAT rates in the data, with all being above 70%. By contrast, the SAT rates in the presence of news and reference answers are both below 30%. Thus, there is evidence that the presence of different answer types may affect satisfaction. Though the data are drawn from dif-ferent sources and under different circumstances, it is also interest-ing to observe the relationship between the SAT rates in Figure 4 and the abandonment rate in Figure 2. For instance, mobile SERPs containing sports and reference answers both have abandonment rates of 52%, but very different SAT rates of 71% and 20%, respec-tively. Similarly, mobile SERPs containing math and news answers have somewhat similar abandonment rates of 41% and 33%, but very different SAT rates of 74% and 27%, respectively. This is in contrast to, say, SERPs containing timezone and math answers, which have very different abandonment rates of 86% and 41%, but similar SAT rates of 80% and 74%, respectively. As an example of bad abandonment, SERPs with reference answers experience an abandonment rate of 52% in Figure 2 and a satisfaction rate of only 20%. Thus most of the abandonment is bad. The findings in this section allow us to begin answering RQ2: How do different an-swer types affect satisfaction in abandoned queries? It is clear that user behavior and whether it indicates good or bad abandonment is influenced by the answer types seen by users. The reason for this is that, as shown in Figures 2 and 3, different answers lead to different user behavior and some answers require clicks, whereas others do not. Thus, we argue that any evaluation of satisfaction and aban-donment in the presence of mobile answers should take the answer type and its properties into consideration.
