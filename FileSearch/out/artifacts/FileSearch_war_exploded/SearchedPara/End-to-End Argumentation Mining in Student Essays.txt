 There has been a surge of interest in argumenta-tion mining in recent years. Argumentation mining typically involves addressing two subtasks: (1) ar-gument component identification (ACI), which con-sists of identifying the locations and types of the components that make up the arguments (i.e., Ma-jor Claims, Claims, and Premises), and (2) relation identification (RI), which involves identifying the type of relation that holds between two argument components (i.e., Support, Attack, None). As a first step towards mining arguments in persuasive essays, Stab and Gurevych (S&amp;G) annotated a corpus of 90 student essays with argument components and their relations (Stab and Gurevych, 2014a). To illustrate, consider the following excerpt from one essay: In this example, premise (3) supports claim (2), which in turn supports major claim (1).

Using their annotated corpus, S&amp;G presented ini-tial results on simplified versions of the ACI and RI tasks (Stab and Gurevych, 2014b). Specifically, they applied their learned ACI classifier to classify only gold argument components (i.e., text spans corre-sponding to a Major Claim, Claim, or Premise in the gold standard) or sentences that contain no gold argument components (as non-argumentative ). Sim-ilarly, they applied their learned RI classifier to clas-sify only the relation between two gold argument components. In other words, they simplified both tasks by avoiding the challenging task of identify-ing the locations of argument components. Conse-quently, their approach cannot be applied in a realis-tic setting where the input is an unannotated essay.
Motivated by this weakness, we examine in this paper argument mining in persuasive student essays in a considerably more challenging setting than that of S&amp;G: the end-to-end setting. In other words, we perform argument mining on raw, unannotated es-says. Our work makes three contributions. First, we present the first results on end-to-end argument mining in student essays using a pipeline approach, where the ACI task is performed prior to the RI task. Second, to avoid the error propagation problem in-herent in the pipeline approach, we perform joint in-ference over the outputs of the ACI and RI classi-fiers in an Integer Linear Programming (ILP) frame-work (Roth and Yih, 2004), where we design con-straints to enforce global consistency. Finally, we argue that the typical objective function used exten-sively in ILP programs for NLP tasks is not ideal for tasks whose primary evaluation metric is F-score, and subsequently propose a novel objective function that enables F-score to be maximized directly in an ILP framework. We believe that the impact of our work goes beyond argument mining, as our F-score optimizing objective function is general enough to be applied to any ILP-based joint inference tasks. Recall that identifying argumentative discourse structures consists of (1) identifying the locations and types of the argument components, and (2) iden-tifying how they are related to each other. Below we divide related works into five broad categories based on which of these subtasks they addressed.
 Argument location identification. Works in this category aimed to classify whether a sentence con-tains an argument or not (Florou et al., 2013; Moens et al., 2007; Song et al., 2014; Swanson et al., 2015). The usefulness of existing works is somewhat lim-ited by the task X  X  coarseness: it won X  X  tell us which portion of a potentially long sentence contains the argument, for instance, but it can serve as a poten-tially useful first step in argument mining. Argument component typing. Works in this cat-egory focused on determining the type of an argu-ment. The vast majority of previous works per-form argument component typing at the sentence level. For instance, Rooney et al. (2012) classi-fied sentences into premises, conclusions, premise-conclusions, and non-argumentative components; Teufel (1999) classified each sentence into one of seven rhetorical classes (e.g., claim, result, pur-pose); Burstein et al. (2003), Ong et al. (2014), and Falakmasir et al. (2014) assigned argumentative la-bels (e.g., claim, thesis, conclusion) to an essay X  X  sentences; Levy et al. (2014) detected sentences that support or attack an article X  X  topic; Lippi and Torroni (2015; 2016) detected sentences containing claims; and Rinott et al. (2015) detected sentences containing evidence for a given claim. Sentence-level argument component typing has limitations, however. For example, it can identify sentences con-taining claims, but it cannot tell how many claims a sentence has or where in the sentence they are. Argument location identification and typing. Some works focused on the more difficult task of clause-level argument component typing (Park and Cardie, 2014; Goudas et al., 2015; Sardianos et al., 2015), training a Conditional Random Field to jointly identify and type argument components. Argument component typing and relation identi-fication. Given the difficulty of clause-level argu-ment component location identification, recent argu-ment mining works that attempted argument compo-nent typing and relation identification are not end-to-end. Specifically, they simplified the task by assum-ing as input gold argument components (Stab and Gurevych, 2014b; Peldszus and Stede, 2015).
 End-to-end argument mining. To our knowl-edge, only Palau and Moens (2009) addressed all the argument mining subtasks. They employed a hand-crafted context-free grammar (CFG) to gener-ate (i.e., extract and type) argument components at the clause level and identify the relations between them. A CFG approach is less appealing in the essay domain because (1) constructing a CFG is a time-and labor-intensive task, (2) which would be more difficult in the less-rigidly structured essay do-main, which contains fewer rhetorical markers indi-cating component types (e.g. words like  X  X eject X , or  X  X ismiss X  which indicate a legal document X  X  conclu-sion); and (3) about 20% of essay arguments X  struc-tures are non-projective (i.e., when mapped to the ordered text, their argument trees have edges that cross), and thus cannot be captured by CFGs. Our corpus consists of 90 persuasive essays col-lected and annotated by S&amp;G. Some relevant statis-tics are shown in Table 1. Each essay is an average of 4.6 paragraphs (18.6 sentences) in length and is written in response to a topic such as  X  X hould high school make music lessons compulsory? X  or  X  X om-petition or co-operation-which is better? X .

The corpus annotations describe the essays X  argu-ment structure, including the locations and types of the components that make up the arguments, and the types of relations that hold between them. The three annotated argument component types include: Ma-jor Claims , which express the author X  X  stance with respect to the essay X  X  topic, Claims , which are con-troversial statements that should not be accepted by readers without additional support, and Premises , which are reasons authors give to persuade read-ers about the truth of another argument component statement. The two relation types include: Support , which indicates that one argument component sup-ports another, and Attack , which indicates that one argument component attacks another. Next, we describe our end-to-end pipeline argument mining system, which will serve as our baseline. In this system, ACI is performed prior to RI. 4.1 Argument Component Identification We employ a two-step approach to the ACI task, where we first heuristically extract argument com-ponent candidates (ACCs) from an essay, and then classify each ACC as either a premise, claim, major claim, or non-argumentative, as described below. 4.1.1 Extracting ACCs We extract ACCs by constructing a set of low pre-cision, high recall heuristics for identifying the lo-cations in each sentence where an argument com-ponent X  X  boundaries might occur. The majority of these rules depend primarily on a syntactic parse tree we automatically generated for all sentences in the corpus using the Stanford CoreNLP (Manning et al., 2014) system. Since argument components are a clause-level annotation and therefore a large major-ity of annotated argument components are substrings of a simple declarative clause (an S node in the parse tree), we begin by identifying each S node in a sen-tence X  X  tree.

Given an S clause, we collect a list of left and right boundaries where an argument component may be-gin or end. The rules we used to find these bound-aries are summarized in Table 2. We then construct ACCs by combining each left boundary with each right boundary that occurs after it. As a result, we are able to find exact (boundaries exactly match) and approximate (over half of tokens shared) matches for 92.1% and 98.4% respectively of all ACs. 4.1.2 Training the ACI Classifier We train a classifier for ACI using MALLET X  X  (Mc-Callum, 2002) implementation of maximum en-tropy classification. We create a training instance from each ACC extracted above. If the ACC X  X  left and right endpoints exactly match an annotated argument component X  X , the corresponding training instance X  X  class label is the same as that of the component. Otherwise, its class label is  X  X on-argumentative X . Each training instance is repre-sented using S&amp;G X  X  structural, lexical, syntactic, in-dicator, and contextual features for solving the same problem. Briefly, the structural features describe an ACC and its covering sentence X  X  length, punctua-tions, and location in the essay. Lexical features de-scribe the 1  X  3 grams of the ACC and its covering sentence. Syntactic features are extracted from the ACC X  X  covering sentence X  X  parse tree and include things such as production rules. Indicator features describe any explicit connectives that immediately precede the ACC. Contextual features describe the contents of the sentences preceding and following the ACC primarily in ways similar to how the struc-tural features describe the covering sentence. 4.2 Relation Identification We consider RI between pairs of argument compo-nents to be a five class classification problem. Given a pair of ACCs A 1 and A 2 where A 1 occurs before A 2 in the essay, either they are unrelated, A 1 sup-ports A 2 , A 2 supports A 1 , A 1 attacks A 2 , or A 2 at-tacks A 1 . Below we describe how we train and apply our classifier for RI.

We learn our RI classifier using MALLET X  X  im-plementation of maximum entropy classification. Each training instance, which we call a training re-lation candidate (RC), consists of a pair of ACCs and one of the above five labels. By default, the in-stance X  X  label is  X  X o relation X  unless each ACC has the exact boundaries of a gold standard argument component and one of the remaining four relations holds between the two gold argument components. We create training RCs as follows. We construct RCs corresponding to true relations out of all pairs of argument components in a training essay having a gold relation. As the number of potential RCs far exceeds the number of gold relations in an essay, we undersample the  X  X o relation X  class in the following way. Given a pair of argument components A and B between which there is a gold relation, we define A p to be the closest previous ACC in the essay as gen-erated in Section 4.1.1 such that A p  X  X  text doesn X  X  overlap with A . We also define A s as the closest succeeding ACC after A such that A s and A do not overlap. We define B p and B s similarly with respect to B . From these ACCs, we generate the the four in-stances ( A p ,B ) , ( A s ,B ) , ( A,B p ) , and ( A,B s of which have the  X  X o relation X  label, as long as the pairs X  text sequences do not overlap. We believe the resulting  X  X o relation X  training instances are infor-mative since each one is  X  X lose X  to a gold relation. We represent each instance using S&amp;G X  X  structural, lexical, syntactic, and indicator features for solving the same problem. Briefly, RC structural features describe many of the same things about each ACC as did the ACC structural features, though they also describe the difference between the ACCs (e.g. the difference in punctuation counts). Lexical features consist primarily of the unigrams appearing in each ACC and word pairs, where each word from one ACC is paired with each word from the other. Syn-tactic and indicator features encode the same infor-mation about each ACC as the ACC syntactic and indicator features did.

We now apply the classifier to test essay RCs, which are created as follows. Given that this is a pipelined argument mining system, in order to en-sure that the RI system X  X  output is consistent with that of the ACI system, we generate test RCs from all possible pairs of ACCs that the ACI system pre-dicted are real components (i.e. it labeled them something other than  X  X on-argumentative X ). 5.1 Motivation There are two major problems with the pipeline ap-proach described in the previous section. First, many essay-level within-task constraints are not enforced. For instance, the ACI task has the constraint that each essay has exactly one major claim, and the RI task has the constraint that each claim has no more than one parent. This problem arises because our ACI and RI classifiers, like those of S&amp;G, classify each ACI and RI test instance independently of other test instances. We propose to enforce such essay-level within-task constraints in an ILP framework, employing ILP to perform joint inference over the outputs of our ACI and RI classifiers so that the re-
The second problem with the pipeline approach is that errors made early on in the pipeline propa-gate. For instance, assume that a Support relation exists between two argument components in a test essay. If the pipeline system fails to (heuristically) extract one or both of these argument components, or if it successfully extracts them but misclassifies one or both of them as non-argumentative, then the pipeline system will not be able to identify the rela-tionship between them because no test RCs will be created from them. The above problem arises be-cause the RI classifier assumes the most probable output of the ACI classifier for each ACC as input. Hence, one possible solution to this problem is to make use of the n-best outputs of the ACI classifier for each argument component type, as this increases the robustness of the pipeline to errors made by the ACI classifier.

We obtain the n-best ACI outputs and employ them as follows. Recall that the ACI system uses a maximum entropy classifier, and therefore its out-put for each ACC is a list of probabilities indicat-ing how likely it is that the ACC belongs to each of the four classes (premise, claim, major claim, or non-argumentative). This means that it is possible to rank all the ACCs in a text by these probabilities. We use this idea to identify (1) the 3 most likely premise ACCs from each sentence, (2) the 5 most likely claim ACCs from each paragraph, and (3) the Given these most likely ACC lists, we combine pairs of ACCs into test RCs for the RI classifier in the following way. As long as the ACCs do not over-lap, we pair (1) each likely premise ACC with every other likely ACC of any type occurring in the same paragraph, and (2) each likely claim ACC with each likely major claim ACC. We then present these test RCs to the RI classifier normally, making no other changes to how the pipeline system works.

Employing n-best ACI outputs, however, intro-duces another problem: the output of the RI classi-fier may no longer be consistent with that of the ACI classifier because the RI classifier may posit a rela-tionship between two ACCs that the ACI classifier labeled non-argumentative. To enforce this cross-task consistency constraint, we also propose to em-ploy ILP. The rest of this section details our ILP-based joint inference approach for argument mining, which addresses both of the aforementioned prob-lems with the pipeline approach. 5.2 Basic ILP Approach We perform joint inference over the outputs of the ACI and RI classifiers by designing and enforc-ing within-task and cross-task constraints in the ILP framework. Specifically, we create one ILP program for each test essay, as described below.

Let Xn i , Xp i , Xc i , and Xm i be binary indi-cator variables representing whether the ILP solver believes ACC i has type none, premise, claim, and major claim, respectively. Let Cn i , Cp i , Cc i , and Cm i be the probabilities that ACC i has type none, premise, claim, and major claim, respectively, as dictated by the ACI maximum entropy classifier X  X  output. 4 Let a be the count of ACCs.

Let Y n i,j , Y s i,j , Y a i,j , Y rs i,j , and Y ra i,j binary indicator variables representing whether the ILP solver believes ACCs i and j have no relation, i is supported by j , i is attacked by j , j is supported by i , and j is attacked by i , respectively, where ( i,j ) appears in the set of RCs B that we presented to the RI system as modified in Section 5.1. We assume all other ACC pairs have no relation. Let Dn i,j , Ds i,j , Da i,j , Drs i,j , and Dra i,j be the probabilities that component candidates i and j have no relation, i is supported by j , i is attacked by j , j is supported by i , and j is attacked by i , respectively, as dictated by
Given these definitions and probabilities, our ILP program X  X  default goal is to find an assignment of these variables X and Y in order to maximize P ( X ) + P ( Y ) , where: subject to the integrity constraints that: (3) an ACC is either not an argument component or it has ex-actly one of the real argument component types, (4) a pair of component candidates ( i,j ) must have ex-actly one of the five relation types, and (5) if there is a relation between ACCs i and j , i and j must each
In the objective function, a and | B | serve to bal-ance the contribution of the two tasks, preventing one from dominating the other. 5.3 Enforcing Consistency Constraints So far we have described integrity constraints, but recall that our goal is to enforce consistency by im-posing within-task and cross-task constraints, which force the ILP solutions to more closely resemble real essay argument structures. Our consistency con-straints fall into four categories.

Our constraints on major claims are that: (6) there is exactly one major claim in each essay, (7) major claims always occur in the first or last paragraph, and (8) major claims have no parents.
Our constraints on premises are that: (9) a premise has at least one parent, and (10) a premise is related only to components in the same paragraph. where i &lt; j and j &lt; k mean ACC i appears before j , and j appears before k , and Par ( j ) is the set of ACCs in j  X  X  covering paragraph.

Our constraints on claims state that: (11) a claim a parent, that parent must be a major claim.
The last category, which comprises constraints that do not fit well into any other category, are: (13) the boundaries of actual components never overlap, (14) each paragraph must have at least one claim or major claim, and (15) each sentence may have at 5.4 F-score Maximizing Objective Function The objective function we employ in the previous subsection attempts to maximize the average prob-ability of correct assignment of variables over the ACI and RI problems. This kind of objective func-tion, which aims to maximize classification accu-racy, was originally introduced by Roth and Yih (2004) in their seminal ILP paper, and has since then been extensively applied to NLP tasks. However, it is arguably not an ideal objective function for our task, where F-score rather than classification accu-racy is used as the evaluation metric.

In this section, we introduce a novel method for constructing an ILP objective function that directly maximizes the average F-score over the two prob-lems. Recall that F-score can be simplified to: where TP, FP, and FN are the counts of true pos-itives, false positives, and false negatives respec-tively. Unfortunately, we cannot use this equation for F-score in an ILP objective function for two rea-sons. First, this equation involves division, which cannot be handled using ILP since ILP can only han-dle linear combinations of variables. Second, TP, FP, and FN need to be computed using gold annotations, which we don X  X  have in a test document. We propose to instead maximize F by maximizing the following: where TP e , FP e , and FN e , are estimated values for TP , FP , and FN respectively, and  X  attempts to balance the importance of maximizing the numera-2 TP term in the denominator because minimizing it would directly reduce the numerator.

To maximize average F-score, we can therefore attempt to maximize the function G c + G r and G r are the values of G in equation 17 as calcu-lated using the estimated values from the ACI and RI problem respectively.

The question that still remains is, how can we es-timate values for TP , FP , and FN mentioned in Equation 17? Our key idea is inspired by the E-step of the Expectation-Maximization algorithm (Demp-ster et al., 1977): while we cannot compute the ac-tual TP , FP , and FN due to the lack of gold anno-tations, we can compute their expected values using the probabilities returned by the ACI and RI classi-fiers. Using the notation introduced in Section 5.2, the expected TP , FP , and FN values for the ACI task can be computed as follows: where g and h can be any argumentative class from the ACI problem (i.e. premise (p), claim (c), or ma-jor claim (m)). The formulas we use to calculate TP e , FP e , and FN e for the RI problem are identi-cal except C is replaced with D , X is replaced with Y , and g and h can be any class from the RI problem other than no-relation. 6.1 Experimental Setup Corpus. As mentioned before, we use as our cor-pus the 90 essays annotated with argumentative dis-course structures by S&amp;G. All of our experiments are conducted via five-fold cross-validation on this corpus. In each fold experiment, we reserve 60% of the essays for training, 20% for development (select-ing features and tuning  X  ), and 20% for testing. Evaluation metrics. To calculate F-score on each task using Equation 16, we need to explain what constitutes a true positive, false positive, or false negative on each task. Given that j is a true argu-ment component and i is an ACC, the formulas for the ACI task are: where gl ( j ) is the gold standard label of j , pl ( i ) is the predicted label of i , n is the non-argumentative class, and i are considered an exact match if they have exactly the same boundaries, whereas they are considered an approximate match if they share over half their tokens.

We perform most of our analysis on approximate match results rather than exact match results as it can be difficult even for human annotators to iden-tify exactly the same boundaries for an argument lating these numbers for the RI problem except that j and i represent a true relation and an RC respec-tively, two relations approximately (exactly) match if both their source and target ACCs approximately (exactly) match, and n is the no-relation class. 6.2 Results and Discussion Approximate and exact match results of the pipeline approach (BASE) and the joint approach (OUR) are shown in Table 3. As we can see, using approximate improvements over the pipelined baseline system by improvements is shown in the last column, where our system outperforms the baseline by 13.9% ab-solute F-score (a relative error reduction of 18.5%). This is the most important result because it most di-rectly measures our performance in pursuit of our ultimate goal, to maximize the average F-score over both the ACI and RI problems. The highly signif-icant improvements in other measures, particularly the improvements of 13.2% and 14.6% in ACI and RI F-score respectively, follow as a consequence of this maximization. Using exact matching, the differ-ences in scores between OUR system and BASE X  X  are smaller and highly significant with respect to a smaller number of measures. In particular, under Exact matching OUR system X  X  performances on the RI-P and RI-R metrics are significant ( p &lt; 0 . 02 ), while under Approx matching, they are highly sig-nificant ( p &lt; 0 . 002 ). 6.3 Ablation Results To analyze the performance gains yielded by each improvement to our system, we show ablation re-sults in Table 4. Each row of the table shows the re-sults of one ablation experiment on the test set. That is, we obtain them by removing exactly one feature set or improvement type from our system.

The B aseline feature sets we remove include those for the A C I task ( C b ) from Section 4.1.2 and those for the R I task ( R b ) from Section 4.2. 13 The I LP improvement sets we remove are the D efault ILP ( I d ) system 14 from Section 5.2, the M ajor claim ( I m ), P remise ( I p ), C laim ( I c ), and O ther ( I o ) con-straints from Section 5.3 Equations 6  X  8, 9  X  10, 11  X  12, and 13  X  15 respectively, and the F -score maximizing objective function from Section 5.4.
Broadly, we see from the last column that all of our improvement sets are beneficial (usually signifi-their removal. Notice also that whenever remov-ing an ILP improvement set harms average F-score, it also simultaneously harms ACI and RI F-scores, usually significantly. This holds true even when the improvement set deals primarily only with one task (e.g. I o for the ACI task), suggesting that our system is benefiting from joint inference over both tasks. 6.4 Error Analysis and Future Work Table 3 shows that OUR system has more trouble with the RI task than the ACI task. A closer inspec-tion of OUR system X  X  RI predictions reveals that its low precision is mostly due to predicted relation-ships wherein one of the participating ACCs is not a true argument component. Since false positives in the ACI task have an outsized impact on RI preci-sion, it may be worthwhile to investigate ILP objec-tive functions that more harshly penalize false posi-tive ACCs.
 The RI task X  X  poor recall has two primary causes. The first is false negatives in the ACI task. It is im-possible for an RI system to correctly identify a re-lationship between two ACs if the ACI system fails to identify either one of them as an AC. We be-lieve ACI recall, and by extension, RI recall, can be improved by exploiting the following observations. First, we noticed that many argument components OUR system fails to identify, regardless of their type, contain words that are semantically similar to words in the essay X  X  topic (e.g., if the topic men-tions  X  X chool X , argument components might men-tion  X  X tudents X ). Hence, one way to improve ACI recall, and by extension, RI recall, would be to cre-ate ACI features using a semantic similarity mea-sure such as the Wikipedia Link-based similarity measure (Milne and Witten, 2008). Second, major claims are involved in 32% of all relationships, but OUR system did an especially poor job at identify-ing them due to their scarcity. Since we noticed that major claims tend to include strong stancetaking lan-guage (e.g., words like  X  X hould X ,  X  X ust X , and  X  X e-lieve X ), it may be possible to improve major claim identification by constructing an arguing language lexicon as in Somasundaran and Wiebe (2010), then encoding the presence of any of these arguing words as ACC features.
 The second major cause of OUR system X  X  poor RI recall is its failure to identify relationships be-tween two correctly extracted ACs. We noticed many of the missed relationships involve ACs that mention some of the same entities. Thus, a coref-erence resolver could help us build features that de-scribe whether two ACCs are talking about the same entities. We presented the first results on end-to-end argu-ment mining in persuasive student essays using a pipeline approach, improved this baseline approach by designing and employing global consistency con-straints to perform joint inference over the outputs of the tasks in an ILP framework and proposed a novel objective function that enables F-score to be max-imized directly by an ILP solver. In an evaluation on Stab and Gurevych X  X  corpus of 90 essays, our ap-proach yields an 18.5% relative error reduction in F-score over the pipeline system.
 We thank the three anonymous reviewers for their detailed comments. This work was supported in part by NSF Grants IIS-1147644 and IIS-1219142.
