 Although data quality has been recognized as an important factor in the broad information systems research, it has received little at-tention in recommender systems. Data quality matters are typically addressed in recommenders by ad-hoc cleansing methods, which prune noisy or unreliable records from the data. However, the set-ting of the cleansing parameters is often done arbitrarily, without thorough consideration of the data characteristics. In this work, we turn to two central data quality problems in recommender sys-tems: sparsity and redundancy. We devise models for setting data-dependent thresholds and sampling levels, and evaluate these using a collection of public and proprietary datasets. We observe that the models accurately predict data cleansing parameters, while having minor effect on the accuracy of the generated recommendations.
Data quality is an important practical consideration in many in-formation systems. It can have a strong effect on the performance of the system and the level of user satisfaction. Data quality re-ceived significant attention in the general context of information systems, but it has yet to be thoroughly investigated through the recommender systems X  prism. For instance, what dimensions of data quality are particularly important for recommenders and what methods can address these? Although there exists some evidence that data quality issues do matter [2, 3, 8], little work has looked into the application of data quality methods to recommenders. These are typically addressed through an ad-hoc data cleansing, such as  X  X rune users with less than X ratings X  or  X  X onsider data from the recent period Y  X . But the setting of the data cleansing parameters is often arbitrary and asks for more methodical solutions.
This work addresses two data quality problems in recommender systems. The first refers to the well-established data sparsity prob-lem. To this end, we devise a novel model for setting data-dependent threshold for filtering of cold items or users, not having enough data
This paper is part of the author X  X  PhD thesis. Partially supported by ISF grant number 571/14.
 c  X  to facilitate generation of reliable recommendations. The second considers the data redundancy problem, which may lead to signif-icant overheads at the recommendation model training stages. We propose a method for adaptive sampling of users that can decrease the model training overheads, while still facilitating the construc-tion of accurate recommendation models. This paradigm is used successfully by Azure Machine Learning recommendations [7].
We propose heuristic models for setting the item threshold and user sampling rate, both without building the recommendation mod-els . These heuristic models are evaluated using a large collection of public and proprietary recommender system datasets from a range of domains. We observe that the models accurately predict the data cleansing parameters, while having only minor effect on the accu-racy of the generated recommendations.

In summary, the contribution of our work is twofold. First, we highlight and demonstrate the importance of data quality matters in recommender systems. Second, we address two practical data qual-ity issues of sparsity and redundancy, by proposing and validating models for adaptive setting of the data cleansing parameters.
Wang and Strong developed a framework encapsulating the fun-damental dimensions of data quality [10]. They derived more than 100 data quality attributes and split these into four dimensions. The intrinsic quality dimension refers to the core data characteristics, e.g., accuracy, objectivity, and reputation. Contextual quality con-siders the data in the context of the task at hand and includes at-tributes like relevancy, completeness, and timeliness. Representa-tional quality refers to the format (representation and consistency) and meaning (interpretability) of the data. Finally, the accessibility dimension primarily considers data security. Pipino et al. turned to the assessment and metrics of data quality [6]. With the attributes proposed in [10] in mind, they presented a methodology for devel-oping objective metrics communicating the fit of data regardless of the application and task at hand.

Specifically in recommender systems, it has been observed that the rating data can be noisy, imprecise, or outdated [2, 8]. Ama-triain et al. demonstrated that offering users to re-rate previously rated items would lead to somewhat different ratings, which sub-stantially change the accuracy of the generated recommendations [2]. Marlin and Zemel questioned the assumption of uniformity in user rating distribution, and showed that this assumption dete-riorated the accuracy of collaborative recommendations [4]. Said et al. evaluated the stability of user ratings over time and offered users to re-rate already rated items [8]. It was found that this omni-present white noise in user ratings poses  X  X he magic barrier X  to the accuracy attainable by recommender systems.
To the best of our knowledge, little work has studied the recom-menders X  data sparsity and redundancy from the data quality per-spective. In this work, we systematically address these two data quality attributes and propose methods for dataset-dependent set-ting of sparsity-and redundancy-related data cleansing parameters.
Most rating-based recommender system datasets contain a con-siderable portion of cold users and items. The small number of rat-ings for these is not sufficient to build a reliable recommendation model, such that the common practice is to omit such items and users outright, as part of the data cleansing process. The real prob-lem, however, is to determine the appropriate cleansing thresholds for a given dataset. A too-low threshold may result in noisy training data and imprecise recommendation models, whereas a too-high threshold may lead to overlooked rating patterns and preclude the system from generating recommendations for these users/items.
A brute-force solution to determining the cleansing threshold could be to exhaustively evaluate all the plausible combinations of item and user thresholds. Even for a small dataset, the number of such combinations is in the thousands, which rules the brute-force solution out for practical business cases. Thus, our aim is to de-velop a heuristic method that predicts the optimal thresholds for a given user-item rating matrix, without building the model. In this work we focus on the optimization of one  X  either item or user  X  threshold, and leave the concurrent optimization of the two for the future. Without the loss of generality, we discuss below the method applied to the item threshold.

We assume that the target threshold value for items is correlated with the average length of the item vectors in the dataset, r erage number of ratings assigned to an item. However, this feature alone is not sufficient for achieving accurate threshold predictions. Hence, another feature we exploit stems from the parameterization of the power-law distribution of ratings. Let H be the distribution of the item vector lengths. We fit H to a power-law distribution Ax  X  m , where x is the length of the item vector. Since, typically, there is a small number of popular items with many ratings and a large number of items with a few ratings, m is positive.
We model the item threshold value as a function of r i and m , and assume positive correlation between r i and the item threshold. This is explained by the more robust nature of longer item profiles. Also, we assume negative correlation between the value of m and the threshold. This is due to the observation that when m increases, the weight of the tail of the power-law distribution decreases and there are fewer items with many ratings. Hence, for high m the sparsity of the data is higher and the item threshold is lower. We parameterize the model by a linear multiplier  X  . In summary, we model 1 the item threshold IT d of a dataset d as
We use 24 public and proprietary datasets with either implicit or explicit item ratings. Among the public datasets are Movielens, Million Songs, Flixster, Moviepilot, Filmtipset, Yelp, Yahoo! Mu-sic (broken down into albums, artists, and tracks), and BookCross-ing. The 14 proprietary datasets (referred to as PD) were obtained
We experimented with several other models of IT and the model in Equation 1 yielded the most accurate performance.
 from various companies and sites, and belong to application do-mains of eCommerce, car sales, real estate, books, software pur-chases, grocery shopping, app downloads, video games, and more.
Each dataset was partitioned into the training and test sets using the 90-10 ratio. In order to assess the accuracy of the predictions, we used the Precision@K metric [9]. Since the datasets are fairly different, the value of K was set dynamically to 10% of the dataset item set size. The split was repeated ten times, as per the N-fold validation methodology, and the reported precision scores are the averages computed across the ten splits.

We first exhaustively found the optimal item threshold IT each dataset d . For this, we gradually increased the value of the item threshold IT , filtered from the data items having less than IT ratings, trained the Matrix Factorization (MF) recommendation model [5, 7] on the cleansed data, and measured the precision score obtained for a fixed test set. The threshold, for which the highest precision was obtained, is referred to as IT opt d , while the corre-sponding precision score is P opt d .

Then, we applied the threshold model in Equation 1 to com-pute the predicted item threshold IT pred d . This was done using leave-one-out cross-validation. That is, one dataset d was with-held, the threshold model was trained on the other 23 datasets, and we applied the model to predict the IT pred d threshold for d . Having set the item threshold to IT pred d , we trained the recommendation model on the data, with items having less than IT pred d ratings being filtered out. Given this model, we computed the precision P the predictions generated by the model for the fixed test set.
This allows us to derive two performance metrics of the threshold predictions. The first, referred to as the normalized threshold error ( NTE ), is computed by NTE d = | IT opt d  X  IT pred d | /IT communicates the error of the item threshold predictions. The sec-ond quantifies the impact of NTE on the predictions of the recom-mendation model for the test set. This is referred to as the accuracy ratio ( AR ) and is computed by AR d = P pred d /P opt d . Note that al-though the threshold model is trained to predict the item threshold IT d , our objective is to cleanse the data in a way that maximizes AR , i.e., P d ( P pred d /P opt d ) , across the 24 datasets.
We present in Figure 1 the individual NTE and AR scores ob-tained for the 24 datasets. Each dataset is represented by two bars: the left represents NTE d and the right  X  AR d . The datasets are sorted in an increasing order of NTE d . As can be seen, the first 14 datasets achieve NTE d  X  0 . 4 , whereas the next 8 achieve 0 . 5  X  NTE d  X  1 , and for the last 2 datasets we observe NTE d  X  2 (these bars are truncated). Overall, the average NTE score across the 24 datasets is 0.632.
However, of a greater interest is the impact of NTE on AR . All the datasets demonstrate AR d  X  0 . 8 , whereas 19 out of the 24 datasets achieve remarkably high AR d  X  0 . 95 . The overall av-erage AR value across all the datasets is 0.966. Thus, despite the observed threshold prediction errors, the recommendation models built using the cleansed data generate predictions comparable to those of the models using the optimal threshold values. We ob-serve correlation of -0.540 between the values of NTE d and AR This aligns with the intuition that lower errors in the item threshold predictions yield more accurate recommendation models.

To better understand the setting of the item threshold, we carry out another experiment, in which we fix the item threshold IT for all the datasets. We compute the AR , averaged for various values of IT across the 24 datasets. The average AR is compared to three baseline AR using the following IT setting: (i) computed by the model in Equation 1; (ii) computed by a model using only the av-erage item length r i ; and (iii) computed by a model using only the exponent m of the item length distribution. The results are shown in Figure 2, where the horizontal axis stands for the value of the IT threshold and vertical  X  for the average AR across the 24 datasets. As expected, the three baselines are independent of IT and their AR scores are constant. We observe that the model in Equation 1 outperforms the individual models using either r i or m by 13.4% and 14.4%, respectively. The fixed threshold model demonstrates an inverse-curve behavior: for low IT the data is noisy, while for high IT too much data is filtered, such that in both cases the predic-tions are inaccurate. The highest AR is achieved for IT = 17 , but this is still 1.5% lower than that of the model in Equation 1. Con-sider also that such a-priori parameterization may not be feasible for recommenders with dynamic user/item sets and the superiority of our parameter-free model becomes evident.
Another important data quality problem is to identify cases, where some data available in the dataset is redundant, and the recommen-dation model can be built using a sample of the data. Focusing on random sampling, our target is to pick the lowest sampling rate that will still result in the recommendation model as close as possible to the model that would have been built using the complete data. This is particularly important for practical recommenders and very large scale datasets, where building the complete model may be costly and time consuming. Again, the challenge is to predict the sampling rate, without building the recommendation model.
Unlike in the item cleansing threshold case, there is no optimal sampling rate, because the recommendation model built using the complete data is always superior to, i.e., more accurate than, the one built using the sampled data. Hence, we define the target sam-pling rate SR as the lowest rate, for which the similarity between the complete recommendation model and the sampled model is greater than a pre-determined parameter  X  . The similarity of the two models is established by comparing the predictions generated by the models for a fixed test set.

In more detail, let us denote by U d , I d , and R d the number of users, items, and ratings, respectively, in a dataset d . We first build the recommendation model using the complete d . As usual in rec-ommender systems, we sample the users in d [1]. Given a sampling rate SR , we retain in the dataset SR  X  U d randomly chosen users. Then, we build the recommendation model using the sampled data and generate predictions for a fixed test set. Given a performance metric, we can finally compare the predictions generated by the recommendation model using the sampled data with the ones gen-erated by the model using the complete data.

Since the sampling is done on the users, we assume the level of redundancy to be positively correlated with U d . We also assume positive correlation with the density of the rating matrix, computed data, which we denote by V-structure . Intuitively, V-structure is the relative increase in the similarity of two users given that they have at least one commonly rated item. We compute V-structure as the ratio between the average pair-wise similarity of users hav-ing at least one jointly rated item and the overall average pair-wise user similarity. Since high V-structure of a dataset reflects a greater amount of common rating patterns observed, we posit that the re-dundancy is positively correlated with V-structure .

We model the redundancy level of a dataset d as a combination of three parameters: number of users, density, and V-structure . Note that the redundancy is inversely correlated with the sampling rate. That is, when the data is redundant, we sample a small portion of users to build a reliable recommendation model. Also, we need to clamp the sampling rate to the [0 , 1] range and keep the func-tion monotonic increasing. We use the hyperbolic tangent function for normalization purposes. In summary, we model 2 the minimal sampling rate SR d of a dataset d as
For the evaluation of the sampled models, we used 19 propri-etary datasets with implicit and explicit ratings. Each dataset d was partitioned again into the training and test sets using the 90-10 ra-tio. Also in this experiment the split was repeated ten times and the reported performance was averaged across the ten splits.
We exhaustively found the optimal sampling rate SR opt d for each dataset d . For this, we first trained MF recommendation model [5, 7] using the complete training dataset and applied this model to generate predictions for a fixed test set. We denote these predic-tions generated by the model using the complete data as complete predictions . Then, we gradually decreased SR by steps of 0.1. For each value of SR we randomly sampled the training data, built the recommendation model using the sampled data, and generated predictions for the same fixed test set.
Here, we also experimented with several other models of SR , and the best performance was achieved by the model in Equation 2.
More fine-grained steps of SR were not sufficiently sensitive.
We used the NDCG metric [9], where the gain of each item was proportional to its rank in the complete predictions, to quantify the predictions generated by the sampled models 4 . We considered the complete model and the sampled model to be sufficiently similar, as long as the NDCG computed by the sampled model for the fixed test set was greater than  X  = 0 . 95 . Thus, we decreased the sample rate SR by steps of 0.1 as long as we managed to obtaine NDCG  X  0 . 95 . The minimal SR , for which this NDCG had been obtained, was considered the optimal sampling rate SR
On top of this, we applied the model in Equation 2 to predict the sampling rate SR pred d for each d . Since the optimal sampling rate SR opt d was an approximation found by search with steps of 0.1, also SR pred d was rounded to the closest 0.1 mark. Having set the sampling rate of d to SR pred d , we created the sampled dataset, then built the sampled MF recommendation model, and generated predictions for the fixed test set. Finally, we evaluated the perfor-mance, NDCG pred d , of the recommendation model built using the predicted sampling rate SR pred d .

Figure 3 presents the results of the sampling rate predictions for the 19 datasets. Each dataset is represented by three bars: namely, SR opt d , SR pred d , and NDCG pred d . The datasets are sorted in a de-creasing order of SR opt d . As can be seen, the values of SR across the datasets from 1 (no sampling is needed, all the users are necessary) to 0.1 (only 10% of users are necessary). For 10 datasets out of the 19 we observe SR opt d = 1 , which aligns with the estab-lished sparsity problem in recommender systems. However, for 6 datasets we observe SR opt d  X  0 . 2 , indicating that some datasets have high degree of redundancy in the data.
 Overall, the predicted sampling rates produced by the model in Equation 2 are close to the optimal ones. We observe that SR and SR pred d are identical for 10 datasets out of the 19 (note that for 7 datasets, we observe SR opt d = SR pred d = 1 , i.e., no sampling needed), for 6 datasets the difference is 0.1 (3 over-sampled and 3 under-sampled), and for 6 datasets the difference is 0.2 ( SR over-samples). The average difference between SR opt d and SR across the 19 datasets is 0.063. Note that when SR opt d 6 = SR we prefer to over-sample, i.e., SR pred d &gt; SR opt d , as in this case, despite keeping unnecessary users, the recommendation model still achieves the desired degree of similarity to the complete model.
We also observe high NDCG pred d scores, such that for 15 datasets we achieve NDCG pred d  X  0 . 95 . These include the 7 datasets with SR opt d = SR pred d = 1 , where no sampling is performed and we
As NDCG combines ranking and predictive accuracy metrics, we deem it to be a reliable model performance indicator. obviously achieve NDCG pred d = 1 . The average NDCG pred tained across the 19 datasets stands at 0.964. Finally, we observe negative correlation of -0.382 between the obtained NDCG pred scores and the absolute value of the difference between the pre-dicted and optimal sampling rate, | SR opt d  X  SR pred d | . This result is not surprising, since the accuracy of the recommendation mod-els built using the sampled data deteriorates with the error in the sampling rate predictions generated by the model in Equation 2.
Our work was driven by the need to instantiate data quality mod-els for recommender systems. To this end, we addressed two prac-tical considerations of large-scale recommenders: sparsity of user ratings and redundancy of users in the datasets. We developed two models for predicting the data cleansing parameters and demon-strated their validity using a large collection of datasets. Notably, these models capitalize only on the parameters of the datasets and do not require the costly recommendation model building.
This work paves the way for future works on data quality in rec-ommender systems. First, the proposed predictive models for data cleansing parameters were evaluated using the MF recommenda-tion model. However, our models should be evaluated with other recommendation techniques, as, for instance, the item threshold may depend on the underlying recommendation model. Second, the impact of data cleansing on other performance metrics. The filtering of cold users/items and the sampling of users can affect the coverage and the diversity of the generated recommendations. Hence, there is a need to strike the balance between data quality assurance and these metrics. Third, we will consider the ways to incorporate content features of the items and demographic features of the users in the proposed predictive models. [1] X. Amatriain, A. Jaimes, N. Oliver, and J. M. Pujol. Data [2] X. Amatriain, J. M. Pujol, N. Tintarev, and N. Oliver. Rate it [3] S. Berkovsky, T. Kuflik, and F. Ricci. The impact of data [4] B. M. Marlin and R. S. Zemel. Collaborative prediction and [5] U. Paquet and N. Koenigstein. One-class collaborative [6] L. L. Pipino, Y. W. Lee, and R. Y. Wang. Data quality [7] R. Ronen, N. Koenigstein, E. Ziklik, M. Sitruk, R. Yaari, and [8] A. Said, B. J. Jain, S. Narr, and T. Plumbaum. Users and [9] G. Shani and A. Gunawardana. Evaluating recommendation [10] R. Y. Wang and D. M. Strong. Beyond accuracy: What data
