 In the recent past there has been a growing interest in analysis of interval-valued data in the learning community [1,2]. Data generated in many real world problems are subject to interval valued uncertainty. In such cases it maynot be possible to describe the data by a precise value but intervals may be a more proper description. For example, in the case of cancer diagnosis, a tumorous tissue is analyzed and features are com puted for each cell nucleus. However, since the features vary among cells of a tissue, usually, the mean and worst-case (minimum/maximum) feature values of the tissues are considered 1 . Micro-array data, obtained by profiling experiments on genetic markers, are another such instances of noisy data. Past research has shown that handling uncertainty in such applications by the representation as interval data leads to accurate learn-ing algorithms [3,1]. Classification formulations which are capable of handling interval data have immense importance from a pragmatic perspective. This pa-per presents a maximum-margin classification formulation which uses means and bounding hyper-rectangles (support) of the interval-valued training examples in order to build the decision function. As shown in the paper, the proposed clas-sifier is robust to interval uncertainty and is also not overly-conservative. The idea is to model interval-valued uncertainty using Chance-Constrained Programming (CCP). The main contribution of the paper is to approximate the CCP as a Second Order Cone Program (SOCP) using Bernstein schemes [4]. SOCPs are well studied convex optimiza tion problems with efficient interior point solvers (e.g. SeDuMi [5]). The key advantage of the Bernstein scheme is that no assumptions on distributions regarding the underlying uncertainty are made and only partial information like support and mean of the uncertain ex-amples is required. Geometric interpretation of the SOCP formulation reveals that the classifier views each example wit h interval uncertain ty as a region of in-tersection of its bounding hyper-rectangle and an ellipsoid centered at its mean. Thus the proposed classifier is far less conservative than the methods which utilize the bounding hyper-rectangle information alone. Since a classifier X  X  con-servativeness directly affect s the classification margin achieved, the proposed classifier is expected to generalize bette r. Methodology of classifying uncertain test examples is discussed and error measures for evaluating performance of in-terval data classifiers are presented. Experimental results show that the proposed classifier outperforms state-of-the-art when evaluated using any of the discussed error measures.

The paper is organized as follows: in section 2, the main contributions of the paper are presented. In section 3, exper iments on real-world and synthetic data are presented. Section 4 summarizes the work and concludes the paper. This section presents the main contribution of the paper, a novel maximum-margin formulation for interval data in section 2.1. A discussion on geometric interpretation of the proposed formulation is presented in section 2.2. The section concludes with a discussion on error measures which evaluate the performance of a classifier on interval data. 2.1 Maximum-Margin Formulation Using Bernstein Bounds In this section, a maximum-margin classification formulation for interval data, which assumes the mean and the bounding hyper-rectangles are known for each example, is presented. It is also assumed that the features describing the data are independent. Let X i =[ X i 1 ...X in ] be the random vector representing i th training example ( n denotes dimensionality of the data) and y i denotes its label E [ X ] denote mean of the random variable X .

Consider the problem of constructing a maximum-margin classifier using the training example X i , which have interval-valued uncertainty. Let the discriminat-ing hyperplane be denoted by w x  X  b = 0. Then the constraints y i ( w X i  X  b )  X  1 ensure that the training data is classified correctly. Slack variables,  X  i  X  0, can be introduced in order to handle outliers. Thus the classification constraints turn X , they cannot be satisfied always. Hence, alternatively, one can ensure that the following chance-cons traints are satisfied: where 0  X   X  1 is a small number denoting the upper bound on misclassification error made on an example and is a user-given parameter. Using these constraints the following maximum-margin formulation, similar in spirit to SVMs [6], can be written: In the following we will show that the CCP (2) can be approximated as an SOCP problem by using Bernstein bounds. To this end, the following theorem is presented, which specia lizes the Bernstein approxim ation schemes described in [4,7]: Theorem 1. Consider the following notation (  X  i =1 ,...,m, j =1 ,...,n ): l  X  ij = E [ X ij ]  X  i =  X  i 1 ... X  in  X  i = diag ([  X  (  X  i 1 ) ... X  (  X  in )]) where  X  (  X  ij ) is given by:  X  (  X  ij )=min  X   X  0 The chance-constraint (1), which represents the classification constraint for i th example, is satisfied if the following cone constraint, in variables w ,b, X  i , z i , holds: Proof. The chance-constraint (1) can be written as: Now, let variables u i , v i be chosen such that: we have that the chance-constraint (1) is satisfied if: the variables u i , v i (6) is to utilize the bounding hyper-rectangle information via the inequality v i X i  X  v i m i + L i v i 1 (also see lemma 2).

Using Markov inequality and independence of random variables X ij ,j = 1 ,...,n ,wehave The Key of modeling chance-constraint (7) now depends on how one upper-bounds the moment generating functions E [exp { tX ij } ] ,t  X  R . To this end, we use the following lemma: Lemma 1. Consider the notation in (3). Then, Proof. The fact that exp { tX ij } is a convex function gives the following inequal-both sides and re-writing the resulting inequality in terms of m ij ,l ij gives:  X  obtain (9). This completes the proof of Lemma 1.
 Using Lemma 1, from (8) we obtain: log Prob u i X i + u i 0  X  0  X   X  ( u i 0 + u we ensure that for certain  X  the right-hand side of the inequality is  X  log( ), then we would satisfy the chance-constraint (7). Choosing  X   X  =  X  u i 0 + u i  X  i L the one minimizing right-hand side of the inequality, we see that (7) and in turn (1) are satisfied if: Substituting the value of u i 0 , eliminating the variable u i from (6), (11) and introducing the variable z i = L i v i gives (5). This completes the proof of the theorem.
 The values of  X  (  X  ij ) (4) can be calculated numeric ally (refer Appendix A). Using theorem 1 and CCP (2), a maximum-margin SOCP formulation for interval data which ensures that the probability of misclassification is less than ,canbe written as follows: C and are user-given parameters. 2.2 Geometric Interpretation of the Formulation In this section, a geometrical interpretation for the proposed formulation (12) is presented. To this end, consider the following lemma: Lemma 2. Consider the notation in (3) and let S i = L 2 i  X  2 i , X  = 2 log(1 / ) . Suppose E (  X  i , S i , X  ) represents the ellipsoid x =  X  i +  X  L i  X  i u : u 2  X  1 and R ( a i , b i ) : The continuum of constraints represented in (13) is satisfied if and only if the constraint (5) holds.
 Proof. The constraint (13) hold if and only if: Note that, max x  X  X  (  X  the set E (  X  i , S i , X  )  X  X  ( a i , b i ) (denoted by I E (  X  support function of intersection of two sets is the infimal convolution of support I Thus we have: (13)  X  1  X   X  i + y i b +inf I E (  X  Now it is easy to see that I E (  X  v i m i + L i v i 1 . Substituting these values in (14) and noting that  X  i = L i  X  i + m i ,gives(6,11).Sincetheconstraints(6,11)areequivalentto(5)(seeproofof theorem 1), we have that (13)  X  (5). This completes the proof.
 The above lemma shows that the proposed classifier (12) views each interval data example as the intersection of the bounding hyper-rectangle and an ellip-soid centered at its mean with semi-axis lengths proportional to l ij , X  (  X  ij ). In this way the proposed formulation takes into account both the mean and bound-ing hyper-rectangle informations. Note that, lemma 2 theoretically proves that the proposed classifier is always less conservative (pessimistic) than classifiers which use the bounding hyper-rectangle information alone [3] (this is because E utilize the bounding hyper-rectangle information alone and classifiers which uti-lize the mean information alone are special cases of the proposed formulation (with =0and = 1 respectively).

It is interesting to note the effect of  X  (  X  ij ) (4) on the proposed classifier. As mentioned earlier, the semi-axis lengths of the uncertainty ellipsoid are propor-tional to  X  (  X  ij ). Table 3 shows that as  X  increases from 0 to 1,  X  (  X  ) decreases from1to0.Inotherwords,asthemeanofexampleshiftsfromcentertoacorner of the bounding hyper-rectangle, the size of the uncertainty ellipsoid decreases. This is very intuitive because, in one extreme case where mean lies at a corner, the datapoint is deterministic and in the other extreme case, where mean is at center of the hyper-rectangle, the uncertainty of the datapoint is maximum. This phenomenon is also illustrated in figure 1, where the bounding hyper-rectangle and the uncertainty region at various positions of the mean are shown. It can be seen that as the mean moves towards a corner, not only does the uncertainty region move but also its size decreases. However, a classifier which does not employ the mean information [3], always views the example as the whole hyper-rectangle. Thus the proposed classifier is r obust to interval-va lued uncertainty, as well as not overly-conservative. 2.3 Classification with Uncertain Examples This section presents the methodology for l abeling interval-valued test examples and discusses error measures for evaluating the performance of interval data classifiers. Depending on the form in which the examples are available, the la-beling methodologies summarized in table 1 can be employed. Here, y pr i denotes the predicted label for test example X i (also refer (3) for notation). Once a test example is labeled using the appropriate methodology, the overall misclassifi-cation error for the given test dataset can be calculated as the percentage of examples in which y pr i and y i do not agree:
Note that, the proposed formulation can be solved when the training examples are in any of the 3 forms shown in table 1. In case of form 2, the support and mean information are readily available and in case of form 3 these partial information can be easily estimated from the replicates. In case of form 1, since no mean information is available the proposed formulation can be solved using =0, which as discussed in section 2.2 is the maximum-margin classifier built using support information alone.

Based on the discussion in section 2.2, a nother interesting error measure can be derived. Given an uncertain test example X i with label y i , one can calculate E (  X  i , S i , X  ) touches the discriminating hyperplane, w x  X  b = 0. Additionally, proof of theorem 1 shows that the true probability of misclassification of the test example is less than or equal to opt . This leads to the following definition of erroroneachtestexample: The overall error, OptErr , can be calculated as percentage of OptErr i over all test examples:
Note that, both NomErr and OptErr can be estimated for any hyperplane classifier and are not specific to the propo sed classifier. Experimental results show that the proposed classifier achieves lower NomErr and OptErr when compared to existing interval data classification methods. In this section, experimental results whi ch compare performance of the proposed interval data classifier (12) (denoted by IC-MBH ) and the maximum-margin classifier which utilizes bounding hyper-rectangle information alone [3] (denoted by IC-BH ): are presented. Note that the only difference between (18) and the one proposed in [3] is minimization of w 2 in the objective rather than minimization of w 1 , which implies maximum-margin classification rather than sparse classification. We have done this in order to achieve a fair comparison of the methodologies. Traditional classifiers like SVMs cannot handle interval-valued data. However, in cases where the means of the uncerta in examples are known or in cases where uncertainty is represented using replic ates (e.g. form 2 and 3 in table 1 respec-tively), SVMs can be trained by considering mean of each example as a training datapoint or by considering each replicate as a training datapoint. Henceforth, let these classifiers be denoted by IC-M and IC-R respectively. Hence, wher-ever applicable, we compare the performance of the proposed classifier with SVM based methods also.

Experiments were performed on syntheti c datasets and two real-world datasets: micro-array data 2 [1] and Wisconsin Diagnostic Breast Cancer (WDBC) dataset 3 . Synthetic datasets were generated using the following methodology: a) nominal (true) examples were generat ed using Gaussian mixture models b) uncertainty was introduced into each nominal point using standard finite-supported distributions (whose parameters are chosen randomly) c) replicates for each nominal example were produced by sampling the chosen noise distribution. The synthetic datasets are named using dimension of the dataset and are subscripted with the distribu-tion used for generating replicates (e.g. synthetic data of dimensionality n with Uniform, truncated skew-Normal and truncated Beta noise distributions are de-noted by n U , n SN and n  X  respectively). In each case, a synthetic test dataset was also generated independently.

The micro-array dataset defines a 4 category classification task, namely that of identifying four kinds of drugs: Azoles ( A ), Fibrates ( F ), Statins ( S )and Toxicants ( T ). Instead of handling a multi-class problem we have defined six binary classification tasks using  X  X ne versus one X  scheme. As a preprocessing step we have reduced the dimension of th e problem to 200 by feature selection using Fisher score. In case of both synthetic and micro-array data, the means and bounding hyper-rectangles were estimated from the replicates provided for each training example.

The task of WDBC is to classify  X  X enign X  and  X  X alignant X  tumours based on 10 features computed from tumour cell nuclei. However, since the measurements are not the same over all tumour cells, the mean, standard-error and maximum values of the 10 features are provided. From this information the bounding hyper-rectangles and means for each training example are estimated.
 In section 3.1, we compare classification margins (2 / w 2 )achievedby IC-BH and IC-MBH , which represent state-of-the-art and the proposed inter-val data classifier respectively. The key results of the paper are presented in section 3.2. These results compare the NomErr (15) and OptErr (17) obtained with various classifiers. 3.1 Comparison of Margin In this section, the margins (2 / w 2 )achievedby IC-BH and IC-MBH at a fixed value of the C parameter are compared. Fig ure 2 summarizes the results. Note that, at all values of , the classification margin with IC-MBH is higher than that with IC-BH . Also, as the value of or dimensionality of the data increases, difference in the margins achieved by IC-BH and IC-MBH also increases. The explanation for this is clear from the geometric interpretation presented in section 2.2. According to Structural Risk minimization principle of Vapnik [6], higher margin implies better generalization. Hence the proposed classifier is expected to achieve good generalization for interval data. As a base-line for comparison, the margin achieved by the SVM trained using means of the examples, IC-M , is also shown in the figure. Since IC-M does not take into account the interval uncertainty and assumes the mean to be the  X  X rue X  training example, it always achieves higher margin than IC-BH, IC-MBH .Thetrend shown in figure 2 remained the same for higher dimensions and with different noise distributions ( n SN , n  X  ). 3.2 Comparison of Generalization Error This section presents results wh ich compare the performance of IC-M, IC-R, IC-BH and IC-MBH when evaluated using the error measures NomErr (15) and OptErr (17). Experiments were done on the synthetic and real-world datasetsdescribedinsection3.Ina ll cases, the hyper-parameters ( C and/or ) for each classifier were tuned using a 3 -fold cross-validation procedure. The results are summarized in table 2. In case of synthetic datasets, the reported val-ues represent the mean testset error ach ieved with the tuned hyper-parameters when trained with 10 different training s ets each generated from the same syn-thetic data template. In case of the real-world datasets, the values represent cross-validation error with tuned hyper-p arameters averaged over three cross-validation experiments. Hence the erro r values reported in the table repre-sent a good estimate of the generalizat ion error of the respective classifiers. Clearly, NomErr and OptErr are least for IC-MBH ; confirming that IC-MBH achieves good generalization for interval data. Moreover, in case of many datasets, the proposed classifier outperforms the existing classifiers in terms of the OptErr error measure. This paper presents a novel maximum-margin classifier which achieves good generalization on data having interval-valued uncertainty. The key idea was to employ chance-constraints in order to handle uncertainty. The main contribution was to derive a tractable SOCP formulation, which is a safe approximation of the resulting CCP, using Bernstein schemes. The formulation ensures that the probability of misclassification on interval data is less than a user specified upper-bound ( ). Also, the geometric interpretation shows that the classifier views each training example as the region of intersection of its bounding hyper-rectangle and an ellipsoid centered at its mean.

The proposed classifier is robust to interval-valued uncertainty and is also not overly-conservative. As shown in the paper, this amounts to achieving higher classification margins and in turn better generalization than methods which employ the bounding hyper-rectangle information alone. As the results showed, the average error with the proposed classifier, in case of many synthetic and real-world datasets, is less than half of that with the existing methods.
The Bernstein relaxations schemes pr esented in this paper not only aid in approximating the original CCP as a convex program, but also open avenues for efficient approximations of other CCP-based learning formulations (e.g. [9] and its variants). By employing rich partial information, the Bernstein schemes lead to less conservative relaxations. Hence exploitation of the Bernstein schemes in the context of learning is a good direction for research.
 SB and CB are supported by a Yahoo! faculty grant.
 In this section, we presen t details of the numerical procedure for computing  X  (  X  ). Consider the following claim: Claim. Let  X  (  X  ) be as defined in (4). Then, 1  X   X  2  X   X  (  X  )  X  1. Proof. Recalling the definition of  X  (  X  ), we have: g then there exists a neighbourhood around t =0where f ( t ) &lt; 0(since f (0) = is a necessary condition for f  X  0. In other words,  X  (  X  )  X  1  X   X  2 . Also, from proofoflemma2wehavethat  X  (  X  )  X  1. This completes the proof.
 Note that, the function f strictly increases with the value of  X  and by claim A we have that 1  X   X  2  X   X  (  X  )  X  1. Thus one can have a simple binary search algorithm for computing  X  . The algorithm starts with  X  l 0  X  1  X   X  2 and  X  u 0  X  1. until a relevant stopping criteria is met. Also, as the proof of claim A suggests, for fixed values of  X ,  X  , the function f has only one minimum wrt. t (this is because g 2 ( t ) is concave above t = t  X  and convex below t = t  X  ). Hence checking whether f min i  X  0forafixedvalue  X  i is also easy. The values of  X  as a function of  X   X  [0 , 1] are shown in table 3. Since the function  X  (  X  ) is symmetric wrt.  X  , we have  X  (  X  )=  X  (  X   X  ).

