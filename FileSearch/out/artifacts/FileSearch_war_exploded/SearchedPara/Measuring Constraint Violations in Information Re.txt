 Recently, an inductive approach to modelling term-weighti ng function correctness has provided a number of axioms (con-straints), to which all good term-weighting functions should adhere. These constraints have been shown to be theoret-ically and empirically sound in a number of works [2, 3, 1]. It has been shown that when a term-weighting function breaks one or more of the constraints, it typically indicate s sub-optimality of that function. This elegant inductive ap -proach may more accurately model the human process of determining the relevance a document. It is intuitive that a person X  X  notion of relevance changes as terms that are ei-ther on or off-topic are encountered in a given document. Ultimately, it would be desirable to be able to mathemati-cally determine the performance of term-weighting functio ns without the need for test collections.

Many modern term-weighting functions do not satisfy the constraints in an unconditional manner [3]. However, the de -gree to which these functions violate the constraints has no t been investigated. A comparison between weighting func-tions from this perspective may shed light on the poor per-formance of certain functions in certain settings. Moreove r, if a correlation exists between performance and the number of violations, measuring the degree of violation could help more accurately predict how a certain scheme will perform on a given collection.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms: Experimentation, Measurement, Perfor-mance Keywords: Information Retrieval, Constraints, Axioms Four constraints (axioms) have been postulated in order to capture the basic principles of term-weighting function correctness. These are detailed in [3] and [1]. The first con-straint (C1) states that adding a new query term to a docu-ment must always increase the score of that document. The second constraint (C2) states that adding a non-query term to a document must always decrease the score of that docu-ment. The third constraint (C3) states that adding succes-sive query terms to a document should increase the score of the document less with each successive addition. The fourth constraint (constraint 4) states that adding more non-quer y terms to a document should decrease the score of a docu-ment less with each occurrence [1]. These constraints (whil e relatively intuitive) can constrain the term-weighting fu nc-tion in complex ways. For example, constraint 1 cannot be guaranteed to be satisfied by modern term-weighting func-tions as the decrease in score due to the document increasing in length (i.e. by normalisation) cannot be guaranteed to be offset by the increase in score due to the query-term being added [1].
The approach used to measure the number of constraint violations in this work takes a stemmed query and docu-ment. The terms in the document remain in the same order in which they naturally appear. A pseudo-document is cre-ated by using the first term appearing in the document. This pseudo-document is then matched against the query using a term-weighting functions and the score is recorded. A fur-ther pseudo-document is created by including the next term appearing in the document. This is then matched against the query and the score is again recorded. This continues until the complete document is scored against the query. The violations of each constraint is measured as new terms are added to the pseudo-document.

When the score of a document does not increase when a query term is added to the pseudo-document a violation of constraint 1 (C1) is recorded. When the score of a document does not decrease when a non-query term is added a viola-tion of constraint 2 (C2) is recorded. If the increase in scor e of the document when a query term is added is equal to or greater than the increase in score when the previous occur-rence of that query term was added, a violation of constraint 3 (C3) is recorded. Finally, when three non-query terms ap-pear in succession and the inverse of the score reduction is not sub-linear a violation of constraint 4 (C4) is recorded.
Due to the computational complexity of such an approach, it is infeasible to do this for an entire test collection. The re-fore, we measure the number of violations of constraints on the top 1000 documents returned from a benchmark term-weighting functions. The top 1000 documents should repre-sent a set of documents with a high number of query terms and therefore is a good sample of documents on which to measure the number of constraint violations.
We use the FBIS, FT, FR collections from TREC disks 4 and 5 as test collections. For topics 251 to 450 we create a short query set (title field only), a medium length query set (title and description), and a long query set (title, descri p-tion and narrative). We also use the OHSUMED collection and its topics. Table 1 shows some of the characteristics of the collections used in this research. As per the origi-nal axiomatic study [3], we performed stemming but did not remove stopwords. A term-weighting function which cor-rectly models relevance should be able to correctly weight all terms.
 We use five term-weighting functions in these experiments. We use the default BM 25 function, the pivoted normalisa-tion function ( P IV ), the I ( n ) L 2 function from the diver-gence from randomness model ( DF R ), a modified BM 25 function ( MBM 25) in which the idf part is replaced with the pivoted document length normalisation idf function and a learned term-weighting function ( ES ) [1].
Table 2 shows the number of constraint violations per doc-ument per query averaged over all the test collections for th e top 1000 documents of the best retrieval run. For example, the original BM 25 function violates all the constraints and, on average, violates constraint 1 an average of 407.5 times for each document for long queries. Thus, this table gives a general view of the constraint violations across the colle c-tions. It is intuitive that there are more constraint violat ions for longer queries as there are more matching query-terms for the average document and therefore more complex inter-actions.

Tables 3 show the performance of the schemes on the indi-vidual collections. The best scheme is in bold and statistic al significance (0.05% level) using a one-tailed t-test compar ed to the next best scheme ( DF R ) is denoted by an asterisks (*). The best performing scheme across the collections is the scheme which breaks constraints less often on the test collections (i.e. the ES scheme).

The  X  measure is Spearman X  X  correlation between the to-tal number of constraint violations of a function on that particular collection and the MAP of the scheme on that collection. We can see that although the sample size is quite small, the data indicates that there is a consistent inverse correlation between the ranking of the schemes by perfor-mance and the ranking of schemes by the total number of constraint violations. The large number of violations of co n-straints on medium and long queries for the original BM 25 schemes explains the very poor performance of this scheme on these types of queries (as indicated in the original work [3]) due to the non-removal of stopwords.

We have outlined an approach that counts the number of actual constraint violations using a inductive framewor k and shown that the total number of constraint violations is inversely correlated with performance.
