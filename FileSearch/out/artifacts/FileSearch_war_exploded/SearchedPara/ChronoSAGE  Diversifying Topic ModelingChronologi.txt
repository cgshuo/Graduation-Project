 In text mining, topic modeling approach prevails, because it can give a compact representation of topics found in a doc ument set as word lists. Latent Dirich-let allocation (LDA) [1] represents each topic as a word probability distribution and extracts a predefined number, say K , of topics from a document set. Conse-quently, LDA provides a summarizing view of documents as K word lists, each expressing a particular subject in a human-readable way (cf. Figure 8 in [1]).
However, recent applications of text mi ning require using metadata to make topic modeling results more persuasive. Especially, timestamps are often con-sidered due to their importance in SNS posts, newswire documents, academic articles, etc. We thus use timestamps so that per-topic word probability distri-butions are time-dependent. We make our approach based on sparse additive generative models (SAGE) [2] and call it ChronoSAGE .WedenotetheLDA-type SAGE in its simplest form, given in Section 4 of [2], as vanilla SAGE . ChronoSAGE has three types of parameters for defining word probabilities: the parameters for topics, those for timestamps, and those for pairs of a topic and a timestamp. The parameters of the first type only follows the original rationale of LDA and vanilla SAGE. Those of the second type are intentionally introduced to filter out the words showing non-informative time-dependency, e.g.  X  X unday X ,  X  X ay X ,  X 2001 X , etc. Further, those of the third type give time-differentiated word probabilities for each topic and make our approach attractive. ChronoSAGE out-puts as many human-readable word lists as timestamps for each topic. That is, when the number of timestamps is T , ChronoSAGE outputs TK word lists.
In the evaluation, we compare ChronoSAGE and vanilla SAGE with LDA in terms of pointwise mutual information [4] to show the basic competence of the SAGE-type approaches. Further, we present an example of time-dependent word lists extracted by ChronoSAGE and discuss them from a qualitative viewpoint. ChronoSAGE is an application of the multi-faceted SAGE [2] for using document timestamps. We give its generative description, but omit the inference details.  X  With respect to each word w , draw a variance parameter  X  kw for each topic k ,  X  Obtain the probability  X  tkw that the word w is used to express the topic k in  X  For each document d , draw a multinomial parameter  X  d =(  X  d 1 ,..., X  dK )
ChronoSAGE has three types of parameters defining word probabilities.  X  kw s make word probabilities dependent on topics. Vanilla SAGE only has this type of parameters.  X  tw s are introduced to find the words showing non-informative time-dependency. Table 1 presents top seven words sorted by  X  tw s for each timestamp t in TDT4, a document set used in our evaluation (cf. Section 3). In TDT4, we gave the same timestamp to the documents belonging to the same range of seven days, e.g. from December 14 to 20, 2000. In Table 1, most words are just the dates falling in the corresponding range. Since  X  tw s remove non-informative time-dependency in this manner,  X  tkw s can make per-topic word probabilities time-differentiated and can give informative word lists as we show later in Fig. 1. We firstly compare ChronoSAGE and vanilla SAGE with LDA in terms of PMI to show the competence of SAGE-type approaches and secondly evaluate the timestamped word lists extracted by ChronoSAGE from a qualitative viewpoint. We used the three document sets given in Table 2. DBLP is a set of paper titles in DBLP CS bibliography 1 . NSF is available at the UCI ML repository 2 .We regarded publication years as timestamps in DBLP and NSF. TDT4 is a corpus for the topic detection and tracking evaluation by LDC 3 .Thenumber K of topics was set to 100 or 300. For each of the compared approaches, i.e., LDA, vanilla SAGE, and ChronoSAGE, we initialized topic assignments randomly, ran Gibbs sampling [3] as pretraining, and ran a variational inference ten times. We thus obtained ten topic modeling results for each approach and for each setting of K .
We adopted an external evaluation measure, called pointwise mutual infor-mation (PMI) [4], to make our evaluation realistic. The reference corpus for PMI was the entire English Wikipedia downloaded on June 6, 2013, contain-ing 7,298,899 entries. We selected top 10 words ( w 1 ,...,w 10 )sortedby  X  kw sfor each k and calculated PMI for every word pair as PMI( w i ,w j )=ln p ( w i ,w j ) p ( w i, j  X  X  1 ,..., 10 } . p ( w i ) is defined as R i /R ,where R i is the number of documents containing w i in the reference corpus, and R is the size of the reference corpus. p ( w i ,w j ) is defined as R ij /R ,where R ij is the number of documents containing both w i and w j in the reference corpus. We compared the three approaches by the median of the PMIs calculated for all word pairs. A larger median is better.
The left panel of Fig. 1 summarizes the comparison in terms of PMI. The ten medians obtained from the ten differe nt runs of the inference are plotted for each approach and for each setting of K . The horizontal axis represents PMI. As this panel shows, ChronoSAGE gave almost the same medians as vanilla SAGE. Further, both methods worked better than LDA for NSF and TDT4 and at least gave a result comparable with LDA for D BLP. Therefore, it can be concluded that SAGE-type topic modeling is a better choice than LDA in terms of PMI. Next, we give an example of the time-differentiated word lists obtained by ChronoSAGE on the right panel of Fig. 1. This example was obtained for DBLP when K = 300. These word lists seemingly related to mobile communications. On the top of the panel, top 15 words are given in the order of their  X  kw s. These words represent the time-independent content of the topic. The size of an ellipse behind each word indicates the magnitude of  X  kw . Below these top 15 words, we present top 10 words sorted by  X  tkw s for each timestamp t . The size of a circle behind each timestamp indicates the largest  X  tkw for each t . In this example, we can find clear trends. The word  X  X SM X , mainly related to 2G networks, appears in the word lists of earlier years. The word  X  X PRS X  comes after it and appears in the lists of 2001 and 2002. The word  X  X TE X  only appears in the word lists of the most recent years. In this manner, Ch ronoSAGE can extract clear trends, because it diversifies topic modeling results chronologically by using  X  tkw s. In this paper, we proposed ChronoSAGE, an application of the multi-faceted SAGE standing on its own merit. The evaluation led to two conclusions. Firstly, ChronoSAGE and vanilla SAGE were superior to LDA in terms of PMI. Sec-ondly, ChronoSAGE extracted informative timestamped word lists, though vanilla SAGE cannot do this extraction due to its model construction. Our important future work is to explicitly model the i nherent dependency among timestamps.
