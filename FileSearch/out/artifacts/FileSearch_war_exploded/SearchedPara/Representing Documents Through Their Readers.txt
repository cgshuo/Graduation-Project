 From Twitter to Facebook to Reddit, users have become accustomed to sharing the articles they read with friends or followers on their social networks. While previous work has modeled what these shared stories say about the user who shares them, the converse question remains unexplored: what can we learn about an article from the identities of its likely readers?
To address this question, we model the content of news articles and blog posts by attributes of the people who are likely to share them. For example, many Twitter users de-scribe themselves in a short profile, labeling themselves with phrases such as  X  X egetarian X  or  X  X iberal. X  By assuming that auser X  X labelscorrespondtotopicsinthearticlesheshares, we can learn a labeled dictionary from a training corpus of articles shared on Twitter. Thereafter, we can code any new document as a sparse non-negative linear combination of user labels, where we encourage correlated labels to ap-pear together in the output via a structured sparsity penalty.
Finally, we show that our approach yields a novel doc-ument representation that can be e ff ectively used in many problem settings, from recommendation to modeling news dynamics. For example, while the top politics stories will change drastically from one month to the next, the  X  X ol-itics X  label will still be there to describe them. We eval-uate our model on millions of tweeted news articles and blog posts collected between September 2010 and September 2012, demonstrating that our approach is e ff ective. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing document modeling, Twitter, structured sparsity  X  Work done while at Carnegie Mellon University.

In today X  X  world, it has become commonplace for read-ers to share news articles and blog posts with their friends and followers on social networking sites. Understanding that much of their future success depends on such tra ffi c, news sites and blogs have made it easy for their readers to share articles they find interesting, from the ubiquitous  X  X hare X  buttons alongside news content, bearing the logos of Facebook, Twitter and others, to so-called  X  X ocial reader X  apps built directly into Facebook. The Guardian newspa-per, for example, recently announced that, for the first time, more visits to their site were coming through Facebook than through Google search. 1 This user behavior gives us an un-precedented chance to study the readers of news articles at alargescalebyanalyzingtheirpublicdigitalfootprint.
In the past, work has been done that uses such data in the setting of personalized news, recommending articles to read-ers based on previous articles that they may have shared or liked [8, 11, 16]. However, in this paper, we seek to inves-tigate a di ff erent question: rather than modeling a reader by the articles she shares, what can we instead learn about an article from the attributes of its readers? Specifically, can we build a valuable, general purpose document repre-sentation by representing new articles X  X ever before seen or shared X  X ith the predicted attributes of their likely readers? To address this question, we utilize the microblogging site Twitter as a testbed, as it is widely used by readers as a pub-lic medium for disseminating articles and interesting links. In particular, Twitter users share articles by tweeting them. Moreover, many Twitter users also describe themselves in a short profile description, using words like  X  X egetarian X  or  X  X unner X  (Figure 1). (Following the convention of previ-ous work [10], we will refer to these user attribute labels as badges .) As most Twitter profiles are public, we can thus scan millions of tweets to learn the relationship between ar-ticles and the badges of users who share them.

To look at an article through the lens of its readers, one could directly analyze the profiles of all Twitter users who have shared the article. This approach, however, is impos-sible to extend to articles not shared extensively on Twit-ter. We thus take the more general approach of associating badges with the content of the articles rather than with the articles themselves. Specifically, we learn a sparse dictionary from a vast collection of tweeted news articles; each column in the dictionary X  X  weight vector over the vocabulary X  http://www.guardian.co.uk/gnm-press-office/ changing-media-summit-tanya-cordrey Figure 1: Example of a Twitter user profile. Labels such as  X  X hysician, X   X  X iscal, X  or  X  X ealthcare X  all describe the user X  X  interests, and we refer to these as badges . corresponds to a specific badge. For example, if users who have the word  X  X egetarian X  in their Twitter profiles often share articles about health food, then we might learn that a X  X egetarian X  X adgeisassociatedwithhighweightsonthe words  X  X ofu X  and  X  X ale. X  By learning such a labeled dictio-nary, we can use badges to represent new articles.
Modeling article content through user attributes, in con-trast with user-oblivious approaches such as latent Dirichlet allocation (LDA) [4], o ff ers a more interpretable representa-tion of the articles for personalization and recommendation algorithms. For example, in content-based filtering, user preferences are commonly represented as weight vectors over avocabularyoftextualfeatures,suchaswordsortopicsfrom atopicmodel[11,24].Byusingbadgesasthesetextualfea-tures, we expect to obtain a more natural representation of user preferences: vegetarian readers can be directly de-scribed by a  X  X egetarian X  badge, rather than by potentially less focused topics from an arbitrary topic model.
Likewise, for personalization applications, a badge-based representation occurs at a more appropriate level of granu-larity than lower-level word-based representations, such as tf-idf. We see this in Figure 2, where we show both word-based and badge-based representations of an article from The Guardian . 2 This article is about a particular militant group operating on the Afghanistan-Pakistan border, and in Figure 2a, we see that the most important words in this doc-ument correspond to the name of this network: the Haqqani group . While informative, such a representation of the arti-cle is likely too specific; we expect a reader of this article to be broadly interested in Afghanistan and Pakistan, and not just singularly focused on the Haqqani group.

Another advantage of using badges to represent articles is that by associating the relatively stationary badges with the highly dynamic latent topics, one can naturally match the corresponding latent topics across di ff erent time periods. For instance, while what it means to be  X  X iberal X  changes from month to month, as expressed in what self-described liberals share on Twitter, the  X  X iberal X  badge is persistent, allowing us to produce a direct correspondence between X  X ib-eral X  topics from di ff erent periods of time. In contrast, in a traditional topic modeling setting, we would be forced either to perform a heuristic bipartite matching on the topic-word distributions from the di ff erent time periods, to best match the unlabeled topics with each other, or to resort to a more complicated model that directly models the time stamp of each article, which can lead to ine ffi cient inference [2].
In the remainder of this paper, we describe our approach for learning a badge-based representation of documents from the self-described attributes of their likely readers. We then perform an extensive evaluation of our approach, and show through both examples and quantitative experiments that http://www.guardian.co.uk/world/2012/sep/07/ haqqani-network-blacklisted-terrorist-us Figure 2: Here, we see the di ff erence between the word repre-sentation and badge representation of the same article from The Guardian , X  X aqqaninetworkisconsideredmostruthless branch of Afghan insurgency X  (September 7, 2012). In (a), the size of a word is proportional to its tf-idf weight, while in (b), the size of a badge is proportional to the weight it is assigned via the approach in Section 3.2. (Throughout this paper, we will display badges in blue and words in black.) incorporating reader information into content analysis yields an article representation that is more interpretable for hu-man understanding and more e ff ective for personalization.
We give a succinct high-level summary of our model and algorithms in this section and provide full details in the fol-lowing sections of the paper: 1. We collect a training data set of tweeted news articles 2. We learn a labeled dictionary X  X hose columns cor-3. Given a new article from the same time period, we
If we have data from multiple time periods, we learn a separate dictionary per time period.
The data we gather from Twitter is threefold: (1) we take each tweeted article, download its content, and represent it as a vector of words following the bag-of-words convention; (2) we associate each article with the users who have tweeted it; (3) we associate each user with a set of descriptive words from his or her profile, which we refer to as badges .
Given this data, we face two challenges: first, we must rep-resent each badge as a weighted set of characteristic words, and, second, we seek to represent any article as a weighted set of badges whose characteristic words collectively best represent the content of the article.

We emphasize that any acceptable solution to these chal-lenges must be scalable ,whileatthesametimeincentivizing sparsity .Anapproachthatisnotscalablecouldnothandle the web-scale data sets we encounter in our setting; in a given month of tweets, we must learn thousands of badges from millions of news articles. Meanwhile, sparsity leads to an interpretable, parsimonious document representation, while simultaneously improving scalability. We emphasize that we want sparsity in two parts of our model: each badge should have a small set of characteristic words, and each article should be described by a small set of badges.
As an obvious first step, we might consider the vast exist-ing literature on probabilistic topic modeling [3]. A standard LDA-based topic model organizes a document collection into so-called topics, where each topic is a distribution over words in a vocabulary. Each document is then represented as a dis-tribution over topics. Standard topic modeling approaches do not incentivize sparsity, leading to dense document and topic representations. More complex topic modeling-based approaches exist that incorporate sparsity (e.g., [23]), but they are not naturally scalable to web-scale data sets.
As such, we take an alternative approach to addressing these challenges that allows us to directly control the spar-sity of the representation while maintaining scalability. For-mally, we let V denote the size of the vocabulary in our training data, N the number of training documents, and K the total number of badges. From a generative perspective, we think of the document i ,representedasa V -dimensional vector over the words, y i , as formed by: B is a non-negative V  X  K matrix with a column for each badge, representing the weighted set of characteristic words for the badge.  X  i is a K -dimensional vector that similarly represents the weighted set of characteristic badges associ-ated with document i .Weborrowatermfrominformation theory and refer to B as our badge dictionary ,whereeach column of B is an entry in the dictionary. Our sparsity as-sumptions translated to this setting mean that both B and  X  must have small numbers of non-zero entries. The train-ing corpus of articles along with the user profile information provides us the y i  X  X  and information about the  X  i  X  X  for many documents with which we can learn the matrix B ;werefer to this phase as X  X earning the dictionary. X  We then can apply the dictionary B to analyze contents of new documents, es-timating their  X  i vectors corresponding to relevant badges; we refer to this phase as  X  X oding the documents. X 
For each document i in our training corpus, we observe the content vector y i ,andthebadgesofthereaderswhoshared document i on Twitter. This set of badges X  X eported by the document X  X  readers X  X oes not give us direct access to  X  ,becauseitmaycontainirrelevantbadgeswhilepoten-tially omitting important badges. However, this di ffi culty is not insurmountable; as we are only interested in a high-level association between badges and article content, drawn from a large collection of users and articles, it is reasonable and su ffi cient to assume that, on average, the documents shared by readers self-identified with a specific badge k will be relevant to badge k ,whiledocumentssharedbyreaders without badge k will be irrelevant to badge k .Therefore,to learn the dictionary B , we approximate  X  i by taking each reader of document i ,andassumeuniformweightsoverthe badges declared in his or her profile. We then aggregate over all of document i  X  X  readers. More precisely, we assume  X  indicating whether user u identifies with badge k .
With each y i given and each  X  i approximated, the badge dictionary B can be learned by choosing a loss function and minimizing the loss objective:
We constrain all entries of B to be non-negative to make the results more interpretable, and use the well-studied $ regularization on the entries of B to encourage sparsity in the learned B matrix.

In our work, we let y i be a term frequency-inverse doc-ument frequency (tf-idf ) vector of the words in document i (cf. [17]), normalized to have $ 2 -norm of 1. The  X  i vec-tor, as described before, gives a uniform weight to the set of all badges of the readers of document i ,normalizedalso to have unit $ 2 -norm. We then minimize a square-error loss and choose the regularization parameter  X  B that achieves a desired level of sparsity in the resulting B matrix:
We optimize Eq. 1 using a simple projected stochastic gradient descent, described further in the supplemental ma-terial. 3 This approach to optimization allows us to operate on large, streaming, web-scale data sets. We further nor-malize each column of B to have unit $ 2 -norm, to prepare us for coding documents, as described in the next section.
Many techniques learn both B and  X  i from the training corpus X  X on-negative matrix factorization and LDA, for ex-ample. However, joint estimation of both B and  X  i is inher-ently more complex; with many more variables to learn, the estimation is slower and the solution quality is poorer. In contrast, our method uses the reader attribute information to guide the estimation of  X  ,thusdrasticallyreducingthe learning complexity.
Astraightforwardapproachforrepresentinganewdocu-ment in terms of badges is to take the same loss-objective as in the dictionary learning phase, and optimize over  X  i instead of B .Thatis,givenanewdocument i ,weoptimize: where we again encourage sparsity in the estimated  X  i by the $ 1 -regularization.

With squared-error, our objective takes the form of the well-known non-negative lasso:
We again borrow a term from information theory and refer to an optimization like in Eq. 2 as coding the article in terms of the badges. Eq. 2 can be solved e ffi ciently through various algorithms, including coordinate descent and Shotgun [5].
In practice, there is a subtle problem with the formula-tion in Eq. 2. Many badges tend to be highly related, such as  X  X rogressive X  and  X  X iberal, X   X  X chool X  and  X  X tudent, X  and  X  X egan X  and  X  X egetarian. X  These closely-related badges tend to model similar content and overlap in explanatory power. Thus, the estimated set of relevant badges X  X he non-zero http://www.cs.cmu.edu/~kbe/badgepaper_supp.pdf Figure 3: Badge representation of an article about Mac OS XLion,withandwithoutgraphregularization.Thesizeof abadgeisproportionaltoitsweight. entries of the estimated  X  i vector, encouraged to be as small as possible by the sparsity regularization X  X ould arbitrarily include, e.g., either  X  X rogressive X  or  X  X iberal, X  but not both. The fact that these choices are arbitrary has undesirable consequences: for instance, given two very similar articles about the liberal political view on education, one may be represented by the badges  X  X rogressive X  and  X  X chool X  and the other by a completely disjoint set of badges,  X  X iberal X  and  X  X tudent X . Any learning algorithm that uses the se-lected badges as features would consequently be misled into treating the two articles as completely dissimilar.
Ameliorating this problem requires two steps: (1) we must detect similarity relations among the badges; and, (2) we must augment the article coding objective so that groups of closely-related badges X  X .g.,  X  X rogressive X  and  X  X iberal X  X  would be selected together in the article representation.
To determine whether two badges are related, we look at co-occurrence counts of the badges in the profiles of Twitter users. Closely related badges might either frequently co-occur in user profiles X  X n cases like  X  X bama X  and  X  X iberal X  X  or each frequently co-occur with some other common badge X  in cases like  X  X iberal X  and  X  X rogressive, X  with the common badge perhaps being, e.g.,  X  X ctivist X  or  X  X logger. X  To ad-dress both cases, we form a weighted undirected graph over the badges where each edge between two badges has a weight proportional to the frequency that these two badges co-occur in Twitter user profiles. More precisely, if s and t represent two distinct badges, we let the weight of the edge between highly related badges would either be neighbors in this graph or be connected by a very short path, where the weights of the edges on the path would be very high.

Given such a graph, we augment our model with the graph-guided fused lasso regularization of Kim et al. [15]: min where w st is the weight of the badge pair ( s, t ) in the co-occurrence graph G ,asdefinedabove. Thegraphfusion regularization encourages  X  is to be close to  X  it for all edges ( s, t )inthegraph,wherethestrengthoftheregularization is proportional to the weight of the edge. In this way, highly related badges, closely connected in G by heavily weighted edges, are incentivized to be turned on or o ff simultane-ously, since similar values of  X  i  X  for such badges lowers the objective. The graph fusion regularization parameter,  X  G regulates how big a role the graph G should play in regular-izing  X  .WereferthereaderstotherecentworkofChenet al. [7] for a detailed discussion of the optimization algorithm for solving Eq. 3, which we use in our approach.

As an example of how this graph regularization addresses our problem, we can consider an article about Mac OS X Lion. 4 Coding this article with the vanilla lasso, without http://www.macobserver.com/tmo/article/my_ favorite_stealthy_lion_features/ graph regularization, leads to a badge representation over-whelmed by the  X  X ions X  badge. This is problematic because, while the  X  X ions X  badge well explains the word  X  X ion, X  which appears often in the article, the main usage of the  X  X ions X  badge occurs in the context of the Detroit Lions football team. As a result, the Mac OS X article could, with respect to the computed badge representation, be more similar to afootballarticlethantoatechnologyarticle. Whenusing the graph-guided fused lasso however, we obtain a more bal-anced coding, with the badges  X  X pple X  and  X  X eek X  now being the most dominant, taking up nearly sixty percent of the squared two-norm of the badge vector (cf. Figure 3).
The reason for this improvement is evident when we con-sider the neighbors of X  X ions X  X nd X  X pple X  X n our badge graph. The strongest links emanating from the  X  X ions X  badge are related to Michigan X  X .g.,  X  X etroit X  and  X  X live X  (a Michi-gan news site) X  X r to animals X  X .g.,  X  X ungle, X  X  X onkey X  and  X  X oar. X  These neighboring badges do not do a good job ex-plaining the Mac OS X article, and so this forces  X  X ions X  to be downweighted. However, if we consider the strongest neighbors of  X  X pple X  in the badge graph, we see words such as  X  X anboy, X   X  X ailbreak X  and  X  X pod, X  which are much more related to the content of the article.
The idea of inferring information about documents from their readers is not new; there is a rich line of research on collaborative filtering , which classifies, filters, or recommends documents by detecting readership patterns which, in some sense, represent the collaborative e ff ort of all the readers [21]. The most common approaches for collaborative filter-ing, such as matrix completion [6], leverage the intuition that similar readers tend to read similar documents, and thus recommend articles to users if they were read by users with similar past behavior. However, such approaches must overcome the cold start problem ,where,forexample,they are unable to infer much meaningful information about arti-cles that do not have a large number of readers. In contrast, our approach avoids this problem altogether by associating user preferences with the content of the articles, and thus can be used to analyze articles which have never been read.
Popular methods for collaborative filtering often assume low-dimensional latent factors in readership patterns. Our approach also involves latent factors, but guides the latent variable discovery by associating each factor with a badge. Thus, our model can handle many latent factors without sacrificing much computational or statistical e ffi ciency.
Because the latent factors in our model associate user preferences with topics in the document contents, our work draws upon the massive existing literature on topic model-ing [3]. Of the countless varieties of topic models, the la-beled LDA model [20] is particularly relevant, as it presents amethodofassociatingeachlatenttopicwithanobserved tag. Though it is reasonable to try to use labeled LDA to tie badges with topics, we prefer our dictionary learning algorithm, as it allows us to better promote sparsity and incorporate badge relations.

Likewise, while we can imagine an alternative discrimi-native formulation of our problem as a multi-label classifier (cf. [22]), we found that it was more natural to express the desired structured sparsity of the output in the form of a generative model.
Finally, our work is inspired by previous work that at-tempted to learn a latent badge representation of individual users based on their Twitter behavior [10]. Our work has adi ff erent goal, in that we seek to build a general-purpose document representation by learning associations between badges and document content from millions of users. More-over, the prior work uses a pre-defined set of approximately 30 badges, while the badge dictionaries we learn using our methodology are comprised of thousands of badges.
We conduct an extensive empirical analysis of our badge-based document representation, focusing on the question we posed at the beginning of the paper. Specifically, we seek to show that by representing documents by attributes of their likely readers, we can create a document representation suitable for personalization.

We begin by describing the large data set we use for our evaluation, followed by both anecdotal descriptions and quantitative comparisons, showing that our badge-based doc-ument representation is useful and insightful.
To evaluate our method, we must obtain a training set of tweeted news articles. We achieve this with access to the Twitter Garden Hose stream, which is an approximately 10% random sample of all tweets. In our experiments, we consider three months-worth of tweets: September 2010, September 2011 and September 2012. 5 For each of these three months, we scan through every tweet in the Garden Hose and extract those that are: (1) a tweet of a link; and, (2) came from a user with a non-empty profile. This leaves us with over 120 million tweets across the three months.
Next, as we are particularly interested in news articles, and not videos, photos, games and other such shared web pages, we filter the tweeted links to match one of 20,000 mainstream news sources, as defined by Google News. We then download each news article shared in this set of tweets that we believe to be written in the English language, re-sulting in a smaller, but extremely rich, data set of nearly 3 million tweeted news articles.

We use standard heuristics to extract the most meaningful unique words in these articles to create a vocabulary for each time period, as well as extract all badges that occur more frequently than a specified threshold. This leaves us
Throughout the development of our approach and algo-rithms, we used a held-out validation set of tweets and tweeted articles, corresponding to July 2011 and July 2012. with 4,460 unique badges in September 2010, 5,029 badges in September 2011, and 5,247 badges in September 2012, and vocabulary sizes of about 55,000 words.

Based on this training data, for each of the three months, we can compute the  X  and y vectors, as well as the undi-rected graph over badges with weights w st ,andcommence with dictionary learning, as described in Section 3.1. We learn a separate badge dictionary for each of the three months; we expect many common badges (because, e.g., there are al-ways  X  X egetarians X ), but we expect the word representations of each badge to change over time. Moreover, it is important to note here the computational e ffi ciency of our dictionary learning method as compared to training a standard topic model: on the largest of our data sets, our algorithm, run-ning on a single core, finishes in 224 minutes, more than six times faster than a state-of-the-art distributed LDA imple-mentation with the same number of topics (cf. Section 5.4).
For the quantitative comparisons, we require a test set of articles. While our training requires the analysis of tweets, any documents X  X ncluding never-before-published ones X  X an be represented using our badge-based document representa-tion. Thus, for our test set, we download eight entire sec-tions from The Guardian ,aleadingBritishnewspaper,over the three months considered in our training set, comprising nearly 14,000 articles. We represent each test article as a tf-idf vector over the time-specific vocabulary constructed during training. We then code each article by optimizing Eq. 3, using the dictionaries learned from the training data.
More details on the data processing pipeline and our op-timization can be found in our supplemental material.
After learning badge dictionaries from the three training sets, we can ascertain how well the badge-labeled topics cap-ture semantic themes in our data.
 Most Prevalent. As a first example, we can examine the badges that we use most often (i.e., highest total weight) to code the Guardian articles from September 2012 in our test set. Three of these top badges are visualized in Fig-ure 4:  X  X lympics X  (ranked #2),  X  X occer X  (ranked #5) and  X  X abour X  (#10). 6 The characteristic words for these badges are precisely what we would expect; for example, the top words corresponding to the  X  X abour X  badge are all related to British politics. We see such high quality associations be-tween badges and their representative words throughout our dictionary. In fact, when ranking the badges by prevalence,
A full listing and visualization of the top ten badges can be found in the supplemental material. Figure 5: Here, we see the relationship between two re-lated badges:  X  X rogressive X  and  X  X cot X  (Top Conservatives on Twitter). The word cloud on the left contains words that are more important for  X  X rogressive X  than for  X  X cot, X  with the size of the word proportional to the di ff erence in weights between the two dictionary elements. On the right, we see a word cloud containing the converse: words that are more important for  X  X cot X  than for  X  X rogressive. X  as above, we have to go down to the 27th position in the ranking before we find a badge with a poor representation: the  X  X iews X  badge, which we visualize in Figure 4d. Dueling Badges. An interesting exercise is to take a pair of antonymous badges, and see how their word representa-tions compare. In Figure 5, we see a comparison of two popular badges related to American politics:  X  X rogressive X  (a popular liberal badge) and  X  X cot X  ( T op C onservatives o n T witter). These dictionary elements were learned from the 2012 data, and thus come from the heat of the American Presidential race between Barack Obama and Mitt Rom-ney. As this race was heavy on negative campaigning (cf., for example, [14]), it is not surprising to see that progressive supporters of Barack Obama were more likely than conser-vatives to share articles about Mitt Romney, and in particu-lar, his controversial ties to Bain Capital, a financial firm he once headed. Likewise, conservatives are more likely than progressives to share articles about Barack Obama, presum-ably critical of him. We note that this analysis requires knowing how the users describe themselves, and is thus in-accessible to traditional topic models.
 Badges Over Time. One motivation for using badges to represent documents is their persistence over time. For ex-ample, even if what it means to be liberal changes from year to year, the  X  X iberal X  badge is always there to represent liberal-leaning documents. Thus, it is instructive to consider examples of both static and dynamic badges.

In Figure 6, we find the  X  X usic X  badge, which is one of the most static badges in our data set; its characteristic words barely change over the two year period from September 2010 to September 2012. Namely, the type of Twitter user who identifies herself with music in her profile is likely to share articles with the words X  X usic, X  X  X and, X  X  X lbum X  X nd X  X ong. X 
In contrast, Figure 7 shows one of the most dynamic badges in our data set: the one representing Vice President Joe Biden. The type of user who identifies himself with  X  X iden X  shares rather di ff erent articles in 2010 and 2012. In September 2010, such a user focuses on the Vice President as well as comedian Stephen Colbert, who at the time was co-hosting a political rally in Washington. However, by 2012, all signs of Joe Biden have diminished, and the primary fo-cus of this badge is on the American Presidential race.
To demonstrate how our badge representation can pro-vide insight on the makeup of a writer X  X  likely readers, we use our model to analyze fourteen notable political colum-Figure 6: The X  X usic X  X adge is one of the most static badges in our data set; its characteristic words barely change over the two year period from September 2010 to September 2012, as can be seen in this pair of word clouds. Figure 7: The  X  X iden X  badge is a dynamic one. In 2010, readers with the badge share articles about Joe Biden and Stephen Colbert, while in 2012, the focus turns to Barack Obama and Mitt Romney, due to the Presidential campaign. nists in the United States. These columnists each specialize in di ff erent topics, from economics to foreign policy, and are perceived to have di ff erent political leanings from very lib-eral to ultra-conservative. We show through various exam-ples that, by understanding the writings of these political columnists through badges, we can characterize their tar-get audiences in interesting ways. We emphasize that we only look at the content of the columnists X  articles; only the badge dictionary is learned from documents shared on Twitter, and thus this analysis does not require that the columnists X  articles appear on Twitter at all.

As a first analysis, we take each article written by each of the fourteen columnists in July 2012, and code the ar-ticle text in terms of badges, using our methodology. For each columnist, we then average the badge representations of the columnist X  X  articles, resulting in an aggregate badge representation for each columnist. Examples can be found in Figure 8. We find that the badge representation, in almost all cases, accurately reflects the topics of expertise of the columnists; for instance, the words  X  X id X  and  X  X frica X  ap-pear prominently in the badge representation for Nicholas Kristof, which makes sense because a reader who is self-described to be interested in X  X id X  X r X  X frica X  X ould be quite likely to read Kristof X  X  analyses of the various humanitarian crises in third world countries. Likewise, the badge represen-tation for Maureen Dowd accurately shows that her likely readers are  X  X rogressive. X  It is critical to point out that Dowd does not in fact use the word  X  X rogressive X  in any of her columns throughout this time period; rather, this coding corresponds to the attributes of her likely readers. Addition-ally, the badges X  X rish X  X nd X  X reland X  X ppear prominently be-cause Maureen Dowd was on assignment in Ireland in July 2012, writing prolifically about the country.

As a second analysis, we compare the political leanings of the likely readers of the fourteen columnists, by coding the columnists X  articles in terms of only the  X  X rogressive X  and  X  X cot X  badges. In Figure 9, we place the columnists on a spectrum, where the location of each columnist is based on the relative weight of the  X  X cot X  badge to the  X  X rogressive X  badge in his or her average badge representation. Thus, columnists appearing on the left side of the spectrum are Figure 8: Word clouds depicting the prominent badges that best represent the writings of two New York Times colum-nists from July 2012. The badge representations of the writ-ers match well with the subject matter of their columns. more likely to appeal to readers self-identified as  X  X rogres-sive X  than to readers self-identified as  X  X cot. X  For example, the location of ultra-conservative writer Ann Coulter on the far right of the spectrum indicates that, based on her writ-ings in July 2012, her likely readership during that month is almost exclusively conservative.

Overall, our ranking of the political columnists roughly lines up with the public perception of the columnists X  po-litical alignments, with outspoken conservative voices like Coulter and Charles Krauthammer placed on the far right, and well known liberal voices like Maureen Dowd on the (not as extreme) left. Likewise, Thomas Friedman, Fareed Zakaria and David Ignatius are clustered together, as they all write primarily about foreign policy. It is important to emphasize, however, that our approach does not attempt to directly classify the political alignment of the columnists. Rather, we instead try to identify what kind of readers would be interested in reading each of the columnist X  X  editorials. For instance, if we take the example of Kathleen Parker, al-though she is a conservative columnist with the Washington Post ,sheoftenlevelsstrongcriticismagainsttheRepublican Party, and even supported Barack Obama for the 2008 pres-idential elections. It is thus sensible that she is placed more to the left in our chart despite being conservative, because politically liberal readers often enjoy reading her columns.
However, our ranking is not perfect. If we consider the location of William Kristol, a neo-conservative icon, we find that he is incorrectly placed on the left side of this spectrum. We posit that this behavior arises from the phenomenon vi-sualized in Figure 5, whereby progressives are more likely to write about Mitt Romney than conservatives. While an unabashed conservative, in the month of July 2012, Kris-tol writes about Romney nearly twice as much as he writes about Obama, which may explain the discrepancy.

Finally, it is interesting to note the relationship between our case study and the well-studied ideal point model from quantitative political science, which assumes legislators and bills lie in a low-dimensional Euclidean space indicating po-litical positions, and the a ffi nity of a legislator for a bill is afunctionofthedistancebetweentheirtwolocations[19]. Early work learned such ideal points from roll call votes, whereas more recent work in machine learning has combined roll call data with text analysis [12]. While perhaps visually similar, Figure 9 is computed based on a completely di ff erent signal than traditional ideal point models.
In this section, we evaluate the utility of our methodol-ogy as a document representation for personalization. We compare our badge-based approach to two commonly used representations: tf-idf and an LDA-based representation. Figure 9: Predicted political alignments of the likely read-ers of fourteen political columnists based on results from representing the columnists X  writing with only two badges:  X  X rogressive X  and  X  X cot. X  The columnists are ranked from left to right based on how well the content of their writing is captured by the left-wing  X  X rogressive X  badge as opposed to the right-wing  X  X cot X  badge.
 Coherence. As a first comparison, we test our hypothesis that the topics we learn with our badge-based representation are more semantically coherent than topics learned through a topic model, such as LDA. Our belief stems from the intu-ition that there is no explicit incentive for a topic model to produce coherent topics corresponding to human interests, while each of our dictionary elements corresponds to a badge directly used by a Twitter user to describe himself.
To quantitatively measure such a notion of coherence, we use the methodology of Mimno et al. [18], and compute a statistic based on how frequently the top words in each topic co-occur in a reference corpus. Mimno and colleagues show that this statistic strongly correlates with human notions of coherence, as validated by a user study. Figure 10a shows that, using Mimno X  X  metric, our learned badge dictionary produces more semantically coherent concepts than LDA, when trained on the full September 2012 training data set.
For a fair comparison, we run LDA with the same num-ber of topics as we have badges for this time period (i.e., 5,247). 7 Moreover, we compute Mimno X  X  coherence statistic using the top 15 words in each topic, following the conven-tion used in their paper. Due to the scale of this problem (over 1.5 million documents, 5,000 topics, 55,000 word vo-cabulary and 375 million total tokens), we run a distributed implementation of collapsed Gibbs sampling for LDA, over 114 cores, provided as part of GraphLab [13].
 Odd-one-out. Our second quantitative comparison exam-ines our hypothesis that the badge-based document repre-sentation is better suited than competing techniques to rep-resent coherent semantic concepts over time .Thisbelief is due to the persistent nature of badges, which allows us to straightforwardly bind together concepts across di ff erent time periods. For example, the  X  X ootball X  badge in Septem-ber 2010 can be directly matched to the  X  X ootball X  badge in September 2012, whereas topics from an unsupervised topic model, separately trained over disjoint data sets, must be matched in less straightforward ways.

To evaluate our hypothesis, we conduct the following in-trusion detection experiment on our Guardian test data: 1. Pick two newspaper sections at random. Call the first
Similar results were obtained for 100-topic LDA. 2. Pick an article h 1 ,uniformlyatrandom,fromthehome 3. Pick an article h 2 ,uniformlyatrandom,fromthehome 4. Pick an article i 2 ,uniformlyatrandom,fromthein-5. We compute the ( $ 2 -normalized) document represen-6. For a given representation (e.g., for LDA), we compute
Adocumentrepresentationwithahigh X  X dd-one-out X  X core indicates that the semantic similarity between articles from the same section is preserved across time. A lower  X  X dd-one-out X  score indicates that a representation can more eas-ily conflate the content of di ff erent news sections, leading to thematic incoherence over time.

We compute this score for our badge-based representa-tion, as well as a 100-topic LDA topic representation and a tf-idf word representation. Specifically, for each pair of home and intruder sections, we draw 1,000 random article triplets, and compute the median odd-one-out score for each method. Figure 10b shows that, overall, aggregating over all pairs of sections, the badge representation significantly outperforms the two competing techniques on this metric. Moreover, in the supplemental material, we show that this significant ad-vantage holds true not just at an aggregate level, but in about 80% of the individual section pairings.
 User Study. Our final quantitative evaluation addresses the fundamental question: can we develop a document rep-resentation that works well for personalization?
To answer this question, we conduct a news recommenda-tion user study on Amazon Mechanical Turk, comparing our badge-based document representation to tf-idf and LDA. We use each of the three as concept representations in the Inter-active Concept Coverage framework of El-Arini [9], allowing us to recommend a diverse set of related articles based on user feedback. Our study is in two phases: first, a user pro-vides feedback on a random set of articles that allows us to quickly estimate his interests, and then we recommend articles to the user and measure how many of them he likes.
Specifically, our study involves the following: 1. Pick at random two time periods from the set: { Septem-2. From the first time period, draw 20 news articles, uni-3. Present these 20 news articles, one at a time, to the 4. Draw a random document representation from the set: 5. Align the average document representation computed 6. Use the transformed average document vector, indi-7. Show the recommended articles to the user, one at a cf. Chapter 3 of El-Arini X  X  thesis for more details [9].
Figure 10c shows that our badge-based representation sig-nificantly outperforms both tf-idf and LDA on this fun-damental news recommendation task. On average, users find the articles we recommend to them to be more in-teresting than the articles recommended via the compet-ing document representations. This is what we expected, and backs our hypothesis that the badge-based representa-tion is a preferable document representation for personaliza-tion tasks X  X articularly ones that cut across periods of time. While tf-idf is excellent at detecting article similarity within atimeperiod,itisworseatdetectingsimilararticlesfrom two completely di ff erent periods of time. Meanwhile, LDA is at the mercy of a successful bipartite matching. The badge-based representation can overcome both challenges, leading to improved performance. (More details on the study can be found in the supplemental material.)
In this work, we proposed a new document representation based on associating articles with attributes of their likely readers. Our approach of learning a labeled dictionary from alarge-scaleTwitterdataset,whichwethenusetocode new articles via a structured sparsity optimization, led to a document representation that was both human interpretable and useful for personalization. Experimentally, we demon-strated that our methodology leads to thematically coherent topics that are more consistent over time than popular alter-native approaches, leading to better performance on a live personalization task. Moreover, our representation allows us to provide interesting insights about writers and the state of political discourse, confirming some widely held beliefs.
However, some challenges remain:
Despite these challenges, we believe that representing doc-uments by their readers is an important, novel contribution.
We are grateful to Brendan O X  X onnor and Noah Smith for providing us with access to the Twitter Garden Hose. This work was partially supported by ONR grant PECASE N000141010672 and NSF grant NETS SCAN CNS0721591. [1] M. Bland. An Introduction to Medical Statistics . [2] D. M. Blei and J. La ff erty. Dynamic topic models. In [3] D. M. Blei and J. La ff erty. Topic models. In [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [5] J. K. Bradley, A. Kyrola, D. Bickson, and [6] E. Cand` es and T. Tao. The power of convex [7] X. Chen, Q. Lin, S. Kim, J. Carbonell, and E. P. [8] G. De Francisci Morales, A. Gionis, and C. Lucchese. [9] K. El-Arini. Beyond Keyword Search: Representations [10] K. El-Arini, U. Paquet, R. Herbrich, J. V. Gael, and [11] K. El-Arini, G. Veda, D. Shahaf, and C. Guestrin. [12] S. Gerrish and D. M. Blei. Predicting legislative roll [13] J. Gonzalez, Y. Low, H. Gu, D. Bickson, and [14] N. Greenstein. Negative ads: A shift in tone for the [15] S. Kim, K.-A. Sohn, and E. P. Xing. A multivariate [16] L. Li, W. Chu, J. Langford, and R. E. Schapire. A [17] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [18] D. Mimno, H. Wallach, E. Talley, M. Leenders, and [19] K. T. Poole and H. Rosenthal. A spatial model for [20] D. Ramage, D. Hall, R. Nallapati, and C. D. Manning. [21] K. Sugiyama, K. Hatano, and M. Yoshikawa. Adaptive [22] G. Tsoumakas, I. Katakis, and I. Vlahavas. Mining [23] S. Williamson, C. Wang, K. A. Heller, and D. M. Blei. [24] Y. Yue and C. Guestrin. Linear submodular bandits
