 Boosting procedures attempt to improve the accuracy of general machine learning algorithms, through repeated executions on reweighted data. Aggressive reweighting of data may lead to poor performance in the presence of certain types of noise [1]. This has been addressed by a number of  X  X obust X  boosting algorithms, such as SmoothBoost [2, 3] and MadaBoost [4] as well as boosting by branching programs [5, 6]. Some of these algorithms are potential-based boosters, i.e., natu-ral variants on AdaBoost [7], while others are perhaps less practical but have stronger theoretical guarantees in the presence of noise.
 The present work gives a simple potential-based boosting algorithm with guarantees in the (arbi-trary noise) agnostic learning setting [8, 9]. A unique feature of our algorithm, illustrated in Figure enables us to prove a strong boosting theorem in which the weak learner need only succeed for one distribution on unlabeled examples. To the best of our knowledge, earlier weak-to-strong boosting theorems have always relied on the ability of the weak learner to succeed under arbitrary distribu-tions. The utility of our boosting theorem is demonstrated by re-deriving two non-trivial results in computational learning theory, namely agnostically learning decision trees [10] and agnostically learning halfspaces [11], which were previously solved using very different techniques. The main contributions of this paper are, first, giving the first provably noise-tolerant analysis of a potential-based boosting algorithm, and, second, giving a distribution-specific boosting theorem that work by Long and Servedio, showing that convex potential boosters cannot work in the presence of random classification noise [12]. The present algorithm circumvents that impossibility result in two ways. First, the algorithm has the possibility of negating the current hypothesis and hence is not technically a standard potential-based boosting algorithm. Second, weak agnostic learning is more which is a weak-learner in the random classification noise setting need not be a weak-learner in the agnostic setting.
 Related work. There is a substantial literature on robust boosting algorithms, including algorithms already mentioned, MadaBoost, SmoothBoost, as well as LogitBoost [13], BrownBoost [14], Nad-Simplified Boosting by Relabeling Procedure Inputs: ( x 1 ,y 1 ) ,..., ( x m ,y m )  X  X  X { X  1 , 1 } , T  X  1 , and weak learner W .
Output: classifier h : X  X  X  X  1 , 1 } . Figure 1: Simplified Boosting by Relabeling Procedure. Each epoch, the algorithm runs the weak bination of weak hypotheses. For our agnostic analysis, we also need to include the negated current adding noise, each example would be replaced with three weighted examples: ( x i ,y i ) with weight w , and ( x i ,  X  1) each with weight (1  X  w t i ) / 2 . aBoost [15] and others [16, 17], including extensive experimentation [18, 15, 19]. These are all sim-ple boosting algorithms whose output is a weighted majority of classifiers. Many have been shown to have formal boosting properties (weak to strong PAC-learning) in a noiseless setting, or partial boosting properties in noisy settings. There has also been a line of work on boosting algorithms that provably boost from weak to strong learners either under agnostic or random classification noise, using branching programs [17, 20, 5, 21, 6]. Our results are stronger than those in the recent work of Kalai, Mansour, Verbin [6], for two main reasons. First, we propose a simple potential-based algorithm that can be implemented efficiently. Second, since we don X  X  change the distribution over unlabeled examples, we can boost distribution-specific weak learners. In recent work, using a simi-lar idea of relabeling, Kalai, Kanade and Mansour[22] proved that the class of DNFs is learnable in a one-sided error agnostic learning model. Their algorithm is essentially a simpler form of boosting. Experiments. Our boosting procedure is quite similar to MadaBoost. The main differences are: (1) there is the possibility of using the negation of the current hypothesis at each step, (2) examples are relabeled rather than reweighted, and (3) the step size is slightly different. The goal of experiments was to understand how significant these differences may be in practice. Preliminary experimental results, presented in Section 5, suggest that all of these modifications are less important in practice than theory. Hence, the present simple analysis can be viewed as a theoretical justification for the noise-tolerance of MadaBoost and SmoothBoost. 1.1 Preliminaries In the agnostic setting, we consider learning with respect to a distribution over X  X  Y . For simplicity, we will take X be to finite or countable and Y = { X  1 , 1 } . Formally, learning is with respect to some class of functions, C , where each c  X  X  is a binary classifier c : X  X  X  X  1 , 1 } . There is an arbitrary distribution  X  over X and an arbitrary target function f : X  X  [  X  1 , 1] . Together these determine respectively defined as, We will omit D when understood from context. The goal of the learning algorithm is to achieve error (equivalently correlation) arbitrarily close to that of the best classifier in C , namely, A  X  -weakly accurate classifier [23] for PAC (noiseless) learning is simply one whose correlation is the agnostic setting. Namely, for some  X   X  (0 , 1) , h : X  X  { X  1 , 1 } is said to be  X  -optimal for C (and D ) if, Hence, if the labels are totally random then a weak hypothesis need not have any correlation over random guessing. On the other hand, in a noiseless setting, where cor( C ) = 1 , this is equivalent to a  X  -weakly accurate hypothesis. The goal is to boost from an algorithm capable of outputting  X  -optimal hypotheses to one which outputs a nearly 1-optimal hypothesis, even for small  X  . Let D be a distribution over X  X { X  1 , 1 } . Let w : X  X { X  1 , 1 }  X  [0 , 1] be a weighting function. We now define the distribution D relabeled by w , R D ,w . Procedurally, one can think of generating a sample from R D ,w by drawing an example ( x,y ) from D , then with probability w ( x,y ) , outputting { X  1 , 1 } . Formally, Note that D and R D ,w have the same marginal distributions over unlabeled examples x  X  X . Also, observe that, for any D , w, and h : X  X  R , This can be seen by the procedural interpretation above. When ( x,y ) is returned directly, which y  X  X  X  1 , 1 } .
 framework. A general ( m,q ) -learning algorithm is given m unlabeled examples  X  x 1 ,...,x m  X  , and may make q label queries to a query oracle L : X  X  { X  1 , 1 } , and it outputs a classifier h : X  X  { X  1 , 1 } . The queries may be active , meaning that queries may only be made to training examples x , or membership queries meaning that arbitrary examples x  X  X may be queried. The active query setting where q = m is the standard supervised learning setting where all m labels may be queried. One can similarly model semi-supervised learning.
 Since our boosting procedure does not change the distribution over unlabeled examples, it offers two advantages: (1) Agnostic weak learning may be defined with respect to a single distribution  X  over unlabeled examples, and (2) The weak learning algorithms may be active (or use membership queries). In particular, the agnostic weak learning hypothesis for C and  X  is that for any f : X  X  advantages of this new definition are: (a) it is not with respect to every distribution on unlabeled realistic as it does not assume noiseless data. Finding such a weak learner may be quite challenging since it has to succeed in the agnostic model (where no assumption is made on f ), however it may be a bit easier in the sense that the learning algorithm need only handle one particular  X  . Definition 1. A learning algorithm is a (  X , 0 , X  ) agnostic weak learner for C and  X  over X if, for any f : X  X  [  X  1 , 1] , with probability  X  1  X   X  over its random input, the algorithm outputs h : X  X  [  X  1 , 1] such that, if D =  X   X ,f  X  , has correlation 0 and the other has minuscule positive correlation. Then, one cannot even identify which one has better correlation to within O ( m  X  1 / 2 ) using m examples. Note that  X  can easily made exponentially small (boosting confidence) using standard techniques.
 Lastly, we define sign( z ) to be 1 if z  X  0 and  X  1 if z &lt; 0 . The formal boosting procedure we analyze is given in Figure 2.
 Inputs:  X  x 1 ,...,x Tm + s  X  , T,s  X  1 , label oracle L : X  X  X  X  1 , 1 } , ( m,q ) -learner W .
Output: classifier h : X  X  X  X  1 , 1 } . 1. Let H 0 = 0 2. Query the labels of the first s examples to get y 1 = L ( x 1 ) ,...,y s = L ( x s ) . 3. For t = 1 ,...,T : 4. Output h = sign( H  X  ) where  X  is chosen so as to minimize empirical error on Algorithm A GNOSTIC B OOSTER (Figure 2) with probability at least 1  X  4  X T outputs a hypothesis h satisfying: the query oracle L is s plus T times the number of calls made by the weak learner (if the weak learner is active, then so is the boosting algorithm). We show that two recent non-trivial results, viz. agnostically learning decision trees and agnostically learning halfspaces follow as corollaries to Theorem 1. The two results are stated below: Theorem 2 ([10]) . Let C be the class of binary decision trees on { X  1 , 1 } n with at most t leaves, membership queries and, with probability  X  1  X   X  , outputs h : { X  1 , 1 } n  X  { X  1 , 1 } such that for U f =  X  X  ,f  X  , err( h, U f )  X  err( C , U f ) + . Theorem 3 ([11]) . For any fixed &gt; 0 , there exists a univariate polynomial p such that the following holds: Let n  X  1 , C be the class of halfspaces in n dimensions, let U be the uniform distribution on { X  1 , 1 } n , and f : { X  1 , 1 } n  X  [  X  1 , 1] be an arbitrary function. There exists a polynomial-time algorithm that, when given m = p ( n log(1 / X  )) labeled examples from U f =  X  X  ,f  X  , outputs a queries.) Note that a related theorem was shown for halfspaces over log-concave distributions over X = R n . The boosting approach here similarly generalizes to that case in a straightforward manner. This illustrates how, from the point of view of designing provably efficient agnostic learning algorithms, the current boosting procedure may be useful. This section is devoted to the analysis of algorithm A GNOSTIC B OOSTER (see Fig 2). As is standard, the boosting algorithm can be viewed as minimizing a convex potential function. However, the proof is significantly different than the analysis of AdaBoost [7], where they simply use the fact that the potential is an upper-bound on the error rate.
 Our analysis has two parts. First, we define a conservative relabeling, such as the one we use, to be one which never relabels/downweights examples that the booster currently misclassifies. We show that for a conservative reweighting, either the weak learner will make progress, returning a hypothesis correlated with the relabeled distribution or  X  sign( H t  X  1 ) will be correlated with the relabeled distribution.
 Second, if we find a hypothesis correlated with the relabeled distribution, then the potential on round t will be noticeably lower than that of round t  X  1 . This is essentially a simple gradient descent analysis, using a bound on the second derivative of the potential. Since the potential is between 0 and 1 , it can only drop so many rounds. This implies that sign( H t ) must be a near-optimal classifier for some t (though the only sure way we have of knowing which one to pick is by testing accuracy on held-out data).
 The potential function we consider, as in MadaBoost, is defined by  X  : R  X  R , Define the potential of a (real-valued) hypothesis H with respect to a distribution D over X  X { X  1 , 1 } as: Note that  X ( H 0 , D ) =  X ( 0 , D ) = 1 . We will show that the potential decreases every round of the algorithm. Notice that the weights in the boosting algorithm correspond to the derivative of the essentially a gradient descent step.
 We next state a key fact about agnostic learning in Lemma 1.
 Definition 2. Let h : X  X  X  X  1 , 1 } be a hypothesis. Then weighting function w : X  X { X  1 , 1 } X  [0 , 1] is called conservative for h if w ( x,  X  h ( x )) = 1 for all x  X  X .
 conservative weighting function is good in the sense that, if h is far from optimal according to the original distribution, then after relabeling by w it is even further from optimal.
 Lemma 1. For any distribution D over X  X { X  1 , 1 } , classifiers c,h : X  X  { X  1 , 1 } , and any weighting function w : X  X { X  1 , 1 } X  [0 , 1] conservative for h , Proof. By the definition of correlation and eq. (1), cor( c,R D ,w ) = E D [ c ( x ) yw ( x,y )] . Hence, cor( c,R D ,w )  X  cor( h,R D ,w ) = cor( c, D )  X  cor( h, D )  X  E Finally, consider two cases. In the first case, when 1  X  w ( x,y ) &gt; 0 , we have h ( x ) y = 1 while Thus the above equation implies the lemma.
 We will use Lemma 1 to show that the weak learner will return a useful hypothesis. The case in which the weak learner may not return a useful hypothesis is when cor( C ,R D ,w ) = 0 , when the optimal classifier on the reweighted distribution has no correlation. This can happen, but in this case it means that either our current hypothesis is close to optimal, or h = sign( H t  X  1 ) is even worse than random guessing, and hence we can use its negation as a weak agnostic learner.
 We next explain how a  X  -optimal classifier on the reweighted distribution decreases the potential. We will use the following property linear approximation of  X  .
 Lemma 2. For any x, X   X  R , |  X  ( x +  X  )  X   X  ( x )  X   X  0 ( x )  X  | X   X  2 / 2 .
 Proof. This follows from Taylor X  X  theorem and the fact the function  X  is differentiable everywhere, and that the left and right second derivatives exist everywhere and are bounded by 1 . Let h t : X  X  { X  1 , 1 } be the weak hypothesis that the algorithm finds on round t . This may either be the hypothesis returned by the weak learner W or  X  sign( H t  X  1 ) . The following lemma lower bounds the decrease in potential caused by adding  X  t h t to H t  X  1 . We will apply the following Lemma on each round of the algorithm to show that the potential decreases on each round, as long as the weak hypothesis h t has non-negligible correlation and  X  t is suitably chosen.
 Lemma 3. Consider any function H : X  X  R , hypothesis h : X  X  [  X  1 , 1] ,  X   X  R , and distribution D over X  X  { X  1 , 1 } . Let D 0 = R D ,w be the distribution D relabeled by w ( x,y ) =  X   X  0 ( yH ( x )) . Then, Proof. For any ( x,y )  X  X  X { X  1 , 1 } , using Lemma 2 we know that: In the step above we use the fact that h ( x ) 2 y 2  X  1 . Taking expectation over ( x,y ) from D , In the above we have used Eq. (1). We are done, by definition of cor( h, D 0 ) .
 Using all the above lemmas, we will show that the algorithm A GNOSTIC B OOSTER returns a hy-pothesis with correlation (or error) close to that of the best classifier from C . We are now ready to prove the main theorem.
 Proof of Theorem 1. Suppose  X  c  X  C such that cor( c, D ) &gt; cor(sign( H t  X  1 ) , D ) + 0  X  + , then applying Lemma 1 to H t  X  1 and setting w t ( x,y ) =  X   X  0 ( H t  X  1 ( x ) y ) , we get that In this case we want to show that the algorithm successfully finds h t with cor( h t ,R D ,w t )  X   X  3 . Let g t be the hypothesis returned by the weak learner W . From Step 3c) in the algorithm: When s = 200  X  2 2 log 1  X  , by Chernoff-Hoeffding bounds we know that  X  t and  X  t are within an with probability 3  X  at this stage, possibly caused by the weak-learner and the estimation of  X  t , X  t . using (3). Thus, even after taking into account the fact that the empirical estimates may be off from Using this and Lemma 3, we get that by setting H t = H t  X  1 +  X  t h t the potential decreases by at When t = 0 and H 0 = 0 ,  X ( H 0 , D ) = 1 . Since for any H : X  X  R ,  X ( H, D ) &gt; 0 ; we can have at most T = 29  X  2 2 rounds. This guarantees that when the algorithm is run for T rounds, on some round t the hypothesis sign( H t ) will have correlation at least sup s = 200  X  2 2 log 1  X  the empirical estimate of the correlation of the constructed hypothesis on each round is within an additive 6 of its true correlation, allowing a further failure probability of  X  each round. Thus the final hypothesis H  X  which has the highest empirical correlation satisfies, Since there is a failure probability of at most 4  X  on each round, the algorithm succeeds with proba-bility at least 1  X  4 T X  . We show that recent agnostic learning analyses can be dramatically simplified using our boosting algorithm. Both of the agnostic algorithms are distribution-specific, meaning that they only work on one (or a family) of distributions  X  over unlabeled examples. 4.1 Agnostically Learning Decision Trees Recent work has shown how to agnostically learn polynomial-sized decision trees using member-ship queries, by an L 1 gradient-projection algorithm [10]. Here, we show that learning decision trees is quite simple using our distribution-specific boosting theorem and the Kushilevitz-Mansour membership query parity learning algorithm as a weak learner [24].
 Lemma 4. Running the KM algorithm, using q = poly( n,t, 1 / 0 ) queries, and outputting the parity with largest magnitude of estimated Fourier coefficient, is a (  X  = 1 /t, 0 ) agnostic weak learner for size-t decision trees over the uniform distribution.
 The proof of this Lemma is simple using results in [24] and is given in Appendix A. Theorem 2 now follows easily from Lemma 4 and Theorem 1. 4.2 Agnostically Learning Halfspaces In the case of learning halfspaces, the weak learner simply finds the degree-d term,  X  S ( x ) with | S | X  d , with greatest empirical correlation 1 The following lemma is useful in analyzing it.
 Lemma 5. For any &gt; 0 , there exists d  X  1 such that the following holds. Let n  X  1 , C be the class of halfspaces in n dimensions, let U be the uniform distribution on { X  1 , 1 } n , and f : { X  1 , 1 } n  X  [  X  1 , 1] be an arbitrary function. Then there exists a set S  X  [ n ] of size | S |  X  d = 20 4 | cor(  X  S , U f ) | X  (cor( C , U f )  X  0 ) /n d . Using results from [25] the proofs of Lemma 5 and Theorem 3 are straightforward and are given in Appendix B. We performed preliminary experiments with the new boosting algorithm presented here on 8 datasets from UCI repository [26]. We converted multi-class problems into binary classification problems by arbitrarily grouping classes, and ran Adaboost, Madaboost and Agnostic Boost on these datasets, using stumps as weak learners. Since stumps can accept weighted examples, we passed the exact weighted distribution to the weak learner.
 Our experiments were performed with fractional relabeling , which means the following. Rather than keeping the label with probability w t ( x,y ) and making it completely random with the remaining respectively. Experiments with random relabeling showed that random relabeling performs much worse than fractional relabeling.
 Table 1 summarizes the final test error on the datasets. In the case of pima and german datasets, we observed overfitting and the reported test errors are the minimum test error observed for all the algorithms. In all other cases the test error rate at the end of round 500 is reported. Only pendigits had a test dataset, for the rest of the datasets we performed 10-fold cross validation. We also added random classification noise of 5%, 10% and 20% to the datasets and ran the boosting algorithms on the modified dataset.
 Table 1: Final test error rates of Adaboost, Madaboost and Agnostic Boosting on 8 datasets. The first column reports error rates on the original datasets, and the next three report errors on datasets with 5%, 10% and 20% classification noise added. done without changing the distribution over unlabeled examples. We show that non-trivial agnostic learning results, for learning decision trees and halfspaces, can be viewed as simple applications of our boosting theorem combined with well-known weak learners. Our analysis can be viewed as a theoretical justification of noise tolerance properties of algorithms like Madaboost and Smoothboost. Preliminary experiments show that the performance of our boosting algorithm is comparable to that of Madaboost and Adaboost. A more thorough empirical evaluation of our boosting procedure using different weak learners is part of future research.

 We present a simple algorithm that boosts without changing distributions over unlabeled examples in the agnostic learning setting. We exploit the fact that since we are in the agnostic setting, weak learners have to be tolerant to noise, and by adding controlled noise to the data, we repeatedly get reasonably accurate hypotheses which we can combine to get a highly accurate
