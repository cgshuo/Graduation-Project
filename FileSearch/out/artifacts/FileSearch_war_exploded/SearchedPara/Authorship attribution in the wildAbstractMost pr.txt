 Abstract Most previous work on authorship attribution has focused on the case in which we need to attribute an anonymous document to one of a small set of candidate authors. In this paper, we consider authorship attribution as found in the wild: the set of known candidates is extremely large (possibly many thousands) and might not even include the actual author. Moreover, the known texts and the anonymous texts might be of limited length. We show that even in these difficult cases, we can use similarity-based methods along with multiple randomized feature sets to achieve high precision. Moreover, we show the precise relationship between attribution precision and four parameters: the size of the candidate set, the quantity of known-text by the candidates, the length of the anonymous text and a certain robustness score associated with a attribution.
 Keywords Authorship attribution Open candidate set Randomized feature set 1 Introduction Authorship attribution has been much studied in recent years and several recent articles (Juola 2008 ; Koppel et al. 2008 ; Stamatatos 2009 ) survey the plethora of methods that have been applied to the problem. A significant fact that examination simplest version of the problem, in which we are given a long anonymous text that must be attributed to one of a small, closed set of candidate authors for each of whom we have (more or less extensive) writing samples.

Unfortunately, this  X  X  X anilla X  X  version of the authorship attribution problem does not often arise in the real world. The situations typically encountered when performing authorship attribution in the wild are significantly more difficult than the vanilla version in one or more of three key ways: 1. There may be thousands of known candidate authors. 2. The author of the anonymous text might be none of the known candidates. 3. The  X  X  X nown-text X  X  for each candidate and/or the anonymous text might be very
These difficulties have very rarely been addressed by the research community (several important exceptions will be discussed below). In this paper, we will present a novel attribution method that attacks all three of these difficulties at once. We will show that even under these conditions, we can achieve very high attribution precision, while paying a tolerable price in recall. Moreover, we will measure the effect of three key factors X  X umber of candidates, size of known-text by candidates, and size of the anonymous text X  X n the reliability of attributions output by our method. 2 Previous work Broadly speaking, methods for automated authorship attribution can be divided into two main paradigms (Stamatatos 2009 ). In the similarity-based paradigm, some metric is used to measure the distance between two documents and an anonymous document is attributed to that author to whose known writing (considered collectively as a single document) it is most similar (Burrows 2002 ; Hoover 2003 ; Argamon 2008 ; Abbasi and Chen 2008 ). In the machine-learning paradigm, the known writings of each candidate author (considered as a set of distinct training documents) are used to construct a classifier that can then be used to classify anonymous documents (cf. Abbasi and Chen 2008 ; Zhao and Zobel 2005 ; Zheng et al. 2006 ; Koppel et al. 2008 ).

Research in the similarity-based paradigm has focused on the choice of features for document representation, on methods for dimensionality reduction (such as PCA) of the feature space, and on the choice of distance metric. Research in the machine-learning paradigm has focused on choice of features for document representation and on choice of learning algorithms.

Virtually all of this work has focused on problems with a small number of candidate authors. Recently, somewhat larger candidate sets have been considered by Madigan et al. ( 2005 ) (114 authors) and Luyckx and Daelemans ( 2008 ) (145 authors). Only Koppel et al. ( 2006 ) have considered candidate sets including thousands of authors. Both Koppel et al. ( 2006 ) and Luyckx and Daelemans ( 2008 ) observed that when there are very many candidate authors, similarity-based methods are more appropriate than machine-learning methods.
Similarly, almost all work in authorship attribution has focused on the case in which the candidate set is a closed set X  X he anonymous text is assumed to have been written by one of the known candidates. The more general case, in which the true author of an anonymous text might not be one of the known candidates, reduces to the binary authorship verification problem: determine if the given document was written by a specific author or not. The authorship verification problem has usually been considered in the context of plagiarism analysis (Clough 2000 ; Meyer zu Eissen et al. 2007 ). One general and effective method for authorship verification is unmasking, proposed by Koppel et al. ( 2007 ). The idea is that two texts are probably by different authors if the differences between them are robust to changes in the underlying feature set used to represent the documents. Koppel et al. ( 2007 ) used a machine-learning paradigm and measured differences using cross-validation accuracy. More generally, however, differences between documents can be more readily measured in the similarity-based paradigm. A document can be verified as having been written by a given author if the degree of similarity between the document and the author X  X  known writing exceeds some threshold (van Halteren et al. 2005 ).

In this paper, we will consider the general authorship attribution problem, where candidate sets are simultaneously large and open. We will integrate the methods previously used to address the large candidate set problem and the open candidate set problem, as described above. The integrated method we propose is in fact simpler than both previous approaches and has the added advantage of being language-independent. 3 The corpus We use a set of 10,000 blogs harvested in August 2004 from blogger.com. The corpus is balanced for gender within each of a number of age intervals. In addition, each individual blog is predominantly in English and contains sufficient text, as will be explained. For each blog, we choose 2000 words of known text and a snippet , consisting of the last 500 words of the blog, such that the posts from which the known text and the snippet are taken are disjoint. Our object will be to determine which X  if any  X  X f the authors of the known texts is the author of a given snippet. Note that we will not necessarily use all the available data in each experiment. We will experiment using various subsets of the available text to determine the impact on attribution of the number of candidates, the quantity of known text for each candidate and the length of the anonymous snippet. 4 Na X   X  ve method We begin by representing each text (both known texts and snippets) as a vector representing the respective frequencies of each space-free character 4-gram . For our purposes, a space-free character 4-gram is (a) a string of characters of length four that includes no spaces or (b) a string of four or fewer characters surrounded by spaces. (Note that the latter case corresponds roughly to words of length four or less, but not exactly; any string of characters, including punctuation, numerals and sundry, is included.) In our corpus, there are just over 250,000 unique (but overlapping) space-free character 4-grams. (There would be considerably more such n-grams if we included those with spaces, but these are certainly adequate for our purposes.)
Character n-grams have been shown to be effective for authorship attribution (Keselj et al. 2003 ) and have the advantage of being measurable in any language without specialized background knowledge. We note that character n-gram statistics capture both aspects of document content and writing style. Although this distinction is often an important one in authorship studies, we do not dwell on it in this paper. For our purposes, we do not particularly care if attributions are based on style or content or both. We are content to show that our method works even using the most primitive and language-independent feature types imaginable.
Now, it is impractical to learn a single classifier for 10,000 classes; nor is it practical to learn 10,000 one-versus-all binary classifiers. Instead, we use a similarity-based method. Specifically, we use a common straightforward informa-tion retrieval method to assign an author to a given snippet. Using cosine similarity as a proximity measure, we simply return the author whose known writing (considered as a single vector of space-free character 4-gram frequencies) is most similar to the snippet vector (Salton and Buckley 1988 ). Testing this rather na X   X  ve method on 1,000 snippets selected at random from among the 10,000 authors, we find that 46% of the snippets are correctly assigned.

While this is perhaps surprisingly high, a precision of 46% is inadequate for most applications. To remedy this problem, we adopt the approach of (Koppel et al. 2006 ) which permits a response of Don X  X  Know in cases where attribution is uncertain. The objective is to obtain high precision for those cases where an answer is given, while trying to offer an answer as often as possible. Our specific method for doing so will differ from that of (Koppel et al. 2006 ) in several ways. Unlike that work, our simpler method includes a natural parameter for recall-precision tradeoff, does not require a training corpus for learning meta-models, and does not use language-dependent lexical features. 5 Improved method The key to our new approach is an insight initially confirmed by Koppel et al. similar to the snippet even as we vary the feature set that we use to represent the texts. Another author X  X  text might happen to be the most similar for one or a few specific feature sets, but it is highly unlikely to be consistently so over many different feature sets.
 This observation suggests using the following algorithm:
Given : snippet of length L1; known-texts of length L2 for each of C candidates Output : arg max A Score(A) if max Score(A) &gt; r *; else Don X  X  Know
The idea is to check if a given author proves to be most similar to the test snippet for many different randomly selected feature sets of fixed size. The number of different feature sets used ( k1 ) and the fraction of all possible features in each such set ( k2 ) are parameters that need to be selected. The threshold r * that serves as the minimal score an author needs to be deemed the actual author is the parameter that we vary for recall-precision tradeoff.

We note that our method is similar in many respects to classifier ensemble methods in which different classifiers are learned using different subsets of features (Bryll et al. 2003 ). 6 Results Preliminary experiments show that the greater the number of iterations, k1 , the better the results, but that the added value of additional iterations begins to vanish as k1 approaches 100. Thus, for all our experiments, we set k1 = 100. Also, except where otherwise stated, we assume that the actual author is in the candidate set. This actually entails no loss of generality, as we will see. 6.1 Feature set size: k2 For our first experiment, we set the snippet length (L1) to 500, the known-text length for each candidate (L2) to 2,000 and the number of candidates to 10,000 and feature set, k2 . Testing on 1,000 snippets, we construct recall-precision curves for various values of k2 (Fig. 2 ). We find that larger feature sets yield greater accuracy. Using 40% (=100,000) of the 250,000 available features per iteration, at r * = .90, we achieve 87.9% precision with 28.2% recall (Fig. 1 ). 6.2 Number of candidate authors: C We now consider how the number of candidate authors affects precision and recall. In Fig. 2 , we show recall-precision curves ( k2 = 40%) for various numbers of candidate authors, using the same L1 and L2 settings as above. Note that, as expected, accuracy increases as the number of candidate authors diminishes. We mark on each curve the point r * = .90. For example, for 1,000 candidates, at r * = .90, we achieve 93.2% precision at 39.3% recall. 6.3 Open candidate sets We have assumed thus far that the author of the snippet is among the candidate authors. We now consider the possibility that none of the candidate authors is the actual author of the snippet. That is, we now wish to consider the open set attribution problem. What we would hope to find is that in such cases the method does not attribute the snippet to any of the candidates.

In fact, testing on 1000 snippets that belong to none of the candidates, we find that at r * = .90, very few are mistakenly attributed to one of the candidate authors: 2.5% for 10,000 candidates, 3.5% for 5000 and 5.5% for 1000. Perhaps counter-intuitively, for snippets by authors not among the candidates, having fewer candidates actually makes the problem more difficult since the fewer competing candidates there are, the more likely it is that there is some consistently most similar (but inevitably wrong) candidate. 6.4 Snippet length: L1 Next, we consider the effect of snippet size. In Fig. 4 , we show recall-precision curves ( k2 = 40%) for 10,000 candidate authors as snippet size is reduced. We see that, although results degrade rapidly with decreasing snippet size, even for as few as 100 words, we get precision of 71% at 10% recall (Fig. 3 ). 6.5 Known-text length: L2 Finally, we consider the effect of known-text size. For simplicity, we use the same amount of known-text for each candidate author. In Fig. 4 , we show recall-precision curves ( k2 = 40%) for 10,000 candidate authors and snippet size of 500 as known-text size is reduced. We see that increased size of known-texts improves results, but that known-text size of 2,000 offers only a marginal improvement over known-text size of 1,500.
 7 Assessing attribution confidence The above results show that the expected precision of an attribution of a snippet to one of a set of candidate authors depends on at least four factors: the number of candidate authors (C), the size of the snippet (L1), the size of the known-text (L2) and the score r . We wish now to assess the importance of each of these four factors. We also wish to use this information to augment the output of our algorithm: in addition to presenting the most likely author of the snippet, we wish to estimate the probability that that author is in fact the correct one. In many applications, especially forensic ones, such confidence measures are crucial for usefulness of the results (e.g., to ensure admissibility in court). (Note that in our experiments below, we assume that the known-texts for all candidates are of uniform length. This is a convenience. When working with known-texts of varying lengths, our estimate of the probability that our attribution is correct will lie somewhere between the confidence value obtained using the shortest known-text and that obtained using the longest known-text.)
We consider a wide variety of combinations of the values C (ranging from 100 to 10,000), L1 (ranging from 20 to 500) and L2 (ranging from 500 to 2,000). For each of 1,000 snippets, we record the score r achieved for each combination of such values. (We range over such values systematically by beginning with maximal candidate sets and known-text and snippets and iteratively (and independently) truncating known-texts and snippets and eliminating candidates.) For each combination of parameter values and each score r , we compute the coverage H (the percentage of snippets for which the score r is obtained) and the precision P (the percentage of those cases for which the author achieving that score is in fact the actual author). 1 In Table 1 , we show these results for selected combinations of such values, for the cases in which the snippet author is among the candidates. We do the same for the case in which the snippet author is not among the candidates (table not shown).

Our goal now is to predict coverage and precision based on the problem parameters (L1, L2, C and r ), using regression. Since coverage for any given score is usually quite small, and thus there is considerable data sparseness, we smooth estimates by substituting for H and P for given r , H and P for the interval [ r -5, r ? 5]. (At the extremes, we use the largest possible interval symmetric around r .)
Formally, our independent parameters are log(L1), log(L2), log(C) and r , each scaled to lie in the interval [0,1]. We applied both ordinary linear regression and logistic regression to predict the dependent variables precision and coverage. Results for predicting precision using logistic regression are given in Table 2 a. We see that prediction is fairly good (r 2 = 0.77) and correlation of P with each of the four independent parameters is significant at p &lt; .0001. Results for predicting coverage using logistic regression are given in Table 2 b. In this case r 2 = 0.75 and correlation of H with each of the three parameters other than log(C) are significant at p &lt; .0001 (though there is no significant correlation with log(C)). Scatterplots showing precision and coverage vs. score are shown in Fig. 5 .

In the case of H , we also observe that running separate regressions for the case 80 B r &lt; 90 and r C 90 yields considerably improved results, with respective r 2 values of 0.77 and 0.85. No such improvement holds for P . Results on all the above using linear regression are essentially the same for P , but not quite as good for H .
We now run identical experiments for the case in which snippets do not belong to any candidate authors and use the same methods to estimate for each combination of parameter values &lt;C, L1, L2, r &gt; the value of E , namely, the probability that some candidate author will achieve a score of r if none of the candidates are the true author.

Finally, we can combine the above results, to assign a probability to some parameter. Denote by p the prior probability that the actual author of a snippet is in the candidate set. Note that in almost all authorship attribution research, p is simply assumed to be 1; that is, it is taken as given that the correct author is in the candidate set. Since we do not make that assumption here, we think of p as a parameter the value of which must be provided by the user. (Of course, in the absence of any information, some reasonable default value for p , perhaps  X  , can be chosen.)
Now consider a given snippet attribution problem with values of C, L1 and L2 and user-provided p . Then if the best candidate receives the score r , the probability that this author is the actual author of the snippet can be estimated as p H P p H  X  X  1 p  X  E : 8 Conclusions We have found that a na X   X  ve similarity-based method can be used to solve even the most difficult authorship attribution problems, provided that results are filtered through a robustness test based on randomized variation of feature sets. Thus, for example, the method can attribute a 500-word snippet to one of 1,000 authors with coverage of 42.2% and precision of 93.2%. Snippets that are not written by any of the candidates are rarely falsely attributed, though interestingly, the fewer candidates the greater the probability of such a false attribution. We note that passable results can be achieved even for snippets as short as 100 words.

Furthermore, we have found that the four parameters, snippet size, known-text size, number of candidates and score, account for most of the variability in coverage and precision, so that for any given attribution we can assign a fairly accurate estimate of the likelihood that the attribution is correct.
 We conclude by briefly surveying the state of authorship attribution in the wild. The case of small closed candidate sets is well handled by standard text categorization methods. The case of large (open or closed) candidate sets is reasonably well handled by the method offered in this paper. The case of small open candidate sets is handled by unmasking (Koppel et al. 2007 ), provided that the anonymous text is very large. The remaining case with no satisfactory solution is that of a small open candidate set and limited anonymous text. The method pursued here cannot be directly applied in such cases since we have found that for small candidate sets, there is the danger that an anonymous text not written by any of the candidates might be attributed to one of them. One promising direction that we leave for future work is to artificially expand the candidate set in some plausible manner and then to apply our method.
 References
