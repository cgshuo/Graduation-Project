 In recent years, the recognition of object categories has be come a major focus of computer vision and has shown substantial progress, partly thanks to the adopti on of techniques from machine learning and the development of better probabilistic representatio ns [1, 3]. The goal has been to recognize object categories, such as a  X  X ar X ,  X  X ow X  or  X  X hirt X . Howeve r, an object also has many other qualities attributes are important for understanding object appearance and for d escribing objects to other people. Figure 1 shows examples of such attributes. Automat ic learning and recognition of attributes can complement category-level recognition and therefore i mprove the degree to which machines perceive visual objects. Attributes also open the door to ap pealing applications, such as more specific different object categories often have attributes in commo n, modeling them explicitly allows part of the learning task to be shared amongst categories, or allo ws previously learnt knowledge about an attribute to be transferred to a novel category. This may r educe the total number of training images needed and improve robustness. For example, learnin g the variability of zebra stripes under non-rigid deformations tells us a lot about the correspondi ng variability in striped shirts. In this paper we propose a probabilistic generative model of visual attributes, and a procedure for learning its parameters from real-world images. When prese nted with a novel image, our method in-fers whether it contains the learnt attribute and determine s the region it covers. The proposed model encompasses a broad range of attributes, from simple colors such as  X  X ed X  or  X  X reen X  to complex pat-terns such as  X  X triped X  or  X  X hecked X . Both the appearance an d the shape of pattern elements (e.g. a single stripe) are explicitly modeled, along with their lay out within the overall pattern (e.g. adjacent stripes are parallel). This enables our model to cover attri butes defined by appearance ( X  X ed X ), by shape ( X  X ound X ), or by both (the black-and-white stripes of zebras). Furthermore, the model takes into account attributes with general appearance, such as st ripes which are characterized by a pattern of alternation ABAB of any two colors A and B, rather than by a s pecific combination of colors. Since appearance, shape, and layout are modeled explictly, the learning algorithm gains an under-weakly supervised setting, given images labeled only by the presence or absence of the attribute, without indication of the image region it covers. The presen ce/absence labels can be noisy, as the training method can tolerate a considerable number of misla beled images. This enables attributes to be learnt directly from a text specification by collecting tr aining images using a web image search engine, such as Google-images, and querying on the name of th e attribute.
 Our approach is inspired by the ideas of Jojic and Caspi [4], w here patterns have constant appearance within an image, but are free to change to another appearance in other images. We also follow the generative approach to learning a model from a set of images u sed by many authors, for example LOCUS [10]. Our parameter learning is discriminative  X  the b enefits of this have been shown before, for example for training the constellation model of [3]. In term of functionality, the closest works to ours are those on the analysis of regular textures [5 , 6]. However, they work with textures covering the entire image and focus on finding distinctive ap pearance descriptors. In constrast, here textures are attributes of objects, and therefore appear in complex images containing many other elements. Very few previous works appeared in this setting [ 7, 11]. The approach of [7] focuses on colors only, while in [11] attributes are limited to indiv idual regions. Our method encompasses also patterns defined by pairs of regions, allowing to captur e more complex attributes. Moreover, we take up the additional challenge of learning the pattern g eometry.
 Before describing the generative model in section 3, in the n ext section we briefly introduce image segments, the elementary units of measurements observed in the model. The basic units in our attribute model are image segments ext racted using the algorithm of [2]. Each segment has a uniform appearance, which can be either a color or a simple texture (e.g. sand, grain). Figure 2a shows a few segments from a typical image.
 Inspired by the success of simple patches as a basis for appea rance descriptors [8, 9], we randomly sample a large number of 5  X  5 pixel patches from all training images and cluster them usin g k-means [8]. The resulting cluster centers form a codebook of patch types . Every pixel is soft-assigned to the patch types. A segment is then represented as a normali zed histogram over the patch types of the pixels it contains. By clustering the segment histogr ams from the training images we obtain a codebook A of appearances (figure 2b). Each entry in the codebook is a prototype segment descriptor, representing the appearance of a subset of the s egments from the training set. Each segment s is then assigned the appearance a  X  A with the smallest Bhattacharya distance to the histogram of s . In addition to appearance, various geometric properties o f a segment are measured, summarizing its shape. In our current implementation, thes e are: curvedness, compactness, elonga-tion (figure 2c), fractal dimension and area relative to the i mage. We also compute two properties of pairs of segments: relative orientation and relative area ( figure 2d). Figure 1 shows various kinds of attributes. Simple attribut es are entirely characterized by properties of a single segment ( unary attributes ). Some unary attributes are defined by their appearance, suc h as colors (e.g. red, green) and basic textures (e.g. sand, gr ainy). Other unary attributes are defined by a segment shape (e.g. round). All red segments have similar a ppearance, regardless of shape, while all round segments have similar shape, regardless of appear ance. More complex attributes have a basic element composed of two segments ( binary attributes ). One example is the black/white stripes of a zebra, which are composed of pairs of segments sharing si milar appearance and shape across all images. Moreover, the layout of the two segments is characte ristic as well: they are adjacent, nearly parallel, and have comparable area. Going yet furthe r, a general stripe pattern can have any appearance (e.g. blue/white stripes, red/yellow stripes) . However, the pairs of segments forming a stripe pattern in one particular image must have the same ap pearance. Hence, a characteristic of general stripes is a pattern of alternation ABABAB. In this c ase, appearance is common within an image, but not across images.
 The attribute models we present in this section encompass al l aspects discussed above. Essentially, attributes are found as patterns of repeated segments, or pa irs of segments, sharing some properties (geometric and/or appearance and/or layout). 3.1 Image likelihood.
 We start by describing how the model M explains a whole image I . An image I is represented by a set of segments { s } . A latent variable f is associated with each segment, taking the value f = 1 for a foreground segment, and f = 0 for a background segment. Foreground segments are those on t he image area covered by the attribute. We collect f for all segments of I into the vector F . An image has a foreground appearance a , shared by all the foreground segments it contains. The like lihood of an image is where x is a pixel, and M are the model parameters. These include  X   X  A , the set of appearances allowed by the model, from which a is taken. The other parameters are used to explain segments a nd are dicussed below. The probability of pixels is uniform wit hin a segment, and independent across segments: with s x the segment containing x . Hence, the image likelihood can be expressed as a product ov er the probability of each segment s , counted by its area N Figure 3: a) Graphical model for unary attributes. D is the number of images in the dataset, S Note that F and a are latent variables associated with a particular image, so there is a different F and a for each image. In contrast, a single model M is used to explain all images. 3.2 Unary attributes Segments are the only observed variables in the unary model. A segment s = ( s by its appearance s gation and curvedness. The graphical model in figure 3a illus trates the conditional probability of image segments
The likelihood for a segment depends on the model parameters M = (  X ,  X , {  X  j } ) , which specify a visual attribute. For each geometric property  X  j = ( X  j , v j ) , the model defines its distribution  X  j over the foreground segments and whether the property is active or not ( v j = 1 or 0 ). Active properties are relevant for the attribute (e.g. elongation is relevant for stripes, while orientation is not) and contribute substantially to its likelihood in (4). Inactive properties instead have no impact properties are active and their foreground distribution.
 The factor p ( s and 0 otherwise (thus it acts as a selector). The scalar value  X  represents a simple background model: all segments assigned to the background have likelihood  X  . During inference and learning we want to maximize the likelihood of an image given the model over F , which is achieved by setting f to foreground when the f = 1 case of equation (4) is greater than  X  .
 appearance only.  X  is some low value, corresponding to how likely it is for non-r ed segments to be assigned the red appearance. No geometric property {  X  j } is active (i.e. all v j = 0 ). 3.3 Binary attributes The basic element of binary attributes is a pair of segments. In this section we extend the unary model to describe pairs of segments. In addition to duplicat ing the unary appearance and geomet-ric properties, the extended model includes pairwise prope rties which do not apply to individual segments. In the graphical model of figure 3b, these are relat ive geometric properties  X  (area, orien-tation) and adjacency  X  , and together specify the layout of the attribute. For example, the orientation of a segment with respect to the other can capture the paralle lism of subsequent stripe segments. Adjacency expresses whether the two segments in the pair are adjacent (like in stripes) or not (like the maple leaf and the stripes in the canadian flag). We consid er two segments adjacent if they share part of the boundary. A pattern characterized by adjacent se gments is more distinctive, as it is less likely to occur accidentally in a negative image.
 Segment likelihood. An image is represented by a set of segments { s } , and the set of all possible pairs of segments { c } . The image likelihood p ( I |M ; F , a ) remains as defined in equation (3), but now a = ( a likelihood of a segment s is now defined as the maximum over all pairs containing it Pair likelihood. The observed variables in our model are segments s and pairs of segments c . A pair c = ( s the model is
The binary model parameters M = (  X ,  X ,  X , {  X  j likelihood. The two sets of  X  j and define the geometric distributions and their associated activation states for each segment in the pair respectively. The layout part of the model captures the interaction between the two segments in the pair. For each relative geometric property  X  k = ( X  k , v k pairs of foreground segments and its activation state v k the pattern is composed of pairs of adjacent segments (  X  = 1 ) or just any pair of segments (  X  = 0 ). other cases (so, when  X  = 1 , p ( c |  X  ) acts as a pair selector). The appearance factor p ( s [ s for this image.
 pairs of appearances from A . The geometric properties  X  elong distributions  X  j have similar values. The layout parameters are  X  = 1 , and  X  rel area ,  X  rel orient are active and peaked at 0 (expressing that the two segments are parallel and have the s ame area). Finally,  X  is a value very close to 0 , as the probability of a random segment under this complex mo del is very low. Image Likelihood. The image likelihood defined in (3) depends on the foreground /background labels F and on the foreground appearance a . Computing the complete likelihood, given only the model M , involves maximizing a over the appearances  X  allowed by the model, and over F :
The maximization over F is easily achieved by setting each f to the greater of the two cases in equation (4) (equation (5) for a binary model). The maximiza tion over a requires trying out all allowed appearances  X  . This is computationally inexpensive, as typically there a re about 32 entries in the appearance codebook.
 Training data. We learn the model parameters in a weakly supervised setting . The training data consists of positive I images contain examples of the attribute to be learnt (figure 4), a considerable proportion don X  X . Conversely, some of the negative images do contain the attri bute. Hence, we must operate under a weak assumption: the attribute occurs more frequently on po sitive training images than on negative. Moreover, only the (unreliable) image label is given, not th e location of the attribute in the image. As demonstrated in section 5, our approach is able to learn fr om this noisy training data. Although our attribute models are generative, learning the m in a discriminative fashion greatly helps given the challenges posed by the weakly supervised setting . For example, in figure 4 most of the overall surface for images labeled  X  X ed X  is actually white . Hence, a maximum likelihood estimator over the positive training set alone would learn white, not r ed. A discriminative approach instead notices that white occurs frequently also on the negative se t, and hence correctly picks up red, as it is most discriminative for the positive set. Formally, the t ask of learning is to determine the model parameters M that maximize the likelihood ratio Learning procedure. The parameters of the binary model are M = (  X ,  X ,  X , {  X  j as defined in the previous sections. Since the binary model is a superset of the unary one, we only explain here how to learn the binary case. The procedure for t he unary model is derived analogously. In our implementation,  X  can contain either a single appearance, or all appearances in the codebook A . The former case covers attributes such as colors, or patter ns with specific colors (such as zebra stripes). The latter case covers generic patterns, as it all ows each image to pick a different appearance a  X   X  , while at the same time it properly constrains all segments/ pairs within an image to share the same appearance (e.g. subsequent pairs of stripe segments h ave the same appearance, forming a pattern of alternation ABABAB). Because of this definition,  X  can take on (1 + |A| ) 2 / 2 different values (sets of appearances). As typically a codebook of |A|  X  32 appearances is sufficient to model the data, we can afford exhaustive search over all possible v alues of  X  . The same goes for  X  , which can only take on two values.
 Given a fixed  X  and  X  , the learning task reduces to estimating the background pro bability  X  , and the geometric properties {  X  j each training image, as it is necessary for estimating the ge ometric distributions over the foreground segments. These are in turn necessary for estimating  X  . Given  X  and the geometric properties we can estimate F (equation (6)). This particular circular dependence in the structure of our model suggests a relatively simple and computationally cheap app roximate optimization algorithm: The algorithm is repeated over all possible  X  and  X  , and the model maximizing (8) is selected. Notice how  X  is continuously re-estimated as more geometric properties are added. This implicitly offers to the selector the probability of an average negative segment under the current model as an up-to-date baseline for comparison. It prevents the model from overspe cializing as it pushes it to only pick up properties which distinguish positive segments/pairs fro m negative ones.
 One last, implicit, parameter is the model complexity: is th e attribute unary or binary ? This is tackled through model selection: we learn the best unary and binary models independently, and then select the one with highest likelihood-ratio. The comparis on is meaningful because image likelihood is measured in the same way in both unary and binary cases (i.e . as the product over the segment probabilities, equation (3)). Learning. We present results on learning four colors (red, green, blue , and yellow) and three patterns (stripes, dots, and checkerboard). The positive t raining set for a color consists of the 14 images in the first page returned by Google-images when queri ed by the color name. The proportion of positive images unrelated to the color varies between 21% and 36% , depending on the color (e.g. figure 4). The negative training set for a color contains all p ositive images for the other colors. Our approach delivers an excellent performance. In all cases, t he correct model is returned: unary, no active geometric property, and the correct color as a specifi c appearance (figure 5a). Stripes are learnt from 74 images collected from Google-ima ges using  X  X triped X ,  X  X tripe X ,  X  X tripes X  as queries. 20% of them don X  X  contain stripes. The positive training set for dots contains 35 images, 29% of them without dots, collected from textile vendors websit es and Google-images (keywords  X  X ots X ,  X  X ot X ,  X  X olka dots X ). For both attributes, the 56 images for colors act as negative training set. As shown in figure 5, the learnt models capture well the na ture of these attributes. Both stripes and dots are learnt as binary and with general appearance, wh ile they differ substantially in their geometric properties. Stripes are learnt as elongated, rat her straight pairs of segments, with largely the same properties for the two segments in a pair. Their layo ut is meaningful as well: adjacent, curved segments, embedded within a much larger segment. Thi s can be seen in the distribution of the area of the first segment, the dot, relative to the area of t he second segment, the  X  X ackground X  on which dots lie. The background segments have a very curved , zigzagging outline, because they circumvent several dots. In contrast to stripes, the two seg ments that form this dotted pattern are not symmetric in their properties. This characterisic is model ed well by our approach, confirming its flexibility. We also train a model from the first 22 Google-images for the query  X  X heckerboard X , 68% of which show a black/white checkerboard. The learnt model i s binary, with one segment for a black square and the other for an adjacent white square, demonstra ting the learning algorithm correctly infers both models with specific and generic appearance, ada pting to the training data. Recognition. Once a model is learnt, it can be used to recognize whether a no vel image contains the attribute, by computing the likelihood (7). Moreover, t he area covered by the attribute is local-ized by the segments with f = 1 (figure 6). We report results for red, yellow, stripes, and do ts. All test images are downloaded from Yahoo-images, Google-imag es, and Flickr. There are 45 (red), 39 (yellow), 106 (stripes), 50 (dots) positive test images. In general, the object carrying the attribute stands against a background, and often there are other objec ts in the image, making the localization task non-trivial. Moreover, the images exhibit extreme var iability: there are paintings as well as pho-tographs, stripes appear in any orientation, scale, and app earance, and they are often are deformed (human body poses, animals, etc.). The same goes for dots, wh ich can vary in thickness, spacing, and so on. Each positive set is coupled with a negative one, in which the attribute doesn X  X  appear, composed of 50 images from the Caltech-101  X  X hings X  set [12] . Because these negative images are rich in colors, textures and structure, they pose a consider able challenge for the classification task. As can be seen in figure 6, our method achieves accurate locali zations of the region covered by the attribute. The behavior on stripe patterns composed of more than two appearances is particularly interesting (the trousers in the rightmost example). The mo del explains them as disjoint groups of binary stripes, with the two appearances which cover the lar gest image area. In terms of recognizing whether an image contains the attribute, the method perform s very well for red and yellow, with ROC equal-error rates above 90% . Performance is convincing also for stripes and dots, espec ially since these attributes have generic appearance, and hence must be recognized based only on geometry and layout. In contrast, colors enjoy a very distinctive, speci fic appearance.

