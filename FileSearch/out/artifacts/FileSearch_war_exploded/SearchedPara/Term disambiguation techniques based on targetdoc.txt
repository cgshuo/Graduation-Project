 1. Introduction
Term disambiguation plays an important role in cross-language information retrieval (CLIR), particularly in the case of using a machine-readable bilingual dictionary to translate the query or the document. In such bilingual dictionaries, two or more translations having different meanings are often listed under one entry term, and therefore we need to resolve the ambiguity of these translations, i.e., to select those relevant to the source term in the query (or in the document) from a set of translation candidates, in the process of CLIR. For example, when the English word  X  X  X ost X  X  in a query is translated into other languages, we have to identify auto-others. It is crucial for enhancing CLIR performance to match correctly representations of the query and each document written in two different languages, and thus effective techniques for translation disambiguation need to be developed.

Several excellent techniques for disambiguation have already been proposed and empirically tested using test collections of TREC, CLEF, or NTCIR. We may classify them roughly into two types in terms of lan-guage resources used for the disambiguation: (1) techniques using a parallel (or comparable) corpus in the two languages of the query and the documents, and (2) techniques based on only the target document collec-tion to be searched in CLIR (for other methods, see Kishida, 2005 ). If we have a parallel corpus as a language resource, it may be possible to extract useful information for term disambiguation (see Boughanem, Chris-ment, &amp; Nassr, 2002; Boughanem &amp; Nassr, 2001; Davis, 1997, 1998 ). However, such extra corpus is not always available for every pair of languages in real situations where a large number of combinations of lan-guages in the world can be requested for CLIR.

In contrast, the techniques based on only the target document collection are widely applicable because they do not need extra language resources other than a bilingual dictionary for translating queries into the docu-ment language. A typical example of this approach is the term co-occurrence based method, in which final search terms are determined according to similarities between translations, computed from co-occurrence sta-tistics in the target document collection. Since Ballesteros and Croft (1998) tested this method, many research-ers have used the term co-occurrence based method for improving CLIR performance.

As another disambiguation technique using only the target collection, Kishida and Kando (2004) have pro-posed a method based on so-called pseudo-relevance feedback (PRF), in which document frequencies of each translation within top-ranked documents searched initially with a set of all translation candidates are used for selecting final translations. Both methods try to extract information for disambiguation from the target doc-ument collection, but different statistics are applied, i.e., the former employs macro-statistics on term co-occurrence over the entire collection, whereas the latter uses local statistics on document frequencies within a part of the collection. The contrast in basic strategy behind the two different methods is interesting, and it is worth comparing their effectiveness.

This paper attempts to compare empirically the performance of these two methods for term disambiguation based on the target document collection. Simultaneously, we examine similarity measures (mutual informa-tion, Cosine and so on) and techniques of selecting translations used in the process of the term co-occurrence based method. So far, these term disambiguation methods and their variations have been used independently in each individual retrieval experiment. Therefore, well-organized empirical comparison between them is needed for extending our knowledge on term disambiguation for CLIR. For the purpose of this paper, the
CLEF 2003 test collection is employed for CLIR experiments, in which the Italian document collection is searched for German queries (i.e., German to Italian bilingual retrieval), and additionally for French queries (French to Italian retrieval) and English queries (English to Italian retrieval). For executing German to Italian and French to Italian searches, we use transitive translation approach using English language as a pivot.
The rest of this paper is organized as follows. First, we describe the procedures of the term co-occurrence based method and PRF based method. Next, the experimental setting and retrieval system used for empirical investigation of disambiguation techniques are explained. After the experimental results are shown and ana-lyzed, we discuss the effectiveness and efficiency of disambiguation techniques. Finally, concluding remarks are given. 2. Methods for term disambiguation using target collection 2.1. Term co-occurrence based techniques
A basic assumption underlying disambiguation techniques based on term co-occurrence is that  X  X  X he correct translations of query terms should co-occur in target language documents and incorrect translations should between translations such as MI (mutual information), Dice coefficient and so on, which are computed from term occurrence and co-occurrence statistics in the target document collection. For example, Bian and Chen (1998), Lin, Lin, Bian, and Chen (1999), Jang, Myaeng, and Park (1999) used the MI measure, ationally such that of observations is normalized by the size of the corpus according to Church and Hanks (1989) . Other defini-for any pair of translations if they are actually used in the collection.

If we try to identify correct translations for a query (or phrase) consisting of just two terms, the procedure is if three or more source terms are included in the original query, the way of determining automatically an opti-mal combination of translations becomes more complicated. For discussing the algorithm, we introduce the following mathematical notation: s j a term in the original source query ( j =1, ... , m )
T j a set of translations in the target language for the source term s sim( t , t 0 ) a similarity between the term t and t 0 (MI and so on) ~ t j a translation selected from the set of T j as a final search term a the maximum number of translations for a source term, i.e., a = max
A simple method would be to choose repeatedly a pair of translations with the highest similarity from those excluding pairs that have already been selected (a similar method seems to be used in Bian &amp; Chen, 1998; Lin lation was previously determined, this translation is just ignored. In other words, we choose a term such that from a set of translations for each source query term respectively. Note that Jang et al. (1999) adopted a sim-ilar strategy, but they considered only MI scores with the pairs selected in the previous step.
If the similarity measure to be used is symmetrical such as MI in Eq. (1) , the number of pairs to be mea-sured for the selection amounts to
It should be noted that M does not exceed a 2 m ( m 1)/2 where m ( m 1)/2 is the number of all combina-tions of two source terms. For example, if we have five source query terms and every set of translations includes five words respectively, we obtain M =(5  X  5)  X  (5  X  4/2) = 250. By computing M similarity values and sorting them, we can easily select translations in Eq. (2) .

However, this algorithm may yield erroneous translations due to the fact that it checks only a local rela-tionship between just two translations at each step for selecting a pair. Suppose that a bilingual dictionary automatically translates three source terms into English equivalents as shown in Table 1 , and that the correct
English translation is  X  X  X nformation management system X  X . In this case, if the pair of  X  X  X dministration X  X  and  X  X  X ystem X  X  has the highest degree of similarity in the target collection, then the correct English translations are never obtained by this algorithm. Since the algorithm in Eq. (2) looks at only a very limited range (i.e., a span of just two terms) in the query, a few pairs having strong relationships out of the context of the query tend to impact excessively on the result.

A straightforward solution to this problem would be to take all relationships between the query terms into consideration. We denote a sequence of translations by s ={ t sequence, i.e., can be computed, the sequence with the highest value of the sum should be selected as the final set of trans-lations. Since Eq. (4) contains all relationships on possible pairs of translations in each sequence, it avoids errors caused by the locality of range in Eq. (2) . This strategy was adopted by Maeda, Sadat, Yoshikawa, and Uemura (2000) , in which a web search engine was used for obtaining co-occurrence statistics (it should be noted that sequences whose score exceeded a threshold were selected as search terms in Maeda et al., 2000 ). Qu, Grefenstette, and Evans (2003) have also tried to select a set of translations according to similar strategies. However, the computational complexity for processing the algorithm based on Eq. (4) is very high. The total number of sequences to be processed is given by (i.e., L = j T j ), and the upper limit of L amounts to a expressed in Eq. (4) , and therefore, in total, we need to deal with a biguation (note that a single pair is repeatedly counted). If we have five source query terms and every set of translations includes five words respectively, the number of sequences amounts to 31,250 (=5 this example shows, it takes much longer to select translations according to Eq. (4) .

To overcome this problem, we can apply the maximal value of similarity between a given term and trans-lations of another source query term, i.e., which was employed by Adriani (2000, 2001) and Gao et al. (2001, 2002) . In this paper, v ( t , T sion X  X  for convenience according to Gao et al. (2001) . In order to solve ambiguity of translations using the quantity, for each term t , we first compute a sum of cohesion over all sets T
Gao et al., 2001 ):
The number of translation pairs to be processed for determining a translation for the j th source term is j T which does not exceed m  X  a  X  ( m 1) a = a 2 m ( m 1). If we have five source query terms and every set of translations includes five words respectively, we obtain M =5  X  5  X  5  X  4 = 500. Although its degree of com-putational complexity is almost the same as that of the method in Eq. (2) , the algorithm using cohesion avoids given source term s j is chosen using information on relationships with all other source terms through the cohe- X  X  X anagement X  X  more than  X  X  X dministration X  X  in the document set (see Table 1 ). In this case, even if the simi-be correctly selected as a translation of the second source term B, i.e., the final sum of v ( t , T ment X  X  may become greater than that for  X  X  X dministration X  X  due to the contribution of high similarity degree between  X  X  X anagement X  X  and  X  X  X nformation X  X .

Logically, under a particular condition, the algorithm in Eq. (6) gives the same result as the method based on similarities between all pairs in each sequence of translations in Eq. (4) . Suppose that there are two trans-lation candidates for the first source term and each of all other source terms has just one translation. If it is assumed that m = 4, the situation can be described as shown in Fig. 1 . The relationships represented by dotted lines have no effect on the choice of a translation for the first source term. Consequently, the algorithm in Eq. means that the strategy in Eq. (6) is actually working. Of course, the condition that all m 1 source terms have a single translation is not always true in real situations. In general, whether the two algorithms give the same result or not depends on the sum of degrees of similarity represented by dotted lines, which the strat-egy in Eq. (6) does not take into consideration. 2.2. Pseudo-relevance feedback based method
As Yamabana, Muraki, Doi, and Kamei (1998) pointed out, unexpected false combinations of translations may be generated by the disambiguation method based on term co-occurrence because it is possible that two translations having no relation within the context of a given source query tend to co-occur frequently in the whole document collection. That is, results by macro-statistics compiled from the whole collection are not always valid in the sense implied by the given query. This problem is similar to that in applying query expan-sion techniques. It is widely known that query expansion techniques using statistical thesauri generated through term co-occurrence statistics cannot achieve better performance than pseudo-relevance feedback (PRF), in which new search terms are locally identified from a restricted set of top-ranked documents searched get document collection may yield better results than global one.

According to the insight about the effectiveness of local analysis, Kishida and Kando (2004) proposed a disambiguation technique using PRF. The idea is simple. In the first stage, the target document collection is searched for a set of all translation candidates, using any search algorithm (e.g., a probabilistic model). Next, we pick up some top-ranked documents from the search result, and count the number of occurrences of each translation candidate. Finally, the most fre-quently appearing translation candidate within the top-ranked documents is selected as the final translation for each source term, i.e., where r t is the number of documents including the term t within the set of top-ranked documents. For the case of Table 1 , in which  X  X  X nformation management system X  X  is assumed to be the correct combination of transla-tions, it is likely that documents containing these three terms will be higher ranked in the results of searching using all six translations as query terms, if the search algorithm tends to give a higher score to the document that includes more search terms. In contrast, it can be reasonably expected that irrelevant translations do not appear together in each document because of the low probability of such irrelevant terms being related, e.g.,  X  X  X ews X  X  and  X  X  X dministration X  X  in Table 1 are unlikely to be frequently included together in a document. There-fore, the number of documents within the set of top-ranked documents can be used for identifying correct translations, as expressed in Eq. (7) . 2.3. Summary on term disambiguation techniques
Table 2 summarizes three versions of disambiguation techniques based on term co-occurrence and a PRF based method for disambiguation. According to our discussion, we expect  X  X  X he best sequence X  X  algorithm using Eq. (4) and  X  X  X he best cohesion X  X  using Eq. (6) to show better performance than  X  X  X he best pair X  X  algorithm using Eq. (2) . However, it may be hard to use the best sequence algorithm in real situations because of too high complexity for computation. In addition, the PRF based method using a kind of local analysis of the docu-ment collection may show better performance. We need to investigate empirically whether the theoretical expectation is true or not.
 It should be noted that disambiguation techniques other than those listed in Table 2 have been proposed.
For example, Monz and Dorr (2005) developed an EM algorithm that estimates probabilities of translating the source term into a target one by incorporating co-occurrence frequencies into the statistical model for estimation, and Liu, Jin, and Chai (2005) proposed another novel statistical model (called  X  X  X aximum coherence model X  X ) that estimates similarly the translation probabilities using word co-occurrence statistics.
These methods are based on relatively complicated statistical algorithms, which are beyond the scope of this paper.

In addition, so-called Pirkola X  X  structured query model ( Pirkola, 1998 ) is widely used for dealing with multi-ted as synonyms at the time of searching with a synonym operator of the INQUERY system ( Turtle &amp; Croft, 1991 ). Although the Pirkola X  X  method is widely known as an effective technique, this paper does not take it into consideration furthermore in order to focus on direct disambiguation, i.e., selecting only one correct translation for each source term.
 3. Experimental setting and system 3.1. Purpose and method
In order to examine empirically the performance of translation disambiguation techniques based on the tar-get document sets, we attempt an experiment of bilingual retrieval from German to Italian using the CLEF 2004 ).

Our method for executing CLIR is basically dictionary-based query translation, and English was used as an intermediary language for translating German into Italian (this is often called the  X  X  X ivot language approach X  X ), i.e., the process is
More precisely, first, each query term in German is replaced with a set of the corresponding English terms from a German to English dictionary, and second, each term in the English sets is again replaced with a set of the corresponding Italian terms using an English to Italian dictionary. Such transitive translation via a pivot language often yields many irrelevant translations because all final translations obtained from irrele-vant terms in the intermediary language (i.e., English) are possibly also irrelevant. Therefore, it is especially important for the dictionary-based pivot language approach to enhance the retrieval performance by using translation disambiguation.

Furthermore, in order to verify findings from German to Italian searches, the same experiments are repeated for the case of French to Italian (pivot language approach) and English to Italian (non-pivot) searches on the same CLEF 2003 test collection. The additional experiments will be described after discussing results from the experiment on German to Italian searches. 3.2. Similarity measures
At the time that an inverted file for the original Italian document collection was created, term co-occurrence was also counted and recorded in another index file enabling us to find quickly information on the occurrence.
In this experiment, sentence-based co-occurrence is adopted, i.e., we count sentences in which both terms appear together and define the total number as  X  X  X he number of co-occurrences X  X . Thus, in this study  X  X  X he num-ber of occurrences X  X  means the number of sentences including the term. As for similarity measures, in addition to MI in Eq. (1) , we use the Cosine coefficient, Dice coefficient, and Overlap coefficient, which are defined as respectively. 3.3. Search algorithm and text processing
The well-known BM25 of the Okapi formula ( Robertson, 1995 ) was used for computing document scores in this study. In order to compute the scores for a given query, both German and Italian texts in queries and documents were processed by the following steps: (1) identifying tokens, (2) removing stopwords, (3) lemma-tization, and (4) stemming. In particular, decomposition of compound words in German was executed using a heuristic rule based on longest matching against headwords included in a German to English dictionary ( Kish-ida &amp; Kando, 2004 ). For example, the German word,  X  X  X riefbombe X  X , is broken down into two headwords listed in the German to English dictionary,  X  X  X rief X  X  and  X  X  X ombe X  X , according to a rule that only the longest headwords included in the original compound word are extracted from it.

Free bilingual dictionaries (German to English and English to Italian) on the Internet ( http://www.free-lang.net/ ) were used for transitive query translation. Stemmers and stopword lists for German and Italian were downloaded from the web site of the Snowball project ( http://snowball.tartarus.org/ ). Stemming for English was performed using the original Porter X  X  algorithm ( Porter, 1980 ).

Before executing transitive translation by two bilingual dictionaries, all terms included in the bilingual dic-tionaries were normalized using the same stemming and lemmatization procedures as those for processing texts of documents and queries. The actual translation process is a simple replacement as explained above.
If no corresponding headword was included in the dictionaries (German X  X nglish or English X  X talian), the unknown word was sent directly to the next step without any change. 4. Results and analysis 4.1. Effectiveness of similarity measures
The Italian document collection contains 157,558 documents in total, and its average document length is 181.86 words. First of all, we examine the effectiveness of similarity measures (i.e., MI, Cosine, Dice and Over-lap) used in the process of the term co-occurrence based method for disambiguation. For comparison of the effectiveness of searches, scores of mean average precision (MAP) were calculated for a set of 51 topics that have one or more relevant documents in the Italian document collection (see Table 3 ). In Table 3 , MAP scores only the  X  TITLE  X  field in the topics of the CLEF 2003 test collection, whereas in the case of long queries, both the  X  TITLE  X  and  X  DESCRIPTION  X  fields were employed. The  X  TITLE  X  field usually includes a few search terms, and a sentence representing the search query is described in the  X  DESCRIPTION  X  field (see
Braschler &amp; Peters, 2004 ). Since the sentence is decomposed according to the procedure described above, each long query tends to contain relatively many terms as a result. Therefore, unfortunately, search results for the  X  X  X est sequence X  X  algorithm in the case of long queries could not be obtained due to excessive computational complexity.

We also attempted post-translation feedback ( Ballesteros &amp; Croft, 1997 ), which is often used for enhancing search performance after the process of translation disambiguation, and simply means using the PRF tech-nique. In this study, the feedback was executed by extracting 10 terms from the top-ranked 30 documents, which were obtained by an initial search using translations selected by each disambiguation method, based on term weighting where N is the total number of documents, R is the number of relevant documents, n documents including term t ,and r t is the number of relevant documents including term t . In the situation of
PRF, since some top-ranked documents are beforehand assumed to be relevant, R corresponds to the number of top-ranked documents from which terms are extracted (i.e., R = 30), and r containing term t within the top-ranked documents, i.e., the definition is the same as that in Eq. (7) (according should discuss mainly the effectiveness of disambiguation methods without the post-translation feedback be-cause other factors would affect the search performance through the feedback. Therefore, the search results with the post-translation feedback will be treated as supplementary findings in this paper.

In Table 3 , it can be observed that the Cosine coefficient gives better search results than any other similarity measure, i.e., the best performance is obtained by using the Cosine coefficient in all cases of searching short queries without feedback (its MAP score is 0.1796), short queries with feedback (0.1893), long queries without feedback (0.2031) and long queries with feedback (0.2398). The averages of MAP scores for the Cosine coef-ficient are also higher than those for other measures (0.1790 without feedback and 0.1939 with feedback).
However, it should be noted that the Dice coefficient is consistently more effective than the Cosine measure in the cases of the best pair and best cohesion methods for short queries, i.e., the Cosine coefficient is not always dominant in every case.

We should also note that the difference of scores is not so large for making a definite conclusion on the dominance of the Cosine coefficient. For example, the difference of MAP scores between the Cosine and
MI is only 0.0049 in the case of the best sequence algorithm for short queries without feedback (i.e., 0.1796 for the Cosine and 0.1747 for MI). Therefore, a statistical test is needed for examining the reliability of the conclusions. In this paper, we use the sign test, which is a non-parametric method for statistically checking if two samples were drawn from a single population. Suppose that we have two different search results, { y and H is the total number of topics (in this experiment, H = 51). We denote the number of topics where y by d . Under the assumption that two samples were drawn from the same population, the number of topics d is theoretically described by a binomial distribution with p = 0.5 and Q trials (where p is a parameter and Q is the total number of topics excluding those where y i = z i statistical test (i.e., the null hypothesis is that y i 6 z
Table 4 shows probabilities between the best two results for two cases of short queries and long queries without feedback. In both the cases, values of Q , i.e., the number of topics where y and 10, respectively, whereas the total number of topics is 51). This means that an identical set of translations is selected in most of the topics. For the case of short queries, the Cosine coefficient outperforms the MI in 11 out of 14 topics, and the probability is 0.0287, which is a statistically significant difference at the 5% level.
Although the difference of MAP scores is very small (i.e., 0.0049), we can conclude that there is statistically significant difference in term of that the Cosine coefficient is dominant for most of topics in our sample. In contrast, for the case of long queries, the Cosine is dominant in only 4 out of 10 topics, which means no sta-tistically significant difference. Hence, we cannot draw a clear conclusion on the dominance of the Cosine coef-ficient for long queries containing many terms.
 4.2. Selection algorithms for term co-occurrence based method
As we expected above, the performance of selecting translations based on the best pairs tends to be rela-tively poor in Table 3 . There is no case in which the MAP score of this algorithm is greater than that of the best cohesion or the best sequence methods (except the results with feedback) although the difference is small. Also, the best sequence algorithm is dominant except in just one case of the Overlap coefficient for short queries (the best cohesion outperforms the best sequence). From these empirical observations, we can repre-sent the relative performance as which is almost consistent with the conclusion derived logically in the above section. This finding is again ob-served in Table 5 , which shows averages of the MAP scores for each disambiguation technique. The highest average for short queries is that of the best sequence algorithm, followed in order by the best cohesion and the best pair. For long queries, the average for the best pair algorithm is lower than that for the best cohesion.
The result of the sign test is shown in Table 6 for only cases using the Dice or Cosine measure without feed-abilities are over 0.05), we can see that the best sequence algorithm outperforms the best pair method, i.e., the probabilities of the best sequence against the best pairs are 0.3438 (Dice) and 0.1133 (Cosine), which are rel-atively small possibilities. For example, the probability 0.1133 is obtained from the binomial distribution with d = 8 and Q = 11. This means that there are 8 topics where y y trast, the evidence that the best sequence outperforms the best cohesion may not be enough, i.e., the proba-bilities for Dice and Cosine are 0.6563 (Dice) and 0.3770 (Cosine) respectively. With regard to the relationship between the best cohesion and the best pair algorithms, only in the cases of long queries with Dice and short queries with Cosine may we conclude that the best cohesion algorithm yields better results than the best pair method, although no statistically significant difference is observed. 4.3. Comparison of term co-occurrence and PRF based methods
MAP scores of search runs using PRF based methods are shown in Table 7 , in which the number of top-ranked documents that are assumed to be relevant is set to be 30, 50, 100, 500 and 1000 respectively (the numbers of top-ranked documents are arbitrarily selected in order to look for the best-possible perfor-mances by tuning with the target collection). It can be seen that the performance is slightly changed by the number of top-ranked documents used for disambiguation. On average, it seems that the case of the top 30 documents is dominant in the situation without feedback, but its degree of dominance is small.
Table 8 summarizes the best scores selected from Tables 3 and 7 , respectively. As indicated, the term co-occurrence based method (with the best sequence algorithm) outperforms the PRF method for short queries whereas the PRF method outperforms the term co-occurrence based method for long queries. This empirical observation suggests that the best sequence algorithm produces higher performance, but unfortunately we cannot use it due to excessive computational complexity in some situations as discussed above (actually, we could not apply the algorithm to long queries in this experiment). If the computation is impossible, we have no alternative than to use the best cohesion algorithm. It is therefore worth comparing the performance between the best cohesion and the PRF based method. As shown in Table 8 , the PRF based method outper-forms the best cohesion algorithm for both short and long queries. However, no statistically significant differ-ence is observed between these methods (see Table 9 ).

Table 8 also shows MAP scores of search results without any disambiguation, which we can consider as a kind of baseline. Clearly, there are large differences of the scores between the baseline and our disambiguation disambiguation and the baseline, e.g., its probability is 0.0008 between the best sequence with the Cosine and no disambiguation for short queries (without feedback) and is 0.0010 between the PRF based method by the top 100 documents and no disambiguation for long queries (without feedback). Finally, recall X  X recision graphs of search runs listed in Table 8 are shown in Figs. 2 and 3 for short and long queries, respectively. 4.4. Additional experiments for verifying research results
In order to confirm reliability of results obtained from our empirical test on German to Italian retrieval, the same experiments are repeated for French to Italian and English to Italian searches using the same test col-lection (CLEF 2003). French topics were transitively translated into Italian by the pivot approach via English like as German to Italian searches, but inevitably, English to Italian bilingual retrieval is non-pivot.
Procedures for searches and experiments are almost similar to those in the German to Italian case, and lan-guage resources (French to English dictionary, French stopword list and French stemmer) were downloaded from the same Web sites used in the case of German to Italian retrieval. However, unlike text processing for German topics, we did not try to decompose any term in French and English topics.
 The results of additional experiments are almost similar to that in the case of German to Italian retrieval.
MAP scores of searches for short queries with no feedback are shown in Table 10 . First, we can conclude again that the dominant method for disambiguation based on term co-occurrence is to select the best sequence since its average scores are the highest in both cases of French and English topics. The best sequence method is followed by the best cohesion (see Table 10 ).

Second, the PRF method shows again good performance. Especially, in these additional experiments, this method outperforms slightly the term co-occurrence based methods ( X  X  X op 100 X  X  and  X  X  X op 50 X  X  are dominant among all runs of French to Italian and English to Italian retrieval, respectively).

Finally, we can see that the Cosine similarity is superior to other measures in the case of French to Italian retrieval. Although the Dice coefficient outperforms the Cosine coefficient in the non-pivot case (English to
Italian), the differences are so small and the dominance of the Dice coefficient was also partly observed in the case of German to Italian searches (see Table 3 ). 5. Discussion 5.1. Case studies In this section, we explore more deeply the disambiguation process for two particular topics, C145 and C164, in the case of German to Italian searches, in order to understand the characteristics of each method. Table 11 shows scores of average precision and stems of each selected translation for topic C145, of which
 X  TITLE  X  is  X  X  X apans Reisimport (Japan X  X  rice import) X  X . Our decomposing algorithm for German words broke automatically  X  X  X eisimport X  X  into  X  X  X eis X  X  and  X  X  X mport X  X , and transitive query translation was executed for the three source terms,  X  X  X apans X  X ,  X  X  X eis X  X , and  X  X  X mport X  X  using German to English and English to Italian dictio-naries. In the process of translation, some irrelevant translations were obtained. For example, the Italian stem  X  X  X iagg X  X , which means  X  X  X ravel X  X , was yielded from the German word  X  X  X eise X  X , which was unfortunately con-verted into  X  X  X eis X  X  by the stemming algorithm when the dictionary was implemented on our system. As shown in Table 11 , the performance of the best pair, cohesion and PRF based methods, is poor due to the fact that this irrelevant term was finally selected, whereas the best sequence algorithm correctly chose the stem  X  X  X is X  X , which means  X  X  X ice X  X .

From Table 12 which shows similarities of  X  X  X is X  X  and  X  X  X iagg X  X  with the other translation candidates, we can understand why the best pair and cohesion algorithms consequently selected the irrelevant translation. As shown, many irrelevant translations were produced due to our simple decomposing algorithm and transitive translation via English. Among them,  X  X  X mportant X  X  has the highest similarity with  X  X  X iagg X  X , and the best pair because  X  X  X iappon X  X  and  X  X  X mport X  X  had already been chosen with higher similarity of 0.018). It is obvious from its selection process (i.e., only a single pair is considered in each step, independently), as discussed above.
For this case, unfortunately, the best cohesion algorithm also made an error. The total value of cohesion of 0.00593 +  X  X  X mportant X  X  0.00735 = 0.01328). If the similarity between the pair of  X  X  X iagg X  X  and  X  X  X iappon X  X  were low, the cohesion method would correctly identify  X  X  X is X  X . However, since the irrelevant translation  X  X  X iagg X  X  happens to have a close relationship with the other correct translation  X  X  X iappon X  X  (e.g.,  X  X  X ravel to Japan X  X ), the irrelevant term was selected by mistake. It should be noted that, although the best cohesion algorithm could not make a correct choice in such cases, more information (i.e., similarity with  X  X  X iappon X  X ) is used.
Therefore, its error probability is expected to be lower than that of the best pair algorithm as indicated empir-ically in the previous section.

In the best sequence algorithm, the similarity between  X  X  X iagg X  X  and  X  X  X mport X  X  has an effect on selection because the Cosine coefficient between  X  X  X mport X  X  and  X  X  X iappon X  X  is very high as described above and conse-quently the combination of the three correct translations  X  X  X mport X  X ,  X  X  X iappon X  X  and  X  X  X is X  X  has a high average irrelevant sequence of  X  X  X mport X  X ,  X  X  X iappon X  X  and  X  X  X iagg X  X  cannot be dominant. As shown in the case, the best sequence algorithm would keep the error probability lower than that of other methods by taking more evi-dence into account.

The PRF based method also erroneously selected the irrelevant translation  X  X  X iagg X  X  due to the fact that five documents in the top-30 documents include  X  X  X iagg X  X  whereas the number of documents including  X  X  X is X  X  is just two. However, in another topic, C164, the PRF based method outperforms the best sequence algorithm. The short query of topic C164 is  X  X  X uropa  X  ische Strafurteile zu Drogen (European sentences for drugs) X  X . Table 13 shows average precision scores and selected translations (stems) for topic C164. The German word  X  X  X trafurte-ile X  X  was decomposed into  X  X  X traf (penal) X  X  and  X  X  X rteile (judgments) X  X  by our algorithm and the best sequence word having more than one meaning, i.e.,  X  X  X udge X  X ,  X  X  X hink X  X ,  X  X  X aintain X  X , and so on, and therefore the best sequence algorithm was inferior in performance to the PRF based method, which identified a more adequate  X  X  X entenz X  X  appears in 28 out of the top-ranked 30 documents (whereas  X  X  X ol X  X  appears in only 8 documents).
The reason why  X  X  X ol X  X  was selected by the best sequence algorithm is that this term has a relatively strong relationship with the correct translation  X  X  X urop (Europe) X  X  (see Table 14 ). As the average precision score in
Table 13 and Cosine similarities in Table 14 indicate, we cannot definitively conclude that  X  X  X ol X  X  is a wrong  X  X  X un X  X  and  X  X  X rog X  X  to some degree. From this viewpoint, the best sequence algorithm never makes a mistake, however, it does not allow us to obtain the more appropriate translation  X  X  X entenz X  X  due to the fact that a gen-eral term  X  X  X urop X  X  having a close relationship with the polysemous term  X  X  X ol X  X  happens to be included in the query. This would be a problem of using macro-statistics for translation selection.
 5.2. Comparison of performance
Through the discussion on the two topics C145 and C164, we observe that success of the translation selec-tion is strongly dependent on other translation candidates having relationships with the correct translation.
This is true not only for the term co-occurrence based method but also for the PRF based method, e.g., in topic C145, the reason why this method erroneously selected  X  X  X iagg X  X  is perhaps because the list of translation and therefore many documents on  X  X  X ravel to Japan X  X  may accidentally be included in the top-ranked docu-ments. The situational dependency may have made it harder for a statistically significant difference to be was too small).

Thus, it is difficult to draw a clear conclusion that be commonly applied to every case, and we obtained just some tentative findings on translation disambiguation, e.g., the best sequence algorithm is likely to be dom-inant, followed in order by the best cohesion and the best pair algorithms, and the PRF based method is likely to outperform the best cohesion and pair algorithms. However, these tentative conclusions are an important starting point, and we may use them as a hypothesis for further experiments based on other test collections. 5.3. Efficiency of processing
One problem for implementing the PRF based method would be that processing takes longer in on-line settings because the document collection is repeatedly searched two times. If only a few translations are entered, we would obtain the search result within a reasonable response time. However, in general, a set of translation candidates tends to contain many terms. Thus, the disambiguation technique based on PRF would be appropriate for batch mode searching rather than usual online searching.

On the other hand, it should be noted that we need no extra device for executing the PRF based method other than the standard search function with document ranking and PRF, which is an advantage of this method. In the case of the term co-occurrence based method, a special index file for quickly searching and extracting co-occurrence information is needed. If the size of the document collection is large, it may be dif-ficult to prepare such an index file because too many pairs of terms are included in the whole collection (huge computer resources may be required for implementation). Thus, we should consider carefully the situation in which translation disambiguation is needed and the computer resources available before deciding which method to use. The experiments in this study suggested that there is no large difference in  X  X  X ffectiveness X  X  between the term co-occurrence and PRF methods, and so efficiency may be a key factor when choosing a disambiguation method. 6. Conclusion
This paper has discussed term disambiguation techniques using only the target document collection as a language resource, i.e., the term co-occurrence based method and the PRF based method. From experiments on German to Italian, French to Italian and English to Italian bilingual retrieval employing the CLEF 2003 test collection, it was shown that the best sequence algorithm for term selection with the Cosine coefficient is dominant, and that the PRF method shows comparable high search performance. However, statistical tests do not adequately support these conclusions except a few cases, and further examinations are needed to confirm the reliability of the findings.

So far, term disambiguation techniques have been applied by many researchers for improving dictionary-based CLIR. Unfortunately, these techniques tend to be independently used in individual experiment, and therefore, we did not have enough knowledge on which method shows better performance. Although the experiment in this paper does not give sufficiently conclusive results as mentioned above, we obtain empirical data on comparisons of several disambiguation methods, which would help us choose a method appropriate to each situation of CLIR.
 As described in Section 1 , there are a large number of possible pairs of languages in real CLIR situations.
For all combinations of languages, rich language resources such as machine translation software or parallel corpora are not always available. However, when a search is executed, there is always the target document collection, and if we also have a bilingual dictionary, then CLIR becomes possible. From this viewpoint, if we can develop a method for extracting more useful information for enhancing CLIR performance from the target document collection, it would help improve CLIR systems. In particular, for many languages, resources for translating from and to English are usually available, because English is now the most dominant language for international communication. This means that the pivot language approach via English adopted in the experiment of this paper provides us a large possibility for executing any combinations of two lan-guages. Therefore, we should explore furthermore the term disambiguation techniques in the situation using pivot language approach for CLIR.
 References
