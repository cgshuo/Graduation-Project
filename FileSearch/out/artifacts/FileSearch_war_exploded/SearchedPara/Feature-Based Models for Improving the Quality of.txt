 Supervised relation extraction from text relies on annotated data. Distant supervision is a scheme to obtain noisy train-ing data by using a knowledge base of relational tuples as the ground truth and finding entity pair matches in a text corpus. We propose and evaluate two feature-based models for increasing the quality of distant supervision extraction patterns. The first model is an extension of a hierarchi-cal topic model that induces background, relation-specific and argument-pair specific feature distributions. The sec-ond model is a perceptron, trained to match an objective function that enforces two constraints: 1) an at-least-one semantics, i.e. at least one training example per relational tuple is assumed to be correct; 2) high scores for a dedicated NIL label that accounts for the noise in the training data. For both algorithms, neither explicit negative data nor the ratio of negatives has to be provided. Both algorithms give improvements over a maximum likelihood baseline as well as over a previous topic model without features, evaluated on TAC KBP data.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; I.2.7 [ Artificial Intelligence ]: Nat-ural language processing X  text analysis Algorithms, Experimentation Information Extraction, Machine Learning, Pattern Learn-ing, Topic Models, Distant Supervision
Relation extraction can be formulated as the task of turn-ing unstructured text into tabularized information. Two relation extraction paradigms can be distinguished: 1) open information extraction, the unsupervised clustering of entity-context tuples [2], and 2) relation extraction for a fixed rela-tion inventory, which is also known as knowledge-base pop-ulation (KBP) [5]. While open information extraction does not require annotated data, it may not always provide the most useful granularity or partitioning for a specific task. In contrast, relation extraction for a pre-specified relation in-ventory may be better tailored for a specific task, but needs labeled training data; however, textual annotation is costly. Databases with fact tuples such as (PERSON, born-in, CITY) are often readily available. However, there is usually no or only very little text annotated according to whether it expresses a relation (e.g. born-in ) between particular enti-ties (e.g. of types PERSON and CITY ). This is used by the paradigm of distant supervision (DS) , [6]: Textual matches of entities from fact tuples are used to automatically gen-erate relation contexts as training instances. For example, the arguments of the fact tuple ( X  X arack Obama X , born-in,  X  X onululu X ) could match in contexts like  X  X arack Obama was born in Honululu X  and  X  X arack Obama visited Honul-ulu X  . Often only a small fraction of such matches indeed express the relation of the fact tuple.

The task of this work is to increase the quality of semi-automatically extracted training data for a given relation inventory. We are considering relation-specific surface pat-terns (e.g.  X  X ARG1] was born in [ARG2] X  ) that are obtained from noisy DS training data and applied to free text (to-gether with a type checker). The aim is to find a scoring algorithm which computes a ranking of noisy patterns that correlates best with the precision of facts extracted by them. We propose and evaluate two feature-based models for es-timating the probability of noisy DS patterns expressing a given relation. The first is an extension of a recently pro-posed hierarchical topic model [1], which is based on the principle that patterns are generated either by a background distribution, a relation-specific distribution or an argument-pair-specific distribution, depending on which value a hidden topic variable takes on. The second model is a multi-class perceptron learner based on the principle that a DS pattern for a particular relation has a high probability of either ex-pressing no relation between two entities or the relation of the corresponding database fact. Additionally, the percep-tron learner assumes that for each relational tuple with a textual match, the training data contains at least one con-text that in fact expresses the relation. Experiments show that both approaches significantly improve the quality of extracted relational patterns. Figure 1: Hierarchical topic models. Intertext model (left) and feature model (right).
We build on the hierarchical topic model used by [1] that aims at extracting relation specific patterns. We refer to this model as the original intertext model, as relational pat-terns are defined only by the text between the arguments and are not broken up into features. This model is the hier-archical topic model for multi-document summarization of [3] with the following correspondences: Pairs of arguments are assumed to form documents, with the surface patterns as their words. Pairs of arguments are grouped together according to the relation they stand in.

The generative process assumes that for each argument pair of a particular relation, all the patterns (intertext con-texts of DS matches) are generated by first generating a hid-den variable z at a position i , depending on a pair-specific distribution  X  (with Dirichlet hyper parameters  X  ). The variable z can take on three values, B for background, R for relation and P for pair. Corresponding vocabulary distri-butions (  X  bg , X  rel , X  pair ) are chosen for generating the con-text pattern at position i . The vocabulary distributions are smoothed by Dirichlet hyper parameters  X  bg , X  rel , X  pair shared on the respective levels. See Figure 1 for a plate di-agram of the basic model. Gibbs sampling is used to infer the topics of the document collection.
The intertext topic model can only treat patterns as a whole. While this may be sufficient for very frequent pat-terns, for the long tail of infrequent patterns evidence on pattern-level may be to sparse to do meaningful inference. We therefore extend the model to include common struc-tural elements on sub-patterns level (i.e. bigrams) that may be indicative and are shared among patterns.

In order to include features in the model, we propose a model with two layers of hidden variables. A variable x rep-resents a choice of B,R or P for every pattern. Each feature is generated conditioned on a second variable z  X  X  B,R,P } (the same way as in the simple intertext model). The fea-tures now range over index i , and patterns over index j . For a pattern at index j , first one hidden variable x is generated, then all z variables are generated for the corresponding fea-tures at indices i (see Figure 1).
 Algorithm 1 At-Least-One Perceptron Learner with NIL  X   X  0 for r  X  X  do
In the following formula a function j ( i ) is used to denote the mapping from a feature index i to the index j of the corresponding pattern. The values B,R or P of z depend on the corresponding x by a transition distribution: where p same is set to . 99 to enforce the correspondence between pattern and feature topics. While the original work reports hyper parameters  X  = (15 , 1 , 15), we found a uniform prior  X  = (1 , 1 , 1) to work slightly better, which we use for the feature-based experiments.
As a second feature-based model, we propose a percep-tron model with an objective function that enforces certain constraints. The model includes log-linear factors for all re-lations (from set of relations R ) as well as a factor for the NIL label, expressing noise or no relation between the en-tities. The estimated probabilities for a relation r given a sentence pattern s are calculated by normalizing over log-linear factors: The factors are defined as: with  X  ( s,r ) the feature vector for sentence s and label as-signment r , and  X  r the feature weight vector.The final scor-ing used to rank the patterns is obtained by:
As we do not use annotated data for the NIL assignment and only have noisy DS data for the relation assignment, the learner is directed by the following semantics: First, for a DS sentence s that has a textual match for relation r , relation r should have a higher probability than any other relation r 0  X  X \ r . Second, as extractions are noisy, we also expect a high probability for NIL . We therefore introduce the constraint that NIL has a higher probability than any relation r 0  X  X \ r for which s is not a DS sentence. Third, at least one DS sentence for an argument pair is expected to express the corresponding relation r . For sentences s an entity pair belonging to relation r , this can be written in the form of the following constraints:
Hence, the model is a multi-class learner to predict either the label r or the label NIL with the final decision con-strained by the at-least-one semantics. An important point to note is that the NIL class is learned from candidates of all relations and not just a negative per-relation classifica-tion decision. It is therefore roughly corresponding to the background vocabulary in the topic model. The violation of any of the above constraints triggers a perceptron up-date. Similar to [4], the update corresponding to a violated at-least-one constraint is applied only to the one sentence that already has the highest score for the correct label. The basic algorithm is sketched in Algorithm 1. The actual im-plementation additionally uses averaging over all updates, weighting of updates by the model score of wrong labels and iterating several times over the data (we use 20 iterations in our experiments).
We measure the ranking quality of the patterns by the ranking quality of their extractions. We use all TAC KBP queries from the years 2009-2011 and the TAC KBP 2009-2011 corpora 1 . The queries consist of 298 query entities with types PERSON or ORGANIZATION ; there are 42 relations to be considered. First, candidate sentences are retrieved from the corpora in which the query entity and a second en-tity with the correct type for a sought relation is contained. Those candidate sentences are then used to provide answer candidates if one of the patterns  X  extracted from the train-ing data  X  matches. An answer candidate is assigned the score of the matching pattern. If several patterns match, the score of the highest scored pattern is assigned. This ranking is then evaluated using the TAC KBP gold anno-tations 2 . The basis for pattern extraction is the (noisy) DS training data of a top-3 ranked system in TAC KBP 2012 [7]. We also use the retrieval components of this system to obtain sentence and answer candidates. The basis of eval-uation consists of 38 , 939 response candidates from pattern matches, ranked according to their respective pattern scores. 951 of the response candidates are correct according to the gold annotation, 38 (out of 42) relations have at least one correct response candidate. Evaluation results are reported as averages over per-relation results.

The hierarchical topic model has originally been evaluated against maximum likelihood estimation by comparison of precision/probability curves [1]. However, note that in the-ory the precision values at probability thresholds can be in-creased (at the expense of recall) also by methods that gener-ally lower relation probabilities without improving the over-all ranking quality. While we include a precision/probability
For more information on TAC KBP see: http://www.nist. gov/tac/tracks/index.html
Note that those annotations are a result of pooling and therefore incomplete and under-estimating precision. How-ever, they allow for a relative comparison of ranking quality. method map gmap p@5 p@10
MLE .253 .142 .263 .232 hier orig .270 .158 .353 * .297 * hier feat .312  X  ** .199  X  ** .347 * .303 * hier orig +burn .286 .181 .379 ** .300 * hier feat +burn .318  X  X  ** .205  X  X  ** .363 ** .321 ** perceptron .330  X  X  ** .210  X  X  ** .379 ** .337 ** Table 1: Ranking quality on TAC KBP extractions.
 Significance (paired t-test) is marked w.r.t. MLE (*p &lt; 0.05, **p &lt; 0.01) and w.r.t. hier orig (  X  p &lt; 0.05,  X  X  p &lt; 0.01). evaluation (Figure 2, left diagram), we focus on comparison of ranking measures (Table 1) and P/R curves (Figure 2, right diagram) to consider recall.

The first baseline we use is the maximum-likelihood es-timator ( MLE ), which scores patterns by the relative fre-quency they occur with a certain relation. For the follow-ing methods, the relative frequency is weighted by the score from the respective models that a certain pattern actually expresses a particular relation. A more advanced base-line is the hierarchical topic model ( hier orig ) as described in [1]. Additional substantial improvements were obtained by averaging counts over the last 10 training iterations, which is known as burn-in ( hier orig +burn ). Performance was improved under most measures by including features (bi-grams) into the hierarchical model ( hier feat +burn ) as de-scribed in Section 2.2. The overall best results were obtained by the perceptron learner ( perceptron ) as described in Sec-tion 3, using bi-gram features.

The two proposed models clearly show the most signifi-cant improvement under the metrics. Mean average preci-sion ( map ), geometric map ( gmap ) and interpolated preci-sion at recall levels provide metrics over the entire ranking quality of all of the evaluation set. Precision at 5 and at 10 ( p@5 , p@10 ) are included for reference  X  however, these met-rics disregard most of the evaluation set and give a coarser picture.
While there is a large body of work on relation extrac-tion, knowledge-base population and distant supervision, an overview over the state of the art can be found in [5].
Feature-based topic models have been used in the context of relation extraction for unsupervised clustering of contexts by [10]. While those clusters have then been used as features in a DS context, the problem of noisy DS candidates has not been tackled by this work. [1] introduced a topic model for improving extraction of candidates, which part of this paper is an extension of so that it is possible to include features.
Employing an at-least-one semantics for relation extrac-tion has been introduced by [11], which use SampleRank [9] and Gibbs-sampling, guided by an at-least-one objective function. [8] use an at-least-one feature to predict aggregate level relation labels. [4] use an at-least-one learner similar to ours to predict the relation of sentence aggregates. In the above cases explicit negative training data is used to increase the probability of the NIL label. Instead of using negative training data, in our work we introduced ranking constraints that  X  together with an at-least-one semantics  X  boost probability estimates for both NIL and the relation labels in order to learn which of the noisy extractions are positive and which negative.
We introduced two feature-based models for increasing the quality of noisy distant supervision relation extraction pat-terns. The first model is an extension of a hierarchical topic model using background, relation and argument-pair spe-cific feature distributions. The second model is a perceptron trained to match an objective function that enforces an at-least-one semantics and high scores for a shared NIL-label. For both algorithms, neither explicit negative data nor the ratio of negatives has to be provided. Both algorithms give significant improvements over a maximum likelihood base-line as well as over a previous topic model without features. Benjamin Roth is a recipient of the Google Europe Fellow-ship in Natural Language Processing, and this research is supported in part by this Google Fellowship. [1] E. Alfonseca, K. Filippova, J.-Y. Delort, and [2] M. Banko, M. J. Cafarella, S. Soderland, [3] A. Haghighi and L. Vanderwende. Exploring content [4] R. Hoffmann, C. Zhang, X. Ling, L. Zettlemoyer, and [5] H. Ji and R. Grishman. Knowledge base population: [6] M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant [7] B. Roth, G. Chrupala, M. Wiegand, M. Singh, and [8] M. Surdeanu, J. Tibshirani, R. Nallapati, and C. D. [9] M. Wick, K. Rohanimanesh, K. Bellare, A. Culotta, [10] L. Yao, A. Haghighi, S. Riedel, and A. McCallum. [11] L. Yao, S. Riedel, and A. McCallum. Collective
