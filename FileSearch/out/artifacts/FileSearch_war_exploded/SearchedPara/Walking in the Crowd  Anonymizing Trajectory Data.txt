 Recently, trajectory data mining has received a lot of at-tention in both the industry and the academic research. In this paper, we study the privacy threats in trajectory data publishing and show that traditional anonymization meth-ods are not applicable for trajectory data due to its chal-lenging properties: high-dimensional, sparse, and sequen tial. Our primary contributions are (1) to propose a new privacy model called LKC -privacy that overcomes these challenges, and (2) to develop an efficient anonymization algorithm to achieve LKC -privacy while preserving the information util-ity for trajectory pattern mining.
 H.2.7 [ Database Administration ]: [Security, integrity, and protection] Algorithms, Performance, Security Privacy, anonymity, trajectory data
In recent years, there has been an explosive growth of location-aware devices such as RFID tags, GPS-based de-vices, cell phones, and PDAs. The use of these devices facil-itates new and exciting location-based applications that c on-sequently generate a huge collection of trajectory data. Re -cent research reveals that these trajectory data can be used for various data analysis purposes such as city traffic contro l, mobility management, urban planning, and location-based service advertisements. Clearly, publication of these tra -jectory data threatens individuals X  privacy since these ra w trajectory data provide location information that identifi es individuals and, potentially, their sensitive informatio n. We Table 2: Anonymous data with L = 2 , K = 2 , C = 50% use an example to illustrate the privacy threats and chal-lenges of publishing trajectory data.

Example 1.1. A hospital wants to release the patient-specific trajectory and health data (Table 1) to a data miner for research purposes. Each record contains a path and some patient-specific information, where the path is a sequence of pairs ( loc i t i ) indicating the patient X  X  visited location loc time t i . For example, ID #2 has a path h f 6  X  c 7  X  e 8 i , meaning that the patient has visited locations f , c , and e at time 6, 7, and 8, respectively. Without loss of generality , we assume that each record contains only one sensitive at-tribute, namely, diagnosis, in this example. We address two types of privacy threats:
Identity linkage : If a path in the table is so specific that not many patients match it, releasing the trajectory data may lead to linking the victim X  X  record and, therefore, her diagnosed disease. Suppose the adversary knows that the data record of a target victim, Alice, is in Table 1, and Alice has visited b 2 and d 3. Alice X  X  record, together with her sensitive value (AIDS in this case), can be uniquely identifi ed because ID #1 is the only record that contains b 2 and d 3. Besides, the adversary can also determine the other visited locations of Alice, such as c 4, f 6 and c 7.

Attribute linkage : If a sensitive value occurs frequently together with some sequence of pairs, then the sensitive in-formation can be inferred from such sequence even though the exact record of the victim cannot be identified. Suppose the adversary knows that Bob has visited b 2 and f 6. Since two out of the three records ( ID #1,7,8) containing b 2 and f 6 have sensitive value AIDS, the adversary can infer that Bob has AIDS with 2 / 3 = 67% confidence.

Many privacy models, such as K -anonymity [7] and its extensions [5][11], have been proposed to thwart privacy threats caused by identity and attribute linkages in the con -text of relational databases. These privacy models are ef-fective for anonymizing relational data, but they are not applicable to trajectory data due to two special challenges . (1) High dimensionality: Traditional K -anonymity re-quires every path to be shared by at least K records. Due to the curse of high dimensionality [2], most of the data have to be suppressed in order to achieve K -anonymity. For ex-ample, to achieve 2-anonymity on the path data in Table 1, all instances of { b 2 , d 3 , c 4 , c 5 } have to be suppressed. (2) Data sparseness: Consider patients in a hospital or passengers in a public transit system. They usually visit only a few locations compared to all available locations. Anonymizing these little-overlapping paths poses a signif -icant challenge for traditional anonymization techniques be-cause it is difficult to identify and group the paths together. Enforcing traditional K -anonymity on high-dimensional and sparse data would render the data useless.
Traditional K -anonymity and its extended privacy mod-els assume that an adversary could potentially use any or even all of the QID attributes as background knowledge to perform identity or attribute linkages. However, in real-l ife privacy attacks, it is very difficult for an adversary to acqui re all the visited locations and timestamps of a victim because it requires non-trivial effort to gather each piece of back-ground knowledge from so many possible locations at differ-ent times. Thus, it is reasonable to assume that the adver-sary X  X  background knowledge is bounded by at most L pairs of ( loc i t i ) that the victim has visited. Based on this assump-tion, we define a new privacy model called LKC -privacy [6] for anonymizing high-dimensional and sparse spatio-tempo ral data.

While protecting privacy is a critical element in data pub-lishing, it is equally important to preserve the utility of t he published data because this is the primary reason for pub-lication. In this paper, we aim at preserving the maximal frequent sequences ( MFS ) because MFS often serves as the information basis for different primitive data mining tasks on sequential data, such as trajectory pattern mining [4].
Our contributions can be summarized as follows. First, based on the practical assumption that an adversary has only limited background knowledge, we formally present a new privacy model, called LKC -privacy, to address the spe-cial challenges of anonymizing high-dimensional, sparse, and sequential trajectory data (Section 3). Second, we present an efficient anonymization algorithm to achieve LKC -privacy while preserving maximal frequent sequences in the anony-mous trajectory data (Section 4). Experimental results are omitted due to space limitation.
Anonymizing High-Dimensional Data. There are some recent works on anonymizing high-dimensional trans-action data [9][12][13]. The methods presented in [9][12] [13] model the adversary X  X  power by a maximum number of known items as background knowledge. This assumption is similar to ours, However, a transaction is a set of items, but a moving object X  X  path is a sequence of visited location-time pairs. Sequential data drastically increases the computa-tional complexity for counting the support counts as com-pared to transaction data because h a  X  b i is different from h b  X  a i . Hence, their proposed models are not applicable to spatio-temporal data.

Anonymizing Moving Objects. Some recent works [1][8][14] address the anonymity of moving objects. In [1], the authors assume that every trajectory is continuous. Thi s assumption is valid for GPS-like devices where the object can be traced all the time, but it does not hold for RFID-based moving objects. The privacy model proposed in [8] assumes that different adversaries have different backgroun d knowledge and thus the data holder needs to have the back-ground knowledge of all the adversaries. In reality, such information is difficult to obtain. Yarovoy et al. [14] con-sider time as a QID attribute. However, there is no fixed set of time for all moving objects, or rather each trajectory has its own set of times as its QID. It is unclear how the data holder can determine the QID attributes for each tra-jectory. Again, none of these works [1][8][14] aim at achiev -ing anonymity and preserving maximal frequent sequences of the trajectories, which is the main theme of our paper.
A trajectory database T is a collection of records in the form h ( loc 1 t 1 )  X  . . .  X  ( loc n t n ) i : s 1 , . . . , s where h ( loc 1 t 1 )  X  . . .  X  ( loc n t n ) i is the path, s the sensitive values, and d i  X  D i are the quasi-identifying (QID) values of an object. Identity and attribute linkages via the QID attributes can be avoided by applying exist-ing anonymization methods for relational data [3][5][10]. In this paper, we focus on eliminating identity and attribute linkages via trajectory data as illustrated in Example 1.1.
As explained in Section 1.1, we assume that the adversary knows at most L pairs of location and timestamp that victim has previously visited. We use q to denote such an a priori known sequence of pairs, where | q |  X  L . T ( q ) denotes a group of records that contains q . A record in T contains q if q is a subsequence of the path in the record. For example in Table 1, ID #1 , 2 , 7 , 8 contains q = h f 6  X  c 7 i , written as T ( q ) = { ID #1 , 2 , 7 , 8 } . Based on background knowledge q , the adversary could launch identity and attribute linkage privacy attacks (Example 1.1). To thwart the identity and attribute linkages, we require that every sequence with a maximum length L in the trajectory database has to be shared by at least a certain number of records, and the ratio of sensitive value(s) in every group cannot be too high. Our privacy model, LKC -privacy , reflects this intuition.
Definition 3.1 ( LKC -privacy). Let L be the maxi-mum length of the background knowledge. Let S be a set of sensitive values. A trajectory database T satisfies LKC -privacy if and only if for any sequence q with | q |  X  L , 1. | T ( q ) |  X  K , where K &gt; 0 is an integer anonymity 2. P ( s | q )  X  C for any s  X  S , where 0 &lt; C  X  1 is a real
LKC -privacy generalizes several traditional privacy mod-els. K -anonymity [7] is a special case of LKC -privacy with C = 100% and L = | d | , where | d | is the number of di-mensions, i.e., number of distinct pairs, in the trajectory database. Confidence bounding [10] is a special case of LKC -privacy with K = 1 and L = | d | . (  X , k )-anonymity [11] is also a special case of LKC -privacy with L = | d | , K = k , and C =  X  . Thus, the data holder can still achieve the traditional models, if needed.
The measure of data utility varies depending on the data mining task to be performed on the published data. In this paper, we aim at preserving the maximal frequent sequences.
Definition 3.2 (Maximal frequent sequence). For a given minimum support threshold K  X  &gt; 0, a sequence x is maximal frequent in a trajectory database T if x is frequent and no super sequence of x is frequent in T .

The set of MFS in T is denoted by U ( T ). Our data utility goal is to preserve as many MFS as possible, i.e., maximize | U ( T ) | , in the anonymous trajectory database.
LKC -privacy can be achieved by performing a sequence of suppressions on selected pairs from T . In this paper, we employ global suppression , meaning that if a pair p is cho-sen to be suppressed, all instances of p in T are suppressed. Global suppression retains exactly the same support counts of the preserved MFS in the anonymous trajectory database as there were in the raw data. In contrast, a local suppres-sion scheme may delete some instances of the chosen pair and, therefore, change the support counts of the preserved MFS. The property of data truthfulness is vital in some data analysis, such as traffic analysis.
 Definition 3.3 (Trajectory Anonymity for MFS).
 Given a trajectory database T , a LKC -privacy requirement, a minimum support threshold K  X  , a set of sensitive values S , the problem of trajectory anonymity for maximal frequent sequences (MFS) is to identify a transformed version of T that satisfies the LKC -privacy requirement while preserving the maximum number of MFS with respect to K  X  .

Finding an optimum solution for LKC -privacy is NP-hard. Thus, we propose a greedy algorithm to efficiently identify a reasonably  X  X ood X  sub-optimal solution.
Given a trajectory database T , our first step is to identify all sequences that violate the given LKC -privacy require-ment. Section 4.1 describes a method to identify violating sequences efficiently. Section 4.2 presents a greedy algorit hm to eliminate the violating sequences with the goal of preser v-ing as many maximal frequent sequences as possible.
An adversary may use any sequence with length not greater than L as background knowledge to launch a linkage attack. Algorithm 1 MVS Generator Input: Raw trajectory database T Input: Thresholds L , K , and C Input: Sensitive values S Output: Minimal violating sequence V ( T ) 2: i = 1; 8: else 10: end if 11: end for 16: end if 17: end for 18: i ++; 19: end while Thus, any non-empty sequence q with | q |  X  L in T is a vi-olating sequence if its group T ( q ) does not satisfy condition 1, condition 2, or both in LKC -privacy in Definition 3.1.
Example 4.1. Let L = 2, K = 2, C = 50%, and S = { AIDS } . In Table 1, a sequence q 1 = h b 2  X  c 4 i is a vio-lating sequence because | T ( q 1 ) | = 1 &lt; K . A sequence q h b 2  X  f 6 i is a violating sequence because P ( AIDS | q 67% &gt; C . However, a sequence q 3 = h b 2  X  c 5  X  f 6  X  c 7 i is not a violating sequence even if | T ( q 3 ) | = 1 &lt; K and P ( AIDS | q 3 ) = 100% &gt; C because | q 3 | &gt; L .
A trajectory database satisfies a given LKC -privacy re-quirement, if all violating sequences with respect to the pr i-vacy requirement are removed, because all possible chan-nels for identity and attribute linkages are eliminated. A naive approach is to first enumerate all possible violating s e-quences and then remove them. This approach is infeasible because of the huge number of violating sequences. Consider a violating sequence q with | T ( q ) | &lt; K . Any super sequence of q , denoted by q  X  X  , in the database T is also a violating sequence because | T ( q  X  X  ) |  X  | T ( q ) | &lt; K .
To overcome this bottleneck of violating sequence enu-meration, our insight is that there exists some  X  X inimal X  violating sequences among the violating sequences, and it is sufficient to achieve LKC -privacy by removing only the minimal violating sequences.

Definition 4.1 (Minimal violating sequence). A vi-olating sequence q is a minimal violating sequence ( MVS ) if every proper subsequence of q is not a violating sequence. Example 4.2. In Table 1, given L = 3, K = 2, C = 50%, S = { AIDS } , the sequence q = h b 2  X  d 3 i is a MVS because h b 2 i and h d 3 i are not violating sequences. The sequence q = h b 2  X  d 3  X  c 4 i is a violating sequence but not a MVS because its subsequence h b 2  X  d 3 i is a violating sequence.
Every violating sequence is either a MVS or it contains a MVS. Thus, if T contains no MVS, then T contains no violating sequences. Algorithm 1 presents a method to efficiently generate all MVS. Line 1 puts all the size-1 sequences, i.e., all distinct pairs, as candidates X 1 of MVS. Line 4 scans T once to compute | T ( q ) | and P ( s | q ) for each sequence q  X  X each sensitive value s  X  S . If the sequence q violates the LKC -privacy requirement in Line 6, then we add q to the MVS set V i (Line 7); otherwise, add q to the non-violating sequence set W i (Line 9) for generating the next candidate set X i +1 , which is a self-join of W i (Line 12). Two sequences q x = h ( loc x 1 t x 1 )  X  . . .  X  ( loc x i t x i ) i and q h ( loc x 1 t x 1 )  X  . . .  X  ( loc x i t x i )  X  ( loc y a candidate q from X i +1 if q is a super sequence of any sequence in V i because any proper subsequence of a MVS cannot be a violating sequence. The set of MVS, denoted by V ( T ), is the union of all V i .

Example 4.3. Consider Table 1 with L = 2, K = 2, C = After scanning T , we divide X 1 into V 1 =  X  and W 1 = { b 2 , d 3 , c 4 , c 5 , f 6 , c 7 , e 8 } . Next, from W V generate X 3 because L = 2.
We propose a greedy algorithm to transform the raw tra-jectory database T to an anonymous table T  X  with respect to a given LKC -privacy requirement by a sequence of sup-pressions. In each iteration, the algorithm selects a pair p for suppression based on a greedy selection function. In general, a suppression on a pair p in T increases privacy because it removes minimal violating sequences (MVS), and decreases data utility because it eliminates maximal frequ ent sequences (MFS) in T . Therefore, we define the greedy func-tion, Score ( p ), to select a suppression on a pair p that max-imizes the number of MVS removed but minimizes the num-ber of MFS removed in T . Score ( p ) is defined as follows: where P rivGain ( p ) and UtilityLoss ( p ) are the number of minimal violating sequence (MVS) and the number of max-imal frequent sequence (MFS) containing the pair p , respec-tively. A pair p may not belong to any MFS, resulting in | UtilityLoss ( p ) | = 0. To avoid dividing by zero, we add 1 to the denominator. The pair p with the highest Score ( p ) is called the winner pair, denoted by w .

Table 3 shows the initial Score ( p ) of every candidate pair for Table 1. Initially, c4 is suppressed since it has the high -est score. After suppressing c4, we update the score of the remaining candidate pairs (Table 4). In the next iteration, b 2 is suppressed and thus all the remaining MVS are re-moved. Table 2 shows the resulting anonymized table T  X  for (2 , 2 , 50%)-privacy. Due to space limitation, we do not elab-orate how we efficiently calculate and update the Score ( p ) of the candidate pairs.
We proposed a new LKC -privacy model based on the as-sumption that an adversary has limited background knowl-edge about the victim. We also presented an efficient algo-rithm for achieving LKC -privacy with the goal of preserv-ing maximal frequent sequences, which serves as the basis of many data mining tasks on sequential data.
The research is supported in part by Discovery Grants (356065-2008) and Canada Graduate Scholarship from the Natural Sciences and Engineering Research Council of Canad a. [1] O. Abul, F. Bonchi, and M. Nanni. Never walk alone: [2] C. C. Aggarwal. On k -anonymity and the curse of [3] B. C. M. Fung, K. Wang, and P. S. Yu. Anonymizing [4] F. Giannotti, M. Nanni, D. Pedreschi, and F. Pinelli. [5] A. Machanavajjhala, D. Kifer, J. Gehrke, and [6] N. Mohammed, B. C. M. Fung, P. C. K. Hung, and [7] P. Samarati and L. Sweeney. Generalizing data to [8] M. Terrovitis and N. Mamoulis. Privacy preservation [9] M. Terrovitis, N. Mamoulis, and P. Kalnis.
 [10] K. Wang, B. C. M. Fung, and P. S. Yu. Handicapping [11] R. C. W. Wong, J. Li., A. W. C. Fu, and K. Wang. [12] Y. Xu, B. C. M. Fung, K. Wang, A. W. C. Fu, and [13] Y. Xu, K. Wang, A. W. C. Fu, and P. S. Yu.
 [14] R. Yarovoy, F. Bonchi, L. V. S. Lakshmanan, and
