 Text clustering methods can be used to structure large sets of text or hypertext documents. The well-known methods of text clustering, however, do not really address the special problems of text clustering: very high dimensionality of the data, very large size of the databases and understandability of the cluster description. In this paper, we introduce a novel approach which uses frequent item (term) sets for text clustering. Such frequent sets can be efficiently discovered using algorithms for association rule mining. To cluster based on frequent term sets, we measure the mutual overlap of frequent sets with respect to the sets of supporting documents. We present two algorithms for frequent term-based text clustering, FTC which creates fiat clusterings and HFTC for hierarchical clustering. An experimental evaluation on classical text documents as well as on web documents demonstrates that the proposed algorithms obtain elusterings of comparable quality significantly more efficiently than state-of-the-art text clustering algorithms. Furthermore, our methods provide an understandable description of the discovered clusters by their frequent term sets. Clustering, Frequent Item Sets, Text Documents. The world wide web continues to grow at an amazing speed. On the other hand, there is also a quickly growing number of text and hypertext documents managed in organizational intranets, representing the accumulated knowledge of organizations that becomes more and more important for their success in today's information society. Due to the huge size, high dynamics, and large diversity of the web and of organizational intranets, it has become a very challenging task to find the truly relevant content for some user or purpose. For example, the slandard web search engines have low precision, since typically a large number of not made or distributed for profit or commercial advantage and that requires prior specific permission and/or a fee. SIGKDD 02 Edmonton, Alberta, Canada 
Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. irrelevant web pages is returned together with a small number of relevant pages. This phenomenon is mainly due to the fact that keywords specified by the user may occur in different contexts, consider for example the term "cluster". Consequently, a web his limited arnounl: of time, processes only the first few results. Thus, a lot of truely relevant information hidden in the long result lists will never be' discovered. Text clustering methods can be applied to structure the large result set such that they can be interactively browsed by the user. Effective knowledge management is a major competitive advantage in today's information society. To structure large sets of hypertexts available in a company's intranet, again methods of text clustering can be used. Compared to previous applications of clustering, three major challenges must be addressed for clustering (hyper)text databases (see also [1]):  X  Very high dimensionality of the data (~ 10,000 terms /  X  Very large size of the databases (in particular, of the  X  Understandable description of the clusters: the cluster A lot of different text clustering algorithms have been proposed in the literature, including Scatter/Gather [2], SuffixTree Clustering [3] and bisecting k-means [4]. A recent comparison [4] demonstrates that bisecting k-means outperforms the other well-known techniques, in particular hierarchical clustering algorithms, with respect to clustering quality. Furthermore, this algorithm is efficient. However, bisecting k-means like most of the other algorithms does not really address the above mentioned problems of text clustering: it clusters the full high-dimensional vector space of term frequency vectors and the discovered means of the clusters do not provide an understandable description of the documents grouped in some cluster. algorithm. Many variants of the k-means algorithm have been proposed for the purpose of text clustering, e.g. [7], in particular to determine a good initial clustering. A recent study [4] has compared partitioning and hierarchical methods of text clustering on a broad variety of test datasets. It concludes that k-means clearly outperforms the hierarchical methods with respect to clustering quality. Note that k-means is also much more efficient than hierarchical clustering algorithms. Furthermore, a variant of k-means called bisecting k-means is introduced, which yields even better performance. Bisecting k-means uses k-means to partition the dataset into two clusters. Then it keeps partitioning the currently largest cluster into two clusters, again using k-means, until a total number of k clusters has been discovered, The above methods of text clustering algorithms do not really address the special challenges of text clustering: they cluster the full high-dimensional vector-space and the centroids / means of the discovered clusters do not provide an understandable description of the clusters. This has motivated the development of new special text clustering methods which are not based on the vector space model. SuffixTree Clustering [3] is a first method following this approach. Its idea is to form clusters of documents sharing common terms or phrases (multi-word terms). Basic clusters are sets of documents containing a single given term. A cluster graph is built with nodes representing basic clusters and edges representing an overlap of at least 50% between the two associated basic clusters. A cluster is defined as a connected component in this cluster graph. The drawback of SuffixTree Clustering is that, while two directly neighboring basic clusters in the graph must be similar, two distant nodes (basic clusters) within a connected component do not have to be similar at all. Unfortunately, Suffix Tree Clustering has not been evaluated on standard test data sets so that its performance can hardly be compared with other methods. Frequent item sets form the basis of association rule mining. Exploiting the monotonicity property of frequent item sets (each subset of a frequent item set is also frequent) and using data structures supporting the support counting, the set of all frequent item sets can be efficiently determined even for large databases. Many different algorithms have been developed for that task, including Apriori [5]. See [8] for an overview on association rule mining. Frequent item sets can also be used for the task of classification. [9] introduces a general method of building an effective classifier from the frequent item sets of a database. [10] presents a modification of this approach for the purpose of text classification. A frequent item-based approach of clustering is promising because it provides a natural way of reducing the large dimensionality of the document vector space. Since we are dealing not with transactions but with documents, we will use the notion of term sets instead of item sets. A term is any preprocessed word within a document, and a document can be considered as a set of terms occurring in that document at least once. The key idea is not to cluster the high-dimensional vector space, but to consider only the low-dimensional frequent term sets as cluster candidates. A well-selected subset of the set of all frequent term sets can be considered as a clustering. Strictly speaking, a frequent term set is not a duster (candidate) but only the description of a cluster (candidate). The corresponding cluster itself consists of the set of documents containing all terms of the frequent term set. Unlike in the case of classification, there are no class labels to guide the selection of such a subset from the set of all frequent term sets. Instead, we propose to use the mutual overlap of the frequent term sets with respect to their sets of supporting documents (the clusters) to determine a clustering. The rationale behind this approach is that a small overlap of the clusters will result in a small classification error, when the clustering is later used for classifying new documents. In this section, we introduce the necessary definitions and present two algorithms for frequent term-based text clustering. 
Let D = {D I ..... D~ } be a database of text (hypertext) documents and T be the set of all terms occurring in the documents of D. Each document D/is represented by the set of terms occurring in 
Dj, i.e. Dj ~T. Let minsupp be a real number, 0 _&lt; minsupp _&lt; 1. For any set of terms S,S ~ T, let coy(S) denote the cover of S, the set of all documents containing all terms of S, i,e. the set of all documents supporting S. More precisely, cov(S) = {Dj ~ D [ S ~ Dj }-respect to minsupp, the set of all term sets contained in at least minsupp of the D documents, i.e. F = IF, ~ T II coy(F,)I &gt; _ minsupp. I D l} A frequent term set of cardinality k is called a frequent k-term set. 
The cover of each element Fi of F can be regarded as a cluster (candidate). 
D such that each document of D is contained in at least one of the sets (dusters). The clusters of a clustering may or may not overlap. We define a clustering description as a subset CD of F which covers the whole database, i.e. a clustering description 
CD = {F i I i~ 1} has to satisfy the condition Ucov(F,) = D-
We want to determine dustedngs with a minimum overlap of the dusters. For an efficiently to calculate measure of the overlap of a given cluster (description) Fi with the union of the other duster (description)s, we use the number of frequent term sets supported by the documents covered by Fi,. Let f~ denote the number of all frequent term sets supported by document D/, i.e. where II denotes the cardinality of some set and R denotes a subset of F, the subset of remaining .frequent term sets, i.e. the difference of F and the set of the already selected frequent term sets. This definition is motivated by the fact that our clustering algorithm will be a greedy algorithm selecting one next cluster (description, i.e. frequent term set) at a time from the set R of the smaller the fj values of its documents are. Ideally, all its duster candidates. Thus, we define the standard overlap of a cluster Ci, denoted by SO(Ci), as the average fj value (-1) of a cluster, i.e. SO(C~) = D~C, 
The standard overlap is easy to calculate but it suffers from the following effect. Due to the monotonicity property of frequent term sets (each subset of a frequent term set is also frequent), each document supporting one term set of size m also supports at least duster candidate described by many terms tends to have a much larger standard overlap than a cluster candidate described by only a few terms. Consequently, the standard overlap favors frequent term sets consisting of a very small number of terms. 
An alternative definition of the overlap, based on the entropy, minimizes this effect. The entropy measures the distribution of the documents supporting some cluster over all the remaining duster candidates. While fj measures the distribution of document Dj over the cluster candidates (frequent term sets), pj fJ represents the probability that document Dj belongs to one only I cluster candidate. On the other hand, pj becomes very small for large~ values. We define the entropy overlap of cluster 
Ci, denoted by EO(Ci), as the distribution of the documents of cluster Ci over all the remaining cluster candidates, i.e. 
The entropy overlap becomes 0, if all documents Dj of Ci do not support any oth~x frequent term set ~ = 1), and it increases monotonically with increasing ~ values. As our experimental evaluation will demonstrate, the entropy overlap yields cluster descriptions with significantly more terms than the standard overlap which results in a higher clustering quality. 
We have defined a (flat) clustering as a subset of the set of all frequent term sets, that covers the whole database. To discover a clustering with a minimum overlap of the clusters, we follow a greedy approach. This approach is motivated by the inherent complexity of the frequent term-based clustering problem: the number of all subsets of F is O(21rl), and, therefore, an exhaustive searcla is prohibitive. In this section, we present two greedy algorithms for frequent term-based clustering.  X  Algorithm FTC (Frequent Term-based Clustering): * Algorithm HFTC (Hierarchical Frequent Term-based 
Algorithm FTC works in a bottom-up fashion. Starting with an empty set, it continues selecting one more element (one cluster description) from the set of remaining frequent term sets until the entire database is contained in the cover of the set of all chosen frequent term sets (the clustering). In each step, FTC selects the remaining frequent term set with a cover having the minimum overlap with the other cluster candidates. Note that the documents covered by the selected frequent term set are removed from the database D and, in the next iteration, the overlap for all remaining cluster candidates is recalculated with respect to the reduced database. Figure 1 presents algorithm FTC in pseudo-code. 
DetermineFrequentTermsets is any efficient algorithm for finding the set of all frequent term sets in database D with respect to a minimum support of minsup. FTC(database D, float minsup) 
SelectedTermSets:= {}; RemainingTermSets: = 
DetermineFrequentTermsets(D, minsup); while Icov(SelectedTermSets)] # n do return SelectedTermSets and the cover 
Figure 2 illustrates the first step of algorithm Frc on a sample database consisting of 16 documents. In this step, the cluster described by { sun, beach, fun} is selected because of its minimum (entropy) overlap, and the documents Ds, D10, DH, Dis are removed from the database as well as from the remaining cluster candidates. frequent term set cluster Candidate {sun} {Dl, D2, D4, Ds, D6, D8, {fun} {D1, D3, D4, D6, DT, Ds, (beach} {D2, D7, Ds, D9, Dlo, Dl2, {surf} {Dl, D2, D6, DT, Dl0, Du, {sun, fun} {Di, D4, D6, Ds, Dl0, Dll, {fun, surf} {D1, D6, D7, Dl0, DH, {sun, beach} {D2, Ds, D9, Dio, DII, {sun, surf} {Db D2, D6, DIo, Dit} {fun, beach} {DT, Ds, D1o, Dl4, DLs} {beach, surf} {D2, D7, Dio, Dl2, DI4} {sun, fun, surf} {Dx, D6, Dl0, DH } {sun, beach, fun} {Ds, Dlo, Dn, Dis} 
Note that algorithm FTC returns a clustering description and a clustering. This clustering is non-overlapping. However, algorithm FTC can easily be modified to discover an overlapping clustering by not removing the documents covered by the selected frequent term set from the database D. 
Due to the monotonicity property, the frequent term sets form a lattice structure: all 1-subsets of frequent 2-sets are also frequent, all 2-subsets of frequent 3-sets are also frequent etc.. This property can be exploited to discover a hierarchical frequent term-based clustering. Algorithm HFTC (Hierarchical Frequent 
Term-based Clustering) is based on the flat algorithm FTC and applies it in a hierarchical way as follows:  X  The algorithm FTC is applied not on the whole set F of * The resulting clusters are further partitioned applying 
Algorithm HFTC discovers a hierarchical clustering with an empty term set in the root (covering the entire database), using frequent l-term sets on the first level, frequent 2-term sets on the second level, etc. HFTC stops adding another level to the hierarchical clustering, when there are no frequent term sets for the next level. Furthermore, the successors of some cluster do not have to cover the whole cluster if that is impossible due to a lack of more frequent term sets of this level. 
When using the original Frc algorithm, we generate a non-overlapping hierarchical clustering. But when modifying FTC so that it discovers an overlapping flat clustering, we can also discover an overlapping hierarchical clustering. Our experimental evaluation has shown that overlapping hierarchical clusterings are clearly superior with respect to clustering quality, therefore HFI'C creates a hierarchical overlapping clustering. Figure 3 depicts a part of a hierarchical clustering for our sample database. It consists of three levels since the database does not contain any frequent 4-term sets with respect to the chosen minsup value. The edges denote subset-relationships between the connected clusters. {D1, D2, D4, Ds, {Dl, D3, D4, D6, {D2, D7, D8, D6, D8, D9, Dlo, DT, Ds, Dlo, DH, D9, Dio, D12, 
Dii, D13, Di5} D14, Dis, D16} Dl3,Dl4, D15} {Dr, D4, D6, Ds, {D2, Ds, D9 ....... 
Dlo, DI1, D15} Dlo, Dii, D15} {sun, fun, surf} {sun, beach, fun} {Di, D6, D10, DH} {Ds, D10, DH, D15} 
FTC and HFTC have been empirically evaluated on real text corpora in comparison with two state of the art text clustering algorithms, bisecting k-means [4] and k-secting k-means [7]. The experiments were performed on a Pentium III PC with 400 MHz clock speed and 256 MB of main memory. We chose java 1.2 as the programming language because it allows fast and flexible development. For a fair comparison, we implemented all algorithms, FTC, HFrc and the k-means variants ourselves and used identical classes whenever possible. For generating the frequent term sets, we used a public domain implementation of the basic Apriori algorithm [11]. In section 4.1, we describe the test data sets. Sections 4.2 and 4.3 report the main experimental results for FTC and HFTC, respectively. 
To test and compare cluster algorithms pre-classified sets of documents are needed. We used three different data sets which are widely used in other publications and reflect the conditions in a broad range of real life applications. These data sets are:  X  Classic: Classic3 containes 1400 CRANFIELD documents from aeronautical system papers, 1033 MEDLINE documents from medical journals and 1460 CISI documents from infommtion retrieval papers. We obtained the data set Reuters: This test data set consists of 21578 articles from the 
Reuters news service in the year 87 (http://kdd.ics.uci.edu/databases/reuters21578/reuters21578. html). Typically, only a subset of 8654 articles, uniquely assigned to exactly one of these classes, is used for evaluation and we follow this procedure.  X  WAP: During the WebACE project [12], 1560 web pages from the Yahoo! subject hierarchy were collected and classified into 20 different classes. We obtained this dataset from one of the authors, George Karypis. A summary description of these data sets is given in Table 1. 
To evaluate the quality of a flat, un-nested clustering we adopted the commonly used entropy measure. Let C be some clustering, classes of a database. The entropy of the clustering C, denoted by 
E(C), is defined as where nj means the size of cluster Cj and p# the probability that a member of Cluster Cj belongs to class Kj. The entropy measures the pureness of the clusters with respect to the classes and returns values in the interval [0...In(IKD]. The smaller the entropy, the purer are the produced clusters. 
In all experiments vrc yields a cluster quality comparable to that of bisecting and 9-secting k-means. However, FTC is significantly more efficient than its competitors on all test data sets. For example, FTC outperforms the best competitor by a factor of 6 on data (for 50 clusters) and by a factor of almost 2 (and factor 4 compared to bisecting k-means) for the WAP data (for 20 dusters). Furthermore, Frc automatically generates a description for the generated dusters while for the k-means type algorithms this has to be done in a extra postprocessing step. We performed a set of experiments to analyze the scalability of 
FTC with respect to the database size. Figure 5 depicts the runtime of Frc with respect to the number of documents on the 
Reuters database. Even for the largest number of 7000 documents improvement compared to other methods. 
To evaluate the clustering quality for hierarchical clustering algorithms, typically the F-Measure is used [7], [4]. We treat each were the relevant set of documents for a query. We then calculate the recall and precision of that duster for each given class. More specifically, for cluster Cj and class Ki, the recall R and precision P are defined as: R(K~, C j) = ned / I K~ I, and 
P(Ki,Cs) = ni.s I ICs I' respectively, where n~ is the number of members of class Ki in cluster Cj. The F-Measure of cluster C i and class Ki, denoted by F(Ki, C s ), is then defined as F(Ki, C 1) = For a hierarchical clustering C, the F-Measure of any class is the maximum value it attains at any node in the tree and an overall value for the F-Measure is computed by taking the weighted average of all values for the F-Measure as follows: 
F(C~ = The F-Measure values are in the interval [0..1] and larger F-Measure values indicate higher clustering quality. We compared HFTC with the hierarchical versions of bisecting k-means and 9-secting k-means. The hierarchical algorithms do not have the number of clusters as a parameter, and the F-Measure does not have a similar bias as the entropy. Therefore, we obtain a single clustering of each test database for each of the competitors. Table 2 reports the resulting F-Measure values. We observe that HFTC achieves similar values of the F-Measure as 9-secting k-means. Bisecting k-means yields significantly better F-Measure values than HI~C on all data sets. Note that the F-Measure forms some average of the precision and recall which favors non-overlapping clustefings. The two variants of k-means generate such non-overlapping dustefings, but HFTC discovers overlapping clusterings. However, overlapping clusters occur naturally in many applications such as in the Yahoo! directory. In this paper, we presented a novel approach for text clustering. We introduced the algorithms Frc and HFTC for flat and hierarchical frequent term-based text clustering. Our experimental evaluation on real text and hypertext data sets demonstrated that FTC yields a duster quality comparable to that of state-of-the-art text clustering algorithms. However, FTC was significantly more efficient than its competitors on all test data sets. Furthermore, FTC automatically generates a natural description for the generated clusters by their frequent term sets. HFTC generates hierarchical clusterings which are easier to browse and more comprehendible than hierarchies discovered by the comparison partners. Finally, we would like to outline a few directions for future research. We already mentioned that the integration of a more advanced algorithm for the generation of frequent term sets could significantly speed-up FTC and HFrc. FTC is a greedy algorithm. Other paradigms such as dynamic programming might also be adopted[ to solve the frequent term-based clustering problem and should be explored. Hierachical clusterings are of special interest for many applications. However, the well-known measures of hierarchical clustering quality do not adequately capture the quality from a user's perspective. New methods should be developed for this purpose. The proposed clustering algorithms have promising applications such as a front end of a web search engine. Due to the similarity of text data and transaction data, our methods can also be used on transaction data, e.g. for market segmentation. We plan to investigate these applications in the future. [1] Chakrabarti S.: Data mining for hypertext: A tutorial survey, 
ACM SIGKDD Explorations, 2000, pp. 1-11. [2] Cutting D.R., Karger D.R., Pedersen J.O., Tukey LW.: Scatter / Gather: A Cluster-based Approach to Browsing 
Large Document Collection, Proc. ACM SIGIR 92, 1992, pp.318-329. [3] Zamir O., Etzioni O.: Web Document Clustering: A 
Feasability Demonstration, Proc. ACM SIGIR 98, 1998, pp. 46-54. [4] Steinbach M., Karypis G., Kumar V.: A Comparison of Document Clustering Techniques, Proc. TextMining 
Workshop, KDD 2000, 2000. [5] Agrawal, R., Srikant R.: Fast Algorithms for Mining Association Rules in Large Databases, Proc. VLDB 94, 
Santiago de Chile, Chile, 1994, pp. 487-499. [6] Kaufrnan L., Rousseeuw P.J.: Finding Groups in Data: An 
Introduction to Cluster Analysis, John Wiley &amp; Sons, 1990. [7] Larsen B., Aone Ch.: Fast and Effective Text Mining Using 
Linear-time Document Clustering, Proc. KDD 99, 1999, pp. 16-22. [8] Hipp J., Guntzer U., Nakhaeizadeh G.: Algorithms for Association Rule Mining -a General Survey and 
Comparison, ACM SIGKDD Explorations, Vol.2, 2000, pp. 58-64. [9] Liu B., Hsu W., Ma Y.: Integrating Classification and 
Association Rule Mining, Proc. KDD 98, pp. 80-86. [10]Zaiane O., Antonie M.-L.: Classifying Text Documents by Associating Terms with Text Categories, Proc. Australasian 
Database Conference, 2002, pp. 215-222. [ll]Yibin S.: An implementation of the Apriori algorithm, http://www.cs.ure~ina.ca/-dbd/cs83 l/notes/itemsets/dic.java, 2000. [12] Han E.-H., Boly D., Gini M., Gross R., Hastings K., Karypis G., Kurnar V., Mobasher B., and Moore J., WebACE: A Web Agent for Document Categorization and Exploration, 
Proc. Agents 98 
