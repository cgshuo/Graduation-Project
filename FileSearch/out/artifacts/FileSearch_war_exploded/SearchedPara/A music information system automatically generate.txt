 1. Introduction and context
Over the past few years, digital music distribution via the World Wide Web has seen a tremendous increase. As a result, music-related information beyond the pure digital music file (musical meta-data) is becoming more and more important as users of online music stores nowadays expect to be offered such additional information. Moreover, digital music distributors are in need of such additional value that represents a decisive advantage over their competitors.

Also music information systems, i.e., systems primarily focusing on providing information about music, not on selling mu-sic, typically offer multimodal information about music artists, raphies, song samples, or images of album covers). In common music information systems, such information is usually collected paper describes methods for building such a system by automatically extracting the required information from the Web at large.
To this end, various techniques to estimate relations between artists, to determine descriptive terms, to extract band members and instrumentation, and to find images of album covers were elaborated, evaluated, refined, and aggregated. Automatically retrieving information about music artists is an important task in music information retrieval (MIR), cf.
Downie (2003) . It permits, for example, enriching music players with meta-information ( Schedl, Pohle, Knees, &amp; Widmer, 2006c ), automatically tagging of artists ( Eck, Bertin-Mahieux, &amp; Lamere, 2007 ), automatic biography generation ( Alani et al., 2003 ), developing user interfaces to browse music collections by more sophisticated means than the textual browsing  X 
Goto, 2007 ), or defining similarity measures between artists. Music similarity measures can then be used, for example, to create relationship networks ( Cano &amp; Koppenberger, 2004 ), for automatic playlist generation ( Aucouturier &amp; Pachet, 2002; Pohle, Knees, Schedl, Pampalk, &amp; Widmer, 2007 ), or to build music recommender systems ( Celma &amp; Lamere, 2007; Zadel &amp; Fujinaga, 2004 ) or music search engines ( Knees, Pohle, Schedl, &amp; Widmer, 2007 ).

In the following, an overview of existing Web mining techniques for MIR is given in Section 2 . Section 3 briefly presents the methods developed and refined by the authors, together with evaluation results. Section 4 describes the application of the techniques from Section 3 for creating the Automatically Generated Music Information System (AGMIS), a system providing information on more than 600,000 music artists. Finally, in Section 5 , conclusions are drawn, and directions for future work are pointed out. 2. Related work
Related work mainly consists of methods to derive similarities between music artists and attribute descriptive terms to an artist, which is also known as tagging . Traditionally, similarities between songs or artists are calculated on some kind of musically relevant features extracted form the audio signal. Such features usually aim at capturing rhythmic or timbral aspects of music. Rhythm is typically described by some sort of beat histogram , e.g., Pampalk, Rauber, and Merkl (2002) and Dixon, Gouyon, and Widmer (2004 et al.) , whereas timbral aspects are usually approximated by Mel Frequency Cepstral
Coefficients (MFCCs), e.g., Aucouturier, Pachet, and Sandler (2005) and Mandel and Ellis (2005) . However, such audio signal-based similarity measures cannot take into account aspects like the cultural context of an artist, the semantics of the lyrics of a song, or the emotional impact of a song on its listener. In fact, the performance of such purely audio-based measures seems to be limited by a  X  X  X lass ceiling X , cf. Aucouturier and Pachet (2004) .

Overcoming this limitation requires alternative methods, most of which have in common the participation of lots of people to form a large information resource. Like typical Web 2.0 applications, such methods benefit from the wisdom of the crowd.
The respective data is hence often called cultural features or community meta-data . Probably the most prominent example of such features are those gained in a collaborative tagging process. Lamere (2008) gives a comprehensive overview of the power of social tags in the music domain, shows possible applications, but also outlines shortcomings of collaborative tag-ging systems. Celma (2008) laboriously analyzed and compared different tagging approaches for music, especially focusing on their use for music recommendation and taking into account the long tail of largely unknown artists.

Cultural features were, however, already used in MIR before the Web 2.0-era and the emergence of folksonomies. Early approaches inferring music similarity from sources other than the audio signal use, e.g., co-occurrences of artists or tracks in radio station playlists and compilation CDs ( Pachet, Westerman, &amp; Laigre, 2001 ) or in arbitrary lists extracted from Web pages ( Cohen &amp; Fan, 2000 ). Other researchers extracted different term sets from artist-related Web pages and built individ-ual term profiles for each artist ( Ellis, Whitman, Berenzweig, &amp; Lawrence, 2002; Knees, Pampalk, &amp; Widmer, 2004; Whitman artist level since there is usually too little data available on the level of individual songs. The most promising approach to transcend these limitations is combining multiple features extracted from different sources. For example, a method that en-riches Web-based with audio-based features to create term profiles at the track level is proposed in Knees, Pohle, et al. (2007) . The authors present a search engine to retrieve music by textual queries, like  X  X  X ock music with great riffs X . Pohle et al. (2007) present an approach to automatic playlist generation that approximates the solution to a Traveling Salesman Problem on signal-based distances, but uses Web-based similarities to direct the search heuristics.

As for determining descriptive terms for an artist, such as instruments, genres, styles, moods, emotions, or geographic locations, Pampalk, Flexer, and Widmer (2005) use a self-assembled dictionary and apply different term weighting tech-niques on artist-related Web pages to assign terms to sets of artists and cluster them in a hierarchical manner. The term weighting functions analyzed were based on document frequency (DF), term frequency (TF), and term frequency inverse document frequency (TF IDF) variations. The conducted experiments showed that considering only the terms in the dictio-nary outperforms using the unpruned, complete set of terms extracted from the Web pages. Geleijnse and Korst (2006) and
Schedl et al. (2006c) independently present an approach to artist tagging that estimates the conditional probability for the artist name under consideration to be found on a Web page containing a specific descriptive term and the probability for the descriptive term to occur on a Web page known to mention the artist name. The calculated probabilities are used to predict particularly try to categorize artists according to their genre, which seems reasonable as genre names are also among the most frequently applied tags in common music information systems like last.fm ( Geleijnse, Schedl, &amp; Knees, 2007 ). Another category of tagging approaches make use of last.fm tags and distill certain kinds of information. For example, Hu, Bay, and Downie (2007) use a part-of-speech (POS) tagger to search last.fm tags for adjectives that describe the mood of a song. Eck et al. (2007) use the machine learning algorithm AdaBoost to learn relations between acoustic features and last.fm tags.
A recent approach to gathering tags is the so-called ESP games ( von Ahn &amp; Dabbish, 2004 ). These games provide some form of incentive 2 to the human player to solve problems that are hard to solve for computers, e.g., capturing emotions evoked when listening to a song. Turnbull, Liu, Barrington, and Lanckriet (2007), Mandel and Ellis (2007) , and Law, von Ahn,
Dannenberg, and Crawford (2007) present such game-style approaches that provide a fun way to gather musical annotations. 3. Mining the Web for music artist-related information
All methods proposed here rely on the availability of artist-related data on the Web. The authors X  principal approach to extracting such data is the following. Given only a list of artist names, we first query a search engine up to 100 top-ranked search results for each artist. The content available at these URLs is extracted and stored for further pro-cessing. To overcome the problem of artist names that equal common speech words and to direct the search towards the desired information, we use task-specific query schemes like "band name" and instrumentation. We do not account for multilingual pages by varying the language of the additional keywords (e.g., "music" , "Musik" , "musique" , "musica" ) as this would considerably increase the number of queries issued to the search engine. It has to be kept in mind, however, that restricting the search space to English pages might yield undiscovered pages which are nevertheless relevant to the artist. In any case, this approach relies on the ranking algorithm of the search engine.
Depending on the task to solve, either a document-level inverted index or a word-level index ( Zobel &amp; Moffat, 2006 ) is then created from the retrieved Web pages. In some cases, especially when it comes to artist tagging, a special dictionary of musi-cally relevant terms is used for indexing. After having indexed the Web pages, we gain artist-related information of various kinds as described in the following.

As an alternative approach to the use of a search engine for Web page selection, we could use a focused crawler ( Chakrabarti, van den Berg, &amp; Dom, 1999 ) trained to retrieve pages from the music domain. We are currently assessing this alternative as it would avoid relying on commercial search engines and would allow us to build a corpus specific to the music domain. On the other hand, companies like Google offer a huge corpus which can be accessed very efficiently. Thus, we still have to compare these two strategies (directed search using a search engine vs. focused crawling) and assess their perfor-mance in depth, which will be part of future work. 3.1. Relations between artists 3.1.1. Similarity Relations
A key concept in music information retrieval and crucial part of any music information system is similarity relations between artists. To model such relations, we propose an approach that is based on co-occurrence analysis ( Schedl, Knees, the artist name i occurs on a Web page that was returned as response to the search query for the artist name j and vice versa.
The formal definition of the similarity measure is given in Formula (1) , where I represents the set of Web pages returned for artist i and df i , J is the document frequency of the artist name i calculated on the set of Web pages returned for artist j . similar artists, i.e., building a recommender system. Evaluation in an artist-to-genre classification task using a k-nearest neighbor classifier on a set of 224 artists from 14 genres yielded accuracy values of about 85% averaged over all genres, cf.
Schedl et al. (2005a) . 3.1.2. Prototypicality relations
Co-occurrences of artist names on Web pages (together with genre information) can also be used to derive information the one-sided, co-occurrence-based similarity measure is exploited as explained below. Taking a look at Formula (1) again retrieved for another artist, it is obvious that, in general, vantages, the most important of which is that they do not allow to induce a metric in the feature space. Moreover, they pro-duce unintuitive and hard to understand visualizations when using them to build visual browsing applications based on clustering, like the nepTune interface ( Knees, Schedl, Pohle, &amp; Widmer, 2007 ). However, the asymmetry can also be benefi-cially exploited for deriving artist popularity or prototypicality of an artist for a certain genre (or any other categorical aspect). Taking into account the asymmetry of the co-occurrence-based similarity measure, the main idea behind our ap-proach is that it is more likely to find the name of a well-known and representative artist for a genre on many Web pages about a lesser known artist, e.g., a newcomer band, than vice versa. To formalize this idea, we developed an approach that is based on the backlink/forward link-ratio of two artists i and j from the same genre, where a backlink of i from j is defined as any occurrence of artist i on a Web page that is known to contain artist j , whereas a forward link of i to j is defined as any occurrence of j on a Web page known to mention i . Relating the number of forward links to the number of backlinks for each pair of artists from the same genre, a ranking of the artist prototypicality for the genre under consideration is obtained. More precisely, we count the number of forward links and backlinks on the document frequency-level, i.e., all occurrences of artist page. To alleviate the problem of artist names being highly ranked due to their resemblance to common speech words, df gives the number of Web pages retrieved for artist i that also mention artist j . This number hence represents a document function kk shifts all values to the positive range and maps them to [0,1].
We conducted an evaluation experiment using a set of 1995 artists from 9 genres extracted from All Music Guide . As ground truth we used the so-called  X  X  X iers X  that reflect the importance, quality, and relevance of an artist to the respective genre, judged by All Music Guide  X  X  editors, cf. amgabout (2007) . Calculating Spearman X  X  rank-order correlation , e.g., Sheskin (2004) , between the ranking given by Formula (2) and the ranking given by All Music Guide  X  X  tiers, revealed an average cor-relation coefficient of 0.38 over all genres. More details on the evaluation can be found in Schedl, Knees, and Widmer (2006) .
To give an example of how the penalization term influences the ranking, we first consider the band  X  X  X ool X , which is clas-sified as  X  X  X eavy Metal X  by All Music Guide  X  X  editors. 5 (only superseded by  X  X  X eath X  and  X  X  X urope X ), which we and also All Music Guide  X  X  editors believe does not properly reflect the band X  X  true importance for the genre, even though  X  X  X ool X  is certainly no unknown band to the metal aficionado. However, when multiplying the ratio with the penalization term, which is 0.1578 for  X  X  X ool X  (according to Formula (3) ), the band is downranked to rank number 29 (of 271), which seems more accurate. In contrast, the artist  X  X  X lice Cooper X , who obviously does not equal a
Formula (3) ,  X  X  X lice Cooper X  still remains at the 10th rank after applying the penalization factor, which we would judge highly accurate. 3.2. Band member and instrumentation detection
Another type of information indispensible for a music information system is band members and instrumentation . In order to capture such aspects, we first apply to the Web pages retrieved for a band a named entity detection (NED) approach. To this end, we extract all 2-, 3-, and 4-grams, assuming that the complete name of any band member does comprise of at least two and at most four single names. We then discard all n -grams whose tokens contain only one character and retain only the 2006 ) to filter out all n -grams where at least one token equals a common speech word. This last step in the NED is essential to suppress noise in the data, since in Web pages, word capitalization is used not only to denote named entities, but often also for highlighting purposes. The remaining n -grams are regarded as potential band members.

Subsequently, we perform shallow linguistic analysis to obtain the actual instrument(s) of each member. To this end, a set ber X  X  role in the band, is applied to the n -grams and the surrounding text as necessary. For I and R , we use lists of synonyms to cope with the use of different terms for the same concept (e.g.,  X  X  X rummer X  and  X  X  X ercussionist X ). We then calculate the document frequencies of the patterns and accumulate them over all seven patterns for each ( M , I )-tuple. In order to suppress uncertain information, we filter out those ( M , I )-pairs whose document frequency falls below a dynamic threshold t under consideration. Consider, for example, a band whose top-ranked singer, according to the DF measure, has an accumu-lated DF count of 20. Using f = 0.06, all potential members with an aggregated DF of less than 2 would be filtered out in this case as t 0.06 =20 0.06 = 1.2. The remaining tuples are predicted as members of the band under consideration. Note that this approach allows for an m : n assignment between instruments and bands.

An evaluation of this approach was conducted on a data set of 51 bands with 499 members (current and former ones). The ground truth was gathered from Wikipedia ( wik, 2009 ), All Music Guide , discogs ( dis, 2009 ), or the band X  X  Web site. We also assessed different query schemes to obtain Google  X  X  top-ranked Web pages for each band:  X  X  band  X + music (abbr. M )  X  X  band  X + music + review (abbr. MR )  X  X  band  X + music + members (abbr. MM )  X  X  band  X + music + lineup (abbr. LUM )
Varying the parameter f , we can adjust the trade-off between precision and recall, which is depicted in Fig. 1 . From the figure, we can see that the query schemes M and MM outperform the other two schemes. Another finding is that f values in the range [0.2,0.25] (depending on query scheme) maximize the sum of precision and recall, at least for the used data set.
Considering that there exists an upper limit for the recall achievable with our approach, due to the fact that usually not all band members are covered by the fetched 100 Web pages per artist, these results are pretty promising. The upper limit for the recall for the various query schemes is: M : 53%, MR : 47%, MM : 56%, LUM : 55%. For more details on the evaluation, a com-prehensive discussion of the results, and a second evaluation taking only current band members into account, the interested reader is invited to consider Schedl and Widmer (2007) . 3.3. Automatic tagging of artists
We perform automatically attributing textual descriptors to artists, commonly referred to as tagging , using a dictionary of about 1500 musically relevant terms in the indexing process. This dictionary resembles the one used in Pampalk et al. (2005) . It contains terms somehow related to music, e.g., names of musical instruments, genres, styles, moods, time periods, and geographical locations. The dictionary is available at http://www.cp.jku.at/people/schedl/music/cob_terms.txt .
As for term selection, i.e., finding the most descriptive terms for an artist, we investigated three different term weighting measures (DF, TF, and TF IDF) in a quantitative user study using a collection of 112 well-known artists (14 genres, 8 artists each), cf. Schedl and Pohle (2010) . To this end, the 10 most important terms according to each term weighting function had been determined. In order to avoid biasing of the results, the 10 terms obtained by each weighting function were then merged into one list per artist. Hence, every participant was presented a list of 112 artist names and, for each name, the cor-responding term list. Since the authors had no a priori knowledge of which artists were known by which participant, the participants were told to evaluate only those artists they were familiar with. Their task was then to rate the associated terms with respect to their appropriateness for describing the artist or his/her music. To this end, they had to associate every term cific for the artist). We had five participants in the user study and received a total of 172 assessments. Mapping the ratings in class + to the value 1, those in class to 1, and those in class to 0 and calculating the arithmetic mean of the values of all assessments for each artist, we obtained a score representing the average excess of the number of good terms over the num-ber of bad terms. These scores were 2.22, 2.43, and 1.53 for TF, DF, and TF IDF, respectively.
 To test for the significance of the results, we performed Friedman X  X  two-way analysis of variance ( Friedman &amp; March, 1940;
Sheskin, 2004 ). This test is similar to the two-way ANOVA, but does not assume a normal distribution of the data. It is hence a non-parametric test, and it requires related samples (ensured by the fact that for each artist all three measures were rated).
The outcome of the test is summarized in Table 1 . Due to the very low p value, we can state that the variance differences in the results are significant with a very high probability. To assess which term weighting measures produce significantly dif-ferent results, we conducted pairwise comparison between the results given by the three weighting functions. To this end, we employed the Wilcoxon signed ranks test ( Wilcoxon, 1945 ) and tested for a significance level of 0.01. The test showed that
TF IDF performed significantly worse than both TF and DF, whereas no significant difference could be made out between the results obtained using DF and those obtained using TF. This result is quite surprising as TF IDF is a well-established term weighting measure and commonly used to describe text documents according to the vector space model, cf. Salton, Wong, and Yang (1975) . A possible explanation for the worse performance of TF IDF is that this measure assigns high weights to terms that are very specific for a certain artist (high TF and low DF), which is obviously a desired property when it comes to distinguish one artist from another. In our application scenario, however, we aim at finding the most descriptive terms  X  not the most discriminative ones  X  for a given artist. This kind of terms seems to be better determined by the simple
TF and DF measures. Hence, for the AGMIS application, we opted for the DF weighting to automatically select the most appropriate tags for each artist. 3.4. Co-Occurrence Browser
To easily access the top-ranked Web pages of any artist, we designed a user interface called Co-Occurrence Browser (COB), brought to the third dimension. The purpose of COB is threefold: First, it facilitates getting an overview of the set of Web pages related to an artist by structuring and visualizing them according to co-occurring terms. Second, it reveals meta-information about an artist through the descriptive terms extracted from the artist X  X  Web pages. Third, by extracting the multimedia contents from the set of the artist X  X  Web pages and displaying them via the COB, the user can explore the Web pages by means of audio, image, and video data.

In short, based on the dictionary used for automatic tagging, COB groups the Web pages of the artist under consideration with respect to co-occurring terms and ranks the resulting groups by their document frequencies. then visualized using the approach presented in Schedl, Knees, Widmer, Seyerlehner, and Pohle (2007) . In this way, COB allows for browsing the artist X  X  Web pages by means of descriptive terms. Information on the amount of multimedia content is encoded most Sunburst represents the video content, the middle one the image content, and the lower one the audio content found on the respective Web pages. 3.5. Album cover retrieval
We presented preliminary attempts to automatically retrieve album cover artwork in Schedl, Knees, Pohle, and Widmer (2006) . For the article at hand, we refined our approach and conducted experiments with content-based methods (using image processing techniques) as well as with context-based methods (using text mining) for detecting images of album covers on the retrieved Web pages. The best performing strategy, which we therefore employed to build AGMIS, uses the text distance between artist and album name and h img i tag as indicator for the respective image X  X  likelihood of showing text, but also the HTML tags of the retrieved Web pages. After having filtered all images that are unlikely to show an album cover, as described below, we output the image with minimum distance between h img i tag and artist name and h img i tag and album name on the set of Web pages retrieved for the artist under consideration. Formally, the selection function is given in Formula (6) , where pos i ( t ) denotes the offset of term t , i.e., its position i in the Web page p , and P retrieved for artist a .

As for filtering obviously erroneous images, content-based analysis is performed. Taking the almost quadratic shape of most album covers into account, all cover images that have non-quadratic dimensions within a tolerance of 15% are rejected. Since images of scanned compact discs often score highly on the text distance function, we use a circle detection technique to filter out those false positives. Usually, images of scanned discs are cropped to the circle-shaped border of the compact disc, which allows to use a simple circle detection algorithm. To this end, small rectangular regions along a circular path that is touched by the image borders tangentially are examined, and the contrast between subareas of these regions is determined using
RGB color histograms. Since images of scanned compact discs show a strong contrast between subareas showing the imprint and subareas showing the background, the pixel distributions in the highest color value bins of the histograms are accumu-lated for either type of region (imprint and background). If the number of pixels in the accumulated imprint bins exceeds or falls short of the number of pixels in the accumulated background bins by more than a factor of 10, this gives strong evidence that the image under evaluation shows a scanned disc. In this case, the respective image is discarded.

On a test collection of 255 albums by 118 distinct, mostly European and American artists, our approach achieved a pre-cision of up to 89% at a recall level of 93%, precision being defined as the number of correctly identified cover images among all predicted images, recall being defined as the number of found images among all albums in the collection. On a more chal-lenging collection of 3311 albums by 1593 artists from all over the world, the approach yielded precision values of up to 73% at a recall level of 80%. 4. An automatically generated music information system
Since we aimed at building a music information system with broad artist coverage, we first had to gather a sufficiently large list of artists, on which the methods described in the previous section were applied. To this end, we extracted from All
Music Guide nearly 700,000 music artists, organized in 18 different genres. In a subsequent data preprocessing step, all artists that were mapped to identical strings after non-character removal genre distribution of the remaining 636,475 artists according to All Music Guide , measured as absolute number of artists in each genre and as percentage in the complete collection. The notably high number of artists in the genre  X  X  X ock X  can be explained by  X  X  X ock X  reveals pop artists as well as death metal bands. Nevertheless, gathering artist names from All Music Guide seemed the most reasonable solution to obtain a real-world artist list.

The sole input to the following data acquisition steps is the list of extracted artist names, except for the prototypicality estimation (cf. Section 3.1.2 ), which also requires genre information, and for the determination of album cover artwork (cf. Section 3.5 ), which requires album names. This additional information was also extracted from All Music Guide .
An overview of the data processing involved in building AGMIS is given in Fig. 3 . The data acquisition process can be broadly divided into the three steps querying the search engine for the URLs of artist-related Web pages, fetching the HTML documents available at the retrieved URLs, and indexing the content of these documents.

Querying . We queried the exalead search engine for URLs of up to 100 top-ranked Web pages for each artist in the collec-tion using the query scheme "artist name" NEAR music . The querying process took approximately one month. Its outcome was a list of 26,044,024 URLs that had to be fetched next.

Fetching . To fetch this large number of Web pages, we implemented a fetcher incorporating a load balancing algorithm to avoid excessive bandwidth consumption of servers frequently occurring in the URL list. The fetching process took approxi-mately four and a half months. It yielded a total of 732.6 gigabytes of Web pages.

Some statistics concerning the retrieved Web pages give interesting insights. Table 2 shows, for each genre, the number of artists for which not a single Web page could be determined by the search engine, i.e., artists with a page count of zero. Not very surprisingly, the percentage is highest for the genres  X  X  X atin X  and  X  X  X orld X  (nearly 30% of zero-page-count-artists), which comprise many artists known only in regions of the world that are lacking broad availability of Internet access. In contrast, a lot of information seems to be available for artists in the genres  X  X  X lectronica X  and  X  X  X ap X  (about 10% of 0-PC-artists). Table 3 depicts the number of Web pages retrieved for all artists per genre (column RP ), the arithmetic mean of Web pages retrieved for an artist (column RP mean ), and the number of retrieved pages with a length of zero, i.e., pages that were empty or could not be fetched for some reason. Since the main reason for the occurrence of such pages were server errors, their relative fre-quencies are largely genre-independent, as it can be seen in the fifth column of Table 3 . The table further shows the median and arithmetic mean of the page counts returned by exalead for the artists in each genre. These values give strong indication that artists in the genres  X  X  X atin X ,  X  X  X ospel X , and  X  X  X orld X  tend to be underrepresented on the Web.
 Indexing . To create a word-level index of the retrieved Web pages ( Zobel &amp; Moffat, 2006 ), the open source indexer Lucene
Java ( luc, 2008 ) was taken as a basis and adapted by the authors to suit the HTML format of the input documents and the requirements for efficiently extracting the desired artist-related pieces of information.

Although indexing seems to be a straightforward task at first glance, we had to resolve certain issues. Foremost some heavily erroneous HTML files were encountered, which caused Lucene to hang or crash, and thus required special handling.
More precisely, some HTML pages showed a size of tens of megabytes, but were largely filled with escape characters. To re-solve these problems, a size limit of 5 megabytes for the HTML files to index was introduced. Additionally, a 255-byte-limit for the length of each token was used.

AGMIS makes use of two indexes. Creating the first one was performed applying neither stopping, nor stemming, nor casefolding as it is used for band member and instrumentation detection (cf. Section 3.2 ) and to calculate artist similarities (cf. Section 3.1.1 ). Since the patterns applied in the linguistic analysis step of our approach to band member detection con-tain a lot of stop words, applying stopping either would have been virtually useless (when using a stop word list whose en-tries were corrected for the words appearing in the patterns) or would have yielded a loss of information crucial to the application of the patterns. Since artist names sought for in our approach to similarity estimation typically also contain stop words, applying stopping would be counterproductive for this purpose as well. The size of the optimized, compressed first index is 228 gigabytes. A second index containing only the terms in the music dictionary was created to generate term gigabytes. 4.1. AGMIS X  user interface
The pieces of information extracted from the artist-related Web pages and inserted into a relational MySQL ( mys, 2008 ) database are offered to the user of AGMIS via a Web service built on Java Servlet and Java Applet technology. The home page of the AGMIS Web site reflects a quite simple design, like the one used by Google . Besides a brief explanation of the system, it only displays a search form, where the user can enter an artist or band name. To allow for fuzzy search, the string entered by the user is compared to the respective database entries using Jaro-Winkler similarity , cf. ( Cohen, Ravikumar, &amp; Fienberg, 2003 ). The user is then provided a list of approximately matching artist names, from which he or she can select one.
After the user has selected the desired artist, AGMIS delivers an artist information page. Fig. 4 shows an example of such a page for the band Dragonforce . On the top of the page, artist name, genre, and prototypicality rank are shown. Below this header, lists of similar artists, of descriptive terms, and of band members and instrumentation, where available and appli-cable, are shown. As a matter of course, the information pages of similar artists are made available via hyperlinks. Moreover, described so far, the Co-Occurrence Browser is integrated into the user interface as a Java Applet to permit browsing the in-dexed Web pages and their multimedia content. The lower part of the artist information page is dedicated to discography information, i.e., a list of album names and album cover images are shown. 4.2. Computational complexity
Most tasks necessary to build AGMIS were quite time-consuming. The querying, fetching, and indexing processes, the cre-ation of artist term profiles, the calculation of term weights, and all information extraction tasks were performed on two stan-dard personal computers with Pentium 4 processors clocked at 3 GHz, 2 GB RAM, and a RAID-5 storage array providing 2 TB of usable space. In addition, a considerable amount of external hard disks serving as temporary storage facilities were required. 4.2.1. Running times
In Table 4 , precise running times for indexing, information extraction, and database operation tasks are shown for those tasks for which we measured the time. Calculating the artist similarity matrix was carried out as follows. Computing the complete 636,475 636,475 similarity matrix requires 202,549,894,575 pairwise similarity calculations. Although perform-ing this number of calculations is feasible in reasonable time on a current personal computer in regard to computational power, the challenge is to have the required vectors in memory when they are needed. As the size of the complete similarity matrix amounts to nearly 800 gigabytes, even when storing symmetric elements only once, it is not possible to hold all data in memory. Therefore, we first split the 636,475 636,475 matrix into 50 rows and 50 columns, yielding 1275 submatrices when storing symmetric elements only once. Each submatrix requires 622 megabytes and thus fits well into memory. Artist similarities were then calculated between the 12,730 artists in each submatrix, processing one submatrix at a time. Aggre-collection were selected and inserted into the database. 4.2.2. Asymptotic runtime complexity
The asymptotic runtime complexities of the methods presented in Section 3 are summarized in Table 5 , supposing that querying, fetching, and indexing was already performed. Querying is obviously linear (in terms of issued requests) in the number of artists, i.e., O  X  n  X  , provided that the desired number of top-ranked search results p retrieved per artist does not exceed the number of results that can be returned by the search engine in one page. Fetching can be performed in O  X  n p  X  , but will usually require less operations (cf. Table 2 , the average number of Web pages retrieved per artist is 40). to be processed.

In Table 5 , n denotes the total number of artists and k the total number of keys in the index. Creating the symmetric sim-ilarity matrix and estimating the prototypicality for each artist both require n log k , the complexity of the whole process is O  X  n 2 log k  X  . The band member detection requires k operations to extract the potential band members, i.e., n -grams, for each of which p operations are needed to evaluate the patterns and obtain their document frequencies, p being the number of patterns in all variations, i.e., all synonyms for instruments and roles counted tagging procedure is in O  X  n k  X  , where k is again the number of terms in the index. However, as we use a dedicated index for the purpose of artist tagging, k 1500, and therefore k n . Finally, the current implementation of our album cover retrieval technique requires n k operations, since all keys in the index have to be sought for h img i tags, artist names, and album names. This could be sped up by building an optimized index with clustered h img i tags, which will be part of future work. 5. Conclusions and future work
This article has given an overview of state-of-the-art techniques for Web-based information extraction in the music domain. In particular, techniques to mine relations between artists (similarities and prototypicality), band members and instrumentation, descriptive terms, and album covers were presented. Furthermore, this article briefly described the
Co-Occurrence Browser (COB), a user interface to organize and access artist-related Web pages via important, music-related terms and multimedia content. It was further shown that the proposed approaches can be successfully applied on a large scale using a real-world database of more than 600,000 music artists. Integrating the extracted information into a single informa-tion system yielded the Automatically Generated Music Information System (AGMIS), whose purpose is to provide access to the large amount of data gathered. The design, implementation, and feeding of the system were reported in detail.
Even though the evaluation experiments conducted to assess the techniques underlying AGMIS showed promising results, they still leave room for improvement in various directions. First, Web page retrieval could be pursued using focused crawling instead of directed search via search engines. This would presumably yield more accurate results, while at the same time limit
Web traffic. Second, deep natural language processing techniques and more sophisticated approaches to named entity detec-tion and machine learning could be employed to derive more specific information, especially in band member and instrumen-tation detection as well as to obtain detailed discography information. For example, temporal information would allow for creating band and artist histories as well as time-dependent relationship networks. Automatically generated biographies would be the ultimate aim. Finally, the information gathered by the Web mining techniques presented here could be complemented with information extracted from the audio signal. Audio signal-based similarity information at the track level would enable enhanced services and applications, like automatic playlist generation or user interfaces to explore huge music collections in virtual spaces. Bringing AGMIS to the track level would also permit to provide song lyrics since approaches to automatically extracting a correct version of a song X  X  lyrics do already exist, cf. Korst and Geleijnse (2006) and Knees et al. (2005) . Employing methods to align audio and lyrics could eventually even allow for applications like an automatic karaoke system.
 Acknowledgments This research is supported by the Austrian Science Fund (FWF) under Project Numbers L511-N15, Z159, and P22856-N23. The authors would further like to thank Julien Carcenac from exalead for his support in the querying process. References
