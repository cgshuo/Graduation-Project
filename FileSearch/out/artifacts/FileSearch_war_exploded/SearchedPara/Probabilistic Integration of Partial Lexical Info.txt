 Nowadays, modern portable devices, such as the smartphones and tablets, are equipped with micro-phone and touchscreen display. With these devices becoming increasingly popular, there is an urgent need for an efficient and reliable text entry method on these small devices. Currently, text entry us-ing an onscreen virtual keyboard is the most widely adopted input method on these modern mobile de-vices. Unfortunately, typing with a small virtual keyboard can sometimes be cumbersome and frus-tratingly slow for many people. Instead of using a virtual keyboard, it is also possible to use hand-writing gestures to input text. Handwriting input offers a more convenient input method for writing systems with complex orthography, including many Asian languages such as Chinese, Japanese and Ko-rean. However, handwriting input is not necessarily more efficient compared to keyboard input for En-glish. Moreover, handwriting recognition is suscep-tible to recognition errors, too.

Voice input offers a hands-free solution for text entry. This is an attractive alternative for text entry because it completely eliminates the need for typ-ing. Voice input is also more natural and faster for human to convey messages. Normally, the average human speaking rate is approximately 100 words per minute (WPM). Clarkson et al. (2005) showed that the typing speed for regular users reaches only 86.79  X  98.31 using a full-size keyboard and 58.61  X  61.44 WPM using a mini-QWERTY keyboard. Evidently, speech input is the preferred text entry method, provided that speech signals can be reli-ably and efficiently converted into texts. Unfortu-nately, voice input relies on automatic speech recog-nition (ASR) (Rabiner, 1989) technology, which re-quires high computational resources and is suscep-tible to performance degradation due to acoustic in-terference, such as the presence of noise.

In order to improve the reliability and efficiency of ASR, Haptic Voice Recognition (HVR) was pro-posed by Sim (2010) as a novel multimodal input method combining both speech and touch inputs. Touch inputs are used to generate haptic events , which correspond to the initial letters of the words in the spoken utterance. In addition to the regular beam pruning used in traditional ASR (Ortmanns et al., 1997), search paths which are inconsistent with the haptic events are also pruned away to achieve further reduction in the recognition search space. As a re-sult, the runtime of HVR is generally more efficient than ASR. Furthermore, haptic events are not sus-ceptible to acoustic distortion, making HVR more robust to noise.

This paper proposes a probabilistic framework that encompasses multiple knowledge sources for combining the speech and touch inputs. This frame-work allows coherent probabilistic models of dif-ferent knowledge sources to be tightly integrated. In addition to the acoustic model and language model used in ASR, haptic model and partial lexi-cal model are also introduced to facilitate the inte-gration of more sophisticated haptic events, such as the keystrokes, into HVR.

The remaining of this paper is organised as fol-lows. Section 2 gives an overview of existing tech-niques in the literature that aim at improving noise robustness for automatic speech recognition. Sec-tion 3 gives a brief introduction to HVR. Section 4 proposes a probabilistic framework for HVR that unifies multiple knowledge sources as an integrated probabilistic generative model. Next, Section 5 describes how multiple knowledge sources can be integrated using Weighted Finite State Transducer (WFST) operations. Experimental results are pre-sented in Section 6. Finally, conclusions are given in Section 7. As previously mentioned, the process of converting speech into text using ASR is error-prone, where significant performance degradation is often due to the presence of noise or other acoustic interference. Therefore, it is crucial to improve the robustness of voice input in noisy environment. There are many techniques reported in the literature which aim at improving the robustness of ASR in noisy environment. These techniques can be largely di-vided into two groups: 1) using speech enhance-ment techniques to increase the signal-to-noise ratio of the noisy speech (Ortega-Garcia and Gonzalez-Rodriguez, 1996); and 2) using model-based com-pensation schemes to adapt the acoustic models to noisy environment (Gales and Young, 1996; Acero et al., 2000).

From the information-theoretic point of view, in order to achieve reliable information transmission, redundancies are introduced so that information lost due to channel distortion or noise corruption can be recovered. Similar concept can also be applied to improve the robustness of voice input in noisy en-vironment. Additional complementary information can be provided using other input modalities to pro-vide cues (redundancies) to boost the recognition performance. The next section will introduce a mul-timodal interface that combines speech and touch in-puts to improve the efficiency and noise robustness for text entry using a technique known as Haptic Voice Recognition (Sim, 2010). For many voice-enabled applications, users often find voice input to be a black box that captures the users X  voice and automatically converts it into texts using ASR. It does not provide much flexibility for human intervention through other modalities in case of errors. Certain applications may return multiple hypotheses, from which users can choose the most appropriate output. Any remaining errors are typi-cally corrected manually. However, it may be more useful to give users more control during the input stage, instead of having a post-processing step for error correction. This motivates the investigation of multimodal interface that tightly integrates speech input with other modalities.

Haptic Voice Recognition (HVR) is a multimodal interface designed to offer users the opportunity to add his or her  X  magic touch  X  in order to improve the accuracy, efficiency and robustness of voice in-put. HVR is designed for modern mobile devices equipped with an embedded microphone to capture speech signals and a touchscreen display to receive touch events. The HVR interface aims to combine both speech and touch modalities to enhance speech recognition. When using an HVR interface, users will input text verbally, at the same time provide ad-ditional cues in the form of Partial Lexical Infor-mation (PLI) to guide the recognition search. PLIs are simplified lexical representation of words that should be easy to enter whilst speaking ( e.g. the prefix and/or suffix letters). Preliminary simulated experiments conducted by Sim (2010) show that potential performance improvements both in terms of recognition speed and noise robustness can be achieved using the initial letters as PLIs. For ex-ample, to enter the text  X  Henry will be in Boston next Friday  X , the user will speak the sentence and enter the following letter sequence:  X  X  X ,  X  X  X ,  X  X  X ,  X  X  X ,  X  X  X ,  X  X  X  and  X  X  X . These additional letter sequence is simple enough to be entered whilst speaking; and yet they provide crucial information that can sig-nificantly improve the efficiency and robustness of speech recognition. For instance, the number of let-ters entered can be used to constrain the number of words in the recognition output, thereby suppress-ing spurious insertion and deletion errors, which are commonly observed in noisy environment. Further-more, the identity of the letters themselves can be used to guide the search process so that partial word sequences in the search graph that do not conform to the PLIs provided by the users can be pruned away.
PLI provides additional complementary informa-tion that can be used to eliminate confusions caused by poor speech signal. In conventional ASR, acous-tically similar word sequences are typically resolved implicitly using a language model where contexts of neighboring words are used for disambiguation. On the other hand, PLI can also be very effective in disambiguating homophones 1 and similar sound-ing words and phrases that have distinct initial let-ters. For example,  X  hour  X  versus  X  our  X ,  X  vary  X  versus  X  marry  X  and  X  great wine  X  versus  X  grey twine  X .
This paper considers two methods of generating the initial letter sequence using a touchscreen. The first method requires the user to tap on the appropri-ate keys on an onscreen virtual keyboard to generate the desired letter sequence. This method is similar to that proposed in Sim (2010). However, typing on small devices like smartphones may require a great deal of concentration and precision from the users. Alternatively, the initial letters can be entered using handwriting gestures. A gesture recognizer can be used to determine the letters entered by the users. In order to achieve high recognition accuracy, each let-ter is represented by a single-stroke gesture, so that isolated letter recognition can be performed. Fig-ure 1 shows the single-stroke gestures that are used in this work. Let O = { o 1 , o 2 ,..., o T } denote a sequence of T observed acoustic features such as MFCC (Davis and Mermelstein, 1980) or PLP (Hermansky, 1990) and H = { h 1 , h 2 ,..., h N } denote a sequence of N haptic features. For the case of keyboard input, each h i is a discrete symbol representing one of the 26 letters. On the other hand, for handwriting input, each h i represents a sequence of 2-dimensional vec-tors that corresponds to the coordinates of the points of the keystroke. Therefore, the haptic voice recog-nition problem can be defined as finding the joint optimal solution for both the word sequence,  X  W and the PLI sequence,  X  L , given O and H . This can be expressed using the following formulation: where according to the Bayes X  theorem:
P ( W , L|O , H ) = The joint prior probability of the observed inputs, p ( O , H ) , can be discarded during the maximisation of Eq. 1 since it is independent of W and L . p ( O|W ) is the acoustic likelihood of the word sequence, W , generating the acoustic feature sequence, O . Simi-larly, P ( H|L ) is the haptic likelihood of the lexical sequence, L , generating the observed haptic inputs, H . The joint prior probability, P ( W , L ) , can be de-composed into: where P ( W ) can be modelled by the word-based n -gram language model (Chen and Goodman, 1996) commonly used in automatic speech recognition. Combining Eq. 2 and Eq. 3 yields:
P ( W , L|O , H )  X  It is evident from the above equation that the prob-abilistic formulation of HVR combines four knowl-edge sources:  X  Acoustic model score: p ( O|W )  X  Haptic model score: p ( H|L )  X  PLI model score: P ( L|W )  X  Language model score: P ( W ) Note that the acoustic model and language model scores are already used in the conventional ASR. The probabilistic formulation of HVR incorporated two additional probabilities: haptic model score , p ( H|L ) and PLI model score , P ( L|W ) . The role of the haptic model and PLI model will be described in the following sub-sections. 4.1 Haptic Model Similar to having an acoustic model as a statisti-cal representation of the phoneme sequence generat-ing the observed acoustic features, a haptic model is used to model the PLI sequence generating the ob-served haptic inputs, H . The haptic likelihood can be factorised as where L = { l i : 1  X  i  X  N } . l i is the i th PLI in L and h i is the i th haptic input feature. In this work, each PLI represent the initial letter of a word. Therefore, l i represents one of the 26 letters. As pre-viously mentioned, for keyboard input, h i are dis-crete features whose values are also one of the 26 letters. Therefore, p ( h i | l i ) forms a 26  X  26 matrix. A simple model can be derived by making p ( h i | l i ) an identity matrix. Therefore, p ( h i | l i ) = 1 if h i = l otherwise, p ( h i | l i ) = 0 . However, it is also possi-ble to have a non-diagonal matrix for p ( h i | l i ) in or-der to accommodate typing errors, so that non-zero probabilities are assigned to cases where h i 6 = l i . For handwriting input, h i denote a sequence of 2-dimensional feature vectors, which can be modelled using Hidden Markov Models (HMMs) (Rabiner, 1989). Therefore, ( h i | l i ) is simply given by the HMM likelihood. In this work, each of the 26 let-ters is represented by a left-to-right HMM with 3 emitting states. 4.2 Partial Lexical Information (PLI) Model Finally, a PLI model is used to impose the com-patibility constraint between the PLI sequence, L , and the word sequence, W . Let W = { w i : 1  X  i  X  M } denote a word sequence of length M . If M = N , the PLI model likelihood, P ( L|W ) , can be expressed in the following form: where P ( l i | w i ) is the likelihood of the i th word, w generating the i th PLI, l i . Since each word is rep-resented by a unique PLI (the initial letter) in this work, the PLI model score is given by P ( l i | w i ) = C sub = On the other hand, if N 6 = M , insertions and dele-tions have to be taken into consideration:
P ( l i = | w i ) = C del and P ( l i | w i = ) = C ins where represents an empty token. C del and C ins denote the deletion and insertion penalties respec-tively. This work assumes C del = C ins = 0 . This means that the word count of the HVR out-put matches the length of the initial letter sequence entered by the user. Assigning a non-zero value to C del gives the users option to skip entering letters for certain words ( e.g. short words). As previously mentioned, the HVR recognition pro-cess involves maximising the posterior probability in Eq. 4, which can be expressed in terms of four knowledge sources. It turns out that these knowl-edge sources can be represented as Weighted Finite State Transducers (WFSTs) (Mohri et al., 2002) and the composition operation (  X  ) can be used to inte-grate these knowledge sources into a single WFST: where  X  A ,  X  L ,  X  P and  X  H denote the WFST repre-sentation of the acoustic model, language model, PLI model and haptic model respectively. Mohri et al. (2002) has shown that Hidden Markov Mod-els (HMMs) and n -gram language models can be viewed as WFSTs. Furthermore, HMM-based hap-tic models are also used in this work to represent the single-stroke letters shown in Fig. 1. Therefore,  X  A ,  X  L , and  X  H can be obtained from the respective probabilistic models. Finally, the PLI model de-scribed in Section 4.2 can also be represented using the WFST as shown in Fig. 2. The transition weights of these WFSTs are given by the negative log prob-ability of the respective models.  X  P can be viewed as a merger that defines the possible alignments be-tween the speech and haptic inputs. Each complete path in  X  F represents a valid pair of W and L such that the weight of the path is given by the negative log P ( L , W|O , H ) . Therefore, finding the shortest path in  X  F is equivalent to solving Eq. 1. Direct decoding from the overall composed WFST,  X  F integrated , is referred to as integrated de-coding . Alternatively, HVR can also operate in a lat-tice rescoring manner. Speech input and haptic in-put are processed separately by the ASR system and the haptic model respectively. The ASR system may generate multiple hypotheses of word sequences in the form of a lattice. Similarly, the haptic model may also generate a lattice containing the most probably letter sequences. Let  X  L and  X  H represent the word and letter lattices respectively. Then, the final HVR output can be obtained by searching for the shortest path of the following merged WFST: Note that the above composition may yield an empty WFST. This may happen if the lattices generated by the ASR system or the haptic model are not large enough to produce any valid pair of W and L . In this section, experimental results are reported based on the data collected using a prototype HVR interface implemented on an iPad. This prototype HVR interface allows both speech and haptic input data to be captured either synchronously or asyn-chronously and the partial lexical information can be entered using either a soft keyboard or handwrit-ing gestures. Figures 3 and 4 shows the screen-shot of the HVR prototype iPad app using the key-board and keystroke inputs respectively. Therefore, there are altogether four input configurations. For each configuration, 250 sentences were collected from a non-native fluent English speaker. 200 sen-tences were used as test data while the remaining 50 sentences were used for acoustic model adaptation. These sentences contain a variety of given names, surnames and city names so that confusions can-not be easily resolved using a language model. Ex-ample sentences used for data collection are shown in Table 1. In order to investigate the robustness of HVR in noisy environment, the collected speech data were also artificially corrupted with additive babble noise from the NOISEX database (Varga and Steeneken, 1993) to synthesise noisy speech signal-to-noise (SNR) levels of 20 and 10 decibels 2 .
The ASR system used in all the experiments re-ported in this paper consists of a set of HMM-based triphone acoustic models and an n -gram language model. The HMM models were trained using 39-dimensional MFCC features. Each HMM has a left-to-right topology and three emitting states. The emission probability for each state is represented by a single Gaussian component 3 . A bigram language model with a vocabulary size of 200 words was used for testing. The acoustic models were also noise-compensated using VTS (Acero et al., 2000) in order achieve a better baseline performance. 6.1 Comparison of Input Speed Table 2 shows the speech, letter and total input speed using different input configurations. For syn-chronous HVR, the total input speed is the same as the speech and letter input speed since both the speech and haptic inputs are provided concurrently. According to this study, synchronous keyboard in-put speed is 86 words per minutes (WPM). This is Keyboard Keystroke slightly faster than keystroke input using handwrit-ing gestures, where the input speed is 78 WPM. This is not surprising since key taps are much quicker to generate compared to handwriting gestures. On the other hand, the individual speech and letter in-put speed are faster for asynchronous mode be-cause users do not need to multi-task. However, since the speech and haptic inputs are provided con-currently, the resulting total input speed for asyn-chronous HVR is much slower compared to syn-chronous HVR. Therefore, synchronous HVR is po-tentially more efficient than asynchronous HVR. 6.2 Performance of ASR HVR Mode SNR WER (%) LER (%) (Keyboard) (Keystroke) First of all, the Word Error Rate (WER) and Letter Error Rate (LER) performances for standard ASR systems in different noise conditions are sum-marized in Table 3. These are results using pure ASR, without adding the haptic inputs. Speech recorded using asynchronous HVR is considered normal speech. The ASR system achieved 22.2%, 30.2% and 33.3% WER in clean, 20dB and 10dB conditions respectively. Note that the acoustic mod-els have been compensated using VTS (Acero et al., 2000) for noisy conditions. Table 3 also shows the system performance considering on the initial let-ter sequence of the recognition output. This indi-cates the potential improvements that can be ob-tained with the additional first letter information. Note that the pure ASR system output contains sub-stantial initial letter errors.

For synchronous HVR, the recorded speech is ex-pected to exhibit different characteristics since it may be influenced by concurrent haptic input. Ta-ble 3 shows that there are performance degradations, both in terms of WER and LER, when performing ASR on these speech utterances. Also, the degra-dations caused by simultaneous keystroke input are greater. The degradation may be caused by phenom-ena such as the presence of filled pauses and the lengthening of phoneme duration. Other forms of disfluencies may have also been introduced to the realized speech utterances. Nevertheless, the addi-tional information provided by the PLIs will out-weigh these degradations. 6.3 Performance of Synchronous HVR
The performance of synchronous HVR is shown in Table 4. Compared to the results shown in Ta-ble 3, the WER performance of synchronous HVR improved by approximately a factor of two. Fur-thermore, the LER performance improved signifi-cantly. For keyboard input, the LER reduced to about 1.0% for all noise conditions. Note that the tradeoffs between the WER and LER performance can be adjusted by applying appropriately weights to different knowledge sources during integration. For keystroke input, top five letter candidates returned by the handwriting recognizer were used. Therefore, in clean condition, the acoustic models are able to recover some of the errors introduced by the hand-writing recognizer, bringing the LER down to as low as 0.3%. However, in noisy conditions, the LER performance is similar to those using keyboard in-put. Overall, synchronous and asynchronous HVR achieved WER comparable performance. 6.4 Performance of Asynchronous HVR
Similar to synchronous HVR, asynchronous HVR also achieved significant performance improve-ments over the pure ASR systems. Table 5 shows the WER and LER performance of asynchronous HVR in different noise conditions. The WER perfor-mance of asynchronous HVR is consistently better than that of synchronous HVR (comparing Tables 4 and 5). This is expected since the speech quality for asynchronous HVR is higher. However, consider-ing the much slower input speed ( c.f. Table 2) and the marginal WER improvements for asynchronous HVR, synchronous HVR appears to be a better con-figuration. 6.5 Integrated Decoding vs. Lattice Rescoring
As previously mentioned in Section 5, HVR can also be performed in two stages using lattice rescor-ing technique. Table 6 shows the performance comparison between integrated decoding and lat-tice rescoring for HVR. Both methods gave similar performance in clean condition. However, lattice rescoring yielded significantly worse performance in noisy environment. Therefore, it is important to tightly integrate the PLI into the decoding process to avoid premature pruning away optimal paths. 6.6 Runtime Performance ASR system searches for the best word sequence using a dynamic programming paradigm (Ney and Ortmanns, 1999). The complexity of the search in-creases with the vocabulary size as well as the length of the input speech. A well-known concept of To-ken Passing (Young et al., 1989) can be used to de-scribe the recognition search process. A set of ac-tive tokens are being propagated upon observing an acoustic feature frame. The best token that survived to the end of the utterance represents the best out-put. Typically, beam pruning technique (Ortmanns et al., 1997) is applied to improve the recognition ef-ficiency. Tokens which are unlikely to yield the op-timal solution will be pruned away. HVR performs a more stringent pruning, where paths that do not con-form to the PLI sequence are also be pruned away.
Table 7 shows the comparison of the runtime fac-tors and the average number of active tokens per frame for ASR and HVR systems. The standard ASR system runs at 1.9, 2.0 and 2.4 times real-time (xRT) 4 . The runtime factor increases with de-creasing SNR because the presence of noise intro-duces more confusions, which renders beam prun-ing (Ortmanns et al., 1997) less effective. The num-ber of active tokens per frame also increases from 6260 to 7168 as the SNR drops from the clean con-dition to 10dB. On the other hand, there are sig-nificant speedup in the runtime of HVR systems. In particular, synchronous HVR achieved the best runtime performance, which is roughly consistent across different noise conditions (approximately 1.0 xRT). The average number of active tokens also re-duces to the range of 3490  X  4442. Therefore, the synchronous HVR using keyboard input is robust to noisy environment, both in terms of WER and run-time performance. The runtime performance using keystroke input is also comparable to that using key-board input (only slightly worse). Therefore, both keyboard and keystroke inputs are effective ways for entering the initial letters for HVR. However, it is worth noting that the iPad was used for the studies conducted in this work. The size of the iPad screen is sufficiently large to allow efficient keyboard entry. However, for devices with smaller screen, keystroke inputs may be easier to use and less error-prone. This paper has presented a unifying probabilistic framework for the multimodal Haptic Voice Recog-nition (HVR) interface. HVR offers users the option to interact with the system using touchscreen during voice input so that additional cues can be provided to improve the efficiency and robustness of voice recognition. Partial Lexical Information (PLI), such as the initial letter of the words, are used as cues to guide the recognition search process. Therefore, apart from the acoustic and language models used in conventional ASR, HVR also combines the hap-tic model as well as the PLI model to yield an inte-grated probabilistic model. This probabilistic frame-work integrates multiple knowledge sources using the weighted finite state transducer operation. Such integration is achieved using the composition oper-ation which can be applied on-the-fly to yield ef-ficient implementation. Experimental results show that this framework can be used to achieve a more efficient and robust multimodal interface for text en-try on modern portable devices.
