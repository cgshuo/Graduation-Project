 When search engine users have trouble finding information, they may become frustrated, possibly resulting in a bad ex-perience (even if they are ultimately successful). In a user study in which participants were given difficult information seeking tasks, half of all queries submitted resulted in some degree of self-reported frustration. A third of all successful tasks involved at least one instance of frustration. By mod-eling searcher frustration, search engines can predict the cur-rent state of user frustration and decide when to intervene with alternative search strategies to prevent the user from becoming more frustrated, giving up, or switching to another search engine. We present several models to predict frustra-tion using features extracted from query logs and physical sensors. We are able to predict frustration with a mean av-erage precision of 65% from the physical sensors, and 87% from the query log features.
 H.3.3 [ Information Search and Retrieval ]: Search pro-cess Experimentation, Measurement user modeling, searcher frustration, query logs, emotional sensors
In this work, we investigate searcher frustration . We con-sider users frustrated in the context of information retrieval (IR) when their search process is impeded. A frustration model capable of predicting how frustrated searchers are throughout their search is useful retrospectively as an effec-tiveness measure. More importantly, it allows for real-time system intervention to help frustrated searchers, hopefully preventing users from leaving for another search engine or abandoning the search altogether.

This work investigates what aspects of users X  interactions with a search engine during a task can be used to predict frustration. Depending on the level of frustration, we may wish to change the underlying retrieval algorithm or the user interface. For example, one source of difficulty in retrieval is a user X  X  inability to sift through the results presented for a query [13, 17]. One way that a system could adapt to address this kind of frustration is to show the user a concep-tual breakdown of the results: rather than listing all results, group them based on the key concepts that best represent them [13]. Using a well worn example, if a user enters  X  X ava X , they can see the results based on  X  X slands X ,  X  X rogramming languages X ,  X  X offee X , etc. Of course, most search engines al-ready strive to diversify result sets, so documents relating to all of these different facets of  X  X ava X  are present, but they might not be clear to some users, causing users to become frustrated.

An example from the IR literature of a system that adapts based on a user model is work by White et al. [15]. They used implicit relevance feedback to detect changes in users X  information needs and alter the retrieval strategy based on the degree of change. The focus of our work is to detect frustrated behavior, and adapt the system based on the type of frustration, regardless of the information need itself.
The goals for our line of research are as follows: first, de-termine how to detect a user X  X  level of frustration; second, determine what the key causes or types of frustration are; and third, determine the kinds of system interventions that can reduce different types of frustration. This work explores the question of whether frustration can be accurately pre-dicted and what features derived from query logs and phys-ical sensors are the most useful in doing so in a controlled lab study.

Our key contributions are (1) the first user study of frus-tration in web search (2) a publicly available data set of the data collected, and (3) a comparison of on-line classification models derived from sensor and query log data to predict frustration.
 The remainder of this paper is organized as follows. In Section 2 we discuss related work from the IR, intelligent tutoring systems (ITS), and information science (IS) litera-ture. We then describe the task and evaluation in Section 3, followed by a description of the user study we conducted in Section 4. In Section 5 we describe the models used followed in Section 6 by a description and analysis of the experiments. In Sections 7 and 8 we discuss the results and design impli-cations of our findings. We end with a summary and future work in Section 9.
In this section, we first describe frustration in the con-text of IR. We then touch upon some of the related work in three areas: searcher satisfaction modeling, work carried out in the field of ITS where frustration has been modeled, and various work pertaining to user modeling in IR, such as predicting when users will switch to another search engine. These works helped to shape the user study we conducted and the models used to predict searcher frustration.
We define frustration in the context of IR as the imped-iment of search progress. Xie and Cool [17] explored help-seeking or problematic situations that arise in searching dig-ital libraries. They identified fifteen types of help-seeking situations that their 120 novice participants encountered. The authors X  use of  X  X elp-seeking situations X  aligns well with our definition of frustration, since the issues encountered by the subjects impeded their search progress. The authors created a model of the factors that contribute to these help-seeking situations from the user, task, system, and interac-tion aspects. The qualitative nature of the study is useful in designing general help systems for digital library systems. However, there was no attempt to model frustration using logged interaction data, which is the goal of our work. In a study examining how children search the Internet, Druin et al. [7] found that all of the twelve participants ex-perienced frustration while searching. The authors point out that children make up one of the largest groups of Inter-net users, making frustration a major concern. In a similar study, Bilal and Kirby [2] compared the searching behav-ior of graduate students and children on Yahooligans! They found that over 50% of graduate students and 43% of chil-dren were frustrated and confused during their searches. In addition, they found that while graduate students quickly recovered from  X  X reakdowns X  (where users were unable to find results for a keyword search), children did not.
Kuhlthau [12] found that frustration is an emotion com-monly experienced during the exploration phase of a search process. She states that encountering inconsistent informa-tion from various sources can cause frustration and lead to search abandonment.
While frustration prediction has not been directly studied in the field of IR, searcher satisfaction has. Satisfaction in search can have different meanings [1, 8, 9]. We define searcher satisfaction as the fulfillment of a user X  X  information need. While satisfaction and frustration are closely related, they are distinct. As a consequence, searchers can ultimately satisfy their information need, but still be quite frustrated in the process [3].

In previous work, satisfaction has been examined at the task or session level 1 [1, 8, 9, 10]. These satisfaction models only cover user satisfaction after a task has been completed, not while a task is in progress. As such, satisfaction models are useful for retrospective analysis and improvement, but
We use task and session interchangeably in this research. not as a real-time predictor. In contrast, with a frustration model that is defined throughout a search, these real-time solutions are available.

In web search study, Fox et al. [8] found there exists an association between query log features and searcher satisfac-tion, with the most predictive features being click-through, the time spent on the search result page, and the manner in which a user ended a search. They also analyzed brows-ing patterns and found some more indicative of satisfaction than others, such as entering a query, clicking on one result, and then ending the task. Clicking four or more results was more indicative of dissatisfaction.

Huffman and Hochster [10] found a relatively strong cor-relation with session satisfaction using a linear model en-compassing the relevance of the first three results returned for the first query in a search task, whether the information need was navigational, and the number of events in the ses-sion. In a similar study of search task success, Hassan et al. [9] used a Markov model of search action sequences to pre-dict success at the end of a task. The model outperformed a method using the DCG of the first query X  X  result set, sug-gesting that a model of the interactions derivable from a query log is better than general relevance in modeling satis-faction.
While we have not found any discussion of predicting frus-tration in the IR literature, we did find studies that model frustration in the ITS literature. Cooper et al. [4] describe a study in which students using an intelligent tutoring system were outfitted with four sensors: a mental state camera that focused on the student X  X  face, a skin conductance bracelet, a pressure sensitive mouse, and a chair seat capable of de-tecting posture.

Cooper et al. found that across the three experiments they conducted, the mental state camera was the best stand-alone sensor to use in conjunction with the tutoring inter-action logs for determining frustration. However, using fea-tures from all sensors and the interaction logs performed best. They used step-wise regression to develop a model for describing each emotion. In another study using the same sensors, but different features, Kapoor, Burleson, and Picard [11] created a model that was capable of classifying when the user of an ITS was going to click an I X  X  frustrated! button with 79% accuracy and a chance accuracy of 58%.
In this section, we summarize several models used in IR prediction tasks that rely, at least in part, on query log data [6, 9, 10, 16]. We are specifically interested in the types of models used (e.g., linear regression) and the key features.

Huffman and Hochster [10] predicted session satisfaction useing a regression model incorporating the relevance of the top three results returned for the first query, the type of information need, and the number of actions in the session. Hassan et al. [9] used a Markov model to predict task success and found that sequences of actions, as well as the time between the actions, are good predictors.

Downey et al. [6] created a Bayesian dependency network to predict the next user browsing action given the previous n actions, parameterized by a long list of user, session, query, result click, non-search action, and temporal features. They found that using an action history with more than just the immediately preceding action hurt performance.

White and Dumais [16] explored predicting when users would switch between search engines. Their goal was  X  X ot to optimize the model but rather to determine the predic-tive value of the query/session/user feature classes for the switch prediction challenge. X  They used a logistic regres-sion model that encompassed query, session, and user level features. They found that using all three feature classes out-performed all other combinations of feature classes and did much better than the baseline for most recall levels.
In this section, we outline the details of the frustration modeling task and its evaluation.
Our goal is to predict whether a user is frustrated at the end of each query interaction during a session. We define a query interaction as all interactions between a user and the the browser pertaining to a specific query up until another query is entered or the session ends. We will refer to these as searches . A session consists of one or more searches directed at fulfilling a specific information need or task. We will refer to these as tasks . At the end of a search, we ask,  X  X s the user frustrated at this point of the task? X  To make the prediction, we can derive features from the search just completed or from all the searches conducted in the task so far. We refer to these feature sets as search and task features, respectively. In addition, features can be derived from a user X  X  other tasks, which we call user features.
In this paper, we consider frustration prediction as a bi-nary task. However, multi-class prediction may also be use-ful, using either regression or a multi-class machine learning method. We also focus on general frustration, but predict-ing types of frustration may also be useful, e.g., predicting the fifteen types of frustration outlined by Xie and Cool [17].
In this section, we describe the metrics that we use to eval-uate frustration models. Our ultimate goal is to use frus-tration models to decide when to intervene to help the user during the search process. Since many interaction methods with which we would like to intervene are not typically used because of their undesirable, frustration-causing attributes (i.e., interaction and latency), we are interested in mini-mizing our false-positives (non-frustrated searchers that our models say are frustrated), potentially at the cost of re-call. For that reason, our predominant evaluation metric is a macro-average (across users) F-score with  X  = 0 . 5, which gives increased weight to precision over recall. We also use 11-point interpolated average precision to compare models across users. This metric tells us how well, on average, a model can rank instances of frustration by user.

Comparing across users rather than with a micro ap-proach avoids one frustrated searcher in the test data skew-ing the results. Un-weighted macro-averaging treats all users equally. A desirable model is one that performs well across all users, not just on one specific user. In Section 6 we re-port macro accuracy, precision, F  X  =0 . 5 , and mean average precision (MAP). To be clear, MAP is uninterpolated, in contrast to 11-point interpolated average precision. Table 1: The information seeking tasks given to users in the user study. Variations are included in brackets.

We use an approximation of Fisher X  X  randomization test to obtain a double sided p-value for significance. Using 100,000 trials for every model comparison, the error at  X  = 0 . 05 is  X  0 . 001 (2% error) [14].
In Fall 2009, we conducted a user study with thirty partic-ipants from the University of Massachusetts Amherst. The mean age of participants was 26. Most participants were computer science or engineering graduates, others were from English, kinesiology, physics, chemical engineering, and op-eration management. Two participants were undergradu-ates. Twenty-seven users reported a  X 5 X  (the highest) on a five-point search experience scale; one reported a  X 4 X  and two a  X 3 X . There were seven females and twenty-three males.
Each participant was asked to complete seven 2 tasks from a pool of twelve (several with multiple versions) and to spend no more than seven minutes on each, though this was not strictly enforced. The order of the tasks was determined by four 12  X  12 Latin squares, which removed ordering effects from the study. Users were given tasks one at a time, so they were unaware of the tasks later in the order. Most of the tasks were designed to be difficult to solve with a search engine since the answer was not easily found on a single page. The complete list of tasks is shown in Table 1. The study relied on a modified version of the Lemur Query Log Toolbar 3 for Firefox. 4 To begin a task, participants had to click a  X  X tart Task X  button. This prompted them with the task and a brief questionnaire about how well they under-stood the task and the degree to which they felt they knew the answer. They were asked to use any of four search en-gines: Bing, Google, Yahoo!, or Ask.com and were allowed to switch at any time. Links to these appeared on the tool-bar and were randomly reordered at the start of each task. Users were allowed to use tabs within Firefox.

For every query entered, users were prompted to describe their expectations for the query. Each time they navigated
Two participants completed eight tasks, but it took longer than expected, so seven tasks were used from then on. http://www.lemurproject.org/querylogtoolbar/ http://www.mozilla.com/en-US/firefox/firefox.html Table 2: Distribution of user-reported frustration for searches.
 Table 3: Distribution of user-reported task success. An error in the logging software caused the  X  X air X  and  X  X ood X  levels to be conflated. away from a non-search page, they were asked the degree to which the page satisfied the task on a five point scale, with an option to evaluate later. At the end of a search (de-termined by the user entering a new query or clicking  X  X nd Task X ), users were asked what the search actually provided relative to their expectations, how well the search satisfied their task (on a five point scale), how frustrated they were with the task so far (on a five point scale), and, if they indi-cated at least slight frustration (2 X 5 on the five-point scale), we asked them to describe their frustration.

When users finished the task by clicking  X  X nd Task X , they were asked to evaluate, on a five point scale, how successful the session was, what their most useful query was, how they would suggest a search engine be changed to better address the task, and what other resources they would have sought to respond to the task.

A total of 211 tasks were completed (one participant com-pleted one fewer task because of computer problems), feed-back was provided for 463 queries, and 711 pages were vis-ited. On the frustration feedback scale,  X 1 X  is not frustrated at all and  X 5 X  is extremely frustrated . In Table 2 we see that users reported frustration for half of their queries. The most common reasons given for being frustrated were: (1) off-topic results, (2) more effort than expected, (3) results that were too general, (4) un-corroborated answers, and (5) seemingly non-existent answers.
We find that users become frustrated even when they suc-ceed at their information seeking task. Table 3 shows the breakdown of user-reported task success. The majority of users reported their tasks were satisfied at the  X  X xcellent X  or  X  X erfect X  levels. Table 4 shows that while not finding the in-formation can be frustrating, even when the information is found, users can get frustrated. Users were successful in 62% of all tasks, but experience some degree of frustration in over a third of those successful tasks. This evidence supports the exploration of frustration modeling and differentiates it from task success or satisfaction prediction.
Since we measure self-reported frustration, the results may depend on the individual X  X  temperament as well as in-trospection. In Figure 1 we see that individuals from the test set do indeed vary in their self-reported frustration. The training set shows a similar trend. One phlegmatic individual did not report any frustration for any task. In our experimental section we will conduct leave-one-user-out Table 4: The number of tasks for which users were highly successful (levels 4 X 5) or not versus whether or not the task had any searchers for which the user was at least somewhat frustration. cross-validation to concentrate on the aspects of frustration that generalize across users.
In this section, we describe the models we use to predict frustration. We consider a number of features that have been used in previous studies, both in the IR and ITS fields. The first set of features include those derived from a client-side query log, while the second set includes those from three physical sensors.
The query log used in this study is client-side. Interactions between the user and the Web were recorded by means of a Firefox plug-in, adapted from the Lemur Query Log Tool-bar. The toolbar captures data including page focuses, click events, navigation events such as pressing the back and for-ward buttons, copy and paste actions, page scrolling, and mouse movements, among others. Every event includes a timestamp.

Given the section of the log that corresponds to a particu-lar task, we can derive search and task features (Section 3.1). The search features include search duration, pages visited, query length, max page scroll, and others. The task fea-tures span the start of the current task through the end of the most recent search. They include aggregates of the search features such as task duration, queries entered, aver-age search duration, total pages visited, average pages vis-ited per search, etc. Due to space constraints, we have not included a full listing of the forty-seven features. However, they are very similar to features used in previous query log analyses [8, 16].
We used three physical sensors in our study: a mental state camera, a pressure sensitive mouse, and a pressure sensitive chair. These are three of the four sensors used by Cooper et al. [4]; we use the same high-level readings. The camera software provides confidence values for six mental states: agreeing, disagreeing, unsure, interested, thinking, and confident. The mouse consists of six pressure sensors X  two on top and two on either side. Following Cooper et al. [4], we calculate the following feature with the values: where MS represents the six mouse sensors and the denomi-nator is the maximum pressure reading provided by any one sensor. This feature has a range from 0 to 6. Finally, the chair has three pressure sensors on the back and three on the seat. We derive three aggregate features: net seat change, net back change, and leaning forward [4]: where SS corresponds to the three seat sensors, BS the three back sensors, and t is the time step at which the feature is being computed. These were found to be useful features by both Cooper et al. [4] and D X  X ello et al. [5].

To derive features, we find the minimum, maximum, mean, and standard deviation for each reading over some time frame. Previous studies used window sizes of 150 sec-onds preceding the event being predicted [4, 11]. In our setting, we used three appropriate time frames: aggregating the features from the beginning of the task, from the be-ginning of the search, and thirty seconds preceding the end of the search where we are predicting frustration. The first two are equivalent to the query log task and search features, respectively. In addition, we decided to use two versions of each window: one that ignores any segments of time where a user was responding to a feedback prompt and a version that uses those time segments. See Section 4 for details about the feedback prompts.

In total, this yields (6 camera readings + 1 mouse read-ing + 3 chair readings)  X  { min | max | mean | stddev }  X  { task | search | 30-seconds }  X  { prompts | no-prompts } = 240 features.
We consider two baselines for this study: (1) always pre-dicting users are frustrated and (2) predicting they are frus-trated only when they have abandoned their query (i.e., they did not click on anything). Prior to the study, we believed this to be a reasonable approximation of frustration.
We construct seven additional models using logistic re-gression. All features were normalized per user prior to training. One model uses all of the features from both the sensors and the query logs and is referred to as QL+Sensors . Three of the models are based on sequential forward fea-ture selection on just the query log features, just the sensor features, and all the features. We name these SFS-QL , SFS-Sensors , and SFS-QL+Sensors , respectively. The sequential forward selection process starts with an empty feature set, considering all features  X  X nused. X  On each iteration of the algorithm, the unused feature that performs best in com-bination with the current pool of  X  X sed X  features is moved from the  X  X nused X  to the  X  X sed X  pool. The algorithm stops when no improvement in performance is made. The  X  X sed X  features are the final selection. For each of the SFS sets, all of the features from the set were available for selection.
We optimized our feature selection for F  X  =0 . 5 using macro precision and recall at the optimal logistic regression score threshold. For example, if a subset of features achieved a macro F-score of 0 . 6 with a score threshold of 0 . 5 and an-other subset achieved an F-score of 0 . 7 at a score threshold of 0 . 4, the latter would be selected and the corresponding threshold noted. The features selected for each model are listed in Table 5. The models show the order in which the features were selected. For the query log features, task Max-QrCharLen is the maximum length (in characters) of any Table 5: The top three models were greedily learned from subsets of the query log and sensor feature sets by sequential forward selection. The bottom model is derived from features that worked well for de-tecting when users would switch search engines [16]. The meaning of the sensor feature names are self-evident based on Section 5.2; query log features are described in Section 5.3. query seen so far in a task; task Duration is the time, in sec-onds, of the task; task QryPropUnq specifies the number of unique queries seen so far in a task; task AvgPgMaxScroll is the mean average max scroll per page per query in the task; search RsltsVisitedPrev is the number of results visited dur-ing a search that were visited previously in the task. We create a seventh model based on the features that White and Dumais [16] found were most important for pre-dicting when users would switch search engines, which we refer to as W&amp;D . The features in this model (Table 5) are: the most recent query X  X  length in characters ( search Qry-CharLen ), the average token length of the most recent query ( search AvgTokenLen ), the duration of the task in seconds ( task Duration ), the number of user actions in the task ( task ActionCount ), and the average number of URLs vis-ited per task for the current user ( user AvgURLCount ).
The eighth model we explore is the Markov Model Likeli-hood (MML) used by Hassan et al. [9] to predict task suc-cess. The input to this model is a sequence of events with time lapses between events included. After being trained on event sequences leading up to frustrated and non-frustrated instances, the model produces two scores: one calculates the likelihood of the event sequence being indicative of frustra-tion; the other calculates the likelihood of the event transi-tion times being indicative of frustration.

The scores produced range from 0  X  X  X  , with scores closer to 0 meaning the sequence is more consistent with frustra-tion, 1 being indifferent, and scores greater than 1 meaning the sequence is more consistent with non-frustration. We use the following variation of Platt smoothing to map the scores into the range 0  X  1.
 where x is the ratio and  X  and  X  are the smoothing param-eters, set to 4 as determined by our personal judgment on the range of ratios output in the training and development phases. Table 6: The event types used in the Markov Model sequences.

The sequence events we used for the MML model are listed in Table 6. The MML uses task-level event sequences X  X ach instance consists of the sequence of events starting from the beginning of the task up until the point where frustration is being predicted. Duplicate events were ignored and the time between events was recorded in seconds. We used the scores as features with a logistic regression classifier. We refer to this model as MML-time in the rest of the paper.

On the training/development data (Section 7), the W&amp;D model performed very well, so we decided to add the MML-time as an additional feature, creating the ninth model W&amp;D+MML-time . We felt that the sequence information cap-tured by the MML model would benefit the static features used by the W&amp;D model.
We randomly selected twenty of the thirty participants X  data for training and development. In the training/devel-opment set, we put each user X  X  data into its own fold, giving us a total of twenty folds. This avoids using a particular user X  X  data for both training and testing for cross valida-tion experiments. We used twenty-fold cross validation to select features and tune the score threshold at which F  X  =0 . 5 precision, and accuracy are computed.

For the results presented here, we re-trained our mod-els on all twenty users in the training/development set and tested on the remaining ten users. The macro-averages were calculated across users.

The training set contained 323 queries (51% of which were frustrated) for which there was feedback across 136 tasks for twenty users. The test set contained 137 query-feedback instances (44% of which were frustrated) across seventy-one tasks for ten users. One query from the training set and two from the testing set were removed due to logging errors that prevented the queries from being properly processed. We should note that during the study, two participants were accidentally given the same ordering of tasks and both users were randomly selected for testing. While this does increase the chances of ordering bias, we believe the effect is small due to the similar performance of the models on the training and testing set. Figure 1 shows the number of total and frustrated instances per user in the test set.

Table 7 shows the results of the experiments. Accuracy is measured across all users. The other three metrics are only measured for nine of the users, as user  X 25 X  never indi-cated frustration, causing the metrics to be undefined. Ac-curacy, precision, and F  X  =0 . 5 are calculated using the binary predictions created by thresholding each models X  output ac-cording to the development set. The no-clicks baseline is undefined for three of the metrics. For precision and F, this is because of undefined values for certain users. For MAP, both no-clicks and always-frustrated are undefined since Figure 1: The total number of feedback instances and frustrated instances per user, ordered by total instances.
 Table 7: Macro-level results for the models on the test set. Accuracy is over all ten users. The other three metrics do not include user  X 25 X . it involves ranking scores and both baselines produce binary scores.

The metrics show that the relatively simple W&amp;D model outperforms the rest for most metrics. Not all differences are significant, however. As there is no concise way to illus-trate significance for all pairs of systems for each metric, we will describe the most critical differences for F  X  =0 . 5 model is statistically different from all other systems with respect to F  X  =0 . 5 except SFS-QL . In turn, SFS-QL is statis-tically different from all the other systems except SFS-all and W&amp;D+MML-time .

Figure 2 shows the 11-point interpolated average precision across users for each model. Using all features outperforms the baseline, but is much worse then selecting only a subset of the features ( SFS-QL+Sensors ). This graph shows the W&amp;D , W&amp;D+MML-time , and SFS-QL+Sensors models in contention for the highest precision at different recall levels, all resulting in precision above 70% with 100% recall.

While not shown, when average precision is broken down by user, it is clear that no one model performs well for all users. This suggests that some form of personalization could be useful in determining how much influence each feature type should have for a particular user.

As we mentioned in Section 3.2, macro F  X  =0 . 5 is the met-ric we are most interested in. While we calculated this value using the score threshold for each model selected during de-velopment as shown in Table 7, observing how the score threshold affects the F-score can help us understand how stable each model is between the development and test sets. Figure 3 shows F  X  =0 . 5 as the score threshold ranges from 0 to 1 for select models. The W&amp;D model reaches its optimum on Figure 2: Macro 11-point interpolated average pre-cision across the test users for each model.
 Table 8: The weights learned for the features in the W&amp;D model ranked by influence (intercept = -0.406). the test set at nearly the same score threshold as in the de-velopment set. In contrast, the W&amp;D+MML-time and MML-time models peak at a lower and higher threshold, respectively, on the test set. While the maximum for SFS-Sensors on the test set is close to its score threshold, there is a substan-tial decrease in performance just past its optimal threshold. This suggests that several of the trained models are sensitive to new data.

Turning to the learned weights for the best performing model, the W&amp;D feature weights are listed in Table 8. The model is likely to predict frustration for a lengthy session in which few URLs have been visited and actions taken, and where the most recent query has many characters overall, but very few per term.
Lab studies such as this one are highly controlled X  X n terms of users, tasks, timing, environment, etc. X  X nd as a result the results are not necessarily directly transferable to more realistic settings. Despite this, we feel that our find-ings are still generally applicable to real Web search and from which many practical observations can be gleaned. In this section we will discuss a few of these observations as well as some questions that arose.

The results suggest that a few relatively simple query log features can reliably predict frustration. This is useful in de-veloping a search system that predicts frustration. However, some information necessary for the best performing model, W&amp;D , is client-side, such as the average number of URLs vis-ited in other tasks and the number of actions performed by Figure 3: F  X  =0 . 5 using macro precision and recall for select models over the ten test users. The points de-note the threshold that was chosen in development for each model. the user during the current task. This means a system would likely need to be implemented an a browser add-on.

The features we found most useful for detecting frustra-tion are the same as those White and Dumais [16] found most useful for detecting when a user will switch search en-gines. This suggests that this feature set may have a broader scope of predictive power for related tasks, such as task sat-isfaction and query abandonment.

One of the surprises of this research for us was the per-formance of the sensors. Given the results of Cooper et al. [4], we expected the sensors would strongly correlate with user-reported frustration and that we would struggle to find a set of query log features to even come close to the per-formance of the sensors. In fact, the opposite was true. There may be several reasons for this. One reason might be that the study occurred in an open room and up to five participants were active in the study at a given time. The presence of other participants may have affected how an in-dividual maintained their composure. Arguments could be made that this is or is not a realistic situation; it proba-bly varies by where people search (e.g., in an open office space vs. a cubicle vs. a living room). Another possibility is that the way the feedback was gathered upset the natural reactions of the participants. However, we hope that includ-ing a set of features that ignores sensor readings during the time intervals when prompts were shown removed such bias. A third reason may be that the users X  physical reaction to frustration may not have aligned with their report of the emotion and our attempts to discretize the sensor readings may have affected the sensors X  predictive ability.

Another surprise was the performance of the MML-time model. We thought that the sequence data would have been more helpful than it was. One reason for its performance may be the event language. We used a simple, high-level set of events. This is in contrast to Hassan et al. [9], who used events such as the type of link clicked on a search re-sults page. Adding more advanced features may be more useful. However, there is another problem with sequences on our data set: data sparsity. While our data set is suffi-cient for static feature classification, there is likely an insuffi-cient number of unique sequences to build a reliable model. A Web-scale data set, such as those used in other studies [6, 9], would be more useful in combination with this model. The trade off is that user-reported frustration is not included with Web-scale search logs.

Lastly, the greedy sequential forward selection algorithm did not provide a feature set that performed as well as or better than the W&amp;D model on the training data. Future work should explore more advanced selection techniques to find a better approximation for this task.
This study has several implications for designing systems that detect frustration. First, the best performing models do not rely on sensor data. Using query log data alone is suf-ficient to predict frustration well. Second, in light of the first implication, a Web browser add-on would be a viable vehi-cle to deliver frustration detection. Browser add-ons have access to all of the query log statistics needed to predict frustration. Third, the model that performed best uses five relatively simple query log features in a logistic regression model, making classification fast, simple, and usable in re-altime. A fourth implication is that one model, such as the W&amp;D model, will not work equally well across all users. Thus, personalization may be important and future work should investigate how it would be useful.
In this paper we used features derived from a client-side query log and three physical sensors to predict user-reported frustration during a controlled study of Web search. We compared several models based on the information retrieval [8, 9, 16] and the intelligent tutoring systems [4, 5, 11] liter-ature. We found that using a few simple query log features performed best.

In addition, the toolbar and all of the data collected dur-ing the study are being made publicly available. 5 more data was collected than was used in this paper, such as user-reported page relevance, query satisfaction, and task satisfaction. All pages viewed were downloaded, including search results pages. Mouse movements were also collected, from which a useful feature could be derived for tasks such as frustration prediction, among others. The data set serves as a means for others to compare against the results of this paper, as well as provide insights into ways to build on the study design used. Future work should explore incorporat-ing other features from this rich data set, such as mouse movements, into frustration models.

One direction of future work is investigating how the mod-els presented in this paper transfer to real searching environ-ments, where there are user-defined, and likely interleaved, tasks. Another direction of future work is finding a set of useful features from the less-rich server-side information. This would allow a system to be built that does not depend on add-ons or other client-side instrumentation.

We intend to explore what types of interventions are ap-propriate for addressing searcher frustration and when to use them. One plausible design is to present a user with a list of search assisting technologies (e.g., explicit relevance feedback) when frustration is detected. Alternatively, clas-sifying types of frustration may be helpful; one or more in-terventions could be presented to the user based on the type of frustration. Future work should consider the effects of interventions and presentation on frustration so that the in-terventions are not counterproductive.
 This work was supported in part by the Center for Intelligent Information Retrieval, by NSF IIS-0910884, and through collaborations with Yahoo! scientists as part of the Yahoo! Faculty Research and Engagement Program. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors. We would like to thank Beverly Woolf, David Cooper, and Winslow Burleson for loaning the sensors and logging software.

