 Even though people are attracted by large, high quality rec-ommendation sets, psychological research on choice overload shows that choosing an item from recommendation sets con-taining many attractive items can be a very difficult task. A web-based user experiment using a matrix factorization algorithm applied to the MovieLens dataset was used to investigate the effect of recommendation set size (5 or 20 items) and set quality (low or high) on perceived variety, recommendation set attractiveness, choice difficulty and sat-isfaction with the chosen item. The results show that larger sets containing only good items do not necessarily result in higher choice satisfaction compared to smaller sets, as the increased recommendation set attractiveness is counteracted by the increased difficulty of choosing from these sets. These findings were supported by behavioral measurements reveal-ing intensified information search and increased acquisition times for these large attractive sets. Important implications of these findings for the design of recommender system user interfaces will be discussed.
 H.1.2 [ Models and principles ]: User/Machine Systems-software psychology; H.4.2 [ Information Systems Appli-cations ]: Types of Systems-decision support ; H.5.2 [ Information Interfaces and Presentation ]: User Interfaces-evaluation / methodology, interaction styles, user centered design Measurement, Design, Experimentation, Human Factors. Decision making, Choice overload, Recommender systems, Information overload
Theoretically, the ever-growing amount of digital content should increase our opportunity to find content that fits our personal needs. However, a user of a typical multimedia sys-tem may experience information overload since only a few of the items in a content catalogue are in the user X  X  field of in-terest. The goal of recommender systems is to support users in content discovery and exploration by providing items that match their personal preferences. Based on preference in-formation (e.g. the user X  X  ratings of known items, or past purchases) a recommender system predicts which items the user will like. The underlying idea of most recommender system prediction methods is that similar users have similar interests, and therefore like the same items. By compar-ing the user X  X  ratings with those of other users, the system predicts the ratings of all other items in the catalogue. Con-sequently, the system produces a ranked list of personalized recommendations, comprising those items with the highest predicted ratings. This pre-filtering of the information space reduces the information overload, as the user does not have to consider irrelevant items anymore.

As H  X  aubl and Trifts [4] have argued before, a recom-mender system is a very helpful tool to overcome the infor-mation overload in the initial screening stage of the decision making process. However, the way recommender systems reduce the information overload does not necessarily imply that the actual choice in the subsequent decision stage be-comes much easier. Recommender systems present the user with recommendations that are all likely to be very rele-vant to the user X  X  needs. Paradoxically, if this set of high-quality recommendations becomes too large, it can move people from a situation of information overload into a situ-ation of choice overload.

Choice overload refers to the difficulty of choosing from a large set of good alternatives. It explains why even though people are more attracted to larger item sets, they usually find it easier to choose from a smaller set, and they are also generally more satisfied with their actual choice when choos-ing from a smaller set [5, 7, 18]. Choice overload, as well as its underlying psychological processes and moderators, has been studied extensively in recent years [3, 5, 13, 15, 14, 18]. However, to our knowledge the phenomenon has not been studied in the context of recommender systems that provide personalized, highly attractive recommendations. As we will argue below, the very nature of these personalized recom-mendations affects important mediators of choice overload, making choice overload especially likely to occur with larger personalized recommendation sets.

The present paper investigates the extent to which choice overload occurs in recommender systems, by means of a web-based user experiment using a movie recommender that pro-vides personalized recommendations. The size and quality of the recommendation set will be varied, and in line with the framework for recommender system evaluation presented in [8] a post-experimental questionnaire will measures the users X  perceived recommendation set attractiveness, choice difficulty, and satisfaction with the chosen item. Addition-ally, behavioral measures such as decision times and infor-mation search will be monitored to understand if and when choice overload occurs.
Iyengar and Lepper [7] were the first to demonstrate the choice overload effect, which refers to the fact that people are attracted by large choice sets, but that such large sets at the same time increase choice difficulty and reduce choice satisfaction. In their study they set up a taste booth for different types of jams at a supermarket, and tested whether a set of 24 types of jam was more attractive than a smaller set consisting of only 6 types of jam. The results showed that people were attracted more by the booth with the large set than by the booth with the small set of jams. However, after tasting the different jams, people who tried jams from the smaller set were more likely to subsequently buy jam (30%) than people who tasted from the large set (3%). Afterwards, people who bought jam from the smaller set expressed a higher satisfaction with their purchase than did people who bought jam from the larger set.

What is the underlying mechanism for the choice overload effect? Primarily, by increasing the size of the item set (in the above example going from 6 to 24 jams), the set becomes more attractive because the total benefit of the set (the sum of the benefits of each option) increases [2]. At the same time however, this increases the psychological costs associ-ated with the decision, making the choice more difficult. The benefits provided by the many options are outweighed by the opportunity costs of having to compare all the alternatives (comparing more items is more work), the increase in poten-tial regret associated with the choice (a larger set increases the chance of making the wrong decision) and the increased expectations that accompany larger sets (items chosen from larger set are expected to have a higher quality) [2, 16, 17]. As a result, for larger sets an increase in choice deferral (not choosing at all) or a decreased satisfaction with the chosen option is observed. Reutskaja and Hogarth [13] argue that the first factor, the benefits of a large set  X  X atiate X , whereas the costs increase at a faster rate than the benefits. This re-sults in a inverted U-shape of satisfaction (with the chosen item) as function of the set-size (see Figure 1).
 Figure 1: Satisfaction as a function of benefits, costs and number of items (adapted from [13] and [17]).

Summarized, choice overload seems to be the result of the interplay of two underlying concepts: item set attractive-ness and choice difficulty, and their opposing forces on the satisfaction with the chosen item. Below we will discuss this interplay more specifically in the context of recommender systems.
The nature of the item set plays an important role in choice overload. Not all large item sets seem to result in choice overload, and there are several studies that fail to replicate the effect. A meta-analysis across 50 studies seems to suggest that the overall effect size is virtually zero [14]. These authors argue that there are some necessary precondi-tions for choice overload to occur. A larger choice set might only result in decreased satisfaction and increased choice deferral when there are no strong prior preferences or domi-nant options in the item set [14]. Choosing from an item set also becomes more difficult when there is little difference in the attractiveness of the items [1, 3]. In the psychological and marketing literature, the choice overload effect is mostly studied using item sets that are not personalized and there-fore contain a wide variety of different items that do not necessarily lie in a person X  X  field of interest. Scheibehenne et al. [14] argue that the lack of control over the similarity and tradeoff difficulty between items might be one reason why studies on choice overload show such different results.
In contrast to these uncontrolled traditional item sets, rec-ommender systems provide sets that are personalized and optimized 1 . The strong effort by the Recommender Systems community to improve recommendation algorithms [6] has resulted in recommender systems that provide very accurate predictions. The  X  X op-N X  recommendations that are gener-ally shown by these systems usually consist exclusively of items that the user finds very attractive, resulting in sets in which dominating alternatives and strong prior preferences for specific items are quite unlikely (as the original set of items from which the recommendation is made is large and many items will be unfamiliar to the user). In other words, all items will have more or less the same high quality, and
In this paper we will therefore make the distinction be-tween traditional  X  X tem sets X  and personalized  X  X ecommen-dation sets. X  none of the recommended items will evidently be better than all the others.

One of the underlying moderators of choice overload is the comparability of items. If items are attractive but incompa-rable it is harder to make tradeoffs, subsequently increasing the potential choice overload [14]. Choice difficulty might arise when an item set encompasses difficult tradeoffs, with items containing unique features that are difficult to com-pare. This has already been indicated by Fasolo et al. [3] who showed that choice effort and difficulty might increase as the entropy of the item set increases. Entropy is a mea-sure of complexity that encompasses the number of items available, the number of levels within each attribute, and most importantly the distribution of items on the attributes. For instance, a movie can have several attributes that are important for a person, such as genre, actors, director etc. The entropy of a set of movies is high if the attribute lev-els are evenly distributed, i.e. the sets contains movies that differ strongly on many of the important attributes, and dif-ferent movies excel at different attributes (e.g., one has a favorite actor, another has a favorite director). Exactly this will often be the case with the personalized item sets gener-ated by recommender systems (recommendation sets): the algorithms predict liking, and therefore might come up with very diverse items that are all liked by the user, but that are not necessarily similar or easy to compare. For example, in the case of movies it is quite likely that a recommendation list contains items of different genres, with a large diversity in actors and movie directors, but that all have a predicted user-rating of five stars. This suggests that recommenda-tion sets that are generated by recommender systems might be fruitful candidates for choice overload, and it raises the question how large recommendation sets should be.

First of all, as recommendation sets are personalized, there is little additional value (compared to regular non-personal-ized sets) in adding items, as the top items will be present in both lists, and as long as the recommendations are relatively accurate, the shorter list is likely to already contain many good items. Second, due to the fact that each item will be highly attractive, a larger recommendation set will result in increased choice difficulty. Especially the trade-off difficulty will be amplified compared to regular non-personalized item sets, as comparisons between items will be hard to make (items are similar in attractiveness, yet dissimilar on many of their attributes).
The theoretical work discussed above suggests that two important factors seem to affect satisfaction with the chosen items in recommender systems. On the one hand, we like to receive sufficiently large and varied attractive item sets to be able to select the best items. Satisfaction thus is driven by item set attractiveness, and more items might potentially increase our satisfaction. On the other hand, large attractive item sets increase the number of difficult tradeoffs that have to be made, increasing the choice difficulty, and potentially decreasing the satisfaction.

To test these predictions, we manipulate both the size of the recommendation set as well as the quality, resulting in three conditions. The baseline is a small set of five recom-mendations. A set of twenty recommendations is used to test the effect of the size of the recommendation set. The effect of recommendation set quality is tested using a set of twenty recommendations comprised of the best five, like in the small set, extended with fifteen items sampled further down the ranked list. This last list has a lower quality and therefore should result in less tradeoff difficulty, as there will be less  X  X ompetition X  between the top items and the items further down the list. However, the effort of having to in-spect and compare twenty items remains, and this condition helps us distinguish whether choice overload is a function of the number of items or the quality of the items (something that cannot be easily tested with regular, non-personalized sets). To study the psychological mechanisms underlying choice overload, a post-experimental questionnaire measures subjective concepts such as recommendation set attractive-ness, choice difficulty, and satisfaction with the chosen item. The questionnaire will also measure the perceived variety of the recommendation set since varied assortments are envi-sioned as more attractive [3, 19, 20]. Variety has also been shown to be a moderator of the choice overload effect [15]. Finally, self-reported expertise is also measured, because in the meta-analysis performed by Scheibehenne et al. [14] ex-pertise was one of the few significant moderators, showing that more expertise reduced the choice overload effect.
Most user-centered studies in decision-making and recom-mender systems literature try to measure subjective con-cepts with single questions [7, 13, 21]. In contrast, we measure our subjective concepts using multiple questions, as these concepts encompass multiple aspects that cannot easily be captured by a single question [9]. Moreover, this enhances the robustness of our measurements and the gen-eralizability of our results. We also capture behavioral mea-sures of choice overload by registering the click streams of users, which can be used to analyze decision times and in-formation search processes.
For the experiment a movie recommender was developed based on the BBC iPlayer 2 User Interface, using a Matrix Factorization algorithm for the calculation of the recom-mendations. The dataset used for the experiment was the 1M MovieLens dataset 3 which consists of one million rat-ings by 6040 users on 3900 movies. We further enriched the MovieLens dataset with a short synopsis, cast, director and a thumbnail image of the movie cover taken from the Internet Movie Database 4 . The Matrix Factorization algo-rithm used 10 latent features, a maximum iteration count of 100, a regularization constant of 0 . 0001 and a learning rate of 0 . 01. Using an 80-20 split validation on the original dataset, this specific combination of data and algorithm re-sulted in a RMSE of 0.875 and a MAE of 0.685, which is up to standards. An overview of metrics is given by [6]. The experiment was designed as a between-subjects experiment in which each participant was assigned to one of three exper-imental conditions. Each condition refers to a manipulation in which we altered the size and contents of the recommen-dation set.
The three recommendation sets were manipulated in such a way that sets differed in either recommendation set size http://http://www.bbc.co.uk/iplayer
Available at: http://www.grouplens.org http://www.imdb.com Top-5 1 2 3 4 5 ---------------Top-20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 or quality. Our baseline recommendation set consisted of the five items with the highest predicted ratings, therefore named Top-5. In order to manipulate the size of the items set, our second recommendation set, named Top-20, con-sisted of the twenty items with the highest predicted ratings. We manipulated the quality of the recommender set by cre-ating a third recommendation set that did not fit the user X  X  preference perfectly, i.e. by using items with lower predicted ratings that are ranked lower in de predicted recommender set. This low quality recommendation set consisted of the Top-5 items appended with fifteen items with a linear spac-ing of 100 ranked positions from each other. Because of the linear spacing, this set will be called Lin-20. The rankings are presented in Table 1. The predicted ratings ranged from 5/5 to 4/5 stars in the Top-5 and Top-20 conditions and from 5/5 to 3/5 stars in the Lin-20 condition. In order to prevent the recommendation set from being too unattractive, items with a lower predicted rating were avoided.
The experiment was designed as a web application and consisted of four parts. The first part introduced the ex-periment explaining the procedure and the different steps in the experiment. The second part acquired rating informa-tion from the participant in order to provide the participant with personalized recommendations. In this phase the par-ticipants were asked to rate a total of ten movies. They were presented with eight randomly selected movies at a time, with the instruction to rate the movies they were fa-miliar with. After inspecting (and possibly rating) the eight movies shown, users could get a new list of movies by press-ing a button. When the participant had entered ten or more ratings in total, pressing the renew button would lead them to the third part.

In the third part the participant received her recommen-dations. Depending on the condition, the participant was shown a rank-order list of either five or twenty movies, rep-resented by a movie title. For half of the participants the predicted rating in stars and one point decimal value were shown next to the title, for the other half no predicted rat-ings were provided 5 . If the participant hovered over one of the titles, additional information appeared in a separate preview panel. This additional information consisted of the movie cover, the title of the movie, a synopsis, the name of the director(s) and part of the cast.

Participants were asked to select the movie they would prefer to watch from this list, thereby choosing one partic-ular movie. After making the choice, the participants were asked to complete a questionnaire consisting of 29 items. These items intended to measure the perceived recommen-dation set variety, recommendation set attractiveness, choice difficulty, and satisfaction with the chosen item, as well
As our analyses did not reveal any major difference between participants that were given predicted ratings and those that were not, we aggregated the data across these two groups as some other relevant covariates (e.g., self-reported movie-expertise) useful for the statistical analysis.
Participants for this study were gathered using an online participant database. Participants were compensated with 3 euro (about 4 US dollars) for participating. After removing participants that did not complete the questionnaire or that took too little time in completing the questionnaire, a total of 174 participants remained (mean age: 26.8 years, sd=8.6, 89 males and 85 females). The questionnaires consisted of 29 questions, all but one of which were measured using a 7-point scale 6 . Most items consisted of statements to which users could agree or dis-agree to some extent; some items used the 7-point scale to measure a specific bipolar quality (e.g.  X  X ery similar X  to  X  X ery varied X ). In factor analysis and structural equation model discussed below, all 7-point scaled items are treated as ordinal measures. The internal quality of the measured constructs was validated using an exploratory factor analysis with unweighted least squares extraction and geomin rota-tion. Subsequently, significant residual correlations were an-alyzed using an exploratory factor analysis in a confirmatory factor analysis framework. Finally, measurement invariance with relation to our conditions was tested using a confirma-tory factor analysis with our condition as a covariate.
Based on these analyses three items were deleted having unexpectedly large significant cross-loadings, one item was deleted due to a high residual correlation with several other items, and one item was deleted due to an insignificant com-munality. The final exploratory analysis resulted in an ade-quate model fit when extracting our five factors: perceived recommendation set variety (4 items, e.g.  X  X any movies in the list differed from each other X ), recommendation set at-tractiveness (4 items , e.g.  X  X he list of recommendations was attractive X ), choice difficulty (3 items, e.g.  X  X he choice task was overwhelming X ), satisfaction with the chosen item (6 items, e.g.  X  X  think I would enjoy watching my chosen movie X ), and movie expertise (3 items, e.g.  X  X ompared to my peers I consider myself a movie expert X ).

Factor determinacies, measuring the squared correlation between the factors and measuring items, were high (.937, .968, .867, .961 and .967 respectively). Interpreting the fac-tor scores, we decided to allow one cross-loading in our struc-tural equation model: one item originally meant to measure choice satisfaction ( X  X  think I chose the best movie among the available options X ) is also (negatively) influenced by choice difficulty. i.e. the question  X  X n the end I was in doubt between . . . movies X  allowed users to enter a numerical value arrows are the coefficients, standard errors and significance of the paths. To corroborate the subjective measurements obtained by the questionnaires, participants X  clicking behavior (process data) was logged. Participants were able to see more infor-mation about a movie by hovering over the movie X  X  title and these events ( X  X cquisitions X ) were logged by the system. The average time participants spend inspecting each item and the number of times a participant looked at each item were extracted for further analysis. Only acquisitions that lasted at least 1 second were taken into account, as shorter acqui-sitions might indicate that the user did not really look at the information, but for example moved the mouse over the item while switching between other items. The acquisition data will provide an additional measure of effort, beside the subjective measurement of choice difficulty.
To understand the interplay between recommendation set attractiveness, choice difficulty and satisfaction with the cho-sen item, a Structural Equation Model (SEM) was fitted on the data. Figure 2 shows the resulting path model of the pri-mary constructs and manipulations in our study. The SEM included both experimental manipulations and the measured subjective constructs. The significant relations among con-structs are being visualized by means of an arrow (with the thickness of the arrow representing the strength of the rela-tionship). The structural equation model showed adequate model fit (Chi-square(29) = 46.442, p = .0212, which is close to the prescribed minimum of .05, CFI = .916, TLI = .936, RMSEA = .059, which is close to the prescribed maximum of .05). R-square values for dependent variables were .088 (perceived recommendation set variety), .284 (recommenda-tion set attractiveness), .234 (choice difficulty), .609 (choice satisfaction). Recommendation set variety is only predicted by our manipulation of the recommendation set; most of the residual variance in this concept depends on the user X  X  ten-dency to call something varied, and on the recommendation process itself. The low R-square value is therefore expected. The same holds for recommendation set attractiveness and choice difficulty.

The path model in Figure 2 depicts the relations between attractiveness, choice difficulty and satisfaction and variety and how these constructs are affected by our manipulations. As hypothesized before, and as suggested in the literature, we observe that satisfaction with the chosen item is the re-sult of two opposing forces: a positive effect of attractive-ness and a negative effect of choice difficulty. Furthermore, attractiveness also affects difficulty, showing that more at-tractive sets increase choice difficulty. Attractiveness itself depends strongly on the perceived variety in the recommen-dation set: the more varied a recommendation set, the more attractive the set.

This path model supports the theoretical relations we ex-pected, but does not tell us whether choice overload did occur in this data set. For this we need to compare the differences in satisfaction by means of our experimental ma-nipulations. A simple test of means (see figure 3) shows that both the Top-20 and Lin-20 conditions did not reveal any increase or decrease in satisfaction, relative to the Top-5 condition, showing that extending the item list did not provide any benefit in terms of satisfaction to the users.
Even though our manipulation does not seem to cause any differences in choice satisfaction, this may likely be due to the inherent tradeoff between recommendation set quality and choice difficulty. In order to test this hypothesis, we consider the effect of our manipulations on the underlying subjective constructs in the path model. For this we com-pare Top-5 with Top-20, and Top-5 with Lin-20 separately. As shown in Figure 2, we dummy-coded our 3 conditions as two factors: Top-20 versus Top-5, and Lin-20 versus Top-5.
Relative to Top-5, the Top-20 condition positively affects the perceived variety of the recommendation set and the Figure 3: Mean factor scores for satisfaction. Error bars represent the 95% confidence interval. difficulty of the choice. As variety increases attractiveness, we see that, relatively to the Top-5 condition, the Top-20 is more varied (and therefore more attractive) but more dif-ficult, and the difficulty levels out the positive effect of a more varied set, which explains the insignificant difference in satisfaction for Top-20 relative to Top-5 in Figure 3.
Comparing Top-5 with Lin-20, we observe that the Lin-20 set is more varied than Top-5, but also directly affects the recommendation set attractiveness in the opposite (neg-ative) way. Interestingly, Lin-20 does not affect choice dif-ficulty directly, but does have a positive residual effect (i.e. controlling for recommendation set quality and choice diffi-culty) on satisfaction, relative to the Top-5 condition. This direct residual effect represents the idea that satisfaction with the chosen item might increase when it can be con-trasted against the inferior other items that are in the tail of the Lin-20 distribution. However, as the set itself is re-garded as less attractive, the net effect of Lin-20 compared to Top-5 in terms of satisfaction is zero.

Our questionnaire also contained items that measured ex-pertise and our analysis shows that an increased expertise positively affects recommendation set attractiveness and per-ceived variety, showing that experts are better at assessing the differences in these recommendation sets.
With regard to the choices made by participants, only 41% of the participants in Top-20 condition selected an item from the first 5 on the list, whereas for the Lin-20, 74% percent selected an item from the first 5. In the Top-20 condition the median rank of the selected option was 8.5, which dif-fered significantly from the median rank of 3.0 for the Lin-20 condition (Mann-Whitney U = 1299 . 50, n 1 = 58, n 2 = 64, p &lt; .01 two-tailed). This indicates that for the Top-20 condi-tion, participants selected items further down the list than for Lin-20, as would be expected given that the items in the tail of Lin-20 condition had lower predicted ratings. As a consequence, this difference in choice pattern would suggest that in Top-20 participants should also be inspecting more items further down the list, which potentially might lead to increased effort in terms of acquisitions and looking times.
Figure 4 depicts, for the Top-20 en Lin-20 condition, the acquisition time and acquisition frequency of each item in the recommendation set. Noteworthy is that acquisition fre-quencies and times are indeed considerably higher for Top-20 than for Lin-20, indicating that participants spend more effort on the Top-20 set, despite the fact that for this condi-tion all options were high on the ranked list (i.e. predicted to be good). A repeated measures ANOVA shows that par-ticipants facing the Top-20 set, as compared to participants facing the Lin-20 set, spend twice as much time looking at each item ( M T op  X  20 = 2 . 8 sec, M Lin  X  20 = 1 . 4 sec, F 4 . 22 , p &lt; . 05). A repeated measures ANOVA on acquisition frequency revealed that average acquisition frequency for each item was significantly higher for Top-20 ( M T op  X  20 analyses indicate that participants in the Top-20 compared to the Lin-20 spent much more effort in making the decision, corroborating the reported choice difficulty in the question-naire.
The goal of this paper was to study the choice overload effect, and to determine whether recommender systems are inherently prone to cause this effect. Our results show that the choice overload effect is controlled by an interplay be-tween recommendation set attractiveness and choice diffi-culty. In our experiment, each type of recommendation set resulted in the same level of satisfaction. The causes of this choice satisfaction are however different for each condition: Top-5 is limited in variety, but easy to choose from; Top-20 is varied but difficult to choose from; Lin-20 is varied and easy to choose from, but has a low attractiveness. Even-tually, the measured effect on choice satisfaction is leveled out. Further behavioral measurements show that there is an increase in cognitive cost (effort) when a user is faced with a large recommendation set containing all good items (Top-20) compared to a large recommender set containing some inferior items (Lin-20).

For the development of recommender systems, our results pose the question whether it is necessary to confront users with a large recommendation set, since a small set may al-ready incorporate enough good items. Although increas-ing the number of items increases the variety of the set, the choice will become more difficult. What is the optimal number of recommendations? If we assume that the under-lying relation between item set size and satisfaction indeed follows an inverted U-shape (see Figure 1), then the iden-tical satisfaction observed for Top-5 versus Top-20 indicate that a recommendation set that contains between five and twenty items has the best of both worlds. Further research is needed to establish the ideal number, but based on our current results we hypothesize that a set of, say, seven to ten items would be very attractive, quite varied, and still be manageable for the users. These results however, should be taken with some consideration. The current experiment measured satisfaction with the chosen item, rather than ac-tual purchases. Though many studies on choice overload have used such a satisfaction measure, further research could test whether other measures and more realistic scenarios in which people could actually try and buy items also reveal the same detrimental effects of larger items sets. the mean.

Our results also suggest that including a number of qual-itatively inferior items to the recommendation set (like in the Lin-20 manipulation) would not be detrimental on sat-isfaction, since this increases the perceived variety, which has again a positive effect on set attractiveness. More-over, users can contrast their choice with these lower-quality items, which also increases choice satisfaction (as was ob-served in a direct positive effect of our Lin-20 manipulation on satisfaction in the path model). These results are in line with those of others, that have shown that the user expe-rience and user satisfaction are not solely affected by the overall accuracy of a recommender system [6, 11, 12]. For example, Ziegler et al. [21] applied a topic diversification approach in order to decrease intra-list similarity and inves-tigated the effect on user satisfaction. Results show that under certain conditions there was a decrease in average ac-curacy while participants noted a higher satisfaction with the overall recommendation set.

Together with the results reported in the present paper, these findings suggest that it is worthwhile to further in-vestigate the user experience of recommender systems from a user perspective, using appropriate subjective and behav-ioral measurements [10]. The present paper also suggests that it is worthwhile to test the underlying assumptions made in recommender systems on how people make deci-sions, using knowledge from the psychology of decision mak-ing, like other research has shown before. For example, com-plex decisions have multiple stages in which decision makers need to be supported differently [4], and experts and novices might differ in the way they want to express their preferences [9]. Approaching the problems from such a psychological perspective might help to pave the way towards better and more user-friendly recommenders systems that are adapted to the cognitive limitations and capabilities of users.
We gratefully acknowledge the funding of our work through the European Commission FP7 project MyMedia (www.mymediaproject.org) under the grant agreement no. 215006. For inquiries please contact info@mymediaproject.org. [1] R. Dhar. The effect of decision strategy on deciding to [2] K. Diehl and C. Poynor. Great Expectations?! [3] B. Fasolo, R. Hertwig, M. Huber, and M. Ludwig. [4] G. H  X  aubl and V. Trifts. Consumer decision making in [5] G. Haynes. Testing the boundaries of the choice [6] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and [7] S. S. Iyengar and M. R. Lepper. When choice is [8] B. Knijnenburg, L. Meesters, P. Marrow, and [9] B. P. Knijnenburg and M. C. Willemsen.
 [10] B. P. Knijnenburg, M. C. Willemsen, and S. Hirtbach. [11] S. M. McNee, I. Albert, D. Cosley, P. Gopalkrishnan, [12] S. M. McNee, J. Riedl, and J. A. Konstan. Being [13] E. Reutskaja and R. Hogarth. Satisfaction in choice as [14] B. Scheibehenne, R. Greifeneder, and P. Todd. Can [15] B. Scheibehenne, R. Greifeneder, and P. Todd. What [16] B. Schwartz. The paradox of choice. Ecco New York, [17] B. Schwartz. The tyranny of choice. Scientific [18] A. Shah and G. Wolford. Buying behavior as a [19] I. Simonson. The effect of purchase quantity and [20] E. Van Herpen and R. Pieters. The variety of an [21] C.-N. Ziegler, S. M. McNee, J. A. Konstan, and
