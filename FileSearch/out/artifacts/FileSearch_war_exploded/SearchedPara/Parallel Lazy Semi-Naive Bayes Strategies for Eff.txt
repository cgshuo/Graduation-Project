 Automatic Document Classification (ADC) is the basis of many important applications such as spam filtering and con-tent organization. Naive Bayes (NB) approaches are a wi-dely used classification paradigm, due to their simplicity, ef-ficiency, absence of parameters and effectiveness. However, they do not present competitive effectiveness when compa-red to other modern statistical learning methods, such as SVMs. This is related to some characteristics of real do-cument collections, such as class imbalance, feature spar-seness and strong relationships among attributes. In this paper, we investigate whether the relaxation of the NB fe-ature independence assumption (aka, Semi-NB approaches) can improve its effectiveness in large text collections. We propose four new Lazy Semi-NB strategies that exploit dif-ferent ideas for alleviating the NB independence assump-tion. By being lazy , our solutions focus only on the most important features to classify a given test document, over-coming some Semi-NB issues when applied to ADC such as bias towards larger classes and overfitting and/or lack of ge-neralization of the models. We demonstrate that our Lazy Semi-NB proposals can produce superior effectiveness when compared to state-of-the-art ADC classifiers such as SVM and KNN. Moreover, to overcome some efficiency issues of combining Semi-NB and lazy strategies, we take advantage of current manycore GPU architectures and present a mas-sively parallelized version of the Semi-NB approaches. Our experimental results show that speedups of up to 63.36 times can be obtained when compared to serial solutions, making our proposals very practical in real-situations.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval; I.5.4 [ Applications ]: Text proces-sing; Text Classification; Semi-Naive Bayes; Parallelization c  X 
The amount of data created and shared nowadays in all ty-pes of platforms reached unprecedented levels, making the organization and the extraction of useful knowledge from this huge amount of data one of the biggest challenges in Computer Science. Machine learning techniques, such as automatic document classification (ADC), have demonstra-ted to be a viable path towards this goal. Particularly, ADC techniques aim at building effective models capable of asso-ciating documents with well-defined semantic categories in an automated way. ADC techniques are the core component of many important applications such as spam filtering [11], organization of topic directories [7], identification of writing styles or authorship [29], among many others.

ADC methods usually exploit a supervised learning para-digm [22], i.e., a classification model is first  X  X earned X  using previously labeled documents (training set), and then used to classify unseen documents (the test set). There is a plethora of supervised ADC algorithms available in the li-terature, such as nearest-neighbor classifiers [27], support vector machines [8], boosting [21] and Bayesian models [17]. In this paper, we focus on the latter approach, due to its simplicity, efficiency, and effectiveness in several scenarios. In particular, we focus on Naive Bayes approaches, the most widely used Bayesian paradigm for text classification.
Although a widely used classification paradigm in ADC, other statistical learning methods, such as SVMs, have pre-sented superior effectiveness when compared to NB approa-ches. This is related to some characteristics present in real document collections, such as class imbalance, feature spar-seness and strong relationship among attributes, that can compromise some of the Naive Bayes premises [19, 14]. The class imbalance problem [18] happens when the number of documents of one or few classes covers most of the docu-ments in a dataset. This naturally introduces a bias in the trained classifier towards assigning most unseen documents to the largest classes, incurring in a poor classification ef-fectiveness in the minority classes, the most important ones in many applications (e.g. email spam, vandalism). The sparseness problem is related to the low frequency of certain features (i.e., terms/words) in some documents [18]. In NB, the conditional probability of a term a j given a class c estimated using all training documents from c i in which a occurs. Such conditional probabilities may be negatively af-fected if a j occurs in only a few documents of a given class, especially in smaller ones (i.e., those with few documents). The NB assumes that each attribute is independent, which actually is a false assumption in most of scenarios. It is unrealistic to expect that this assumption will hold in the real world, as correlations among attributes are common but not captured as dependencies in the NB model, decreasing its classification effectiveness.

There are some proposals in the literature that try to im-prove the NB effectiveness known as the Semi-Naive Bayes methods, which relax the NB attribute independence as-sumption [17], by reduction of data [5] or, mainly, by means of extensions of the structure of the learning model to re-present feature dependencies [9, 12]. These strategies have produced some interesting gains in small datasets, such as those related to bioinformatics [9], but due to their high computational costs, implied by the complexity of represen-ting the term dependencies, they cannot scale to ADC or other large classification tasks. Thus, investigating whether the relaxation of the NB attribute independence assumption is effective in large ADC tasks is still an open problem.
In this context, we introduce four original lazy Semi-NB strategy proposals which exploit the information of the do-cument to be classified to reduce the complexity of the Semi-NB learned model. Our experimental results point out that further improvements can be obtained with these models, depending on some dataset characteristics. Furthermore, in order to make our proposals capable of running in large text collections, all of them are parallelized using graphics proces-sing units (GPUs). Our implementation, exploiting intrinsic aspects of this type of architecture is a novel contribution.
In summary, the main research questions we address in this paper are: (Q1) Are our new Semi-NB proposals ca-pable of improving a robust implementation of NB model? (Q2) Are these proposals competitive with state-of-the-art ADC classifiers such as SVMs? (Q3) Can we design an effici-ent Semi-NB implementation that allows testing interesting, but very costly, proposals that relax the NB independence assumption in large ADC tasks?
Overall, our main contributions include: (i) the proposal of four new lazy Semi-Naive Bayes approaches specially de-signed for ADC; (ii) the implementation of well structured GPU-based parallelization of the Semi-NB solutions; (iii) an investigation on the real impact of the NB attribute indepen-dence assumption in real text collections using our proposed Semi-NB approaches, thanks to the scalability offered by our GPU-based implementation; (iv) a large experimental evaluation considering many state-of-the-art baselines and representative datasets. Besides the originality of our pro-posals and investigations, our extensive experimental study is by itself a substantial contribution, as Semi-NB models have not been evaluated in the ADC scenarios we exploit.
The remainder of this paper is organized as follows. Sec-tion 2 covers related work. Section 3 describes the traditi-onal NB and its variants, the baseline Semi-NB approaches and basic concepts of parallelism and GPU. Section 4 pre-sents our Semi-NB proposals. Section 5 details our GPU-based Semi-NB implementations. Section 6 describes our experimental evaluations and Section 7 closes the paper.
The Naive Bayes is one of the most popular machine le-arning methods. Its simplicity, combined with its efficiency, makes it a very attractive method in various classification scenarios. However, this simplicity in the construction of the classification model can significantly compromise the ef-fectiveness of this method in some scenarios, such as textual document classification. In [19] the authors argue that some characteristics of document collections such as imbalance among classes and data sparsity, significantly compromise the learning model proposed by the traditional NB model (the Multinomial one [17]). The authors also propose a sim-ple heuristic that is based on data transformations and sim-ple adjustments in the construction of the learning model, which was called Transformed Weight-normalized Comple-ment Naive Bayes (TWCNB). The main idea of this heuristic is to perform a number of feature frequency transformations to improve the NB modeling text classification. Following this line, [28] presents another proposal for the construc-tion of the model that combines Multinomial NB with the technique one-versus-all [28], widely used in multiclass clas-sification scenarios. One more recent study shows that smo-othing NB may improve the classification performance [1]. The authors applied the concept of linear interpolation smo-othing to Naive Bayes Spam Classification. Another rese-arch line proposed to make the NB algorithm more effective in ADC is the use of different feature weighting strategies. In [13] the authors propose a per-document length normali-zation approach by introducing multivariate Poisson model for NB text classification. In addition, the authors propose a weighting method that can improve the effectiveness of the Naive Bayes classification in rare classes. Although not an explicit goal of this paper, we conducted a large investigation on the combination of different models and weighting strate-gies (described further below) aiming at defining a strong ba-seline NB baseline to compare with our Semi-NB proposals.
As mentioned earlier, NB is based on the premise that feature occurrences in documents of different classes are in-dependent [17]. The main reason for the adoption of this premise is the time complexity, since the construction of a complete and optimal Bayesian network is an NP-hard problem [6]. Thus, a strategy that has being considered (called semi-Naive Bayes) is based on heuristics that relax the assumption of independence without ensuring the op-timal model. The main group of Semi-NB techniques se-eks to extend the structure of the NB, approximating it as much as possible to a Bayesian network, either by some la-tent variables to connect correlated features [9] or by ex-plicitly representing feature dependencies [12]. Interdepen-dencies among features are also allowed, being usually mo-deled by the z-dependence concept, introduced in [20], in which each feature is considered dependent on the class and on other z features, making possible the construction of Bayesian networks. The Naive Bayes establishes a 0-dependence while most existing methods of this group works with a 1-dependence, such as Tree Augmented Naive Bayes (TAN) [9]. In the case of this latter work, learning is per-formed using a conditional mutual information of each pair of features, keeping the pairs that increment quality classi-fication the most. The Super Parent TAN (SP-TAN) [12] is a heuristic that extends TAN. The heuristic is greedy and optimizes the search space by correlated features. According to experimental results conducted by the authors, the stra-tegy is more effective than traditional NB. However, due to its high computational cost, in both time and memory con-sumption, all these strategies have been evaluated only in small datasets, such as small bioinformatics databases. As far as we know, the evaluation of the effectiveness of sup-pressing this premise in large ADC tasks has not yet been investigated.
In order to make this evaluation computationally feasible, we also present as a contribution of this work a paralle-lized version of the Naive Bayes method on a many core GPU. GPU-based parallel implementations of classification algorithms have recently called attention of many resear-chers [16], since these implementations are able to achieve high levels of parallelism and lower power consumption [24]. A first GPU-based parallel implementation of the Naive Bayes was presented in [2], however, despite the good re-sults achieved, it lacked the full exploitation of important GPU resources, such as coalesced memory access and shared memory. Unlike this proposal, the parallel implementation presented in this study achieves high levels of parallelism through better use of GPU memory hierarchy, multiproces-sor occupancy and coalesced memory access. In addition, the implementation deals with the semi-Naive Bayes strate-gies that impose new challenges. In particular, we present new semi-Naive Bayes approaches constituting lazy exten-sions of the work presented in [12], aimed at efficiency and scalability. This allows us to evaluate how the assumption of independence between features limits the effectiveness of the NB algorithm in large ADC tasks.
For a better understanding of our proposals, in this section we briefly describe the original Naive Bayes algorithm and some of its extensions to better deal with ADC tasks. After, we present details of Semi-NB methods that constitute the basis for our work. And finally, we briefly describe some concepts of parallelism and GPU to facilitate understanding of our parallel strategies.
The Naive Bayes [17] algorithm is based on probabilistic models. It calculates the  X  X core X  of a class c i as the proba-bility of a document d t being assigned to class c i . Based on this score, a class ranking is created and NB assigns d t class in the top of the ranking. More formally, let P ( c the probability of a test document d t ( a 1 ,a 2 ,...,a j long the the c i class, where ( a 1 ,a 2 ,...,a j ) is a feature vector (binary or weighted) representing the term set of document d . This probability is calculated using the Bayes Theorem. However, the calculation of P ( c i | d t ) has a high computatio-nal cost, given that the number of possible vectors d t is very high due to the potential number of term dependencies. To attenuate this problem, the NB algorithm assumes that the occurrence of all terms is independent from each other (the NB independence assumption), a simplification that makes the classification problem computationally viable. Thus, P ( c i | d t ) is defined as P ( c i | d t ) = P ( c i ) Q serving the Equation, we see that the fundamental question in the P ( c i | d t ) definition is how to estimate P ( a number of occurrences of term a j in class c i ; P v  X  V Tc the summation of the occurrences of all terms in documents belonging to c i and | V | is the vocabulary size. To avoid inconsistencies (e.g., division by zero), a weighted Laplace smoothing is usually applied, consisting of an addition of an  X  parameter to the numerator and as a multiplication factor to the vocabulary size in the denominator.

In ADC, the goal is to find the  X  X est X  class for d t , usu-ally defined as the one with the highest conditional proba-bility. To avoid computational precision losses (e.g., due to floating point underflow), it is usually more adequate to add the log of the probabilities instead of performing the previously defined multiplication. Thus, this maximi-zation, in most NB implementations is defined as c NB ( d ) = argmax
The NB model is the basis of Semi-NB strategies, since most of these strategies initialize their prediction model from the NB. So it is very important to have a robust NB model in order to build effective Semi-NB model. So we re-visit in the literature some strategies that propose adjustments in the NB model, making it more robust for text classification. These strategies are briefly described below: Interpolated Naive Bayes (INB) [1] combines the Jelinek-
One-versus-All Naive Bayes (OVA NB) [28] combi-
Complement Naive Bayes (CNB) learning model, ex-
Over the years various techniques have been proposed to relax the independence assumption of the original NB mo-dels. In [9], the authors compared the Naive Bayes algo-rithm with Bayesian networks, which are more robust mo-dels to represent probability dependencies, since they have no restrictions. Surprisingly in some scenarios, Bayesian networks degraded the quality of classification. Thus, the authors proposed a restricted Bayesian model that combi-nes the ability of Bayesian networks to represent dependen-cies between attributes and the simplicity of Naive Bayes. This representation, called Tree Augmented Naive Bayes (TAN) [9], is defined by a main constraint: attributes may depend on only one other attribute in addition to the class. This restriction makes feasible the probabilistic calculati-ons, since it is computationally very costly to compute mul-tiple dependencies for any given attribute 1 . The classifi-cation model can be represented by the following maximi-zation: c TAN ( d ) = argmax where  X  ( a j ) is the parent attribute of the attribute a The Super Parent TAN (SP-TAN) [12] is a variant of TAN, adopting the same representation. However, in or-der to add dependencies to the NB model, it uses a greedy search method (Hill Climbing Search). The construction of the SP-TAN network is given by the concepts of Super Pa-rent and Favorite Child.  X  Super Parent (SP): Given a Bayesian network, if we create a dependency to an attribute x for all the orphans attri-butes, it is called a Super Parent. Orphans attributes are the attributes that have only the class as a parent.  X  Favorite Child (FC): If we extend a dependency of a Su-per Parent for each orphan and evaluate the quality of each dependence individually, the attribute that improves accuracy the most is the Favorite Child.

The choice of SP and FC attributes are conducted using a validation set, since the heuristic uses a prediction metric for evaluating the classification quality of the networks built by each candidate as SP. Thus, all dependencies are selec-ted through this validation set and subsequently evaluated on the test set.

SP-TAN produced some gains compared with the tradi-tional NB [12] in small datasets. However, when applied to real-world scenarios, such as large ADC datasets, this technique tends to lose performance due to overfitting is-sues caused by the strong dependencies found only in the validation set. Moreover, the two strategies (TAN and SP-TAN) are extremely costly in terms of computation time, preventing the execution of these heuristics in large datasets. Despite presenting different approaches to find dependencies between attributes, both strategies have the same time com-plexity, O ( D train  X | C | + D test  X | C | + | V | X | C |  X | D train being the density 2 of training documents, D test the density of test documents, | C | the number of classes, and |
V | the number of attributes. Although the SP-TAN and TAN heuristics present the same complexity in the worst case, the SP-TAN heuristic has an average case less com-plex, since its complexity varies according to the number of selected dependencies. Thus, the heuristic only reaches the same complexity of the TAN if V  X  1 dependencies are found, which is actually difficult to happen, since the SP-TAN tends to select fewer dependencies than TAN, keeping the effectiveness [12].
 In here, we propose a heuristic, called Lazy Super Parent Tree Augmented Naive Bayes (LSPTAN) that seeks to solve the problems discussed above, enabling the application of a semi-NB techniques in large ADC tasks. Thus, we can evaluate whether the premise of independence among attri-butes, assumed by NB, impacts effectiveness in large ADC tasks, an open research problem.
We will use the terms attribute and feature interchangeably, from now on.
Over the last years, the computer industry has shifted from pushing clock frequency to parallelism. Rather than increasing the speed of its individual processor cores, tradi-tional CPUs (Central Processing Units) have increased the number of cores (multi-core processors) as the primary basis for increasing system performance. This trend has also been followed by many core GPU (Graphics Processing Unit) chips which have now thousands of processing cores. The general perception is that processors are not getting faster, but instead are getting wider, and the only way to improve performance is through the exploitation of parallelism.
CPUs are optimized for low-latency access and excel in running single-threaded processes whereas GPUs are opti-mized for high throughput and can deal with massive multi-threaded parallelism. GPUs can handle massive amounts of data, which is critical for Information Retrieval (IR), howe-ver its different architecture and memory hierarchy requi-res the design of novel algorithms and new implementation approaches. This situation has hampered the exploitation of this opportunity by the IR community. However, recent works on Database Scalability, Document Clustering and Le-arning to Rank [4, 23, 26] have produced some encouraging results.

GPUs are specialized architectures originally intended for graphics computations. However since mid-2000s they be-came a powerful accelerator for general purpose computing (GPGPU). Due to their high computation power and im-proved programmability, they became increasingly used for high performance scientific computing and, more recently, for big data analytics.

The GPU architecture has two levels of parallelism, whe-rein the first level there are P streaming multiprocessors (SMs) and within each multiprocessor there are p streaming processors (SPs). Thus a parallel program can be first divi-ded into blocks of computation that can run independently on the P SMs (fat cores), without communicating with each other. These blocks, in turn, have to be further divided into smaller tasks (threads) that execute on the SPs (thin cores), but with each thread being able to communicate with other threads in the same block. Each of these threads has ac-cess to a larger global memory as well as to a small but fast shared memory and registers.

The GPU supports thousands of light-weight concurrent threads and, unlike the CPU threads, the overhead of crea-tion and switching is negligible. To hide the high latency of the global memory, it is important to have more threads than the number of SPs and to have threads accessing consecu-tive memory addresses that can be easily coalesced. Another important data movement channel is the PCIExpress con-nection, whereby CPU and GPU can exchange data between each one X  X  address space but in a much slower speed. The GPU programming model requires that part of the applica-tion runs on the CPU while the computationally-intensive part is accelerated by the GPU.
 A GPU program exposes parallelism through a data-parallel SPMD (Single Program Multiple Data) special function, cal-led kernel function. During implementation, the program-mer can configure the number of threads to be used. Threads execute data parallel computations of the kernel and are or-ganized in groups (thread blocks) that are further organized into a grid structure. When a kernel is launched, the blocks within a grid are distributed on idle SMs while the threads are mapped to the SPs.
Our Lazy Super Parent TAN (LSPTAN) heuristic is a postergated version of the SP-TAN that constructs a Tree Augmented Naive Bayes for each test example. Dependen-cies between attributes are generated based on information from the example that is being classified. To build this ver-sion we adapted the method of evaluation and the selection of candidates for Super Parents and Favorite Children.
The SP-TAN algorithm exploits accuracy to select a can-didate to SP ( a sp ). In our strategy, we select the candidate a sp whose classification model generates the highest proba-bility P ( d t | c i ,a sp ) for the document d t . Thus, each candi-date a sp builds its own model and classifies the document d , returning the predicted class c i and its respective score P ( d t | c i ,a sp ). Based on the best scores for each candidate a sp , we build a prediction score rank. This heuristic selects as best a sp the attribute that holds the first position of this rank using the information of the (test) document being clas-sified. This helps to reduce the number of candidates to SP and Favorite Child, scaling the overall classification process.
Building a Tree Augmented Naive Bayes for each test example is computationally expensive. Therefore, the LSP-TAN builds a simpler network than SP-TAN. We select only the best SP to a test document. But there is no limitation for the choice of the Favorite Children. Thus, all the children attributes that increment the probability that the document belongs to a class, are included in the classification model. The LSPTAN heuristic initially builds the model based on NB and initializes a set of orphans O , inserting into O all the attributes of the vocabulary. For each test document, the technique evaluates each attribute as a SP and, in the end, it selects as a sp the attribute that has the highest probability P ( d t | c i ,a sp ). The next step is to find the Favorite Children of the SP attribute. Each attribute a j is individually evalu-ated given the a sp and subsequently selected as FC conside-ring if the possible dependence of a sp for a j ( sp 6 = j ) incre-ases the probability of the document belonging to class c
We also propose three novel strategies that exploit diffe-rent ideas for building a LSPTAN model. These strategies are described below:
The LSPTAN SP is a simpler strategy that does not se-lect the Favorite Children. The idea is to analyze the impact of only selecting the best candidate to SP of all attributes without their children. This strategy has the potential to be much more efficient and scalable, although less effective in theory, since it captures less information in the model.
The LSPTAN Class is very similar to the strategy presen-ted previously. So it uses the same method of evaluation and selection of candidates of the LSPTAN, however, this stra-tegy selects a SP for each class. There is no limitation for the number of Favorite Children selected. The idea of this strategy is to capture the dependencies between attributes in each class. Thus, we give a chance to other SP candidates to increase their relevance when related only to their Favo-rite Children. The LSPTAN class heuristic initially builds the model based on NB and initializes a set of orphans O , in-serting into O all the attributes of the vocabulary. Then, for each test document, the technique evaluates each attribute as a SP and, in the end, for each class c i it selects as a attribute that has the highest probability P ( d t | c the class c i . In the end of this step, | C | a sp attributes will be selected. The next step is to find the Favorite Children of each SP attribute. Each attribute is individually evalu-ated given the a sp ( P ( a j | c i ,a sp )) and subsequently selected as FC considering if the possible dependence of a sp for a ( sp 6 = i ) increases the probability of the document belonging to class c i .
 The LSPTAN Restricted is a more efficient version of LSP-TAN, in which the SP candidates are evaluated with their Favorite Children. In this strategy, Favorite Children are the attributes that have P ( a j | c i ,a sp ) higher than the probability P ( a j | c i ). Therefore, the SP candidates are associated with their Favorite Children rather than being associated with all orphans attributes. So, only the dependencies between attributes that increase the probability of a document be-longing to a class c i are included in the classification model. The SP candidates also have a restriction: they must exist in the test document in order to be chosen as the best SP. Thus, we limit the search space of candidates, avoiding that a very informative candidate (probably belonging of a ma-jority class) is chosen.

The LSPTAN Restricted heuristic initially builds the mo-del based on NB and initializes a set of orphans O , inserting into O all the attributes of the vocabulary. For each test document, the technique evaluates each attribute which be-longs to the test document as a SP and it selects as a sp attribute that has the highest probability P ( d t | c the class c i . The next step is to find the Favorite Children of each SP attribute. Each attribute is individually evalu-ated given the a sp ( P ( a j | c i ,a sp )) and subsequently selected as FC considering if the possible dependence of a sp for a ( sp 6 = j ) respect the condition of P ( a j | c i ,a sp
The Semi-NB (LSPTAN) heuristic presents many oppor-tunities for a highly parallel GPU implementation. Since the LSPTAN is an extension of the Naive Bayes, the terms of a document can be processed independently, both the mo-del generation and the classification task phases can take advantage of the high computational power of GPUs. The LSPTAN strategy opens up new opportunities for exploiting parallelism in a massive multi-threaded way. In this sec-tion we present our proposal of a new version of Semi-NB parallelized using Graphics Processors Units (GPU), which corresponds to another contribution of this paper.
Given that document collections are usually sparse, with terms occurring in a few documents, we chose to represent the documents by a compact data structure. For this, we used four vectors: docIndexV ector representing the docu-ments; docAttributeV ector that stores the attributes of each document; docV alueV ector that stores the weights of the at-tributes for each document; and finally, docClass that sto-res the classes of all documents. At docIndexV ector the index represents the document, and in each position of this vector we store the position where the list of terms of the document begins in vector docAttributeV ector and the fre-quency in the vector docV alueV ector . At docClass each index also represents a document, and each position stores the respective document class.

Figure 1 shows an example of the data structure used. As can be seen, the document 0 of the database, starts its list of terms at the index 0 of the vector docAttributeV ector and has two terms (terms 0 and 1), each of which has frequency equal to 1 as can be seen in vector docV alueV ector . In turn, the document 4 begins its list of terms in the position 8 of the vectors docAttributeV ector and docV alueV ector and has only the term 1, with frequency also equal to 3. The document classes are in the vector docClass , where the documents 0 and 4 have classes 0 and 1 respectively.
Our parallelization proposal for the LSPTAN strategy is based on eight kernels implemented using the C language in CUDA. Kernel is the name given to the functions that run on the GPU. These kernels exploit the maximum possible pa-rallelism, minimizing data dependencies between threads. It also focuses on maximizing the amount of computation per-formed by each thread. Since the computation of each SP and FC candidates can be done independently (as well as the actual classification) for each test document, the problem takes full advantage of the GPU parallelism. The following describes the parallelization strategy used to implement the eight kernels on the GPU: 1. Naive Bayes model: Given the frequencies of the terms occurred in the classes and the test documents data, the classification of the test documents can be performed in parallel. Thus, given a test document, each thread is res-ponsible for calculating the conditional probability P ( a of an attribute a j for each class c i , cooperating to produce the INB model described in Section 3.1. 2. Cumulative frequency of the attributes give the SP candidates within the classes: This kernel is res-ponsible for computing the summation of occurrences of all terms in documents that occur together with the respective SP candidate in each class, i.e., P v  X  X  V | Tc i ,a there is no dependency between the SP candidates, this ker-nel can take full advantage of the GPU parallelism. While reading the training documents data, each thread is associ-ated with a SP candidate. Thus, each thread computes the cumulative frequency for each class. 3. Finding the documents having SP candidates: In text collections, usually (basically always) the documents do not have all the vocabulary words. Thus, this kernel is responsible for selecting the training documents data that have the SP candidates. While reading the training docu-ments data, each thread is responsible for checking the res-pective term as a SP candidate. If one of the threads finds the SP candidate, the training document is tagged. These tagged documents will be used in the next kernel. 4. Computing the frequency of the attributes gi-ven the SP candidate: Once a candidate attribute is se-lected as a SP, this kernel computes the frequency of the attributes given the SP for each class, i.e., Tc i ,a sp ( a 5. Calculation of the probabilities of the terms in the classes given a SP: It calculates the conditional probability of the attributes given the SP for each class. Thus, each thread is responsible for computing the conditi-onal probability of an attribute a j given the SP candidate a 6. Construction of the classification model given the SP: Given the probability matrix built in the previous kernel for the SP candidate, this kernel works in the same way that the Naive Bayes kernel does. Thus, given a test document each thread is responsible for calculating the con-ditional probability ( P ( a j | c i , a sp ) ) of an attribute a class given the SP candidate and thus cooperating to pro-duce the LSPTAN model. The classification model can be represented by the maximization: 7. Construction of the classification model given the SP and the FC: After selecting the best SP, the clas-sification of documents for each Favorite Child can be per-formed in parallel. Each thread represents a FC candidate. Thus, each thread is responsible for classifying all test do-cuments for a FC candidate for each class given the best SP. The classification model for a FC can be represented by the maximization: 8. Updating of the classification model: After cal-culating the probabilities of the Favorite Children given the best SP for a test document data, this kernel has the func-tion of updating the model built by NB, adding the proba-bility of the FC given the best SP that increased the model probability. Each thread is responsible for an attribute and only the thread corresponding to a Favorite Child adds its probability to the LSPTAN model.

Figure 2 shows the sequence of execution of the LSPTAN strategies through the kernels. Thus, the LPSTAN SP and LPSTAN Restricted strategies are faster than the other LSP-TAN strategies because they only execute until the kernel 6.
In this section we present the optimizations applied in the kernels, in order to exploit the maximum parallelism pos-sible, minimizing the dependence of data between threads. It also focuses on maximizing the amount of computation performed by each thread.

The kernels 1 and 6 exploit two levels of parallelism. In the first, it associates a block of threads with each test do-cument and, at the second level, each of these threads is associated with a term of the same document. Thus, each block is responsible for classifying a specific test document while each thread of the block is responsible for calculating the probability of an attribute for each class, cooperating to produce the NB model. If the number of attributes is grea-ter than the number of threads of a block, the document is split into k parts, where k is the rounded up ratio between the number of attributes of the document and the number of threads in the block. Then, each k part is calculated one at a time, in a round robin fashion, and the results are accumu-lated. As for the calculation of the logarithmic summation of the classification model, we implemented a binary tree parallel reduction using shared memory. Therefore, we allo-cated an array in shared memory whose size is equal to the number of threads in the block. Then, each thread stores the conditional probability in its respective position in the shared memory vector. The vector is divided into two sub vectors and then these sub-vectors are added. The process is repeated until the first position of the vector contains the total sum of the logarithms.

The kernels 3 and 4 also exploit two levels of parallelism: the first associates a block of threads to training documents and, at the second level, each of the thread is associated with a term of the same document. Thus, in the second le-vel, each kernel performs their respective roles, as described previously. In the kernel 4 as we are calculating the overall frequency of the attributes given the SP candidate in each class, we use the atomicAdd CUDA function to prevent ac-cess conflicts.
In this section we analyze the amount of time used to le-arn and evaluate the SP candidates, and then the amount of time used to select the Favorite Children of the best se-lected SP. The first step of learning a model given the SP candidate is based on computing the attribute frequencies in order to build the probability table for each class given the candidate. These frequencies are obtained in kernels 2 and 4. During the execution of these kernels, the training do-cuments data D train are read in parallel, while each thread block is responsible for a document. However, as described in section 3.3, the GPU has only a few ( P ) multiprocessors (SMs) and so there will be more thread blocks than SMs. Although the GPU does the block scheduling automatically, the parallelism is limited to P multiprocessors. In addition, inside each SM there are only p processors (SPs) which again limits the second level of parallelism, inside each SM, to a total of p processors. Therefore, the time spent in the ker-nels 3 and 4 is given by  X  3 =  X  4 = O ( P V d is the document vocabulary whose terms are processed in as many steps of p threads as needed. The time spent in kernel must perform two passes over the vocabulary, first to check if the SP candidate belongs in the document and, if so, to compute the attribute frequencies.

After computing all the frequencies, the next step is to build the probability table given the SP candidate. This pro-bability table is two-dimensional: vocabulary by class ( V  X  C ). The time spent to build this table is  X  5 = O ( V P  X  p because each thread is responsible for an attribute. Since we are working with real collections, it is not feasible in terms of memory space to build a three-dimensional table ( SP  X  V  X  C ), that stores the probability of all the SP can-didates. We have a trade-off between space and time. So we have to update this probability table and the respective attribute frequencies (kernel4) whenever the strategy needs to evaluate a new candidate.

The next step consists in building the classification mo-del (Bayesian network) for each test document given the SP candidate for each class. The time spent to build the classi-fication model for the test documents is very similar to the time spent to compute the attribute frequencies given the SP candidate in the kernels 3 and 4, because we use the same two-level parallelism strategy. However, in this case test documents data D test are read (instead of training do-cuments data) for each class. The time 3 spent in this step is  X  1 =  X  6 = O ( P Once all the SP candidates are evaluated and the best SP is selected, the Favorite Children must be selected. For this, we must compute the attribute frequencies and build the probability table for the best SP. Then, the classifi-cation models are built considering only the dependencies between a FC attribute and its respective SP. Thus the time spent to evaluate all Favorite Children candidates is  X  ponsible for each Favorite Child candidate. When a FC can-didate is selected, the classification model must be updated. So the time spent to update the model is  X  8 = O ( V d P  X  p
The complexity of LSPTAN SP and LSPTAN Restricted is summarized by the time spent in finding the best SP attri-bute which is O (  X  3 +  X  4 +  X  5 +  X  6  X | V | ). The LSPTAN and LSPTAN Class are a bit slower with total time corresponding to O (  X  3 +  X  4 +  X  5 +  X  7 + | V | X   X  8 ) to select the FC attributes.
In order to evaluate the proposed combinations, we consi-der five real-world textual datasets, namely, 20 Newsgroups, Four Universities, Reuters, ACM Digital Library and RCV1 datasets. For all, we performed a traditional reprocessing task: we removed stop-words, using the standard SMART list, and applied a simple feature selection by removing terms with low  X  X ocument frequency (DF) X  4 . Next, we give a brief description of each dataset. 4 Universities (4UNI), a.k.a, WebKB this dataset contains Web pages collected from Computer Science de-partments of four universities by the Carnegie Mellon Uni-versity (CMU) text learning group. There is a total of 8,277
The kernels 1 and 6 have the same complexity, because they exploit the same parallelism strategy, the only difference lies on the probabilities used to build the learning model.
We removed all terms with DF  X  2 web pages, classified in 7 categories (such as student, faculty, course and project web pages).

Reuters (REUT90) this is a classical text dataset, com-posed by news articles collected and annotated by Carnegie Group, Inc. and Reuters, Ltd. We consider here a set of 13,327 articles, classified into 90 categories.

ACM-DL (ACM) a subset of the ACM Digital Library with 24,897 documents containing articles related to Com-puter Science. We considered only the first level of the taxo-nomy adopted by ACM, whereas each document is assigned to one of 11 classes. 20 Newsgroups (20NG) this dataset containing 18,805 newsgroup documents, partitioned almost evenly across 20 different newsgroups categories. 20NG has become a popu-lar dataset for experiments in text applications of machine learning techniques.

RCV1 Uni The Reuters Corpus Volume 1 (RCV1) is a dataset with 804,427 English language news stories. We considered the complete topics taxonomy comprised of 103 classes. However, the original RCV1 is a multilabel dataset with the multilabel cases needing special treatment, such as score thresholding, etc. (see [15] for details). As our cur-rent focus is on unilabel tasks, to allow a fair comparison among the other datasets (which are also unilabel) and all baselines (which also focus on unilabel tasks), we decided to transform all multilabel cases into unilabel ones. In order to this fairly, we randomly selecting,among all documents with more than one label, a single label to be attached to that document. This procedure was applied in about 20% of the documents of RCV1 which happened to be multilabel.

The LSPTAN strategies were compared using micro ave-raged F 1 (Mic F 1 ) and macro averaged F 1 (Mac F 1 ), standard information retrieval measures [27]. While the Mic F 1 mea-sures the classification effectiveness over all decisions (i.e., the pooled contingency tables of all classes), the Mac F 1 asures the classification effectiveness for each individual class and averages them. All experiments were executed using a 5  X  fold cross-validation [3] procedure. The parameters were set via cross-validation on the training set 5 , and the effec-tiveness of the algorithms, running with distinct types of features, were measured in the test partition. All experi-ments were run on a Quad-Core Intel Xeon E5620, running at 2.4GHz, with 16Gb RAM. The GPU experiments were run on a GeForce GTX TITAN Black, with 6Gb RAM.

We would like to point out that some of the results obtai-ned in some datasets may differ from the ones reported in other works for the same datasets (e.g., [13, 19, 10]). Such discrepancies may be due to several factors such as differen-ces in dataset preparation 6 , the use of different splits of the datasets (e.g., some datasets have  X  X efault splits X  such as
In other words, a portion of the training set was separated, as a type of validation set, to be used for parameterization.
For instance, some works do exploit complex feature weigh-ting schemes or feature selection mechanisms that do favor some algorithms in detriment to others.
 REUT and RCV1 7 ), the application of some score threshol-ding, such as SCUT, PCUT, etc., which, besides being an important step for multilabel problems, also affects classi-fication performance by minimizing class imbalance effects, among other factors. We would like to stress that we ran all alternatives under the same conditions in all datasets, using standardized and well accepted cross-validation procedures that optimize parameters for each alternatives, and applying the proper statistical tools for the analysis of the results. All our datasets are available for others to replicate our results and test different configurations.

In order to evaluate the performance of our LSPTAN ap-proaches, we compare then with SVM and KNN classifi-ers. Regarding SVM classifier, we adopt the LIBLINEAR [8] implementation on which the regularization parameter was chosen among eleven values from 2  X  5 to 2 15 by using 5  X  fold cross-validation on each training dataset. In the KNN clas-sifier, the size of neighborhood was set via cross-validation in the training set.

We also conduct controlled experiment to evaluate the effectiveness of classifiers combined with four different fea-ture weighting schemes. We perform this investigation be-cause previous works have demonstrated that the weighting schemes may have a profound influence in the NB perfor-mance [19] .The names and the descriptions of each feature weighting are given as follows: (1) Term Frequency me-asures the number of occurrences of a term in a text docu-ment [18]; (2) Log of Term Frequency  X  in collections in which the documents vary widely in size, the use of the loga-rithm of tf ( log ( tf )), instead of tf , is usually a better option, since the logarithm smoothies the frequency of occurrence of the terms; (3) TF-IDF penalizes terms by their document frequency [18]; (3) Relative Frequency (RF) is a length normalization method that normalizes the term frequency by dividing it by a normalization factor given by the maxi-mum frequency of a term occurring in any text document of indicate the average number of unique terms in a document over the collection and du j means the number of tokens and unique terms in a document d j . This linear interpolation smooths the length of each document using the characteris-tics of document collection. As mentioned earlier, the value of the  X  parameter is obtained with cross-validation in the training set.
We start by demonstrating the effectiveness of the feature weighting schemes combined with the ADC classifiers. For each dataset, we tested the four described feature weighting schemes with KNN, SVM and the four discussed NB vari-ants: the traditional one and its adaptations for the ADC idiosyncrasies. Table 3 presents the best obtained results.
In general, for the NB strategies, either the traditional or its variants, the best weighting scheme was RF. Regarding the NB variants the best one was INB + RF , which surpas-sed the results of traditional NB with the same weighting scheme. Therefore, as this combination of INB with the RF weighting scheme produced the best results, this will be the  X  X trongest X  version of Naive Bayes (hereafter called
In fact, we do believe that running experiments only in the default splits is not the best experimental procedure as it does not allow a proper statistical treatment of the results. Improved-NB) that we will use as the basis to build our lazy Semi-NB strategies. The best overall results for SVM and KNN were obtained using the log ( TF )  X  IDF weighting scheme, therefore these versions will be used as baselines.
We can see in Table 3 that the Improved NB version out-performs or ties with KNN in eight out of ten cases . When compared to SVM, the Improved-NB ties or outperforms this classifier also in eight out of ten cases, loosing in other two (regarding Mic F 1). In fact, if we look at the Mac F 1 re-sults, Improved-NB outperformed or tied SVM and KNN in all five datasets. This is consistent with the fact that most of the learning models and weighting scheme adaptations were designed to deal with the class imbalance issue, which tends to favor Mac F 1.

We present next the experiments performed to evaluate the effectiveness of our Semi-NB strategies. As mentioned before, the original SP-TAN proposal does not scale to the size of the datasets we experiment with, therefore we can not present results for this proposal.

Table 4 shows the results of our four LSPTAN strategies applied on the five real-world textual datasets. We com-pare our Semi-Naive Bayes strategies with Improved-NB , as we want to know if theses strategies are capable of exten-ding this already strong baseline, improving its classification effectiveness. For comparison purposes, we select the best results of Naive Bayes (Improved-NB). In the LSPTAN co-lumn is the strategy that selects the SP attribute and their Favorite Children in a lazy fashion, while the other columns contain the results of the proposed variants. As mentioned before, we initialize the LSPTAN network using the INB mo-del with the RF feature weighting scheme, since this model generated the best results. INB is also the simplest model to be extended when compared to CNB and OVA NB, be-cause it has fewer likelihoods to calculate, besides being the closest to the Multinomial version.
 We can see in Table 4 that the best overall strategy was LSPTAN SP , tying or improving the Improved-NB results in all ten results. In fact, some MicF1 and specially MacF1 results for 4UNI and RCV1 8 are among the best results ever reported for these datasets, with gains of up to 6.7% in Ma-croF1 in 4UNI and 12% in MicroF1 for this same dataset. The results of LSPTAN SP over LSPTAN also show that the costly process of selecting the Favorite Children is un-necessary in most cases, helping to scale the technique. We also can observe that the LSPTAN Class reach the best re-sults observed for the LSPTAN for the REUT90 dataset, tying with the Improved-NB. The best results for ACM and 20NG were obtained with for LSPTAN Restricted , also with gains over Improved-NB.

We can observe in Table 5 that in nine out of ten cases we find a LSPTAN strategy equal or better than SVM (two ties and seven wins). When compared to KNN, the LSPTAN ties or outperforms this classifier in eight out of ten cases (one tie and seven wins). Overall, we can conclude that taking into account  X  X ome X  dependencies (e.g., based on Super Parents and Favorite Children) may help to improve effectiveness in (large) datasets, depending on their characteristics. Howe-ver, we were not able to choose the best LSPTAN strategy,
For RCV1 only, due to its size and high dimensionality, we reduced the training set in about 40% and adopted a constraint applied in [25], in which only features occurring in more than 30 documents are candidates for SP. Even with these limitations, we were able to obtain improvements. since three of the four strategies have proven effective in different datasets.
Table 2 shows the average time (seconds) to evaluate a SP candidate using our CPU and GPU implementations. Note that the evaluation of the SP candidate includes the building of the predictive model and the actual classification of all test documents. More precisely, we measure the execution time of kernels 1 to 6, where the kernels 3 to 6 are execu-ted for each SP candidate. Since the whole vocabulary (large number of attributes) is evaluated for finding SP candidates, we measure the time to evaluate each candidate in a test fold. Notice that the average time to evaluate a SP candidate is practically the time to compute the attribute frequencies, build the probability table given the candidate and classify all test examples. As can be seen in Table 2, our GPU im-plementation shows significant speedup, ranging from 8 . 34 to 63 . 36 when compared to the sequential implementation. Table 2: Efficiency of the GPU-based implementation in select the best Super Parent -time in seconds.
In this paper we have provided answers for important rese-arch questions regarding the relaxation of the NB attribute independence assumption. The answers for these research questions were mostly positive in the sense that, (i) we were able to propose four original lazy Semi-NB techniques capa-ble to outperform in all the cases a strong implementation of NB, robust to ADC tasks. We also showed that (ii) our four LSPTAN solutions were able to tie or outperform the state-of-the-art textual classifiers: SVM and KNN. In fact, gains of more than 18.5% (MacF1 in REUT90) can be obtained by some of our strategies. Reminding that with our code availa-bility, it is easy to check the best alternatives in a validation set. We also proposed a GPU-based parallel implementation of the LSPTAN strategies. In our proposal, we exploit a very compact data structure to index the documents aiming at minimizing memory consumption, as well as maximizing the opportunities for massive parallelization of the algorithm in GPUs. In our GPU-based implementation we exploit mas-sive parallelism, minimizing data dependence between thre-ads and maximizing the amount of computation performed by each thread. Thus, (iii) our Semi-NB approaches were able to take full advantage of the proposed parallel imple-mentation since these approaches require intensive compu-tation once the dataset has been transferred to the GPU.
As future work, we intend to investigate more deeply fac-tors that affect the potential gains of using our lazy Semi-NB strategies, depending on the dataset characteristics. We believe this characterization will allow us to propose new improvements in the model structure. We also intend to expand our experimental evaluation adding other popular text classification methods, both eager and lazy, to see how they measure up when compared to our LSPTAN strate-gies. Finally, we will investigate how Feature Selection may affect the proposed Semi-NB methods, as they may remove dependencies among attributes while increasing efficiency.
