 We describe efficient techniques for construction of large term co-occurrence graphs, and investigate an application to the discovery of numerous fine-grained (specific) topics. A topic is a small dense subgraph discovered by a random walk initiated at a term (node) in the graph. We observe that the discovered topics are highly interpretable, and re-veal the different meanings of terms in the corpus. We show the information-theoretic utility of the topics when they are used as features in supervised learning. Such features lead to consistent improvements in classification accuracy over the standard bag-of-words representation, even at high training proportions. We explain how a layered pyramidal view of the term distribution helps in understanding the algorithms and in visualizing and interpreting the topics.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval X  Clustering Algorithms unsupervised learning, text mining, co-occurrence graphs, topic discovery, feature induction, feature augmentation
Consider trying to obtain quick insights into how a term is mentioned in a corpus of text (in news articles, emails, abstracts, postings, and so on). The term of interest may correspond to an entity, such as a person, an organization, or a product, or to an event, etc. ( e.g. ,  X  X ill Clinton X ,  X  X eli-gion X ,  X  X il spill X ). Reading each document that contains the term is one approach, but this can be very time consum-ing, and it may not readily reveal the salient  X  X opics X , i.e. , events, activities, roles, or associations that the term (the concept corresponding to the term) participated in. Algo-rithms for quickly extracting such specific topics would find applications in intelligence analysis, discovery of user inter-ests, document and corpus summarization, document tag-ging and routing, and so on. In this paper, we explore the use of co-occurrence graphs on terms for the discovery of such topics. Co-occurrence graphs find diverse uses in var-ious information retrieval and language processing tasks [4, 2, 14, 15, 5]. In this paper, we first briefly present techniques for large-scale construction of such graphs, and then develop an application to fine-grained topic discovery.

In many scenarios, such as for the topic discovery applica-tion described here, the graphs may need to be continually updated or otherwise rebuilt periodically, therefore mem-ory and/or time efficiency can be important. We describe methods for efficient construction of a type of co-occurrence graph wherein edges reflect conditional probabilities, or a close variant, as we explain. We compute term co-occurrence information by sliding a window over each term appearing in a document ( e.g. , [9, 14, 5]). During graph construc-tion, new edges reflecting co-occurrence may be added to the graph, and edges with small weights may be dropped. We distinguish between three types of edge dropping, and explain the effects of each. Unlike other types of graphs, the nodes (terms) in the co-occurrence graph should not be treated the same way: the graph should not be viewed as  X  X lat X . We explain how consideration of term frequencies, in particular the pyramid view of the term distribution, finds several uses in algorithm design and in visualizing and mak-ing a better sense of the discovered topics.

We mine an association graph for relatively small dense subgraphs (10s of terms), or semicliques, i.e. , our  X  X opics. X  We find that the semicliques correspond to a variety of sub-jects and events such as natural disasters, elections, sports events, group and organizational activities, and political con-troversies. Naturally, a given term, such as a person X  X  name, can participate in several topics, often reflecting different senses of the term, or roles for the corresponding referent. We explore the application of the topics as features for super-vised learning, finding that the topics improve performance even at high training proportions. We also compare to latent Dirichlet allocation [1], and highlight the advantages of co-occurrence analysis for fine-grained topic discovery. Thus, we provide evidence that topic discovery via co-occurrence analysis is an effective way to mine text collections, com-plementing the more commonly studied methods such as document clustering and existing term-document analysis Table 1: The data sets we use. N , #uni, | V | , | d | ,and T , techniques. The expanded version of this paper contains further descriptions and experiments [11].
Table 1 shows our data sets: newsgroups [7], the Reuters collection [13], and TDT5 (Topic Detection and Tracking, for 2004, [8]). We performed some tokenization, such as lower casing words, some stemming, and changing numbers to NUM. The vocabulary includes both unigrams and a sub-set of possible bigrams and trigrams (phrases such as  X  X ew jersey X ). We generated the ngrams via the same graph con-struction process, which we explain next. Experiments were run on a Dual-Core AMD Opteron (25GB RAM, 2.8GHz), and the graph construction and semiclique discovery code was written in both Java and C++.
 Figure 1 presents our main graph construction algorithm. The algorithm begins with an empty graph (no edges), and processes the corpus documents in a random order. The al-gorithm keeps and updates edge weights for each term as well as a special term v 0 ( v 0 is used to determine statistical significance). The edge weights could simply be conditional probabilities. In particular, w v,u can be viewed as P ( u meaning the probability that term u is seen sufficiently close Figure 2: The layered pyramidal view of terms. (a) A to v ,within L terms on either side of v , given that a ran-dom position is picked from a randomly picked document, and v is seen in that position. More generally, they can be expected proximity values [11]. By sliding a window of size L over each position (term occurrence) in the document, and keeping counts, the weights (conditional probabilities) are updated. The algorithm can drop edges during graph construction for memory efficiency (there are other for pre-serving efficiency). Finally, after the corpus processing is fin-ished, the graph is pruned further: edge weights that are not sufficiently statistically significant and those that do not pass a test of sufficiency of  X  X nformativeness X  or  X  X urprise-level X  are dropped (like point-wise mutual-information ([12])). In-stead of the two-tier updates, first summarizing (averaging) the document, then updating the global weights, we could have done the updates in one step. The two tier update lessens the effects of long documents. This choice is also application dependent.

The Pyramid View. Terms (single words and mean-ingful phrases) are approximately distributed according to Zipf X  X  law, the number of terms growing rapidly with de-creasing frequency. The terms can therefore be viewed as residing in a pyramid, where a few most frequent terms resid-ing at the top (Figure 2(a)). We note that if two terms have roughly the same frequency, or equal (nonzero) marginal probabilities, then their conditional probabilities must be equal: P ( u )= P ( v )  X  P ( u | v )= P ( v | u ). More generally, with k&gt; 0, tions of equivalence (1) are several for the way the graph computation works: edges are reciprocated and have a sim-ilar weight when the terms are in the same tier of the pyra-mid, i.e. , when the terms have close frequencies. Moreover, with the edge dropping using the minimum threshold W 0 , Figure 3: The immediate neighbors of  X  X hristianity X  (to-Figure 4: A topic is a maximal semiclique. Each term outgoing edges of a term tend to connect it to terms in the same or above layers more than to terms in the lower layers (Figure 2(b)). An effect that dropping edges by PMI (or similar constraints) has is that it tends to cut edges that go all the way to top tiers (most frequent words). Such terms are often too general to be informative. With our edge re-movals, we found that the outdegrees were in the 10s and low 100s for all terms except the very infrequent ones.
An Example Neighborhood. Figure 3 shows the neigh-borhood for  X  X hristianity X  from processing the newsgroups data set, for those neighbors whose edge weight are greater than 0.05 (and pass the PMI constraint). Showing the neigh-bors in a hierarchy helps understand the generality of terms in the corpus. As may be expected, edges going upward have higher weights than the corresponding reverse edge.
Although it is generally difficult to determine the bound-aries for groupings of graph nodes, i.e. , deciding what to include and what not, selection of groups of terms ( X  X om-munities X ) with high inter-connectivity, semicliques, can be useful in various tasks, such as discovering events or topics in test corpora, document tagging, discovery of user interests, and so on. Semicliques, for us, are simply maximal suffi-ciently connected subgraphs. Sufficient connectivity means each term is connected to at least r fraction of the other terms in the semiclique. The process for semiclique finding is given in Figure 4. The input is a co-occurrence graph. Here, unless otherwise specified, this is the graph output us-ing all the steps in Figure 1 (with L = 20, W 0 =0 . 005, PMI filtering, etc), with edges under W min =0 . 01 (the minimum weight of interest) removed. The graph is made undirected (either direction suffices for connection), and we also ignore edge weights in clique discovery. Let Q denote the set of semicliques, initially empty. Repeatedly, a term is picked as seed, one or more iterations of maximal semiclique gen-eration is conducted, and those candidate semiclique (with minimum size 3) that either do not highly overlap with mem-bers of Q , or otherwise have more nodes, are added to Q (the smaller members with high overlap are removed), in order to avoid redundancy. Various operations such as checks for adding a term to a semiclique or checking for intersection size among semicliques are performed efficiently via efficient data structures such as hash maps. Semiclique generation takes in the order of several minutes (newsgroups) to a few hours for the larger data sets. Thousands of semicliques are created ( e.g. , ten thousand for newsgroups with r =0 . 6, over a hundred thousand on Reuters).

We briefly explain some of the choices for semiclique dis-covery. The constraint r  X  0 . 5 is a simple way of keeping the topic focused and avoiding the loose semicliques ( i.e. , with small cuts, or possibly spanning multiple meanings of a term). We experimented with different parameters for win-dow size L ,and L  X  5 yielded very few semicliques, while L  X  40 yielded often relatively topically loose semicliques. We experimented with several techniques for semiclique dis-covery (greedy, etc). We found that multiple random walks from each node yields considerably more useful semicliques than deterministic ( e.g. , greedy) generation.

The generated semicliques, reflecting highly connected terms that tend to occur close to one another, tend to be very inter-pretable. Two example semicliques containing  X  X ohn Kerry X  (democratic senator and 2004 presidential candidate) from TDT5 are: 1. [(joseph lieberman,153)(john kerry,394) (democratic,14198)(gephardt,181)(sen,3138)(sens,174) (massachusetts 1524)(presidential,9908)] 2. [(aspirant,36)(democratic,14198)(john kerry,394) (presidential,9908)(gani fawehinmi,22)(sen,3138)]
Tagging Documents with Topics. Intuitively, the higher the fraction of the semiclique members that appear in the document, the more relevant the topic is to the doc-ument. For determining the tag value of topic S on a docu-ment d (both treated as sets), we simply consider the terms in the intersection of the document and the topic ( d  X  S ), and experimented with Boolean (plain) weighting, log (docu-ment) frequency weighting, and inverse document frequency weighting: P where f ( v ) = 1 for Boolean weighting, and log ( N/df ( v )) and 1 /df ( v ) for log and inverse frequency weighting respec-tively, where N is corpus size. Boolean weighting was notice-ably inferior in both our subjective assessments and in our supervised learning experiments (see next). Log frequency performed the best. Table 2: Improvements, via augmenting tfidf feature
Boost in Supervised Learning. If semicliques capture many of the topics discussed in a corpus, we expect that some such semicliques would align well with the high-level classes assigned to the documents by humans ( X  X ports X ,  X  X ol-itics X , etc). More generally, we expect that the semicliques capture useful regularities. We found that augmenting the regular word-based features with the topic features (those with log df or inv. df tag value exceeding a threshold, such as 0.1 or higher) significantly improved supervised learn-ing performance (the F1 score), even at high training pro-portions (such as 80-20 splits, using SVM classifiers). We compared with LDA topics [1] on newsgroups as well as a subset of Reuters (under different number of hidden topics, k=20, 40, 100, 300, 600). Table 2 displays a subset of our results. Semiclique features, being in the thousands, had an advantage at higher training proportions, while LDA topics yielded higher performance boosts at lower training propor-tions. LDA topics were substantially slower to compute for k in the 100s. 1 With fairly broad (LDA) topics, it is hard to discern what the role of a given term is within the topic. On the other hand, recovering more general (or abstract) top-ics, for example via relaxing the connectivity ratio r and/or increasing the window size, should result in semicliques that are broader and in effect more similar to LDA topics. This is a future direction.
Term co-occurrence (or co-location) relations find a num-ber of applications in diverse tasks, such as semantic dis-tance computation, synonymy, and query expansion ( e.g. ,[4, 2, 14, 15, 5]). Focusing on recovering salient connections and removing weak edges has also been used in large-scale mul-ticlass learning [10]. Here, the goal is not pure prediction, but learning associations. LSI is an elegant matrix method based on term-document co-occurrence patterns [3], and to-gether with the probabilistic extensions, these methods have become very versatile and are well-studied ( e.g. , [6, 1]). As we saw with LDA, these methods appear to be more appro-priate for discovering relatively few (low 100s) and relatively broad topics, and interpretation and efficiency can be an is-sue. There is substantial work on computing semicliques (or quasicliques), dense subgraphs, clustering, and communities in networks. Here, it is very important to allow for overlap-ping groupings to support multiple senses and events, unlike much (graph) clustering work.
Several days. We used the C implementation available at cs.princeton.edu/  X blei/topicmodeling.html
We presented efficient algorithms for computing graphs of co-occurrence relations, and explored extracting highly dense subgraphs as candidate topics. We highlighted the utility of the layered pyramid view of terms. The proposed approach yields numerous fine-grained topics that are inter-pretable groupings of terms. We observed that the topics could augment the standard term features to improve su-pervised learning performance. There are many directions for future work, for example in exploring variations to topic finding methods.

Acknowledgments. Many thanks to Shahin Saadati for his assistance, and to Erik Yeh for suggestions on supervised learning. This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) un-der Contract No. FA8750-07-D-0185/0004, in the context of research on an adaptive cognitive system (CALO), under DARPA X  X  PAL (Perceptive Assistant that Learns) program. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA), or the Air Force Re-search Laboratory (AFRL).
