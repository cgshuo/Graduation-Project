 Changyou Chen 1 , 3 cchangyou@gmail.com Nan Ding 2 ding10@purdue.edu Department of Computer Science, Purdue University, USA Wray Buntine 3 , 1 Wray.Buntine@nicta.com.au National ICT, Canberra, ACT, Australia Dirichlet processes and their variants are popular in re-cent years, with applications found in diverse discrete domains such as topic modeling (Teh et al., 2006), n -gram modeling (Teh, 2006), clustering (Socher et al., 2011), and image modeling (Li et al., 2011). These models take as input a base distribution and produce as output another distribution which is somewhat sim-ilar. Moreover, they can be used hierarchically. To-gether this makes them ideal for modeling structured data such as text and images.
 When modeling dynamic data or data from multi-ple sources, dependent nonparametric Bayesian mod-els (MacEachern, 1999) are needed in order to harness related or previous information. Among these models, the hierarchical Dirichlet process (HDP) (Teh et al., 2006) is the most popular one. However, a basic as-sumption underlying the HDP is the full exchange-ability of the sample path, which is often violated in practice, e.g. , we could assume the content of ICML depends on previous years X  so order is important. To overcome the full exchangeability limitation, sev-eral dependent Dirichlet process models have been pro-posed, for example, the dynamic HDP (Ren et al., 2008), the evolutionary HDP (Zhang et al., 2010), and the recurrent Chinese Restaurant process (Ahmed &amp; Xing, 2010). Dirichlet processes are used because of simplicity and conjugacy (James et al., 2006). These models are constructed by incorporating the previous DP X  X  into the base distribution of the current DP. De-pendent DPs have also been constructed using the un-derlying Poisson processes (Lin et al., 2010). However, recent research has shown that many real datasets have the power-law property, e.g. , in images (Sudderth &amp; Jordan, 2008), in topic-word distributions (Teh, 2006) and in document topic (label) distributions (Rubin et al., 2011). This makes the Dirichlet process an im-proper tool for modeling these datasets.
 Although there also exists some dependent nonpara-metric models with power-law phenomena, their de-pendencies are limited. For example, Bartlett et al. (2010) proposed a dependent hierarchical Pitman-Yor process that only allows deletion of atoms, while Sudderth &amp; Jordan (2008) construct the depen-dent Pitman-Yor process by only allowing dependen-cies between atoms.
 In this paper, we use a larger class of stochastic pro-cesses called normalized random measures with in-dependent increments (NRM) (James et al., 2009). While this includes the Dirichlet process as a special case, some other versions of NRMs have the power-law property. This class of discrete random measures can also be constructed from Poisson processes. Given this, following (Lin et al., 2010), we analogically de-fine superposition , subsampling and point transition on these normalized random measures, and construct a time dependent hierarchical model for dynamic topic modeling. By this, the dependencies are flexibly con-trolled between both jumps and atoms of the NRMs. All proofs and some extended theories are available in (Chen et al., 2012). 2.1. Background and Definitions This background on random measures follows (James et al., 2009).
 Let ( S , S ) be a measure space where S is the  X  -algebra of S . Let  X  be a measure on it. A Poisson process on S is a random subset  X   X  S such that if N ( A ) is the number of points of  X  in the measurable set A  X  S , then N ( A ) is a Poisson random variable with mean  X  ( A ), and N ( A 1 ) ,  X  X  X  ,N ( A n ) are independent if A ,  X  X  X  ,A n are disjoint.
 Based on the definition, we define a complete random measure (CRM) on ( X , B ( X )) to be a linear functional of the Poisson random measure N (  X  ), with mean mea-sure  X  (d t, d x ) defined on a product space S = R +  X  X Here  X  (d t, d x ) is called the L  X  e vy measure of  X   X  . It is worth noting that the CRM is usually written in the form  X   X  ( B ) = P  X  k =1 J k  X  x 0 are called the jumps of the process, and x 1 ,x 2 ,  X  X  X  are a sequence of independent random variables drawn from a base measurable space ( X , B ( X )) 1 . A normalized random measure (NRM) on ( X , B ( X )) is defined as  X  = unnormalized counterpart.
 Taking different L  X  e vy measures  X  (d t, d x ), we can ob-tain different NRMs, and the form we consider is de-scribed in Section 2.3. Here we consider the case  X  (d t, d x ) = M X   X  (d t ) H (d x ), where H (d x ) is the base probability measure , M is the total mass acting as a concentration parameter, and  X  is the set of other hy-perparameters, depending on the specific NRM X  X . We use NRM( M, X ,H ) to denote the corresponding nor-malized random measure. 2.2. Slice sampling NRMs We briefly introduce the ideas of slice sampling nor-malized random measures discussed in ( X  X lice 1 X  ver-sion, Griffin &amp; Walker, 2011). It deals with the nor-malized random measure mixture of the type  X  (  X  ) = where r k = J k / P  X  l =1 J l ,  X  k  X  X  are the component of the mixture model drawn i.i.d. from a parameter space H (  X  ), s i denotes the component that x i belongs to, and g 0 (  X |  X  k ) is the density function to generate data from component k . Given the observations ~x , a slice latent variable u i is introduced for each x i so that it only considers those components whose jump sizes J k  X  X  are larger than the corresponding u i  X  X . Furthermore, an auxiliary variable v is introduced to decouple each individual jump J k and their infinite sum of the jumps P l =1 J l appeared in the denominators of r k  X  X . It is shown in (Griffin &amp; Walker, 2011) that the posterior of the infinite mixture model (2) with the above auxiliary variables is proportional to
P ( ~  X ,J 1 ,  X  X  X  ,J K ,K,~u,L,~s,v | ~x,H, X   X  )  X  exp ( v where 1( a ) is an indicator function returning 1 if a L = min { ~u } , and p ( J 1 ,  X  X  X  ,J K ) = Q K k =1  X  is the distribution for the jumps which are larger than L derived from the underlying Poisson process. Sampling for this mixture model iteratively cycles over { ~  X , ( J 1 ,  X  X  X  ,J K ) ,K,~u,~s,v } based on (3). Please refer to (Section 1.3 Chen et al., 2012) for more details. 2.3. Normalized generalized Gamma processes In this paper, we consider the normalized generalized Gamma processes. Generalized Gamma processes (Li-joi et al., 2007) (GGP) are random measures with the L  X  e vy measure By normalizing the GGP, we obtain the normalized generalized Gamma process (NGG) 2 . One of the most familiar special cases is the Dirichlet process , which is a normalized Gamma process where a  X  0 and b = 1 and the concentration parameter appears as M . Crucially, unlike the DP, the NGG can produce the power-law phenomenon.
 Proposition 1 ((Lijoi et al., 2007)) Let K n be the number of components induced by the NGG with pa-rameters a an b or the Dirichlet process with total mass M . Then for the NGG, K n /n a  X  S ab almost surely, where S ab is a strictly positive random variable param-eterized by a and b . For the DP, K n / log( n )  X  M . Therefore, in order to better analyze certain kinds of real data, we propose to use the NGG in place of the Dirichlet process. In the next section, we propose a dy-namic topic model which extends two major advances of the Dirichlet process: the HDP (Teh et al., 2006) and the dependent Dirichlet process (Lin et al., 2010), to normalized random measures. Our main interest is to construct a dynamic topic model that inherits partial exchangeability, meaning that the documents within each time frame are ex-changeable, while between time frames they are not. To achieve this, it is crucial to model the dependency of the topics between different time frames. In particu-lar, a topic can either inherit from the topics of earlier time frames with certain transformation, or be a com-pletely new one which is  X  X orn X  in the current time frame. The above idea can be modeled by a series of hierarchical NRMs, one per time frame. Between the time frames, these hierarchical NRMs depend on each other through three dependency operators: superposi-tion , subsampling and point transition , which will be defined below. The corresponding graphical model is shown in Figure 1(left) and the generating process for the model is as follows:  X  Generating independent NRMs  X  m for time frame  X  Generating dependent NRMs  X  0 m (from  X  m and  X  Generating hierarchical NRM mixtures (  X  mj , 3.1. The three dependency operators Adapting from the dependent Dirichlet process (Lin et al., 2010), the three dependency operators for the NRMs are defined as follows.
 Superposition of normalized random measures Given n independent NRMs  X  1 ,  X  X  X  , X  n on X , the superposition (  X  ) is:  X  1  X   X  2  X  X  X  X  X  X   X  n := c 1  X  1 + c 2  X  2 +  X  X  X  + c n  X  n . malized version of  X  m .
 Subsampling of normalized random measures Given a NRM  X  = P  X  k =1 r k  X   X  parameter q  X  [0 , 1], the subsampling of  X  , is defined as where z k  X  Bernoulli( q ) are Bernoulli random vari-ables with acceptance rate q .
 Point transition of normalized random measures Given a NRM  X  = P  X  k =1 r k  X   X  tion of  X  , is to draw atoms  X  0 k from a transformed base measure to yield a new NRM as T (  X  ) := P  X  k =1 r k  X   X  Point transitions can be done in different ways with different transition kernels T (  X  ). In this paper, follow-ing (Lin et al., 2010), when inheriting from NRM  X  , we draw atoms  X  0 k from the base measure as  X  conditioned on its current statistics. Other ways of constructing transition kernels are left for further research. 3.2. Properties of the dependency operators The three dependency operators on the NRMs inherit some of the nice properties from the underlying Pois-son process. It not only enables quantitatively con-trolling dependencies introduced after and before the operations, as is shown in (Section 4 Chen et al., 2012), but also maintains a nice equivalence relation between the NRM X  X  and the corresponding CRM X  X . In the fol-the three operations on their corresponding CRM X  X  3 . Theorem 2 The following time dependent random measures (9) and (10) are equivalent:  X  Manipulate the normalized random measures:  X  Manipulate the completely random measures: Furthermore, the resulting NRMs  X  0 m  X  X  give the follow-ing: where q m  X  j  X   X  is the random measure with L  X  e vy mea- X   X  ). T m  X  j (  X  ) denotes point transition on  X  for ( m  X  j ) times . 3.3. Reformulation of the proposed model Theorem 2 in the last section allows us to first take su-perposition , subsampling , and point transition on the completely random measures  X   X  g  X  X  and then do the nor-malization. Therefore, we make use of Theorem 2 to obtain the dynamic topic model in Figure 1(right) by expanding the recusive formula in (10), which is equiv-alent to the left one.
 The generating process of the new model is:  X  Generating independent CRM X  X   X   X  m for time  X  Generating  X  0 m for time frame m &gt; 1, following  X  Generating hierarchical NRM mixtures (  X  mj , The reason for this reformulation is because the infer-ence on the model in Figure 1(left) appears to be infea-sible. In general, the posterior of an NRM introduce complex dependencies between jumps, thus sampling is unclear after taking the three dependency operators. On the other hand, the model in Figure 1(right) is more amenable to computation because the NRMs and the three operators are decoupled. It allows us to first generate the dependent CRM X  X , then use the slice sam-pler introduced in Section 2.2 to sample the posterior of the corresponding NRMs. From now on, we will focus on the model in Figure 1(right). In the next section, we discuss its sampling procedure. To introduce our sampling method we use the familiar Chinese restaurant metaphor ( e.g. (Teh et al., 2006)) to explain key statistics. In this model customers for the variable  X  mj correspond to words in a document, restaurants to documents, and dishes to topics. In time frame m ,  X  x mji : the customer i in the j th restaurant.  X  s mji : the dish that x mji is eating.  X  t mjr : the table r in the j th restaurant.  X   X  mjr : the dish that the table t mjr is serving. We will do the sampling by marginalizing out  X  mj  X  X . As it turns out, the remaining random variables that require sampling are s mji , n 0 mk , as well as Note the t mjr and  X  mjr are not sampled as we sample the n 0 mk directly. Thus our sampler deals with the following latent statistics and variables: s mji , n 0 J mk , J 0 mk and some auxiliary variables are sampled to support these. Sampling J mk . Given  X  n mk , we use the slice sampler introduced in (Griffin &amp; Walker, 2011) to sample these jumps, with the posterior given in (3). Note that the mass M m  X  X  are also sampled, see (Sec.1.3 Chen et al., 2012). The resulting { J mk } are those jumps that ex-ceed a threshold defined in the slice sampler, thus the number of jumps is finite.
 Sampling J 0 mk . J 0 mk is obtained by subsampling of { J We compute the posterior p ( z mk = 1 |  X   X  m , {  X  n decide whether to inherit this jump to  X   X  0 m or not. These posteriors are given in (Corollary 3 Chen et al., 2012). In practice, we found it mixes faster if we inte-grate out z mk  X  X . (Lemma 9 Chen et al., 2012) shows that q -subsampling of a CRM with L  X  e vy measure  X  (  X  ) results in another CRM with L  X  e vy measure q X  (  X  ), thus the jump sizes in the resultant CRM are scaled by q , After the sampling of { J 0 mk } , we normalize it and ob-tain the NRM  X  0 m ,  X  0 m = P k r mk  X   X  J Sampling s mji , n 0 mk . The following procedures are similar to sampling an HDP. The only difference is that  X  mj and  X  0 m are NRMs instead of DPs. The sampling method goes as follows:  X  Sampling s mji : We use a similar strategy as the  X  Sampling n 0 mk : Using the similar strategy as 5.1. Power-law in the NGG We first investigate the power-law phenomena in the NGG, we sample it using the scheme of (James et al., 2009) and compare it with the DP in Figure 2. 5.2. Datasets We tested our time dependent dynamic topic model on 9 datasets, removing stop-words and words appearing less than 5 times. ICML, JMLR, TPAMI are crawled from their websites and the abstracts are parsed. The preprocessed NIPS dataset is from (Globerson et al., 2007). The Person dataset is extracted from Reuters RCV1 using the query  X  X erson X  under Lucene. The Twitter datasets are updates from three sports twitter accounts: ESPN FirstTake (Twitter 1 ), sportsguy33 (Twitter 2 ) and SportsNation (Twitter 3 ) obtained with the TweetStream API (http://pypi.python.org/pypi/tweetstream) to collect the last 3200 updates from each. The Daily Kos blogs (BDT) were pre-processed by (Yano et al., 2009). Statistics for the data sets are given in Table 1. Illustration: Figure 3 gives an example of topic evo-lutions in the Twitter 2 dataset. We can clearly see that the three popular sports in the USA, i.e. , basketball, football and baseball, evolve reasonably with time. For example, MLB starts in April each year, showing a peak in baseball topic, and then slowly evolves with de-creasing topic proportions. Also, in August one foot-dataset vocab docs words epochs ICML 2k 765 44k 2007 X 2011 JMLR 2.4k 818 60k 12 vols TPAMI 3k 1108 91k 2006 X 2011 NIPS 14k 2483 3.28M 1987-2003 Person 60k 8616 1.55M 08/96 X 08/97 Twitter 1 6k 3200 16k 14 months Twitter 2 6k 3200 31k 16 months Twitter 3 6k 3200 25k 29 months BDT 8k 2649 234k 11/07 X 04/08 ball topic is born, indicating a new season begins. Fig-ure 4 gives an example of the word probability change in a single topic for the JMLR. 5.3. Quantitative Evaluations Comparisons We first compare our model with two popular dynamic topic models where the author X  X  own code was available for our use: (1) the dynamic topic model by Blei and Lafferty (Blei &amp; Lafferty, 2006) and (2) the hierarchical Dirichlet process, where we used a three level HDP, with the middle level DP X  X  represent-ing the base topic distribution for the documents in a particular time. For fair comparison, similar to (Blei &amp; Lafferty, 2006), we held out the data in previous time but used their statistics to help the training of the cur-rent time data, this is implemented in the HDP code by Teh. Furthermore, we also tested the proposed model without power-law, which is to use a DP instead of an NGG. We tested our model on the 9 datasets, for each dataset we used 80% for training and held out 20% for testing. The hyperparameters for DHNGG is set to a = 0 . 2 in this set of experiments with subsam-pling rate being 0.9, which is found to work well in practice. The topic-word distributions are symmetric Dirichlet with prior set to 0.3. Table 2 shows the test log-likelihoods for all these methods, which are calcu-lated by first removing the test words from the top-ics and adding them back one by one and collecting the add-in probabilities as the testing likelihood (Teh et al., 2006). For all the methods we ran 2000 burn in iterations, followed by 200 iterations to collect sam-ples. The results are averages over these samples. From Table 2 we see the proposed model DHNGG works best, with an improvement of 1%-3% in test log-likelihoods over the HDP model. In contrast the time dependent model iDTM of Ahmed &amp; Xing (2010) only showed a 0.1% improvement over HDP on NIPS, implying the superiority of DHNRM over iDTM .
 Hyperparameter sensitivity In NGG, there are hyperparameters a and b , where a controls the behav-ior of the power-law. In this section we study the influ-ences of these two hyperparameters to the model. We the subsampling rate to 0.9 in this experiment. We run these settings on all these datasets, the training likelihoods are shown in Figure 5. From these results we consider a = 0 . 2 to be a good choice in practice. Influence of the subsampling rate One of the distinct features of our model compared to other time dependent topic models is that the dependency comes partially from subsampling the previous time random measures, thus it is interesting to study the impact of subsampling rates to this model. In this experiment, we fixed a = 0 . 2, and varied the subsampling rate q shown in Figure 6. From Figure 6, it is interesting to see that on the academic datasets, e.g. , ICML,JMLR, the best results are achieved when q is approximately equal to 1; these datasets have higher correlations. While for the Twitter datasets, the best results are achieved when q is equal to 0 . 5  X  0 . 7, indicating that people tend to discuss more changing topics in these datasets. We proposed dependent hierarchical normalized ran-dom measures. Specifically, we extend the three de-pendency operations for the Dirichlet process to nor-malized random measures and show how dependent models on NRMs can be implemented via dependent models on the underlying Poisson processes. Then we applied our model to dynamic topic modeling. Exper-imental results on different kinds of datasets demon-strate the superior performance of our model over ex-isting models such as DTM, HDP and iDTM.
 Twitter 3 BDT Ahmed, A. and Xing, E.P. Timeline: A dynamic hierarchical Dirichlet process model for recovering birth/death and evolution of topics in text stream. In UAI , pp. 411 X 418, 2010.
 Bartlett, N., Pfau, D., and Wood, F. Forgetting counts: constant memory inference for a depen-dent hierarchical Pitman-Yor process. In ICML  X 10 . 2010.
 Blei, D. and Lafferty, J. Dynamic topic models. In ICML  X 06 . 2006.
 Chen, C., Buntine, W., and Ding, N. Theory of dependent hierarchical normalized random mea-sures. Technical Report arXiv:1205.4159, ANU and
NICTA, Australia, May 2012. URL http://arxiv. org/abs/1205.4159 .
 Globerson, A., Chechik, G., Pereira, F., and Tishby, N. Euclidean Embedding of Co-occurrence Data. JMLR , 8:2265 X 2295, 2007.
 Griffin, J.E. and Walker, S.G. Posterior simulation of normalized random measure mixtures. J. Comput. Graph. Stat. , 20(1):241 X 259, 2011.
 James, L.F., Lijoi, A., and Pr  X  u nster, I. Conjugacy as a distinctive feature of the Dirichlet process. Scand. J. Stat. , 33:105 X 120, 2006.
 James, L.F., Lijoi, A., and Pr  X  u nster, I. Posterior anal-ysis for normalized random measures with indepen-dent increments. Scand. J. Stat. , 36:76 X 97, 2009. Li, L., Zhou, M., Sapiro, G., and Carin, L. On the in-tegration of topic modeling and dictionary learning. In ICML . 2011.
 Lijoi, A., Mena, R.H., and Pr  X  u nster, I. Controlling the reinforcement in Bayesian non-parametric mixture models. J. R. Stat. Soc. Ser. B , 69(4):715 X 740, 2007. Lin, D., Grimson, E., and Fisher, J. Construction of dependent Dirichlet processes based on Poisson processes. In NIPS . 2010.
 MacEachern, S. N. Dependent nonparametric pro-cesses. In Proc. of the SBSS . 1999.
 Ren, L., Dunson, D.B., and Carin, L. The dynamic hierarchical Dirichlet process. In ICML , pp. 824 X  831, 2008.
 Rubin, T., Chambers, A., Smyth, P., and Steyvers,
M. Statistical topic models for multi-label document classification. Technical Report arXiv:1107.2462v2, University of California, Irvine, ASA, Nov 2011. Socher, R., Maas, A., and Manning, C.D. Spectral
Chinese restaurant processes: Nonparametric clus-tering based on similarities. In AISTATS . 2011. Sudderth, E. B. and Jordan, M. I. Shared segmenta-tion of natural scenes using dependent Pitman-Yor processes. In NIPS . 2008.
 Teh, Y.W. A hierarchical Bayesian language model based on Pitman-Yor processes. In ACL , pp. 985 X  992, 2006.
 Teh, Y.W., Jordan, M.I., Beal, M.J., and Blei, D.M. Hierarchical Dirichlet processes. Journal of the ASA , 101(476):1566 X 1581, 2006.
 Yano, T., Cohen, W., and Smith, N.A. Predicting response to political blog posts with topic models. In Proc. of the NAACL-HLT , 2009.
 Zhang, J., Song, Y., Zhang, C., and Liu, S. Evolu-tionary hierarchical Dirichlet processes for multiple
