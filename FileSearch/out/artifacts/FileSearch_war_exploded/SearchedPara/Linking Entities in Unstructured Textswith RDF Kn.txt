 Nowadays, search engines have been widely used as the most convenient way of ac-cessing information within the huge amount of unstructured texts on the Web. How-ever, techniques adopted by most existing search engines are based on straightforward matches of terms within queries and those within the unstructured texts. This often re-sults in that, users are frustrated in frequently adjusting their query terms to retrieve the desired results. To address this problem, semantic search has been proposed and widely studied [1]. In semantic search, the contextual meaning of terms in the unstruc-tured texts is very important to enrich the semantics of terms. In this paper, we study the problem of enriching contexts of named entities within unstructured texts, by link-ing the detected named entities with existing RDF knowledge bases. By doing this, we are able to extract important concepts and topics out of the plain texts, and effectively support the semantic search over the unstructured texts.

With the continued progress of Semantic Web and information extraction techniques, more and more RDF data emerge on the web. They form many huge RDF knowledge bases (KBs) such as Yago[2], Freebase[3], DBPedia[4] et al. Such RDF KBs contain billions of RDF triple facts either extracted from Web pages or contributed manually by users, describing information about hundr eds of millions of entitie s. The entity infor-mation in these KBs is so abundant and diverse that they are very suitable for enriching the contexts of named entity mentions within the unstructured texts.

The entity linking (also called entity annotation in the paper) task is defined as to map a named entity mention m in the free texts of a Web page to a corresponding entity e in a KB. In our work, a KB refers to an RDF KB without further specification. We list some applications that such a way of entity linking can be applied as follows:  X  Enhancing search results of semantic search. Through linking entities with RDF  X  Cleaning the extracted named entities. The named entity mentions extracted by  X  Finding relations between entities. Given a pair of entities, their relations can be One major challenge of entity linking is the disambiguation problem. A named en-tity mention has the disambiguation problem because a number of entities (of different types) in the KB can have same name mention. When this happens, we need to accu-rately link the entity mention to a proper KB entity. This is called entity resolution (or entity disambiguation) problem. For example, a mention  X  X ichael Jordan X  may map to at least two entities in a KB. One is a famous retired NBA basketball player, and the other is a famous computer scientist in U.C. Berkeley. Many existing approaches use a lot of features from contexts to address the entity disambiguation problem. However, an RDF KB can be treated as a huge labeled graph, by considering the properties of the graph, we observe that the relevant entities of mentions in the same context often are close pairs in the KB graph. Based on the observation, we propose an effective approach that accomplish the entity disa mbiguation problem with only two proposed features.
An open domain RDF KB has a large number of entities. However, many existing approaches[8,9] are designed on small-scal e data set, or can only process one document a time[21], which are not scalable. For a huge RDF KB, entity linking will have to be done in a parallel and distributed fashion fo r guaranteeing the efficiency and scalability. Fortunately, the MapReduce[6] framework on cloud computing provides an easy-to-use way of dispatching the expensive annotating and disambiguating tasks over clusters. We therefore propose an efficient entity disamb iguation algorithm that is able to conduct the entity linking task in parallel based on the MapReduce framework.

The main contributions of the paper are as follows:  X  We propose a simple but effective algor ithm to accurately link named entity men- X  We propose an MapReduce-based approach to linking mentions of unstructured  X  We test the performance of our approach es over two real-life RDF KBs. The re-The rest of the paper is organized as follows. Section 2 reviews the related work. Section 3 describes the solution of entity disambiguation using RDF KBs. In Section 4, we propose an efficient framework of conductin g the entity linking task on MapReduce framework. Section 5 presents the results of e xperimental study. Finally, the paper ends with conclusion and future work in Section 6. Entity linking has received much attention recently[7,15,14], especially after the emerg-ing of large knowledge bases such as Yago[2], Freebase[3], DBPedia[4]. Most of these works[8,9] link free texts to real world knowledge bases. The most important and chal-lenging issue in entity linking task is the named entity disambiguation problem.
Named entity disambiguation is also called co-reference resolution or word sense disambiguation in some other studies. Most studies in this area tackle the challenges based on rich contexts where named entity mentions occur. Bagga and Baldwin[11] use vector space model to resolve the ambiguities in persons X  names. They represent the context of the entity mentions with the bag of words model and compute similarity between vectors. An early work [7] identifies the most proper meaning of ambiguous words by measuring their context overlaps. In [10], the authors derive a set of features from Wikipedia to compute similarity between context of mentions and the texts of Wikipedia.

In word sense disambiguation, there are two main streams of solutions[17,18]. Some researchers use knowledge extracted from d ictionaries to identify the correct word in a given context[8,16]. The others coll ect probabilities from large amounts of sense-annotated data, and then use machine learning approaches to solve the problem[17,18]. The authors of [12] implement and combine these two approaches. They choose the right and left word of the ambiguous word as its context. For each word, they extract a training feature vector from Wikipedia links, and then integrate the features in Naive Bayes classifier. These works in word sense disambiguation are similar to the entity linking task. However, they are more likely to assign dictionary meanings to the poly-semous words, which is not as difficult as the entity linking task.

Weikum and Theobald[19] propose their na med entity disambiguation approach in facts harvesting research works. In [20], they interconnect RDF data and Web contents via LOD (Linked Open Data)[5]. They construct an entity mention graph and use a coherence graph algorithm to solve the en tity disambiguation problem. Through com-puting the coherence between ambiguous en tities, the mentions are connected to at most one entity in a KB.

The most similar work to ours are Linden[21]. Linden first builds a dictionary based on four kinds of Wikipedia pages: entity page, redirect page, disambiguation page and hyper links. By using the dictionary, Linde n generates a candidate linking list for each mention. In order to conduct entity disambi guation, it creates feature vectors of four dimensions to rank the entities in candidate list. However, Linden can process only one document at the same time. For real world applications, there will be much more mentions from more than one document to be processed simultaneously. Moreover, it generates many features to rank the candidate linking entities. Some of them (e.g., the global coherence feature) are relativ ely hard to be accurately computed.

RDF (Resource Description Framework) is recommended by W3C Consortium to describe information on the Web. The basic information unit of RDF data is a triple t = &lt;s,p,o&gt; which stands for subject, predicat e and object respectively. An RDF KB contains a finite set of RDF triples. Assuming subjects and objects are entities, predicates are relations between entities, an RDF KB then forms labeled entity relation graphs. An example is shown in Fig. 1.

To the best of our knowledge, there is no entity linking work based on RDF knowl-edge base over MapReduce framework. Because RDF data can be represented as graphs, we can use the properties of the graph to design a simple but effective approach. The frequently used notations in this paper are summarized in Table1.
 The entity linking task basically links named entity mentions with entities in a KB. If it is na  X   X vely conducted based on string match, quite often, a mention will be mapped to more than one entities in the KB because of the homonymy problem. In our work, we use Jaccard to compute the similarity between two strings (entity mention and entity in KB), an entity mention will be mapped to an entity in KB if the similarity between them is higher than some threshold. For example, a mention  X  X ichael Jordan X  in unstructured texts will be mapped to more than one entity in KB, for instance,  X  X ichael I. Jordan X  and  X  X ichael J. Jordan X . This requires a solution of entity disambiguation.
In this study, we take two measures into account for addressing the entity disam-biguation problem. One is the relevance, wh ich measures the relation between a men-tion and an entity. The other one is the homogeneity, that is to find the proper linking entity which has closer hyponyms with the menti on. By studying the properties of exist-ing RDF KBs, we introduce two features to measure the relevance and the homogeneity of an entity in a KB to a given mention of certain context. 3.1 Construct Semantic RDF KB Graph An RDF KB can be represented as a graph, which is called RDF KB graph. Based on the RDF KB graph G , we construct a semantic KB graph SG . Through SG we propose approaches to linking mentions with proper entities in an RDF KB.
 Definition 1 (RDF KB Graph). Given an RDF KB, let  X  be the set of all entities in the KB, R be the set of relations between entities in the KB. An RDF KB Graph is G =( V,E,L ) ,where V is the set of vertexes, E is the set of edges and L is the set of literals. V  X   X  , E  X  R , L v is the set of labels on vertex, while L e is the set of labels on edges. L = L v  X  L e .
 Definition 2 (Semantic KB Graph). Given an RDF KB graph, a semantic KB graph ( SG )is SG =( V,E,L,C ) ,where V is the set of vertexes, E is the set of edges, L is the set of literals and C is the set of classes.  X  v  X  V,C v  X  C .
 To construct the semantic KB Graph, we firstly recognize entities and relations in a given RDF KB, where an entity is the triples with same subject(s), and a relation be-tween entities is the predicate (p) which c onnects two entities. Based on entities and relations, we can easily construct an RDF KB graph. Then, we pick predicates such as  X  X df:type X  in the RDF KB graph as initial cla ss nodes set. Duplicates and noises in the set are cleaned. Some new class nodes are added based on WordNet. We finally built the taxonomy of the RDF KB. For pages limitation, we will not discuss the details of taxonomy construction. An RDF KB graph expanded by the taxonomy is called seman-tic KB graph. Fig. 1 is an example of an RDF KB graph, while Fig. 2 is an example of the corresponding semantic KB graph. For instance, after expanded as Semantic Graph, node  X  X ichael Jordan X  and node  X  X avid E. Rumelhart X  in Fig. 1 have class node as  X  X rofessor X  in Fig. 2. 3.2 Context Dependency The mentions appearing in the same contex t often have a high probability to be under the same topic. Meanwhile, in an RDF KB gra ph, the relevant en tities (e.g., under the same topic) are likely to be close with each other in terms of the graph distances. For example, if  X  X ichael Jordan X  refers to the famous basketball player, mentions such as  X  X BA X  and  X  X hicago Bulls X  are more likely to be found than mentions such as  X  X achine learning X  in the context. Moreover, in the KB graph, the shortest path distance between the entities X  X ichael Jordan X  (basketball player) and  X  X BA X  is likely to be shorter than the distance between  X  X ichael Jordan X  (computer sc ientist) and  X  X BA X . Accordingly, we are able to measure the relevance of entities to a given mention.
We denote the mentions in same document with m as the contexts of m ,thatis C .An entity e  X  X  is the linking entity of a named entity mention m  X  M . Suppose c k  X  X  is the k th context mention, e k  X  X  is the linking entity of c k (where we choose the c s from C which have only one linking entity. Mentions have no linking entity are useless. Mentions with more than one linking entities will lead to a recursive processing.), we formally define the context dependency of an entity e , denoted as CD ( e ) as: where Dist ( e k ,e ) is a function measuring the distance between e k and e as where D k is the shortest path distance from e to c k . If the average distance between e  X  X  and all e be the highest. 3.3 Semantic Similarity Most entity nodes in an RDF KB have at least one corresponding class node (see square nodes in Fig.2). A class node may have a super class node as its ancestors. Therefore, when a mention m  X  M is linked to an entity e , m will have the same class node and super class nodes as e . For these mentions without direct connected class node, we choose the class with the shortest path to m as its class node. As shown in Fig.2, if a mention links to an entity  X  X ichael Jordan X  (the basketball player), it will have a  X  X asketball player X  as its class node, and  X  X portsman X  as a super class node.
Assuming that the taxonomy of the KB is a tree, for two entities e i and e j in KB, the classes in taxonomy for them are C i and C j respectively, where C 0 ( i, j ) is the lowest common ancestor (LCA) class (super class) of C i and C j . Suppose e i is the linking entity of m i , while c ik is one of the context mention of m i , e j is the linking entity of c . The semantic similarity between e i and e j can be computed by equation 3. the level L in the taxonomy tree, P is the shortest path distance between C 0 ( i, j ) and C i in the tree. Larger P leads to less r ( i, j ) , which means less similarity of C j and C i . Therefore, the semantic similarity for entity e can be defined as:
SS ( e ) shows the type similarity between the context mention c and the mention m , it measures the homogeneity of e . For example, e 1 and e 2 are two candidate linking entities of m , if the class nodes of linking entities ( e )for c are more closer to e 1 than e , e 1 is more likely to be the proper linking entity of m . Higher value of SS ( e ) means closer class structure to the candidate linking entity. 3.4 Ranking Candidate Entities The score of each candidate entity in the candidate list of a mention m can be computed by combining CD ( e ) and SS ( e ) : where  X  are weights of CD ( e ) and SS ( e ) . A max-margin technique introduced in [23,21] is applied to learn the weights.

When there are more than one entities in the candidate linking list ( CL ) for a mention m , the one with the highest score e top =max e  X  CL Score ( e ) is chosen as the proper linking entity of m . Algorithm 1 provides the details of linking entity disambiguation. Algorithm 1. Linking Entity Disambiguation (LED) Nowadays, as numerous data emerge both on Web pages and RDF KBs, the entity link-ing task faces with the big data problem. Therefore, efficiency and scalability become important issues of the entity linking solu tion. In this section, we propose a frame-work for efficient entity linking based on t he entity disambiguation algorithm proposed above. We assume that the mentions need to be linked to entities have been extracted and recognized from web pages. We also assume the mentions have potential linking entities in the KB. In order to linking a mention to an RDF KB, we propose a framework including the following modules:  X  Generate Candidate Entity Linking List . For each mention m  X  M , obtain the  X  Linking Entities Disambiguation . If the candidate linking list has more than one Each mention m in Web pages will be compared with the entities e in KB, if they match, e will be add into the candidate linking list of m . The length of linking list, | L | , must be in one of the following three cases. (1) | L | =0 , which means no matching entity exists in the knowledge base. (2) | L | =1 , which means there is only one match in KB for m .(3) | L | X  1 , which means two or more e in knowledge base are the possible entity links of m . For case 1, NIL will be returned to m ; for case 2, the only e is returned to show there is a link between m and e ; only for case 3 we need to use the disambiguation algorithm described in Section3 to find the proper linking entity of m . The algorithm is executed on MapReduce framework. In the map phrase, mentions and entities with the same key are dispatched to the same reduce node; In the reduce phrase, mentions(entities) and their nei ghbors in the semantic KB graph are gathered to compute CD and SS . After several iterative map and reduce phases, the algorithm can output the proper linking entity of m . With MapReduce framework, the algorithm is conducted in a parallel fashion, which makes the approach more scalable. 5.1 Experimental Setup All the experiments are run on the Renda-Cloud platform, which is a Hadoop cluster. Each node in the cluster has 30GB memory, 2TB disk space and 2.40 GHz core 24 processor, running Hadoop 0.20.2 under Ubuntu 10.04 Linux 2.6.32-24.
 In order to evaluate our approach, we choose two real world RDF KBs, YAGO and FreeBase, to conduct the experimental study. YAGO is a large knowledge base devel-oped at Max-Planck-Institute Sarrbr  X  ucken . It contains information from Wikipedia and linked to Wordnet . There are more than 10 million entities and 120 million facts about the entities. YAGO contains not only entities and relations between entities but also semantics of entities. The semantics relations include TYPE, subClassOf, domain et al. Based on the structure, we can build se mantic KB graph over YAGO. FreeBase is a large collaborative knowledge base developed by the Metaweb company. It contains data from Wikipedia, NNDB et al., with 125M tuples and more than 4000 types of enti-ties. The facts in FreeBase are on different fiel ds, such as sports, arts and entertainment.
In the experiments, we use Accuracy(Accu.) to evaluate the performance of our so-lution. It is calculated as in Equation 6: where N l is the number of mentions which are correctly linked to entities, N m is the total number of mentions.

Sections 5.2 and 5.3 give the experimental results on accuracy and scalability over the two RDF KBs respectively. 5.2 Experiments on YAGO Accuracy. In order to evaluate the performance, we firstly extract 1000 mentions from 50 Wiki pages as 50 sets of Mentions. menti ons in each set are contexts for each other. We t e s t Accu. by using different features in score function. Table 2 shows the highest, lowest and average Accu. with different features for the 50 groups of mentions. Then we compare our solution with Linden, which is the most similar work with ours in the existing solutions. we use the 1000 mentions in one group and test our solution on 2 Hadoop nodes, while test Linden on a PC with a 1.8Ghz Core 4 Duo processor, 10 GB memory and running 64-bit Linux Redhat kernel. Table 3 shows the results.
 feature set Accu. Max Min Avg.
 CD + SS 0.9487 0.9232 0.9411
From Table 2 we can see that the accuracy of CD is higher than SS . By combining the two features, our solution achieves the best accuracy. It indicates that in most cases, relatedness is more useful in dealing w ith entity linking problem. Although Linden has good performance, the comparison results in Table 3 shows that the accuracy of our solution is better than that of Linden.
 Scalability. In order to test the scalability of the algorithm, we varying the number of mentions from 1000 to 100,000 and execute the algorithms on 2 Hadoop nodes, 8 Hadoop nodes, 20 Hadoop nodes respectively. The results are shown in Table 4. The time consumption includes generating candi date linking list and scoring candidate link-ing entities, for these are the main jobs in linking entity task.

Results in Table 4 show the scalability of our solution. With more nodes in Hadoop cluster, the computing capability are efficiently enhanced. The minimum execution time in our solution is above 10 seconds. Note that the initial time for MapReduce job is at least 10 seconds. 5.3 Experiments on Freebase In this part of experiment, we choose 30GB Freebase KB data and more than 250,000 mentions to evaluate our solution in processing a large amount of data. The mentions are extracted from IMDB, which is a database about movies, directors, actors and rating from users et al. We store the data from IMD B into a flat table, each row in table are extracted from one page, therefore the cell c ontents in same row are treated as context to each other. We also test the accuracy and the scalability to see the performance of our solution.
 Accuracy. In the experiment, we firstly process 250,000 mentions in 50 group, the results in Table 5 show the maximum, the minimum and the average accuracy with different features used in the score functi on. Then we compare our solution of feature SS with baseline: LCA(Lowest Common Ancestor) with one group of 5,000 mentions (results are shown in Table 6), for both SS in our approach and LCA are annotated entities by using the information of their classes (types). Where LCA is simply assign all cells in one column with their lowest common ancestor.
 feature set Accu. Max Min Avg.
 CD + SS 0.9407 0.9185 0.9366 From Table 5 we can see that the accuracy results are worse than that in Table 2. We just store data extracted from IMDB wit hout any preprocessing, because they have little noisy than data form Wikipedia. However, the accuracy is still above 0.93. The results in Table 6 show that with only the feature SS our solution still outperforms the baseline. Scalability. In this part of experiments, we test the time consumption in KB preprocess-ing to see the scalability of our solution. The main work of KB preprocessing includes construction of RDF KB graph and semantic KB graph. We treat it as a preprocessing step, because once the RDF KB graph and semantic KB graph has been constructed, we can do entity linking on top of them whe n needed without of reconstruction.
We choose part of connected data from Freebase, so that we can vary the KB size from 5M to 30G. We execute KB preprocessing steps on 2 Hadoop nodes, 8 Hadoop nodes, 20 Hadoop nodes respectively. The time consumption results are shown in Table 7. The results show that with more nodes in Hadoop cluster ,we can handle the linking entity task over KB size above tens of Gigabytes. It also shows that the algorithm we proposed in the paper has high scalability. Our solution gives an efficient way to handle the linking entity problem on big data. In this paper, we propose a simple but effect ive algorithm to accurately link entity men-tions with proper entities in an RDF KB. The algorithm is designed on the MapReduce framework, which can annotate multiple docum ents in parallel. The experimental study over huge real-life RDF knowledge bases d emonstrates the accuracy and scalability of the approach.

Based on our solution we can study semantic search techniques using the annotated unstructured texts. For example, by linking unstructured texts to RDF KB, we are able to build a huge extended labeled graph, through which we are able to apply and extend semantic keyword search over text graphs.
 Acknowledgements. This work is supported by the National Science Foundation of China under grant No. 61170010 and No. 61003085, and HGJ PROJECT 2010ZX01042-002-002-03.

