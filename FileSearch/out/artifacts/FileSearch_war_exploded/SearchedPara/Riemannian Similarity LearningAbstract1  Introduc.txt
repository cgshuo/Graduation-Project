 Li Cheng chengli@bii.a-star.edu.sg Bioinformatics Institute, A*STAR, Singapore School of Computing, National University of Singapore, Singapore We consider a similarity-score based learning problem: Denote by  X  x  X  R  X  n and x  X  R n instances from two different sources, predict whether a pair of unseen in-stances ( X  x , x ) belongs to the same class. The problem is key to many seemingly-unrelated real-life applica-tions, where we are asked to make predictions given access to either heterogeneous sources that differ from training to testing phases, or provided only incomplete knowledge of the set of possible class labels, or even both. In particular, they include the applications of domain adaptation as well as pair-matching.
 In domain adaptation (Pan &amp; Yang, 2010), data from the source and target domains are often very different, and might reside in separate spaces. The closest re-search here might be the work of (Saenko et al., 2010; Kulis et al., 2011): The Symm method of (Saenko et al., 2010) is developed based on metric learning with cross-domain constraints. Symm aims to learn a sym-metric transformation to project the source and target domain data into a domain-invariant space. The ARC-t method of (Kulis et al., 2011) extends this idea fur-ther by instead learning a asymmetric transformation to map the target domain data to the source domain. Designated to work with square similarity matrices, it remains cumbersome to address the  X  n 6 = n scenar-ios often seen when dealing with heterogeneous data sources such as sound tracks and videos. The appli-cations of pair-matching have been raised in e.g. face or action recognitions (Chen et al., 2005; Kliper-gross et al., 2012), where there is a practical demand to iden-tify a new face image or a clip of unseen action, a class label that does not exist in the training set. Note this pair-matching application is also referred to as face verification (Kumar et al., 2009; Yin et al., 2011). One distinct feature of pairwise similarity learning lies in its ability to make predictions on novel class labels not provided in training set. Recently its potential has drawn increasing attention, ranging from theoreti-cal studies (Balcan &amp; Blum, 2006; Wang et al., 2009b) to real-life applications such as face and action recog-nition (Chen et al., 2005; Huang et al., 2007; Kliper-gross et al., 2012). This pairwise similarity learning problem is in spirit similar to the metric learning re-search work (Davis et al., 2007; Jain et al., 2012; Bellet et al., 2012; Mensink et al., 2012): One main difference is here we need to deal with non-symmetric and non-square similarity matrices, versus the more well-posed metric functions 1 usually considered in metric learn-ing. This line of research is also related to one-shot learning or learning from one example (Miller et al., 2000; Fei-fei et al., 2006). The algorithm development is usually emphasized on the specific scenario where the similarity function is symmetric  X  in other words,  X  x and x have to come from the same source.
 We consider a general similarity learning scenario where  X  x and x are from the different sources and even reside in different dimensions,  X  n 6 = n . Our aim is to learn this  X  n by n similarity matrix W , which is well-known to be a smooth ( C  X  ) manifold (e.g. Exam-ple 8.14 of (Lee, 2003)). However, this often leads to a huge number of unknowns, as both dimensions could be large. Two strategies natural follow as a consequence: (1) Work with sparse methods to ex-ploit the sparsity (e.g. ` 1 -norm) or structural spar-sity (e.g. sparsity induced norm) of the matrix (Bach et al., 2012), or (2) Utilize matrix factorization tech-niques (Hubert et al., 2000; Eld  X en, 2007), in order to uncover the inherit matrix structure. Meanwhile, an important observation here is that the rank of this matrix is usually very low , as being reported by re-lated research work (Ying et al., 2009; Bi et al., 2011), which is also empirically supported by our synthetic experiments in Sec. 4. This inspires us to consider a rank-related matrix factorization, and a natural choice is SVD. Recent development in manifold-based opti-mization (Absil et al., 2008) offers new geometric in-sight into the underlining geometric structures for this type of low rank matrices. By resorting to computa-tional differential geometry principles and techniques, competitive or even superior performance has been achieved on matrix-related applications (Shalit et al., 2012; Vandereycken, 2013).
 Our Contributions The main contributions in this paper are three-fold. First, we propose to work with similarity learning in the general setting, as such we aim at learning a non-symmetric, non-square similar-ity matrix with fixed rank. This is less-restrictive com-paring to the existing work discussed earlier. It thus enables to directly work with the  X  n 6 = n settings. Sec-ond, our algorithm offers a new geometric interpre-tation for similarity learning, and by this we wish to bring some insights in understanding the characteris-tics of the similarity matrix. Third, our work brings in-teresting connections between the applications of pair-matching and domain adaptation. The proposed geo-metric algorithm also connects to and makes possible the exploitation of Riemannian manifold-based opti-mization (Absil et al., 2008). Empirical experiments verify the competitive performance of our algorithm in real-life vision tasks of object and action recognition. Related Work The closest work is that of Kulis et al. (Kulis et al., 2011), which also aims to learn a non-symmetric similarity matrix. Meanwhile, there is no explicit rank or sparsity constraints on the matrix to be learned, leaving the potential for overfitting. The works in Mahalanobis metric learning e.g. (Bellet, 2012) are closely related, where the main focus is on symmetric positive semi-definite matrices. Another thread of intimately related works is bilinear learn-ing (Tenenbaum &amp; Freeman, 2000; Pirsiavash et al., 2009; Pirsiavash &amp; Ramanan, 2012; Akhter et al., 2012). Denote the trace operator of a matrix as tr(  X  ), and factorize the matrix W as W := W l W T r , with W l and W r being a  X  n  X  d and a n  X  d matrix, respec-tively. The bilinear function of interest usually takes the following form On the other hand, significant progress has been made over the past few years in optimization on Riemannian manifolds (Absil et al., 2008). Recently research has also been carried on toward the topic of geometric low-rank matrix factorization (Vandereycken, 2013; Mishra et al., 2012). Our Riemannian approach can be viewed as an adaptation of the works of (Vandereycken, 2013; Absil et al., 2008) to similarity learning. We briefly recall the related notions of matrix man-ifold and retraction. Motivated readers can refer to (do Carmo, 1992; Lee, 2003; Absil et al., 2008) for fur-ther details.
 Stiefel Manifold (Lee, 2003) Define the set of  X  n  X  d orthonormal matrices as st ( X  n,d ) := { U  X  R  X  n  X  d : U T U = I d } , with I d an identity matrix of size d  X  d , and  X  n  X  d . It is a differentiable manifold that can be locally approximated by a set of Euclidean spaces. Consider an arbitrary point in the manifold, U  X  st ( X  n,d ). To perform differential calculus, de-fine the tangent space at U as T U st ( X  n,d ), which is also a subset of  X  n  X  d matrices. It is easy to check U T  X  =  X   X U T for any tangent vector  X   X  T U st ( X  n,d ). Retraction (Absil et al., 2008) Intuitively, re-traction generalizes the notion of moving in the direc-tion of a vector in Euclidean space to manifolds. An ideal retraction is the exponential map (Lee, 2003): In our context, the exponential map at point W , R W (  X  ), maps a tangent vector  X   X  T W M to a point in the manifold M , as projecting along a geodesic curve started at W in the direction of  X  . In practice, usually a computationally less demanding retraction is used instead of the exact exponential map. Formally, a re-traction on a manifold M at point W is a function R
W : T W M X  X  that satisfies two properties (Absil et al., 2008). (1) R W (0) = W . (2) The geodesic curve defined by  X  W (  X  t ) := R W (  X  t X  ) satisfies  X   X  W  X  t being a real. In general, a retraction can approx-imate the exponential map at least to its first order Taylor expansion.
 Vector Transport (do Carmo, 1992; Absil et al., 2008) In a way similar to retractions vs. exponential mapping, the vector transport (Absil et al., 2008) is an approximation of parallel transport (do Carmo, 1992), by transporting a tangent vector  X  from a point W  X  M to a point W 0 := R W (  X  )  X  M along a geodesic curve defined by  X  . More specifically, it is defined w.r.t. an existing retraction R as (  X , X  ) 7 X  T  X  (  X  )  X  T W 0 for any  X , X   X  T W M and W 0 := R W (  X  ), satisfying T (  X  ) =  X  , and T  X  ( a X  + b X  ) = a T  X  (  X  ) + b T  X  (  X  ), with a,b  X  R .
 Riemannian Connection (do Carmo, 1992) and Riemannian Hessian (Absil et al., 2008) In an Euclidean space, A connection  X   X   X  amounts to the directional derivative of  X  along  X  at a point W , as  X  tor fields. This is the special case of affine connection on a manifold M , which satisfies three properties: i)  X  f X  + g X   X  = f  X   X   X  + g  X   X   X  , ii)  X   X  a X  + b X  = a  X   X   X  + b  X   X   X  , and iii) Leibniz X  rule:  X   X  f X  =  X f  X  + f  X   X   X  . where  X , X , X , X   X  T M , f,g are real functions of M , and a,b  X  R . A Riemannian metric on a differen-tiable manifold is a correspondence that associates to each point W  X  M an inner product,  X  X  ,  X  X  W in the tangent space of M W . A differentiable man-ifold with a given Riemannian metric is termed a Riemannian manifold. A Riemannian Connection is thus the unique affine connection  X  of the Rieman-nian manifold M that is i) symmetric and ii) com-patible with its Riemannian metric. The Riemannian Hessian is related to the Riemannian Connection as Hess J ( W )[  X  ] :=  X   X  Grad W J ( W ), for any  X  in the tan-gent space of M W .
 Riemannian Trust Region Methods (Nocedal &amp; Wright, 2006; Absil et al., 2008) Trust region methods (Nocedal &amp; Wright, 2006) work by simulta-neously choose the descending direction and the step-size, by explicitly approximating the objective function J ( W ) with a quadratic model m W . In analogy to its Euclidean space counterparts, at each iteration of the Riemannian trust region methods (Absil et al., 2008), the following trust-region subproblem is to be solved: := J ( W ) +  X  Grad W J ( W ) , X   X  + subject to constraint  X   X , X   X   X   X  2 , where  X  is current trust-region radius. The solution this subproblem pro-vides a descending direction in the tangent space of current point W  X  X  . In our context, an example is represented as a triplet ( X  x,x,y ), with pair of instances  X  x  X  R  X  n and x  X  R as well as y  X  { X  1 } depending on whether the labels of the two instances are equivalent. We consider a bi-linear form f W : ( X  x,x ) 7 X   X  x T Wx , with W a matrix of size  X  n by n taking a fixed rank d min {  X  n,n } . In general, the Left Hand Side (LHS) feature vector X  X  dimension  X  n is not necessarily the same as the RHS di-mension n , therefore W is rectangular. An important fact is the existence of a factorization W = U  X  V T by SVD (Golub &amp; Loan, 1996):  X  := diag(  X  1 ,..., X  d contains positive singular values uniquely determined by W , while U and V form the eigen-bases of WW T and W T W , respectively. As a result, through f W the vector  X  x is mapped to a d -dimensional vector space,  X  1 2 U T  X  x , and x mapped to the same space via  X  1 2 V T x . This fact can be subsequently used to construct a hypothesis function h for prediction problems, for example, h ( X  x,x ) := sgn( f W ( X  x,x )) = sgn  X  1 2 U T  X  x T  X  1 2 V T x . Intuitively, h ( X  x,x ) = 1 if the inner product is positive (i.e. along the same direction).
 The problem is then ready to be formulated as solving an optimization problem over a set of t training triplets A variety of differentiable loss function can be con-sidered in our context w.r.t. f W ( X  x,x ), including for example the squared hinge loss l hinge 2 ( X  x,x,y ; f max { 0 , 1  X  yf W ( X  x,x ) } 2 , the log loss l log ( X  x,x,y ; f log 1 + e  X  yf W ( X  x,x ) , and the ridge loss l 2 ( X  x,x,y ; f f W ( X  x,x )  X  y 2 . We have empirically experimented with a number of loss functions, and it turns out that the log loss consistently delivers a top perfor-mance. So from now on we focus our attention to the log loss. Note here we choose not to consider the non-differentiable loss functions such as the hinge loss, which are more intricate for convergence anal-ysis. Nevertheless we would like to mention that the set of manifold subgradients can be obtained for non-differentiable functions, and certain types of sub-gradient descent algorithms are also known to con-verge (Ferreira &amp; Oliveira, 1998). Furthermore, the regularizer term is necessary to ensure W a bounded matrix. In this paper we restrict our attention to the Frobenius norm,  X ( W ) := k W k F .
 Geometric Meaning of SVD Denote a set M
W := { W = U  X  V T } , s.t. U  X  st( X  n,d ), V  X  st( n,d ), and  X  = diag(  X  1 ,..., X  d ), with  X  i &gt; 0 ,  X  i . In other words, diag(  X  1 ,..., X  d ) denotes a d  X  d positive diag-onal matrix. It has been shown by (Absil et al., 2008; Vandereycken, 2013) that M W is indeed a Riemannian manifold, with its tangent space at point W being es-tablished by for M being arbitrary d by d matrices, U T p U = 0, and V p V = 0. The Riemannian metric becomes  X   X , X   X  = tr (  X  T  X  ), with tr (  X  ) being the matrix trace. From Euclidean gradient to Riemannian Gra-dient and Riemannian Hessian Denote the Eu-clidean gradient of our objective function w.r.t. W as grad W J  X  R  X  n  X  n , by (3) its Riemannian gradient is computed as a projection onto the tangent space of M with shorthand notations P H U := UU T and P V U := I  X  UU T for U , as well as similar notations for V . It turns out impossible in our context to compute an analytic form of the Riemannian Hessian Hess J ( W ). We instead perform finite difference approximation of the Hessian by forward difference of the Riemannian gradients (Nocedal &amp; Wright, 2006; Absil et al., 2008), where the vector transport is utilized to move the Rie-mannian gradients from the forward point back to the tangent space of the current point.
 Retraction For any tangent vector  X   X  T W M , its retraction R W (  X  ) into the manifold is naturally deter-mined by It is easy to verify that it results in a closed form so-lution with  X  i , u i and v i being the i -th singular values and vectors of SVD of W +  X  , respectively.
 RSL As an adaptation of the Riemannian trust-region method of (Absil et al., 2008), the proposed algorithm for Riemannian Similarity Learning, or RSL in short, is presented in Algorithm 1. At each itera-tion, the trust-region subproblem is solved following the truncated conjugate gradient method presented in (Absil et al., 2008) chapter 7, which produces a candidate tangent vector  X  k . To decide whether to ac-cept  X  k as well as the new trust-region radius  X  k , we evaluate the following  X  k value Ideally  X  k is expected be close to 1, then we accept  X  k and the trust-region radius  X  k +1 may be enlarged; If  X  k is however less than a small positive  X  0 , then the model is not accurate and in this case we must reject the candidate and reduce the trust-region radius; If  X  k is not too small (  X   X  0 ) but still away from 1 ( i.e.,  X  ), we accept the candidate while still reducing the radius; Lastly if  X  k 1, the model is inaccurate while there is a significant decrease in objective function J , we can choose to accept the candidate and increase the radius, in the hope of a greater decrease in J value. This explains the logic underlying Algorithm 1. The initial model parameter W 0 is started randomly as U 0 I d V T 0 , with the orthonormal matrices U 0 and V 0 obtained from SVD of a randomly generated real matrix A of size  X  n  X  n . Throughout experiments, the following parameters are fixed:  X  is computed as ( X  n + n )  X  k ,  X  0 =  X  4 , k max = 300,  X  0 = 0 . 1, and = 1 e Kernelization It is straightforward to work with the kernelized input instances, by considering implicit feature maps  X  : x 7 X   X  ( x ) for x , and likewise  X   X  for  X  x . The kernelized version of  X  x T Wx becomes  X  K 1 2 with  X  K ( i,j ) =  X   X  ( X  x i ) T  X   X  ( X  x j ), K ( i,j ) =  X  ( x and L being a t  X  t similarity matrix. The correspond-ing kernelized optimization problem is thus written as Where  X   X , X  denote the selector functions, and for the modification, the proposed Algorithm 1 can also be applied to learn the similarity matrix L . Denote  X  and X the column-stacked LHS and RHS examples during training, respectively. At test run, a new pair of examples  X  x,x is observed, and its corresponding score can be computed as Algorithm 1 Riemannian Similarity Learning: RSL Input: The set of triplets ( X  x,x,y ) Output: W  X  W k
Init: , W 0 , k max ,  X  0 ,  X ,  X  0  X  (0 ,  X ), and k  X  0 repeat until Grad W J 2 k ( X,x ) :=  X  ( X ) T  X  ( x ) a t  X  1 column vector, respec-tively.
 Generalization Analysis One may concern the generalization ability of the proposed algorithm on un-seen examples. Bounds on the generalization error is related to the capacity of the function class, which can be captured by its VC-dimension for 0-1 loss as well as its pseudo-dimension (Mohri et al., 2012) general-ization for other loss functions. Denote e := exp (1). It has been shown in (Srebro et al., 2004) that  X  d , the VC-dimension / pseudo-dimension of our matrix W , is upper bounded by: Note that the VC-dimension of our rank-d matrix is much smaller than the VC-dimension of a full rank matrix which is roughly  X  n by n . This is key in our scenario of learning a similarity function with satisfac-tory prediction performance. Following this guidance, in practice we usually choose a relative small d , e.g. d = 10.
 Convergence Analysis Trust region methods in Euclidean space R n are well-known to converge to local fixed points under mild conditions (see e.g. (Nocedal &amp; Wright, 2006)). It is also known (Yang, 2007; Absil et al., 2008) that on matrix manifolds they enjoy simi-lar convergence properties as their analogs in Euclidian space. Similarly it can be shown that our RSL algo-rithm is guaranteed to decrease the objective function value monotonically as iteration increases, thus con-verges to local fixed points. Moreover, it has also been proved by Theorem 7.4.11 of (Absil et al., 2008) that Riemannian trust region methods such as our RSL al-gorithm have a super-linear convergence rate.
 Computational Analysis At each iteration, the main computational load is from computing the Eu-clidean gradient grad W J , its projection onto the man-ifold tangent space Grad W J , and the retraction step R
W . It takes up to t  X  nn flops to compute the Eu-clidean gradient for our loss functions. Since d min {  X  n,n } , the computational cost of computing the manifold gradient is up to 4( X  n + n ) d 2 , and up to 14( X  n + n ) d 2 for retraction. The overall complexity is O 18  X  n + n d 2 + t  X  nn , with a mild constant when e.g. the maximum number of iterations k max is considered. We first examine the proposed RSL algorithm on syn-thetic datasets. This is followed by evaluations on two real-life applications: the ASLAN challenge (Kliper-gross et al., 2012) of pairwise similarity prediction for action recognition, and domain adaptation for object recognition (Saenko et al., 2010). The following pa-rameter values are used: The trade-off parameter  X  is fixed to 1; Except for the synthetic experiments, the subspace dimension d is set to 10.
 Synthetic Experiments Experiments are con-ducted on synthetic datasets, in order to analyze the characteristics of the RSL algorithm under con-trolled settings. As illustrated in both plots of in Fig 1, We focus on the accuracy measure as a function over increased difficulty levels, ranging from being lin-early separable to highly inseparable. The synthetic datasets are constructed for binary classes, with each class being sampled from a Gaussian distribution with fixed covariance matrix. For the linearly separable cases, all instances that would be misclassified by the Bayes optimal classifier are dropped off. The difficulty level is increased as we decrease the ratio of between-class dispersions and within-class dispersions (similar to what has been used in Linear Discriminant Anal-ysis (Bishop, 2006)). In practice, this is achieved by shortening the distance of two Gaussian means while keeping the rest unchanged. Unless stated explicitly, by default the following set-up is adopted: The number of training examples is t = 5 , 000 with a half for each class, the LHS feature vector X  X  dimension is  X  n = 50 and for RHS n = 100, the subspace dimension (i.e. rank of W ) is set to d = 5. Each accuracy value is obtained by averaging over 20 repeats.
 The first experiment is carried out by varying the RHS dimension n while fixing other factors. As displayed in Fig 1 top panel, there is no clear trend of perfor-mance changes, although there are slight variations of accuracies w.r.t. different n . This supports our hy-pothesis that the influence of the two dimensions ( X  n and n ) being the same or not is insignificant. To examine the influence of different subspace dimen-sions d (i.e. rank of W ) toward the overall perfor-mance, we run a second experiment by varying d val-ues, which is presented in Fig 1 bottom panel. The empirical results suggest that the model matrix W here resides in a low-dimensional subspace. Moreover, except for the extreme cases where d equals to 1 for taking a too small (or possibly too large) values, the overall performance seems otherwise rather close for a rather broad set of d values between 5 and 20. In other words, the performance is relatively stable over a rather broad range of matrix ranks for W . As a result, throughout the remaining experiments of this paper, we consider a fixed low rank of d = 10 for W .
 Pairwise Action Recognition The ASLAN Chal-lenge (Kliper-gross et al., 2012) is a recent ac-tion recognition benchmark. It contains 3,697 hu-man action clips collected from YouTube, spanning over 432 unique action categories that is organized around a tree hierarchy from CMU Motion Capture Dataset (cmu). The sample distribution over cate-gories is highly uneven, with 116 classes possessing only one video clip. Instead of explicitly performing a multiclass classification, the aim of the benchmark is to make  X  X ame X  or  X  X ot-same X  binary prediction each time on a pair of actions.
 Following (Kliper-gross et al., 2012), from each video local salient points are obtained (Wang et al., 2009a), where each point possesses three types of descriptors: Histogram of Oriented Gradients (HoG), Histogram of Optical Flow (HoF), and a concatenation of both (referred to as HnF). k-means clustering method is then used to produce a codebook of c =5,000 code-words from the training data, which naturally gives rise to a bag-of-words (BoW) representation (Fei-Fei &amp; Perona, 2005). An action video is thus collectively represented as a histogram vector of length c . We work with the view-2 benchmark (the major one for perfor-mance evaluation), where the categories are split into 10 disjoint subsets, to be used for 10-fold cross vali-dation. For each subset, 300 same and 300 not-same pair of action videos are randomly selected. Together it forms a benchmark of 10-splits that each contains mutually exclusive action labels: If videos of a cer-tain action appear in one split, no videos of that same action will appear in any other split.
 Under the same protocol of (Kliper-gross et al., 2012), results of the proposed algorithm are produced and are compared with the state-of-the-art methods, which in-clude the 13 methods reported in (Kliper-gross et al., 2012). To save space, for each experiment, only the best of the 13 results are reported here and is col-lectively referred to as  X  best of (Kliper-gross et al., 2012) X . The detailed performance of these methods can be found in (Kliper-gross et al., 2012). The best result of (Kliper-Gross et al., 2011) on one-short sim-ilarity metric, using CSML initialization and cosine similarity score (Kliper-Gross et al., 2011), is also in-cluded and referred to as  X  OSS-CS  X .
 The results of competing algorithms are shown in Ta-ble 1 on the basic ASLAN benchmark as well as in Table 2 on the ASLAN-KTH benchmark (Kliper-gross et al., 2012). Overall RSL outperforms the comparison methods over these different spatial-temporal features. Moreover, RSL is able to work reasonably well in sce-narios where the LHS and RHS features are from dif-ferent sources. This is in contrast with existing meth-ods of this benchmark, which are unable to cope with such a situation. In particular, on the basic ASLAN benchmark, RSL produces 58 . 43% for HoG  X  HoF, and 60 . 19% for HoG  X  HnF, respectively.
 local features best of RSL HoG  X  HoG 82.78 84.21 HoF  X  HoF 85.44 88.01 HnF  X  HnF 90.00 90.44 Cross-domain Object Recognition We further evaluate on the cross-domain object recognition ap-plication, using the benchmark dataset of Berke-ley31 (Saenko et al., 2010; Kulis et al., 2011). This Berkeley31 dataset contains images taken from three different image domains, with each having 31 object categories. The first domain consists of product im-ages downloaded from Amazon, which are in a canoni-cal pose and with a white background. The second do-main comprises images taken with a digital SLR cam-era in office. They are high-resolution images with varying poses and backgrounds. The third domain includes low-resolution webcam images with varying poses and backgrounds. These three domains are also referred to as amazon , dslr , and webcam , respectively. A summary of the dataset is presented in Table 3. Our protocol follows that of (Kulis et al., 2011). Specifi-cally, all images are resized to the same size (300  X  300) and converted to gray-scale, and are extracted to form SURF feature points. Each image is then rep-resented in BoW: 800 codewords are used for webcam and amazon domains, and 600 for dslr domain.
 The methods are evaluated on two experimental set-tings: (1) All 31 categories are available in both train-ing and testing phases; (2) During training only the first 15 categories are available. The trained model is then evaluated in testing phase on the rest 16 cate-gories. For training in both settings, 20 images per category from source domain and 3 images per cate-gory from target domain are randomly selected, while the rest is retained for evaluation. Since the number of training instances is much lower than the feature di-mension, the competing methods considered here are applied with kernels. Here RBF kernel is used for all methods. The final results presented are averaged over 10 rounds of randomly sampled training examples.
Source SVM-t Symm ARC-t HFA RSL webcam 49.1 53.2 53 . 0 54.3 54.6 amazon 49.1 50.1 53.2 55.4 55.3 webcam 37.7 48 . 3 51.8 A list of competing methods is provided below. In particular, three state-of-the-art methods, namely Symm (Saenko et al., 2010), ARC-t (Kulis et al., 2011), and HFA (Duan et al., 2012), are considered: SVM-t A standard SVM model is trained only with Symm (Saenko et al., 2010) Labels from both do-ARC-t (Kulis et al., 2011) Labels from both domains HFA (Duan et al., 2012) Labels from both domains The average classification accuracies are reported in rows 2-3 of Table 4 for setting (1), and the bottom row for setting (2). Here 2 cross-domain tasks are con-sidered: from webcam or amazon as  X  X ource domain X  to dslr as the default  X  X arget domain X . In both set-tings, our RSL method performs competitively com-paring with the state-of-the-art methods. In particu-lar, RSL outperforms the ARC-t method, which is a dedicated method (Kulis et al., 2011) for addressing the object recognition problem using this particular dataset, and is closely related to our approach. The competitive performance can be largely attributed to the manifold-based matrix factorization approach we have adopted. In this paper, we consider a similarity-score based paradigm that can be used to address scenarios where either the class labels are only partially available in training, or the training and testing data are drawn from heterogeneous sources. Our aim in this context becomes that of learning a rectangular similarity ma-trix of a fixed rank. We formulate the problem as manifold-based optimization, propose a trust-region type algorithm, and apply to applications in recogniz-ing visual objects and actions. Empirical performance on both applications suggest the competitiveness of the proposed approach.
 For future work, we plan to develop and analyze im-proved algorithms, as well as investigate their applica-tions into related problems such as bilinear learning. Acknowledgements This research was partially supported by A*STAR JCO grants (1231BEG034, 1231BFG040, 12302FG010, and 1231BFG047). We would like to thank Nengli Lim, Sinno Jialin Pan, and Hwee Kuan Lee for helpful discussions.

