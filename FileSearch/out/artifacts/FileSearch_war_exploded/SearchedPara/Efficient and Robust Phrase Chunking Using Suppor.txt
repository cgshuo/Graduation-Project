 With the exponential growth of textual information available in the World Wide Web, there has been many research efforts addressed on effectly discover important knowl-mate goal of text chunking is to identify the non-recursive, non-overlap phrase struc-tures in natural language text which is also the intermediate step between part-of-speech tagging (POS) and full parsing. Text chunking does not only support knowl-edge-based systems to extract syntactic relatio ns, but also provide an important knowledge representation fundamental. For example, Sagae (Sagae et al., 2005) adopted a machine learning-based evaluating system to measure the numerical score for grammatical complexity to children instead of human judgments. In addition, chunk information is also the key technology in natural language processing areas, e.g., chunking-based full parsing (Tjong Kim Sang, 2002), clause identification (Car-reras et al., 2005), and machine translation (Watanabe et al., 2003). To effect acquire and represent knowledge from text, a high-performance phrase chunking system is indispensable. To handle huge text dataset, the efficiency of a chunking system should be taken into account. 
Over the past few years, many researches have been addressed on developing a high-performance chunking system and have applied various machine learning meth-ods to it (Ando and Zhang, 2005; Carreras and Marquez, 2003; Kudoh and Matsu-moto, 2001; Molina and Pla, 2002). CoNLL-2000 provided a competition on arbitrary phrase chunking where all systems should be evaluated on the same benchmark cor-many high performance phrase chunking systems had been proposed, such as SVM-based (Kudoh and Matsumoto, 2001; Wu et al, 2006). SVM-based algorithms showed an excellent performance in terms of accuracy. However, its inefficiency in actual analysis limits practical purpose, such as information retrieval and question answer-ing. For example, a linear kernel SVM-bas ed phrase chunker (Wu et al., 2006) runs at language model-based approaches (Molina and Pla, 2002) can process 40000-50000 essing where fast chunking of large quantities of text is indispensable. 
In this paper, we propose two methods that make the SVM-based chunking model substantially more efficient in terms of training and testing. These methods are appli-ging and named entity recognition (NER) that do not conflict to the selected kernels. The proposed two fast classification algorithms are designed based on the word-classification model (Kudoh and Matsumoto, 2001; Ando and Zhang, 2005; Zhang et al., 2002). One is C-OVA (Constraint One-Versus-All), which is an extension of traditional one-versus-all classification scheme. The other is HC-OVA (Hierarchical Constraint One-Versus-All) where the classification process can be viewed as visiting a skewed binary tree. By applying HC-OVA, both training and testing times are largely reduced. Both theoretical and experimental results show that HC-OVA and C-OVA outperform conventional methods for testing (247% and 152%) and substan-our method can handle 13000 tokens per second while the performance is kept in 94.09 in F rate. Ramshaw and Marcus (1995) firstly proposed an inside/outside label style to repre-sent noun phrase chunks. This method involves in three main tags, B, I, and O. I tag beginning of a chunk which immediately follows another chunk, O tag means the Tjong Kim Sang [12] derived the other th ree alternative versions, IOB2, IOE1, and IOE2. 
IOB2: is different from IOB1, which uses the B tag to mark every beginning token 
IOE1: An E tag is denoted as the ending token of a chunk which is immediately be-IOE2: The E tag is given for every token that is the end of a chunk. 
Let us illustrate the four representation styles with an example, considering an in-complete sentence  X  X n early trading in busy Hong Kong Monday X . The four represen-tation styles of the sentence are listed in Table 1. This example only encodes the noun beginning of a verb phrase (VP) in the IOB2 style. 2.1 Phrase Chunking Model In general, the contextual information is often used as the basic feature type; the other features can then be derived based on the surrounding words, such as words and their POS tags. The chunk tag of the current token is mainly determined by the context information. Similar to previous researches (Gimenez and Marquez, 2003; Kudo and Matsumoto, 2001), we employ a classification algorithm (i.e. SVM) learns to classify adopt the following feature types.  X  Lexical information (Unigram/Bigram)  X  POS tag information (UnPOS/BiPOS/TriPOS)  X  Affix (2~4 suffix and prefix letters)  X  Previous chunk information (UniChunk/ BiChunk)  X  Orthographic feature type (See (Wu et al., 2006))  X  Possible chunk classes (See (Wu et al., 2006))  X  Word + POS Bigram (current token + next token X  X  POS tag) 
In addition, the chunking directions can be reversed from left to right into right to directions, i.e. the class of the current token is determined after chunking all preced-from the last token of the sentence to the first token. We name the original chunking process as forward chunking , while the reverse process as backward chunking . 
In this paper, we employ SVM light (Joachims, 1998) as the classification algorithm, which has been shown to perform well on classification problems (Gimenez and Marquez, 2003; Kudoh and Matsumoto, 2001). Since the SVM algorithm is a binary classifier, we have to convert it into multiple binary problems. Here we use the One-Versus-One (OVO) type to solve the problem. As discussed in (Gimenez and Marquez, 2003; Wu et al., 2006), working on linear kernel is much more efficient than polynomial kernels. To take the time efficiency into account, we choose the linear kernel type. 2.2 Preliminary Chunking Results To fairly compare with previous studies, we use the chunking data set from CoNLL-2000 shared task. This corpus is derived from the English treebank Wall Street Jour-for each token is mainly produced by Brill-tagger (Brill, 1995) as consistent with the CoNLL-2000 shared task. In this task, there are 11*2+1 chunk classes (11 phrase shows the experimental results of the four representation styles with forward (F) and backward (B) chunking directions. 
As shown in Table 2, the best system performance is obtained by applying IOE2 with backward chunking (94.25 in F rate) in the CoNLL-2000 chunking task. Com-pared to previous best results (94.12 reported by (Wu et al., 2006), we further im-prove the chunking accuracy by combining word+POS bigram feature and IOE2-B 5200 tokens per second. Although the better system performance is reached, the cost produced by SVM, i.e., deterministic chunking. This simplified version largely in-creases the testing time efficiency (from 1200 to 5200 tokens per second). 
Similar results were obtained by porting our chunking system to Chinese base-representation styles with combining forward and backward chunking directions. The best performance is obtained by employing the IOB2-F. Testing corpus of the Chinese base-chunking task was derived from the Chinese Treebank, 0.27 million words for training while 0.08 million words for testing. In this task, the training time is 16 hours while the chunking speed is 4700 tokens per second. 3.1 Hierarchical Constraint-One-Versus-All For OVA, the chunk class of each token is mainly determined by comparing with all existing categories. The linear kernel SVM compares each example once for each category. But when the number of category scales to high, the comparison times also increase. In the CoNLL-2000 chunking dataset, there are 11 phrase types that produce 11*2+1=23 chunk classes, while in the Chinese base-chunking task, 23 phrase types contribute to 23*2+1=47 chunk classes. The situation is even worse for other non-(one-versus-one) multiclass strategy that involves in |C|*(|C|-1)/2 comparisons where |C| is the number of chunk classes. Although the original chunking system is some-what fast (by applying linear kernel SVM), when the phrase type tends to be hun-dreds, the large number of categories considerably decrease the chunking efficiency. For instance, in the Chinese base-chunking task, the chunking speed decreases to 4700 compared to the CoNLL-2000 shared task where the number of chunk classes increase from 23 to 47. 
The main reason is given rise to the more number of chunk classes that increase several times larger for SVM classifying. In chunking task, the chunk tag is encoded as IOB styles as described in Section 2.1. About half of the categories are unnecessar-ily compared. These comparisons are useless to determine the chunk class of current token. For example, when previous token was chunked as B-NP, the next token is impossible to be the I-VP, I-PP,...etc. 
To find the relationships between chunk classes, a consistent matrix is used, which the consistent matrix of noun phrase (NP) and verb phrase (VP) types using IOB2. The first column indicates the chunk class of previous token while the first row gives possible chunk classes of current token. Intuitively, if previous chunk tag was classi-fied as B-NP, the current chunk class should belong to one of the four chunk classes, B-NP/B-VP/I-NP/O, whereas I-VP is invalid. We then generalize this relation to the other three IOB styles, IOB1, IOE1, and IOE2. Here, we define all chunk classes should belong to one of the following two groups. Begin-group: start of a chunk, for example, the B-tag for IOB2, I-tag for IOE2. Interior-group: inside of a chunk, fo r example, I-tag for IOB2, and E-tag for IOE2. the consistent matrix by connecting the validity between the two groups. That is, whether previous chunk class is Begin or Interior-group, the chunk class of current token potentially belongs to any member of the Begin-group or a specific chunk class of the Interior-group in which the phrase type is equivalent to previous. In other words, from Begin/Interior-groups to Begin-group is valid, while from Begin/Interior-groups to Interior-group is valid only when they share the same chunk. and backward chunking directions, manual-constructing this matrix requires lots of human effort especially to the numerous chunk types, like Chinese. To overcome this, we propose an algorithm (see algorithm 1) to build the consistent matrix automatically. 
As outlined in algorithm 1, the first two steps aim to initialize the consistent matrix by scanning the previous-current chunk class pairs and extracting the IOB tag from training data. In the third step, we check the consistency of a chunk class pairs of the same IOB-tag in the initial consistent matrix. If the two chunk classes are valid, then the IOB tag belongs to Begin-group. Since for Interior-group, the validity only exists when previous and current chunk share the same phrase type. Once the Begin and Interior groups were determined, the final consistent matrix could be generated by connecting the relationships between the two types. It is worth to note that we ignore the O-tag here since it should be the Begin-group clearly. which are set to Interior-group initially. For B tag, we find that the validity occurs in tag) in the initial consistent matrix. Thus, B tag is assigned as Begin-group. For I tag, we conclude that the validity only exists when previous and current chunk classes belong to the same phrase type, for example, NP and VP. Therefore I-tag is still in the Interior-group. After classifying Begin/Interior-groups for each IOB tag, the final consistent matrix is reconstructed by bridging the relationship between the two types. That is, we set the column of each Begin-group as valid, and the validity of the Inte-this step, we can automatically construct the consistent matrix as Table 3. Even the consistent matrix method can reduce half of the comparison times for SVM. However when the number of chunk types is large, comparing with all the categories is time-consuming. Analytically, in the chunking task, we observe that and PP cover 90% of the training data. In Chinese base-chunking, the proportion of NP in the training set is 57% and the three phrase types NP, ADVP and VP denomi-nate 83%. In most cases, the chunk class of each token should be one of those high-frequent phrase types. Therefore, we design a new multi-class strategy for SVM to restrict the comparison of high-frequent chunk classes at higher priority. The main spirit of this method is to reduce unnecessary comparisons through comparing the tree (see Fig. 2.). 
Every internal node in Figure 2 represents a decision of one class and the remain-the chunk class observed, the higher level it is in the tree. Determining the chunk class of a token is equivalent to visit the skewed binary tree. Once the leaf node is visited in the higher level, the remaining nodes will no longer be compared. Therefore, most rare chunk classes are not compared in testing phase. Combining with the consistent matrix, some of the internal nodes can be ignored by checking the validity. The over-all training and testing algorithm is described as follows. 
As outlined in algorithm 2, we construct an ordering list via estimating the fre-quency in the training data. For example, in Figure 1, the item of Or[0] is I-NP. These chunk classes are trained by following this order (the third step). Note that iteratively, we discard a subset of training example th at belongs to previous chunk class. On the other hand, the testing algorithm (algorithm 3) is to compare each support vector comparisons only when previous-current chunk class pair is valid in the matrix. 3.2 Constraint-One-Versus-All The main idea of constraint OVA method is to reduce half of the comparison times whole Begin-group and one from Interior-group. As described in Section 3.1, the Interior-group should follow the Begin-group with the same phrase type. For the pre-vious example, three chunk classes belongs to Begin-group, B-NP, B-VP, and O, while the I-NP and I-VP was assigned to Interior-group via algorithm 1. In testing phase, for the first token, we only focus on classifying with all of the chunk classes in the Begin-group, i.e. B-NP, B-VP, O. If the first token is classified as B-NP, then the second token should not be the other chunk classes in Interior-group other than I-NP. Therefore the B-NP, B-VP, O and I-NP is used. 
Based on this constraint, the training examples of the Interior-group could be fur-ther reduced. It is impossible to compare two chunk classes of the Interior-group simultaneously. On the contrary, the Begin-group is invited to be compared with a specified chunk class of Interior-group. Considerably, we can not reduce any training example for Begin-group. In overall, training the chunk class of the Begin-group is equivalent to classic one-versus-all method, while training the Interior-group, only one chunk class is used to against to the whole Begin-group. 3.3 Time Complexity Analysis The training time complexity of single SVM is ranged from O( m 2 ) to O( m 3 ) (assume which we train | C | single SVM for each category. Testing time for OVA involves in comparing with all categories, i.e. | C | times denoted by O(| C |). 
Different from OVA, the OVO (One-Versus-One) constructs ) ( | | 2 C SVMs for arbi-trary two class pairs. When training data is balanced, i.e. for each category there are m /| C | examples, the training time complexity is time complexity is O((| C |-1) m 2 ). 
In 2000, Plat [10] had proved that the DAG (Directed acyclic graph)-based multi-class SVM had the same training time complexity as OVO method, while testing time complexity was equivalent to OVA by assuming training single SVM is O( m 2 ). The main idea of DAG is based on the results of the OVO where testing phase is similar to visit the acyclic graph. But the selecting of the classification order is heuristically. On the other hand, we analyze the time complexity of the proposed two methods, COVA, and HC-OVA. For HC-OVA, the training time complexity is only O( m 2 ) when data is familiar to one class. In the balanced case, each category contains m /| C | examples, we have, 
As described in Section 3.1, at each time, we use single SVM learns to classify one from the whole set. The above equation can be simplified as follows. 
For testing time complexity, in the best case HC-OVA only compares once for test-ing, i.e., O(1), while in the worst case, it should compare all of the chunk classes that belong to Begin-group and a class from Interior-group, i.e. O(B+1) where B is the number of chunk classes in the Begin-group. On the other hand, the training time complexity of C-OVA in the balanced case is: Begin-group: O(B m 2 ) 
Interior-group: )
Overall: ) )
In the unbalanced case, when dataset biased to the Begin-group, the training time complexity is O(B m 2 ). On the contrary, if the dataset biased to a chunk class of Inte-rior-group, the overall training time is O(B m 2 + m 2 ). For the testing time, C-OVA com-pares the whole Begin-group and one chunk class from Interior-group, thus, the time complexity is precisely O(B+1) which is the same as the worst case of HC-OVA. In this paper, we focus on the chunking task which is not usually a balanced case. VP. In the biased situation, the proposed HC-OVA and C-OVA is more efficient than the other methods since | C | &gt; B (usually B=| C |/2). Although the training times of our methods are worse than DAG and OVO in the balanced case, our method provide more efficient theoretical testing speed for fast text analysis. In the unbalanced case, we had shown the proposed methods more efficient than the other approaches. In this section we concern the actual chunking performance of the two methods. We combine the two methods with the original chunking model (as discussed in Section 2) and applied them to the same chunking tasks. Table 5 lists the actual results of the CoNLL-2000 shared task, and the experimental results of the Chinese base-chunking task is given in Table 6. In the CoNLL-2000 shared task, the actual training time of HC-OVA/C-OVA are 1.85 and 4 hours which outperforms the conventional OVA method (4.5 hours). Besides, the testing speeds of HC-OVA/C-OVA are 12902 and 7915 tokens per second that are 247% and 152% substantially Chinese base-chunking task, the testing times were 219% and 126% faster than OVA. In average, for each token, the HC-OVA gives rise to compare 2.34 times to deter-mine the chunk class, while C-OVA and OVA are 10.92, and 22 respectively in the CoNLL-2000 shared task. In Chinese base-chunking task, the three methods cost 2.80, 14.20 and 40 times for comparing in average. 
As shown in Table 5, although the HC-OVA is very efficient in terms of testing, it decreases the chunking accuracy. However, the decreasing rate is somewhat marginal (from 94.25 to 94.09). Compared to previous studies, our method is not only efficient but also accurate. In this comparison, we only focus on pure training with the CoNLL-2000 provided dataset, while the use of external knowledge, like parsers, or additional corpus is not compared. It is no conflict to employ these methods to further improve the performance. Zhang [16] had shown that the use of external parsers could enhance their chunker from 93.57 to 94.17. In his later research [1], he also incorpo-rated with unlabeled data to boost the learner which increased the accuracy from 93.60 to 94.39. 
We should truly report a very efficient method based on hidden markov model [8] which can handle 45000~50000 tokens in one second and only spend few seconds for training. But this method is not accurate. Molina made use of one million words train-ing data which is four times larger than us to obtain the improved performance where the F rate is enhanced from 92.19 to 93.25. In this paper, we present two methods to speed up training and testing for phrase chunking. Experimental results show that our method can handle 13000 tokens per second which kept the accuracy in 94.09 in the CoNLL-2000 shared task. The online (http://dblab87.csie.ncu.edu.tw/bcbb/fast_chunking.htm). 
