 Department of Computer Science, Columbia University Microsoft Research New England, 1 Memorial Drive, Cambridge, MA 02446 Many standard methods for estimation and statistical learn-ing are designed for optimal behavior in expectation, yet they may be suboptimal for high-probability guarantees. For instance, the population mean of a random variable can be estimated by the empirical mean, which is minimax-optimal with respect to the expected squared error. How-ever, the deviations of this estimator from the true mean may be large with constant probability unless higher-order moments are controlled in some way, such as a subguas-sianity assumption (Catoni, 2012); similar issues arise in multivariate and high-dimensional estimation problems, such as linear regression and convex loss minimization. In many practical applications, distributions are heavy-tailed and thus are not subgaussian X  X hey may not even have fi-nite high-order moments. Thus, standard techniques such as empirical averages may be inappropriate, in spite of their optimality guarantees under restrictive assumptions. A case in point is the classical problem of linear regres-sion, where the goal is to estimate a linear function of a random vector X (the covariate) that predicts the response (label) Y with low mean squared error. The common ap-proach for this problem is to use ordinary least squares or ridge regression, which minimize the loss on a finite la-beled sample (with regularization in the case of ridge re-gression). The analyses of Srebro et al. (2010) and Hsu et al. (2012) for these estimators give sharp rates of conver-gence of the mean squared error of the resulting predictor to the optimal attainable loss, but only under assumptions of boundedness. Audibert &amp; Catoni also analyze these es-timators using PAC-Bayesian techniques, and manage to remove the boundedness assumptions, but they only pro-vide asymptotic guarantees or guarantees which hold only if n  X   X (1 / X  ) . The failure of these estimators for gen-eral unbounded distributions may not be surprising given their inherent non-robustness to heavy-tailed distributions as discussed later in this work.
 To overcome the issues raised above, we propose simple and computationally efficient estimators for linear regres-sion and other convex loss minimization problems. The es-timators have near-optimal approximation guarantees, even when the data distributions are heavy-tailed. Our esti-mator for the linear regression of a response Y on a d -dimensional covariate vector X converges to the optimal loss at an optimal rate with high probability, with only an assumption of bounded constant-order moments for X and Y (see Theorem 1). For comparison, the only previous re-sult with a comparable guarantee is based on an estima-tor which requires prior knowledge about the response dis-tribution and which is not known to be computationally tractable (Audibert &amp; Catoni, 2011). Furthermore, in the case where X is bounded and well-conditioned (but the distribution of Y may still be heavy-tailed), our estima-tor achieves, with probability  X  1  X   X  , a multiplicative constant approximation of the optimal squared loss, with a sample size of n  X  O ( d log( d )  X  log(1 / X  )) (see Theo-rem 2). This improves on the previous work of Mahdavi &amp; Jin (2013), whose estimator, based on stochastic gradi-ent descent, requires under the same conditions a sample size of n  X  O ( d 5 log(1 / (  X L sq ? ))) , where L sq ? is the opti-mal squared loss. We also prove an approximation guar-antee in the case where X has a bounded distribution in an infinite-dimensional Hilbert space, as well as general results for other loss minimization problems with smooth and strongly-convex losses.
 Our estimation technique is a new generalization of the median-of-means estimator used by Alon et al. (1999) and many others (see, for instance, Nemirovsky &amp; Yudin, 1983, p. 243). The basic idea is to repeat an estimate several times by splitting the sample into several groups, and then se-lecting a single estimator out of the resulting list of candi-dates with an appropriate criterion. If an estimator from one group is good with better-than-fair chance, then the selected estimator will be good with probability exponen-tially close to one. Our generalization provides a new sim-ple selection criterion which yields the aforementioned im-proved guarantees. We believe that our new generalization of this basic technique will be applicable to many other problems with heavy-tailed distributions. Indeed, the full version of this paper (Hsu &amp; Sabato, 2013) reports addi-tional applications to sparse linear regression and low-rank matrix approximation. In an independent work, Minsker (2013) considers other variations of the original median-of-means estimator.
 We begin by stating and discussing the main results for lin-ear regression in Section 2. We then explain the core tech-nique in Section 3. The application of the technique for smooth and convex losses is analyzed in Section 4. Sec-tion 5 provides the derivations of our main results for re-gression. In this section we state our main results for linear re-gression, which are specializations of more general re-sults given in Section 4. Unlike standard high-probability bounds for regression, the bounds below make no assump-tion on the range or the tails of the response distribution other than a trivial requirement that the optimal squared loss be finite. We give different bounds depending on con-ditions on the covariate distributions.
 Let [ n ] := { 1 , 2 ,...,n } for any natural number n  X  Let Z be a data space, X a parameter space, D a distri-bution over Z , and Z a Z -valued random variable with distribution D . Let ` : Z  X  X  X  R + be a non-negative loss function, and for w  X  X , let L ( w ) := E ( ` ( Z, w )) be the expected loss. Also define the empirical loss with respect to a finite sample T  X  Z (where T is a multiset), L
T ( w ) := | T |  X  1 P z  X  T ` ( z, w ) . Let Id be the identity op-erator on X , and L ? := min w L ( w ) . Set w ? such that L ? = L ( w ? ) .
 For regression, we assume the parameter space X is a Hilbert space with inner product  X  X  ,  X  X  X , and Z := R . The loss is the squared loss ` = ` sq , defined as ` (( x ,y ) , w ) := 1 2 ( x &gt; w  X  y ) 2 . The regularized squared loss, for  X   X  0 , is `  X  (( x ,y ) , w ) := 1 2 (  X  x , w  X   X   X  w , w  X  X ; note that ` 0 = ` sq . We analogously define L Let X  X  X be a random vector drawn according to the marginal of D on X , and let  X  : X  X  X be the second-moment operator a 7 X  E ( X  X  X , a  X  X ) . For a finite-dimensional X ,  X  is simply the (uncen-tered) covariance matrix E [ XX &gt; ] . For a sample T := { X 1 , X 2 ,..., X m } of m independent copies of X , de-note by  X  T : X  X  X the empirical second-moment opera-The proposed algorithm for regression (Algorithm 1) is as follows. First, draw k independent random sam-ples i.i.d. from D , and perform linear regression with  X  -regularization on each sample separately, to obtain k linear regressors. Then, use several independent estimations of the covariance matrix  X  from i.i.d. samples to select a sin-gle regressor from the k regressors at hand. The variant in Step 5 may be used to obtain tighter bounds in some cases discussed below.
 Algorithm 1 Regression for heavy-tails input  X   X  0 , sample sizes n,n 0 , confidence  X   X  (0 , 1) . output Approximate predictor  X  w  X  X . 1: Set k := d C ln(1 / X  ) e . 2: Draw k random i.i.d. samples S 1 ,...,S k from D , each 3: For each i  X  [ k ] , let w i  X  argmin w  X  X L  X  S 4: Draw a random i.i.d sample T of size n 0 , and split it to 5: For each i  X  [ k ] , let r i be the median of the values in 7: Return  X  w := w i ? .
 First, consider the finite-dimensional case, where X = R d and assume  X  is not singular. In this case we obtain a guar-antee for ordinary least squares with  X  = 0 . The guarantee holds whenever the empirical estimate of  X  is close to the true  X  in expectation , a mild condition that requires only bounded low-order moments. For concreteness, we assume the following condition. 1 Condition 1 (Srivastava &amp; Vershynin 2013) . There exists c, X  &gt; 0 such that Pr for every orthogonal projection  X  in R d .
 Under this condition, we show the following guarantee for least squares regression.
 Theorem 1. Assume  X  is not singular. If X satisfies Con-dition 1 with parameters c and  X  , then there is a con-stant C = C ( c, X  ) such that Algorithm 1 with  X  = 0 , n  X  Cd log(1 / X  ) , and n 0  X  C log(1 / X  ) , with probability at least 1  X   X  , L sq (  X  w )  X  Define the following finite fourth-moment conditions:  X   X  Under these conditions, E k  X   X  1 / 2 X ( X &gt; w ?  X  Y ) k 2  X   X  2 dL sq ? (via Cauchy-Schwartz); if  X  1 and  X  2 are con-stant, then we obtain the bound with probability  X  1  X   X  . In comparison, the recent work of Audibert &amp; Catoni (2011) proposes an estimator for linear regression based on optimization of a robust loss function (see also Catoni, 2012) which achieves essentially the same guarantee as Theorem 1 (with only mild differ-ences in the moment conditions, see the discussion fol-lowing their Theorem 3.1). However, that estimator de-pends on prior knowledge about the response distribution, and removing this dependency using Lepski X  X  adaptation method (Lepski, 1991) may result in a suboptimal conver-gence rate. It is also unclear whether that estimator can be computed efficiently. Theorem 1 can be specialized for other specific cases of interest. For instance, suppose X is bounded and well-conditioned in the sense that there exists R &lt;  X  such that Pr[ X &gt;  X   X  1 X  X  R 2 ] = 1 , but Y may still be heavy-tailed (and, here, we do not assume Condition 1). Then, the fol-lowing result can be derived using Algorithm 1, with the variant of Step 5 for slightly tighter guarantees. Theorem 2. Assume  X  is not singular. Let  X  w be the output of the variant of Algorithm 1 with  X  = 0 . With probability at least 1  X   X  , for n  X  O ( R 2 log( R ) log(1 / X  )) and n O ( R 2 log( R/ X  )) , Note that E ( X &gt;  X   X  1 X ) = E tr( X &gt;  X   X  1 X ) = tr(Id) = d , therefore R =  X ( total sample size of O ( d log( d ) log(1 / X  )) suffices to guar-antee a constant factor approximation to the optimal loss. This is minimax optimal up to logarithmic factors (see, e.g. , Nussbaum, 1999). We also remark that the boundedness assumption can be replaced by a subgaussian assumption on X , in which case the sample size requirement becomes O ( d log(1 / X  )) .
 In recent work of Mahdavi &amp; Jin (2013), an algorithm based on stochastic gradient descent obtains multiplica-tive approximations to L ? , for general smooth and strongly convex losses ` , with a sample complexity scaling with log(1 /  X  L ) . Here,  X  L is an upper bound on L ? , which must be known by the algorithm. The specialization of Mahdavi &amp; Jin X  X  main result to square loss implies a sample complexity of  X  O ( dR 8 log(1 / (  X L sq ? )) if L sq ? is known. In comparison, Theorem 2 shows that  X  O ( R 2 log(1 / X  )) suffice when using our estimator.
 It is interesting to note that here we achieve a constant factor approximation to L ? with a sample complexity that does not depend on the value of L ? . This contrasts with other parametric learning settings, such as classification, where constant approximation requires  X (1 /L ? ) samples, and even active learning can only improve the dependence to  X (log(1 /L ? )) (see, e.g. , Balcan et al., 2006). Finally, we also consider the case where X is a general, infinite-dimensional Hilbert space,  X  &gt; 0 , the norm of X is bounded, and Y again may be heavy-tailed.
 Theorem 3. Let V &gt; 0 such that Pr[  X  X , X  X  X  X  V 2 ] = 1 . Let  X  w be the output of the variant of Algo-rithm 1 with  X  &gt; 0 . With probability at least 1  X   X  , as soon as n  X  O (( V 2 / X  ) log( V/ O (( V 2 / X  ) log( V/ (  X  If the optimal unregularized squared loss L sq ? is achieved by  X  w  X  X with  X   X  w ,  X  w  X  X  X  B 2 , the choice  X  =  X ( p L sq ? V 2 log(1 / X  ) / ( B 2 n )) yields that as soon as n  X   X  O ( B 2 V 2 log(1 / X  ) /L sq ? ) and n 0  X  O ( B 2 V 2 log(1 / X  ) /L sq ? ) , L sq (  X  w )  X  L sq + O By this analysis, a constant factor approximation for L sq is achieved with a sample of size  X  O ( B 2 V 2 log(1 / X  ) /L As in the finite-dimensional setting, this rate is known to be optimal up to logarithmic factors (Nussbaum, 1999). In this section we present the core technique from which Algorithm 1 is derived. We first demonstrate the underly-ing principle via the median-of-means estimator, and then explain the generalization to arbitrary metric spaces. 3.1. Warm-up: median-of-means estimator Algorithm 2 Median-of-means estimator input Sample S  X  R of size n , number of groups k  X  N output Population mean estimate  X   X   X  R . 1: Randomly partition S into k groups S 1 ,S 2 ,...,S k , 2: For each i  X  [ k ] , let  X  i  X  R be the sample mean of S 3: Return  X   X  := median {  X  1 , X  2 ,..., X  k } .
 We first motivate our procedure for approximate loss min-imization by considering the special case of estimating a scalar population mean using a median-of-means estimator, given in Algorithm 2. This estimator, heavily used in the streaming algorithm literature (Alon et al., 1999, though a similar technique also appears in the textbook by Ne-mirovsky &amp; Yudin, 1983 as noted by Levin, 2005), par-titions a sample into k equal-size groups, and returns the median of the sample means of each group. The input pa-rameter k is a constant determined by the desired confi-dence level ( i.e. , k = log(1 / X  ) for confidence  X   X  (0 , 1) ). The following result is well known.
 Proposition 1. Let x be a random variable with mean  X  and variance  X  2 &lt;  X  , and let S be a set of n independent copies of x . Assume k divides n . With probability at least 1  X  e  X  k/ 4 . 5 , the estimate  X   X  returned by Algorithm 2 on input ( S,k ) satisfies |  X   X   X   X  | X   X  p 6 k/n .
 Proof. Pick any i  X  [ k ] , and observe that S i is an i.i.d. sam-ple of size n/k . Therefore, by Chebyshev X  X  inequality, Pr[ |  X  i  X   X  |  X  p 6  X  2 k/n ]  X  5 / 6 . For each i  X  [ k ] , let b i := 1 {|  X  i  X   X  | X  p 6  X  2 k/n } . The b i are independent indicator random variables, each with E ( b i )  X  5 / 6 . By Hoeffding X  X  inequality, Pr[ P k i =1 b i &gt; k/ 2]  X  1  X  e In the event { P k i =1 b i &gt; k/ 2 } , at least half of the  X  within p 6  X  2 k/n of  X  , so the same holds for the median of the  X  i .
 Remark 1. It is remarkable that the estimator has O (  X / even though the random variable x may have heavy-tails ( e.g. , no bounded moments beyond the variance). Catoni (2012) also presents mean estimators with these properties and also asymptotically optimal constants , although the es-timators require  X  as a parameter.
 Remark 2. Catoni (2012) shows that the empirical mean cannot provide a qualitatively similar guarantee: for any  X  &gt; 0 and  X   X  (0 , 1 / (2 e )) , there is a distribution with mean zero and variance  X  2 such that the empirical average  X   X  emp of n i.i.d. draws satisfies Therefore the deviation of the empirical mean necessarily scales with 1 /  X (  X  ) ). 3.2. Generalization to arbitrary metric spaces We now consider a generalization of the median-of-means estimator for arbitrary metric spaces, with a metric that can only be crudely estimated. Let X be the parameter (solu-tion) space, w ?  X  X be a distinguished point in X (the target solution), and  X  a metric on X (in fact, a pseudomet-ric suffices). Let B  X  ( w 0 ,r ) := { w  X  X :  X  ( w 0 , w )  X  r } denote the ball of radius r around w 0 .
 The first abstraction captures the generation of candidate solutions obtained from independent subsamples. We as-sume there is an oracle APPROX  X , X  which, upon query-ing, returns a random w  X  X satisfying We assume that the responses of APPROX  X , X  are gener-ated independently. Note that the 2 / 3 could be replaced by another constant larger than half; we have not made any attempt to optimize constants.
 To second abstraction captures the limitations in calculat-ing the metric. We assume there is an oracle DIST  X  which, if queried with any x , y  X  X , returns a random number DIST  X  ( x , y ) satisfying
Pr h  X  ( x , y ) / 2  X  DIST  X  ( x , y )  X  2  X  ( x , y ) i Algorithm 3 Robust approximation with random distances input Number of candidates k , query access to output Approximate solution  X  w  X  X . 1: For each i  X  [ k ] , let w i be the response from querying 2: For each i  X  [ k ] , let r i := median { DIST  X  ( w i , w 3: Return  X  w := w i ? .
 We assume that the responses of DIST  X  are generated in-dependently (and independent of APPROX  X , X  ). Note that the responses need not correspond to a metric. More-over, we will only query DIST  X  for the pairwise dis-tances of k fixed points (the candidate parameters W = to be mutually independent.
 The proposed procedure, given in Algorithm 3, generates k candidate solutions by querying APPROX  X , X  k times, and then selects a single candidate using a randomized gener-alization of the median. Specifically, for each i  X  [ k ] , the radius of smallest ball centered at w i that contains more than half of { w 1 , w 2 ,..., w k } is approximated using calls to DIST  X  ; the w i with the smallest such approximation is returned. Again, the number of candidates k determines the resulting confidence level. The following theorem pro-vides a guarantee for Algorithm 3. The idea of the proof is illustrated in Figure 1. A similar technique was proposed by Nemirovsky &amp; Yudin (1983), however their formulation relies on knowledge of and the metric. Theorem 4. With probability at least 1  X  ( k + 1) e  X  k/ 45 Algorithm 3 returns  X  w  X  X satisfying  X  ( w ? ,  X  w )  X  9  X  . Proof. For each i  X  [ k ] , let b i := 1 {  X  ( w ? , w i Note that the b i are independent indicator random vari-ables, each with E ( b i )  X  2 / 3 . By Hoeffding X  X  inequality, Pr[ P k i =1 b i &gt; 3 k/ 5]  X  1  X  e  X  k/ 45 . Henceforth condition the w i are contained in B  X  ( w ? , X  ) .
 Suppose w i  X  B  X  ( w ? , X  ) , and let y i,j := 1 { DIST  X  ( w i , w j )  X  4  X  } . Observe that for every w j  X  B  X  ( w ? , X  ) ,  X  ( w i , w j )  X  2  X  by the triangle inequality, and thus for such w j , i.e. , E ( y i,j )  X  8 / 9 . Therefore E k/ 2 . By Hoeffding X  X  inequality, Pr[ P k i =1 y i,j  X  k/ 2]  X  e  X  k/ 45 . Thus, with probability at least 1  X  e  X  k/ 45 , r median { DIST  X  ( w i , w j ) : j  X  [ k ] } X  4  X  . Now suppose w i 6 X  B  X  ( w ? , 9  X  ) . Let z i,j := 1 { DIST  X  ( w i , w j ) &gt; 4  X  } . Observe that for every w B  X  ( w ? , X  ) ,  X  ( w i , w j )  X   X  ( w ? , w i )  X   X  ( w ? the triangle inequality, and thus
Pr h DIST  X  ( w i , w j ) &gt; 4  X  i for such w j , i.e. , E ( z i,j )  X  8 / 9 . Therefore, as be-fore E ( P k j =1 z i,j )  X  8 k/ 15 &gt; k/ 2 . By Hoeffding X  X  inequality, with probability at least 1  X  e  X  k/ 45 , r median { DIST  X  ( w i , w j ) : j  X  [ k ] } &gt; 4  X  . Now take a union bound over the up to k events described above (at most one for each w i  X  W ) to conclude that with probability at least 1  X  ( k +1) e  X  k/ 45 , (i) | W  X  B  X  3 k/ 5 &gt; 0 , (ii) r i  X  4  X  for all w i  X  W  X  B  X  ( w and (iii) r i &gt; 4  X  for all w i  X  W \ B  X  ( w ? , 9  X  ) . In this event the w i  X  W with the smallest r i must satisfy w i  X  B  X  ( w ? , 9  X  ) . In this section, we apply our core technique to the prob-lem of approximately minimizing strongly convex losses, which includes least squares linear regression as a special case.
 We employ the definitions for a general loss ` : Z  X  X  X  R + given in Section 2. To simplify the discussion through-out, we assume ` is differentiable, which is anyway our primary case of interest. We assume that L has a unique minimizer w ? := arg min w  X  X L ( w ) . 2 Suppose ( X , k X k ) is a Banach space. Denote by k X k  X  the dual norm, so k y k  X  = sup { X  y , x  X  : x  X  X , k x k  X  1 } for y  X  X  X  . Also, denote by B k X k ( c ,r ) := { x  X  X : k x  X  c k X  r } the ball of radius r  X  0 around c  X  X .
 The derivative of a differentiable function f : X  X  R at x  X  X in direction u  X  X is denoted by  X  X  X  f ( x ) , u  X  . We say f is  X  -strongly convex with respect to k X k if for all x , x 0  X  X ; it is  X  -smooth with respect to k X k if for all x , x 0  X  X We say k X k is  X  -smooth if x 7 X  1 2 k x k 2 is  X  -smooth with respect to k X k .
 Fix a norm k X k on X with a dual norm k X k  X  . The met-ric  X  used by Algorithm 3 is defined by  X  ( w 1 , w k w 1  X  w 2 k . We denote  X  by k X k as well. We imple-ment APPROX k X k , X  based on loss minimization over sub-samples, as follows: Given a sample S  X  Z , randomly partition S into k equal-size groups S 1 ,S 2 ,...,S k , and let the response to the i -th query to APPROX k X k , X  be the loss minimizer on S i , i.e. , arg min w  X  X L S i ( w ) . We call this implementation subsampled empirical loss minimization . We further assume that there exists some sample size n k that allows DIST k X k to be correctly implemented using any i.i.d. sample of size n 0  X  n k . Clearly, if S is an i.i.d. sam-ple from D , and DIST k X k is approximated using a separate sample, then the queries to APPROX k X k , X  are independent from each other and from DIST k X k . Thus, to apply Theo-rem 4, it suffices to show that Eq. (3) holds.
 We assume k X k  X  is  X  -smooth for some  X  &gt; 0 . Let n  X  de-note the smallest sample size such that the following holds: With probability  X  5 / 6 over the choice of an i.i.d. sample T of size | T | X  n  X  from D , for all w  X  X , L
T ( w )  X  L T ( w ? )+  X  X  X  L T ( w ? ) , w  X  w ?  X  + In other words, the sample T induces a loss L T which is  X  -strongly convex around w ? . We assume that n  X  &lt;  X  for some  X  &gt; 0 .
 The following lemma proves that Eq. (3) holds under these assumptions with Lemma 1. Assume k divides n , and that S is an i.i.d. sam-ple from D of size n  X  k  X  n  X  . Then subsampled empirical loss minimization using the sample S is a correct imple-mentation of APPROX k X k , X  for up to k queries.
 Proof. It is clear that w 1 , w 2 ,..., w k are independent by the assumption. Fix some i  X  [ k ] . Observe that  X  L ( w E (  X  ` ( Z, w ? )) = 0 , and therefore, since k X k is  X  -smooth, E k X  L S &amp; Nemirovski, 2008). By Markov X  X  inequality, Moreover, the assumption that n/k  X  n  X  implies that with probability at least 5 / 6 , Eq. (5) holds for T = S i . By a union bound, both of these events hold simultaneously with probability at least 2 / 3 . In the intersection of these events, letting w i := arg min w  X  X L S i ( w ) , (  X / 2) k w i  X  w ? k 2 where the last inequality follows from the definition of the dual norm, and the optimality of w i on L S i . Rearranging and combining with the above probability inequality im-plies Pr[ k w i  X  w ? k X   X  ]  X  2 / 3 .
 Combining Lemma 1 and Theorem 4 gives the following theorem.
 Theorem 5. Assume k := C d log(1 / X  ) e (for some univer-sal constant C &gt; 0 ) divides n , S is an i.i.d. sample from D of size n  X  k  X  n  X  , and S 0 is an i.i.d. sample from D of size n 0  X  n k . Further, assume Algorithm 3 uses the subsampled empirical loss minimization to implement APPROX k X k , X  , where  X  is as in Eq. (6), as well as imple-mentation of DIST k X k using S 0 . Then with probability at least 1  X   X  , the parameter  X  w returned by Algorithm 3 sat-isfies, (for some universal constant C) We give an easy corollary of Theorem 5 for the case where ` is smooth.
 Corollary 1. Assume the same conditions as Theorem 5, and also that: (i) w 7 X  ` ( z, w ) is  X  -smooth with respect to k X k for all z  X  Z , and (ii) w 7 X  L ( w ) is  X   X  -smooth with respect to k X k . Then with probability at least 1  X   X  , (for some universal constant C &gt; 0 ) Proof. Due to the smoothness assumption on ` , k X  ` ( z, w ? ) k 2  X   X  4  X ` ( z, w ? ) for all z  X  Z (Srebro et al., 2010, Lemma 2.1). Thus, E [ k X  ` ( Z, w ? ) k 2  X  ]  X  4  X L ( w The result follows using Theorem 5 and since smoothness of L and the optimality of L ( w ? ) .
 Corollary 1 implies that for smooth losses, Algorithm 3 provides a constant factor approximation to the optimal loss with a sample size max { n  X  , X  X   X   X / X  2 } X  O (log(1 / X  )) (with probability at least 1  X   X  ). In subsequent sections, we exemplify cases where the two arguments of the max are roughly of the same order, and thus imply a sam-ple size requirement of O (  X   X   X  X / X  2 log(1 / X  )) . Note that there is no dependence on the optimal loss L ( w ? ) in the sample size, and the algorithm has no parameters besides k = O (log(1 / X  )) .
 Remark 3. The problem of estimating a scalar population mean is a special case of the loss minimization problem, where Z = X = R , and the loss function of interest is the square loss ` ( z,w ) = ( z  X  w ) 2 . The minimum population loss in this setting is the variance  X  2 of Z , i.e. , L ( w  X  . Moreover, in this setting, we have  X  =  X  =  X   X  = 2 , so the estimate  X  w returned by Algorithm 3 satisfies, with probability at least 1  X   X  , In Remark 2 a result from Catoni (2012) is quoted which implies that if n = o (1 / X  ) , then the empirical mean  X  w emp := arg min w  X  R L S ( w ) = | S |  X  1 P z  X  S z ( i.e. , em-pirical risk (loss) minimization for this problem) incurs loss
L (  X  w emp ) =  X  2 + (  X  w emp  X  w ? ) 2 = (1 +  X  (1)) L ( w with probability at least 2  X  . Therefore empirical risk mini-mization cannot provide a qualitatively similar guarantee as Corollary 1. It is easy to check that minimizing a regular-ized objective also does not work, since any non-trivial reg-ularized objective necessarily provides an estimator with a positive error for some distribution with zero variance. We now show how to apply our analysis for squared loss minimization using an appropriate norm and an upper bound on n  X  . Assume X is a Hilbert space with inner prod-uct  X  X  ,  X  X  X , and that L T is twice-differentiable (which is the case for square loss). By Taylor X  X  theorem, for any w  X  X there exist t  X  [0 , 1] and  X  w = t w ? + (1  X  t ) w such that for any sample T  X  Z . Therefore, to establish a bound on n , it suffices to find a size of T such that for an i.i.d. sam-ple T from D ,
Pr inf For ease of exposition, we start with analysis for the case where Y is allowed to be heavy-tailed, but X is assumed to be light-tailed. The analysis is provided in Section 5.1 and Section 5.2. The analysis for the case where X can also be heavy tailed is provided in Section 5.3.
 Recall that for a sample T := { X 1 , X 2 ,..., X m } of m independent copies of a random vector X  X  X ,  X  T is the empirical second-moment operator based on T . The fol-lowing result bounds the spectral norm deviation of  X  from the population second moment operator  X  under a boundedness assumption on X .
 Lemma 2 (Specialization of Lemma 1 in Oliveira 2010) . Fix any  X   X  0 , and assume  X  X , (  X  +  X  Id)  X  1 X  X  X  X  r 2 almost surely. For any  X   X  (0 , 1) , if m  X  80 r 2  X  ln(4 m then with probability at least 1  X   X  , for all a  X  X , We use the boundedness assumption on X for sake of simplicity; it is possible to remove the boundedness as-sumption, and the logarithmic dependence on the cardinal-ity of T , under different conditions on X ( e.g. , assuming  X   X  1 / 2 X has subgaussian projections, as in Litvak et al. 2005). 5.1. Finite-dimensional ordinary least squares Consider first ordinary least squares in the finite-dimensional case. In this case X = R d and Algorithm 1 can be used with  X  = 0 . It is easy to see that Algorithm 1 is a specialization of Algorithm 3 with subsampled empiri-cal loss minimization when ` = ` sq . We now prove Theo-rem 2. Recall that in this theorem we assume the variant of Algorithm 1, in which step 5 uses the covariance matrix of the entire T sample,  X  T , instead of separate matrices  X  Thus the norm we use in Algorithm 3 is k X k T , defined as k a k T = that always provides the correct distance.
 Proof of Theorem 2. The proof is derived from Corollary 1 as follows. First, it is easy to check that the dual of k X k is 1 -smooth. Let the norm k X k  X  be defined by k a k  X  =  X  a &gt;  X  a . By Lemma 2, if n 0  X  O ( R 2 log( R/ X  )) , with probability at least 1  X   X  , (1 / 2) k a k 2  X   X  k a k 2 T for all a  X  R d . Denote this event E and assume for the rest of the proof that E occurs. Since ` sq is R 2 -smooth with respect to k X k  X  , and L sq is 1 -smooth with respect to k X k  X  , the same holds, up to constant factors, for k X k T Moreover, for any sample S , By Lemma 2 with  X  = 0 , if | S |  X  80 R 2 log(24 | S | then with probability at least 5 / 6 ,  X   X   X  R d \ { 0 } ,  X   X  S  X  /  X  &gt;  X   X   X  1 / 2 . Therefore Eq. (7) holds for k X k with  X  = 1 / 4 and n 1 / 4 = O ( R 2 log R ) . We can thus ap-ply Corollary 1 with  X  = 1 / 4 ,  X  = 4 R 2 ,  X   X  = 4 ,  X  = 1 , and n 1 / 4 = O ( R 2 log R ) , so with probability at least 1  X   X  , the parameter  X  w returned by Algorithm 1 (with the variant) satisfies as soon as n  X  O ( R 2 log( R ) log(1 / X  )) . A union bound over the probability that E also occurs finishes the proof. 5.2. Ridge regression In a general, possibly infinite-dimensional, Hilbert space X , the variant of Algorithm 1 can be used with  X  &gt; 0 . In this case, the algorithm is a specialization of Algorithm 3 with subsampled empirical loss minimization when ` = `  X  , with the norm defined by k a k T, X  = p a &gt; (  X  T +  X  Id) a . Proof of Theorem 3. First, it is easy to check that the dual of k  X  k T, X  is 1 -smooth. As in the proof of Theorem 2, by Lemma 2 if n 0  X  O (( V 2 / X  ) log( V/ (  X  probability 1  X   X  the norm k a k T, X  is equivalent to the norm k X k  X , X  = p a &gt; (  X  +  X  Id) a up to constant factors. More-over, since we assume that Pr[  X  X , X  X  X  X  V 2 ] = 1 , we have  X  x , (  X  +  X I )  X  1 x  X  X  X   X  x , x  X  X / X  for all x  X  Pr[  X  X , (  X  +  X I )  X  1 X  X  X  X  V 2 / X  ] = 1 . Therefore `  X  is (1 + V 2 / X  ) -smooth with respect to k X k  X , X  . In addition, L  X  is 1 -smooth with respect to k X k with r  X  = V/ X  , we have, similarly to the proof of Theo-rem 2, n 1 / 4 = O (( V 2 / X  ) log( V/  X  = 4(1 + V 2 / X  ) ,  X   X  = 4 ,  X  = 1 , and n 1 / 4 as above, to match the actual norm k X k T, X  , we have with probability 1  X   X  ,
L  X  (  X  w )  X  1 + O as soon as n  X  O (( V 2 / X  ) log( V/ We are generally interested in comparing to the minimum square loss L sq ? := inf w  X  X L sq ( w ) , rather than the min-imum regularized square loss inf w  X  X L  X  ( w ) . Assuming the minimizer is achieved by some  X  w  X  X with  X   X  w ,  X  w  X  B 2 , the choice  X  =  X ( p L sq ? V 2 log(1 / X  ) / ( B 2 n )) yields
L sq (  X  w ) +  X   X   X  w ,  X  w  X  X  X  L sq ? + O as soon as n  X   X  O ( B 2 V 2 log(1 / X  ) /L sq ? ) . 5.3. Heavy-tailed covariates In this section we prove Theorem 1. When the regression covariates are not bounded or subgaussian as in the two pre-vious sections, the empirical second-moment matrix may deviate significantly from its population counterpart with non-negligible probability. In this case we use Algorithm 1 with the original step 5 so that for any i  X  [ k ] , the responses For simplicity, we work in finite-dimensional Euclidean space X := R d and consider  X  = 0 . The analysis shows that Algorithm 1 is an instance of subsampled empirical loss minimization for ` sq with the norm k a k  X  = Recall that we assume Condition 1 given in Section 2. The following lemma shows that under this condition, O ( d ) samples suffice so that the expected spectral norm distance between the empirical second-moment matrix and  X  is bounded.
 Lemma 3 (Corollary 1.2 from Srivastava &amp; Vershynin 2013, essentially) . Let X satisfy Condition 1, and let X 1 , X 2 ,..., X n be independent copies of X . Let b  X  :=  X  , such that for any  X  (0 , 1) , if n  X   X   X  2  X  2 / X  d , then E Lemma 3 implies that for the norm k X k  X  , n 1 / 2 = O ( c where c 0  X  =  X   X  2 O (1+1 / X  ) . Therefore, for k = O (log(1 / X  )) , subsampled empirical loss minimization requires n  X  k  X  n 1 / 2 = O ( c 0  X  d log(1 / X  )) samples to correctly implement Step 5 in Algorithm 1 implements DIST k X k every i , { DIST k X k independent samples T j . We now need to show that this implementation satisfies Eq. (4). By Lemma 3, for every i,j  X  [ k ] an i.i.d. sample T j of size O ( c 0  X  ) suffices so that with probability at least 8 / 9 , Thus for k = O (log(1 / X  ) , the total size of the sample T in Algorithm 1 needs to be n 0 = O ( c 0  X  log(1 / X  )) . Setting  X  = 1 / 2 ,  X  = 1 and n  X  = O ( c 0  X  d ) , Theorem 1 is now derived from Theorem 5, by applying the identity Alon, Noga, Matias, Yossi, and Szegedy, Mario. The space complexity of approximating the frequency moments.
Journal of Computer and System Sciences , 58:137 X 147, 1999.
 Audibert, Jean-Yves and Catoni, Olivier. Robust linear least squares regression. Ann. Stat. , 39(5):2766 X 2794, 2011.
 Balcan, M.-F., Beygelzimer, A., and Langford, J. Agnostic active learning. In Twenty-Third International Confer-ence on Machine Learning , 2006.
 Catoni, Olivier. Challenging the empirical mean and em-pirical variance: a deviation study. Ann. Inst. H. Poincar Probab. Statist. , 48(4):1148 X 1185, 2012.
 Hsu, Daniel and Sabato, Sivan. Approximate loss minimization with heavy tails. ArXiv e-prints , arXiv:1307.1827, 2013. Arxiv preprint.
 Hsu, Daniel, Kakade, Sham M., and Zhang, Tong. Ran-dom design analysis of ridge regression. In Twenty-Fifth Conference on Learning Theory , 2012.
 Juditsky, Anatoli and Nemirovski, Arkadii S. Large devi-ations of vector-valued martingales in 2-smooth normed spaces. ArXiv e-prints , arXiv:0809.0813, 2008.
 Lepski, O. V. Asymptotically minimax adaptive estimation I: Upper bounds. optimally adaptive estimates. Theory Probab. Appl. , 36(4):682 X 697, 1991.
 Levin, Leonid A. Notes for miscellaneous lectures. CoRR , abs/cs/0503039, 2005.
 Litvak, Alexander E., Pajor, Alain, Rudelson, Mark, and
Tomczak-Jaegermann, Nicole. Smallest singular value of random matrices and geometry of random polytopes.
Adv. Math. , 195(2):491 X 523, 2005. ISSN 0001-8708. doi: 10.1016/j.aim.2004.08.004. URL http://dx. doi.org/10.1016/j.aim.2004.08.004 .
 Mahdavi, Mehrdad and Jin, Rong. Passive learning with target risk. In Twenty-Sixth Conference on Learning The-ory , 2013.
 Minsker, Stanislav. Geometric median and robust estima-tion in banach spaces. arXiv e-prints , arXiv:1308.1334, 2013.
 Nemirovsky, A. S. and Yudin, D. B. Problem Com-plexity and Method Efficiency in Optimization . Wiley-Interscience, 1983.
 Nussbaum, M. Minimax risk: Pinsker bound. In Kotz, S. (ed.), Encyclopedia of Statistical Sciences, Update Vol-ume 3 , pp. 451 X 460. Wiley, New York, 1999.
 Oliveira, Roberto. Sums of random Hermitian matrices and an inequality by Rudelson. Electron. Commun. Probab. , 15(19):203 X 212, 2010.
 Srebro, Nathan, Sridharan, Karthik, and Tewari, Ambuj. Smoothness, low noise and fast rates. In Advances in Neural Information Processing Systems 23 , 2010.
 Srivastava, N. and Vershynin, R. Covariance estimation for distributions with 2 + moments. Annals of Probability ,
