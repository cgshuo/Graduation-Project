 Sentence compression is a basic operation in text simplification which has the potential to improve statistical machine translation and automatic sum-marization (Berg-Kirkpatrick et al., 2011; Klerke et al., 2015), as well as helping poor readers in need of assistive technologies (Canning et al., 2000). This work suggests using eye-tracking recordings for im-proving sentence compression for text simplification systems and is motivated by two observations: (i) Sentence compression is the task of automatically making sentences easier to process by shortening them. (ii) Eye-tracking measures such as first-pass reading time and time spent on regressions, i.e., dur-ing second and later passes over the text, are known to correlate with perceived text difficulty (Rayner et al., 2012).

These two observations recently lead Klerke et al. (2015) to suggest using eye-tracking measures as metrics in text simplification. We go beyond this by suggesting that eye-tracking recordings can be used to induce better models for sentence compression for text simplification. Specifically, we show how to use existing eye-tracking recordings to improve the induction of Long Short-Term Memory models (LSTMs) for sentence compression.

Our proposed model does not require that the gaze data and the compression data come from the same source. Indeed, in this work we use gaze data from readers of the Dundee Corpus to improve sentence compression results on several datasets. While not explored here, an intriguing potential of this work is in deriving sentence simplification models that are personalized for individual users, based on their reading behavior.

Several approaches to sentence compression have been proposed, from noisy channel models (Knight and Marcu, 2002) over conditional random fields (Elming et al., 2013) to tree-to-tree machine trans-lation models (Woodsend and Lapata, 2011). More recently, Filippova et al. (2015) successfully used LSTMs for sentence compression on a large scale parallel dataset. We do not review the literature here, and only compare to Filippova et al. (2015). Our contributions  X  We present a novel multi-task learning ap- X  Our method is fully competitive with state-of- X  Our code is made publicly available at Readers fixate longer at rare words, words that are semantically ambiguous, and words that are mor-phologically complex (Rayner et al., 2012). These are also words that are likely to be replaced with simpler ones in sentence simplification, but it is not clear that they are words that would necessarily be removed in the context of sentence compression.
Demberg and Keller (2008) show that syntac-tic complexity (measured as dependency locality) is also an important predictor of reading time. Phrases that are often removed in sentence compression X  like fronted phrases, parentheticals, floating quanti-fiers, etc. X  X re often associated with non-local de-pendencies. Also, there is evidence that people are more likely to fixate on the first word in a con-stituent than on its second word (Hy  X  on  X  a and Pol-latsek, 2000). Being able to identify constituent borders is important for sentence compression, and reading fixation data may help our model learn a rep-resentation of our data that makes it easy to identify constituent boundaries.

In the experiments below, we learn models to pre-dict the first pass duration of word fixations and the total duration of regressions to a word. These two measures constitute a perfect separation of the to-tal reading time of each word split between the first pass and subsequent passes. Both measures are de-scribed below. They are both discretized into six bins as follows with only non-zero values contribut-ing to the calculation of the standard deviation (SD): 0: measure = 0 or 1: measure &lt; 1 SD below reader X  X  average or 2: measure &lt; .5 SD below reader X  X  average or 3: measure &lt; .5 above reader X  X  average or 4: measure &gt; .5 SD above reader X  X  average or 5: measure &gt; 1 SD above reader X  X  average First pass duration measures the total time spent reading a word first time it is fixated, including any immediately following re-fixations of the same word. This measure correlates with word length, fre-quency and ambiguity because long words are likely to attract several fixations in a row unless they are particularly easily predicted or recognized. This ef-fect arises because long words are less likely to fit inside the fovea of the eye. Note that for this mea-sure the value 0 indicates that the word was not fix-ated by this reader.
 Regression duration measures the total time spent fixating a word after the gaze has already left it once. This measure belongs to the group of late measures, i.e., measures that are sensitive to the later cognitive processing stages including interpretation and integration of already decoded words. Since the reader by definition has already had a chance to recognize the word, regressions are associated with semantic confusion and contradiction, incongruence and syntactic complexity, as famously experienced in garden path sentences. For this measure the value 0 indicates that the word was read at most once by this reader.

See Table 1 for an example of first pass duration and regression duration annotations for one reader and sentence.
 Most recent approaches to sentence compression make use of syntactic analysis, either by operating directly on trees (Riezler et al., 2003; Nomoto, 2007; Filippova and Strube, 2008; Cohn and Lapata, 2008; Cohn and Lapata, 2009) or by incorporating syn-tactic information in their model (McDonald, 2006; Clarke and Lapata, 2008). Recently, however, Filip-pova et al. (2015) presented an approach to sentence compression using LSTMs with word embeddings, but without syntactic features. We introduce a third way of using syntactic annotation by jointly learning a sequence model for predicting CCG supertags, in addition to our gaze and compression models. Bi-directional recurrent neural networks (bi-RNNs) read in sequences in both regular and re-versed order, enabling conditioning predictions on both left and right context. In the forward pass, we run the input data through an embedding layer and compute the predictions of the forward and back-ward states at layers 0 , 1 , . . . , until we compute the softmax predictions for word i based on a linear transformation of the concatenation of the of stan-dard and reverse RNN outputs for location i . We then calculate the objective function derivative for the sequence using cross-entropy (logistic loss) and use backpropagation to calculate gradients and up-date the weights accordingly. A deep bi-RNN or k -layered bi-RNN is composed of k bi-RNNs that feed into each other such that the output of the i th RNN is the input of the i + 1 th RNN. LSTMs (Hochreiter and Schmidhuber, 1997) replace the cells of RNNs with LSTM cells, in which multiplicative gate units learn to open and close access to the error signal.
Bi-LSTMs have already been used for fine-grained sentiment analysis (Liu et al., 2015), syntac-tic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015). These and other re-cent applications of bi-LSTMs were constructed for solving a single task in isolation, however. We in-stead train deep bi-LSTMs to solve additional tasks to sentence compression, namely CCG-tagging and gaze prediction, using the additional tasks to regu-larize our sentence compression model.

Specifically, we use bi-LSTMs with three lay-ers. Our baseline model is simply this three-layered model trained to predict compressions (encoded as label sequences), and we consider two extensions thereof as illustrated in Figure 2. Our first exten-sion, M ULTI -TASK -LSTM, includes the gaze pre-diction task during training, with a separate logistic regression classifier for this purpose; and the other, C
ASCADED -LSTM, predicts gaze measures from the inner layer. Our second extension, which is su-perior to our first, is basically a one-layer bi-LSTM for predicting reading fixations with a two-layer bi-LSTM on top for predicting sentence compressions. At each step in the training process of M ULTI -TASK -LSTMand C ASCADED -LSTM, we choose a random task, followed by a random training instance of this task. We use the deep LSTM to predict a label sequence, suffer a loss with respect to the true labels, and update the model parameters. In C ASCADED -LSTM, the update for an instance of CCG super tag-ging or gaze prediction only affects the parameters of the inner LSTM layer.
 Both M ULTI -TASK -LSTM and C ASCADED -LSTM do multi-task learning (Caruana, 1993). In multi-task learning, the induction of a model for one task is used as a regularizer on the induction of a model for another task. Caruana (1993) did multi-task learning by doing parameter sharing across sev-eral deep networks, letting them share hidden layers; a technique also used by Collobert et al. (2011) for various NLP tasks. These models train task-specific classifiers on the output of deep networks (informed by the task-specific losses). We extend their models by moving to sequence prediction and allowing the task-specific sequence models to also be deep mod-els. 4.1 Gaze data We use the Dundee Corpus (Kennedy et al., 2003) as our eye-tracking corpus with tokenization and mea-sures similar to the Dundee Treebank (Barrett et al., 2015). The corpus contains eye-tracking recordings of ten native English-speaking subjects reading 20 newspaper articles from The Independent . We use data from nine subjects for training and one subject for development. We do not evaluate the gaze pre-diction because the task is only included as a way of regularizing the compression model. 4.2 Compression data We use three different sentence compression datasets, Z IFF -D AVIS (Knight and Marcu, 2002), B
ROADCAST (Clarke and Lapata, 2006), and the publically available subset of G OOGLE (Filippova et al., 2015). The first two consist of manually com-pressed newswire text in English, while the third is built heuristically from pairs of headlines and first sentences from newswire, resulting in the most ag-gressive compressions, as exemplified in Table 1. We present the dataset characteristics in Table 2. We use the datasets as released by the authors and do not apply any additional pre-processing. The CCG use sections 0-18 for training and section 19 for de-velopment. 4.3 Baselines and system Both the baseline and our systems are three-layer bi-LSTM models trained for 30 iterations with pre-trained (S ENNA ) embeddings. The input and hid-den layers are 50 dimensions, and at the output layer we predict sequences of two labels, indicating whether to delete the labeled word or not. Our base-line (B ASELINE -LSTM) is a multi-task learning bi-LSTM predicting both CCG supertags and sen-tence compression (word deletion) at the outer layer. Our first extension is M ULTITASK -LSTM predict-ing CCG supertags, sentence compression, and read-ing measures from the outer layer. C ASCADED -LSTM, on the other hand, predicts CCG supertags and reading measures from the initial layer, and sen-tence compression at the outer layer. 4.4 Results and discussion Our results are presented in Table 3. We observe that across all three datasets, including all three annotations of B ROADCAST , gaze features lead to improvements over our baseline 3-layer bi-LSTM. Also, C ASCADED -LSTM is consistently better than M
ULTITASK -LSTM. Our models are fully compet-itive with state-of-the-art models. For example, the best model in Elming et al. (2013) achieves 0.7207 on Z IFF -D AVIS , Clarke and Lapata (2008) achieves Filippova et al. (2015) achieves 0.80 on G OOGLE with much more training data. The high numbers on the small subset of G OOGLE reflects that newswire headlines tend to have a fairly predictable relation to the first sentence. With the harder datasets, the im-pact of the gaze information becomes stronger, con-sistently favouring the cascaded architecture, and with improvements using both first pass duration and regression duration, the late measure associated with interpretation of content. Our results indicate that multi-task learning can help us take advantage of inherently noisy human processing data across tasks and thereby maybe reduce the need for task-specific data collection.
 Yoav Goldberg was supported by the Israeli Science Foundation Grant No. 1555/15. Anders S X gaard was supported by ERC Starting Grant No. 313695. Thanks to Joachim Bingel and Maria Barrett for preparing data and for helpful discussions, and to the anonymous reviewers for their suggestions for improving the paper.

