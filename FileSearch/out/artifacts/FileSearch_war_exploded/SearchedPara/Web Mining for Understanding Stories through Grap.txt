
Rich information spaces (like the Web or scientific publi-cations) are full of  X  X tories X : sets of statements that evolve over time, manifested as, for example, collections of news-paper articles reporting events relating to an evolving crime investigation, sets of news articles and blog posts accom-panying the development of a political election campaign, or sequences of scientific papers on a topic. In this paper, we propose a method and a visualisation tool for mapping and interacting with such stories. In contrast to existing approaches, our method concentrates on relational infor-mation and on local patterns rather than on the occurrence of individual concepts and global models. In addition, we present an evaluation framework. A real-life case study is used to illustrate and evaluate the method and tool.
The Web has led to a proliferation of news (and other broadcast media like blogs) that continuously report on cur-rent events and other topics. Several search-engine innova-tions of the past few years like the grouping of news articles by topic in Google News have made it easier to keep abreast when one reads the news every day. However, a Web user who misses several days or who wants to gain an overview of major events and developments in a  X  X tory X  that lies in the past, is today faced with a situation that is reminiscent of the early days of the Web. Search in most archives is based on keyword search and therefore returns an unmanageable number of results. Summarisations like that provided by Google Trends 1 or BlogPulse X  X  Trend Search 2 show surges in publication and query activity in certain time periods, but these tools require that one knows which sub-topic to look for (and how to describe it in keywords).

The same problem arises in other areas with high publi-cation intensity and readers who aim to gain, refresh, and/or extend overviews of topical developments  X  scholarly pub-lications are a prime example.

This situation calls for systems that (a) identify topical sub-structure in a set (generally, a time-indexed stream) of documents constrained by being about a common topic, (b) show how these substructures emerge, change, and disap-pear (and maybe re-appear) over time, and (c) give users in-tuitive interfaces for interactively exploring the topic land-scape and at the same time the underlying documents. In an extension of [20], we call the resulting problem evolu-tionary theme patterns discovery, summary and exploration (ETP3).

The first contribution of the paper is a (re-)appraisal of the ETP3 problem as one that requires a semi-automatic solution, and a proposal for a system that offers such a semi-automatic solution. Specifically, we believe that such a system should not be overly prescriptive. In particular, the user X  X  interpretation of subdivisions within a topic will depend on her current tasks and other situational variables. We therefore aim, in contrast to the existing approaches, not at a global model of the topic (such as a clustering into exhaustive sub-topics); instead, we are interested in high-resolution local patterns and interaction options that support users in finding and exploring their own interpretations. The second contribution is an evaluation framework for ETP3 and a demonstration using a case study.

The paper is structured as follows: In Section 2, we give an overview of related research. Sections 3 and 4 present our solution approach  X  X TORIES X : Section 3 describes the computational method and Section 4 the tool. A case study demonstrates method and tool in Section 5. Section 6 de-scribes the evaluation method and results. Section 7 con-cludes with an outlook.
Our work builds on several areas of research, in par-ticular the identification and tracking of topics in text streams, the identification of  X  X ursty X  events, the use of co-occurrence information for content extraction, and informa-tion visualisation. Temporal text mining. [20] described evolutionary theme pattern discovery as one key subproblem of temporal text mining. They presented a fully automatic method that ex-tracts subtopics and creates a graph that shows their life cy-cles and dependencies on each other. A mixture model was used to model documents as expressing (potentially several) themes (corresponding to sub-topics). These word clusters are tracked over time. The use of clustering models for find-ing emergent sub-topics and tracking them over time is also the subject of [24, 15]. The publications show that the meth-ods can be applied to scholarly publications as well as Web news. Evolutionary theme pattern discovery is related to topic detection and tracking, specifically first story detec-tion [2]. However, it is more fine-grained than TDT since it delves into a topic X  X  substructure, and its aim is not only to classify something as a new (or old) topic, but to describe it. Evolutionary theme pattern discovery is also related to the document update problem in text summarisation, which is discussed in more detail in Section 6.

These methods rely on the notion of sub-topics that cover the space of reported content, such that it is difficult to iden-tify local details and their changes over time.
 Burstiness. (Sub)topics may be particularly interesting when they are bursty [16], i.e. when publication activity on them is very strong in a certain time period, picking up vol-ume fast at this period X  X  beginning and (usually) disappear-ing again as fast. Burstiness has been explored with respect to various domains and phenomena including  X  X uzz X  in text and news streams [11, 12, 13]. [11] group  X  X ursty features X  into  X  X ursty events X  based on co-occurrence, thereby creat-ing an analogue of sub-topics.

So far, burstiness has only been investigated as a feature of single text features (words or topics). We extend this to an analysis of burstiness of associations.
 Co-occurrence analysis. The analysis of bursty events points to the merits of focussing on specific parts of contents and their relations with each other, rather than on finding a global model. In general, the analysis of co-occurrences allows for a more fine-grained analysis of texts and has been investigated for example in text summarisation. [3] show that topic signatures [19] provide a simple and effec-tive way to summarise multiple documents. [25] used co-occurrences to find historical associations between places and times in a digital library. He analysed how various in-terestingness measures rank these associations and showed that they behave differently, for example in the ranking of rare events. This indicates that different interestingness measures may be more or less adequate for the analysis of different corpora, domains and/or different tasks, an in-terpretation also supported by the findings of [10]. These authors found co-occurrence lift to be an adequate interest-ingness measure to analyse perceptions of (car) brands and markets in user forums. [7] propose application-domain interpretations of tem-poral changes in the frequencies of co-occurrences. They argue that agents (person names in the texts) can exist inde-pendently of each other, join, split again, etc. These devel-opments create specific  X  X tory lines X .

All these approaches are restricted to analysing co-occurrences between typed elements (names, places, ...). We take a more general approach and identify  X  X tory lines X  between arbitrary words or concepts. [1] applied text summarisation to news streams, their fo-cus was however more on finding the best sentences to be (re-)used in the summaries than on distilling concepts from these sentences. In contrast to this work, we focus not only on content that is new (i.e., different from what was reported before), but on content that is characteristic for a time pe-riod (i.e., also different from what was reported later). Visualisation. The main focus of most of the above stud-ies were challenges (a) and (b) mentioned in the Introduc-tion. Visualisations are probably best suited to displaying the complex relationships found. [25] provided users with an interactive map browser for exploring the location-time co-occurrences. This is a good example of how to meet challenge (c) in a way that is adapted to the application domain. [30] show a domain-independent way of visu-alising pairwise associations of words that also takes into account when these associations were strong. They plot words against time and show co-occurrences by connect-ing lines in a format that is related to parallel coordinates. Their graphs provide an excellent overview of the occur-rence or recurrence of pairwise associations over a whole timeline. However, because time takes up one visual di-mension, higher-order patterns of associations cannot easily be detected. In contrast to this, we will show associations per time point/period. This  X  X napshot X  idea is the same as that used in the graph sequences used for visualising sci-entific publications and topics by, e.g., [5, 6, 15]. In con-trast to that, we use a layout strategy that is more amenable to highlighting emerging and disappearing topics, and offer the alternative of a dynamic layout between successive time periods (morphing), similar to [17].
The basic assumptions of our method are that (a) there is a set of time-stamped documents that, when read by a human reader, reveal the story and its evolution and (b) the words in these documents also reveal the story and its evo-lution when processed by simple text mining methods. We conceptualise  X  story basics as the high-ranking terms (words, com- X  story elements as the high-ranking relationships be- X  story stages as networks of salient story elements in a  X  story evolution as the temporal sequence of story
This basic scheme can be operationalised in several ways. To create a baseline, we have started with very simple versions of each of these constructs X  operational-isations. Specifically, the method involves the following stages. First, a corpus of text-only documents is trans-formed into a sequence-of-terms representation. Subse-quently, basic term statistics are calculated to identify can-didates for story basics. We chose term frequency TF for the whole corpus, which is defined as (# occurrences of the term in the whole corpus) / (# all terms in the whole corpus) . We define the content-bearing terms as the 150 top-TF terms.
Next, the whole corpus T is partitioned into sets of docu-ments that were published in time periods following one an-other, e.g. within one calendar week. Thus, T is the union of all document sets t i , with i = 1 , . . . , I the time periods.
For each t i , the frequency of the co-occurrence of all pairs of content-bearing terms within a window of w terms in documents is calculated as follows: 3 This measure of frequency and therefore relevance is nor-malised by its counterpart in the whole corpus to yield the measure time relevance : This measure is based on the domain relevance metric [21] which measures the relevance of a term in a (subject-domain) subcorpus relative to the whole corpus. When used, as here, for time-specific subcorpora, it also measures  X  X urstiness X . Thresholds are applied to avoid singular as-sociations in small sub-corpora and to concentrate on those associations that are most characteristic of the period and most distinctive relative to others . We define two sets N(on-singular) and C(haracteristic) : for some thresholds  X  1 ,  X  2 . This gives rise to  X  the story stage i : N i  X  C i . This can also be expressed as  X  the story elements : all edges of the story stage.  X  the story basics : all nodes of the story stage.  X  the story evolution : the sequence of story stages.
To obtain a smoother story evolution, we use the moving average of co-occurrence frequency values. This was done by replacing for each period t i , the document base set in both numerator and denominator of the right-hand side of the freq definition by the union over periods i, . . . , ( i + l  X  1) .
Investigations of different parameter settings showed that in most cases, only associations with TR &gt;  X  2 = 3 are interesting and allow for a tractable graph. However, the ad-vantage of an interactive approach is that we can let the user explore different values of  X  2 and thereby create their indi-vidual story stages. Visualisation options (see Section 4) help to accentuate the differences in time relevance. Users are also able to control  X  1 .
The method can be applied to textual documents such as news obtained from the Web. In this section, we describe the data cleaning and further pre-processing applied to this kind of data.
 Data cleaning represented a challenging first step in data preparation. Virtually all news sources present their content in Web pages with a multitude of other content: naviga-tion menus, advertising, ... The best approaches developed so far, such as [9], essentially suggest to learn a wrapper by comparing different articles from the same source; the idea is that this will identify the  X  X oise X  by equality over different  X  X ontent X  pages (the  X  X ontent X  should be the only subtree in the DOM tree that changes). Unfortunately, this turned out to not work for many of the sources we inves-tigated, because several elements of the DOM tree change across different articles, even if published on the same day in the same content area. We therefore included an auto-mated wrapper-induction component in the tool; however in order to not conflate data cleaning issues with content extraction issues, in the case study below we extracted the content into ASCII by manual copy-and-paste.
 Text pre-processing. The documents were first tokenized; subsequently, several further pre-processing options were investigated. Named entity recognition (NER) was done as a two-phase process. In the first phase, the Open Calais 4 mantic toolkit was used to extract NEs. Pilot tests showed that pronoun resolution did not work well on our materials; therefore pronoun resolution was filtered out using a stop-word list. Since Open Calais operates on a per-document basis, it cannot map a term to named entities if the named entity does not appear in the document that is currently in-spected, despite the fact that in the entire corpus the same term is mapped to the same named entity. To overcome this problem, in the second phase, each term x that was mapped to some named entity in at least one document in the first phase, was treated as follows: Let x 1 , ..., x n be the NEs to which x was mapped in the first phase. Let x max be the NE from x 1 , ..., x n to which x was mapped most of-ten. Then, in each document containing x but not x max , we map x to x max . (A similar NER solution was proposed by [8].) This was followed by lemmatization using the Tree-Tagger 5 . Stopwords were removed using the stopword list from the Terrier project 6 , manually enhanced by HTML-code and application-specific words.

All parameters for text pre-processing can easily be con-figured, and the architecture provides the needed modular-ity for, e.g., using different interestingness measures and thereby re-using and/or evaluating other proposals for tem-poral text mining.
 The graphical usage interface We implemented the method in a series of php scripts interacting with a MySQL database, and generated visualisations using GUESS 7 . The visualisations comprise static visualisations of the story stages of individual periods, and a morphing sequence that traces story evolution through the sequence of all periods. In addition to this  X  X canning X , users can  X (un)zoom X  by adapting the period-window size l .

The visualisations are enhanced by salience slide rulers that allow the user to filter out story elements below indi-vidually set  X  1 (absolute number of occurrence of an asso-ciation) or  X  2 (time relevance) thresholds. A configurable colour scheme accentuates time relevance differences. The figures included in this paper use a sequential scheme op-timised for printing, going from black (high TR ) to light grey (low). For on-screen viewing, different users expressed preferences for sequential schemes using other colours or for divergent schemes, in particular the harmonious colours from blue (high TR ) via red (medium) to yellow (low). By clicking on an edge, the user gets a list of documents con-taining that co-occurrence in a browser window. All pro-grams can be executed on a local computer; after an initial download of documents. A screenshot is shown in Fig. 3. In the remaining figures, the graphs have been extracted from the tool environment for better legibility.
For demonstration, we used a real-life story with a com-paratively clear and well-known course of events: the dis-appearance of Madeleine McCann on May 3rd, 2007, and the development of the criminal investigation. 8
Two main events in this investigation were the early sus-picion of a man with the initials R.M. 9 as kidnapper, the dis-covery of Madeleine X  X  blood in a car rented by the parents (established as hers by a DNA test) and the associated police questioning and suspicion of Madeleine X  X  parents. These were interspersed by long periods of less media attention with little to report (or misleading incidents like the arrest of two people unrelated to the case). 10 The corpus. We used articles from the Google News archive 11 between May and December 2007 (week 17 in which the girl disappeared until week 52) and restricted the results as follows: only English-language articles; for each month, the first 100 hits, and of those, only those that were still freely available in April 2008. After a first round of analysis, these were restricted to documents from weeks 17 X 37, the  X  X ventful X  weeks of that story. This resulted in a corpus of 215 documents. This was regarded as a good approximation of the real-life situation confronted by a de-ployed STORIES algorithm: Articles are found to be candi-dates based solely on keyword matching (in this case: using the first and last name of the missing girl as the query in the Google News archive), they come from sources of varying quality, and there is no ranking on the news sources in the Google News archive after some months. This set was extended by the set of all retrievable, English-language news articles referenced in the Wikipedia article [28], from the investigated time period. This pro-vided another 91 articles. This selection constitutes a kind of opposite extreme of the first document selection, because the occurrence of an article in the reference list indicates that its content passed a manual quality control and was in-tegrated into the Wikipedia article. Due to the collaborative authoring of the Wikipedia article, this selection can also be said to represent a wide variety of viewpoints and (poten-tially) consensus on the quality of the individual articles.
The combined corpus contained 306 articles with 174,886 words, resulting in an average of 572 words per article. The corpus contained 8,075 (6,089) unique words (lemmas).
 Results Figures 2 X 5 show selected individual story stages. 12 In particular, Fig. 2 shows the description of an event (missing British child MM). Figure 3 illustrates how the key first suspect becomes an (also visually)  X  X entral X  element of the story. Figures 5 (a) and (b) show how the interface is used by changing the threshold in order to  X  X n-cover X  a story stage . Specifically, Fig. 5 (b) explains some of the reasons for the connections in Fig. 5 (a). An event-less period in a story is characterised by a small number of disconnected subgraphs like the ones in Fig. 4.
Temporal text mining is still a young area, so unlike for example in TDT, no standards exist yet for evaluating ap-proaches, and the existing literature often restricts itself to plausibility checks. Therefore, the quest for an evaluation of the STORIES approach involves finding answers to the following questions: (1) Can an existing evaluation framework and/or dataset be used as benchmark? (2) How should the ground truth be defined? (3) Can evaluation be (partially) automated to cut human evaluators X  workload? (4) What instructions should human evaluators get? (5) How can the results be interpreted? We address questions (1) X (5) in turn. (1) Existing evaluation frameworks
The ETP3 problem can be decomposed into two sub-problems: Evolutionary theme patterns discovery and sum-mary on the one hand, and evolutionary theme patterns exploration on the other hand. Evolutionary theme pat-terns discovery is related to the update task first formulated in the Document Understanding Conference (DUC) 2007:  X  X he update summary pilot task will be to create short (100-word) multi-document summaries under the assump-tion that the reader has already read a number of previous documents. X  13 This contest supplied a test corpus of news stories (documents assigned to 10 topics, each divided into 3 time periods, were supplied), summaries of the updates manually generated by 4 independent human raters, and de-tailed evaluation reports (precision, recall and F1) of base-lines and all the contenders. The evaluation reports were generated with the ROUGE software ( X  X ecall-oriented un-derstudy of gisting evaluation X ) that was kindly provided to us by its creator Chin-Yew Lin.

The DUC/ROUGE concept is not directly applicable to STORIES because it assumes that the summaries are natural-language texts, whereas we generate graphs. Yet, we created a way of applying the ROUGE evaluation frame-work and software to our representation (see (3) below).
However, the DUC/ROUGE dataset cannot be used to benchmark our approach. The reason is that the dataset is not a stream (a large set of documents following in quick succession and with usually relatively small differences to the previous one). Rather, it is (for each topic) a set of 3 small-cardinality (usually below 10) sets of documents that were published in 3 disjoint and subsequent time periods, but have very little connection to the other 2 periods X  con-tent. This resulted in all interesting co-occurrences being  X  X ursty X  in each of the 3 periods, resulting in an impossibil-ity to select the really important ones.

Another candidate dataset is the Tsunami dataset used and provided by [20] 14 . However, since it is not associ-ated with a ground truth and since it is not straightforward to compare the output of STORIES with the output of the method of [20], this dataset could not be used either.
We therefore decided to use our own case-study dataset and to concentrate on defining a method for evaluation. (2) Finding a ground truth
One of the biggest problems of finding a ground truth is that in many text tasks, the agreement between human raters is not very high [e.g., 27]. Thus, it is necessary to have a ground truth that reflects a wide range of human raters. In some evaluation frameworks, this goal is achieved by em-ploying several ground truths by different people (e.g., 4 in the DUC/ROUGE evaluation, see (1) above).

Fortunately, the Web itself provides us not only with streams of news, but also with documents that come close to the goal of a multi-rater truth. Specifically, Wikipedia articles (especially those on contested themes such as our case study) are generally written and revised by hundreds of authors, cf. for example [4].

Wikipedia articles are often very long, full of detail, and only occasionally written with a story progression in mind. Therefore, such a document must be transformed in order to serve as a ground truth to be used in a (machine or human) evaluation. We proceeded as follows: First, all sentences that contained a date were extracted from the article. To minimise bias and errors, we had two independent raters extract these sentences (and if necessary perform minor re-formulations to make them understandable out of context). Only those assertions that both found in the text, plus a max-imum of five others from each rater, were included in the final set of ground truth assertions. In the case study, this resulted in a total of 31 ground-truth events .

All ground-truth events were indexed by the calendar week in which they had occurred, such that they could later be assembled easily into the ground-truth of the time win-dow (e.g., 3 weeks) that was covered by the method. (3) Partially automating the evaluation
The goal was to present both the STORIES output and the ground truth to human evaluators in order to determine precision, recall and F1 values. However, in a pilot study with human raters, we had found that this was a very la-borious task and could not easily be repeated for different settings because after seeing the first setting, a human rater knows the story.

Therefore, prior to presenting people with the ground truth and the algorithm output, the best parameter setting had to be found. Recall from Section 3 that the method has as parameters l (the number of weeks that make up a story stage, where story stages are overlapping when l &gt; 1 ), w (the window size within the texts that is in-spected for co-occurrences),  X  1 (the minimum total number of co-occurrences), and  X  2 (the minimum time relevance of a co-occurrence). The pilot study had also suggested that for humans to be able to read the graph, the cardinal-ity of the story stage (the number of edges of each graph, | StoryStage | ) should be limited.

We varied l = [1 , 3] , kept w = 5 and  X  1 = 5 based on common values found in the literature, and, starting from a value of  X  2 = 2 , varied | StoryStage | from 10 to 30, in increments of 5.  X  2 = 2 was an intuitive value based on the pilot study ( X  X t least twice as frequent in this period than on average X ), and 10 to 30 was considered to be a realistic range for human graph reading usability.

To evaluate this large number of combinations, we used the findings of [18], who showed that the automated word-pair matching rules of ROUGE correspond to human rat-ings in the following sense: The ranking of quality as-sessments by humans corresponds to the ranking of quality assessments by ROUGE. This does not necessarily mean that the absolute values of precision or recall correspond to each other. However, it means that the setting with the best ROUGE results should be chosen for presentation to humans.

ROUGE evaluates natural-language texts against other texts (the ground truth). STORIES outputs graphs instead of natural-language texts. These two forms of represen-tation are not directly comparable (see for example [14]); however, pilot tests showed us that people interpret paths in the STORIES graphs in a similar way as sentences. We therefore used the following heuristic: We extracted all paths from each STORIES graph 15 and ordered them by de-scending average TR path weight. We then truncated these  X  X seudo-sentences X  at 100 characters to generate ROUGE-style summaries.

ROUGE has different evaluating functions. In our case, the applicable ones were ROUGE-1 (overlap of unigrams), which however always favoured larger graphs  X  a result that is in conflict with the usability requirement of smaller graphs. The only other applicable function is ROUGE-SU4, which measures the overlap of skip-bigrams of at most length 4. A skip-bigram is any pair of words in their sen-tence order, allowing for arbitrary gaps. Skip-bigram cooc-currence statistics measure the overlap of skip-bigrams be-tween an automatically generated text and a ground-truth text. We used the ROUGE parameter values that were em-ployed in the DUC 2007 update task evaluation.

The resulting ranking for the corpus was (pairs denote | StoryStage | and l ): 20  X  2 , 30  X  3 , 25  X  2 , 15  X  3 , 20  X  3 , 25  X  3 , 10  X  3 , 15  X  2 , 30  X  2 , 10  X  2 , 10  X  1 , 25  X  1 , 30  X  1 , 15  X  1 , 20  X  1 . Thus, 20 edges and a window of two weeks produced the best story stage descriptions. (4) Procedure of the manual evaluation
Two raters from different backgrounds, both with a good command of English and only a superficial knowledge of the story, volunteered to rate the STORIES summaries. They were given the 20-2 graphs for a temporal subset of the case-study corpus. The raters received the GUESS soft-ware and the graphs together with a driver script, an Excel sheet with one tab for each time window of 2 weeks and one ground-truth event per line, and a set of instructions. They then worked individually at their own pace.

The raters were asked to inspect the graphs in temporal order using the slide ruler for  X  2 , starting from a high value so as to  X  X ncover X  the graph, and to rate the first 20 edges as follows: If it describes an aspect of an event, then annotate the event with the edge number (visualised by a change in the script). If multiple matches seem appropriate, multiple annotations should be made. The raters were asked to stop when they reached 20 edges or before if the graph  X  X topped making sense X . The filled-in Excel sheets were the basis for the following quantitative evaluation. (5) Results and interpretation
The measured outcomes were precision at n = 5, 10, 15 and 20 (since edges were numbered, the top-TR edges could easily be identified) and recall at the same n , the latter
Figure 1. Events (ground-truth), edges and their burstiness profile (average TR), and rep-resented events. defined as the number of correctly retrieved events for the top-n edges. This notion of recall differed slightly from the standard one because the number of events differed between periods (such that 1 mapping edge would give rise to a recall of 0 . 2 in a period with 5 events, but 0 . 5 in a period with 2 events). Some graphs contained fewer than n edges, for these, precision and recall at n are not defined.
The results are shown in Table 1. They illustrate that (a) the judgements by both raters were highly similar in terms of overall quality; (b) recall was quite high  X  on average, nearly half the events were found in a graph as small as 5 edges, and over 80% in a graph of 20 edges; (c) preci-sion was uniformly acceptable but strictly lower than recall (about one third of all edges were content-bearing). The rel-atively large values of the standard deviation indicate that the quality of representation varies by week. To investigate whether certain  X  X round-truth event patterns X  cause these variations, we illustrate in Figure 1 more detail by plotting the number of events against the number of events that were represented in the graphs (for 20 edges, averaged over the two raters). It indicates that the number of events does not influence the quality of representation as measured by re-call. The figure also plots the average TR values of the top 20 edges; they too follow the same pattern as the events. Thus, the burstiness measured by T R is a good measure of ground-truth  X  X ventfulness X .

Only in week 25 is there is a marked difference between a high average TR and a low number of events. The reason is that in week 26 (which affects 25 due to l = 2 ), a cou-ple had been implicated and arrested. These soon turned out to be con artists who had nothing to do with the case. The incident is not reported in Wikipedia (which we used as  X  X round truth X ), but made headlines at the time.
This paper has presented a new problem in the area of temporal text mining: the tracking of story evolution. More specifically, the ETP3 (evolutionary theme patterns discovery, summary and exploration) problem consists of (a) identifying topical sub-structure in a set (generally, a time-indexed stream) of documents constrained by being about a common topic, (b) showing how these substructures emerge, change, and disappear (and maybe re-appear) over time, and (c) giving users intuitive interfaces for interac-tively exploring the topic landscape and at the same time the underlying documents. The problem is related to, but extends known problems, in particular evolutionary theme pattern discovery, cf. [20, 24, 15] and the document update problem [18], as well as the detection of bursty events, cf. [11].

By using simple co-occurrence measures on elements that make up a story through the STORIES method, we cre-ated a tool that allows users to look at and actively explore story evolution from their individual perspectives. A case study on a well-publicised story over a long period of time showed the usefulness of the proposed method. An easily-usable, interactive GUI for tracking story evolution is a spe-cific focus of this work. Graphs that consist of elements of a co-occurrence network are an easy and understandable way of presenting the development of a story.

We also presented an evaluation framework for ap-proaches to the ETP3 problem and demonstrated the rep-resentation quality of the approach, using a real-life case study. This represents an advancement over the state of the art because so far, evaluation with respect to a  X  X round truth X  is mostly lacking from temporal text mining (with TDT and the DUC update tasks, which however address dif-ferent computational problems, notable exceptions). In the future, we want to extend this framework to also allow for a comprehensive cross-evaluation of different methods (such as  X  X lobal X  clustering or (P)LSA-based methods vs.  X  X o-cal X  co-occurrence analysis) and interestingness measures for patterns (such as time relevance or other measures of burstiness). In addition, we will complement our IR/data-mining oriented evaluation by usability assessments.
Another area of improvement is the detection of events represented by bursty features as suggested by [11]. In or-der to discover more precise story elements, next versions of the method will also look more into further natural lan-guage processing methods, including the investigation of more complex terms and concepts (e.g., n-grams) and syn-tactical analysis including POS tagging. These variations will be investigated with respect to their usefulness for dif-ferent kinds of corpora (news, blogs, scientific publications, ...). Also, the end-user tool will be developed further to pro-vide more interactions with the corpora. are excluded). Empty cells in a row  X  X  X  denote a week with &lt; n edges.
Automatic language processing of the type presented here has a number of limitations. These concern both natural-language understanding and media reception. For example, methods that focus on words/terms, whether local or global, cannot detect negation well. Our method cannot detect possible multiple meanings of one term (homonyms), and a dictionary would be needed to conflate different terms with the same meaning (synonyms). Frequency-based inter-estingness measures like our time relevance generally sin-gle out dominant themes (or ways of reporting) and, by design, neglect outliers that may still be important. Also, the method at present has no notion of or differentiation be-tween news sources of different quality. Further method and tool developments and evaluations will address these issues. ( TR  X  5 ): ... in relation with the blood found in the car.
