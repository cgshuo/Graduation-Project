
Let D 1 and D 2 be two databases (i.e. multisets) of d strings, over an alphabet  X  , with overall length n . We study the problem of mining discriminative patterns between D 1 and D 2  X  e.g., patterns that are frequent in one database but not in the other, emerging patterns, or patterns sat-isfying other frequency-related constraints. Using the al-gorithmic framework by Hui (CPM 1992), one can solve several variants of this problem in the optimal linear time with the aid of suffix trees or suffix arrays. This stands in high contrast to other pattern domains such as item-sets or subgraphs, where super-linear lower bounds are known. However, the space requirement of existing solu-tions is O ( n log n ) bits, which is not optimal for |  X  (in particular for constant |  X  | ), as the databases themselves occupy only n log |  X  | bits.

Because in many real-life applications space is a more critical resource than time, the aim of this article is to re-duce the space, at the cost of an increased running time. In particular, we give a solution for the above problems that uses O ( n log |  X  | + d log n ) bits, while the time requirement is increased from the optimal linear time to O ( n log n ) .Our new method is tested extensively on a biologically relevant datasets and shown to be usable even on a genome-scale data.
In many applications, e.g., in computational biology, the goal is to find interesting string patterns that discriminate well between two classes of data. Application areas are, among others, finding discriminative features for sequence classification or segmentation [4], discovering new binding motifs of transcription factors [6], or computation of the classical ranking scores in Information Retrieval [3]. quency constraints, i.e., predicates over patterns depending solely on the frequency of their occurrence in the data [13]. This category encompasses combined minimum/maximum support constraints, constraints concerning emerging sub-strings, and other constraints concerning statistically signif-icant substrings. While most of these problems have their motivation in itemset mining [1], data miners also consider them for the domain of strings [8], as plenty of naturally occurring data can be modeled as strings (biological se-quences, MIDI-data, etc.).
 substring patterns, where optimal linear time algorithms can be obtained, which stands in high contrast to other pattern domains such as itemsets or sub-graphs, where super-linear lower bounds are known [33, 9]. Much of the related work in the domain of strings studies more complicated pattern classes, where the search space is typically of exponential size, and the objective is to optimize the time needed per each output element satisfyin g the frequency constraints, see e.g. [28, 2] for recent res ults on this line of research. of very large data sets, such as the genome-scale sequences of molecular biology. For such data sets, one needs to pay special attention to the space usage. Even if the algorithm takes linear space proportional to the overall length n of se-quences in the database, th is may be too much: Measured in bits, a data structure having O (1) integers per text char-acter occupies asymptotically O ( n log n ) bits, whereas the database can be stored in n log |  X  | bits, where  X  is the un-derlying alphabet. Especially on DNA sequences (where |
 X  | =4 ) this is a significant difference. ing tasks on exact substring patterns can be solved in much less space than previously known. We improve the known O ( n log n ) bits space usage into O ( n log |  X  | + d log n ) bits with a logarithmic penalty in computation time against the optimal linear time algorithm [13]. Here, d&lt; &lt;n is the number of strings in the databases. We emphasize that our algorithmic framework is general enough to handle all data mining tasks whose predicates are based on the frequency of strings alone (e.g., frequent substrings, emerging sub-strings, strings passing the  X  2 -test, ...); this approach is much more general than designing individual solutions for each of these tasks seperately.

We have also tested our method empirically on realistically-sized data sets from computational biology and shown that in practice space is reduced by a factor of 6 X 7 compared to the optimal algorithm [13], while the running time is increased by a factor of about 80 X 90. Given that users are usually willing to wait longer if they can handle larger data sets in exchange, the increase in running time is compensated by the fact that due to the use of more input data (which is nowadays available in fast-evolving domains such as computational biology), the results of the mining tasks will be more significant in practice.

Sadakane [31] gives another succinct version for calcu-lating frequencies. However, his problem setting is quite different from ours, as he designs a succinct index that al-lows to answer frequency queries for a given pattern .Our work, on the other hand, is situated in the field of data min-ing, where the goal is to extract interesting strings from sta-tistical constraints alone. B ecause Sadakane X  X  index needs
O ( n log n ) bits at construction time [31], we cannot use it for our task, as it would result in no advantage at all over the non-succinct version. Moreover, our algorithm can be modified to give a space-efficient algorithm to build a part of Sadakane X  X  succinct index.
In the following, we first give the formal definitions of the mining tasks we consider (  X  2). In  X  3, we explain the existing optimal algorithm. Then we show how the optimal algorithm can be carefully re-engineered to use less space (  X  4). The paper continues with other space/time-tradeoffs that can be obtained for the problem (  X  5), and with appli-cations to Sadakane X  X  succinct index for storing document frequencies. Experiments are reported in  X  6.
For a finite ordered alphabet  X  ,a (text) string T  X   X  is a chain T 1 ...T n of letters T i  X   X  . Here,  X  is the set of all strings over  X  . We write T i..j to denote the substring of T ranging from position i to j .Weuse | T i..j | to denote the length j  X  i +1 of T i..j . Substrings T 1 ..j , 1  X  are called prefixes and substrings T i..n , 1  X  i  X  n ,are called suffixes of T . For strings  X ,  X   X   X  we write  X   X  if  X  is a substring of  X  .Then lcp(  X ,  X  ) gives the length of the longest common prefix (lcp) of  X  and  X  .Forexam-ple, lcp( aab , abab )=1 . When clear from the context, we also use lcp for the longest common prefix itself (not its length). Given a multiset D X   X  with strings over  X  (called database ), we write |D| to denote the number of strings in D ,and D to denote their total length, i.e., D =  X   X   X  in D as follows: Note that this is not the same as counting all occurrences of a  X  in D , because one database entry could contain multiple occurrences of  X  . In data mining applications it is important to use this definition of freque ncy, as one is usually looking for patterns that are highly relevant for the whole database, and not only for one or a few of its entries.

Now the support of a pattern  X   X   X  in D can be defined as The first example problem that can be solved with our method is as follows (cf. [1]).
 Problem 1 Given m databases D 1 ,..., D m of strings over  X  , and m pairs of support thresholds (  X  i , X  i ) i =1 ,...,m isfying 0 &lt; X  i  X   X  i  X  1 for all i ,the Frequent Pattern Mining Problem is to return all strings  X   X   X  that satisfy  X   X  supp (  X , D
As another example mining problem that can be solved with our algorithm, we consider a 2-class problem for a (positive) database D 1 and a (negative) database D 2 .For this, we define the growth-rate from D 2 to D 1 of a string  X  as growth D and growth D 2  X  X  1 (  X  )=  X  otherwise. The following def-inition is motivated by the problem of mining Emerging Itemsets [10]: Problem 2 Given two databases D 1 and D 2 of strings over  X  , a support threshold  X  s ( 1 / |D 1 | X   X  s  X  1 ), and a minimum growth rate  X  g &gt; 1 ,the Emerging Substrings Mining Problem is to find all strings  X   X   X  such that supp (  X , D 1 )  X   X  s and growth D
The patterns satisfying both the support-and the growth-rate condition are called Emerging Substrings (ESs). ESs with an infinite growth-rate are called Jumping Emerging lcp-values.
 Substrings (JESs), because they are highly discriminative for the two databases. 1 Example 1 Let D 1 = { aaba , abaaab } , D 2 = { bbabb , abba } ,  X  from D 2 to D 1 are aa , aab and aba . These are also the JESs.
This section introduces two fundamental data structures that we need for our algorithm.

The suffix array SA (see [26]) for T describes the lex-icographic order of T  X  X  suffixes, in the sense that it  X  X nu-merates X  the suffixes from the smallest to the largest. More formally, SA [1 ,n ] is an array of integers s.t. its entries form a permutation of [1 : n ] ,and T SA [ i ] ..n is lexicographically less than T SA [ i +1] ..n for all 1  X  i&lt;n .The generalized suffix array of two databases D 1 = {  X  1 ,..., X  |D 1 | D 2 = {  X  1 ,..., X  |D 2 | } of strings is simply the suffix array built on top of the concatenated string The # j  X  X  are (conceptual) new characters (lexicographi-cally smaller than other characters) to separate the individ-ual strings. See Fig. 1 for an example, which builds on the databases D 1 and D 2 from Ex. 1.

The suffix array for T can be computed in O ( n ) time, ei-ther indirectly by constructing a suffix tree for T , or directly with some recent methods, e.g. [21].

Instead of suffix array SA we can use a compressed suf-fix array [27]. Different tradeoffs between space and access time to SA [ i ] are possible, e.g. one can access any SA [ i ] within time t SA = O (log n ) for an arbitrary 0 &lt;  X  1 ,us-inganindexofsize O ( n ( H 0 +loglog |  X  | )) = O ( n log bits [15, 29]. Here H 0 = H 0 ( T )  X  log |  X  | is the 0  X  X h order empirical entropy of the text T (lower bound for the average number of bits needed to code a symbol using a fixed code table).
 0 .Thatis, LCP contains the lengths of the longest com-mon prefixes of T  X  X  suffixes that are consecutive in lexico-graphic order. Kasai et al. [20] gave an algorithm to com-pute LCP in O ( n ) time. Sadakane [30] shows that repre-senting the LCP-array can be done using 2 n + o ( n ) bits (and constructed within that space), while accessing LCP [ i ] then takes O ( t SA ) time. cessing of the lcp-array such that range minimum queries (RMQs) can be answered in constant time. The reason for using RMQs on LCP is that they generalize the lcp-array, in the sense that we can compute the lcp between arbitrary suffixes, and not only between those that are lex-icographically adjacent. Formally, for two given indices i and j the query RMQ LCP ( i, j ) asks for the position of the minimum element in LCP [ i, j ] , i.e., RMQ LCP ( i, j ):= argmin k  X  X  i,...,j } { LCP [ k ] } . We return the smallest index if the minimum is not unique.
 Lemma 1 Let T  X   X  be a text and LCP be the lcp-array for T . Then for all 1  X  i&lt;j  X | T | , 1 ,j )] .
 This follows immediately from the definition of the lcp-array. Stated differently, Lemma 1 says that the i  X  X h-and the j  X  X h-smallest suffix of t are equal in exactly their LCP [ RMQ LCP ( i +1 ,j )] first characters.

It is well known that a linear preprocessing of any input array A is sufficient to find RMQ A ( i, j ) in time O (1) .This method has recently been refined to use only 2 n + o ( n ) bits in addition to the input array, also at construction time [12]. With the succinct representation of the lcp-array, we thus need 4 n + o ( n ) space to answer arbitrary lcp-queries in O ( t SA ) time.
This section reviews the basic algorithm for computing the string frequencies. It is a tight integration of Kasai et al. X  X  algorithm for visiting all branching 2 substrings of a text [20], and Hui X  X  color set size technique [18]. Note that it is enough to visit all branching substrings, as by definition the frequencies of all other strings are equal to the frequency of their longest branching prefix. From now on, let T denote the string formed from the (positive and negative) databases
D and D 2 as explained in  X  2. Let d denote the total number of strings in the databases ( d = |D 1 | + |D 2 | ), and n denote
T  X  X  length ( n = D 1 + D 2 + d ).
First, we summarize the algorithm for visiting all branch-ing substrings [20]. The idea is to simulate a depth-first traversal of the (virtual) suffix tree, solely by means of the suffix-and lcp-array. This works by visiting the leaves of the suffix tree in lexicographic order (i.e., in the order of the suffix array), keeping on a stack R just the rightmost path of the part of the suffix-tree that has been seen so far. Step i first deletes the elements from R that are removed from the rightmost path, and then inserts new elements to R .The details are as follows.

Consider step i +1 of the algorithm, so we are just about to visit suffix SA [ i +1] (see also Fig. 2). The stack R con-tains the lengths of the prefixes of T SA [ i ] ..n that are branch-ing (the so-called string-depths of nodes on the rightmost path). Then we pop all elements from R whose string-depth is greater than LCP [ i +1] , because LCP [ i +1] is the string-depth of the lowest common ancestor (lca) v of the leaves represented by SA [ i ] and SA [ i +1] .If v is not already present in R , we push it on R (with string-depth
LCP [ i +1] ). Finally, we push SA [ i +1] on R (with string-depth n  X  SA [ i +1]+1 ). It is shown in [20] that this algo-rithm visits all branching substrings of T . (The basic insight is that every internal node is the lca of at least one pair of leaves.) Figure 2. Going from suffix T SA [ i ] ..n to strings. Solid nodes are on the stack, dashed nodes not yet. v is pushed if necessary, leaf
SA [ i +1] is always pushed.
Let us now describe Hui X  X  approach [18] to calculate freq (  X , D ) for all branching substrings  X  of D .Theidea is to calculate two counters S (  X , D ) and C (  X , D ) for each  X  (simply S and C if clear from the context), such that freq = S  X  C . S counts the number of all occurrences of  X  in D ,and C counts the number of duplicates of  X  in the same string in D . More formally, S (  X , For what follows, we need to define an additional array D [1 ,n ] on top of the generalized suffix array such that D [ i ] gives the string number where suffix T SA [ i ] ..n points to, i.e. D [ i ]= j if the first string separator in T SA [ i ] ..n remembering the number h of the last string in D 1 , D also allows to infer whether a suffix points to D 1 or D 2 .
The S -counters are easy to calculate during the simu-lated suffix-tree traversal: simply initialize them correctly for leaves, and when popping a node v from the stack, add v  X  X  S -value to its parent-node on the stack. More formally, when pushing a leaf l = T SA [ i +1] ..n on R , we initialize S ( l, D 1 ) with1and S ( l, D 2 ) with0if D [ i +1]  X  h ,or vice versa if D [ i +1] &gt;h . When popping a node u from R ,weadd S ( u, D j ) to S ( w, D j ) to the direct ancestor w of u for j =1 , 2 . Note that w is either the predecessor of u on R (if the string-depth of this predecessor is  X  LCP [ i +1] ), or the newly pushed internal node v (otherwise).
 Calculation of the C -counters is a little bit more intricate. We will just describe how to calculate C (  X , D 1 ) ; the ideas for D 2 are similar. Hui X  X  key insight is that if a substring  X  is repeated within a string d  X  X  1 from D 1 ,then  X  must be a prefix of the lcp of two different suffixes from d .Evenmore, if we enumerate d  X  X  suffixes in any order for all d  X  X  1
Figure 3. Determining the C -counters. Node v represents the longest common prefix of two suffixes from the same string j ,so C [ v ] has to be increased by 1. in the order they appear in SA ), then the number of times that  X  is a prefix of the lcp of consecutive suffixes (in that order) gives C (  X , D 1 ) .

With Lemma 1, this gives us all the tools we need to cal-culate the C -counters  X  X n the fly X  while visiting all branch-ing substrings (see also Fig. 3): remember the position of the previous suffix of string j before position i for each j  X  [1 : d ] in an array P [1 ,d ] (i.e., P [ j ]=max { p i : D [ p ]= j } ). Then when at position i +1 , calculate the desired lcp as l = LCP [ RMQ LCP ( P [ D [ i +1]]+1 ,i +1)] , and increment by 1 the C -counter of the node v on R that has string-depth exactly l . (Note that such a node must be on R , as it is on the path from SA [ i +1] to the root.) The easiestwaytofind v in R is to use another array of size n .
Like with the S -counters, when popping a node from R , we need to add the C -counters to its parent node. This step takes care of the fact that the RMQs from the above para-graph just locate the longest duplicates; but all prefixes of duplicates are duplicates as well. 3.3. Determining Interesting Patterns
In total, the integration of the above two techniques im-plies that when a node v is popped from the stack, the fre-quency of the respective substring is given by freq = S  X  From this, we check if the string passes the frequency-based predicate (e.g., if it is an emerging substring). If so, we out-put T SA [ i +1]+ d  X  1 for all values d between the string-depth of v (inclusive) and that of its parent-node (exclusive).
The problem with the algorithm from the previous para-graph is that it still needs O ( n log n ) bits of space in the worst case, even if we take compressed representations of suffix-and lcp-arrays. This is because for degenerated suf-fix trees, the rightmost path could contain O ( n ) nodes from the suffix tree; hence the space for stack R and all the coun-ters would be O ( n log n ) bits. We show in this section how to achieve O ( n log |  X  | + d log n ) bits of space. 4.1. New Data Structures
We now step through the data structures from  X  3and show how to reduce the space for each of these. 4.1.1 Representing D and P We use array P as is, but we will represent array D im-plicitly. Array P occupies d log n bits. During the algo-rithm one needs to inquery P [ D [ i +1]] when inserting the ( i +1) -th suffix array element, after which one needs to up-date P [ D [ i +1]]= i +1 .Value D [ i +1] can be computed in time O ( t SA ) as follows: Store a bit-vector B that marks the boundaries of documents in the concatenation T by set-ting B [ j ]=1 if position j starts a new document in T , otherwise B [ j ]=0 . Preprocess B for rank -queries, where rank ( B, j ) gives the number of bits set in B [1 ,j ] .That is, rank ( B, j ) gives the document number in which the j -th position in T belongs to. It is possible attach to B an auxiliary structure of size o ( n ) bits so that rank ( B, j ) can be answered in constant time for any j [19]. Now we have D [ i +1]= rank ( B, SA [ i +1]) . Using compressed suffix array, the computation of SA [ i +1] takes time O ( t SA ) and the rank -query on B takes constant time. The space used in addition to the already used compressed suffix array is n + o ( n ) bits for the bit-vector B and its rank -structure. 4.1.2 Representing R Stack R needs more functionality than being accessed only from top, as in order to increase the correct C -counter as described in  X  3.2, we need to quickly find the node on R with string-depth l = LCP [ RMQ LCP ( P [ D [ i +1]]+1 ,i +1)] . Thus, storing R as a difference-encoded list in O ( n ) bits [16] would result in having to scan R in O ( n ) time in the worst case after each RMQ.

Instead, we represent R via a dynamic succinct data structure for searchable partial sums . This is a data struc-ture maintaining a sequence of symbols A = a 1 ...a m supporting the following operations:  X  sum ( A, i ) : returns  X  search ( A, j ) : returns the smallest i such that  X  update ( A, i,  X ) : adds  X  to a  X  insert ( A, i, x ) :inserts x between a  X  delete ( A, i ) : deletes a It has been shown in [25] that each of these operations can be supported in O (log n ) time, by using an extension of the data structure from [5]. The space occupied by this data structure is only n + o ( n )+ O ( m )= O ( n ) bits, provided that the sum of the numbers in A is always  X  n .

In our case, if the elements in A represent the number of letters on the incoming edges of the nodes on R (and 0 for the root of the suffix tree), then the condition m i =1 a is naturally satisfied (because the longest suffix has length n ). A query sum ( A, i ) gives the string-depth of the internal nodes (needed for popping all nodes with a string-depth  X  LCP [ i +1] ). The index (on R ) of the node with string-depth l can be found by r = search ( A, l ) .

Note that we do not need the full functionality of the dynamic searchable partial sum structure, as the function update () is not used at all, and we only have to insert and delete at the end of the sequence (corresponding to pushes and pops on R ). 4.1.3 Representing C -counters The C -counters (for counting the duplicates in a string) are also stored in a searchable partial sum data structure (see the previous section). This time, we only need the functions insert (when a new node is pushed on the stack), delete (when a node is popped), and update ( A, i,  X ) (with  X =1 when updating a r after an RMQ, or with  X  being the C -value of the node that has just been popped from the stack).
This structure needs again n + o ( n )+ O ( m )= O ( n ) bits if we can assure that that the sum of all C -counters on the stack is always less than n . But this is true, as a C -counter of u is added to its parent node if and only if u  X  X  subtree has been traversed completely. So each suffix can contribute at most1tothesetofall C -counters, hence the bound on their sum. 4.1.4 Representing S -counters The S -counters (for counting the total number of occur-rences of a string) are easier to handle, as they only need to be accessed from top of the stack. The only prop-erty we need to know is that the sum of the S -counters is never greater than n , as they  X  X over X  disjoint sub-arrays in SA . Thus, we can encode them with variable-length prefix codes, e.g., Elias- X  -code [11]. This takes again n + o ( n )+ O ( m )= O ( n ) bits, while supporting deletions and insertions at the ends in O (1) time. 4.2. Space and Time Analysis
The Compressed Suffix Array takes O ( n ( H 0 + log log |  X  | )) bits of space. Compressed LCP and RMQ val-ues take overall 4 n + o ( n ) bits of space. The database oc-cupies n log |  X  | bits. Array P takes d log n bits. Bit-vector B and its rank -structure take n + o ( n ) bits.

Interesting points are the peak space consumption of the data structures we use and their construction time. The Compressed Suffix Array can be constructed using O ( n log |  X  | ) bits of space in O ( n log log |  X  | or even within space of the final structure [23] but us-ing O ( n log n ) time. Once the Compressed Suffix Ar-ray is given, the lcp-representation can be constructed in O ( n log n ) time using no extra space in addition to the fi-nal structure [16]. Then, given the Compressed Suffix Array and the compressed lcp-repres entation, the linear time algo-rithm to construct the RMQ structure [12] takes O ( n log n ) time using no extra space.
 During the main algorithm, we also allocate space for R , C and S . This space is bounded by O ( n ) bits as analyzed earlier. The algorithm makes O ( n ) queries and updates to the data structures. The most costly operations are the searches on R and updates on C which both take O (log n ) time.
 Theorem 1 There is an algorithm for determining all F strings satisfying a frequency-based predicate in O ( n log |  X  | + d log n ) bit of space and O ( n log n ) time. Writing the output takes additional O ( | F | log n + F ) time, where 0 &lt;  X  1 affects the constant of the space usage and F is the total length of the output. 5.1. Less Space, More Time
Other tradeoffs are possible in Theorem 1 by using dif-ferent variants of compressed suffix arrays. It is possible to obtain nH k + o ( n log |  X  | )+ d log n bits of space with the running time increasing to O ( n log n (1 + log |  X  | log log n k = o (log |  X  | n ) and H k is the order-k entropy of This is achieved by using the FM-index variant in [25, 14], and building on top of it the additional structures needed for the full functionality of compressed suffix arrays as in [32]. In this case, the text is not required to be stored at all (after the construction), as the structure is self-index and supports displaying any text substring of length in O (( +log 1+  X  n )log |  X  | ) time. That is, outputting the result of the algorithm in Theorem 1 takes in this case O ( | F | log 1+  X  n log |  X  | + F log |  X  | ) time, for any  X &gt; 0 affecting the constants of the sub-linear structures. 5.2. Application to Storing Document Fre-
Sadakane [31, Section 5.2] gives a succinct index struc-ture that stores the document frequencies, i.e. values S [ i ]
C [ i ] that we compute on-the-fly in our algorithm. His struc-ture consists basically of the compressed suffix array and a unary coding of the frequency values in the inorder of the virtual suffix tree. Sadakane shows that the final structure occupies | CSA | +2 n + o ( n ) bits, where | CSA | is the size of the compressed suffix array used. He does not give a space-efficient construction algorithm (a suffix tree is used as an intermediate stru cture, hence taking O ( n log n ) bits).
We can construct the required unary coding during our algorithm as follows: We maintain the balanced parenthe-ses (BP) representation as in [32] using a dynamic bit-vector occupying n + o ( n ) bits. This gives us the preorder of the virtual suffix tree nodes at each step. We use another dy-namic bit-vector to store the C -counter values in the same unary coding as Sadakane uses. Using rank and select ( select ( i ) returns the position of the i  X  X h 1 [19]) on both bit-vectors gives us a mapping between BP bit-vector and the C -counter bit-vector. Inserting a new node in BP means inserting 1 in the corresponding place of the C -counter bit-vector. Incrementing a C -counter works by finding the cor-responding node in BP, mapping the position to C -counter bit-vector, and inserting 0 there. Finally, after all values are computed, the preorder of C -counter bit-vector can be turned into the inorder used i n Sadakane X  X  scheme, and the intermediate structures can be deleted. The algorithm uses the same space and time as reported in Theorem 1.
We have implemented the algorithm from  X  4inC++ 3 and compared it to the C++-implementation of the optimal algorithm from  X  3. Our implementation deviates from our theoretical proposal as we use a compressed suffix array that is based on sampling . We use a standard sampling rate of log n that minimize the space us age, as this is the main ob-jective of our approach. However, we will also see that a smaller (fixed) sampling rate d rastically decreases the ex-ecution time, while leading only to a moderate increase in space usage. We present test results of time and maximum memory usage for different datasets of protein and genome data.
We used two datasets consisting of the primary struc-ture of all protein data from human and mouse, which were obtained from Swissprot using the keywords HUMAN and MOUSE in the NEWT taxonomy browser. The hu-man dataset ( D 1 ) contained 71,622 proteins of total length  X  27.3MB, and the mouse dataset ( D 2 ) contained 62,562 proteins of total length  X  26.3MB. It is interesting to com-pare these data sets because the genome of human and mouse is known to be largely the same, but (given the dif-ferent phenotype of the two species!) must contain some significant differences.
 tium 4 CPU with 3GB of main memory. All output was redirected to the  X  X ull X -device in order to remove influences from secondary storage units.
 strings (Probl. 1) and emerging substrings (Probl. 2). The results for the frequent substring mining are reported in Tbl. 1. Here, the maximum support threshold  X  for D 2 was fixed to . 95 . The more striking propert y of the space-efficient al-gorithm is that it uses only 183.1MB of memory, while the optimal algorithm needs 1,267.3MB. This means a space reduction of a factor of  X  6 . 9 . As already mentioned in the introduction, space is often a more critical resource than time; e.g., users are often willing to wait 3 X 4 hours instead of 3 minutes, if this allows them to apply their methods to much bigger data sets.
 reported in Tbl. 2 (for  X  g =1 . 3333 ) and Tbl. 3 (for  X  2 . 0 ). The results are largely along the lines of the frequent string mining tasks: space is reduced by a factor of 6 X 7, and the running time is increased by about two orders of magnitude.
 tation time and space usage, we tested mining frequent sub-strings from different size prefixes of the protein data. Tests were run on dataset prefixes of total length 2 X 50MB and on the whole dataset of length  X  53.6MB. Here, the maxi-
Table 2. Running time s (seconds) for mining emerging strings for the optimal (  X  3) and the space-efficient (  X  4) algorithm.  X  s is the min-imum support threshold. The growth rate  X  g washeldfixedat 1 . 3333 . The last column de-notes the size of the output.
Table 3. Running time s (seconds) for mining emerging strings for the optimal (  X  3) and the space-efficient (  X  4) algorithm.  X  s is the mini-mum support threshold. The growth rate  X  g was held fixed at 2 . 0 . The last column de-notes the size of the output. mum support threshold  X  was fixed to . 95 and the minimum threshold  X  to . 40 . Maximum memory usage for the opti-mal (  X  3) and the space-efficient (  X  4) algorithm on different size datasets are reported in Fig. 4. Running times for the same datasets are shown in Fig. 5. Both figures show also a time-space tradeoff for the sp ace-efficent algorithm by us-ing a fixed samplerate ( =3 ) inside CSA  X  we see that a modest increase in memory consumption (Fig. 4) decreases the running time by one order of magnitude (Fig. 5)!
Inspired by this, we tested v arious time-space tradeoffs for the space-efficient algorithm by using a denser sampling inside CSA. The test was run on the whole protein dataset with the same parameteres as above. The samplerate of
CSA was given values between 3 X 25. The resulting time and space usage for each samplerate is reported in Fig. 6.
Note that the default samplerate used by the algorithm is log n ( =25 for the whole dataset). We thus conclude that in practice we should choose a denser sampling.
Figure 4. Maximum memory usage while min-ing frequent substrings from protein data for the optimal (  X  3) and the space-efficient (  X  4) algorithm. A time-space tradeoff with dense sampling is shown for the space-efficent al-gorithm.
To show that the space-efficient algorithm works also on genome-scale data, we used a DNA dataset of 22 human chromosomes (build NCBI34) of total length 2.9 billion base pairs. We measured time and maximum memory us-age to solve the frequent substrings and emerging substrings problems. Time to output the result X  X  substrings was ex-cluded from these test results. Tests were run on a 3.0GHz Intel Xeon CPU with 128GB of main memory.

The space-efficient algorithm used 39.8 hours and re-quired maximum of 9.3GB of memory for mining the whole genome. With a different time-space tradeoff (by denser sampling inside CSA) we achieved a running time of only 22.6 hours with 14.9GB of maximum memory usage.

The implementation of the optimal-time algorithm was tuned for 32 bit word length which is not enough for genome-scale data. However, we can estimate the time and space requirements from tests with 9 small chromosomes (a quarter of the whole genome). For this small portion of the genome, the optimal time algorithm used  X  17.3 minutes and required maximum of  X  13.1GB of heap. This suggests that genome-scale mining would require about an hour of time and about 50GB of memory. Note that the integer ar-
Figure 5. Running times for frequent sub-string mining with the optimal (  X  3) and the space-efficient (  X  4) algorithm on a logarith-mic scale. A time-space tradeoff with dense sampling is shown for the space-efficent al-gorithm. rays for SA , LCP and C -counters already require 3 n log n bytes of memory (  X  32.0GB for the whole genome).
Let us briefly describe other algorithms for mining sub-strings. Algorithms VST [8] and FAVST [24] rely on a data structure called Version Space Tree . Because this tree is basically a suffix trie with O ( n 2 ) nodes in the worst case, these algorithms suffer from high memory requirement. In practice, we could not test these algorithms on any of the datasets above, as they are only applicable to input sizes up to several hundred kilobytes.

Chan et al. [7] presented an algorithm for the emerging substrings problem, but we could not find an implementa-tion of their approach. Furthermore, no information about the practical memory requirement is reported. The algo-rithm itself is based on a merged suffix tree with O ( n ) nodes that suggests a space requirement of O ( n log n ) bits. It has already been shown in a previous study [13] that suffix-array based methods are superior to those built on suffix trees or tries both in terms of time and space.

Two very recent improvements [22, 34] of the origi-nal linear-time algorithm [13] also addresses the problem
Figure 6. Time-space tradeoff for the space-effient algorithm with samplerates 3 X 25 on protein data. Decreasing the samplerate in-creases memory usage but makes our algo-rithm significantly faster. of lowering the space consumption; however, they only achieve about half the space of the orignal algorithm. Fur-thermore, their theoretical space guarantee is no better than the O ( n log n ) bits of the original solution. Nevertheless, it must be said that due to the simplicity of these two algo-rithms [22, 34] they work faster in practice.
We hence conclude from the experiments that the prac-tical performance of the two algorithms are in accordance with their theoretical guarantees. However, we must also say that the constants involved in the time-performance of the succinct data structures (  X  4) are still large. Thanks to the discussions with Luis Russo after the Workshop on Compression, Text, and Algorithms, orga-nized by Gonzalo Navarro at University of Chile, Novem-ber, 2007, we were able to improve our result significantly.
