 In many real world problems, the proportion of data points in different classes is highly skewed: some classes dominate the data set (majority classes), and the remaining classes may have only a normal transactions, and yet once we identify a new type of fraud transaction, we are well on our way to stopping similar future fraud transactions [2]. Another example is in astronomy. Most of the objects in sky survey images are explainable by current theories and models. Only 0.001% of category detection is also a bottleneck in reducing the sampling complexity of active learning [1, compact clusters, while in outlier detection, the outliers are typically scattered. Currently, only a few methods have been proposed to address this challenge. For example, in [8], the authors assumed a mixture model to fit the data, and selected examples for labeling according overlap, which affects negatively the performance of these methods.
 We typically start de-novo , no category labels, though our algorithm makes no such assumption. local density around each example. In each iteration, the algorithm selects an example with the maximum change in local density on a certain scale, and asks the oracle for its label. The method stops once it has found at least one example from each class (given the knowledge of the number of classes). When the minority classes form compact clusters and the majority class distribution is locally smooth, the method will select examples both on the boundary and in the interior of the and real data sets show the superiority of our method over existing methods.
 The rest of the paper is organized as follows. In Section 2, we introduce our method and provide imental results. Finally, we conclude the paper in Section 4. 2.1 Problem definition which corresponds to y i = 1 , and all the other classes are minority classes. 2.2 Rare category detection for the binary case First let us focus on the simplest case where m = 2 , and Pr[ y i = 1]  X  Pr[ y i = 2] = p , i.e. method for rare category detection based on nearest neighbors, which is presented in Algorithm 1. The basic idea is to find maximum changes in local density, which might indicate the location of a rare category.
 first estimate the number K of minority class examples in S . Then, for each example, we record its distance from the K th nearest neighbor, which could be realized by kd-trees [7]. The minimum n maximum value be the score of x i . The example with the maximum score is selected for labeling neighborhood where the scores of the examples are re-calculated and continue.
 chance of finding a minority class example. 2.3 Correctness where minority class examples occur with high probability.
 To be precise, we make the following assumptions. Algorithm 1 Nearest-Neighbor-Based Rare Category Detection for the Binary Case (NNDB) Require: S , p 3: for t = 1 : n do 4:  X  x i  X  S , if x i has not been selected, then s i = max 5: Query x = arg max x i  X  S s i . 6: If the label of x is 2, break. 7: end for Assumptions With the above assumptions, we have the following claim and theorem. Note that variants of the Claim 1.  X   X ,  X  &gt; 0 , if n  X  max { 1 2  X  r 0  X  r and | with radius r 0 .
 max
Pr[ r 0 &gt; r or r 0 &lt;  X  Pr[ r 0 &gt; r ] + Pr[ r 0 &lt;  X  Pr[ | NN ( b, r ) | &lt; K ] + Pr[max = Pr[ | where the last inequality is based on Hoeffding bound.
 n  X  1 Based on Claim 1, we get the following theorem, which shows the effectiveness of the proposed method.
 Main Theorem. If then with probability at least 1  X   X  , after d 2  X  r Note that  X  is always bigger than r . Based on the above inequalities, we have may be even smaller.
 k x In Step 4 of the proposed method, we gradually enlarge the neighborhood to calculate the change of s be equal to  X  , the value of t would be d  X  r 0 e X d 2  X  r To this end, we need to calculate the maximum probability mass of the majority class within B 2 . Therefore, the probability mass of the majority class within B 2 is:  X  2.4 Rare category detection for multiple classes from each class.
 The method proposed in subsection 2.2 can be easily generalized to multiple classes, which is pre-value in the same manner as NNDB. Then, we calculate the local density at each example based on of Step 11, we gradually enlarge the neighborhood to calculate the score of each example. This is the same as NNDB, except that we preclude the examples that are within a certain distance of any the same discovered class. The inner loop stops when we find an example from an undiscovered class. Then we will update the r 0 value and resume the inner loop. If the minority classes form compact clusters and are far apart from each other, NNDM is able to detect examples from each minority class with a small number of label requests.
 Algorithm 2 Nearest-Neighbor-Based Rare Category Detection for Multiple Classes (NNDM) Require: S , p 2 , . . . , p m 1: for i = 2 : m do 2: Let K i = np i . 3: For each example, calculate the distance between this example and its K th i nearest neighbor. 4: end for 6: for i = 1 : m do 8: end for 9: while not all the classes have been discovered do 11: for t = 1 : n do 12: for each x i that has been selected and labeled y i ,  X  x  X  S , s.t. k x  X  x i k  X  r 0 y 13: Query x = arg max x i  X  S s i . 14: If x belongs to a class that has not been discovered, break. 15: end for 16: end while In NNDB and NNDM, we need the priors of the minority classes as the input. As we will see in the next section, our algorithms are robust against small perturbations in the priors. In this section, we compare our methods (NNDB and NNDM) with the best method proposed in [8] (Interleave) and random sampling (RS) on both synthetic and real data sets. In Interleave, we use the number of classes as the number of components in the mixture model. For both Interleave and RS, we run the experiment multiple times and report the average results. 3.1 Synthetic data sets the minority class is uniform within a small hyper-ball. There are 1000 examples from the majority using RS, we need to label 101 examples, and using NNDB, we only need to label 3 examples in order to sample one from the minority class, which are denoted as  X  X  X  in Figure 1(b). Notice that the first 2 examples that NNDB selects are not from the correct region. This is because the number of examples from the minority class is very small, and the local density may be affected by the randomness in the data. the four characters  X  X IPS X  correspond to four minority classes, which consist of 138, 79, 118, and 206 examples respectively. Using Interleave, we need to label 1190 examples, using RS, we need to label 83 examples, and using NNDM, we only need to label 5 examples in order to get one from Interleave is even worse than RS. This might be because some minority classes are located in the majority-class mixture-model component. 3.2 Real data sets The first data set consists of 4177 examples, described by 7 dimensional features. The examples a smaller data set with 4515 examples, described by 9 dimensional features. The examples come is 0.13%.
 The comparison results are shown in Figure 3(a) and Figure 3(b) respectively. From these figures, all the classes, Interleave needs 280 label requests, RS needs 483 label requests, and NNDM only requests, RS needs 512 label requests, and NNDM only needs 87 label requests. This is because as the number of components becomes larger, the mixture model generated by Interleave is less reliable due to the lack of labeled examples, thus we need to select more examples. Furthermore, the other hand, NNDM does not assume a generative model for the data, and only focuses on the change in local density, which is more effective on the two data sets. 3.3 Imprecise priors the robustness of NNDM against modest mis-estimations of the class priors. The performance of NNDB is similar to NNDM, so we omit the results here. In the experiments, we use the same data sets as in subsection 3.2, and add/subtract 5%, 10%, and 20% from the true priors of the minority to small perturbations in the priors. For example, with Abalone data set, if we subtract 10% from the true priors, only one more label request is needed in order to find all the classes. with the maximum change in local density, and depending on scaling, it will select class-boundary over existing methods is demonstrated by extensive experimental results on both synthetic and real data sets. Moreover, it is very robust to modest perturbations in estimating true class priors. This paper is based on work in part supported by the Defense Advanced Research Projects Agency (DARPA) under contract number NBCHD030010.
 [3] C. Blake and C. Merz. Uci repository of machine learning databases. In [4] P. Brazdil and J. Gama. Statlog repository. In [5] S. Dasgupta. Coarse sample complexity bounds for active learning. In Advances in Neural [6] S. Fine and Y. Mansour. Active sampling for multiple output identification. In The 19th Annual [7] A. Moore. A tutorial on kd-trees. Technical report, University of Cambridge Computer Labo-[8] D. Pelleg and A. Moore. Active learning for anomaly and rare-category detection. In Advances
