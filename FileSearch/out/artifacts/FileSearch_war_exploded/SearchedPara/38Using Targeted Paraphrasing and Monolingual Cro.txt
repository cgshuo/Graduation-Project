 PHILIP RESNIK, OLIVIA BUZEK, YAKOV KRONROD, CHANG HU, ALEXANDER J. QUINN, and BENJAMIN B. BEDERSON , University of Maryland For most of the world X  X  languages, the availability of translation is limited to two pos-sibilities: high quality at high cost, via professional translators, and low quality at low cost, via Machine Translation (MT). The spectrum between these two extremes is very poorly populated, and at any point on the spectrum the ready availability of translation is limited to only a small fraction of the world X  X  languages. There is, of course, a long history of technological assistance to translators, improving cost effec-tiveness using translation memory [Laurian 1984; Bowker and Barlow 2004] or other interactive tools to assist translators [Esteban et al. 2004; Khadivi et al. 2006]. And there is a recent and rapidly growing interest in crowdsourcing with nonprofessional translators, which can sometimes be remarkably effective [Huberdeau et al. 2008; Munro 2010; Hester et al. 2010; Meedan 2011; Twitter 2011; TED 2011; Facebook 2011; 99Translations 2011; GetLocalization 2011; Zaidan and Callison-Burch 2011]. However, all these alternatives face a central availability bottleneck: they require the participation of humans with bilingual expertise.

In this article, we report on a new exploration of the middle ground, taking advantage of a virtually unutilized resource: speakers of the source and target language who are ef-fectively monolingual , that is, who each only know one of the two languages relevant for the translation task. The solution we are proposing has the potential to provide a more cost-effective approach to translation in scenarios where machine translation would be considered acceptable to use, if only it were generally of high enough quality. This would clearly exclude tasks like translation of medical reports, business contracts, or literary works, where the validation of a qualified bilingual translator is absolutely necessary. However, it does include a great many real-world scenarios, such as following news reports in another country, reading international comments about a product, or gener-ating a decent first draft translation of a Wikipedia page for Wikipedia editors to im-prove. It also has the potential to vastly reduce the burden of human effort for use cases in which bilingual translators post-edit machine translation output [Guerra 2003].
We call the technique described here targeted paraphrasing . 1 In a nutshell, target-language monolinguals identify parts of an initial machine translation that don X  X  ap-pear to be right, and source-language monolinguals provide the MT system with alter-native phrasings that might lead to better translations; these are then passed through MT again and the best scoring hypothesis is selected as the final translation.
The use of monolingual participants in a human-machine translation process is not entirely new. Callison-Burch et al. [2004] pioneered the exploration of monolingual post-editing within the MT community, an approach extended more recently to provide richer information to the user by Albrecht et al. [2009] and Koehn [2009]. Shahaf and Horvitz [2010] use machine translation as a specific instance of a general game-based framework for combining a range of machine and human capabilities. There have also been at least two independently developed human-machine translation frameworks that employ an iterative protocol involving monolinguals on both the source and target side. In the first of these, Morita and Ishida [2009] describe a system in which target and source language speakers perform editing of MT output to improve fluency and adequacy, respectively; they utilize source-side paraphrasing at a course-grain level, although their approach is limited to requests to paraphrase the entire sentence when the translation cannot be understood. The second of these provides the broader context for the work reported here: we have independently developed a protocol similar in spirit to that of Morita and Ishida, in which cross-language communication is enhanced by metalinguistic communication in the user interface [Bederson et al. 2010; Hu et al. 2011]. The technique we describe in this article can be viewed as compatible with the richer protocol-and game-based approaches, but it is considerably simpler.
In Sections 2 through 5 we describe our method and present evaluation results on Chinese-English translation. Unlike other work on translation using monolingual human participants, the technique we present here also offers clear opportunities to replace human participation with machine components if the latter are up to the task; we present promising results along these lines in Section 6. In Section 7 we briefly summarize the collaborative translation protocol that provides the broader context for this work, including evaluation results [Hu et al. 2011], before wrapping up in Section 8 with conclusions and directions for future work. The starting point for the work we report on in this article is an observation: the source sentence provided as input to an MT system is just one of many ways in which the meaning could have been expressed, and for any given MT system, some forms of expression are easier to translate than others. The same basic observation has been applied quite fruitfully over the past several years to deal with statistical MT challenges involving segmentation, morphological analysis, and more recently, source language word order [Dyer 2007; Dyer et al. 2008, 2010; Dyer and Resnik 2010]. For example, Figure 1 shows a source translation lattice that contains alternative segmentations of a Chinese input sentence. A decoder enabled to handle lattice input, for example, Dyer et al. [2010], can exploit inputs of this kind by identifying the path through the lattice that leads to the best scoring translation hypothesis, making subsentential choices as to which segmentation best contributes to a good hypothesis score. For example, given the input in the figure, a translation model lacking a good translation for the first Chinese word in its entirety (spanning nodes 0 to 4) could instead choose to traverse arcs 0 X 1, 1 X 2, and 2 X 4, taking advantage of better translation possibilities for the smaller translation units.

Here we apply the same core idea X  X roviding a wider range of subsentential alter-natives for source language phrases X  X ot at the level of segmentation of morphological analysis, but to the surface expression of meaning. For example, consider the following real example of translation from English to French by an automatic MT system.  X  Source. Polls indicate Brown, a state senator, and Coakley, Massachusetts X  Attorney
General, are locked in a virtual tie to fill the late Sen. Ted Kennedy X  X  Senate seat.  X  System. Les sondages indiquent Brown ,uns  X  enateur d X   X  etat, et Coakley, Massachu -s  X  enateur Ted Kennedy X  X  si ` ege au S  X  enat.
 A French speaker can look at this automatic translation and see immediately that the underlined parts are wrong, even without knowing the intended source meaning. We can identify the spans in the source English sentence that are responsible for these badly translated French spans, and change them to alternative expressions with the same meaning (e.g., changing Massachusetts X  Attorney General to the Attorney General of Massachusetts ); if we do so and then use the same MT system again, we obtain a translation that is still imperfect (e.g., cravate means necktie), but is more acceptable.  X  System. Les sondages indiquent que Brown, un s  X  enateur d X   X  etat, et Coakley, le pro-cureur g  X  en  X  eral du Massachusetts, sont enferm  X  es dans une cravate virtuel ` a pourvoir le si  X  ege au S  X  enat de Sen. Ted Kennedy, qui est d  X  ec  X  ed  X  er  X  ecemment.
Operationally, then, translation with targeted paraphrasing includes the following steps (Figure 2).
 Initial machine translation. For this article, we use the Google Translate Research API, which, among other advantages, provides word-level alignments between the source text and its output. In principle, however, any automatic translation system can be used in this role, potentially at some cost to quality, by performing post hoc target-to-source alignment.

Identification of mistranslated spans. This step identifies parts of the source sen-tence that lead to ungrammatical, nonsensical, or apparently incorrect translations on the target side. In the experiments of Sections 3 and 4, this step is performed by having monolingual target speakers identify likely error spans on the target side, as in the French example before, and projecting those spans back to the source spans that generated them using word alignments as the bridge [Hwa et al. 2005; Yarowsky et al. 2001]. In Section 6, we describe a heuristic but effective method for performing this fully automatically. Du et al. [2010] explore the use of source paraphrases without targeting apparent mistranslations, using lattice translation [Dyer et al. 2008] to effi-ciently represent and decode the resulting very large space of paraphrase alternatives.
Source paraphrase generation. This step generates alternative expressions for the source spans identified in the previous step. In this article, it is performed by mono-lingual source speakers who perform the paraphrase task: the speaker is given a sentence with a phrase span marked, and is asked to replace the marked text with a different way of saying the same thing, so that the resulting sentence still makes sense and means the same thing as the original sentence. To illustrate in English, someone seeing John and Mary took a European vacation this summer might supply the paraphrase Mary went on a European , verifying that the resulting John and Mary went on a European vacation this summer preserves the original meaning. This step can also be fully automated [Max 2009] by taking advantage of bilingual phrase-table pivoting [Bannard and Callison-Burch 2005]; see Max [2010] for a related approach in which the paraphrases of a source phrase are used to refine the estimated probability distribution over its possible target phrases.

Generating sentential source paraphrases. For each sentence, there may be multiple paraphrased spans. These are multiplied out to provide full-sentence paraphrases. For example, if two nonoverlapping source spans are each paraphrased in three ways, we generate 9 sentential source paraphrases, each of which represents an alternative way of expressing the original sentence. 2
Machine translation of alternative sentences. The alternative source sentences, pro-duced via paraphrase, are sent through the same MT system.

Hypothesis selection. A single-best translation hypothesis is selected, for example, on the basis of the translation system X  X  model score. In principle, one could also combine the alternatives into a lattice representation and decode to find the best path using lattice translation [Dyer et al. 2008]; see Du et al. [2010]. One can also present trans-lation alternatives to a target speaker for selection, similarly to Callison-Burch et al. [2004].

Notice that with the exception of the initial translation, each remaining step in this pipeline can involve either human participation or fully automatic processing. The targeted paraphrasing framework therefore defines a rich set of intermediate points on the spectrum between fully automatic and fully human translation, of which we explore only a few in this article. In order to assess the potential of our approach, we conducted a small pilot study, using eleven sentences in simplified Chinese selected from the article on  X  X ater X  in Chinese Wikipedia (http://zh.wikipedia.org/zh-cn/%E6%B0%B4). This article was cho-sen because its topic is well known in both English-speaking and Chinese-speaking populations. The first five sentences were taken from the first paragraph of the article. The other six sentences were taken from a randomly chosen paragraph in the article. As a preprocessing step, we removed any parenthetical items from the input sentences, such as  X (H 2 0) X . The shortest sentence in this set has 12 Chinese characters, the longest has 54. 3
Human participation in this task was accomplished using Amazon Mechanical Turk, an online marketplace that enables human performance of small Human Intelligence Tasks (HITs) in return for micropayments. For each sentence, after we translated it automatically (using Google Translate), three English-speaking Mechanical Turk workers ( X  X urkers X ) on the target side performed identification of mistranslated spans. Each span identified was projected back to its corresponding source span, and three Chinese-speaking Turkers were asked to provide paraphrases of each source span. These tasks were easy to perform (no more than around 30 seconds to complete on average) and inexpensive (less than $1 for the entire pilot study). 4 The Chinese source span paraphrases were then used to construct full-sentence paraphrases, which were retranslated, once again by Google Translate, to produce the output of the targeted paraphrasing translation process.

The initial translation outputs from Google Translate (GT) and the results of the tar-geted paraphrasing translation process (TP) were evaluated according to widely used criteria of fluency and adequacy. 5 Fluency ratings were obtained on a 5-point scale from three native English speakers without knowledge of Chinese. Translation ade-quacy ratings were obtained from three native Chinese speakers who are also fluent in English; they assessed adequacy of English sentences by comparing the communicated meaning to the Chinese source sentences.

Fluency was rated on the following scale. Adequacy was rated on the following scale. For each GT output, we averaged across the ratings of the alternative TP to produce average TP fluency and adequacy scores. The average GT output ratings, measuring the pure machine translation baseline, were 2.36 for fluency and 2.91 for adequacy. Averaging across the TP outputs, these rose to 3.32 and 3.49, respectively. One could argue that a more sensible evaluation is not to average across alternative TP outputs, but rather to simulate the behavior of a target-language speaker who simply chooses the one translation among the alternatives that seems most fluent. If we select the most fluent TP output for each source sentence according to the English speakers X  average fluency ratings, we obtain average test set ratings of 3.58 for fluency and 3.73 for adequacy. These are respective gains of 0.82 and 1.21 over the baseline initial MT output, each on a 5-point scale (Figure 3).
 Figure 4 shows a selection of outputs: we present the two cases where the most fluent TP alternative shows the greatest gain in average fluency rating (best gain + 2.67); two cases near the median gain in average fluency (median + 1); and the worst two cases with respect to effect on average fluency rating (worst  X  0.33). The table accurately conveys a qualitative impression corresponding to the quantitative results: the overall quality of translations appears to be improved by our process consistently, despite the absence of any bilingual input in the improvements. Encouraged by our pilot study, we conducted a more extensive evaluation using Chinese-English test data taken from the NIST MT X 08 machine translation evalua-tion, in order to obtain fully automatic translation evaluation scores. The NIST MT X 08 test set contains 1,357 test sentences. These underwent the same targeted paraphras-ing process as in the pilot study, with the addition of a basic step to filter out cheaters: we disregarded as invalid any responses consisting purely of ASCII characters (signi-fying a non-Chinese response) or responses that were identical to the original source text. (The issue of cheating is discussed further in Section 5.) Target English speakers identified 4292 potential mistranslation spans, or 3.17 spans per sentence, that yielded at least one source paraphrase on the source Chinese side. Chinese speakers provided 1513 valid paraphrases. This process produced 649 sentences with both valid iden-tification of error spans and valid proposals for source-side paraphrase alternatives. The entire cost for the human tasks in this experiment was $408.386, or a bit under $0.30 per sentence in the test set on average. 6
In order to get a sense of improvements and how much improvement is possible, using the proposed technique, we computed preliminary results on a subset of 49 sentences obtained before the full crowdsourcing process was complete. Figure 5 summarizes an evaluation in standard fashion using the BLEU evaluation metric [Papineni et al. 2002], evaluating against the four English MT X 08 references for each Chinese sentence and using one-best output from Google Translate (GT) as the baseline.

Since the targeted paraphrasing translation process (TP) produces multiple hypotheses X  X ne automatic translation output per sentential paraphrase X  X e selected the one-best output for each sentence by selecting the highest scoring English transla-tion, according to the translation score delivered with each output by the Google Trans-late Research API. (The original translation was, of course, included among the candi-dates for selection.) This achieves an improvement of 1.68 BLEU points over baseline on the 49-sentence test set (listed as Targeted paraphrasing (TP) one-best in the figure).
One could argue that this outcome is simply a result of having more hypotheses to choose from, not a result of the targeted paraphrasing process itself. In order to rule out this possibility, we generated ( n + 1)-best Google translations, setting n for each sentence to match the number of alternative translations generated via targeted paraphrasing. We then chose the best translation for each sentence, among the ( n + 1)-best Google hypotheses, via oracle selection, using the TERp metric [Snover et al. 2009] to score each hypothesis in the ( n + 1)-best list against the reference translations. 7 The resulting BLEU score for the full set showed negligible improvement (Google Translate (GT) n-best oracle). 8 We did a similar oracle-best calculation for targeted paraphrasing (Targeted Paraphrasing (TP) oracle). Here, instead of picking the best targeted paraphrasing output via the Google Translate score, as we did for Targeted Paraphrasing (TP) one-best, we evaluated those outputs against the reference translations using TERp [Snover et al. 2009], picking the one with the best score as the oracle translation, exactly as was done for Google Translate (GT) n-best oracle. The result shows a potential gain of 2.46 BLEU points over the baseline, selecting the oracle translation in this way.
In addition to aggregate evaluation using BLEU, we also looked at oracle results on a per-sentence basis using TERp (since BLEU is known to be more appropriate to use at document level, not sentence level). Identifying the best sentential paraphrase alternative using TERp as an oracle, we find that the TERp score would improve for 32 of the 49 test sentences, 65.3%. For those 32 sentences, the average gain is 8.36 TERp points. 9 A fairer measure is the average obtained when scoring zero gain for the 17 sentences where no improvement was obtained; taking these into account, that is, assuming an oracle who chooses the original translation if none of the paraphrase-based alternatives is better, the average improvement over the entire set of 49 sentences is 5.46 TERp points.

Finally, the last line in Figure 5 shows a human upper bound computed using the reference translations via cross-validation; that is, for each of the four reference trans-lations, we evaluate it as a hypothesized translation using the other three references as ground truth; these four scores are then averaged. The value of this upper bound is quite consistent with a bound computed similarly by Callison-Burch [2009].
This preliminary evaluation on a small subset of the full NIST MT X 08 test set con-firmed the qualitative impressions in Figure 4 and the subjective ratings results ob-tained in our pilot study in Section 3. We then moved on to a similar analysis for the full 649-sentence set.

Figure 6 presents the key results. Targeted paraphrase yielded an average improve-ment of + 1.6 BLEU. The table breaks out these improvements by the number of error spans identified per sentence on the target side as likely to contain errors. We conjec-tured that there might be a  X  X weet spot X  for the number of segments of a sentence to be paraphrased. This appears, anecdotally, to be the case, given that the 79 sentences with 4 error spans each seem to have outperformed the other sets by a substantial gain in BLEU score. In terms of the quantifiable improvements obtained for the various sentence sets, it is worth noting that gains of roughly + 0.6 BLEU or higher tend to be considered meaningful by MT researchers, and gains of + 1.5 BLEU are generally considered substantial.

Our oracle results establish that by taking advantage of monolingual human speak-ers, it is possible to obtain quite substantial gains in translation quality. Figure 7 provides a qualitative sense of how the targeted paraphrase results differ from the auto-matic MT output. The TP one-best results demonstrate that the majority of that oracle gain is obtained in automatic hypothesis selection, simply by selecting the paraphrase-based alternative translation with the highest translation score. In the previous analysis, we saw that our approach leads to promising improvements to the baseline Google translations, even in the absence of human bilingual expertise. However, two issues merited further investigation. First, in the full version of the study, we failed to obtain substantial gain over Google n-best translation for the larger set of sentences. This is problematic, since one would hope that the gains of the approach would not depend simply on having a larger number of hypotheses available to consider, but rather that they would reflect the specific value of generating these alternatives via targeted paraphrase. Second, and related, was the question of why our overall gain in quality was not even larger, since typically one would view an oracle evaluation as the upper bound on expected improvements if a process were to be deployed in the real world. Here we provide some additional analysis focused on these questions.
Figure 8 illustrates the problem. If we look in more detail at the results of the full study, we see that even though we had very substantial gains against the baseline (1-best Google Translate output), our performance against the Google n-best oracle was variable. In the table, the original results in Figure 6, namely the deltas of our approach against the 1-best Google Translation baseline, are labeled Orig  X  Para, and we now explicitly show the deltas in comparison with Google Translation n -best output as Nbest  X  Para. As before, these are grouped into bins by the number of target-side error spans identified for the sentence, that is, the number of potential paraphrases to be done on the source side. As the table shows, comparisons with Google X  X  n-best translation output yields improvements in only two out of the five subsets of sentences, with a significant decrease of  X  1.3 BLEU in one of the bins.

Based on a consideration of our overall approach, we hypothesized that the potential of our method might be suffering noise in the system, in the form of poor source-side paraphrases. Quality assurance is a well-known problem in the crowdsourcing community, and our task, like any other, is susceptible to both cheating and to people attempting the task in good faith but simply doing a poor job. This hypothesis led us to a useful analysis of paraphrase quality and its effects, followed by initial investigation of an automatic method for mitigating the problem.

To assess paraphrase quality, we conducted a study on Amazon Mechanical Turk (MTurk) in which we had workers judge the source paraphrases that had been collected in the prior experiment, on a scale from 1 to 5, using the same basic definition of adequacy that is used for studying preservation of meaning in machine translation evaluation. Crucially, the judgments were contextual. That is, to evaluate the quality of paraphrase p for an original p , the worker was shown the original sentence  X  p  X  and the sentence  X  p  X  with the paraphrase substituted into the identical context, and asked the extent to which the latter, as a full sentence, had the same meaning as the former. 10 It is important to emphasize that this form of quality control is another task that requires only monolingual expertise, on the source side.

We collected multiple in-context judgements for every paraphrase. 11 We found that roughly half of all original paraphrases were poor quality, not reflecting the meaning of the reference sentence according to independent judgments (average ratings  X  2 on the 5-point scale). While this reflects the risks of working with Mechanical Turk, it also highlights how easy it is to implement human quality control in the service.
The natural question to ask next is what happens when paraphrases are restricted to cases with reasonable quality, that is, what the results would look like if we added human quality control to Mechanical Turk, or, by the same token, how they would look if we used an alternative approach to crowdsourcing in which participants could be relied upon to be working in good faith. 12 We therefore did an analysis similar to the preceding, but based only on the subset of sentences containing error spans for which at least one paraphrase was found to be of reasonable quality (mean rating  X  3), and throwing away paraphrases of lower quality for those sentences.

Considering this quality-controlled set of sentences, we find that the number of error spans per sentence is drastically reduced, and the number of sentences with more than five paraphrases is essentially negligible. The plurality of sentences now only contain exactly one error span per sentence. Figure 9 shows the results in the quality-controlled case.

As the table shows, we now see much stronger improvements over the Google Trans-lation 1-best baseline, as well as a sharp increase in improvement over Google Trans-lation n-best results. Particularly interesting is the + 1.8 BLEU improvement over original for the 340 sentences with only one error span and the + 1.4 BLEU improve-ment over the Google n-best for the same set. Equally important, the approach now also demonstrates meaningful gains over Google Translation, + 0.72 BLEU points, even when the comparison is with Google X  X  n-best rather than 1-best output.
 These new results directly address both issues raised at the beginning of this section. The analysis demonstrates a much stronger improvement over the real-world Google Translation 1-best baseline, and it also shows that the method based on targeted paraphrase is doing more than simply exploiting selection from a larger number of translation hypotheses.

These results are very encouraging on their own, and lead to an additional follow-up question: is it possible to perform the quality control step automatically, rather than relying on source-side input from monolinguals?
To address the question, we first conducted an analysis to see whether human judg-ments of quality of paraphrases actually correspond to gains in the TERp score, that is, gains in translation performance, for individual sentences. The results are thoroughly reassuring: we found that the judgments correlate with improvement in performance with overwhelming statistical significance ( p &lt; 1 e  X  10). This enabled us to be confi-dent that our Mechanical Turk task of paraphrase evaluation was actually leading to removal of poor paraphrases that were detracting from our performance.

Next, we explored the possibility of making this distinction automatically. To this end, we developed six very simple heuristics that can be evaluated easily for each paraphrase. These are described briefly with reference to Figure 10.  X  partialCopy . This feature identifies whether the provided paraphrase is just a partial copy of the original error span. In other words, it identifies paraphrases that are substrings of the error span on which they are based. An example can be seen in
Figure 10 where the paraphrase is just the first three characters of the 5-character error span.  X  rearrange. This feature indicates whether the paraphrase consists of the same char-acters as the error span, but in a different order. The example in Figure 10 shows a paraphrase that is just the reverse of the marked error span.  X  othercopy. This feature is similar to the partialCopy feature, but it identifies para-phrases that are copies of parts of the original sentence not covered by the error span. The example in Figure 10 shows a situation where the paraphrase is a copy of the 5th character to the left of the final ellipsis.  X  superset. This feature identifies paraphrases that contain the entire error span as well as padding additional characters either before or after it in the original sentence.
This is a common form of cheating, for our Mechanical Turk HITs, since people could just copy and paste a segment surrounding the error span. The example in Figure 10 shows a paraphrase that is just the error span itself, plus 4 preceding characters and 2 following characters in the original sentence.  X  english. This feature identifies sentences where the Chinese paraphrase included
English (either by itself, or mixed in with the Chinese). Even though there are situations where this is not necessarily cheating, it is a possible indication that someone has proposed a poor paraphrase. An example is found in Figure 10.  X  sizeDiff. The  X  X izeDiff X  feature is a continuous variable that represents the ratio of the paraphrase X  X  length to the length of the error span. We conjecture that values diverging significantly from 1.0 represent inaccurate or improper paraphrases. Ex-amples of four situations, ranging from a sixfold expansion to a sixfold reduction, are shown in Figure 10.
 We created a linear model (multiple linear regression, implemented in R), predicting the TERp improvement from the six heuristics. Five of the six were significant predictors, with three significant at p &lt; 0 . 001, and all significant predictors correlated in the expected direction (i.e., the presence of one of the binary heuristics, and an increase in the ratio of original to paraphrase, all lead to a decrease in the performance of the sentence with the paraphrase included). The multiple correlation is significant at p &lt; 1 e  X  15.

As a first foray into automatic quality control, we used the linear model in a predictive fashion (on items not used to create the model) to determine whether or not a proposed paraphrase should be considered unacceptable. Thresholding the value predicted by the linear model at 0.1, we reject 492 paraphrases, with 465 of them being correct positive rejections. In terms of an accuracy/coverage trade-off, this corresponds to 94.5% accuracy on determinations made on 9.8% of the total data. Applied to the data in our experiment, this would yield a roughly 20% automated removal of poor-quality paraphrases from the system with no human involvement. We are optimistic that with more intricate heuristics and a deeper insight into patters of paraphrasing that represent both cheating and potential improvement, even greater automatic gains will be possible. As we noted in Section 2, the targeted paraphrasing translation process defines a set of human-machine combinations that do not require bilingual expertise. Sections 4 and 5 described human identification of mistranslated spans on the target side, human generation of paraphrases for problematic subsentential spans on the source side, and both automatic hypothesis selection and human selection (via fluency ratings, in Section 3). Human and automatic quality control for paraphrasing was explored in Section 5.

In this section, we take a step toward automating the central piece of the process involving human targeting, by replacing human identification of mistranslated spans with a fully automatic method. 13 The idea behind our automatic error identification is straightforward: if the source sentence is translated to the target and then back-translated, a comparison of the result with the original is likely to identify places where the translation process encountered difficulty. 14 Briefly, we automatically translate source F to target E, then back-translate to produce F X  in the source language. We compare F and F X  using TERp X  X hich, in addition to its use as an evaluation metric, is a form of string-edit distance that identifies various categories of differences between two sentences. When at least two consecutive edits are found, we flag their smallest containing syntactic constituent as a potential source of translation difficulty. 15
In more detail, we posit that if an area of back-translation F X  has many edits rel-ative to original sentence F, then that area probably comes from parts of the target translation that did not represent the desired meaning in F very well. We only consider consecutive edits in certain of the TERp edit categories, specifically, Deletions (D), In-sertions (I), and Shifts (S); the two remaining categories, Matches (M) and Paraphrases (P), indicate that the words are identical or that the original meaning was preserved. Furthermore, we assume that while a single D, S, or I edit might be fairly meaning-less, a string of at least two of these types of edits is likely to represent a substantive problem in the translation.

In order to identify reasonably meaningful paraphrase units based on potential errors, we rely on a source language constituency parser. Using the parse, we find the smallest constituent of the sentence containing all of the tokens in a particular error string. At times, these constituents can be quite large, even the entire sentence. To weed out these cases, we restrict constituent length to no more than 7 tokens.
For example, given the following, spans in the italicized phrase in F would be iden-tified, based on the TERp alignment and smallest containing constituent, as shown in Figure 11. F: The most recent probe to visit Jupiter was the Pluto-bound New Horizons spacecraft in late February 2007.
 sonda New Horizons a fines de febrero de 2007.
 F X : The latest research visit Jupiter was the Pluto-bound New Horizons spacecraft in late February 2007.

In order to evaluate this approach, we again use NIST MT08 data, this time going in the English-to-Chinese direction since we are assuming source language resources not currently available for Chinese. 16 We used English reference 0 as the source sentence, and the original Chinese sentence as the target. 17
The dataset contains 1,357 sentence pairs. Using the earlier described algorithm to automatically identify possible problem areas in the translation, with the Google Translate API providing both the translation and back-translation, we generated 1,780 potential error spans in 1,006 of the sentences, and, continuing the targeted paraphras-ing process, we obtained up to three source paraphrases per span, for the problemantic spans in 1,000 of those sentences. (For six sentences, no paraphrases were suggested for any of the problematic spans.) These yielded full-sentence paraphrase alternatives for the 1,000 sentences, which we again evaluated via an oracle study.
 For this study we used the TER metric [Snover et al. 2006] rather than TERp. Comparing with the GT output, we find that TP yields a better-translated paraphrase sentence in 313 of the 1000 cases, or 31.3%, and for those 313 cases, TER for the oracle-best paraphrase alternative improves on the TER for the original sentence by 12.16 TER points. Also taking into account the cases where there is no improvement over the baseline, the average TER score improves by 3.8 points. The cost for human tasks in this study X  X ust paraphrases, since identifying problematic spans was done automatically X  X as $117.48, or a bit under $0.12 per sentence. In this article, we introduce and evaluate the idea of targeted paraphrasing in isolation. However, as noted in the Introduction, it is also consistent with a context for crowd-sourced translation in which error targeting and paraphrasing are parts of a broader collaborative protocol involving crowds of effectively monolingual users on the source and target side. In Hu et al. [2011], we report on experimentation using a collaborative interface that permits a wider variety of monolingual tasks. Here we briefly summa-rize that approach, since it provides a broader context for the work, as well as the key results presented there.
 The protocol presented in Hu et al. [2011] is illustrated schematically in Figure 12. Monolingual tasks on the target side include not only error span detection, as in this article, but also voting for preferred hypotheses (i.e., human hypothesis selection) and manual creation of new translation candidates (in the same spirit as post-editing). On the source side, monolingual tasks include not only subsentential paraphrasing based on targeted error spans, as in this article, but also voting (based on back-translation of target hypotheses) and  X  X xplaining X  error spans by manually annotating them, for example, with images (via Google image search) and URLs (for example, links into Wikipedia).
 This protocol was tested using translation of children X  X  books between Spanish and German as the task X  X  specific instance of the problem that originally motivated this line of research, namely the real-world need for a cost-effective way to translate literally thousands of books in the International Children X  X  Digital Library across more than 50 languages [Hourcade et al. 2003].

Figure 13 illustrates dramatic improvements in fluency and adequacy (as judged by bilingual evaluators) for the output of the collaborative protocol. As a single figure of merit, the original output of Google Translate produced correct results for only 10% of 162 sentences, aggregating across several books, and this number improved to 68% after the collaborative protocol.  X  X orrect X  was defined quite conservatively here, as ratings of 5 for both fluency and adequacy by both of two independent bilingual evaluators. In this article we have focused on a relatively less-explored space on the spectrum between high-quality and low-cost translation: sharing the burden of the translation task among a fully automatic system and monolingual human participants, without requiring human bilingual expertise. The monolingual participants in our framework perform straightforward tasks: they identify parts of sentences in their language that seem to have errors, they provide subsentential paraphrases in context, and they judge the fluency of sentences they are presented with (or, in a variant still to be explored, they simply select which target sentence they like the best). Unlike other proposals for exploiting monolingual speakers in human-machine collaborative translation, the human steps here are amenable to automation: in addition to evaluating a mostly human variant of our targeted paraphrasing translation framework, we also assessed a version in which the identification of mistranslated spans to be paraphrased is done automatically. Our experimentation yielded a consistent pattern of results, supporting, via several different measures, the conclusion that targeted paraphrasing can lead to significant improvements in translation.

These initial studies leave considerable room for future work. One important step will be to better characterize the relationship between cost and quality in quantitative terms: how much does it cost to obtain how much quality improvement, and how does that compare with typical professional translation costs? Since our experimentation thus far has been done at a small scale and without  X  X roduction quality X  software, we believe that the approaches we are pursuing could certainly be done both faster and less expensively. But even so, our current results are promising: average cost for the human tasks in our experimentation was under $0.30 per sentence in the test set, and in our experience, professional bilingual translators typically charge $0.15 to $0.25 per word. We did not measure per-sentence translation latency, since our experimentation was based on batch translation, but translating a 1,357-sentence test set took two to three days in an experiment where time was not emphasized and where the real-world setting permits nearly arbitrary degrees of parallelism. This can be compared with a typical turnaround of at least one to two days when hiring professional translators for even modestly sized jobs. Exploring these questions at scale will involve a compli-cated ecosystem of workers and cheaters, tasks, and motivations and incentives [Quinn and Bederson 2011]. Zaidan and Callison-Burch [2011] provide detailed discussion of cost and quality with a focus on bilingual translation crowdsourcing, and we have re-cently begun a collaboration with Callison-Burch to explore issues of monolingual and bilingual translation crowdsourcing within a unified framework.

A related crowdsourcing issue requiring further study is the availability of mono-lingual human participants for a range of language pairs, in order to validate the argument that drawing on monolingual human participation will significantly reduce the severity of the availability bottleneck. And, of course, in the upper bound in Table V makes quite clear the crucial value added by bilingual translators, when they are avail-able; we hope to explore whether the targeted paraphrasing translation pipeline can improve the productivity of post-editing by bilinguals, making it easier to move toward the upper bound in a cost-effective way.

Another set of issues concerns the underlying translation technology. The value of the approach taken here is likely to vary depending upon the quality of the underly-ing translation system, and the approach may break down at the extrema, when the baseline translation is either already very good or completely awful. We chose to use Google Translate for its wide availability and the fact that it represents a state-of-the-art baseline to beat; however, in future work we plan to substitute a statistical MT system to which we have developer-level access, such as cdec [Dyer et al. 2010], which will permit us to experiment across a range of translation model and language model LM training set sizes, and therefore to vary quality while keeping other system details constant.

More directly connected to research in machine translation, this framework provides a variety of opportunities for advancing the state-of-the-art by combining human and machine components in flexible ways. As one example, the human feedback we are obtaining can provide information about the kinds and distribution of errors in the machine translation system X  X  output; as another, a statistical analysis of manually annotated mistranslation spans could help to identify source-side properties of input spans that are likely mistranslated. Errors could be analyzed according to a taxonomy of translation error types like the one introduced by Vilar et al. [2006], which might contribute valuable data for automatic detection of mistranslations. 18 Our framework also makes it possible to compare human paraphrases with those obtained by auto-matic methods (e.g., Bannard and Callison-Burch [2005], Callison-Burch et al. [2006], Callison-Burch [2008], and Marton et al. [2009]) on a potentially large scale, which may help improve both our own collaborative translation process and also the state-of-the-art in automatic paraphrasing. More generally, any component in Figure 2 that is represented by a rectangle can be a task for either humans or machines, which means that any such component can serve both as a source of data for evaluation and devel-opment of automated methods and as a testbed for those methods. This leads quite naturally to a fully automated pipeline, using algorithms for error span detection (e.g., Section 6), automatic source-side paraphrasing (e.g., Bannard and Callison-Burch and other references cited before), and translation of targeted paraphrase lattices (e.g., Max [2010] and Du et al. [2010]). We plan to implement a fully automatic pipeline of this kind. Finally, we intend to explore the application of our approach in scenarios involv-ing less-common languages, by using a more common language as a pivot or bridge [Habash and Hu 2009].
