 For implementing content ma nagement solutions and en-abling new applications associated with data retention, reg-ulatory compliance, and litigation issues, enterprises need to develop advanced analytics to uncover relationships among the documents, e.g., content similarity, provenance, and clus-tering. In this paper, we evaluate the performance of four syntactic similarity algorithms. Three algorithms are based on Broder X  X   X  X hingling X  technique while the fourth algorithm employs a more recent approach,  X  X ontent-based chunking X . For our experiments, we use a specially designed corpus of documents that includes a set of  X  X imilar X  documents with a controlled number of modifications. Our performance study reveals that the similarity metric of all four algorithms is highly sensitive to settings of the algorithms X  parameters: sliding window size and fingerprint sampling frequency. We identify a useful range of these parameters for achieving good practical results, and compare the performance of the four algorithms in a controlled environment. We validate our re-sults by applying these algorithms to finding near-duplicates in two large collections of HP technical support documents. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; H.3.4 [ Systems and Software ]: Performance evaluation (efficiency and effectiveness) Algorithms, Documentation, Management, Performance Syntactic similarity, document management, performance.
The explosion of electronic documents, along with new regulations and new trends in litigation discovery, require enterprises to rethink their information management strate-gies. Managing the enterprise X  X  unstructured information presents many challenges. There is increasing pressure to provide greater visibility into enterprise information for en-abling different new applications, e.g., e-discovery or de-creasing corporate risk from regulatory non-compliance. In particular, when compliance rules say you can (have to) delete some files, how do you identify all the users who might have a copy of these files? Often, this problem is further complicated due to the possibility that there could be many different versions of the same files (earlier or later drafts, edits of the final documents) and the same files could be (slightly) modified/altered/edited by the users over time. So, the problem transforms into one of identifying all the users who have files that are  X  X imilar X  to a given one.
The focus during the e-discovery process is on identifying relevant data with high precision and recall. In many cases, once a critical document is found then the next stage is to identify and retrieve a complete set of related documents: all earlier or later versions of the same document. Often the research prototype of a s uccessful commercial product is developed under some project  X  X ode X  name. Therefore, in the enterprise archives, there could be different sets of versioned (similar) documents that reflect such project name changes and evolution of details during the project lifecycle. Failure to include a set of documents because they used a different code name could be disastrous during litigation, again making robust discovery of similar documents critical to the enterprise.

Another issue in enterprise information management is keeping document repositories with up-to-date information. Many enterprises have large collections of technical support documents, manuals, white papers and knowledge briefs on different topics. It is essential to have such knowledge stored in electronic form so that it can be searched and shared by professionals who need it. Yet inevitably these collections start suffering from different versions added over time, with older documents describing the obsolete solutions, or refer-encing discontinued products. Furthermore, duplicates may occur when a newly created document includes large por-tions from existing documents . As a part of enterprise con-tent management, it is important to identify and filter out the documents that are largely duplicates of newer versions in order to improve the quality of the collection. Solving these information management problems requires establish-ing the relationships that exist within the unstructured in-formation and techniques to exploit these relationships, e.g., content similarity, provenance, and clustering.
 The problem of document similarity is not a new problem. The question is which of the already existing approaches and algorithms might be appropriate for solving document sim-ilarity in the enterprise environment? The problem of find-ing near-duplicate web pages was an area of active research in the recent past. Since near-duplicate web pages create problems for web search engines, a variety of algorithms for detecting these pages was proposed in the literature [15, 10, 19, 2, 3, 8]. A similar problem was addressed in the context of Digital Libraries [18]. The similarity method is called syntactical similarity if it is entirely based on the syntactic properties of the documents rather than on their semantics. A familiar method in this group is the diff utility in UNIX. For designing an efficient solution, Broder et al. [4] used a shingling technique, which characterizes a document via a set of k -words sequences of adjacent words. Instead of using a full set of fingerprinted shingles for document compari-son, an unbiased deterministic sampling technique is used to select a subset of shingles for creating a small, yet repre-sentative file signature.

Another popular technique for identifying similar files is called  X  X ompare-by-hash X  X 20] or content-based chunking [16, 9]. Originally it was proposed to take advantage of the fact that applications frequently read or write data that is iden-tical to already existing data. To exploit these inter-file sim-ilarities, the file is divided into variable-size chunks, which are indexed by their hash value. This way, one can quickly and with high probability determine whether the contents of two files are similar by comparing only their chunk hashes and not their contents. This technique can be also classified as a syntactic similarity approach since it is entirely defined by the syntactic properties of the document.

In this paper, we compare four popular syntactic simi-larity algorithms: three algorithms are based on the  X  X hin-gling X  technique [2, 3], and the fourth algorithm is defined via  X  X ontent-based chunking X  [16, 9]. We define these algo-rithms in a unified manner using two parameters :a sliding window -that represents a contiguous sequence of bytes, and a sampling frequency -that defines which of these windows are included in the compact representation of a file, called afile signature . We observe that often these algorithms are used with very different parameter settings [4, 7, 8, 12]. The question is how important the choice of sliding window size and sampling frequency for the algorithm efficiency? How sensitive is the similarity metric to the different values of these parameters?
For our experiments, we use a specially designed corpus of documents that includes a set of  X  X imilar X  documents with a controlled number of modifications. Our performance study reveals that a similarity metric and performance of all four algorithms is highly sensitive to different values of the sliding window size and sampling frequency parameters. We iden-tify a useful range of these parameters for achieving good practical results. We discuss potential strengths and weak-nesses of each algorithm in the enterprise environment, and conclude our study by applying these algorithms to finding similar documents in two large collections of HP technical support documents. The remainder of the paper presents our results in more detail.  X  Document Similarity Definition Two documents are considered to be similar if they are  X  X oughly the same X , i.e., they have the same content except for minor modifications, edits, or formatting. In the same way, we can define that one document is  X  X oughly contained X  within another one. To capture the intuitive definition of  X  X oughly the same X  and  X  X oughly contained X  A. Broder [2] suggested the mathematical concepts of resemblance and containment based on the shingling technique. Under this approach each document is represented by the set of con-tiguous terms (where each term is a word) or shingles ,and then two documents are compared by the number of match-ing shingles. For a given document D its w-shingling S w ( D ) is defined as the subset of all unique shingles of size w con-tained in D (if w is fixed we use denotation S ( D ) instead of S w ( D )). Then for a given shingle size, the resemblance or similarity of two documents A and B is defined as
The containment of A in B is defined as  X  Shingling-Based Approach Rather than comparing shingles directly, it is more conve-nient to deal with fingerprints of shingles. Typically, 64-bit Rabin fingerprints [17] are used for this purpose since they have a very fast software implementation.

Furthermore, in order to reduce the number of compar-isons that are needed for computing the document X  X  sim-ilarity and containment as defined above, a few optimiza-tion techniques were proposed to approximate sim ( A, B ) and cont ( A, B ). They sample the set of all the shingles and build a relatively small document signature ,andthen compare these signatures. There are different ways one can sample the set of all documents shingles that result in the following three alternative similarity algorithms:  X  Content-Based Chunking Approach Recently, a new approach to define file similarity using content-based chunking has appeared. Content-based chunking, as introduced in [16, 15], is a way of breaking a file into a se-quence of chunks so that chunk boundaries are determined by the local content of the file.

The Basic Sliding Window (BSW) algorithm [16] is one of the initial content-based chunking algorithms. This algo-rithm works as follows. A sliding window of fixed width w is moved across the file, and at every position k in the file, the fingerprint, F k , of the contents of this window is com-puted using 64-bit Rabin X  X  fingerprints [17]. k is a chunk boundary if F k mod n =0. Thevalueof n determines the average size of the chunk, e.g., for n = 100 the algorithm produces variable-size chunks with an average chunk size of around 100 bytes. After that the  X  X ompare-by-hash X  X ethod is used to compare the chunks occurring in different files [11]. Typically, the MD5 algorithm that produces a 64 bit hash is sufficient in practice. The rationale for using the content-based chunking algorithm is that a small, local modification impacts at most two chunks surrounding the modification, and the remaining chunks would stay the same.

However, from the definition above it is apparent that it might work poorly when there are small distributed ed-its/modifications in a file. The issue is that a simple hash of the chunk is not resilient to small changes within the chunk like the addition or deletion of the word  X  X he X , a stem change to the term, etc. For a relatively small file, say 2 KB, even if we use  X  100-byte chunking (i.e., n = 100) we would have  X  20 chunks. If there are 10 tiny edits to the document, they potentially may impact more than 50% of the existing chunks (and their corresponding hashes). To avoid this po-tential drawback, we introduce a slightly modified version of the basic sliding window algorithm below.
Summary: While all the four algorithms are applicable for solving a similarity problem, not all the algorithms can approximate the containment of two documents. When doc-ument B is contained in A (say, B is the first section of a 4-section document A ) then the n smallest fingerprints over sub-document B (algorithm Min n ) might have nothing in commonwiththe n smallest fingerprints of the entire docu-ment A . The same applies to the Sketch n algorithm. Table 1 summarizes the properties of the four algorithms. Table 1: Summary of the algorithm properties.

Our intent is to compare the performance of the four al-gorithms described above. The original Min n , Mod n and Sketch n algorithms are defined using w -shingles, i.e., the set of w contiguous terms (words), while the BSW n algorithm uses as a unit a sliding window of w -bytes. To allow a fair comparison among all the algorithms, we use a sliding win-dow of w -bytes in place of the w -shingle for the remainder of the paper.  X  Overall Process of Finding Similar Files There are two essential steps in the process of finding similar files in a file collection.

The first step is to compute a file signature (according to any of the four similarity algorithms described above).
The second step is to compare file signatures for common entries and report only clusters of those files whose signature intersection is above a given similarity threshold. This step can be done in the same fashion for different file signatures and is well described in literature [4, 15, 9]. We skip a detailed description of this step due to lack of space.
In order to fairly compare the four algorithms introduced in the previous section, we created a special environment for our experiments.

First, we selected 100 different HPLabs technical reports from 2007 (http://www.hpl.hp.com/techreports/2007/) and converted them to a text format. This collection, called Research Corpus (abbreviation RC orig ), has documents of different length as shown in Figure 1. We ordered the doc-uments by their length: a higher Document ID represents a longer document. As shown in Figure 1, our collection has documents ranging from 9 KB to 540 KB. Figure 1: The document size distribution in RC orig .
Second, we wrote a program that can introduce modifica-tions to the documents from our collection in a controlled way. For example, it can either add or remove words to/from the document a predefined number of times. These modi-fications can be done in a random fashion or be uniformly spread through the document. The intent is to compare the four selected algorithms by using a document collection for which we have full knowledge about a number and type of modifications present in the documents.

We started with the simplest modification type: our pro-gram creates near-duplicates for each original document from RC orig by inserting  X  X   X  1 into the document 1, 2, ..., 50 times following a random distribution. Thus, the created collection RC i a (1  X  i  X  50) denotes the collection where for each original document there is a near-duplicate with i -times randomly inserted  X  X   X  into the original document. For this modification type we know the  X  X dit distance X  2 between the original and the modified document.

Let D k and D k ((1  X  k  X  100) be the original document and its near-duplicate with a controlled number of modifica-tions i in the collection RC i a . We will compute the average similarity metric for RC i a as follows (remember, that there are 100 documents in the original collection RC orig ):
Our goal is to compare the average similarity metric re-ported by the four algorithms for the created collections of near-duplicates as a function of the algorithm X  X  parameters and the introduced modifications. In addition, we would like to understand the correlation of the reported similarity metric and the document size, i.e., whether these algorithms perform equally well on short and long documents.
The four algorithms introduced in Section 2 have two ex-plicit parameters: For the algorithms Min n and Sketch n the document signa-ture is a fixed-size feature vector with n entries.
For the algorithms Mod n and BSW n the parameter n defines the frequency of fingerprint sampling, and therefore the number of entries in the document signature. The Mod n technique selects the fingerprints whose value modulo n is zero. For BSW n n works in a similar way: first, it iden-tifies the fingerprints whose value modulo n is zero as a chunk boundary, and then the numerically smallest finger-print within this chunk is selected for inclusion into the doc-ument signature.

There are performance studies that use these algorithms with widely different parameter settings. For example, in [4], 10-word shingles are used: this is approximately equiv-alent to an 80-100 byte sliding window. In other studies [7, 8], the authors use 5-word shingles which is approximately equivalent to a 40-50 bytes sliding window, while in [12], 6-word shingles are used.

There is a similar situation with respect to sampling fre-quency n that defines the number of entries that constitute a document signature. In [4], the authors use n =25for the Mod n algorithm. In [7, 8, 12], the authors use n =84 for the Sketch n algorithm, where each function f i is a 64-bit Rabin fingerprinting function. The 84 fingerprinters use different primitive polynomials of degree 64. Therefore for this implementation of Sketch n the parameter n  X  84. In our implementation of Sketch n ,weuseBobJenkins X  X amily of hash functions [14] for speed and ease of parametrization of n to any positive value.

Question: How important is the choice of sliding window size w and sampling frequency n ? How sensitive is the sim-ilarity metric to different values used for these parameters?
The next two sub-sections aim to answer these and related questions.
The sliding window size defines the granularity for ob-serving and sampling the contiguous terms (bytes) in the document. Intuitively, a smaller window introduces docu-ment sampling at the level of words, while a larger window produces sampling with a size closer to the sentence level. To understand the impact of the sliding window size in the four similarity algorithms under study, we use the document collections RC i a ,1  X  i  X  50 as described in Section 3.
We compute the average similarity metric aver sim ( RC i a (1  X  i  X  50) under the four algorithms with different values for the sliding window: window = 5, 10, 20, 40, ..., 100 bytes.
Figure 2 shows these results for the Mod n and BSW n algorithms. (Since all the algorithms are very sensitive to the sliding window size and the sensitivity trends are similar, we omit graphs for Min n and Sketch n due to lack of space). (a) (b)
The x-axis reflects the number of changes i (1  X  i  X  50) in the document collection under test, i.e., RC i a .They-axis shows the average similarity metric aver sim ( RC i a )(seea formal definition in Section 3). Multiple lines in the figure show aver sim ( RC i a ) with different values of sliding window. First of all, the average similarity metric for a collection RC i a decreases as a higher number of modifications are intro-duced in each document. Moreover, for a larger sliding win-dow and increasing number of modifications in the created near-duplicates, the similarity metric is much smaller than under a smaller sliding window. For example, for Mod 100 al-gorithm and window = 100 bytes, aver sim ( RC 50 a ) = 66%, while under window = 5 bytes, the average similarity metric is much higher: aver sim ( RC 50 a ) = 97%. The trends for the BSW 400 algorithm are the same.

Does this mean that window = 5 bytes is a good parameter for using in the similarity algorithms?
To make a definite conclusion, we investigate the impact of a window size on the similarity metric for the original research corpus RC orig , where all the 100 documents are truly different. How much overlap or  X  X alse positive X  sim-ilarity information is observed in these distinct documents while varying the sliding window size?
Figure 3 show these results for the Mod n and BSW n al-gorithms. This figure shows the CDF (Cumulative Distribu-tion Function) of the similarity metric for all the document pairs in the RC orig under different window sizes (it only shows the CDF of pairs with non-zero, positive similarity metric). We can see that algorithm Mod n with window = 5 bytes identified more than 5% of the pairs having similar-ity above 20%, with some documents exhibiting nearly 30% similarity. Moreover, practically any pair of distinct docu-ments does exhibit some overlap under window =5,10bytes as shown in Figure 3. Increasing sliding window size helps to reduce the amount of overlap measured in distinct doc-uments. In summary, if the window size is too small then there are too many  X  X imilar X  fingerprints in different (dis-tinct) documents. We can see that window =10bytesis also a bad parameter. The same observations are valid for BSW n algorithm shown in Figure 3(b). We omit figures for algorithms Min n and Sketch n because they show the same trend. (a) (b) Figure 3: CDF of the similarity metric for all the doc-
We choose window = 20 bytes as a good trade-off for achieving high similarity metric for near-duplicates (Fig-ure 2) while having a low amount of  X  X alse positive X  simi-larity information for truly distinct documents (Figure 3).
Finally, Figure 4 shows CDF of the similarity metric for all the document pairs in the RC orig under window =20 and the four similarity algorithms Min n , Mod n , Sketch n ,and BSW n . Figure 4: CDF of similarity metric in RC orig with win-
An interesting observation is that while algorithms Mod n and BSW n operate under a similar sampling frequency in this example, and both algorithms produce a variable-size document signature, algorithm Mod 100 has  X  X dentified X  a significantly higher number of  X  X lightly-similar X  documents: 28% of pairs are less that 4% similar under Mod 100 while only 10% of such pairs are similar under BSW 100 .These numbers are important for the second stage of the similarity algorithms when the clusters of similar documents are built. High percentages of  X  X lightly-similar X  documents lead to a higher computational complexity at this step.
The sampling frequency parameter n has a different def-inition in the similarity algorithms Min n , Mod n , Sketch and BSW n . For example, the algorithms Min n and Sketch n define a fixed, n -size signature for document characteriza-tion (see Section 2 for details). The algorithms Mod n and BSW n produce a variable length document signature, where the number of fingerprints in the signature is proportional to the document length and dependent on n .Thequestion is whether the choice of a sampling frequency n has a sig-nificant impact on the similarity results.

Intuitively, having only a few samples to represent the entire document might be dangerous for two (opposite) rea-sons. Let documents A and B have quite a few similar por-tions but overall be quite different. If there are only a few fingerprint samples to represent A and B , then these sam-ples might easily  X  X and X  on the similar portions of the doc-uments, and hence, report a high similarity for these docu-ments while missing their difference. For the same reason, the opposite might happen: if these few fingerprint samples land on the modified portions of A and B then the same pair of documents A and B might be reported as having almost no similarity while missing that these documents do have similar portions.

Figure 5 shows aver sim ( RC i a ), 1  X  i  X  50, for Sketch and BSW n algorithms with different sampling frequency. Sketch 10 represents the document by using 10-feature vec-tor, while Sketch 100 by using 100-feature vector. At first glance, the sampling frequency does not have much impact on the average similarity metric shown in Figure 5 (a). (a) (b) Figure 5: Impact of sampling frequency on average sim-
BSW 1000 algorithm produces variable-size chunks (with the average chunk size around 1000 bytes), while BSW 100 defines a finer granularity chunking with the average chunk size around 100 bytes. Given a 10 KB document, BSW 1000 will produce a document signature with approximately 10 en-tries, while BSW 100 will have  X  100 entries. Again, at first glance, this difference does not seem to have much impact on the average similarity metric shown in Figure 5 (b).
Now, let us have a closer look at the similarity of the orig-inal and near-duplicate documents in the collection RC 50 under different sampling frequency parameters. Remember, the original document and its near-duplicate in RC 50 a are differentiated by the random insertion of  X  X   X  in 50 places in the document. Figure 6 shows these results. The simi-larity metric reported by Sketch 100 is much more consistent (a) (b) Figure 6: Similarity of the original and near-duplicate than the similarity metric reported by Sketch 10 for the same pairs of documents. In many cases, Sketch 10 misses all dif-ferences in the pairs of near-duplicates and reports 100% similarity values as shown in Figure 6 (a). At the same time, we can see that there are situations when Sketch 10 ports much lower similarity metric compared to Sketch 100 because these few sampled fingerprints  X  X and X  on the modi-fied portions of the near-duplicate documents. Figure 6 (b) shows the same trend for BSW 1000 and BSW 100 .Using a coarser chunking leads to the same high variance in the similarity results.

Note that for shorter documents (remember, that docu-ments with smaller ID represent shorter documents) the im-pact of the sampling frequency in the presence of significant number of  X  X mall X  modifications is especially pronounced. Due to lack of space we omit graphs for Min n and Mod n which exhibit similar trends.
In this section, we compare the performance and accu-racy of the four similarity algorithms. Using guidance from Sections 4.1 and 4.2 on the sensitivity of the algorithms to sliding window size and frequency sampling, we choose win-dow =20bytes,and n =100 for Min n and Sketch n .

Note, that Min 100 and Sketch 100 produce a fixed 100-feature vector for any document in the set, while Mod n and BSW n generate a feature vector that is proportional to the document size: the rule of thumb is that if document D is S
D bytes long then its feature vector has on average S D /n entries. Since the average file size in the research corpus is  X  50 KB, we choose n =500 for Mod n and BSW n .This way the average feature vectors of different algorithms under study are approximately the same (for the Research Corpus used in our experiments).

Figure 7(a) shows the average similarity metric for RC i a (1  X  i  X  50) under the four algorithms Mod 500 , Min 100 , Sketch 100 ,and BSW 500 . For a small number of changes in the near-duplicate documents (less than 10) the average similarity metric reported by all four algorithms is nearly the same. Then with an increasing number of changes in the near-duplicate documents the reported average similar-ity metrics under different algorithms start to diverge as shown in Figure 7(a). In order to verify the validity of these trends, we performed additional experiments where the near-duplicates are generated according to different mod-ifications of the original documents. (a) (b) Figure 7: Average similarity metric under the four sim-
Figure 7(b) shows the average similarity metric computed for a case when near-duplicates are generated by a random removal of a word from the original document (1 to 50 times accordingly). While the values of average similarity metric are slightly lower (due to a higher  X  X dit X -distance between the original and the modified documents), the trends of Fig-ures 7(a) and (b) are the same. (In fact, we performed other experiments with a different distribution of changes. Uni-formly distributed changes have a stronger negative impact on the similarity metric, while we still observe similar trends between the algorithms).

The summary of Figure 7 is that for the same set of near-duplicate documents Mod n and Min n report on average a higher (more accurate) similarity metric than algorithms Sketch n and BSW n .

To analyze the divergence of similarity metrics under dif-ferent algorithms in more detail, we plotted a similarity met-ric for pairs of original and modified documents under dif-ferent number of modifications as shown in Figure 8. While unfortunately these graphs are  X  X ery busy X  they convey well the overall trend. When the modified document has a rel-atively small number of changes (less than 10), the perfor-mance of all the algorithms is alike across different document sizes as shown in Figures 8 (a). However, under a higher number of changes in modified documents, the algorithms Sketch n and BSW n  X  X eflect X  more of such changes in their feature vectors, especially for shorter documents as shown in Figures 8 (b).

An interesting observation is that Sketch n and BSW n are the representatives of somewhat different approaches: Sketch n produces a fixed-size feature vector while BSW n generates a feature vector that is proportional to the doc-ument size. However, they both are more sensitive to the document modifications than Mod n and Min n . (a) (a)
Hewlett-Packard has large collections of technical support documents on different topics. As a part of content man-agement it is important to identify and filter out support documents that are largely duplicates of newer versions in order to improve the quality of the existing collection.
Our goal was to process given collections of documents with the four similarity algorithms under study and compare the results. Figure 10: The document size distribution in two en-
We processed two collections of documents with 5040 and 2500 documents respectively. The document length distri-butions for both collections are shown in Figure 10. The documents in the first collection were relatively short (600-1500 bytes) with a small subset of longer documents. The second collection had slightly longer documents (2-4 KB) and a small subset of long (larger than 10 KB) documents.
Since these two collections had mostly short files, we tuned a sampling frequency in Mod n and BSW n to produce finer granularity by using n =100. Even then, Mod 100 and BSW 100 had 7 times fewer (on average) hashes in the document sig-nature than Sketch 100 or Min 100 ( Sketch 100 and Min 100 produce 100-entry feature vectors independent of the file size). For the second collection this ratio was 2.5 times.
Figure 9 shows the number of similar file pairs for varying the similarity theshold from 40% to 90% identified under different similarity algorithms in the two enterprise collec-tions. Typically, the pairs of files with similarity metrics higher than 70% might be good candidates for being near-duplicates. We analyzed pairs of documents with similarity above 70% reported by different algorithms, and Table 2 presents the summary of the results. For both collections, Table 2: Summary of the results for Collection 1 and Collection 2 .
 Mod 100 and Min 100 identified a higher number of poten-tially similar pairs compared to Sketch n and BSW n .At thesametime,for Collection 1 , the combined number of potentially similar pairs reported by the four algorithms in-dependently is 65. However, only 12 of them are reported by all 4 algorithms. For Collection 2 , the combined number of pairs reported by all four algorithms independently is 81, but only 45 of them are reported by all 4 algorithms.
The trends in reported results agree with our observa-tions for the four algorithms derived with the Research Cor-pus experiments (except a closer match between BSW n and Sketch n ). However, in the case of the two enterprise col-lections we do not know the  X  X round truth X , i.e., which of the reported documents are indeed similar, and which ones might be false positives. To understand this we performed an additional document analysis in the following way. For each of the reported similar pair of documents D and D , we used UNIX diff utility to compute the edit distance. In most of the cases, the outcome of diff was very compact with easily observable difference in the pair of documents due to document number, or date, or some other small details. In these cases, it was clear that documents D and D were indeed similar. In a few cases, the outcome of diff was sig-nificant and, indeed, documents D and D were apparently different. Using this approach, we identified which of the similar pairs were reported correctly and which were false positives.

It was interesting to see the groups of similar documents and what constitute overlapping and different portions of these documents. For example, there were pairs of techni-cal support documents with different dates and document numbers, but very similar root cause analysis and solution information written about different applications, say X and Y . After talking to experts it became apparent that Y was a new generation of product that replaced X .

As shown in Table 2, for the first collection, Mod 100 had highest number of correctly identified documents (48), but atthesametime13documents(13=61  X  48) were false pos-itives. Min 100 and BSW 100 both had 1 false positive pair. While Sketch 100 did not have any false positives, it had the lowest number of reported similar pairs among the four algorithms. All the false positives happened for short doc-uments. One of the explanations is that the documents in both collections had a  X  X oose X  configuration template defin-ing content of the documents, and for very short documents it introduced by itself a significant amount of  X  X oise X . For the second collection of documents, where the documents were longer, none of the algorithms reported false positives as shown in Table 2.

In summary, Mod n significantly outperformed the other algorithms, with Min n being second, BSW n and Sketch n sharing the third place. These findings are consistent with our observations and experiments in Section 4 with the Re-search Corpus.

An interesting extension of this work will be incorporat-ing some additional document analysis for identified pairs of similar documents to avoid false positives and helping experts to more easily filter them out. Once the clusters of similar documents are built, one can apply more expen-sive methods to  X  X ighlight X  the exact difference in identified pairs of documents to semi-automate the document analysis process at this stage.
We performed a comparison of the document processing time under different algorithms and their parameters. Fig-ure 11 shows the processing time of a 500 KB file under Mod n , Min n , Sketch n and BSW n . While this figure is specific to our implementation of these algorithms, it still reflects some general trends. Document processing under Mod n , Min n ,and BSW n is based on Rabin fingerprints [17] that have a very fast software implementation and are in-dependent of a sliding window size. Processing time under Sketch n algorithm involves concurrent computation with n different hash functions (we used the Bob Jenkins hash func-tion for speed). Moreover hash computation over a larger sliding window has a significant additional overhead.
The results in Figure 11 are for a longer file to stress the difference. Obviously, for short files this difference is less pronounced. When choosing a particular similarity algo-rithm, one needs to carefully estimate the length of the file signature as well as the document processing cost based on the average file size. However, in general, the processing cost under Mod n , Min n ,and BSW n is lower than under Figure 11: Comparison of the processing time for a 500 Sketch n .The Sketch n algorithm must compute a family of n hash functions rather than just a single hash function, and this significantly impacts its speed.
Many document repositories, such as newswire, web sites, and digital libraries typically have a significant amount of repeated or similar information. The extent to which docu-ments are considered similar to each other defines a similar-ity spectrum. On one side there is a broad topical (semantic) similarity with document identity on the other side. In this paper, we studied a few algorithms that can be classified as a syntactic similarity group. The similarity of documents under this definition is entirely defined by the syntactic prop-erties of the documents.

Duplicate and near-duplicate web pages cause problems for web search engines: increased index size , slower searches, and lower quality results. A variety of syntactic similar-ity algorithms proposed for detection of near-duplicate web pages are based on a  X  X hingling X  technique [2, 3]. While these algorithms were proposed more than 10 years ago, we are not aware of any performance study comparing them, especially in the context of enterprise information manage-ment. In [12], M. Henzinger compares the performance of the Sketch n algorithm with Charikar X  X  algorithm [5] based on random projections of words in a document. More ex-actly, a variation of the Sketch n algorithm with super-shingles is used in this work. Under this approach a set of shingles is combined into a super-shingle to simplify the second step  X  clustering process. It is interesting work that involves a large-scale evaluation, namely a set of 1.6B distinct web pages. The author finds that neither of the algorithms works well for finding near-duplicate pairs on the same site, while both achieve high precision for near-duplicate pairs on dif-ferent sites. In this work, the author chooses a set of param-eters for these algorithms based on the earlier use cases. It might be interesting to repeat the same algorithms under dif-ferent parameters and compare the outcome. In [7], the au-thors apply Sketch n algorithm with super-shingles for iden-tifying near-duplicate web pages but pursue a different goal of understanding how the clusters of near-duplicates evolve over time. Another interesting performance study, [19], an-alyzes the sources and amount of document replication on the web.

There is a group of similarity algorithms that aim to detect copyright violations and plagiarism [1, 18, 10, 13]. In [18], the authors study a variety of choices for chunking policies (fixed size, variable size, small vs. large chunks, overlapping vs. non-overlapping chunks) where the chunks are defined at the granularity of words, group of words, and sentences. The authors compare the cost (in terms of storage) of different chunking schemes and accuracy of finding the overlapping chunks in documents from DL. The chunking approach in their paper is closer to a classic Basic Sliding Window al-gorithm with a difference that a  X  X ord X  is the basic unit (compared to a byte in our approach). In [9], authors suc-cessfully applied the content-based chunking algorithm to identify similar documents in large enterprise repositories. However, in [18, 9], the authors use the entire chunk hashes for a file signature (compared to the smallest hash of the win-dow within the chunk in our approach). This is exact docu-ment fingerprinting; all the policies considered in our paper use approximate fingerprinting. They find that a coarser chunking misses many overlaps within the test set, while a finer granularity chunking improves the outcome. In [13], the authors concentrate on identifying plagiarism and dis-cuss how this problem is different from a more traditional problem of finding similar documents. They investigate a number of approaches that can be applied to this problem, and mostly concentrate on the 1-to-n problem, i.e., finding the derivatives for a given document. A good survey of dif-ferent methods proposed for finding similar documents to a given one is provided in [6]. For solving the n-to-n similarity problem the authors promote an I-match approach [6] which is based on term collection statistics. It is closer to a seman-tic document analysis rather than a pure syntactic approach. I-match identifies a smaller number of near-duplicates (but with a higher accuracy) compared to a shingling technique because it requires an exact match for the document terms remaining after the syntactic filtration process.
Many enterprises are building new applications associ-ated with document management, data retention, regula-tory compliance, and litigation issues as a part of informa-tion management solution. The problem of robust discovery of similar enterprise documents is an essential part of this solution. In this paper, we evaluate and compare the perfor-mance of four syntactic similarity algorithms. Three algo-rithms are based on Broder X  X   X  X hingling X  technique while the fourth algorithm employs a more recent approach,  X  X ontent-based chunking X . Our performance study reveals that the similarity metric reported by all four algorithms is highly sensitive to settings of the sliding window size and sampling frequency parameters. We identify a useful range of these parameters for achieving good practical results, and com-pare the performance of the four algorithms in a controlled environment. We validate our results by applying these algo-rithms to finding similar documents in two large collections of HP technical support documents.

In this work, we used a slightly modified version of a tradi-tional Basic Sliding Window algorithm: instead of taking the hash of the entire chunk, we used the numerically smallest fingerprinted sliding window within this chunk. This modi-fication made the original algorithm more resilient to small changes within the chunk, and improved its performance for small documents. Additionally, this change enabled a fair comparison between the four similarity algorithms since they were defined using the same type parameters such as the sliding window size and sampling frequency.

For a family of shingling -based algorithms the sampling is essential, because it drastically reduces the amount of data (fingerprinted shingles) to compare and makes the whole approach feasible. However, sampling always introduces a degree of  X  X uzziness X  and a probability for false positives.
The traditional Basic Sliding Window algorithm produces a compact file signature without sampling by using the chunk hashes. This file signature encodes exact information about the file. If two files do have 9 out of 10 hashes in com-mon, then we know precisely which portions of the files are identical. This is an appealing property. We believe that traditional Basic Sliding Window algorithm might be a more promising approach for longer files than the  X  X ampling X -based technique and is worth future investigation.
