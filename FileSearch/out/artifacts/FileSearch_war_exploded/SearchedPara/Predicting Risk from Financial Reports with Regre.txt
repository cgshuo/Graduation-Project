 We consider a text regression problem: given a piece of text, predict a R -valued quantity associated with that text. Specifically, we use a company X  X  annual financial report to predict the financial risk of invest-ment in that company, as measured empirically by a quantity known as stock return volatility .

Predicting financial risk is of clear interest to anyone who invests money in stocks and central to modern portfolio choice. Financial reports are a government-mandated artifact of the financial world that X  X ne might hypothesize X  X ontain a large amount of information about companies and their value. Indeed, it is an important question whether mandated disclosures are informative, since they are meant to protect investors but are costly to produce.
The intrinsic properties of the problem are attrac-tive as a test-bed for NLP research. First, there is no controversy about the usefulness or existential reality of the output variable (volatility). Statisti-cal NLP often deals in the prediction of variables ranging from text categories to linguistic structures to novel utterances. While many of these targets are uncontroversially useful, they often suffer from eval-uation difficulties and disagreement among annota-tors. The output variable in this work is a statistic summarizing facts about the real world; it is not sub-ject to any kind of human expertise, knowledge, or intuition. Hence this prediction task provides a new, objective test-bed for any kind of linguistic analysis.
Second, many NLP problems rely on costly anno-tated resources (e.g., treebanks or aligned bilingual corpora). Because the text and historical financial data used in this work are freely available (by law) and are generated as a by-product of the American economy, old and new data can be obtained by any-one with relatively little effort.

In this paper, we demonstrate that predicting fi-nancial volatility automatically from a financial re-port is a novel, challenging, and easily evaluated nat-ural language understanding task. We show that a very simple representation of the text (essentially, bags of unigrams and bigrams) can rival and, in combination, improve over a strong baseline that does not use the text. Analysis of the learned models provides insights about what can make this problem more or less difficult, and suggests that disclosure-related legislation led to more transparent reporting. Volatility is often used in finance as a measure of risk . It is measured as the standard deviation of a stock X  X  returns over a finite period of time. A stock will have high volatility when its price fluctu-ates widely and low volatility when its price remains more or less constant.
 between the close of trading day t  X  1 and day t , where P t is the (dividend-adjusted) closing stock price at date t . The measured volatility over the time period from day t  X   X  to day t is equal to the sample s.d.: where  X  r is the sample mean of r t over the period. In this work, the above estimate will be treated as the true output variable on training and testing data.
It is important to note that predicting volatility is not the same as predicting returns or value . Rather than trying to predict how well a stock will perform, we are trying to predict how stable its price will be over a future time period. It is, by now, received wisdom in the field of economics that predicting a stock X  X  performance , based on easily accessible pub-lic information, is difficult. This is an attribute of well-functioning (or  X  X fficient X ) markets and a cor-nerstone of the so-called  X  X fficient market hypoth-esis X  (Fama, 1970). By contrast, the idea that one can predict a stock X  X  level of risk using public in-formation is uncontroversial and a basic assumption made by many economically sound pricing mod-els. A large body of research in finance suggests that the two types of quantities are very different: while predictability of returns could be easily traded away by the virtue of buying/selling stocks that are under-or over-valued (Fama, 1970), similar trades are much more costly to implement with respect to predictability of volatility (Dumas et al., 2007). By focusing on volatility prediction, we avoid taking a stance on whether or not the United States stock market is informationally efficient. Given a text document d , we seek to predict the value of a continuous variable v . We do this via a parameterized function f : where w  X  R d are the parameters or weights. Our approach is to learn a human-interpretable w from a collection of N training examples { X  d i ,v i  X  X  N where each d i is a document and each v i  X  R .
Support vector regression (Drucker et al., 1997) is a well-known method for training a regression model. SVR is trained by solving the following op-timization problem: min where C is a regularization constant and controls the training error. 1 The training algorithm finds weights w that define a function f minimizing the (regularized) empirical risk.

Let h be a function from documents into some vector-space representation  X  R d . In SVR, the func-tion f takes the form: where Equation 4 re-parameterizes f in terms of a kernel function K with  X  X ual X  weights  X  i . K can be seen as a similarity function between two docu-ments. At test time, a new example is compared to a subset of the training examples (those with  X  i 6 = 0 ); typically with SVR this set is sparse. With the linear kernel, the primal and dual weights relate linearly:
The full details of SVR and its implementation are beyond the scope of this paper; interested readers are referred to Sch  X  olkopf and Smola (2002). SVM light (Joachims, 1999) is a freely available implementa-tion of SVR training that we used in our experi-ments. 2 In the United States, the Securities Exchange Com-mission mandates that all publicly-traded corpora-tions produce annual reports known as  X  X orm 10-K. X  The report typically includes information about the history and organization of the company, equity and subsidiaries, as well as financial information. These reports are available to the public and pub-lished on the SEC X  X  web site. 3 The structure of the 10-K is specified in detail in the legislation. We have collected 54,379 reports published over the period 1996 X 2006 from 10,492 different companies. Each report comes with a date of publication, which is im-portant for tying the text back to the financial vari-ables we seek to predict.

From the perspective of predicting future events, one section of the 10-K is of special interest: Section 7, known as  X  X anagement X  X  discussion and anal-ysis of financial conditions and results of opera-tions X  (MD&amp;A), and in particular Subsection 7A,  X  X uantitative and qualitative disclosures about mar-ket risk. X  Because Section 7 is where the most im-portant forward-looking content is most likely to be found, we filter other sections from the reports. The filtering is done automatically using a short, hand-written Perl script that seeks strings loosely matching the Section 7, 7A, and 8 headers, finds the longest reasonable  X  X ection 7 X  match (in words) of more than 1,000 whitespace-delineated tokens.
Section 7 typically begins with an introduction like this (from ABC X  X  1998 Form 10-K, before to-kenization for readability; boldface added):
Not all of the documents downloaded pass the fil-ter at all, and for the present work we have only used documents that do pass the filter. (One reason for the failure of the filter is that many 10-K reports include Section 7  X  X y reference, X  so the text is not directly included in the document.)
In addition to the reports, we used the Center for Research in Security Prices (CRSP) US Stocks Database to obtain the price return series along with other firm characteristics. 4 We proceeded to calcu-late two volatilities for each firm/report observation: the twelve months prior to the report ( v (  X  12) ) and the twelve months after the report ( v (+12) ).
Tokenization was applied to the text, including punctuation removal, downcasing, collapsing all digit sequences, 5 and heuristic removal of remnant markup. Table 1 gives statistics on the corpora used in this research; this is a subset of the cor-pus for which there is no missing volatility informa-tion. The drastic increase in length during the 2002 X  2003 period might be explained by the passage by the US Congress of the Sarbanes-Oxley Act of 2002 (and related SEC and exchange rules), which im-posed revised standards on reporting practices of publicly-traded companies in the US. Volatility displays an effect known as autoregressive conditional heteroscedasticity (Engle, 1982). This means that the variance in a stock X  X  return tends to change gradually. Large changes in price are pre-saged by other changes, and periods of stability tend to continue. Volatility is, generally speaking, not constant, yet prior volatility (e.g., v (  X  12) ) is a very good predictor of future volatility (e.g., v (+12) ). At the granularity of a year, which we consider here because the 10-K reports are annual, there are no existing models of volatility that are widely agreed to be significantly more accurate than our histor-ical volatility baseline. We tested a state-of-the-art model known as GARCH(1 , 1) (Engle, 1982; Bollerslev, 1986) and found that it was no stronger than our historical volatility baseline on this sample.
Throughout this paper, we will report perfor-mance using the mean squared error between the predicted and true log-volatilities: 6 In our experiments, we vary h (the function that maps inputs to a vector space) and the subset of the data used for training. We will always report perfor-mance over test sets consisting of one year X  X  worth of data (the subcorpora described in Table 1). In this work, we focus on predicting the volatility over the year following the report ( v (+12) ). In all experi-ments, = 0 . 1 and C is set using the default choice h ( d ) &gt; h ( d ) over the training data. 7 6.1 Feature Representation We first consider how to represent the 10-K reports. We adopt various document representations, all us-ing word features. Let M be the vocabulary size derived from the training data. 8 Let freq( x j ; d ) de-note the number of occurrences of the j th word in the vocabulary in document d .  X  TF : h j ( d ) = 1  X  LOG 1 P : h j ( d ) = log(1 + freq( x j ; d )) . Rather Note that each of these preserves sparsity; when freq( x
For interpretability of results, we use a linear ker-nel. The usual bias weight b is included. We found it convenient to work in the logarithmic domain for the predicted variable, predicting log v instead of v , since volatility is always nonnegative. In this setting, the predicted volatility takes the form: Because the goal of this work is to explore how text might be used to predict volatility, we also wish to see whether text adds information beyond what can be predicted using historical volatility alone (the augmented with an additional feature, defined as h tion, it is always available when the 10-K report is published. These models are denoted TF +, TFIDF +,
The performance of these models, compared to the baseline from Section 5, is shown in Table 2. We used as training examples all reports from the five-year period preceding the test year (so six ex-periments on six different training and test sets are shown in the figure). We also trained SVR models weights ( b in Eq. 7); these are usually worse and never signficantly better than the baseline.

Strikingly, the models that use only the text to predict volatility come very close to the historical baseline in some years. That a text-only method (
LOG 1 P with bigrams) for predicting future risk comes within 5% of the error of a strong baseline (2003 X 6) shows promise for the overall approach. A combined model improves substantially over the baseline in four out of six years (2003 X 6), and this difference is usually robust to the representation used. Table 3 shows the most strongly weighted terms in each of the text-only LOG 1 P models (in-cluding bigrams). These weights are recovered us-ing the relationship expressed in Eq. 5. 6.2 Training Data Effects It is well known that more training data tend to im-prove the performance of a statistical method; how-ever, the standard assumption is that the training data are drawn from the same distribution as the test data. In this work, where we seek to predict the future based on data from past, that assumption is obviously violated. It is therefore an open question whether more data (i.e., looking farther into the past) is helpful for predicting volatility, or whether it is better to use only the most recent data.

Table 4 shows how performance varies when one, two, or five years of historical training data are used, averaged across test years. In most cases, using more training data (from a longer historical period) is helpful, but not always. One interesting trend, not shown in the aggregate statistics of Table 4, is that recency of the training set affected perfor-mance much more strongly in earlier train/test splits (2001 X 3) than later ones (2004 X 6). This experiment leads us to conclude that temporal changes in fi-nancial reporting make training data selection non-trivial. Changes in the macro economy and spe-cific businesses make older reports less relevant for prediction. For example, regulatory changes like Sarbanes-Oxley, variations in the business cycle, and technological innovation like the Internet influ-ence both the volatility and the 10-K text. 6.3 Effects of Sarbanes-Oxley We noted earlier that the passage of the Sarbanes-Oxley Act of 2002, which sought to reform financial reporting, had a clear effect on the lengths of the 10-K reports in our collection. But are the reports more informative? This question is important, be-cause producing reports is costly; we present an em-pirical argument based on our models that the legis-lation has actually been beneficial.

Our experimental results in Section 6.1, in which volatility in the years 2004 X 2006 was more accu-rately predicted from the text than in 2001 X 2002, suggest that the Sarbanes-Oxley Act led to more in-formative reports. We compared the learned weights (
LOG 1 P +, unigrams) between the six overlapping five-year windows ending in 2000 X 2005; measured in L 1 distance, these were, in consecutive order,  X  52.2, 59.9, 60.7, 55.3, 52.3  X  ; the biggest differ-ences came between 2001 and 2002 and between 2002 and 2003. (Firms are most likely to have be-gun compliance with the new law in 2003 or 2004.) The same pattern held when only words appearing in all five models were considered. Variation in the recency/training set size tradeoff (  X  6.2), particularly during 2002 X 3, also suggests that there were sub-stantial changes in the reports during that time. 6.4 Qualitative Evaluation One of the advantages of a linear model is that we can explore what each model discovers about dif-ferent unigram and bigram terms. Some manually selected examples of terms whose learned weights ( w ) show interesting variation patterns over time are shown in Figure 1, alongside term frequency pat-terns, for the text-only LOG 1 P model (with bigrams). These examples were suggested by experts in fi-nance from terms with weights that were both large and variable (across training sets).
 A particularly interesting case, in light of Sarbanes-Oxley, is the term accounting policies . Sarbanes-Oxley mandated greater discussion of ac-counting policy in the 10-K MD&amp;A section. Be-fore 2002 this term indicates high volatility, per-haps due to complicated off-balance sheet transac-tions or unusual accounting policies. Starting in 2002, explicit mention of accounting policies indi-cates lower volatility. The frequency of the term also increases drastically over the same period, sug-gesting that the earlier weights may have been in-flated. A more striking example is estimates , which averages one occurrence per document even in the 1996 X 2000 period, experiences the same term fre-quency explosion, and goes through a similar weight change, from strongly indicating high volatility to strongly indicating low volatility.

As a second example, consider the terms mort-gages and reit (Real Estate Investment Trust, a tax designation for businesses that invest in real estate). Given the importance of the housing and mortgage market over the past few years, it is interesting to note that the weight on both of these terms increases over the period from a strong low volatility term to a weak indicator of high volatility. It will be interest-ing to see how the dramatic decline in housing prices in late 2007, and the fallout created in credit markets in 2008, is reflected in future models.

Finally, notice that high margin and low mar-gin , whose frequency patterns are fairly flat  X  X witch places, X  over the sample: first indicating high and low volatility, respectively, then low and high. There is no a priori reason to expect high or low margins would be associated with high or low stock volatil-ity. However, this is an interesting example where bigrams are helpful (the word margin by itself is uninformative) and indicates that predicting risk is highly time-dependent. 6.5 Delisting An interesting but relatively infrequent phenomenon is the delisting of a company, i.e., when it ceases to be traded on a particular exchange due to dissolution after bankruptcy, a merger, or violation of exchange rules. The relationship between volatility and delist-ing has been studied by Merton (1974), among oth-ers. Our dataset includes a small number of cases where the volatility figures for the period following the publication of a 10-K report are unavailable be-cause the company was delisted. Learning to predict delisting is extremely difficult because fewer than 4% of the 2001 X 6 10-K reports precede delisting.
Using the LOG 1 P representation, we built a lin-ear SVM classifier for each year in 2001 X 6 (trained on the five preceding years X  data) to predict whether a company will be delisted following its 10-K re-port. Performance for various precision measures is shown in Table 5. Notably, for 2001 X 4 we achieve above 75% precision at 10% recall. Our best (or-acle) F 1 scores occur in 2002 and 2003, suggesting again a difference in reports around Sarbanes-Oxley. Table 5 shows words associated with delisting. In NLP, regression is not widely used, since most natural language-related data are discrete. Regres-sion methods were pioneered by Yang and Chute (1992) and Yang and Chute (1993) for information retrieval purposes, but the predicted continuous vari-able was not an end in itself in that work. Blei and McAuliffe (2007) used latent  X  X opic X  variables to predict movie reviews and popularity from text. Lavrenko et al. (2000b) and Lavrenko et al. (2000a) modeled influences between text and time series fi-nancial data (stock prices) using language models. Farther afield, Albrecht and Hwa (2007) used SVR to train machine translation evaluation metrics to match human evaluation scores and compared tech-niques using correlation. Regression has also been used to order sentences in extractive summarization (Biadsy et al., 2008).

While much of the information relevant for in-vestors is communicated through text (rather than numbers), only recently is this link explored. Some papers relate news articles to earning forecasts, stock returns, volatility, and volume (Koppel and Shtrim-berg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler and Frank (2004) ask whether messages posted on message boards can help explain stock performance, while Li (2005) measures the associ-ation between frequency of words associated with risk and subsequent stock returns. Weiss-Hanley and Hoberg (2008) study initial public offering disclo-sures using word statistics. Many researchers have focused the related problem of predicting sentiment and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic val-ues like prediction markets (Lerman et al., 2008).
In contrast to text regression, text classification comprises a widely studied set of problems involv-ing the prediction of categorial variables related to text. Applications have included the categorization of documents by topic (Joachims, 1998), language (Cavnar and Trenkle, 1994), genre (Karlgren and Cutting, 1994), author (Bosch and Smith, 1998), sentiment (Pang et al., 2002), and desirability (Sa-hami et al., 1998). Text categorization has served as a test application for nearly every machine learning technique for discrete classification. We have introduced and motivated a new kind of task for NLP: text regression , in which text is used to make predictions about measurable phenomena in the real world. We applied the technique to pre-dicting financial volatility from companies X  10-K re-ports, and found text regression model predictions to correlate with true volatility nearly as well as his-torical volatility, and a combined model to perform even better. Further, improvements in accuracy and changes in models after the passage of the Sarbanes-Oxley Act suggest that financial reporting reform has had interesting and measurable effects.

