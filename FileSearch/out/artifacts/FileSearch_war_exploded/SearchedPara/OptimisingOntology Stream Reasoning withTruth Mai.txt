 So far researchers in the Description Logics / Ontology communi-ties mainly consider ontology reasoning services for static ontolo-gies. The rapid development of the Semantic Web and its emerg-ing data ask for reasoning technologies for dynamic knowledge streams. Existing work on stream reasoning is focused on light-weight languages such as RDF and RDFS. In this paper, we intro-duce the notion of Ontology Stream Management System (OSMS) and present a stream-reasoning approach based on Truth Mainte-nance System (TMS). We present optimised EL ++ algorithm to reduce memory consumption. Our evaluations show that the op-timisation improves TMS-enabled EL ++ reasoning to deal with relatively large volumes of data and update efficiently. I.2.3 [ Artificial Intelligence ]: Deduction and Theorem Proving X  Inference engines Algorithms Ontology, Stream Reasoning, Truth Maintenance System Ontologies are the knowledge infrastructures of the Semantic Web and many intelligent systems. The standard Web Ontology Language OWL is based on the Description Logics (DLs). So far researchers in the DL/ Ontology communities mainly consider on-tology reasoning services for static ontologies. However, in real world data and knowledge is usually subject to change. Parsia et. al. [12] described several typical scenarios such as ontology editing, web service matchmaking, sensor networks and mobile semantic web [11]. Considering the following running example:  X 
This paper is an extension of [13], an earlier work presented in a workshop.

E XAMPLE 1. Given the schedule of a conference and researchers X  personal information and activities, such as who is attending which events, a knowledge management system would be able to collect data and provide recommendations, in real time.

In this paper, we assume such data has been extracted and for-malised, and focus on the data processing and reasoning aspect. Due to the real-time nature of these user generated information and ongoing events, it is adequate to consider them as data streams.
There have been many works regarding querying streaming data on the semantic web. A. Bolles et al. [7] proposed an exten-sion of SPARQL to process data streams. Similarly, C-SPARQL [5], another extension of SPARQL can continuously query from a RDF knowledge base. More related work focus on continuously and in-crementally updating and materialising ontological knowledge bases . R. Volz et al. [14] adopted the Delete and Re-derive (DRed) algorithm [9] from traditional data stream management systems and proposed a declarative variant of it. When change occurs, the  X  X tream reasoner X  first overestimates the consequences of the dele-tion, then  X  X ash-back X  the over-deleted consequences that can be derived by other facts, and finally adds new entailments that are derived from the new facts. Further optimisation was proposed [6] with an additional assumption that the time window of stream is fixed and known to the stream reasoner. The relation between a consequence and a time point can be maintained so that when the time comes, the stream reasoner expires corresponding conse-quences. In their evaluation, this approach outperforms a naive ap-proach with up to 13% of RDF triples changed, which is better than the 3% limit of [14]. However, these approaches are still limited to relatively simple languages such as RDF, RDFS. The optimisation also requires a known fixed time window. Otherwise, the optimisa-tion can not work. This also means deleting axioms in real-time is not allowed. For example, if data collected from different sources make the ontology inconsistent, such inconsistency can not be ac-tively resolved by removing conflicting knowledge.

In this paper, we introduce the notion of ontology stream man-agement system (OSMS). We present an ontology stream reason-ing approach by applying Truth Maintenance Systems (TMS [8]) for OSMS. A TMS maintains not only reasoning results, but also intermediate results and the deduction relations among them. Thus on the one hand, impacted results of removed knowledge can be retrieved without prior knowledge of fixed time window. On the other hand, further inference can be carried on from the interme-diate results, instead of the original asserted knowledge. Both will significantly increase the efficiency of stream reasoning to facilitate real-time response. However, TMS has a well-known disadvan-tage of excessive memory consumption. To address this issue, we present an optimised algorithm for EL ++ , the logic underpinning OWL 2 EL, by reducing the number of unnecessary intermediate results. Our evaluation shows that our approach can outperform naive re-computation with relatively large volumes of knowledge base (up to 30000 axioms) and updates (up to 10% ). A signature = ( CN ; RN ; IN ) consists of three disjoint sets CN , RN , IN , where CN is the set of atomic concepts, RN the set of atomic roles and IN is the set of individuals. Given a signature,  X  the top concept,  X  the bottom concept, A an atomic concept, a an individual, r an atomic role, EL ++ concept expres-sions C , D can be composed with the following constructs: We slightly abuse the notion of atomic concepts to include also  X  and nominals (i.e. concepts of form { a } ). Given a knowledge base K , we use CN K , RN K , IN K to denote the set of atomic concepts, atomic roles and individuals in K , respectively.
A DL ontology O = &lt; T ; A &gt; is composed of a TBox T an ABox A . A TBox is a set of concept and role axioms. EL ++ supports general concept inclusion axioms (GCIs, e.g. C  X  and role inclusion axioms (RIs, e.g. r  X  s , r 1  X   X   X   X   X  If C  X  D and D  X  C , we write C  X  D . An ABox is a set of concept assertion axioms, e.g. a : C , role assertion axioms, e.g. ( a; b ) : R , and individual equality axioms, e.g. a = b , and individual inequality axioms, e.g. a  X  = b .
 E XAMPLE 2. Here are some simple EL ++ ontologies:
Ontology O : { ActiveT alk  X   X  in:Session , hasT opic  X  interest  X  recommend ,  X  recommend: { David }  X  T alk 4 Dave , ActiveT alk  X  T alk 4 Dave  X  T argetT alk , ( ontology; Daivd ) : interest , ( talk 1 ; ontology ) : hasT opic , ( talk 2 ; ontology ) : hasT opic , } specifies that an ActiveTalk must be presented in some Session ; A talk can be recommended to someone if the talk has topic that interests the person; A talk is for David if it can be recommended to David and such a talk will be a T argetT alk if it is active; It also says that David is interested in ontology . There are two talks talk 1 and talk 2 that have topic about ontology . O (0) = { talk 0 : ActiveT alk } shows that talk 0 is an ActiveT alk . O (1) = { talk 1 : ActiveT alk } shows that talk 1 is an ActiveT alk . O (2) = { talk 2 : ActiveT alk } shows that talk 2 is an ActiveT alk .
If an axiom can be derived from an ontology O , we say that is entailed by O , denoted by O | = . is an entailment of an ontology O and its entailment , a set of axioms J O ( ) a justification of iff J O ( ) | = and J  X   X | = for all J For EL ++ , a single justification can be computed in PTIME [4].
Franz Baader et. al. [1] presents a completion-based algorithm to classify an EL ++ TBox. Given an EL ++ TBox T , it can be normalised in linear time [3] into a normal form TBox T  X  all axioms are in one of the following forms, where A i  X  CN and r i  X  RN T  X  , by introducing fresh atomic concepts and roles: For any two A; B  X  CN T , T | = A  X  B iff T  X  | = A  X  B [2]. Given a normalised EL ++ TBox T , the algorithm uses a set of completion rules (Table 1, in R6 X ; R A iff there exists C ; : : : ; C k  X  CN T s.t. C 1 = X or C 1 = { b } , C j  X   X  for some r j  X  RN T (1  X  j  X  k ) and C k = A ) to compute, for each two A; B  X  CN T , entailed subsumption T | = A  X  B .
Reasoning with rules R1-R8 is PTIME-Complete [2]. ABox rea-soning can also be reduced to TBox reasoning with these rules [2]. Alternati vely, we can internalize ABox axioms into TBox axioms as follows to perform reasoning. In the rest of the paper, we always internalize ABox into TBox.
In this section, we introduce the notion of ontology stream man-agement systems (OSMS). A typical ontology stream management system is illustrated in Fig. 1. It can retrieve certain information from streams. Data Aggregation is the type of retrieval that does not require reasoning. For example, in the above stream, we can ask which takes are active. This type of retrieval has been widely addressed by, e.g., the C-SPARQL proposals. Stream Reasoning requires reasoning to be performed on the stream to yield correct answers. Their output are streams of results.

In stream reasoning [6] a streaming RDF knowledge base is de-fined as a set of pairs ( ; t ) where each RDF triple is associated with a timestamp t . By grouping triples of the same timestamp t we have the ontology at time point t . Such a versioning idea re-sembles the concept of Linear Version Space (LVS) [10], which is a sequence of ontologies S = ( O 0 ; O 1 ; : : : ; O n ) . We recast these concepts and present a unified definition:
D EFINITION 1. (Discrete Ontology Stream) An discrete ontol-ogy stream O n m from point of time m to point of time n is a se-quence of ontologies O n m ( t 0 ) ; O n m ( t 1 ) ; : : : ; m  X  t 1  X   X   X   X   X  t x = n . O n
For conciseness, in the rest of the paper we use the name ontol-ogy stream or stream for short. We also assume that a stream always starts from time point 0 and is updated at each following integer time point, i.e. O n 0 = O n 0 (0) ; O n 0 (1) ; : : : O n 0 ( i ) to stream is consequently updated from O n 0 to O n +1 0 . It X  X  important to note that the updates are not necessarily known to stream reasoners in advance. That means any axiom can be added into, or removed from the changing ontology at any time.
 From Example 2 we construct a stream O 1 0 = O  X  O (0) ; O  X  O (1) . The stream is illustrated in Fig. 2. We assume all ABox axioms have been internalised. This stream specifies that the fea-ture of ActiveT alk , the role chain of recommend , the condition of T alk 4 Dave and T argetT alk , David  X  X  interest on ontology , and the topics of the two talks preserve from time point 0 to 1 . talk 0 is active in time point 0 and talk 1 is active in time point 1 .
In this paper, we focus on such stream reasoning services. More formally, for a stream O n 0 and a reasoning problem Q , applica-tions usually ask for reasoning results of Q on snapshot O n denoted by Ans ( O n 0 ( i ) ; Q ) . For example, with the information in Fig. 2, we can ask for all ongoing talks that are interesting to David, which can be regarded as Q = retrieving instances of T argetT alk . Such a request can be simply fulfilled by applying the comple-tion rules on the snapshots. By manual checking we know that If we update O 1 0 to O 2 0 with O 2 0 (2) = O  X  O (2) . A new talk talk 2 is being presented at time point 3 (Fig. 3). Now David wants to know, what is Ans ( O 2 0 (0) ; Q ) , given the results of Ans (
We can directly compute the new results on O 2 0 (2) . We call this approach the naive approach . This approach is not efficient be-cause the time of computing the new results is determined by the entire ontology, instead of the changed part. Even if the change is very small, the updating of results can take very long time if the ontology is big. More interestingly, people would like to compute results on the latest snapshot based on results on the previous snap-shot, so that partial results can be reused without re-computation.
As we mentioned in the last section, existing work is limited to relatively simple languages and have certain restrictions. In the next sections, we propose a truth maintenance system-based ap-proach for EL ++ and which requires no fixed time window. In this section, we present how to provide stream reasoning in OSMS. Different from the naive approach, which re-compute ev-erything from scratch, we propose to maintain the (intermediate) results and their inferencing relations, so that when updating, en-tailments affected or not affected by the removed axioms can be easily distinguished and intermediate results can be re-used. Such maintenance can be realised by a Truth Maintenance System (TMS [8]).
A TMS maintains both beliefs and their dependencies in the form of a dependency network, in which nodes are beliefs and edges are the inference steps from which the nodes are derived.

D EFINITION 2. (Truth Maintenance System) Given an ontol-ogy O , a TMS G O =  X  N O ; E O  X  of O is a directed graph such that (1) O  X  N O ; (2) N O  X  { |O | = } ; (3) for any  X  N { | ( ; )  X  E O } is a minimal set of axioms that entails . The 3rd property is important such that if all inbound nodes of an entailment are preserved, then the entailment is preserved. Oth-erwise the entailment should be removed unless a different set of axioms can entail it.

It is obvious that a node can have multiple inbound edges if the entailment is derived from multiple other entailments. Tautology axioms do not have any inbound edges. Asserted axioms only have inbound edges from themselves. It is also apparent that an ontology can have multiple or even infinite TMSes, depending on how many entailments are preserved. For example, applying R1-8 in Table 1, a TMS of O 1 0 (1) from our running example is illustrated in Fig. 4. We first show how the TMS can be used in stream reasoning. After that we explain how the nodes and edges of a TMS can be constructed. Suppose we have an ontology stream O n 0 , a rea-soning request Q and a TMS G O n Ans ( O n 0 ( n ) ; Q ) , when an update occurs, i.e. we have a new snap-shot O n +1 0 ( n + 1) . We can update the entailments in N 2. Remove all the erased axioms and their reachable nodes from 3. Add all new axioms in O n +1 0 ( n + 1) to N O n +1 4. Check which new result of Ans ( O n +1 0 ( n +1) ; Q ) can be en-As a example, applying the above procedure on our example (2) will produce the following new TMS (Fig. 6). In this TMS the green nodes and edges are newly added while the others are preserved from G O 1
As we can see, the answer to David X  X  question has been updated after the updating of ongoing events in the conference. During this procedure, intermediate results { talk 2 }  X   X  recommend: and { talk 2 }  X  T alk 4 Dave have been preserved and reused to in-fer the new results, without being re-computed.

The way in which a TMS is constructed has significant impact on its structure and performance. A most straightforward way is to di-rectly connect reasoning results with their justifications. However this approach contradicts with the intention of stream reasoning as it omits all intermediate results. This leads to potentially redundant computation of justifications and potentially redundant edges. For example, if ;  X  Ans ( O ; Q ) and O | = only if O | = , then is needed to entail and J O ( )  X  J O ( ) . Obviously, when com-puting J O ( ) , the J O ( ) should be computed at the same time, and all edges from axioms in J O ( ) to can be replaceable by a single edge from to .

To overcome the above drawbacks and improve efficiency, one can also develop a glass-box approach. The basic idea is to cap-ture all the run-time states of a reasoner. For example, whenever a new belief is entailed from the ontology, we include it as a node of the TMS and connect it with the beliefs or assumptions we use to entail it, which should also be included as nodes if not included in previous reasoning steps. If an assumption is made, we include this assumption as a new node into the TMS. If it is withdrawn, we remove this assumption. All nodes derived from it, and all cor-responding edges, should also be removed, etc. In this approach, the construction of the TMS does not affect the termination, sound-ness, completeness of the original reasoning algorithms. It does not change the reasoning results. These are all because the TMS does not interfere with the reasoning procedure. It does not require addi-tional reasoning, and can be performed on-the-fly with reasoning.
Due to the fact that the completion-based algorithm of EL ++ uses entailments as antecedents and consequents of rules, and re-quires no assumption in reasoning, it naturally fit with the construc-tion of a TMS. And the glass-box approach for a completion-based algorithm can be described by the following algorithm A-1 : Algorithm A-1 : GlassboxT M S ( O ; Q ) INPUT : an ontology O , a reasoning request Q OUTPUT : a directed graph G =  X  N O ; E O  X  1: N O := O , E O := { ( ; ) |  X  O} 2: while a rule R = 5: return  X  N O ; E O  X  It is not difficult to show that the constructed directed graph is a TMS of O containing all results in Ans ( O ; Q ) : T HEOREM 1. For an ontology O and a reasoning request Q , GlassboxT M S ( O ; Q ) =  X  N O ; E O  X  as computed by A-1 is a TMS of O and Ans ( O ; Q )  X  N O .

TMS can thus be constructed and updated. First of all, given an EL ++ ontology O = ( T ; A ) , we internalize it into O  X  ( T  X  ;  X  ) and we initialise a TMS G =  X  N T  X  ; E T  X   X  as N and E T  X  = { ( ; ) |  X  A ;  X  T  X  and is internalised from }  X  { ( ; ) |  X  T  X  A} . Then we normalise the TBox T  X  its normal form T  X  X  . Meanwhile we extend the TMS as N T  X  and E T  X  := E T  X   X  { ( ; ) |  X  T  X  ;  X  T  X  X  and is nor-malised from } . Then in reasoning, we apply completion rules to add new entailments into T  X  X  , which is also the node set of the TMS. At the same time, we add edges between all antecedents and consequent into E T  X  . For example, for R1 , we have E E T  X   X  { ( X  X  A; X  X  B ) ; ( A  X  B; X  X  B ) } . For R5 , we have E
T  X  := E T  X   X  { ( X  X   X  r:A; X  X   X  ) ; ( A  X   X  ; X  X   X  ) When no rule can be executed, the reasoning terminates and the TMS is constructed. When the TMS is updated from O n 0 (n) to 0 (n+1), the same procedure is applied on the previous TMS. 5. OPTIMISING TMS IN EL ++
Although completion-based algorithms are suitable for stream reasoning with TMS. They could still unnecessarily consume some time and memory if not all intermediate results are contributing to answering the reasoning request. Taking EL ++ algorithm as an example, it performs the Classification of a TBox by computing results of form A  X  B and A  X   X  r:B . The former is useful when computing the concept hierarchies of the ontology, or retrieving the types of individuals (in form of { x }  X  A ), while the later is of less use in such scenarios. For example, in Fig. 4 and Fig. 6, we can see that inference results { talk 1 }  X   X  in:Session and {  X  in:Session are not needed.

In order to address this issue, we further optimise completion-based algorithms. The key point is, instead of directly performing the forward-chaining reasoning procedure and apply all executable rules, we develop some backward-chaining control mechanism to determine which intermediate results will NOT be useful and thus they are not needed to be computed.
 As we can see from Table 1, the intermediate results of the form A  X   X  r:B are involved in the following rules: 1. R3 , R7 and R8 use them to generate results of the same form. 2. R4 and R6 use them to generate new results of form A  X 
This means that, all results of form A  X   X  r:B will eventually contribute to inference of results of form A  X  B through R4 and R6 . If we know which axioms of form A  X   X  r:B will (not) be used in R4 and R6 , we can inductively know which of them will (not) be used in the entire reasoning process to compute all results of form A  X  B , without even executing any rule. Apparently, this is related to the role r in the entailment.

Given an internalised and normalised EL ++ TBox T , we as-sume that all role subsumption closures have been pre-computed [1]. We analyse the roles as follows:
In R4: If X  X   X  r:A , A  X  A  X  ,  X  r:A  X   X  B then X  X  B . It is obvious that antecedents of form X  X   X  r:A is worth entailed only if there is some  X  r:A  X   X  B  X  T . Note that the later will not be computed from any rule and can only exist in the original TBox. Therefore for a role r , if there is no axiom of form  X  r:A all results of form X  X   X  r:A will not be useful in R4 .
In R6: If X; A  X  { a } , X ; R A then X  X  A . It is obvious that antecedent axioms of form C j  X   X  r j :C j +1 is worth entailed only if there is some X  X  { a } entailed, which eventually requires there being some { a } appearing on the RHS (Right Hand Side) of some GCI. Therefore, if there is no nominal { a } on the RHS of any GCI, all results of form X  X   X  r:A will not be used in R6 .
From the above analysis, we define the classification-relevant roles as follows: D EFINITION 3. (Classification-relevant role) Given a normalised EL ++ TBox T , a role r is classification relevant (C-R for short) if r is on the LHS (left hand side) of some GCI, or there exists some GCI whose RHS (right hand side) is a nominal, or r is on the LHS of some RI whose RHS is a C-R role.
 The first two conditions correspond to R4 and R6 , respectively. The last condition corresponds to R7 and R8 . According to this definition, in our example, roles hasT opic , interest , recommend are C-R, and role in is not. All C-R roles can be identified in poly-nomial time. They control which entailment A  X   X  r:B will be useful in classification:
T HEOREM 2. When classifying an EL ++ TBox T with R1-8 , an entailed A  X   X  r:B will contribute to execution of R4 or R6 only if r is a C-R role.

In classification of an EL ++ TBox, we computes intermediate results of form A  X   X  r:B only if r is C-R. Thus the corresponding R3 , R7 and R8 should be revised as follows:
Such an optimisation is not only applicable on TMS construc-tion. It is a generic optimisation on EL ++ classification algorithm and preserves soundness and completeness of classification:
T HEOREM 3. For any normalised EL ++ TBox T , A  X  B can be inferred by R1-8 iff A  X  B can be inferred by R1-2, R3 X , R4-6, R7 X -8 X  .

When constructing TMS with a glass-box approach, the TMS enjoys the same optimisation. The construction of TMS will follow the same rule as before. The fact that some role r is C-R is not necessary to be maintained as a node. For example, the optimised TMS of O 1 0 (1) is illustrated in the Fig. 7:
Such modifications will help eliminate unnecessary intermedi-ate results and will not require additional reasoning. Therefore it should improve both the efficiency and scalability. We implemented the proposed approach and optimisations in our TrOWL 1 reasoner. We perform experiments to evaluate whether the optimisation presented in Sec.5 can improve the efficiency of TMS-enabled reasoning and reduce memory consumption, and fur-thermore how does such an optimised TMS-enabled EL ++ stream reasoner perform in reality. Therefore, we first evaluate the optimi-sation, then the stream reasoning. All experiments were conducted in an environment of 64-bit Windows 7 Enterprise with 1.60 GHz CPU and 3G RAM allocated to JVM 1.6.0.07.

In this optimisation evaluation, our test cases are two ontolo-gies generated from the Galen ontology 2 . Both of them are ontologies. The stats of these two ontologies are summarized in Table 2. Although they don X  X  contain ABox, the optimisation eval-uated here will have the similar effects on ABoxes.

In this evaluation we perform ontology classification to retrieve entailed subsumptions between all pairs of atomic concepts. For each of the ontologies, we classify it with two TMS-enabled reasoners. The one without optimisation implemented is called TMS . And the one with optimisation implemented is called TMS-Opt . For each reasoner, we measure the time used to classify the ontology. Due to the difficulty of precise measurements of the memory consumption, we calculate the number of intermediate re-sults of the form A  X   X  r:B for each reasoner. For TMS-Opt we further calculate the number of C-R roles. The results of our eval-uation is shown in Table 2.

From results we can see that in both ontologies, not all roles are necessary for classification. With TMS-Opt , the number of un-necessary intermediate results ( # Ext: ) is significantly reduced. In NotGalen  X  , only 15 : 3% is computed. In FullGalen  X  only 39 : 8% is computed. With less unintended and unnecessary inference, the ef-ficiency is improved ( 24% in NotGalen  X  and 44% in FullGalen Also, the memory consumption of maintaining such inference should also be significantly reduced.

With the positive results on TMS optimisations, we evaluate its performance in stream reasoning. Due to the lack of stream reason-http://tro wl.eu http://www.opengalen.org/ ing benchmark, we generated our test data from the FullGalen tology and simulated streams. We randomly partition the ontology into 45 sub-ontologies of same size. We call them K 0 ; K We construct an ontology stream O 10 0 in a way that O 10 0 ( i ) each update, 2 : 9% of the ontology is changed. Each snapshot con-tains about 30000 axioms. Each update changes about 855 axioms. For each snapshot O 10 0 ( i ) , we perform classification to compute the subsumption between all atomic concepts. The reasoner does not know which axioms will be updated in advance. We accomplish such a reasoning task with both the naive approach (re-do reason-ing on O 10 0 ( i ) from scratch) and the TMS-Opt . We record the time of both approaches to evaluate the effectiveness of the TMS-Opt . The results are summarised in the following table (Table 3). Table 3: Evaluation Results. t 0 shows the time for the initial ontology. M ax ( t i ) , M in ( t i ) and Ave ( t i ) show the maximal, minimal and average time for the updated ontologies, respec-tively. Time unit is second.

From the table we can see that, TMS-Opt took more time to classify the original ontology due to the construction of TMS. But in updating, TMS-Opt was significantly faster.

It is interesting to see how many percentages of the ontology can be updated while the TMS stream reasoning approach remains faster than the naive approach. To answer this question, we further increase to size of update to 3 : 33% , 5% and 10% of the original ontology, respectively. We perform classification on such ontol-ogy streams and calculate the relative time as ( Ave ( t i Ave ( t i ) TMS-Opt ) =Ave ( t i ) Naive . Our experiments and calculation yield 75 : 1% , 86 : 0% and 96 : 5% relative time for the above relative size of updates, respectively. From these results, we can see that although TMS gradually loses its efficiency advantage to Naive ap-proach when the relative volume of update increases, it still pays off when about 10% of ontology is changed. This figure is bet-ter than the maintenance program approach [14] and comparable to the fixed-window approach [6]. Considering the higher expressive power and difficulty of the test ontology FullGalen  X  and its large volume, our evaluation justifies the usability of our approach.
In this paper, we introduce Ontology Stream Management Sys-tems (OSMS). We focus on how to provide stream reasoning ser-vices based on TMS for OSMS. We chose EL ++ , the logic under-pinning the tractable OWL 2 EL language and developed a classification-relevant role optimisation for EL ++ by controlling the kind of in-termediate results that should be inferred. Evaluation on real world ontologies shows that our approach and optimisation work nicely in practice with relatively large volume of knowledge base and up-date. In the future, we would like to investigate stream reasoning in complete OWL 2 EL, QL and RL profiles, and even OWL 2 DL, as well as related optimisations.
 RCUK dot.rural and the EU K-Drive projects. [1] F. Baader, S. Brandt, and C. Lutz. Pushing the EL Envelope. [2] F. Baader, S. Brandt, and C. Lutz. Pushing the EL Envelope [3] F. Baader, C. Lutz, and B. Suntisrivaraporn. Is Tractable [4] F. Baader and B. Suntisrivaraporn. Debugging SNOMED CT [5] D. F. Barbieri, D. Braga, S. Ceri, E. D. Valle, and [6] D. F. Barbieri, D. Braga, S. Ceri, E. D. Valle, and [7] A. Bolles, M. Grawunder, and J. Jacobi. Streaming sparql [8] J. Doyle. A truth maintenance system. Artificial Intelligence , [9] A. Gupta, I. S. Mumick, and V. S. Subrahmanian.
 [10] Z. Huang and H. Stuckenschmidt. Reasoning with [11] M. Luther and S. Bohm. Situation-Aware Mobility: An [12] B. Parsia, C. Halaschek-wiener, and E. Sirin. E.s.: Towards [13] Y. Ren, J. Z. Pan, and Y. Zhao. Towards Scalable Reasoning [14] R. Volz, S. Staab, and B. Motik. Incementally maintaining
