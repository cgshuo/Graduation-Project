 Methods for detecting sentences in an input document set, which are both relevant and novel with respect to an information need, would be of direct benefit to many systems, such as extractive text summarizers. However, satisfactory levels of agreement between judges performing this task manually have yet to demonstrated, leaving researchers to conclude that the task is too subjective. In previous experiments, judges were asked to first identify sentences that are relevant to a general topic, and then to eliminate sentences from the list that do not contain new information. Currently, a new task is proposed, in which annotators perform the same procedure, but within the context of a specific, factual information need. In the experiment, satisfactory levels of agreement between independent annotators were achieved on the first step of identifying sentences containing relevant information relevant. However, the results in-dicate that judges do not agree on which sentences contain novel information.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Soft-ware; H.1.2 [ User/Machine Systems ]: Human factors Design, Experimentation, Human Factors Novelty, Summarization
A core challenge for retrieval systems is to find information that is not only relevant to a user X  X  need, but is also novel [1]. To this end, the task of  X  X ovelty detection, X  or identifying the textual  X 
This work was conducted while the first author was at the Univer-sity of Michigan X  X  School of Information.
 units that express interesting and previously unseen information, has been put forward. In contrast to systems that retrieve all rel-evant information (e.g. standard search engines), systems incor-porating novelty detection aim to reduce the amount of redundant information seen by the user.
 A major effort in sentence-level novelty detection was the TREC Novelty Track 1 . In this evaluation, the goal was to train systems that perform a two-stage task. Given a TREC topic query and a set of relevant documents, the systems should first retrieve all sentences that are relevant to the stated topic. In the second step, the systems should choose, from the list of relevant sentences, the novel sentences, defined as those containing  X  X reviously unseen in-formation X  [3, 5]. Several problems were noted by the organizers in creating the manually-labeled data sets for the training and eval-uation of the systems. For example, in the first year of the track, the assessors chose few relevant sentences, resulting in many negative and few positive relevance examples available for training systems. While this was resolved in later years, one persistent problem was the large assessor effect, such that the judges did not reach a high level of agreement as to which sentences were relevant and novel.
Recently, Schiffman created his own corpus of novelty judg-ments, and tried to improve on the TREC annotation results by showing judges only two news articles at a time [4]. One article was presented as background information, while the other was consid-ered a new document. The judges were then to choose the spans of text in the new document that presented novel information not con-tained in the background article. However, the new annotation task also yielded a low rate of agreement between independent judges.
Currently, we propose a new sentence-level annotation task -fact-based relevance and novelty detection. We assume that a user has a general topic of interest, and has identified a set of relevant documents. Next, we assume that the user has a set of specific facts of interest about the topic. For simplicity, the user states each fact as a natural language question. Our task is, for a given fact, to first identify the set of sentences in the document set that contain rel-evant information. A sentence contains relevant information only if it provides an answer to the question. In a second step, only the sentences containing previously unseen information about the fact of interest are kept. In the current paper, we evaluate the repro-ducibility of the two steps of the proposed task.
The data for our experiment come from the 2003 TREC Novelty track test data [5]. Novelty track clusters consist of a topic query and a set of 25 relevant news articles. We chose two of the  X  X vent X  http : //trec.nist.gov clusters for our experiment and show their attributes in Table 1. We read through all of the documents in each of the two stories, and created a list of ten factual questions that are central to each story. The questions ask about key facts in the stories, that may change with time as news sources publish additional information, and that expect atomic answers such as a number, name of a person, or a place.

Six paid subjects were hired for the study. Three were randomly assigned to the test (fact-based) setting and three to the control (topic-based) setting. Each subject performed the respective task on both clusters. In both settings, the judges were given the 25 documents in a given cluster, ordered chronologically. Judges in the control setting were given the TREC topic query, while those in the test setting were given the set of factual questions. Both sets of subjects were asked to first familiarize themselves with the news story by reading either the given query or set of questions, as well as by skimming through the news articles. Judges in the con-trol setting were asked to complete the TREC task -that is, to first identify those sentences that are relevant to the topic, and then to reread the relevant sentences, eliminating those that do not contain information that has not been previously seen [5]. Annotators in the test setting were to find, for each question, the set of relevant sentences containing an answer to the question. They were then asked to reread through their set of sentences in chronological or-der, eliminating those that do not provide a new answer to the given question.
Table 2 shows the agreement on the task of finding sentences that are relevant to the topic (in the control setting) and relevant to the factual questions (in the test setting). For the topic-based judgments, over the two clusters N4 and N33, there were a total of 1,636 sentence judgments. The three judges agreed on the rel-evance status of 39% of the sentences. The corresponding Kappa statistic, which factors out the expected (chance) agreement, is also shown [2]. In the fact-based setting, there are a total of 16,360 sentence-level relevance judgments (i.e. 10 questions for each doc-ument cluster). The three subjects X  judgments were in agreement in 99% of these cases, which corresponds to a Kappa of 0.67. The agreement on the novelty judgments is shown in Table 3. In order to calculate novelty agreement, we first found the union of the judges X  sets of relevant sentences. In other words, we con-sidered the agreement on novelty status among the sentences that any judge had labeled as being relevant. As can be seen in the ta-ble, the three judges agreed on 21% of the sentences in the control setting. This level of agreement is actually less than what is ex-pected by chance (corresponding to a negative Kappa of -0.06). In the fact-based setting, we see that the judges agreed on 52% of the sentences. However, while agreement is better in the test setting as compared to the topic-based setting, a Kappa of 0.18 certainly does not indicate a sufficiently high level of agreement on the fact-based novelty annotation task.
We proposed the problem of fact-focused relevance and novelty detection at the sentence level. We conducted a preliminary evalua-tion of its reproducibility, both on the relevance and novelty stages of the task. In addition, we reproduced the TREC Novelty anno-tation experiment, in which the judges found relevant and novel sentences with respect to a general topic query. Our results thus far suggest that a higher level of agreement can be reached on relevance judgments when they are constrained to apply to a set of factual questions, as compared to the case of topic-based judg-ments. In addition, while the agreement on novelty judgments in the fact-based task was better than in the topic-based case, the level of agreement was not very satisfactory. In our immediate future work, we will further analyze the data from the current experiment in a number of ways. In particular, we plan to qualitatively analyze the annotations of our judges in order to determine if there are fea-tures that can be used to identify sentences upon which judges are unlikely to agree as to relevance and novelty status. This work was partially supported by the U.S. National Science Foundation under the following grant: 0329043  X  X robabilistic and link-based Methods for Exploiting Very Large Textual Reposito-ries X  administered through the IDM program. All opinions, find-ings, conclusions, and recommendations in this paper are made by the authors and do not necessarily reflect the views of the National Science Foundation. The authors would like to thank the members of the CLAIR research group and the anonymous SIGIR reviewers for their feedback and comments on this work. [1] J. Allan, B. Carterette, and J. Lewis. When Will Information [2] J. Carletta. Assessing Agreement on Classification Tasks: The [3] D. Harman. Overview of the TREC 2002 novelty track, 2002. [4] B. Schiffman. Learning to Identify New Information .PhD [5] I. Soboroff and D. Harman. Overview of the TREC 2003
