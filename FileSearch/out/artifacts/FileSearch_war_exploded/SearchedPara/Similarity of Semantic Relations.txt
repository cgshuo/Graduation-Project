 National Research Council Canada
There are at least two kinds of similarity. Relational similarity is correspondence between re-
When two words have a high degree of attributional similarity, we call them synonyms .When two pairs of words have a high degree of relational similarity, we say that their relations are analogous . For example, the word pair mason:stone is analogous to the pair carpenter:wood. word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score
VSM approach, the relation between a pair of words is characterized by a vector of frequencies
The patterns are derived automatically from the corpus, (2) the Singular Value Decomposition used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions, semantic relations, LRA achieves similar gains over the VSM. 1. Introduction
There are at least two kinds of similarity. Attributional similarity is correspondence be-tween attributes and relational similarity is correspondence between relations (Medin,
Goldstone, and Gentner 1990). When two words have a high degree of attributional similarity, we call them synonyms . When two word pairs have a high degree of relational similarity, we say they are analogous .
 a riverbed. A street carries traffic; a riverbed carries water. There is a high degree of
In fact, this analogy is the basis of several mathematical theories of traffic flow (Daganzo 1994).
 relational similarity can be reduced to attributional similarity, since mason and carpen-ter are attributionally similar, as are stone and wood. In general, this reduction fails. similar. Street and riverbed are only moderately attributionally similar.
 ity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and
Pedersen 2003). Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms (Landauer and Dumais 1997), information retrieval (Deerwester et al. 1990), determining semantic orientation (Turney 2002), grading student essays (Rehder et al. 1998), measuring textual cohesion (Morris and Hirst 1991), and word sense disambiguation (Lesk 1986).
 as measures of attributional similarity, the potential applications of relational similarity are not as well known. Many problems that involve semantic relations would benefit from an algorithm for measuring relational similarity. We discuss related problems in natural language processing, information retrieval, and information extraction in more detail in Section 3.
 a query, a search engine produces a ranked list of documents. The documents are ranked in order of decreasing attributional similarity between the query and each document. Almost all modern search engines measure attributional similarity using the VSM (Baeza-Yates and Ribeiro-Neto 1999). Turney and Littman (2005) adapt the
VSM approach to measuring relational similarity. They used a vector of frequencies of patterns in a corpus to represent the relation between a pair of words. Section 4 presents the VSM approach to measuring similarity.
 we call Latent Relational Analysis (LRA). The algorithm learns from a large corpus of unlabeled, unstructured text, without supervision. LRA extends the VSM approach of Turney and Littman (2005) in three ways: (1) The connecting patterns are derived automatically from the corpus, instead of using a fixed set of patterns. (2) Singular Value
Decomposition (SVD) is used to smooth the frequency data. (3) Given a word pair such as traffic:street, LRA considers transformations of the word pair, generated by replacing one of the words by synonyms, such as traffic : road or traffic : highway .
 multiple-choice word analogy questions from the SAT college entrance exam. ample of a typical SAT question appears in Table 1. In the educational testing literature, solution and the incorrect choices are distractors . We evaluate LRA by testing its ability to select the solution and avoid the distractors. The average performance of college-bound senior high school students on verbal SAT questions corresponds to an accuracy of about 57%. LRA achieves an accuracy of about 56%. On these same questions, the
VSM attained 47%. 380 modifier pairs (Turney and Littman 2005). In Section 7, we evaluate the performance of LRA with a set of 600 noun-modifier pairs from Nastase and Szpakowicz (2003).
The problem is to classify a noun-modifier pair, such as  X  X aser printer, X  according to the semantic relation between the head noun (printer) and the modifier (laser). The 600 pairs have been manually labeled with 30 classes of semantic relations. For example,  X  X aser printer X  is classified as instrument ; the printer uses the laser as an instrument for printing.
 supervised learning problem. The 600 pairs are divided into training and testing sets and a testing pair is classified according to the label of its single nearest neighbor in the training set. LRA is used to measure distance (i.e., similarity, nearness). LRA achieves an accuracy of 39.8% on the 30-class problem and 58.0% on the 5-class problem. On the same 600 noun-modifier pairs, the VSM had accuracies of 27.8% (30-class) and 45.7% (5-class) (Turney and Littman 2005).
 tion 8 and we conclude in Section 9. 2. Attributional and Relational Similarity
In this section, we explore connections between attributional and relational similarity. 2.1 Types of Similarity Medin, Goldstone, and Gentner (1990) distinguish attributes and relations as follows:
Gentner (1983) notes that what counts as an attribute or a relation can depend on the context. For example, large can be viewed as an attribute of X , between X and some standard Y , LARGER THAN ( X , Y ).
 on the degree of correspondence between the properties of A and B . A measure of attributional similarity is a function that maps two words, A and B , to a real number, sim a ( A , B ) the greater their attributional similarity. For example, dog and wolf have a relatively high degree of attributional similarity.
 depends on the degree of correspondence between the relations between A and B and the relations between C and D . A measure of relational similarity is a function that maps two pairs, A : B and C : D , to a real number, sim correspondence there is between the relations of A : B and C : D , the greater their relational similarity. For example, dog : bark and cat : meow have a relatively high degree of relational similarity.
 from words that are semantically similar ( deer X  X ony ), although they recognize that some words are both associated and similar ( doctor X  X urse ) (Chiarello et al. 1990). Both of these are types of attributional similarity, since they are based on correspondence between mammals).

As these examples show, semantic relatedness is the same as attributional similarity (e.g., hot and cold are both kinds of temperature, pencil and paper are both used for writing). Here we prefer to use the term attributional similarity because it emphasizes the contrast with relational similarity. The term semantic relatedness may lead to confusion when the term relational similarity is also under discussion.

Thus semantic similarity is a specific type of attributional similarity. The term semantic similarity is misleading, because it refers to a type of attributional similarity, yet rela-tional similarity is not any less semantic than attributional similarity.
 similarity, following Medin, Goldstone, and Gentner (1990). Instead of semantic sim-taxonomical similarity , which we take to be a specific type of attributional similarity. We interpret synonymy as a high degree of attributional similarity. Analogy is a high degree of relational similarity. 382 2.2 Measuring Attributional Similarity Algorithms for measuring attributional similarity can be lexicon-based (Lesk 1986; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003), corpus-based (Lesk 1969;
Landauer and Dumais 1997; Lin 1998a; Turney 2001), or a hybrid of the two (Resnik 1995; Jiang and Conrath 1997; Turney et al. 2003). Intuitively, we might expect that lexicon-based algorithms would be better at capturing synonymy than corpus-based algorithms, since lexicons, such as WordNet, explicitly provide synonymy in-formation that is only implicit in a corpus. However, experiments do not support this intuition.
 tions taken from the Test of English as a Foreign Language (TOEFL). An example of one of the 80 TOEFL questions appears in Table 2. Table 3 shows the best performance on the TOEFL questions for each type of attributional similarity algorithm. The results support the claim that lexicon-based algorithms have no advantage over corpus-based algorithms for recognizing synonymy. 2.3 Using Attributional Similarity to Solve Analogies
We may distinguish near analogies ( mason : stone :: carpenter : wood )from far anal-In an analogy A : B :: C : D , where there is a high degree of relational similarity between analogy.
 in which case they can be solved using attributional similarity measures. We could score each candidate analogy by the average of the attributional similarity, sim and C and between B and D :
This kind of approach was used in two of the thirteen modules in Turney et al. (2003) (see Section 3.1).
 our collection of 374 SAT questions. The performance of the algorithms was measured by precision, recall, and F , defined as follows:
Note that recall is the same as percent correct (for multiple-choice questions, with only zero or one guesses allowed per question, but not in general).
 example, using the algorithm of Hirst and St-Onge (1998), 120 questions were answered correctly, 224 incorrectly, and 30 questions were skipped. When the algorithm assigned the same similarity to all of the choices for a given question, that question was skipped. The precision was 120 / (120 + 224) and the recall was 120 / (120 + 224 + 30). Similarity package. 2 The sixth algorithm (Turney 2001) used the Waterloo MultiText System (WMTS), as described in Terra and Clarke (2003).
 dom guessing is statistically significant with 95% confidence, according to the Fisher
Exact Test (Agresti 1990). However, the difference between the highest performance (Turney 2001) and the VSM approach (Turney and Littman 2005) is also statistically significant with 95% confidence. We conclude that there are enough near analogies in the 374 SAT questions for attributional similarity to perform better than random guessing, but not enough near analogies for attributional similarity to perform as well as relational similarity. 384 3. Related Work
This section is a brief survey of the many problems that involve semantic relations and could potentially make use of an algorithm for measuring relational similarity. 3.1 Recognizing Word Analogies
The problem of recognizing word analogies is, given a stem word pair and a finite list of choice word pairs, selecting the choice that is most analogous to the stem. This problem was first attempted by a system called Argus (Reitman 1965), using a small hand-built semantic network. Argus could only solve the limited set of analogy questions that its programmer had anticipated. Argus was based on a spreading activation model and did not explicitly attempt to measure relational similarity.

The final output of the system was based on a weighted combination of the outputs of each individual module. The best of the 13 modules was the VSM, which is described in detail in Turney and Littman (2005). The VSM was evaluated on a set of 374 SAT questions, achieving a score of 47%.
 (2004) applied a lexicon-based approach to the same 374 SAT questions, attaining a score of 43%. Veale evaluated the quality of a candidate analogy A : B :: C : D by looking for paths in WordNet, joining A to B and C to D . The quality measure was based on the similarity between the A : B paths and the C : D paths.
 of the VSM approach, which reached 56% on the 374 SAT questions. Here we go beyond Turney (2005) by describing LRA in more detail, performing more extensive experiments, and analyzing the algorithm and related work in more depth. 3.2 Structure Mapping Theory
French (2002) cites Structure Mapping Theory (SMT) (Gentner 1983) and its imple-mentation in the Structure Mapping Engine (SME) (Falkenhainer, Forbus, and Gentner 1989) as the most influential work on modeling of analogy making. The goal of com-putational modeling of analogy making is to understand how people form complex, structured analogies. SME takes representations of a source domain and a target domain and produces an analogical mapping between the source and target. The domains are given structured propositional representations, using predicate logic. These de-scriptions include attributes, relations, and higher-order relations (expressing relations between relations). The analogical mapping connects source domain relations to target domain relations.
 of the atom (Falkenhainer, Forbus, and Gentner 1989). The solar system is the source domain and Rutherford X  X  model of the atom is the target domain. The basic objects in the source model are the planets and the sun. The basic objects in the target model are the electrons and the nucleus. The planets and the sun have various attributes, such as mass(sun) and mass(planet), and various relations, such as revolve(planet, sun) and attracts(sun, planet). Likewise, the nucleus and the electrons have attributes, such as charge(electron) and charge(nucleus), and relations, such as revolve(electron, nucleus) and attracts(nucleus, electron). SME maps revolve(planet, sun) to revolve(electron, nu-cleus) and attracts(sun, planet) to attracts(nucleus, electron).
 cleus)) in an analogical mapping implies that the connected relations are similar; thus,
SMT requires a measure of relational similarity in order to form maps. Early versions of SME only mapped identical relations, but later versions of SME allowed similar, nonidentical relations to match (Falkenhainer 1990). However, the focus of research in analogy making has been on the mapping process as a whole, rather than measuring the similarity between any two particular relations; hence, the similarity measures used in SME at the level of individual connections are somewhat rudimentary.
 may enhance the performance of SME. Likewise, the focus of our work here is on the similarity between particular relations, and we ignore systematic mapping between sets of relations, so LRA may also be enhanced by integration with SME. 3.3 Metaphor
Metaphorical language is very common in our daily life, so common that we are usually unaware of it (Lakoff and Johnson 1980). Gentner et al. (2001) argue that novel metaphors are understood using analogy, but conventional metaphors are simply recalled from memory. A conventional metaphor is a metaphor that has become entrenched in our language (Lakoff and Johnson 1980). Dolan (1995) describes an algorithm that can recognize conventional metaphors, but is not suited to novel metaphors. This suggests that it may be fruitful to combine Dolan X  X  (1995) algorithm for handling conventional metaphorical language with LRA and SME for handling novel metaphors.
 claim that metaphorical language is ubiquitous. The metaphors in their sample sen-tences can be expressed using SAT-style verbal analogies of the form A : B :: C : D .Thefirst column in Table 5 is a list of sentences from Lakoff and Johnson (1980) and the second column shows how the metaphor that is implicit in each sentence may be made explicit as a verbal analogy. 3.4 Classifying Semantic Relations 386 interesting relations, such as antonymy, that do not occur in noun-modifier pairs. How-instance, WordNet 2.0 contains more than 26,000 noun-modifier pairs, although many common noun-modifiers are not in WordNet, especially technical terms.
 modifier relations in the medical domain, using Medical Subject Headings (MeSH) and
Unified Medical Language System (UMLS) as lexical resources for representing each noun-modifier pair with a feature vector. They trained a neural network to distinguish 13 classes of semantic relations. Nastase and Szpakowicz (2003) explore a similar ap-domain, such as medicine), using WordNet and Roget X  X  Thesaurus as lexical resources.
Vanderwende (1994) used hand-built rules, together with a lexical knowledge base, to classify noun-modifier pairs.
 any classification of semantic relations necessarily employs some implicit notion of re-lational similarity since members of the same class must be relationally similar to some extent. Barker and Szpakowicz (1998) tried a corpus-based approach that explicitly used which limited its ability to generalize. Moldovan et al. (2004) also used a measure of relational similarity based on mapping each noun and modifier into semantic classes in WordNet. The noun-modifier pairs were taken from a corpus, and the surrounding context in the corpus was used in a word sense disambiguation algorithm to improve the mapping of the noun and modifier into WordNet. Turney and Littman (2005) used the VSM (as a component in a single nearest neighbor learning algorithm) to measure relational similarity. We take the same approach here, substituting LRA for the VSM, in Section 7.
 modifier pairs by inserting the prepositions of, for, in, at, on, from, with, and about . For example, reptile haven was paraphrased as haven for reptiles . Lapata and Keller (2004) achieved improved results on this task by using the database of AltaVista X  X  search engine as a corpus. 3.5 Word Sense Disambiguation
We believe that the intended sense of a polysemous word is determined by its semantic relations with the other words in the surrounding text. If we can identify the semantic relations between the given word and its context, then we can disambiguate the given word. Yarowsky X  X  (1993) observation that collocations are almost always monosemous is evidence for this view. Federici, Montemagni, and Pirrelli (1997) present an analogy-based approach to word sense disambiguation.
 dustrial plant or a living organism. Suppose plant appears in some text near food .A typical approach to disambiguating plant would compare the attributional similarity of food and industrial plant to the attributional similarity of food and living organism since industrial plants often produce food and living organisms often serve as food. It would be very helpful to know the relation between food and plant in this example. In the phrase  X  X ood for the plant, X  the relation between food and plant strongly suggests text  X  X ood at the plant, X  the relation strongly suggests that the plant is an industrial plant, since living organisms are not usually considered as locations. Thus, an algorithm disambiguation. 3.6 Information Extraction
The problem of relation extraction is, given an input document and a specific relation R , to extract all pairs of entities (if any) that have the relation R in the document. The prob-lem was introduced as part of the Message Understanding Conferences (MUC) in 1998.
Zelenko, Aone, and Richardella (2003) present a kernel method for extracting the relations person X  X ffiliation and organization X  X ocation . For example, in the sentence John
Smith is the chief scientist of the Hardcom Corporation, there is a person X  X ffiliation relation between John Smith and Hardcom Corporation (Zelenko, Aone, and Richardella 2003). that information extraction focuses on the relation between a specific pair of entities extraction.
 for instance. Each example would be represented by a vector of pattern frequencies.
Given a specific document discussing John Smith and Hardcom Corporation, we could construct a vector representing the relation between these two entities and then mea-training vectors. It would seem that there is a problem here because the training vectors would be relatively dense, since they would presumably be derived from a large corpus, but the new unlabeled vector for John Smith and Hardcom Corpora-given document. However, this is not a new problem for the VSM; it is the standard situation when the VSM is used for information retrieval. A query to a search en-gine is represented by a very sparse vector, whereas a document is represented by a relatively dense vector. There are well-known techniques in information retrieval for coping with this disparity, such as weighting schemes for query vectors that are different from the weighting schemes for document vectors (Salton and Buckley 1988). 388 3.7 Question Answering
In their article on classifying semantic relations, Moldovan et al. (2004) suggest that an important application of their work is question answering (QA). As defined in the Text
Retrieval Conference (TREC) QA track, the task is to answer simple questions, such as  X  X here have nuclear incidents occurred? X , by retrieving a relevant document from a large corpus and then extracting a short string from the document, such as The Three
Mile Island nuclear incident caused a DOE policy crisis . Moldovan et al. (2004) propose to map a given question to a semantic relation and then search for that relation in a corpus of semantically tagged text. They argue that the desired semantic relation can easily be inferred from the surface form of the question. A question of the form  X  X here...? X  is likely to be looking for entities with a location relation and a question of the form  X  X hat did ... make? X  is likely to be looking for entities with a product relation. In Section 7, we show how LRA can recognize relations such as location and product (see Table 19). 3.8 Automatic Thesaurus Generation corpus and Berland and Charniak (1999) describe how to learn meronym ( part of ) relations from a corpus. These algorithms could be used to automatically generate a thesaurus or dictionary, but we would like to handle more relations than hyponymy and meronymy. WordNet distinguishes more than a dozen semantic relations between words (Fellbaum 1998) and Nastase and Szpakowicz (2003) list 30 semantic relations for noun-modifier pairs. Hearst and Berland and Charniak (1999) use manually generated rules to mine text for semantic relations. Turney and Littman (2005) also use a manually generated set of 64 patterns.

Instead of manually generating new rules or patterns for each new semantic relation, it is possible to automatically learn a measure of relational similarity that can handle arbitrary semantic relations. A nearest neighbor algorithm can then use this relational similarity measure to learn to classify according to any set of classes of relations, given the appropriate labeled training data.
 relations from a corpus. Like Hearst (1992) and Berland and Charniak (1999), they use manually generated rules to mine text for their desired relation. However, they supplement their manual rules with automatically learned constraints, to increase the precision of the rules. 3.9 Information Retrieval
Veale (2003) has developed an algorithm for recognizing certain types of word analo-gies, based on information in WordNet. He proposes to use the algorithm for analog-ical information retrieval. For example, the query Muslim church should return mosque and the query Hindu bible should return the Vedas. The algorithm was de-Christian : church :: Muslim : mosque .
 high relational similarity between the pair A : X and the pair Y : B . For example, given
A = Muslim and B = church ,return X = mosque and Y = Christian. (The pair Muslim : mosque has a high relational similarity to the pair Christian : church .) by clustering words from two different corpora. Each cluster of words in one corpus is coupled one-to-one with a cluster in the other corpus. For example, one experiment used a corpus of Buddhist documents and a corpus of Christian documents. A cluster of words such as { Hindu, Mahayana, Zen, ... } from the Buddhist corpus was coupled with a cluster of words such as { Catholic, Protestant, ... } from the Christian corpus. Thus the algorithm appears to have discovered an analogical mapping between Buddhist schools and traditions and Christian schools and traditions. This is interesting work, but it is not directly applicable to SAT analogies, because it discovers analogies between clusters of words rather than individual words. 3.10 Identifying Semantic Roles
A semantic frame for an event such as judgement contains semantic roles such as judge , evaluee ,and reason , whereas an event such as statement contains roles such as speaker , addressee ,and message (Gildea and Jurafsky 2002). The task of identifying semantic roles is to label the parts of a sentence according to their semantic roles. We believe that it may be helpful to view semantic frames and their semantic roles as sets of semantic relations; thus, a measure of relational similarity should help us to identify semantic semantic relations (Section 3.4), since semantic roles always involve verbs or predicates, but semantic relations can involve words of any part of speech. 4. The Vector Space Model
This section examines past work on measuring attributional and relational similarity using the VSM. 4.1 Measuring Attributional Similarity with the Vector Space Model
The VSM was first developed for information retrieval (Salton and McGill 1983; Salton (Baeza-Yates and Ribeiro-Neto 1999). In the VSM approach to information retrieval, queries and documents are represented by vectors. Elements in these vectors are based on the frequencies of words in the corresponding queries and documents. The frequen-cies are usually transformed by various formulas and weights, tailored to improve the effectiveness of the search engine (Salton 1989). The attributional similarity between a query and a document is measured by the cosine of the angle between their correspond-ing vectors. For a given query, the search engine sorts the matching documents in order of decreasing cosine.
 words (Lesk 1969; Ruge 1992; Pantel and Lin 2002). Pantel and Lin (2002) clustered words according to their attributional similarity, as measured by a VSM. Their algo-rithm is able to discover the different senses of polysemous words, using unsupervised learning.
 using the Singular Value Decomposition (SVD) to smooth the vectors, which helps 390 to handle noise and sparseness in the data (Deerwester et al. 1990; Dumais 1993;
Landauer and Dumais 1997). SVD improves both document-query attributional sim-ilarity measures (Deerwester et al. 1990; Dumais 1993) and word X  X ord attributional similarity measures (Landauer and Dumais 1997). LRA also uses SVD to smooth vec-tors, as we discuss in Section 5. 4.2 Measuring Relational Similarity with the Vector Space Model
Let R 1 be the semantic relation (or set of relations) between a pair of words, A and B , and let R 2 be the semantic relation (or set of relations) between another pair, C and D .
We wish to measure the relational similarity between R 1 and R are not given to us; our task is to infer these hidden (latent) relations and then compare them.
 vectors, r r r 1 and r r r 2 , that represent features of R of R 1 and R 2 by the cosine of the angle  X  between r 1 and r by counting the frequencies of various short phrases containing X and Y . Turney and
Littman (2005) use a list of 64 joining terms, such as of, for, and to, to form 128 phrases that contain X and Y , such as XofY , YofX , XforY , YforX , XtoY ,and YtoX . These phrases are then used as queries for a search engine and the number of hits (matching documents) is recorded for each query. This process yields a vector of 128 numbers.
If the number of hits for a query is x , then the corresponding element in the vector r r r is log( x + 1). Several authors report that the logarithmic transformation of frequencies improves cosine-based similarity measures (Salton and Buckley 1988; Ruge 1992; Lin 1998b).

SAT analogy questions, achieving a score of 47%. Since there are five choices for each question, the expected score for random guessing is 20%. To answer a multiple-choice analogy question, vectors are created for the stem pair and each choice pair, and then cosines are calculated for the angles between the stem pair and each choice pair. The best guess is the choice pair with the highest cosine. We use the same set of analogy questions to evaluate LRA in Section 6. in a supervised nearest neighbor classifier for noun-modifier semantic relations (Turney and Littman 2005). The evaluation used 600 hand-labeled noun-modifier pairs from
Nastase and Szpakowicz (2003). A testing pair is classified by searching for its single nearest neighbor in the labeled training data. The best guess is the label for the training pair with the highest cosine. LRA is evaluated with the same set of noun-modifier pairs in Section 7.
 information required to build vectors for the VSM. Thus their corpus was the set of all
Web pages indexed by AltaVista. At the time, the English subset of this corpus consisted of about 5  X  10 11 words. Around April 2004, AltaVista made substantial changes to their search engine, removing their advanced search operators. Their search engine no longer supports the asterisk operator, which was used by Turney and Littman (2005) for stemming and wild-card searching. AltaVista also changed their policy toward automated searching, which is now forbidden. 3 documents (Web pages) matching a given query, but LRA uses the number of passages (strings) matching a query. In our experiments with LRA (Sections 6 and 7), we use a lo-cal copy of the Waterloo MultiText System (WMTS) (Clarke, Cormack, and Palmer 1998;
Terra and Clarke 2003), running on a 16 CPU Beowulf Cluster, with a corpus of about 5  X  10 10 English words. The WMTS is a distributed (multiprocessor) search engine, designed primarily for passage retrieval (although document retrieval is possible, as a special case of passage retrieval). The text and index require approximately one terabyte of disk space. Although AltaVista only gives a rough estimate of the number of match-ing documents, the WMTS gives exact counts of the number of matching passages. performance of LRA significantly surpasses this combined system, but there is no real contest between these approaches, because we can simply add LRA to the combination, as a fourteenth module. Since the VSM module had the best performance of the 13 modules (Turney et al. 2003), the following experiments focus on comparing VSM and
LRA. 5. Latent Relational Analysis
LRA takes as input a set of word pairs and produces as output a measure of the relational similarity between any two of the input pairs. LRA relies on three resources, a search engine with a very large corpus of text, a broad-coverage thesaurus of synonyms, and an efficient implementation of SVD.
 subsections, we will give a detailed description of the algorithm, as it is applied in the experiments in Sections 6 and 7.
 392 occur rarely in the corpus. The hope is that we can find near analogies for the original pairs, such that the near analogies co-occur more frequently in the corpus. The danger is that the alternates may have different relations from the originals. The filtering steps above aim to reduce this risk. 5.1 Input and Output
In our experiments, the input set contains from 600 to 2,244 word pairs. The output similarity measure is based on cosines, so the degree of similarity can range from (dissimilar;  X  = 180  X  )to + 1 (similar;  X  = 0  X  ). Before applying SVD, the vectors are completely non-negative, which implies that the cosine can only range from 0 to + 1, but
SVD introduces negative values, so it is possible for the cosine to be negative, although we have never observed this in our experiments. 5.2 Search Engine and Corpus In the following experiments, we use a local copy of the WMTS (Clarke, Cormack, and
Palmer 1998; Terra and Clarke 2003). 4 The corpus consists of about 5 words, gathered by a Web crawler, mainly from US academic Web sites. The Web pages cover a very wide range of topics, styles, genres, quality, and writing skill. The WMTS is well suited to LRA, because the WMTS scales well to large corpora (one terabyte, in our case), it gives exact frequency counts (unlike most Web search engines), it is designed for passage retrieval (rather than document retrieval), and it has a powerful query syntax. 5.3 Thesaurus
As a source of synonyms, we use Lin X  X  (1998a) automatically generated thesaurus. This thesaurus is available through an on-line interactive demonstration or it can be down-loaded. 5 We used the on-line demonstration, since the downloadable version seems to contain fewer words. For each word in the input set of word pairs, we automatically query the on-line demonstration and fetch the resulting list of synonyms. As a cour-tesy to other users of Lin X  X  on-line system, we insert a 20-second delay between each two queries.
 consisting of text from the Wall Street Journal, San Jose Mercury ,and AP Newswire (Lin 1998a). The parser was used to extract pairs of words and their grammatical relations.
Words were then clustered into synonym sets, based on the similarity of their grammat-ical relations. Two words were judged to be highly similar when they tended to have the same kinds of grammatical relations with the same sets of words. Given a word and its part of speech, Lin X  X  thesaurus provides a list of words, sorted in order of decreasing attributional similarity. This sorting is convenient for LRA, since it makes it possible to focus on words with higher attributional similarity and ignore the rest. WordNet, in contrast, given a word and its part of speech, provides a list of words grouped by the possible senses of the given word, with groups sorted by the frequencies of the senses.
WordNet X  X  sorting does not directly correspond to sorting by degree of attributional similarity, although various algorithms have been proposed for deriving attributional similarity from WordNet (Resnik 1995; Jiang and Conrath 1997; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). 5.4 Singular Value Decomposition
We use Rohde X  X  SVDLIBC implementation of the SVD, which is based on SVDPACKC (Berry 1992). 6 In LRA, SVD is used to reduce noise and compensate for sparseness. 394 5.5 The Algorithm
We will go through each step of LRA, using an example to illustrate the steps. Assume that the input to LRA is the 374 multiple-choice SAT word analogy questions of Turney choices), the input consists of 2,244 word pairs. Let X  X  suppose that we wish to calculate the relational similarity between the pair quart : volume and the pair mile : distance , taken from the SAT question in Table 6. The LRA algorithm consists of the following 12 steps: 1. Find alternates: For each word pair A : B in the input set, look in Lin X  X  2. Filter alternates: For each original pair A : B ,filterthe2 3. Find phrases: For each pair (originals and alternates), make a list of 4. Find patterns: For each phrase found in the previous step, build patterns 396 5. Map pairs to rows: In preparation for building the matrix X ,createa 6. Map patterns to columns: Create a mapping of the top num patterns 7. Generate a sparse matrix: Generate a matrix X in sparse matrix format, 8. Calculate entropy: Apply log and entropy transformations to the sparse 9. Apply SVD: After the log and entropy transformations have been applied 10. Projection: Calculate U k  X  k (we use k = 300). This matrix has the same 11. Evaluate alternates: Let A : B and C : D be any two word pairs in the input 398 12. Calculate relational similarity: The relational similarity between A : B and
Steps 11 and 12 can be repeated for each two input pairs that are to be compared. This completes the description of LRA.
 the solution for this question; LRA answers the question correctly. For comparison, column 2 gives the cosines for the original pairs and column 3 gives the highest cosine.
For this particular SAT question, there is one choice that has the highest cosine for all three columns, choice (b), although this is not true in general. Note that the gap between the first choice (b) and the second choice (d) is largest for the average cosines (column 1). This suggests that the average of the cosines (column 1) is better at discriminating the correct choice than either the original cosine (column 2) or the highest cosine (column 3). 6. Experiments with Word Analogy Questions
This section presents various experiments with 374 multiple-choice SAT word analogy questions. 6.1 Baseline LRA System
Table 12 shows the performance of the baseline LRA system on the 374 SAT questions, using the parameter settings and configuration described in Section 5. LRA correctly answered 210 of the 374 questions; 160 questions were answered incorrectly and 4 questions were skipped, because the stem pair and its alternates were represented by zero vectors. The performance of LRA is significantly better than the lexicon-based approach of Veale (2004) (see Section 3.1) and the best performance using attributional (Agresti 1990).
 and meaningful relation between the solution words, whereas the distractors may only occur together rarely because they have no meaningful relation. This strategy is signif-cantly worse than random guessing. The opposite strategy, always guessing the choice pair with the lowest co-occurrence frequency, is also worse than random guessing (but not significantly). It appears that the designers of the SAT questions deliberately chose distractors that would thwart these two strategies.
 400 there are 2,244 pairs in the input set. In step 2, introducing alternate pairs multiplies the number of pairs by four, resulting in 8,976 pairs. In step 5, for each pair A : B ,weadd
B : A , yielding 17,952 pairs. However, some pairs are dropped because they correspond to zero vectors (they do not appear together in a window of five words in the WMTS corpus). Also, a few words do not appear in Lin X  X  thesaurus, and some word pairs appear twice in the SAT questions (e.g., lion:cat). The sparse matrix (step 7) has 17,232 rows (word pairs) and 8,000 columns (patterns), with a density of 5.8% (percentage of nonzero values).
 the steps used a single CPU on a desktop computer, except step 3, finding the phrases for each word pair, which used a 16 CPU Beowulf cluster. Most of the other steps are
Beowulf cluster. All CPUs (both desktop and cluster) were 2.4 GHz Intel Xeons. The desktop computer had 2 GB of RAM and the cluster had a total of 16 GB of RAM. 6.2 LRA versus VSM
Table 14 compares LRA to the VSM with the 374 analogy questions. VSM-AV refers to the VSM using AltaVista X  X  database as a corpus. The VSM-AV results are taken from Turney and Littman (2005). As mentioned in Section 4.2, we estimate this corpus contained about 5  X  10 11 English words at the time the VSM-AV experiments took place.
VSM-WMTS refers to the VSM using the WMTS, which contains about 5 words. We generated the VSM-WMTS results by adapting the VSM to the WMTS.
The algorithm is slightly different from Turney and Littman X  X  (2005), because we used passage frequencies instead of document frequencies.
 95% confidence, using the Fisher Exact Test (Agresti 1990). The pairwise differences in precision between LRA and the two VSM variations are also significant, but the differ-ence in precision between the two VSM variations (42.4% vs. 47.7%) is not significant.
Although VSM-AV has a corpus 10 times larger than LRA X  X , LRA still performs better than VSM-AV.
 skipped (34 for VSM-WMTS versus 5 for VSM-AV). With the smaller corpus, many more of the input word pairs simply do not appear together in short phrases in the corpus.
LRA is able to answer as many questions as VSM-AV, although it uses the same corpus as VSM-WMTS, because Lin X  X  thesaurus allows LRA to substitute synonyms for words that are not in the corpus.
 2005), compared to 9 days for LRA. As a courtesy to AltaVista, Turney and Littman (2005) inserted a 5-second delay between each two queries. Since the WMTS is running locally, there is no need for delays. VSM-WMTS processed the questions in only one day. 6.3 Human Performance
The average performance of college-bound senior high school students on verbal SAT questions corresponds to a recall (percent correct) of about 57% (Turney and Littman 2005). The SAT I test consists of 78 verbal questions and 60 math questions (there is also an SAT II test, covering specific subjects, such as chemistry). Analogy questions are only a subset of the 78 verbal SAT questions. If we assume that the difficulty of our 374 analogy questions is comparable to the difficulty of the 78 verbal SAT I questions, then we can estimate that the average college-bound senior would correctly answer about 57% of the 374 analogy questions.
 2000). On this subset of the questions, LRA has a recall of 61.1%, compared to a recall of 51.1% on the other 184 questions. The 184 questions that are not from Claman (2000) seem to be more difficult. This indicates that we may be underestimating how well
LRA performs, relative to college-bound senior high school students. Claman (2000) suggests that the analogy questions may be somewhat harder than other verbal SAT 402 questions, so we may be slightly overestimating the mean human score on the analogy questions.
 calculated by the Binomial Exact Test (Agresti 1990). There is no significant difference between LRA and human performance, but VSM-AV and VSM-WMTS are significantly below human-level performance. 6.4 Varying the Parameters in LRA
There are several parameters in the LRA algorithm (see Section 5.5). The parameter values were determined by trying a small number of possible values on a small set of questions that were set aside. Since LRA is intended to be an unsupervised learning algorithm, we did not attempt to tune the parameter values to maximize the precision and recall on the 374 SAT questions. We hypothesized that LRA is relatively insensitive to the values of the parameters.
 are adjusted. We take the baseline parameter settings (given in Section 5.5) and vary each parameter, one at a time, while holding the remaining parameters fixed at their baseline values. None of the precision and recall values are significantly different from the baseline, according to the Fisher Exact Test (Agresti 1990), at the 95% confidence level. This supports the hypothesis that the algorithm is not sensitive to the parameter values.
 the parameters it is possible to reuse cached data from previous runs. We limited the experiments with num sim and max phrase because caching was not as helpful for these parameters, so experimenting with them required several weeks. 6.5 Ablation Experiments As mentioned in the introduction, LRA extends the VSM approach of Turney and
Littman (2005) by (1) exploring variations on the analogies by replacing words with synonyms (step 1), (2) automatically generating connecting patterns (step 4), and (3) smoothing the data with SVD (step 9). In this subsection, we ablate each of these three components to assess their contribution to the performance of LRA. Table 17 shows the results.
 drop is not statistically significant with 95% confidence, according to the Fisher Exact
Test (Agresti 1990). However, we hypothesize that the drop in performance would be significant with a larger set of word pairs. More word pairs would increase the sample size, which would decrease the 95% confidence interval, which would likely show that
SVD is making a significant contribution. Furthermore, more word pairs would increase the matrix size, which would give SVD more leverage. For example, Landauer and Dumais (1997) apply SVD to a matrix of 30,473 columns by 60,768 rows, but our matrix 404 here is 8,000 columns by 17,232 rows. We are currently gathering more SAT questions to test this hypothesis.
 (from 56.1% to 49.5%), but the drop in precision is not significant. When the synonym component is dropped, the number of skipped questions rises from 4 to 22, which demonstrates the value of the synonym component of LRA for compensating for sparse data.

Again, we believe that a larger sample size would show that the drop in precision is significant.

VSM-WMTS is the patterns (step 4). The VSM approach uses a fixed list of 64 patterns to generate 128 dimensional vectors (Turney and Littman 2005), whereas LRA uses a dynamically generated set of 4,000 patterns, resulting in 8,000 dimensional vectors. We can see the value of the automatically generated patterns by comparing LRA without synonyms and SVD (column 4) to VSM-WMTS (column 5). The difference in both Fisher Exact Test (Agresti 1990).
 (step 1) in LRA, but the contribution of SVD (step 9) has not been proven, although we believe more data will support its effectiveness. Nonetheless, the three components together result in a 16% increase in F (compare column 1 to 5). 6.6 Matrix Symmetry carpenter is to wood implies stone is to mason as wood is to carpenter. Therefore, a good measure of relational similarity, sim r , should obey the following equation: symmetrical, so that equation (8) is necessarily true for LRA. The matrix is designed so that the row vector for A : B is different from the row vector for B : A only by a permutation of the elements. The same permutation distinguishes the row vectors for C : D and D : C .
Therefore the cosine of the angle between A : B and C : D must be identical to the cosine of the angle between B : A and D : C (see equation (7)).
 that symmetry is no longer preserved. In step 5, for each word pair A : B that appears in the input set, we only have one row. There is no row for B : A unless B : A also appears in the input set. Thus the number of rows in the matrix dropped from 17,232 to 8,616. columns at 8,000. In step 4, we selected the top 8,000 patterns (instead of the top 4,000), distinguishing the pattern  X  word 1 Pword 2  X  from the pattern  X  word considering them equivalent). Thus a pattern P with a high frequency is likely to appear in two columns, in both possible orders, but a lower frequency pattern might appear in only one column, in only one possible order. 56.1% to 55.3% and precision dropped from 56.8% to 55.9%. The decrease is not sta-tistically significant. However, the modified algorithm no longer obeys equation (8).
Although dropping symmetry appears to cause no significant harm to the performance equation (8) is satisfied.
  X  X tone is to mason as carpenter is to wood. X  In general (except when the semantic relations between A and B are symmetrical), we have the following inequality:
Therefore we do not want A : B and B : A to be represented by identical row vectors, although it would ensure that equation (8) is satisfied. 6.7 All Alternates versus Better Alternates
In step 12 of LRA, the relational similarity between A : B and C : D is the average of the cosines, among the ( num filter + 1) 2 cosines from step 11, that are greater than or equal to the cosine of the original pairs, A : B and C : D . That is, the average includes only those alternates that are  X  X etter X  than the originals. Taking all alternates instead of the better alternates, recall drops from 56.1% to 40.4% and precision drops from 56.8% to 40.8%. Both decreases are statistically significant with 95% confidence, according to the Fisher
Exact Test (Agresti 1990). 6.8 Interpreting Vectors
Suppose a word pair A : B corresponds to a vector r in the matrix X . It would be con-venient if inspection of r gave us a simple explanation or description of the relation between A and B . For example, suppose the word pair ostrich:bird maps to the row vector r . It would be pleasing to look in r and find that the largest element corresponds of r reveals no such convenient patterns.
 vector; it is not concentrated in a few elements. To test this hypothesis, we modified step 10 of LRA. Instead of projecting the 8,000 dimensional vectors into the 300 dimen-sional space U k  X  k , we use the matrix U k  X  k V T k . This matrix yields the same cosines as
U
 X  k , but preserves the original 8,000 dimensions, making it easier to interpret the row values to zero. The idea here is that we will only pay attention to the N most important normalized to unit length; see equation (7)), so the change to the vector lengths has no impact; only the angle of the vectors is important. If most of the semantic content is in the N largest elements of r , then setting the remaining elements to zero should have relatively little impact.
 are significantly below the baseline LRA until N  X  300 (95% confidence, Fisher Exact 406
Test). In other words, for a typical SAT analogy question, we need to examine the top 300 patterns to explain why LRA selected one choice instead of another.
 pattern why one choice is better than another. We have had some promising results, but this work is not yet mature. However, we can confidently claim that interpreting the vectors is not trivial. 6.9 Manual Patterns versus Automatic Patterns
Turney and Littman (2005) used 64 manually generated patterns, whereas LRA uses 4,000 automatically generated patterns. We know from Section 6.5 that the automatically generated patterns are significantly better than the manually generated patterns. It may be interesting to see how many of the manually generated patterns appear within the patterns can be found in the automatic patterns. If we are lenient about wildcards, and count the pattern not the as matching * not the (for example), then 60 of the 64 manual patterns appear within the automatic patterns. This suggests that the improvement in performance with the automatic patterns is due to the increased quantity of patterns, rather than a qualitative difference in the patterns.
 by other researchers. For example, Hearst (1992) used the pattern such as to discover hy-ponyms and Berland and Charniak (1999) used the pattern of the to discover meronyms. LRA.
 show that a vector contains more information than any single pattern or small set of patterns; a vector is a distributed representation. LRA is distinct from Hearst (1992) and
Berland and Charniak (1999) in its focus on distributed representations, which it shares with Turney and Littman (2005), but LRA goes beyond Turney and Littman (2005) by finding patterns automatically.
 their goal is to mine text for instances of word pairs; the same goal as Hearst (1992) and
Berland and Charniak (1999). Because LRA uses patterns to build distributed vector representations, it can exploit patterns that would be much too noisy and unreliable for the kind of text mining instance extraction that is the objective of Hearst (1992), Berland and Charniak (1999), Riloff and Jones (1999), and Yangarber (2003). Therefore LRA can simply select the highest frequency patterns (step 4 in Section 5.5); it does not need the more sophisticated selection algorithms of Riloff and Jones (1999) and Yangarber (2003). 7. Experiments with Noun-Modifier Relations
This section describes experiments with 600 noun-modifier pairs, hand-labeled with 30 classes of semantic relations (Nastase and Szpakowicz 2003). In the following ex-periments, LRA is used with the baseline parameter values, exactly as described in
Section 5.5. No adjustments were made to tune LRA to the noun-modifier pairs. LRA is used as a distance (nearness) measure in a single nearest neighbor supervised learning algorithm. 7.1 Classes of Relations The following experiments use the 600 labeled noun-modifier pairs of Nastase and Szpakowicz (2003). This data set includes information about the part of speech and
WordNet synset (synonym set; i.e., word sense tag) of each word, but our algorithm does not use this information.
 of Nastase and Szpakowicz (2003), with some simplifications. The original table listed several semantic relations for which there were no instances in the data set. These were relations that are typically expressed with longer phrases (three or more words), rather than noun-modifier word pairs. For clarity, we decided not to include these relations in Table 19.
 ple, in flu virus, the head noun ( H )is virus and the modifier ( M )is flu (*). In English, presenting concerts ( V is present ) or holding concerts ( V is hold )( talized terms in the Relation column of Table 19 are the names of five groups of semantic relations. (The original table had a sixth group, but there are no examples of this group in the data set.) We make use of this grouping in the following experiments. 7.2 Baseline LRA with Single Nearest Neighbor
The following experiments use single nearest neighbor classification with leave-one-out cross-validation. For leave-one-out cross-validation, the testing set consists of a single noun-modifier pair and the training set consists of the 599 remaining noun-modifiers.
The data set is split 600 times, so that each noun-modifier gets a turn as the testing word pair. The predicted class of the testing pair is the class of the single nearest neighbor in the training set. As the measure of nearness, we use LRA to calculate the relational similarity between the testing pair and the training pairs. The single nearest neighbor algorithm is a supervised learning algorithm (i.e., it requires a training set of labeled 408 data), but we are using LRA to measure the distance between a pair and its potential neighbors, and LRA is itself determined in an unsupervised fashion (i.e., LRA does not need labeled data).
 culating 374  X  5  X  16 = 29, 920 cosines. The factor of 16 comes from the alternate pairs, step 11 in LRA. With the noun-modifier pairs, using leave-one-out cross-validation, each test pair has 599 choices, so an exhaustive application of LRA would require calculating 600  X  599  X  16 = 5, 750, 400 cosines. To reduce the amount of computation required, we first find the 30 nearest neighbors for each pair, ignoring the alternate pairs (600  X  599 = 359, 400 cosines), and then apply the full LRA, including the alternates, to just those 30 neighbors (600  X  30  X  16 = 288, 000 cosines), which requires calculating only 359, 400 + 288, 000 = 647, 400 cosines.
 pairs multiplies the number of pairs by four, resulting in 2,400 pairs. In step 5, for each pair A : B ,weadd B : A , yielding 4,800 pairs. However, some pairs are dropped because they correspond to zero vectors and a few words do not appear in Lin X  X  thesaurus. The sparse matrix (step 7) has 4,748 rows and 8,000 columns, with a density of 8.4%. and also by the macroaveraged F measure (Lewis 1991). Macroaveraging calculates across all classes. Microaveraging combines the true positive, false positive, and false negative counts for all of the classes, and then calculates precision, recall, and F from the combined counts. Macroaveraging gives equal weight to all classes, but microaveraging gives more weight to larger classes. We use macroaveraging (giving equal weight to all classes), because we have no reason to believe that the class sizes in the data set reflect the actual distribution of the classes in a real corpus.
 can collapse the 30 classes to 5 classes, using the grouping that is given in Table 19. For example, agent and beneficiary both collapse to participant . On the 30 class problem, LRA with the single nearest neighbor algorithm achieves an accuracy of 39.8% (239/600) and a macroaveraged F of 36.6%. Always guessing the majority class would result in an accuracy of 8.2% (49 / 600). On the 5 class problem, the accuracy is 58.0% (348/600) and the macroaveraged F is 54.6%. Always guessing the majority class would give an accuracy of 43.3% (260 / 600). For both the 30 class and 5 class problems, LRA X  X  accuracy is significantly higher than guessing the majority class, with 95% confidence, according to the Fisher Exact Test (Agresti 1990). 7.3 LRA versus VSM
Table 20 shows the performance of LRA and VSM on the 30 class problem. VSM-AV is VSM with the AltaVista corpus and VSM-WMTS is VSM with the WMTS corpus.
The results for VSM-AV are taken from Turney and Littman (2005). All three pairwise differences in the three F measures are statistically significant at the 95% level, according cantly higher than the accuracies of VSM-AV and VSM-WMTS, according to the Fisher significant.

The accuracy and F measure of LRA are significantly higher than the accuracies and 410
F measures of VSM-AV and VSM-WMTS, but the differences between the two VSM accuracies and F measures are not significant. 8. Discussion
The experimental results in Sections 6 and 7 demonstrate that LRA performs signifi-cantly better than the VSM, but it is also clear that there is room for improvement. The accuracy might not yet be adequate for practical applications, although past work has shown that it is possible to adjust the trade-off of precision versus recall (Turney and
Littman 2005). For some of the applications, such as information extraction, LRA might be suitable if it is adjusted for high precision, at the expense of low recall. questions. However, with progress in computer hardware, speed will gradually become less of a concern. Also, the software has not been optimized for speed; there are several places where the efficiency could be increased and many operations are parallelizable.
It may also be possible to precompute much of the information for LRA, although this would require substantial changes to the algorithm.
 is sensitive to the size of the corpus. Although LRA is able to surpass VSM-AV when the
WMTS corpus is only about one tenth the size of the AV corpus, it seems likely that LRA would perform better with a larger corpus. The WMTS corpus requires one terabyte of hard disk space, but progress in hardware will likely make 10 or even 100 terabytes affordable in the relatively near future.
 provements. With 600 noun-modifier pairs and 30 classes, the average class has only 20 examples. We expect that the accuracy would improve substantially with 5 or 10 times more examples. Unfortunately, it is time consuming and expensive to acquire hand-labeled data.
 scheme for the semantic relations. The 30 classes of Nastase and Szpakowicz (2003) might not be the best scheme. Other researchers have proposed different schemes (Vanderwende 1994; Barker and Szpakowicz 1998; Rosario and Hearst 2001; Rosario,
Hearst, and Fillmore 2002). It seems likely that some schemes are easier for machine learning than others. For some applications, 30 classes may not be necessary; the 5 class scheme may be sufficient.
 work suggests that a hybrid approach, combining multiple modules, some corpus-based, some lexicon-based, will surpass any purebred approach (Turney et al. 2003).
In future work, it would be natural to combine the corpus-based approach of LRA with the lexicon-based approach of Veale (2004), perhaps using the combination method of Turney et al. (2003).
 experimented with Non-negative Matrix Factorization (NMF) (Lee and Seung 1999),
Probabilistic Latent Semantic Analysis (PLSA) (Hofmann 1999), Kernel Principal Com-ponents Analysis (KPCA) (Scholkopf, Smola, and Muller 1997), and Iterative Scaling (IS) (Ando 2000). We had some interesting results with small matrices (around 2,000 rows by 1,000 columns), but none of these methods seemed substantially better than
SVD and none of them scaled up to the matrix sizes we are using here (e.g., 17,232 rows and 8,000 columns; see Section 6.1).
 and discard the remaining patterns. Perhaps a more sophisticated selection algorithm would improve the performance of LRA. We have tried a variety of ways of selecting patterns, but it seems that the method of selection has little impact on performance. We hypothesize that the distributed vector representation is not sensitive to the selection method, but it is possible that future work will find a method that yields significant improvement in performance. 9. Conclusion This article has introduced a new method for calculating relational similarity, Latent Relational Analysis. The experiments demonstrate that LRA performs better than the
VSM approach, when evaluated with SAT word analogy questions and with the task of classifying noun-modifier expressions. The VSM approach represents the relation be-tween a pair of words with a vector, in which the elements are based on the frequencies of 64 hand-built patterns in a large corpus. LRA extends this approach in three ways: (1) The patterns are generated dynamically from the corpus, (2) SVD is used to smooth the data, and (3) a thesaurus is used to explore variations of the word pairs. With the WMTS corpus (about 5  X  10 10 English words), LRA achieves an F of 56.5%, whereas the F of VSM is 40.3%.
 sures of relational similarity. Just as attributional similarity measures have proven become widely used. Gentner et al. (2001) argue that relational similarity is essential to understanding novel metaphors (as opposed to conventional metaphors). Many 412 researchers have argued that metaphor is the heart of human thinking (Lakoff and
Johnson 1980; Hofstadter and the Fluid Analogies Research Group 1995; Gentner et al. 2001; French 2002). We believe that relational similarity plays a fundamental role in the mind and therefore relational similarity measures could be crucial for artificial intelligence.
 the fact that LRA matches average human performance on SAT analogy questions is encouraging.
 Acknowledgments References 414
