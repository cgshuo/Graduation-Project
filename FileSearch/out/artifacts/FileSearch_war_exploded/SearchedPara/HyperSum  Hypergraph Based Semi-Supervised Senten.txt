 Graph based sentence ranking algorithms such as PageRank and HITS have been successfully used in query-oriented summarization. With these algorithms, the documents to be summarized are often modeled as a text graph where nodes represent sentences and edges represent pairwise similarity relationships between two sentences. A deficiency of conventional graph modeling is its incapability of naturally and effectively representing complex group relationships shared among multiple objects. Simply squeezing complex relationships into pairwise ones will inevitably lead to loss of information which can be useful for ranking and learning. In this paper, we propose to take advantage of hypergraph, i.e. a generalization of graph, to remedy this defect. In a text hypergraph, nodes still represent sentences, yet hyperedges are allowed to connect more than two sentences. With a text hypergraph, we are thus able to integrate both group relationships formulated among multiple sentences and pairwise relationships formulated between two sentences in a unified framework. As essential work, it is first addressed in the paper that how a text hypergraph can be built for summarization by applying clustering techniques. Then, a hypergraph based semi-supervised sentence ranking algorithm is developed for query-oriented extractive summarization, where the influence of query is propagated to sentences through the structure of the constructed text hypergraph. When evaluated on DUC data sets, performance of the proposed approach is remarkable. I.7.5 [ Document Capture ]: Document analysis Design, Algorithms, Theory, Experimentation Query-oriented summarization, text hypergraph, semi-supervised ranking Automatic text summarization has a long history that dates back to the 1960s. Up to now extractive summarization that directly extracts sentences from documents to generate summaries is still the mainstream regardless of the nature and the goals of the tasks. Under this framework, sentence ranking is the issue of most concern for sure. For query-oriented summarization, as summarization is driven by query, query relevance must be measured in certain ways. Commonly used unsupervised graph based ranking formulates query X  X  effect either on sentence nodes by sentence-query relevance or on links of sentences by query-sensitive similarity. Both of these two measures need to be defined manually. Supervised ranking, on the other hand, automatically learns how sentence similarity is skewed by query from the documents and the corresponding human abstracts that have already been available [1]. An obvious problem with such kind of supervised ranking is that no one can preciously scale the difference between the sentence similarity in itself and the sentence similarity subjected to the given query. We believe that a more convenient and effective way to impose query X  X  influence on the graph is to apply semi-supervised learning. That is, model the given query itself as an independent node on the graph and treat the query node as the only labeled data. The label here indicates query relevance. Initially, query relevance of sentences is supposed to be unknown. Through the structure of the graph, query relevance is then recursively propagated to the sentences to be ranked. A deficiency of conventional pairwise graph based modeling that one can observe is its inability to completely capture n-ary associations among multiple sentences. For example, a group of causes/consequences of an event. Modeling complex associations among multiple sentences using pairwise similarity illustrated in a text graph will inevitably lead to loss of information. To alleviate the problem, we propose to model sentences and their associations as a hypergraph, i.e. a generalization of graph that is able to formulate both pairwise and group relationships. Two basic issues addressed in this paper are: (1) how to construct a text hypergraph for summarization; and (2) how to develop hypergraph based semi-supervised learning algorithm for sentence ranking. The contributions of the work presented here include to investigate a more natural and appropriate text representation that can characterize as much as useful associations among sentences, and to integrate semi-supervised ranking and hypergraph modeling into a unified summarization framework. The remainder of this paper is organized as follows. Section 2 briefly reviews related work on graph based summarization, the background of hypergraph modeling, and graph/hypergraph based semi-supervised learning. Sections 3 and 4 provide fundamentals of hypergraph modeling and introduce text hypergraph construction and hypergraph based semi-supervised sentence ranking. Section 5 reports experime nts and evaluations. Finally, Section 6 concludes the paper. Existing graph based summarization techniques are mainly inspired by link analysis based ranking algorithms. Fox example, LexRank [2], a Pagerank like summarization algorithm, models documents as a stochastic graph and calculates sentence ranking scores as the stationary distribution of the stochastic graph. While PageRank-like algorithms normally consider similarity or association between sentences, Zha [3] proposes to accomplish key phrase extraction and generi c summarization simultaneously by modeling text as a weighted undirected bipartite graph. Then, significance scores of key phrases and sentences are generated based on mutual reinforcement of terms and sentences. A hypergraph is a generalization of a graph, where edges can connect any number of nodes. Hypergraph has proved to be a successful tool to represent and model complex concepts and structures in computer science. To our best knowledge, there is no previous work reported on hypergraph based summarization yet. Graph based semi-supervised learning has attracted a lot of research recently. The goal of graph based semi-supervised learning is to estimate a function on the graph in which nodes stands for labeled and unlabeled examples and edge weight reflects the similarity of exmpales. Based on the prior assumption [4] that nearby points and the points on the same manifold are likely to have the same label (or similar scores), the function to be estimated is required to be consistent with labeled data and meanwhile locally smooth on the graph. These two constraints are usually formulated by a regularization function that consists of a fitness regularizer and a smoothness regularizer, while the later is the focus of research. The normalized graph Laplacian in the local and global consistency method [4] is one of the most widely used smoothness regularizers. The success of graph based semi-supervised learning has prompted researchers to further study semi-supervised learning on hypergraph. Zhou et al. [5] propose the hypergraph Laplacian which is deduced from a hypergraph based smoothness function similar to the one used for graph in the local and global consistency method. Zhou et al. X  X  work is fundamental to the hypergraph based semi-supervised sentence ranking algorithm introduced in Section 4. Our focus in this work, however, is semi-supervised ranking, other than classification. A hypergraph is a generalization of graph. While graph edges are pairs of nodes, hyperedges are arbitrary subsets of the node set, i.e. the hyperedges connect two or more nodes on a hypergraph. Formally, a hypergraph can be defined as  X  X   X   X , X   X  , where  X  X  X  X  X   X   X ,  X   X ,...,  X   X  is the set of nodes and  X  X   X   X   X  a set of hyperedges, where  X   X   X  X  for  X ,..., X 1,2  X  . . A weighted hypergraph  X  X   X   X , X , X   X  is a hypergraph that has a positive weight  X  X  X  X  X  associated with  X  X  X  . For a node  X  X  X  , its degree is defined by And the degree of  X  X  X  is defined to be worth noting that there is one to one correspondence between a hypergraph and a 0-1 matrix. Given a hypergraph, its matrix representation H, called the incidence matrix , is defined as Using the incidence matrix, the degrees of the node v and the edge e can be simply denoted as and Let  X   X  and  X   X  denote the diagonal matrices containing the node and the hyperedge degrees respectively, and  X  denote the diagonal matrix containing the weights. These symbols will be used in our following algorithms. Unlike in the applications such as citation analysis where the construction of hypergraph based on the relationships among authors and articles is straightforw ard, the construction of text hypergraph is not direct but requires an additional process to discover group relationships. Though there are many different ways to define the group relationships among the sentences, we consider the topic relationship in this paper. Suppose the sentence set is  X  X   X   X ,  X   X ,  X   X ...,  X   X  in which  X  query and  X   X   X ,  X   X ...,  X  are sentences in the document set. To identify the topics conveyed in the document set, we apply the DBSCAN algorithm to cluster the sentences (including the query), since it doesn X  X  need a predefined threshold and can filter noise node by nature. Once the clusters are obtained, we construct the text hypergraph as follows: (1) add a node in the hypergraph for each sentence  X   X  (  X  X 0 X  n ) (2) if the cosine similarity between the two sentence  X   X  and  X   X  is positive, then we add a hyperedge to connect them with the weight as  X  X  X  X  X  X   X   X   X   X ,  X   X  ; (3) for sentences  X   X ,  X  cluster  X   X  , add a hyperedge to connect them with the weight defined as  X  X  X  X  X  X   X   X   X   X ,  X   X  X  , where  X  X  X  X  X  X   X   X   X  similarity between the cluster  X   X  and the document set (including importance of cluster hyperedges compared with pairwise hyperedges. Given the hypergraph representation of a document set, one way to make convenient and better use of the information carried on the hypergraph is to treat sentence ranking as a semi-supervised learning process, in which the query is regarded as the only labeled node, whist the sentences in documents as unlabeled nodes. In fact, the only information we have at the very beginning about how to select most significant sentences to generate the summary is the query alone. Hence, initially the query is assigned with a positive significance score (e.g., 1 in Algorithm 1) indicating its relevance and significance scores of other sentences are assigned zero. Then sentence scores are learned step by step from the query through the structure of the hypergraph. The key idea behind hypergraph based semi-supervised ranking is that the nodes which have many incident hyperedges in common should be assigned with very similar scores. This just fits right in the context of summarization. In particular, we have the following two assumptions (or to say constraints): 1. The sentences with higher cosi ne similarity should have more similar significance scores. 2. The sentences in the same cluster that talk about the same topic should have similar scores. Given a weighted hypergraph  X  X , X , X  X  X  X  X  , which represents the documents to be summarized, and a scoring function f over  X  , which assigns the sentence v the score  X  X  X  X  X  . f can be deemed as a to formalize the above-mentioned two assumptions. The function  X   X   X   X  sums the changes of the scoring function  X  over the hyperedges on the hypergraph. On the one hand, a good scoring function should make  X   X   X   X  as small as possible, i.e. make the sentences which share higher cosine similarity (or to say in the same cluster) have more similar scores. On the other side, a good scoring function should be consistent with the given initial score vector. As mentioned previously, the initial score of the query is assumed to be 1 and the initial scores of the other sentences 0. Let  X  denote the initial score vector, then the significance scores of the sentences are learned recursively by solving the following optimization problem: where  X 0 X  is the parameter specifying the tradeoff between the two competitive terms. Zhou has proved that this optimization problem has a closed form solution [5]. where  X  X  X  X 1 X 1 X  X   X  and  X  X   X   X   X  X / X   X  X   X   X  X   X   X   X   X   X  X / X  equation can then be re-formulated as Algorithm 1 :  X  X , X , X  X  X  X  X , X  X  X  X  X  X  X  X  X  X 
Input : The document Set  X  X  X  X   X   X   X ,  X   X ...,  X   X  ,  X  X  X  X  X  X  X  parameters  X  and  X  
Output: The summary of the document set  X   X  X  X   X  X   X   X  X  X  X  X  X  X  X   X  X   X   X ,  X   X ,  X   X ...,  X   X  using the method describled in section 3.2. 
Let H ,  X , X   X  ,  X   X  be the corresponding hypergraph incident matrix, the diagonal hyperedge weight matrix, hypergraph edge degree matrix and hypergraph vertex degree matrix respectively. END Once sentence significance scores are ready, we rank sentences in descending order of the calculated scores and select the highest ranked sentences into the summary. To avoid the redundancy in the generated summary, each time when the top ranked sentence is examined as a candidate summary sentence, the cosine similarity between it and each of the previously selected summary sentences is calculated. If the similarity exceeds a pre-defined threshold, the candidate sentence is discarded. This procedure is repeated until the summary length reaches the limit. We call this query-oriented summarization approach based on the hypergraph representation and semi-supervised learning paradigm  X  X  X  X  X  X  X  X  . The algorithm of  X  X  X  X  X  X  X  X  is detailed in Algorithm 1. Our experiments are mainly conducted on the DUC 2006 data set, ROUGE, which has been officially adopted by the DUC for automatic evaluations, is used to evaluate system-generated summaries. In this section, we present the performance evaluation of two sets of experiments which are conducted (1) to examine the influences of the two parameters of  X  X  X  X  X  X  X  X  , i.e. the learning factor  X  and the cluster importance factor  X  , on summarization performance; (2) to verify the advantage of the text hypergraph representation by comparing  X  X  X  X  X  X  X  X  with conventional graph based ranking algorithms like topic sensitive PageRank and HITS, which have previously been adopted in query-oriented summarization The combination of the learning factor  X  (  X 1 X  X 0 X  and the cluster importance factor  X  (  X 0 X  ) makes it hard to find a global optimized solution. So we apply a gradient search strategy. At Then performance using different values of  X  ranging from 0 to 1 is evaluated. Then, we conduct experiments to find appropriate value for  X  in the range from 0.0 to 2.0, given that  X  is set to the value with which achieves the best performance. cluster hyperedge is assumed to be as important as the pairwise cosine similarity hyperedge. As shown in figure 1, we find that the performance of  X  X  X  X  X  X  X  X  gets better as  X  increases from 0.1 to 1. It reaches the peak at ar ound 0.97-0.98 and drops afterwards. As mentioned in Section 4,  X  can be viewed as a factor that specifies the proportion of how much a sentence should learn from its neighbors and how much it should learn from its corresponding initial score vector. The experimental result suggests that the majority of the significance score for a sentence is learned from its neighbor sentences. Next, we set the learning factor  X  at 0.98. When  X 0 X  , it means that the weight of the cluster hyperedge is 0 and thus only the pairwise cosine similarity information is considered for the calculation of sentence significance scores. As we can see, when the cluster information is considered, i.e.  X 0 X  , the performance is improved in most cases. This justifies the use of text hypergraph, which incorporate the group information and the pairwise similarity information, to represent the documents. To verify the effectiveness of the proposed hypergraph based sentence ranking algorithm, we compare  X  X  X  X  X  X  X  X  with another two conventional graph based sentence ranking algorithms. One is developed based on topic sensitive PageRank, called query sensitive LexRank in [2], the other is developed based on Kleinberg X  X  HITS. Due to page limitations we do not present the algorithm of query sensitive LexRank, readers can refer to [2] for details. When implementing HITS for query-oriented summarization, we simply select top 10% of the high query relevant sentences to construct the text graph according to our previous experience. And then use the mutual reinforcement between terms and sentences to calculate sentence siginficance scores: where  X   X  and  X   X  are the sentence and term score vector respectively,  X  is the affinity matrix for sentences and terms, i.e. if sentence  X   X  contain term  X   X  , then  X   X , X   X  X  X  X   X  inverse sentence frequency of term  X   X  , and  X  X   X , X  frequency of term  X   X  in sentence  X   X  ).  X   X  is the transpose of  X  . Table 1. Comparison with conventional graph based models Query Sensitive PageRank (  X  =0.15) 0.09016 0.14768 We can see that  X  X  X  X  X  X  X  X  is above query sensitive LexRank by 7.04% of Rouge-2 and 3.27% of Rouge-SU4, respectively. As for HITS, the improvements are 17.07% and 10.13%. These are definitely exciting achievements since the performance of the top ranking systems are very close, i.e. the second-best system is only above the fourth-best system 1.60% on Rouge-2 and 0.86% on Rouge-SU4. Furthermore, it clearly shows that  X  X  X  X  X  X  X  X  is comparable with those state-of-the-art systems. Moreover, experiments show that our system ranks the first in DUC 2006. In this paper, we propose to model the document(s) as a text hypergraph and develop a hypergraph based semi-supervised sentences ranking algorithm for query-oriented summarization. In addition to the pairwise relationships formulated in existing graph based models, the proposed approach, namely  X  X  X  X  X  X  X  X  , is able to integrate group relationships among multiple sentences with pairwise ones in a unified framework. Experimental results demonstrate the effectiveness and robustness of  X  X  X  X  X  X  X  X  . In evaluation. The work described in this paper was in part supported by a grant from HK RGC (PolyU5217/07E), the Hong Kong Polytechnic University internal the grant (G-YG80), NSFC programs (No: 60603093 and 60875042), and 973 National Basic Research Program of China (2004CB318102). [1] Y. OuYang, S. Li and W. Li, Developing Learning Strategies [2] J. Otterbacher, G. Erkan and D. Radev. 2005. Using Random [3] H. Zha. 2002. Generic Summarization and Key Phrase [4] D. Zhou, O. Bousquet, T.N. Lal, J. Weston and B. Sch  X  lkopf. [5] D. Zhou, J. Huang and B. Sch  X  lkopf . 2005. Beyond Pairwise 
