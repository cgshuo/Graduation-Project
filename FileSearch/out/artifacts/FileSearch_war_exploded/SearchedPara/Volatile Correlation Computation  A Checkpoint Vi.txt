 Recent years have witnessed increased interest in computing strongly correlated pairs in very large databases. Most pre-vious studies have been focused on static data sets. How-ever, in real-world applications, input data are often dy-namic and must continually be updated. With such large and growing data sets, new research efforts are expected to develop an incremental solution for correlation computing. Along this line, in this paper, we propose a CHECK-POINT algorithm that can efficiently incorporate new transactions for correlation computing as they become available. Specifi-cally, we set a checkpoint to establish a computation buffer, which can help us determine an upper bound for the corre-lation. This checkpoint bound can be exploited to identify a list of candidate pairs, which will be maintained and com-puted for correlations as new transactions are added into the database. However, if the total number of new transactions is beyond the buffer size, a new upper bound is computed by the new checkpoint and a new list of candidate pairs is identified. Experimental results on real-world data sets show that CHECK-POINT can significantly reduce the cor-relation computing cost in dynamic data sets and has the advantage of compacting the use of memory space.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining Algorithms Pearson X  X  Correlation Coefficient,  X  Correlation Coefficient, Volatile Correlation Computing, Checkpoint
Given a set of data objects, the problem of correlation computing is concerned with identification of strongly-related (e.g. as measured by Pearson X  X  correlation coefficient for pairs [13]) groups of data objects. Many important applica-tions in science and business [2, 6, 12, 14] depend on efficient and effective correlation computing techniques to discover relationships within large collections of information. Despite the development of traditional statistical correlation com-puting techniques [4, 10, 8, 11, 9, 15, 16], researchers and practitioners are still facing increasing challenges to mea-sure associations among data produced by emerging data-intensive applications.

Indeed, the size of real-world datasets is growing at an extraordinary rate, and these data are often dynamic and need to be continually updated. With such large and grow-ing data sets, new research efforts are expected to develop an incremental solution for correlation computing. To that end, in this paper, we limit our scope to provide a pilot study of incrementally querying all item pairs with correla-tions above a user specified minimum correlation threshold when new data become available.

A straightforward approach is to recompute the correla-tions for all the item pairs every time that new data of trans-actions become available. However, for large data sets, this approach is infeasible, particularly if the application needs the results in a timely fashion. An alternative method is to use more space to save the time. Along this line, we de-scribe a SAVE-ALL algorithm, which saves the intermediate results for all item pairs. When new transactions are added into the database, SAVE-ALL only updates the stored val-ues corresponding to each item pair and computes the corre-lation query results with the intermediate values. Obviously, the SAVE-ALL method compromises space for time. If the number of items in the data set becomes considerably large, the number of pairs grow even larger, to the extent that it is impossible to save the intermediate computing results of all item pairs in the memory space. This motivates our interest in incremental correlation computing.

Specifically, we propose a CHECK-POINT algorithm that makes a time-space tradeoff and can efficiently incorporate new transactions for correlation computing as they become available. In the CHECK-POINT algorithm, we set a check-point to establish a computation buffer, which can help us to determine a correlation upper bound. This checkpoint bound can be exploited to identify a list of candidate pairs, which will be maintained and computed for correlations as new transactions are added into the database. However, if the total number of new transactions exceeds the buffer size, a new upper bound is computed by the new checkpoint and a new list of candidate pairs is identified.
The rationale behind CHECK-POINT is that, if the num-ber of new transactions is much smaller than the total num-ber of transactions in the database, the correlation coeffi-cients of most item pairs do not change substantially. In other words, we only need to establish a very short list of candidate pairs at the checkpoint and maintain this can-didate list in the memory as new transactions are added into the database. Unlike SAVE-ALL, CHECK-POINT only maintains the intermediate computing results of a very small portion of the item pairs. This can greatly compact the use of the memory space by using slightly more time.

As demonstrated by our experimental results on several real-world data sets, CHECK-POINT can significantly re-duce the computational cost compared to existing correla-tion computing benchmark algorithms, i.e. TAPER, in a dynamic data environment. Also, we observe that there is a trade-off between the use of space and the time by setting different checkpoint values. Indeed, the use of space in-creases with the increase of the checkpoint size (the number of transactions between two nearest checkpoints). In con-trast, the average computational cost is reduced with the increase of the checkpoint size. Finally, our experimental results show that CHECK-POINT, as compared to SAVE-ALL, can greatly reduce the use of memory space.

Overview. The remainder of this paper is organized as follows. In Section 2, we introduce some basic concepts and formulate the problem. Section 3 provides a checkpoint view for dynamic all-strong-pairs correlation queries. In Section 4, we describe the CHECK-POINT, r-TAPER, and SAVE-ALL algorithms. Section 5 shows the experimental results. Finally, in Section 6, we provide the concluding remarks.
In this section, we first introduce some basic concepts and notations that will be used in this paper. Then, we provide the problem formulation.
The  X  correlation coefficient [13] is the computation form of the Pearson X  X  correlation coefficient [5] for binary variables. In a 2  X  2 contingency table shown in Table 1, the calculation of the  X  correlation coefficient reduces to where P ( ij ) , for i  X  X  0 , 1 } and j  X  X  0 , 1 } , denotes the num-ber of samples which are classified in the i th row and j th column of the table, and N is the total number of samples. Furthermore, we let P ( i +) denote the total number of sam-ples classified in the i th row, and we let P (+ j ) denote the total number of samples classified in the j th column. Thus, P Table 1: A two-way contingency table of item A and item B .

Hence, when adopting the support measure of association rule mining [1], for two items a and b in a market basket database, we have supp ( a ) = P (1+) /N , supp ( b ) = P and supp ( a, b ) = P (11) /N . In Xiong et al. (2004) [15] the support form of the  X  correlation coefficient has been derived, as shown in Equation 2.  X 
Xiong et al. (2004) has also identified an upper bound for  X  { a,b } [15]. Without loss of generality, if we assume that supp ( a )  X  supp ( b ), then an upper bound for  X  { a,b }
For the purpose of simplicity, we denote N a as the number of transactions in the database that contain item a , N b number of those containing item b , and N ab as the number of those containing both items. Then supp ( a ) = N a /N , supp ( b ) = N b /N , and supp ( a, b ) = N ab /N . Substituting into Equation 2, we can calculate the  X  correlation coefficient for items a and b as
Here, we introduce the problem formulation. Let D be a transaction database, which has M items and N transac-tions. In this data set, a common task of correlation com-puting is to find all item pairs whose correlation coefficients are above a user-specified threshold  X  . This is known as the all-strong-pairs correlation query problem [15]. In this paper, we investigate the all-strong-pairs correlation query problem in a dynamic data environment.

Specifically, every time a data set of S new transactions is added into the original database D , we want to have the dynamically updated results from the all-strong-pairs cor-relation query. In other words, this all-strong-pairs cor-relation query can be a frequent task in a dynamic data environment. As a result, our goal is to develop an incre-mental, practical, and computation-efficient solution to this all-strong-pairs correlation query problem.
In this section, we first introduce the checkpoint princi-ple. Then, we provide some theoretical foundations for the checkpoint framework.
In general, there are three ways for developing the incre-mental solutions for frequent and dynamic all-strong-pairs correlation queries.

First, the simplest way is to recompute the correlation val-ues for all the item pairs every time new data sets of trans-actions become available. Along this line, we can use an efficient static all-strong-pairs correlation query algorithm, such as TAPER [15], for each computation. However, for very large data sets, this approach is infeasible if data up-dates are very frequent and the application needs the result in a timely manner. The second way is to use more space to save time [3]. Specifically, we can save the support of each item pair and update the values every time new data are added into the database. In this way, once all the intermediate computing results are saved, the all-strong-pairs correlation queries can be done very efficiently, but the memory requirement is very high. For instance, let us consider a database of 10 6 items, which may represent the collection of books available at an e-commerce Web site. There are ` 10 6 2  X   X  0 . 5  X  10 12 possi-ble item pairs, which need a huge amount of memory space to store intermediate computing results. In practice, this memory requirement cannot be satisfied for data sets with a large number of items.

Finally, we look for an answer between the above two solu-tions. Specifically, instead of saving the intermediate com-puting results for all item pairs, we propose to save them for only selected item pairs. Aiming for a tradeoff between time and space, we use a checkpoint to establish a computa-tion buffer, which can help us determine a correlation upper bound. Specifically, at a checkpoint, assuming that we know that  X  N ( X  N &lt;&lt; N ) new transactions will be added into the database before the next checkpoint, we can develop an upper bound for all the item pairs on N + X  N transactions. This upper bound has taken the newly added  X  N trans-actions into consideration. Therefore, based on this upper bound, we can establish a list of candidate item pairs whose upper bounds are greater than or equal to the threshold  X  . This list of candidate item pairs can be treated as a compu-tation buffer for all-strong-pairs correlation queries. While new transactions can be added into the buffer dynamically, we only need to maintain the intermediate results for item pairs in this candidate list as long as the cumulative number of new transactions is less than  X  N . The above process is illustrated in Figure 1.
 Figure 1: An Illustration of the Checkpoint Process.
The reason that the candidate list can remain unchanged (as long as the cumulative number of new transactions is less than  X  N ) is as follows. With a checkpoint at N +  X  N , we identify upper bounds for all item pairs for all N known transactions and  X  N unknown transactions. In other words, these upper bounds are the maximum possible values they can achieve no matter what kind of  X  N transactions have been added into the database. Then, if the cumulative num-ber of new transactions is less than  X  N , the upper bounds for all the item pairs in N +  X  N transactions will remain unchanged. Therefore, the candidate list will also remain unchanged. We call this the checkpoint principle.
Once the cumulative number of new transactions is greater than  X  N , we need to set a new checkpoint at N + 2 X  N . This iterative process will form an incremental solution for the dynamic all-strong pairs query problem. The rationale behind the checkpoint principle is that a small number of new transactions will not cause a significant effect on the correlation coefficients of most item pairs in the database if the total number of transactions is very large.
In Section 2.1 we have shown that the  X  correlation co-efficient can be computed by Equation 4. In the original database D , the frequencies for items a , b , and item pair { a, b } are denoted as N a , N b , and N ab , respectively. Suppose that at a checkpoint, we set the next checkpoint right after  X  N new transactions. In the  X  N new transactions, we as-sume that there are  X  N a ,  X  N b , and  X  N ab new transactions containing item a , item b , and item pair { a, b } , respectively. Then according to Equation 4, at the next checkpoint the new  X  correlation will be where N  X  = N +  X  N , N  X  a = N a +  X  N a , N  X  b = N b +  X  N
Unfortunately, we do not have any information about the  X  N new transactions, so  X  N a ,  X  N b and  X  N ab are all un-known. Thus, we cannot compute the new  X   X  directly. How-ever, because the size of the new data set is much smaller than that of the original database, for any pair of items, the new  X   X  is expected not to change greatly from the cur-rent  X  value. For this reason, we aim to derive an upper bound for  X   X  , and use it as a criterion regarding whether we should save the intermediate computation result for that pair. Specifically, if upper (  X   X  ) is less than the threshold  X  , then we can guarantee that item pair { a, b } will never be-come a strongly-correlated pair prior to the next checkpoint. As a result, we can save intermediate computation results only for pairs having upper (  X   X  ) beyond the threshold  X  .
However it is difficult to derive an exact upper bound for  X  , because the denominator and the numerator are both af-fected by common variables, and they do not change mono-tonically by these variables. In the following subsections, we derive a loose upper bound for  X   X  { a,b } when  X  N new transactions are added into the databases.

Looking closely at Equation 5, we can split the right hand side into three parts. Let u = N  X  N  X  ab  X  N  X  a N  X  b , v = N N ), and w = N  X  b ( N  X   X  N  X  b ), then Equation 5 can be re-written as
The upper bound is calculated as the maximum possible value of u divided by the product of the minimum possible values of v and w . The ratio is a loose upper bound for  X  because the maximum and the minimum values may not be achieved simultaneously. Since we do not know the exact value of  X  N a ,  X  N b , and  X  N ab to come, our derivations are only based on the fact that 0  X   X  N ab  X   X  N a  X   X  N , 0  X   X  N ab  X   X  N b  X   X  N , and  X  N a +  X  N b  X   X  N ab  X   X  N .
In this subsection, we derive the maximum possible value for the numerator  X   X  , u . Given N , N a , N b , N ab , and  X  N , the numerator of  X   X  { a,b } , written as u = N  X  N  X  ab  X  N  X   X  N )( N ab +  X  N ab )  X  ( N a +  X  N a )( N b +  X  N b ), is large when  X  N ab is large, and  X  N a and  X  N b are small. Specifically, we have the following lemma.

Lemma 1. Given N , N a , N b , N ab , and  X  N , the maxi-mum possible value for u , the numerator of the  X  correlation coefficient  X  { a,b } at the next checkpoint, is where f ( x ) = ( N +  X  N )( N ab + x )  X  ( N a + x )( N b N  X  N a  X  N b , and x  X  = ( N  X  N a  X  N b +  X  N ) / 2 .
Proof. Because of symmetry, we can assume without loss of generality that  X  N a  X   X  N b . Let  X  N ab = x , x  X  0;  X  N a = x + c 1 , c 1  X  0;  X  N b = x + c 1 + c 2 , c 2  X  0. In the following we derive the maximum value for u by taking first and second partial derivatives [7].

First, because  X  X / X  X  2 =  X  ( N a + x + c 1 ) &lt; 0, u increases monotonically as c 2 decreases. In order to reach the maxi-mum of u , c 2 must take the minimum value in its range, 0. Similarly, because  X  X / X  X  1 =  X  ( N b + x + c 1 + c 2 )  X  ( N x + c 1 ) &lt; 0, u takes the maximum value when c 1 = 0.
As a result,  X  X / X  X  = ( N +  X  N )  X  ( N b + x + c 1 + c 2 )  X  ( N a + x + c 1 ) = N  X  N a  X  N b +  X  N  X  2 x . Since  X  2 u/ X  X   X  2 &lt; 0, u reaches its maximum value when  X  X / X  X  = 0. Let  X  X / X  X  = 0, then the solution of the equation is x  X  = ( N + X  N  X  N a  X  N b  X  2 c 1  X  c 2 ) / 2 = ( N  X  N a  X  N b
However, because 0  X  x =  X  N ab  X   X  N , the above value can be reached only if  X   X  N  X  N  X  N a  X  N b  X   X  N . If N  X  N a  X  N b  X  X  X   X  N , then  X  X / X  X  = N  X  N a  X  N b +  X  N  X  2 x  X  X  X   X  N +  X  N  X  2 x =  X  2 x  X  0, therefore u reaches its maximum when x takes its minimum possible value 0. On the other hand, if N  X  N a  X  N b  X   X  N , then  X  X / X  X  = N  X  N a  X  N b + X  N  X  2 x  X   X  N + X  N  X  2 x = 2( X  N  X  x )  X  0. u reaches its maximum when x takes the maximum value  X  N . Now we have completed the proof of Lemma 1.
In this subsection, we derive the minimum value of the denominator of  X   X  , minimum value of vw .

First, a lower bound of vw can be reached by taking the minimum value of v and the minimum value of w . Again, the minimum values of v and w may or may not be reached simultaneously, so the lower bound for the denominator we derive here is also a loose bound.

Lemma 2. Given N , N a , and  X  N , the minimum possible value for v in Equation 6 is where h ( x ) = x ( N +  X  N  X  x ) is a function with respect to x , defined on the range [0 , N +  X  N ] .

Proof. Since v = N  X  a ( N  X   X  N  X  a ) = ( N a +  X  N a )( N +  X  N  X  N a  X   X  N a ) = h ( N a +  X  N a ), finding the minimum of v is equivalent to finding the minimum value of function h ( x ) within the range of N a +  X  N a . Since 0  X   X  N a  X  N , we have N a  X  N a +  X  N a  X  N a +  X  N . Now we will prove that the minimum possible value of h ( x ) in the range [ N a , N a +  X  N ] is either h ( N a ) or h ( N a +  X  N ).
Obviously h ( x ) is a quadratic function of x . It is concave and symmetric with respect to x = ( N +  X  N ) / 2. Thus, the further x = N a +  X  N a is from ( N +  X  N ) / 2, the smaller f ( x ) is. Because N a  X  0 and N a +  X  N  X  N +  X  N , N a  X  N a  X  [ N a , N a +  X  N ]  X  [0 , N +  X  N ]. The minimum value of f ( N a + X  N a ) must be at some end of the range [ N a  X  N ], depending on whether N a or N a + X  N is further from ( N +  X  N ) / 2. Now Lemma 2 has been proven.

Lemma 3. Given N , N b , and  X  N , the minimum possible value for w in Equation 6 is where h ( x ) = x ( N +  X  N  X  x ) is a function with respect to x , defined on the range [0 , N +  X  N ] .

Proof. Similar to the proof of Lemma 2, we can simply prove that the minimum possible value of h ( x ) in the range [ N b , N b +  X  N ] is either h ( N b ) or h ( N b +  X  N ).
In this section, we can derive a loose upper bound for  X  correlation coefficient  X  = u/ as the following:
Lemma 4. A loose upper bound for the new  X  correlation coefficient at the next checkpoint,  X   X  = u/ where u max follows Equation (7), v min follows Equation (8), and w min follows Equation (9).

Proof. The proof is straightforward, since we have u max , v min , and w min from Lemmas 1, 2, and 3.
In this section, we describe three different solutions to the all-strong-pairs correlation query problem in a dynamic data environment. First, we present a straightforward solution, named r-TAPER, which recomputes correlation coefficients for all the item pairs every time new data are added into the database. The second SAVE-ALL approach stores the intermediate computing results for all the item pairs and can greatly save the computational cost. Finally, we pro-vide an incremental solution, called CHECK-POINT, which strikes a balance between the use of memory space and the computational efficiency.
In this method, we need to recompute correlation coef-ficients for all the item pairs every time the database has been updated. No intermediate result has been reused for the next correlation computing practice. Since we have al-ready known that a brute-force way to compute all-strong-pairs correlation queries is computationally expensive [15], we apply the TAPER algorithm [15] in this study. TAPER is an efficient algorithm for the all-strong-pairs correlation query on static data [15]. Figure 2 shows the pseudo code of the r-TAPER algorithm which computes all-strong-pairs queries in a dynamic data environment. In the figure, we can see that the r-TAPER algorithm repeatedly calls the TAPER procedure every time new transactions are added into the database. Note that the implementation details of TAPER can be found in [15].
In the r-TAPER algorithm, we have observed the fact that the computational bottleneck for the all-strong-pairs query is to count the frequencies of all item pairs on the fly, and no intermediate computing results have been reused for the next correlation computation. On the contrary, the SAVE-ALL method stores the frequencies of all item pairs and incrementally update the stored values while new data are added into the database. In this way, SAVE-ALL uses more space for the sake of saving computation time.
 Figure 3 shows the implementation details of the SAVE-ALL algorithm. In this figure, Lines 1-5 process the dy-namic data, and update frequencies of items and item pairs as needed. The frequencies of individual items are saved on the main diagonal. Lines 6-12 compute the  X  correlation coefficient for each item pair and check if it is beyond the threshold  X  . Since the frequencies of all the item pairs are stored in the memory, the computation of  X  correlation co-efficient for each item pair is very efficient. However, the drawback of this SAVE-ALL method is that, if the number of items becomes extremely large, we may not have enough memory space for running the algorithm.

The above mentioned two algorithms are quite straight forward. However, they represent two extreme cases of the all-strong-pairs correlation query problem in a dynamic data environment. The r-TAPER algorithm, which repeat the query every time new data become available, disregards the previously computed results, and thus wastes a lot of com-putation. On the other hand, the SAVE-ALL algorithm requires an extremely large amount of memory space for saving the intermediate computing results. This becomes infeasible when the number of items is very large. As we have discussed in Section 3, we look for a solution in be-tween the above two methods. This solution should have the capabilities in storing some intermediate computing re-sults to save the computation, and do not overuse the mem-ory space. Based on what we have discovered in Section 3, we have developed a new algorithm called CHECK-POINT, which is described in Figure 4.

In the figure, we can see that the CHECK-POINT algo-rithm consists of three parts. The first part, Lines 1-7, reads the new data and only updates the frequencies of item pairs on the candidate list. The second part, Lines 8-15, computes the  X  correlation for each candidate pair and outputs if the correlation coefficient exceeds the threshold. Note that it is safe to ignore all other item pairs which are not on the candi-date list. This is guaranteed by the way that the candidate list is constructed (please refer to the checkpoint principle i n Section 3). The last part, described in Lines 16 through 19, calls a sub-procedure U pdateCandidateList , only if there is a checkpoint scheduled at the end of this step. In other words, the cumulative number of new transactions is greater than the computation buffer  X  N .

The U pdateCandidateList sub-procedure, described in Fig-ure 5, shows what happens at each checkpoint. Given that the original data set has been updated, and that the fre-Figure 5: The UpdateCandidateList subprocedure in CHECK-POINT. quency of each item is readily available, for each item pair { a, b } , we can compute an upper bound of its future  X  cor-relation coefficient if we know that the next checkpoint is scheduled after  X  N new transactions to come. If the upper bound is no less than the threshold, then the item pair is put into the candidate list; otherwise we know that the item pair will never have a correlation coefficient above the user-specified correlation threshold  X  even if another  X  N new transactions are added into the database.
In this section, we present the experimental results to evaluate the performance of the CHECK-POINT algorithm. Specifically, we study: (1) the computational performance of CHECK-POINT compared with r-TAPER and SAVE-ALL algorithms; (2) the performance of the CHECK-POINT al-gorithm in terms of the use of space.
Our experiments were conducted on three real-world data sets: chess , connect , and pumsb . The first two data sets, chess and connect , are from UCI Machine Learning Repository (http://archive.ics.uci.edu/ml/). The last data set, pumsb , which is often used as a benchmark data set for evaluating frequent pattern mining algorithms, is from FIMI (http://fimi.cs.helsinki.fi/data/).
 Table 2: Basic Characteristics of the Data Sets.

Data set # Items # Transactions Source chess 75 3196 UCI Repository connect 127 67557 UCI Repository pumsb 2113 49046 FIMI
Table 2 summarizes basic characteristics of the data sets used in our experiments. In this paper, our goal is to incre-mentally perform the all-strong-pairs correlation queries in a dynamic data situation. To mimic this dynamic real-world scenario, we did the following preprocessing on these three benchmark data sets. First, we generated a base data set of 100000 transactions for each data set by random sampling with replacement from the corresponding original data set. To make the results more comparable across different meth-ods, step sizes, and checkpoint sizes, we fixed the number of new transactions as 6000, a 6% increment of the base data. Again, these 6000 new transactions were generated by random sampling from the original data sets.

The difference between a step and a checkpoint is that a step is the number of new transactions after which we need an updated output of the strongly-correlated-pairs query, while a checkpoint is where we update the candidate list of item pairs. For example, an online store, which updates its bundle recommendations once they have received 1000 new transactions, has a step size 1000. The checkpoint size, how-ever, is a parameter for the CHECK-POINT method, which the store can choose regarding how many transactions are to be collected between two neighboring checkpoints. Ob-viously, checkpoints do not apply to r-TAPER and SAVE-ALL, but these three methods can be compared with respect to the number of transactions processed.

For each data set, we carried out three groups of experi-ments. The first group aimed to evaluate the effect of the checkpoint size for a fixed step size. Specifically, we fixed the step size as 100 and used checkpoint sizes 200, 500, 1000, 1500, and 2000. The second group of experiments evaluated the effect of step sizes, in which we fixed the checkpoint size as 1000, whereas tried step sizes 10, 50, 100, 200, and 250. The last group of experiments fixed the ratio of checkpoint size versus step size. When using different step sizes, such as 10, 50, 100, 150, and 200, we inserted a checkpoint at the end of every 10 steps, no matter what the step size was. The experimental groups are summarized in Tables 3, 4, and 5.
Step Size Checkpoint Size # Steps # Checkpoints
Step Size Checkpoint Size # Steps # Checkpoints
Step Size Checkpoint Size # Steps # Checkpoints
Experimental Platform. All the experiments were per-formed on a Dell Optiplex 755 Minitower with Intel 2 Quad processor Q6600 and 4 GB of memory running the Microsoft Windows XP Professional operating system.
In this subsection, we show a comparison of computational performance for three algorithms: r-TAPER, SAVE-ALL, and CHECK-POINT.

Figure 6 illustrates the running time of each step for the chess , connect , and pumsb datasets. Unsurprisingly, the SAVE-ALL algorithm costs the least time, because the pair-wise frequencies are stored in the memory. r-TAPER takes much longer time than SAVE-ALL and CHECK-POINT, because all the pairwise frequencies are not available and need to be counted every step when new data become avail-able. The time consumed by the CHECK-POINT algorithm is almost always as little as SAVE-ALL, except for the run-ning time at checkpoints, where CHECK-POINT need to take time for building a new candidate list of item pairs.
Even though CHECK-POINT takes longer time to update the candidate list at the checkpoints than one-running time of r-TAPER, overall the CHECK-POINT algorithm takes much less time than r-TAPER. In Figure 7, we illustrate the accumulative time at each step. Because the SAVE-ALL method is so fast, as time goes, its accumulative time grows very slowly. On the contrary, the accumulative time by r-TAPER increases much faster. The CHECK-POINT algo-rithm lies in between SAVE-ALL and r-TAPER. Its accu-mulative time increases slowly except for checkpoints where larger jumps can be observed. However, the overall trend shows that it increases much more slowly than r-TAPER. Figure 8 shows a comparison of the CHECK-POINT, r-TAPER, and SAVE-ALL algorithms at different threshold levels. We have already known that the higher the thresh-olds, the more item pairs are pruned by the r-TAPER algo-rithm; however in the dynamically growing databases, as the threshold goes down, the running time of CHECK-POINT increases much more slowly than that of r-TAPER.

Figure 9 shows the effect of the checkpoint size on the running time. Since the checkpoints are the most costly steps, the more frequent we update the candidate list, the more time in total we will need. However, using too few checkpoints will result in using more space. There must be a trade-off between time and space, on which we will show more detail in the next subsection.

To study the effect of step sizes, we fix the checkpoint size and plot the accumulative running time in Figure 10. We can see that all the curves are almost overlapping each other. Again, the reason is that the checkpoints are the most costly steps, thus the choice of step sizes does not affect the performance greatly. The implications on real world applications is that sizes of steps, where we want an update-to-date output of all the strongly correlated pairs, can be determined by the application itself. All we need to decide is the choice of the checkpoint size, so that both the running time and the space required is reasonably balanced and practical.
 Finally, when fixing the step size and the checkpoint size, Figure 11 shows the effect of correlation thresholds on the running time. It is easy to see that, overall, the lower the threshold, the more running time is needed because of the expanded candidate list. Our experiments on other data sets and parameters show similar trends. Due to the page limit, we do not present those experimental results in this paper. The trend is similar to that in the case of TAPER [15, 16], where the higher the threshold, the more item pairs are pruned and the less computation is needed. In this section, we investigate the use of space by the CHECK-POINT algorithm. For different data sets and pa-rameters, our goal is to find out how many item pairs need to be maintained in the candidate list. ( S = 100 , C = 1000 ). Figure 9: The accumulative running time at each step of the CHECK-POINT algorithm on chess . ( S = 100 ,  X  = 0 . 6 ).

To study the pruning effect of different densities of check-points, we fix the step size and the correlation threshold, and then get the plots in Figure 12. In this figure, similar trends can be found for different data sets.

First of all, the denser the checkpoints, the fewer candi-dates are needed. In each of the subgraphs, each data point corresponds to a checkpoint. We can see that curves with fewer checkpoints lie higher than those with more check-points. This indicates that the less frequent the checkpoint is, the more candidates are needed to be stored.

Secondly, as time goes, the number of candidates may vary. Our experiments show that the number of candidates increases or decreases only slightly over time. The reason is that we generated the data sets uniformly at random, which eliminates the evolving trend over time in the original data sets. However, this may not be the case in real applications.
Finally, the pruning ratio is data dependent. As an ex-ample, Table 6 shows the sizes of candidate lists for three test data sets. In the table, we can see that the number of candidate item pairs for chess is only 4, meaning that 99.86% of all possible item pairs are pruned. Instead of stor-ing the frequencies of all 2775 item pairs, we only need to track the change of 4 pairs. Similarly, the connect data set has a pruning ratio of 97.60%. The pruning ratio for pumsb is only 45 . 57%, which explains why the computa-tional performance of CHECK-POINT on the pumsb data set is not as good as that on the other two data sets. The reason is that the chess and connect data sets are much Figure 10: The accumulative running time at each step of the CHECK-POINT algorithm on chess . ( C = 1000 ,  X  = 0 . 6 ). denser than pumsb . Our proposed CHECK-POINT algo-rithm works better on dense data sets.
 Table 6: The Sizes of Candidate Lists ( S = 10 , C = 100 ,  X  = 0 . 7 ).

In this paper, we studied the problem of correlation com-puting in large and dynamically growing data sets. Specifi-cally, we proposed a CHECK-POINT algorithm, which can incrementally search for all the item pairs with correlations above a user-specified minimum correlation threshold. The key idea is to establish a computation buffer by setting a checkpoint for dynamic input data. This checkpoint can be exploited to identify a list of candidate pairs, which are maintained and computed for correlations as new transac-tions are added into the database. However, if the total number of new transactions is beyond the checkpoint, a new candidate list is generated at the new checkpoint. Experi-mental results on real-world data sets show that CHECK-POINT can compact the use of memory space by maintain-ing a candidate pair list, which is only a very small portion Figure 11: The accumulative running time at each step of the CHECK-POINT algorithm on chess . ( S = 100 , C = 1000 ). of all the item pairs. Also, CHECK-POINT can significantly reduce the correlation computing cost in dynamic data sets with a large number of transactions. This research was partially supported by the Rutgers Seed Funding for Collaborative Computing Research. Also, this research was supported in part by a Faculty Research Grant from Rutgers Business School-Newark and New Brunswick. [1] R. Agrawal, T. Imielinski, and A. Swami. Mining [2] C. Alexander. Market Models: A Guide to Financial [3] J. Bentley. Programming Pearls . Addison-Wesley, Inc., [4] S. Brin, R. Motwani, and C. Silverstein. Beyond [5] J. Cohen, P. Cohen, S. West, and L. Aiken. Applied [6] P. Cohen, J. Cohen, S. G. West, and L. S. Aiken. [7] R. Courant and F. John. Introduction to Calculus and [8] W. DuMouchel and D. Pregibon. Empirical bayes [9] I. F. Ilyas, V. Markl, P. J. Haas, P. Brown, and [10] C. Jermaine. The computational complexity of [11] C. Jermaine. Playing hide-and-seek with correlations. [12] W. Kuo, T. Jenssen, A. Butte, L. Ohno-Machado, and [13] H. T. Reynolds. The Analysis of Cross-classifications . [14] H. V. Storch and F. W. Zwiers. Statistical Analysis in [15] H. Xiong, S. Shekhar, P. Tan, and V. Kumar.
 [16] H. Xiong, S. Shekhar, P.-N. Tan, and V. Kumar.
