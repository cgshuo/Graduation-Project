 Yan Liu, Mohammad Taha Bahadori { yanliu.cs, mohammab@usc.edu Hongfei Li liho@us.ibm.com IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 USA Time series analysis and modeling have been exten-sively studied in the literature and successfully found applications across domains (Box &amp; Jenkins, 1990; Hamilton, 1994). In many applications, such as cli-mate science, social media analysis and smart grid, we are mostly interested in revealing the temporal depen-dence and make predictions of extreme events. For example, climate change is mostly characterized by increasing probabilities of extreme weather patterns (IPCC, 2007), such as temperature or precipitation reaching extremely high value. Therefore quantifying the temporal dependence between the extreme events from different locations and make effective predictions are important for disaster prevention; in social me-dia analysis, burst of topics, i.e., buzz, is reflected by extremely high frequency of related words. Uncover-ing the temporal dependencies between buzzes could reveal valuable insights into information propagation and achieve much better accuracy for buzz prediction. Identifying temporal dependencies between multiple time-series data is a topic of significant interest (Arnold et al., 2007; Lozano et al., 2009a;b). Many algorithms are proposed to automatically recover the temporal structures, such as autocorrelation, cross-correlations (Box &amp; Jenkins, 1990), randomization test (Edgington &amp; Onghena, 2007), Granger causal-ity (Granger, 1980), transfer entropy (Beirlant et al., 1997; Barnett et al., 2009), and so on. However, uncov-ering temporal dependency for extreme values is much more challenging than classical observations since the distributions of extreme values are more complex and significantly different from the commonly used Gaus-sian distribution. In addition, the lack of sufficient past observations on extreme events poses difficulties in modeling and attributing such events.
 The statistical approach we can utilize to solve these important problems is the theory of extreme value modeling (Coles, 2001; Beirlant et al., 2004), which provides a natural family of probability distributions for modeling the magnitude of the largest (or smallest) of a large number of events, and a canonical stochas-tic process model (Coles, 2001) for the occurrence of events above a very high (or below a very low) thresh-old. In the past decade, extreme value modeling has attracted a lot of research efforts in statistics, finance, and environmental science, particularly on modeling temporal and spatio-temporal extreme value (Coles &amp; Tawn, 1996; Ferro &amp; Segers, 2003; Huerta &amp; Sanso, 2007). However, all of the work above model tempo-ral or spatial dependence with predefined covariance structures (e.g. without independence considerations). Furthermore, most general discussions of dependen-cies in multivariate extreme value modeling has been focused on pairwise relationships. This is obviously unrealistic and demands a significant contribution on automatically learning the temporal structures from the data for better analysis and modeling.
 In this paper, we propose a sparse latent space model, namely sparse-GEV model, to solve the problem. The basic idea of our approach is to model the multivariate extreme value time series as a latent space model. The latent variables, corresponding to the location param-eters (which determine the mode) of extreme value dis-tributions for time series at certain time, are modeled by the location parameters of all time series in history, through a dynamic linear model (DLM). By impos-ing an L 1 -penalty with respect to the regression coef-ficients in DLM, we could establish meaningful tempo-ral dependencies between a small subset of time series and the concerned time series of extreme values. To estimate parameters of the model, we develop a itera-tive searching algorithm based on the generalized EM-algorithms and sampling with particle filtering. Our model is significant because it is among the first models to reveal the temporal dependencies between multiple extreme value time series. In addition, our experiment results demonstrate the superior performance of our model to other state-of-art methods on both learning temporal dependence and predicting future value. The rest of the paper is organized as follows: we first describe the details of our proposed model in Section 2, then we review the existing work and discuss their connections to our model in Section 3, we show the experiment results in Section 4, and finally we sum-marize the paper with hints on future work. Preliminaries Before diving into the details of our model, we first briefly review the extreme value the-ory (Coles, 2001). Let X 1 ,  X  X  X  ,X m be a sequence of independent and identically distributed random vari-ables, and let M m = max { X 1 ,  X  X  X  ,X m } . If there exist sequences of constants a m &gt; 0 and b m such that for some non-degenerate distribution function G , then G should belong to the generalized extreme value (GEV) families, namely defined on { z : 1 +  X  ( z  X   X  ) / X  &gt; 0 } , where  X  (  X  X  X  &lt;  X  &lt;  X  ) is the location parameter,  X  (  X  &gt; 0) is the scale, and  X  (  X  X  X  &lt;  X  &lt;  X  ) is the shape parameter  X  , which governs the tail behavior of the distribution. One popular GEV distribution is the Gumbel distri-bution when  X   X  0, whose pdf is defined as p ( z |  X , X  ) = It has been shown that the maximum value in a sam-ple of a random variable following an exponential fam-ily distribution (such as Gaussian, Lognormal and Gamma distributions) converge to the Gumbel distri-bution. One special property of the Gumbel distri-bution is that the mode is determined solely by the location parameter  X  . 2.1. Model Description Given multivariate time series data, our goal is to build an effective model that can recover temporal depen-dence between extreme value time series (block max-ima or peaks over threshold) and make accurate pre-dictions for future extreme events. To achieve a robust and interpretable model, a natural choice is to cap-ture the temporal dependence via linear models; how-ever, this is not directly achievable on extreme value variables since their temporal dependence is obviously nonlinear. To solve the problem, we propose latent models in which the location parameters of GEV dis-tributions are latent variables and the temporal de-pendence between extreme value variables is captured via the latent variables through dynamic linear model. We choose the location parameters because they cap-ture the mode of extreme value variables and can be modeled reasonably well by linear dependence.
 Formally, let x 1 ,..., x P denote P number of extreme value time series and each time series x i have T ob-probability of observations { x i t } and their associated location parameter {  X  i t } as: where p ( x i t |  X  i t , X  i ) can be modeled by a GEV distribu-tion such as the Gumbel distribution in eq(3) with  X  i as the scale parameter specific to time series x i , {  X  j is the history of all time series at time t with a maxi-a dynamic linear model as follows, where c i is the offset specific to time series i ,  X  i are the coefficients, and is a Gaussian noise with variance  X  . As we can see, the temporal dependence between x i and x j is now captured via the coefficients  X  . By adding a shrinkage Laplace prior over  X  when maxi-mizing the likelihood function, i.e., {  X   X  ,  X   X  ,  X  c } = arg max ` ( x 1 ,..., x P ;  X  ,  X  , c )+ where  X  is the regularization parameter, we can ob-tain the sparse solution of  X  . Finally, we determine that x i temporally depends on x j if the correspond-ing value of  X  i j is non-zero. In this way, our model not only can provide better understanding of potential causes of the extreme events, but also helps to achieve more accurate prediction of the extreme events in the future. This model is later referred to as the Sparse-GEV model. 2.2. Inference and Learning Given the existence of hidden variables in the sparse-GEV model, directly maximizing the likelihood as in eq(6) is not feasible. Therefore we applied the gener-alized EM-algorithm to solve the problem.
 Next, we use Gumbel distribution as an example to demonstrate how we can make efficient inference and learning in the proposed model. In the EM algorithm, we optimize the following function via two steps: E Step Directly calculating the expectations in eq(7) is infeasible given the form of the posterior prob-ability, therefore we apply sampling algorithms for approximation. In order to generate samples from algorithm (Doucet &amp; Johansen, 2009). The major challenge is that in each iteration of particle filtering, we need to draw samples from p (  X  i t | x i t , {  X  j t  X  l cannot be calculated analytically. Instead, we use the following proposal function: where W 0 is the Lambert W function,  X  i =  X / X  i , and  X   X  i t is calculated using the history, i.e.,  X   X  c this choice is to approximate the posterior distribution p  X  | X ,  X  old ,  X  old , c old with a Gaussian distribution with the same mode and similar variance.
 Notice that particle filtering may encounter the chal-lenge of  X  X iniscule weights X  if the sequence length is long. Therefore the resampling step is usually applied at each time stamp to resolve the issue (Doucet &amp; Jo-hansen, 2009). For very long time series, particle filter-ing does face some other challenges, but can be fixed using particle smoothing (Doucet &amp; Johansen, 2009). M Step The optimization problem for updating  X  i and c i is as follows: min where the expectation is computed from the samples. As we can see, the optimization function has the Lasso format and can be solved efficiently by algorithms such as coordinate descent (Wu &amp; Lange, 2008). The parameter estimation for the Gumbel distribution it-self is not a trivial problem. In general, the MLE is the widely accepted approach to estimate the shape and scale parameters, and Newton-Raphson or quasi-Newton methods can be applied to solve the resulting optimization problem (Evans et al., 2000). Therefore we estimate  X  by the Newton-Raphson algorithm. 2.3. Prediction In order to make predictions on the future value of extreme events, for example x i T +1 , given the extreme value time series up to time T , we can first estimate the mean  X   X  i T +1 using the samples drawn from the posterior distribution with the learned parameters. Based on the model defined in eq(4), we can predict x i T +1 as 0 . 5771) is the Euler constant. 2.4. Scalability The computational complexity of Sparse-GEV de-pends on two factors: the number of EM iterations required for convergence and the scalability of E-Step and M-Step. In Section 5, we empirically show that EM usually converges within a small number of itera-tions. In the M-Step, while there are efficient solvers for both equations, the problems for different time se-ries are independent and can be implemented in par-allel. The particle filtering in E-Step is notoriously efficient for sampling from time series mainly due to three reasons: (i) it requires only one iteration to gen-erate the samples, (ii) the generated samples are inde-pendent; no burn-in period or decoupling is required and (iii) at each time stamp the sampling procedures in different locations are independent from each other and can be implemented in parallel. Therefore our al-gorithm is scalable and could be easily applicable to practical applications. Very recently, a few advanced approaches have been explored to uncover temporal dependence from time series data, including Lasso-Granger (Arnold et al., 2007), transfer entropy (Schreiber, 2000), and the cop-ula approach (Liu et al., 2009). In this section, we discuss how these algorithms can be applicable to ex-treme value time series analysis and their connections to Sparse-GEV. 3.1. Related Work Granger causality In (Arnold et al., 2007), the Lasso-Ganger algorithm, an effective and efficient ap-proach to learn sparse temporal graphs, is developed by combining Granger causality with sparse neighbor-hood selection using L 1 -penalized regression. More specifically, given p number of time series, x 1 ,..., x p where x i = { x i t : t = 0 ,...,T } , let X Lagged t,L sent the concatenated vector of all the lagged vari-ables (with a maximal lag of L) of up to time t , i.e., { x j : j = 1 ,...,p,l = 1 ,...,L } . Then the tempo-ral graphs can be learned by the following regularized regression:  X   X  i (  X  ) = arg min  X  where there is an edge from x j to x i if and only if at least one of the corresponding coefficients in  X   X  i is non-zero. The Lasso-Granger algorithm can be di-rectly applied to extreme value observations to infer the dependency graph, but obviously this violates the common assumptions of linear dependency in Granger causality.
 Transfer Entropy Solution Transfer entropy is usu-ally employed when the data do not follow the auto-regressive model and a nonlinear generalization of the Granger causality framework is desirable. In the Transfer entropy framework (Schreiber, 2000), time se-ries x i is thought to be a cause of another time series x j if the values of x i in the past significantly decrease the uncertainty in the future values of x j given its past. The amount of decrease in the uncertainty can be quantified as T where H ( x ) is the Shannon entropy of the random variable x . Since the transfer entropy is a pairwise quantity, we can use its output as input to a graph learning algorithm, for example, IAMB (Tsamardi-nos et al., 2003), to uncover the temporal dependency among multiple time series.
 The transfer entropy approach can be used to uncover causality relationship among extreme value time series since it does not rely on any particular assumptions on the distribution of the time series.
 Copula Approach The copula approach has been proposed for dependency analysis of time series with non-Gaussian marginal distributions (Embrechts et al., 2002). It has been used for forecast in time series (Leong &amp; Valdez, 2005) and learning sparase dependency structures (Liu et al., 2009). In a copula framework, e.g., Gaussian copula, the marginal distri-bution of the time series X i are estimated as  X  F i . Next the observations are transformed to the Gaussian cop-ula domain as U i t =  X   X  1 (  X  F i ( X i t )), where  X  is the cdf of the unit Gaussian distribution. Finally the tempo-ral causal graph can be uncovered by analysis of de-pendency among U i t using algorithms such as glasso algorithm (Friedman et al., 2008). We report an edge from node i to node j if the precision matrix has at least one non-zero element from lagged U j t  X  ` to U i t `  X  1. The method in (Leong &amp; Valdez, 2005) can be used for predicting the future values of the time series. In order to uncover temporal dependencies among ex-treme value time series, we can either estimate the marginals with a non-parametric density estimator or use the GEV distribution to estimate the marginal dis-tribution. For extreme value time series, the latter is preferred since the non-parametric approximation of the marginal distributions could lead to over-fitting when the number of observations is scarce. 3.2. Connections to existing algorithms The connections between our algorithm with existing algorithms can be established by considering Sparse-GEV, transfer entropy and the copula approach as extensions of the Granger causality framework. The copula approach leverages the marginal distribution of the time series to map the observations to another space and assumes linear dependence in the new space. Sparse-GEV discovers the Granger causality relation-ship among the latent variables from which the ob-servations have been generated. The transfer entropy approach generalizes the Granger causality framework by finding the Granger causality type relationships from the uncertainty of the time series. In fact, when the data are distributed according to Gaussian lin-ear model, transfer entropy is equivalent to Granger causality (Barnett et al., 2009).
 For high-dimensional time series, the number of obser-vations is much less than the parameters of the model. The Lasso-Granger algorithm benefits from the vari-able selection properties of Lasso. (Meinshausen &amp; B  X uhlmann, 2006) show that the Lasso variable selec-tion loss, and subsequently the Lasso-Granger X  X  loss (Arnold et al., 2007), vanishes with an exponential rate. For the copula approach, (Liu et al., 2009) show that when copula-based model is the true model, the copula-based structure learning algorithm with non-parametric estimation of marginals converges to  X   X  (0 , 1), which is far slower than the exponential convergence of Lasso-Granger. The performance of transfer entropy heavily relies on the accuracy of en-tropy estimations, which require a large number of observations, especially for high dimensional distribu-tions, to achieve robust estimation (Beirlant et al., 1997). For example, the Nearest Neighbor Estimator converges with root-n rate, which is again far slower than the convergence rate of Lasso-Granger. However, Sparse-GEV inherits the variable selection advantages of Lasso-Granger while allows a more flexible marginal distribution for the observations. It is fully paramet-ric, and together with proper ` 1 penalization can avoid over-fitting while capturing non-linear dependencies. In order to evaluate the effectiveness of our algorithm, we conduct experiments on four datasets, including one synthetic dataset, one weather dataset and two Twitter datasets. The experiment results are evalu-ated on both how well we uncover the temporal de-pendence graphs and how accurately we can predict the future value of extreme events using the learned temporal dependence. 4.1. Datasets Synthetic Dataset We generate eight synthetic datasets, each composed of nine time-series with dif-ferent types of temporal dependence, one of which is shown in Figure 1(a). Time series of length T = 40 are generated in two steps: (i) A set of observations of the location variables  X   X  is generated according to eq (4), with the offset { c i } generated from N (0 . 2 , 0 . 05), the coefficients  X  set to have stationary time series,  X  2 set to 0.1 and the time lag L set to 2; (ii) The observations  X  x are generated from a Gumbel distribution with the corresponding location parameters  X   X  and scale param-eter  X  i = 0 . 05 for all time series.
 Climate Dataset The study of extreme value of wind speed and gust speed is of great interest to the climate scientists and wind power engineers. A collection of wind observations is provided by AWS Convergence Technologies, Inc. of Germantown, MD. It consists of the observations of surface wind speed (mph) and gust speed (mph) every five minutes. We choose 153 weather stations located on a grid laying in the 35 N  X  50 N and 70 W  X  90 W block. Following the traditions in this domain, we generated extreme value time series observations, i.e, daily maximum values, at different weather stations. The objective is examine how the wind speed (or gust speed) at different locations affects each other and how well we can make predictions on future wind speed.
 Twitter Dataset In social media analysis,  X  X uzz X  refers to those topics or memes that many people are talking about at the same time with rapid growth and impact. Buzz modeling and predictions are the funda-mental problems in computational social science, but they are extremely challenging since the distributions of these time series observations have heavy tails and most existing models fail miserably. Given the defini-tion of buzz, i.e., extremely high frequency of certain words within a time interval, it is natural to model them via extreme value theory. We collected two Twit-ter datasets to evaluate the effectiveness of our model: one is the most popular 20 meme phrases during a 28-day interval from Nov-Dec 2009, and the other is pop-ular hashtags around  X  X ccupy wall street X  during a 21-day interval in Oct-Nov 2011. Some example phrases in the first dataset are  X  X aiti earthquake X ,  X  X rammy Awards X ,  X  X Pad release X , and  X  X cott Brown X  X  Sen-ate Election X ; some example hashtags in the second dataset are #OWS , #OccupyLA , #OccupySF , #OccupyDC and #OccupyBoston . For those phrases and hashtags, we count the number of mentions in tweets within a interval of one hour. We are interested in uncovering how different buzzes affect each other and how well we can make predictions on future buzz. 4.2. Performance Comparison We compare the performance of our Sparse-GEV model with three baselines, including Granger causal-ity, transfer entropy, and the copula method (with Gaussian copula), on two tasks: uncovering the un-derlying dependency among time series, and predict-ing the future values of time series. The first one re-quires knowledge about the true dependency structure, which is only available in the synthetic dataset. For evaluation, we use the Area Under the Curve (AUC) score, i.e., the probability that the algorithm will as-sign a higher value to a randomly chosen positive (ex-isting) edge than a randomly chosen negative (non-existing) edge in the graph. In the prediction task, we conduct experiments via the sliding window ap-proach: given time series observations of length T and a window size S , we train a model on observations of x s ,...,x T  X  S + s  X  1 and test it on the ( T  X  S + s ) sample, for s = 1 ,...,S . We set S to 10 for all the datasets and use the root mean squared error (RMSE) measure (averaged over S experiments and all nodes) as the evaluation metric.
 In the experiment, the regularization parameter  X  is set via cross-validation. All the observations are nor-malized into interval [0 , 1] prior the experiments. Temporal Dependence Discovery Table 1 lists the average accuracy of uncovering the underlying de-pendence structures by different algorithms on the syn-thetic data (consisting of 8 different datasets). As we can see, our Sparse-GEV model significantly outper-forms the baseline methods. Figure 1 shows an ex-ample of the graphs learned by different algorithms: our model can recover the ground-truth graph more accurately than other methods.
 Fig. 2 shows the inferred temporal dependencies from the extreme value time series of wind speed and wind gust speed by Sparse-GEV. Given the limited space, we limit our discussion on the new york region. The main observation is that the weather in the inland re-gions are heavily influenced by the coastline region. The wind gust graph (Fig. 2(b)) indicates two clus-ters. One is at the top of the graph, starting from Middletown to Danbury across Fishkill. The other one is located at the bottom of the graph, which passes through several cities, such as Stamford, Fairfield and Brookhaven, around Long Island Sound, then goes to inland cities in New Jersey through New York City. The top cluster gives an example of wind gust path in inland while the bottom one shows the coastal impact of Long Island Sound and the impact extends to inland New Jersey. Comparably, in addition to the Middle-town to Danbury inland cluster in wind gust graph, the wind graph (Fig. 2(a)) shows another inland clus-ter centered at Bridgewater, which has strong tempo-ral dependencies with neighboring cities (confirmed by the climatologists).
 Fig. 3 shows the inferred temporal dependencies from the extreme value time series of Twitter data on a sub-set of buzz. From Fig. 3(a), we can see that the tempo-ral dependence between different buzz are sparse (since they are quite different topics); however, the buzz on  X  X aiti Earthquake X  generates a huge impact on the whole Twitter universe and significantly changes the future mentions of other popular meme phrases. An interesting observation in Fig. 3(b) is that the hashtag on the general theme #OWS has direct temporal depen-dence with the city-specific hashtags, such as #O-LA and #O-DC , while city-specific hashtags do not affect each other.
 Prediction Performance As discussed in Section 2, Sparse-GEV can also be used for predicting future extreme events. For other baseline methods, we use the approaches discussed in Section 3.1 for predic-tions. Table 2 shows the prediction accuracy of differ-ent algorithms on all datasets. As we can see, Sparse-GEV outperforms all the other algorithms across all datasets. This can be attributed to two properties of Sparse-GEV: its flexibility in modeling complex distri-butions and its effectiveness in utilizing the samples. The assumptions of Lasso-Granger and copula meth-ods about the distribution of the data can be respon-sible for their lower performance. Transfer entropy re-quires many observations to perform well, which could be a potential issue in the real applications. 4.3. Parameter Sensitivity Assessment Like other latent state models, Sparse-GEV model has many parameters, which could affect its performance significantly. In our last experiment, we assess the pa-rameter sensitivity. Fig. 4(a) shows that in a large range of values for the regularization parameter  X  , the graph learning accuracy remains unchanged and little effort in selection of the regularization parameter leads to the optimal performance. Fig. 4(b) suggests that in less than 10 EM iterations, our algorithm converges to the optimal point. Fig. 4(c) illustrates the effect of  X  on the performance of Sparse-GEV. Small values of  X  result in smoother estimation of E [  X  | X ], while higher values lead to sensitive estimation (as a result E [  X  | X ] closely follows the observation time series). This ob-servation suggests that we should monitor the sam-ple mean of the latent variables and choose a value of  X  that allows smooth latent variables to capture the trend of observations. In this paper, we propose sparse-GEV, a sparse la-tent space model, to uncover the sparse temporal de-pendency from multivariate extreme value time series. To estimate the parameters of the model, we develop an iterative searching algorithm based on the general-ized EM-algorithm and sampling with particle filter-ing. Through extensive experiments, we demonstrate that Sparse-GEV outperforms the state-of-the-art al-gorithms such as copula and transfer entropy. For fu-ture work, we are interested in the theoretical analysis on the consistency of the Sparse-GEV model.
 We thank the anonymous reviewers for their valuable comments. This research was supported by the NSF research grants IIS-1134990.

