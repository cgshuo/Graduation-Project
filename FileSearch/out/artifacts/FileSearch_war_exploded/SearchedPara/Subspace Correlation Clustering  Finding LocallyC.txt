 The necessity to analyze subspace projections of complex data is a well-known fact in the clustering community. While the full space may be obfuscated by overlapping patterns and ir-relevant dimensions, only certain subspaces are able to reveal the clustering structure. Subspace clustering discards irrel-evant dimensions and allows objects to belong to multiple, overlapping clusters due to individual subspace projections for each set of objects. As we will demonstrate, the obser-vations, which originate the need to consider subspace pro-jections for traditional clustering, also apply for the task of correlation analysis.

In this work, we introduce the novel paradigm of subspace correlation clustering: we analyze subspace projections to find subsets of objects showing linear correlations among this sub-set of dimensions. In contrast to existing techniques, which determine correlations based on the full-space, our method is able to exclude locally irrelevant dimensions, enabling more precise detection of the correlated features. Since we analyze subspace projections, each object can contribute to several correlations. Our model allows multiple overlapping clusters in general but simultaneously avoids redundant clusters de-ducible from already known correlations. We introduce the algorithm SSCC that exploits different pruning techniques to efficiently generate a subspace correlation clustering. In thor-ough experiments we demonstrate the strength of our novel paradigm in comparison to existing methods.
 Categories and Subject Descriptors: H.2.8 Database management: Database applications [Data mining] Keywords: linear correlations, overlapping clusters
The goal of correlation clustering 1 is to identify groups of objects that exhibit dependencies between the features of the dataset. Considering correlations, the attribute values in one dimension depend on the attribute values of other di-mensions. In contrast to clustering, the objects do not have to be closely located together but should describe the same regression, e.g., they are located near the same line or plane. Fig. 1: 4-d database with 15 objects and 4 subspace correlation clusters Knowing about the dependencies of attributes provides a mo-tive to reason about causalities and is advantageous for, e.g., trend analysis, decision strategies, and prediction.
Mistakenly, in the literature correlation clustering is char-acterized as generalized subspace clustering [16]. Contrarily, we show in the following how traditional correlation cluster-ing falls short of the fundamental observations and assump-tions which originated the research field of subspace cluster-ing and that the actual transfer of the correlation principles towards subspace clustering still needs to be done.
It have been two crucial observations that led the subspace clustering community to the conclusion that solely consider-ing the full-space might deny access to a meaningful cluster-ing structure of the data. First, for high-dimensional data, objects do not necessarily show cohesion regarding all at-tributes but only w.r.t. a subset of attributes. As each clus-ter can have its individual set of relevant attributes, global dimensionality reduction does not do the trick. Second, if ob-jects are clustered based on differing characteristics, they can easily belong to multiple clusters when considering different views, i.e., attribute subsets of the data. Given a multitude of overlapping clusters in different subspaces, an immediate consequence is that the clustering structure might be totally obscured in the full-space [10]. This is the key challenge to be solved by subspace clustering and multi-view clustering [18, 16], where clusters highly overlap and multiple partitionings in different views can be determined. Both aspects, the ob-fuscation of clusters in the full-space and highly overlapping clusters, are not considered by existing correlation clustering methods. Nonetheless, these observations do also hold for correlation clustering as Fig. 1 illustrates. By considering the 2-dim. subspace { d 1 ,d 2 } in Fig. 1, two different (local) cor-relations can be detected: The objects denoted by a cross are positively correlated on a line, while the objects denoted by a circle are negatively correlated on a different line 2 .Consid-ering the subspace { d 3 ,d 4 } , different correlations supported by different sets of objects can be detected. Thus, observable correlations might be fulfilled only for a subset of objects and a subset of attributes and also objects might contribute to several correlations of different attributes.

Why do we highlight this problem? One might argue that given an appropriate set of objects, correlation clustering can easily determine the subset of correlated dimensions and ex-cludes the irrelevant dimensions by a post-processing. The crux, however, is to find the  X  X ppropriate X  set of objects in the first place. If patterns are hidden in subspace projections of the data this is an almost impossible task for approaches op-erating in the full-space. Since correlation clustering analyses the full-space, noisy dimensions and overlapping clusters will obfuscate the local correlations of objects leading to nearly arbitrary groupings in the full-space (similar to traditional full-space clustering). In Fig. 1 it is very unlikely to find a group of objects in the full-space that corresponds to a strong correlation in a subspace. If, however, already wrong sets of objects have been clustered, determining the true subset of correlated dimensions in a post-processing is almost impos-sible. Thus, correlation clustering will not just miss some clusters, but the resulting clusters are highly questionable as for the traditional full-space clustering approaches.
A novel mining paradigm. In this paper, we show that the observations made for (subspace) clustering also have to be transferred to the task of correlation analysis and we introduce the novel paradigm of subspace correlation cluster-ing : First, correlations between dimensions can be restricted to subspace projections, i.e., not the whole set of dimen-sions is correlated. Second, correlations can be hidden in local and possibly overlapping patterns, i.e., only subsets of the objects may be correlated and objects may contribute to several correlations. With our paradigm we detect locally correlated dimensions in subspace projections. This enables to find more precise local patterns and unfolds the potential of correlation clustering also for higher dimensional datasets.
Challenges for subspace correlation clustering. By analyzing subspace projections, we inherit two crucial chal-lenges of traditional subspace clustering. The first one arises from the exponential number of subspaces to be analyzed. Obviously, a naive exploration of all possible projections is not recommendable. Therefore, we present an efficient algo-rithm that uses already acquired information to apply prun-ing strategies. The second challenge is originated by allowing each object to contribute to several correlations. While over-lapping clusters are beneficial for detecting multiple patterns in the data, analyzing each subspace projection and simply reporting any pattern, results in highly redundant informa-tion. For example a line detected in subspace S is also a line in any of its projections S  X  S (with | S | X  2). Since these projections represent similar correlations, they are not ben-eficial for the user. Even worse, some correlations just exist due to other patterns and, thus, they represent misleading (and redundant) information. Therefore, we have to develop a redundancy model handling these scenarios to ensure inter-pretable results. Overall, the contributions of our work for the novel paradigm of subspace correlation clustering are:
In the following, we review paradigms related to the topic of subspace correlation clustering. The general clustering ob-jective is to group objects based on their mutual similarity. For traditional clustering, similarity of vector data is deter-mined based on all attributes of the feature space.
Subspace Clustering [16], in contrast, determines clusters that excel by a high compactness or density in projections of the original feature space. The main challenges are to handle the exponentially many subspaces and to avoid redundancy in the result. To reduce the number of potentially relevant subspaces, several techniques exploit the apriori principle [6, 17]. Redundancy is mostly evaluated based on the clusters X  similarity w.r.t. objects and attributes [6, 7, 14].
Projected Clustering is related to subspace clustering. How-ever, projected clustering performs an (almost) partitioning of the objects, i.e., for each partition a set of relevant di-mensions is detected. As consequence, it misses many of the hidden clusters or the resulting solution is of low quality since it tears multiple clusters apart to fit one partitioning.
Correlation Clustering aims at identifying object groups describing correlations between different features. Since the clusters X  dimensions are not restricted to subsets of the orig-inal attributes but correspond to arbitrarily oriented sub-spaces, correlation clustering is often denoted as generalized subspace clustering [16]. This, however, is inaccurate; exist-ing methods are rather generalized projected clustering meth-ods and have the same limitations: They are not able to find multiple overlapping clusters, since they are limited to find only disjoint or, in the case of [8], nearly disjoint clusters. Even more serious is the ignorance of the obfuscation pro-voked by highly overlapping clusters in different subspaces, causing most approaches to fail in detecting the true corre-lation clusters. For completeness, we review existing correla-tion clustering methods and discuss further limitations. We will restrict to linear correlation clustering in the following. The basic technique utilized by most approaches is PCA. Agglomerative methods (ORCLUS [5], 4C [11], COPAC [4], ERiC [3]) assume that local neighborhoods are in line with a global trend and perform PCA on those neighborhoods, which, however, are strongly influenced by the similarity in the full-space. Therefore these techniques are not appro-priate for noise or multiple views in the data as both neg-atively influence the choice of the neighborhood. To avoid the problem of defining a proper local neighborhood, divisive methods, e.g., [8], were proposed, which, however, usually re-quire the number of clusters to be specified beforehand. The method of [15] is even restricted to find disjoint lines. In-stead of PCA, the work of [1] utilizes the Hough transform to detect correlations within the attributes. The paradigm of pattern-based clustering [20, 16] such as co-clustering or biclustering is related; though, it is limited to positive corre-lations and to one-dimensional correlations.

Overall, none of the existing correlation clustering meth-ods is able to find multiple overlapping subspace correlation clusters as it is possible with our method.

Subspace correlation analysis , e.g. CARE [21] and REDUS [22], tries to identify subsets of attributes in which the ma-jority of data entries exhibit a correlation. This differs from clustering in that it does not analyze several local subsets of objects. Therefore these methods are restricted to finding only a single correlation per subspace and the datasets have to contain a small degree of noise concerning objects.
Generally speaking, a subspace correlation cluster is a set of objects O  X  DB that exhibits a correlation in a set of dimensions S  X  Dim . We call this set of attributes the sub-space of the cluster. In Fig. 2 for example, the whole set of objects is correlated in the 2-dim. subspace { x, y } ;theob-jects form a line. Considering the 3-dim. space, however, we do not have a valid subspace correlation cluster since the at-tribute z is not correlated to the other ones. Correlation is a measure of the effect of independent variables on a depen-dent variable [12]. As shown in Fig. 2, however, we cannot draw any conclusion about the attribute values in dimen-sion z given the attribute values in the remaining dimen-sions. Thus, the plane does not represent a valid subspace correlation cluster. Unlike existing correlation clustering ap-proaches we will not report such invalid correlation clusters as they encourage highly misleading interpretations. Instead we will use this principle of induced clusters for redundancy modeling and efficiency improvements in Section 4.
In Fig. 3, the illustrated plane consisting of all objects in the 3-dim. space, however, is a valid correlation. Given the attribute values of an object in dimension x and y ,the attribute value in dimension z is dependent. An object in this correlation is described by two independent variables .If we just consider a subset of the objects, we are even able to find a line in the 3-dim. subspace, i.e., given the attribute value in dimension x for example, the attribute values of dimension y and z are completely dependent. For a line we have one independent variable , i.e., one degree of freedom.
As shown in the examples, for subspace correlation clus-ters we have two different types of dimensionalities: First, the subspace dimensionality : e.g., the 3-dim. subspace of { x, y, z } . Second, the cluster dimensionality :Withineach subspace each cluster/correlation has its own intrinsic di-mensionality. E.g., a plane corresponds to a 2-dim. cluster (two degrees of freedom) while a line is a 1-dim. cluster. Thus, in general a subspace correlation cluster is defined by a three-tuple ( O,S, X  ) with a set of objects O  X  DB ,aset of correlated dimensions S  X  Dim , and the clusters dimen-sionality  X  , representing the degrees of freedom. We now formalize the definition of such subspace correlation clusters.
The basic idea to describe correlations in the data is by considering the data X  X  principal components. We adapt the notions of [2] to define our clusters. Since we design our model to detect linear correlations we will use the term  X  X or-relation X  in place of  X  X inear correlation X .
 Definition 1. Basic notions We assume a database DB  X  R d of d -dimensional objects is given. Dim = { 1 ,...,d } is the set of dimensions. With o we denote the projection of an object o  X  DB to the subspace S  X  Dim . Accordingly, O | of objects O  X  DB . The (sample) covariance matrix for O | i.e., for a set of objects O  X  DB in a subset of dimensions S  X  Dim , is denoted by  X  O,S  X  R | S |  X  R | S | . The eigendecom-position of  X  O,S is  X  O,S = V O,S  X  E O,S  X  V T O,S . The eigenvalue matrix E O,S is a diagonal matrix storing the | S | eigenval-ues in decreasing order, i.e., E O,S = diag ( e 1 ,...,e | e 1  X  e 2  X  ...  X  e | S | . The eigenvector matrix V O,S is an orthonormal matrix storing the unit eigenvectors v i corre-sponding to e i . Therefore, the i -th principal component is given by e i  X  v i .
 Given a set of objects O projected to the subspace S , i.e., O
S , the minimum number of principal components needed to retain a significant level  X  of the data X  X  variance corre-sponds to the cluster X  X  intrinsic dimensionality. Considering the line (defined by a subset of the objects) in the 3-dim. subspace in Fig. 3, we just need one principal component to describe most of the variance. This is indicated by one large eigenvalue and two small ones. In contrast, for the plane we need two principal components. Thus, by using the eigenval-ues of the covariance matrix, we are able to determine the cluster dimensionality.
 Definition 2. Cluster dimensionality The cluster dimensionality of the (projected) set of objects O
S w.r.t. a significance level  X  is using the entries e i of the eigenvalue matrix E O,S .
In real world scenarios we cannot expect to observe a set of objects that perfectly fits a line or another linear regres-sion model. Due to errors and noise in the data, the ob-jects slightly deviate from the perfect model. We can mea-sure the strength of a correlation by the degree of variation not explained by the regression model. Since the regression model corresponds to the hyperplane spanned by the first  X  ( O,S ) principal components with the largest eigenvalues (strong principal components), the distance along the last |
S | X   X  ( O,S ) principal components with the smallest eigen-values (weak principal components) determines the strength of a correlation. Intuitively, this corresponds to the distance between an object and, e.g., the perfect line. The smaller these distances, the stronger the correlation. Formally, the correlation distance can be computed by determining the Eu-clidean distance after projecting the objects and the cluster mean onto the weak principal components (please note: the objects are first projected to the subspace S , i.e., the weak components are linear combinations of the dimensions in S ): Definition 3. Subspace correlation distance The subspace correlation distance of an object p  X  DB to a correlation defined by a set of objects O in subspace S is: scdist ( p, O, S )= ( p | S  X   X  | S ) T  X  V O,S  X   X  E  X  V T with mean vector  X  of the objects in O , V O,S is the eigenvec-tor matrix, and  X  E is a diagonal matrix where the first  X  ( O,S ) entries are 0 and the remaining | S | X   X  ( O,S ) entries are 1 .
Thus, we obtain a strong correlation if each object of the cluster has a small subspace correlation distance. Using these basic ideas, we first introduce our novel cluster definition and afterwards we highlight the important characteristics. Definition 4. Subspace correlation cluster A subspace correlation cluster C =( O,S, X  ) is a set of objects O  X  DB , a set of dimensions S  X  Dim ,anditscluster dimensionality  X  =  X  ( O,S ) , such that 1. the cluster is sufficiently large, i.e., | O | X  minSize 2. the subspace correlation distance is small for each object 3. the set of objects is maximal: any object not in the 4. the cluster dimensionality is smaller than the subspace 5. uncorrelated dimensions are not included, i.e.,  X  d  X  S : Fig. 2: Valid correlation in sub-space { x, y } , invalid in { x, y, z }
The first property ensures that each cluster has sufficiently large support since, e.g., a line composed by only two points is not interesting. By the second property, we ensure that each cluster exhibits a strong correlation; we do not include objects with large subspace correlation distances. Accord-ingly, we include any object with a small distance, i.e., the cluster should be as large as possible (third property). The last two properties are the most important ones: With Prop. 4 the case  X  = | S | is prohibited (  X &gt; | S | is not possi-ble by definition). For  X  = | S | , the cluster dimensionality and the subspace dimensionality would be identical. This is not meaningful since each dimension in the current sub-space would be an independent variable. Thus, none of the attribute values depends on the others, which does not rep-resent a correlation.

Property 5 is even more important to avoid  X  X eaningless X  correlations (and is one large advantage in contrast to tradi-tional correlation clustering). As shown in Fig. 2, the plane is not a valid correlation; dimension z is not correlated to the other ones. This, however, cannot be detected by considering just the eigenvalues. The cluster dimensionality is 2 as for any other plane. Thus, the first 4 properties of the previous definition hold. However, we can detect this invalid correla-tion by the following principle: Lets assume we have detected a set of objects with dimensionality  X  in subspace S \{ d adding a further dimension, i.e., to get the subspace S ,the novel cluster dimensionality obviously has to be either  X  +1 or  X  . The first case occurs if we add an uncorrelated/noisy dimension to S \{ d } , e.g., a line in a 2-dim. subspace becomes a plane in a 3-dim. subspace (cf. Fig. 2). The latter case oc-curs if we add a correlated dimension, e.g., a line still is a line (Fig. 3). Thus, Property 5 checks for each lower-dimensional subspace S \{ d } whether the cluster dimensionality remains stable. Otherwise (  X  ( O,S \{ d } ) &lt; X  ( O,S )), at least one di-mension d would be uncorrelated and the cluster is not valid.
Overall, our cluster definition enables us to detect groups of objects O that are correlated in the set of dimension S with a specific degree of freedom  X  by simultaneously excluding uncorrelated dimensions. Note that the smaller  X  the more interesting is a cluster; a line is better than a plane. Contrar-ily, the larger S the more interesting is a cluster; a 3-dim. subspace is better than a 2-dim. subspace.
Based on Def. 4 we are able to determine all valid clusters, which are allowed to group diverse object sets in diverse sub-spaces. Thus, we allow overlapping clusters in general and we are able to detect multiple different correlations per object, e.g., due to different attribute subsets. We are not limited to disjoint clusters as previous methods. However, simply using the set All of all valid subspace correlation clusters ac-cording to Def. 4 would lead to an overwhelming result size, containing highly redundant information. This redundancy occurs due to two reasons:
Collinearity: When one dimension d  X  S is highly cor-related with some other dimension x , the remaining dimen-sions S \{ d } will be correlated to x too. This phenomenon is called (multi)collinearity [13]. When this happens, we can derive many clusters describing the same information, e.g., if { infer that { d 1 ,d 3 } , { d 1 ,d 4 } , ... are correlated as well. In-stead of reporting several collinear correlations, we represent them by just one cluster (in a subspace of higher cardinality) and thus avoid redundancy. To handle collinearity, we have to ensure that a correlation cluster in subspace S is not sim-ply a projection of another cluster in a supersubspace S  X  As illustrated in Fig. 4, the correlation cluster in subspace { x, y } is a projection of two clusters in subspace { x, y, z green and red clusters) and a few noise objects (black dots). Such a cluster does not provide much additional insight and therefore should be excluded from the final result.
Induced clusters: With a few extra points, a correlation cluster can induce a cluster of higher dimensionality (e.g., a line induces a plane). In Fig. 3, the 1-dim. correlation clus-ter (blue line) induces the 2-dim. correlation cluster (plane) which has only a few extra objects. Induced clusters can be misleading as they mainly take credit for another cluster X  X  objects. In this example, the plane is mainly supported by the objects of the line. If we took the line X  X  objects out of consideration, the plane would have too little support to be a valid correlation cluster. The plane is just valid because of the line but not by itself. Thus, we should exclude induced clusters from the final result.

The  X  X otentially redundant X  rule. The main question is, which clusters could lead to a redundancy of other clusters either due to collinearity or due to induction. The following definition clearly states this: Definition 5.  X  X otentially redundant X  rule A subspace correlation cluster C =( O,S, X  ) is potentially redundant to C =( O ,S , X  ) , for short C  X  pot red C ,iff  X   X   X   X | S \ S | X  X  X  S  X   X  S : | S  X  | =  X  +1  X   X   X   X  &gt; | S We continue by discussing why the above rule covers all clus-ters C to which C is potentially redundant to. We distin-guish three cases: (1)  X &lt; X  (the cluster dimensionality of C is lower (better) than the one of C ): In this case, the rule is always evaluated to false as the left sides of both inequalities are negative while their right sides are at least 0. This means a lower dimensional cluster (e.g., a line) is never redundant w.r.t. a higher dimensional cluster (e.g., a plane). This is desirable because high-dim. clusters cannot induce low-dim. ones. (2)  X  =  X  (both clusters have the same dimensionality): In this case, the rule will be true if and only if S  X  S .Other-wise the right side of the first inequality will be greater than 0, and the second part of the rule is violated in any case since a strict &gt; is required. This means a cluster in a higher-dimensional subspace is more important than its projections. As we can derive clusters in lower-dimensional subspaces by simply projecting the cluster of a higher-dimensional sub-space, clusters in lower-dimensional subspaces need not to be presented explicitly (cf. collinearity). (3)  X &gt; X  (the cluster dimensionality of C is higher (worse) than the ones of C ): This case is the most complex one and corresponds to induced correlation clusters, e.g., C is a plane while C is a line. As before, the rule is true for the case S  X  S . Though, it can even hold for S  X  S .

Let us consider an example: C is a 4-dim. cluster in sub-space S = { 1 , 2 , 3 , 4 , 5 } and C is a 1d-cluster (line) in sub-space S = { 1 , 2 , 3 , 8 , 9 } .Howdoes C look like in S ?Ifwe project C to { 1 , 2 , 3 } = S  X  S , it has to be still a line. By now adding two further dimensions, e.g., { 4 , 5 } to reach S , the dimensionality of the 1d-cluster increases at most by two (if both dimensions are uncorrelated). Thus, in the worst case the dimensionality of C in S is 1+2=3, which is better than the 4d-cluster C .Hence C could potentially induce C . In general, this holds if  X  + | S | X  X  S  X  S | &lt; X   X  X  S \
But there is more: Assume C to be a 4-dim. cluster in subspace S = { 1 , 2 ,..., 7 } . Obviously, the line C does look like a 5-dim. cluster in S . It cannot induce the cluster C . However, in our model the cluster C represents all of its collinear projections, e.g., also the 4-dim. cluster in subspace { 1 ,..., 5 } = S  X   X  S . As discussed, the cluster in this subspace may be induced by C and hence could be redundant. Conse-quently, if the projection S  X  is redundant then this informa-tion should not be contained in the  X  X arent X  cluster. We also have to denote C as potentially redundant to C . In general, the equation  X   X   X  &gt; | S  X  \ S | has to be checked for any subset S  X   X  S with | S  X  | =  X  +1 since these are the lowest dimensional subspaces represented by collinear information of C .
Overall model. Based on the  X  X otentially redundant X  rule, we define the overall redundancy of a cluster. As discussed and illustrated in Fig. 3, the plane should be discarded as redundant because its support is too small after removing the line. Similarly, in Fig. 4, the line in subspace { x, y } discarded, since the support is mainly due to the two lines in subspace { x, y, z } . Thus, we define a cluster C as redundant w.r.t. a set of other clusters Result if C  X  X  support is too small after removing all objects that are already grouped in clusters C  X  Result to which C is potentially redundant. Definition 6. Redundancy of clusters A subspace correlation cluster C =( O,S, X  ) is redundant to a set of clusters Result , for short C  X  red Result ,iff with Red = { C  X  Result \{ C }| C  X  pot red C }
Finally, we define the overall subspace correlation cluster-ing. The final clustering should be redundancy free, i.e., it should not contain induced clusters or clusters present due to collinearity. Moreover, it should be maximal, i.e., should contain as many clusters as possible without introducing re-dundancy. Based on these two principles, we define the over-all clustering model as follows: Definition 7. Subspace correlation clustering Given the set All of all valid subspace correlation clusters, a subspace correlation clustering Result  X  All fulfills the fol-lowing conditions:
Our novel clustering model enables the detection of corre-lations in subspace projections of the data, it allows objects to contribute to multiple, overlapping correlations, and si-multaneously prevents redundant information in the result. In the following section we briefly introduce our algorithm SSCC, which determines a clustering result according to Def-inition 7. We refer to an efficient approximation since, as known from traditional subspace clustering, the number of cluster candidates is exponential in the number of objects and the number of dimensions. Furthermore, generating all cluster candidates in a first step and selecting the final clus-tering afterwards is highly inefficient since most of the clus-ters will be rejected as redundant anyway. Thus, our SSCC avoids the bottleneck of generating all possible clusters by trying to directly generate only non-redundant clusters.
The general processing of SSCC is shown in Algo. 1. We first describe the general idea of our approach. Three major principles are used to avoid generating redundant clusters: (1) Based on the redundancy definition, a  X  -dim. cluster can only be redundant to clusters with dimensionality  X   X   X  . Thus, we can first mine all (non-redundant) low-dimensional clusters (e.g., lines) before mining higher-dimensional ones (e.g., planes). This is shown in line 2 of the algorithm. (2) The support of a non-redundant  X  -dim. cluster in subspace S has to be high enough after removing the objects con-tained in clusters that make him potentially redundant (cf. Def. 6). We use this idea for pruning objects: Since we al-ready know the non-redundant  X  -dim. clusters with  X   X   X  in similar subspaces, we remove all objects of the database (in this subspace) that are already clustered (line 5). Thus, the set of objects to be considered is dramatically reduced (or even too small; line 6) and finding clusters is more efficient. (3) We exploit the fact of collinearity to avoid analyzing the exponential number of possible subspaces. Each  X  -dim. clus-ter in subspace S is also a  X  -dim. cluster in any subspace S  X  S with | S | =  X  +1. Thus, we only mine  X  -dim. clusters in subspaces with cardinality  X  +1 (cf. lines 4, 7) and we merge these clusters to obtain clusters with higher subspace cardi-nality (line 8). For example, if we have already found similar clusters in subspace { d 1 ,d 2 } and { d 1 ,d 3 } , we merge them to { d 1 ,d 2 ,d 3 } . If the cluster is still valid here, we do not have to analyze { d 2 ,d 3 } since in our model the collinearly correlated dimensions are represented by a single cluster.
Based on these strategies, the lines 3-10 of Algo. 1 effi-ciently generate a set of  X  -dim. clusters (located in subspaces of arbitrarily high cardinality) that is non-redundant with high probability. To finally guarantee a non-redundant re-sult, this set is refined (lines 11-14): To remove redundant clusters, we can use an efficient incremental approach since based on Def. 5 we can process the clusters of highest sub-space cardinality first (line 11,12) and we just have to com-pare these against the current result set (line 13). Finding  X  -dim. clusters in a (  X  +1) -dim. subspace. Asdescribedabove,wejusthavetominethe  X  -dim. clus-ters in (  X  + 1)-dim. subspaces. To achieve this, we apply the method of COPAC [4] on the current subspace projec-tion; however, with two important differences: First, COPAC partitions the objects according to their local correlation di-mensionality and finds clusters in any of these partitions. Since we are just interested in  X  -dim. clusters, we just have to analyze a single partition; the one corresponding to  X  . This is far more efficient. Note that we still find clusters of higher dimensionality due to our overall processing. Second, our approach avoids uncorrelated dimensions, i.e., clusters containing principal components that are nearly parallel to any axis of the current subspace are rejected. Overall, we efficiently generate the desired set of  X  -dim. clusters. Merging clusters to higher-dimensional subspaces.
 By our merging principle we avoid to analyze any possible subspace projection. Given the set Tmp of newly detected  X  -dim. clusters (cf. line 7), we try to merge these with the already known  X Clusters to reach subspaces of higher cardi-nality. A pseudo-code for this subroutine is given in Algo. 2. For each cluster C =( O,S, X  )  X  Tmp we first determine those C i =( O i ,S i , X  )  X   X Clusters that fulfill | S  X  and S i  X  S . Only these clusters are potentially collinear to C .Ifnosuch C i exists, we can simply add C to the current set of  X Clusters . Otherwise, for each potentially collinear cluster C i we do the following steps:
We generate the candidate C merge =( O  X  O i ,S  X  S i , X  )and check if it is a valid cluster. If so, we first add C merge since it potentially can be merged with further clusters later on (Note: Tmp acts as a queue, where we successively re-move and add elements; the method stops if Tmp is empty). Second, we check whether C i is redundant to C merge .Since the  X  X otentially redundant X  rule is automatically fulfilled, we simply have to test | O i | X  X  O  X  O i | &lt;minSize .If C dant, we remove it from  X Clusters , ensuring a manageable number of clusters at any time. Similarly, we check the re-dundancy of C w.r.t. C merge . If it is redundant, we mark C .
If each C i is processed, i.e., its merging with C has been analyzed, we remove C from Tmp .If C is not marked as redundant, we finally add C to the set  X Clusters .
Note that the termination of the merging principle is guar-anteed due to the condition S i  X  S . Since the subspace car-dinality of the merged clusters increases, at some point in
Algorithm 2: Generate high-dim. subspaces by merging time no further merging partners can be found and the set Tmp will become empty. Also note that the merging is in-voked within the for-loop (Alg. 1, line 4). Thus, the set Tmp is usually small and the clusters in  X Clusters may already be of much higher subspace cardinality than  X  +1. Over-all, our merging principle efficiently generates  X  -dimensional clusters of subspace cardinality larger than  X  +1.
Object pruning exploiting the redundancy model. Our pruning lowers the number of objects of the current subspace S that have to be analyzed for clustering structure. Accord-ing to Def. 6, a non-redundant cluster C S of dim.  X  must have sufficiently high support even if the object sets of some other clusters (based on Def. 5) are removed. Since we already know the non-redundant clusters with dimensionality  X  &lt; X  , we can select those clusters C  X  Result that fulfill the  X  X o-tentially redundant X  rule w.r.t. the current subspace S , i.e., those clusters C for which C S  X  pot red C holds 3 . Clusters ful-filling the rule might be the reason for induced correlation clusters in S and hence their object sets can safely be re-moved. If C S is a valid non-redundant cluster in S , it will still be so even after removing the objects determined above.
But we can remove even more objects: The merging step generates clusters with subspaces of cardinality larger than  X  +1. For example, based on clusters in { d 1 ,d 2 } and { we may get a cluster C m in { d 1 ,d 2 ,d 3 } ; it has the same clus-ter dimensionality  X  but higher subspace cardinality. Thus, before analyzing the subspace { d 2 ,d 3 } we can also remove the objects of C m since the  X  X otentially redundant X  rule holds for this cluster. In general, we can prune objects contained in clusters C  X   X Clusters with C S  X  pot red C .Thisprevents the detection of redundant collinear clusters with lower sub-space cardinality. Hence, our merging principle introduced above also leads to an efficiency gain within this subroutine. Formally, the set of non-pruned objects is given as NonPruned S = { o  X  DB | X  X  C  X  Result  X   X Clusters :
Please note that dependent on the current subspace S dif-ferent sets of objects are pruned since the  X  X otentially re-dundant X  rule may be evaluated differently. Thus, objects can still contribute to several correlations due to different subspace projections as desired by our model.

Overall, for each subspace S a large amount of objects might be already removed based on the clusters in Result (to prevent induced clusters) and  X Clusters (to prevent collinear clusters). Only in the remaining set of objects novel clusters have to be detected.

Summary. SSCC generates clusters bottom-up w.r.t. their dimensionality and simultaneously removes redundant clus-ters top-down w.r.t. their subspace cardinality. Due to the redundancy model and based on the increasing set of known clusters, we are able to prune a large amount of objects that needs not to be considered at all. By utilizing the collinear-ity phenomenon and merging several clusters, we avoid ana-lyzing many subspace projections. Overall, SSCC efficiently determines a non-redundant subspace correlation clustering.
Setup. We compare SSCC against all (non-hierarchical) algorithms implemented in the framework ELKI 4 ,namely ORCLUS [5], COPAC [4], and 4C [11]. For subspace correla-tion analysis we choose CARE [21] since, unlike REDUS [22], it is exclusively designed for linear correlations. To revisit our statement of the introduction that simple post-processing is not sufficient in most subspace scenarios to reveal the cluster-ing structure, we implement a post-processing method that discards all dimensions being approximately parallel to the principal components of a cluster and recalculates the clus-ter X  X  dimensionality. We name this step PP and apply it to all results of full-space correlation clustering methods. We measure clustering quality by the CE measure (clustering error) [19], which also considers the subspaces in its evalu-ation. For easier interpretation we depict the results of 1-CE, where 1 indicates perfect quality and 0 lowest possible quality. For each algorithm we determine optimal parameter settings w.r.t. the CE value. Efficiency is measured by the approaches X  runtime. For comparability all experiments were conducted on Opteron 2.3GHz CPUs using Java6 64bit.
We start by evaluating the approaches based on synthetic data to analyze their performance for the different subspace scenarios. We continue with different scalability experiments and will confirm important observations for the real world datasets  X  X ages X  5 and  X  X mage Segmentation X  6 .

Test scenarios for subspace clustering. For Figures 5-10 we examine the algorithms X  results for different correla-tion scenarios, especially for subspace settings. Due to space limitations we provide only visual descriptions of the used datasets attached to the left of the evaluation results of each particular test scenario. Each colored region corresponds to one cluster covering a specific set of objects in a specific sub-set of dimensions.  X  indicates the cluster X  X  dimensionality. Attribute values in white regions are noise.

For the first simple test scenario A (Fig. 5), with only full-space clusters, the full-space approaches COPAC and OR-CLUS perform better than SSCC. Since clusters are well separated in the full-space but are likely to be merged in subspace projections, the merging strategy of SSCC tends to assign few objects to wrong clusters. Surprisingly, 4C does not yield good results, which probably is originated by its sensitivity to the setting of the neighborhood range, which however might be different for clusters of different dimen-sionality  X  , and its requirement of spatially connectedness of the clusters. Since the CARE approach requires a clus-ter to comprise the majority of a dataset X  X  objects, it does not perform well for datasets with more than one cluster per subspace, which will be confirmed by the other test cases.
In this scenario, we also examine the importance of re-dundancy handling when considering subspace projections: to this aim, we applied COPAC on each subspace projec-tion. It generated 52 clusters, instead of the four hidden clusters. This matches the observation that each cluster C =( O,S, X  ) appears as projection in each subspace S  X  S with | S | X  max { 2 , X  } , yielding for this data to the theoret-ical result of 49 redundant clusters. Clearly, results obfus-cated with redundancy would be unmanageable for higher dimensional data; naively using existing correlation cluster-ing in subspace projections is not a choice to detect subspace correlation clusters. In contrast, SSCC detects only the four non-redundant clusters.

Test case B (Fig. 6) shows a simple subspace scenario, where clusters are disjoint and only few dimensions per clus-ter are noisy. SSCC manages to achieve even better cluster-ing results for this setting. The results of COPAC and OR-CLUS are significantly lower compared to scenario A. Noise dimensions obfuscate the clustering structure in the full-space and even post-processing only marginally improves the qual-ity. In contrast, CARE is able to achieve better results than for test case A, as clusters overlap less per dimension.
Although scenario C (Fig. 7) seems to be the easiest sub-space setting, as neither dimensions nor objects of clusters do overlap, it poses severe challenges for full-space algorithms. Since the degree of noise dimensions exceeds the one of rele-vant dimensions for all clusters, COPAC, ORCLUS, and 4C do not manage to reveal the true clustering structure. Even
Fig. 11: Effect of noise dimensions post-processing achieves no further improvement. SSCC is the only approach being able to uncover the correlations. For the clusters of scenario D (Fig. 8) not objects but subspaces are disjoint. As all approaches besides SSCC partition the data, they will not recover the clustering structure by de-sign. However, since the clusters X  overlap is arbitrary, the correlations interfere with each other and no clear statement about correlations is possible in the full space. Still, SSCC nearly perfectly recovers the hidden correlations. Although, the degree of overlap of clusters w.r.t objects is increased in scenario E (Fig. 9) compared to scenario D, clustering results of COPAC and ORCLUS are better. Clusters either agree perfectly or not at all regarding their objects or dimensions. Therefore the projection into full-space still enables the de-tection of a certain clustering structure. Thus not only the partitioning of the data hinders the cluster detection for full-space algorithms but also the arbitrary interference shown in scenario D. The last test case F (Fig. 10) shows a sce-nario typically described for subspace clustering. For these combined characteristics of the previous cases, we once more observe that none of the examined approaches but SSCC is able to reveal the underlying correlation clustering structure. For further experiments we will restrict our comparison of SSCC to COPAC and ORCLUS, as the remaining methods have shown to be not effective to typical subspace scenarios.
Number of noise dimensions. To confirm our obser-vations of scenario B, in Fig. 11 we gradually add noise di-mensions to the dataset of Fig. 5. While SSCC is mostly unaffected by noise dimensions, COPAC and ORCLUS in-creasingly fail to discover the hidden correlations. For few noise dimensions, post-processing slightly increases the qual-ity of COPAC. In general, however, post-processing cannot eliminate the problems of full-space clustering.

Overlap of clusters. In Fig. 12 we increase the percent-age of overlapping objects between clusters. For a 4-dim. dataset with 1000 objects, 2 clusters each with 500 objects in disjoint 2-dim. subspaces, we increase the clusters X  overlap without varying the clusters X  sizes. SSCC shows perfect re-sult in any case. ORCLUS and COPAC with post-proc. are able to reveal the true correlations for small overlap. How-ever, with increasing overlap, both algorithms are destructed by the interference of the two clusters in the full-space, shown by worse post-proc. quality. Even worse, COPAC groups the overlapping objects into a separate cluster, which is more beneficial according to CE than to regard them as noise.
Number of views. In Fig. 13 we increase the number of views for a dataset of 1000 objects and 10 dimensions. Com-parably to scenario E (Fig. 9), all views have disjoint, nearly equally sized subspaces and contain two 1-dim. correlation clusters. We observe a fast decrease of clustering quality for COPAC and ORCLUS with increasing number of views. SSCC constantly gets high quality results.

Scalability w.r.t. database size. Fig. 14 (left) shows the results for a varying number of objects in a dataset with 5 dimensions and 4 equally sized full space clusters without noise. All three algorithms scale linearly with the database size. Note the logarithmic scale of both axes. Although SSCC has to cope with an exponential number of subspaces, its runtime is still in range of COPAC X  X  runtime.

Scalability w.r.t. dimensionality. For a dataset similar to scenario E (Fig. 9), except that all clusters comprise 500 objects, we consecutively concatenate the dataset to generate datasets of higher dimensionality in Fig. 14 (right). Although the number of subspaces grows exponentially with the num-ber of dimensions, SSCC scales linearly. COPAC shows super linear behavior and thus exceeds SSCC X  X  runtime. We again applied COPAC to any subspace projection to evaluate the redundancy and efficiency challenge. Already for the 10-dim. dataset COPAC needed over 4 hours and reported 1013 (re-dundant) clusters. Thus, removing redundancy, as done by SSCC, is also important for the efficiency.

Real world data. Due to space limitations, we only refer to the results of SSCC and COPAC in the following. For Wages (534 objects, 4 numerical attributes, 7 categories) we only use the four numerical attributes. Both algorithms only detected a single 2-dim. cluster, which is visualized in Fig. 15. All clustered objects are colored red, noise is colored blue. For the 2-dim. correlation in the Wages data, SSCC ( = 0 . 002,  X  =0 . 85, minSize = 80) captures the objects much better than COPAC. COPAC misses many objects since in the full-space they do not belong to this correlation.
For the Image Segmentation data we have 19 numerical at-tributes describing pixel regions and one class attribute. We removed the class attribute and the constant region-pixel-count attribute. Although, trying a wide range of parameter settings for COPAC, it was not able to detect any cluster for this dataset. For SSCC ( =0 . 002,  X  =0 . 85, minSize = 50) two exemplary clusters out of the eight found ones are plot-ted in Fig. 16. Clearly, the clusters detected by SSCC corre-sp ond to strong correlations, which can be visually verified and are explainable from the dataset X  X  description: the first two dimensions are the measures of excess green and excess blue that are defined as exgreen.mean =(2 G +( R + B )) and exblue.mean =(2 B +( R + G )). Here R denotes the average
Fig. 14: Scalability: database size &amp; dimensionality
Fig. 15: Results of SSCC and COPAC for Wages over the raw values of red, B for blue, and G for green. The dimension value.mean is a 3-dim. non-linear transformation of the values R , G ,and B . Obviously, from the definition of these dimensions we can expect some correlation in this sub-space. This correlation is successfully identified by SSCC. Note that these correlations appear only for subsets of the attributes. Thus, full-space approaches are not able to de-tect these results, confirming the need for our novel subspace correlation clustering paradigm.
In this work, we have demonstrated that the observations of traditional subspace clustering analogously apply for the paradigm of correlation clustering. A simple post-processing to refine the clustering result determined in the full-space is not sufficient for typical subspace scenarios. Instead we have to analyze subspace projections of the data to find meaning-ful strong correlations supported by subsets of objects .Our introduced approach reveals linear correlations in subspace projections, allows objects to contribute to multiple correla-tions, and simultaneously ensures a result of manageable size containing only non-redundant subspace correlation clusters. For this, we carefully differentiate between non-redundant correlation clusters and ones originated due to collinearity or induction. We designed the efficient algorithm SSCC exploit-ing various pruning techniques. The experiments demon-strate that transferring ideas from the subspace clustering paradigm leads to more precise correlation clustering results compared to state of the art techniques in this domain.
As future work, we will extend our method to also handle non-linear correlations.
 Acknowledgment. This work has been partly funded by the DFG grant SE1039/6-1 and by the UMIC Research Cen-tre, RWTH Aachen University, Germany. [1] E. Achtert, C. B  X  ohm, J. David, P. Kr  X  oger, and [2] E. Achtert, C. B  X  ohm, H.-P. Kriegel, P. Kr  X  oger, and [3] E. Achtert, C. B  X  ohm, H.-P. Kriegel, P. Kr  X  oger, and [4] E. Achtert, C. B  X  ohm, H.-P. Kriegel, P. Kr  X  oger, and [5] C. C. Aggarwal and P. S. Yu. Finding generalized [6] R. Agrawal, J. Gehrke, D. Gunopulos, and [7] I. Assent, R. Krieger, E. M  X  uller, and T. Seidl. Inscy: [8] M. S. Aziz and C. K. Reddy. A robust seedless [9] N. Bansal, A. Blum, and S. Chawla. Correlation [10] K. S. Beyer, J. Goldstein, R. Ramakrishnan, and [11] C. B  X  ohm, K. Kailing, P. Kr  X  oger, and A. Zimek. [12] J. Cohen, P. Cohen, S. G. West, and L. S. Aiken. [13] R. Freund, W. Wilson, and P. Sa. Regression analysis: [14] S. G  X  unnemann, E. M  X  uller, I. F  X  arber, and T. Seidl. [15] R. Harpaz and R. M. Haralick. Mining subspace [16] H.-P. Kriegel, P. Kr  X  oger, and A. Zimek. Clustering [17] P. Kr  X  oger, H.-P. Kriegel, and K. Kailing. [18] D. Niu, J. G. Dy, and M. I. Jordan. Multiple [19] A. Patrikainen and M. Meila. Comparing subspace [20] J. Yang, W. Wang, H. Wang, and P. S. Yu. [21] X. Zhang, F. Pan, and W. Wang. Care: Finding local [22] X. Zhang, F. Pan, and W. Wang. Redus: finding
