 Major strides have been made in understanding the detailed d ynamics of decision making in sim-combination of probabilistic and dynamic programming tool s, it has been shown that when the de-Under similar experimental conditions, it appears that hum ans and animals accumulate information and make perceptual decisions in a manner close to this optim al strategy [2 X 4], and that neurons algorithm [6]. However, in most 2AFC experiments, as well as in more natural behavior, the de-to the observer X  X  internal timer, the deadline can be viewed as a stochastic quantity. numerical simulations to examine the optimal policy in some specific examples (Sec. 3). , all the inputs for the trial are generated from a probabilit y density f the pressure of a stochastic decision deadline.
 with exceeding the deadline, where both c and d are normalized against the (unit) cost of making guessing at  X  . This avoids situations in which we prefer to exceed the dead line. A decision-policy  X  is a sequence of mappings, one for each time t , from the observations so far pling. We define  X   X  , and  X  and  X   X   X  , inf { t  X  N :  X  t ( x t )  X  { 0 , 1 }} pling if the deadline were to fail to occur. Then  X  Our loss function is defined to be l (  X ,  X  ;  X , D ) = 1 find a decision-policy  X  which minimizes the total expected loss 2.1 Dynamic Programming  X  p C other over time and eventually meet.
 We will use tools from dynamic programming to analyze this pr oblem. Our approach is illustrated p p (Lemma 4), the continuation region is an interval delimited by where the costs of continuing and is always larger than that of continuing one more for a given a mount of belief (Lemmas 2 and 3), that  X  X indow X  of continuation narrows over time (Main Theor em). This method of proof parallels that of optimality for the classic sequential probability r atio test in [10]. Before proving the lemmas and the theorem, we first introduce some additional definitions. The value function V : N  X  [0 , 1] 7 X  R cumulative evidence for  X  = 1 is p t : V ( t, p t ) , inf the form time t , known as the Q-factor for stopping and denoted by Q , is easily computed as where the infimum is obtained by choosing  X  =0 if p t  X  . 5 and choosing  X  =1 otherwise. stopping, and to choose  X  = 0 or  X  = 1 to minimize the probability of error given the accumulated evidence (see [10]). That is,  X   X  = inf { t  X  0 : Q ( t, p t )  X  Q ( t, p t ) } and  X   X  = 1  X  terms of Q ( t, p ) and Q ( t, p ) , computing Q ( t, p ) is difficult in general. Lemma 1. The function p 7 X  Q ( t, p t ) is concave with respect to p t for each t  X  N . Proof. We may restrict the infimum in Eq. 3 to be over only those  X  and  X  depeding on D and the future observations x x by different  X  and  X  for different values of p t but removing explicit dependence on p t from the of policies, we note that the distribution of the future obse rvations x and so we have h l (  X ,  X  ;  X , D ) |  X , p t i values of  X  , we may then write: h l (  X ,  X  ;  X , D ) | p t i  X ,D, Eq. (3) can then be rewritten as: where this infimum is again understood to be taken over this se t of policies depending only upon p D given that the deadline has distribution D , and denote  X  t V ( t, p )  X  V ( t, p ) and Q  X  ( t, p )  X  Q ( t, p ) for all t and p .
 assumption that V  X  ( t + 1 , p  X  )  X  V ( t + 1 , p  X  ) for all p  X  .
 Now consider a finite horizon version of the problem where  X   X  is only optimal among stopping the infinite horizon version of the problem follows by taking the limit as T  X  X  X  . Q at V ( t, p ) = Q ( t, p )  X  Q  X  ( t, p ) = V  X  ( t, p ) by the induction hypothesis. grows as time moves forward, the continuation region may exp and with time rather than contract. as large as that of making an error.
 rapidly.
 Lemma 3. For each t  X  N and p  X  (0 , 1) , Q ( t  X  1 , p t  X  1 = p )  X  Q ( t, p t = p )  X  c . over which the infimum defining Q ( t  X  1 , p ) is taken, joint distribution of ( p s ) of Q ( t, p ) .
 Lemma 4. For t  X  N , Q ( t, 0) = Q ( t, 1) = c ( t + 1) + d P { D = t + 1 | D &gt; t } .  X  = t +1 ,  X   X  =0 . Thus, Q ( t, 0) becomes is  X   X  = t +1 ,  X   X  =1 . Thus, Q ( t, 1) = c ( t +1) + d P { D  X  t + 1 | D &gt; t } . We are now ready for the main theorem, which shows that C t is either empty or an interval, and in p (Lemma 1) whose value at the endpoints p = 0 , 1 are greater than the corresponding values of c and Q ( t, p ) is at least as large as the adjustment c , which we have done in Lemma 3. Theorem. At each time t  X  N , the optimal continuation region C t is either empty or a closed interval, and C t +1  X  C t .
 follows trivially, so consider the case when C t +1 6 =  X  . Choose p  X  C t +1 . Then a Similarly, b t &lt; 1 . Thus, a t , b t  X  (0 , 1) .
 Q ( a t  X   X  ) &gt; Q ( a t  X   X  ) . This implies together with Q ( t, a t ) = Q ( t, a t ) that the continuation region must eventually narrow to nothing.
 Proposition. If P { D &lt;  X  X  = 1 then there exists a T &lt;  X  such that C T =  X  . Proof. First consider the case when D is bounded, so P { D  X  T + 1 } = 1 for some time T &lt;  X  . Q ( T, p T )  X  Q ( T, p T )  X  d + c  X  1 / 2 &gt; 0 , and C T =  X  .
 We conducted a series of simulations in which we computed the continuation region and distribu-d , and for the distribution of the deadline D . We chose the observation x variable under both f our simulations we chose q istic deadline fixed to some known constant; and second for a g amma distributed deadline. The where  X ( ) is the gamma function. The parameters k and  X  , called the shape and rate parameters tribution since the gamma distribution has mean k/ X  and variance k/ X  2 . A fixed deadline T may such that k/ X  = T is fixed.
 We used the table-look-up form of the backward dynamic progr amming algorithm (see, e.g., [11]) to compute the optimal Q-factors. We obtained approximatio ns of the value function and Q-factors functions and Q-factors for previous times recursively acc ording to Bellman X  X  equation: p Q ( t, p t ) = ( c ( t +1)+ d ) P { D  X  t +1 | D &gt; t } + P { D &gt; t +1 | D &gt; t } [ of q due to the discrete Bernoulli process.
 off nearly exponentially. class of deadline distributions (including gamma, normal, exponential, delta), we showed that the when the deadline cost is smaller.
 D gamma distribution, the normal distribution, and the unifo rm distribution on an interval. Finally, a gamma distributed random variable with mean  X  may be written as the sum of k =  X  X  independent exponential random variables with mean 1 / X  , so if the brain were able to construct an exponential-distributed timer whose mean 1 / X  were on the order of milliseconds, then it could dynamics on the order of milliseconds.
 This work makes several interesting empirical predictions . Subjects who have more internal un-relative costs of error, time, and deadline.
 Acknowledgments We thank Jonathan Cohen, Savas Dayanik, Philip Holmes, and W arren Powell for helpful discus-grant AFOSR-FA9550-05-1-0121.

