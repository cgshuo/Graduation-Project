 is to maximize the cumulative reward over all time steps. is arguably the most important issue for designing algorithms of CBP. -Greedy [ 3 ] and Linear Upper Confidence Bound (LinUCB) [ 10 ] are two representative algorithms for CBP. -Greedy learns one model per action for many practical applications [ 10 ].
 The traditional CBP setting assumes that the algorithm receives the reward work has carefully studied the CBP under the piled-reward setting. In this paper, we study how LinUCB can be applied under the piled-reward design a novel algorithm, Linear Upper Confidence Bound with Pseudo Reward (LinUCBPR), which is a variant of LinUCB that allows more strategic use of that LinUCBPR results in strong and stable performance in practice. This paper is organized as follows. Section 2 formalizes the CBP with the Sect. 6 .
 to denote the set { 1 , 2 , ..., K } .
 number of rounds and K be the number of actions. In each round t Upon observing the context x t , the algorithm chooses an action a reward r t,a algorithm is to maximize the cumulative reward after T rounds. observes n contexts x t each round. We use t i to denote the i -thstepinround t . For example, 3 the 5-th step in round 3. Upon observing the context x t rithm chooses an action a t the next context x t r for a = a t at time steps { 1 1 , 1 2 , ..., 1 n , 2 1 , 2 2 , ..., t of the piled-reward setting when n =1.
 assume that r t tors u 1 , u 2 , ..., u K  X  R d with u i 2  X  1. That is, E a the cumulative reward is equivalent to minimizing the regret. rithm for the traditional CBP ( n = 1). LinUCB maintains K weight vectors w vectors are calculated by ridge regression time steps before round t and a  X  = a ,and r ( t  X  1) element representing the corresponding reward for each context in X A to ( 1 )is w t When a new context x t action a : the estimated reward  X  r t x the weight vector w t tion is received,  X  r t the action with the highest expected reward. Since no rewards are received before the end of the current round t ,the fixed w t tainty c t beginning of round t as the solution to ( 1 ) with ( X ( t traditional setting replaced by ( X ( t  X  1) ting. In addition, A t LinUCB. If similar contexts come repeatedly in the same round, because w and A t from making the low-reward choice repeatedly before the end of the round. The question is, can we do even better? Our idea is that the contexts x (context, action) gathered at time steps { t 1 ,t 2 ,...,t to learn a more decent model. In other words, we hope to design some semi-algorithm towards more strategic exploration within the round. tion of c t Algorithm 1. LinUCBPR under the piled-reward setting A a ,theterm c t ing c t in the same round.
 each context x t step, before receiving the true reward r  X ,a to be the true reward and allow the algorithm to keep updating the model strategic exploration in our framework. We name the framework Linear Upper Confidence Bound with Pseudo Reward (LinUCBPR). The framework updates the weight vector and the estimated covariance matrix by  X  w t where  X  X t and a  X  = a ,and p t corresponding pseudo reward for each context in  X  X t the framework of LinUCBPR in Algorithm 1 .
 LinUCBPR with estimated reward (LinUCBPR-ER). Another variant is to be Algorithm 2. BaseLinUCB under the piled-reward setting at round t the round, at the cost of more computation. We name the variant LinUCBPR with underestimated reward (LinUCBPR-UR). ensures that the assumption holds.
 define a ( dK )-dimensional vector  X  u to be the concatenation of u and define a ( dK )-dimensional context  X  x  X ,a per action with x 00  X  X  X  0x All  X 
X and  X  w  X ,a . 4.1 Regret for LinUCB Under the Piled-Reward Setting called BaseLinUCB. We first prove the theoretical guarantee of BaseLinUCB. Let  X  c Lemma 1 (Li et al. [ 5 ] , Lemma 1). Suppose the input time step set  X  { 1 1 , 1 2 , ..., ( t text  X  x t variables with means  X  x t probability at least 1  X   X / ( nT ) that  X  x t bound to be related to the rounds, and hence establish Lemmas 2 and 3 . Lemma 2. Let  X  t beasubsetof { t 1 ,t 2 , ..., t n } .Suppose  X  BaseLinUCB. Then, the eigenvalues of  X  A t  X  Proof. The proof can be done by combining Lemmas 2 and 8 in [ 5 ]. Lemma 3. Let  X  t +1 = { t | t  X  [ T ] and  X  j such that t |
 X  Lemma 4. For each s  X  [ S ] ,  X  s T +1  X  5 n  X  2 s (1 +  X  ) dK  X  regret bound of LinUCB under the piled-reward setting.
 Theorem 1. For some  X  = O ( ln( nT K/ X  )) ,withprobability 1 of LinUCB under the piled-reward setting is O ( dn 2 TK ln Proof. Let  X  0 = { 1 1 , 1 2 , ..., T n }\ s  X  [ S ]  X  s T +1 given the previous lemmas and Jensen X  X  inequality, we have Regret = and applying Azuma X  X  inequality, we obtain Theorem 1 .
 original bound multiplied by 4.2 Regret for LinUCBPR-ER Under the Piled-Reward Setting We first prove two lemmas for LinUCBPR-ER.
 Lemma 5. After updating with the context x t  X  r , the estimated reward of LinUCBPR-ER is the same. That is,  X  r Proof. Because p t the reward stays the same.
 Lemma 6. After updating with the context x t  X  r , the uncertainty of LinUCBPR-ER for the context is non-increasing. That is, for any x , x  X  A  X  1 t Proof. By Sherman-Morrison formula, we have x  X  A The second term is greater than or equal to zero, and implies the lemma. reward  X  x t have x  X  A  X  1 t BaseLinUCBPR-ER. Similar to LinUCB, we can then establish the following theorem. The proof is almost identical to Theorem 1 .
 Theorem 2. For some  X  = O ( ln( nT K/ X  )) ,withprobability 1 of LinUCBPR-ER under the piled-reward setting is O ( dn 2 TK ln datasets to evaluate our idea.
 u the context x t reward is generated by r t and consider parameters d  X  X  10 , 30 } , K  X  X  50 , 100 } ,and n LinUCBPR-UR under the piled-reward setting. We also compare Queued Partial delay rewards. Furthermore, we consider an  X  X deal X  LinUCB under the tradi-search, where  X ,  X   X  X  0 . 05 , 0 . 10 , ..., 1 . 00 } and reward setting suffers some penalty when compared with the original bound. Next, we focus on the influence of the pseudo rewards. LinUCBPR-ER and LinUCBPR-UR are consistently better than LinUCB on all datasets. Figures 1 is t/T , when d = 10, K = 100, and n = 500. Note that LinUCBPR algorithms algorithms encourage more strategic exploration by using pseudo rewards. the round based on randomness rather than pseudo rewards. Table 1 suggests tiveness of the strategic exploration. We also compare LinUCBPR algorithms with QPM-D. Table 1 shows that LinUCBPR algorithms are consistently better mented by updating A t Supervised-to-Contextual-Bandit Datasets. Next, we take 8 public multi-setting is the same as the one for artificial datasets.
 tently better than others, and LinUCBPR-UR is competitive with others. The results again confirm that LinUCBPR algorithms are useful under the piled-performance of LinUCB-ER.
 Real-World Datasets. Finally, We use two real-world datasets R6A and R6B we let algorithms maintain a weight vector w t to achieve an unbiased off-line evaluation. For R6B and R6B-clean, we let nT = 10000 for parameter tuning and nT = is the same as the one for artificial datasets.
 middle, which leads to some dropping of CTR.
 LinUCBPR algorithms and QPM-D usually perform better than LinUCB and -Greedy in these datasets. LinUCBPR-ER is stable among the better choices, while LinUCBPR-UR and QPM-D can sometimes be inferior. The results again bound for both LinUCB and the LinUCBPR with estimated reward (-ER), and long term. Most importantly, LinUCBPR-ER yields promising performance on
