 Abstract In the field of constituency parsing, there exist multiple human-labeled treebanks which are built on non-overlapping text samples and follow different annotation standards. Due to the extreme cost of annotating parse trees by human, it is desirable to automatically convert one treebank (called source treebank ) to the standard of another treebank (called target treebank ) which we are interested in. Conversion results can be manually corrected to obtain higher-quality annotations or can be directly used as additional training data for building syntactic parsers. To perform automatic treebank conversion, we divide constituency parses into two separate levels: the part-of-speech (POS) and syntactic structure (bracketing structures and constituent labels), and conduct conversion on these two levels respectively with a feature-based approach. The basic idea of the approach is to encode original annotations in a source treebank as guide features during the con-version process. Experiments on two Chinese treebanks show that our approach can convert POS tags and syntactic structures with the accuracy of 96.6 and 84.8 %, respectively, which are the best reported results on this task.
 Keywords Automatic treebank conversion Feature-based approach Part of speech Constituency syntactic structure 1 Introduction In the field of syntactic parsing, there often exist treebanks that are constructed on non-overlapping text samples and follow different annotation standards. For instance, we can find (at least) three treebanks for Chinese constituency parsing: Penn Chinese Treebank (CTB) (Xue et al. 2005 ), Tsinghua Chinese Treebank (TCT) (Zhou 1996 ), and Peking Chinese Treebank. 1 To obtain more training data for building syntactic parsers, it is desirable to use all the data together. However, such treebanks cannot be simply combined because they follow different annotation standards. To solve this problem, recent years have seen research efforts on automatic treebank conversion which is defined to be the transformation of a treebank (source treebank) to fit a different standard which is exhibited by another treebank (target treebank). For example, we would like to assign CTB-style parse trees to the sentences in TCT because CTB is currently the most widely used Chinese treebank. In this article, we focus on the scenario of conversion between constituency treebanks of distinct annotation standards (Wang et al. 1994 ; Zhu et al. 2011a ).

Since a source treebank to be converted contains human-labeled parses, previous approaches to automatic treebank conversion always utilize annotations in the source treebank to guide treebank conversion (Wang et al. 1994 ; Zhu et al. 2011a ). The motivating idea is illustrated in Fig. 1 which depicts a sentence annotated with standards of TCT (left-side) and CTB (right-side) respectively. Suppose that the conversion is in the direction from the TCT-style parse to the CTB-style parse. The constituents vp:[ /will /surrender], dj:[ /enemy /will /surrender], and np:[ /intelligence /experts] in the TCT-style parse strongly suggest a resulting CTB-style parse also bracket these words as constituents.

One central issue is how to incorporate annotations in a source treebank as guide information. Zhu et al. ( 2011a ) showed the effectiveness of using bracketing structures in a source treebank (source-side bracketing structures in short) as parsing constraints when a target treebank-based parser analyzes the sentences in the source treebank. However, using source-side bracketing structures as parsing constraints is problematic in some cases. As illustrated in the shadow part of Fig. 1 , the TCT-style parse takes  X  X  /deems X  X  as the right boundary of a constituent while in the CTB-style parse,  X  X  /deems X  X  is the left boundary of a constituent. According to the criteria used in Zhu et al. ( 2011a ), any CTB-style constituents with  X  X  /deems X  X  being the left boundary are thought to be inconsistent with the bracketing structure of the TCT-style parse and are pruned. However, if we prune such  X  X  X nconsistent X  X  constituents, the correct conversion result (right side of Fig. 1 ) has no chance to be generated.

The problem comes from boolean distinctions used in the approach of Zhu et al. ( 2011a ). With boolean distinctions, constituents generated by a target treebank-based parser are judged to be either consistent or inconsistent with source-side bracketing structures. That approach prunes inconsistent constituents which instead might be correct conversion results. In this article we propose to use a feature-based approach for treebank conversion where source-side bracketing structures are encoded as a set of features. The advantage is that inconsistent constituents can be scored with a function based on features rather than ruled out as impossible candidates. This article is an extension of Zhu et al. ( 2011b ). In brief, we make the following main extensions in this article.  X  Zhu et al. ( 2011b ) applied the feature-based approach to the conversion of  X  Zhu et al. ( 2011b ) used a classifier-based shift-reduce parser (Sagae and Lavie  X  We evaluate conversion accuracy with a bigger test set which makes experimental In the experiments, we conduct conversion in two directions between TCT and CTB. In the experiments from TCT to CTB, the results show that our approach outperforms the previous work in Zhu et al. ( 2011a ) and reaches the accuracy of 96.6 and 84.8 % for the conversion of POS tags and syntactic structures, respectively. 2 Our approach Similar to the previous work in Zhu et al. ( 2011a ), the task of automatic treebank conversion in this article is divided into two subtasks: POS tag conversion and syntactic structure conversion. Both subtasks are solved with the following approach. Note that we ignore the differences of segmentation standards because the differences are limited (Low et al. 2005 ). 2.1 Generic conversion approach The conversion approach proceeds in three steps. In the following description, the labeler should be replaced with a POS tagger (a syntactic parser) when applying the approach to POS tag (syntactic structure) conversion. In addition, Data s and Data t refer to a portion (or the whole set) of source and target treebanks, respectively.
Step 1: Build a labeler on Data s and then apply the labeler to assign source-style annotations to the sentences in Data t . The sentences in Data t thus contain two types of annotations: auto-labeled source-style and human-labeled target-style annotations. The labeler built on Data s is referred to as source labeler . Step 2: Build a labeler by using the two types of annotations on the sentences of
Data t . Such a labeler is named heterogeneous labeler because it incorporates information derived from source and target treebanks, which, according to the task definition, contain annotations that follow different standards.

Step 3: Apply the resulting heterogeneous labeler to process Data s or any data that contains human-labeled source-style annotations. A heterogeneous labeler takes source-style annotations as input and outputs target-style annotations.
Note that the source and heterogeneous labelers can be based on different models. To instantiate the above approach for specific conversion subtasks, we need to decide on the following four factors: (1) the model for building the source labeler, (2) the model for building a heterogeneous labeler, (3) features for building the source labeler, and (4) features for building the target labeler. 2.2 POS tag conversion In terms of the models for building source and heterogeneous POS taggers, we adopt linear-chain conditional random fields (CRFs) (Lafferty et al. 2001 ) because of the theoretical advantages of CRFs in modeling sequence data. We first briefly review the CRF model and then describe in detail the features for building source and heterogeneous POS taggers. 2.2.1 Conditional random fields Conditional random fields (CRFs) are a popular probabilistic method for structured prediction. For the task in this article, the structure is simply a linear chain (POS sequence). Formally, we employ a second-order linear-chain CRF which is defined as follows. where the f k represents feature functions defined on the input word sequence x and the corresponding POS sequence y , and the Z ( x ) denotes the partition function. With respect to parameter estimation, algorithms like Limited-Memory BFGS and Stochastic Gradient Descent can be used. In the respect of decoding, the Viterbi algorithm is used to find an optimal POS sequence for an input sentence. 2.2.2 Features Table 1 shows the feature representation for building the source and heterogeneous POS taggers. The source POS tagger is built with features generated by single-side feature templates on Data s . We name the feature templates as single-side features because they are applied either on the source side or on the target side. The heterogeneous POS tagger is built with features generated by single-side and guide feature templates. Guide features encode information from source annotation guide the conversion process. Here, single-side features for the heterogeneous POS tagger are extracted from Data t while guide features include source-style POS tags that are assigned by the source POS tagger to the sentences in Data t . In terms of guide features for heterogeneous POS taggers, we also consider combination of source-style POS tags and single-side features. Note that we only list surrounding words w n ( -2, -1, 0, 1, 2) for feature combination because combining other single-side features with source-style POS tags leads to performance degradation in the experiments. 2.3 Syntactic structure conversion In principle, any parsing models can be applied to build a source syntactic parser. In this article we use the Berkeley parser (Petrov and Klein 2007 ) because the parser has relatively higher performance than the shift-reduce parser used in this article. The Berkeley parser is an unlexicalized parser which uses an Expectation-Maximization algorithm to automatically refine (split and merge) grammar labels. This way, context free assumption implicit in context free grammars can be relived. In terms of heterogeneous parsers, we use a feature-based shift-reduce parsing model (Zhang and Clark 2009 ) due to the capacity of incorporating rich and (possibly) overlapping features. In addition, shift-reduce parsers have the advan-tages of implementation simplicity and running efficiency. 2.3.1 Shift-reduce parser The shift-reduce process The shift-reduce process in the baseline parser assumes binary-branching trees, so binarization and debinarization are required for trans-forming training data and parsing output, respectively Zhang and Clark ( 2009 ). Given an input sentence (words and POS tags), any possible parse tree yielding the sentence corresponds to exactly one sequence of states. Formally, each state in the sequence is denoted by a tuple h S ; Q i ; where S is a stack containing partial parses and Q is a queue of word-POS pairs that remain unprocessed. In particular, the initial state is h / ; w 1 ... w n i where S is empty and Q contains the entire input sentence. The final state is h S ; / i where S contains a single parse tree with a pre-designated root label and Q is empty. Thus, the shift-reduce parsing process is a transition process from the initial state to the final state by performing a sequence of the following actions. 1. Shift, which moves a pair of word and POS tag from the head of the queue to 2. Reduce-unary-X, which extends the top item on the stack by applying a unary 3. Reduce-binary-{L/R}-X, which moves top two items out of the stack and 4. Terminate, which pops the root node off the stack and ends parsing. This action
Beam search extension The shift-reduce parsing process described above can be extended with beam search, as presented in Algorithm 1. The algorithm starts by initializing a beam of size K with the initial state. In each iteration after the initialization, states are popped in turn out of the beam. For each popped state, all applicable actions are then evaluated with respect to the state. Scored action-state pairs are sorted in a temporary priority queue. When the beam becomes empty, top K highest-scored action-state pairs are fetched from the priority queue and next states corresponding to the action-state pairs are inserted back into the beam. If the highest-scored state in the beam is a final state, it will be returned as the parsing result; else the iteration continues. The algorithm has time complexity of O ( nK ), where n is the sentence length.
 Model and learning algorithm To score an action A with respect to a state Y  X h S ; Q i ; we use a linear model as defined by the following formula.
 where f i  X h A ; Y i X  are features extracted jointly from the action A and state Y . To learn parameters k i , we use the generalized perceptron algorithm proposed in Collins ( 2002 ).

Generalized perceptron is an online learning algorithm that learns from one parse tree at a time. The basic procedure is to use the beam-search parsing algorithm (Algorithm 1) to parse the yield of a gold parse tree. Whenever the gold partial parse is pruned from the beam, parameters will be updated immediately and the learner moves to the next training instance. Such a strategy is known as  X  X  X arly-update X  X  (Collins and Roark 2004 ). Finally, model parameters are set to be an average of the weight vectors obtained during the online learning. 2.3.2 Features This section describes the features used in the heterogeneous parser. The features can be divided into two groups. The first group of features is derived solely from Data t so they are referred to as target-side features . In this article, we use target-side features that are completely identical to features used in Zhang and Clark ( 2009 ). For completeness, the features are repeated in Table 2 where the symbol S i denotes the i th item from the top of the stack, and the symbol Q i refers to the i th item from the front end of the queue. Moreover, w represents the lexical head of an item; t denotes the head POS for an item; and c refers to the constituent label.
We also have guide features extracted jointly from target-style and source-style parse trees. These features are generated by consulting a source-style parse (denoted T ) while the parser is learning parameters on the corresponding target-style parse tree. To better understand the feature functions, we use the parse trees depicted in Fig. 1 as an illustrating example. Suppose that we use a shift-reduce-based heterogeneous parser to convert the TCT-style parse to the CTB-style parse and that the stack S currently contains two partial parses: S 1 :[NP (NN )(NN )] and S : (VV ).

Constituent features F c ( S i , T s ) This feature schema covers three feature func-on stack S correspond to a constituent in the source-style parse T s . Specifically, F ( S i , T s ) =? if S i has a bracketing match (ignoring constituent labels) with any constituent in T s . S 0 S 1 represents a concatenation of spans of S 0 and S 1 . In our example, we can see that spans of both S 1 and S 1 S 0 correspond to constituents in T but that of S 0 does not.

Dependency features F d ( w 1 , w 2 , T s ) This feature template checks whether two words have a dependency relation in a source-style parse T s . To extract dependency relations from constituency trees, we resort to manually created head-finding rules (cf. Bikel 2004 for a detailed explanation). In this article, this feature schema is F ( S 0 w , Q 0 w , T s ). Specifically, if w 1 and w 2 have a dependency relation in T s , the feature function returns a flag ( LEFT or RIGHT ) to indicate that w 1 is the head of w 2 or vice versa, and NONE otherwise. In our example, S 1 w is a modifier of S 0 w while Q w has no dependency relationship with S 1 w or S 0 w .
 N returns SIBLING ; returns NONE otherwise. In our example, N s ( S 0 )is dj and N s ( S 1 )is np ,so N s ( S 0 ) and N s ( S 1 ) are neither identical nor siblings in T s .
Frontier-words feature F f ( RF ( S 0 ), Q 0 ) A feature function which decides whether the right frontier word of S 0 and Q 0 are in the same base phrase in T s . Here, a base phrase is defined to be any phrase which dominates no other phrases.

The guide features are summarized in Table 2 . Note that Zhu et al. ( 2011b ) used a syntactic path feature which is not considered in the article due to its marginal effect on conversion accuracy. The values of these features with respect to our illustrating example are collected in Table 3 . 3 Experimental setup 3.1 Data preparation Our experimental data came from two treebanks: CTB 5.1 and the TCT corpus released by the CIPS-SIGHAN-2010 syntactic parsing competition. 2 Regarding CTB 5.1, we used a widely-accepted division: articles 001 X 270 and 400 X 1151 were used as training data, articles 271 X 300 were held for performance evaluation, and articles 301 X 325 were used for system development. With respect to the TCT these two corpora are shown in Table 4 .

To evaluate conversion accuracy, we used the testing data that was used in Zhu et al. ( 2011a ). The testing data is actually the test set of CTB 5.1 which has TCT-style parse trees added manually. Henceforth, this test set is named CTB-Conversion-Set . We conducted treebank conversion experiments on two directions. One direction is to convert TCT-style parse trees (POS sequences and syntactic structures) in CTB-Conversion-Set to the CTB standard by using the CTB and TCT training data. The other direction is to convert CTB-style parse trees in CTB-Conversion-Set to the TCT standard.
 3.2 Heterogeneity of treebanks Before evaluating the performance of our approach, we would like to see to what degree the annotation standards of CTB and TCT are different. In the POS level, CTB has 33 POS categories and TCT uses 46 categories. On the contrary, TCT uses less constituent label types than CTB (16 vs. 28). We can see that CTB is annotated with a much more detailed set of constituent categories while TCT uses a finer-grained POS tagset. In addition, CTB and TCT POS categories are overlapping: the CTB POS set is not a subset of the TCT POS set, or vice verse. It is the same for constituent labels. So it is infeasible to design simple mapping rules to convert POS and constituent labels.

Actually, we are more interested in the comparison of bracketing schemes of these two treebanks. If we have sentences with both CTB and TCT annotations, we can formally define concepts of compatible constituents and conflicting constituents . Any two constituents, with constituent label being ignored and represented by their corresponding spans, say [i,j] and [m,n], are conflicting if they satisfy the constraints constraints are said to be compatible. The example in Fig. 1 demonstrates the case of constituents being conflicting. Specifically, the TCT constituent [dj (intelligence) (experts) (deem)] conflicts with any constituent in CTB that starts with the word  X  X  (deem) X  X . Such a TCT constituent is said to be inconsistent with the CTB standard. By contrast, Fig. 2 shows an example of compatible constituents. We can see that the sequential words  X  X  (border) (open) (city) X  X  are annotated differently according to the CTB and TCT standards: CTB adopts a flat structure for the noun phrase while TCT prefers more deeply nested trees. However the TCT constituent [np (open) (city)] does not prohibit the CTB constituent [NP (boarder) (open) (city) ]. In such a sense, we say the constituents are compatible with each other. We then empirically analyzed the ratio of constituents in a treebank which conflict with the standard of another treebank. Here we used the CTB-Conversion-Set which contains 348 sentences with both CTB-style and TCT-style parses. Statistically, the CTB-style parse trees in the data set contain 8,127 constituents, 509 (6.26 %) instances among which conflict with the TCT standard. With respect to the TCT-style constituents, 8.53 % (527/6,176) instances conflict with the CTB standard. From the statistics we can see the necessity of avoiding boolean distinctions that were used in previous work (Zhu et al. 2011a ). 3.3 Evaluation metrics We used two evaluation metrics for the experiments of POS tag conversion: per-token accuracy which is defined to be the ratio of correctly labeled words over all the words in a test set, and sentence accuracy which is the ratio of sentences that are correctly tagged. For the experiments of syntactic structure conversion, we used the standard PARSEVAL metrics (Collins 1999 ) provided by EVALB , including labeled precision (LP) , labeled recall (LR) , and F-score . 3 We also use Exact Match (EX. in short) to measure the ratio of sentences that are correctly parsed. 4 Experimental results We first conducted conversion from TCT-style parse trees to the CTB standard (Sects. 4.1 and 4.2 ) and then reversed the direction to convert CTB-style parse trees to the TCT standard (Sect. 4.3 ). 4.1 POS conversion from TCT to CTB We first built a CRF-based source POS tagger using single-side features extracted from the TCT training data and then applied it to assign POS tags to the sentences in the CTB training data. Thus the sentences in the CTB training data have both CTB-style and TCT-style POS sequences, on which a CRF-based heterogeneous POS tagger was built with single-side features extracted from CTB-style POS sequences and guide features defined on TCT-style POS tags. The baseline system used in the experiments is a POS tagger employing single-side features extracted from the CTB training data. Table 5 shows the conversion results on the CTB-Conversion-Set with guide features being added incrementally to the baseline. As the results show, only using TCT-style POS tags as guide features improves conversion accuracy significantly (1.3 % per-token accuracy and 8.6 % sentence accuracy). Combinations of TCT-style tags with unigram words achieve an improvement of 0.2 % per-token accuracy but degrade the performance when measured with sentence accuracy. We also tried to combine TCT-style POS tags with other single-side features but we achieved negative results.

We examined the most significant errors when the conversion accuracy is 96.6 %, as shown in the confusion matrix in Table 6 . The row labels are the correct POS tags while the column labels represent the tags assigned by the conversion approach. For example, the number 27 in the (NN, VV) position means the frequency that words of POS tag NN were incorrectly assigned to VV . From the table we can see that the most significant sources of errors are the confusion between NN X  X V , NN X  X J , and DEC X  X EG . 4 In addition, Table 7 shows the overall assignment accuracies of the POS tags that appear in the confusion matrix. There we list the results of the baseline and the results of our approach separately.
We also examined the conversion results as a function of the size of target training data. To this end, we randomly sampled different numbers of sentences from the CTB training data and always used the TCT training data to train the source POS tagger. The results are shown in Fig. 3 . Here the conversion results were achieved with guide features of tag s and tag s w n  X  2 ; 1 ; 0 ; 1 ; 2  X  : As the results show, POS tag conversion is superior to the baseline, especially when target goal of treebank conversion is to enlarge a data set which might have a limited size.
Finally, we compared our conversion results with previous work in Zhu et al. ( 2011a ) which incorporated mapping probabilities between source and target tags into the decoding phase of a second-order Hidden Markov Model. The best reported conversion accuracy in Zhu et al. ( 2011a ) is 95.9 % while the conversion accuracy in this article is 96.6 %. 4.2 Syntactic structure conversion from TCT to CTB In parallel to POS tag conversion, we first trained the Berkeley parser on the TCT training data and then applied the resulting parser to process the sentences in the CTB training data. Thus the sentences in the CTB training data were annotated with CTB-style and TCT-style parse trees. On the new data, a heterogeneous parser was built by using the shift-reduce parser described above with the beam size set to 16 for both training and decoding. In addition, since the heterogeneous parser requires CTB-style POS sequences as input, we first converted TCT-style POS tags in the CTB-Conversion-Set into CTB-style tags (with the accuracy of 96.6 %) before we started to convert TCT-style syntactic structures.

Table 8 shows the conversion accuracies when guide features were added incrementally to the baseline. Here the baseline is the heterogeneous parser only using target-side features (see Zhu et al. 2012 for a comparison of our baseline parsing algorithm with state-of-the-art parsers). From the results we can see that guide features improve conversion accuracy. Specifically, adding the constituent features ( F c ) achieves an absolute improvement of 2.0 % F-score and adding the dependency features ( F d ) on the base of F c yields an improvement of 0.4 % F-score. The relation ( F r ) and frontier ( F f ) features also improve the performance, though the improvements are marginal. This observation is contrary to the results in Zhu et al. ( 2011b ) where the relation and frontier features achieve an improvement of 0.8 % F-score. One possible reason is that the dependency features provide information that is overlapping with the information provided by the relation and frontier features. Zhu et al. ( 2011b ) did not use dependency features. Finally, we conducted a significance test for the overall improvement (2.6 % F score) by using the paring evaluation comparator provided by Danial Bikel. 5 We find that the improvement is significant on the level of p &lt;10 -4 .

We also examined the conversion accuracy and error reduction on sentences of different lengths. Table 9 shows the results where the sentences in the CTB-Conversion-Set are divided into five groups according to their lengths. We can see that guide features achieve bigger absolute improvements on long sentences. But when we consider error reduction, we can see that guide features are also effective on short sentences. In addition, we compared the conversion accuracy of our approach with the accuracy reported in Zhu et al. ( 2011a ) and we find that our approach outperforms the approach used in Zhu et al. ( 2011a ) by 0.6 % (84.8 vs. 84.2 %). Finally, we studied the effect of using different sizes of target training data, as shown in Fig. 4 .Notethatin these experiments, POS tag and syntactic structure conversion on the CTB-Conversion-Set always used the same sample of target training data. As the results show, the effect of guide features is quite stable with different sizes of target training data. 4.3 Conversion from CTB to TCT To further verify the effectiveness of our approach, we converted the CTB-style parse trees in the CTB-Conversion-Set to the TCT standard. Table 10 reports the POS conversion results and Table 11 shows the results of converting syntactic structures. From the results we can see that guide features improve POS conversion accuracy by 2.7 % and syntactic structure conversion accuracy by 3.2 %. A significance test shows that the improvement on syntactic structure conversion accuracy is significant on the level of p &lt;10 -4 . 4.4 Applying conversion results We converted the TCT training data into the CTB standard and used the conversion results as additional training data for training POS taggers and syntactic parsers. Table 12 presents the POS tagging and syntactic parsing results on the CTB test set, as a function of the varying size of target training data. For example, when the target training data contains 1k sentences, the sentences were used to convert POS tags and syntactic structures in the whole TCT training data; the conversion results were combined with the 1 k sentences to train a POS tagger (CRF ? ) and shift-reduce parser (SR-Parser ? ). CRF and SR-Parser are the baseline POS tagger and syntactic parser that were trained only on the 1k sentences. For data combination, we simply gave the target training data a relative weight of one. Note that in syntactic parsing, we always used the baseline in Table 5 to assign POS tags to the test set (per-token accuracy of 95.1 %) for SR-Parser and SR-Parser ? . Such a setting enables us to ignore the effect of POS tagging on parsing accuracy and to compare parsing results in a fair condition. The row of SR-Parser ?? uses the same parser as SR-Parser ? but employs CRF ? , instead of the baseline POS tagger to assign POS tags to the test set.

From the results we can see that the new data generated by the conversion helps to boost POS tagger and syntactic parser most of the time, especially when the size of target training data is small. On the other hand, we also notice that the rise incurred by the new data levels out with the increment of the size of target training data. When the whole CTB training data is used, adding the new data even has negative effect on parsing accuracy. To improve parsing accuracy, one solution is to manually correct (with relatively low cost) new data to improve the quality of the conversion results. Another possible solution is to extract partial information such as lexical dependencies from the new data rather than directly use whole sentences of the new data. The obtained information can be incorporated back into the parser to achieve better parsing accuracy.

The experiments presented above reminds us of self-training (McClosky et al. 2006 ) which is widely used semi-supervised approach to syntactic parsing. In self-training, a base parser is used to process (large-scale) unlabeled data and the resulting auto-parsed data is then combined as additional training data to build a new parsing model. Compared to self-training, our approach has the following advantages: (1) treebanks conversion aims to make use of resources (treebanks) that have been created before and, (2) treebank conversion provides new data for parsing that has higher quality than auto-parsed data. On the other hand, unlabeled data is generally available in large scale while our approach suffers from limited sizes of treebanks. 5 Related work For constituency treebank conversion, Wang et al. ( 1994 ) proposed to use source-side bracketing structures to select as conversion results from k-best lists that are generated by a parser trained on target treebank. The approach is quite generic in the sense that it can be used for conversion between treebanks of different grammar formalisms, for example from a dependency treebank to a constituency treebank (Niu et al. 2009 ). However, it suffers from limited variations in k-best lists (Huang 2008 ). To solve this problem, Zhu et al. ( 2011a ) proposed to incorporate bracketing structures as parsing constraints in the decoding phase of a CKY-style parser. Their approach shows significant improvements over Wang et al. ( 1994 ). However, it suffers from binary distinctions (compatible or conflicting), as discussed in Sect. 1 . The proposed approach in this article is reminiscent of co-training (Blum and Mitchell 1998 ; Sagae and Lavie 2006b ) and up-training (Petrov et al. 2010 ). Moreover, it coincides with the stacking method used for dependency parser combination (Martins et al. 2008 ; Nivre and McDonald 2008 ), the Pred method for domain adaptation (Daume  X  III and Marcu 2006 ), and the method for annotation adaptation of word segmentation and POS tagging (Jiang et al. 2009 ). As one of the most related works, Jiang and Liu ( 2009 ) presented a similar approach to conversion between dependency treebanks. In contrast to Jiang and Liu ( 2009 ), this paper focuses on the task of constituency treebank conversion and most of our efforts are put onto designing guide features. 6 Conclusion and future work In this article, we proposed a feature-based approach to constituency treebank conversion, which was applied to POS tag and syntactic structure conversion respectively. We also designed a set of guide features to bridge annotations in source and target treebanks. Experimental results on CTB and TCT showed that the proposed guide features are effective on improving conversion accuracy. Specif-ically, when converting from TCT to CTB, our approach outperformed previously reported results in Zhu et al. ( 2011a ), reaching 96.6 and 84.8 % for POS tag and syntactic structure conversion respectively. We also used newly-gained data to improve POS tagging and syntactic parsing, and found that new data was helpful when the size of original training data was limited.

There are many ways in which this research can be continued. First, taking into consideration that syntactic information is beneficial to disambiguating POS tags, one possible direction is to incorporate syntactic information into POS tag conversion. Second, we are concerned with whether unlabeled data can help treebank conversion. To this end, we propose to apply the semi-supervised approach used in Chen et al. ( 2009 ) which encode lexical dependency information from auto-parsed data as features for syntactic parsing. Third, we will adapt the feature-based approach to treebank conversion between different grammar formalisms, for example, from dependency treebanks into constituency treebanks.
 References
