 Anne M. Denton  X  Christopher A. Besemann  X  Dietmar H. Dorr Abstract Clustering of time series subsequence data commonly produces results that are unspecific to the data set. This paper introduces a clustering algorithm, that creates clusters exclusively from those subsequences that occur more frequently in a data set than would be expected by random chance. As such, it partially adopts a pattern mining perspective into clustering. When subsequences are being labeled based on such clusters, they may remain without label. In fact, if the clustering was done on an unrelated time series it is expected that the subsequences should not receive a label. We show that pattern-based clusters are indeed specific to the data set for 7 out of 10 real-world sets we tested, and for window-lengths up to 128 time points. While kernel-density-based clustering can be used to find clusters with similar properties for window sizes of 8 X 16 time points, its performance degrades fast for increasing window sizes.
 Keywords Density-based clustering  X  Time series subsequence clustering  X  Clustering noisy data  X  Noise elimination  X  Time series labeling 1 Introduction Clustering of time series subsequences is of interest in its own right and as a preprocessing step to other data mining tasks [ 7 ]. Introducing a meaningfulness measure, Keogh et al. were able to show that, in this setting, some standard clustering algorithms produce results that are largely independent of the data set [ 23 ]. Their study was, however, limited to k -means and hierarchical clustering. For k -means clustering, Ide [ 18 ] has shown formally that sinusoidal cluster centers are to be expected. The proof assumes X  X s is the case for k -means X  X hat all subsequences are being clustered. Other clustering techniques, in particular density-based techniques, recognize the concept of outliers and are known to be much more robust to noise [ 6 , 12 , 17 ]than k -means. Kernel-density-based clustering has been adapted to the time-series subsequence context, and is capable of producing robust, data-set-specific results for window sizes of 8 X 16 time points [ 10 ]. The effectiveness of density-based clustering concepts for time-series subsequences, using the meaningfulness measure that Keogh originally proposed, was shownin[ 9 ]. Techniques based on Gaussian kernels are, however, limited to small window sizes [ 32 ]. The work presented in this paper builds on the outlier detection capabilities of density-based techniques, using a sharp cutoff in analogy to a flat kernel, and introduces a rigorous comparison with random walk data.
 For time-series subsequence data, not all windows are expected to represent a clear pattern. Figure 1 illustrates the importance of disregarding noise in time series data. An artificial data set is shown that has two recognizable patterns, one bell-shaped (pattern 1) and one funnel-shaped (pattern 2). Intermediate regions do not follow any particular pattern, and it is therefore appropriate that they should not be assigned to a cluster. The labeling is done based on the two cluster representatives that are shown in the top right plots. The labeling is an experimental result and is discussed in more detail in Sect. 4 . Note that regions without obvious patterns are labeled  X - X , indicating that these are outliers or noise. Conventional k -means has no mechanism for excluding noise. In fact, cluster members that are far from a cluster center have a particularly large impact on the location of k -means cluster centers, since the mean minimizes the sum square error of a set of values. Density-based clustering techniques disregard any data points that are in low-density regions of the attribute space. As a result, only those data points contribute to the cluster center definition that are in high-density regions. This approach brings clustering of time series subsequences closer to pattern discovery [ 28 , 35 ].

Ide X  X  proof [ 18 ] that cluster centers must be sinusoidal, as well as observations on hidden constraints by Keogh et al. [ 23 ], are both based on the assumption that all subsequences are used in the clustering process. The presented algorithm removes that assumption and provides a systematic approach towards identifying data points that can be considered meaningful. Disregarding outliers is an important step in developing any robust clustering algorithm. The full potential of the radial distribution (RD) clustering algorithm derives from its rigorous identification of outliers. Rather than relying on ad hoc threshold values, we compare directly with what would be expected from random data. This comparison is done on a set of length scales to avoid limiting assumptions. Such a step can be seen as integrated outlier detection, similar to existing outlier detection algorithms [ 2 , 27 ]. However, data points are not compared against the bulk of the data as is typically the case in outlier detection, but rather against the distribution of random walk data. The rationale is that in time series data the majority of the data can be noise, and the assumption that outliers are rare may then no longer be satisfied.

It should be noted that for the example in Fig. 1 the shape of cluster representatives closely matches the patterns that a human observer would have identified in the time series. Section 4 will show that the same commonly holds for real-world data sets. Much of this property can be attributed to the density-based cluster definition. Note, however, also that cluster representatives are required to be actual data points rather than averages. Some other density-based algorithms share this constraint [ 12 ], and even in partition-based clustering it is often viewed as an advantage of medoid-based techniques [ 21 , 26 ] over centroid-based techniques, such as k -means, that the cluster representatives are guaranteed to be data points [ 16 ]. The risk of clusters not representing the data well is lower when they are centered on a data point.

With the goal of demonstrating the meaningfulness of clustering results, we apply a classification-style evaluation. A clustering is considered data-set-specific if it can be used as a classifier that labels a subsequence as true if it comes from the same sequence and as false otherwise. K -means and other partition-based techniques will always assign a label to a data point. This corresponds to a classifier that classifies all data points as true .The rationale for using classification-based evaluation measures in this context is as follows: Labels are assumed to represent patterns of the time series. If a subsequence is labeled based on a pattern that was derived from an unrelated time series the meaning of that assignment is questionable.

There are other criteria that determine whether a clustering is useful, besides it being data-set-specific. A strict optimization of the selectivity of clusters leads to a large number of cluster centers, that are awkward to use in any kind of labeling. A single characteristic pattern may occur in several positions within a window, resulting in a different cluster center for each position. This problem is addressed by grouping cluster centers into labeled groups, which are determined using a sliding window comparison among cluster centers. Note that Fig. 1 shows those RD centers that are representatives of their labeled groups. The labeling step can also be viewed in the context of symbolization of time series data [ 8 , 25 ].
The complete algorithm can be summarized as follows: For each data point we examine the number of neighbors as a function of Euclidean distance from that point. This function (radial distribution or RD-function) can be analytically calculated for a Gaussian random walk model, and a normalization that places data on a hypersphere. The number of observed data points, compared with the number of expected ones, is used as an indicator of how exceptional a data point is. The resulting cluster centers are filtered such that each center is represented by the cluster with highest confidence within its range. After that, clusters are combined into groups, each of which receives a label.

The rest of the paper is organized as follows. In Sect. 2 ,wegiveabriefoverviewofthe relevant literature and define terms. Section 3 discussed the components of the algorithm including radial distribution functions as a way of quantifying the neighborhood of a data point. Section 4 presents results from our evaluation on real and synthetic data. Section 5 concludes the paper. 2 Related work and definitions 2.1 Time series data After Eamonn Keogh challenged the data mining community by showing that both the k -means and hierarchical clustering algorithms return meaningless results [ 23 ] when applied to time series subsequence data, if all subsequences are used, several researchers sought to resolve the issue by addressing the specific nature of time-dependent data [ 3 , 31 ]. Cluster cores are introduced in [ 29 ], and can be viewed as related to our concept of cluster centers, as well as to modes in kernel-density-based clustering [ 6 ] and to core points in DBscan [ 12 ]. It has been observed that a large number of clusters is beneficial [ 29 ], which is also supported by the presented work, and related to the motivation of arbitrarily shaped clusters in the density-based clustering literature [ 6 , 12 , 17 ].

In [ 9 , 10 ] we showed that kernel-density-based techniques that take into account the ran-dom walk nature of typical time series data can make the clustering results data-set-specific using a discrete [ 9 ] or continuous [ 10 ] random walk model. In [ 9 ] the original meaningful-ness measure, introduced by Keogh et al. [ 23 ], was used for comparison purposes, while [ 10 ] presented the sensitivity and specificity measures used in this paper. It should be noted that while the meaningfulness measure provides good initial insights into the problem, it is not robust against the number of cluster centers being used. K -means, in the limiting case of k equal to the number of data points, will trivially result in a perfect score, assuming that each cluster center is initialized to a different data point.

Chen [ 3 , 4 ] observes that the sliding-window-based extraction of time series subsequence data can be considered a special case of an embedding of a time series into a delay vector space, as developed for dynamic systems [ 20 ]. Using this analogy he develops a clustering technique for cyclic data series that is based on a distance measure designed to fit the delay vector space. [ 5 ] also observes that restricting the cluster space to regions visited by the time series will result in cluster centers that maintain the character of the time series from which they came.

Finally, [ 15 ] show that the results of k -means clustering from different time series can be distinguished using a newly introduced distance measure. It is important to recognize that our work differs from Goldin X  X  in that the latter attempts to identify whether k -means clustering results stem from the same time series, even if the results do not well represent the time series from which they originate. Our goal is to demonstrate that results from RD clustering do represent their respective time series well, and that those results do not nearly as well represent a different time series.

In the following, we assume that a time series is a sequence of real-valued data, collected at regular time intervals, and that subsequences are extracted using a sliding-window approach. Definition 1 A subsequence of time series T = t 1 ,..., t n , with length w , is a sequence S = t m ,..., t m + w  X  1 with 1  X  m  X  n  X  w + 1.
 Subsequences can be viewed as vectors in a w -dimensional vector space. In this paper, Euclidean distance is used as the norm on that vector space. Specialized distance mea-sures, such as dynamic time warping (DTW) [ 1 ] and longest common subsequence similarity (LCSS) [ 33 ], have been defined for time series data. These measures, however, would result in performance problems when applied to the many comparisons that are necessary in RD clustering, and would also complicate the comparison with noise. The results in Sect. 4 sup-port the use of the Euclidean distance measure on real-world time series: Typically the results are best for w = 32 or w = 64 rather than for w = 8or w = 16 despite the expectation that the Euclidean distance should become less appropriate as dimensionality increases.
The use of the Euclidean distance does cause problems on time series that show reco-gnizable shapes with variable scaling in time. A classic example of such a time series is the Cylinder X  X ell X  X unnel series that was suggested in the time series classification context [ 30 ], and has been used extensively since then, e.g., [ 24 ]. At this point, the algorithm is not desi-gned to recognize shapes of different length as similar. If the time series contains such shapes then a large number of cluster centers is returned. We use a toy series that has fixed length cylinder-, bell-and funnel-shapes, separated by variable-length intervals for demonstration purposes, but this should be clearly distinguished from the originally proposed series. Note also that the noise that is added to form the CBF series is white noise and not random walk noise.

Noisy time series data often have the property that differences between values at successive time points are randomly distributed while the white noise model assumes that individual values are randomly distributed.
 Definition 2 A strict white noise time series is a normally distributed sequence of values { e } corresponding to time t . Mean  X  and variance  X  are assumed to be the same for all time points.

A time series, for which differences between values at successive time points are randomly distributed is best modeled by a random walk time series.
 Definition 3 A Gaussian random-walk time series is a sequence of values X t , for which the difference between consecutive values e t is a normally distributed random variable that satisfies where E is the expected value, var the variance, and cov the covariance.

For data mining purposes, it is useful to apply a coordinate transformation such that the resulting distribution can be described as white noise. We perform such a transformation by taking differences between successive time points.
 Time points within windows of length w are interpreted as components of vectors z and are normalized using the inverse of their norm.
This transformation can be shown to be equivalent to the one used in [ 10 ], in the sense that both coordinate systems differ by only a rotation. In contrast to some approaches [ 13 ], we do not subtract the mean after taking the derivative. 2.2 Density-based clustering Density-based clustering algorithms share the general concept of clusters as regions in attribute space that have a high density of data points. In kernel-density-based clustering a density landscape is defined using a kernel-density estimator, which for n data points x , i = 1 ,..., n in a d -dimensional space is given by where the kernel function K ( x ) is normalized
Local maxima, which are also called modes or attractors, signify clusters. For a differen-tiable kernel function, such as a Gaussian kernel, maxima can be found using a hill-climbing procedure [ 6 , 17 ]. Local maxima may either be identified as cluster centers directly or multiple connected regions may be combined [ 17 ].
 In high-dimensional spaces a Gaussian kernel is a problematic choice. Verleysen and Fran X ois [ 32 ] show that a Gaussian function decreases only slightly between distances that are closer than those to most neighbors and distances that are larger, for normally distributed data. As a consequence, most points in the data set contribute similarly to any one peak in the density landscape that is constructed in kernel-density-based clustering. In high-dimensional spaces a uniform kernel, therefore, has significant advantages. For a uniform kernel, hill climbing is no longer an option, since such a kernel is not differentiable. Neither is it practical to explore the space in a grid-based fashion. A grid-based algorithm that naively evaluates the density at fixed intervals in an n -dimensional space would have to explore a space that grows exponentially with n . A common strategy for restricting the search space of a density landscape in high dimensions is to only search data points. DBScan [ 12 ] is an efficient clustering algorithm that has some similarity to kernel-density-based clustering with a uniform kernel. DBScan only considers real data points as cluster centers and does not explore the intermediate space. This strategy does not guarantee that all local maxima will be found. It can, however, be argued that cluster centers should be real data points regardless of other considerations as was discussed in the introduction.

A further limitation applies to both kernel-density-based clustering and DBScan: The range over which inhomogeneities are weighted is considered constant for the complete data set. In kernel-density-based clustering the range enters as the kernel width. While the kernel width is typically chosen by the user, we automate the process for the comparison runs in this paper, to ensure objectivity. We also show that our automated process outperforms earlier results [ 10 ] with a manually set kernel width. In DBScan the radius of hypervolume is called  X  X ps X . DBScan inherently requires user-interaction for determining  X  X ps X , and hence we were not able to objectively compare our results to those of DBScan. RD-clustering, in contrast, uses a local definition for the radius of hyperspheres. Section 3.4 explains how that radius is determined. 3 The RD clustering algorithm We will start with an overview over the steps that are involved in the RD Clustering algorithm. These steps will be discussed in the subsequent sections.

Preprocessing The data are prepared as follows. First we extract subsequences, using a sliding window. Then we take differences between successive time points, see Eq. ( 3 ), and divide by the norm of the resulting vectors, see Eq. ( 4 ). Means are not subtracted to maintain the trend of the time series.

Cluster Candidate Generation This step is at the heart of the algorithm and is discussed in detail in Sect. 3.1  X  3.4 . Cluster candidates must satisfy the following requirements:  X  Clusters are hyperspheres, defined by their center and radius.  X  Cluster centers must be data points.  X  Cluster candidates must have more members than would be expected by random chance.  X  The radius of a cluster center maximizes the confidence that the neighborhood of the
Filtering Cluster candidates are filtered based on two considerations, see Sect. 3.6 :  X  If a cluster has a cluster center as member that has a higher confidence of being non- X  A cluster is discarded if it is entirely enclosed in another cluster
Forming Labeled Groups The algorithm, up to this point, is likely to produce a large number of clusters, many of which are related through shifting of patterns within the win-dow. The final step maps patterns that are related through time-shifting to a single label.
Section 3.7 discusses the cluster-label mapping process. 3.1 RD functions for cluster candidate generation The objective of this paper is to develop a clustering algorithm that identifies source-specific patterns, and labels data points accordingly. We look for regions, in which the density of data points most exceeds what would be expected from random data. With this goal in mind, we calculate the number of points that are within a given radius of any one data point, and compare the resulting distribution with its counterpart for random data. Figure 2 gives a schematic view of the concept. The point at position r 2 is within radius r of the central point c and hence contributes to the sum of data points at that radius, while r 1 does not. For low-dimensional spaces the increase of the number of points for increasing radius is gradual. For higher dimensionality the distribution becomes increasingly steep, as will be seen quantitatively in Fig. 4 .
 We define a radial distribution function for center location c and radius r as where the integral d d r is performed over an infinite hypervolume of the given dimensionality d .Notethat d = w  X  1, where w is the window size, as a result of the normalization, which involves differencing between successive data points. H is the Heaviside step function, which points, where each data point is represented by a Dirac  X  function.
 with N being the number of data points and r i their location.

The radial distribution function can be seen as representing the density at a data point as a function of range. In kernel-density-based clustering with a uniform kernel of radius r , the density at location c would be proportional to P ( c , r ) . Note that in kernel-density-based clustering only one radius is used, whereas we chose an appropriate radius for each data point by comparing P ( c , r ) for data and noise. 3.2 Numerical computation of the RD function The radial distribution of points around a central point, P ( c , r ) , is of central importance for the algorithm. The numerical calculation of this quantity is straightforward. Given a central point c , the Euclidean distance with respect to all N other data points, p ( k ) , is computed. where n is the number of intervals, and 2 is the maximum achievable distance given the normalization using ( 3 )and( 4 ). We get Pseudocode for the evaluation of this quantity is given in Algorithm 1 .

For noise, two approaches can be taken. A sample distribution of random data can be produced or the distribution can be derived from theoretical considerations. We will pursue both routes. The derivation from theoretical considerations is more computationally efficient but less numerically stable for high dimensions. 3.3 Analytical evaluation of the RD function We will now show how the radial distribution function can be evaluated analytically if the data are assumed to be random. The ensemble average over configurations of data points that follow the normalization in Sect. 2.1 is where N is the number of data points and S d is the surface area of a hypersphere in d dimensions with radius 1 The calculation of this quantity, while being straightforward, is not necessary in practice, as will be seen later.

To simplify notation, we pick a coordinate system that is centered on the data point of interest, i.e., c = 0. We use cylindrical coordinates, such that the z -axis is defined by the line through the origin of the coordinate system, in which the data are defined, and the shifted center. Figure 3 illustrates the setup.
 In the new coordinate system, the radial distribution function takes the following shape P ( r ) = where the integration d d  X  2 d  X  2 covers the solid angle of a hypersphere in d  X  1dimen-sions. Since the setup does not depend on any of the angles in d  X  2 , this integration can be performed immediately where S d  X  1 is the surface of a hypersphere in d  X  1 dimensions. This notation follows the convention of [ 34 ]. Note that S d  X  1 topologically represents a ( d  X  2 ) -dimensional space, which is also apparent in the ( d  X  2 ) -dimensional integration. The hypersphere is defined within a subspace of the d -dimensional space, perpendicular to the z -dimension, hence the d  X  1in S d  X  1 .
 The radial distribution function can be normalized based on the total number of points. Hence there is no need to explicitly calculate constant factors, and they will be omitted from further discussion. As a further simplification, we rewrite the Heaviside step function as an integral over a  X  -function The resulting radial distribution function is This integral can be evaluated in a straightforward way, using the relationship The integration over a in Eq. ( 17 ) is performed first. The argument of the second  X  -function has its zero at and the derivative of f ( a ) is The radial distribution function can now be written as Using Eq. ( 18 ) a second time with g ( z ) = the z -integration can be performed The remaining integral is done analytically in Matlab as part of the computational process. For d = 7, corresponding to time series subsequences with w = 8, the resulting radial distribution function is For even dimensions the analytical representation has more terms but this does not constitute a problem since Matlab returns a solution for even as well as odd dimensions.

When computing numerical radial distribution functions, we limit ourselves to data points as centers. For the analytical computation the probability of there being a data point in the center is, however, vanishingly small. We must, therefore, add the center point explicitly to the analytically calculated solution. We use the following effective radial distribution function
For w = 64 the variable precision data type available in Matlab has to be used because of numerical instabilities. For larger window sizes ( w = 128 in the experimental evaluation) the analytical solution becomes unstable, and the numerical computation based on random data is used. Note that the numerical evaluation remains stable for all dimensions.
Figure 4 shows a comparison between the analytical solution for the radial distribution function with its numerically calculated counterpart for w = 8 , 16 , 32 , and 64 and a time series length of 500, averaged over all data points as centers of the data set. It is important to note that even for random data the individual radial distribution functions do show large fluctuations. In fact, we will use those fluctuations as a way of identifying what behavior can be expected for random data. In Fig. 4 the average over each data point as a center was taken, resulting in distributions that show negligible variations compared with their analytical counterparts. We will refer to the averaged radial distribution function as collective RD function. 3.4 Confidence optimization for determining the cluster radius We are interested in the range, within which the actual distribution of neighbors most deviates from the distribution that is expected by the random noise model (Fig. 5 ). Experiments showed that a useful measure is provided by what we call a cluster confidence where p random ( R ) is the radial distribution for random data (Eq. 24 )and p ( R ) the local, numerically calculated, radial distribution function for a particular data point in the data set of interest (Eq. 11 ). c ( R ) is a measure of our confidence that a point within the cluster is a data point rather than noise: Given, that a point is located in a cluster, our confidence, that the unlikely that a point in the cluster is due to noise. If c ( R ) is about 0 . 5 then the distribution of the data set around the data point of interest is very similar to the distribution of random data. Such a point would not be suitable as cluster center, since it is not at all characteristic ofthedataset.

It is important to note that even in random data there are likely to be points with a cluster confidence c ( R )&gt; 0 . 5. We use the largest occurring cluster confidence in a random walk data set as our threshold for determining potential cluster centers. The definition based on random data relieves us from the need of specifying parameters. Throughout this paper we avoid the use of parameters that can be set by the user since such parameters introduce arbitrariness and difficulty of use. The next section explains how the concept of a radial distribution function can also be used to make kernel-density-based clustering parameter-free. 3.5 Parameter-free kernel-density-based clustering using RD functions only specify one parameter, as partition-based clustering techniques such as k -means and k -medoids [ 21 ] do, but two. Traditional implementations of kernel-density-based clustering using a Gaussian kernel require choice of a width of the kernel function  X  , which has a direct impact on the number of clusters that can be distinguished. Additionally they require setting a threshold t that is used to identify outliers. We already showed in [ 10 ] that the threshold t can be determined automatically through a comparison with a noise model. The of the Gaussian kernel based on the over-all radial distribution function. For that purpose we weight the radial distribution function by the Gaussian kernel function and calculate a weighted radius of maximum selectivity for each data set. The resulting range depends on the data set and the noise threshold and therefore has to be determined separately for each data set. All computations in the evaluation section use automatically set parameters. 3.6 Filtering If all cluster candidates were used as clusters the algorithm would not be sufficiently selective. Some points are cluster center candidates on the basis of a high-density group of points close to their periphery. A further step is necessary for finding those cluster centers that have no better cluster candidates within their range: The data points within each cluster candidate are examined, and the point with the highest confidence is picked. After this has been done, only one copy of each cluster is kept. This step can be compared with the distinction between core and border points in DBScan [ 12 ].

Figure 6 shows the projection of a clustering that was done based on time series subse-quences with w = 4 from the speech data set described in Sect. 4 . Note that we use w = 4 for demonstration purposes only, since in practice subsequences with w&lt; 8 are expected to be of little interest to the user. The process of taking derivatives reduces the dimensionality of the data set to 3, and the normalization places data points on a sphere of radius 1, which is then projected onto the first two dimensions. Filled points identify cluster centers and the circles or ellipses that surround them indicate the cluster range. Note that clusters are seen as ellipses because of the projection from three onto two dimensions. It can be seen that the clusters are overlapping. In the labeling process, if a point belongs to multiple clusters, it is assigned to the one for which the cluster center is closest.

As an alternative we evaluate a second selection criterion. Instead of dropping a potential cluster center if a point with higher confidence is found within its range, one could also consider the reverse. If a member of a potential cluster has a lower cluster confidence than the current center, then the center with lower confidence could be dropped. The alternative algorithm is, however, much less effective. Figure 7 shows an example of a clustering that was done on the same data as Fig. 6 , using the prescription that poor cluster members should be dropped. In the bottom right corner a point can be seen that is considered as cluster center although the regions of high density are very far from its center. Since none of the more confident cluster centers have this point within their range the point remains in the set of cluster centers although it clearly is not a good candidate. This could not happen if poorer cluster centers were replaced by better ones in their range. Since the point in the bottom right has many more suitable cluster centers within its range, the point itself would be dropped. The algorithm we use, thereby, leads to overall smaller clusters with higher cluster confidence.
It is interesting to note that the points in the bottom left corner of Figs. 6 and 7 are not considered as potential cluster centers. Based on the over-all density of points they are clearly no closer than could be expected for random data. This is important information that could not be gained from algorithms that allow a user to pick thresholds based on intuition. 3.7 Cluster X  X abel X  X apping The filtering still leaves a large number of clusters that represent related patterns. Figure 8 shows 15 windows, all of which clearly represent the cylinder pattern of the cbf toy series discussed in Sect. 4 . When assigning labels to a time series, all of these should be recognized as the same pattern. We, therefore, need a mapping from mutually related clusters to labels.
For this mapping we compare cluster centers using a second, smaller sliding window ( w = f w&lt;w ). All cluster centers that are within range for any comparison of subwindows, are given the same label, and are represented to the user through their most confident cluster specification step clusters are replaced until no cluster representative has a member of higher confidence.

The scaling factor f w is the only parameter, other than the window size w ,thatwasleft flexible in the algorithm. We used values of f w = 0 . 5and f w = 0 . 75 in the evaluation. The choice of f w strongly affects the number of representative clusters. It is important to understand that the cluster X  X abel X  X apping has no impact on whether a window will be assigned a label at all. The selectivity with respect to noise is entirely determined through the cluster centers and their radii. Once a subsequence is assigned to a cluster it is automatically also given a label through the cluster X  X abel X  X apping. 3.8 Summary of RD clustering The algorithm is summarized in the pseudocode 1 . Since Matlab is used in the implementation we preserve an array-based representation in 1 . The following list summarizes the steps that are taken in the clustering process:
Normalization Subsequences from the real and the random walk time series are normalized using Eqs. ( 3 )and( 4 ).

Calculating RD function for expected distribution The collective RD function for noise is calculated for a predefined set of R -values at fixed intervals. This calculation can be done either based on the analytical RD function, see Eq. ( 24 ) or numerically by averaging over all points in the random walk data set, see Eq. ( 11 ). The averaging is necessary to minimize fluctuations. In the experimental evaluation we used the analytical version for all window sizes other than the largest. For w = 128 we used the numerical evaluation of the radial distribution function because of numerical instability of the analytical solution.
Determining confidence threshold Individual points in random data can have a substan-tially different radial distribution function compared with the averaged collective distribu-tion function. We use the maximum of the cluster confidence for random data as cutoff value for other data. For this, we run Algorithm 1 for random walk data, with a threshold f thresh = 0. To evaluate the cutoff, the random data set is chosen to be equally large as the data set of interest.

Confidence optimization For each point in the data set of interest, the numerical radial is calculated for all predefined R -values and the maximum is taken, see line 9 of the pseudocode.
Replacing cluster centers with their most confident members Each potential cluster center is replaced by its cluster member with highest confidence (which could be the center itself), and only non-redundant clusters are kept, see line 13. The process of finding mem-bers with higher confidence is not repeated recursively since otherwise coverage would be limited too strongly.

Selection of cluster candidates The cluster confidence is compared with the threshold that was determined based on random data in the second step, see line 14. Only data points with higher cluster confidence are kept as cluster candidates.

Discarding redundant clusters Clusters that are completely enclosed by another cluster are removed since they do not contribute information. This step is not included in the pseu-docode but is straightforward to implement.

Cluster X  X abel X  X apping Groups of clusters with related subpatterns are assigned uniform labels. This is implemented a separate program that compares all subwindows of length w = f w w ,where0 . 5  X  f w  X  0 . 75 in the evaluation. All subwindows of length w of cluster centers are extracted and compared all against all using a cluster range that is scaled with f w to account for the lower dimensionality. The representative is selected in a step that is equivalent to line 13 of the pseudocode.

The assignment of labels to subsequences based on clusters is a separate step. Given the cluster centers and associated radii cluster membership can be evaluated through a simple distance evaluation. Note that a data point may be within multiple hyperspheres. To address this, a disambiguation step is added that picks the smaller distance. Multiple clusters cor-respond to one label. Once the cluster label X  X apping has been established as part of the clustering process, the mapping amounts to a simple table-lookup. Below are the steps that are involved in the labeling.
 Normalization Subsequences are normalized as in the clustering process.

Membership test The distance of a data point to cluster centers is compared with the res-pective cluster radii.

Disambiguation If a point is member of multiple clusters then it is assigned to the one for which its distance to the cluster center is smallest.

Labeling A label is determined based on the cluster X  X abel-mapping of the cluster, of which it is considered a member. This is done using a simple 1-dimensional array, in which the index stands for the cluster number and the array element for the label of the representative cluster. 4 Implementation and evaluation 4.1 Implementation and data sets The algorithms are implemented in Matlab. The comparison code using kernel-density-based clustering is an extension of a toolbox for kernel-density estimation by Ihler [ 19 ], and modes are evaluated using a hill-climbing algorithm.
 We use the same data sets with the same preprocessing as in [ 10 ] to maintain consistency. Most of the data sets are taken from the UCR Time Series Data Mining Archive [ 22 ]andare described there. The ECG data (MIT-BIH Arrhythmia Database: mitdb100) is from Physio-Bank [ 14 ]( ecg ), and is compressed by averaging over 20 consecutive values. The buoy series is derived by averaging the buoy sensor series (from [ 22 ]) over 4 consecutive values. These data sets are the same as those in [ 10 ]. Additionally, we included the  X  X tandardandpoor500 X  data set from the UCR Archive, listed as stock and four artificial data sets.

Furthermore, some artificial data sets were considered. The random walk series is a nor-mally distributed random walk series from the UCR Archive. sine is a sine wave with period 2  X  , sawtooth and rectangle are signals of the respective shape with period 6. rectangle has a pulse width of 3. For demonstration purposes we also use a toy Cylinder X  X ell X  X unnel series cbf toy that is composed of the patterns [0,5,5,5,5,5,0], [0,1,2,3,4,5,0], and [0,5,4,3,2,1,0]. Patterns are separated by a variable (random) number of 0 values in the range of 0 X 11. Nor-mally distributed white noise with standard deviation 0.2 is added to all points of the time series. In the introduction a time series with only the funnel and bell shape is used to which normally distributed white noise with a standard deviation of 0.5 has been added. 4.2 Results on example sequences We will first look at some example sequences. Figure 9 shows the ecg series on the left. On the top right the representative RD cluster centers are shown for a window size of w = 16. A scaling factor f w = 0 . 75 is used, indicating that cluster centers that match over w = f w w = 12 time points are assigned to the same representative cluster. Both cluster centers clearly represent observed patterns in the time series. If the scaling factor is chosen even smaller shown in the left panel of Figure 9 was not used in the clustering process that produced the representative subsequences on the right side. The same is true for Figs. 1 , 10 ,and 11 . This is important for ensuring that the labeling (see x-axis in the left panel) represents the effectiveness of the algorithm on unseen data.

The time series is labeled using the index of the representative cluster. It can be seen that windows that have the high peak in their center or close to their center are correctly labeled as 1. Other windows are labeled as 2. Note that the time series extends to beyond the boundaries of the plot and labels may represent information that is not visible within the window that is displayed. All windows in this series have been assigned a label, and none of them is considered to be noise. This is unusual for a real time series and is an indication that the ecg series consistently shows patterns that can be clearly distinguished from noise. Despite the obvious patterns in the time series, k -means results display the usual sinusoidal behavior. Cluster centers for k = 2 are shown at the bottom right.

Figure 10 shows the artificially created cbf toy series. Again the RD cluster representatives clearly show the patterns in the data, while the k -means cluster centers do not. Labeling was done using f w = 0 . 75 as before. The time series on the left shows some misassigned windows, but mostly appropriate labeling. The misassignments can be understood by recognizing that only subwindows of length 12 have to match the representative center. Figure 8 shows all cluster centers that are labeled as representing the cylinder pattern. Clearly some of these centers show similarity to either of the other shapes. Such misassignments can be avoided if a larger value of f w is used. A sharp increase or decrease at the edge of the window would then be considered a separate representative cluster that is not limited to any one of the distinct patterns.

As a final example we consider the glassfurnace series. The series has characteristic properties, as can be seen in the left panel of Fig. 11 . The representative RG cluster centers show those properties well, while the k -means windows show oscillations that are not clearly related to the time series. Since the series has a rather erratic behavior, the patterns of the cluster centers are less clearly recognizable in the series. For this series the labeling was done using f w = 0 . 5, which means that only half of the window has to match a cluster center for it to be labeled as such. For f w = 0 . 75 nine representative clusters would be selected by the algorithm. It can be seen that for this time series many windows remain unassigned. It is not unusual for real time series data that windows are not being labeled, but for the glassfurnace data set the number of unassigned windows is exceptionally large. The next section will discuss this behavior quantitatively. 4.3 Experimental process The experimental process can be motivated through Figs. 9 , 10 and 11 . In these Figures, the number of windows that are assigned to clusters compared with the total number can be viewed as a sensitivity measure. In Fig. 11 , 24 out of 40 windows are given the label of a glassfurnace cluster, the time series that was used in the clustering. This can be interpreted as a sensitivity of 0.6. Note that the clustering and labeling are done based on non-overlapping parts of the sequence, in accordance with what is expected in a classification-based evaluation.
If we had tried to label the same time series using clusters from the ecg series we would have found only one match corresponding to a specificity of 1  X  1 / 40 = 39 / 40 = 0 . 975. These two time series are very different and not all real time series show such specific patterns. To get a fair estimate of the number of inappropriately assigned clusters, we used a total of 14 time series in the comparison.
 Formally sensitivity and specificity are defined as follows where TP (true positive) instances are subsequences that are assigned to a cluster and come from the same source as was used for clustering; FN (false negative) instances are not assigned to a cluster although they come from the same source; TN (true negative) instances come from a different source and are (correctly) not assigned to a cluster; FP (false positive) instances come from a different source than was used in the clustering, but are (incorrectly) assigned to a cluster.

Note that the time points used in testing are not part of any clustering step. For each time series the first 500 time points are used for the clustering. Subsequences are extracted from the next 500 time points and are labeled based on the clusters that were created from the first 500 time points. 4.4 Sensitivity and specificity results All subsequences were tested against all clusterings and results are listed together with the name of the data set that was used in clustering. Figure 12 shows a scatter plot of specificity against sensitivity for RD clustering. Three types of data are distinguished. Synthetic signal data is shown in black and occupies the top right corner (sensitivity = 1 and specificity = 1), corresponding to excellent performance. The results for seven of the real data sets, shown as blue/gray, are scattered in the upper right section of the plot, corresponding to consistent, albeit not perfect, performance. Note that circles ( w = 8) are primarily scattered around the periphery, whereas results for longer subsections, diamonds (w=16), squares ( w = 32), triangles ( w = 64), and stars ( w = 128), are found more frequently close to the top right corner. Only one real data set, the ecg data set, has excellent performance even for w = 8, but many data sets have similar performance for longer subsequences.

Four data sets (white symbols) have a performance that is not substantially different from what would be expected of a clustering that randomly groups data (solid black line): the buoy , steamgen , stock ,and random walk data sets. For the random walk we expect to see this: The algorithm is designed to not return clusters that cannot be distinguished from random walk behavior. Note that the evaluation is done on separate data. Nevertheless, not a single cluster was identified for random walk data at any window size, and consequently the sensitivity is 0 and specificity is 1 for all window sizes. This can be seen as confirmation of the effectiveness of the algorithm in the limit of completely random behavior.

Other poorly performing data sets share a small sensitivity and high specificity at small window sizes. For the stock data set the sensitivity is &lt; 0 . 2 for all but w = 128. That means that under our clustering algorithm, and for window sizes w  X  64, the stock series shows properties that barely differ from a random walk series. For w = 128 clusters are returned but they all have large radii (more than half of the maximum distance within the data set). As a result we observe that while 0 . 42 of unseen subsequences from the stock series fall into clusters that are identified at that window size, so do X  X n average X 0 . 32 of subsequences from other time series. The buoy and steamgen series also show random-walk -like behavior for small window sizes, and non-specific clusters with large radii otherwise. We considered automating the recognition of poor clusterings but have not so far implemented this.
Figure 13 shows the corresponding scatter plot for kernel-density-based clustering. For synthetic signal data, results are largely the same as for RD-clustering. This is not surprising plot, corresponding to moderate performance. Longer subsequences, some of which showed impressive performance in RD clustering, are now mostly absent from the top right area. Some diamonds ( w = 16) and one square ( w = 32) can be identified on the top left of the plot, having a sensitivity of between 0.1 and 0.4, and there is one diamond in the top right ( ecg , w = 16) corresponding to near-perfect performance. However, the majority of results for real data with w&gt; 8 clump in the top left corner, corresponding to sensitivity = 0, and specificity = 1, which can be considered a useless result. For w = 64 all sensitivity values for real data vanish.

Figure 14 shows averaged results as a function of the window size. Three groups of data sets were distinguished for this purpose. Synthetic signal data include the sine , sawtooth ,and rectangle data sets. For these data sets sensitivity and specificity is consistently excellent for both algorithms. As a visualization tool, real data were grouped into good performers ( ecg , balloon , glass , speech , earthquake , ocean shear ,and darwin ) and poor performers. Good performers show consistently high sensitivity and specificity. Both measures increase up to a window size of w = 64 and then decrease. For kernel-density-based clustering, in contrast, the sensitivity decreases sharply for w&gt; 8. The average sensitivity of RD clustering for window sizes greater than 8 was consistently higher than that of kernel-density-based clustering at any window size.

Those real data sets that did not show a convincing performance in RD clustering form the third group ( buoy , steamgen , stock ). For these data sets the sensitivity in RD clustering increases with window-size, but that increase in sensitivity is compensated by a decrease in specificity, leading to a result that is overall not useful. These three data sets also have a negligible sensitivity in kernel-density-based clustering.

Ta b l e 1 lists the full experimental results. It confirms that while for w = 8 the results for the RD algorithm are comparable to kernel-density-based clustering, the sensitivity of kernel-density-based clustering decreases strongly for increasing window size. A comparison with results from [ 10 ] shows that automatically setting the kernel width in kernel-density-based clustering, see Sect. 3.5 , overall improves the results. Note that this automated choice of parameters did not only improve the quality of results, but that it removed any ambiguity about parameter choices. This resolves one criticism of kernel-density-based clustering, which is that the need to chose two parameters makes the algorithm hard to use. Our results do, at the same time, identify different problems with kernel-density-based clustering in regard to high dimensionality: In a high-dimensional data set a Gaussian kernel function is inappropriate to delineate clusters. In most cases, in which Table 1 shows a sensitivity of 0, kernel-density-based clustering identified a single broad peak that was not distinguishable from what a random data set would have produced. 4.5 Efficiency Test runs were done on a system with a mobile Pentium 4 processor with 1.83 GHz and 1 GB RAM. Figure 15 shows that RD clustering has an essentially quadratic scaling with the number of data points. For this paper we did not make an attempt to improve over this default scaling, since the clustering of all 14 data sets, as well as testing all data sets against each clustering takes no more than 3 X 4 min.

It should be noted that RD-clustering is 1 X 2 orders of magnitude faster then kernel-density-based clustering. Furthermore, the memory requirements of kernel-density based clustering exceeded the available memory for data sets with 1,024 or more data points while RD clustering could be performed in memory for up to 4,096 data points. For kernel-density-based clustering we saw additional challenges besides the overall lower speed. A trivial estimate of the complexity would find a scaling for both kernel-density-based clustering and RD clustering that is largely independent of dimensionality, i.e., window size, since the kernel-density algorithm we use is based on hill climbing starting from each data point rather than mapping out the feature space. However, it has to be considered that for high dimensionality the probability increases that the hill climbing is slowed down dramatically through saddle points in the density landscape. At our largest window size, w = 128, the density-based calculation did not terminate in reasonable time for one of the data sets ( darwin ). This problem is significantly more pronounced in high dimensions, since only stationary points with all negative eigenvalues are maxima. Stationary points with positive and negative eigenvalues are saddle points, and the number of combinations of positive and negative signs increases as 2 d  X  2 with the number of dimensions, d .

Figure 15 also shows that the analytical evaluation of the radial distribution function for random data is significantly faster than the computational evaluation. For this comparison the numerical evaluation is based on 512 data points, which proved to result in a satisfactory approximation to the analytical distribution function.

It can, furthermore, be seen that the assignment of labels has a slightly better than quadratic scaling. The scaling is expected to be linear in the number of tested points, which was chosen to be the same as the number of points used for clustering. The labeling also scales linearly in the number of cluster centers which increases with the number of subsequences used in clustering. The increase in execution time for deriving the cluster X  X abel mapping is another consequence of the typically increasing number of cluster centers with increasing number of input sequences. 5 Conclusions and outlook We have introduced an algorithm that specifically addresses the goal of identifying source-range of window sizes, providing both high sensitivity and specificity in recognizing data from the same source. We have shown that kernel-density-based clustering loses its ability of identifying clusters in real-world data at window sizes of about 16  X  32. For RD clustering, in contrast, the best performance was seen for window sizes of w = 16 to w = 128. Both clustering algorithms produce excellent results for synthetic signal data, for all window lengths, confirming that even kernel-density-based clustering has, in principle, the ability of finding source specific clusters for any window size. Among real-world data sets, three were found not to provide satisfactory results using either algorithm, one of those stock market data. Seven data sets showed consistent performance, with an average sensitivity of more than 80% for window sizes w&gt; 8, and specificity &gt; 90% for all window sizes.
Much of our approach could be applied beyond the task of time series subsequence clus-tering since many assumptions are also relevant to other clustering settings, in which noise is an important feature, such as clustering of gene expression data [ 11 ].Evaluationinthe time series subsequence context is particularly suitable for several reasons. The sliding win-dow approach allows defining the problem of time series subsequence clustering in arbitrary dimensions (8 X 128 dimensions are used in this paper). Furthermore time series subsequence clustering is considered especially challenging. Finally, many time series data sets are avai-lable, providing ample opportunity for testing. Future work will show how well the algorithm generalizes to other settings.
 References Author Biographies
