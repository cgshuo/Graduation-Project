 Zhengdong Lu zhengdon@csee.ogi.edu Todd K. Leen tleen@csee.ogi.edu Yonghong Huang huang@csee.ogi.edu Deniz Erdogmus derdogmus@ieee.org Time series classification is a supervised learning prob-lem aimed at labeling temporally structured sequences of variable length. The most common approach re-duces time series classification to a static problem by suitably transforming the input sequences into vectors in Euclidean space. One can either summarize each time series with attributes pertinent to classification (called feature extraction )(Keogh &amp; Pazzani, 1998), or use a properly sampled and aligned subsequence (called sampling )(Parra et al., 2003). Unfortunately, the feature extraction method is still more art than sci-ence, and the performance depends heavily on the de-signer X  X  domain knowledge and the particular heuristic implemented. The sampling method, although pre-serving most of the information, is accused of ignoring the important temporal structure of the series. In-deed, the sampled sequences, if treated as vectors in Euclidean space, lead to the same classifiers after any permutation of the vector entries. Moreover, the sam-pling strategy does not apply to situations where we have only sparse observations that are made at irreg-ular times.
 In this paper, we propose a principled non-parametric distance measure for time series by representing each time series with a smooth curve in a reproducing kernel Hilbert space (RKHS) with a kernel learned from data. This new distance measure circumvents the limitations of the two above mentioned strategies.
 Paper Roadmap In Section 2, we give the back-ground of the Bregman divergence, and then generalize it to function space for a proper distance measure of smooth curves. In Section 3 we propose a family of new distance measures for time series with only dis-crete observations. Section 4 is devoted to the non-parametric mixed-effect model, which helps to further specify the proposed distance measure. In Section 5, we apply the proposed distance measure to two real-world time series classification problems. Finally we discuss the related work in Section 6. The Bregman divergence is a natural generalization of squared Euclidean distance and KL-divergence. A Bregman divergence corresponding to a strictly convex function  X  ( x ) (called seed function) is defined as d  X  ( x 1 || x 2 ) =  X  ( x 1 )  X   X  ( x 2 )  X  X  X   X  ( x 2 ) , x Bregman divergence is closely connected to the expo-nential family (Banerjee et al., 2005). For any distri-bution in the exponential family we know that the log likelihood can be re-written as where  X  is the conjugate function of  X  and (  X  ) =  X   X (  X  ) is the expectation parameter corre-sponding to  X  . We go one step further to argue that d ( x 1 || x 2 ) should be a proper model-weighted diver-gence measure between any x 1 and x 2 . It is straight-forward to show that for multi-variate Gaussian dis-tribution N ( a,  X ), the corresponding Bregman diver-gence is given by which is also suggested in (Tipping, 1999) as a model-weighted distance for Gaussian distribution. 2.1. Extension to Function Space We generalize our discussion on the Bregman diver-gence and the exponential family to function spaces. To facilitate our discussion, we adopt the language of functional integral, which, although allegedly not rig-orously defined, provides a powerful technique for de-scribing the probability on functions (Simon, 1979). Gaussian processes (GPs) (Rasmussen &amp; Williams, 2006) generalize the multivariate Gaussian distribu-tion to function space, which model any function f with f 0 being the mean function and |||| H the norm for the reproducing kernel Hilbert space (RKHS) H . We use K to denote the reproducing kernel, which will also be noted as the covariance function for the Gaussian process expressed in Eq.(5) (Seeger, 2004). In regularization theory, the norm |||| H is often related to a particular type of smoothness of function, with large (even infinite) || f || H for non-smooth function f . After generalizing Eq.(1) to the functional case (Frigyik et al., 2006), we get the Bregman divergence between function f 1 and f 2 , with a seed functional g [ ] d ( f 1 || f 2 ) = g [ f 1 ]  X  g [ f 2 ]  X  where Dg [ f ] is the Fr  X echet derivative. The Gaussian process expressed in Eq.(5) can be viewed as a member of the exponential family extended to distributions on functions (Altun et al., 2004). Then a direct general-ization of Eq.(3) leads to g [ f ] = 1 GP-related divergence for smooth functions We consider k time series, using y i to denote the N i observations from the i th time series made at times t i The subscript i on t i and N i indicates that the obser-vation times and even the number of observations are generally different for each individual. The time series are called synchronized if all the t i are the same. We can define a distance measure for such time se-ries by associating the observations { t i , y i } with a (smooth) curve. We assume the observations for each individual i is generated from a independent Gaussian process f i with the same covariance function K (and therefore H ) and mean f 0 . The observation is modeled as where  X  in is a white observation noise with standard deviation  X  for all i and n .
 We choose to summarize each individual time series i with the expectation of f i ( t ) given the discrete noisy observation { t i , y i } .  X  f ( t ) = E [ f i ( t ) | y i , f 0 ; t i , K ] (8) where f 0 ,i . = [ f 0 ( t i 1 ) , f 0 ( t i 2 ) , , f 0 ( t ues of f 0 at times t i , and K ( t i , t i ) is the N i  X  N matrix with the ( n, m ) entry being K ( t in , t im ). With a smooth f 0 , we have ||  X  f i || H &lt; +  X  , which can be loosely interpreted as that  X  f i is smooth according to K . In Fig.1, we give an example of using such a curve  X  f to represent the noisy observations (black crosses). We then use the distance between  X  f i and  X  f j as the distance between time series { t i , y i } and { t j , y j which is given by Eq.(6) as Since H is the RKHS induced by the kernel K , this distance measure is well-defined d = = where v i = ( K ( t i , t i ) +  X  2 I )  X  1 ( y i  X  f 0 ,i reproducing kernel property the distance measurement can be simplified as d It is important to note that this distance does not re-quire all the time series to be synchronized, an advan-tage when sequences are of different lengths, or the observations are made at different times, as shown in our first experiment in Section 5. When the observa-tions for all individuals are synchronized, we have t i = t = [ t 1 , t 2 , , t N ] T with N as the total number of ob-servations for each individual. Letting K = K ( t , t ), we can re-write d ij as d Temporal Structure In Eq.(11)-(14), the temporal regularity is incorporated in the distance via the kernel K . It is most clear when we notice that K models the correlation of f value at different time The norm || f i  X  f j || H measures the irregularity de-fined by K , in contrast to the Euclidean distance R wise difference between f i and f j . It is also important to notice the particular temporal structure incorpo-rated varies greatly with the choice of K . For example, the widely used Mat  X ern (including Gaussian) kernel or rational quadratic kernel promote different types and level of smoothness. On the other hand, the temporal structure is often problem specific and hard to deter-mine beforehand. In the next section, we will discuss learning this temporal structure from the data. In Section 3, we assume a Gaussian process with known mean and covariance function. However in practice it is often not the case. Instead we may want to learn the characteristic of Gaussian process from examples. One situation of interest to us is when a population of similar time series are available. This prior learning scheme is known in statistics as the em-pirical Bayesian or the hierarchial Beyesian (Gelman, 2004). Particularly, the model is called mixed-effect model when the hyper-prior is a Gaussian, on which the maximum likelihood (ML) solution can be found with Expectation-Maximization (EM) algorithm. Traditional mixed-effect models are parametric, which assume a  X  -parameterized regression model for each individual. Since the model parameters vary across individuals, it is natural to consider them generated by the sum of a fixed and a random piece  X  =  X  +  X  i , where  X  is called the fixed effect , and  X  i , called random effect , is assumed distributed N (0 , D ) with unknown covariance D . The fitting of mixed-effect model is to find  X  , D , and the variance of observation noise. In non-parametric mixed-effect models, the individual regression models do not take a parametric form. In-stead, we assume the observations are generated by k smooth curves { f 1 , f 2 , , f k } fluctuating around a mean (fixed-effect) function f 0 . We use e f i = f i  X  f to denote the deviation of f i from f 0 (random effect). The prior of both f 0 and e f i can be summarized with the following equations: where H and H 0 are generally different Hilbert spaces, with the corresponding reproducing kernel denoted as K and K 0 . Also we assume the observation noise to be white Gaussian with variance  X  2 , from which follows p ( y i | e f i , f 0 ; t i )  X  We assume H 0 (and thus the form of p 0 [ ]) is pre-determined, while the fixed effect f 0 is to be de-cided. Also unknown are the noise variance  X  2 and the Hilbert space H for random effects (or equivalently K ). Our learning task is therefore to jointly optimize over { f 0 , K,  X  } by maximizing the following probability of Y = { y 1 , y 2 , , y k } . p ( Y | f 0 ; K,  X  ) p 0 [ f 0 ] = where the integral over  X  (Simon, 1979). Using the Gaussian property, Eq.(17) can be further reduced to a standard integral p ( Y | f 0 ; K,  X  ) p 0 [ f 0 ] = where f i = [ e f i ( t i 1 ) , e f i ( t i 2 ) , , e f i tivariate Gaussian p ( f i ; K ) = In general, there is no unique solution of K that max-and time index ( n, m ), we will have This situation can be circumvented in two ways. First we can restrain K in a particular parametric family, such as the widely used Gaussian kernel. Second, we can instead optimize only over the entry K ( t in , t im ) for all individual i , and time index ( n, m ). Both strategies will be addressed in this paper. 4.1. Optimization with the EM Algorithm The task is to find the set M = { f 0 , K,  X  } that shown in Eq.(18), we can rewrite the data likelihood variables which enables us to employ the EM algorithm in find-ing M . In the following, we will give the results of the expectation step (E-step) and the maximization step (M-step).
 E-step: In each EM iteration:
Q ( M , M g ) = E { f where M g stands for the parameters from the last iteration. After some algebra, we can re-arrange Q ( M , M g ) into the following form
Q ( M , M g ) =  X   X  M-step: In M-step, we find the and use M  X  to update the model parameters. The op-timization in Eq.(22) can be divided into two separate parts. The first three terms on the left hand side of Eq.(21) is a function of only ( f 0 ,  X  ); The last (fourth) term is a function of only K . To find the solution of f 0 and  X  , we need to solve the following optimization problem: (  X   X  , f  X  0 ) = arg min 1 Particularly, with any fixed  X  , maximizing Q ( M , M g ) over f 0 becomes a regularized regression problem f  X  0 = arg min 1 The optimization over K is K = arg max where K is the set of feasible K , and i is the posterior mean E [ f i | y i ; M ] that can be calculated as and C i is the posterior covariance of f i C i = K ( t i , t i )  X  K ( t i , t i )( K ( t i , t i ) +  X  4.2. Parametric Covariance Estimation We assume the covariance function K is of the para-metric form K ( x, y ;  X  ). For example, the Gaussian ker-nel with scale a and kernel width s or as suggested in (Lanckriet et al., 2004) a convex combination of a set of kernels { K 1 , K 2 , , K M } K ( x, y ;  X  ) =  X  1 K 1 ( x, y )+  X  2 K 2 ( x, y )+ +  X  M In this case, the optimization of K in the M-step can be reduced to the following parameter estimation  X   X  = arg max form of K is appealing in either one of the following two situations:  X  when the observation are sparse, since the para- X  when the time series are not synchronized (as in 4.3. Non-parametric Covariance Estimation When all the time series all synchronized, we have t i = with K  X  K ( t , t ), and rewrite the optimization into the matrix form K = arg max If we let P be the set of positive definite matrix, the solution of Eq.(27) is simple The non-parametric fitting of kernel matrix K is ap-pealing since it does not assume a particular form for the covariance matrix and thus can fully exploit the in-formation in the samples. However it can only be used when the time series are synchronized. One example of this modeling choice is given in Section 5.2. We tested the proposed distance measure on two real-world applications. The first one is an algorithm for cognitive decline detection based on longitudinal clin-ical observations of motor ability. The second one is an target identifier system based on electroencephalo-graph (EEG) signal.
 In each experiment, we employ support vector machine (SVM) (Burges, 1998) with Gaussian kernel defined as follows where d ij is the squared distance between the time series i and j and the kernel width r is usually obtained using cross-validation. It is easy to see the G is a Mercer kernel. 5.1. Cognitive Decline Detection Based on Research by our group and others show that motor changes, such as in walking and finger tapping rates, can effectively predict cognitive decline several years before impairment is manifest (Camicioli et al., 1998). It would be useful to build a system to detect cogni-tive decline (at least partially) from motor behavior, since they can be obtained by unintrusive in-home as-sessment (Hayes et al., 2004). Our research focuses on using clinical motor behavior and data from the Ore-gon Brain Aging Study (OBAS) (Green et al., 2000). All 143 subjects in the cohort were healthy at entry, and when the data were drawn 46 of them had de-veloped into mild cognitive impairment, while 97 re-mained cognitively healthy. We divide all the subjects into the impaired group and the normal group accord-ing to their state when the data were drawn from the would develop into cognitive impairment based on his or her motor behavior before a clinical diagnosis (if any). In this experiment, this task reduces to pre-dicting the group membership for each subject. This classification is difficult due to the fact that motor ob-servations are sparse and noisy, as shown in Fig.2(left panel). We examined four motor behaviors summa-rized in Table 1. Usually as the subjects age or be-come impaired, the seconds and steps increase, while tappingD and tappingN decrease. We fit the non-parametric mixed-effect model to each motor behavior with the parameterized kernel where s 0 is predetermined and { a, s } are to be learnt. The right panel of Fig.2 shows the seconds time series from the 143 subjects (black  X  X  X  X  ) and the fit fixed effect (red line). Once the model is fit, the distance between any two subjects i and j is calculated as in Eq.(11).
 For comparison, we also examined a parametric feature based on the least-square (LSQ) fit coefficients for lin-ear regression: x i = arg min x by the observation that the intercept and the slope of the motor behavior trajectory are predictor of fu-ture cognitive decline and dementia (Marquis et al., 2002). Based on the LSQ feature we get another dis-tance measure d ij = || x i  X  x j || 2 . We employ a SVM as the classifier with kernels calculated with Eq.(29). Fig.3 compares the ROC curves using the proposed distance measure and the Euclidean distance between the LSQ features. It is clear that SVM with proposed distance measure outperforms the SVM with the LSQ features in terms of the area under curve (AUC). There are two reasons for the superiority of the proposed dis-tance over the LSQ feature:  X  The simple heuristic features such as the intercept  X  The feature extraction is not robust enough for 5.2. EEG-based Image Target Detection The system reported here exploits the perceptual ca-pabilities of expert humans for searching objects of interest (e.g., a golf course in a satellite image) within large image sets. The technique uses event related po-tentials (ERPs), neural signals linked to critical events, such as interesting/novel visual stimuli. The basic idea of the ERP-based image triage system is to collect electroencephalograph (EEG) signals from a subject X  X  scalp when he or she performs visual target detection, and then detect the ERPs associated with the target stimuli. We focus on single-trial ERP detection using 32 EEG sensors, which is challenging due to the low signal-to-noise ratio.
 This detection task is then boiled down to classifying the EEG segments into target-associated EPRs and distractors. After proper alignment and sampling, the EEG segments are transformed into synchronized se-quence of length 4128, which are denoted y i for each individual trial i . In this experiment, we collected the EEG data from three human experts, each of them performed 1 training session and 7 test sessions. In each training session, the human expert was fed with  X  600 images with  X  50 targets among them. In each test session, there are 1-4 targets within  X  3000 distrac-tors. Fig.4 (left panel) shows single-trial EEG signals associated with a target and a distractor stimulus. Due to the high dimensionality, the EM algorithm will be fairly slow due to the extensive use of inverse of K (4128  X  4128). To keep the computation at a rea-sonable level, we simplify the model by assigning a flat prior to the fixed effect f 0 , or equivalently letting || f || H instantly leads to the following results.  X  The optimal solution of f 0 is simply the data mean  X  The data likelihood is independent of  X  2 as long Based on the above two results, we can pick a  X  and then calculate the optimal covariance K with Eq.(28) in one iteration. Once the optimal f 0 and K are obtained, the distance between any time series i and j can be calculated us-ing Eq.(14). In addition to directly using the distance, we isometrically embed the time series { y i } into Eu-clidean space while preserving the distance expressed in Eq.(14). The embedded vectors, called ISO feature , will then be used directly in linear classifiers. One obvious choice is the non-degenerated linear transfor-mation where K 1 2 could be any matrix A  X  R N  X  N with AA T = K . We tested both the proposed distance and the (squared) Euclidean distance 4 || y i  X  y j || 2 as the distance term d ij in the Gaussian kernel G and compared the performance of the SVM with the two distance measures. In addition, we also tried a linear logistic classifier (LLC) with both the raw feature y i and ISO feature x i as the input. In our experiment, the SVM parameters and kernel width were selected using 10-fold cross validation.
 Due to the extremely low probability of targets and the high cost of misdetection, we aim for zero-miss and minimum false alarm rate (MFAR), which is defined as the percentage of false alarms among all classifications while all targets are correctly detected. We test both SVM and LLC on the 21 (=3  X  7) test sessions. Ta-ble 1 summarizes the detection results when different distance or features are used. The criteria of compari-son include the average MFAR across the 21 sessions, the number of sessions with low MFAR (  X  10%)and very low MFAR (  X  2%). Clearly, the LLC with ISO features outperforms the LLC with raw feature by giv-ing low average MFAR, more low MFAR sessions, and more very low MFAR sessions. The story is similar when using SVM as the classifier: the proposed dis-tance outperforms the the Euclidean distance on all three criteria.
 Clearly the temporal structure is important in de-scribing the EEG signal, and thus plays a crucial role in deciding the distances between EEG time series. The proposed distance measure successfully incorpo-rates the temporal structure information learnt with a rather simple algorithm, and yields significantly better classification than the Euclidean distance that simply adds the index-by-index differences. The connection between Bregman divergence and ex-ponential family is first proposed by (Forster &amp; War-muth, 2000), and later used by several authors in de-riving a proper distance measure for either clustering (Banerjee et al., 2005) or dimension reduction (Collins et al., 2001). Our work also depends heavily on the functional Bregman divergence, an idea first fully ex-plored in (Frigyik et al., 2006). The non-parametric LLC(I) 8.99% 12 16 LLC(R) 18.18% 2 12 SVM(P) 4.91% 13 19 SVM(E) 6.31% 7 16 mixed-effect model is a natural generalization to the hierarchical Bayesian Gaussian process proposed by (Schwaighofer et al., 2005) to functional form where synchronized and non-synchronized time series can be treated in a unified framework.
 This work can be viewed as a particular example of the functional data analysis (Ramsay &amp; Silverman, 1997). Particularly, in an early effort towards the functional PCA (Ramsay &amp; Dalzell, 1991), the authors suggested to map the discrete observations ( t i , y i ) to a smooth function through the following regularized regression  X  f i ( t ) = arg min f where D is a linear operator. The solution to Eq.(31) is the expectation in Eq.(9) if we let  X  =  X  2 and K be the Green X  X  function of the operator D  X  D . The difference, however, are that (1) our model also assumes a non-zero mean (fixed effect) f 0 and (2) the kernel K is learned from a population of time series.

