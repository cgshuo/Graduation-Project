 A fundamental building block of many data mining and analysis ap-proaches is density estimation as it provides a comprehensive statis-tical model of a data distribution. For that reason, its application to transient data streams is highly desirable. A convenient, nonpara-metric method for density estimation utilizes kernels. However, its computational complexity collides with the rigid processing re-quirements of data streams. In this work, we present a new ap-proach to this problem that combines linear processing cost with a constant amount of allocated memory. Our approach also supports a dynamic memory adaptation to changing system resources. G.3 [ Probability and Statistics ]: Nonparametric statistics Algorithms, Performance Data Streams, Kernel Density Estimation
The mining and analysis of transient data streams has come to the fore in recent years [3]. To be applicable to data streams, a mining technique has to meet stringent processing requirements [2].
Taking those requirements into account, we address in this work the adaptation of a fundamental building block of many analysis techniques, namely density estimation, to the data stream scenario. Density estimation reveals the unknown proba bility density func-tion of a distribution, given solely a representative sample of obser-vations. With a well-defined density estimate at hand, a variety of mining and analysis issues can be addressed [5], [4].

In most real-world applications over streams, we have no a pri-ori knowledge about the stream. Hence, the class of nonparamet-ric density estimation approaches is very appealing as they make no assumptions on the unknown density function;  X  the data speak strictly for themselves  X  [5]. A theoretically well-founded and also practically approved approach for nonparametric density estima-tion utilizes kernels. Kernel-based density estimators can approxi-mate any distribution arbitrarily good (i n probabilistic terms), pro-vided that the associated sample is sufficiently large [5]. Hence, an adaptation of kernel density estimation to data streams seems to be a highly promising approach. However, the high computational cost of kernel density estimators is a severe obstacle; their mem-ory allocation is linear in the sample size, accompanied by linear evaluation cost. Since these facts violate the given processing re-quirements, we can not directly build kernel density estimators over data streams.

In this paper, we present resour ce-aware kernel density estima-tors over streaming data that fully comply with these processing requirements. We build on the general idea of M-Kernels [1], a previous approach for kernel density estimation over data streams. Specifically, we solve crucial deficiencies of M-Kernels concerning their parameter settings and essential processing steps. For the core operation of the algorithm, the merge of two adjacent kernels, we introduce a new optimal method based on a cost measure which is inexpensive to compute. An experimental study confirmed that our estimators perform well for synthetic and real-world data streams; they outperformed M-Kernels with respect to accuracy and main-tenance cost.
A one-dimensional data stream consists of an unbounded se-quence X 1 ,X 2 , ... of numbers with X i  X  R for i  X  N . Except where otherwise stated, we assume that the stream represents at each time instant a sample with independent and identically distrib-uted ( iid ) observations of an unknown continuous random variable X . The premise of independence of two arbitrary stream elements is reasonable for most applications as data sources typically send their elements autonomously, e.g. traffic sensors. The premise of an identical distribution is weakened in Section 3.
One of the core concepts in mathematical statistics is the proba-bility density function (pdf). Essentially, each continuous random variable X has a unique pdf f .As f provides a comprehensive summary of X , its knowledge is crucial to the analysis [4]. In real-world scenarios, however, neither X nor its pdf are known. Typically, we only have observations of X in form of a sample X , ..., X n . Density estimation provides suitable methods to esti-mate a pdf using a sample as major ingredient. We particularly fo-cus on kernel density estimation [5], a nonparametric approach that solely bases on the sample. A kernel density estimator (KDE) with kernel function K and bandwidth h ( n ) is defined as for iid observations X 1 , ..., X n drawn from a continuous random variable X , whose pdf f is unknown. Generally, the setting of the bandwidth is crucial to the quality of a KDE. To guarantee proba-bilistic convergence, the bandwidth has to decrease with the sample size [5]. Contrary to the bandwidth, the setting of the kernel func-tion is minor.

Since we assume a data stream to be an iid sample of an un-known random variable X , the adaptation of kernel density esti-mation seems straightforward. However, the computational cost of KDEs collides with the processing requirements of streams: they request memory linear in the sample size, i.e., in the size of the stream. Furthermore, each processed stream element must be ac-cessible anytime because common bandwidth strategies require ac-cess to the whole sample.
In the following, we present our approach to KDEs over stream-ing data, starting from (1) as point of origin. Let n be the current number of processed stream elements.
As underlying kernel function, we use the Epanechnikow ker-nel [5]. Not only is this kernel asymptotically optimal among all kernels, it is also simple and has a bounded support.

Concerning the bandwidth, the vital parameter of a KDE, we use the normal scale rule [5]. It defines the bandwidth as h ( n ) =1  X   X  ( n ) .Since  X  ( n ) can be estimated in a single pass, we can apply this rule to streaming data.
Kernel entries are the main building blocks of our KDEs. A kernel entry X ( n ) i ,c ( n ) i ,L 2 costs ( n ) i consists of a mean X bor X . As we are only allowed to allocate a constant amount of memory [2], we confine the maximum number of kernel entries to a constant m . We utilize the entirety of kernel entries to build a KDE: Note that each kernel entry has weight 1 /n in (2). By using expo-nentially decreasing weights for older entries instead, we can em-phasize recent data. Thus, we can also cope with evolving streams.
Let us now discuss the processing of kernel entries during run-time. For a new stream element X n +1 , we build a new kernel entry
X n +1 , 1 ,L 2 costs ( n +1) , provided X n +1 is not equal to the mean of an existing kernel entry. If that is the case, we only increment the weight of this entry.

If the total number of kernel entries exceeds m , we determine and merge the adjacent kernel entries X ( n ) i ,c ( n ) i
X kernel is defined as X  X  ,c ( n ) i + c ( n ) j ,L 2 costs is the minimum of
L 2 costs ( X )= Due to the simple form of the Epanechnikow kernel, (3) can be converted into a closed formula. We determine for each pair of adjacent kernel entries their merge kernel and set their merge costs as L 2 costs ( X  X  ) . If a merge is required, we choose the pair with overall minimum merge costs.

Generally, our KDEs can flexibly adapt to a changing m .If m is increased, we build a separate kernel entry for each new element. If m is decreased, we merge kernel entries sufficiently often. The processing of M-Kernels [1] is similar to that sketched above. However, some of their settings severely limit their applicability. First, their use of the Gaussian kernel as underlying kernel function exacerbates an efficient evaluation due to its unbounded support. Second, the proposed bandwidth strategy does not ensure band-widths decreasing with the sample size; but this is a fundamental prerequisite for the probabilistic convergence of KDEs. Third, the mean of the merge kernel is numerically approximated which in-troduces additional computational effort and less exact values.
We scrutinized our approach in an experimental study with syn-thetic as well as real-world data. Concerning the estimation quality, our KDEs have proved to be very robust. For all examined streams, their estimation error with respect to the best  X  X ffline X  KDE rapidly decreased for an increasing number of processed elements. In com-parison to, M-Kernels were clearly inferior; their estimation error mostly remained constant in the average. M-Kernels were also in-ferior in terms of processing time. In another experiment, we ex-amined the resource-awareness of our KDEs by abruptly varying their maximum number of kernel entries during runtime. The ex-perimental results indicate that our KDEs reacted very flexible and recovered fast, even after significant reductions of m .
In this work, we tackled kernel density estimation over stream-ing data. Our approach utilizes simple building blocks, namely kernel entries, to build KDEs anytime while processing the stream. In comparison to M-Kernels, a previous kernel method for data streams, our KDEs were clearly superior as our experimental study revealed. In future work, we aim at generalizing our approach to multidimensional data streams. We will also couple our KDEs with change point detection methods from the area of mathematical sta-tistics in order to locate and react to concept drifts in the stream. This work has been supported by the German Research Society (DFG) under grant no. SE 553/4-3. [1] Z. Cai, W. Qian, L. Wei, and A. Zhou. M-Kernel Merging: [2] P. Domingos and G. Hulten. A general framework for mining [3] M. Gaber, A. Zaslavsky, and S. Krishnaswamy. Mining data [4] C. Heinz and B. Seeger. Exploring Data Streams with [5] B. Silverman. Density Estimation for Statistics and Data
