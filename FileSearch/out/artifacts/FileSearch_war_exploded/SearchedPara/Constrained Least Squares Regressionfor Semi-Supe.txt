 Lack of sufficiently labeled data is a big problem when building supervised learner in real applications. Semi-supervised learning (SSL) can bridge the gap between labeled and unlabeled data, as it combines limited labeled samples with rich unlabeled samples to enhance the learner X  X  ability [20]. As an important branch of SSL, graph based semi-supervised learning (GSSL) propagates the su-pervisory information (class labels) on a pre-defined graph and aims to make the similar samples share the common labels [12]. Under the cluster assumption [4] or the manifold assumption [8], there are many GSSL methods have been pro-posed, including Gaussian fields and harmonic functions (GFHF) [21], local and global consistency (LGC) [19], manifold regularization [2], and etc. For GSSL, there are several key issues to be solved including graph construction, labeled sample selection, learning model formulation, parameter adjustment and etc. In this paper, we will limit to highlight the former two issues.
An adaptive graph construction is a main challenge of GSSL. Neighborhood-driven methods (e.g., k -nearest-neighbors ( k -NN) [16], -ball neighborhood [1] and b -matching graph [7]) are unable to reflect the overall views of data and sen-sitive to noise. Recently, some research ers formulate the graph building process into a subspace learning problem. Under the subspace assumption, each sample can be represented as a linear combination of other samples, and intuitively, the representation coefficients could be accepted as a proper surrogate of similar-ity metric. In the literature, this measur ement is referred to as self-expressive similarity [6]. There are several methods such as sparse representation (SR) [6], low-rank representation (LRR) [10], least squares regression (LSR) [13] to obtain the representation coefficients.

Although these approaches have gained great effects in some domains, there are still some drawbacks. First, labeled samples only work at propagating stage, so the supervisory information cannot directly influence the affinity learning process. Second, regardless of noise and outliers, data points may not strictly lie in a union of subspaces, which indicates that the graph X  X  adaptability is restricted owing to the utilization of a single metric. Third, in the context of the subspace assumption, when we have t o select some samples as a labeled set, however, the existing method, random sampling, does not leverage the structural characteristic of the original dataset.

Inspired by the work [13], we propose an effective graph construction frame-work, called constrained least squares regression (CLSR), and try to improve GSSL from three perspectives:  X  The labeled samples are effectively integ rated into the graph learning process  X  Both local and global data structures are considered to build a more flexible  X  A greedy-like strategy is d esigned to pick out more representative samples { x with cardinality | X u | = n  X  l contains unlabeled points. The target of graph learning is to generate a proper graph or weight matrix W  X  IR n  X  n and its element W ij denotes the similarity between the i th point and the j th point under some measurement. SR [6] and LRR [10] are two popular affinity representation techniques. SR aims to construct a sparse graph or  X  1-graph [17], where each point could be reconstructed by a combination of other limited points, and thus the sparse coefficients correspond to a kind of similarity. Basic SR is formulated as the following optimization problem: where Z =[ z 1 ,z 2 ,...,z n ]  X  IR n  X  n denotes the coefficient matrix, Z 1 is the  X  1-norm of Z which can promote sparse solution, Z 1 = n i =1 n j =1 | z ij | . Then, the graph weight matrix W could be easily obtained by W =( | Z | + | Z | T ) / 2
Compared with the k -NN graph, the  X  1-graph avoids evaluating the hyper-parameter k and therefore it outputs more robust result. Nevertheless, both  X  1-graph and k -NN graph are lack of the global views of data, so their perfor-mance would be degenerated when there is no  X  X lean X  data available [22]. In order to capture the global data structure, Liu et al. [10] proposed LRR method which enforces a rank minimization constraint on the coefficient matrix. The basic LRR problem can be formulated as: where Z  X  denotes the nuclear norm of Z , which is a usual surrogate of rank function, i.e., the sum of the singular values. Since the sparseness and low-rankness are merits of a graph, Zhuang et al. [22] presented a non-negative low-rank and sparse graph (NNLRS) lea rning method. Recently, Lu et al. [13] pointed out that, besides  X  1-norm and nuclear norm, Frobenius norm is also an appropriate constraint for the coefficient matrix Z , and presented the LSR model with noise as follows: where  X &gt; 0 is the regularization parameter. Note there are little differences in (1), (2) and (3), but (3) has a close-form solution In this case, LSR can be solved efficiently.

Even though all the above approaches could output suitable graphs for GSSL, the graph learning itself is still unsup ervised. In recent work [15], Shang et al. presented an enhanced spectral kernel (ESK) model, which makes use of pairwise constraints to favor graph learning, and is solved as a low-rank matrix approximation problem [9]. The main d ifference between ESK and our approach is as follows. ESK uses the Gaussian kernel to initialize the weight matrix, and encodes the known labels as the pairwise constraints. While in CLSR, we adopt the regression coefficients to measure t he correlations among data points, and consider additional local constraints to promote the model X  X  flexibility.
Additionally, the quality of labeled points play an important role in GSSL, thus, it is necessary to select the samples with high representability and discrim-inability as labeled set. In [5] and [11], k-means algorithm has been verified as an effective method for sample selectio n. But for GSSL, an extra step is needed to estimate the labels of clustering cen ters. Recently, some researchers pointed out that collaborative representation is a promising method for sample selection [18], [14]. In this paper, we propose a simple and effective method which applies minimal reconstruction error criterion to labeled sample selection.
 In this section, we first introduce the label consistent penalty for encoding known labels, then integrate it with original LSR, and finally design its optimization algorithm. 3.1 Label Consistent Penalty Given two sets for labeled points, ML = { ( x i ,x j ) } includes must-link con-straints, where x i and x j havethesamelabel,and CL = { ( x i ,x j ) } covers cannot-link constraints, where x i and x j have different labels. Let  X  be a set of indices which correspond to all pairwise constraints. The label consistent penalty is defined as: where  X  denotes the element-wise product. The sampling matrix S  X  IR n  X  n is defined as: The constraint matrix L  X  IR n  X  n is defined as: Equation (5) is a squared lose function to measure the consistency between the predicted affinity matrix induced by Z and the given pairwise constraints. Here, the pairwise constraints are expected to reflect the data structure. However, the number of labeled samples is usually few so that it is hard to sufficiently capture the essential structure of data with them. Thus, it is necessary to bring in more local pairwise constraints which are encoded as L  X  IR n  X  n : where N i stands for the set of k -nearest neighbor of x i . Actually, L employs a k -NN graph to roughly recover the local relations among data points by 0/1 assignments, and thus it will result in some wrong assignments. One way to fix these incorrect assignments is to utilize the original L with correct assignments from labeled samples, and the fixed L f  X  IR n  X  n is defined as:
From the perspective of matrix approximation, these wrong assignments in (5) can be taken as one kind of sparse noise. Therefore, the  X  1-norm is used here instead of the Frobenius norm and we have 3.2 Objection Function After adding the label consistent penalty to the LSR model, the objective func-tion of CLSR is written as: where E  X  IR n  X  n denotes the sparse error,  X  e and  X  s are parameters to trade off other terms. In (11), the first two items are used to hold the global structure of data by Z and the third item introduces the pairwise constraints by L which is defined in (9). 3.3 Optimization Equation (11) could be solved by the alternating direction method of multipliers (ADMM) [3] method. To start, we introduce an auxiliary matrix A  X  IR n  X  n for variables separation, then obtain The augmented Lagrangian function of (12) can be written as: where Y 1  X  IR n  X  n and Y 2  X  IR n  X  n are two Lagrange multipliers. ADMM ap-proach updates the variables Z , A and E alternately with other variables fixed, and we can get the updating rules as:
Z where 1  X  IR n  X  n stands for an all-one matrix, and S  X  (  X  ) is the shrinkage-thresholding operator [9] which is defined as: The complete algorithm is summarized in Algorithm 1.
 Algorithm 1. Solving Problem (11) via ADMM In many real applications, we need select a small part of data set as a labeled set. Usually, a natural and simple method, random sampling is adopted. However, this method cannot guarantee the quality of labeled samples. Based on the sub-space assumption, we could select a more representative data subset to upgrade graph X  X  performance in GSSL.

In the CLSR framework, it is convenient to use the basic LSR model for labeled sample selection. We randomly select c subsets { X i } c i =1 from X,X i  X  IR m  X  p and each subset contains p samples, p n . We consider each subset as a tiny dictionary and use it to reconstruct the whole data set, consequently, the representative ability of each subset could be ranked by the corresponding reconstruction error, ther efore, the smaller r econstruction error it has, the more representative it is . The reconstruction error can be solved by where X i  X  IR m  X  p denotes the selected subset, E i  X  IR m  X  n is the reconstruction error, and Z i  X  IR p  X  n is the coefficient matrix of X i . Note problem (18) has a close-form solution
The labeled sample selection met hod is summarized in Algorithm 2. Algorithm 2. Labeled Sample Selection via Mi nimal Reconstruction Error In this section, we integrate CLSR with a popular label propagation approach, LGC [19], for semi-supervised classification. Define a label set F = { 1 ,...,k } , and an initial label matrix Y  X  IR m  X  k with Y ij =1for x i is labeled as j and Y ij = 0 otherwise. The iterative scheme for propagation is where W is a normalized affinity matrix with W = D  X  1 / 2 WD  X  1 / 2 and D is a diagonal matrix whose diagonal entries are equal to the sum of corresponding rows. We fix the parameter  X  to 0.01 in following experiments. The detail of the algorithm is summarized in Algorithm 3.
 Algorithm 3. CLSR for Semi-Supervised Classification In this section, we evaluate the performance of CLSR and other popular graph construction methods on six public databases. 6.1 Datasets and Settings We use two categories of public datasets in the experiments, including UCI data and image data (see Table 1).
 1. UCI data 1 . WeperformexperimentsonthreeUCIdatasetsincluding WDBC, 2. Extended YaleB database 2 . This face database contains 38 individuals 3. ORL database 3 . There are 40 distinct subj ects and each of them has 10 4. COIL20 database 4 . This database consists of a set of gray-scale images We compare following six graph construction algorithms. There are some param-eters in each algorithm, and we tune the parameters on each dataset for every algorithm and record the best results. 1. k -NN: the Euclidean distance is used as similarity metric, and the Gaussian 2. ESK: Following the lines of [15], a low-rank kernel is learned as the affinity 3. LSR: Compared with CLSR, LSR [13] does not consider the pairwise con-4. LRR: Following [10], we construct the low-rank graph and adopt  X  2 , 1 -norm 5. NNLRS: Following [22], we construct the non-negative low-rank and sparse 6. CLSR: In CLSR, the neighbor relations are encoded as the additional pair-6.2 Results and Discussions All experiments are repeated 20 times, f or each dataset, the label rate varies from 10% to 40%. Table 2 lists the average accuracies.

From Table 2 we can get following observations. 1. LSR, LRR and NNLRS generally outperform k -NN and ESK on YaleB and 2. NNLRS usually achieves better performance than LSR and LRR, owing to 3. ESK generally outperforms k -NN with the increasing of the sampling per-4. In most cases, CLSR outperforms other algorithms, since it takes advantage
Next, we study the effectiveness of sam ple selection strategy based on mini-mal reconstruction error. We first randomly select 50 labeled subsets from each dataset, and then sort them in ascending order to form a subset-residual array according to the representative residua l of each subset. Secondly, these labeled subsets are used as the supervisory information for classification and the average accuracies are recorded. Fur thermore, another two results are listed for compari-son, one is the average accuracy of the top 10% of the array (denoted as AT-10%), the other is the average accuracy of the lowest 10% of the array (denoted as AL-10%). The percentage of labeled samples is 5% on WDBC, Parkinsons, YaleB, COIL20 and Sonar, because the selection strategy could be useful in case that there are only limited labeled samples av ailable, especially, we select 20% of sam-ples from ORL, since there are only 10 samples in each class of ORL.
The results are plotted in Fig. 1(a-f). It shows that our method is almost ef-fective for all graph construction approaches on each dataset, except Parkinsons. The result on Parkinsons is unstable. The reason is that there are two classes in Parkinsons, but its imbalance ratio is nearly 3. In this case, our method tends to select more samples from the majority class to minimize the total reconstruction error, which leads to that the selected sa mples are incapable of capturing the true geometric structure of the dataset. We balance the sizes of two classes by randomly selecting some samples from the majority class, and the result shown in Fig. 1(g) is consistent with the other datasets X . We propose a new graph based semi-supervised learning approach called CLSR, which utilizes the pairwise constraints to guide the graph learning process. Beside the labeled information, there constraints also bring in local neighbor relations to enhance the graph X  X  flexibility. In addition, based on CLSR, we design a labeled sample selection strategy which is used to select more representative points as a labeled set. Experimental results on real world datasets demonstrate the effectiveness of our method. Further more, given a small size of labeled set (e.g., 5% of total samples), our sample selection strategy could generally improve the performance of several state-of-the-art methods on most of the datasets used in the experiments.
 Acknowledgments. This work was supported in part by the National Natural Science Foundation of China unde r Grant 61375062, Grant 61370129, and Grant 61033013, the Ph.D Programs Fo undation of Ministry of Education of China un-der Grant 20120009110006,the National 863 project under G rant 2012AA040912, the Opening Project of State Key Laboratory of Digital Publishing Technology, and the Fundamental Research Funds for the Central Universities.

