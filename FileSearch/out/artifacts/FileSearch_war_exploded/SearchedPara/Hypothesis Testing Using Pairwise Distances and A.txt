 Dino Sejdinovic ? dino.sejdinovic@gmail.com Arthur Gretton ?,  X  ,  X  arthur.gretton@gmail.com Bharath Sriperumbudur ?,  X  bharath@gatsby.ucl.ac.uk Kenji Fukumizu  X  fukumizu@ism.ac.jp The problem of testing statistical hypotheses in high dimensional spaces is particularly challenging, and has been a recent focus of considerable work in the statis-tics and machine learning communities. On the statis-tical side, two-sample testing in Euclidean spaces (of whether two independent samples are from the same distribution, or from different distributions) can be accomplished using a so-called energy distance as a statistic (Sz X kely &amp; Rizzo, 2004; 2005). Such tests are consistent against all alternatives as long as the ran-dom variables have finite first moments. A related de-pendence measure between vectors of high dimension is the distance covariance (Sz X kely et al., 2007; Sz X kely &amp; Rizzo, 2009), and the resulting test is again consis-tent for variables with bounded first moment. The dis-tance covariance has had a major impact in the statis-tics community, with Sz X kely &amp; Rizzo (2009) being accompanied by an editorial introduction and discus-sion. A particular advantage of energy distance-based statistics is their compact representation in terms of certain expectations of pairwise Euclidean distances, which leads to straightforward empirical estimates. As a follow-up work, Lyons (2011) generalized the notion of distance covariance to metric spaces of negative type (of which Euclidean spaces are a special case). On the machine learning side, two-sample tests have been formulated based on embeddings of probability distributions into reproducing kernel Hilbert spaces (Gretton et al., 2012), using as the test statistic the difference between these embeddings: this statistic is called the maximum mean discrepancy (MMD). This distance measure was applied to the problem of test-ing for independence, with the associated test statis-tic being the Hilbert-Schmidt Independence Criterion (HSIC) (Gretton et al., 2005a; 2008; Smola et al., 2007; Zhang et al., 2011). Both tests are shown to be con-sistent against all alternatives when a characteristic RKHS is used (Fukumizu et al., 2008; Sriperumbudur et al., 2010). Such tests can further be generalized to structured and non-Euclidean domains, such as text strings, graphs or groups (Fukumizu et al., 2009). Despite their striking similarity, the link between en-ergy distance-based tests and kernel-based tests has been an open question. In the discussion of Sz X kely &amp; Rizzo (2009), Gretton et al. (2009b, p. 1289) first ex-plored this link in the context of independence testing, and stated that interpreting the distance-based inde-pendence statistic as a kernel statistic is not straight-forward, since Bochner X  X  theorem does not apply to the choice of weight function used in the definition of Brownian distance covariance (we briefly review this argument in Section A.3 of the Appendix). Sz X kely &amp; Rizzo (2009, Rejoinder, p. 1303) confirmed this con-clusion, and commented that RKHS-based dependence measures do not seem to be formal extensions of Brow-nian distance covariance because the weight function is not integrable. Our contribution resolves this question and shows that RKHS-based dependence measures are precisely the formal extensions of Brownian distance covariance, where the problem of non-integrability of weight functions is circumvented by using translation-variant kernels, i.e., distance-induced kernels , a novel family of kernels that we introduce in Section 2.2. In the case of two-sample testing, we demonstrate that energy distances are in fact maximum mean discrepan-cies arising from the same family of distance-induced kernels. A number of interesting consequences arise from this insight: first, we show that the energy dis-tance (and distance covariance) derives from a partic-ular parameter choice from a larger family of kernels: this choice may not yield the most sensitive test. Sec-ond, results from Gretton et al. (2009a); Zhang et al. (2011) may be applied to get consistent two-sample and independence tests for the energy distance, with-out using bootstrap, which perform much better than the upper bound proposed by Sz X kely et al. (2007) as an alternative to the bootstrap. Third, in relation to Lyons (2011), we obtain a new family of characteristic kernels arising from semimetric spaces of negative type (where the triangle inequality need not hold), which are quite unlike the characteristic kernels defined via Bochner X  X  theorem (Sriperumbudur et al., 2010). The structure of the paper is as follows: In Section 2, we provide the necessary definitions from RKHS theory, and the relation between RKHS and semimet-rics of negative type. In Section 3.1, we review both the energy distance and distance covariance. We re-late these quantities in Sections 3.2 and 3.3 to the Maximum Mean Discrepancy (MMD) and the Hilbert-Schmidt Independence Criterion (HSIC), respectively. We give conditions for these quantities to distinguish between probability measures in Section 4, thus ob-taining a new family of characteristic kernels. Empir-ical estimates of these quantities and associated two-sample and independence tests are described in Sec-tion 5. Finally, in Section 6, we investigate the per-formance of the test statistics on a variety of testing problems, which demonstrate the strengths of the new kernel family. In this section, we introduce concepts and notation required to understand reproducing kernel Hilbert spaces (Section 2.1), and distribution embeddings into RKHS. We then introduce semimetrics (Section 2.2), and review the relation of semimetrics of negative type to RKHS kernels. 2.1. RKHS Definitions Unless stated otherwise, we will assume that Z is any topological space.
 Definition 1. ( RKHS ) Let H be a Hilbert space of real-valued functions defined on Z . A function k : Z  X Z  X  R is called a reproducing kernel of H if (i)  X  z  X  Z , k (  X  ,z )  X  H , and (ii)  X  z  X  Z ,  X  f  X  H ,  X  f,k (  X  ,z )  X  H = f ( z ) . If H has a reproducing kernel, it is called a reproducing kernel Hilbert space (RKHS). According to the Moore-Aronszajn theorem (Berlinet &amp; Thomas-Agnan, 2004, p. 19), for every symmetric, positive definite function k : Z  X  Z  X  R , there is an associated RKHS H k of real-valued functions on Z with reproducing kernel k . The map  X  : Z  X  H k ,  X  : z 7 X  k (  X  ,z ) is called the canonical feature map or the Aronszajn map of k . We will say that k is a nondegenerate kernel if its Aronszajn map is injective. 2.2. Semimetrics of Negative Type We will work with the notion of semimetric of nega-tive type on a non-empty set Z , where the  X  X istance X  function need not satisfy the triangle inequality. Note that this notion of semimetric is different to that which arises from the seminorm, where distance between two distinct points can be zero (also called pseudonorm). Definition 2. ( Semimetric ) Let Z be a non-empty set and let  X  : Z  X  Z  X  [0 ,  X  ) be a function such that  X  z,z 0  X  Z , (i)  X  ( z,z 0 ) = 0 if and only if z = z semimetric space and  X  is called a semimetric on Z . If, in addition, (iii)  X  z,z 0 ,z 00  X  Z ,  X  ( z 0 ,z 00 )  X   X  ( z,z  X  ( z,z 00 ) , ( Z , X  ) is said to be a metric space and  X  is called a metric on Z .
 Definition 3. ( Negative type ) The semimetric space ( Z , X  ) is said to have negative type if  X  n  X  2 , z ,...,z n  X  X  , and  X  1 ,..., X  n  X  R with P n i =1  X  i = 0 , Note that in the terminology of Berg et al. (1984),  X  satisfying (1) is said to be a negative definite function. The following theorem is a direct consequence of Berg et al. (1984, Proposition 3.2, p. 82).
 Proposition 4.  X  is a semimetric of negative type if and only if there exists a Hilbert space H and an injective map  X  : Z  X  X  , such that This shows that ( R d , k X  X  X  X  2 ) is of negative type. From Berg et al. (1984, Corollary 2.10, p. 78), we have that: Proposition 5. If  X  satisfies (1) , then so does  X  q , for 0 &lt; q &lt; 1 .
 Therefore, by taking q = 1 / 2 , we conclude that all Euclidean spaces are of negative type. While Lyons (2011, p. 9) also uses the result in Proposition 4, he studies embeddings to general Hilbert spaces, and the relation with the theory of reproducing kernel Hilbert spaces is not exploited. Semimetrics of negative type and symmetric positive definite kernels are in fact closely related, as summarized in the following Lemma based on Berg et al. (1984, Lemma 2.1, p. 74). Lemma 6. Let Z be a nonempty set, and let  X  be a semimetric on Z . Let z 0  X  Z , and denote k ( z,z 0 ) = if and only if  X  satisfies (1) .
 We call the kernel k defined above the distance-induced kernel , and say that it is induced by the semimetric  X  . For brevity, we will drop  X  X nduced X  hereafter, and say that k is simply the distance kernel (with some abuse of terminology). In addition, we will typically work with distance kernels scaled by 1 / 2 . Note that k ( z 0 ,z 0 ) = 0 , so distance kernels are not strictly pos-itive definite (equivalently, k (  X  ,z 0 ) = 0 ). By vary-ing  X  X he point at the center X  z 0 , one obtains a fam-ily K  X  = 1 2 [  X  ( z,z 0 ) +  X  ( z 0 ,z 0 )  X   X  ( z,z 0 )] tance kernels induced by  X  . We may now express (2) from Proposition 4 in terms of the canonical feature map for the RKHS H k (proof in Appendix A.1). Proposition 7. Let ( Z , X  ) be a semimetric space of negative type, and k  X  X   X  . Then: 1. k is nondegenerate, i.e., the Aronszajn map z 7 X  2.  X  ( z,z 0 ) = k ( z,z ) + k ( z 0 ,z 0 )  X  2 k ( z,z 0 Note that Proposition 7 implies that the Aronszajn map z 7 X  k (  X  ,z ) is an isometric embedding of a metric space ( Z , X  1 / 2 ) into H k , for every k  X  X   X  . 2.3. Kernels Inducing Semimetrics We now further develop the link between semimetrics of negative type and kernels. Let k be any nonde-generate reproducing kernel on Z (for example, every strictly positive definite k is nondegenerate). Then, by Proposition 4, defines a valid semimetric  X  of negative type on Z . We will say that k generates  X  . It is clear that every distance kernel  X  k  X  K  X  also generates  X  , and that  X  can be expressed as:  X  k ( z,z 0 ) = k ( z,z 0 ) + k ( z 0 ,z 0 )  X  k ( z,z 0 )  X  k ( z for some z 0  X  Z . In addition, k  X  K  X  if and only if k ( z 0 ,z 0 ) = 0 for some z 0  X  Z . Hence, it is clear that any strictly positive definite kernel, e.g., the Gaussian kernel e  X   X  k z  X  z 0 k 2 , is not a distance kernel. Example 8. Let Z = R d and write  X  q ( z,z 0 ) = k z  X  z 0 k q . By combining Propositions 4 and 5,  X  q is a valid semimetric of negative type for 0 &lt; q  X  2 . It is a metric of negative type if q  X  1 . The corresponding distance kernel  X  X entered at zero X  is given by Example 9. Let Z = R d , and consider the Gaussian kernel k ( z,z 0 ) = e  X   X  k z  X  z 0 k 2 . The induced semimetric kernels that generate  X  , however; for example, the dis-tance kernel induced by  X  and  X  X entered at zero X  is  X  In this section, we begin with a description of the en-ergy distance, which measures distance between dis-tributions; and distance covariance, which measures dependence. We then demonstrate that the former is a special instance of the maximum mean discrepancy (a kernel measure of distance on distributions), and the latter an instance of the Hilbert-Schmidt Indepen-dence criterion (a kernel dependence measure). We will denote by M ( Z ) the set of all finite signed Borel measures on Z , and by M 1 + ( Z ) the set of all Borel probability measures on Z . 3.1. Energy Distance and Distance Covariance Sz X kely &amp; Rizzo (2004; 2005) use the following measure of statistical distance between two probability mea-sures P and Q on R d , termed the energy distance : where Z,Z 0 i.i.d.  X  P and W,W 0 i.i.d.  X  Q . This quantity characterizes the equality of distributions, and in the scalar case, it coincides with twice the Cramer-Von Mises distance. We may generalize it to a semimetric space of negative type ( Z , X  ) , with the expression for this generalized distance covariance D E, X  ( P,Q ) being of the same form as (6), with the Euclidean distance replaced by  X  . Note that the negative type of  X  im-plies the non-negativity of D E, X  . In Section 3.2, we will show that for every  X  , D E, X  is precisely the MMD associated to a particular kernel k on Z .
 Now, let X be a random vector on R p and Y a ran-dom vector on R q . The distance covariance was in-troduced in Sz X kely et al. (2007); Sz X kely &amp; Rizzo (2009) to address the problem of testing and mea-suring dependence between X and Y , in terms of a weighted L 2 -distance between characteristic functions of the joint distribution of X and Y and the product of their marginals. Given a particular choice of weight function, it can be computed in terms of certain ex-pectations of pairwise Euclidean distances, where ( X,Y ) and ( X 0 ,Y 0 ) are i.i.d.  X  P XY . Recently, Lyons (2011) established that the generalization of the distance covariance is possible to metric spaces of neg-ative type, with the expression for this generalized dis-tance covariance V 2  X  as (7), with Euclidean distances replaced by metrics of negative type  X  X and  X  Y on domains X and Y , respec-tively. In Section 3.3, we will show that the general-ized distance covariance of a pair of random variables X and Y is precisely HSIC associated to a particular kernel k on the product of domains of X and Y . 3.2. Maximum Mean Discrepancy The notion of the feature map in an RKHS (Section 2.1) can be extended to kernel embeddings of probabil-ity measures (Berlinet &amp; Thomas-Agnan, 2004; Sripe-rumbudur et al., 2010).
 Definition 10. ( Kernel embedding ) Let k be a ker-nel on Z , and P  X  M 1 + ( Z ) . The kernel embedding of P into the RKHS H k is  X  k ( P )  X  H k such that E Z  X  P f ( Z ) =  X  f, X  k ( P )  X  H k for all f  X  X  k . Alternatively, the kernel embedding can be defined by the Bochner expectation  X  k ( P ) = E Z  X  P k (  X  ,Z ) . By the Riesz representation theorem, a sufficient condi-tion for the existence of  X  k ( P ) is that k is Borel-measurable and that E Z  X  P k 1 / 2 ( Z,Z ) &lt;  X  . If k is a bounded continuous function, this is obviously true for all P  X  X  1 + ( Z ) . Kernel embeddings can be used to induce metrics on the spaces of probability measures, giving the maximum mean discrepancy (MMD), where Z,Z 0 i.i.d.  X  P and W,W 0 i.i.d.  X  Q . If the re-striction of  X  k to some P ( Z )  X  M 1 + ( Z ) is well de-fined and injective, then k is said to be characteristic to P ( Z ) , and it is said to be characteristic (without further qualification) if it is characteristic to M 1 + ( Z ) . When k is characteristic,  X  k is a metric on M 1 + ( Z ) , i.e.,  X  ( P,Q ) = 0 iff P = Q ,  X  P,Q  X  X  1 + ( Z ) . Conditions under which kernels are characteristic have been stud-ied by Sriperumbudur et al. (2008); Fukumizu et al. (2009); Sriperumbudur et al. (2010). An alternative interpretation of (8) is as an integral probability met-ric (M X ller, 1997): see Gretton et al. (2012) for details. In general, distance kernels are continuous but un-bounded functions. Thus, kernel embeddings are not defined for all Borel probability measures, and one needs to restrict the attention to a class of Borel proba-bility measures for which E Z  X  P k 1 / 2 ( Z,Z ) &lt;  X  when discussing the maximum mean discrepancy. We will assume that all Borel probability measures considered satisfy a stronger condition that E Z  X  P k ( Z,Z ) &lt;  X  (this reflects a finite first moment condition on random variables considered in distance covariance tests, and will imply that all quantities appearing in our results are well defined). For more details, see Section A.4 in the Appendix. As an alternative to requiring this condition, one may assume that the underlying semi-metric space ( Z , X  ) of negative type is itself bounded, i.e., that sup z,z 0  X  X   X  ( z,z 0 ) &lt;  X  .
 We are now able to describe the relation between the maximum mean discrepancy and the energy distance. The following theorem is a consequence of Lemma 6, and is proved in Section A.1 of the Appendix. Theorem 11. Let ( Z , X  ) be a semimetric space of neg-ative type and let z 0  X  Z . The distance kernel k in-duced by  X  satisfies  X  2 k ( P,Q ) = 1 2 D E, X  ( P,Q ) . In par-ticular,  X  k does not depend on the choice of z 0 . There is a subtlety to the link between kernels and semimetrics, when used in computing the distance on probabilities. Consider again the family of distance kernels K  X  , where the semimetric  X  is itself generated from k according to (3). As we have seen, it may be that k /  X  K  X  , however it is clear that  X  2 k ( P,Q ) =
D E, X  ( P,Q ) whenever k generates  X  . Thus, all ker-nels that generate the same semimetric  X  on Z give rise to the same metric  X  k on (possibly a subset of) + ( Z ) , and  X  k is merely an extension of the met-ric  X  1 / 2 on the point masses. The kernel-based and distance-based methods are therefore equivalent, pro-vided that we allow  X  X istances X   X  which may not satisfy the triangle inequality. 3.3. The Hilbert-Schmidt Independence Given a pair of jointly observed random variables ( X,Y ) with values in X  X Y , the Hilbert-Schmidt In-dependence Criterion (HSIC) is computed as the max-imum mean discrepancy between the joint distribution P
XY and the product of its marginals P X P Y . Let k X and k Y be kernels on X and Y , with respective RKHSs H k X and H k Y . Following Smola et al. (2007, Section 2.3), we consider the MMD associated to the kernel RKHS H k isometrically isomorphic to the tensor prod-uct H k X  X  X  k Y . It follows that  X  :=  X  2 k ( P XY ,P X with where in the last step we used that  X  f  X  g,f 0  X  g 0  X  H It can be shown that this quantity is the squared Hilbert-Schmidt norm of the covariance operator be-tween RKHSs (Gretton et al., 2005b). The following theorem demonstrates the link between HSIC and the distance covariance, and is proved in Appendix A.1. Theorem 12. Let ( X , X  X ) and ( Y , X  Y ) be semimetric spaces of negative type, and ( x 0 ,y 0 )  X  X   X Y . Define Then, k is a positive definite kernel on X  X Y , and  X  We remark that a similar result to Theorem 12 is given by Lyons (2011, Proposition 3.16), but without mak-ing use of the RKHS equivalence. Theorem 12 is a more general statement, in the sense that we allow  X  to be a semimetric of negative type, rather than a met-ric. In addition to yielding a more general statement, the RKHS equivalence leads to a significantly simpler proof: the result is an immediate application of the HSIC expansion of Smola et al. (2007). Lyons (2011, Theorem 3.20) shows that distance co-variance in a metric space characterizes independence if the metrics satisfy an additional property, termed strong negative type . We will extend this notion to a semimetric  X  . We will say that P  X  M 1 + ( Z ) has a finite first moment w.r.t.  X  if for some z 0  X  Z . It is easy to see that the integral  X   X  d ([ P  X  Q ]  X  [ P  X  Q ]) =  X  D E, X  ( P,Q ) converges whenever P and Q have finite first moments w.r.t.  X  . In Appendix A.4, we show that this condition is equivalent to E Z  X  P k ( Z,Z ) &lt;  X  , for a kernel k that generates  X  , which implies the kernel embedding  X  k ( P ) is also well defined.
 Definition 13. The semimetric space ( Z , X  ) is said to have a strong negative type if  X  P,Q  X  X  1 + ( Z ) with finite first moment w.r.t.  X  , The quantity in (10) is exactly  X  2  X  2 k ( P,Q ) for all P,Q with finite first moment w.r.t.  X  . We directly obtain: Proposition 14. Let kernel k generate  X  . Then ( Z , X  ) has a strong negative type if and only if k is character-istic to all probability measures with finite first moment w.r.t.  X  .
 Thus, the problems of checking whether a semimetric is of strong negative type and whether its associated kernel is characteristic to an appropriate space of Borel probability measures are equivalent. This conclusion has some overlap with Lyons (2011): in particular, Proposition 14 is stated in Lyons (2011, Proposition 3.10), where the barycenter map  X  is a kernel embed-ding in our terminology, although Lyons does not con-sider distribution embeddings in an RKHS. In the case of two-sample testing, we are given i.i.d. samples z = { z i } m i =1  X  P and w = { w i } n i =1  X  Q . The empirical (biased) V-statistic estimate of (8) is  X   X  k,V ( z , w ) = Recall that if we use a distance kernel k induced by a semimetric  X  , this estimate involves only the pairwise  X  -distances between the sample points.
 In the case of independence testing, we are given i.i.d. samples z = { ( x i ,y i ) } m i =1  X  P XY , and the resulting V-statistic estimate (HSIC) is (Gretton et al., 2005a; 2008) where K X , K Y and H are m  X  m matrices given by ( K X ) ij := k X ( x i ,x j ) , ( K Y ) ij := k Y ( y H ij =  X  ij  X  1 m (centering matrix). As in the two-sample case, if both k X and k Y are distance kernels, the test statistic involves only the pairwise distances between the samples, i.e., kernel matrices in (12) may be replaced by distance matrices.
 We would like to design distance-based tests with an asymptotic Type I error of  X  , and thus we require an estimate of the (1  X   X  ) -quantile of the V-statistic dis-tribution under the null hypothesis. Under the null hypothesis, both (11) and (12) converge to a particular weighted sum of chi-squared distributed independent random variables (for more details, see Section A.2). We investigate two approaches, both of which yield consistent tests: a bootstrap approach (Arcones &amp; Gin X , 1992), and a spectral approach (Gretton et al., 2009a; Zhang et al., 2011). The latter requires em-pirical computation of the spectrum of kernel integral operators, a problem studied extensively in the con-text of kernel PCA (Sch X lkopf et al., 1997). In the two-sample case, one computes the eigenvalues of the centred Gram matrix  X  K = HKH on the aggregated samples. Here, K is a 2 m  X  2 m matrix, with entries K ij = k ( u i ,u j ) , u = [ z w ] is the concatenation of the two samples and H is the centering matrix. Gret-ton et al. (2009a) show that the null distribution de-fined using these finite sample estimates converges to the population distribution, provided that the spec-trum is square-root summable. The same approach can be used for a consistent finite sample null distri-bution of HSIC, via computation of the eigenvalues of  X  K X = HK X H and  X  K Y = HK Y H (Zhang et al., 2011). Both Sz X kely &amp; Rizzo (2004, p. 14) and Sz X kely et al. (2007, p. 2782 X 2783) establish that the energy dis-tance and distance covariance statistics, respectively, converge to a particular weighted sum of chi-squares of form similar to that found for the kernel-based statistics. Analogous results for the generalized dis-tance covariance are presented by Lyons (2011, p. 7 X  8). These works do not propose test designs that at-tempt to estimate the coefficients in such represen-tations of the null distribution, however (note also that these coefficients have a more intuitive interpre-tation using kernels). Besides the bootstrap, Sz X kely et al. (2007, Theorem 6) also proposes an independence test using a bound applicable to a general quadratic form Q of centered Gaussian random variables with E [ Q ] = 1 : P Q  X   X   X  1 (1  X   X / 2) 2  X   X  , valid for 0 &lt;  X   X  0 . 215 . When applied to the distance co-variance statistic, the upper bound of  X  is achieved if X and Y are independent Bernoulli variables. The authors remark that the resulting criterion might be over-conservative. Thus, more sensitive tests are pos-sible by computing the spectrum of the centred Gram matrices associated to distance kernels, and we pursue this approach in the next section. 6.1. Two-sample Experiments In the two-sample experiments, we investigate three different kinds of synthetic data. In the first, we com-pare two multivariate Gaussians, where the means dif-fer in one dimension only, and all variances are equal. In the second, we again compare two multivariate Gaussians, but this time with identical means in all dimensions, and variance that differs in a single dimen-sion. In our third experiment, we use the benchmark data of Sriperumbudur et al. (2009): one distribution is a univariate Gaussian, and the second is a univari-ate Gaussian with a sinusoidal perturbation of increas-ing frequency (where higher frequencies correspond to harder problems). All tests use a distance kernel in-duced by the Euclidean distance. As shown on the left plots in Figure 1, the spectral and bootstrap test de-signs appear indistinguishable, and they significantly outperform the test designed using the quadratic form bound, which appears to be far too conservative for the data sets considered. This is confirmed by check-ing the Type I error of the quadratic form test, which is significantly smaller than the test size of  X  = 0 . 05 . We also compare the performance to that of the Gaus-sian kernel, with the bandwidth set to the median dis-tance between points in the aggregation of samples. We see that when the means differ, both tests perform similarly. When the variances differ, it is clear that the Gaussian kernel has a major advantage over the dis-tance kernel, although this advantage decreases with increasing dimension (where both perform poorly). In the case of a sinusoidal perturbation, the performance is again very similar.
 In addition, following Example 8, we investigate the performance of kernels obtained using the semimet-ric  X  ( z,z 0 ) = k z  X  z 0 k q for 0 &lt; q  X  2 . Results are presented in the right hand plots of Figure 1. While judiciously chosen values of q offer some improvement in the cases of differing mean and variance, we see a dramatic improvement for the sinusoidal perturbation, compared with the case q = 1 and the Gaussian ker-nel: values q = 1 / 3 (and smaller) yield virtually error-free performance even at high frequencies (note that q = 1 corresponds to the energy distance described in Sz X kely &amp; Rizzo (2004; 2005)). Additional experiments with real-world data are presented in Appendix A.6. We observe from the simulation results that distance kernels with higher exponents are advantageous in cases where distributions differ in mean value along a single dimension (with noise in the remainder), whereas distance kernels with smaller exponents are more sensitive to differences in distributions at finer lengthscales (i.e., where the characteristic functions of the distributions differ at higher frequencies). This ob-servation also appears to hold true on the real-world data experiments in Appendix A.6. 6.2. Independence Experiments To assess independence tests, we used an artificial benchmark proposed by Gretton et al. (2008): we generate univariate random variables from the ICA benchmark densities of Bach &amp; Jordan (2002); rotate them in the product space by an angle between 0 and  X / 4 to introduce dependence; fill additional dimen-sions with independent Gaussian noise; and, finally, pass the resulting multivariate data through random and independent orthogonal transformations. The re-sulting random variables X and Y are dependent but uncorrelated. The case m = 1024 (sample size) and d = 4 (dimension) is plotted in Figure 2 (left). As ob-served by Gretton et al. (2009b), the Gaussian kernel does better than the distance kernel with q = 1 . By varying q , however, we are able to obtain a wide range of performance; in particular, the values q = 1 / 6 (and smaller) have an advantage over the Gaussian kernel on this dataset, especially in the case of smaller an-gles of rotation. As for the two-sample case, bootstrap and spectral tests have indistinguishable performance, and are significantly more sensitive than the quadratic form based test, which failed to reject the null hypoth-esis of independence on this dataset.
 In addition, we assess the test performance on sinu-soidally dependent data. The distribution over the random variable pair X,Y was drawn from P XY  X  1 + sin( `x ) sin( `y ) for integer ` , on the support X  X Y , where X := [  X   X , X  ] and Y := [  X   X , X  ] . In this way, in-creasing ` caused the departure from a uniform (inde-pendent) distribution to occur at increasing frequen-cies, making this departure harder to detect from a small sample size. Results are in Figure 2 (right). We note that the distance covariance outperforms the Gaussian kernel on this example, and that smaller ex-ponents result in better performance (lower Type II error when the departure from independence occurs at higher frequencies). Finally, we note that the set-ting q = 1 , which is described in Sz X kely et al. (2007); Sz X kely &amp; Rizzo (2009), is a reasonable heuristic in practice, but does not yield the most powerful tests on either dataset. We have established an equivalence between the energy distance and distance covariance, and RKHS measures of distance between distributions. In particular, en-ergy distances and RKHS distance measures coincide when the kernel is induced by a semimetric of nega-tive type. The associated family of kernels performs well in two-sample and independence testing: interest-ingly, the parameter choice most commonly used in the statistics literature does not yield the most powerful tests in many settings.
 The interpretation of the energy distance and distance covariance in an RKHS setting should be of consider-able interest both to statisticians and machine learning researchers, since the associated kernels may be used much more widely: in conditional dependence testing and estimates of the chi-squared distance (Fukumizu et al., 2008), in Bayesian inference (Fukumizu et al., 2011), in mixture density estimation (Sriperumbudur, 2011) and in other machine learning applications. In particular, the link with kernels makes these applica-tions of the energy distance immediate and straight-forward. Finally, for problem settings defined most naturally in terms of distances, and where these dis-tances are of negative type, there is an interpretation in terms of reproducing kernels, and the learning ma-chinery from the kernel literature can be brought to bear.

