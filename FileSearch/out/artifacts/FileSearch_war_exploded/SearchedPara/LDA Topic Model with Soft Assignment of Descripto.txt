 Daphna Weinshall daphna@cs.huji.ac.il Dmitri Hanukaev dmitri.hanukaev@mail.huji.ac.il Gal Levi gal.levi@mail.huji.ac.il of Jerusalem, Jerusalem, Israel 91904 This work is motivated by the following general ques-tion: how do we catalogue a collection of multi media documents of continuous data (such as still images, video or music), discovering along the way the un-derlying structure in order to address such tasks as summarization, similarity computation, and novelty detection? We are particularly interested in novelty detection, where the task is to decide whether a new document, never seen before, is unlike any other docu-ment in the training sample. One way to address this question for discrete data employs the generative LDA model.
 Latent Dirichlet Allocation (Blei et al., 2003) is a gen-erative probabilistic model which recovers structure from a collection of discrete data, such as text doc-uments. Documents are represented as bags of words, maintaining only the count of each word in the doc-ument (thus ignoring the order of words). The gen-erative process of document sampling assumes a set of topics, where each document is sampled from a mixture of topics, and each topic defines some unique multinomial probability over the words in the dictio-nary. When fitting a corpus of documents with the LDA model, the topics which are discovered often reveal insightful information about the relations and shared structure between documents.
 The LDA model has proven very useful for modeling text documents. With other documents such as images and video, it has been noted that these documents can often be described by bags of descriptors, where a de-scriptor is typically represented by a vector in R d for some d . In order to get a  X  X ag of words X  from such  X  X ag of descriptors X , one typically starts by defining a dic-tionary of descriptors, obtained by vector quantization (or clustering) of the set of observed descriptors. Now each descriptor in the bag can be assigned a word (us-ing, e.g., nearest neighbor in R d ), and a bag of words representation is obtained (Csurka et al., 2004). This procedure is very effective, and has led to intriguing use of the basic LDA model and its extensions with non-textual data, see for example (Sivic et al., 2005; Fei-Fei &amp; Perona, 2005; Sivic et al., 2008; Wang et al., 2009; Tuytelaars et al., 2010; Philbin et al., 2011) and (Wang &amp; Grimson, 2007; Sudderth et al., 2005; Cao &amp; Fei-Fei, 2007) for extensions which consider the spatial location of image patches.
 However, the assignment of each descriptor to a single word can be problematic, and often it seems more ap-propriate to describe a descriptor by a mixture prob-ability over words (Farquhar et al., 2005; Perronnin, 2008; Philbin et al., 2008; van Gemert et al., 2010). In particular, when one is interested in the modeling of new documents never seen before, as we do here, some descriptors in the bag of the new document may not correspond well to any word in the dictionary, but can still be effectively modeled by a mixture of words. Can we still use LDA to model documents when this soft assignment representation is used? This is the technical question we address in this paper. In Sec-tion 2 we describe the extended LDA model, where documents are described as bags of continuous vector descriptors in R d , and each descriptor is modeled by a mixture probability over words. Thus defined, infer-ence in this model can be achieved using MC meth-ods and Gibbs sampling (Griffiths &amp; Steyvers, 2004). Pursuing the complementary path and deterministic approximations, in Section 3 we derive effective varia-tional algorithms for inference and parameter estima-tion in this model.
 Our method is a generalization of the variational infer-ence and parameter estimation algorithms described in (Blei et al., 2003) of similar low complexity. The main difference is that pseudo-counts of words (or summed responsibility) take the role of word counts (bin his-tograms), and the derivation requires further approx-imations as discussed in Section 3.2. Interestingly, for parameter estimation, our algorithm uses in addition the average covariance matrix between the pseudo-count vectors. Consequently inference and parameter estimation depend on both the first and second order statistics of pseudo-counts of words in the document. In this work it is assumed that the dictionary is given -it is a collection of words, each defining a probability distribution over the space R d of descriptors. This as-sumption may be questioned, in particular in light of the body of work on dictionary learning in the context of sparse coding which may be of relevance here (Ol-shausen et al., 1997; Ramirez et al., 2010). It may seem appealing to define a single framework where dictio-nary learning and topic modeling are done simultane-ously, as is done for example in (Larlus &amp; Jurie, 2009). (Rematas et al., 2012) goes a step further and mod-els the distribution of descriptors non-parametrically. Both methods solve more difficult problems and rely on stochastic methods and Gibbs sampling, with many local minima and the ensuing dependence on initializa-tion.
 We chose to separate the problems and assume a pre-computed dictionary for a number of reasons. The primary reason is computational. On the one hand, simultaneous dictionary learning and topic modeling poses a rather difficult non convex optimization prob-lem. On the other hand, given soft assignment to words in the dictionary, it would seem that any dic-tionary which can approximate the distribution of de-scriptors effectively via a mixture model will do well enough for the task. We therefore chose to use an approximate (possibly non optimal) dictionary and spend more effort on topic learning with soft assign-ment to words. The second reason lies in our appli-cation -novelty detection; optimal dictionary for the training data may not be optimal for truly novel ex-amples.
 In this work, therefore, dictionary learning is done dur-ing pre-processing of the training data. The dictionary is a set of words which act as the components of a mix-ture distribution, and the task of dictionary learning boils down to the task of estimating the components of a mixture distribution which models the empirical distribution of the observed descriptors in the training set. The distribution of each component (or word) can be estimated using, for example, a generative paramet-ric approach such as Gaussian Mixture Modeling. In the application pursued in this paper we describe ex-periments with one such approach, and an alternative one which employs generative words and models de-scriptors with a generative probabilistic model called dynamic texture (Doretto et al., 2003). In the latter approach dictionary learning resorts to the selection of a fixed number of DT X  X , which can generate with high likelihood most of the observed video events (Chan &amp; Vasconcelos, 2008).
 It is not easy to evaluate the efficacy and success of topic models, since there is no ground truth of ac-tual topics to compare with. We chose to evaluate our method in the task of novelty detection. In Section 4 we use the method described above to model video data and identify novel video events. Video events are represented using a dictionary of ISA or dynamic tex-ture features. We test our ability to identify novel events using a benchmark dataset designed for the evaluation of novelty detection algorithms (Mahade-van et al., 2012), demonstrating significant improve-ment obtained by soft assignment as compared with hard assignment (for the same dictionary).
 2.1. Notations We use notations very similar to the language of text collections, making the necessary distinctions along the way. Our goal is to provide a generative model for a collection of documents, each represented by a set of un-ordered descriptors. Like in the bag of words model, the bag of descriptors model ignores the order and relative location of descriptors. Unlike the bag of words model, a descriptor is not a discrete word but a continuous measurement, whose distribution we model by a generative process as well -a mixture over a set of discrete components which we will henceforce call words. In other words, the given dictionary is a col-lection of generative word models (aka components), and each word assigns a probability to every measured descriptor (aka responsibility).
 Formally, the model is described as follows:  X  A descriptor x is a basic unit of continuous data.  X  A document is a sequence of N descriptors x =  X  A corpus is a collection of M documents repre-As in other topic models, we aim to find a probabilistic model of the corpus that assigns high probability to similar documents which we have not yet seen. 2.2. LDA with Soft Word Assignment In many ways, our model is similar to the original LDA model described in (Blei et al., 2003). Docu-ments are represented as mixtures over latent  X  X opics X , and each topic is characterized by a distribution over  X  X ords X . There is one additional step, where the de-scriptor is sampled from the generative word model. Thus the model assumes the following generative pro-cess for each document F in the corpus (see Fig. 1): 1. Choose probability coefficients over topics  X   X  2. For each of the N descriptors x n in the bag of The difference between this model and the original LDA model lies in step 2c, the last box in Fig. 1. Un-like before, each descriptor is not assigned to a single word, but is itself generated by a mixture model of discrete words. Thus, the probability of the obser-vation x n is the mixture probability p ( x n | z n , X  ) = P j =1 p ( w j | z n = i, X  ) p ( x n | w j ) = P j  X  ij p ( x Importantly, we assume that the dictionary is given, in terms of the mixture components. The dictionary is therefore a list of V probability functions, from which we compute f nj = p ( x n | w j ); here x n are the observ-ables,  X ,  X  are the model X  X  parameters, and  X , z , w its hidden variables.
 Given the parameters  X ,  X  , the joint distribution of the observables and the hidden variables is: The probability of the corpus is the following p ( D |  X , X  ) = We now describe variational inference and param-eter estimation algorithms for the model described above, extending the procedures described in (Blei et al., 2003) and using the same variational param-eters. Interestingly, the ensuing inference algorithm gives misleadingly similar update rules of similar com-plexity. For parameter estimation we provide a new lower bound which, when maximized, once again gives update rules very similar to those obtained for the original LDA model. Finally, we show how inference and parameter estimation can be efficiently used with a representation similar to bag of words, where each word is represented by its pseudo-count -the sum of its likelihood over all descriptors in the document. We start by stating our goal, which is to estimate the probability Estimating this probability is intractable, and we fol-low (Blei et al., 2003) and their choice of variational inference to approximate this function. 3.1. Variational Inference Since, given descriptor x n , the dictionary provides the vector of conditional probabilities f n where f nj = p ( x n | w j ), we marginalize over the hidden variable w and attempt to infer  X , z . To accomplish this goal, we define the following variational distribution where q (  X  )  X  Dir (  X  ) and q ( z n )  X  Multinomial (  X  The function q (  X , z |  X , X  ) is a surrogate for the poste-rior distribution (marginalized over w ) p (  X , z , x |  X , X  ). Following (Blei et al., 2003)), we use Jensen inequal-ity to obtain a lower bound on log p ( x |  X , X  ) for any surrogate function q (  X , z |  X , X  ): log p ( x |  X , X  ) = log In order to approximate log p ( x |  X , X  ), one maximizes the lower bound L (  X , X  ;  X , X  ) with respect to all pa-rameters  X , X , X , X  .
 Specifically, let
L (  X , X  ;  X , X  ) = E q [log p (  X  |  X  )] + E q [log p ( z |  X  )] This expression is almost identical to the one we get with the original LDA model, with one difference in the third term above: E q [log p ( x | z, X  )] = After substituting the expressions for the functions p,q , we may take derivatives with respect to the vari-ational parameters and set them to 0, thus obtaining: where  X  is the digamma function. 3.2. Parameter Estimation Next, we wish to estimate the model parameters  X , X  given corpus D following the same Bayesian proce-dure and variational approximation. Differentiating (3) with respect to  X  , one still gets which can be solved with the Newton-Raphson algo-rithm as in (Blei et al., 2003).
 In order to compute  X  , we need to maximize (4) summed over all the documents, and subject to the constraint that the columns of  X  sum to 1. Adding Lagrange multipliers, a solution can be obtained by differentiating
X with respect to  X  ij and  X  i , and setting the derivatives to 0. This turns out to be hard to do in closed form, and the solution can be approximated using gradient descent.
 Instead, we derive a lower bound on the function we aim to maximize. Using the concavity of the log func-tion, it follows that log[ where F dn = P V a =1 f dna . We maximize this function with respect to  X  under the normalization constraint, to obtain It is now possible to improve this estimate of  X  ij using gradient ascent with respect to (7). 3.3. Pseudo-count Histograms for Bags of When computing variational inference for the LDA model as described in (Blei et al., 2003), it follows from the exchangeability assumption that it is not necessary to keep an account of the actual list of words in the document, but only the count of words (or word his-togram). In this section we will show that the same holds for our extended LDA model, where the count is replaced by  X  X seudo-count X  -the sum of word prob-abilities over the document.
 In the original LDA model, we first note that due to exchangeability,  X  n for a given document (we omit the index d for clarity) is the same for all word locations n where the same word w a is observed in the original bag of words model. We can therefore define a vector of length V  X   X  a , where  X  n =  X   X  a for every word  X  a  X  such that w n a = 1 (i.e., word w a is observed in location n ). The update rules for  X   X  remains: We can express both variational parameters in terms number of times word w a appeared in the document. Thus, the original LDA variational inference algorithm essentially relies only on word counts in the document. In the extended model, we can still define  X   X  and update it as in (10). We then derive very similar update rules for the variational parameters: where PseudoCnt ( a ) = P N n =1 f na = P N n =1 p ( x n | w denotes the sum of probabilities for word w a over all descriptors in the document.
 As to be expected by now, parameter estimation is more complicated in the extended model. The com-putation of  X  depends only on the variational param-eters, and can therefore be done using the histogram of pseudo-counts. The approximation of  X  in Eq. (9) can be done as follows: where (11) is used in the transition from the first to the second line.
 Let A d denote the average scaled covariance matrix for the words in document d : Then (12) can be written as In vector notation, where ~  X  i denotes the probabilities for word assignment in topic i and ~  X  di the variational vector of probabilities for word assignment in docu-ment d for topic i , In conclusion, in order to infer the hidden variables of the modified LDA model with the variational approx-imation, we do not need to keep the vector f n for each word, but only the histogram of word pseudo-counts -a histogram h where h ( a ) = P N n =1 f na for 1  X  a  X  V the index of the word in the dictionary. In order to ap-proximate the model parameter  X  , we also need matrix A d which measures the covariance between the words in document d . 3.4. Smoothing and LDA In (Blei et al., 2003) Blei et al. described a slightly different model where they proposed to place a Dirich-let prior on the hyper parameter  X  . This model turned out to be better suited for online implemen-tation (Hoffman et al., 2010), and we use it here for the same purpose with an online algorithm and the extended LDA model. In this model the probability of the data becomes: where p ( D |  X , X  ) corresponds to the LDA model de-scribed above.
 Now, the variational distribution takes the form: It can easily be verified that the update equation for  X  and  X  become:  X  ij =  X  +  X   X  ai  X  exp(log(  X  ia |  X  i ) exp( X (  X  i )  X   X ( Successfully identifying novel video events can be very useful for such applications as video surveillance and video annotation. An effective way to identify novel events is to use extended recordings to learn the  X  X or-mal X  state of the system, and then identify those events whose posterior probability with respect to the learnt model is low. In this section we use the extended LDA model described above to model the given collection of video recordings. 4.1. Representation of Video Events In order to represent video events, each video doc-ument is decomposed into a set of spatio-temporal patches extracted from the video. Each spatio-temporal patch is represented via the ISA features (Le et al., 2011), or the generative Dynamic Textures model which readily assigns a probability to each 3D-patch . The dictionary is a set of words pre selected based on the respective representation of all patches in the training data, and each spatio-temporal video patch is represented as a mixture of these categori-cal words. In estimating the model parameters, and in particular the parameter  X  , we effectively learn the mixture coefficients of the words in each topic. More specifically, for ISA spatio-temporal features we use first layer features trained on Hollywood2 dataset as provided by the authors (Le et al., 2011). We obtain a dictionary with the k-means algorithm (Euclidean distance) using subset of the train set. We estimate p ( x | w ) for each of the words in the dictionary by fit-ting a 1 D Gaussian, using distances between features assigned to w by the k-means algorithm.
 Dynamic Texture (DT) (Doretto et al., 2003) is a video generative model that captures both the dynamic and the texture of the video. It consists of a random pro-cess containing an observed variable y t , which encodes the appearance component (video frame at time t), and a hidden state variable x t , which encodes the dy-namics (evolution of the video over time). The state and observed variables are related through the linear dynamical system defined by: where x t  X  X  n denotes the hidden state variable, y t  X  R m the observable, A  X  R n  X  n the state transition matrix, and C  X  R m  X  n an observation matrix. v t  X  noise in the hidden and the observed states.
 Given a training set of video sequences, each se-quence is divided into a set of video events which are object-size spatio-temporal video pieces, see Fig. 2. Each video events is further divided into small spatio-temporal 3D-patches ; these patches together define the ensemble of basic video units, which are used to learn the words in the dictionary using the k-means algo-rithm. With the DT representation we use the follow-ing initialization procedure: we pick V random 3D-patches , learn for each patch the DT that maximizes its likelihood, and use these V DT  X  X  as initial centers for the k-means algorithm.
 4.2. Database To evaluate our algorithm we used the UCSD Ped2 dataset (Mahadevan et al., 2012), consisting of videos taken while monitoring a crowded pedestrian walkway scene. The dataset consists of 16 training videos con-taining only pedestrians, and 12 test videos containing also abnormal events such as bicycles and skateboard riders; examples of abnormal events detected by our algorithm are shown in Fig. 3. The dataset also con-tains frame level ground truth of frames that contain abnormal events, and there is a subset of 9 videos pro-vided with pixel-level binary masks.
 Each video has 120-180 frames of resolution 360  X  240. We split each video to events of size 24  X  24  X  21 (21 is the temporal length). With DT features each event is decomposed into 4  X  4  X  3 3D-patches of size 9  X  9  X  10, with a spatial/temporal displacement of 5 pixels/frames between each two events and 3D-patches . With ISA features each event is decomposed into 3  X  3  X  4 3D-patches of size 16  X  16  X  10 (Le et al., 2011). To improve performance we filtered out events and 3D-patches with no movement. From the collec-tion 3D-patches a dictionary was computed.
 When determining the dictionary size, we seek the smallest dictionary which will allow for good perfor-mance and will benefit generalization. At the same time, hard assignment to words would appear to ben-efit from a larger dictionary size when competing with the richer soft assignment to words. We therefore choose the size of the dictionary based on the perfor-mance of the hard assignment algorithm on the train set, searching for the best size over a feasible range. For both feature types the optimal dictionary size was 75 words. With DT words we used hidden state size of 10 learnt as described in Section 4.1, see Fig. 4. 4.3. LDA Model and Results Given the dictionary, each 3D-patch was assigned a vector of soft probabilities, reflecting the likelihood that each word generated the patch. In soft assign-ment of this nature it is customary to set to zero some of the lower assignment, typically those below the av-erage assignment (Coates et al., 2010). Here it was necessary to use a higher threshold and set to zero up to 95% of the lower assignments, because the DT dic-tionary words in particular have high overlap. Next, each video event in the dataset was encoded according to the 3D-patches composing it as described in Sec-tion 2. Finally, using events from the training video an extended LDA model was learnt as described in Section 3, and subsequently used to estimate the like-lihood of each event in the test videos.
 Using event likelihood, we identify those events whose likelihood is below some threshold as abnormal events. Varying the threshold value, we obtained the ROC curves shown in Fig. 5. Following (Mahadevan et al., 2010), we tested our method using frame-level and pixel-level measurements, where in the former a de-tection in a frame is successful regardless of the ab-normality location within the frame, and in the later a detection is successful if at least 40% of the truly anomalous pixels are detected. The frame level EER values, compared with the results reported in (Ma-hadevan et al., 2010), are tabulated in Table 1. Some examples of successful detection are displayed in Fig. 3. We report results with hard assignment to words, soft assignment to words with the batch LDA algorithm described in Sections 3.1-3.2, and soft assignment to word with an online LDA algorithm (Hoffman et al., 2010) which computes the smooth LDA model with pseudo-count representation as outlined in Section 3.4. The online algorithm used both types of features, DT and ISA. In our comparisons soft assignment achieved top performance, better than alternative methods and much better than hard assignment. 4.4. Discussion The most interesting result for evaluating the added value of the new model presented in this paper, is seen in the big difference in performance between soft and hard assignments of visual words. Using the same dic-tionary and all else being equal, hard assignment of video patches to words and the original LDA model achieved poor performance in novelty detection, while soft assignment and the extended LDA model (either batch or online) achieved state of the art performance. Moreover, this result was seen for two very different kinds of representation, one with generative feature model ( DT ) and one with an adaptive model trained on a different dataset (ISA). We note that the number of soft assignments had to be restricted in order for the batch LDA algorithm to converge to a non-trivial solution (i.e., non degenerate topic distribution). Latent Dirichlet Allocation has proven very effective in modeling text and other multi-media documents. Here we extended this model to allow for a richer represen-tation, to answer a need when dealing with videos and images which are less naturally represented by bags of words. Documents are now represented by bags of con-tinuous descriptors, and each descriptor is represented by its vector of affinities to the words in the dictionary. We derived variational inference and parameter esti-mation procedures for this model, which resemble the original algorithm in an appealing way. We demon-strated how the incorporation of soft assignment to words very significantly improved the effectiveness of the LDA model in the detection of novel video events. Specifically, when using the same data and the same dictionary, the traditional LDA model achieved poor performance in novelty detection, while our extended LDA method achieved state of the art performance.
