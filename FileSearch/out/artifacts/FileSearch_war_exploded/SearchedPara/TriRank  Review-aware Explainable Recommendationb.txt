 Most existing collaborative filtering techniques have focused on modeling the binary relation of users to items by ex-tracting from user ratings. Aside from users X  ratings, their affiliated reviews often provide the rationale for their rat-ings and identify what aspects of the item they cared most about. We explore the rich evidence source of aspects in user reviews to improve top-N recommendation. By extracting aspects ( i.e., the specific properties of items) from textual reviews, we enrich the user X  X tem binary relation to a user X  item X  X spect ternary relation. We model the ternary relation as a heterogeneous tripartite graph, casting the recommen-dation task as one of vertex ranking. We devise a generic algorithm for ranking on tripartite graphs  X  TriRank  X  and specialize it for personalized recommendation. Experiments on two public review datasets show that it consistently out-performs state-of-the-art methods. Most importantly, Tri-Rank endows the recommender system with a higher degree of explainability and transparency by modeling aspects in reviews. It allows users to interact with the system through their aspect preferences, assisting users in making informed decisions.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -information filtering Reviews, Aspects, Explanable Recommendation, Top-N Rec-ommendation, Tripartite Graph Ranking
Recommender systems serve to help users discover choice products to consume, matching users to items of potential interest ( e.g., products, businesses, movies, etc. ). Among the various recommendation techniques, collaborative filter-ing (CF) is widely used, due to its effectiveness in providing personalized recommendation based on the wisdom of the crowds. By and large, existing CF techniques have focused on modeling user X  X tem relations, such as ratings. As a single rating only indicates a user X  X  overall satisfaction for the item, it is hard to infer the actual rationale of the rating. Differ-ent users may care about different aspects of the same item. For example, in the restaurant domain, one user may give a 5-star rating for food quality, while another user may give the same rating, but due to favorable ambiance. Since ex-isting CF techniques largely lack such fine-grained analysis, they can fail to accurately model a target user X  X  interests.
Aside from ratings, most Web 2.0 systems also give users the opportunity to leave reviews on the provided items. These reviews often justify a user X  X  rating, offering the un-derlying reasons for the rating by discussing the specific properties of the item. Thus they are well-suited as a com-plementary data source for collaborative filtering. In this work, we leverage on this key observation to address the task of top-N recommendation ; that is, to produce an or-dered list of N items that will be most appealing to a user.
Leveraging user reviews (or interchangably,  X  X omments X ) is non-trivial for systems, as these reviews are written by users freestyle, exhibiting noise and irrelevant content. Re-cent research [10, 11, 17, 19] have largely modeled reviews at the word level, distilling them into latent topics to com-bine with the latent factor model. Although these methods achieve good prediction accuracy, the recommendation pro-cess is not transparent and the generated recommendations are not explainable to users, a well-known drawback of the latent factor model [14]. To improve users X  experience and trust, transparency and explainability become increasingly important for practical recommender systems [27]. More-over, improvements in rating prediction (a lower error rate) may not directly translate into improvements in top-N rec-ommendation [3]. As systems usually make only their top suggestions visible, targeting improvement at the top ranks is most beneficial for recommendation in practice.

Instead of modeling reviews at the word level, we propose to model reviews in the level of distilled aspects , which are the specific properties of items and expressed in reviews as noun words or phrases [31]. For example, battery and screen are aspects of digital products, food quality and ambiance are aspects of restaurants. Given the aspects extracted from reviews, we first model the user X  X tem X  X spect relation as a tripartite graph. Then we devise TriRank, a generic al-gorithm based on the graph regularization framework [9, 36] for ranking in tripartite graphs. TriRank ranks vertices by accounting for both the structural smoothness (encod-ing collaborative filtering and aspect filtering effects) and fitting constraints (encoding personalized preferences). Be-sides TriRank X  X  superior performance, there are two key properties making it suitable in practice: first, the recom-mendation process is transparent by explaining its recom-mendations with respect to aspects, allowing users to cus-tomize their recommendations to fit their preferences ( i.e., scrutability [27]). Second, TriRank is tolerant of noisy as-pects, common in automated aspect extraction, such that porting to new domains can be done without manual effort.
It is instructive to clarify that this work solely relies on the automatically detected item aspects in user reviews, and not any other additional information, such as sentiment (a separate area of study). We believe regardless of a user X  X  sentiment on a reviewed aspect, its mention at least reflects the user X  X  interest in the aspect. We believe by mining such item aspects latent in reviews, we can infer user preference at a finer granularity, thus providing better personalized rec-ommendation.

This paper is organized as follows. In Section 2, we discuss the process of aspect extraction from reviews. We detail our TriRank method in Section 3, and conduct experiments and empirical studies on aspects in Section 4. We review related work in Section 5, before concluding the paper in Section 6.
Aspect extraction, also termed as feature or attribute ex-traction, has a long history in review mining (see [31]). As-pects can be seen as the components, attributes, or prop-erties of an item. Early seminal work [12] proposed several language rules to extract product features from reviews. The rules have been widely used and extended by later work, e.g., [32] considered specific phrase patterns and sentence patterns. Aside from the unsupervised rule-based methods, supervised sequence labeling techniques such as the Condi-tional Random Field have been adopted to learn aspects [13].
As our focus is to leverage aspects from user reviews, we do not contribute to aspect extraction, but instead seek to maximally exploit technologies that can perform it. As such, we apply an existing state-of-the-art aspect extraction toolkit [33] that constructs a sentiment lexicon from user reviews. It creates entries that are feature X  X pinion word pairs with an associated sentiment polarity, represented as ( F,O,S ). For example, an entry such as ( service, excellent, positive ) might be extracted from a restaurant review. The key feature extraction part of the tool is a rule-based system, mainly adopting and extending the rules proposed by [12]. As each feature is a noun word or phrase, representing the item X  X  property that a user comments on, we can directly use it as an aspect.

We apply the tool with its default settings, extracting 6 , 025 and 1 , 617 distinct features ( i.e., aspects) from our datasets culled from Yelp and Amazon Electronics, respec-tively (datasets described later in Section 4). Table 1 shows top features extracted from the two datasets, ranked by their tf  X  idf score. We notice that the tool produces some fea-tures that are good but also many noisy features, such as  X  X ve X  ( X  X  X  X e X ),  X  X icturesmy X ,  X  X 50 X , which are quirks of the corpus. Also, some top features are domain-specific stop words ( e.g. ,  X  X ood X ,  X  X estaurant X ,  X  X roduct X ), which do not Table 2: Statistics of aspects extracted from reviews. represent the specific properties of items. Despite this signif-icant level of noise, we do not perform any post filtering on the extracted aspects to test the robustness of our proposed method to noise.

Note that in terms of manifestation, aspects and tags (in social tagging systems) look alike  X  they are both usually short noun phrases. However, they differ fundamentally in nature and hence utility. Tags are simple keywords that are directly annotated by users to categorize and manage items. Aspects, on the other hand, describe specific attributes of items, and are implicitly extracted from free-text reviews.
Table 2 summarizes the statistics of extracted aspects on the two datasets. We note that the densities of the user X  aspect and item X  X spect matrix are much higher than that of the user X  X tem rating matrix (usually less than 1%, see Table 3); a good signal that the aspect matrices contain rich information useful for addressing the sparseness of the original rating matrix.
We now present our proposed method for review-aware recommendation, by introducing the tripartite graph to model the user X  X tem X  X spect ternary relation. We then devise the generic TriRank algorithm for ranking on tripartite graphs. Finally, we detail how to operationalize TriRank for person-alized recommendation.
Let G = ( U  X  P  X  A,E UP  X  E UA  X  E P A ) be a tripartite graph, where U,P and A are vertex sets that represent users, items and aspects, respectively. Let E UP , E UA and E P A edges that represent user X  X tem, user X  X spect, item X  X spect re-lations, respectively. Each input triple &lt;u i ,p j ,a that user u i has rated item p j with a review mentioning as-pect a k , is then represented as a triangle with edges e and e jk (as in Figure 1). Each edge carries a weight to denote the strength of two connected vertices; edges with higher weight denote stronger more significant relations be-tween vertices. For example, we can model user u i  X  X  rating on item p j as an edge with weight of e ij . Without loss of generality, we use the symbol R , Y and X to denote the edge weight matrix of user X  X tem, user X  X spect and item X  X spect re-lations, respectively.
The goal for item recommendation is to devise a ranking function f : P  X  R , which maps each item in P to a real number such that the value reflects the target user u  X  X  (pre-dicted) preference on the item. Then, sorting the resultant items by score yields u  X  X  personalized item ranking. Since Figure 1: An example tripartite structure of the given inputs (the dashed line illustrates the addi-tional input &lt; u 1 ,p 3 ,a 2 &gt; ).
 TriRank induces scores for all vertices in the graph, it has the important side-affect of assigning scores to the aspect and user vertices: these denote u  X  X  interest on aspects and similarity with other users, respectively.

In a nutshell, TriRank assigns the ranking score of ver-tices by enforcing the structural smoothness and fitting con-straints of the graph. By smoothness and fitting constraints, we adopt the same definitions as those common to the graph regularization framework [9, 36]:  X  Smoothness implies local consistency: that nearby vertices should not vary too much in their scores.  X  Fitting encodes prior belief: that the ranking function should not cause much deviation from the observations.
TriRank seeks to assign each vertex a score such that the graph is sufficiently smooth and the prior belief is retained . In the following, we first illustrate how the two constraints in the tripartite graph capture the intuition for recommen-dation, before describing the TriRank algorithm.
Let us first see how the smoothness works by considering the example in Figure 1. We decompose it into two sub-graphs in Figure 2 for ease of exposition. The left subfigure gives the user X  X tem structure, where edge weights denote ratings. Assume we want to recommend items to u 1 , who has only rated p 1 with a score of 5. As p 1 is connected more strongly to u 2 than u 3 , u 2 is given a higher score than u Finally, since the edge weights of &lt;u 2 , p 2 &gt; and &lt;u identical, we infer that p 2 should receive a higher score than p . Such smoothness constraints on the user X  X tem relation alone yields the traditional CF effect.

Considering aspects can provide additional evidence that influences the recommendation process. Let us continue to recommend for u 1 but base our decision on item X  X spect structure (Figure 2(b)), where edge weight denotes the num-ber of an item X  X  reviews mentioning an aspect. As u 1 only previously mentions aspect a 1 , enforcing smoothness would rank p 3 higher than p 2 , as p 3 is more strongly connected to a , in contrast to p 2 . This example also shows that predict-ing based on CF and aspect filtering yield different results; and that the smoothness constraint on the whole graph to combine them may be beneficial.

The fitting constraint serves as a means to personalize the ranking for each user ( cf. shaded vertices of Figure 2). For a target user u , the past ratings and reviewed aspects indicate u  X  X  prior (known) preference on the vertices. It should guide the ranking process such that the resultant ranking function should be consistent with the prior belief. Figure 2: Smoothness constraints on decomposed graphs from Figure 1. Assume u 1 previously rated item p 1 with mentioning aspect a 1 (shaded vertices).
We now define the regularization function to implement the two constraints for ranking vertices.

Smoothness. Similar to the previous work [9] that de-fines a smoothness regularizer on bipartite graphs, we devise the regularizer on user X  X tem structure as follows: where f ( u i ) and f ( p j ) denote the final ranking scores ( i.e., parameters to learn); r ij is the edge weight between u i p ; d u i and d p j are the weighted degrees (sum of edge weights) of u i and p j , respectively, for normalization. The counter-part user X  X spect and item X  X spect smoothness regularizers for aspect filtering can be obtained similarly.

This smoothness regularizer can be seen as a graph kernel that measures the similarity of vertices. Although there are various kernels [25], we have purposefully chosen this one (originally introduced by [36]) due to its effective encoding of the CF effect in bipartite graph scenarios. To see this, assume we recommend for the target user u . First, mini-mizing Eq. (1) constrains a vertex X  X  score based on its neigh-bors  X  if a user is strongly connected by many high-scoring items ( e.g., rated items of u ), the user will be given a high score ( i.e., more similar with the u ); likewise, if an item is strongly connected by many high-scoring users ( i.e., similar users), it will have a high score. Second, the quadratic na-ture of the normalization suppresses the popularity of highly connected vertices; this property is essential to prevent a ranking from being dominated by popular vertices [1].
Fitting. Let the target user X  X  prior preference on item p be p 0 j ; then the regularizer to enforce the fitting constraint on items is defined as:
We can similarly achieve such fitting regularizers on users and aspects. This fitting regularizer corresponds to the squared error loss that is commonly used by machine learn-ing models in recommendation. Different with the latent fac-tor model [14, 19] that only optimizes for rated items, Eq. (2) also importantly takes unrated items into account ( cf. sum-ming over all items). This property is very desirable for the top-N task, as it aims at ranking unrated items [3]. Another option for top-N recommendation is to optimize a ranking-based loss function, such as AUC used by Bayesian Person-alized Ranking [23]. This is an interesting extension to be explored in the future.
Regularization function. To account for the hetero-geneous structure of the tripartite graph, we combine the smoothness regularizer on each relation type with the fit-ting regularizer using different weights for each vertex type: where  X , X  and  X  denote the weight of smoothness on user X  item, item X  X spect and user X  X spect relation, respectively;  X   X 
P and  X  A denote the weight of fitting constraint on users, items and aspects, respectively (we discuss how to set the prior preference u 0 i ,p 0 j and a 0 k later in Section 3.3).
We now minimize Eq. (3) to derive the final ranking scores ( i.e., model parameters). As the objective function is strictly convex, standard optimization techniques find a unique so-lution regardless of initialization. Two widely used tech-niques are stochastic gradient descent (SGD) and alternating least squares (ALS). SGD updates all parameters towards the negative gradients for each training instance, while ALS minimizes the objective function per parameter until a joint optimum is found ( i.e., coordinate-wise descent). For this scenario, we adopt ALS over SGD as the objective function can be analytically solved for each parameter, and impor-tantly, it does not need to set the learning rate, which is crucial to SGD X  X  effectiveness. Additionally, it usually yields a faster convergence and is easier to parallelize than SGD.
By differentiating Q ( f ) with respect to f ( u i ), f ( p f ( a k ), respectively, and letting the derivatives be 0, we ob-tain the iterative update rules. Let the ranking vector for ~u,~u 0 for users, and ~a,~a 0 for aspects. The equivalent update rules in matrix form are as follows: where S R is the symmetric normalized form of matrix R , defined as [ r ij  X  larly. We note that a closed-form solution can be obtained analytically, but omit them due to space limitations.
Given the general TriRank algorithm, we need to cover how we obtain the initial graph (specifically, edge weights and the target user X  X  prior preference) to concretize the generic algorithm for our review-based recommendation scenario. Edge weights. User X  X tem edge weights from relation R can be set as in traditional CF: in cases with explicit feedback, it can be the rating score; for implicit feedback, whether the user has interacted with or browsed the item
Algorithm 1: TriRank for review-aware top-N item recommendation.
 (measured as either a binary yes/no, or an integer view count). Our datasets provide explicit user ratings, so we use these ratings as-is.

For the user X  X spect relation Y and the item X  X spect re-lation X , edge weights connote the degree of user interest (item speciality) with respect to the aspect. Once aspects are identified in reviews, we can use either the actual count (number of mentions) within all a user X  X  (item X  X ) reviews, or the review frequency (number of reviews that mention the aspects). As reviews vary in length, an aspect may occur multiple times in long reviews, but may not imply that the user pays more attention to the aspect 1 . As such, we use review frequency in our experiments. As in general IR, we take the logarithm of the review frequency, to dampen the effect of aspects that appear very frequently.

Prior preference. We need to set the prior preference vectors for the three vertex types, with respect to the target user u i for personalization.

For items , the item prior preference vector ~p 0 takes a pos-itive value if the target user has interacted with the item, otherwise, 0. Thus we adopt the i th row vector of R as the ~p 0 for the target user u i . Similarly, for aspects , the aspect preference vector ~a 0 is set as the row vector of user X  X spect matrix Y . As the smoothness part of Eq. (3) normalizes the edge weight by a vertex X  X  degree, we also apply the L 1 norm on ~p 0 and ~a 0 for meaningful combination.

The user preference vector ~u 0 should denote the target user X  X  similarity with other users. We can set ~u 0 based on a user X  X  social network when it is available. We can also adopt standard user-based CF, and set ~u 0 by measuring user sim-ilarity from the rating matrix R . In this work, we adopt the most basic approach, simply setting the target user herself as 1, and all other users as 0. Note that this variable does not directly reflect user X  X  preference on aspects or items, so its design is beyond the scope of this work.

TriRank works by enforcing the collaborative filtering and aspect filtering effects, and is summarized in Algorithm 1. For convergence, one can monitor Q ( f ) X  X  value until it sta-bilizes or set a maximum number of iterations.

The iterative solution Eq. (4) presents a more transparent view on the ranking process. The scores of items, users and aspects mutually reinforce each other  X  a score increase in an item will increase the score of its connected users and
Note that this is the same argument for the analogous doc-ument frequency over collection frequency, in general IR. Figure 3: Mock user interface for showing the ratio-nale behind recommending Chick-Fil-A to a user. aspects; similarly, for users and aspects. The overall solu-tion can be seen as a semi-supervised learning process [36] on graphs  X  with the prior preference as labeled data, the algorithm propagates the labels to other unlabeled vertices.
There are three properties of TriRank that merit a more detailed discussion: explainability, insensitivity to noisy as-pects, and structural ambiguity.

Explainability. As TriRank ranks items in an easily ex-plainable way, it provides users more transparency in un-derstanding the system behavior. We can attribute recom-mendations to the top-ranked aspects matching the target user and recommended item. Figure 3 shows a mock-up interface for explaining recommendation based on aspects, inspired by tag-based explanation [28]. Aspects are sorted by item X  X  speciality by default, but a user can sort accord-ing to her predicted preference. This property makes the system scrutable [27], allowing a user to control how the system utilizes her reviews. For example, if a user dislikes a recommendation due to inaccurately-captured aspects or she has updated preference not captured in her reviews, she can edit her aspect preference. TriRank can then encode the new aspect query vector ( i.e. , ~a 0 ) and return the revised recommendations (shown later in Section 4.3).

This is a major advantage over the recent solutions [19, 34] which integrate reviews using a latent factor model (LFM). Such LFM methods only provide single-shot recommenda-tion where the rationale for the recommendation is opaque. In contrast, the scrutability provided by our method easily allows recommendation to become a cyclical process  X  a user can iteratively interact with the recommender system, where her actions improve the system X  X  recommendations in turn. This iterative and scrutable nature are becoming increas-ingly important for real-world recommender systems [27].
Insensitivity to noisy aspects. As mentioned in Sec-tion 2, extracted aspects are noisy. For noisy aspects which are outliers ( e.g.,  X  X icturemy X ,  X 150 X ), they usually occur less frequently in reviews as compared with those from nor-mal aspects. As such, they will have smaller edge weights in the tripartite graph, thus exerting less impact on the ranking (see x jk and y ik of Eq. (3)). For noisy aspects which are domain-specific stop words, although they have high frequency in reviews, they actually distribute evenly for all users and items ( i.e., column vector of S X and S Eq. (4)). As a result, they will contribute evenly across all items X  ranking scores, hence not changing the relative rank-ing among items. As such, TriRank is relatively insensitive to noisy aspects (either outliers or stop words).

Structural ambiguity. Given a list of triples as inputs, we can uniquely represent them as a tripartite graph, but not vice versa. This is because in the tripartite graph, we cannot attribute a specific edge to an input tuple, as the conversion to the tripartite graph does not represent tuple association. More specifically, let the reviewed aspects of user u i and item p be A i and A j , respectively. Assume u i interacts with p then the tripartite structure can not differentiate the aspects in A i  X  A j for the interaction r ij . When such ambiguities occur, they can act like unseen additional inputs, which can complement the actual observed data in a manner similar to transitive reasoning.
We first introduce our experimental settings, and then compare its performance with other methods. We then study the utility of aspects in depth. Finally, we perform a few case studies of TriRank X  X  recommendation output.
Datasets. We experiment with two publicly accessible datasets: Yelp 2 and Amazon Electronics [19]. 1. Yelp . This is the Yelp Challenge dataset published on April 2013. It includes 11,537 items, 229,907 reviews and 45,981 users. The dataset is very sparse  X  49.6% of users only made one review, making it difficult for evaluation. 2. Amazon . This dataset contains user ratings and reviews on Amazon products of Electronics category, pub-lished by [19]. The original dataset contains over 800K users, 80K items and 1.3M reviews. It is more sparse than the Yelp dataset  X  with 77.9% of users making only one review.
Following the common practice by other works [4, 34] in evaluating recommender algorithms, we filter out the items and users having fewer than 10 reviews. Table 3 summarizes the statistics of the filtered datasets. We split each dataset into three parts for training, validation and testing by time. For each user, we sort her reviews in chronological order. The first 80% are used for training, and the remaining most recent 20% are randomly split as validation set (for param-eter tuning only) and test set (for evaluation).

Evaluation Metrics. Given a user, each algorithm pro-duces a ranked list of items. To assess the ranked list with the ground-truth item set (GT), we adopt Hit Ratio (HR), which has been commonly used in top-N evaluation [15, 29]. If a test item appears in the recommended list, it is deemed a hit. HR is calculated as:
As the HR is recall-based metric, it does not reflect the accuracy of getting top ranks correct, which is crucial in many real-world applications. To address this, we also adopt Normalized Discounted Cumulative Gain (NDCG), which assigns higher importance to results at top ranks, scoring successively lower ranks with marginal fractional utility: http://www.yelp.com/dataset_challenge where Z K is the normalizer to ensure the perfect ranking has a value of 1; r i is the graded relevance of item at position i . We use the simple binary relevance for our work: r i = 1 if the item is in the test set, and 0 otherwise.
 For both metrics, larger values indicate better performance. In the evaluation, we calculate both metrics for each user in the test set, and report the average score.

Baselines. We compare TriRank with the following com-monly used and competitive methods in top-N evaluation: 1. Item Popularity (ItemPop) . Items are ranked by their popularity judged by number of ratings. Although it is not personalized, it is surprisingly competitive in top-N evaluation [3], as users tend to consume popular items. 2. ItemKNN [24] . This is standard item-based CF, and has been used commercially by Amazon [16] and Movie-Lens [28]. We adopt cosine similarity to measure the similar-ity among items. We test the method with different number of neighbors, finding that using all neighbors works best. 3. PureSVD [3] . A state-of-the-art for top-N recom-mendation, which performs Singular Value Decomposition on the whole matrix, thus directly considering all instances. unlike other latent factor methods that optimize against er-ror only on rated instances. This property is important when applying matrix factorization models for top-N evalua-tion. We follow the implementation in [3], using the package SVDLIBC 3 , tuning the number of latent features from 10 to 200, finding the best performance at 30. 4. Personalized PageRank [8] . This is a widely used graph method for top-N recommendation, e.g., by [15, 29]. We perform Personalized PageRank on the user X  X tem graph 4 and set the personalized vector same with TriRank X  X  prior item vector ~p 0 . The damping parameter ( i.e. , weight of the personalized vector) was respectively optimized to 0 . 9 and 0 . 3, for Yelp and Amazon datasets. 5. ItemRank [6] . This is another graph based method that recommends based on the item X  X tem correlation graph. Similar to Personalized PageRank, we set the personalized vector identically as ~p 0 and tune the damping factor. 6. TagRW [35] . This is the state-of-the-art method to model tags for top-N item recommendation. As we have mentioned that aspects are similar with tags in terms of for-mat, we need to compare with such a method to study how tag-aware methods perform on the task of modeling aspects. http://tedlab.mit.edu/~dr/SVDLIBC
We also evaluated Personalized PageRank on the user X  item X  X spect tripartite graph. Even with optimal tuning of each edge and vertex type, performance did not improve; thus we only report Personalized PageRank X  X  performance on the standard user X  X tem graph.
 TagRW enhances ItemRank [6] by incorporating tags into building the item X  X tem graph and performing an additional random walk on user X  X ser graph. We feed aspects (all the same settings with TriRank) as tags into the method, and tune the five parameters of the method in a sequential way, as suggested by their paper.

As the existing review-aware methods [4, 17, 19, 34] are optimized for predicting observed ratings, it is unfair to compare with them for top-N evaluation. We validate this by evaluating the Hidden Factors and Topics model [19], which is state-of-the-art for review-aware rating prediction. It achieves poor top-N performance in our settings, worse than Item Popularity. Thus we do not further compare with other methods designed for rating prediction.

TriRank has six regularization parameters to tune  X  three for the traditional collaborative filtering effect (  X , X  and three for the aspect filtering effect (  X , X , X  A ). As per-forming grid-search on all six parameters simultaneously is time-consuming, we separately tune those for CF and those for aspects  X  first fixing  X , X , X  A as 0, searching for  X , X  then performing the reverse with the optimal  X , X  U , X  I . Per-formance was stable across many parameter settings, thus we report results for a selected set.
Figure 4 plots the performance when K ranges from 10 to 50, and Table 4 shows the concrete scores obtained at posi-tion 10 and 50. We first focus on results of the Yelp dataset. From Figure 4(a) and (b), we see that both metrics exhibit the same trend: TriRank performs best, outperforming all other methods with a large margin; followed by PageRank and ItemKNN, where PageRank performs slightly better than ItemKNN. PureSVD, ItemRank and TagRW obtain similar HR scores, while NDCG tells the quality of rank-ing: PureSVD ranks correct items higher than ItemRank and TagRW. Item Popularity performs the worst, indicating the importance of modeling users X  personalized preferences, rather than just recommending popular items.
 Surprisingly, TagRW does not always outperform Item-Rank, although it utilizes additional aspect information. It shows that their method for integrating tags into recom-mendation may not be effective for aspects. Analyzing the results, we believe that there are two reasons responsible for TagRW X  X  inferior performance. First, they integrate aspects by transforming to the item X  X tem and user X  X ser similarity graph, which may lead to signal loss. Second, noisy as-pects may have an adverse impact on their method, and the impact is highly dependent on the similarity measure they use. Our proposed TriRank mitigates both of these nega-Dataset Yelp Amazon Metric (%) HR@10 NDCG@10 HR@50 NDCG@50 HR@10 NDCG@10 HR@50 NDCG@50 ItemPop 3.06 1.85 10 . 61 4 . 08 2.38 1.36 6 . 13 2 . 37 ItemKNN 4.90 3.21 15 . 72 6 . 37 11.17 9.75 12 . 69 10 . 15 PureSVD 4.79 3.17 14 . 94 6 . 16 11.52 9.64 14 . 94 10 . 55 PageRank 4.79 3.24 15 . 90 6 . 52 12.10 10.33 17 . 49 11 . 78 ItemRank 4.64 3.05 14 . 67 6 . 01 10.92 8.97 16 . 84 10 . 58 TagRW 4.36 2.85 15 . 25 6 . 02 11.23 8.96 17 . 47 10 . 65
TriRank 5.92  X  X  X  3.97  X  X  X  18.58  X  X  X  7.69 tive factors by 1) directly modeling aspects into the user X  item relation as a tripartite graph, and 2) ranking vertices by regularizing the tripartite graph.

With respect to the Amazon dataset, TriRank again achieves the best performance on both metrics ( p &lt; 0 . 01 in most cases). Focusing on Figure 4(c) that shows the HR scores, TriRank is followed by PageRank and TagRW, which signif-icantly outperform other methods. When K is set to 30 X  40, the HR differences between TriRank and PageRank and TagRW are small, but the NDCG reveals significant gaps among the three methods, indicating that TriRank success-fully orders the correct items more effectively than the other two. Meanwhile, TagRW and ItemRank better PureSVD, as evaluated by HR ( K  X  15), but not by NDCG, which indi-cates the matches of TagRW and ItemRank actually occur at lower ranks. This reinforces our point that a good recall score does not necessarily translate to a high-quality rank-ing, hence the necessity to evaluate by ranking based mea-sures, such as NDCG. ItemKNN performs worst among all the non-trivial personalized methods. ItemPop performed very weak, and as such, was entirely omitted in the figure to better highlight the performance of the other methods.
Looking at the interesting performance variations across the two datasets, we first notice that ItemPop only per-forms well on the Yelp dataset. We believe this is caused by consumption behavior differences across the two domains  X  people may visit popular restaurants or businesses rated in Yelp, but only purchase certain products on demand from Amazon. Similarly, ItemKNN performs strongly on the Yelp dataset (better than PureSVD), but poorly on the Amazon. One possible reason comes from data sparsity: as in Ta-ble 3, each item of the Amazon dataset only has 3 . 9 reviews on average. In such cases, the similarity measure fails in neighbor-based CF. An interesting finding is that PageRank consistently outperforms ItemRank, although both rely on Personalized PageRank with the same personalized vector. We believe the explanation is due to the fact that ItemRank ranks based on the transformed item X  X tem correlation graph. Transforming the user X  X tem graph to an item X  X tem correla-tion graph will lose signal especially when the data is sparse, e.g. , when two items have no common users reviewing them. In such cases, it is more beneficial to directly rank from the user X  X tem graph. Finally, TagRW betters ItemRank only on the Amazon dataset, indicating that the tag-based method to integrate aspects does not lead to consistent improvement. Our proposed TriRank achieves the best performance on the two datasets evaluated by both metrics, demonstrating its superiority in providing personalized item ranking to users by mining aspects in reviews. Table 5: Performance of TriRank with different pa-rameter settings at rank 50 .

There are natural issues about aspects that we also wish to address: 1. How do the aspect-related components ( e.g., item X  2. How does the quality of aspects impact the perfor-
As TriRank is modular, with parameters for each type of vertices and edges, it is easy to answer the first ques-tion by varying the aspect-related parameters:  X  and  X  to control the smoothness for the item X  X spect and user X  X spect relation, respectively, and  X  A for the aspect query vector. Setting a parameter to 0 removes the corresponding effect.
Table 5 shows TriRank X  X  performance with different pa-rameter settings, evaluated at rank 50. In both datasets, when item X  X spect smoothness is eliminated by setting  X  = 0 (Row 1), performance drops significantly. This indicates the importance of item X  X spect relation, and validates our motivation that modeling user preference via decomposed aspects can yield more fidelity over modeling user X  X tem rat-ings only. In contrast, when user X  X spect smoothness is re-moved (Row 2), the performance remains unchanged. This shows that user X  X spect smoothness contributes substantially less to TriRank X  X  performance; however, we note that at least the target user X  X  portion of the user X  X spect relation can not be removed in recommendation, as the rated aspects of a user form her aspect query vector needed for recommenda-tion. Row 3, which exhibits low performance, validates this point, as here we have removed the aspect query vector by setting  X  A as 0. If we remove the modeling of aspects in its entirety (Row 4), TriRank degrades to the Bipartite User X  Figure 5: TriRank performance with respect to per-centage of top aspects selected.
 Item Ranking algorithm [9] on the user X  X tem graph, and performs even worse. This further reveals the importance of modeling aspects for quality recommendation.

To round out our study, Row 5 shows the performance of removing user X  X tem smoothness, which encodes standard CF in the terminology of our regularization approach. The resulting performance is worst among all settings. The user X  item relation is still fundamental to model and is most im-portant, followed by the importance of the item X  X spect smooth-ness; user X  X spect smoothness contributes least and may be removed. However, the reviewed aspects of a particular tar-get user are still critical for capturing her personalized pref-erence.
For the second question, we first rank aspects by their tf  X  idf score in the item X  X spect matrix, and then select top scoring aspects to build the tripartite graph and inspect Tri-Rank X  X  performance.

Figure 5 shows TriRank X  X  performance with respect to percentage of top aspects selected. As we can see, both datasets show the same trend: when the filtering ratio is moderate, performance remains largely unchanged. The filtering inflection point for both datasets is around 30%. When we filter out aspects beyond this point, performance starts to drop significantly. This indicates that the aspects with tf  X  idf play a dominant role in modeling users X  prefer-ence for quality recommendation. We further validate this conclusion by filtering in the reverse direction (not shown; i.e. , dropping the top x % aspects ranked by tf  X  idf ), finding that even a small amount of filtering (1%) leads to signif-icant degradation. We conclude that one can safely filter out the low tf  X  idf scoring aspects for efficiency, as they contribute less to recommendation performance.

Interestingly, TriRank X  X  performance does not improve when only high tf  X  idf aspects are utilized. Although slight im-provements can be obtained with tuning, they are not statis-tically significant. We further evaluate TriRank with the 124 high-quality aspects selected in [34] X  X  work on the same Yelp Challenge dataset. Even after all parameters are re-tuned, test performance is not improved. This validates the nice property of TriRank of being relatively insensitive to noisy aspects, which are also expected to have less impact in the ranking outputs as previously explained (Section 3.4). Tri-Rank can effectively utilize the merits in the automatically extracted aspects, without the need to filter out noisy as-pects manually. Compared to the Explicit Factor Model [34] that integrates only high-quality aspects into a matrix fac-torization model and then generates recommendations by optimizing the predicted ratings in an opaque manner, Tri-Figure 6: Training reviews of a sampled Yelp user. Rank is more transparent in leveraging aspects and also is more tolerant of low-quality aspects.
While macro-level empirical analysis are useful, it is also instructive to examine actual results to better understand the outputs of TriRank. To this end, we give two case stud-ies drawn from the Yelp dataset to demonstrate its explain-ability and scrutability. Figure 6 shows four training reviews of a sampled user 5 . From the first two reviews, we can see the user is inter-ested in  X  X hicken X , although she gives low ratings to the two businesses. In the heldout test set, she reviews the business Chick-Fil-A with a comment  X  I love Chick-Fil-A... the spicy chicken sandwhiches [sic], the lemonade, the soup, the brownies  X , which further validates her preference for  X  X hicken X . As expected, TriRank ranks Chick-Fil-A highly (6 position), mainly due to chicken being a top aspect of this business (3 of its 7 training reviews mentioned  X  X hicken X ). Examining the top items returned by PageRank, none have  X  X hicken X  as a top aspect, and most of them are popular items with more than 100 reviews. This is because random walk models are easily biased to popular items, as reported by [1]. Moreover, the third and fourth reviews show that the user is also interested in  X  X hrimp X . As a result, TriRank ranks the seafood restaurant Red Lobster highly in the 3 position. Although it is evaluated as a loss as the test set does not contain the item, when we checked her complete history in Yelp.com, we found she actually reviewed this restaurant later (outside of the dates in the Yelp Challenge dataset), mentioning  X  X hrimp X . Again, the recommended Red Lobster is not a popular item with only 7 training re-views. This case study demonstrates TriRank X  X  capability of recommending more relevant and personalized items (not just popular items) according to a user X  X  reviewed aspects.
Another key property of our TriRank instantiation is the encoding of aspect query vector ~a 0 , serving as the gateway to edit a target user X  X  preference. We simulate the process on a sampled user 6 .

For this user, 9 of the 14 training reviews mentioned  X  X er-vice X , which is the top aspect, followed by  X  X eer X . However in the test set, he reviews the business  X  Total Wine &amp; More  X , whose top aspects are  X  X ine X  and  X  X iquor X . In this case, both
User ID  X  8fTTvS499XCz4oP49kxq8A  X . Only part of each review is shown as the original is long.
User ID  X  omoEjYFKVV7e-DtnezeUOw  X . TriRank and PageRank fail to recommend the correct item, and all top items returned do not have wine as a specialty. We simulate user feedback by editing the aspect query vec-tor to set  X  X ine X  to a higher value, and re-run TriRank with all other parameters unchanged. In the updated ranked list, 8 of the top-10 items have  X  X ine X  as the top aspect, and the correct item  X  Total Wine &amp; More  X  is ranked in 2 nd position.
While collaborative filtering systems perform well in gen-eral, their performance suffers when the amount of user feed-back is insufficient ( i.e., in cold-start). Another important shortcoming is that they do not capture the rationale for user X  X  rating, and thus can not accurately capture a target user X  X  preference. To overcome these weaknesses, various forms of side information have been incorporated into CF, including tags [35], geo-location [11] and user reviews [19]. In this section, we first study the area of review-aware methods, and then examine graph-based recommendation techniques as TriRank falls into this category.
User reviews have been utilized to assist recommender systems in many domains, for movies [4], hotels [20], restau-rants [5] and e-commerce [19]. Regardless of domain, we can categorize the approaches based on how reviews are inte-grated into the recommender: 1) word-based, 2) sentiment-based, and 3) aspect-based methods.

Word-based. These approaches directly factorize the re-view words into CF. [26] used words to measure similarity, whereas [11] modeled each word as a latent vector within the latent factor model. As the original word space is large and sparse, dimension reduction techniques have been adopted. McAuley and Leskovec [19] employed Latent Dirichlet Allo-cation (LDA) [2] to winnow down the word space, and com-bined with the standard latent factor model. Subsequently, [17, 30] adopted a full Bayesian treatment to combine topics and latent factors for rating prediction.

Sentiment-based. These approaches utilize the user X  X  explicitly mentioned opinions on items. [21] proposed to fill in the missing ratings with a predicted sentiment score before applying neighbor-based CF. [22] built a user X  X tem opinion matrix, where each entry was the aggregated sen-timent score of a review, and then applied traditional CF on the opinion matrix. More recently, [4] proposed an inte-grated graphical model to jointly model the sentiment and ratings for movie recommendation.

Aspect-based. Our work falls into this category. Early work [5] along this line manually annotated six aspects in the restaurant domain ( e.g., service, ambiance, etc. ), and classified sentences with respect to these aspects. Their re-gression method validated the usefulness of aspects for rat-ing prediction. Musat et al. [20] built topical profiles of users and items from reviews, and predicted ratings at the topic level. They tested two ways to extract topics  X  LDA and opinion word frequency  X  finding the latter produced higher quality topics. Recently, Zhang et al. [34] jointly factorized the user X  X tem rating matrix by inserting aspects, decompos-ing it into item X  X spect and user X  X spect matrices, where the aspects were automatically extracted.

Several hybrid methods have also integrated aspect and sentiment [4, 5, 34] as they are closely related. In con-trast, our work focuses on integrating aspect into CF for explainable recommendations, thus we forgo incorporating sentiment, to minimize the reliance on sentiment analysis accuracy. Compared to the above review-aware works, our method explores a graph model to integrate aspects, which has not been previously been investigated. Moreover, our proposed TriRank affords the recommender a finer degree of user interaction  X  aspect preference  X  allowing for both more accurate and transparent recommendation.
Graphs form a natural representation for modeling the relationship among data objects. In recommender systems, graph models have been used widely and commercially ( e.g., by Twitter [7] and YouTube [1]), due to their good inter-pretability in generating recommendations. A typical work-flow is first representing items as vertices of a graph, and then admitting recommendation as a vertex-ranking prob-lem. For example, in YouTube video recommendation, [1] built a user X  X ideo co-view graph for video items, adopting label propagation for selecting important videos.

Besides directly working on the heterogeneous user X  X tem graph, another family of approaches [6, 18, 35] projects the user X  X tem graph to an item X  X tem graph (as the ranking tar-get is item), and then applies homogeneous graph ranking techniques, such as personalized PageRank [8]. Specifically, [6] recommended based on an item correlation graph, where entries denoted the likelihood that two items are co-rated. [18] proposed an item preference graph, where entries de-noted the strength that users prefer one item over another.
We point out that a key advantage of retaining the user X  item structure is that additional information can be eas-ily incorporated by adding new types of vertices. However, existing ranking algorithms do not cater to heterogeneous structures, as they have primarily focused on homogeneous graphs [8, 36] or bipartite graphs [9]. Thus, a corresponding algorithm must be devised to suit the specific heterogeneous graph and ranking purpose. For example, [29] modeled long-term and short-term user preference by introducing session nodes, ranking vertices by propagating user preference via Breadth-First-Search ; [15] incorporated contexts ( e.g. , loca-tion, time) as vertices in the user X  X  side, and adjusted PageR-ank for ranking in such a mixed bipartite graph. Similar to above works, our method retains the user X  X tem structure, extending it to a user X  X tem X  X spect tripartite graph for mod-eling aspects. One major difference is that we specifically consider the ternary relationship between user, item and as-pect, which has not been studied before.
We have studied how to utilize item aspects in user re-views for top-N recommendation. We model the user X  X tem X  aspect relation as a tripartite graph, and propose TriRank, a generic algorithm for ranking the vertices of tripartite graph by regularizing the smoothness and fitting constraints. We employ TriRank for review-aware recommendation, where the ranking constraints directly model the collaborative and aspect filtering, and also personalization. TriRank achieves state-of-the-art performance over two public review datasets, even with automatically extracted aspects that have signifi-cant noise. We validate TriRank as being largely insensitive to low-quality aspects, a desirable property when porting to other domains as it avoids manual efforts in filtering out noisy aspects. Most importantly, TriRank X  X  incorporation of aspects provides users with more transparency into the recommender system behavior and affords user interaction to further improve recommendations.

Our introduction to TriRank is in its basic form, which has already shown significant utility. TriRank can be fur-ther extended in many ways, for example by adopting a more suitable loss function, or by extending to more general n -partite graphs. We hope to extend TriRank to such cases, by jointly modeling the additional information latent in user reviews that may be useful for recommendation: temporal factors [9], category taxonomies [10] and sentiment [21]. An-other open issue is in optimal parameter settings. Our cur-rent work reports results when the regularization parameters are set uniformly for all users; however in some exploratory work, we found that a specific parameter setting for a subset of users improves performance. This reveals the potential for improvement by setting parameters individually or for groups of similar users based on their preference on aspects. We thank Yongfeng Zhang, Haochen Zhang and the Ts-inghua Information Retrieval group for developing and shar-ing their aspect extraction tool. We would also like to thank the anonymous reviewers for their valuable comments.
