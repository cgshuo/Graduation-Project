 Jurij Leskovec Jurij.Lesk ovec@ijs.si Jozef Stefan Institute, Ljubljana, Slovenia John Shawe-Taylor John@cs.rhul.a c.uk Royal Hollo way Univ ersit y of London, Egham, UK Boosting is a metho d of com bining so-called weak learners that individually only perform sligh tly bet-ter than a random classi er into a weigh ted com bi-nation that classi es with high accuracy . In general boosting has been sho wn to exhibit a remark able re-sistance to over tting. An explanation for this phe-nomenon suggests that this results from boosting op-timising the margin of the underlying weigh ted com-bination of weak learners [10].
 This interpretation has suggested a num ber of mo di-cations of the underlying boosting strategy through changing the measure of the margin distribution that is optimised [8]. Taking a 1-norm of the slac k variables and optimising the 1-norm of the coecien ts leads to a linear programme. If this is solv ed by so-called col-umn generation, the resulting algorithm can be seen as a boosting algorithm [2, 3], where the primal solution gives the weigh tings of the weak learners and the dual solution the distributions over examples.
 The question of how to adapt boosting to handle un-even training sets was considered by Karak oulas and Sha we-T aylor [7]. They introduced the U-b oost algo-rithm that biased the distribution weigh ting in favour of the examples from the smaller class.
 The aim of the curren t pap er is to introduce a version of the linear programming boosting that is tuned for unev en datasets. This is a natural approac h to han-dling unev en datasets as the algorithm optimises a cost criterion that can be adapted to re ect the unev enness of the dataset.
 At the same time we presen t extensiv e exp erimen ts comparing a num ber of di eren t boosting strategies and metho ds of weak learner generation. Our main test bed for the exp erimen tal work is the Reuters doc-umen t collection. In this section we introduce the boosting algorithms considered in our exp erimen ts, including the adapta-tion of the linear programming algorithm for unev en datasets.
 Boosting is a metho d to nd a highly accurate classi-cation rule by com bining man y weak or base hypothe-ses . Eac h weak hypothesis may be only mo derately ac-curate. Weak learners are trained sequen tially . Eac h weak learner is trained on examples whic h the preced-ing weak learners found most dicult to classify . 2.1. AdaBo ost amples. Eac h instance x i belongs to a domain X and has assigned a single class y i . Eac h class y i belongs to a nite lab el space Y . In this pap er we focus on binary classi cation problems in whic h Y = f 1 ; +1 g . We call examples having y i = +1 positiv e and those having y i = 1 negativ e examples.
 A weak learning algorithm accepts as input a sequence of training examples S and a distribution D t (where D ( i ) could be interpreted as the misclassi cation cost of the i -th training example). Based on this input the weak learner outputs a weak hypothesis h . We inter-pret the sign of h ( x ) as the predicted lab el and the magnitude j h ( x ) j as the con dence in that prediction. The parameter t is chosen to measure the imp ortance Giv en and a training set: S = ( x 1 ; y 1 ) ; : : : ; ( x where x i 2 X ; y i 2 f 1 ; +1 g Initialise: D 1 ( i ) = w For t = 1 ; : : : ; T : Output the nal hypothesis: f ( x ) = P T t =1 t h t ( x ) of the weak hypothesis h t in the nal linear com bina-tion of weak hypotheses.
 At eac h round of boosting AdaBo ost [11] increases the weigh ts of wrongly classi ed training examples (i.e. if the signs of h t and y i di er) and decreases the weigh ts of correctly classi ed examples. 2.2. AdaUBo ost A common scenario when learning on imbalanced data sets is that the trained classi er classi es all examples to the ma jor class. In text classi cation we often en-coun ter this problem when we are learning a binary classi er to separate a small topic from the rest of the documen ts. In order to avoid this problem, we would like to emphasise the imp ortance of the smaller class. We assume that the smaller class is the positiv e class while the dominan t class is the negativ e one. AdaUBo ost [7] utilises most of the ideas of Ad-aBo ost [11], but introduces an unequal loss function and mo di ed weigh t updating rule. The choice of a parameter will force unev en misclassi cation costs for training examples ( gure 1).
 Examples from the smaller, positiv e, class will initially get assigned times larger weigh ts than negativ e ex-amples. The parameter will also guide the weigh t updating rule to increase the weigh t of false negativ es more aggressiv ely than of false positiv es. On the other hand it will decrease the weigh ts of true positiv es more conserv ativ ely than of true negativ es. Under this up-dating rule the weigh ts of positiv e examples typically main tain higher values. Eac h weak hypothesis there-fore tends to correctly classify more positiv e examples, since they main tain higher weigh ts. The nal hypoth-esis, a linear com bination of the weak hypotheses, will also correctly classify more positiv e examples. 2.3. LPBo ost LPBo ost [5] is a linear program (LP) approac h to boosting. The pap er [5] sho ws that taking a 1-norm of the slac k variables and optimising the 1-norm of the coecien ts leads to a linear programme. If this is solv ed by so-called column generation, the resulting algorithm can be seen as a boosting algorithm [2, 3], where the primal solutions give the weigh tings of the weak learners and the dual solutions the distributions over examples.
 LPBo ost iterativ ely optimises dual misclassi cation costs and dynamically generates weak hypotheses to mak e new LP columns. In con trast to gradien t boost-ing algorithms, whic h may only con verge in the limit, LPBo ost con verges in a nite num ber of iterations to a globally optimal solution satisfying well-de ned op-timalit y conditions.
 One would exp ect LPBo ost to be computationally ex-pensiv e. We found, however, that an iteration of LP-Boost is sligh tly more exp ensiv e than an iteration of AdaBo ost, but on the other hand LPBo ost needs far few er iterations than AdaBo ost to con verge. 2.4. LPUBo ost The LPBo ost algorithm [5] is motiv ated from a gener-alisation analysis that bounds the test error in terms of the 1-norm of the vector of coecien ts, margin achiev ed and the slac k variables. The minimisation of the bound leads to a Linear Programme that gives the form ulation above when con verted to the dual. We now give a similar motiv ation for the unev en ver-sion of the LPBo ost algorithm, we have named LPU-Boost. In this case we assume that the cost of mis-classifying a positiv e example (assuming that the pos-itiv e class is the less populated) is larger than that of misclassifying a negativ e example. Hence, the loss L f asso ciated with a classi cation function f is given by where as with AdaUBo ost &gt; 1 gives the higher cost of misclassi cation of positiv e examples. We can upp er bound this loss as follo ws L ( x; y ) where &gt; 0 and ( ) + denotes the function that is the iden tity if its argumen t is greater than 0 and 0 other-wise. We can now apply the Rademac her technique [1] Giv en and a training set: S = ( x 1 ; y 1 ) ; : : : ; ( x where x i 2 X ; y i 2 f 1 ; +1 g Initialise: = 0, n = 0, LP = 0 Repeat End = Lagrangian multipliers from last LP Output the nal hypothesis: f ( x ) = P n j =1 j h j ( x ) to bound the loss in terms of its empirical value and the Rademac her complexit y of the function class. The Rademac her complexit y of a class does not increase if we move to its con vex hull and so the bound involves the 1-norm of the slac k variables scaled by the inverse margin plus the Rademac her complexit y of the weak learners again scaled by the inverse margin. We omit the details because of space constrain ts. The result-ing bound is optimised by the follo wing Linear Pro-gramme: The parameter D con trols the trade o between max-imising the margin and con trolling the slac k vari-ables. Those asso ciated with positiv e examples re-ceiv e times the weigh t of those from negativ e ex-amples in line with the bound (1), while the condition P i =1 i = 1 ensures the resulting function lies in the con vex hull of the set of weak learners. The dual Lin-ear Programme of the above is solv ed by the algorithm LPUBo ost of Figure 2.4 with D = D and D + = D . For LPUBo ost we tak e the same LP form ulation of boosting as in LPBo ost, but we introduce a new pa-rameter having the same role as in AdaUBo ost. Positiv e examples have a times higher bound on choice of parameter D LB con trols the lower bound on the misclassi cation cost u i . We could set it to 0, but we rather set D = 1 m , where 2 (0 ; 1), and sensitivit y of LP and LPU boost to the value of pa-rameter , whic h has to be tuned with some care to obtain a good con vergence rate.
 LPUBo ost com bines AdaUBo ost having an unev en loss function and LPBo ost having a well-de ned stop-ping criterion with a mec hanism to prev ent over-tting. This mak es LPUBo ost a good algorithm for learning on unev en training sets: main taining higher weigh ts on positiv e examples while using an LP to ob-tain an optimal com bination of weak hypotheses with resp ect to a well-motiv ated optimisation criterion. A weak learning algorithm is a pro cedure for comput-ing weak hypotheses. Boosting nds a set of weak hypotheses by rep eatedly calling a weak learning al-gorithm. The weak hypotheses are linearly com bined into a single rule. The input of the weak learning algo-rithm is a distribution or vector of weigh ts (misclassi-cation costs), and a training data set. Weak learning algorithms use the weigh ts to nd a weak hypothesis whic h has a mo derately low error with resp ect to the weigh ts.
 Due to the weigh t updating rule examples whic h are hard to classify will get incremen tally higher weigh ts while the examples easy to classify get lower weigh ts. The e ect is to force the subsequen t weak learners to concen trate on hard-to-classify examples. 3.1. AdaBo ost.MH class of weak hypotheses Boosting is a general purp ose metho d that can be com-bined with any weak learner. In practice it has been com bined with a wide variet y of classes including de-cision trees and neural net works.
 In this pap er we focus on boosting using very simple classi ers. All classi ers considered in this section have the form of a one level decision tree ( if-then rule). The test is to chec k for the presence of a word in a given documen t. Based on the presence of the word the weak hypothesis outputs a prediction. We interpret the sign of the prediction as a predicted class (recall we are dealing with binary problems) and the magnitude of the output as the con dence in that prediction. For example: if we try to predict whic h documen ts belong to the Sports category , we will train a classi-er to mak e a distinction between Sports documen ts (positiv e class) and the rest of the documen ts (nega-tive class). Then our weak hypothesis could be a rule: \if the word football occurs in a documen t, then we are highly con den t the documen t belongs to Sports category . On the other hand, if football does not occur in a documen t then we predict the documen t does not belong to the Sports category with low con dence." Formally , we write w 2 x to mean a term w occurs in documen t x . So we de ne a weak hypotheses h whic h mak es predictions: where c + and c are real num bers.
 Let us also de ne: given a curren t distribution D t and a term w : let X + be a set of documen ts having the term w , X + = f x : w 2 x g , X = f x : w = 2 x g and b; l 2 f + ; g then we calculate W has word class : The weak learners presen ted in this subsection all have the same form of weak hypotheses, but they imp ose di eren t restrictions on the values c + , c and t and use di eren t criterion to choose a weak hypothesis at eac h round of learning.
 At eac h round of learning our weak learners searc h all possible terms. For eac h term, the values c + and c and a score are assigned to that particular weak learner. After all terms have been searc hed, a learner with best score is chosen. Di eren t weak learners use di eren t scores.
 For AdaBo ost.MH a bound on empirical loss (frac-tion of misclassi ed examples) has been pro ven by Schapire and Singer [11]. They sho wed that the Ham-ming loss (MH stands for minim um Hamming loss) of the boosted function obtained using AdaBo ost.MH is at most Q T t =1 Z t , where Z t is a normalization factor at round t . This upp er bound can be a used as a guide-line for choosing t and the design of the weak learning algorithm. 3.1.1. Real AdaBoost.MH The rst algorithm is called AdaBo ost.MH ( Real.MH ) with real-v alued predictions [12]. We permit c + and c to be unrestricted real valued predictions. It was sho wn in [11] that Z t is minimised by choosing and setting t = 1 implies that: Thus we choose the term w for whic h Z t has the min-imal value. As suggested in Schapire and Singer [12] we smo oth the values of c b to limit the magnitudes of predictions We use = 1 m . Since W b + and W b 2 [0 ; 1], this bounds j c j by roughly 1 2 ln(1 = ). 3.1.2. Disc AdaBoost.MH AdaBo ost.MH with discrete predictions ( Disc.MH ) forces the predictions c b of the weak hypothesis to be either +1 or 1. This is a more traditional setting where predictions do not carry con dences, and t is a measure of con dence in the weak hypothesis. We still minimise Z t for a given term w . Using the same notation as introduced in the previous section, we set: We can interpret the choice of c b as a weigh ted ma-jorit y vote over training examples. Let r t = j W + + W + j + j W order to minimise Z t we should choose: giving Z t = p 1 r 2 t . So we choose a weak hypothesis (a term w ) whic h has the smallest Z t . 3.1.3. Disc and Real AdaBoost.LP The LPBo ost algorithm [5] (see Section 2.3) suggests that at eac h round of boosting a weak hypothesis h with maximal sum of misclassi cation costs D t multi-plied by class value y i 2 f 1 ; +1 g and prediction h : should be chosen. Since this is a di eren t criterion for choosing the best weak-h ypothesis in AdaBo ost.MH, we obtain two new weak learners. We call them Real.LP and Disc.LP . They di er from AdaBo ost.MH in the way weak hypothesis is chosen, instead of min-imising Z t , DtSum is maximised. 3.1.4. Disc and Real AdaBoost.U Karak oulas and Sha we-T aylor in their pap er on boost-ing imbalanced training sets [7] prop osed a new metho d for calculating Z t and choosing t . Note that in case of an unev en loss function (AdaUBo ost, LPUBo ost) we have an additional pa-rameter . Positiv e examples will get times higher weigh t than negativ e examples. The parameter will also guide the weigh t updating rule to increase the weigh t of false negativ es more aggressiv ely than of false positiv es. We de ne Z t as: where i = 1 = if y i = +1 and 1 if otherwise. To minimise the error we seek to minimise Z t with re-spect to t . By taking the rst deriv ativ e of (10) and equating it to zero and introducing notation W c;p = P we get: exp( t = ) W ++ = + exp( t = ) W + = + exp( t ) W + exp ( t ) W = 0. Substituting Y = exp( t ) we obtain: where C 1 = W ++ = ; C 2 = W + = ; C 3 = W + ; C 4 = W . The root of equation (11) can be found numerically . Z 00 t ( t ) &gt; 0 implies Z t ( t con vex and has only one minim um.
 A weak hypothesis h with minimal Z t is chosen. This is another way of calculating t and two new weak learners can be obtained ( Real.U, Disc.U ). They di er from AdaBo ost.MH only in the way Z t is calculated and for discrete learners we set t to be the solution of (11) whic h minimises Z t . The follo wing sections describ e the exp erimen tal setup. We also describ e and analyse exp erimen ts per-formed using the four boosting algorithms and six text categorization weak learners that were describ ed in the previous sections. 4.1. Experimen tal setup We performed empirical evaluation on the Mo dApte split of the Reuters-21578 dataset compiled by David Lewis. The split consists of 12 ; 902 documen ts of whic h 9 ; 603 are used for training and 3 ; 299 for testing. The follo wing prepro cessing was performed: all words were con verted to lower case and punctuation marks were remo ved. We remo ved stop words from a list of 523 English words. We used the Porter stemmer [9] and retained only those terms having documen t fre-quency larger than 3. After the prepro cessing the cor-pus con tained 6 ; 242 distinct features (terms). We compare some of the results with Supp ort Vector Mac hines [4]. We used the SV M lig ht [6] implemen ta-tion of SVM with a linear kernel. 4.2. Performance on Reuters categories Based on category size we have chosen a set of 16 Reuters-21578 categories. Some of them are large ( earn , acq ) and some really small ( potato , platinum ) having only a few examples.
 We trained boosting binary classi ers to mak e predic-tions whether a documen t belongs to a category or not. We assigned all documen ts having the category a positiv e class and all other documen ts a negativ e class. We ran a set of 120 exp erimen ts for a single Reuters category using com binations of all the describ ed boost-ing algorithms and weak learners. In all exp erimen ts a num ber of rounds of learning was set to 300. We tested the com binations of the follo wing parameters: : 0.1, 0.2; D LB : 0, 10, 50, 100; : 2, 4, 8. For eac h boosting algorithm we displa y the best ex-perimen t using the standard information retriev al F1 score on the test dataset. We realise that choosing the best performance on the test set invalidates their sta-tus. But the aim of this exp erimen t was to get the idea about the best possible performance (upp er bound) of various algorithms. Table 1 sho ws results for chosen categories.
 As we can see from the averages LPUBo ost (LPU) is dominan t, follo wed by the LPBo ost (LP) and AdaUb oost (U). AdaBo ost (Ada) and linear SVM are far behind. On large categories Ada, U and SVM are a little better than LP and LPU, but as we move to smaller categories the qualities of LPU (and also LP and U to some exten t) seem to app ear. AdaUBo ost has the feature of unev en loss function whic h helps and LPBo ost has the mec hanism to nd an optimal com bination of weak hypotheses; both these features are com bined in LPUBo ost.
 On small categories Ada and U over t the training data ( gure 3). AdaUBo ost is more resistan t to over-tting than AdaBo ost. On the other hand the perfor-mance of LP and LPU on the training set decreases with decreasing category size, but on the test set it remains at about the same level. We can see that LP and LPU are sup erior to Ada and SVM while U does a little worse than LP and LPU.
 Secondly we performed the same set of 120 exp eri-men ts using strati ed 5 fold cross validation. Based on average F1 score over 5 trials, we have chosen best parameter con guration for eac h algorithm. Table 2 sho ws the average F1 on test set.
 We can see that results obtained by cross validation (table 2) are not far from the optimal (table 1). For algorithms with a small set of parameters (AdaBo ost, SVM) the di erence between optimal and cross vali-dation performance is small. There is a surprisingly large gap for U and even larger for LP.
 SVM performs best of all algorithms on categories with more than 1% of positiv e training examples, but as we decrease the category size it is no more comp etitiv e. LPUBo ost performs best and not far from optimal. We also observ ed that optimal parameter settings are di eren t from those obtained by cross validation. Per-formance of LPUBo ost is quite stable on the whole range of categories of various sizes.
 Figure 4 sho ws typical learning curv es of boosting algo-rithms. In the top row we see the typical performance of AdaBo ost on large categories and the oscillations we noticed in AdaUBo ost. The bottom row sho ws LP and LPU. We observ e larger jumps in performance than for instance with Ada or U. Typically at the early rounds of learning LP (LPU) is not stable, but when we move forw ard performance gets more stable, when nally the algorithm con verges. LP and LPU use man y few er weak hypotheses than Ada or U . We trained Ada and U for 300 rounds (300 weak hypotheses were chosen). LP and LPU con-verged in around 50 rounds for large and around 5 or less rounds of learning for small categories. For some of the smallest categories LPU picked just 1 or 2 weak hypotheses and made no error { for category platinum it is sucien t to chec k for the presence of a word plat-inum in order to mak e perfect predictions.
 Considering chosen weak learners we see Real.MH gives best performance for most of the categories. This is in accordance with the results rep orted in [12]. Real.MH is follo wed by Disc.MH and Real.LP . Special weak learners did not impro ve the performance { not even in com bination with the boosting algorithm they were designed to work together. 4.3. Discarding positiv e training examples We took 2 largest categories: earn and acq . Training set consists of all negativ e and a num ber of randomly selected positiv e training examples. Test set is un-changed. By selecting a num ber of positiv e training examples we arti cially created a small category . We performed the same set of 120 exp erimen ts as in section 4.2. For eac h boosting algorithm we displa y the best exp erimen t using the F1 score on the test set. This means we sho w the upp er bound of the algorithm. Figure 5 sho ws the results whic h are surprising. Pre-vious exp erimen ts sho wed that LPU is best on unev en training sets, because it does not over t and picks a sucien tly small num ber of weak learners. But gure 5 sho ws a di eren t picture. As the num ber of positiv e training examples decreases, the performance of LPU (LP) also dramatically decrease, but the performance of U (Ada) remains almost at the same level. We observ ed almost the same things with acq : U is still the best, closely follo wed by LPU. Performance of Ada and LP is poor and after the num ber of positiv e training examples drops bello w 50, F1 is less than 0.1. Since exp erimen t sho wed that the best possible per-formance of LP and LPU are far behind from Ada and U, we didn't run strati ed cross validation. LPU (LP) performed very well on naturally unev en datasets. But when we arti cially create an small cat-egory , the performance of LPU decreased dramatically . This suggests that there is a fundamen tal di erence be-tween naturally small and arti cially small categories. We think that Reuters' editors categorizing documen ts made earn very div erse (broad and not speci c), while a small category like platinum is very speci c. Note that the test set is unc hanged so it resem bles original category (has the same distribution as original cate-gory). To mak e good predictions using small training (and large div erse testing) earn one has to \over t" by taking all (not necessarily signi can t) features of the training data. On the other hand we have to be very careful and tak e only really signi can t features to mak e good predictions on platinum . This pap er introduces LPUBo ost boosting algorithm. We pro vide both theoretical and empirical evidence that LPUBo ost is well suited for text categorization for unev en data sets.
 LPUBo ost has man y bene ts over gradien t-based ap-proac hes: nite termination at globally optimal so-lution, well-de ned con vergence criteria, unequal loss function and use few er weak hypotheses.
 [1] Peter L. Bartlett and Shahar Mendelson.
 [2] Kristin P. Bennett, Ayhan Demiriz, and John [3] Kristin P. Bennett, Ayhan Demiriz, and John [4] N. Cristianini and J. Sha we-T aylor. An Intro-[5] Ayhan Demiriz, Kristin P. Bennett, and John [6] Thorsten Joac hims. Text categorization with sup-[7] Grigoris Karak oulas and John Sha we-T aylor. Op-[8] Huma Lodhi, Grigoris Karak oulas, and John [9] M. F. Porter. An algorithm for sux stripping. [10] R. Schapire, Y. Freund, P. Bartlett, and W. Sun [11] Rob ert E. Schapire and Yoram Singer. Impro ved [12] Rob ert E. Schapire and Yoram Singer. Boostex-
