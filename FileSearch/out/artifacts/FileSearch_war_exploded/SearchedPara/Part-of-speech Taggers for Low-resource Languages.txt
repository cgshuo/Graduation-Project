 We address the challenge of creating accurate and robust part-of-speech taggers for low-resource languages. We aim to apply our methods to the hundreds, and potentially thousands, of languages with meager electronic resources. We do not as-sume the existence of a tag dictionary, or any other sort of prior knowledge of the target language. In-stead, we base our methods entirely on the exis-tence of parallel data between the target language and a set of resource-rich languages.

Fortunately, such parallel data exists for just about every written language, in the form of Bible translations. Around 2,500 languages have at least partial Bible translations, and somewhere between 500 and 1,000 languages have complete transla-tions. We have collected such electronic Bible translations for 650 languages. Figure 1 breaks down the number of languages in our collection according to their token count. The majority of our languages have at least 200,000 tokens of Bible translations.
 While previous studies (T  X  ackstr  X  om et al., 2013; Ganchev and Das, 2013) have addressed this gen-eral setting, they have typically assumed the exis-tence of a partial tag dictionary as well as large quantities of non-parallel data in the target lan-guage. These assumptions are quite reasonable for the dozen most popular languages in the world, but are inadequate for the creation of a truly world-wide repository of NLP tools and linguistic data.
In fact, we argue that such ancillary sources of information are not really necessary once we take into account the vastly multilingual nature of our parallel data. Annotations projected from individ-ual resource-rich languages are often noisy and unreliable, due to systematic differences between the languages in question, as well as word align-ment errors. We can thus think of these languages as very lazy and unreliable annotators of our tar-get language. Despite their incompetence, as the number of such annotators increases, their com-bined efforts converge upon the truth, as idiosyn-cratic biases and random noise are washed away.
Our assumption throughout will be that we have in our possession a single multilingual corpus (the Bible) consisting of about 200,000 tokens for several hundred languages languages, as well as reasonably accurate POS taggers for about ten  X  X esource-rich X  languages. We will tag the Bible data for the resource-rich languages, word-align them to one another, and also word-align them to the remaining several hundred target languages.
Of course, our goal is not to produce a tagger restricted to the Biblical lexicon. We therefore assume a small unannotated monolingual sample of the target language in an entirely unrelated genre (e.g. newswire). We use this sample trans-ductively to adapt our learned taggers from the Biblical genre. In our experiments, we use the CoNLL 2006 and 2007 shared-task test data for this purpose. Of course tagged data does not exist for truly resource-poor languages, so we evaluate our methodology on the resource-rich languages. Each such language takes a turn playing the role of the target language for testing purposes.
The goal of the paper is to introduce a gen-eral  X  X ecipe X  for successful cross-lingual induction of accurate taggers using meager resources. We faced three major technical challenges:  X  First, word alignments across languages are  X  Second, when using multiple resource-rich  X  Third, the parallel data at our disposal is of an
To address these challenges, we forgo the typi-cal sequence-based learning technique of HMM X  X  and CRF X  X  and instead adopt an instance-learning approach using latent distributional features. To induce these features, we introduced a new method using Canonical Correlation Analysis (CCA) to generalize the aligned information to new words. This method views each word position as consist-ing of three fundamental views: (1) the token view (word context), (2) the type view, and (3) the pro-jected tags in the local vicinity. We perform a CCA to induce latent continuous vector represen-tations of each view that maximizes their correla-tions to one another. On the test data, a simple multi-class classifier then suffices to predict accu-rate tags, even for novel words. This approach out-perform a state-of-the-art baseline (T  X  ackstr  X  om et al., 2013) to achieve average tag accuracy of 85% on newswire text.
 Figure 1: The breakdown of languages by the number of tokens in their available Bible trans-lations. The horizontal axis gives the number of tokens, and the vertical axis gives the number of languages in each token range. 2.1 Multilingual Projection The idea of projecting annotated resources across languages using parallel data was first proposed by Yarowsky et al. (2001). This early work recognized the noisy nature of automatic word alignments and engineered smoothing and filter-ing methods to mitigate the effects of cross-lin-gual variation and alignment errors. More recent work in this vein has dealt with this by instead transferring information at the word type or model structure level, rather than on a token-by-token ba-sis (Das and Petrov, 2011; Durrett et al., 2012). Current state-of-the-art results for indirectly su-pervised POS performance use a combination of token constraints as well as type constraints mined from Wiktionary (Li et al., 2012; T  X  ackstr  X  om et al., 2013; Ganchev and Das, 2013). As we argued above, the only widely available source of infor-mation for most low-resource languages is in fact their Bible translation. Perhaps surprisingly, our experiments show that this data source suffices to achieve state-of-the-art results.

Several previous authors have considered the advantage of using more than one resource-rich language to alleviate alignment noise.

Fossum and Abney (2005) found that using two source languages project-sources gave better re-sults than simply using more data from one lan-guage. McDonald et al. (2011) also found advan-tages to using multiple language sources for pro-jecting parsing constraints. In more of an unsu-pervised context (but using small tag dictionaries), adding more languages to the mix has been shown to improve part-of-speech performance across all component languages (Naseem et al., 2009).

In our own previous multilingual work, we have developed the idea that supervised knowledge of some number of languages can help guide the un-supervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013a; Kim and Snyder, 2013b). We have showed that cross-lingual supervised learning leads to signif-icant performance gains over monolingual mod-els. We point out that the previous tasks have con-sidered as word-level structural analyses and our present case as a sentence-level analysis. 2.2 Word Alignment Most of the papers surveyed above rely on auto-matic word alignments to guide the cross-lingual transfer of information. Given our desire to use highly multilingual information to improve pro-jection accuracy, the question of word alignment performance becomes crucial. Our hypothesis is that multiple language projections are beneficial not only in weeding out random errors and id-iosyncratic variations, but also in improving the linguistic consistency of the alignments them-selves. Instead of simply aligning each source lan-guage to the target language in isolation, we will instead use a confidence model to synthesize in-formation from multiple sources.

While there are not many well-known papers that have explored word alignment on a multilin-gual scale 1 , there have been related efforts to sym-metrize bilingual alignment models, using a vari-ety of techniques ranging from modifications of EM (Liang et al., 2006), posterior-regularized ob-jective function (Ganchev et al., 2010), and by considering relaxations of the hard combinato-rial assignment problem (DeNero and Macherey, 2011). 2.3 Canonical Correlation Analysis (CCA) Our method for generalizing the projections to unseen words and contexts is based on Canoni-cal Correlation Analysis (CCA), a dimensionality reduction technique first introduced by Hotelling (1936). The key idea is to consider two groups of random variables with corresponding observa-tions and to find linear subspaces with highest cor-relation between the two views. This can be seen as a kind of supervised version of Principal Com-ponents Analysis (PCA), where each view is pro-viding supervision for the other. In fact, it can be shown that CCA directly generalizes both multi-ple linear regression and Fisher X  X  Latent Discrimi-native Analysis (LDA) (Glahn, 1968).

From a learning theory perspective, CCA is in-teresting in that it allows us to prove regret-based learning bounds that depend on the  X  X ntrinsic X  di-mensionality of the problem rather than the ap-parent dimensionality (Kakade and Foster, 2007). This seems especially relevant to natural language processing scenarios, where the ambient dimen-sion is extremely large and sparse, but reductions to dense lower-dimensional spaces may preserve nearly all the relevant semantic and syntactic in-formation. In fact, CCA has recently been adapted to learning latent word representations in an inter-esting way: by dividing each word position into a token view (which only sees surrounding con-text) and a type view (which only sees the word itself) and performing a CCA between these two views (Dhillon et al., 2012; Kim et al., 2014; Stratos et al., 2014; Stratos et al., 2015; Kim et al., 2015c). CCA is also used to induce label rep-resentations (Kim et al., 2015d) and lexicon repre-sentations (Kim et al., 2015b).

Our technique will extend this idea by addition-ally considering a third projected tag view. Cru-cially, it is this view which pushes the latent repre-sentations into coherent part-of-speech categories, allowing us to simply apply multi-class SVM for unseen words in our test set. In this section, we describe two methods for incor-porating transferred tags from resource-rich lan-guages: sequence-based learning (T  X  ackstr  X  om et al., 2013; Kim et al., 2015a) and instance-based learning. In the former, the transferred tags are used to train a partially-observed CRF (PO-CRF) by maximizing the probability of a constrained lat-tice. In contrast, instance-based learning views each word token as an independent classifica-tion task, but uses latent distributional information gleaned from surrounding words as features. 3.1 A sequence learning example of partially A first-order CRF parametrized by  X   X  R d de-fines a conditional probability of a label sequence y = y 1 ...y n given an observation sequence x = x 1 ...x n as follows: where Y ( x ) is the set of all possible label se-quences for x and  X ( x,y )  X  R d is a global fea-ture function that decomposes into local feature functions  X ( x,y ) = the first-order Markovian assumption. Given fully training method is to find  X  that maximizes the log likelihood of the label sequences under the model with l 2 -regularization:  X   X  = arg max We used an l 2 penalty weight  X  of 1. Unfortu-nately, in our setting, we do not have fully labeled sequences. Instead, for each token x j in sequence x 1 ...x n we have the following two sources of la-bel information:  X  A set of allowed label types Y ( x j ) . (Label  X  Labels  X  y j transferred from resource rich
Following previous work of T  X  ackstr  X  om et al. (2013), we first define a constrained lattice Y ( x,  X  y ) = Y ( x 1 ,  X  y 1 )  X  ...  X Y ( x n ,  X  y n each position j a set of allowed label types is given as:
And then we can define a conditional probabil-ity over label lattices for a given observation se-quence x : Given a label dictionary Y ( x j ) for every token new training method is to find  X  that maximizes the log likelihood of the label lattices: Since this objective is non-convex, we find a local optimum with a gradient-based algorithm. The gradient of this objective at each example
This is the same as the standard CRF train-ing except the first term where the gold features features in the constrained lattice Y ( x ( i ) ,  X  y ) .
An important distinction in our setting is that our token and type constraints are generated by only using the transferred tags whereas T  X  ackstr  X  om et al. (2013) generate type constraints induced from Wiktionary. Our setting is more realistic for at least two reasons; 1) Wiktionary is not always available. 2) transferable information is not lim-ited, but Wiktionary is (e.g., semantic role and named entity). 3.2 Cross-lingual instance-based learning The proposed method for cross-lingual instance-based learning has three steps: 1. Select training tokens based on the confi-2. Induce distributional features over these 3. Train a multi-class classifier with these in-We will describe each step below. 3.2.1 Selecting training words Since transferred tags are not always reliable, all words in the parallel data are not necessary help-ful in training. Since this method trains on words Figure 2: Graphical representation of the confi-dence model. Unobserved variable y denotes the true target-language tag for a token. Each of the L resource-rich languages displays a project of y , as y ` , with an indicator variable z ` determining the fidelity of the projection. instead of sequences, it is easy to discard words which have unreliable or highly conflicting pro-jections from different resource-rich languages.
To select our set of training tokens, we define a simple probability-based confidence model, illus-trated in Figure 2. Suppose we have L resource-rich languages with alignments to the word in question. If the true tag is y , we assume that the projected tag for language ` will be identical to y with probability 1  X  ` , where ` is a language-specific corruption probability. With probability , the projection will instead be chosen randomly (uniformly).

To make this explicit, we introduce a corruption indicator variable z ` with: Given z ` , the probability of the projected tag y ` is given by: where m is the total number of possible tags. We can now compute a conditional distribution over the unknown tag y , marginalizing out the unknown corruption variables for each language: p ( y | y 1 ,...,y n ) = where Y is all possible tags. For simplicity, we simply set all ` to 0.1 and use y as a training label when the conditional probability of the most likely value is greater than 0.9. 3.2.2 Inducing distributional features In this section we discuss our approach for deriv-ing latent distributional features. Canonical Cor-relation Analysis (CCA) is a general method for inducing new representations for a pair of vari-ables X and Y (Hotelling, 1936). To derive word embeddings using CCA, a natural approach is to define X to represent a word and Y to represent the relevant information about a word, typically context words (Dhillon et al., 2012; Kim et al., 2015c). When they are defined as one-hot encod-ings, the CCA computation reduces to performing an SVD of the matrix  X  where each entry is where count ( w,c ) denotes co-occurrence count of word w and context c in the given corpus, count ( w ) = P The resulting word representation is given by U &gt; X where U is a matrix of the scaled left singu-lar vectors of  X  (See Figure 3). In our work, we use a slightly modified version of this definition by taking square-root of each count: This has an effect of stabilizing the variance of each term in the matrix, leading to a more effi-cient estimator. The square-root transformation also transforms the distribution of the count data to look more Gaussian (Bartlett, 1936): since an interpretation of CCA is a latent-variable with normal distributions (Bach and Jordan, 2005), it makes the data more suitable for CCA. It has been observed in past works (e.g., Dhillon et al. (2012)) to significantly improve the quality of the resulting representations. 3.3 Feature Induction Algorithm We now describe our algorithm for inducing la-tent distributional features both on the multilin-gual parallel corpus, as well as the monolingual, newswire test data. This algorithm is described in detail in Figure 4. The key idea is to per-form two CCA steps. The first step incorporates word-distributional information over both the mul-tilingual corpus (the Bible) as well as the exter-This provides us with word representations that are general, and not overly specific to any single genre. However, it does not incorporate any pro-jected tag information. We truncate this first SVD
After this CCA step is performed, we then re-place the words in the multilingual Bible data with their latent representations. We then perform a second CCA between these word representations and vectors representing the projected tags from all resource-rich languages. This step effectively adapts the first latent representation to the infor-mation contained in the tag projections. We trun-cate this second SVD to the first 50 dimensions.
We now have word embeddings that can be ap-plied to any corpus, and are designed to maximize correlation both with typical surrounding word context, as well as typical projected tag context. These embeddings serve as our primary feature vectors for training the POS classifier (described in the next section). We concatenate this primary feature vector with the embeddings of the previous and subsequent words, in order to provide context-sensitive POS predictions. 3.3.1 Multi-class classifier To train our POS tagger, we use a linear multi-class SVM (Crammer and Singer, 2002). It has a parameter w y  X  R d for every tag y  X  T and defines a linear score function s ( x ,j,y ) := y  X ( x ,j ) . Given any sentence x and a position j , it predicts arg max y  X  X  s ( x ,j,y ) as the tag of x . We use the implementation of Fan et al. (2008) with the default hyperparameter configurations for training. 4.1 Datasets and Experimental Setup There are more than 4,000 living languages in the world, and one of the most prevalently translated books is the Bible. We now describe the Bible dataset we collected. Figure 3: Algorithm for deriving CCA projections from samples of two variables.

We first collect 893 bible volumes span-ning several hundred languages that are freely available from three resources (www.bible.is, www.crosswire.org, www.biblegateway.com) and changed to UTF-8 format. The distribution of to-ken in each bible in the unit of a language is in Figure 1.

Note that the Bible scripts are not exactly trans-lated by sentences but by verses. We thus assume that each verse in a chapter has the same meaning if the number of verses is exactly same in a same chapter. We also assume that the whole chapters have the same meaning if the number of chap-ters in a book are exactly the same. In the same manner, we also assume the volumes that have the same number of chapters are the same. That is, their volume size should be as similar as possible Figure 4: Algorithm for deriving word vectors for the (unannotated) test data that use the projected tags in the Bible data. with the respect to the number of verses, chapters, and books.

Based upon these assumptions, we choose the best translation in a language based on a compar-ison to a reference Bible, the Modern King James Version (MKJV) in English. We choose the trans-lation for each language that best matches this ref-erence version in terms of chapter and verse num-bering.

There are other factors considered if there are more than one candidates satisfying this matching. We focus on the contents of the bible such as the publication time. For instance, 1599 Geneva Bible in English contains old vocabulary with different spelling systems, causing unexpected errors when tagged by POS annotation tools. Also, some of volumes such as Amplified Bible (AMP) contains extraneous comments on verses themselves, caus-ing errors for word alignments.

After the choice of the best volume, we finally criteria to select resource rich languages are hav-ing i) the matched bible scripts both on the Old and New testament and ii) reliable parts-of-speech an-notation tools. If these two requirements are satis-fied, we can freely add more languages as resource rich languages in the future research. We use Hun-pos tagger for CS, DA, DE, EN, and PT, Treetag-ger for BG, ES, IT, and NL, and Meltparser for FR. 4.2 Test Data We use CoNLL parts-of-speech tagged data (selected resource-rich languages), plus Basque (EU), Hungarian (HU) and Turkish (TR)) as our test data. It consists of 5,000-6,000 hand-labeled tokens. The accuracy of each supervised tagger on this data is about 94% on average. Since there is no French tagged CoNLL data, we exclude French on testing but still use it in Training. The accuracy of each supervised tagger on this data is shown in Table 1.

The tag definitions used in CoNLL data are not exactly matched the ones used in the taggers when converted to universal POS tags. For instance in Spanish, we initially follow mapping of Petrov et al. (2011) for CoNLL data. The  X  X p X  tag for words sus, su, mi are mapped to DET but they are mapped to PRON in the bible data because of the Treetagger definitions. Whenever we find this kind of issues, we analyze them and choose the one of mappings for compatibility. For the  X  X p X  tag, we choose to map PRON. 4.3 Alignments We perform two kinds of alignments in our data sets; (i) the verse alignment and (ii) the word alignment. When the tagged bible volumes are prepared, we align verses across all resource rich languages. For verse alignments, we pre-process to remove extraneous information such as in-line reference (e.g. [REV 4:16]) and HTML tags. These alignments between two languages occurred only when volumes have the exact same number of chapters and verses. For instance, Mark must have 16 chapters and the first chapter of the Mark must have 45 verses in our criteria. The cor-rect number of chapters and verses are pre-defined on MKJV volume, and the number of matched verses on each volume is greater than 30,500.
After performing verse alignments, we then per-form word alignments. The quality of tags in resource poor languages is highly dependent on the quality of word alignments because parts-of-speech tags will be projected through this align-ment path. First, we use GIZA++ for initial one-to-many alignments and we symmetrize by taking their intersection. This ensures that the resulting alignments are of high quality. 4.4 Results Table 2: Baseline model CONLL performance de-pending on criterion for selecting tag projection.
In all experiments, we hold out the tags of the test language. EU, HU and TR used projected tags from 10 resource-rich languages, 9 resource-rich languages are used for the remaining languages. In our first experiment, we consider the state-of-the-art PO-CRF baseline. This model trains a par-tially observed CRF based on a single projected tag for each token. We experiment with different methods of choosing the projected tags. The re-sults are shown in Table 2. The majority method is to choose the most common tag from the projected tags of the current token. We then experiment with taking the union of all projected tags (i.e. only constraining the lattice based on unanimity of the resource-rich languages). Finally, we considered choosing the high confidence tags, based on our confidence model. The confident tags are defined by a method described in Section 3.2.1 If this ratio is greater than 0.9, we assume that this token has high confidence. As the results indicate, this final method yielded the best tagging performance on the CONLL test data, achieving average accuracy of 82%.

In the remaining experiments we will adopt the confidence-based selection criterion for both the baseline as well as our method.
 Table 3: Performance on multilingual Bible data
In order to isolate the errors due to projection mismatch versus domain variation, we first test both models on the Bible data itself. To do so, we assume that the tags produced by the test-language X  X  supervised tagger are in fact the ground truth. This experiment allows us to compare to tag projection models using (1) PO-CRF and (2) CCA+SVM. Results are given in Table 3. Unsur-prisingly, PO-CRF performs better on the multi-lingual corpus than on the CONLL data, due to the beneficial constraint of the projected tags. Per-haps interestingly, the CCA+SVM method, which is a simple instance-based classifier using cleverly constructed features, outperforms the sequence la-
In our third experiment we use CoNLL test data and compare the PO-CRF models with different settings. See Table 4. This experiment is to show the effects of suffix and Brown cluster features on PO-CRF to relieve the unseen words issue. We also show that the more projecting languages are suffix and cluster features are used, respectively. included the better the results gets.

For the features, we used word identity, suffixes of up to length 3, Brown cluster and three indi-cators of (1) capitalization for the first character, (2) containing a hyphen or (3) a digit. Especially, Brown clusters was induced from more than 2 mil-lion line documents, making the setting unrealistic for resource-poor language.

With just the word features, the averaged per-formance is 0.6993 and other indicator features increase the performance to 0.7768. Also note that the suffix and Brown cluster features increase the performance from 0.7768 to 0.8314. As re-ported, PO-CRF mitigates the adverse effects of the unseen word issues and almost meets the per-formance in the previous experiment (0.8375) of T  X  ackstr  X  om et al. (2013) by using these features.
In fourth and final experiment, we used the same features for PO-CRF, with Brown clusters induced on a realistically obtainable sized (3k) corpus for a low resource language. We compare directly to our CCA+SVM model (which does not use Brown clustering features at all). We achieved 0.7983 on PO-CRF with all features and our cor-responding model on CCA achieved about 0.8474, shown in Table 5. As reported, our model outper-forms the PO-CRF with the realistic settings for resource poor languages. We addressed the challenge of POS tagging low-resource languages. Our key idea is to use a mas-sively multilingual corpus. Instead of relying on a single resource-rich language, we leverage the full Table 5: Performances on our test data, CoNLL document. array of currently available POS taggers. This re-moves alignment-mismatch noise and identifies a subset of words with highly confident tags. We then use a CCA procedure to induce latent fea-ture representations across domains, incorporating word contexts as well as projected tags. We then train an SVM to predict tags.

Experimentally, we show that this procedure yields accuracy of about 85% for languages with nearly no resources available, beating a state-of-the-art partially observed CRF formulation. In the near future, this technique will enable us to re-lease a suite of POS taggers for hundreds of low-resource languages.
