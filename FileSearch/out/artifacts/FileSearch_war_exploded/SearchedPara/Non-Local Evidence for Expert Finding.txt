 The task addressed in this paper, finding experts in an enterprise setting, has gained in importance and inter-est over the past few years. Commonly, this task is ap-proached as an association finding exercise between people and topics. Existing techniques use either documents (as a whole) or proximity-based techniques to represent candi-date experts. Proximity-based techniques have shown clear precision-enhancing benefits. We complement both docu-ment and proximity-based approaches to expert finding by importing global evidence of expertise, i.e., evidence ob-tained using information that is not available in the imme-diate proximity of a candidate expert X  X  name occurrence or even on the same page on which the name occurs. Exam-ples include candidate priors, query models, as well as other documents a candidate expert is associated with.

Using the CERC data set created for the TREC 2007 En-terprise track we identify examples of non-local evidence of expertise. We then propose modified expert retrieval mod-els that are capable of incorporating both local (either doc-ument or snippet-based) evidence and non-local evidence of expertise. Results show that our refined models significantly outperform existing state-of-the-art approaches.
 H.3 [ Information Storage and Retrieval ]: H.3.1 Con-tent Analysis and Indexing; H.3 .3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [ Information Systems Applications ]: H.4.2 Types of Systems; H.4.m Miscellaneous Algorithms, Measurement, Performance, Experimentation Expert finding, enterprise search, query models, language models, priors
Expert finding addresses the task of finding the right per-son with the appropriate skills and knowledge [5]. Expert finding systems rank candidate experts with respect to a given topic. A key ingredient of such systems is to compute associations between candidates and topics that capture how strong the two are related. Usually, such associations are de-termined by considering the documents in which candidates and topics co-occur and more recently such associations have been computed not at the document-level but more locally, using text windows or snippets around occurrences of names of candidate experts.

On the whole, it has been found that the use of lo-cal, proximity-based evidence for computing associations be-tween candidate experts and topics improves precision on the overall expert finding task. This is not a surprise. How-ever, there are additional types of evidence of a candidate X  X  expertise in a given topical area that are distinctively non-local in character. By non-local evidence we mean evidence of expertise that is not available from an individual text snippet or even from an individual page. To make mat-ters concrete we provide a number of examples. We take these examples from the experimental setting provided by the TREC 2007 Enterprise track [2] and its scenario of sci-ence communicators in a knowledge intensive organization (CSIRO, [1]) that have to recommend experts in response to outside requests for experts; despite this specific choice, we believe that the phenomenon of non-local indicators of expertise is completely general and generic.

One type of non-local evidence relates to clickstream data; if we have seen the topic for which expertise is being sought before, say in a document retrieval setting, and we have ex-amples of key documents that are often clicked on, how can we use this information about the topic to improve the dis-covery of associations between candidate experts and topics?
Another type of non-local evidence concerns the (relative) importance of a candidate for a given document (( p ( ca | d )). A candidate expert that is related to many documents may not have been particularly important for the creation of a given document d ; thus, in turn, d probably should not con-tribute a lot as evidence in support of associations between the candidate ca and topics discussed in d . And similarly, if the documents associated with a candidate are not se-mantically related to a given document d , then, again, this particular document probably should not count heavily as evidence in support of associations between the candidate ca and topics discussed in d .

A final example suggests that we should consider global properties of candidate experts when computing expert-topic associations: the mere co-occurrence of a person with a topic need not be an indication of expertise of that person on the topic. A case in point is provided by the science com-municators in the CSIRO enterprise search test set: they are mentioned as a contact person on many pages and, hence, frequently co-occur with many topics.

Our aim in this paper is to identify, model, and esti-mate non-local sources of evidence for expert finding and to integrate such evidence into existing language modeling-based approaches to expert finding. Concretely, we aim to find out to which extent rich query modeling with non-local evidence improves the effectiveness of expert finding. Sec-ond, we seek to determine how different ways of computing document-expert associations (with different types of global statistics) impact expert finding. And, we explore to which extent priors on candidate experts (based on their global co-occurrence behavior) impact expert finding effectiveness.
Our main contributions are a comparison of existing ex-pert search approaches on the TREC 2007 enterprise plat-form (CERC collection), the identification of a number of non-local sources of expert finding as well as ways of esti-mating and modeling these in an effective way, based on ex-isting document and proximity-based approaches to expert finding.

The remainder of the paper is organized as follows. We discuss related work in Section 2. We detail our models and ways of estimating both local and non-local evidence for expertise in Section 3. Our experimental setup is detailed in Section 4 and we report on our experiments in Section 5. Section 6 contains an analysis of our experimental results and we conclude in Section 7.
To reflect the growing interest in entity ranking in general and expert finding in particular, TREC introduced an expert finding task at its Enterprise track in 2005 [11]. At this track it emerged that there are two principal approaches to expert finding X  X r rather, to capturing the association between a candidate expert and an area of expertise [11, 26, 2]. The two models have been first formalized and extensively com-pared by Balog et al. [5], and are called candidate and docu-ment models, or Model 1 and Model 2 , respectively. Model 1 X  X  candidate-based approach is also referred to as profile-based method in [12] or query-independent approach in [19]. These approaches build a textual (usually term-based) rep-resentation of candidate experts, and rank them based on query/topic, using traditional ad-hoc retrieval models. Con-ceptually, these approaches are similar to the P@noptic sys-tem [10]. The other type of approach, document models, are also referred to as query-dependent approaches in [19]. Here, the idea is to first find documents which are relevant to the topic, and then locate the associated experts. Thus, Model 2 attempts to mimic the process one might undertake to find experts using a document retrieval system. Nearly all sys-tems that took part in the 2005 X 2007 editions of the Expert Finding task at TREC implemented (variations on) one of these two approaches. In this paper we focus on (variations on) Model 1.

Building on either candidate or document models, further refinements to estimating the association of a candidate with the topic of expertise have been explored. For example, in-stead of capturing the associations at the document level, they may be estimated at the paragraph or snippet level [3]. The generative probabilistic framework naturally lends itself to such extensions, and to the inclusion of other forms of evi-dence, such as document and candidate evidence through the use of priors [12], document structure [28], and of hierarchi-cal, organizational and topical context and structure [19, 6]. For example, Petkova and Croft [19] propose another ex-tension to the framework, where they explicitly model the topic, in a manner similar to relevance models for document retrieval [13]. The topic model is created using pseudo-relevance feedback, and is matched against document and candidate models. Serdyukov and Hiemstra [23] propose a person-centric method that combines the features of both document-and profile-centric expert finding approaches.
Fang and Zhai [12] demonstrate how query/topic expan-sion techniques can be used within the framework; the au-thors also show how the two families of models (i.e., Model 1 and 2) can be derived from a more general probabilistic framework. Petkova and Croft [20] introduce effective formal methods for explicitly modeling the dependency between the named entities and terms which appear in the document. They propose candidate-centered document representations using positional information, and estimate p ( t | ca,d ) using proximity kernels. Their approach is similar to the window-based models that we use below, in particular, their step function kernel corresponds to our estimate of p ( t | ca,d ) in Eq. 10 below. Balog and de Rijke [4] introduce and com-pare a number of methods for building document-candidate associations. Empirically, the results produced by such mod-els have been shown to deliver state of the art performance (see [5, 19, 20, 12, 6]).

Finally, we highlight two alternative approaches that do not fall into the categories above (i.e., candidate or doc-ument models). Macdonald and Ounis [15] propose rank-ing experts with respect to a topic based on data fusion techniques, without using collection-specific heuristics; they find that applying field-based weighting models improves the ranking of candidates. Macdonald et al. [17] enhance their voting approach by considering proximity, moreover, experiment with integrating additional evidence by identify-ing home pages of candidate experts and clustering relevant documents. The authors report experimental results on the TREC 2007 platform (CERC) in [14, 17]. Rode et al. [22] represent documents, candidates, and associations between them as an entity containment graph, and propose relevance propagation models on this graph for ranking experts.
Independent of the basic model adopted, various teams have worked on improved query modeling in the setting of expert finding and, more generally, enterprise search. E.g., Macdonald and Ounis [16] studied better query modeling with query expansion for expert finding and Balog et al. [7] explored query expansion in the setting of enterprise search using so-called example documents (sample key pages are provided with the topic description; see the description of  X  X eedback runs X  below). In some of the manual runs pro-duced at TREC 2007 improved query modeling was obtained by manually tuning queries derived from the narrative field of the topic statements [29].

In Table 1 we list the highest scoring results achieved us-ing the TREC 2007 test set (CERC, [1]) that we have been able to find in the literature. We distinguish between three types of runs: automatic , feedback , and manual . Manual runs involve humans in the loop at any stage, for example Method/model MAP P5 P10 MRR TREC 2007 best .4632 .2280 TREC 2007 best feedback .3660 .2040 TREC 2007 best manual .4787 .2720 Voting model [14] .3406 .1224 Voting model [17] .3519 .4730 Voting model+proximity [17] .4319 .5742 Relevance prop. [24] .4528 .5840 Model 1 [3] .3801 .2000 .1340 .5571 Model 2 [3] .4142 .2400 .1620 .5671 Model 1B [3] .4633 .2600 .1620 .6236 Model 2B [3] .4323 .2560 .1600 .5790 Table 1: Numbers reported so far in the literature on the TREC 2007 Enterprise platform. composing queries from the topics, manual term expansion, relevance feedback, or manual combination of results. Feed-back runs can be thought of as simulating one type of click-based system. They involve the use of the title and page fields of the topics in the TREC 2007 topic set; the page field contains examples of key reference URLs (on average 4 per topic) X  X hese simulate the situation where we have seen the query before (in a document retrieval setting) and a few URLs were often clicked: the URLs in the pages field.
The first group of results in Table 1 are the highest scoring runs at TREC 2007 [2]. The second group is produced using Macdonald and Ounis X  X  fusion techniques. The third group represents the best scores obtained using the graph-based approach of [24]. The fourth and fifth group represent the original candidate and document models and their window-based refinements, respectively.

It was found, both at TREC 2007 and afterwards, that performance depends on two critical factors: the ability to accurately recognize name occurrences in document 1 and the choice of parameters: wherever possible, we use the best or optimal parameter settings as reported in the literature.
Within an organization, there may be many possible can-didates who could be experts on a given topic. For a given query, the problem is to identify which of these candidates are likely to be an expert. Following [5] we can state this problem as follows: That is, we wish to determine p ( ca | q ), and rank candidates ca according to this probability. The candidates with the highest probability given the query are deemed to be the most likely experts for that topic. The challenge, of course, is how to accurately estimate this probability. Instead of calculating this probability directly we apply Bayes X  rule and rewrite it to where p ( ca ) is the probability of a candidate and p ( q ) is the probability of a query. Since p ( q ) is a constant (for a given
To facilitate comparison we release a list of 3,490 names along with the documents associated with them at http: //es.csiro.au/cerc/data/balog ; the list is available for registered licensees of the CERC collection. query), it can be ignored for the purpose of ranking. Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p ( q | ca ), weighted by the a priori belief that candidate ca is an expert ( p ( ca )): In most existing work [5, 9] p ( ca ) is assumed to be uni-form. However, as was shown in [12], a reasonable prior can improve retrieval accuracy. In this paper we will use can-didate priors to distinguish between science communicators and proper experts; the estimation of this prior is detailed in Section 3.4.

According to Model 1 of Balog et al. [5], the candidate is represented by a multinomial probability distribution over the vocabulary of terms. Therefore, a candidate model  X  ca is inferred for each candidate ca , such that the probability of a term given the candidate model is p ( t |  X  ca ). The model is then used to predict how likely a candidate would produce a query q .

Assuming that each query term is sampled identically and independently, the query likelihood is obtained by taking the product across all the terms in the query, such that: where n ( t,q ) denotes the number of times term t is present in query q .

Instead of calculating this probability directly, we move to the log domain to prevent numerical underflows, as proposed in [3]. We rewrite Eq. 3 as follows: In this alternative formulation we also replaced n ( t,q ) with p ( t |  X  q ), which can be interpreted as the weight of term t in query q . We will refer to  X  q as the query model . Note that maximizing the query-likelihood in Eq. 4 provides the same ranking as minimizing the KL-divergence between the query and candidate models (that is, ranking by  X  KL(  X  q ||  X  ca is pointed out in [7]).

Next, we discuss the estimation of the three components of our modeling: (i) the candidate model ( p ( t |  X  ca )) in Sec-tion 3.1, (ii) the query model ( p ( t |  X  q )) in Section 3.3, and (iii) candidate priors ( p ( ca )) in Section 3.4. Along the way, in Section 3.2, we discuss a key ingredient of our candidate models, viz. document-candidate associations ( p ( ca | d )).
To obtain an estimate of p ( t |  X  ca ), we must ensure that there are no zero probabilities due to data sparsity. In doc-ument language modeling, it is standard to employ smooth-ing: where p ( t | ca ) is the probability of a term given a candidate, and p ( t ) is the probability of a term in the document repos-itory.

To approximate p ( t | ca ), we use the documents as a bridge to connect the term t and candidate ca in the following way: That is, the probability of selecting a term given a candi-date is based on the strength of the co-occurrence between a term and a candidate in a particular document ( p ( t,ca | d )). Below, we first discuss two ways of building candidate mod-els: based on documents associated with them and based on terms in proximity to candidate name mentions.
Our first approach to estimating candidate models as-sumes that the document and the candidate are condition-ally independent. That is where p ( t | d ) is the probability of the term t in document d . We approximate it with the standard maximum-likelihood estimate of the term, i.e., the relative frequency of the term in the document [5].
Model 1 assumes conditional independence between the document and the candidate. However, this assumption is quite strong as it suggests that all the evidence within the document is descriptive of the candidate X  X  expertise. This may be the case if the candidate is the author of the doc-ument, but here we consider an alternative. We can factor the conditional probability p ( t,ca | d ) as follows: That is, we base p ( t,ca | d ) on the strength of the co-occurrence between a term and a candidate in a particular document; both the document and the candidate determine the probability of the term.

One natural way in which to estimate the probability of co-occurrence between a term and a candidate is by consid-ering the proximity of the term given the candidate in the document, the idea being that the closer a candidate is to a term the more likely that term is associated with their expertise.

Here, we assume that the candidate X  X  name, email, etc. have been replaced within the document representation with a unique candidate identifier, which can be treated much like a term, referred to as ca . The terms surrounding either side of ca form the context of the candidate X  X  expertise and can be defined by a window of size w within the document. For any particular distance (window size) w between a term t and candidate ca , we can define the probability of a term given the candidate, distance and document: where n ( t,ca,w,d ) is the number of times the term t co-occurs with ca at a distance of at most w in document d . Now, the probability of a term given the candidate and docu-ment is estimated by taking the sum over all possible window sizes W : where p ( w ) is the prior probability that defines the strength of association between the term and the candidate at dis-tance w , such that P w  X  W p ( w ) = 1. Estimating Model 1B this way essentially corresponds to the step function kernel in [20].

When we put together our choices so far, the formula we use for ranking candidates is the one shown in Eq. 11. So far we have discussed the estimation of p ( t,ca | d ) and p ( t ). Next, we discuss three additional components of the model: (i) document-candidate associations ( p ( d | ca )) in Section 3.2, (ii) the query model ( p ( t |  X  q )) in Section 3.3, and finally, (iii) candidate priors ( p ( ca )) in Section 3.4.
A feature common to both models introduced above, and shared by many of the models mentioned in Section 2, is their reliance on associations between people and docu-ments. E.g., if someone is strongly associated with an impor-tant document on a given topic, this person is more likely to be an expert on the topic than someone who is not associated with any documents on the topic or only with marginally relevant documents. In our framework this component is re-ferred to as document-candidate associations , and the like-lihood of candidate ca being associated with document d is expressed as a probability ( p ( ca | d )) in Eq. 7 for Model 1 and in Eq. 8 for Model 1B.

The probability p ( ca | d ) can be estimated at the level of the document d itself, or at the sub-document level, where associations link people to specific text segments. To remain focused, we build associations on the document level only in this section: to date, many open issues remain even at the document level. We leave a systematic exploration of candidate- X  X ext snippet X  associations for later research.
We assume that the recognition of candidate occurrences is taken care of by an external extraction component. We briefly discuss this process in technical terms in Section 4. The output of this extraction procedure is a preprocessed document format where candidate occurrences are treated as terms. The number of times the candidate ca is recognized in the document d is denoted by n ( ca,d ).

We take a baseline approach to computing p ( ca | d ) to-gether the two best performing approaches as suggested by Balog and de Rijke [4]. The simplest possible way of set-ting p ( ca | d ) is referred as the boolean model (BL). Under this boolean model, associations are binary decisions; they exist if the candidate occurs in the document, irrespective of the number of times the person or other candidates are mentioned in that document. Formally, it is expressed as: For a better estimate, a lean document representation is used which consists of only candidate mentions. First the candi-date X  X  (local) frequency in the document (TF) and (global) frequency (IDF) is combined (and referred as TFIDF) (note that it is computed only for candidates that occur in the document ( n ( ca,d ) &gt; 0): p ( ca | d )  X  n ( ca,d ) P Note that this is a clear example of the use of non-local information (as we need global statistics to determine IDF).
Finally, we use an alternative way of measuring a candi-date X  X  importance given a document. A candidate is repre-sented by its semantic relatedness to the given document, instead of its actual frequency. This method will be referred to as SEM. We use n 0 ( ca,d ) instead of n ( ca,d ) in Eq. 13, where Again, we need global statistics to compute this way of de-termine a candidates importance, as evidenced by Eqs. 5 and 6, where the candidate model  X  ca is being defined.
We will use these three methods (BL, TFIDF, SEM, in that order) in combination with both Models 1 and 1B in our experimental evaluation (reported in Section 5).
As to the query model, we consider two flavors. Our base-line query model (BL) consists of terms from the topic title only, and assigns the probability mass uniformly across these terms: As before, n ( t,q ) is the frequency of term t in q .
The baseline query model has two potential issues. Not all query terms are equally important, hence, we may want to reweigh some of the original query terms. Also, p ( t | q ) is extremely sparse, and, hence, we may want to add new terms (so that p ( t |  X  q ) amounts to query expansion). At this point, we consider yet another form of non-local evidence.
The TREC 2007 Enterprise track simulates a type of click-based system, where we have observed a given topic mul-tiple times and where a small number of documents were often clicked. We refer to those documents as example doc-uments . Balog et al. [7] propose an effective method of ex-ploiting these example documents. Unlike previous work on relevance modeling [13] and blind relevance feedback mech-anisms [21], here it is assumed that these expansion terms are sampled independently from the original query terms. That is, we use a non-local approach to query expansion. The original (baseline) query model ( p ( t | q )) is combined with the expanded query model ( p ( t |  X  q )) (EX) as follows. The expanded query is sampled from a set of example docu-ments S . First, we estimate a  X  X ampling distribution X  p ( t | S ) using example documents d  X  S . Next, the top k terms with highest probability p ( t | S ) are taken and used to formulate the expanded query  X  q : By summing over all example documents, we obtain P ( t | S ). Formally, this can be expressed as This resembles the way the candidate model is constructed in Eq. 3. We approximate p ( t | d ) with the maximum-likeli-hood estimate and set p ( d | s ) to be uniform (i.e., all example documents are equally important).

An example TREC topic, with the corresponding query models obtained using BL (Eq. 15) and EX (Eq. 16), is shown in Figure 1. We clearly see the reweighing and ex-pansion effect of our new query model. Figure 1: Query models generated for topic CE-039: cane toads
Our goal with introducing candidate priors is to demon-strate another form of incorporating non-local evidence into our modeling. Estimating this prior without training or manually encoding organizational knowledge is difficult (as was also found in [20]). We explored several approaches, in-cluding binning people by their document frequency or by the coherence of the set of documents in which they occur. While reasonably effective in distinguishing science commu-nicators (and web masters and others whose names occur in many documents) from  X  X roper X  experts, we decided to use a simple pattern-based approach. We extracted a list names and positions within an organization from the contact blocks of documents (where this block existed). A large portion of these people are science communicators (SC) (often called communication officer/manager/advisor or manager public affairs communication ).

We then set the candidate prior as follows: This simply means that we identified science communicators and filtered them out from the list of names returned.
To address our research questions (repeated below) we ran experiments using the CSIRO Enterprise Research Col-lection (CERC), a crawl of *.csiro.au (public) websites con-ducted in March 2007. The crawl contains 370,715 docu-ments, with a total size 4.2 gigabytes [1].

In the 2007 edition of the TREC Enterprise track, CERC was used as the document collection [1]. CSIRO X  X  science communicators played an important role in topic creation. They, the envisaged end-users of systems taking part in the TREC Enterprise track, read and create outward-facing web pages of CSIRO to enhance the organization X  X  public image and promote its expertise. A total of 50 topics were created by the science communicators; systems had to return  X  X ey contacts X  for these topics, i.e., names that could be listed on the topic X  X  overview page. These key contacts are con-sidered as relevant experts, thus, used as the ground truth. It was not assessed whether there is evidence present in the collection to support the person X  X  expertise.
The measures we will use are (Mean) Average Precision (MAP), P5, P10 (precision at rank 5 and 10, respectively), and (Mean) Reciprocal Rank (MRR). MAP is appropriate since it provides a single measure of quality across recall levels. MAP is the main measure used for the expert finding task at the TREC Enterprise Track [11, 26, 2].

As to P5, P10, and MRR, we argue that recall (i.e., find-ing all experts given a topic or listing all expertise areas of a given person) may not always be of primary importance to our target users. Expertise retrieval can be seen as an ap-plication where achieving high accuracy, i.e., high precision in the top ranks is paramount. For this purpose P5, P10, MRR are appropriate measures [25].
In the 2007 edition of the Expert Search task at TREC, candidates are identified by their primary e-mail addresses, which follow the Firstname.Lastname@csiro.au format. No canonical list of experts has been made available, there-fore, e-mail addresses have to be extracted from the docu-ment collection, and then normalized to the primary format. This presents a number of challenges, including overcoming various spam protection measures, the use of alternative e-mail addresses, and of different abbreviations of names.
The list of candidates we use is taken from [3] and com-prises 3,490 unique names in total. References of these peo-ple in documents were replaced by a unique identifier. See [3] for the description of the candidate extraction procedure. We use the best performing query model from [7], EX-QM-EM, with k = 30 feedback terms. The original and the expanded query models are combined with equal weights:  X  = 0 . 5. All example documents were considered equally important ( p ( d | S ) is uniform).
Note that our method for estimating the proximity-based model (Model 1B) allows for a weighted combination of var-ious windows sizes (see Eq. 10). To remain focused, here we restricted ourselves to a single fixed window ( W = { w } ) and the size of this window was set to 125 (based best empirical results after performing a sweep on a set of possible window sizes from 20 ... 250); see [3] for details.
It is well-known that smoothing can have a significant impact on the overall performance of language modeling-based retrieval methods [27]. Our candidate models employ Bayes smoothing with a Dirichlet prior [18] to improve the estimated language models. Specifically, we set  X  =  X   X  + | ca | where | ca | is the sum of the number of terms associated with a given candidate.

Based on an empirical investigation of smoothing values reported in [3] we set  X  = 90 , 000 for Model 1 and  X  = 100 for Model 1B.
We repeat our research questions from the introduction and then present the results of the experiments performed to answer our questions.
We aim to find out to which extent rich query modeling with non-local evidence improves the effectiveness of expert finding: how do BL and EX compare across multiple experi-mental conditions. Second, we seek to determine how differ-ent ways of computing document-expert associations (with different types of global statistics) impacts expert finding: how do BOOL, TFIDF and SEM compare across multiple experimental conditions. And we determine to which ex-tent priors on candidate experts (based on their global co-occurrence behavior) impact expert finding effectiveness.
Table 2 lists the retrieval scores obtained for the vari-ous experimental conditions: the top half lists the scores based on Model 1, the bottom half lists the scores for Model 1B. Superscripts report on the outcome of significance tests (paired t-test, rows 2 X 6 vs. row 1, rows 8 X 12 vs. row 7, row 7 vs. row 1; (1) = . 05, (2) = . 01, (3) = . 001).

Model p ( ca | d )  X  q MAP P5 P10 MRR 1B BOOL BL .4633 (2) .2600 (3) .1620 (2) .6236 Table 2: Results overview. Document-candidate as-sociations: (BOOL) Boolean, (TFIDF) Frequency-based using TF.IDF weighting, (SEM) Semantic re-latedness; Query model: (BL) Baseline, (EX) Ex-panded (using example documents provided with the topic statement).
How does rich (non-local) query modeling help expert finding? Moving from the baseline (BL) to more refined query formulations (EX) always improves and the improve-ment can be up to 19% in MAP, 18% in P5, 7% in P10, and 16% in MRR (even vs. odd rows of Table 2).
How do (non-local) document-candidate associations help? Moving from local (BOOL) to more and more non-local approaches (TFIDF and SEM) improves across the board and significantly, irrespective of the candidate and query models. On the other hand, the improvement gained by moving to non-local approaches is more substantial for Model 1 than for Model 1B.
Finally, we implement our candidate priors on top of the best performing configurations of Model 1 and Model 1B; see Table 3. Using priors result in significant improvements over these best performing configurations (MAP and MRR). Our scores reported in Table 3 outperform any previously published results that we are aware of.
 Table 3: Results of adding candidate priors on top of the best performing configurations of Model 1 and Model 1B. Significance testing is done against these best forming configurations.
We start our analysis by contrasting the two extreme ends of the spectrum described in Table 2: a local approach M1-BOOL-BL (row 1 in Table 2) and a local approach mixed with non-local features, M1B-SEM-EX (row 12 in Table 2). Figure 2 shows a topic-level comparison. We find that in the majority of topics non-local features improve, and the improvement can be up to + . 9655 Average Precision (AP) (topic CE-015: life cycle assessment ). On the other hand, in a small number of cases it hurts performance X  X he rightmost bar corresponds to topic CE-024: Double Helix Science Club where AP drops by . 4167. See Figure 2.
AP difference Figure 2: M1-BOOL-BL (baseline) vs. M1B-SEM-EX; row 1 vs row 12 of Table 2.
 When we consider the move from Boolean to TFIDF-based document-candidate associations, we see that some topics are hurt, but on the whole more are helped by the move to TFIDF-based associations, independent of the query model being used (BL or EX); see Figure 3. The gains/losses (in numbers of topics) for the four pairwise comparisons shown in Figure 3 are: 30/10, 19/17, 31/8, 21/13, respectively.
Going from TFIDF to SEM we see that some topics are hurt, but more are helped, and by a bigger margin (Fig-ure 4). The gains are more modest X  X oth per topic and averaged X  X han the gains obtained by moving from BOOL to TFIDF (Figure 3). This is reflected in the gain/loss num-bers: 17/15, 22/7, 12/16 (!), 15/9, respectively.

Next, we contrast runs with and without the expanded query model. Figure 5 shows the contrastive plots. On the whole, moving to richer query models has a positive effect, although some topics are hurt. Interestingly, we ob-serve almost identical gain/loss patterns across Model 1 and Model 1B (top row vs. bottom row) and independent of the underlying association. The gain/loss numbers are (for plots (a) X (f)): 25/8, 24/12, 22/11, 20/12, 24/12, and 23/12, re-spectively.

Let us zoom in on the candidate models estimated us-ing our document-and proximity-based models (Model 1 and 1B, respectively); Table 4 displays the terms associated with candidate Manny Noakes with the highest probability. Manny Noakes is leader of the research team that developed the Total Wellbeing Diet , published as a book (together with Dr Peter Clifton). 2 As we move from M1-BOOL to M1-SEM we can observe new terms emerging, such as weight and nu-trition . Also, we can observe that several other associated terms move up in the ranking, e.g., diet and health . Switch-ing from document-based to proximity-based models (i.e., from M1 to M1B) continues the progress in the direction of nutrition science, by adding terms like protein and exercise , while general terms, such as industry and technology have dropped out of the top 20. Finally, as we contrast M1B-BOOL and M1B-SEM, we observe slight refinements in the allocation of the probability mass; contrast, for example the probability of nutrition and australia .

Manny Noakes is an expert on topic CE-013: human clinical trials (according to the ground truth provided by CSIRO X  X  science communicators). The two query models (BL, EX) for this topic are listed in Table 5. The ranking of Manny Noakes for this topic (using the different combi-nations of query model and candidate profile) is as follows: Query mod. M1-BOOL M1-SEM M1B-BOOL M1B-SEM As the query model gets richer, Manny Noakes X  ranking im-proves, and, similarly, as the degree of non-locality improves. Given the query models and candidate profiles listed in Ta-bles 5 and 4, we see why: the best performing models and profile are simply very similar.

Finally, when investigating the effect of candidate priors we find that these affect only a handful of topics, but the effect is always positive; see Figure 6.
 According the literature (and our own previous publica-tions), the document-based approach ( X  X odel 2 X ) was iden-tified as a clearly preferred model as it is robust, is only slightly affected by smoothing and can be implemented effi-ciently on top of an existing document search engine [5, 8, 3]. However, when we contrast the numbers in Table 1 with the http://www.csiro.au/people/Manny.Noakes.html ordered by difference in AP. 6, and 10 vs. 12, respectively. Topics ordered by difference in AP. by difference in AP. Table 5: Query models generated for topic CE-013: human clinical trials (Left) BL, (Right) EX. Figure 6: Topic level comparison when using the SC prior. (Left) M1-SEM-EX. (Right) M1B-SEM-EX.
 In terms of rows in Table 3: 1 vs. 2 and 3 vs. 4, respectively. Topics ordered by difference in AP. best performing configurations we obtained in this section, we find that while Model 1 starts from a lower baseline, as additional non-local features are combined, it outperforms Model 2 and delivers state-of-the-art performance. We also added the non-local features discussed in this paper on top of Model 2, but this had only marginal effects [3].
We briefly summarize the pro and cons of Model 1. The cons include the need for maintenance of candidate models (as these have to be calculated offline, to be able to operate the retrieval system with an acceptable response time), and finding the optimal smoothing setting needs training mate-rial. Further, concerning Model 1B, calculating proximity could be done in more advanced ways (e.g., using proximity kernels as proposed in [20]). The pros include performance, and the fact that these models are  X  X eadable X  for the user and can even be visualized as simply as tag-clouds.
We explored the use of non-local evidence for the task of expert finding. On top of existing document and proximity-based language modeling approaches to the task, we consid-ered three types of non-local evidence: obtained from query models, obtained from people-document associations, and as candidate priors. Starting from very competitive baselines we found that non-local evidence from query models helps improve expert finding effectiveness in all experimental con-ditions that we considered. Non-local aspects of document-candidate associations as modeled by the TFIDF approach improved over a Boolean baseline, while a semantics-based approach improved even more. On top of the best per-forming combinations of (non-local) query modeling and document-candidate associations, a final type of non-local evidence (candidate priors) leads to further improvements. Overall, our refined models outperform existing state-of-the-art approaches to expert finding.

Future work will concern ways of estimating within-document non-local evidence of expertise; many documents in the CSIRO test collection have additional (internal) struc-ture, evidenced (among other things) by the presence of multiple text blocks X  X uch blocks may be used to improve precision (just like proximity-based approaches), but at the same time evidence of associations between a candidate and a given topic may be scattered across multiple blocks: how can we identify text blocks that matter for candidate-topic associations?
We thank Wouter Weerkamp for helping us prepare this paper and our anonymous reviewers for their valuable feed-back. This research was supported by the E.U. IST pro-gramme of the 6th FP for RTD under project MultiMATCH contract IST-033104, and by the Netherlands Organisation for Scientific Research (NWO) under project numbers 220-80-001, 017.001.190, 640.001.501, 640.002.501, 612.066.512, STE-07-012, 612.061.814, 612.061.815.
