 A crucial operation in memory-based collaborative filtering (CF) is determining nearest neighbors (NNs) of users/items. This paper addresses two phenomena that emerge when CF algorithms per-form NN search in high-dimensional spaces that are typical in CF applications. The first is similarity concentration and the second is the appearance of hubs (i.e. points which appear in k -NN lists of many other points). Through theoretical analysis and experimental evaluation we show that these phenomena are inherent properties of high-dimensional space, unrelat ed to other data properties like sparsity, and that they can impact CF algorithms by questioning the meaning and representativeness of discovered NNs. Moreover, we show that it is not easy to mitigate the phenomena using dimension-ality reduction. Studying these phenomena aims to provide a better understanding of the limitations of memory-based CF and motivate the development of new algorithms that would overcome them. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.4.m [ Information Systems Applications ]: Mis-cellaneous Theory, Experimentation Collaborative filtering, nearest n eighbors, curse of dimensionality, similarity concentration, cosine similarity, hubs
The authors would like to thank Zagorka Lozanov-Crvenkovi  X  c for valuable comments and suggestions. Alexandros Nanopou-los gratefully acknowledges the partial co-funding of his work through the European Commission FP7 project MyMedia (www.mymediaproject.org) under the grant agreement no. 215006. Milo X  Radovanovi  X  c and Mirjana Ivanovi  X  c thank the Serbian Min-istry of Science for support through project Abstract Methods and Applications in Computer Science , no. 144017A.

Memory-based collaborative filtering (CF) is a successful rec-ommender system technology that produces recommendations by matching user preferences to thos e of other users. Preferences are usually represented by a user-item matrix M , whose element M is the rating user u assigned to item i . In user-based (UB) collabo-rative filtering, similarity is defined between rows of M , whereas in item-based (IB) CF [8] similarities are defined between its columns. Fusion schemes for UB and IB collaborative filtering have also been proposed [9]. A fundamental operation in memory-based CF is the search for nearest neighbor (NN) users (UB CF) or items (IB CF), whose ratings are then combined to generate predictions.
Data sparsity is regarded as a major limiting factor for CF, be-cause it causes the reduced coverage problem  X  due to the lack of common ratings similarity may be undefined for many pairs of users or items. Therefore, data sparsity has attracted significant at-tention (see [4] for a recent survey). Besides sparsity, high dimen-sionality can limit memory-based CF as well. Since the user-item matrix has a large number of rows and columns, NN search, both in UB and IB cases, is performed in a high-dimensional space. In this article we focus on the impact of high dimensionality on NN search used by memory-based CF.
One counter-intuitive property of high-dimensional spaces is dis-tance concentration [2]. It refers to the tendency of distances be-tween all pairs of points in a high-dimensional data set to become almost equal. Concentration questions the meaningfulness of NN search in high-dimensional spaces, as it is hard to distinguish the nearest from the farthest neighbor [2, 5]. Recently, a new aspect of distance concentration has been described by the property of hub-ness [6], which refers to the fact that the distribution of the number of times each data point occurs among the NNs of all other points in a high-dimensional data set is considerably skewed. Hubness renders several points, called hubs, more influential NNs, because they appear to be NNs of many other points.

Despite the impact of distance concentration and hubness, up to our knowledge they have not been examined thoroughly in the con-text of memory-based CF, but only for general data mining tasks and mostly for l p distances. 1 However, memory-based CF algo-rithms use similarity measures, e.g., Pearson correlation or adjusted cosine [8], which will be shown to present characteristics concern-ing the property of concentration different to those of l The effect of the size of the user-item matrix in CF has been mainly related to scalability and efficiency issues [4, 7]. Few studies indi-rectly refer to distance concentration, without examining its causes and consequences [10]. Dimensionality reduction, based on fac-torizing the user-item matrix w ith singular value decomposition (SVD) [7], is a popular approach against the problems of sparsity and polysemy. However, it has not been recognized if dimension-ality reduction can address distance concentration and hubness.
In this article, we study the causes and effects of concentration and hubness on the fundamental operation of NN search that is used by memory-based CF. We provide analytical results for concen-tration (Section 2) and discuss its relationship with hubness (Sec-tion 3). We also provide experimental evidence for these two prop-erties (Section 4).

Our main findings are the following. (i) Similarity measures that are commonly used in CF concentrate as the size of the user-item matrix increases. Therefore, it becomes difficult to identify effec-tive neighborhoods of similar users or items in order to generate predictions. (ii) Due to high dimensionality, several users/items be-come hubs by being NNs of unexpect edly many other users/items, and are thus less capable of providing distinctive information about their respective n eighborhoods. (iii) Both concentration and hub-ness are inherent properties of high-dimensional space  X  they are not related to sparsity or the skewness of the distribution of rat-ings. The two phenomena may present significant limitations for memory-based CF by questioning the meaningfulness of computed user/item neighborhoods. (iv) Dimensionality reduction does not constitute an easy mitigation f or concentration and hubness.
Our findings provide a basis to better understand the fundamental limitations of memory-based CF and motivate the development of new algorithms that would overcome them.
To ease comprehension, we first examine the concentration of the cosine similarity measure and then extend to cosine-like measures that are commonly used in memory-based CF.
Concentration of cosine similarity is will be considered for two random d -dimensional vectors p and q with iid components. Such vectors can represent rows (in UB case) or columns (in IB case) of the user-item matrix. Our examination treats the two cases equiva-denote the cosine similarity between p and q , which is defined in Equation 1. 2
Note that by making the assumption that the coordinates of p and q equal to 0 indicate the absence of a rating, S cos ( computed over co-rated items in both p and q , which is the com-mon approach for sparse data. Ther efore, our examination treats sparse and dense data in the same way (concentration occurs in both cases).

From the extension of Pythagoras X  theorem we have Equation 2 that relates S cos ( p, q ) with the Euclidean distance between p and q .
Define the following random variables: X = p , Y = q , and Z = p  X  q .Since p and q have iid components, we assume that X and Y are independent of each other, but not of Z .Let C Equation 2, with simple algebraic manipulations and substitution of the norms with the corresponding random variables, we obtain Equation 3.

Let E( C ) and V( C ) denote the expectation and variance of C , respectively. An established way [2] to demonstrate concentra-tion is by examining the asymptotic relation between E(
C ) when dimensionality d tends to infinity. To express this asymptotic relation, we first need to express the asymptotic behav-ior of E( C ) and V( C ) with regards to d . Since, from Equation 3, C is related to functions of X , Y ,and Z , we start by studying the expectations and variances of these random variables.
 The same holds for random variable Y .

P ROOF . Follows directly from Theorem 1 and the fact that, since vectors p and q have iid components, vector p  X  q also has iid com-ponents.
 lim d  X  X  X  (E( X 2 ) /d )= const , and lim d  X  X  X  (V( X 2 ) /d The same holds for random variables Y 2 and Z 2 .
 P ROOF . From Theorem 1 and the equation E( X 2 )=V( X )+ E(
X ) 2 it follows that lim d  X  X  X  (E( X 2 ) /d )= const .Thesame holds for E( Y 2 ) and, taking into account Corollary 1, for By using the delta method to approximate the moments of a func-tion of a random variable with Taylor expansions [1], we have V(
X 2 )  X  (2E( X )) 2 V( X ) . From Theorem 1 it now follows that lim d  X  X  X  (V( X 2 ) /d )= const . Analogous derivations hold for V( Y 2 ) and V( Z 2 ) .
 Based on the above results, the following two theorems show that p
V( C ) reduces asymptotically to 0, while E( C ) asymptotically remains constant (proof sketches are given in the Appendix).
It is worth noting that, as mentioned in Section 1.1, the concen-tration of cosine similarity results from different reasons than the concentration of the l 2 distance. For the latter, its standard devi-ation converges to a constant [2], whereas its expectation asymp-totically increases with d . Nevertheless, in both cases the relative relationship between the standard deviation and the expectation is similar, e.g., their ratio asymptotically goes to 0 (providing that E(
C ) does not equal zero). Intuitively, in both cases concentration has the same effect, as it makes it difficult to distinguish the closest from the farthest nearest neighbors.
A similarity measure that is often used in memory-based CF (mainly in the UB case) is Pearson correlation. For a vector dis-tribution, the basic geometrical relationship between cosine and Pearson correlation is that the latter is equivalent to first center-ing the points by subtracting the mean of the data set from each vector, and then applying cosine similarity. Since we assume vec-tor components are iid, the means of the components are all equal, thus centering produces a vector distribution with iid components. As shown in Section 2.1, cosine similarity in the centered space concentrates, implying that Person correlation in the original space also concentrates.

Another similarity measure used in CF (mainly in the IB case) is adjusted cosine [7], which first subtracts from each component of a vector the mean of all its components, and then computes co-sine similarities. A lthough the subtraction renders vector space components mutually dependent (preventing direct application of results derived in Section 2.1), concentration still appears (see ex-perimental evidence in Section 4) since intrinsic dimensionality is not significantly altered. We refer to [2] for further discussion on component dependence.
For vector p ,let N k ( p ) denote the number of times p occurs among the k NNs of all other vectors in a data set. It was shown that, as dimensionality increases, the distribution of N k considerably skewed to the right, resulting in the emergence of hubs, i.e., vectors which appear in many more k -NN lists than other vectors [6]. The skewness of N k was linked to the phenomenon of concentration and examined mainly for Euclidean distance. For cosine and cosine-like similarity measures, the skewness of N k is also related to concentration. High-dimensional random vec-tors with iid components, due to concentration, tend to have almost equal similarities among them, thus it can be said that they are ly-ing on a hypersphere centered at the data set mean. For a high but finite number of dimensions, the distribution of similarities has a low but non-zero variance (Theorem 2). Hence, the existence of a non-negligible number of vectors closer to the data set mean is expected in high dimensions. By being closer to the mean these vectors are closer to all other vectors. This tendency is amplified by high dimensionality, making vectors closer to the mean have increased inclusion probability into k -NN lists [6]. With real high-dimensional data it was established that hubs tend to appear in the proximity of cluster centers [6], instead of being near one global data set mean. In real CF applications, on the other hand, the distri-bution of ratings in the user-item matrix is also skewed. However, as will be verified in Section 4, this skewness is not related to the skewness of N k .

In memory-based CF we foresee that hubs, by being NNs of many other vectors, can become less representative NNs. That is, hubs can act like noise in k -NN lists, in an analogy to k -NN classification [6]. Since hubness is an inherent property of high di-mensionality, we believe the issue warrants careful investigation, especially considering the fact that dimensionality reduction may not easily eliminate the phenomenon, as will be demonstrated in the next section.
This section provides experimental evidence on concentration and hubness. We start with a simple experiment that demonstrates concentration. We generated 1,000 d -dimensional vectors having zero/one coordinates. The percentage of ones is denoted as spar-sity , whereas ones are uniformly distributed. The average cosine similarity, E( C ) , and its standard deviation, sured. Figures 1(a) and (b) present the results against dimension-ality d for sparsity equal to 10% and 90%, respectively. The fol-lowing measurements are plotted from top to bottom: (i) maximum observed similarity between all pairs of vectors (red dash-dotted line), (ii) E( C )+ solid line), (iv) E( C )  X  imum observed similarity between all pairs of vectors (red dash-dotted line). True to the results from Section 2, as d increases p
V( C ) reduces, whereas E( C ) remains constant. These results also show that concentration of cosine similarity appears regardless of sparsity. We obtained similar results for zipfean distribution of ones (omitted due to space considerations). Therefore, concentra-tion appears both for skewed and uniform distributions of ratings. Figure 1: Demonstration of the concentration of the cosine sim-ilarity measure with synthetic data.

To examine concentration in real data, we used the Movielens 100K data set. 3 We measured adjusted cosine for the IB case and Pearson correlation for the UB case. In both cases we obtained varying dimensionalities by performing dimensionality reduction using SVD (each dimensionality is given as fraction of the origi-nal one). Figure 2(a) presents the relative relationship between the standard deviation ties through their ratio. 4 In accordance with the results in Section 2, the ratio tends to zero as dimensionality increases. Figure 2: Concentration (a) and hubness (b) in real data.
To examine hubness, we measured for Movielens the distribu-tion of N 10 for adjusted cosine (IB) and Pearson correlation (UB). We characterize hubness through skew( N 10 ) , i.e., the standardized third moment of N 10 . Figure 2(b) plots skew( N 10 ) against the fraction of kept dimensions when using SVD dimensionality re-duction. Starting from high dimensionality, skew( N 10 ) remains relatively constant. This means that the distribution of N 10 is stays considerably skewed, because there exist vectors with much higher N after a point at which the intrinsic dimensionality is reached, where further reduction may incur loss of information. Thus, dimension-ality reduction may not effectively address hubness. The aforemen-tioned findings have been verified for various k values.
Finally, we examined our hypothesis that the skewness of N related to the similarity with the mean vector. For both UB and IB we found a significant (Pearson) correlation (0.9 and 0.75, respec-tively) between N k ( k =30 ) and the similarity of vectors with the mean vector (at 0.05 confidence level). Conversely, we could not establish a significant correlation between N k and the number of ratings in the vector (measured by its norm). It is worth mention-ing that we have confirmed the emergence of skewness of N synthetic data (like those in the first measurement) with a uniform distribution of ratings. Thus, hubness is an inherent property of high dimensionality related to concentration, but not to sparsity or skewness of the distribution of ratings.
We examined the consequences of high dimensionality on mem-ory-based CF in terms of concentration of similarity, and hubness. Presented results provide insights into their causes and the limita-tions they can impose on memory-based CF. Our findings indicate that concentration and hubness are inherent properties of high di-mensionality, not of data properties like sparsity or skewness of the distribution of ratings, and that they both impact CF algorithms by questioning the meaning and representativeness of discovered NNs. We also showed that dimensionality reduction may not constitute an easy remedy for these limitations. In future work we plan to extend our work towards developing memory-based CF algorithms that will take into account concentration and hubness and address them to improve prediction quality. [1] G. Casella and R. L. Berger. Statistical Inference, 2nd ed. [2] D. Fran X ois, V. Wertz, and M. Verleysen. The concentration [3] L. A. Goodman. On the exact variance of products. J. Am. [4] M. Gr  X  car, D. Mladeni  X  c, B. Fortuna, and M. Grobelnik. Data [5] A. Hinneburg, C. C. Aggarwal, and D. A. Keim. What is the [6] M. Radovanovi  X  c, A. Nanopoulos, and M. Ivanovi  X  c. Nearest [7] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl. Application [8] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl. Item-based [9] J. Wang, A. P. de Vries, and M. J. T. Reinders. Unifying [10] K. Yu, X. Xu, M. Ester, and H.-P. Kriegel. Feature weighting
