 Many application problems need to simultaneously predict several quantities using a common set of variables, e.g. predicting multi-channel signals within a time frame, predicting concentrations of several chemical constitutes using the mass spectra of a sample, or predicting expression levels of many genes using a common set of phenotype variables. These problems can be naturally formulated in terms of multivariate regression.
 In particular, let we assume X = [0 , 1] d and the true model on y j is : also assume that the noise terms This is a general setting of the nonparametric multivariate regression. From the minimax theory, we a d -dimensional Sobolev ball with order  X  and radius C , the best convergence rate for the minimax risk is p  X  n  X  2 = (2 + d ) . For a fixed  X  , such rate can be very slow when d becomes large. However, in many real world applications, the true regression function f may depend only on a small set of variables. In other words, the problem is jointly sparse : S has been given, the minimax lower bound can be improved to be p  X  n  X  2 = (2 + r ) , which is the which adaptively achieves this faster rate of convergence without knowing S in advance. Previous research on these problems can be roughly divided into three categories: (i) parametric lin-ear models, (ii) nonparametric additive models, and (iii) nonparametric tree models. The methods in the first category assume that the true models are linear and use some block-norm regulariza-tion to induce jointly sparse solutions [16, 11, 13, 5]. If the linear model assumptions are correct, accurate estimates can be obtained. However, given the increasing complexity of modern appli-cations, conclusions inferred under these restrictive linear model assumptions can be misleading. Recently, significant progress has been made on inferring nonparametric additive models with joint f ( x ) = tions might still be too stringent for real world applications.
 A family of more flexible nonparametric methods are based on tree models. One of the most popular onally splitting the axes at locally optimal splitting points, then prunes back the full tree to form a subtree. Theoretically, CART is hard to analyze unless strong assumptions have been enforced [8]. In contrast to CART, dyadic decision trees (DDTs) are restricted to only axis-orthogonal dyadic lems, [15] showed that DDTs using a special penalty can attain nearly optimal rate of convergence in a minimax sense. [1] proposed a dynamic programming algorithm for constructing DDTs when the on all terminal nodes. Though intensively studied for classification problems, the dyadic decision tree idea has not drawn much attention in the regression settings. One of the closest results we are aware of is [4], in which a single response dyadic regression procedure is considered for non-sparse learning problems. Another interesting tree model,  X  X ayesian Additive Regression Trees (BART) X , is proposed under Bayesian framework [6], which is essentially a  X  X um-of-trees X  model. Most of the existing work adopt the number of terminal nodes as the penalty. Such penalty cannot lead to sparse models since a tree with a small number of terminal nodes might still involve too many variables. To obtain sparse models, we propose a new nonparametric method based on multivariate dyadic regression trees (MDRTs). Similar to DDTs, MDRTs are constructed using penalized empirical risk minimization. The novelty of MDRT is to introduce a sparsity-inducing term in the penalty, which explicitly induces sparse solutions. Our contributions are two-fold: (i) Theoretically, we show that MDRTs can simultaneously adapt to the unknown sparsity and smoothness of the true regression functions, and achieve the nearly optimal rate of convergence for the class of (  X , C ) -smooth functions. (ii) Empirically, to avoid computationally prohibitive exhaustive search in high dimensions, we propose a two-stage greedy algorithm and its randomized version that achieve good performance in both function estimation and variable selection. Note that our theory and algorithm can be straightforwardly adapted to univariate sparse regression problem, which is a special case of the multivariate one. To the best of our knowledge, this is the first time such a sparsity-inducing penalty is equipped to tree models for solving sparse regression problems.
 The rest of this paper is organized as follows. Section 2 presents MDRTs in detail. Section 3 studies the statistical properties of MDRTs. Section 4 presents the algorithms which approximately compute the MDRT solutions. Section 5 reports empirical results of MDRTs and their comparison with CARTs. Conclusions are made in Section 6. We adopt the notations in [15]. A MDRT T is a multivariate regression tree that recursively divides the input space X by means of axis-orthogonal dyadic splits. The nodes of T are associated with to the cell B = associated to the subcells B k; 1 and B k; 2 : The set of terminal nodes of a MDRT T is denoted as term( T ) . Let B t be the cell in X induced by For each terminal node t , we can fit a multivariate m -th order polynomial regression on data points A ( b ) can be an arbitrary subset of { 1 , . . . , d } satisfying two rules: For a MDRT T , we define F m T to be the class of p -valued measurable m -th order polynomials MDRTs such that no terminal cell has a side length smaller than 2  X  L .
 Given integers M and N , let F M;N be defined as The final MDRT estimator with respect to F M;N , denoted as b f M;N , can then be defined as To define in detail pen( f ) for f  X  F M;N , let T and m be the MDRT and the order of polynomials corresponding to f , pen( f ) then takes the following form: relevant dimensions and There are two terms in (3) within the parenthesis. The latter one penalizing the number of terminal nodes |  X  ( T ) | has been commonly adopted in the existing tree literature. The former one is novel. Intuitively, it penalizes non-sparse models since the number of relevant dimensions r T appears in the exponent term. In the next section, we will show that this sparsity-inducing term is derived by bounding the VC-dimension of the underlying subgraph of regression functions. Thus it has a very intuitive interpretation. In this section, we present theoretical properties of the MDRT estimator. Our main technical result is Theorem 1, which provides the nearly optimal rate of the MDRT estimator.
 To evaluate the algorithm performance, we use the L 2 -risk with respect to the Lebesgue measure  X  (  X  ) , which is defined as R ( b f , f ) = E estimate constructed from n observed samples. Note that all the constants appear in this section are generic constants, i.e. their values can change from one line to another in the analysis. tions.
 function g : R d  X  R is called (  X , C ) -smooth if for every  X  = (  X  1 , . . . ,  X  d ) ,  X  i  X  N 0 , q , the partial derivative @ q g In the following, we denote the class of (  X , C ) -smooth functions by D (  X , C ) . f ( x ) = f j ( x S ) with r = | S | X  d .
 Theorem 3.2 of [9] shows that the lower minimax rate of convergence for class D (  X , C ) is exactly the same as that for class of d -dimensional Sobolev ball with order  X  and radius C . Proposition 1 The proof of this proposition can be found in [9]. jointly sparse with the index set S and r = | S |  X  d , the best rate of convergence can be improved The following is another technical assumption needed for the main theorem.
 Assumption 2 Let 1  X   X  &lt;  X  , we assume that This condition is mild. Indeed, we can even allow  X  to increase with the sample size n at a certain variables, this assumption easily holds with  X  = O ( term to the final rate of convergence.
 The next assumption specifies the scaling of the relevant dimension r and ambient dimension d with respect to the sample size n .
 Assumption 3 r = O (1) and d = O (exp( n )) for some 0 &lt;  X  &lt; 1 .
 Here, r = O (1) is crucial, since even if r increases at a logarithmic rate with respect to n , i.e. 1 /e . On the other hand, the ambient dimension d can increase exponentially fast with the sample size, which is a realistic scaling for high dimensional settings.
 The following is the main theorem.
 Theorem 1 Under Assumptions 1 to 3, there exist a positive number  X  that only depends on  X ,  X  and r , such that For large enough M, N , the solution b f M;N obtained from (2) satisfies where c is some generic constant.
 Remark 1 As discussed in Proposition 1, the obtained rate of convergence in (5) is nearly optimal up to a logarithmic term.
 Remark 2 Since the estimator defined in (2) does not need to know the smoothness  X  and the sparsity level r in advance, MDRTs are simultaneously adaptive to the unknown smoothness and sparsity level.
 Proof of Theorem 1 : To find an upper bound of R ( b f M;N , f ) , we need to analyze and control the approximation and estimation errors separately. Our analysis closely follows the least squares regression analysis in [9] and some specific coding scheme of trees in [15].
 Without loss of generality, we always assume b f M;N obtained from (2) satisfies the condition that obtain the desired result in Theorem 1.
 Let S m T be the class of scalar-valued measurable m -th order polynomials corresponding to  X  ( T ) , and let G m T be the class of all subgraphs of functions of S m T , i.e.
 Lemma 1 Let r T and N T be defined as in (3) , we know that Sketch of Proof : From Theorem 9.5 of [9], we only need to show the dimension of G m T is upper ward combinatorial analysis.
 The next lemma provides an upper bound of the approximation error for the class D (  X , C ) . polynomials h 1 , . . . , h p  X  X  X  T  X  X  K S m T where K  X  N , c is a generic constant depends on r .
 Sketch of Proof : This is a standard approximation result using multivariate piecewise polynomials. The main idea is based on a multivariate Taylor expansion of the function f j at a given point x 0 . Then try to utilize Definition 1 to bound the remainder terms. For the sake of brevity, we omit the technical details.
 The next lemma is crucial, it provides an oracle inequality to bound the risk using an approximation term and an estimation term. Its analysis follows from a simple adaptation of Theorem 12.1 on page 227 of [9].
 First, we define e R ( g, f ) = Lemma 3 [9] Choose for some prefix code [[ T ]] &gt; 0 satisfying One appropriate prefix code [[ T ]] for each MDRT T is proposed in [15], which specifies that (3 + log d/ log 2) |  X  ( T ) | .
 Remark 3 The derived constants in the Lemma 3 will be pessimistic due to the very large numerical validation to choose the tuning parameters.
 To prove Theorem 1, first, using Assumption 1 and Lemma 2, we know that for any K  X  N , there satisfying f  X  ( x ) = f  X  ( x S ) and |  X  ( T  X  ) | X  ( K + 1) r such that and The desired result then follows by plugging (9) and (10) into (8) and balancing these three terms. Exhaustive search of b f M;N in the MDRT space has similar complexity as that of DDTs and could be computationally very expansive. To make MDRTs scalable for high dimensional massive datasets, using similar ideas as CARTs, we propose a two-stage procedure: (1) we grow a full tree in a greedy manner; (2) we prune back the full tree to from the final tree. Before going to the detail of the algorithm, we firstly introduce some necessary notations.
 Given a MDRT T , denote the corresponding multivariate m -th order polynomial fit on  X  ( T ) by b f local squared error (LSE) on node t by b R m ( t, A ( t )) : then be computed by the following equation: The total cost of T , which is defined as the the right hand side of (2), then can be written as: Our goal is to find the tree structure with the polynomial regression on each terminal node that can minimize the total cost.
 The first stage is tree growing , in which a terminal node t is first selected in each step. We then perform one of two actions a1 and a2 : In each tree growing step, we need to decide which action to perform. For action a1 , we denote the drop in LSE as: child of node t . The drop in LSE takes the following form: For each terminal node t , we greedily perform the action a  X  on the dimension k  X  , which are deter-mined by In high dimensional setting, the above greedy procedure may not lead to the optimal tree since suc-cessively locally optimal splits cannot guarantee the global optimum. Once an irrelevant dimension has been added in or split, the greedy procedure can never fix the mistake. To make the algorithm more robust, we propose a randomized scheme. Instead of greedily performing the action on the dimension that leads the maximum drop in LSE, we randomly choose which action to perform ac-cording to a multinomial distribution. In particular, we normalize  X  b R such that: dimension k  X  . In general, when the randomized scheme is adopted, we need to repeat our algorithm many times to pick the best tree.
 The second stage is cost complexity pruning . For each step, we either merge a pair of terminal nodes cost. We repeat this process until the tree becomes a single root node with an empty active set. The tree with the minimum cost in this process is returned as the final tree. The pseudocode for the growing stage and cost complexity pruning stage are presented in the Appendix. Moreover, to avoid those actions that lead to  X  R = 0 . In addition, whenever we perform the m th order polynomial regression on the active set of a node, we need to make sure it is not rank deficient. In this section, we present numerical results for MDRTs applied to both synthetic and real datasets. We compare five methods: [1] Greedy MDRT with M = 1 (MDRT(G, M=1)); [2] Randomized MDRT with M = 1 (MDRT(R, M=1)); [3] Greedy MDRT with M = 0 (MDRT(G, M=0)); [4] Randomized MDRT with M = 0 (MDRT(R, M=0)); [5] CART. For randomized scheme, we run 50 random trials and pick the minimum cost tree.
 As for CART, we adopt the MATLAB package from [12], which fits piecewise constant on each parameter playing the same role as  X  in (3).
 Synthetic Data : For the synthetic data experiment, we consider the high dimensional compound symmetry covariance structure of the design matrix with n = 200 and d = 100 . Each dimension x j is generated according to where W 1 , . . . , W d and U are i.i.d. sampled from Uniform(0,1). Therefore the correlation between x j and x k is t 2 / (1 + t 2 ) for j We study three models as shown below: the first one is linear; the second one is nonlinear but additive; the third one is nonlinear with three-way interactions. All these models only involve four relevant variables. The noise terms, denoted as  X  , are independently drawn from a standard normal distribution.
 We compare the performances of different methods using two criteria: (i) variable selection and (ii) function estimation. For each model, we generate 100 designs and an equal-sized validation set per design. For more detailed experiment protocols, we set n max = 5 and L = 6 . By varying the values of  X  or  X  from large to small, we obtain a full regularization path. The tree with the minimum MSE successful. The numerical results are presented in Table 1. For each method, the three quantities reported in order are the number of success out of 100 designs, the mean and standard deviation of the MSE on the validation set. Note that we omit  X  X DRT X  in Table 1 due to space limitations. From Table 1, the performance of MDRT with M = 1 is dominantly better in both variable selection and estimation than those of the others. For linear models, MDRT with M = 1 always select the correct variables even for large t s. For variable selection, MDRT with M = 0 has a better performance compared with CART due to its sparsity-inducing penalty. In contrast, CART is more in function estimation. Moreover, the performance of randomized scheme is slightly better than its deterministic version in variable selection. Another observation is that, when t becomes larger, although the performance of variable selection decreases on all methods, the estimation performance of t , all methods tend to select more variables. Due to the high correlations, even the irrelevant variables are also helpful in predicting the responses. This is an expected effect. Real Data : In this subsection, we compare these methods on three real datasets. The first dataset is the Chemometrics data (Chem for short), which has been extensively studied in [3]. The data are from a simulation of a low density tubular polyethylene reactor with n = 56 , d = 22 and p = 6 . Following the same procedures in [3], we log-transformed the responses because they are skewed. The second dataset is Boston Housing 1 with n = 506 , d = 10 and p = 1 . We add 10 irrelevant variables randomly drawn from Uniform(0,1) to evaluate the variable selection performance. The third one, Space ga 2 , is an election data with spatial coordinates on 3107 US counties. Our task is to predict the x , y coordinates of each county given 5 variables regarding voting information. For Space ga, we normalize the responses to [0 , 1] . Similarly, we add other 15 irrelevant variables randomly drawn from Uniform(0,1). For all these datasets, we scale the input variables into a unit cube.
 For evaluation purpose, each dataset is randomly split such that half data are used for training and parameter  X   X  and  X   X  . We then train MDRTs and CART on the entire training data using  X   X  and  X   X  . We repeat this process 20 times and report the mean and standard deviation of the testing MSE in L = 6 . Moreover, for randomized scheme, we run 50 random trials and pick the minimum cost tree. From Table 2, we see that MDRT with M = 1 has the best estimation performance. Moreover, randomized scheme does improve the performance compared to the deterministic counterpart. In particularly, such an improvement is quite significant when M = 0 . The performance of MDRT(G, M=0) is always worse than CART since CART can have more flexible splits. However, using ran-domized scheme, the performance of MDRT(R, M=0) achieves a comparable performance as CART. As for variable selection of Housing data, in all the 20 runs, MDRT(G, M=1) and MDRT(R, M=1) never select the artificially added variables. However, for the other three methods, nearly 10 out of 20 runs involve at least one extraneous variable. In particular, we compare our results with those data. Our experiments confirm this result since in 15 out of the 20 trials, MDRT(G, M=1) and MDRT(R, M=1) never select these four variables. Similarly, for Space ga data, there are only 2 and 1 times that MDRT(G, M=1) and MDRT(R, M=1) involve the artificially added variables. We propose a novel sparse learning method based on multivariate dyadic regression trees (MDRTs). Our approach adopts a new sparsity-inducing penalty that simultaneously conduct function estima-tion and variable selection. Some theoretical analysis and practical algorithms have been developed. for high dimensional sparse learning problems. [1] G. Blanchard, C. Sch  X  afer, Y. Rozenholc, and K.-R. M  X  uller. Optimal dyadic decision trees. [2] Leo Breiman, Jerome Friedman, Charles J. Stone, and R.A. Olshen. Classification and regres-[3] Leo Breiman and Jerome H. Friedman. Predicting multivariate responses in multiple linear [4] R. Castro, R. Willett, and R. Nowak. Fast rates in regression via active learning. NIPS , 2005. [5] Xi Chen, Weike Pan, James T. Kwok, and Jamie G. Carbonell. Accelerated gradient method [6] Hugh A. Chipman, Edward I. George, and Robert E. McCulloch. Bart: Bayesian additive re-[7] Jerome H. Friedman. Multivariate adaptive regression splines. The Annals of Statistics , 19:1 X  [8] S. Gey and E. Nedelec. Model selection for cart regression trees. IEEE Tran. on Info. Theory , [10] Han Liu, John Lafferty, and Larry Wasserman. Nonparametric regression and classification [11] Han Liu and Jian Zhang. On the estimation consistency of the group lasso and its applications. [12] Wendy L. Martinez and Angel R. Martinez. Computational Statistics Handbook with MATLAB . [13] G. Obozinski, M. J. Wainwright, and M. I. Jordan. High-dimensional union support recovery [14] Pradeep Ravikumar, Han Liu, John Lafferty, and Larry Wasserman. Spam: Sparse additive [15] C. Scott and R.D. Nowak. Minimax-optimal classification with dyadic decision trees. IEEE [16] B.A. Turlach, W. N. Venables, and S. J. Wright. Simultaneous variable selection. Technomet-
