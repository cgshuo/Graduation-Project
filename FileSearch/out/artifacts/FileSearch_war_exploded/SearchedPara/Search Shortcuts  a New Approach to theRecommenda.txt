 The recommendation of queries, known as query suggestion, is a common practice on major Web Search Engines. It aims to help users to find the information they are looking for, and is usually based on the knowledge learned from past interactions with the search engine. In this paper we pro-pose a new model for query suggestion, the Search Shortcut Problem , that consists in recommending  X  successful  X  queries that allowed other users to satisfy, in the past, similar infor-mation needs. This new model has several advantages with respect to traditional query suggestion approaches. First, it allows a straightforward evaluation of algorithms from available query log data. Moreover, it simplifies the ap-plication of several recommendation techniques from other domains. Particularly, in this work we applied Collaborative Filtering to this problem, and evaluated the interesting re-sults achieved on large query logs from AOL and Microsoft. Different techniques for analyzing and extracting informa-tion from query logs, as well as new metrics and techniques for measuring the effectiveness of recommendations are pro-posed and evaluated. The results obtained clearly show the importance of several of our contributions, and open an in-teresting field for future research.
 H.3.3 [ Information Search and Retrieval ]: Query for-mulation, Search process Algorithms, Experimentation, Theory Search shortcut, Collaborative Filtering, Query Suggestion, model, evaluation
The main objective of a Web search engine is to help the user fulfill his information need efficiently. In this sense, any assistance provided to users in order to reduce the time spent searching is very valuable. In fact, major search en-gines, besides being able to answer queries in a few hundred milliseconds, usually provide the user with suggestions, in the form of queries that are somehow related to the user information need. Suggestions are generated on the basis of the query submitted by the user, the knowledge learned from past interactions, and whatever context information available.

The design and evaluation of effective and efficient algo-rithms for such suggestions is a complex and challenging task. For example, the performance of the methods pro-posed is traditionally evaluated by means of user studies. Although human-based evaluation has been found very pre-cise, its main inconvenience is the non repeatability of the experiments, which makes difficult an extensive comparison of such techniques.

In this work we define formally the Search Shortcut Prob-lem (SSP) as a problem related with the recommendation of queries in search engines and the potential reductions ob-tained in the users session length. This new problem for-mulation allows a precise goal for query suggestion to be devised: recommend queries that allowed in the past users that used a similar search process, to successfully find the information they were looking for.
 Actually, considering the recommendation of queries as a SSP has two important advantages with respect to previous query suggestion approaches:
First, the different techniques used to solve it can be eas-ily evaluated by exploiting query log data. In this way, the evaluation can be performed offline, without the need of real users to check the relevance of the recommendations, thus simplifying the comparison among different techniques. In this work, we present an evaluation methodology, and we propose a new metric specifically designed to evaluate the precision of algorithms for the SSP. The main advan-tage with respect to traditional metrics is that our approach clearly defines what relevance means in this context, allow-ing an effective evaluation to be easily conducted.
Furthermore, this new problem simplifies the application of several recommendation techniques to the suggestion of queries. This opens an interesting and promising new field to the big and active recommender systems research commu-nity. Specifically, techniques such as Collaborative Filtering, specially successful in e-commerce, can be easily applied to this context. We just need to extract the implicit relevance (ratings) of each query from the query log data.

In fact, in this paper we address this problem, by study-ing different techniques to extract the information from the query logs, and evaluating several collaborative filtering al-gorithms. The remaining of the paper is organized as fol-lows. In the next Section we present some related work, in the field on query recommendation and collaborative filter-ing. Then, in Section 3, we introduce the SSP, we define its theoretical model, and finally we present a new evaluation metric. Following, we discuss the application of Collabora-tive Filtering methods to this problem (Section 4), and the challenges that should be addressed. In section 5, we intro-duce the experiments performed and we discuss the results obtained. Finally we present some conclusions and outline future research directions.
The problem we presented in this paper is related to two different research fields that, although related, have been traditionally addressed from different points of view. We are talking about query suggestion and recommender systems.
Recommender systems have been used in several domains, being specially successful in electronic commerce. They can be divided in two broad classes: those based on content fil-tering , and those on collaborative filtering . As the name suggests, content filtering approaches base their recommen-dations on the content of the items to be suggested. They faced serious limitations when dealing with multimedia con-tent and, more importantly, their suggestions are not influ-enced by the human-perceived quality of contents. On the other side, collaborative filtering algorithms are based on the preferences of other users.

There are two main types of collaborative filtering algo-rithms: memory-based and model-based. Memory-based approaches use the whole past data to identify similar users [21], items [19], or both [25]. Generally, memory-based algo-rithms are quite simple and produce good recommendations, but they usually face serious scalability problems. On the other hand, model-based algorithms construct in advance a model to represent the behavior of users, allowing to pre-dict more efficiently their preferences. However, the model building phase can be highly time-consuming, and models are generally hard to tune, sensitive to data changes, and highly dependent on the application domain. In the liter-ature different approaches can be found: based on linear algebra methods [5, 18], clustering [24],
On the other hand, query suggestion techniques address the problem of recommend queries to users of a search en-gine. The techniques adopted are very different, but most of them are based on query log data, such as click-through data information [29]. Some of these employ clustering algo-rithms to determine queries that lead to similar documents [26, 2], or are focused on mining association rules from query logs [7]. Others employ reformulation of queries by previous users [12], or even let users choose the techniques to measure similarity among queries [28]. Query expansion techniques [6] have also been used.

The usage of query logs to evaluate the relevance of a given result, focused on improving the search result rank-ing, is tightly related with this problem. Existing techniques take into account information such as the document posi-tion, user click behavior, time user spends on a page, etc. Traditional approaches usually extract the information from a single query at a time [1], although there are several works that take into account the chains of queries that belong to the same search process [17].

In the context of collaborative web search [22], it has been proposed the suggestion of repeated queries that lead to sim-ilar relevant results [3].

The idea we present in this paper takes a completely new approach. First, we infer the relevance of a query based on whether it successfully ends a search session (i.e. if the query is useful to find the information the user is searching for). This relevance measure, extracted from query log data, can be used to fill some cells in the rating matrix typically used by collaborative filtering algorithms. Thus, such effective technique can be applied to the recommendation of queries.  X  Successful sessions  X  [23, 22] have already been taken into account as a way o evaluate search result promotions. In this paper, instead,  X  Satisfactory searches  X  (as opposed to successful sessions) are taken as the key factor in building query recommendations.
The aim of the SSP is to recommend queries that allowed users, in the past, to successfully find the information they were looking for using a similar search process. For exam-ple, let us suppose a (sufficiently) high number of users have queried the engine for q 1 , q 2 , q 3 , and finally, after asking for q , they found the information they needed. Therefore, we can consider query q 4 relevant for users interested in topics related to q 1 , q 2 , and q 3 . Whenever another user starts to search for topics related to q 1 , q 2 , or q 3 , the query q be proposed as a shortcut . Obviously, the earlier a relevant shortcut is shown during the user session, the more effective it has to be considered. Moreover, a shortcut for a session may not need to be the last query submitted: a query that will anticipate the positive ending of a session is still accept-able. The more a shortcut (potentially) reduces the length of a session the more important it is.

Of course, our problem formulation defines a collaborative approach to query suggestion: the main idea is to exploit the experience of previous users to drive other users with similar information needs in the right direction.

We have validated such assumption by analyzing the ac-tual behavior of web search engine users recorded in the AOL query log [14]. This query log is composed of approximately 36 million query records, containing about 20 million web queries collected from about 650,000 users over three months (March-May 2006). From the query log we extracted search sessions, taking into account that a session expires if the user spends, at least, 30 minutes without any action in the search engine. This is a limit commonly used in query log analysis [27]. As a result, we extracted 10,765,687 search sessions with an average length of 2,03 queries.

From these search sessions, we considered only the ses-sions involving two or more queries (4,339,683 sessions with an average length of 4.88 queries/session) and we grouped together all the sessions starting by the same query. The sessions with just one query were removed because they ob-Figure 1: Percentage of satisfactory query paths in the AOL query log, by popularity of the first query. viously do not contain any query path. We want to analyze the query paths followed by different users who started with the same query, assuming they could have the same infor-mation need. Some of the users may follow a  X  X ight X  path and end up after visiting some documents proposed by the search engine, but others may end the session without vis-iting any result. Therefore, for each of these sessions, we examined the final query and checked if the query was suc-cessful or not: a query was considered successful if the user clicked, at least, one result, unsuccessful otherwise. The ses-sions that ended with a successful final query are considered satisfactory query paths.

Summarizing, we found 140,165 different initial queries (not unique), which are the starting point for several ses-sions (at least two). Considering these query paths, 64% of the sessions were satisfactory, while 36% ended with a failed query. In Fig. 1 we show the percentage of satis-factory query paths, sorted by the logarithm of the rank of the initial queries (similarly to the common Zipf graph). In the left hand side of the figure, which represents the query paths associated with the most frequent initial queries, we can see that the majority of sessions ended successfully, but at the same time there are several sessions ended up with-out clicking on any document, although they started at the same point. This shows that the information provided by the satisfactory sessions could lead the failed sessions to a successful ending point, which is the main goal behind this work.

For completeness, the right hand side contains the less frequent queries and the graph represents the dots in the common fractions, which produces its particular shape. For example, for all the initial queries repeated twice, we have a dot at: 2/2, 1/2 and 0/2; for the initial queries repeated three times, we have a dot at: 3/3, 2/3, 1/3 and 0/3; and so on. In this case, the potential benefits are less clear because the experience that could be extracted from similar query paths is more reduced.
Let U = { u | u is a user } be the set of all users of a search engine. Let Q = { q | q is a user query } be the set of all the queries submitted by the users of a search engine.

Definition 1. A query session for a user u  X  U is a sequence of queries u has submitted to a search service look-ing forward to satisfying an information need. In symbols,  X  session are related to the same user search task.

We will drop the superscript u in the specification of  X  , e.g.  X  = &lt; q 1 ...q n &gt; , whenever u is clear from the context. The i -th element of a session is denoted by  X  i .
 The set of all sessions is defined as S .

Definition 2. We define a function c : S  X  [1 ..n ]  X  { 0 , 1 } as c (  X ,i ) = 1 if in  X  the user has clicked on at least a then, by definition, c (  X ,i ) = 0 .

Definition 3. We define a session  X  = &lt; q 1 ...q n &gt; of length n , to be satisfactory if and only if c (  X ,n ) = 1 , unsatisfactory otherwise.

Definition 4. We define a k -way shortcut , with 1  X  k  X  |Q| , as a function h : S  X  2 Q taking as argument a session and returning a set of queries of cardinality | h (  X  ) | X  k .
 H is the set of all possible shortcut functions.

Definition 5. The head  X  t | of  X  = &lt; q 1 ...q n &gt; up to t  X  n is the sequence of the first t queries in  X  , i.e.  X  q ,...,q t &gt;
Definition 6. The tail  X  | t of  X  = &lt; q 1 ...q n &gt; from t  X  n is the sequence of the last n  X  t queries in  X  , i.e.  X 
Definition 7. Let  X  = &lt; q 1 ...q n &gt; be a satisfactory ses-sion. The similarity of a k -way shortcut h on a head  X  t | a tail  X  | t is defined as
Where f (  X  ) is a monotonic increasing function. The func-tion [ q =  X  m ] = 1 for 1  X  if and only if the query q is equal to the query  X  m .

The similarity function defined in (1) can be used as an objective evaluation measure for the SSP. For example, to evaluate the effectiveness of a shortcut function h on S , the sum or average of s on all sessions in S can be computed. Note that the similarity function can be rewritten to include the function c to give importance only to those queries that had a result clicked.

It is worth noticing, that the main difference between query shortcuts and query suggestion is actually represented by the function  X  q = `  X  | t  X  m  X  in equation (1). By relaxing the strict = requirement and by replacing it by a similarity relation, i.e.  X  q  X  `  X  | t  X  m  X  (that is  X  q  X  `  X  | t only if the query q is similar to the query  X  m ) the problem reduces, basically, to query suggestion. By defining an ap-propriate similarity function the equation in (1) can be used to evaluate query suggestion effectiveness as well.
Finally, we should consider the influence the function f ( m ) has in the definition of scoring functions. Actually, depend-ing on how f is chosen, different features of a shortcut gen-erating algorithm may be tested. For instance, by setting f ( m ) to be the constant function f ( m ) = c , we measure simply the number of queries in common between the query shortcut set and the queries submitted by the user. A non-constant function can be used to give an higher score to queries that a user would have submitted later in the session. The exponential function f ( m ) = e m was chosen to assign an higher score to shortcuts suggested early. Smoother f functions can be used to modulate position effects.
As metioned above, several algorithms can be applied to the SSP. In this work, we have studied the application of traditional Collaborative Filtering techniques. Those tech-niques have obtained very good results in other recommen-dation domains, also based on the experiences from previous users, so they seem suitable for our purposes.

Collaborative filtering deals with a set of users U , and a set of of items I . User preferences are taken into account as item ratings, a numeric value representing the utility of an item to a given user. The subset of valid ratings is de-noted as R . Ratings can be explicitly introduced by users, or implicitly extracted from user interaction (e.g. from query log data). Preferences for all users are stored in a user-item matrix, known as the rating matrix V . Each entry v ui of V represents the rating of user u for item i , with v ui  X  R  X  X  where {  X  } indicates that the user has not rated the item yet.
Thus, to apply collaborative filtering to the SSP, we need to fill such matrix with the information in the query log data.

First, the concepts of the SSP (users, queries, terms and sessions) have to be mapped to the pure collaborative filter-ing problem (users and items). As the goal in the SSP is to recommend queries for a given session, it seems reasonable to treat each session as a user , and each query as an item .
Second, the query ratings must be inferred from the in-formation in the query log. As a preliminary approach, in this work we rate the queries focusing in the last query of each session. If such last query was successful (the user has clicked at least one result), then a positive rating (10.0) is given to the query. Otherwise, it is given a negative rating (0.0). All remaining queries are considered neutral (5.0).
Finally, we should note that this particular problem of-fers some important challenges to traditional collaborative filtering algorithms. In fact, domains where they have been usually applied are much more dense than a query log, i.e., they have much more relations (ratings) between users and items. For example, in an electronic commerce site most products have been rated or purchased by several customers. Thus, we can obtain information about an item from several users. However, in query session logs there are many queries that only appear in a single session. This lack of informa-tion is the well-known sparsity problem [11]. In addition, web search query logs usually contain much more data than those collected in traditional domains like e-commerce, and their size grows continuously at a very high rate. Hence, both prediction and training computational efficiency should be considered.

Moreover, different alternatives can be applied to identify unique queries among all queries in the log. A simple ap-proach consists in treating the query string as a whole, thus considering two queries as unique only if they are exactly the same. The problem with this approach is that queries with the same terms, but in different order, are not considered the same query, thus incrementing the dataset sparsity. So, a better approach should take into account the terms that compose each query, reordering them properly if needed. In-deed, techniques commonly applied in information retrieval, such as term stemming or stopwords removal, can also be used in order to reduce matrix sparsity.
Different settings and collaborative filtering algorithms have been evaluated using real query log data, from both the AOL [14] and the MSN 1 Web search engines. We have derived ten different datasets from each query log, by ap-plying different techniques for log preprocessing. Firstly, we have selected the unique queries in the log, by applying the following techniques discussed above: Table 1: Results in terms of relative density  X  10 5 for the different log preprocessing techniques applied to the AOL and MSN query logs.

Then, we filtered out sessions having a length lower than a given threshold. The results reported in Table 1, shows that both the first and second techniques allow to reduce the data sparsity, which is supposed to be advantageous to improve accuracy of the results. The value reported in the column labeled Thr indicates the threshold applied to the session length, where No means no threshold. Specially, ig-noring sessions with few queries has a significant impact in dataset density. When only sessions with at least 3 queries are considered, sparsity is highly reduced, which can be eas-ily explained by the fact that a great number of sessions remain out of the final dataset. In fact, the total number of sessions is reduced up to 80% on the MSN dataset, and up to 70% on the AOL.

In the same way, applying any of the log preprocessing techniques also helps to reduce data sparsity, although they have less impact than discarding sessions with few queries. In this case, density is incrementing because these tech-niques reduce the number of unique queries. Best results
This dataset refers to the Microsoft 2006 RFP dataset pro-vided for the Web Search Click Data 2009 conference
We have used the Porter Stemming Algorithm [16] are obtained when both stopword removal and term stem-ming are applied, specially if small sessions are discarded too.

To perform the evaluation, we have divided the data in two subsets: training and evaluation, following a two-steps process. First, we have randomly chosen 80% of the sessions as the data to be used to train the algorithms. Then, we fed the algorithms with the first two queries from each session in the evaluation set. These queries represent the session head the algorithm needs as prediction context.

Results are then evaluated using the similarity measure proposed in Section 3.2, specially designed for the SSP. We have used several variations of the monotonic increasing function f ( m ) (see equation 1), in order to evaluate different aspects of each algorithm: a constant function, f ( m ) = 1; a linear function, f ( m ) = m ; a quadratic function, f ( m ) = m 2 ; and an exponential function, f ( m ) = e m .

For completeness, we have also used traditional metrics in the evaluation. These metrics can be classified on prediction, classification and rank accuracy metrics.

Prediction accuracy metrics measure the difference be-tween the rating the system predicts and the real rating, made by the user. Among them, we have used the Mean Absolute Error (MAE), as it is the most popular metric in the literature.

On the other side, classification accuracy metrics aim to evaluate the quality of a recommendation list. We have used precision and recall and ROC Curves [10]. These evaluate an algorithm based on whether the recommended items are actually good. Rank accuracy metrics, such as Half-Life Utility [4], could also be useful as they also take into account the order of the items in the list.

Finally, we have also found valuable to report the cov-erage of the algorithm, this is, the percentage of items the algorithm is able to make a prediction for. Algorithms with lower coverage will tend to recommend the same items, and therefore, they are less interesting from the search shortcuts point-of-view. Figure 2: Results on MAE -left axis-and coverage -right axis-on both AOL and MSN datasets
First, we analyze the prediction accuracy of the different algorithms. In Figure 2, the results measured according to the MAE metric are shown. It can be observed that most algorithms present pretty accurate results, specially on the AOL dataset, with a mean absolute error around 5%-10%. This good results, however, should be taken with care.
If we take a look at the coverage of each algorithm (also presented in Fig. 2), we can see that good results on MAE (this is, accurate predictions) are often related with a low coverage. The reason is that some algorithms extract very few information from available data, so they can only pre-dict ratings for items with many ratings. These are usually popular items, that are often  X  X asy-to-predict X , so the accu-racy is better on those items. However, they are actually not very useful for the user [30]. This is the case of both user and item-based algorithms, that take into account only some parts of the available data (the relations among users or items), and thus they present a low coverage on sparse datasets. On the other side, some model-based approaches, such as Trends-Based or Personality Diagnosis present over-all good results.

Indeed, the MAE metric has also another important limi-tation in our context, related to the way we implicitly assign ratings. Taking into account that we have given a neutral rating to all queries in the session but the last one, and we are just evaluating relatively long sessions, most of the queries result to have a neutral rate. Thus, for the algo-rithm it is easier to obtain a prediction for such items. For example, an algorithm that always return a neutral rating would obtain exact results on most items, and those would compensate bad results obtained on actually good items.
Evaluation with classification and rank accuracy metrics avoids this problem, although it also has important limita-tions. When the evaluation is performed on a sparse, large offline dataset like those used in our experiments, it is highly probable that the recommendations computed by the algo-rithm do not correspond to any of the items we have in the evaluation subset. Thus, evaluation cannot be performed in a satisfactory way. To minimize this problem, we have constrained the algorithm to recommend items in the evalu-ation set, and since the recommendation target is restricted to few items, an error has a great impact on the final preci-Figure 3: Correlation between Search Shortcuts Similarity and precision metric for different choice of function f on both the AOL -up-and MSN -bottom-datasets. sion. This is the reason of the modest results observed, with a precision below 30% on AOL and below 10 % on MSN for all the algorithms.

Moreover, traditional metrics deal with the concept of  X  X tem relevance X , that is ambiguous in the query recommen-dation domain. Of course, a query should be considered relevant when it leads to the information the user was look-ing for, but this cannot be extracted from query log data. So, researchers can choose among several ways of comput-ing  X  X elevance X : whether the query has finished a session or not, number of results clicked, time spent looking over the results, etc.

The  X  X earch Shortcut Similarity X  metric we have proposed in Section 3.2 overcomes this ambiguity, by clearly defining what we should evaluate: the amount of queries the shortcut has saved to the user.

We have studied the relation between this new metric and traditional classification accuracy metrics. Specially inter-esting is to study the correlation between our metric and the traditional precision, as both aim to measure similar char-acteristics of the algorithms. As we can see in Figure 3, correlation between both metrics depends on the function f chosen. In particular, if we choose a constant function f ( x ) = 1, our metric is strongly correlated with precision. This is not surprising, as with a constant function our metric measures the percentage of queries in the evaluation subset that appear on the original session, which are the  X  X elevant X  queries from a query shortcuts point-of-view. This is a very important feature, because it means that results in our met-ric can be interpreted in terms of precision. However, differ-ently from precision metric, our similarity measure offers a clear interpretation of what relevance means in Search Short-cut context, and how it should be computed. On the other hand, if we choose a different f , we can evaluate different characteristics of the algorithms, not covered by traditional metrics. For example, by using the exponential function f ( x ) = e x , we are evaluating not only if the queries sug-gested are  X  X elevant X , but also how relevant they are, this is, how close they are to the final query in the session, and thus the time they could potentially save.

An evaluation of the different algorithms considered ac-cording to this metric can be seen in Tables 2 and 3. Results on both datasets show clear differences between memory-based approaches and some model-based algorithms. When we choose a constant f , thus measuring equally any shortcut despite its position in the original session, user-based algo-Table 2: Results of several algorithms according to Search Shortcuts Similarity on AOL dataset Table 3: Results of several algorithms according to Search Shortcuts Similarity on MSN dataset rithms offer the best results. However, when we change f to evaluate how useful the recommendations are in order to re-duce the session length, we can see that both Trends-Based and Personality Diagnosis present more accurate results. In-deed, this observation is consistent among most metrics, on both datasets.

Moreover, one observation that can be extracted from the results obtained on all kind of metrics is the negative impact of the extreme sparsity of the dataset. This is an expected result, as traditional Collaborative Filtering algorithms have been designed for contexts with more relatively dense data. Thus, techniques to reduce the dataset density, such as the ones we have proposed in Section 4, are expected to improve the results. As introduced in Section 5.1, we have studied two approaches:
In Fig. 4 we can see that ignoring sessions with less than 3 queries has a significant impact in accuracy. For some model-based algorithms, the improvement obtained is greater than 30%, and grows up to near 50% when combined with log preprocessing techniques. This is an expected re-sult, as collaborative filtering algorithms do not perform well with users that have rated few items, which in Search Short-cuts context is equivalent to small sessions. It corresponds to the well-known cold-start problem [20], that, as seen, can be easily addressed in our context. On memory-based al-gorithms (in the middle of each chart), however, results are not very remarkable. This can be explained by the fact that these algorithms have serious limitations to extract informa-Figure 4: Percentage of improvement in algorithm accuracy, when sessions with less than 3 queries are ignored, on both the AOL -right-and MSN -left-datasets. Order of the algorithms is the same as in Fig. 2, 3 and 4 tion from sparse datasets. The effect of this preprocessing techniques on dataset density (see Table 1) is not enough for such algorithms, and even if, in some cases, it can lead to slightly worse results.

On the other hand, we have also studied the impact of log preprocessing techniques on their own, without session threshold. Results highly depend on the dataset. On the AOL, only term reordering seems to improve the accuracy of the algorithms, while other techniques have a negative impact. However, on the MSN dataset, both term stem-ming or a combination of stemming and stopword removal have positive impact on the accuracy, up to near 10% of im-provement, which is an outstanding result. On the datasets studied, only stopword removal, when not combined with stemming, shows always bad results. It seems clear that log preprocessing techniques are much more useful when com-bined with session threshold.
In this paper we have proposed a new model for suggest-ing queries to users of a search engine, based on Search Shortcuts . Directly associated with it, we have presented a well-defined model and evaluation methodology that al-lows the comparison and evaluation of algorithms using an offline dataset. Moreover, our model allows the application of recommendation techniques to the problem of query rec-ommendation.

In particular, we have studied the application of collabo-rative filtering techniques. Evaluation has been done using real query log data from AOL and MSN datasets. We have obtained notably accurate results, which demonstrate that this kind of algorithms fits quite well this problem. However, we have also observed the low coverage of some algorithms, which is related with the sparsity of datasets. Specially, tra-ditional user and item-based approaches present very low coverage, due to the fact that they are only based on a small part of the information available.

To reduce the impact of the sparsity problem, we have studied several log preprocessing techniques. We have shown the great impact of some of such methods, significantly im-proving the results. In particular, ignoring sessions with few queries has been shown to be a very successfully technique, with improvements up to 50% in accuracy. In the same way, log preprocessing techniques, such as term stemming or stop-word removal, are also useful, specially when combined with the removal of small sessions.

When metrics other than accuracy are taken into account, the obtained results are modest. We have related this with the nature of some algorithms, the high volume and spar-sity of data, and the limitations of traditional metrics and evaluation methodology. Classification accuracy metrics ob-tain valuable results, but only if the details of the evaluation methodology are taken into account. In particular, the us-age of offline and sparse datasets imposes several limitations in the evaluation, as items considered relevant by the algo-rithm cannot be compared in many cases with real data.
To overcome these limitations we have proposed a new metric, specially designed for the evaluation of the Query Shortcuts Discovery Problem. Its main advantage is that its results can be easily interpreted in terms of how good are the algorithm recommendations to reduce the session length, this is, to help the users find the information they need in less time. Moreover, it allows to evaluate different aspects of the algorithm, including its precision according to the traditional meaning of that term.

Finally, we have found several limitations that should be addressed in future researching. First, the three level rating (positive, negative, neutral) does not perform as expected for collaborative filtering algorithms, specially because most queries are in fact neutral. Also, the idea of interpreting a click as a success should be improved taking into account more information available in the query log, such as whether the user has explored extra result pages, time spent until (s)he queries again, etc. In the same way, in real query log data a single session can contain queries pertaining to several searches. Techniques to overcome this problem should also be studied.

In addition, we have found some limitations of traditional algorithms, mainly related with the sparsity of query log data, so future work should develop new algorithms specially designed for this new problem. The comparison of collabo-rative filtering algorithms and traditional query suggestions techniques is also planned. [1] E. Agichtein, E. Brill, and S. Dumais. Improving web [2] R. A. Baeza-Yates, C. A. Hurtado, and M. Mendoza. [3] E. Balfe and B. Smyth. Improving web search through [4] J. S. Breese, D. Heckerman, and C. M. Kadie.
 [5] J. Canny. Collaborative filtering with privacy via [6] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. Ma.
 [7] B. M. Fonseca, P. B. Golgher, E. S. de Moura, and [8] V. Formoso, F. Cacheda, and V. Carneiro.
 [9] J. Herlocker, J. A. Konstan, and J. Riedl. An [10] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and [11] Z. Huang, H. Chen, and D. Zeng. Applying associative [12] R. Jones, B. Rey, O. Madani, and W. Greiner.
 [13] D. Lemire and A. Maclachlan. Slope one predictors for [14] G. Pass, A. Chowdhury, and C. Torgeson. A picture of [15] D. Pennock, E. Horvitz, S. Lawrence, and C. L. Giles. [16] M. F. Porter. An algorithm for suffix stripping. pages [17] F. Radlinski and T. Joachims. Query chains: learning [18] J. D. M. Rennie and N. Srebro. Fast maximum margin [19] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl. [20] A. I. Schein, A. Popescul, L. H. Ungar, and D. M. [21] U. Shardanand and P. Maes. Social information [22] B. Smyth. A community-based approach to [23] B. Smyth, E. Balfe, O. Boydell, K. Bradley, P. Briggs, [24] L. Ungar and D. Foster. Clustering methods for [25] J. Wang, A. P. de Vries, and M. J. T. Reinders. [26] J.-R. Wen, J.-Y. Nie, and H.-J. Zhang. Clustering user [27] R. W. White, M. Bilenko, and S. Cucerzan.
 [28] O. R. Za  X   X ane and A. Strilets. Finding similar queries [29] Z. Zhang and O. Nasraoui. Mining search engine [30] C.-N. Ziegler, S. M. McNee, J. A. Konstan, and
