 Pseudo-feedback-based automatic query expansion yields e f-fective retrieval performance on average, but results in pe r-formance inferior to that of using the original query for man y information needs. We address an important cause of this robustness issue, namely, the query drift problem , by fusing the results retrieved in response to the original query and to its expanded form. Our approach posts performance that is significantly better than that of retrieval based only on the original query and more robust than that of retrieval using the expanded query.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Retrieval Models General Terms: Algorithms, Experimentation Keywords: query expansion, pseudo feedback, robust query expansion, fusion, query drift
Pseudo-feedback-based query expansion methods augment a query with terms from the documents most highly ranked by an initial search [4]. While the state-of-the-art approa ches post effective performance on average, their performance is sometimes quite inferior to that of using only the original query [2, 6, 5]. One of the causes for this robustness prob-lem is query drift [11]: the change in underlying  X  X ntent X  between the original query and its expanded form.

Most approaches for query-drift prevention  X  X mphasize X  the query terms when constructing the expanded form [12, 13, 1]. In contrast, we demonstrate the merits in  X  X eward-ing X  documents that are retrieved in response to the ex-panded form and that are  X  X aithful X  to the original query. Specifically, inspired by work on combining multiple query representations [3] we fuse the lists retrieved in response to the original query and to its expanded form.
We use q , d , and Score init ( d | q ) to denote a query, a docu-ment, and a score assigned to d in response to q by some ini-tial search, respectively; D init denotes the list of documents most highly ranked according to Score init ( d | q ). We assume that some pseudo-feedback-based query expansion approach uses information from some documents in D init for ranking construction [14]. We use only the  X  terms to which RM1 as-signs the highest probability, and denote the resultant (no r-malized) distribution by  X  p RM 1 (  X  ; n,  X ,  X  ) [1]. Then, we set Score pf ( d | q ) = exp(  X  CE  X  p Dir [  X  ] d (  X  )  X   X 
We use RM3 [1] as a reference comparison for our meth-ods. RM3 performs query-anchoring at the language model level by interpolating (with parameter  X  )  X  p RM 1 (  X  ; n,  X ,  X  ) with a maximum likelihood estimate of the query terms.
We used the TREC corpora from Figure 1 for experi-ments. (Topics X  titles serve as queries.) We applied Porter stemming via the Lemur toolkit (www.lemurproject.org), and removed INQUERY stopwords.

We set D init to the 1000 documents with the highest ini-tial ranking score Score init ( d | q ). To create a set P F ( D init ) of 1000 documents, we select the values of RM1 X  X  free param-eters from the following sets so as to optimize MAP@1000 (henceforth  X  X AP X ) performance: n  X  X  25 , 50 , 75 , 100 , 500 , 1000 } .  X  , which controls query-anchoring in the interpola-tion and RM3 algorithms, is chosen from { 0 . 1 , . . . , 0 . 9 } to optimize MAP;  X  is set to 1000 [14].

We determine statistically significant MAP differences us-ing Wilcoxon X  X  two-tailed test at a confidence level of 95%. We also present for each method the percentage of queries (denoted by  X  &lt; Init  X ) for which the (M)AP performance is worse than that of the initial ranking. Lower values of  X  &lt; Init  X  correspond to improved robustness. We see in Figure 1 that all fusion-based methods yield MAP performance that is better to a statistically significan t degree than that of the initial ranking that utilizes only th e original query. The interpolation algorithm is the best MAP performing fusion-based method, but it incorporates a free parameter while combMNZ and re-rank do not.

Figure 1 also shows that all fusion-based methods are more robust than RM1. (Refer to the  X  &lt; Init  X  measure.) Furthermore, combMNZ and interpolation post MAP per-formance that is never worse to a statisticaly significant de -gree than that of RM1. We also observe that combMNZ and re-rank, which use fusion of retrieved results for query -anchoring, are more robust than RM3 that performs language-model-based query-anchoring; RM3, however, posts the best MAP performance in Figure 1.
