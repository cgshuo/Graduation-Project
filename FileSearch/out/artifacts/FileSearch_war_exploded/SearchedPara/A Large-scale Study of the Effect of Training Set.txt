 In this work we describe the results of a large-scale study on the effect of the distribution of labels across the different grades of relevance in the training set on the performance of trained ranking functions. In a controlled experiment we generate a large number of training datasets wih different label distributions and employ three learning to rank algo-rithms over these datasets. We investigate the effect of these distributions on the accuracy of obtained ranking functions to give an insight into the manner training sets should be constructed.
 Categories and Subject Descriptors: H. Information Systems; H.3 Information Storage and Retrieval; H.3.3 In-formation Search and Retrieval:Retrieval models General Terms: Experimentation, Measurement, Theory Keywords: Learning-to-Rank, Document Selection Method-ologies
Learning to rank has become one of the most prominent frameworks for constructing ranking functions. While much thought and research has been placed on the development of sophisticated learning to rank algorithms, relatively little research has been conducted on the construction of training datasets, nor on the effect of this on the effectiveness of learning to rank algorithms.

Aslam et al. [1] considered the problem of constructing effective training datasets with low cost by investigating a number of methods for selecting only a handful of documents to be labeled per query. A post-hoc analysis of the effective-ness versus the characteristics of the constructed datasets revealed that the ratio of relevant to non-relevant docu-ments in the dataset was one of the most influential factors. The dataset used by Aslam et al. was based on TREC 6, 7 and 8 ad-hoc corpora. It contained 150 queries with bi-nary labels and 22 features per query-document pair. Hence, even though this dataset allowed Aslam et al. to quantify performance as a function of the constructed training sets, the conclusions remain limited to small TREC collections
We gratefully acknowledge the support provided by the Eu-ropean Commission grant FP7-PEOPLE-2009-IIF-254562 and the NSF grant IIS-1017903.
 ear Regression and Ranking SVM [3]. The performance of each of the algorithms is measured by nDCG at cut-off 10 (nDCG@10). We tuned the algorithms carefully by prior in-vestigation of the effect of feature normalization and on the validation sets corresponding to each of our derived training sets. In the case of SVM we found prior to our study that feature normalization greatly affects the learning ability of SVM. We took the log of pagerank and similar static doc-ument features to put those features on the same scale as text-based (e.g. language model derived) features. Further, we applied zero-mean unit variance normalization in order to make the features scale-free. RankBoost was not sensitive to feature normalization and we ran it without transforming the features in any way.
To quantify the distribution of labels over the four rele-vance grades we use two summaries which were found to be particularly informative about the effectiveness of learning-to-rank algorithms as measured by nDCG@10, (a) the nor-malized cumulative gain of the judgment set, and (b) the variance over the judgment set.

Figure 1 demonstrates the interplay between the distribu-tion of the labels over the different grades in the training sets and the three learning to rank algorithms. In Figure 1 (left column), the normalized cumulative gain of the grades of the labeled urls in the training set per query is computed. For instance, for the example distribution (4,1,1,2) above with 4 completely irrelevant documents, 1 marginally relevant etc., the cumulative gain is, 4 0 +1 1 +1 2 +2 3 .Thelargerthenor-malized cumulative gain the more skewed is the distribution of labeled documents towards perfectly relevant documents; the smaller it is, the more it is skewed towards completely irrelevant ones. NCG values in the middle of x axis range indicate a balanced training set regarding the  X  X ositive X  and  X  X egative X  instances. As it can be observed from all three plots on the left, the optimal performance is indeed reached when there is a balance of labels in the dataset. Note that this is not a contradiction to Aslam et al. who report a dropping performance when the ratio of  X  X ositive X  to  X  X eg-ative X  instances exceeds some threshold. In that work and due to the manner the datasets were constructed one can only view the right tail of what appears a bell-shape curve in the left-hand side plots of Figure 1.

Obviously, a number of different datasets can be con-structed that have a level of balance in the labels. For instance, a uniform distribution is similarly balanced to an equal distribution of labels in the two middle grades (marginally relevant and relevant) or an equal distribution of labels in the two extreme grades (completely irrelevant and perfectly relevant). To distinguish these cases of balanced datasets we also compute the variance of the grades in the datasets. For instance, for the example distribution (4,1,1,2) above, the variance would be the variance of the flattened list of relevance grades: [[0,0,0,0], [1], [2], [3,3]]. The higher the variance the farther the labels are from the middle grades, while the smaller the variance the more uniform the distri-bution. The effectiveness of the learning algorithms as a function of variance is shown in Figure 1 (right column). As the figure demonstrates the higher the variance the more effective the algorithms are. Hence, optimal performance is reached when instances are mostly taken from the extreme grades of relevance.

