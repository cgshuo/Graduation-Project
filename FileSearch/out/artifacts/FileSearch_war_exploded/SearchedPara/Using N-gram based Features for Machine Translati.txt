 assigned in various ways. Fiscus ( 1997) used voting by frequency of word occurrences. Mangu et. al., ( 2000 ) computed a word posterior probabilit y based on voting of that word in different hypotheses . Moreover, the overall confidence score is usually formulated as a log -linear model including extra features including language model (LM) score, word count, etc . extensively studied in past work ( Matusov , et. al., 2006, Rosti , et. a l., 2007, He, et. al., 2008) . However, utilization of n -gram agreement information among the hypotheses has not been fully explored yet . Moreover, i t was argued that the confusion network decoding may introduce undesirable spur words t hat break coherent phrases (Sim, et. al., 2007) . Therefore, we would prefer the consensus translation that has better n -gram agreement among outputs of single systems. proposed a n n -gram posterior probability based LM for MT . For each source sentence, a LM is trained on the n -best list produced by a single MT system and is used to re -rank that n -best list itself . On the other hand, Mat us ov et al. (2008) proposed a n  X  X dapted X  LM for system combination , where t h is  X  X dapted X  LM is trained on translation hypotheses of the whole test corpus from all single MT systems involved in system combination . features based on n -gram agreement measure to improve the performance of sys tem combination . The first one is a sentence specific LM built on translation hypotheses of multiple systems ; the second one is n -gram -voting -based confidence . E xperimental results are presented in the context of a large -scale Ch inese -English translation t ask. where  X   X  denotes the hypothesis set,  X   X  ,  X  denotes the Kronecker function, and  X  (  X   X  |  X  ) is the posterior probability of translation hypothesis  X   X  , which is expressed as the weighted sum of the system specific posterior probabilities through the systems that contains hypothesis  X   X  ,  X   X   X  =  X   X   X  (  X  where  X   X  is the weight for the posterior probability of the k th system  X   X  , and 1  X  is the indicator function. posteriors are derive d based on a rank -based scoring scheme. I.e., i f translation hypothesis  X   X  is the r th best output in the n -best list of system  X  , posterior  X   X   X   X   X  ,  X  is approximated as: where  X  is a rank smoothing parameter.
 straightforward approach of using n -gram fractional counts is to formu late it as a sentence specific online LM . Then the online LM score of a path in the confusion network will be added as an additional feature in the log -linear model for decoding. The online n -gram LM score is computed by : T he LM score of hypothesis  X  is obtained by: Since new n -grams unseen in original translation hypotheses may be proposed by the CN decod er , LM smoothing is critical . In our approach, the score of the online LM is smoothed by taking a l inear interpolation to combine scores of differ ent orders. W e evaluate the proposed n -gram based features on the Chinese -to -English (C2E) test in the past NIST Open MT Evaluations. The experimental results are reported in case sensitive BLEU sco re ( Papineni, et. al., 2002) .
 combination parameter training, is the newswire and newsgroup parts of NIST MT06 , which contain s a total of 10 99 sentences . The test set is the " current " test set of NIST MT08 , which contains 1357 sentences of newswire and web -blog data. Both dev and test sets have four reference translations per sentence. systems were combined for consensus translations. These selected systems are based on various translation paradigms, such as phrasal, hierarchical, and syntax -based systems. Each system produces 10 -best hypotheses pe r translation. T he BLEU score range for the eight individual systems are from 2 6 . 1 1% to 3 1 . 0 9% on the dev set and from 2 0 .4 2 % to 2 6.24 % on the test set. In our experiments, a state -of -the -art system combination method proposed by He, et. al. ( 2008) is implemented as the b a seline . T he true -casing model proposed by Toutanova et al. (2008) is used.
 LM feature . Different LM orders up to four are tested . R esults show that u sing a 2 -gram online LM yields a half BLEU point gain over the baseline . However, the gain is saturated after a LM order of three, and fluctuates after that . gram -voting -based confidence features . Th e best result of 3 1 . 0 1 % is achieved when up to 4 -gram confidence f eatures are used . T he BLEU score keeps improving when longer n -gram confidence features are added . This indicates that the n -gram voting based confidence feature is robust to high order n -grams.
 both features in the log -linear model and reported the results in Table 3. Given the observation that the n -gram voting based confidence feature is more robust to high order n -grams, we further tested using different n -gram orders for them. As shown in Table 3, using 3 -gram online LM plus 2~ 4 -gram voting 
