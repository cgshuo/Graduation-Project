 Personalized retrieval models aim at capturing user inter-ests to provide personalized results that are tailored to the respective information needs. User interests are however widely spread, subject to change, and cannot always be cap-tured well, thus rendering the deployment of personalized models challenging. We take a different approach and study ranking models for user intent. We exploit user feedback in terms of click data to cluster ranking models for historic queries according to user behavior and intent. Each cluster is finally represented by a single ranking model that captures the contained search interests expressed by users. Once new queries are issued, these are mapped to the clustering and the retrieval process diversifies possible intents by combining relevant ranking functions. Empirical evidence shows that our approach significantly outperforms baseline approaches on a large corporate query log.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval  X  Relevance feedback, Search process, Clustering Algorithms, Experimentation, Measurement Search engine, ranking, training, clickthrough data, rele-vance judgement, clustering, search behavior
This research has been co-financed by the European Union (European Social Fund -ESF) and Greek national funds through the Operational Program  X  X ducation and Lifelong Learning X  of the National Strategic Reference Framework (NSRF) -Research Funding Program: Heracleitus II. In-vesting in knowledge society through the European Social Fund.

Modern data collections and recordings of historic user interaction pave the way for personalized information re-trieval which exploits user profiles and historic usage data to re-rank and filter retrieved documents to serve individual information needs.

Personalized retrieval aims at computing a ranking model for every user or groups of similar users. Different approaches including the impact of short-and long-term search histories [21, 22], context [14, 21], query categories [8, 24], and search behavior and feedback [1, 9, 12, 16] have been studied. Ad-ditionally, collaborative filtering techniques for personalized search [22] and learning to rank-based approaches [1, 6, 12, 17, 19, 26] also proved effective in many scenarios. Many of the above techniques are also applicable to registered users of search engines, however, to have all users benefit from the re-ranking they need to be perfectly disambiguated. This is, particularly on shared computers, an issue and renders per-sonalized web search difficult in practice.

In this paper, we study an orthogonal approach to re-ranking for web search which does not share these limita-tions, so that all users benefit equally from re-ranking the results. Our approach is based on the observation that ex-isting approaches mainly focus on the retrieved content and on users search histories, thus leaving an important aspect unaddressed: The analysis of user search behavior. The user behavior is directly observable by user feedback in form of clicks on the result page and allows to reason about the intent of the users. The intent therefore acts like an unob-served, latent variable and is (partially) captured by user behavior.
 Consider a user who issues a query for a new mobile phone. Her search history so far contains only unrelated queries. A personalized model would have to resort to the average user model for processing the query and possibly return text doc-uments about phones. By contrast, our approach does not rely on user-specific models but aims at capturing the user intent by grouping queries entailing similar behavior. The results proposed to the user thus consist of different media types (e.g. reviews, videos, etc) that have been associated with mobile phones in the past. In other words, our sys-tem re-ranks the retrieved results, so that they represent the broad spectrum of user behavior for a given query.
To build models for user intent, we propose to cluster queries with respect to the user intent and learn a rank-ing function for every cluster. Optimally, the clustering and the ranking models are optimized jointly to capture inter-dependencies between the tasks. The corresponding opti-mization problem however turns out to be a mixed-integer problem with cubic constraints in the number of queries and and renders large-scale deployment infeasible. We there-fore present an approximation that consists of three stages: Firstly, a ranking function is learned for every query to cap-ture the user behavior by adaptation to user feedback given by click data. Secondly, the ranking models are grouped so that the resulting clusters correspond to similar user intents. Thirdly, a ranking function is learned for each cluster to rep-resent the contained intent. At deployment time, queries are mapped to the clustering to compute scores expressing how likely the intent of the query is captured by the respective cluster. The final ranking is then induced by a weighted linear combination of ranking functions that are likely to cover the intent of the user, given the query. Combining the ranking functions of several clusters diversifies the results in terms of the captured intents.

Empirically, we observe our approach to capture user in-tent better than baseline methods on a large sample from the Yahoo! query log. Our method achieves higher preci-sion values on top-ranks compared to content-based base-lines. Additionally, the underlying clustering is observed to effectively group queries with similar intents together while content-based baselines do not exhibit interpretable cluster-ings.

The remainder is organized as follows. Section 2 reviews related work. We present our main contribution, the joint optimization problem and its approximation, in Section 3. Section 4 reports on the empirical evaluation and Section 5 concludes. In [10] the author proposes a topic-based refinement of the PageRank algorithm that allows the offline computation of a fixed number of PageRank vectors corresponding to certain topic categories. The final result is a weighted combination of these vectors, where weights are proportional to the sim-ilarity of the query and the respective topic. In [20] the authors utilize concept hierarchies, like ODP 1 , to categorize queries and to generate user profiles. Query results are re-ranked based on those profiles using collaborative filtering techniques. By contrast, our method does not rely on user profiles and is independent of static topic hierarchies.
Another prominent strand of research is based on exploit-ing historic user feedback. The impact of short-term versus long-term histories has been studied by [22, 23] while [5, 21] aim at capturing the context of the users, for instance by taking documents on the virtual desktop into account. The resulting models are essentially user profiles that are used to expand future queries and to refine the retrieved results. Compared to our method, these approaches focus on content similarity and do not exploit collaborative user data.
Many approaches incorporate state-of-the-art machine le-arning techniques to improve ranking results. [4] study mod-ifications of ranking support vector machines to reduce the error on top-ranks and to increase the importance of queries with only a few relevant documents in the training sample. In [17], the authors propose to learn multiple ranking func-tions for different ranks whi ch are aggregat ed to induce the final ranking. By contrast, we propose to learn different ranking functions for different behavior and intents. Fur-http://www.dmoz.org/ thermore, the above approaches do not take the inherent relations between queries and their clickthrough data into account.

The closest work to ours is [3] who propose to learn mul-tiple ranking models by clustering queries based on the top-ical information extracted by their results. They represent queries by aggregating feature vectors which are then clus-tered to obtain specific ranking models. The final ranking for new queries is being made by combining the models. Their work differs in several aspects, the two main differences be-ing as follows: Firstly, the method in [3] relies on pseudo feedback to extract the top results of each query and does not distinguish between positive and negative judgements. Secondly, the proposed approach computes the mean fea-ture representation of the results for a given query and uses these averages to group queries. By contrast, we propose to cluster the ranking functions themselves.

Finally, clustering methods are studied in combination with learning to rank strategies. [15] propose to cluster re-sults to discard probably redundant examples from a large training sample to render the resulting optimization feasi-ble, while [7] cluster personalized ranking functions to group users for recommendation purposes.
In this section we present our main contribution, rank-ing models for user intent. The following section introduces the problem setting and notation. Section 3.2 presents a joint optimization problem that directly solves the problem in theory but is infeasible in practice. In Section 3.3 we de-vise an efficient approximation that can be solved on large scales. Section 3.4 details the application of the model for new queries at execution time.
We are given n historic queries q 1 ,...,q n and their top-m retrieved documents ( x ( q ) 1 ,y ( q ) 1 ) ,..., ( x ( q 1if x ( q ) j was clicked and 0 otherwise. The click feedback induces a partial ranking on the documents such that holds. We collect the preference relations for query q in the ranking function f :( q, x )  X  R can now be adapted to the pairwise preferences P = q P q .Inthispaperwefocus on linear models of the form f ( q, x )= w,  X  ( q, x ) ,where  X  ( q, x ) denotes a joint embedding of query and document in some feature space. To avoid overloading the notation, we X  X l use  X  ( q, x )= x in the remainder and note that gener-alizations are straight forward, see for instance Table 2 for the features we used in the experiments. Following a large-margin approach leads to the optimization problem [13] where  X &gt; 0 determines the trade-off between margin max-imization and error minimization. The latter is the sum of individual losses  X  ij and constitutes an upper bound on the 0 / 1-loss of mistaken preference relations. The constraints enforce w, x i &gt; w, x j whenever possible and penalize vi-olations thereof. Once optimal parameters w  X  have been
Figure 1: Visualization of the problem setting. found, these are used as plug-in estimates to induce rank-ings of the documents for new queries.
In a nutshell, we aim at learning ranking functions for similar queries, where similar refers to the latent user intent. Figure 1 shows a simple two-dimensional visualization of the problem setting, focusing on pdf (dimension x 1 ) and video (dimension x 2 ) results. Different queries (e.g., racing cars videos , web search ) are visualized by relevant clicked (red squares) and not clicked results (green circles) documents. The task is to group the queries so that similar intents are close with respect to some distance measure in the feature space so that they are clustered together.

Since there is no ground-truth for the intrinsic cluster-ing, the respective error of the ranking functions serves as a makeshift for the missing performance measure at the clus-tering stage. That is, if the erro r-rate of a ranking function is high, the queries in the respective cluster are too diverse to allow for a good fit; the goal is therefore to find a grouping of the queries such that the ranking models are well adapted. Thus, a natural approach is to jointly optimize the clustering and the ranking models.

Let K be the number of desired clusters. We intend to find (i) K ranking models w 1 ,..., w K ,oneforeachcluster, and (ii) find a clustering c 1 ,..., c K with c kj =1ifquery q j belongs to cluster k and c kj = 0 otherwise, that gives rise to an optimal fit of the ranking models. The following optimization problem realizes this task straight forwardly, where we defined P ( k )= j : c kj =1 P q j as the union of all members of cluster k , and trade-off parameters  X  k &gt; 0.
The above optimization problem suffers from major draw-backs. Firstly, the optimization interweaves real and integer variables; that is, directly solving the mixed-integer program is expensive and one usually resorts to relaxing the binary 1: for 1  X  j  X  n do 3: end for 5: for 1  X  k  X  K do 7: end for variables to the interval [0 , 1] to obtain an approximate so-lution. Secondly and more severely, the number of triangle inequalities guaranteeing a proper clustering in Eq. (1) is cubic in the number of queries and renders the optimization infeasible at larger scales. We present an efficient approxi-mation and propose a pipelined approach in the next section.
We now present a sequential model that approximates the infeasible optimization problem and that can be solved effi-ciently on large scales. The novel approach consists of three stages and generates the desired ranking models for each cluster of queries: Firstly, we learn a ranking function for every query. Secondly, these ranking functions are clustered, and thirdly, we learn a ranking function for each cluster us-ing the original queries and documents. The algorithm in pseudo-code is depicted in Table 1.
The initial step of the approximation consists in learning a ranking model for every query. To this end we solve the stan-dard ranking SVM for every query and the respective pref-erence relations assembled from the click data. Analogously to Section 3.1, the -th optimization problem can either be solved by quadratic programming or online gradient-based approaches [12, 18, 13] and is given by In general, the trade-off parameter  X  needs to be set appro-priately to obtain optimally adapted models. In our large-scale experiments, tuning the parameters manually or de-ploying model selection techniques like cross-validation is not feasible due to the large amount of data. Anecdotal evidence however shows that for binary representations and features in the interval [0 , 1], values around  X   X  1areoften a reasonable choice. We thus use  X  = 1 for the initial rank-ing SVM models and note that there is potentially room for improvement. The result of this step is n ranking functions w ,..., w n , one for each query.
The goal of the second step of our approach is to group similar ranking models together as they capture similar in-tents. As the absolute locations of the w i are negligible and only the direction of the vectors is of interest, the ranking functions are 2 -normalized by w  X  w/ w so that they lie on the unit hyperball. The similarity of two ranking Figure 2: Query-specific models on the unit sphere. functions w and w can now be measured by their cosine which reduces to the inner product for normalized vectors, cos( w, w )= w, w . Unit vectors are usually modeled by a von Mises-Fisher distribution [2], given by p ( x Z (  X  )exp {  X   X , x } where  X  =1 and  X   X  0and d  X  2and partition function Z d (  X  )=  X  d/ 2  X  1 / (2  X  ) d/ 2 I d/ I (  X  ) denotes the modified Bessel function of the first kind and order r . Applied to the n ranking functions w 1 ,..., w a mixture model of von Mises-Fisher distributions with K components (clusters) has the density with mixing parameters  X  i with 0  X   X  i  X  1and  X  i =1. The latent variables c i  X  X  1 ,...,K } indicate the generating components for the w i ;thatis, c i = k indicates that the ranking function w i is sampled (generated) from the k -th component p (  X |  X  k , X  k ). 2 If the latent variables were known, finding maximum likelihood estimates for the parameters  X  ,...,  X  k and  X  1 ,..., X  k would be trivial. Since this is not the case, we resort to a constrained Expectation Maximiza-tion approach to jointly optimize the log-likelihood.
Given the clustering induced by the latent variables c i of the previous section, we now learn a ranking function for each cluster. The approach is similar to learning the initial ranking models for the queries, however, this time, all queries in the cluster have to be taken into account. The optimization for the k -th cluster can again be solved with the ranking SVM and is given by
Once the ranking functions are adapted to the clusters, our method can be deployed to re-rank retrieved documents for new queries. Our approach aims at diversifying possible
Note that the variables c k in Section 3.2 are analogous bi-nary encodings of the latent variables c i .Thatis,ifthe j -th query is in the k -th cluster, we have c kj =1and c j = k , respectively. We overloaded the notation to indicate that both represent the actual clustering.
 intents as the same query might end up in more than just one cluster, for instance if users clicked on different media types (e.g., videos, pdfs, etc.). Thus, the goal is to map a new query to the clustering and combine the respective ranking functions of the top matching clusters.

To this end, we represent historic queries together with their positively judged results as pseudo documents which are indexed and made searchable by a search engine. In our implementation we used the Lucene 3 IR engine, however, other choices are straight forward. Given a new query q ,the Lucene scoring function is used to obtain historic queries which are similar to q .

We select the top-u most similar historic queries and the clusters they belong to. By doing so, we compute a weighted mapping of the new query to the clustering as follows. Let v ,1  X  j  X  u , be the scores for the top-u historic queries q these are 1 -normalized and translated into cluster-scores s 1  X  k  X  K , such that s qk = c are the latent cluster memberships. That is, if a cluster occurs more than once, the respective scores are aggregated. Due to the normalization, the scores s qk act like probabil-ities, quantifying the likelihood that cluster k contains the intent expressed by query q .

Finally, the ranking of the documents for the query q is assembled from the clustering by weighting the contribution of each cluster k by its score s qk .Let r kj denote the ranking of the j -th document by the ranking function of cluster k ,the final ranking score is given by linearly weighting the cluster rankings r kj with the cluster importance s qk for query q ,
For the experimental evaluation, we sample queries from the Yahoo! query log. From the sample, we discard queries with less than 5 results, queries without clicks, and queries from users with less than 100 searches. This leaves us with 76,037 queries posed by 453 distinct users. We split the obtained data, that is query and top-10 results, chronologi-cally into 30,053 (40%) queries for training and 45,984 (60%) queries for test set.

Ground-truth is given by user clicks in terms of relevance judgments [12, 18] as follows: If a document x i has been clicked, the relevance judgment equals y i = 1. Unclicked documents that are higher ranked than clicked results re-ceive a relevance judgment of y j = 0 which is also used for unclicked results occuring right after a clicked result. This http://lucene.apache.org/ process results in a total of 96 , 030 relevance judgments for the training dataset and 144 , 021 for the test set. This gives an average of about 3 . 2 relevance judgments per query on the data. The query-result pairs are represented by feature vectors. The respective features are depicted in Table 2.
We compare our method, denoted as Intent with four al-ternative approaches for re-ranking search results: Firstly, we deploy a single ranking SVM ( Single ) for all users which is trained on all available training data and used to rank the documents for the test queries. Secondly, we train an SVM for every user ( User ) to capture state-of-the-art per-sonalization approaches. According to [22], short-and long-term search histories are well ca ptured by personalized, user-specific models and we thus expect the User baseline to perform best while the Single baseline is expected to be too simple to capture the diverse behavior in the data.
Furthermore, we apply Content-1 which clusters queries in the training set based on their content similarity and learns a ranking SVM for each cluster which are finally com-bined to re-rank documents for the test queries. Note that  X  except for the clustering  X  the processing pipeline is exactly the same as in our method; at the clustering stage, queries are grouped based on their textual similarity including text from their positive results (the clicked documents). Finally, we apply a variant of topical RankSVMs [3] ( Content-2 ). The document representation is extended by incorporating means and variances as dimensions for each feature; the new representation is computed by using the top-5 results of each query. Note however that this baseline is not identical to [3] in the sense that we use the standard ranking SVM for solv-ing the optimization problems.
The first experiment aims at measuring the performance of the algorithms in a static environment. We use the com-plete training set for the learning processes and all available test queries for evaluation. We report on MAP, Precision@ n , and NDCG@ n .

Results for MAP are shown in Table 3. Unsurprisingly, learning user specific models performs best, achieving about 14% precision increase compared to the a single model that serves everyone. The setting resembles an ideal scenario and the baselines Single and User constitute the expected lower and upper bound on the performance, respectively. Note that a real-world deployment of the personalized user model would require perfect disambiguation of users which is still an open problem.

By contrast, Content-1 , Content-2 ,and Intent are user independent and form groups of similar content or intent, respectively. In that sense, they constitute realizable ap-proaches. However, they differ significantly in terms of pre-dictive performance. Among these three, Content-2 is the weakest method although it still increases the performance over the Single baseline by 3.5%. Content-1 allows for im-provements about 5.5% and Intent even by 6.3%.

A similar picture is drawn by the precision at n scores that are displayed in Figure 3 (top). The methods are indifferent for n&gt; 1 due to the relatively small number of relevance judgments (on average 3 . 2 per query). More specifically, for the 45 , 984 test queries, there are 51 , 089 positive relevance judgements (user clicks) which translate to about 1 . 1 clicks per query on average. At P@1, however, we observe signif-icant differences in performance that confirm the previous findings. Single and User establish lower and upper bounds and Intent performs better than Content-1/2 .Figure3(bot-tom) corroborates the ob servations for NDCG@ n .
To shed light on the nature of intent-and content-based methods, we analyze and compare respective clusterings for Intent , Content-1 ,and Content-2 inTable4. Wepicked clusters with queries for which the respective methods per-form well.

The qualitative results are as follows. Firstly the ap-proaches differ significantly in the amount of clusters, where the optimal number of clusters is determined by model se-lection for each method. While the content-based methods generate between 20 ( Content-1 )and32( Content-2 )clus-ters, the solution of Intent consists of 75 distinct clusters. Though clusterings of this size are generally difficult to inter-pret, the numbers already indicate that the solution found by Intent is more specialized than the content-based ones due to the, on average, smaller clusters. In fact, it turns outthatthe Intent performs well in many specific informa-tion needs as Table 4 (left) shows. The first set of queries corresponds to a cluster that contains information needs in textual form, perhaps enriched with pictures while the sec-ond group contains specific questions which are probably best answered by appropriate text documents, too.

By contrast, Table 4 (center and right) show exemplary clusters for the two content-based methods. The former shows two clusters for Content-1 . While the top cluster is similar to corresponding one of the Intent , the bottom is more or less a random collection of queries expressing a diverse set of information needs. Finally, the right column of Table 4 shows examples for well performing clusters for Content-2 . The baseline exhibits typical content-based clus-ters formed by common tokens. The noisy membership can be explained by keywords which are central for the cluster and only occur on the result documents and not in the query.
At first sight our method seems to be outperformed by a personalized solution. However, the latter is not always applicable. Consider, for instance, scenarios such as web search where only a fraction of all users are registered and can be disambiguated only after the login. Including the personalized user model thus mirrors an ideal but unrealistic scenario. As an alternative for scenarios that do not allow personalized methods, we propose to deploy ranking models for user intent. Our method significantly increases MAP and also outperforms traditional content-based baselines for P@ n and NDCG@ n .

In our setting, the increase in P@ n and NDCG@ n perfor-mance is achieved by a significant increase in P@1, that is, Intent performs well in ranking relevant result on top. This observation is explained by the model itself: by grouping queries into clusters with similar intent, multiple ranking models are established, each one based on queries with sim-ilar user clicks in terms of the resulting types of documents. Results for new queries are re-ranked using the clustering; the final ranking score is computed by a linear mixture of relevant ranking functions. In case the textual matching is inaccurate, for instance because textual similarity does not necessarily imply similar search intentions, the final score di-versifies the most likely intents and counterbalances possible errors at earlier stages.
In this paper, we presented a methodology for improving the quality of ranking functions for web search by capturing and exploiting latent search behavior. The underlying idea grounds on the observation that search behavior is not nec-essarily content-dependent and we show that it can be used to train more effective ranking models.

Our method clusters ranking models trained on search queries and their results. The produced clusters represent implicit search behavior and are used to train ranking mod-els for user intent. The experimental evaluation demon-strates the effectiveness of our method compared to tra-ditional content-based baselines, leading to significant in-creases in MAP, P@1 and NDCG@1. An analysis of the resulting clusterings revealed that the novel method groups similar queries together while the content-based baselines suffer from noise that is incorporated by additional content from the documents. Although our approach cannot com-pete with personalized methods, we note that it is generally deployable and does not rely on user disambiguation. It thus proved a valid alternative for scenarios in which personalized models cannot be applied such as web search.

