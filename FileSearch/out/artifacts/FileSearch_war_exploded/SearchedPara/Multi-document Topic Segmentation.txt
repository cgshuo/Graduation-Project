 Multiple documents describing the same or closely related sets of events are common and often easy to obtain: for example, consider document clusters on a news aggregator site or multiple reviews of the same product or service. Even though each such document discusses a similar set of topics, they provide alternative views or complimentary informa-tion on each of these topics. We argue that revealing hidden relations by jointly segmenting the documents, or, equiva-lently, predicting links between topically related segments in different documents would help to visualize documents of in-terest and construct friendlier user interfaces. In this paper, we refer to this problem as multi-document topic segmenta-tion. We propose an unsupervised Bayesian model for the considered problem that models both shared and document-specific topics, and utilizes Dirichlet process priors to deter-mine the effective number of topics. We show that topic segmentation can be inferred efficiently using a simple split-merge sampling algorithm. The resulting method outper-forms baseline models on four datasets for multi-document topic segmentation.

Multiple documents conveying the same or closely related information are common on the Web and often easy to ob-tain. For example, a query to a search engine often returns documents describing the same facts, a document cluster on a news aggregator site covers the same events, and multi-ple online customer reviews express evaluations of the same product or service. Even though each document from such a set discusses a similar set of topics, they provide alternative views or complimentary information on each of these topics. Additionally, many documents in the set are likely to in-clude fragments specific only to this very document, such as subjective evaluations or information not available to other media sources. That is, each document is likely to consist of a set of fragments related to the entire set and document-specific fragments introducing new facts or opinions. Dis-covery of these inherent relations between content in such groups of documents could offer a great convenience to users: for instance, an individual following an event through multi-ple media could find related segments and use them to detect complimentary information or reveal inherent biases of each media source. In this paper, we argue that this hidden re-lation can be revealed by jointly segmenting documents, or, equivalently, predicting links between topically related seg-ments in different documents. We refer to such a problem as multi-document topic segmentation .

Topic segmentation of the multiple related documents is a novel and challenging problem, as previous research has mostly focused on linear segmentation of isolated texts (e.g., [21]). The most straightforward approach would be to use a pipeline strategy, where an existing segmentation algorithm finds topic boundaries of each document in isolation, and then the segments are aligned. Or, conversely, a sentence-alignment stage can be followed by a segmentation stage. However, as we will see in our experiments, these strate-gies may result in poor segmentation and alignment quality. Therefore, joint modeling of segmentation and alignment is required in multi-document topic segmentation. While un-supervised topic modeling [7, 18, 19] can be used to address this problem, little has been done to directly address joint modeling in the context of topic segmentation.

In this paper we explore generative probabilistic modeling for multi-document topic segmentation. We present a non-parametric Bayesian model for unsupervised joint segmen-tation and alignment of multiple documents. In contrast to the discussed pipeline approaches, our method leverages the inter-document lexical cohesion [20] in modeling multiple related documents. We hypothesize that topically related segments display a compact and consistent lexical distribu-tion, and this insight is encoded in our model. In compar-ison with related work [9, 33], our method has two impor-tant advantages: (1) it induces two types of topics, one type for shared topics and one for document-specific information, and (2) ensures that the effective number of segments can grow adaptively. In addition, we propose a simple split-merge sampling algorithm which is fast to converge in our experiments.

We evaluate the proposed model on texts coming from 4 different domains: news, biographies, biological reports and lectures. In our experiments, we demonstrate that joint modeling is beneficial for the considered problem and the improvement is consistent across the domains. For example, on segmentation and alignment of news and biography doc-uments our model achieves F1-scores of 73.9% and 65.9%, and this compares favorably with 53.7% and 47.0% shown by the pipeline approach.
 The remainder of this paper is structured as follows. In Section 2 we give the background of the considered problem and a formal definition of the multi-document topic segmen-tation task. Section 3 presents our unsupervised generative model which utilizes Dirichlet process priors to determine the effective number of shared and document-specific topics. In Section 4 we describe a simple split-merge Metropolis-Hastings algorithm for our model. Section 5 provides an empirical evaluation of the proposed method. In Section 6 we conclude with examination of additional related work.
In this section we will formulate the task of multi-document topic segmentation. Here, we will assume that we are pro-vided with groups of documents presenting the same or sim-ilar information. Our task is, roughly, to induce a segmen-tation of each document in a group, and to align segments between documents within each group.
As an illustration of why uncovering inherent parallel struc-ture may be beneficial, consider Figure 1, where we present three examples taken from three different domains (the cho-sen datasets are discussed in Section 5). In the figure, the documents are segmented, and, for each group of docu-ments, segments indexed with the same numeral are as-sumed aligned. For example, in Figure 1a each news ar-ticle consists of two or more segments describing the same or closely related facts ( motivation , results and implications of a study of social media addiction). The second example is drawn from a dataset of biographies. Biographies of the same persons tend to focus on common facts, and we can observe here parts describing early life , education , and en-trance in politics of Abraham Lincoln (Figure 1b). Similarly, consider the third example from the lecture domain (Figure 1c), where the pair of documents are an original text and the corresponding commentary given by a language teacher. In this example, the alignment describes the following re-lation between fragments: a segment of the original text is discussed in the corresponding fragment of the commentary.
We conclude that joint segmentation provides two types of information to the user. First, relations between shared seg-ments simplify analysis of multiple information sources: the user can easily compare descriptions of a specific fact pro-vided in different documents. This comparison would help to obtain maximal information about the fact (e.g., only one of the texts in Figure 1a mentioned that the study was held at the University of Maryland), or to reveal biases of the con-sidered media sources (e.g., conservative vs. liberal media). Second, document-specific topics are likely to correspond to the information not mentioned in any other document; hence they could be potentially more interesting to the user. Sum-marizing the above observations, we can conclude that both of these types of information are important, and therefore, the goal in multi-document topic segmentation is to discover both shared and document-specific topics.
In this section we formally define our problem. Let us assume that we are given a set of K related documents x = { x k } k =1: K . 1 A document x k is a sequence of bag-of-word vectors { x k 1 , x k 2 ,..., x k N k } , where N of document k . Note that the bag-of-word vector, x k n , could be defined at any level of granularity, e.g. word, sentence, or paragraph. While we use both sentences and paragraphs in our experiment, we refer to x k n as a sentence for conve-nience. Our task is to break a sequence of sentences into topically coherent contiguous segments. We represent the segmentation as a sequence of hidden topic variables, z . More formally, a topic sequence for document k is defined as z k = { z k 1 ,z k 2 ,...,z k N k } , where each topic variable z  X  z is described by a topic type z.t  X  { Shared,DocSpecific } and a topic label z.l  X  N . Then, we say that a segment (or sentence) is global when z.t = Shared and otherwise, local .
The task we will consider is to find segmentation z which results in a compact distribution of words assigned to each topic label z.l . The previous work on segmentation [3, 6, 10, 15, 17, 21, 24], has been mostly focused on modeling individ-ual documents, and therefore the produced segmentation did not define topical links between segments in different doc-uments. One approach would be to use a straightforward pipeline strategy, where topic boundaries are first indepen-dently found for each document and only then their align-ment is predicted. An alternative technique would be to use unsupervised topic segmentation methods [19, 30] which in-duce collection-level topics, and therefore can produce the desired parallel structure on each set of related documents. However, as we will see in our experiments, both of these strategies fail to achieve good results on the multi-document segmentation task. To tackle their limitations, we propose a new model for our problem.
We propose a hierarchical Bayesian model for joint seg-mentation of multiple documents. Similarly to previous work on segmentation of isolated documents [15, 21], our model leverages the lexical cohesion phenomenon [20] in the generative framework. Unlike this previous work, we hy-pothesize that not only a segment in each isolated docu-ment but also each group of topically related segments in different documents displays a compact and consistent lex-ical distribution, and our generative model leverages this inter-document cohesion assumption.

More formally, we rewrite the document x k as a set of segments, i.e. x k = { s k z } , in which s k z = { x k z.l,z k n .t = z.t } n =1: N k . Note that the effective number of segments is unknown. We treat each text in a segment as
In practice, we are normally given multiples sets of such related documents. However, we do not exploit relations between different sets in our model, as our model factorizes over these sets. faces. draws from a multinomial language model. As we use sparse Dirichlet priors, the model will assign higher probability to segments with more sparse (i.e. compact) distributions of word counts, whereas fragments with a very flat distribution of counts will have lower probability under the model. This property is exactly what we need to model lexical cohesion: from the Figure 1, we may conclude that there is a tendency of word repetition between each shared segment, illustrating our hypothesis of compactness of their joint distribution.
As it was previously considered in the context of contex-tual text mining [27], our model defines two distinct types of topics, global and local . We assume that each sentence of a document k can be assigned to a single topic, e.g., a global topic i or a local topic j k . To model this, we de-fine global language models {  X  i } i =1 , 2 ,... and local language models {  X  k j } j =1 , 2 ,... for each document k . Note that we do not assume that we know the effective number of both global topics and local topics. Each language model  X  i cor-responds to a shared topic, and each local language model  X  j corresponds to a single topic of the document k . Thus, each global language model creates a topical link or align-ment between sentences assigned to the corresponding topics in different texts of the document set. The local language models, in turn, define a linear segmentation of the remain-ing unaligned text. In this description, we do not encode the constraint that each topic corresponds to a contiguous seg-ment. For simplicity, we assume that this property can be enforced as a constraint on admissible states of the model. However, as we will discuss later, it can be directly encoded in the generative story.

When analyzing non-structured sources, such as news, a model needs to decide on the number of effective segments, or, in our case, on the number of effective segments of both types. In contrast with most previous systems which assume that the number of segments is given, we do not make this assumption, and use the Dirichlet process priors [16] to de-termine the effective number of segments. We incorporate them in our model in a way that is similar to how it has pre-viously been done for the latent Dirichlet allocation (LDA) model [34]. Unlike the standard LDA, the topic proportions are chosen not from a Dirichlet prior but from the marginal distribution GEM (  X  0 ) and GEM (  X  k 0 ) defined by the stick breaking construction [32], where  X  0 and  X  k 0 are the con-centration parameter of the underlying Dirichlet process for global and local topics, respectively. GEM (  X  ) defines a dis-tribution of partitions of the unit interval into a countable number of parts.

In Figure 2 we present a representation of the graphical model. The underlying generative process is as follows: Figure 2: A graphical model representation of our model. Shaded circles denote hyper-parameters.

Our model is similar in spirit to standard topic models [7, 18]. But rather than generating a collection of documents using a mixture of topic-specific language models, we con-strain the induced topic labels to correspond to contiguous fragments of the document.

In fact, we can obtain an essentially equivalent model by modifying the generative story. In this generative story in-stead of choosing a topic for a sentence and then immediately generating words in the sentence, we would first generate a bag of topics for each topic type (local or global), and then generate a random contiguous segmentation using only topic labels form the previously generated bag. This segmentation would be chosen uniformly over all the legal segmentations. A similar technique has been previously considered in [9]. We chose our way of presenting this model, as it results in a much simpler generative story.
In this section we describe the inference algorithm we use for our model. Our goal is to estimate the model and obtain a likely segmentation for each document according to this model. As exact inference is intractable, we have to resort to approximate inference methods. We chose to use Markov Chain Monte Carlo (MCMC) methods, as they are easy to implement, do not require additional assumptions about the properties of the distributions, and have been successfully used in previous text segmentation methods [15]. MCMC methods are a class of algorithms which construct a Markov chain with the stationary distribution coinciding with the joint distribution of the considered model. A state of such a chain corresponds to an assignment to all the latent vari-ables of the model with the observed variables held fixed. Therefore, a sample from the joint distribution can be ob-tained by running this Markov chain for long enough. We use a collapsed MCMC sampler [18] which analytically in-tegrates over all the distributional parameters and samples only assignment of sentences to topics (and topic types). Before starting with our sampling algorithm, we will clarify the computation of the joint distribution under our model, as it is needed to construct our sampler.
The joint distribution P ( z , x ) can be decomposed into the product of the likelihood P ( x | z ) and the prior P ( z ). The likelihood P ( x | z ) factorizes over segments and can be written as P ( x | z ) =
Y  X 
Y where the effective number of topics, I and J k , are selected using the Dirichlet process priors. Now, we consider, for both shared and document-specific segments, the probability of each segmentation, P seg ( y |  X  ), in which y indicates a set of sentences assigned to one type of topics, and  X  is a Dirichlet prior (either  X  0 or  X  k 0 ). Given the text y , let c ( w, y ) be the number of appearances of word w in y , where w ranges from 1 to the vocabulary size W . For  X  = (  X  1 ,..., X  W probability of a segment is equal to: where  X  is a hidden language model, l = P W w c ( w, y ) is the document length, m = P W w  X  w is the sum of the Dirich-let priors parameters ( W X  if the prior is symmetric), and  X  is the gamma function. This distribution (2) is known as the Dirichlet compound multinomial distribution or the multivariate Polya distribution, the result of integrating out multinomial parameters  X  with a Dirichlet prior  X  [4]. Note that, we can evaluate this probability as a function of non-zero counts c ( w, y ) only, since  X ( c ( w, y ) +  X  w ) /  X (  X  c ( w, y ) = 0.

Similarly, to the standard Dirichlet process mixture model, the latent segmentation z can be sampled using the general-ized Polya urn scheme (for more details, we refer the readers to [5] and [16]), which yields the following prior distribution: P ( z ) = where c z ,i is the number of sentences assigned to topic i in z , and the total number of global and local topics is defined the following section, our MCMC sampler will only modify adjacent segments at each sampling move. If we denote by z ij the subset of z which consists of two segments i and j , the probability of this change in segmentation simplifies con-siderably as P ( z ij )  X  ( c z ,i  X  1)!( c z ,j  X  1)!. We will use this fact to sample a new state of the Markov chain efficiently. We use MCMC sampling techniques to infer our model. Although the Gibbs sampling method is straightforward and easy to implement, it can be slow to mix in our case. In its basic form it assigns only one hidden variable at a time, making it difficult to introduce large changes in the topic segmentation, as local minima in the joint probability are often separated by regions of low probability. We instead use a split-merge algorithm [11, 22], which relies on Metropolis-Hastings (MH) proposals to merge two segments into one or split an existing segment into two.

At each iteration of the MH algorithm, a new poten-tial segmentation z 0 is drawn from a proposal distribution  X  ( z 0 | z ), where z is the current segmentation. The proposal state z 0 is accepted with the probability In order to implement the MH algorithm for our model, we need to define the set of potential moves (i.e. admissible changes from z to z 0 ), and the proposal distribution over these moves. For the standard Dirichlet process mixture model, two standard proposals, split and merge of the clus-ters, are sufficient [22]. In our case, however, a more complex set of moves is required.

The set of moves we consider in this work is divided into two groups: split-merge and increase-decrease . The former one is the set of proposal moves that generates a new topic or removes one of the topics, and the latter one reassigns the topic variables in a single considered document. More specifically, we define four split-merge moves (two for each topic type): (a) GlobalSplit : A move that splits an existing global topic (b) GlobalMerge : A move that removes an existing global (c) LocalSplit : A move that splits an existing segment in a (d) LocalMerge : A move that removes an existing local topic,
Although the split-merge moves allow for relatively com-plex joint segmentation, they constrain unnecessarily the segmentations: namely, all the documents are assumed to have the same sequence of global topics. When there are only two relevant documents in the considered document set (i.e. K = 2), the split-merge moves are sufficient, but in more general cases K &gt; 2 (e.g. Figure 1a and 1b), we need a way to assign a different sequence of global topics to each document. In addition, to decrease the mixing time, a move which shifts a segment border is introduced: (e) Increase : A move that introduces a global topic into (f) Decrease : A move that removes a global topic from a Algorithm : MultiSeg 1: Initialize z with one global topic, and iter := 1 2: repeat 5: z  X  6: end if 7: for all document k  X  X  1 , . . . , K } do 8: Randomly select two segments z i and z j 10: r  X  Unif orm (0 , 1) 14: end if 16: end for 17: iter  X  iter + 1 (g) Shift : A move that shifts the segment boundary. Given Note that this set of moves changes only the number of topics discussed in a single document, whereas moves GlobalSplit and GlobalMerge affect the entire set of documents.
An important property of the proposal moves defined here is that they only depend on a pair of adjacent segments in each document. Therefore, the acceptance ratio can be computed efficiently. Formally, the ratio of the posterior distributions P ( z 0 | x ) /P ( z | x ) can be rewritten as the ratio of the likelihoods and the priors, i.e. [ P ( x | z [ P ( z 0 ) /P ( z )], and we can calculate each ratio over the par-tition z ij or the set of partitions { z ij } .

Similarly, we use the following ratio of proposal distribu-tions:
In our experiments, we start with one global topic, and re-peatedly sample the potential state z 0 and evaluate it using Eq. (1)-(5). If the proposal is accepted, z 0 is selected as the next state of the Markov chain, and otherwise the original segmentation z is preserved. The algorithm is presented in Figures 3-4.
In this section we present quantitative and qualitative ex-periments. For quantitative experiments we show that our model outperforms baselines on four different datasets. For qualitative analysis we inspect types of errors made by our model and the considered baselines, and discuss properties of topics discovered by our method.
We evaluate our method on four datasets: News, Biogra-phy, Report and Lecture (Table 1). All the datasets were collected and annotated by human annotators, and auto-matically tokenized. The number of annotated segments vary across the datasets and across groups of related doc-uments within each dataset. For the Report and Lecture dataset, a document is a sequence of automatically-split sen-tences, but for the News and Biography domains we treat each document as a sequence of paragraphs separated by the HTML tag &lt;p&gt; . The paragraph break information was not available for the Report and Lecture datasets and, therefore, we could not apply our approach at this granularity level. For the News domain we remove only punctuation symbols, whereas for the Biography, Report and Lecture domains we also use a standard list of stop-words and a stemmer.
In the News and Biography domains, each group of doc-uments refers to the same event or person (see examples in Figure 1a and 1b). To create the News dataset, we collected document clusters of 50 news events over the period of April 28  X  May 6, 2010 from science and technology section in news.google.com . The size of each set of related documents in the News dataset varies between 2 and 6. For the Biog-raphy dataset, we collected biographies of 30 persons from four web sites; en.wikipedia.org , simple.wikipedia.org , biography.com , and notablebiographies.com . Documents in these datasets contain both shared and document-specific segments, and the proportion of paragraphs assigned to global topics vary across documents in each dataset. Our purpose is collecting this set of texts to uncover hidden shared top-ics as well as document-specific topics by jointly segmenting multiple news stories and biography articles.

The Report dataset consists of reports describing a plant growth lab, an assignment for a biology class [33]. There are only two sets of related documents: for the first set of documents (100 examples) there are two annotated global segments ( an introduction of plant hormones and descrip-Table 1: Datasets: the number of sets of re-lated documents (#Set), the total number of doc-uments (#Doc), the average number of sequences per document (paragraphs for News and Biogra-phy, sentences for Report and Lecture) per domain (AvgSeq), the average number of annotated seg-ments per document (AvgSeg) and the average size of vocabulary per set (AvgVoc).
 Domain #Set #Doc AvgSeq AvgSeg AvgVoc News 50 184 11.1 3.0 563.8 Biography 30 120 34.5 8.1 2,222.6 Report 2 160 13.8 2.4 1,440.0
Lecture 200 400 51.4 18.2 209.0 tion of the experiment ), for the second set (60 documents) there are four global segments. Note that this dataset is annotated only with global segments.

For the Lecture domain, we use a dataset of  X  X nglish as a second language X  (ESL) podcast [28] containing 200 episodes (podcast no. 174  X  373 posted over June 19, 2006  X  May 16, 2008) from www.eslpod.com . Each episode consists of two parts: a story (an example monologue or dialogue) and an explanatory lecture discussing meaning and usage of English expressions appearing in the story (see example in Figure 1c). The objective here is to divide the lecture transcript into discourse units (i.e. focused paragraphs of the lecture) and to align each unit to the related segment of the story. Predicting the segmentation and alignment for the ESL pod-cast could be the first step in development of an e-learning system and a podcast search engine for ESL learners.
To measure the quality of segmentation predicted by our model, we compute the precision and recall scores of a pre-dicted segmentation against a reference segmentation. Pre-cision is the fraction of correctly identified and aligned seg-ments among all the predicted segments; recall is the frac-tion of correct segments that are identified by the algorithm. We also present the F1-score in our results, which is the har-monic mean of recall and precision. Although precision and recall are standard measures for many information retrieval tasks, some researchers [29] argue that these measures are not always appropriate, as they are not sensitive to near misses, i.e. small shifts of segmentation boundaries. Conse-quently, we also use two standard metrics, Pk [3] and Win-dowDiff [29], but both of these metrics disregard topic labels and alignment, and score only segmentation. In addition, we present the ratio of the predicted number of segments to the reference number of segments (NR). This ratio will be close to 1 if the method is able to detect the correct number of segments.

Our MultiSeg algorithm presented in Figures 3 and 4 has two important properties: (1) it can model both document-specific and shared topics and (2) each document can include a different subset of shared topics, as result of application of the increase-decrease moves. To quantify the benefits of these two properties of our method, we compare our model not only with other baselines but also with three more re-stricted versions of the model itself. The resulting 4 versions of our model are: The baseline models are: Table 2: Result on News dataset. Columns are pre-cision (Prec), recall (Rec), and F1-score (F1); Pk and WindowDiff (WD); the ratio of the predicted number of segments to the reference number of seg-ments (NR).
 Coarse .309 1.00 .472 .335 .335 0.41 Fine 1.00 .129 .228 .502 .755 4.15 K-means .402 .718 .516 .436 .502 1.76 Pipeline .393 .849 .537 .318 .326 0.70 HTMM .378 .735 .499 .415 .467 1.47 MultiSeg-1 .554 .860 .674 .287 .307 0.90 MultiSeg-2 .624 .831 .713 .262 .288 0.89 MultiSeg-2 X  .572 .789 .663 .275 .309 1.09 MultiSeg-3 .712 .769 .739 .236 .275 1.14 Coarse .086 1.00 .159 .233 .233 0.19 Fine 1.00 .136 .239 .351 .501 3.99 K-means .254 .500 .337 .312 .440 2.89 Pipeline .338 .768 .470 .186 .208 0.81 HTMM .208 .443 .283 .312 .431 2.51 MultiSeg-1 .335 .879 .485 .202 .213 0.77 MultiSeg-2 .548 .853 .667 .174 .191 0.89 MultiSeg-2 X  .395 .684 .501 .175 .193 0.85
MultiSeg-3 .577 .769 .659 .168 .192 0.97
In order to infer segmentation with our model, we run the inference algorithm from five randomly chosen initialization states, and take the 100,000th iteration of each chain as a sample. Results are then averaged over these five samples. Similarly, results of the three baselines are also initialized with five random states and averaged. While the global topic Dirichlet hyperprior  X  0 is set to 0 . 2, all document-specific topic Dirichlet priors  X  k 0 are set to 0 . 1, encouraging sparser distributions. All the concentration parameters,  X  and  X  k 0 , are set to 0 . 1.

Our method is implemented in Java and the experiments were executed on a dual-core 3.06 GHz machine. The run-ning time varies across domains from 2 seconds per docu-ment on the Lecture domain to 19 seconds per document on the Biography domain.
First, we show that our proposed model is appropriate for multi-document topic segmentation (Table 2 -5), and then we provide some more detailed analysis of its behavior.
In the tables, the first two lines present results of the mini-mal baselines: one which places all sentences in the same seg-ment ( Coarse ) and one which assigns each sentence to its own segment ( Fine ). We use these baselines only to give a bet-ter sense for the performance metrics. For all the datasets, the segmentation errors of the Pipeline approach, measured http://code.google.com/p/openhtmm/ Coarse .375 1.00 .546 .227 .227 0.45 Fine 1.00 .003 .005 .755 .958 9.63 K-means .609 .669 .638 .509 .594 3.35 Pipeline .401 .772 .526 .254 .255 0.76 HTMM .578 .677 .623 .450 .515 2.58 MultiSeg-1 .693 .916 .789 .089 .089 0.90 MultiSeg-2 .796 .858 .826 .075 .077 1.05 Coarse .061 1.00 .115 .199 .199 0.06 Fine 1.00 .227 .370 .313 .463 2.93 K-means .454 .641 .531 .260 .357 2.17 Pipeline .422 .763 .543 .173 .190 0.91 HTMM .414 .599 .489 .228 .270 1.14 MultiSeg-1 .708 .828 .764 .138 .160 1.04
MultiSeg-2 X  .772 .777 .775 .141 .172 1.22 with Pk and WD metrics, are generally lower than those of K-means and HTMM (lower numbers for Pk and WD met-rics correspond to the better segmentation quality). For the K-means baseline every sentence is considered in isolation, whereas HTMM captures only Markovian dependencies in topic transitions and disregards contiguity constraints. Both of these properties result in over-generation of segments (not topics), even though the true number of clusters and the true number of topics were given to these baselines by an oracle. This result suggests that encoding the discourse-level con-straint, that topics are normally non-recurring in text [20], is crucial for the success of the method. For all the domains except for the Report domain, Pipeline performs better than two other baselines. The results of the HTMM model come close to those of K-means .

Our model MultiSeg-1 substantially outperforms the base-lines on all the datasets. The difference is statistically sig-nificant at p &lt; . 001 level measured with the permutation test [14]. The significant improvement over the Pipeline re-sults demonstrates benefits of joint modeling for the con-sidered problem. Moreover, additional improvement is ob-tained by using different sequences of global topics for dif-ferent documents ( MultiSeg-2 ), and from using document-specific topics ( MultiSeg-2 X  ). Note that for two domains more restricted models are sufficient: no document-specific topics are needed for the Report dataset and no increase-decrease move are required for the Lecture dataset. Table 2 -3 show that performance is improving as the model is becom-ing more and more flexible. For the News domain, our full model ( MultiSeg-3 ) performs better than other constrained versions. Although the F1-score of MultiSeg-2 is slightly higher than that of the full model on Biography domain, we can observe that this does not translate into improvement in recall, as the segmentation produced by MultiSeg-2 is too fine-grained (see the NR column).

Next, we compare our method to previous work [33], which also considered the Report dataset. However, their dataset, experimental set-up and evaluation metrics are somewhat Reference 1234... 00000122222222222344444444444...
 MultiSeg-2 X  1234... 00000122222222 333 344444444 000 ... MultiSeg-1 1234... 11111 122222222 333 344444444444... HTMM 24 34... 155112 4115566 222 7 5 4898778 4411 ... Pipeline 000 4... 222222 22222222222 2 44444444 222 ... Figure 5: Comparison of segmentation on  X  X lterna-tive Medicine X  episode in the Lecture domain different, and, therefore, we had to replicate their set-up and ran an additional experiment. They used a subset of the Report dataset that consists of 102 documents and two topics. Our model, MultiSeg-2 , achieves the error rate of 3.4% whereas they reported the error rate of 2.8%. This difference is certainly not significant, and also it is impor-tant to note that they assume that the number of segments is given, whereas we detect the number of segments auto-matically. One reason for the very high accuracy of both models on this dataset is that it is easy to induce language models for 2 segment types given a large set of related doc-uments. These 2 language models present compact but, to significant degree, non-overlapping distribution over the vo-cabulary. For example, an introduction part in this dataset often discusses background knowledge on plant hormones using a specific set of biological terms (e.g., cell , auxin and cytokinin ), and the experimental parts is characterized by terms mostly related to plant growth or stages of the exper-iment (e.g., treatment , experiment , and data ).

In order to better understand types of errors made by the different methods, we now turn to qualitative analysis. In Figure 5 we show predictions of our model against two base-lines on an example from the Lecture domain. Each digit indicates a distinct topic identifier assigned to the sentence, the digit 0 denotes any document-specific segment. 3 topic assignments predicted by HTMM are non-contiguous and therefore the number of segments is exceedingly large. Conversely, the Pipeline method results in too coarse-grained segmentation. MultiSeg-1 is only able to model shared top-ics, and the first sequence of errors it made (sentence 1-5) is due to this deficiency. Extending the sampler for our model with two new moves ( MultiSeg-2 X  ) remedies this prob-lem, but errors are still made due to over-generation of the document-specific topics. In fact, often there is more than a single granularity level for a  X  X ood X  segmentation, and prior knowledge encoded in the form of prior distributions (or a limited amount of labeled data) may be required to force the model to use the required granularity level.

Top words for 3 topics discovered by our model in News and Biography domains are presented in Table 6. To im-prove readability, we manually de-stemmed words. The top-ics are also manually labeled to reflect our interpretation of their meaning. The top words suggest that the discovered segments indeed correspond to semantically coherent topics. Importantly, many of these top words (and the correspond-ing semantic topics) are irrelevant to any other document in the collection (for example,  X  X otivation of the study X  or  X  X ivil war X ) and therefore they are unlikely to be induced if topic modelling is done at the collection level [7].
We rearranged the randomly ordered topic numbers to im-prove readability. Table 6: Top words extracted from our model. All words are listed in order of descending values.
Topic segmentation has been an active area of research in the last decade. However, previous research has mostly focused on the linear segmentation of isolated texts [3, 6, 10, 15, 17, 21, 24]. Our work can be regarded as an exten-sion of the Bayesian segmentation model [15] for the multi-document topic segmentation problem. A preliminary ver-sion of this work was presented in [23], where a more re-stricted inference algorithm was considered, and the tech-nique was evaluated only on the simpler ESL dataset.
A similar problem has been studied in the context of topic detection and tracking (TDT) [1]. There, after segmenting a news stream into stories discussing a single topic (seg-mentation stage), similar stories are grouped into a clus-ter (detection stage). Joint segmentation and detection are closely related to the multi-document topic segmentation problem considered in this paper. However, most previous studies have focused on solving these tasks independently. Moreover, rather than modeling multiple related documents, TDT often assumes that only a single long stream is avail-able.

Although interest in exploiting multiple related documents has been growing recently, the task of multi-document topic segmentation has not received much attention. We are aware of only two previous methods [33, 9] focusing on joint seg-mentation and alignment of multiple texts. In [33] it is shown that leveraging multiple documents improves the seg-mentation performance. Even though the problem defini-tion is similar, there are differences in the set-up, and our approaches are also quite different. First, we assume that there exist both document-specific and shared topics, and not all the shared topics are necessary mentioned in each document, whereas they assume that the same set of topics is discussed in each document. We believe that our problem formulation is not only more general but also more realis-tic in multi-document topic segmentation. Second, we do not assume that the number of segments is provided to the model, whereas they used the actual number as an input. Though it is a common practice in segmentation commu-nity, it is not a realistic assumption in many cases (e.g., consider analysis of newswire). Our model uses Dirichlet process priors to determine the effective number of topics. Finally, while they use similarity functions and an iterative greedy algorithm [33], we use the generative framework to model lexical cohesion.

The closest model to ours is that of [9] which is focused on learning preferred ordering of topics in a given collection. As they were considering modeling of wikipedia text collections, their main goal was to detect a compatible set of topics for a large collection of documents. In our case we focused on detecting shared and document-specific topics for a set of re-lated documents in less structured data-sources (e.g., news). Here, a significant part of each document is not related to any shared topic (and to any other document), and, as our results suggest, modeling both types of topics is beneficial. Also, conceptually these set-ups are quite different, as we can assume that each segment is weakly equivalent to an aligned segment in other documents, and instead of generating all aligned segments from a single language model, a translation model can be used to generate them jointly. This approach, which we regard as a potential future direction, is not ap-propriate for modeling general collections of documents but only appropriate for modeling groups of related documents. Again, the authors also assume that the number of topics is given, which may be a reasonable assumption for model-ing collection level topics, but clearly results in sub-optimal performance with small sets of related documents.

Our work is also related to research on multi-document summarization [8, 13, 31, 35]. In multi-document summa-rization, the goal is to generate a summary consisting of sentences extracted from a set of documents. Our work is different in that we do not try to extract summary sentences but rather aim to find coherent fragments with maximally overlapping lexical distributions. Similarly, passage retrieval (e.g., [25]) and abstract/document alignment [12, 26] are also related but they focus on selection of most relevant passages and sentences given a query (or an abstract) rather than on jointly segmenting multiple related documents.
We studied the problem of multi-document topic segmen-tation, where the goal is to jointly segment multiple doc-uments detecting both aligned and non-aligned segments. Our model achieves favorable results on four datasets, demon-strating that the use of the Dirichlet process priors and structured topic models can lead to improved segmenta-tion quality. Accurate prediction of these hidden relations between documents would open interesting possibilities for constructing friendlier user interfaces. One example being an application which, given a document cluster of a news event, produces a graph-based visualization of the shared topic segments.

In future research, we plan to investigate models which are specifically suited to jointly modeling small sets of doc-uments, as word overlap between related segments in such documents may be too small and not sufficient to detect their relevance. This approach may borrow techniques from statistical machine translation and document summariza-tion. Another interesting direction would be integration of user feedback to decide on the necessary granularity level. The authors acknowledge the support of the Excellence Cluster on Multimodal Computing and Interaction (MMCI), and also thank Andrew Gargett and the anonymous review-ers for their valuable comments, and Bingjun Sun for pro-viding the Report dataset. [1] J. Allan, editor. Topic detection and tracking: [2] D. Arthur and S. Vassilvitskii. k-means++: the [3] D. Beeferman, A. Berger, and J. Lafferty. Statistical [4] J. Bernardo and A. Smith. Bayesian Theory . John [5] D. Blackwell and J. B. MacQueen. Ferguson [6] D. M. Blei and P. J. Moreno. Topic segmentation with [7] D. M. Blei, A. Ng, and M. I. Jordan. Latent Dirichlet [8] G. Carenini, R. Ng, , and A. Pauls. Multi-document [9] H. Chen, S. Branavan, R. Barzilay, and D. R. Karger. [10] F. Y. Y. Choi, P. Wiemer-Hastings, and J. Moore. [11] D. B. Dahl. Sequentially-allocated merge-split sampler [12] H. Daum  X e and D. Marcu. A phrase-based hmm [13] H. Daum  X e and D. Marcu. Bayesian query-focused [14] P. Diaconis and B. Efron. Computer-intensive methods [15] J. Eisenstein and R. Barzilay. Bayesian unsupervised [16] T. S. Ferguson. A Bayesian analysis of some [17] M. Galley, K. R. McKeown, E. Fosler-Lussier, and [18] T. L. Griffiths and M. Steyvers. Finding scientific [19] A. Gruber, Y. Weiss, and M. Rosen-Zvi. Hidden topic [20] M. A. K. Halliday and R. Hasan. Cohesion in English . [21] M. Hearst. Multi-paragraph segmentation of [22] S. Jain and R. Neal. A split-merge Markov chain [23] M. Jeong and I. Titov. Unsupervised discourse [24] X. Ji and H. Zha. Domain-independent text [25] X. Liu and W. B. Croft. Passage retrieval based on [26] D. Marcu. The automatic construction of large-scale [27] Q. Mei and C. Zhai. A mixture model for contextual [28] H. Noh, M. Jeong, S. Lee, J. Lee, and G. G. Lee. [29] L. Pevzner and M. Hearst. A critique and [30] M. Purver, K. Kording, T. Griffiths, and [31] D. R. Radev, H. Jing, M. Stys, and D. Tam.
 [32] J. Sethuraman. A constructive definition of Dirichlet [33] B. Sun, P. Mitra, C. L. Giles, J. Yen, and H. Zha. [34] K. Yu, S. Yu, and V. Tresp. Dirichlet enhanced latent [35] L. Zhou, M. Ticrea, and E. Hovy. Multi-document
