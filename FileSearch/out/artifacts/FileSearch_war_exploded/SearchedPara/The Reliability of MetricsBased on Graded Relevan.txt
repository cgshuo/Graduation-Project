 After a decade of TREC evaluations based on binary relevance, the importance of information retrieval (IR) evaluation based on graded relevance is receiving attention [4,5,6,7,8]. Graded relevance metrics based on (discounted) cumulative gain ((d)cg) [4,5] are particularly popular.
 them is as widely-used as traditional binary relevance metrics such as TREC Average Precision (AveP) at present. For example, even though the NTCIR CLIR track series [2] uses graded relevance (S, A and B in decreasing order of relevance),  X  X elaxed X  AveP (which treats S-, A-and B-relevant documents as just  X  X elevant X ) and  X  X igid X  AveP (wh ich ignores B-relevant ones) are used for ranking systems, thereby wasting the rich relevance data. The objective of this paper is to improve such situations by clarifying which of the (d)cg-based metrics are reliable and useful. To this end, we use two test collections with sub-mitted runs from the Chinese IR and English IR tasks in the NTCIR-3 CLIR track to examine a variety of (d)cg-based metrics using methods proposed by Buckley/Voorhees [1] and Voorhees/Buckley [12] as well as Kendall X  X  rank cor-relation [3,5,7,8]. We test 14 graded relevance metrics plus 10 binary relevance ones from the viewpoint of reliability and resemblance of system rankings. Our experiments suggest that graded relevance may give more stability and discrimi-ation power than binary relevance, and show that some graded relevance metrics such as Q-measure [7,8] are more reliable and useful than others.
 we used for comparing the reliability of metrics. Section 4 presents the results of our analyses, and Section 5 provides discussions. Section 6 compares previous work with the present study, and Section 7 concludes this paper. We first define TREC Average Precision (AveP), R-Precision (R-Prec), Precision at document cut-off l (PDoc l ), Q-measure and R-measure: Here, R : number of relevant documents; count ( r ): number of relevant documents in top r of the ranked output; L : size of the ranked output; isrel ( r ): 1 if the document at Rank r is relevant and 0 otherwise; cg ( r ): cumulative gain [4] at Rank r of the system X  X  output, obtained by cig ( r ): cumulative gain at Rank r of the ideal ranked output of Q-measure inherits the properties of both precision and weighted precision cg ( r ) /cig ( r ). Using weighted precision instead of the blended ratio in Equa-tion (4) is not good because cig ( r )= cig ( R )holdsfor r&gt;R :Thatis, cig ( r ) freezes after Rank R , and therefore weighted precision cannot penalise late arrival of relevant documents. Q-measure solves this problem by using cig ( r )+ r as the denominator. The following are some properties of Q-measure and R-measure:  X  If the absolute gain values are small, Q-measure (R-measure) behaves like  X  Q-measure is equal to one iff a system output (s.t. L  X  R ) is an ideal one;  X  R-measure is equal to one iff all of the top R documents are (at least par- X  With binary relevance, R-measure reduces to R-Prec.
 graded relevance metrics that are somewhat akin to PDoc l .Their Normalised Cumulative Gain and Average Normalised Cumulative Gain at l are given by: If the above cumulative (ideal) gains are replaced with discounted cumulative (ideal) gains, which are based on gains divided by the logarithm of each rank, we have (Average) Normalised Discounted Cumulative Gain (nDCG l and AnDCG l ).
 larger than R , because of the aforementioned freezing problem: Let gain ( S )= 3 ,gain ( A )=2 ,gain ( B ) = 1 (which are the default values used in this pa-per), and consider a topic such that R = R ( B )=5,where R ( X ) denotes the number of X -relevant documents and R = X R ( X ) in general. The se-Suppose that System A retrieves only one relevant document at Rank 5, while System B retrieves one at Rank 1000. Then, with System A, cg ( r )=1for r  X  5, while, with System B, cg ( r )=1for r  X  1000. Thus, for both systems, nCG 1000 = cg (1000) /cig (1000) = 1 / 5=0 . 2. There are at least three ways to avoid this problem: (a) Use discounting as nDCG l and AnDCG l do; (b) Aver-age across document ranks as AnCG l and AnDCG l do; or (c) Use Q-measure instead [7,8].
 ter models user behaviour. However, the choice of l is arbitrary, and this may seriously affect system ranking, as is the case with PDoc l [1]. Later, we shall show that the choice of l is in fact crucial for these (d)cg-based metrics. (nDCG l and AnDCG l depend on another parameter: namely, the logarithm base for dis-counting [4,5]. We use base 2 throughout our experiments.) CG l = cg ( l ) are out of the scope of this paper as these metrics are not even bounded by one: They do not average well. This section describes our adaptation of two existing methods for assessing the reliability of test collections and effectiveness of metrics. submitted to a particular task, and let x and y denote a pair of runs from S .Let Q denote the entire set of topics used in the task, and let c be a constant. Let M ( x, Q i )denotethevalueofmetric M for System x averaged over a topic set Q (  X  Q ). Then, after counting GT M ( x, y ), GT M ( y, x )and EQ M ( x, y )asshown in Figure 1, the minority rate and the proportion of ties of M ,givena fuzziness value f , can be computed as: clusion about a pair of runs using a given metric, while the proportion of ties reflects its discrimination power. From the algorithm, it is clear that GT M ( x, y )+ GT M ( y, x )+ EQ M ( x, y ) = 1000 holds for each run pair, and that a larger fuzziness value yields larger EQ M ( x, y ) values, and therefore a larger propor-tion of ties and a smaller minority rate. As a fixed fuzziness value may im-ply different trade-offs for different metrics, we vary the fuzziness value ( f = paring the stability of different metrics [9].
 formance difference between two runs. We first prepare 21 performance dif-ference bins , where the first bin represents performance differences such that 0  X  d&lt; 0 . 01, the second bin represents those such that 0 . 01  X  d&lt; 0 . 02, and so on, and the last bin represents those such that 0 . 20  X  d .Let BIN ( d ) denote a mapping from a difference d to one of the 21 bins where it belongs. The algorithm shown in Figure 2 calculates a swap rate foreachbin.(Ourtest is stricter than the original one by Voorhees/Buckley, in that our  X  X waps X  in-clude cases in which only one of d M ( Q i )and d M ( Q i ) is zero. This is because Voorhees/Buckley X  X  original test, whic h increments the swap counter only when one of the differences is positive and the other is negative, tends to underrate the swap rates for near-zero bins as the differences are actually quite often zero. We have verified that this modification gives graphs that look more stable, but do not affect our conclusions.) the original topic set Q . (Voorhees/Buckley have used extrapolation for larger topic set sizes, but we stick to the statistics actually measured in our study as our objective is to compare the sensitivity of different metrics under the same condition.) By plotting swap rates against the performance difference bins, one can discuss the performance difference required in order to conclude that a run is better than another with a given confidence level, e.g. 95%.
 swap rates due to the use of disjoint subsets Q i and Q i . However, the objective of the method is to guarantee a given confidence level by considering the worst case in which the properties of the two topic sets are completely different, and not to obtain accurate estimates of the true swap rates which would be obtained by sampling topics directly from the population of real topics P ,where P&gt;&gt; Q . In fact, we have verified that using topic sampling with replacement (with possible overlaps between Q i and Q i ) instead of the original Voorhees/Buckley method does not affect our general conclusi ons regarding the relative sensitivity of different metrics. (This alternative topic selection method has been studied earlier by Ian Soboroff at NIST [11].) Thus, this paper presents results using the original Voorhees/Buckley method, and we will discuss the effect of using alternative topic sampling methods elsewhere [10]. 4.1 Data For our experiments, we used the Chinese IR and English IR data from NTCIR-3 CLIR [2], containing 45  X  X -runs X  and 24  X  X -runs X  [7,8]. As there are 42/32 topics for the C/E-runs, respectively, we let c = 20 for the C-runs and c =15 for the E-runs throughout our experiments in order to obtain disjoint topic sets for the swap rate calculation [12]. We use the top 30 C-runs and the top 20 E-runs for calculating minority rates and swap rates, and the full sets of runs for calculating rank correlations. Since our C-run experiments use more runs and more topics than our E-run ones, the results of the former are probably more reliable. 4.2 Minority Rates Figures 3 and 4 show the minority-rate / proportion-of-ties curves of 10 binary relevance metrics and 14 graded relevance metrics, based on the C-runs. Ta-ble 1 provides a summary by slicing the graphs (and ones for the E-runs, not shown here due to lack of space) at 5% minority rate and roughly grouping the metrics according to the proportion of ties. The C-run results suggest that the use of graded relevance may stabilise evaluation: Figures 3 and 4 together (and Table 1(a)) show that, at 5% minority rate, metrics such as (A)nDCG 1000 and Q-measure have smaller number of ties than Relaxed and Rigid AveP. (R-Prec and R-measure appear to be relatively unstable for the E-runs, probably because R is generally small for the NTCIR-3 English test collection [2].) 4.3 Swap Rates Based on graphs that plot swap rates against the 21 performance difference bins, Tables 2 and 3 show the discrimination power of metrics at 95% confidence level (i.e. 5% swap rate) for the C-and E-runs. For example, Table 2(a) shows that when 20 topics are used for ranking the C-runs with Relaxed AveP, an absolute difference of at least 0.11 (or 20% in terms of relative difference) is required in order to conclude that a run is better than another with 95% confidence. Of the 435,000 comparisons (30*29/2=435 system pairs, each with 1000 trials), 23.7% actually had this difference. The metrics have been sorted by this measure of discrimination power (Column (iv)). We can observe that:  X  In terms of discrimination power, the best graded relevance metrics appear  X  As was mentioned in Section 2, nCG 1000 is not useful: it cannot even guar- X  The use of graded relevance may enhance discrimination power. For the C-4.4 Rank Correlations Sections 4.2 and 4.3 discussed the stability and discrimination power of each metric. We now examine how the system rankings produced by different metrics resemble each other.
 rics in terms of Kendall X  X  rank correlation [3,5,7,8], based on the C-and E-run rankings. Correlations higher than 0.9 are shown in bold. We can observe that:  X  Q-measure is more highly correlated with Relaxed/Rigid AveP than any  X  R-measure is more highly correlated with Relaxed R-Prec than any other  X  AnDCG 1000 ,nDCG 100 and nDCG 1000 are also highly correlated with Re-rank correlation, based on the C-and E-run rankings. For example, Row (A) Column (B) shows the rank correlation between Q-measure and R-measure. We can observe that:  X  Q-measure, R-measure, AnDCG 1000 ,nDCG 100 and nDCG 1000 are highly  X  For the metrics that rely on l , changing the value of l can affect the system from that based on Relaxed AveP. As the 45 C-runs were numbered after sorting by Relaxed AveP, an increase in a curve represents an inconsistency with Relaxed AveP. While the Q-measure curve appears to be a smooth blend of the Relaxed and Rigid AveP curves [7,8], AnDCG 1000 and nDCG 1000 give quite different rankings, even at top ranks. For example, nDCG 1000 declares that System 4 outperforms System 3, disagreeing with all other metrics in this figure. One could argue that Q-measure is  X  X onservative X , while AnDCG 1000 and nDCG 1000 are  X  X ovel X . 4.5 Changing Gain Values The previous sections used the default gain values, namely, gain ( S )=3, gain ( A )=2and gain ( B ) = 1. This section examines the effect of using dif-ferent gain values with the graded relevance metrics, using the C-runs only. The  X  X lat X  assignment uses gain ( S )= gain ( A )= gain ( B ) = 1, i.e. binary relevance. The  X  X teep X  assignment uses gain ( S ) = 10, gain ( A )=5, gain ( B )=1,thus emphasising the relevance levels. The third option, which we call  X  X djust X , uses the per-topic gain value adjustment proposed in [6]: Starting with the default assignment, the gain values are automatically modified for each topic according to the proportion of X -relevant documents in the entire set of relevant docu-ments: where Y is the relevance level that is one level lower than X .(If X is the lowest relevance level, then gain ( Y ) is taken to be zero. Moreover, the above transformation is not applied if R ( X )= R .) This was proposed based on the observation that the ratio R ( S ): R ( A ): R ( B ) differs considerably across topics for the NTCIR CLIR test collections. For example, if there are very few S/A-relevant documents and many B-relevant documents, gain ( B ) is set to a very small value compared to gain ( S )and gain ( A ).
 with default, flat, steep and adjusted gain value assignments, which are denoted by Q3:2:1, Q1:1:1, Q10:5:1 and Qadjust, respectively. It can be observed that  X  X lattening X  the gains hurts stability, but otherwise the different gain value as-signments do not affect it significantly. Similar trends were observed for other graded relevance metrics such as AnDCG l as well.
 different gain value assignments at 95% confidence level. (The  X  X efault X  column has been duplicated from Table 2(b).) Values higher than 20% are shown in bold. The table shows that Q-measure, R-measure and AnDCG 1000 have high discrimination power regardless of the gain value assignment.
 justed gain value assignments with the de fault assignment for each metric using Kendall X  X  rank correlation. It can be observed that the Q/R-measure rankings are very robust to the changes in gain values. Other metrics appear to be less robust. Our minority rate experiments suggest that Q-measure, AnDCG l ,nDCG l and AnCG l (with large l ) are more stable than other graded relevance metrics (Ta-ble 1). Whereas, our swap rate experiments suggest that Q-measure, R-measure, AnDCG l and nDCG l (with large l )arethemostuseful(Tables2and3).How are these results related to statistical significance tests? a statistical significance test was conducted based on the sign test, and the run pairs were sorted by the p-value. Then, we created two more sorted lists of the 435 pairs, one sorted by min( GT M ( x, y ) ,GT M ( y, x )) (See Equation 8) and the other sorted by the swap counters summed across the 21 bins for each run pair x and y (See Figure 2). Thus, the first list represents the confidence ranking in terms of the sign test, which uses the complete topic set Q and counts the number of topics with which System x outperformed System y and vice versa. The second lists represents the confidence ranking in terms of the minority rate, which uses 1000 topic subsets for each run pair. The third list represents that in terms of the swap rate, which uses 1000 disjoint pairs of topic subsets (i.e. 2000 subsets). These ranked lists of run pairs can also be compared using Kendall X  X  rank correlation.
 the minority and the swap rates for Relaxed AveP, Q-measure and AnDCG 1000 . The sign test ranking, minority/swap r ate rankings are represented by  X  X ign X ,  X  X inority X  and  X  X wap X , respectively. It can be observed that the correlation values are surprisingly low. (The table shows weak positive correlations: note that rank correlations lie between  X  1 and 1.) It appears that the minority/swap rate calculation methods themselves require further studies.
 ings have shown that Q-measure is very highly correlated with both Relaxed and Rigid AveP, and that R-measure, AnDCG l and nDCG l (with large l )are also highly correlated with Relaxed AveP (Table 4). AveP is known to be a very reliable binary relevance metric [1], so this is good news. Moreover, by using different gain value assignments, we have shown that Q-measure may be more robust to the choice of gain value assignment than others (Tables 8 and 9).
 best graded relevance metrics that are based on the document cut-off l ,provided that a sufficiently large value is used for l . However, if one wants to avoid the parameter l , or if one wants a  X  X onservative X  metric that closely resembles AveP, then Q-measure is probably the best choice. other graded relevance metrics. More recently, Kek  X  al  X  ainen [5] investigated the rank correlations among (n)(D)CG l and PDoc l using TREC data with their own graded relevance assessments. Whereas, Sakai [7,8] investigated the rank correla-tions among AveP, R-Prec, Q-measure and R-measure using NTCIR data. This paper extends these studies in that (a) It covers more metrics; (b) It examines the reliability of metrics in addition to resemblance among metrics. uous relevance [3], but ADM is not suitable for traditional document ranking tasks as it simply accumulates the absolute differences between User Relevance Scores (URSs) and System Relevance Scores (SRSs): Suppose that the URSs for documents A, B and C are 0.3, 0.2 and 0.1 for a topic, and the SRSs for these documents are 0.5, 0.4 and 0.2 acccording to System x , and 0.1, 0.2 and 0.3 according to System y . In terms of document ranking, Systems x is perfect, while System y is not. However, in terms of ADM, System y (sum of differences: 0.4) is better than System x (sum of differences: 0.5). This paper compared 14 metrics designed for IR evaluation with graded rele-vance, together with 10 traditional metrics based on binary relevance, in terms of reliability and resemblance of system rankings using two different sets of data from the NTCIR-3 CLIR track. Our results suggest that graded relevance may provide more reliability than binary relevance. More specifically, we showed that AnDCG l and nDCG l are good metrics provided that a large value of l is chosen; However, if one wants to avoid the parameter l altogether, or if one requires a metric that closely resembles traditio nal Average Precision, then Q-measure appears to be the best choice. We plan to repeat these experiments with other graded relevance test collections, hopefully with larger topic sets.
