 University of Sheffield Introduction
The title of this piece refers to Newton X  X  only known modest remark:  X  X f I have seen farther than other men, it was because I was standing on the shoulders of giants. X  Since he himself was so much greater than his predecessors, he was in fact standing on the shoulders of dwarfs, a much less attractive metaphor. I intend no comparisons with
Newton in what follows: NLP/CL has no Newtons and no Nobel Prizes so far, and quite rightly. I intend only to draw attention to a tendency in our field to ignore its intellectual inheritance and debt; I intend to discharge a little of this debt in this article, partly as an encouragement to others to improve our lack of scholarship and knowledge of our own roots, often driven by the desire for novelty and to name our own systems. Roger
Schank used to argue that it was crucial to name your own NLP system and then have lots of students to colonize all major CS departments, although time has not been kind to his many achievements and originalities, even though he did build just such an Empire.
But to me one of the most striking losses from our corporate memory is the man who is to me the greatest of the first generation and still with us: Vic Yngve. This is the man who gave us COMIT, the first NLP programming language; the first random generation of sentences; and the first direct link from syntactic structure to parsing processes and storage (the depth hypothesis). I find students now rarely recognize his name, and find that incredible.
 gineering to do the scholarship. It is something endemic in the wider field of Computer
Science and Artificial Intelligence, although bottom-up wiki techniques are now filling many historical gaps for those who know where to look, as the generation of pioneers has time to reminisce in retirement. 1 There are costs to us from this general lack of awareness, though: a difficulty of  X  X tanding on the shoulders X  of others and acknowl-edging debts, let alone passing on software packages. Alan Bundy used to highlight this in the AISB Quarterly with a regular column where he located and pilloried reinventions in the field of AI; he also recommended giving obituaries for one X  X  own work, and this paper could be seen in that way, too. Early Academic Life
My overwhelming emotion on getting this honor was, after surprise, a feeling of in-adequacy in measuring up to previous honorees, but nonetheless, I want to grasp at this moment of autobiography, or at what in his own acceptance paper Martin Kay called:  X  X ut one chance for such gross indulgence. X  I was born in 1939 in London at just about the moment the Second World War started in Europe; this was, briefly, a severe career slowdown. However, the British Government had a policy of exporting most children out of the range of bombs and I was sent to Torquay, a seaside town in southwest England that happened to have palm trees on all the main streets, a fact it is often difficult to convince outsiders of. The town had, and has, a Grammar School for Boys, which had a very good Cambridge-trained mathematician as its headmaster, and eventually I made my way back across England to Pembroke College, Cambridge, to study mathematics, a college now for ever associated with my comedian contem-poraries: Peter Cook, Clive James, Eric Idle, Tim Brooke-Taylor, and similar wastrels. I began a series of changes of subject of study, downhill towards easier and easier ones: from mathematics to philosophy to (what in the end after graduation became) NLP/AI.
It was not that I could not do the mathematics, but rather that I experienced the shock that many do of finding how wide the range of talent in mathematics is, and that being very good in a provincial grammar school does not make one very good at Cambridge.
This is a feeling peculiar to mathematics, I think, because the talent range is so much wider than in most subjects, even at the top level.
 was the philosophy tutor for my college, although her main vocation was running the Institute she had founded, outside the University in a Cambridge suburb: CLRU, the
Cambridge Language Research Unit. It was an eccentric and informal outfit, housed in what had been a museum of Buddhist art, some of whose sculptures were built into the walls. MMB (as she was known) ran the CLRU from the mid 1950s to the early 1980s on a mix of US, UK, and EU grants and did pioneering work in MT, AI, and IR. Of those honored by the ACL with this award over the last five years, three have been graduates of that little Buddhist shed, and include Martin Kay and Karen Sp  X  arck Jones, a remarkable tribute to MMB. The lives and work of we three have been quite different but all in different ways stem from MMB X  X  interests and vision: She had been a pupil of Wittgenstein and, had she known it, would have approved of Longuet-Higgins X  X  remark that  X  X I is the pursuit of metaphysics by other means. X  She believed that practical research into the structure of language could give insight into metaphysics, but was in no way other-worldly: She was the daughter of a Cabinet Minister and knew what it was to command.
 executor: She had never written a book and wanted me to construct one from her papers posthumously. It took me twenty years to get the required permissions but the volume finally appeared in 2005 (Masterman et al. 2005).
 Thesis Building and CLRU
When I started work at CLRU in 1962 to do a doctorate, it had no computer in the normal sense, only a Hollerith card sorter of the sort built for the US census half a century before. Basically, you put a stack of punched cards into one of these things X  which looked like a metal horse on four legs X  X nd the cards fell into (I think) 10 slots 472 depending on how you had plugged in a set of wires at the back to identify destination slots for sorted cards with hole patterns on the cards. With some effort, these could be turned into quite interesting Boolean machines; my first task was to take a notion of Fred Parker-Rhodes that a Hallidayan grammar could be expressed as a lattice of typed classes, and then program the card sorter so that repeated sorts of punched cards could be used to parse a sentence. It was triumph of ingenuity over practicality. Later the CLRU owned an ICL 1202 computer with 1,200 registers on a drum, but it was a so-called bini-ten machine designed for UK cash transactions when there were still 12 pennies to a shilling, and so the 1202 has print wheel characters for 10, 11, and 12 (as well as 0 X 9), a fact on which Parker-Rhodes built a whole world of novel print conventions for his research. This was the period at CLRU when Karen Sp  X  arck Jones was completing her highly original thesis (published twenty years later as Jones [1986]) on unsupervised clustering of thesaurus terms X  X hose goal was to produce primitives for MT, it is often forgotten X  X ntil she had to move her computations to a real computer at the University Computing Laboratory, where she eventually created a new career in
IR, essentially using the same clump algorithms X  X reated by Parker-Rhodes and her husband Roger Needham X  X o do IR.
 message detection using an interlingua X  (Masterman 1961), an area in which Martin
Kay had also originally worked on an interlingua for MT. My thesis computation was done in LISP 1.6 on an IBM360 (under a one-man US Air Force contract, administered by E. Mark Gold, who later became famous as the founder of learnability theory), at
SDC in Santa Monica, where I was attached loosely in 1966 to the NLP group there run by Bob Simmons. My thesis was to be entitled  X  X rgument and proof in Meta-physics from an empirical point of view X  and my advisor was MMB X  X  husband, Richard
Braithwaite, Knightbridge Professor of Moral Philosophy at the University. He was a philosopher of science and a logician, and was given the chair of moral philosophy X  a subject about which he knew nothing X  X ecause it was the only one available at
Cambridge at the time. This produced an extraordinary inaugural lecture in which he effectively founded a new subject:  X  X he theory of games as a tool for the moral philosopher. X  a favor to MMB. My interest was the demarcation of metaphysical text: what it was, if anything, that distinguished it from ordinary language text. Wittgenstein had once said that words were  X  X n holiday X  in metaphysical text, but also that he wanted to  X  X ring words back from their metaphysical to their everyday usage X  (Wittgenstein 1973). This is exactly what I wanted to capture with computation, and the thesis was eventually submitted to the Cambridge Philosophy faculty in 1967 X  X hen called Moral Sciences X  with a large appendix of LISP program code at the back, something they had never seen before, or since. The thesis was bound in yellow, though the regulations stipulated black or brown bindings; I must have had some extraordinary idea that someone might cruise the long corridors of Cambridge theses looking for one that stood out by color X  the arrogance of youth! (1937) and his claim that meaningfulness in text could be determined by  X  X ogical syntax X  X  X ules of formation and transformation (a notion which may well sound famil-iar; Chomsky was a student of Carnap). My claim was that this was a bad demarcation and a better criterion of meaningfulness would be to have one interpretation rather than many, namely, that word-sense discrimination (WSD) was possible for a given text. On that view, the  X  X eaningless X  text had too many interpretations rather than none (or one). A word in isolation is thus often meaningless. Preference Semantics was a WSD program to do just that, and to provide a new sense where WSD failed.
 metaphysical discourse, entitled  X  X ome Remarks on Spinoza X  X  Ethics. X  He argued that
Spinoza X  X  logical arguments are all false, but that what Spinoza was actually doing is rhetorical , not logical : imposing a new sense on the reader. The system as implemented was, of course, a toy system, in the sense that all symbolic NLP systems were in that era. It consisted of an analysis of five metaphysical texts (by Wittgenstein, Spinoza,
Descartes, Kant, and Leibniz) along with five randomly chosen passages from editorials in the London Times , as some sort of control texts.

Boguraev declared the average size of vocabularies in working NLP systems to be 36 words. The semantic structures derived X  X ia what we would now call chunk parsing X  X onsisted of tree structures of primitives (from a set of about 80), one tree for each participating word sense in the text chunk, that fitted into preformed triples called templates . These templates were subject X  X redicate X  X bject triples that defined well-formed sequences of the triples of trees (i.e., the first tree for the sense of the subject, the second for the action and so on), whose tree-heads had to fit those of the template X  X  three primitive items in order. The overall system selected the word senses that fitted into these structures by means of a notion of  X  X emantic preference X  (see subsequent discussion), and then declared those to be the appropriate senses for the words, thus doing a primitive kind of WSD.
 did not work, which tried to identify some sense of a word in the text whose representa-tion would fit in the overall structure derived, and so could be declared a suitable  X  X ew X  sense for the word which had previously failed to fit in. Unsurprisingly, it identified, say, a sense of  X  X od X  in the Spinoza text with an existing sense of  X  X ature X  so that, after this substitution, the whole thing fitted together and WSD could proceed, and thus the passage be declared meaningful, given the criterion of having a single, ambiguity-free, interpretation. This was the toy procedure that allowed me to argue that Spinoza X  X  real aim, whether he knew it or not, was to persuade us that the word  X  X od X  could have the sense of  X  X ature X  and that this was the real point of his philosophy X  X xactly in line with what Bosanquet had predicted.
 versity philosophy journal, although the meaningfulness criterion appeared in Mind in 1971 under the title  X  X ecidability and Natural Language X  (Wilks 1971). Since publishing in Mind was, at the time, the ambition of every young philosopher, I was now satisfied and could move to the simpler world of NLP. The thesis, shorn of the metaphysics, appeared as my first book, Grammar, Meaning and the Machine Analysis of Language (Wilks 1972); the title was intended as a variation on the title of some strange German play, popular at the time, and whose actual name I can no longer remember.
 Preference Semantics I returned from California to CLRU but left again for the Stanford AI Lab in 1969.
I had fantasized at CLRU about all the things one could do with a methodology of trying to base a fairly complex compositional semantics on a foundation of superficial pattern matching. This had earlier produced speculations like my 1964 CLRU paper  X  X ext searching with templates, X  procedures that we could not possibly have carried 474 I.1 ((*ANI 1)((SELF IN)(MOVE CAUSE))(*REAL 2))  X  (1(*JUDG )2 ) I.2 (1 BE (GOOD KIND))  X  ((*ANI 2 )WANT 1 ) out with the machines then available, but which I now choose to see as wanting to do Information Extraction: though, of course, it was Naomi Sager who did IE first on medical texts at NYU (see Sager and Grishman 1975).

MIT; Schank, then starting to build his Conceptual Dependency empire; and Colby and his large team building the PARRY dialogue system, which included Larry Tesler, later the Apple software architect. Schank and I agreed on far more than we disagreed on and saw that we would be stronger together than separately, but neither of us wanted to give up our notation: He realized, rightly, that there was more persuasive power in diagrams than in talk of processes like  X  X reference. X  It was an extraordinary period, when AI and
NLP were probably closer than ever before or since: Around 1972 Colmerauer passed through the Stanford AI Lab, describing Prolog for the first time but, as you may or may not remember, as a tool for machine translation! I spent my time there defining and expanding the coherence-based semantics underlying my thesis, calling it  X  X reference
Semantics X  (PS), adding larger scale structures such as inference rules (see Figure 1) and thesauri, and building it into the core of a small semantics-based English-to-French machine translation system programmed in LISP. At one point the code of this MT system ended up in the Boston Computer Museum, but I have no idea where it is now.
The principles behind PS were as follows:
One could put some of these, admittedly programmatic and imprecise, points as follows: Constructible Belief Systems I returned to Europe in the mid 1970s, first to the ISSCO institute in Lugano, where
Charniak was and Schank had just left, and then to Edinburgh as a visitor before taking a job at Essex. I began a long period of interest in belief systems, in particular seeking some representation of the beliefs of others, down to any required degree of nesting X  for example, A X  X  belief about B X  X  belief about C X  X hat could be constructed recursively at need, rather than being set out in advance, as in the pioneering systems emerging from the Toronto group under Ray Perrault (Allen and Perrault 1980). I began thinking about this with Janusz Bien of the University of Warsaw, who had also published a paper arguing that CL/NLP should consider  X  X east effort X  methods: in the sense that the brain might well, due to evolution, be a lazy processor and seek methods for understanding that minimized some value that could be identified with processing effort. I had argued in PS for choosing shortest chains of inferences between templates, and that the most connected/preferred template structure for a piece of text should be the one found first. I am not sure we ever proved any of this: It was just speculation, as was the preference for the most semantically connected representation, and the representation with the least information. All this is really only elementary information theory: a random string of words contains the maximum information, but that is not very helpful. Clearly, the preferred interpretation of  X  X e was named after his father X  (i.e., named the same rather than later in time ) is not the least informative, since the latter contains no information at all X  X eing necessarily true X  X o one would have to adapt any such slogan to:  X  X refer the interpretation with the least information, unless it is zero! X  and John Barnden, has not been a successful paradigm in terms of take-up, in that it has not got into the general discourse, even in the way that Fauconnier X  X   X  X ental
Spaces X  (Fauconnier 1985) has. That approach uses the same spatial metaphor, but for strictly linguistic rather than belief and knowledge purposes. But I think the VIEWGEN belief paradigm, as it became, had virtues, and I want to exploit this opportunity to remind people of it. It was meant to capture the intuition that if we want, for language 476 understanding purposes, to construct X X  X  beliefs about Y X  X  beliefs X  X hat I called the environment of Y-for-X X  X hen: 1. It must be a construction that can be done in real time to any level of 2. It must capture the intuition that much of our belief is accepted by default 3. We must be able to maintain apparently contradictory beliefs, provided
In VIEWGEN, belief construction is done in terms of a  X  X ush down X  metaphor: A permeable  X  X ontainer X  of your beliefs is pushed into a  X  X ontainer X  of my beliefs and what percolates through the membrane, from me to you, will be believed and ascribed to you, unless it is explicitly contradicted, namely, by some contrary belief I already ascribe to you, and which, as it were, keeps mine from percolating through. The idea is to construct the appropriate  X  X nner belief space X  at the relevant level of nesting, so that inference can be done, and to derive consequences (within that constrained content space) that also serve to model, in this case, you the belief holder in terms of goals and desires, in addition to beliefs. This approach is quite different not only from the
Perrault/Toronto system of belief-relevant plans but also to AI theories that make use of sets-of-support premises, since this is about belief-inheritance-by-default. It is also quite distinct from linguistic theories like Wilson and Sperber X  X  Relevance Theory, which take no account at all of belief as relative to individuals, but perform all operations in some space that is the same for everyone, which is an essentially Chomskyan ideal competence-style notion of belief that is not relative to individuals X  X hich is of course absurd.
 approach and linked it to dialogue and other applications, but there has been no major application showing its essential role in a functioning conversational theory where complex belief states are created in real time. However, the field is, I believe, now moving in that direction (e.g., with POMDP theories [Williams and Young 2007]) since the possibility of populating belief theories with a realistic base from text by means of
Information Extraction or Semantic We bparsing to RDF format is now real (a matter we shall return to subsequently).
 and Preference Semantics, in terms of meaning and its relation to processes. First, there was the role of choice and alternatives, crucial to PS, in that an assigned mean-ing interpretation for a text was no more than a choice of the best available among alternatives, because preference implies choice, in a way that generative linguistics X  though not of course traditions like Halliday X  X  X  X lways displayed alternatives but considered choice between them a matter for mere performance. What was dispensable to generative linguistics was the heart of the matter, I argued, to NLP/CL. Secondly,
VIEWGEN suggested a view of meaning, consistent locally with PS, dependent on which individuals or classes one chose to see in terms of each other X  X he key notion here was seeing one thing as another and its consequences for meaning. So, if one chose to identify (as being the same person under two names) Joe (and what one believed about him) with Fred X  X  father (and what one knew about him), the hypothesis was that a belief environment should be constructed for Joe-as-Fred X  X -father by percolating one set of beliefs into the other, just as was done by the basic algorithm for creating A X  X -beliefs-about-B X  X -beliefs from the component beliefs of A and B. This process created a hybrid entity, with intensional meaning captured by the set of propositions in that inner environment of belief space, but which was now neither Joe nor Fred X  X  father but rather the system X  X  point of view of their directional amalgamation: Joe-as-Fred X  X -father (which might contain different propositions from the result of Fred X  X -father-as-Joe). days, such as knowledge representations for Navy ships X  captains genuinely uncertain as to whether ship-in-my-viewfinder-now was or was not to be identified with the stored representation for enemy-ship-number-X. The important underlying notion was one going back to Frege, and which first had an outing in Winograd X  X  thesis (Winograd 1972), where he showed you could have representations for blocks that did not in fact exist on the Blocks World table. A semantics must be able to represent things without knowing whether they exist or not; that is a basic requirement.
 ing process of conflating two belief objects was extended to the representation of  X  X etaphorical objects, X  which could be described, quite traditionally in the literature, as A-viewed-as-B (e.g., an atom viewed as a billiard ball). The metaphorical object atom-as-billiard-ball was again created by the same push-down or fusion of belief sets as in the basic belief point-of-view procedure. All this may well have been fanciful, and was never fully exploited in published work with programs, but it did have a certain intellectual appeal in wanting to treat belief, points of view, metaphor and identification of intensional individuals X  X ormally quite separate issues in semantics X  as being modellable by the same simple underlying process (see Ballim, Wilks, and
Barnden 1991). One novel element that did emerge from this analysis was that, in the construction of these complex intensional identifications, such as between  X  X oday X  X 
Wimbledon winner X  and  X  X he top male tennis seed, X  one could choose directions of  X  X iewing as X  with the belief sets that led to objects which were neither the classic de re nor de dicto outcomes: Those became just two among a range of choices, and the others of course had no handy Latin names.
 Adapting to the  X  X mpirical Wave X  in NLP
For me, as with many others, especially in Europe, the beginning of the empirical wave in NLP was the work of Leech and his colleagues at Lancaster: CLAWS4 (a name which hides a UK political joke), their part-of-speech tagger based on large-scale annotation of corpora. Such tagging is now the standard first stage of almost every NLP process and it may be hard for some to realize the skepticsm its arrival provoked:  X  X hat could anyone want that for? X  was a common reaction from those still preoccupied by computational syntax or semantics. That system was sold to IBM, whose speech group, under Jelinek,
Mercer, and Brown, subsequently astonished the CL/NLP world with their statistical machine translation system CANDIDE. I wrote critical papers about it at the time, not totally unconnected to the fact that I was funded by DARPA on the PANGLOSS project 478 at NMSU (along with CMU and ISI/USC) to do MT by competing, but non-statistical, methods.
 old peasant folk-tale of the traveler who arrives at a house seeking food and claiming to have a stone that makes soup from water. He begs a ham bone to stir the water and stone and eventually cons out of his hosts all the ingredients for real soup. The aspect of the story I was focusing on was that, in the CANDIDE system, I was not sure that the  X  X tone, X  namely IBM X  X   X  X undamental equation of MT, X  was in fact producing the results, and suggested that something else they were doing was giving them their remarkable success rate of about 50% of sentences correctly translated. As their general methodology has penetrated the whole of NLP/CL, I no longer stand by my early criticisms; IBM was of course right, and had everything to teach the rest of us. its successes in, say, POS tagging, that its methods could extend to the heartland of semantics and pragmatics. Like others, I came to see this assumption was quite untrue, and myself moved towards Machine Learning (ML) approaches to word-sense disam-biguation (e.g., Stevenson and Wilks 2001) and I now work in ML methods applied to dialogue corpora (as I shall mention subsequently). But the overall shift in approaches to semantics since 1990 has not only been in the introduction of statistical methods, and
ML in particular, but also in the unexpected advantages that have been gained from what one might call non-statistical empirical linguistics, and in particular Information Extraction (IE; see Wilks 1997).
 semantic parsing, and that it was in fact some form of superficial pattern matching onto language chunks that was then transformed to different layers of compositional semantic representation. There were obvious relations between that general approach and what emerged from the DARPA competitions in the early 1990s as IE, a technology that, when honed by many teams, and especially when ML techniques were added to it later, had remarkable success and a range of applications; it also expanded out into other, traditionally separate, NLP areas such as question answering and summarization.
This approach is not in essence statistical at all, however, although it is in a clear sense  X  X uperficial, X  with the assumption that semantics is not necessarily a  X  X eep X  phenomenon but present on the language surface. I believe the IE movement is also one of the drivers behind the Semantic Web movement, to which I now turn, and which
I think has brought NLP back to a position nearer the core of AI, from which it drifted away in the 1980s.
 Meaning and the Semantic Web The Semantic We b(SW; Berners-Lee, Hendler, and Lassila 2001) is what one could call
Berners-Lee X  X  second big idea, after the World Wide Web; it can be described briefly as turning the Web into something that can also be understood by computers in the way that it is understood by people now, as a web of texts and pictures. Depending on one X  X  attitude to this enterprise, already well-funded by the European Commission at least, it can be described as any of the following: 1. As a revival of the traditional AI goal (at least since McCarthy and Hayes 2. As a hierarchy of forms of annotation X  X r what I shall call augmentation 3. As a system of access to trusted databases that ground the meanings of
There is also a fourth view, much harder to express, that says roughly that, if we keep our heads, the SW can come into being with any system of coding that will tolerate the expansion of scale of the system, in the way that, miraculously, the hardware under-pinnings of the World Wide We bhave tolerated its extraordinary expansion without major breakdown. This is an engineering view that believes there are no fundamental problems about the meanings and reference of SW terms in, for example, the ontologies within the SW, and everything will be all right if we just hold tight.
 (3) has no special privilege because it is the World Wide Web founder X  X  own view: Marx was notoriously not a very consistent Marxist, and one can find multiple examples of this phenomenon. View (3) is highly interesting and close to philosophical views of meaning expressed over many years by Putnam, which can be summarized as the idea that scientists (and Berners-Lee was by origin a database expert and physicist) are  X  X uardians of meaning X  in some sense because they know what terms really mean, in a way that ordinary speakers do not. Putnam X  X  standard example is that of metals like molybdenum and aluminum, which look alike and, to the man in the street, have the same conceptual, intensional meaning, namely light, white, shiny metal. But only the scientist (says Putnam) knows the real meanings of those words because he knows the atomic weights of the two metals and methods for distinguishing them.
 are in charge of what terms mean, and not any expert X  X t all seriously can even consider such a view. On the view we are attributing to Wittgenstein, the terms are synonymous in a public language, just as water and heavy water are, and any evidence to the contrary is a private matter for science, not for meaning.
 researchers: They have, of course, changed tack considerably and produced formalisms for the SW, some of which are far closer to the surface of language than logic (what is known as RDF triples), as well as inference mechanisms like DAML-OIL that gain advantages over traditional AI methods on the large and practical scale the SW is intended to work over. On the other hand there are those in AI who say they have ignored much of the last 40 years of AI research that would have helped them. This dispute has a conventional flavor and it must be admitted that, in more than 40 years,
AI itself did not come up with such formalisms that stood any chance at all of working on a large scale on unstructured material (i.e., text). 480 partially in NLP terms, however much Berners-Lee rejects such a view and says NLP is irrelevant to the SW. The whole trend of SW research, in Europe at least, has been to build up to higher and higher levels of semantic annotation X  X  technology that has grown directly out of IE X  X  success in NLP X  X s a way of adding content to surface text.
It seems to me obvious that any new SW will evolve from the existing WWW of text by some such method, and that method is basically a form of large-scale NLP, which now takes the form of transducers from text to RDF (such as the recently advertised
Reuters API). The idea that the SW can start from scratch in some other place, ignoring the existing World Wide Web, seems to me unthinkable; successful natural evolution always adapts the function of what is available and almost never starts again afresh. important to see that the SW movement X  X t least as I interpret it herein, and that does seem pretty close to the way research in it is currently being funded, under calls and titles like  X  X emantic content X  X  X s one that links to the themes already developed in this paper in several ways, and which correspond closely to issues in my own early work, but which have not gone away: 1. The SW takes semantic annotation of content as being a method X  X hether 2. The SW accords a key role to ontologies as knowledge structures: partially 3. The RDF forms, based on triples of surface items, as a knowledge
The most important interest of the SW, from the point of view of this paper, is that it provides at last a real possibility of a large-scale test of semantic and knowledge coding: One thing the empirical movement has taught us is the vital importance of scale and the need to move away from toy systems and illustrative examples. I mentioned earlier the freely available Reuters API for RDF translation which Slashdot advertised under the title  X  X s the Semantic We ba Reality at Last? X  This is exactly the kind of move to the large scale that we can hope will settle definitively some of these ancient issues about meaning and knowledge.
 A Late Interest in Dialogue: The Companions Project
My only early exposure to dialogue systems was Colby X  X  PARRY: As I noted earlier, his team was on the same corridor as me at Stanford AI La bin the early 1970s. I was a great admirer of the PARRY system: It seemed to me then, and still does, probably the most robust dialogue system ever written. It was available over the early ARPANET and tried out by thousands, usually at night: It was written in LISP and never broke down; making allowances for the fact it was supposed to be paranoid, it was plausible and sometimes almost intelligent. In any case it was infinitely more interesting than
ELIZA, and it is one of the great ironies of our subject that ELIZA is so much better known. PARRY remembered what you had said, had elementary emotion parameters and, above all, had something to say, which chatbots never do. John McCarthy, who ran the AI Lab, would never admit that PARRY was AI, even though he tolerated it under his roof, as it were, for many years; he would say  X  X t doesn X  X  even know who 482 the President is, X  as if most of the world X  X  population did! PARRY was in fact a semi-refutation of the claim that you need knowledge to understand and converse, because it plainly knew nothing; what it had was primitive  X  X ntentionality, X  in the sense that it had things  X  X t wanted to say. X  the late 1990s by David Levy, who had written 40 books on chess and ran a company that made chess machines. He already had a footnote in AI as the man who had bet
McCarthy, Michie, and other AI leaders that a chess machine would not beat him within ten years, and he won the bet more than once. In the 1990s he conceived a desire to win the Loebner Prize 2 for the best dialogue program of the year, and came to us at Sheffield to fund a team to win it for him, which we did in 1997. I designed the system and drew upon my memories of PARRY, along with obvious advances in the role of knowledge bases and inference, and the importance of corpora and machine learning. For example, we took the whole set of winning Loebner dialogues off the Web so as to learn the kinds of things that the journalist-testers actually said to the trial systems to see if they were really humans or machines.
 34-year old female British journalist living in New York, and it owed something to
PARRY, certainly in Catherine X  X  desire to tell people things. It was driven by frames corresponding to each of about 80 topics that such a person might want to discuss; death, God, clothes, make-up, sex, abortion, and so on. It was far too top-down and unwilling to shift from topic to topic but it could seem quite smart on a good day, and probably won because we had built in news from the night before the competition of a meeting Bill Clinton had had that day at the White House with Ellen de Generes, a lesbian actress. This gave a certain immediacy to the responses intended to sway the judges, as in  X  X id you see that meeting Ellen had with Clinton last night? X  persisted for a decade and is now exercised through COMPANIONS (Wilks 2004), a large EU 15-site four-year project that I run. COMPANIONS aims to change the way we think about the relationships of people to computers and the Internet by developing a virtual conversational  X  X ompanion. X  This will be an agent or  X  X resence X  that stays with the user for long periods of time, developing a relationship and  X  X nowing X  its owner X  X  preferences and wishes. It will communicate with the user primarily by using and un-derstanding speech, but also using other technologies such as touch screens and sensors. cannot serve all social groups well, and it is one of our objectives to empower citizens (including the non-technical, the disabled, and the elderly) with a new kind of interface based on language technologies. The vision of the Senior Companion X  X urrently our main prototype X  X s that of an artificial agent that communicates with its user on a long-term basis, adapting to their voice, needs, and interests: A companion that would entertain, inform, and react to emergencies. It aims to provide access to information and services as well as company for the elderly by chatting, remembering past con-versations, and organizing (and making sense of) the owner X  X  photographic and image memories. This Companion would assume a user with a low level of technical knowl-edge, and who might have lost the ability to read or produce documents themselves unaided, but who might need help dealing with letters, messages, bills, and getting in-formation from the Internet. During its conversations with its user or owner, the system builds up a knowledge inventory of family relations, family events in photos, places visited, and so on. This knowledge base is currently stored in RDF, the Semantic Web format, which has two advantages: first, a very simple inference scheme with which to drive further conversational inferences, and second, the possibility, not yet fulfilled, of accessing arbitrary amounts of world information from Wikipedia, already available in RDF, which could not possibly have been pre-coded in the dialogue manager, nor elicited in a conversation of reasonable length. So, if the user says a photo was taken in
Paris, the Companion should be able to ask a question about Paris without needing that knowledge pre-coded, but only using rapidly accessed Wikipedia RDFs about Paris. An ultimate aim of this aspect of the Senior Companion is the provision of a life narrative, an assisted autobiography for everyone, one that could be given to relatives later if the owner chose to leave it to them. There is a lot of technical stuff in the Senior Companion: script-like structures X  X alled DAFs or Dialogue Action Forms X  X esigned to capture the course of dialogues on specific topics or individuals or images, and these DAFs we are trying to learn from tiled corpora. The DAFs are pushed and popped on a single stack, and that simple virtual machine is the Dialogue Manager, where DAFs being pushed, popped, or reentered at a lower stack point are intended to capture the exits from, and returns to, abandoned topics and the movement of conversational initiative between the system and the user. We are halfway through the project and currently have two prototype Companions: The other, based not at Sheffield but at Tampere, is a Health and Fitness Companion (HFC). 3 It is more task-oriented than the Senior Companion and aims to advise on exercise and diet. The HFC is on a mobile phone architecture as well as a PC, and we may seek to combine the two prototypes later. The central notion of a Companion is that of the same  X  X ersonality, X  with its memory and voice being present no matter what the platform. It is not a robot, and could be embodied later in something like a chatty furry handbag, being held on a sofa and perhaps reminding you about the previous episodes of your favorite TV program.
 Finale
This article has had something of the form of a life story, and everyone wants to believe their life is some kind of narrative rather than a random chase from funding agency to funding agency, with occasional pauses to carry out a successful proposal. But let us return to Newton for a moment in closing; for us in CL he is the great counter-example, of why we do not do science or engineering in that classic solitary manner:
The emphasis there for me is on alone , which is pretty much unthinkable in our research world of teams and research groups. Our form of research is essentially corporate and cooperative; we may not be sure whose shoulders we are standing on, but we know whose hands we are holding. I have worked in such a way since my thirties and, at 484
Sheffield, my work would not have been possible without a wide range of colleagues and former students in the NLP group there over many years and including Louise Guthrie, Ro bGaizauskas, Hamish Cunningham, Fa bio Ciravegna, Mark Stevenson,
Mark Hepple, Kalina Bontcheva, Roberta Catizone, Nick Webb, and many others. In recent years, what one could call  X  X ARPA culture X  X  X f competitions and cooperation subtly mixed X  X s well as the great repositories of software and data like LDC and ELRA, have gone a long way to mitigate the personal and group isolation in the field.
We have no Newtons and will never have any. That is not to deny that we need real ideas and innovations, and now may be a time for fresh ones. We have stood on the shoulders of Fred Jelinek, Ken Church, and others for nearly two decades now, and the strain is beginning to tell as papers still strive to gain that extra 1% in their scores on some small task. We know that some change is in the air and I have tried to hint in this article as to some of the places where that might be, even if that will mean a partial return to older, unfashionable ideas; for there is nothing new under the sun. But locating them and exploiting them will not be in my hands but in yours, readers of Computational Linguistics ! Acknowledgments References
