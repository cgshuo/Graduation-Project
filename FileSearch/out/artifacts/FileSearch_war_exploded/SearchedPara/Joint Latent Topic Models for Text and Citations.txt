 I.2.6 [ Artificial Intelligence ]: Learning; H.2.8 [ Database Management ]: Database Applications X  data mining Algorithms, Experimentation Topic models, LDA, PLSA, variational inference, hyperlink s, influence, citations  X  This work was done while the author was at CMU.

Proliferation of large electronic document collections su ch as the web, news articles, blogs and scientific literature in the recent past has posed several new, interesting challeng es to researchers in the data mining community. In particular, there is an increasing need for automatic techniques to visu -alize, analyze and mine these document collections. In the recent past, latent topic modeling has become very popular as a completely unsupervised technique for topic discovery in large document collections. These models, such as PLSA [9] and LDA [4], exploit co-occurrence patterns of words in documents to unearth semantically meaningful probabilist ic clusters of words called topics . These models also assign a probabilistic membership to documents in the latent topic-space, allowing us to view and process the documents in this lower-dimensional space.

Most of the models in this framework such as Dynamic topic models [5, 15], Pachinko Allocation [11], Correlated Topic Model [3], etc., model various aspects of document collections such as time, hierarchy of topics, correlation s be-tween topics respectively. However, all the above mentione d models ignore a rich feature that contains valuable infor-mation, namely, the citation or hyperlink structure. It is a known fact in information retrieval that a citation between two documents not only indicates topical similarity of the two documents but also authoritativeness of the cited docu-ment [10]. This idea has been exploited by algorithms such as PageRank [16] which are now de facto techniques in search engine technology.

In our work, we aim at addressing the problem of jointly modeling text and citations in the topic modeling frame-work. Our hope is that explicit modeling of citations cap-tures the topicality of documents in the collection better, and thereby improves the predictive power of these models.
The rest of the paper is organized as follows. In section 2, we discuss some of the past work done on joint models of topics and citations in the framework of latent topic models . We introduce two new models in section 3 and their corre-sponding learning and inference techniques using variatio nal approximations. In section 4, we describe the data sets, the tasks and evaluation we used for our experiments. Section 5 reports and analyzes the results of our experiments. We conclude the discussion in section 6 with a few remarks on directions for future work.

Note that in the rest of the paper, we use the terms  X  X ita-tion X ,  X  X yperlink X  and  X  X ink X  interchangeably. Likewise, note that the term  X  X iting X  is synonymous to  X  X inking X  and so is Table 1: Notation of some frequently occurring variables
Table 2: Generative process for the Link-LDA model  X  X ited X  to  X  X inked X . The reader is also recommended to refer to table 1 for some frequent notation used in this paper.
In one of the first efforts in applying topic models to mod-eling citation data, Cohn and Hoffman [6] built an extension to the PLSA [9] model, called PHITS. This model defines a generative process not only for text but also for citations (hyperlinks). The generation of each hyperlink in a docu-ment d is modeled as a multinomial sampling of the tar-get document d  X  from the topic-specific distribution  X  over documents. The model assigns high probability  X  kd  X  to a document d  X  with respect to topic k , if the document is hyper-linked from several documents that discuss that topi c. The authors showed that the document X  X  representation in topic-space obtained from this model improves the perfor-mance of a document-classifier, compared to the representa-tion obtained from text alone. Henceforth, we will refer to this model as Link-PLSA, for consistency of notation in this paper.

A similar model called mixed membership model was de-veloped by Erosheva et al [8], in which PLSA was replaced by LDA as the fundamental generative building block. We will refer to this model as Link-LDA for notational consis-tency. The generative process for this model is shown in table 2 and the corresponding graphical representation is displayed in figure 1. As shown in the figure, the genera-tive processes for words and hyperlinks are very similar and they share the same document-specific topic distribution  X  to generate their respective latent topics. Thus, this mode l (as well as Link-PLSA) captures the notion that documents that share the same hyperlinks and same words, tend to be on the same topic.

Both Link-PLSA and Link-LDA define hyperlinks as just values taken by a random variable (similar to words in the vocabulary). In other words, these models obtain proba-bilistic topical clusters of hyperlinks exactly the same wa y as the basic LDA and PLSA models discover topical clus-ters of words. Such methods fail to explicitly model the topical relationship between the text of the citing (linkin g) document and the text of cited (linked) document. One can Figure 1: Graphical representation of the Link-hope to obtain better quality of topics by exploiting this additional information.

Recently, Dietz et al [7] proposed a new LDA based ap-proach that allows flow of influence from the text of the cited documents to the text of the citing documents. In their ap-proach, each citing document borrows topics from one of its citations in generating its own text. In choosing a citation to borrow topics from, the document uses its own distribu-tion over its citations. This distribution is interpreted a s the influence of each citation on the citing document. This model however does not e xplicitly model topicality of cita-tions. In addition, this model assumes citations as input data, whereas in our work, we will propose models that can generate as well as predict citations for unseen documents. In this model, we combine the LDA model with the Mixed Membership Stochastic Block (MMSB) model [1], previously used in modeling protein-protein interactions. The MMSB model assigns probabilistic membership for proteins into topics based on their interactions as follows: for each pair of proteins ( d, d  X  ), we first draw a topic z dd  X  for protein d from its own distribution  X  d over topics. Likewise, we also draw z d  X  d from  X  d  X  . Then the presence or absence of an interac-tion between d to d  X  is generated as a binary random variable from a Bernoulli distribution whose parameter  X  z specified by the topics sampled from the corresponding pro-teins for this particular interaction.
In this work, we extend this model to text by consid-ering documents as analogous to proteins. Thus for each pair of documents, we generate the presence or absence of a citation represented by a Bernoulli random variable. The parameter of this distribution depends on the latent topics sampled from each of these documents. Note that protein-protein interaction was modeled in [1] as a symmetric in-teraction. However, a citation is directional and hence in-herently asymmetric. To account for this, for each pair of documents ( d, d  X  ), we assign the directionality of the citation based on the time-stamps of the documents. For example, if d  X  is older than d , then we assume the citation is from d Table 3: Generative process for Pairwise Citation LDA to d  X  . In addition, if z dd  X  is the latent topic sampled from d for this interaction and z d  X  d is the corresponding topic from rameter used to generate the citation will be  X  z not  X  z asymmetric (  X  kk  X  6 =  X  kk  X  ), in order to capture the direction-ality of the citation.

The generative process for words remains same as that of LDA. Note that the document specific topic proportions  X  , used in generating the words, is same as the one used in generating links for that document. A more detailed descrip -tion of the generative process of the model is described in table 3. It is not possible to represent citations between al l pairs of documents in the plate notation, hence we present a simplified graphical representation in figure 2, in which we show citations from one set of documents (citing set) to the other (cited set). It is clear from figure 2 that topicality of the cited document and the citing document are explicitly made dependent through the V-structure at the variable c , which is observed. Figure 2: Graphical representation of the pairwise Cita-
The log-likelihood of the observed data with respect to this model is as follows:
We use the following mean-field variational approximation for the posterior distribution: Q (  X  , z ) = For reasons of space, we do not derive the steps involved in inference, but only present the final update equations below . The interested user may refer to [20, 4] for more details of the standard inference procedure. where  X () is the digamma function,  X  v ( w ) is the delta func-tion given by  X  v ( w ) = 1 if w = v and 0 otherwise.
In terms of implementation, first, we execute step (1) for each position in each document to compute the variational multinomial  X  . Then, for each document pair ( d, d  X  ), we run step (2) for both documents alternatively until the parame-ters  X  dd  X  and  X  d  X  d converge. Next we update  X  d ,  X  and  X  , using steps (3) through (5) by using the sufficient statistics computed in steps (1) and (2). This process is repeated in an outer loop until the lower bound on the log-likelihood of the entire training set converges.

The implementation of inference for citing documents (whic h we will use in our experiments on log-likelihood) is slightl y different. We follow steps (1), (2) and (3) as before, but we only update  X  and  X  for the citing documents, keeping the  X   X  X  for the cited documents fixed at the values learned during training. Also we skip steps (4) and (5) since, we are only performing inference.
The Pairwise Link-LDA model is a true generative model for text and citations, and is capable of modeling arbitrary link structure. However, since it requires explicit modeli ng of the presence or absence of links between each pair of doc-uments, it becomes prohibitively expensive to model large scale document collections. Hence, scalability is a big iss ue with this model.
As discussed above, Pairwise Link-LDA model is not very scalable. The Link-LDA model, on the other hand, models a citation as a multinomial sampling of the target document, and hence does not require comparing every pair of docu-ments. As a result, it is more scalable. However, as we noted in section 2, this model fails to model the topical de-pendence between the cited and citing documents explicitly . As a compromise, we propose a new Link-PLSA-LDA model that combines the best properties of these two models. The new model follows the multinomial sampling process for gen-erating citations, and thereby retains the scalability of t he Link-LDA model. At the same time, it also explicitly mod-els the topical dependence between the cited and the citing documents.
 In order to achieve this objective, the new Link-PLSA-LDA model makes a simplifying assumption that the link structure in the corpus is a bipartite graph with all links emerging from the set of citing documents and pointing to the set of cited documents. In other words, we assume each document can either be cited or be a citing document, but not both.
In this model, the generative process for the content and citations of the citing documents is the same as in Link-LDA. In addition, in order to explicitly model information flow from the citing document to the cited document, we de-fined an explicit generative process for the content of cited documents, that makes use of the same distribution  X  . In this new generative process, we view the set of cited doc-uments as bins that are to be filled with words. We first associate a topic mixing proportions  X  for the entire set of cited documents. Then words are filled into the bins N  X  times, where N  X  is the sum total of the document lengths of the set of cited documents, as follows: each time, we first sample a topic k from the mixing proportions  X  , then pick a bin d  X  from  X  k and fill a word occurrence from  X  into the bin. This process is exactly same as the symmetric a combination of PLSA for cited documents and Link-LDA for citing documents to jointly model content and hyper-links, we call this new model Link-PLSA-LDA.

The entire generative process is displayed step-by-step in table 4 and the corresponding graphical representation is shown in figure 3. One can see that dependencies prop-agate from the cited documents to the citing documents through the unobserved  X  , as per the d-separation principle in Bayesian networks [2].
The likelihood of the observed data in this model is given
P ( w, d ) = P z P ( w | z ) P ( d | z ) P ( z ) as opposed to the more common P ( w, d ) = P z P ( w | z ) P ( z | d ). Figure 3: Graphical representation of the Link-PLSA-Table 4: Generative process for the Link-PLSA-as follows.
 where w is the entire text of cited and citing documents and c is the set of hyperlinks/citations.

As in the case of the Pairwise Link-LDA model, we will employ the mean-field variational approximation for the pos -terior distribution of the latent variables as shown below. where d  X  n is the document index of the n th word in the cited set. Using standard variational inference procedure [20, 4 ], we arrive at the following update equations: These updates are performed iteratively in the same order as above, until convergence. Since the updates in steps (6) through (8) depend on each other, we also perform an inner iterative loop involving these equations, until they conve rge.
For performing inference only on the citing documents, we only iterate between steps (6) through (8) until convergenc e.
It is clear that one of the limitations of the model is the assumption of the bipartite link structure. This may ap-pear as a very restrictive assumption, but it can be easily overcome in practice (see section 4.1 for more details).
Also, the Link-PLSA-LDA model defines the topical dis-tribution for citations,  X  , over a fixed set of cited docu-ments. This means that new documents can only cite doc-uments within this fixed set. Hence this model is not fully generative, a weakness also shared by the PLSA and the Link-LDA models. We believe, in practice, it is not entirely unreasonable to assume that the set of cited documents is known at modeling time, and will not change. For exam-ple, the cited and citing documents could respectively cor-respond to previously published papers and currently sub-mitted ones in the scientific domain; or last month X  X  blog postings and current blog postings in a blog domain.
For our experiments, we used two different types of linked data: scientific literature from Citeseer containing citations, and blog data containing hyperlinks.

Both data sets exhibit arbitrary link structure, but since the Link-PLSA-LDA model can only handle bipartite link structure, we transformed our link structure to a bipartite graph. This will allow us to compare all the models on equal footing. The way it is done is as follows: we assign each document to one of two disjoint sets called the cited set or the citing set based on whether they contain incoming links or outgoing links respectively. If a document contains both types of links, we create a duplicate of the document and assign one copy to each set such that only the incoming links are stored in the cited set and only the outgoing links are stored in the citing set. This reduces the original link grap h to a bipartite one. In fact, this strategy has been successfu lly adopted by Dietz et al [7] in their work on modeling citation influences.

Since we made the graph bipartite, when we run the Pair-wise Link-LDA model on this data set, we do not consider the existence of links between all pairs of documents, but only between all pairs ( d  X  , d ) such that d  X  is in the cited set and d is in the citing set. This saves us some unnecessary computational effort.
This data is a pre-processed subset of the larger, pub-available by Lise Getoor X  X  research group at University of the vocabulary size is 3703 unique words. We pruned this collection to include only those documents that cite or are cited by at least two other documents. We did this so that the computational costs of running the Pairwise Link-LDA model remain within reasonable limits. This reduced the corpus size to 1168 documents, of which only 186 docu-ments (15.9%) have both incoming and outgoing links. We duplicated these documents to create a bipartite link struc -ture between a cited set of 591 documents and a set of 763 ments into 10 sets with 50-50 random splits for training and testing, to allow computation of variance.
The data set consists of 8,370,193 postings on the blogo-sphere collected by Nielsen Buzzmetrics 5 between 07/04/2005 and 07/24/2005. We processed this data set as follows. First, there are many postings that are mistakenly assigned their respective site-URLs as their permalinks. These non-unique identifiers make it difficult to disambiguate between their incoming hyperlinks. Hence, we filtered these posting s out, which left us with 7,177,951 postings. Next, we pruned this graph until we are left with postings, each of which has at least 2 outgoing or 2 incoming hyperlinks. We are finally left with 2,248 postings with at least 2 outgoing links each and 1,777 documents with at least two incoming links each. Of these only 68 postings (3.8%) have both incoming links and outgoing links, which we duplicated as described above, to create a bipartite graph.
 Next, we pre-processed and indexed these postings using Lemur 6 tool-kit employing the Krovetz stemmer and a stan-dard stop-word list. We pruned the vocabulary of this data further by ignoring words that contain numerals, that are less than 3 characters long, or those that occurred in less than 5 documents. The vocabulary size of the resulting cor-pus is 13,506. We split the set of citing postings uniformly at random into two equal sets (which we call set I and set II) of 1,124 postings each for training and testing purposes .
In this task, we measure how well the models predict unseen data in terms of log-likelihood. The higher log-likelihood the model assigns to unseen data, better is its predictive power and generalizability.

Note that the Link-PLSA-LDA model and the Link-LDA mod-els can only generate new citing documents, while the Pair-wise Link-LDA model has no such restriction. However, for comparison purposes, we restrict our experiments to mea-suring the likelihood of only unseen citing documents.
Our experimental set-up is as follows. We first train each model X  X  parameters using the entire set of cited postings and one of the training splits of the citing set. Using these estimated model parameters, we perform inference on the corresponding test set of the citing documents. Using these inferred variational parameters, we compute the variation al lower-bound on the cumulative log-likelihood of the citing test set (both text and citations included). We repeat this process for all the train-test splits of the data and report t he average values.
In this task, we use the learned model to predict hyper-links for documents that are not seen in the training phase. The Pairwise Link-LDA model can predict both incoming links as well as outgoing links for any new document, but the Link-PLSA-LDA and Link-LDA models can only predict outgoing links (onto a fixed set of cited documents) for new documents. Hence, for a fair comparison, for all models, we only predict outgoing links for new documents onto the set of cited documents.

Our experimental design is very similar to that of subsec-tion 4.2.1, but is described below for clarity. We first learn the parameters of each of the models using the entire set of cited documents and one of the training splits of the of cit-ing documents. Then, providing only the text of the citing documents from the corresponding test set, we performed inference to obtain the posterior topic distribution for ea ch citing document in this set using inference updates that use only text as evidence. For all the models, this would involve the following two equations, which are computed iterativel y until convergence. Using these probabilities and the model parameters learned during the training phase, we can compute the conditional probability of citation to any document in the cited set d  X   X  X  1 , , M  X  } given the content of the citing document w d . For the Link-LDA and Link-PLSA-LDA models, this probability would be as follows: For the Pairwise Link-LDA model, the probability of the existence of a citation is computed as follows: For each citing document, we use these conditional proba-bilities to rank the documents in the cited set. We measure the effectiveness of this ranking with respect to the ground truth, which is the set of actual citations of the citing docu -ment. Drawing analogy to an information retrieval scenario , we assume each citing document to be a query and the set of its true citations to be the set of relevant documents, and the set of all documents in the cited set to be the retrieved set. One of the standard metrics used in information re-trieval to evaluate the quality of a ranked list against a tru e set of relevant documents is average precision. However, we believe this metric is not suited for the task of link predic-tion in blog domain for two reasons: (i) this metric assumes that the true set is exhaustive, i.e., we have the complete set of relevant documents and (ii) the metric assigns high importance to precision at the top of the ranked list. While this may be appropriate for a key-word based search engine, the scenario for citations is quite different: citations are not assigned to all topically relevant documents, but only to a few documents known to the author. Hence the set of true citations does not represent an exhaustive set of topically relevant documents and it does not make sense to assign high importance to the top of the ranked list. Instead, we should focus on how well the model ranks the true citations; in other words, the measure should be more recall oriented. Hence, we use the value of the rank at 100%-recall as our evaluation measure. In short, we call this RKL , an abbrevia-tion for RanK of the Last relevant document. This measure looks at how high in the ranked list the model places all the true citations. Higher the rank (lower in terms of numerical value), the better is the performance of the model.
Note that there are other models for link prediction avail-able in the literature [12, 19, 17, 18], but we limit our com-parison to Link-LDA , Pairwise Link-LDA and Link-PLSA-LDA models, since they are our main interest in this paper.
In the learning phase, for all the models, we terminate the outer iterations if the fractional decrease in the lower bound of the log-likelihood of the entire observed data, in of iterations exceeds than 100.

For the inner iterations involving variational parameters , the stopping condition is when the fractional decrease in the lower bound of the log-likelihood of the data involved two successive iterations, or when the number of iterations exceeds 20. The same criteria are also applied at inference time, for all the models.

Lastly, for all the models, we could technically estimate the value of  X   X  using an empirical Bayes technique, but we fixed it at 0 . 1 for simplicity.
For the visualization of Pairwise Link-LDA model, we ran a 10 topic model on the cited set and citing set I of the blog corpus. We selected 4 representative topics from the output and displayed in figure 4 the top words using the learned  X  values and also the  X  kk  X  values in the form of edges, which in-dicate the likelihood of a citation from topic k to topic k  X  . As the figure indicates, the new model not only displays the con-tents of topics, but also the citation strength between them . For example, the figure indicates that within-topic citatio n probability is very high (indicated by the self-pointing ar -rows). In addition, we see some interesting patterns, such a s strong citation probability between  X  X raq War X  and X  X ondon Bombings X  (the discourse of the two topics is very similar),  X  X raq War X  and  X  X upreme Court Nominations X (both are of interest to the community of conservative bloggers). Also, there is almost zero citation probability between  X  X ondon Bombings X  and  X  X IA Leak X  (indicated by the absence of an edge), which is not surprising, because they are completely unrelated. Figure 4: Visualization of the Pairwise Link-
We ran the Link-PLSA-LDA model on set I of the citing postings and the cited postings with the number of topics K fixed at 25. We displayed 2 salient topics discovered by the Link-PLSA-LDA model in table 5. Like the previous model above, Link-PLSA-LDA tells us the most likely terms in each topic. For example, in the  X  X IA leak X  topic, the model rightly identifies  X  X arl X ,  X  X ove X ,  X  X ush X ,  X  X lame X   X  X ia X  an d  X  X eak X  as key entities in the topic. The name  X  X ooper X  in the list refers to Matt Cooper , a reporter for the Time magazine, who Table 5: Visualization of the Link-PLSA-LDA model: testified in the CIA leak case. Similarly, the top terms in other topic are also equally illustrative of the topic conte nt.
In addition, through the parameter  X  kd  X  , Link-PLSA-LDA also tells us the blog postings that are most influential in a topic k , as measured by both hyperlinks as well as by con-tent. The most influential blogs for each topic are displayed at the bottom of table 5. As some of the titles of these blogs indicate, they seem topically very relevant. The blog s for the first topic are clearly political blogs. The second topic,  X  X earch Engine Market X , has all technology related blogs. The  X  X IA leak X  topic has a mix of orientations ( bill-mon and tom tomorrow are Democratic blogs, the others are Republican), hence the topic is most likely a mixture of argumentation back and forth between Democrats and Republicans.

The topic specific statistics described here are also learne d by the Link-LDA model. There is, however, an additional statistic that the Link-PLSA-LDA model learns that is not directly learned by the Link-LDA model, namely the impor-tance of topics (in terms of its occurrence in the set of cited postings), as measured by the parameter  X  . In table 5, we display the importance of each topic below its title. The numbers indicate that the  X  X IA-leak X  topic is more popular than the  X  X earch-Engine Market X  topic as far as this corpus is concerned.
Figure 5 compares the performance of the three models on log-likelihood evaluation (cf. section 4.2.1) on the Cites eer data, averaged over 10 runs. It is clear that both the new models are significantly better than the the Link-LDA base-line. Clearly, additional information of topical similari ty between the text of the documents on either side of a link, leveraged by these models, is helping them predict new data better. Between the two models, it is apparent that Link-PLSA-LDA is clearly the better performer.
 Surprisingly, the likelihood values for the Pairwise Link-LDA model decrease with increasing number of topics. We believe this is due to the fact that Pairwise Link-LDA con-verges much slower than the other models due to its larger number of variational parameters. Hence the stopping cri-terion we used (cf. section 4.3) does not allow the model to reach its optimum state. Hence, it is unable to predict the test data as well as it should, at a higher number of topics. Despite this fact, it is appreciable that its performance is significantly better than that of the Link-LDA model. Figure 5: Likelihood performance of the three models
We performed a similar evaluation on the blog data as well, but without the Pairwise Link-LDA model this time. The blog data is a larger corpus, and it takes exceedingly long time for this model to converge on this data, especially for higher number of topics. Hence we ignored it in these experiments.

In figure 6, we plotted the cumulative log-likelihood values for the Link-LDA and Link-PLSA-LDA models as a function of number of topics, using cross validation on the two sets of citing postings (cf. section 4.1.2). The plot again clear ly shows that Link-PLSA-LDA predicts the data much better than the Link-LDA model.
Figure 6: log-likelihood of blog data: higher is better
Figure 7 compares the RKL performance of the three mod-els as a function of number of topics K , on the Citeseer data, averaged over 10 runs. Again, Link-PLSA-LDA significantly outperforms the other models. However, Link-LDA signifi-cantly outperforms Pairwise Link-LDA this time. Since link prediction and log-likelihood have different objective fun c-tions, there is no reason to believe that models that do well on one task should perform well on the other too. Figure 8 compares the performance of Link-PLSA-LDA with Link-LDA as a function of number of topics K , on the blog Figure 7: RKL (Rank at 100% recall) performance com-data, averaged over 2 runs. Again, we did not use the Pairwise Link-LDA model due to scalability issues. As was the case earlier, Link-PLSA-LDA again significantly outper -forms Link-LDA at all values of K . Further, the perfor-mance only gets better as the number of topics is increased from 10 to 100. Figure 8: RKL (Rank at 100% recall) performance com-
In this paper, we presented two novel topic models for joint modeling of links and text. We introduce the Pair-wise Link-LDA for the first time, but limited preliminary results for Link-PLSA-LDA were presented recently in [14]. This work focused only on the single domain of blog data, and compared the Link-PLSA-LDA model to only the Link-LDA baseline. In this paper, we extend this work by per-forming more rigorous experiments on an additional data set and also by comparing the model to Pairwise Link-LDA , a new truly generative model for text citations.
 Although the Pairwise Link-LDA is more expressive than Link-PLSA-LDA in terms of modeling arbitrary link struc-ture, our experiments on Citeseer data and blog data show that the Link-PLSA-LDA model outperforms the former on both log-likelihood and link prediction. We believe the su-perior performance of the multinomial based Link-PLSA-LDA model compared to the multiple Bernoulli based Pair-wise Link-LDA model has interesting parallels with the ex-perience of Multinomial vs. Bernoulli distributions for te xt classification. It has been observed by McCallum et al [13] that modeling a document in terms of words that occur in the document using a multinomial distribution rather than in terms of presence and absence of all the words in the vo-cabulary using a multiple Bernoullis yields superior perfo r-mance. One of the reasons for this behavior could be because features that are present are much more important than fea-tures that are absent. The same reasoning could be extended to the behavior of the two models in the present context. Our hypothesis is that the Pairwise Link-LDA model suf-fers to due to significant expenditure of modeling effort on citations that are absent.

Apart from its superior performance, another attractive feature of the Link-PLSA-LDA model is its relative scal-ability to large document collections in comparison to the Pairwise Link-LDA model. To demonstrate its scalability, in figure 9, we plotted the average variational EM iteration (outer loop) run time for each model, as a function of num-ber of topics. The measurements were made on a standard Linux machine with Intel Pentium III 0.7 GHz processor and 4GB memory. Clearly, there is a huge gap in computational time between Pairwise Link-LDA and the other two mod-els. The run times of Link-PLSA-LDA and Link-LDA are comparable, but the former is the faster. The main reason for the prohibitive costs of the Pairwise Link-LDA model is that its computational complexity is quadratic in both the number of documents as well as number of topics. On the other hand, Link-PLSA-LDA and Link-LDA models are both linear in the number of documents and number of top-ics, making them more scalable. Figure 9: Comparison of average run times of the three Despite the superior performance and scalability of the Link-PLSA-LDA model, we believe the Pairwise Link-LDA mode l is still very desirable owing to its superior semantics. Its generative process not only captures arbitrary link struc-ture, but allows us to generate new cited as well as citing documents. Hence it may be more attractive in situations where capturing the true link structure is more important.
As part of our future work, we will try to build more tractable approximations to the Pairwise Link-LDA model. non-scalability and restrictive assumptions respectivel y. [1] M. Airodi, D. Blei, E. Xing, and S. Fienberg. Mixed [2] C. M. Bishop. Pattern Recognition and Machine [3] D. Blei and J. Lafferty. Correlated topic models. In [4] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet [5] D. M. Blei and J. D. Lafferty. Dynamic topic models. [6] D. Cohn and T. Hofmann. The missing link -a [7] L. Dietz, S. Bickel, and T. Scheffer. Unsupervised [8] E. Erosheva, S. Fienberg, and J. Lafferty.
 [9] T. Hoffman. Probabilistic Latent Semantic Analysis. [10] J. M. Kleinberg. Authoritative sources in a [11] W. Li and A. McCallum. Pachinko allocation: [12] D. Liben-Nowell and J. Kleinberg. The link prediction [13] A. McCallum and K. Nigam. A comparison of event [14] R. Nallapati and W. Cohen. Link-LDA-PLSA: a new [15] R. Nallapati, J. Lafferty, W. Cohen, K. Ung, and [16] L. Page, S. Brin, R. Motwani, and T. Winograd. The [17] B. Shaparenko and T. Joachims. Information [18] T. Strohman, W. B. Croft, and D. Jensen.
 [19] B. Taskar, Ming-FaiWong, P. Abbeel, and D. Koller. [20] M. Wainwright and M. Jordan. Graphical models,
