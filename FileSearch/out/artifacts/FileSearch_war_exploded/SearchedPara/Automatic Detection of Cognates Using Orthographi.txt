 Cognates are words in different languages having the same etymology and a common ancestor. In-vestigating pairs of cognates is very useful in his-torical and comparative linguistics, in the study of language relatedness (Ng et al., 2010), phy-logenetic inference (Atkinson et al., 2005) and in identifying how and to what extent languages change over time. In other several research ar-eas, such as language acquisition, bilingual word recognition (Dijkstra et al., 2012), corpus lin-guistics (Simard et al., 1992), cross-lingual infor-mation retrieval (Buckley et al., 1997) and ma-chine translation (Kondrak et al., 2003), the con-dition of common etymology is usually not essen-tial and cognates are regarded as words with high cross-lingual meaning and orthographic or pho-netic similarity.

The wide range of applications in which cog-nates prove useful attracted more and more at-tention on methods for detecting such related pairs of words. This task is most challenging for resource-poor languages, for which etymologi-cally related information is not accessible. There-fore, the research (Inkpen et al., 2005; Mulloni and Pekar, 2006; Hauer and Kondrak, 2011) focused on automatic identification of cognate pairs, start-ing from lists of known cognates.

In this paper, we propose a method for automat-ically determining pairs of cognates across lan-guages. The proposed method requires a list of known cognates and, for languages for which ad-ditional linguistic information is available, it can be customized to integrate historical information regarding the evolution of the language. The rest of the paper is organized as follows: in Section 2 we present and analyze alternative methods and related work in this area. In Section 3 we intro-duce our approach for detection of cognates us-ing orthographic alignment. In Section 4 we de-scribe the experiments we conduct and we report and analyze the results, together with a compari-son with previous methods. Finally, in Section 5 we draw the conclusions of our study and describe our plans for extending the method. There are three important aspects widely investi-gated in the task of cognate identification: seman-tic, phonetic and orthographic similarity. They were employed both individually (Simard et al., 1992; Inkpen et al., 2005; Church, 1993) and com-bined (Kondrak, 2004; Steiner et al., 2011) in or-der to detect pairs of cognates across languages. For determining semantic similarity, external lexi-cal resources, such as WordNet (Fellbaum, 1998), or large corpora, might be necessary. For measur-ing phonetic and orthographic proximity of cog-nate candidates, string similarity metrics can be applied, using the phonetic or orthographic word forms as input. Various measures were investi-gated and compared (Inkpen et al., 2005; Hall and Klein, 2010); Levenshtein distance (Levenshtein, 1965), XDice (Brew and McKelvie, 1996) and the longest common subsequence ratio (Melamed, 1995) are among the most frequently used metrics in this field. Gomes and Lopes (2011) proposed SpSim, a more complex method for computing the similarity of cognate pairs which tolerates learned transitions between words.

Algorithms for string alignment were success-fully used for identifying cognates based on both their forms, orthographic and phonetic. Delmestri and Cristianini (2010) used basic sequence align-ment algorithms (Needleman and Wunsch, 1970; Smith and Waterman, 1981; Gotoh, 1982) to ob-tain orthographic alignment scores for cognate candidates. Kondrak (2000) developed the ALINE system, which aligns words X  phonetic transcrip-tions based on multiple phonetic features and com-putes similarity scores using dynamic program-ming. List (2012) proposed a framework for au-tomatic detection of cognate pairs, LexStat, which combines different approaches to sequence com-parison and alignment derived from those used in historical linguistics and evolutionary biology.
The changes undergone by words when enter-ing from one language into another and the trans-formation rules they follow have been successfully employed in various approaches to cognate detec-tion (Koehn and Knight, 2000; Mulloni and Pekar, 2006; Navlea and Todirascu, 2011). These ortho-graphic changes have also been used in cognate production, which is closely related to the task of cognate detection, but has not yet been as inten-sively studied. While the purpose of cognate de-tection is to determine whether two given words form a cognate pair, the aim of cognate produc-tion is, given a word in a source language, to automatically produce its cognate pair in a tar-get language. Beinborn et al. (2013) proposed a method for cognate production relying on statis-tical character-based machine translation, learn-ing orthographic production patterns, and Mul-loni (2007) introduced an algorithm for cognate production based on edit distance alignment and the identification of orthographic cues when words enter a new language. Although there are multiple aspects that are rel-evant in the study of language relatedness, such as orthographic, phonetic, syntactic and semantic differences, in this paper we focus only on lexical evidence. The orthographic approach relies on the idea that sound changes leave traces in the orthog-raphy and alphabetic character correspondences represent, to a fairly large extent, sound correspon-dences (Delmestri and Cristianini, 2010).

Words undergo various changes when entering new languages. We assume that rules for adapting foreign words to the orthographic system of the target languages might not have been very well defined in their period of early development, but they may have since become complex and proba-bly language-specific. Detecting pairs of cognates based on etymology is useful and reliable, but, for resource-poor languages, methods which require less linguistic knowledge might be necessary. Ac-cording to Gusfield (1997), an edit transcript (rep-resenting the conversion of one string to another) and an alignment are mathematically equivalent ways of describing relationships between strings. Therefore, because the edit distance was widely used in this research area and produced good re-sults, we are encouraged to employ orthographic alignment for identifying pairs of cognates, not only to compute similarity scores, as was previ-ously done, but to use aligned subsequences as features for machine learning algorithms. Our in-tuition is that inferring language-specific rules for aligning words will lead to better performance in the task of cognate identification. 3.1 Orthographic Alignment String alignment is closely related to the task of sequence alignment in computational biology. Therefore, to align pairs of words we employ the Needleman-Wunsch global alignment algorithm (Needleman and Wunsch, 1970), which is mainly used for aligning sequences of proteins or nu-cleotides. Global sequence alignment aims at de-termining the best alignment over the entire length of the input sequences. The algorithm uses dy-namic programming and, thus, guarantees to find the optimal alignment. Its main idea is that any partial path of the alignment along the optimal path should be the optimal path leading up to that point. Therefore, the optimal path can be deter-mined by incremental extension of the optimal subpaths (Schuler, 2002). For orthographic align-ment, we consider words as input sequences and we use a very simple substitution matrix, which gives equal scores to all substitutions, disregard-ing diacritics (e.g., we ensure that e and ` e are matched). 3.2 Feature Extraction Using aligned pairs of words as input, we extract features around mismatches in the alignments. There are three types of mismatches, correspond-ing to the following operations: insertion, deletion and substitution. For example, for the Romanian word exhaustiv and its Italian cognate pair esaus-tivo , the alignment is as follows:
The first mismatch (between x and s ) is caused by a substitution, the second mismatch (between h and -) is caused by a deletion from source lan-guage to target language, and the third mismatch (between -and o ) is caused by an insertion from source language to target language. The features we use are character n -grams around mismatches. We experiment with two types of features: i) n -grams around gaps, i.e., we account only ii) n -grams around any type of mismatch, i.e.,
The second alternative leads to better perfor-mance, so we account for all mismatches. As for the length of the grams, we experiment with n  X  X  1 , 2 , 3 } . We achieve slight improvements by combining n -grams, i.e., for a given n , we use all i -grams, where i  X  { 1 ,...,n } . In order to provide information regarding the position of the features, we mark the beginning and the end of the word with a $ symbol. Thus, for the above-mentioned pair of cognates, (exhaustiv, esaustivo) , we extract the following features when n = 2 : For identical features we account only once. Therefore, because there is one feature ( xh&gt;s-) which occurs twice in our example, we have 8 fea-tures for the pair (exhaustiv, esaustivo) . 3.3 Learning Algorithms We use Naive Bayes as a baseline and we exper-iment with Support Vector Machines (SVMs) to learn orthographic changes and to discriminate be-tween pairs of cognates and non-cognates. We put our system together using the Weka work-bench (Hall et al., 2009), a suite of machine learn-ing algorithms and tools. For SVM, we use the wrapper provided by Weka for LibSVM (Chang and Lin, 2011). We use the radial basis function kernel (RBF), which can handle the case when the relation between class labels and attributes is non-linear, as it maps samples non-linearly into a higher dimensional space. Given two instances x i and x j , where x i  X  R n , the RBF kernel function for x i and x j is defined as follows: where  X  is a kernel parameter.

We split the data in two subsets, for training and testing, with a 3:1 ratio, and we perform grid search and 3-fold cross validation over the train-ing set in order to optimize hyperparameters c and  X  . We search over { 1 , 2 ,..., 10 } for c and which optimize accuracy on the training set are re-ported, for each pair of languages, in Table 3. 4.1 Data We apply our method on an automatically ex-tracted dataset of cognates for four pairs of languages: Romanian-French, Romanian-Italian, Romanian-Spanish and Romanian-Portuguese. In order to build the dataset, we apply the method-ology proposed by Ciobanu and Dinu (2014) on Romanian. We discard pairs of words for which the forms across languages are identical (i.e., the Romanian word matrice and its Italian cognate pair matrice , having the same form), because these pairs do not provide any orthographic changes to be learned. For each pair of languages we de-termine a number of non-cognate pairs equal to the number of cognate pairs. Finally, we ob-3,477 for Romanian-Italian, 5,113 for Romanian-Spanish and 7,858 for Romanian-Portuguese. Be-cause we need sets of approximately equal size for IT iu &gt; io un &gt; on l-&gt; le t$ &gt; -$ -$ &gt; e$ FR un &gt; on ne &gt; n-iu &gt; io t  X i &gt; ti e$ &gt; -$ Table 1: The most relevant orthographic cues for each pair of languages determined on the entire datasets using the  X  2 attribute evaluation method implemented in Weka. IT -$ &gt; e$ -$ &gt; o$  X  a$ &gt; a$  X  &gt; re t  X i &gt; zi FR e$ &gt; -$ un &gt; on ne &gt; n-iu &gt; io t  X i &gt; ti ES -$ &gt; o$ e$ &gt; -$ t  X i &gt; ci  X  a$ &gt; a$ at &gt; ad PT -$ &gt; o$  X  a$ &gt; a$ e$ &gt; -$ -$ &gt; r$ -$ &gt; a$ Table 2: The most frequent orthographic cues for each pair of languages determined on the cognate lists using the raw frequencies. comparison across languages, we keep 400 pairs of cognates and 400 pairs of non-cognates for each pair of languages. In Tables 1 and 2 we provide, for each pair of languages, the five most relevant 2-gram orthographic changes, determined using the  X  2 distribution implemented in Weka, and the five most frequent 2-gram orthographic changes in top ranked orthographic cues occurs at the begin-ning of the word, while many of them occur at the end of the word. The most frequent operation in Tables 1 and 2 is substitution. 4.2 Results Analysis We propose a method for automatic detection of cognate pairs using orthographic alignment. We experiment with two machine-learning ap-proaches: Naive Bayes and SVM. In Table 3 we report the results of our research. We report the n -gram values for which the best results are ob-tained and the hyperparameters for SVM, c and  X  . The best results are obtained for French and Span-ish, while the lowest accuracy is obtained for Por-tuguese. The SVM produces better results for all languages except Portuguese, where the accuracy is equal. For Portuguese, both Naive Bayes and SVM misclassify more non-cognates as cognates than viceversa. A possible explanation might be the occurrence, in the dataset, of more remotely related words, which are not labeled as cognates. We plan to investigate this assumption and to ap-ply the proposed method on other datasets in our future work. 4.3 Comparison with Previous Methods We investigate the performance of the method we propose in comparison to previous approaches for automatic detection of cognate pairs based on or-thographic similarity. We employ several ortho-graphic metrics widely used in this research area: the edit distance (Levenshtein, 1965), the longest common subsequence ratio (Melamed, 1995) and In addition, we use SpSim (Gomes and Lopes, 2011), which outperformed the longest common subsequence ratio and a similarity measure based on the edit distance in previous experiments. To evaluate these metrics on our dataset, we use the same train/test sets as we did in our previous ex-periments and we follow the strategy described in (Inkpen et al., 2005). First, we compute the pair-wise distances between pairs of words for each orthographic metric individually, as a single fea-criminating between cognates and non-cognates, we run a decision stump classifier (provided by Weka) on the training set for each pair of lan-guages and for each metric. A decision stump is a decision tree classifier with only one internal node and two leaves corresponding to our two class la-bels. Using the best threshold value selected for each metric and pair of languages, we further clas-sify the pairs of words in our test sets as cognates or non-cognates. In Table 4 we report the results for each approach. Our method performs better than the orthographic metrics considered as indi-vidual features. Out of the four similarity met-rics, SpSim obtains, overall, the best performance. These results support the relevance of accounting for orthographic cues in cognate identification. In this paper we proposed a method for automatic detection of cognates based on orthographic align-ment. We employed the Needleman-Wunsch al-gorithm (Needleman and Wunsch, 1970) for se-quence alignment widely-used in computational biology and we used aligned pairs of words to extract rules for lexical changes occurring when words enter new languages. We applied our method on an automatically extracted dataset of cognates for four pairs of languages.

As future work, we plan to extend our method on a few levels. In this paper we used a very simple substitution matrix for the alignment algo-rithm, but the method can be adapted to integrate historical information regarding language evolu-tion. The substitution matrix for the alignment al-gorithm can be customized with language-specific information, in order to reflect the probability of a character to change into another. An important achievement in this direction belongs to Delmestri and Cristianini (2010), who introduced PAM-like matrices, linguistic-inspired substitution matrices which are based on information regarding ortho-graphic changes. We plan to investigate the con-tribution of using this type of substitution matrices for our method.

We intend to investigate other approaches to string alignment, such as local alignment (Smith and Waterman, 1981), and other learning algo-rithms for discriminating between cognates and non-cognates. We plan to extend our analysis with more language-specific features, where linguistic knowledge is available. First, we intend to use the part of speech as an additional feature. We assume that some orthographic changes are dependent on the part of speech of the words. Secondly, we want to investigate whether accounting for the common ancestor language influences the results. We are interested to find out if the orthographic rules de-pend on the source language, or if they are rather specific to the target language. Finally, we plan to make a performance comparison on cognate pairs versus word-etymon pairs and to investigate false friends (Nakov et al., 2007).

We further intend to adapt our method for cog-nate detection to a closely related task, namely cognate production, i.e., given an input word w , a related language L and a set of learned rules for orthographic changes, to produce the cognate pair of w in L .
 We thank the anonymous reviewers for their help-ful and constructive comments. The contribution of the authors to this paper is equal. Research sup-ported by CNCS UEFISCDI, project number PN-II-ID-PCE-2011-3-0959.
