 1. Introduction context, or do it at a small extent. By using traditional search engines, some problems may arise due to the fact that search results are the same for a same query formulated by different users in a same location. It is instead plausible to assume that users do not have the same interests and preferences; thus a different and more context focused results selection and rank-ing for a same query formulated by distinct users would be desirable. To overcome this limitation personalized approaches to Information Retrieval have been proposed ( Liu, Yu, &amp; Meng, 2004 ). Personalized search is based both on modeling the user context by a user profile that represents the user preferences, and on the definition of processes that exploit the knowledge represented in the user profile to tailor the search outcome to the user X  X  needs ( Pasi, 2010 ).
 ambiguating word senses in short queries (generally only 2 X 3 terms per query are specified by users). For example, a user who is a musician interested in the purchase of an economic flute would expect in response to such a query only Web pages related to the musical instrument, and not those related to the glass.
A user profile represents the user interests and preferences; these can be captured either explicitly or implicitly ( Agich-proach, users proactively communicate useful information to the system, e.g., by compiling questionnaires. Instead, by the implicit approach, users interests are automatically inferred based on the analysis of the actions they perform, for example, during Web search (e.g., how users move from one Web page to another, or which documents they download, etc.). The accurate definition of a user profile plays a central role in the effectiveness of any approach to personalization. Only if a user profile represents faithful information about the user it is possible to successfully exploit it to improve the search outcome. To the aim of defining a user profile the following activities are usually undertaken: (1) individuating and collecting the knowledge related to the user X  X  preferences and interests, (2) selecting a formal language to represent the collected knowledge, and (3) defining a strategy for updating the profile. Bags of words, vectors and conceptual taxo-nomies, generally defined based on the use of external knowledge resources (i.e., WordNet Degemmis, Lops, &amp; Semeraro, 2007 , or ODP Sieg, Mobasher, &amp; Burke, 2007 ) have been mainly employed as users profiles. Ontologies have been more re-cently considered as a powerful expressive means for representing users profiles; the advantage offered by ontological lan-guages is that they allow a more structured and expressive knowledge representation with respect to the above mentioned approaches ( Staab &amp; Studer, 2004 ).

In this paper an approach to the definition of a  X  X  X ersonal ontology X  X  (a user profile represented as an ontology) is pro-posed. The core of this approach is a knowledge extraction process that, based on some information related to the user inter-ests (either explicitly or implicitly collected), integrates it with additional knowledge extracted from the general purpose ontology YAGO ( Suchanek, Kasneci, &amp; Weikum, 2007, 2008 ). YAGO consists of several million of entities and facts; to the purpose of integrating in a more structured and semantically meaningful way the collected user information, the proposed knowledge extraction process identifies in YAGO only the entities and facts that match one (or more) set (s) of starting  X  X  X eeds X  X  constituted by one (or more) bag of words. Each bag of words is related to a specific user interest, so as the proposed knowledge extraction process is able to extract from YAGO all the chunks of knowledge that better represent the user interests.

With respect to the preliminary ideas presented in Calegari and Pasi (2011) , the research reported in this paper defines a novel and complete extraction process. Furthermore, in this paper the effectiveness of the proposed methodology is assessed by two types of evaluations: qualitative and quantitative. The performed qualitative evaluations are aimed at measuring the amount of noisy knowledge collected by the proposed extraction process; in fact, the addressed task is not simple as noisy information may be gathered. The quantitative evaluations are instead aimed to assess the topological properties of the de-fined ontology; they are based on metrics proposed in the social network analysis context.

The paper is organized as follows: Section 2 summarizes the related literature; Section 3 addresses the issue of how per-sonal ontologies can be constructed by using a general knowledge base. Section 4 introduces and motivates the proposed method, while in Section 5 the proposed knowledge extraction process is presented. Finally, in Section 6 the performed eval-uations are discussed. 2. Related work
It is worth noting that the approach proposed in this paper is strictly related to important issues addressed in the liter-ature, such as ontological engineering ( Su X rez-Figueroa, G X mez-P X rez, Motta, &amp; Gangemi, 2012 ), and context representation Fergerson, &amp; Musen, 2000 ) that support ontological engineering ( Su X rez-Figueroa et al., 2012 ) have been defined in order to design and code the knowledge represented in an ontology. Ontological engineering addresses several tasks such as ontolog-activities related to ontological engineering are often accomplished by domain experts; as the number of ontologies on the Web is increasing, it is not always possible to have experts available to re-adapt a large ontology according to the user X  X  tasks. To manage large ontologies in an automatic way is a key issue in any ontological engineering activity. The approach presented in this paper provides a possible solution for both the customization and the summarization tasks as it is able to automatically define a personal ontology by extracting the portion of YAGO related to the user interests. Thus, a personal ontology offers a formal representation of the topical user context.

In this section, two main issues are addressed: (1) which formal strategies can be used to represent the knowledge in user profiles, and (2) which strategies have been proposed in the literature to define user profiles based on ontologies. Finally, the most significant research works that make use of the YAGO ontology are briefly presented to justify the choice of this ref-erence knowledge.

User profiles are generally represented as sets of weighted keywords, semantic networks or hierarchies of concepts ( Gauch, Speretta, Chandramouli, &amp; Micarelli, 2007 ). Each formal representation makes use of different knowledge granules, thus affecting the personalized approaches that will make use of such profiles. Keyword based profiles offer the simplest way to represent the user interests as a set of keywords. To each keyword a weight can be assigned, which expresses the signif-icance of the keyword as a representative of the profile contents ( Montebello, Gray, &amp; Hurley, 1998; Chen &amp; Sycara, 1998;
Sorensen &amp; Mcelligot, 1995 ). One of the main drawbacks of keyword-based representations is related to the problem of term polysemy ( Gauch et al., 2007 ). To overcome this limitation semantic networks have been used to represent user profiles, tic network contains a set of unlinked nodes, and as soon as more information about the user is gathered the profile is en-riched with new weighted keywords that are associated with the right concept-nodes. In Magnini and Strapparava (2001) and Semeraro, Lops, and Degemmis (2005) the use of semantic networks has proved to be not sufficient to solve the poly-semy problem. WordNet has been adopted as an external knowledge resource to enrich the semantics of the concept-nodes with synonyms.
 that represent the user interests and preferences. Generally, predefined structures such as hierarchies of concepts are used: interests are mapped to the corresponding categories ( Ma, Pant, &amp;Sheng,Shen ). Although several works refer to the ODP as an the ODP is instead a taxonomy of concepts, where each concept identifies a category, and the leaves are Web pages. In addition to the subsumption relation the ODP also defines  X  X  X ymbolic X  X  and  X  X  X elated X  X  relations. Thus, the ODP can be considered at most as a light-ontology ( Mizoguchi, 2004 ).
 the above approaches. Furthermore, they allow a more expressive representation of the information represented in a profile; in fact, several relations can be defined (not only the subsumption one), thus helping to reduce term polysemy and ambiguity ( Staab &amp; Studer, 2004 ). In the literature several approaches have been proposed to define user profiles represented by ontol-ogies. The existing models to learn user profiles based on ontologies include approaches mainly relying on data mining or fuzzy relational algebra, and approaches that make use of external knowledge resources.
 evant terms are extracted by using basic text processing techniques (such as stop-word removal and stemming). Pattern rec-ognition techniques are then applied to group the obtained terms, then groups are linked by association rules. In Zhong (2002) a large repository of information on users topical interests is examined to define a related domain ontology for each of the user interests. The definition of a domain ontology is performed in several steps: (1) a probabilistic classifier is pro-posed to semi-automatically classify documents related to a same topic (user interest), (2) relevant terms from documents are extracted with standard text analysis techniques (3) a conceptual relationship analysis is performed to calculate a weight that indicates the strength of correlation of two terms, and (4) a variant of the Hopfield network is used to link terms. In
Mylonas, Vallet, Castells, Fernandez, and Avrithis (2008) an ontological user profile is built by a formal methodology founded on fuzzy relational algebra ( Klir &amp; Yuan, 1995; Zadeh, 1965 ). This formal methodology allows to extract the relevant con-cepts from the user interests (documents or interests directly specified by the user), and to link them by relations, the semantics of which is specified by domain experts.

Boughanem (2009) and Daoud et al. (2010) a short term user profile is built by aggregating graph-based query profiles re-lated to a same search session. The initial query context is defined as a set of keywords extracted from the user X  X  documents; it is then mapped to the reference ontology to build an initial weighted concept set. In a Web search session, with each user X  X  query a new weighted concept set is associated and added to the previous one. The terms extracted from the user X  X  queries are associated with the concepts of the ODP. Then, the obtained profile is represented as a weighted graph of semantically related concepts in the ODP. In Speretta and Gauch (2009) a Web based system (named Miology) is presented, which allows to define personal ontologies from the analysis of Web documents based only on domain ontologies. A user has to validate the acquired knowledge, for example by eliminating the noisy terms. Moreover, as indicated by the authors in Speretta and
Gauch (2009) , a general purpose ontology that covers all possible user personal interests is preferred with respect to the use of several domain ontologies. In fact, it is not possible to know a priori what a user interest is; this implies to have an ontol-ogy repository where not necessarily the domain ontology associated with a specific user interest is stored.
 from some information related to the user (documents, Web pages, emails), with the knowledge extracted from the Library of Congress Subjects Headings (LCSHs) via user interaction. The ontology X  X  skeleton is given by the concepts classified by the users (either positive or negative), and by the relations that link such elements in LCSH. A methodology is defined to add the knowledge mined by the analysis of the user X  X  information into an ontology initially defined by the user.
 purpose ontology YAGO is presented. In the literature, YAGO has been employed to leverage several tasks, such as informa-tion extraction, search results categorization, and semantic search ( Suchanek, Kasneci, &amp; Weikum, 2008 ). In Ren, Du, and
Wang (2009) YAGO is used to categorize search results into single categories (single label classification) of the ontology. Gi-ven a user X  X  query, the objective is to provide the appropriate categories to classify search results. This way a user is helped in locating the Web results by navigating the obtained category list extracted from YAGO.
YAGO has also been used as a knowledge base for the semantic search engines ESTER ( Bast, Chitea, Suchanek, &amp; Weber, 2007 ), and NAGA ( Kasneci, Suchanek, Ifrim, Ramanath, &amp; Weikum, 2008 ). ESTER combines full text search and ontological search, and it is composed of a query engine and of an entity recognizer. Even if a specific syntax has been defined to for-mulate queries, the authors show how ESTER can process basic SPARQL queries. YAGO is involved in the ESTER X  X  entity rec-ognizer component by assigning YAGO X  X  entities to words or phrases in the text collection. This allows ESTER to deliver hybrid answers that incorporate both data from the text and from the ontology. NAGA searches the sub-parts of YAGO that match the user X  X  query, and these portions of knowledge are the results obtained by the user as a response. In Dudev, Elbassuoni, Luxenburger, Ramanath, and Weikum (2008) the authors have proposed an approach to personalize the user X  X  search in NAGA. The objective is to define a personalized scoring that makes use of the user interests. A user profile is man-ually defined by the user that explores the YAGO knowledge with NAGA to select his/her interests from the obtained results (i.e., the related sub-portions of YAGO). To facilitate this phase, an ad hoc interface has been developed, but this limits the interoperability of the approach as a user can interact with the system only by formulating queries with a specific query language. 3. From a global knowledge representation to a personal ontology
As previously outlined, the user profile plays a key role in personalized search. A user profile represents an excerpt of the knowledge related to a user, such as personal data, background knowledge, topical preferences, etc. It then describes various user X  X  knowledge dimensions offering an insight of the user context. The process of generating a user profile based on an external knowledge resource involves three distinct steps. The first step consists in selecting a knowledge base (e.g., Word-Net, thesauruses, Wikipedia). The second step consists in individuating the user preferences. The third step consists in defin-ing a methodology aimed at building the user profile by combining the user local information (i.e., the information related to the user preferences) with the external knowledge resource.

Ontologies are powerful tools for knowledge representation that can be used to formally define user profiles in terms of entities and relations. An ontological user profile (or personal ontology) may be defined by taking into account the user pref-erences related to his/her interests on specific topics, and by completing their representation by means of an external knowl-edge resource that should cover an exhaustive range of topics. In this paper we adopt the general purpose ontology YAGO, which is one of the largest knowledge bases actually available composed of entities and relations ( Suchanek et al., 2008 ). Currently, the YAGO knowledge base contains about 1.95 million entities and 19 million facts. Entities constitute arguments of a fact, and a single fact is a triple constituted by two entities and the relation linking them. An example of fact is ( Dennis Hopper, actedIn, Apocalypse Now ), with the meaning that the actor Dennis Hopper acted in the movie Apocalypse Now . More-over, with each fact a fact identifier is associated. This allows to link, for example, URL information with the knowledge of a specific fact. If the fact ( Dennis Hopper, actedIn, Apocalypse Now ) has the identifier #1 , then it is possible to generate a new fact as ( #1, foundIn, http://en.wikipedia.org/wiki/Apocalypse_Now ). YAGO facts have been defined by a process which has unified two of the most important knowledge resources today: Wikipedia and WordNet. The knowledge extraction process has been undertaken by using a set of advanced rule-based and heuristic methods. In the research reported in Suchanek et al. (2008) , the authors have evaluated that the YAGO facts accuracy is around 95%.

As outlined by YAGO authors, the YAGO model has been defined as an extension of RDFS as explained in Suchanek et al. (2007, 2008) where entities represent all objects in a world knowledge base-model. YAGO authors have defined entities as ties that are neither fact identifiers nor relations are defined as common entities . Common entities that are not classes have been called individuals . Thus, the set of YAGO entities is defined as E  X I[C[R , where I is the set of fact identifiers, C is the set of common entities and R is the set of relation names. As previously outlined, YAGO facts are defined as triples; formally: F # E R E , where E is the set of YAGO entities, and R is the set of relation names. The two entities are called arguments of a fact, and each fact is associated with a fact identifier .
 Based on the previous definitions of YAGO entities and YAGO facts, we define the world knowledge base-model of the YAGO ontology as: Definition 1. WKB YAGO denotes the YAGO world knowledge base; it is defined as: where C is the set of YAGO common entities, R is the set of YAGO name relations, I is the set of YAGO facts identifier, and F is the set of YAGO facts. Fig. 1 shows a graphic representation of a small portion of WKB YAGO dealing with the topic music and the topic scientist .

We define as  X  X  X ersonal ontology X  X  an ontology representing the user interests on specific topics. The objective of our ap-proach consists in extracting from the global knowledge WKB YAGO both the common entities, the name relations, the facts identifiers and the facts related to the user interests. In Section 5 the proposed extraction process will be described. This pro-cess is activated by what we call a set of  X  X  X eeds X  X  represented by keywords related to user topical interests. We make this assumption as most approaches in Information Retrieval are based on bags of words. In Section 4 we explain how such a starting information can be obtained. Based on such a bag of words, we have defined a knowledge extraction strategy, which is aimed at selecting from WKB YAGO a structured and meaningful representation of the knowledge related to the user inter-ests on specific topics. The outcome of the proposed knowledge extraction process is, in fact, a personal ontology represent-ing the topical interests of a user related to the starting seeds.
 Definition 2. Let us denote by O U the personal ontology related to user interests. It is defined as: where C U is a set of common entities, C U # C ; R U is a set of name relations, R U # R ; I U is a set of fact identifiers, I U # I ; F U is a set of facts, F U # F .

By Definition 2 , it follows that O U # WKB YAGO . Fig. 2 shows a graphic representation of a very simple personal ontology related to the topic music extracted from the portion of WKB YAGO depicted in Fig. 1 . In particular, it is assumed that a user is interested on the singer Britney Spears. The input of the knowledge extraction process is in this case a bag of words includ-ing the keyword Britney Spears , and the YAGO ontology. After the execution of the extraction process, the set of common Britney Spears are mined from WKB YAGO . For sake of conciseness, fact identifiers will not be explicitly indicated in the fol-lowing example, as well as in the description in Section 5.3 ; when a fact is extracted by our approach the corresponding fact identifier is automatically extracted too. In Fig. 2 , the dark nodes identify the common entities associated with the singer (keyword) Britney Spears, i.e. C U  X f Spears ; Britney Spears ; American Pop Singer ; Singer ; Person g , while the bold labels on the arrows are the selected relation names, i.e. R U  X f SubClassOf ; Type ; familyNameOf g . Then, the corresponding set of facts F PopSinger  X  ;  X  Spears ; familyNameOf ; Britney  X g .

The process of knowledge extraction acts as a lens that focuses only the portions of the considered global knowledge base that are related to the topical interests of a user. Thus, the outcome of the proposed methodology is a set of sub-graphs ex-tracted from WKB YAGO that constitute the personal ontology. 4. Overview of the personal ontology generation process We assume that the information related to the user has been previously acquired and stored in what we call the Local Information Repository . As outlined in Section 3 , the proposed extraction process makes use of two knowledge resources: the local information repository, and a global knowledge base (YAGO). From the local information repository, a set of starting seeds constituted by keywords related to user topical preferences is extracted. The objective of the proposed knowledge extraction method is to individuate in YAGO the knowledge related to these seeds. In Fig. 3 a general overview of the pro-posed procedure for eliciting a personal ontology is sketched. The local information repository can be constituted by any kind of information related to user preferences: documents stored by the user during his/her Web searches, query logs, where information on the queries is stored, clicked documents, etc. Based on the collected information, a bag of words can be ex-tracted by one of the several methods proposed in the literature ( Manning, Raghavan, &amp; Sch X tze, 2008 ). As strategies final-ized at text analysis are beyond the scope of the work presented in this paper, we assume that such a bag of words is produced by an indexing procedure (for this reason, in Fig. 3 the Analysis of the Local Information step is represented as a black box). In the experiments reported in this paper, we have considered as a starting point a set of documents representing a user interest, and we have generated a set of keywords consisting of both single terms and pairs of terms (2-g). Moreover, we assume that a weight is associated with each term, which is computed by applying one of the classic formulae (e.g., the stan-dard normalized Tf-Idf Robertson, 2004 ) during the indexing procedure based on the analysis of the local information repos-itory. The novelty of the approach proposed in this paper consists in the Knowledge Extraction Process . The proposed technique requires two inputs: the bag of words (set of keywords) representative of the user interests (which we call inter-est-terms ), and a global knowledge base, in our case WKB YAGO . The main research problem addressed is how to associate the set of interest-terms with the right entities and facts in WKB YAGO in order to extract the sub-parts of WKB YAGO that will rep-resent the personal ontology O U . The knowledge extraction process is split into three sub-processes: (1) common entity iden-tification, (2) common entity disambiguation, and (3) rules for knowledge extraction, respectively.
 The common entity identification process allows to associate each interest-term with one or more common entities in WKB YAGO ; this process is semantically enriched with the support of WordNet. The common entity disambiguation process is aimed at reducing the noise, i.e. to prune the candidate common entities not aligned with the user topical interests. Once the final set of common entities is obtained, the defined rules of knowledge extraction are applied to discover in WKB YAGO the knowledge (i.e., the final set of entities and facts) related to the topical interests of a user. In particular, only the facts con-taining the set of common entities extracted by the common entity disambiguation process are analyzed; this means that the common entity disambiguation process assumes a key role: if several ambiguous common entities are obtained, then irrel-evant information can be acquired by the application of the defined rules. The output of the knowledge extraction process is the personal ontology O U formally represented as a set of sub-graphs. As a last step, the resulting personal ontology is con-The use of an ontology editor allows to convert the personal ontology in different ontological languages (as OWL 4 ) in order to make the user profile portable. As previously outlined, the personal ontology is built in a fully automatic way. To import O U into an ontology editor can allow expert users to manually improve the ontology quality, for example by eliminating some noise gathered by the knowledge extraction process. 5. The knowledge extraction process cedure aimed at identifying in WKB YAGO the common entities related to the user topical interests is presented. Then, the noise removal step is described. As a last step, the rules for knowledge extraction are defined that allow to mine from WKB YAGO the semantic associations of the obtained common entities set.
 5.1. Common entity identification process terms IT that constitute the output of the Analysis of the Local Information process (see Section 4 ).
 common entities in C . To each int 2IT , and each c 2C , a strategy containment operator is applied: int v c ; this means that int interest-term int 2IT the function m is defined as: mon entities, C # C , related to all the analyzed interest-terms.

Common entities are mainly defined as compound terms of at least 2-g, with the consequence that cases of partial string matching are predominant on those of exact string matching as we have interest-terms of at most 2-g.
 If an interest-term is not directly found in WKB YAGO (i.e., m ( int )= ; ) its synonyms are identified by using WordNet. In WordNet, a set of words that share one sense is called synset, and words having multiple meanings belong to several synsets. Moreover, in WordNet with each word in a synset its frequency is associated. Thus, an interest-term may belong to more than a synset; as proposed by the authors of YAGO, the obtained synsets are ranked according to the frequency of the word in each of them. Then, we select the synset with the highest frequency value. The words contained in the selected synset are considered as new interest-terms, and added to the set of interest-terms IT , and function m is applied to each of them.
Let us consider now the non ambiguous interest-term Oscar Wilde ; common entities related to it can be: { Oscar Wilde, the ambiguous interest-term like Oscar Wilde does not reduce the possibility of having non ambiguous common entities as a re-sult of the common entity identification process. In fact, in our example, the common entity MS Oscar Wilde identifies a cruise-ferry that is not related to the writer Oscar Wilde . For this reason, the common entities disambiguation process de-scribed in Section 5.2 assumes a key role in the whole process of knowledge extraction. 5.2. Common entity disambiguation process
Common entity disambiguation is a quite important process aimed at disambiguating common entities in C obtained by the common entity identification process. This is a crucial process as the presence of a great number of ambiguous or wrong common entities can preclude the effectiveness of the whole knowledge extraction process. By considering the example of Section 5.1 , if the common entity MS Oscar Wilde is not removed at the end of the common entity disambiguation process, then irrelevant information would be added into the personal ontology.

The method proposed to disambiguate a common entity c  X  in C needs two types of information: local knowledge and global knowledge, respectively. The local knowledge on c  X  allows to determine its importance with respect to the set of inter-est-terms IT . The global knowledge on c  X  will allow to explore its possible interpretations in WKB YAGO . The disambiguation phase assigns two weights, w c LK and w c GK ,to c  X  , which express its importance in terms of local knowledge and global knowl-edge, respectively.

Let c  X  be a common entity in C , and IT be the set of interest-terms, then the local knowledge of c  X  in IT is obtained by the function m 0 defined as: The value m 0 ( c  X  ) identifies the sub-set of interest-terms related to the common entity c  X  . The local knowledge-weight related to c  X  is determined by considering the importance of each interest-term in m 0 ( c  X  ). The importance of an interest-term int is expressed by its weight, w int , computed during the indexing phase (see Section 4 ). Given c  X  , w c LK is computed by aggregating the weights associated with each int 2 m 0 ( c  X  ): Fig. 4 shows how the local knowledge-weights are computed for the common entity MS Oscar Wilde and the common entity Bram Stoker award for best non-fiction . It is assumed that the set IT is related to the user preferences on the literature as top-ical interest.

To analyze the global knowledge related to c  X  consists in mining from WKB YAGO additional common entities that can be associated with it. The aim of new common entities identification is to individuate all the possible semantic interpretations of common entities in C . The Type relation can be used to have an indication of the semantic meaning of a common entity. For example, given the common entity Oscar Wilde by analyzing the Type relation the following fact (Oscar Wilde,Type,writer) is obtained with the meaning that Oscar Wilde is a writer . To discover the possible interpretations of each common entity c 2C , the function m Type is defined as: relation, i.e. m Type ( c  X  )={ c 1 , c 2 , c 3 , ... , c m }.

The function m Type is applied to each common entity c  X  in C , this means to obtain a number of sub-sets m Type ( c  X  ) equal to the cardinality of C . Then, the global knowledge weight of each c  X  , w c GK , is computed on the basis of how many other common same knowledge, namely the same sub-sets obtained by the application of m Type ; instead, outlier common entities are those that share the same sub-set with a few or no other common entities. Thus, w c GK is defined as: Fig. 5 shows an example of how the global knowledge weight is computed. In the considered example, for sake of concise-ness we show the computation of the global knowledge weight only for common entities listed in the bottom table of Fig. 5 ; we have then C  X f Oscar Wilde ; The letters of Oscar Wilde ; MS Oscar Wilde ; Bram Stoker ; Bram Stoker award for no v el ; Bram Stoker award for best non fiction g . Function m Type is then applied to common entities in C , and facts in which they appear as the first argument (where the Type relation is specified) are considered. The common entities that constitute the second argument in these facts are then extracted to define m Type ( c  X  ). A higher value of w c GK is obtained if the common entities in
Type ( c  X  ) appear in many other sub-sets of common entities individuated by the application of m Type to all the common entities in C . The idea is to individuate common entities that share the same knowledge on a given topic as happens for the writer Bram Stoker who shares some knowledge portion with the writer Oscar Wilde , as they are instances of the common entities writer,Irish novelist and person .

To associate an overall score with a common entity c 2C , a linear combination of its weights w c LK and w c GK is finally applied: where 0 6 a 6 1. When a has a value of 0, the local knowledge is not considered, and the final weight is equivalent to the weight obtained by analyzing the global knowledge. If a has a value of 1, the global knowledge is ignored and only the local knowledge is considered. The importance of local knowledge with respect to global knowledge can be balanced by varying the value of parameter a .

The set of common entities C U that will be part of the personal ontology O U is then defined as the sub-set of C , i.e. C U # C , as follows: The use of a threshold value t allows to control the number and the  X  X  X uality X  X  of common entities to be included in the per-sonal ontology. Let us consider the previous example where the ambiguous common entity MS Oscar Wilde has been in-cluded into C as a possible candidate to add in C U . From Figs. 4 and 5 , we have that w MS Oscar Wilde = 0.6  X  0.55 + (1 0.6)  X  0 = 0.33, where a = 0.6. With a threshold value of t = 0.5, the uncorrelated common entity MS Oscar Wilde will not be included in C U as expected. 5.3. Rules for knowledge extraction
Rules for knowledge extraction are defined to identify in WKB YAGO the sub-graphs of knowledge related to the user topical preferences. This is obtained by selecting the facts directly associated with common entities in C U . As outlined in Section 3 , YAGO entities constitute the arguments of a fact; for sake of simplicity, we denote as arg 1 the YAGO entity that appears as the first argument of a fact, and as arg 2 the YAGO entity that appears as the second argument of a fact. The objective is to extract the facts in WKB YAGO that are related to the common entities C U by the application of specific rules. The identification of these rules has been implied by the categorization of the YAGO relations into four classes. Such a classification has been based both on an heuristic analysis of the information role of arg 1 and arg 2, and on their semantic influence on the identification of a common entity. As it will be shown in the example here below, four situations may occur, which are formalized by means of four rules: Rule1: only arg 1 carries information useful to a common entity identification, Rule2: only arg 2 carries information useful to a common entity identification, Rule3: both arg 1 and arg 2 together carry information useful for a common entity identification, Rule4: either arg 1or arg 2 may contain information useful to a common entity identification.
In Table 1 some examples of rule association are reported. The first column indicates the name of a YAGO relation, the second and the third columns are the admissible arguments (i.e., arg 1 and arg 2) for facts where the relation name appears, whereas the last column indicates the rule to be applied for automatically analyzing a fact having such a structure. For a given relation the general structure of the information associated with each argument is stated. For example, by analyzing the first row where the bornIn relation is considered, it is intuitive to see that a user who expressed a preference for a person (e.g., arg1= X  X  X lfred Nobel X  X  ) could be interested where this person was born (e.g., arg2= X  X  X tockholm X  X  ). On the contrary, if the common entities C U associated with a user contain a city name, it does not mean that a user is interested in all people born in the considered city. A similar consideration is applied to the facts of rows 2 X 4. By considering the fifth row, it can be seen that the first name of an individual (e.g., arg1= X  X  X scar X  X  ) is not useful in a matching because it would identify all individuals with the same first name; then the relation  X  X  X ivenNameOf X  X  is associated with Rule2, which requires a matching only on arg 2.
Relation names in rows 6 X 7 have a different nature where Rule4 is applied; for instance, relation actedIn should be matched on both arguments (like arg 1or arg 2) because both of them may carry information that could be relevant to a user. A user who likes the actor  X  X  X ean Stockwell X  X  will certainly be interested in all the movies in which he acted; but it could also be the other way round, i.e. a big fan of this movie could be interested in the cast. The same reasoning is applied when analyzing the relation directed .
 four rules that we have informally explained are formally defined; with each rule a function is associated, and it is defined as Rule j , with j 2 {1,2,4}:
The above functions take value 1 when the common entities specified as arguments match the fact specified as an argument, with the consequence of extracting fact f from WKB YAGO (and added into the personal ontology), 0 otherwise.
 Rule 1 is defined by analyzing the first argument of a fact as: A fact is extracted by Rule 1 if and only if a common entity c U in C U occurs in arg 1.
 Rule 2 is defined by analyzing the second argument of a fact as: A fact is extracted by Rule 2 if and only if a common entity in C U occurs in arg 2.
 Rule 3 is defined by jointly analyzing the arguments of a fact as:
A fact is extracted by Rule 3 if and only if there exist two distinct common entities in C U , one of which is in arg 1, and one of which is in arg 2, for a same fact.
 Rule 4 is defined by analyzing both the arguments of a fact as: the application of the four above rules. The set of relations R U is given by the relations involved in each fact in F U . Moreover, from F U new common entities C # C are obtained. This means that the final set of common entities C U is updated by con-sidering the set of common entities obtained in the rules for knowledge extraction phase, i.e. C U new  X C U [C , where we de-note by C U new the final set of common entities.
 5.4. Updating the personal ontology the knowledge related to the user preferences, (2) to select a formal language to represent the collected knowledge, and (3) to define a strategy for updating the user profile. The first two points have been tackled in the previous sections of the paper.
In this section a possible strategy for the user profile updating is shortly sketched. As a personal ontology is representative of long term user preferences, the updates of a user profile are non frequent. However, the defined knowledge extraction pro-cess makes the process of knowledge integration for each personal ontology simple.
 the set of new interest-terms IT new , the global knowledge base WKB YAGO , and an existent user profile O U . Here, the set of interest-terms IT new denotes the interest-terms extracted from the new knowledge collected about the user. After the com-mon entity identification phase applied to the new set of interest-terms, it is necessary to eliminate from C common entities already stored in O U in order to extract new knowledge in the next steps (i.e., common entity disambiguation process and rules for knowledge extraction ). This way, after the application of the rules for knowledge extraction, a new user profile O U new is obtained, which is updated by considering the knowledge in O 0 U and in O U .
 Algorithm 1. Building and updating a user profile Input: WKB YAGO , IT new , and O U .

Output: The updated user profile O U new . 1: C = CommonEntityIdentificationProcess  X IT new  X  ; 2: C new  X C C U ; 3: C 0 U = CommonEntityDisambiguationProcess C new ; 4: O 0 U = RulesforKnowledgeExtraction C 0 U ; 6. Personal ontology evaluation
In this section, the effectiveness of the proposed knowledge extraction approach for the definition of personal ontologies is evaluated. The aim of our evaluations is twofold: (i) to assess the quality of the knowledge gathered in the user profile, and (ii) to assess how the acquired portions of information are structurally linked to each other. To this purpose two types of evaluations have been performed: qualitative and quantitative.

Qualitative evaluations are performed to test the quality of the information stored in the personal ontology. This consists in identifying how many uncorrelated common entities have been obtained by the knowledge extraction process described in Section 5 . As the input of the proposed methodology is constituted by one (or more) set (s) of interest-terms related to one (or more) user topical interest (s), our intuition is that if the percentage of uncorrelated interest-terms is high, it can be more difficult to extract from WKB YAGO the knowledge related to the right user topical interests. Thus, first some evaluations to compare the quality of the set of the extracted interest-terms versus the quality of the set of extracted common entities are presented. In particular, we consider both the set of common entities obtained by the common entity identification phase, to have a first indication of the behavior of the proposed process, and the final set of common entity stored in the user profile.

Quantitative metrics, as defined in the social network analysis context, are considered in order to evaluate and then examine the structure of the obtained personal ontologies, where the topological properties of an ontology are analyzed. As outlined in the paper, the outcome of our knowledge extraction process is constituted by one or more sub-graphs of WKB YAGO , related to the user topical interests. If the common entities extracted are linked to each other, this means that they can refer to a same portion of knowledge defined in WKB YAGO .

To evaluate the proposed methodology of knowledge extraction from WKB YAGO , we have asked ten users to collect the documents that are more representative of their interests, downloaded for example during their Web search sessions. The topics selected by the users are related to a broad spectrum of knowledge, and this allows to test the effectiveness of our methodology in different areas of interest. It is important to outline that the knowledge extraction process presented in this paper can be iteratively applied to several user topical interests by the procedure explained in Section 5.4 to distinct set of bag of words. This means that a same user may have more than one topical interest, each represented by a distinct set of interest terms. The iterative application of the proposed knowledge extraction method will extract from YAGO distinct sub-parts related to the various interests. For sake of clarity in the performed evaluations we have assumed that each user has only a topical interest with no loss of generality.

Thus, the ten user X  X  collections (one per each user) are representative of the following topics: architecture, astronomy, bot-interest on the selected topic, for example for the collection literature the documents are on some user X  X  preferred authors, beyond the scope of the work presented in this paper; thus, we have analyzed the user X  X  documents with standard Informa-tion Retrieval techniques in order to extract the interest-terms. Each document has been analyzed in two steps: (1) docu-ment preprocessing, where standard text processing techniques have been applied, such as stop-word removal, and (2) term frequency analysis by the standard normalized Tf-Idf formula to compute the index terms weights. The open source software Lucene 5 has been used to the purpose of documents indexing. The ten users had also a significant role during the qual-itative evaluations. In fact, they have acted as assessors by evaluating the quality of both the bag of words related to their doc-uments, and the set of common entities C U . Each user has expressed a judgement on each interest-term (common entity) by classifying such interest-terms (common entities) into two distinct groups: a set of positive interest-terms (positive common entities), and a set of uncorrelated interest-terms (uncorrelated common entities).
 tion 6.3 some considerations on both qualitative and quantitative evaluations are reported. 6.1. Qualitative evaluations measure is used to evaluate automatic query expansion, and it is defined as: RI  X  Q  X  X  n  X  n j Q j , where Q is the set of query, n number of queries helped, and n is the number of queries hurt by methods for automatic query expansion.
 sented knowledge extraction process. In fact, the extracted set of common entities C U may contain some noisy information, identifies the positive common entities, and C U identifies the uncorrelated common entities. A common entity is identified as positive when it is semantically correlated to the considered user topical interest; on the contrary, a common entity is iden-tified as uncorrelated when it is out of topic with respect to the considered user interest. We consider RI in order to evaluate the robustness of the knowledge extraction process in terms of uncorrelated knowledge represented by the personal ontol-C U be the set of uncorrelated common entities, then the RI metric is defined as: where C  X  U  X C U  X jC U j and 1 6 RI  X C U  X  6 1. Clearly, C U  X  1 if all common entities are classified as positive, while C U  X  1 if all common entities are classified as uncorrelated. 6.1.1. Experiments tion process presented in this paper. As previously stated, we evaluate the number of uncorrelated common entities acquired by the proposed methodology (after the common entity disambiguation process and after the rules for knowledge extraction phase). As the input of the methodology is constituted by the set of interest-terms related to a specific user topical interest, if a great number of uncorrelated interest-terms is presented, then for the knowledge extraction process will be more difficult to extract from WKB YAGO the knowledge related to the right user topical interests. The RI measure is then also applied to the set of interest-terms in order to understand how much a noisy information in input can affect the quality of the knowledge topical interest in order to individuate positive information and uncorrelated information. For sake of readability, we denote with C U the set of common entities obtained by the common entity disambiguation process (see Section 5.2 ).
 user profile. If there is a high number of uncorrelated interest-terms, it is plausible to assume that also a high number of uncorrelated common entities will be obtained by the extraction process.
 second column indicates the number of common entities obtained by the common entity identification process. The third column reports the number of disambiguated common entities, and the fourth column the number of uncorrelated common entities from the ones that have been disambiguated. For each identified common entity, a weight that combines the local knowledge and the global knowledge is computed by applying Eq. (3) of Section 5.2 . In this phase, for the 10 profiles the parameter a has been set to the value 0.6 in order to give a slightly higher importance to the knowledge acquired by ana-lyzing the user local information. The last analysis has concerned the setting of the t parameter (the threshold value defined executions with distinct t values have been performed to discover the significant number of common entities. In fact, a wrong t value can produce either a high or a low number of common entities in C U . Based on the performed runs, the fol-lowing optimal t values have been determined for the considered profiles: t = 0.5 for architecture, astronomy, botany, health and fitness, and literature; t = 0.35 for tennis and wine; t = 0.45 for kitchen and music, and t = 0.65 for travel.
By comparing the values of Tables 2 and 3 , it emerges that a lower number of uncorrelated common entities than the number of uncorrelated interest-terms is obtained for each user profile but the wine profile. This indicates a good behavior of the proposed solution for knowledge disambiguation.

In Table 4 some statistics about the knowledge produced by the proposed methodology are presented. The first consid-eration is related to the limited number of extracted facts with respect to the great amount of facts stored in WKB YAGO . This means that the knowledge extracted from WKB YAGO is strictly related to the user interests, and that there is a low presence of noisy information. For example, for the user profile related to the topical interest cuisine only 227 facts have been gathered. In this experiment, evaluations are performed by analyzing user profiles made up of a unique user interest, but it is possible to assume to have user profiles based on several user interests. In this last case, if we define a user profile as an aggregation of all the considered topics of Table 4 , the resulting number of facts is 5041, that is again a limited number with respect to the million of facts stored in WKB YAGO .

Moreover, after the application of the rules for knowledge extraction some new common entities are acquired by analyz-ing the facts in WKB YAGO (see Section 5.3 ). As expected, the number of common entities has increased with respect to the ones in Table 3 .
 the set of common entities in C U , and the set of common entities in C U are evaluated as positive by users. But both RI C U and RI  X C U  X  outperform RI  X IT  X  . This means that the number of uncorrelated elements (interest-terms in IT , and common entities of uncorrelated common entities to the total number of common entities is lower for almost all user profiles. 6.2. Quantitative evaluations
To evaluate the topological structure of the personal ontologies generated by the proposed knowledge extraction process, we refer to the work presented in Gangemi, Catenacci, Ciaramita, and Lehmann (2006) where evaluation methods of ontologies represented as graphs are defined. Three evaluation dimensions are considered: structural measures, that are typ-ical of ontologies represented as graphs; functional measures, that are related to the intended use of an ontology and of its components; and usability-profiling measures, that depend on the level of annotation of the considered ontology. In this work, we refer to structural measures where the topological structure of an ontology can be measured by means of a metric. According to Gangemi et al. (2006) , we can represent a personal ontology O U as a directed graph (or a set of directed graphs) G =( V , L ), where: analysis: density and centrality . Centrality is constituted by several sub-dimensions, each of them evaluated by a different metric; in this paper we consider: indegree, outdegree and closeness metrics. Both density and centrality dimensions are defined for directed graphs as defined in Wasserman, Faust, and Iacobucci (1994) .
 where j L j is the total number of edges in the graph G , and j V j is the number of nodes in G .
 indegree of a node u 2 V , denoted as indegree ( u ) G , is defined as: where j V j is the number of nodes in G , and x ju = 1 iff j is an inlinks to u , x ju = 0 otherwise. Thus, indegree G  X  P j V j from u are taken into account. It is defined as: where j V j is the number of nodes in G , and x uj = 1 iff j are the elements of outlinks from u , x uj = 0 otherwise. Thus, (i.e., indegree G and outdegree G ) are equal.
 where j V j is the number of nodes in G , and d is the geodesic distance 7 from node u to node j . Thus, C G  X  P j V j u  X  1 C  X  u  X  6.2.1. Experiments
To analyze the topological structure of the ten user profiles by the previously introduced metrics, we have used a network analysis tool called ORA. 8 ORA is a dynamic meta-network assessment and analysis tool developed by the CASOS center at the Carnegie Mellon University, Pittsburgh, PA. ORA can be considered one of the most complete tools for social network analysis presented in the literature. It contains hundreds of network metrics, procedures for grouping nodes, identifying local patterns, comparing and contrasting networks, etc. One of the possible usages of ORA consists in examining how a network can evolve in terms of space and time. This feature can be useful to graphically evaluate the changes of a user profile, for example to asses which the new topical interests of a user are.

For our analysis we have used the ORA 2.3.6 version posted on September 2011. Each user profile has been converted into an adjacency matrix in order to import it in the ORA tool. The elements of the adjacency matrix are the common entities in O , where the weight 1 between two common entities is set if they are linked with a relation name in O U , i.e. there exists a fact linking them in F U . Each graph is then analyzed with ORA to compute the metrics presented in Section 6.2 .In Table 5 the high density value for the user profile on travel indicates that the knowledge represented in this graph is more connected. Higher values of indegree and outdegree for the graph on music indicate that there are more central nodes with respect to the other user profiles. Thus, some parts of the obtained profile can be more interesting to the user: this means that for a given topic, the knowledge is focused on some nodes (common entities and facts identifiers), called hubs of a graph. By analyzing the closeness values, a higher value for the graph of the user profile on travel indicates that the topological structure of the related profile is more compact. By comparing the indegree and the closeness measures for the profiles music and travel ,it emerges that in the music profile there are more hubs than in travel profile, but the information is more connected in the travel profile.
 A graphic representation of a user profile allows to have an indication of how the knowledge is distributed in the graph. Fig. 7 shows the graph related to the user profile on music . This is the unique profile where the number of facts (i.e., the num-ber of links) is higher than the number of common entities (see Table 4 ). This confirms the presence of a higher number of hubs in the graph. In Fig. 7 a the knowledge is mainly focused on four common entities. By using ORA, it is possible to select a node and to have information related to it, for example which its name is, and which its neighbors nodes are. In Fig. 7 aan example is sketched, where a node is selected and the information related to it is presented to the user, such as the name jon bon jovi , and its neighbors nodes that report knowledge related to websites, to discography, to how the name is translated in other languages, etc.

Moreover, by using the software ORA it is possible to generate the cloud representation of a graph. To build a cloud rep-resentation of a user profile allows a user to have an indication of what the information extracted from WKB YAGO is with re-spect to the user interest. Fig. 7 b depicts the cloud visualization of the user profile on music based on the most significant forty common entities. The words having a higher size are the hubs depicted in Fig. 7 a, i.e. John Lennon, Bon Jovi (group), Jon Bon Jovi, guitar, rock, etc. 6.3. Combining qualitative and quantitative evaluations
Personal ontologies obtained by the knowledge extraction process presented in this paper are made up of several sub-graphs extracted from WKB YAGO . In the experiments presented in this paper, a user profile describes the knowledge related to a specific user interest. In this case, our intuition is that the presence of several sub-graphs can be an indicator of how many uncorrelated common entities have been gathered, because uncorrelated common entities are not linked with the ones that share the same knowledge on a topical preference. Thus, a possible analogy between qualitative and quantitative evaluations can exist. By comparing the RI  X C U  X  results (see Fig. 6 ) with the D G values (see Table 5 ) obtained for each user profile some considerations can be done. For example, to user profiles having less uncorrelated common entities higher density values correspond, see architecture, literature, music, travel . In fact, higher density values are associated with more connected graphs, and in this case they also correspond to user profiles where a few uncorrelated common entities appear in the personal ontologies. This assumption is confirmed also by considering the user profile where the worst behavior has been obtained. In fact, for the user profile on wine the lower density value corresponds to the highest amount of uncorrelated com-mon entities stored in the personal ontology. However, further investigations are necessary to deepen these preliminary considerations. 7. Conclusions and future works believe that ontologies are worth of investigation as an interesting support for structuring knowledge in user profiles. To this aim a set of interest terms has been considered as the evidence of users X  interests. In this paper, we have defined a strategy that allows to generate a personal ontology by extracting sub-parts of the YAGO knowledge. The knowledge extraction pro-cess we proposed extracts from YAGO only the common entities, relations, facts identifiers and facts which match the user interests.

Moreover, we have proposed both qualitative and quantitative evaluations to estimate the effectiveness of the user pro-files obtained by applying our knowledge extraction process. The analysis of a personal ontology by using both qualitative and quantitative measures allows to make some important considerations. A qualitative evaluation allows to analyze a user profile in terms of amount of noisy information gathered by the knowledge extraction process. A quantitative evaluation (as defined in the social network analysis context) allows to examine the structure of a user profile in terms of topological prop-erties. Moreover, it allows to individuate how the information is linked, and which the predominant interests are for a given personal ontology. This can be useful, for example, to suggest to a user the most significant common entities to improve his/ her query formulation.

We plan to improve the methodology presented in this paper by considering other sources of information (i.e., query logs) to extract the user interests. Furthermore, additional evaluations will be done to compare the personal ontology obtained by applying our strategy with other approaches presented in the literature. In particular, we will investigate the use of user pro-files based on ontologies in personalized search. The idea is to test if richer information defined in ontological user profiles can improve the search outcome to users than other user profiles representations.
 References
