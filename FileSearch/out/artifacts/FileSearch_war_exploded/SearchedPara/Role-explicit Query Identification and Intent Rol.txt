 Understanding the information need or intent encoded within a query has long been regarded as an essential factor of effective information retrieval. For better query representation and understanding, two intent roles ( kernel-object and modifier ) are introduced to structurally parse a class of role-explicit queries, which constitute a majority of common user queries. Furthermore, we focus on two research problems: RP -1 : Given a role-explicit query, how to identify the kernel-object and modifier, namely intent role annotation; RP -2 : How to determine whether an arbitrary query is role-explicit or not. To solve RP-1, we propose a simplified word n-gram role model (SWNR), which quantifies the generating probability of a role-explicit query and performs intent role annotation effectively. Using a set of discriminative features, we build classifiers to address RP-2 in a supervised manner. The experimental results show that: (1) SWNR can achieve a satisfactory performance, more than 73% in terms of different metrics; (2) The classifiers can achieve more than 90% precision in identifying role-explicit queries; (3) Compared with traditional techniques for query representation and understanding, e.g., name entity recognition in query and class-level query intent inference, intent role annotation provides a more flexible framework and a number of applications can benefit from annotating role-explicit queries, such as intent mining and diversified document ranking. H.3.3 [ Information Search and Retrieval ]: Query formulation , Search process Query understanding, Role-explicit, Kernel-object, Modifier Web search engines are playing an increasingly dominant role in people X  X  daily information access on the web. Given the sheer volume of data or information available on the Internet, web search engines help users to drill down to a small number of valuable documents. When an information need is conceived, a user generally turns to his or her favorite web search engine, submits a query representing the information need and browses the ranked result list for desired information. As a carrier of information needs, queries generally consist of a sequence of words. However, they are known to be ambiguous and/or underspecified [7, 23]. In real-life search, instead of formulating natural language queries, the vast majority of users are submitting short queries, which generally show no grammatical or syntactical structure. For example, the query  X  X indows X  could refer to the commercial Microsoft Windows software or to house windows. In the category of Microsoft Windows, a user may be interested in  X  X indows XP X ,  X  X indows update X , etc [23 ]. This problem is more crucial for East Asian languages, e.g., Chinese, Japanese and Korean, which is written with no word delimiters. Reasons resulting in this problematic issue can be classified as: (1) Search engines are often used to search unfamiliar topics and users may be unaware of the exact words matching their information needs; (2) Queries are often submitted in the absence of any knowledge of user X  X  preference or context; (3) The senses of submitted words may be used in ways beyond the original imagination of users. Moreover, due to the diversity of users X  expressing behavior, queries formulated by a large population of users are heterogeneous, which also complicates the information retrieval problem [3, 24]. Analogous to the semantic role labeling in natural language processing [8, 17], to get a better query representation and understanding, we introduce two intent roles: kernel-object and modifier to structurally annotate a user query. In our study, a user query is viewed as a sequence of semantic units (a semantic unit is regarded as a word). Kernel-object refers to the dominant word that abstracts the core object or topic of the underlying intent encoded within this query. Modifier refers to the co-appearing words with kernel-object, which explicitly specify user X  X  interested attributes or concrete aspects. A user query that can be represented with kernel-object and modifier is defined as role-explicit . Otherwise, it is defined as role-implicit . For example,  X   X  X  X  X  X  X  X  X  X  (Harry Potter game)  X  is a role -explicit query,  X   X  X  X  X  X  X  (Harry Potter )  X  is the kernel -object ,  X   X  X  X  (game )  X  is a co-appearing modifier. Annotating role -explicit queries with intent roles not only ac hieves a concise query representation and understanding , but also facilitate s mining the knowledge hidden in a large number of role -explicit queries performed by different users (section 7) . In this paper, on one hand, we study how to determine whether an arbitrary query is role -explicit or not. On the o ther ha nd, f or a role -explicit query, we wish to efficiently annotate the intent role of each word. A simplified word n -gram role model is proposed to quantify the generating probabili ty of a role -explicit quer y and perform s intent role annotation effectively . Us ing a set of predictive features, we build different classifiers to identify whether an arbitrary query is role-explicit or not in a supervised manner. The experimental results demonstrate the feasibility and effectiveness of the proposed approaches, which pave the way for further role-explicit query oriented studies. The rest of the paper is organized as follows: Section 2 details the related work. Section 3 formalizes the two target research problems RP -1 and RP-2. In section 4, we present the simplified word n-gram role model for solving RP-1. Section 5 details a set of features for building classifiers, which are used to address RP-2. In section 6, experiments are conducted to evaluate the proposed approaches. Section 7 details the potential values of intent role annotation. We conclude our work in section 8. To capture the underlying information need encoded within diverse user queries, considerable works have been conducted from various aspects. Research works related to our study are those that focus on query-centric annotation and query intent inference.  X   X  X  X  X  X  X  X  X  X  (Harry Potter game)  X  and  X   X  X  X  X  X  X   X  X  X  (Harry Potter England )  X  are real user quer ies used to clarify the related techniques.
 Query annotation is a common practice in query understanding, such as query segmentation [11, 25], name entity recognition [9, 10] and part-of-speech tagging [2] . Query segmentation techniques segment a query into a number of semantic units, which is a basic preprocess in information retrieval. F or  X   X  X  X  X   X  X  X  X  (Harry Po tter game)  X  , the optimal result is  X   X  X  X  X  X  X  (Harry Potter ) /  X  X  X  (game )  X , where  X / X  is used as a word delimiter . Yu [36 ] presents a supervised approach using the CRF model. An unavoidable drawback is the requirement of a tagged training corpus. Unsupervised ap proaches are widely adopted in this field . Tan and Peng [25 ] introduce a generative language model to perform query segmentation. They estimate the parameters through the expectation -maximization algorithm, which optimizes the minimum description length on a partial corpus. Different from the language model oriented approaches, Hagen [11] proposes an approach called na X ve query segmentation, which derives a score for each potential segment leveraging its raw web frequency. The Google n -gram corpus is used a s an external resource to compute the frequencies. Name entity recognition in query (NERQ) techniques not only identify the possible name entity inside a query, but also label class attribute to the identified name entity , e .g., Game class for  X   X  X  X   X  X  X  (Harry Potter )  X  . Pasca [14] presents a template -based approach to extract name entities. Guo [10] investigates how to perform NERQ with queries that include a single name entity. In their study, a query including a name entity is divided into two parts: the name entity and the remaining part as its context. A weakly supervised topic model is proposed to quantify the probability of a possible name entity. However, the problem of NERQ is far from being resolved. Defining fine -grained types for entities on the web i s still an unresolved problem. As reported by Sekine [21 ], a fine -grained classification of entities scales up to hundreds of types. The current approaches mostly restrict their recognition to a number of pre -defined classes. Barr [2] annotates part -of-spe ech tags to English queries and show s that it helps to create features for enhancing the r elevance of web search results. As for query intent inference, we mainly discuss the word-level approaches that look into the inner feature of words inside a query. Yin [33 ] proposes algorithms to build a hierarchical taxonomy of generic intents for a class of name entities. The words or phrases co-appearing with name entities are used to represent generic intents. E.g.,  X   X  X  X  (game )  X  co-appearing with  X   X  X  X  X  X  X  (Harry Potte r)  X  will be viewed as a generic intent for the Game class. Similarly, Wang [28 ] studies how to extract broad query aspects that are applicable to a class of queries to help users accomplish query reformulation. However, there are cases where we need to go beyond generic intents or broad aspects at a hard class level. Our research shows that the instances of fine -grained classes will hold different aspect words for particular intents . E.g.,  X   X  X  X  X  X  X  (Harry Potter )  X  may refer to a movie , a game or a book . The intent phrase co-appearing with  X   X  X  X  X  X  X  (Harry Potter )  X  may be not suitable to other name entities of the same class that have no movie edition or game edition . Intent role annotation proposed in this paper can address this drawback by aggregat ing queries at a per -kernel -object level. For g eographic intent (Geo Intent) analysis , Welch and Cho [29 ] divide a query into a contextual location and a base query. E.g.,  X   X  X  X  X  X  X  X  X  X  (Harry Potter England )  X  will be parsed into a location part:  X   X  X  X  (England)  X  and a base query  X   X  X  X  X  X  X  (Harry Potter )  X  . They use the features derived from base queries to identify queries that have an implicit Geo intent. They deem that some users would prefer to see results prioritiz ed by their geographical proximity. Similar ly, Yi [3 2] builds a city language model to probabilistically represent the language surrounding the mention of a city in queries. Several features derived from the city language model are used to identify user  X  s implicit Geo inte nt. They further clarify the Geo intent into three levels: local Geo queries, neighbor region Geo queries and others. As a further endeavor to the previous works, we differentiate the utility of each semantic unit inside a query through two intent roles. Although our idea is simple, it provides a more flexible framework compared with prior works (detailed in section 7). In our study, a user query that can be decomposed into kernel-object and modifier is defined as role-explicit. Furthermore, we hypothesize that: for a role-explicit query 1 ... rn words, word i w plays either the role of a kernel-object or the role of a modifier. There is one and only one kernel-object inside a query. The number of modifiers is not limited. Modifier can be empty, which means a single kernel-object query distinguish from that include at least one modifier. Ignoring the grammar and order information, a role-explicit query is roughly a kernel-object plus a set of modifiers ( ,{ }) r q ko mo  X  , where ko denotes the kernel-object and mo d enotes a co-appearing modifier. With respect to role-explicit query oriented study , in this paper we investigate two basic research problems as follows: RP -1 : For a role-explicit query r q , we wish to efficiently annotate the intent role of each word inside We cast RP-1 into the task of intent role annotation (IRA), which involves word boundary detection and intent role labeling. Word boundary detection aims at properly segmenting a role-explicit query 12 ... rm q c c c  X  ( m characters) into a number of semantic units 12 ... rn q w w w  X  ( n words). Intent role labeling aims at correctly annotating the intent role of word i w : either kernel-object or modifier. As can be observed in query logs, some Chinese words, such as  X   X  X  X  (download)  X  ,  X   X  X  X  (type)  X  and  X   X  X  X  (review)  X  , have a larger probability to be submitted as a modifier rather than a kernel -object . Some Chinese words , such as  X   X  X  X  X  X  X  (Harry Potter)  X  ,  X   X  X  X  X  X  (audio fiction)  X  ,  X   X  X   X  (retirement pension)  X  or other entities, have a larger probability to be submitted as a kernel -object and rarely occur as a modifier . While other words , such as  X   X  X  X  (film)  X  , sometimes it play the role of a kernel -object , e.g., in query  X   X  X  X  X  X  X  (film download)  X  . Sometimes it play the role of a modifier, e.g., in query  X   X  X  X  X  X  X  X  X  (fil m farewell my concubine)  X  . Given a set of role -explicit queries, once we decompose each query into a kernel -object and a set of modifier s, we will get a vocabulary of words annotated as kernel -object and /or modifier. This observ ation leads us to hypothesize that: for a word i two probability distributions: () i pw  X  and () i pw  X  , the probability of i w being a kernel-object , () i probability of i w being a modifier. Going further, there exists a hidden generative model  X  that describes how a specific role-explicit query is generated from a finite vocabulary of kernel-objects and modifiers. Essentially, the vocabulary is structured as a set of unique words which are annotated with two intent roles: kernel-object and modifier. This hypothesis gives us an insight that the discovery of model problem RP-1. In this paper we present a simplified word n-gram role model, which can be viewed as an instance of the hidden generative model RP -2 : For an arbitrary query q , we also wish to efficiently identify whether can be represented with a kernel-object and a set of modifiers. By hypothesizing that role-explicit queries can be differentiated from role-implicit queries by a set of predictive features, we formalize RP-2 as a binary classification problem. Leveraging on a set of predictive features, we build different classifiers to solve RP-2 in a supervised manner. Without loss of generality, we use 12 ... rm q c c c  X  to denote a role-explicit query of m characters. As for query segmentation, it is 
W w w w j  X   X  X  , where j W denotes a possible segmentation, 
W denotes the number of words in j W . For a possible segmentation represents a possible intent role annotation consisting of one kernel-object and ( | | 1 j W  X  ) modifiers, k w  X  denotes a kernel-object , k w  X  denotes a modifier. All possible annotations for query r q would be 1 { | 1,..., 2 ; 1,...,| |} km jj W j k W total number of probability ( | ) k j pW  X  , which reflects the likelihood of observing W . If we enumerate all possible annotations, the optimal IRA thus can be accomplished by searching In equation 1, ( | ) k rj p q W indicates the probability that unique query text. For r q , there are only two possibilities: either W can generate r q or not. Put another way, the value o f p q W can only be 1 or 0. To proceed with IRA, we only and prefer the one with the maximum probability . In the following subsections, we briefly mention the language model and its extended version of n-gram class model, which lay a fundamental theoretical basis for our probabilistic formulization. We then describe our proposed SWNR model which can be viewed as an instance of the hidden generative model  X  . Under SWNR, we are able to quantify the generating probability of each The language model has been widely used in natural language processing. When used in information retrieval, it can be formulated as a probability distribution () pq over string q , which reflects the likelihood that string The word n-gram language model assumes that the probability of observing the th i word i w in the context history of the preceding 1 i  X  words depends on the shortened context history of the preceding 1 n  X  words [6], which is expressed as The probability of observing query 1 ... m q w w  X  is given as p w w p w w p w w p w p w w  X  X   X  X  where j i w denotes the sequential words ... ij ww . For a vocabulary of size V , the number of parameters for n-gram Brown [5] proposes n-gram class model to address the drawback of n-gram language model that a huge number of parameters are required to be estimated. In n-gram class model, each word is associated to a unique class () ii wc  X   X  and  X  is a function that maps a word i w into its class i c (a total of C classes). The conditional probability is given by where i c represents the word class to which the word belongs . j i c represents the sequential classes ... corresponding to the sequential words ... ij ww . For a vocabulary of size V , the number of independent parameters of n-gram role of word i w , {0,1} e  X  , 0 i r denotes a modifier , 1 kernel-object. If we encapsulate intent roles for word n-gram language model , analogous to equation 4, the conditional probability can be given as: Different from n-gram class model, the probability are conditioned on not only the preceding 1 n  X  words but also the intent roles of the preceding observing the role-explicit query 1 ( ... ) rm p q w w  X  can be given as p w w p w w p w p w r p r w  X  X   X  X  We denote the model defined by equation 6 as word n-gram role model (WNR) , which can be viewed as an extended version o f word n-gram language model. Figure 1 (a, b, c) illustrates the topological differences among the three models, where the red grid circles the conditional factors. (a) Word n-gram model (b) N-gram class model (c) WNR model (d) SWNR model For word n-gram model (Figure 1-a), the conditional probability depends on the preceding 1 n  X  words. For n-gram class model (Figure 1-b), the conditional probability depends on the class attributes of the preceding the current word. For WNR model (Figure 1-c), the conditional probability depends on not only the preceding also the intent roles of the preceding possible intent role of the current word. Similar to word n-gram model, WNR suffers from the drawback that a huge number of parameters are required to be estimated, as many as (2 ) n V . To reasonably decrease the parameters of WNR, we resort to a simplified model by introducing the following assumption s on how users conceive a role-explicit query. (1) (kernel-object)-dependence : Since kernel-object abstracts the core object or topic of an information need and the modifiers are used to modify the kernel-object , it is reasonable to assume that: users firstly conceive the kernel-object, the co-appearing modifiers are conceived subsequently and (kernel-object)-dependent. (2) modifier mutual-independence : For a role-explicit query including multiple modifiers, we assume that modifiers are selected independently. Thus for a possible annotation The cumulative probability of all possible annotations corresponding to segmentation 12 ... jn W w w w  X  of query ( ) ( ... ... ) ( ) ( | ) We call the model defined by equation 8 as simplified word n-gram role model (SWNR) . SWNR can be interpreted as a quasi-bigram model. Figure 1-d illustrates its topological structure , which breaks down the context history. All modifier-words are for 2 n  X  , the n-gram is at most a bigram level modeling. The number of parameters for SWNR is decreased to 2 2 VV  X  . SWNR can also be viewed as an instance of the hidden generative model  X  that interprets how a role-explicit query is generated from a finite vocabulary of kernel-objects and modifiers. More specifically, the core of SWNR corresponds to a model lexicon  X  , which consists of two parts: The 1-gram part consists of 2 V independent parameters of the form () modifier and () probability that word j w being a modifier on condition that word w being a kernel-object. The process of generating a role-explicit query under SWNR can be described as: Firstly conceive a kernel-object w conceived kernel-object, the model will then select a set of mutual-independent modifiers {} j w  X  , each of which holds the probability of ( | ) ji p w w  X  X  , thus a role-explicit query will be formulated. Tow ard this way, the discovery of model SWNR can be casted into a problem of how to learn the model lexicon Once we obtain the model lexicon annotation k j W , we can calculate its generating probability as In this subsection, we study how to learn the model lexicon  X  in a statistical way. Suppose there is a corpus of sufficient role-explicit queries: 1 1 1 { ... ... | 1,..., } intent roles of each query are pre-annotated. Put another way, a voc abulary of words annotated as kernel-object and/or modifier can be distilled from this corpus. Using the maximum likelihood estimation, the parameters in the model lexicon  X  can be estimated as follows: For 1-gram parameters, where () i cw means the number of times word i w occurs in as a modifier . () i pw reflects the likelihood that word it is estimated using the Laplace Smoothing technique as To handle the words that did not occur in the learned model lexicon, we include a universal symbol &lt;unk&gt; for unknown words. The quasi-bigram parameters are smoothed with a unigram probability using an interpolation technique named Absolute Discounting Smoothing [13], which is expressed as, where ( , ) ji c w w  X  X  means the number of co-occurred times of and i w in one query on the condition that word j w is a modifier and word i w is a kernel-object. In equation 13, the linear interpolation factor ( 1  X   X  ) is calculated as where 1 () i Nw  X  means the number of unique words that co-occurred with i w , D is a discounting constant , the total number of word pairs with exactly one and two counts. As stated above, a dataset of sufficient role-explicit queries is the prerequisite for learning a quality model lexicon. To obtain a large corpus of annotated role-explicit queries, we rely on an unsupervised framework consisting of two stages: Stage-1 : user session based extraction and annotation . Users commonly keep a substring invariable when reformulating queries to clarify their intents, moreover, this invariable substring generally corresponds to a semantic unit [9]. Instead of pre-segmenting the individual queries into words, we employ a na X ve strategy . We sequentially scan the queries in a user session using a sliding window. For two consecutive queries in a user session, we regard the longest common substring as the kernel-object of the two individual queries . The remaining part is viewed as a concatenation of modifiers, and will be further segmented using forward-maximum-matching method [30] . If a query has no common substring with its neighbor queries, it will be directly skipped. Stage-2 : pattern-based purification . The false identification of kernel-object in stage-1 may result in unreliable role-explicit query extraction and annotation. We further perform a pattern-based purification using the algorithm proposed by Pasca [15]. Different from Pasca that restricts the extraction to a set of pre-defined classes, we use latent classes through clustering the gathered context templates into a number of clusters and each cluster represents a latent class. With the availability of large amount of query logs, little cost is required to establish a corpus of sufficient role-explicit queries. Therefore, we can learn the desired model lexicon of () lexicon for efficient access. Table 1 illustrates two parameters in model lexicon  X  learned in experiments (we use the logarithm of a probability to avoid small numbers in calculation).  X   X  X  X  X  X  X  (Harry Potter)  X  is often used as a name entity that may refer to a famous film or a popular game, therefor it has a larger probability to be submitted as a kernel -object .  X   X  X  X  (wallpaper)  X  and  X   X  X  X  (game)  X  are two modifiers , which are commonly used to res trict  X   X  X  X  X  X  X  (Harry Potter)  X  into a particular aspect . Under the model lexicon  X  , we can deduce that: role-explicit query  X   X  X  X  X  X  X  X  X  X  (Harry Potter game)  X  is likely derived from a kernel -object  X   X  X  X  X  X  X  (Harry Potter)  X  plus a modifier  X   X  X  X  (game)  X  with a probability of -5.158 (10 based log) larger than any other possible annotations, e.g.,  X   X  X  X  (Harry )  X  as the kernel -object with  X   X  X  X  (Potter )  X  and  X   X  X  X  (game )  X  as the modifiers. Analogously, the same situation holds true for  X   X  X  X  X  X  X  X  (personal inc ome tax)  X  oriented queries. For a role-explicit query r q , we segment it into a number of words in all possible ways. For each possible segmentation we annotate possible annotation k j W , we calculate its generating probability using the learned model lexicon  X  . Finally the annotation with the highest probability is selected as the optimal result for IRA. The calculation seems infeasible as the number of possible annotations grows exponentially with query length. Fortunately, most role-explicit queries are short, the number of words per query is small on average (2.35) [22 ]. The calculation thus can be performed efficiently indeed. As for RP-2 of determining whether an arbitrary query is role-explicit or not, we cast it as a binary classification problem . Towards this direction, several typical classifiers are built to solve RP-2. The adopted features are listed as follows: (1) Usage of SWNR : The SWNR model formalizes a probability distribution which quantifies the likelihood of observing a role-explicit query. For an arbitrary query q , if we assume it is role-explicit, under SWNR we can obtain a probability, which reflects the likelihood of instance that will be reasonably high; otherwise, a negative instance that role-implicit will leads to a zero probability or an extreme low probability. To capture this intuition, we consider the generating probability calculated by SWNR as a training feature. (2) Query length : Keyword queries simply consist of several words and show no grammatical structure. While queries constructed as sentences or questions contain more words and show obvious syntactical structure. The length of a query thus reflects its complexity to some extent. For a query that includes name entities, especially places and organizations, it commonly has a bigger character length. However, these name entities act as one semantic unit indeed. So the word-length is more reasonable than the raw character-length to reflect a query X  X  complexity. Role-explicit queries are mostly short queries and show no obvious grammatical structure. To differentiate from the verbose queries that rarely occur as role-explicit queries, the word-length of a query is employed as a training feature. (3) Name entity : Kernel-objects in role-explicit queries mainly consist of name entities, nouns and noun phrase. Therefore, including name entities or not is a positive feature for identifying role-explicit queries. Instead of performing sophisticated name entity recognition techniques, we rely on a Wikipedia-based approach. Wikipedia, which is known as the largest online encyclopedia, contains millions of entries. Most of the entries are typical name entities and keyword phrases throughout a large number of domains. Many name entity related works employ Wikipedia as their knowledge resource. In our study, for a specific query, we firstly segment it into a sequence of words using forward-ma ximum-matching method [30]. For each word, we identify whether it appears as an entry of Wikipedia. If appears, we deem that this query contains a name entity. (4) Interrogative words : This feature is inspired by the observation that many question queries tend to contain interrogative words (the underlined word in bold) . E.g.,  X   X  X  X   X   X  X  X  X  ( How many kinds of trees are there)  X  , which can X  X  be well parsed with kernel -object and modifier . Moreover, t he interrogative words are rarely observed in role -explicit querie s. For this reason, we collect a set of frequen t interrogative words (Table 2) as feature words when training our classifiers.  X  (?)  X  (where)  X  X  X  (when)  X  X  (whether)  X  X  X  X  (why)  X  X  (what)  X  (who)  X  X  X  (how many)  X  X  X  (how)  X  X  X  (how)  X  X  (which)  X  X  X  (how a bout) (5) User distribution : The definition of kernel-object and modifier derives from the different utility of words performed by users in expressing their intents. Despite the simplicity, they effectively depict the concise structure of role-explicit queries. For a specific query, a large population of users submitting the same query implies a consistence with the utility of composing words when clarifying an information need. While the number of users who formulate the same complex query, e.g., long-sentence query, question query, tends to drop quickly. For this reason, we count the number of unique users who submit the same query as a training feature. The former features stem from the inner characteristics of an individual query, while user distribution can be viewed as an external feature aggregated from different users. A series of experiments are conducted to evaluate the proposed approaches for target problems RP-1 and RP-2. Across our study, Chinese query log SogouQ is adopted as the data source, which is widely used in query suggestion, query classification and search performance evaluation. SogouQ spans a month (June in 2008) and contains about 30 million clicks. Each record in SogouQ consists of the same information: session id, query string, rank of clicked URL, click order and clicked URL . The notion of session has been well used in prior works such as Boldi [4] and Piwowarski [16], where a timeout threshold is generally used to split sessions. Because of the inherent drawback of SogouQ that no access-time is provided, we extract sessions as follows: The records of each day are grouped by automatically assigned session id and ordered by submitting sequence. The queries with the same session id in a day are regarded to be within one user session. SogouQ is a raw dataset and contains a lot of noises. Before our utilization, we clean this query log by filtering the ill -formed records, e.g., the record that has a rank value larger than 50, a query that contains irregular characters. After cleaning, the basic statistics of the whole dataset are: #days : 30 , #queries : 21,311,479 , #unique queries : 3,113,231 , #sessions : 6,922,826. In this subsection, experiments are designed to evaluate the proposed SWNR for solving RP-1, namely intent role annotation for role-explicit queries. The whole dataset of SogouQ is split into two parts: the former 24-day part (denoted as 24  X  ) is used to learn the model lexicon  X  . The latter 6-day part (denoted as is used to construct the testing collection. We randomly extract queries from consisting of 500 role-explicit queries. The intent roles of each que ry are manually annotated. Since both kernel-object and modifier are semantic units, the intent role annotation of a role-explicit query is viewed as correct if and only if the segmentation and the intent role of each word are all correct. Thus we employ metrics used in the field of query annotation to test the performance at different levels. Word Level : This metric is a variation of the metrics for query segmentation [11, 25]. If we treat the manually annotated role-explicit queries as a set of single words with intent roles, we can measure how well the automatically annotated queries recover these words. Toward this way, the well-known measures precision , recall and f-measure can be analogously defined as: where  X  represents the word set of manually annotated queries , word is viewed as  X  relevant  X  if and only if its intent role is correctly annotated. Query Level : This metric is a variation of the metric ( Top N Accuracy ) [10] for NERQ, which not only identify the name entity inside a query, but also label a class attribute. The intent role of each word can be viewed as an attribute, so this metric can be analogously employed. The correction of an annotation depends on not only the query segmentation, but also the annotated intent role of each word. Therefore, the query level metric becomes more rigid in judging how well the predicted annotation matches the manual annotation at a query granularity. The Top N Accuracy ( acc@N ) is defined as: the result of an algorithm is viewed as correct if at least one of the top-n results is correct, acc@N is the fraction of correctly annotated queries over the manually annotated queries, namely where  X  is the set of manually annotated queries, '  X  is the set of automatically annotated queries. For a specific role-explicit query, ' () N  X  means that top-n results of the output are considered, the final result is viewed as correct if at least one of the top-n results is correct against the manual annotation. To our knowledge, there is no prior work conducted for IRA as defined in this paper, in order to evaluate the overall performance of the proposed framework, we adopt the traditional language model based approach for named entity identification as the baseline [34 ]. Given a sequence of words 1 { ,..., } entities can be identified by searching the Viterbi path as where 1 ,..., m cc are name entity classes. Indeed the model defined by equation 17 is equivalent with the one depicted by equation 6. The difference is that the model represented by equation 17 works on English words, while equation 6 considers all possible segmentations of a Chinese query. They both determine the optimal annotation through sequential maximum likelihood estimation. Therefore, we employ the word 2-gram role model and word 3-gram role model as baselines, denoted as 2-GRM and 3 -GRM respectively. 2  X  GRM and 3-GRM represent a straightforward utilization of the language modeling approach. Table 3 summari zes the overall performance evaluated through the word-level metric. Table 4 summarizes the overall performance evaluated through the query-level metric . The word level metric reveals the accuracy that the intent role of a word inside a query can be predicted. The query level metric becomes more rigid in judging how well the predicted annotation of a query matches the manual annotation. As shown by Table 3 and Table 4, 3-GRM performs slightly better that 2  X  GRM across each metrics. It implies that 3-GRM is more robust than 2-GRM when encapsulating word attributes. The S WN R model outperforms all baseline methods. The reasons can be summarized: S WN R emphasizes the dependency between modifier and kernel-object inside a query, and weakens the association among modifiers. Put another way, SWNR restricts the rigid bigram to word pairs consisting of kernel-object and modifier. 2  X  GRM and 3-GRM treat kernel-object and modifier with an equal statistical way. As queries are often short, SWNR is more robust to model the user queries with intent roles. In this subsection, we evaluate the way of solving RP-2 in a supervised manner. Different machine learning classifiers are built and evaluated. We construct a hand-annotated corpus consisting of 1000 user queries, which are randomly extract ed from 6  X  . Using the features detailed in section 5, each query is specifically, the SWNR model learned in section 6.1 is used to calculate the generating probability ( gp ), which is denoted as a float value. The query length ( ql ) and user distribution ( ud ) are their raw values. The existence/absence of name entity ( ne ) and interrogative word ( iw ) are denoted as a 1/0 value. The target class attribute of role-explicit is denoted as yes/no . To investigate the effectiveness of the proposed features, we use the WEKA toolkit [12] to build different classifiers including Na X ve Bayes, Decision Tree (J48), Support Vector Machine (LibSVM) and AdaBoost (AdaBoostM1), and Na X ve Bayes is selected as the baseline classifier. These supervised learners are well suited to classification tasks. For each classifier, we util ize the k-fold cross validation strategy, which can prevent the overfitting problem to some extent . We set k=10 in our experiment. The performances are compared in terms of weighted average precision ( wa_p ) , weighted average recall ( wa_r ) and weighted average f-measure ( wa_f ). Table 5 summarizes the overall performances of different classifiers. As shown in Table 5 , the Na X ve Bayes classifier performs the worst. The strong independence assumption underlying this classifier is that the presence or absence of a particular feature is unrelated to any other features. Therefore, a possible reason for its poor performance could be the existence of dependencies among the features. The SVM classifier separates the target classes with the maximal margin in a higher dimensional space. The AdaBoost classifier is an algorithm for constructing a strong classifier through combining a set of weak classifiers. Table 5 shows that they generate moderate results. Different from other classifiers that jointly consider a set of features to discriminate targets classes in a single decision step, the Decision Tree classifier performs classification in a multistage step based on a tree-like graph. Table 5 shows that the Decision Tree classifier achieves the best result. Moreover, the acceptable results of SVM, AdaBoost and Decision Tree justify the way of identifying role-explicit queries in a supervise manner. To further investigate the correlation between different feature combinations and classification performance, we enumerate all feature combinations and test their effectiveness. As the Decision Tree classifier performs the best, we utilize it for each feature combination. Table 6 shows all the possible feature combinations . Each row consists of the combinations with the same number of features. The combination in each row that generates the best result is underlined, and the best result is listed on the left side . wa_p wa_r wa_f Feature combination 0.906 0.907 0.907 &lt; ud,ql,ne,i w &gt;, ... , &lt; gp,ql,ne,i w &gt; 0.901 0.903 0.902 &lt; ud,ne,i w &gt;, ..., &lt; gp,ql,i w &gt; 0.893 0 .896 0.894 &lt; ud,i w &gt;, &lt; gp, ql &gt;, ... , &lt; gp,ud &gt; 0.881 0.879 0.88 &lt; gp &gt;, &lt; ql &gt;, ... , &lt; i w &gt; From Table 6, we can observe that: (1) The results of feature combinations consisting of a single feature show that the query length at a word granularity is the most discriminating feature; (2) Feature combinations consisting of multiple features generate better results than feature combinations consisting of fewer features, which demonstrates that different features hold different discriminative abilities; (3) the same result of &lt; gp, ql, shows no positive effect when combined with other four features. Annotating role-explicit queries with intent roles, on one hand, provides us a new view of query representation, on the other hand, the utilities of words inside a role-explicit query in expressing a particular information need are effectively differentiated. As an example, Table 7 illustrates a number of annotated role-explicit queries. E.g.,  X   X  X  X  X  X  X   X  X  X  X  (Harry Potter game )  X  can be represented as:  X  ko:  X  X  X  X  X  X  (Harry Potter)  X  plus  X  mo:  X  X  X  X  (game )  X  . Moreover, we can deduce that the user X  X  interest is restricted at the  X   X  X  X  X  (game )  X  aspect of the entity  X   X  X  X  X  X  X  (Harry Potter)  X  , which facilitates query unde rstanding to some extent.
 Despite a structural parsing at a per-query granularity, intent role annotation enables us to mine the knowledge hidden in a group of role-explicit queries performed by different users. As an illustration, Figure 2 depicts a directed graph by aggregating a number of role-explicit queries that share the same kernel-object . In Figure 2, both purple circle and purple diamond represent a kernel-object. A purple circle denotes that the kernel-object has ever been submitted as a ko q query. Both green circle and green diamond denote a modifier. The word concatenation corresponding to the path starting from a green circle to its nearest kernel-object has ever been submitted as a query. An edge (path) from a kernel-object to the satellite modifiers means a co-appearance in one query . The edge is directed from a more frequent modifier to a less frequent modifier. If we use () Q ko to denote a set of role-explicit queries in a query log that share the same kernel-object (the queries in distinct from each other), for a specific kernel-object ko , the queries in () Q ko are represented by word concatenations of ko and its satellite modifiers. A directed edge between two kernel-object s means a co-appearance in one user session, which represents that a user turns to a new kernel-object oriented intent after searching the source kernel-object oriented intent. In Figure 2, the edge width is proportional to co-occurrence frequency of two linked words. As can be observed from Figure 2, the aggregated role-explicit queries performed by different users are meaningful. The aggregated graph can be interpreted as the wisdom of crowds on clarifying their intents. The role-explicit queries within () Q ko correspond to a star topology, which reflects a set of ko -oriented intents conceived by different users. For () Q ko that includes a q query and several mo q queries, the different modifiers in underspecified and/or ambiguous. And the edges among kernel-objects indicate associated search intents. For example, for ko =  X   X  X  X  X  X  X  (Harry Potter)  X  , it has ever been submitted not only as a ko q query, but also as the kernel-object of many other queries, such as  X   X  X  X  X  X  X  X  X  X  X  X  (Harry Potter movie song)  X  ,  X   X  X  X  X  X  X  X  X  X  X  (Harry Potter game)  X  , etc. For the same kernel -obje ct, some users prefer information from the perspective of  X   X  X  (fiction)  X  , some users p refer information from the perspective of  X   X  X  X  (game)  X  . While in the category of  X   X  X  (film)  X  ,  X   X  X  X  (song)  X  and  X   X  X  X  (movie)  X  are both popular aspects. Furthermore, t he satel lite modifiers of  X   X  X  X  X  X  X  (Harry Potter)  X  indicate that it would be h ighly underspecified and/or ambiguous if directly submitted as a any context or user  X  X  preference, it is impossible to know the exact intent encoded within a single (Harry Potter)  X  . Unf ortunately ,  X   X  X  X  X  X  X  (Harry Potter)  X  was directly submitted as a times in our adopted query log. However, the aggregated modifiers provide us a way of how to capture the underlying intents of Analogously, the same situation holds true for other kernel -object s, e.g.,  X   X  X  X  X  X  (audio fiction)  X  ,  X   X  X  X  X  X  X  X  (Omega watch) X , e tc. The work by Yu and Ren [35 ] demonstrates the effectiveness of organizing modifiers for intent (or subtopic) mining, a modifier is denoted as qualifier in their work. The algorithms proposed by Yin [3 3 ] can be utilized to hierarchically organize the mod ifiers per -kernel -object . Moreover, we deem that the hierarchical modifiers can be further used in interactive information retrieval, like Tvarozek [26] and Wang [27] . Despite query-end intent understanding, the modifiers that act as strong indicators of diverse intents are also useful for document ranking. When providing a search result list, a search engine must balance the intents of different users. So an effective organization of aggregated modifiers is also valuable for providing a diversified result to maximize the probability that a user is satisfied. E.g., f or underspecified query  X   X  X  X  X  X  X  (Harry Potter)  X  , the pre -mined modifiers  X   X  X  X  ( game )  X  ,  X   X  X  (fiction )  X  and  X   X  X  X  X  (Chinese website )  X  give us a hint of how to rank the related documents. It is natural that the result list, which matches a topical relevance from different aspects, e.g.,  X   X  X  X  (game )  X  ,  X   X  X  (fiction)  X  and  X   X  X  X  X  (Chinese website )  X  , will enhance use r experience than a result list that matches a single topical relevance. For a more explicit query  X   X  X  X  X  X  X   X  X  X  (Harry Potter online )  X  , the modifier s  X   X  X  (read )  X  and  X   X  X  X  (watch)  X  impl y two lines of information needs desired by different users . We structurally parse a user query by introducing two intent roles rather than a raw query segmentation. Compared with NERQ merely focusing on name entities, intent role annotation involves each composing unit inside a query and differentiates the utility of words in expressing user  X  s information need. Furthermore, kernel-object is not limited to name entities but a superset of name entity, e.g., common nouns like  X   X  X  X  (film)  X  in query  X   X   X  X  X  X  (film download)  X  are also involved , which also avoids the requirement of a fine -grained classification of entities on the web. T he class -level techniques for intent inference s uffer from the following drawbacks : (1) they need to a ddress the b ottleneck of pre -defined classes when parsing overwhelming web queries in practice ; (2) they do not take into consideration the fact that significant differences exist among individual entities or queries, even if the entities of the same class . The kernel -object oriented query understanding achieves a per -kernel -object granularity. B ut it is not limited to a per -kernel -object level, the kernel -object s can also be further organized at a class -level if needed . As for Geo intent inference, i f we v iew Geo intent involving location words as a geographic personalization, there can be a commercial personalization involving shopping words or an academic personalization involving research words , which corresponds to a fine -grained classification of modif iers that reflect users  X  intents . Thus , it can be concluded that i ntent role annotation provides a more flexible framework.
 To get a general sense of the distribution of role-explicit queries among common user queries, 500 unique queries are randomly extracted from our adopted query log. Three researchers are asked to do the following job: examine whether a query is role-explicit, if it is, annotate the intent roles of the words inside this query. Finally, the annotat ed ratios are : 92.40%, 93.60% and 98.40% respectively. Going further, 88.20% queries are consistently regarded as role-explicit . The role-implicit queries can be classified into the following types: (1) Question query . E.g.,  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (the TV plays that XiaoXiao has starred in); (2) Verbose que ry . E.g.,  X  X  X  X  X  X  X  X  X  X  X  X  (cultivate children X  X  reading habits). T hese queries can X  X  be well expressed with kernel -object and modifier. There are specific algorithms [1, 18 , 19, 20, 31 ] to solve queries of these types . Our contributions are two-fold: Firstly, given an arbitrary query q , we proposed an approach to determine whether q is role-explicit or not. We extensively compared the typical machine learning models to justify the effectiveness of classifiers constructed using a set of predictive features. Secondly, given a role-explicit query r q , we present ed the SWNR model to perform intent role annotation. SWNR can be learned without any labor force. The experimental results demonstrated that SWNR can achieve a satisfactory performance. The definition of intent roles provides a new perspective on structurally parsing role-explicit queries, which facilitates query representation and understanding. Moreover, a number of applications can benefit from annotating intent roles, such as intent mining and diversified document ranking. We believe that our proposed approaches accomplish a basic work, which paves the way for further role-explicit query oriented applications. In the future, we plan to investigate how to perform diversified document ranking leveraging on annotated role-explicit queries. This research has been partially supported by the Ministry of Education, Science, Sports and Culture of Japan under Grant-in-Aid for Scientific Research (A) No. 22240021. [1] Bailey, P., White, R. W., Liu, H., &amp; Kumaran, G. Mining [2] Barr, C., Jones, R., &amp; Regelson, M. The linguistic structure [3] Bendersky, M., &amp; Croft, W. B. Discovering key concepts in [4] Boldi, P., Bonchi, F., Castillo, C., Donato, D., Gionis, A., &amp; [5] Brown, P. F., Desouza, P. V., Mercer, R. L., Pietra, V. J. D, [6] Chen, S. F., &amp; Goodman, J. An empirical study of [7] Clarke, C. L., Kolla, M., &amp; Vechtomova, O. An [8] Daniel, G., &amp; Daniel, J. Automatic labeling of semantic [9] Du, J. W., Zhang, Z. M., Yan, J., Cui, Y., &amp; Chen, Z. Using [10] Guo, J. F., Xu, G., Cheng, X. Q., &amp; Li, H. Named entity [11] Hagen, M., Potthast, M., Stein, B. &amp; Brautigam, C. Query [12] Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, [13] Ney, H., Essen, U., &amp; Kneser, R. On structuring [14] Pasca, M. Weakly-supervised discovery of named entities [15] Pasca, M. Organizing and searching the world wide web of [16] Piwowarski, B., &amp; Zaragoza, H. Predictive user click [17] Pradhan, S. S., Ward, W. &amp; Martin, H. J. Towards robust [18] Prager, J., Brown, E., Coden, A., &amp; Radev, D. Question-[19] Ren, F. J., &amp;David, B. Advanced Information Retrieval. [20] Ren, F. J. From cloud computing to language engineering, [21] Sekine, S. Extended named entity ontology with attribute [22] Silverstein, C., Marais, H., Henzinger, M., &amp; Moricz, M. [23] Song, R. H., Zhang, M., Sakai, T., Kato, M. P., Liu, Y. Q., [24] Song, Y. &amp; He, L. W. Optimal rare query suggestion with [25] Tan, B., &amp; Peng, F. C. Unsupervised query segmentation [26] Tvarozek, M., Bielikova, M. Personalized faceted [27] Wang, X. H., Tan, B., Shakery, A., &amp; Zhai, C. X. Beyond [28] Wang, X. H., Chakrabarti, D., &amp; Punera, K. Mining broad [29] Welch, M. J., &amp; Cho, J. Automatically identifying [30] Wong, P. K., &amp; Chan, C. Chinese word segmentation based [31] Yang, Y., Jiang P. L., Tsuchiya S., &amp; Ren F. J. Effect of [32] Yi, X., Raghavan, H., &amp; Leggetter, C. Discovering users X  [33] Yin, X. X., &amp; Shah, S. Building taxonomy of web search [34] Yoshihiko, G., &amp; Steve, R. Statistical language modeling. [35] Yu, H. T., Ren, F. J., &amp; Liu, S. Qualifier mining for [36] Yu, X. H., &amp; Shi, H. X. Query segmentation using 
