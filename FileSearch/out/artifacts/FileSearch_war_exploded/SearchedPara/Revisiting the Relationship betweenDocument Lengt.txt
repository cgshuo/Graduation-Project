 The scope hypothesis in Information Retrieval (IR) states that a relationship exists between document length and rel-evance, such that the likelihood of relevance increases wit h document length. A number of empirical studies have pro-vided statistical evidence supporting the scope hypothesi s. However, these studies make the implicit assumption that modern test collections are complete (i.e. all documents ar e assessed for relevance). As a consequence the observed evi-dence is misleading. In this paper we perform a deeper anal-ysis of document length and relevance taking into account that test collections are incomplete. We first demonstrate that previous evidence supporting the scope hypothesis was an artefact of the test collection, where there is a bias to-wards longer documents in the pooling process. We evalu-ate whether this length bias affects system comparison when using incomplete test collections. The results indicate th at test collections are problematic when considering MAP as a measure of effectiveness but are relatively robust when usin g bpref. The implications of the study indicate that retrieva l models should not be tuned to favour longer documents, and that designers of new test collections should take measures against length bias during the pooling process in order to create more reliable and robust test collections. H.3.4 [ Information Storage and Retrieval ]: Information Search and Retrieval Experimentation, Performance Information Retrieval, Document Length, Relevance, Eval-uation, Pooling
This paper investigates the assumed relationship between relevance and document length. As highlighted by Robert-son and Walker [14], the relationship between document rel-evance and length can be explained either by: i) the scope hypothesis , the likelihood of a document X  X  relevance increases with length due to the increase in material covered; or ii) th e verbosity hypothesis , where a longer document may cover a similar scope than shorter documents but simply uses more words and phrases.

It is currently believed that the scope hypothesis prevails over the verbosity hypothesis. This belief is supported by a number of empirical studies investigating the relationsh ip between document relevance and length. The probability of a document X  X  relevance (to an information need) is con-sidered to be positively correlated with the length of the document. For instance, Singhal et al. illustrated that doc-ument relevance increases proportionally with length acro ss a number of early TREC test collections [16]. Similar result s have also been reported for later  X  X d hoc X  test collections [ 10,
Accounting for document length during retrieval was rec-ognized as an important research topic in early indexing models [7]. Recently, it has been reported that retrieval pe r-formance can be improved through appropriate term weight-ing strategies based on document length [6]. Therefore, de-signing IR models with an a priori preference for longer doc-uments has been viewed as a way to improve retrieval perfor-mance [8, 10, 11, 12, 9, 2]. Additionally, a number of studies have proposed adjustments to well known retrieval models in order to account for document length appropriately. For instance, pivoted document length normalization [16] was defined to correct the excessive promotion of shorter doc-uments in the cosine similarity measure within the Vector Space IR model. Similarly, the probabilistic model Okapi BM25 was extended to minimise the bias towards longer documents [15]. The document length correction that is in-herent to Statistical Language Models of IR has also been studied in depth [19, 1, 12]. evance and document length in the context of the ad hoc retrieval task. However, for other tasks the relationship b e-tween document length and relevance has found to be dif-ferent. For instance, on the web entry page task in the web collections, there was no correlation found between docu-ment length and relevance [11]. Table 1: Basic statistics of the TREC adhoc collec-tions, including sizes of the bins for the document length study.

A common theme throughout these studies has been the implicit assumption made that the underlying test collec-tions are complete (i.e. the assumption that all documents i n the collection have been assessed for relevance for all topi cs). However, modern collections are incomplete due to factors related to collection size, cost and effort [6]. In this paper we re-investigate the correlation between relevance and do c-ument length in the context of incompleteness across seven TREC ad hoc test collections. We illustrate that the posi-tive correlation between document length and relevance is a consequence of assuming that test collections are complete . When incompleteness is taken into consideration a very dif-ferent trend is observed: the probability of relevance does not linearly increase with document length. Therefore, the previous trends observed cannot simply be assumed to be a causal link between relevance and document length but potentially an artefact of test collection creation. This fi nd-ing provides strong evidence refuting the scope hypothesis and the naive heuristic of favouring longer documents for retrieval performance improvement.

The remainder of this paper is as follows. In section 2, we first replicate and extend the original collection wide analy-sis of document length and relevance performed in previous papers [16, 11], to include all available TREC ad hoc col-lections available. We then go beyond existing analysis by taking into account test collection incompleteness by per-forming a pool wide analysis. This analysis shows distinc-tive trends providing evidence to refute the hypothesis: a strong correlation exists between relevance and document length. In section 3, we perform an extensive investigation into whether the length bias within the pools affects the comparison between systems in terms of both ranking and performance. Finally, in section 4 we summarize the results of this study and discuss the implications of this work on the comparison, design and evaluation of IR systems.
It has been commonly regarded that longer documents have an increased likelihood of relevance. This is a long held belief stemming from studies such as the one reported in [16], where it was shown that relevant documents tend to be longer than documents within the entire collection. Similarly, in [10, 11], the authors show that the probabilit y of relevance is correlated with document length for a num-ber of adhoc and web TREC collections. In order to study these issues in depth and over time, we consider the seven adhoc collections from the well-established TREC bench-marks (from TREC-2 to TREC-8). We now outline this analysis. index the collections. Documents were preprocessed using Porter X  X  stemmer [13] but no stop wording was applied. As a result, all lengths reported are based on the count of all the tokens occurring within the documents. To compute the different length patterns (i.e. the distribution of rele -vance given length), the methodology designed in [16] was adapted as follows. For each test collection, we ordered the documents in the collection by the length and then divided them into equal size bins. Next, we computed the probabil-ity of relevance associated to each bin. We set the bin size to be 2% of the collection size (i.e. 50 bins per collection). The statistics about each test collections, the TREC topics and bin sizes are reported in Table 1.
The computation of the relevance pattern in the collection is performed using the corresponding set of relevance judg-ments available for each TREC adhoc test collection. The number of (query, relevant document) pairs are counted and p ( d  X  bin i | d is rel ) is computed as the ratio between the number of pairs that have their document from bin i , and the total number of (query, relevant document) pairs. This relevance pattern is shown as a solid line (labeled as Rel. ) in fig. 1. These curves show the trends reported also in [16, 10]: the probability of relevance grows with the length of the documents . The general tendency is that the bins with longer documents have a higher probability of relevance.
One might be tempted to infer that relevance evolves as shown in the solid lines in fig. 1. However, caution must be taken when considering these patterns, as these test collec -tions are created through a process known as system pooling. System pooling is used to address the intractability of the completeness assumption [6]. Pooling is a focused sampling of the document collection that attempts to discover all po-tentially relevant documents with respect to a search topic (e.g. approximate the actual number of relevant documents for a given topic). To do so, a number of (diverse) retrieval strategies are combined to probe the document collection for relevant documents. Each system will rank the collec-tion for a given topic, then the top  X  documents from the subsequent ranked lists are collated, removing duplicates , to form a pool of unique documents. All documents in this pool are then judged for relevance by human assessors.
As a consequence, the question we need to ask is: is the set of documents assessed representative of the collection in terms of length? For example, if the assessed documents are longer in length relative to the collection then there is an increase in uncertainty concerning the relevance of shorte r documents. In this case, the probability of relevance as-sociated to shorter documents may be inaccurate because a smaller proportion of shorter documents will have been assessed for relevance.

To determine whether this is problematic, we counted the number of assessed documents per bin and computed the probability p ( d  X  bin i | d is judged ) as the ratio between the number of assessed documents from bin i , and the total number of assessed documents. This assessment pattern is
TREC 2
TREC 3
TREC 4
TREC 5
TREC 6
TREC 7 TREC 8 Figure 1: Probability of finding relevant (solid line), assessed (dash-dot line) and collection (dotted line) documents given each bin in the seven TREC adhoc collections. The X axis denotes the bins in increas-ing order of length. The Y axis denotes the proba-bility of a document being drawn from this bin for a particular group (i.e. the entire collection (Col.), the pool of judged documents (Pool), or the set of relevant documents (Rel.)) presented in fig. 1 as a dash-dotted line (labeled as Pool ). If the documents were a representative sample from the collec-tion, then we would expect that the number of documents in each bin would be approximately equal. However, the figure indicates that the bins representing longer document s contain more assessed documents in comparison to those bins with shorter documents. Overall, the bins with the longest documents contribute more to the pool of assessed documents. Actually, in terms of the proportion of their contribution there is a very substantial difference between the last few bins and the others. In fact, between 10% and 20% of the assessed documents were found in the last bin. This suggests that the reason for longer relevant documents may be because more longer documents were assessed, as opposed to longer documents being increasingly more likely to be relevant.

The mean and median of document lengths for the col-lection, pool of assessed documents, set of relevant, and se t of (assessed) non-relevant documents are reported in Table 2. To determine the differences between the length distri-butions, we formally tested whether the distribution of doc -ument length within these sets were statistically different using the Mann-Whitney U-test. In all cases, the hypothe-sis that the documents come from the same distribution was rejected at a 5% significance level. Further, we found that the relevant documents are significantly longer on average than the entire document collection and that the pool of assessed documents contains documents significantly longe r on average than both the set of relevant documents and the entire document collection.

Since the pool of assessed documents is not representative of the collection in terms of document length, as it is heavil y biased towards longer documents, the finding that longer documents are more likely to be relevant may be an artefact of the pooling process. In other words, the pool of assessed documents is over-represented by longer documents. As a result, there is a higher likelihood of longer documents bei ng judged as relevant, which in turn over-inflates the estimate of the probability of relevance for longer documents.
A more appropriate way to estimate a relevance pattern consists of restricting the analysis to the set of assessed d oc-uments for each topic. Since all documents in this set are judged, there is no need to take any assumption about the relevance/non-relevance of non-assessed documents.
The probability of relevance within the set of judged doc-uments can be computed as follows. The probability p ( d is rel | d  X  bin i &amp; d is judged ) is estimated as the number of (query, relevant document) pairs whose document belongs to bin i divided by the number of (query, judged document) pairs whose document belongs to bin i . This analysis is re-liable and robust in comparison to the approach reported above. This is because for each bin the relevance counts are divided by the number of assessments in the bin, and therefore, the final curve is not influenced by the skewed distribution of the number of assessments across bins.
The resulting relevance pattern is shown in fig. 2 (right-hand side). For comparison purposes, the relevance pattern computed assuming that non-judged documents are non-relevant, p ( d is rel | d  X  bin i ) , is also shown in the figure (left-hand side). There is a substantial difference between the trends shown in these two sets of graphs. The left-hand sets of (assessed) non-relevant documents
TREC 2
TREC 3
TREC 4
TREC 5
TREC 6
TREC 7 TREC 8 i &amp; d is judged ) , respectively. graph provides evidence to accept the scope hypothesis, as it clearly shows that probability of relevance increases wi th length. However, the right-hand graph, which is based on only the assessed documents provides strong evidence to re-ject the scope hypothesis.

The distribution of relevance within the assessed docu-ments (right-hand graph) tends to be more uniform than the distribution of relevance in the p ( d is rel | d  X  bin tern (left-hand graph). In the right-hand graph, the bins with highest probability of relevance are those containing documents of average length (i.e. bins 20-40) and the prob-ability of relevance falls dramatically for the longest doc u-ments (last bins). For the early bins (1-20) the probability of relevance is usually somewhat lower, presumably because a document needs to be large enough to provide sufficient details in order to be relevant. Also, it was observed that the probability of relevance tends to be flatter in the latter TREC collections.

This analysis provides important new insights into the is-sue of document length and relevance. In particular, this study indicates that previous empirical evidence supporti ng the scope hypothesis is over-exaggerated. In the next sec-tion, we study whether these length biases affect the evalu-ation of systems that retrieve many short documents.
In this section we evaluate whether the document length biased pools associated with ad hoc test collections affect evaluation. In other words, do systems that retrieve longer documents on average perform better than systems that re-trieve shorter documents? To answer this question, we con-duct a thorough analysis on the reliability and robustness for system comparisons under incompleteness.
As explained in section 2, the creation of a complete test collection is impractical [6]. Therefore, only a small focu sed sample of documents are judged (i.e. assessed) for relevanc e. The goal of pooling is to maximise the number of relevant document found within a collection while minimising the amount of effort required to perform the assessments. Thus, a small subset of the collection can be assessed in order to obtain a relatively good estimate on the number of relevant documents in the collection.

A fundamental assumption required for pooling is that each run that contributes to the pool is assumed to be inde-pendent [17]. If runs are independent then the more runs participating in the pooling the more diverse the relevant documents found are. However, in the construction of the ad hoc TREC collections, the independence assumption has been somewhat relaxed. Many pooled systems implement similar retrieval strategies and, additionally, runs from the same group are likely to show high overlap. This system reinforcement bias and the effect of pooling on systems that did not participate in the pool ( system omission ) have been previously evaluated in the literature [20, 18]. It has been shown that the relative order of the systems is quite stable (i.e. the system rankings constructed in decreasing order o f a given performance measure such as Mean Average Preci-sion (MAP) are not significantly affected by reinforcement or omission). However, no one has specifically considered whether the length bias within pools affects the evaluation of systems. For instance, it might be the case that pooling is fair to most of the omitted systems (i.e. their position in th e rank would not change significantly when they are included into the pool) but it is unfair to a few omitted systems that favor short documents.

In the remainder of this section, we test to determine whether the pools from the ad hoc test collections enable robust comparisons; such that the relative performance of systems that favor longer or shorter documents is not over or underestimated because of the non-representative sam-ple used to form the pools. In order to examine whether the pools provide a robust evaluation against length, we have created different samples of assessments from the offi-cial judgments with varying distribution of document lengt h. This helps to study the influence of document length on the evaluation. The relative changes in system rankings and the magnitude of the change in performance scores are carefully analyzed. In particular, we study the correlation between these variations and the length of the documents retrieved by the systems involved.
In what follows, p off ( . ) refers to the probabilities com-puted from the original set of official assessments whereas p sam ( . ) refers to the probabilities computed from a given sample of the original assessments.

To evaluate how sensitive the relative performance of the systems is with respect to the length of the assessed doc-uments, we created the following samples from the official relevance assessments: Table 3: Size of the original assessments vs Size of the sampled assessments: towards_prel_in_pool ( tpip ), long_removed ( lr ), short_removed ( sr ) and tails_removed ( tr )
These samples help to study the effect of document length bias on the relative comparison of systems: each sample simulates a pooling scenario related to a specific form of length bias. The influence of this bias was then evaluated over a large number of system comparisons. The last three samples are straightforward to construct. We ordered the documents in the pool in decreasing order of length and the top 25% documents are removed ( long_ removed ), the bottom 25% documents are removed ( short_ removed ) or both the top 25% documents and the bottom 25% documents are removed ( tails_removed ). This re-moval process was done globally rather than on a query-
To obtain the towards_prel_in_pool sample we first com-pute the sample size. This is determined by the number of assessments in the original pool, the distribution of as-sessments across the bins and the distribution p off ( d is rel | d  X  bin i &amp; d is judged ) (which is the probability dis-tribution we want the sample to follow). Next, we draw randomly documents from the original assessments, ensurin g that each bin obtains the required number of assessments.
The new sample of assessed documents now reflects a pat-tern similar to the relevance pattern within the set of origi nal assessments. More specifically, when taking these sampled judgments to estimate the probability of relevance within the collection as a whole (i.e. assuming that non-judged documents are non-relevant) one can observe that the shape of the curve reflects now the probability pattern within the assessed documents. This is shown in fig. 3, whose rele-vance pattern resembles the one shown in fig. 2 (right-hand graph). This means that the new set of judgments is more representative of the relationship between relevance and d oc-ument length.

The size of the original assessments and the size of all these sampled assessments are reported in Table 3.
In this section we check whether or not the measured rel-ative performance of TREC participants is reliable. As ar-gued above, one might be tempted to think that systems biased towards short documents could be harmed by the high population of long documents in the TREC pools. For by-query sampling and the trends and conclusions found are the same as those discussed here. TREC 2
TREC 3
TREC 4
TREC 5
TREC 6
TREC 7 TREC 8 Figure 3: Probability of relevance given bin com-puted from the towards_prel_in_pool sampled assess-ments. The X axis represents the bin number and the Y axis represents p ( d  X  bin i | d is rel ) . this part of the study, we used the pooling runs submitted to the TREC adhoc task from TREC-5 to TREC-8, which ments and the four sets of assessments reported in the last section, we computed the system rankings using both bpref and MAP. Unlike MAP, bpref does not assume that non-assessed documents are non-relevant but estimates perfor-mance using the judged set of documents only [4].
The association between the different system rankings was quantified applying Kendall X  X  tau. A similar method was taken in [18], where Voorhees evaluated the stability of sys -tem rankings with respect to judgments created by differ-ent sets of human assessors. Kendall X  X  tau is defined as the minimum number of transpositions required to turn one ranking into the other. This number is normalized such that two identical rankings produce a correlation equal to 1, whilst the correlation between a ranking and its inverse is equal to -1. The correlation results are presented in Ta-ble 4. The table reports the correlation between the rank-ing produced from the official assessments and each ranking for earlier TREC years. Therefore, we focus our pooling analysis on TRECs 5 to 8. Table 4: Kendall X  X  tau correlation between the official system rankings and the system rankings obtained with the sampled assess-ments: towards_prel_in_pool ( tpip ), long_removed ( lr ), short_removed ( sr ) and tails_removed ( tr ). produced from every sample of assessments and every per-formance measure. Additionally, the table informs about the p-values obtained for testing the hypothesis of no cor-relation. The results are very conclusive. For all collecti ons and pairwise comparisons, there is a very high correlation between the official system rankings and the rankings pro-duced from different length-biased samples. This provides evidence to show that the official rankings are relatively in-sensitive to the distribution of document lengths in the poo ls of assessed documents. This is good news to current IR eval-uation standards as it shows that, although there is a bias in favour of long documents in these pools, this bias does not significantly affect the general conclusions drawn in TREC reports.

Observe also that the correlations computed with MAP tend to be lower than the correlations computed with bpref. This demonstrates that bpref is less sensitive to the releva nce judgments utilized than MAP: the resemblance between the bpref system rankings computed from the samples and the official bpref rankings is higher than the resemblance be-tween the MAP system rankings computed from the sam-ples and the official MAP rankings. This is evidence to support bpref as a robust measure to deal with incomplete judgments.

These correlation values demonstrate clearly that there are only minor changes in the systems rankings. Still, it could be the case that the few systems that obtain a sub-stantially different rank are somehow affected by its retriev al trends against document length. To further check this issue , we ordered the runs in increasing order of the average length of the documents retrieved in the top 100. Next, the runs were divided into four bins: the first bin contains the runs that retrieve shorter documents while the last bin contains the runs that retrieve longer documents. For each bin, we computed the average difference between the system rank obtained from the official judgments and the system rank obtained with the towards_prel_in_pool sample. If this difference is positive then the system was promoted by the sample. In contrast, a negative value means that the sys-Figure 4: Difference between the rank obtained by a system with the official assessments and the rank obtained with the towards_prel_in_pool sampled as-sessments. The figures refer to the mean difference computed across the runs in the bin. tem was demoted by the sample. This permits to analyze whether or not there is any promotion/demotion tendency against the length retrieval behaviour of the runs. The re-sults are reported in fig. 4. Two main observations can be made here. First, the system rank computed using bpref is quite stable across bins, meaning that there is not any ten-dency to promote or demote runs that retrieve either shorter or longer documents. Second, with MAP, the runs that re-trieve shorter documents (bin 1) show a clear tendency to be promoted by the sample, while the runs retrieving longer documents (e.g. bin 4) tend to be demoted by the sample. This means that the official judgments tend to underrate (overrate) the runs that retrieve shorter (longer) documen ts when used to rank systems with MAP. Although the cor-relations reported above show that the rankings with the official judgments and the rankings with the sampled judg-ments are quite similar, we found here that the variations are not distributed uniformly across runs but there is a ten-dency to harm runs that retrieve shorter when the official judgments are used. This is strong evidence to reject MAP as a performance measure to rank systems because MAP, together with the biased pools, establish a preference for particular types of systems. In contrast, bpref handles wel l the incompleteness of the judgments and treats fairly the runs that retrieve shorter documents.
Having analyzed pooling for the systems that contributed to the pools, it is also interesting to analyze the effect of length for runs that did not have an opportunity to con-tribute to the pool. A given test collection might be reliabl e for relative comparison of the pooled systems but, in con-trast, the collection might be not reusable because it does not handle fairly the non-pool runs.
To investigate this issue, we adopt the methodology pro-posed by Zobel [20], which was later referred to as Leave Out Uniques (LOU) [3]. Each run that contributed to the pool is evaluated first against the official assessments and, next, the same run is evaluated using the pool with the documents contributed only by the run removed. The latter evaluation simulates the situation where the run did not participate in the pools. The difference in the evaluation ratios be-tween both cases (averaged over all runs) is a measure of the degree to which contributing to the pool improves effec-tiveness. This measurement is a natural way to check the reusability of a given collection.

Since runs from the same group tend to show a high over-lap in the retrieved documents, it is also interesting to tes t an alternative to the LOU test that consists of removing all documents from the judgment set that were contributed solely by runs from the same organization. This method was proposed in [3] as a more stringent variant of the LOU test. In the following, LOU refers to the original test as proposed by Zobel whereas LOUG ( Leave Out Uniques Group ) stands for the group-oriented variant.

Table 5 presents the results obtained with bpref and MAP for both LOU and LOUG test. It is interesting to observe that there are not major differences here between bpref and MAP. Both measures show small improvements in perfor-mance, similar to those reported in [20]. This supports the belief that TREC collections are reusable because, on av-erage, participating in the pool does not produce a major benefit in terms of performance.

It is interesting to observe that TREC-7 and TREC-8 ap-pear more reliable because the average improvements are smaller in these collections. This can be explained as follo ws. First, the mean length of the documents in the pools is lower in TREC-7 and TREC-8 than in TREC-5 and TREC-6 (re-fer to Table 2). Second, if we consider the ratios between the successive TREC years, we can consider the trend over time. Figure 5 illustrates that the ratio tends to decrease avg ( col ) ( median ( col ) ) is the average (median) docu-ment length in the whole collection, while avg ( pool ) ( median ( pool ) ) is the average (median) document length of the assessed documents.
 Figure 5: Evolution across the years of the ratios of mean and median document length in collection and pool. with each new TREC collection. This observation would ap-pear to indicate that over time those IR systems contribut-ing to the pooling process compensate more appropriately for document length, such that pools from the latter TREC collections are less biased in terms of length and more rep-resentative of the collection.

Returning to the main point of our analysis, we were in-terested in analyzing carefully the improvements in perfor -mance against these document length retrieval trends. To do so, we grouped the system runs into bins using the same strategy explained in the previous section. Figure 6 plots the relative improvement in performance (averaged across the runs in the bin) obtained from the participation in the pool against the bin number. The graph includes a plot for each combination of performance measure and type of leave-out test. In this way, we can check whether or not there exists any length effect. If a given set of non-pool runs promoting short documents was unfairly penalized by the of-ficial judgments then it would get substantial improvements Figure 6: Improvement in performance computed as the percentage difference between MAP/bpref on the official pool and MAP/bpref on the LOU/LOUG modified pool. The figures refer to the mean im-provement computed across the runs in the bin. when included in the pools. Conversely, non-pool runs pro-moting long documents would obtain little benefit because current pools are mostly populated by long documents. In the figure, we can observe that the improvements are tiny and relatively uniform across bins in TREC-7 and TREC-8. In contrast, in TREC-5 and TREC-6, the runs retrieving shorter documents (bins 1 and 2) tend to present higher im-provements. This indicates that, in these collections, the re is a disproportionate tendency to underrate non-pool runs that retrieve shorter documents. In conjunction with the trend analysis above, this provides supporting evidence to suggest that the more representative the pool is in terms of length the more reusable the collection will be.
This paper provides statistical evidence to reject a com-monly held assumption made in the IR literature [16, 10, 9, 8, 2], the scope hypothesis: the hypothesis that the proba-bility of relevance increases with document length in ad hoc retrieval. The link between length and relevance has been based on a series of empirical studies. These studies make the implicit assumption that test collections are complete , overlooking the methods used to construct the test collec-tions such as system pooling. As a result, the presence of longer documents returned from pooling (relative to the av-erage document length in the collection), strongly influenc es the shape of the relevance pattern.

In the context of test collections, this investigation illu s-trated that for all TREC ad hoc collections, the pool of assessed documents have a larger frequency of longer doc-uments in comparison to the test collection. However, the probability of relevance associated with the longest set of documents is lower than the probability of relevance associ -ated to the smallest set. This is an indication that systems forming the assessment pool bias towards longer documents. The pool is therefore a biased sample of the collection. More importantly, the assessment pool of documents is not rep-resentative when considering the sample of relevance and judged documents. Additionally, the probability of rele-vance given document length is more uniform in the later collections, where the bias towards longer lengths is less e x-treme. This is an important result indicating that relevanc e is not strongly associated with document length.

Therefore, it was crucial to assess whether these length-biased pools were problematic for comparing IR algorithms. Here, we showed that the rankings of the systems partic-ipating in TREC were not significantly affected by such bias. However, our study did indicate that when using MAP to estimate system performance, the performance of those systems retrieving longer documents tended to be overes-timated, while the performance of those system retrieving shorter documents was underestimated. In contrast, the sys -tem rankings computed using bpref are quite insensitive to this document length bias. This highlights an important dis -tinction between bpref and MAP. In comparison to measures such as MAP, bpref does not make the assumption that those documents not assessed are not relevant. Instead, bpref es-timates performance using only the judged set of documents thereby minimising potential bias, and providing a more ro-bust IR measure.
 A further general finding from the study indicated that the TREC-7 and TREC-8 collections appear to be more reusable than TREC-5 and TREC-6. This is because the earlier col-lections tend to underestimate significantly the performan ce of those systems that were not included within the pooling process which retrieve shorter documents on average. As stated previously, to minimise this potential system omis-sion bias, metrics that account for incompleteness, such as bpref, should be considered over MAP.

In the context of retrieval algorithms, this study indicate d that the probability of relevance does not necessarily in-crease with document length. Therefore, the assumption that longer documents are more likely to be relevant, im-plicit in many retrieval models, should be reconsidered. Fo r example, a model that sets a document prior that grows in-creasingly with document length will appear to obtain bet-ter performance. However, this is because the model is over-fitted towards the set of length-biased relevance assessmen ts. As a result, the retrieval model may have limitations when generalising to other topics, collections, or even to updat es in the collection.

This paper opens new research lines into IR evaluation techniques. We argue that this bias in the pooling process, towards longer documents, should be corrected. Instead of simply taking the union of the top  X  retrieved documents from a series of contributing systems, the pools could be constructed taking into account the shape of the relevance pattern against document length. In other words, form a representative sample taking into account the actual rele-vance pattern against document length. This line of researc h is similar to the one followed in [20, 5], where the authors designed new pooling methods that find more relevant doc-uments in fewer total documents judged. In [20], Zobel ap-plied variable-depth pooling to judge more documents for topics that are predicted to have many relevant documents. In [5], the authors suggest to insert more documents into the pools from the runs that returned more relevant docu-ments recently. However, none of these studies consider any document length policy. Our future work will consider these issues.

Finally, another interesting line of research consists of c on-ducting a real world study into length and relevance and analyzing what kind of relationship exits outwith the lab-oratory setting e.g. within the context of an information seeking study.
This work was partially supported by projects TIN2005-08521-C02-01 (Ministerio de Educaci X n y Ciencia, Spain) and PGIDT07SIN005206PR (Xunta de Galicia, Spain). We thank the TREC project managers for providing us the re-trieval runs needed to conduct our study. [1] L. Azzopardi and D. Losada. Fairly retrieving [2] R. Blanco and A. Barreiro. Probabilistic document [3] C. Buckley, D. Dimmick, I. Soboroff, and E. Voorhees. [4] C. Buckley and E. Voorhees. Retrieval evaluation with [5] G. Cormack, C. Palmer, and C. Clarke. Efficient [6] D. Harman. TREC:Experiment and Evaluation in [7] S. Harter. A probabilistic approach to automatic [8] D. Hiemstra. A probabilistic justification for using tf x [9] J. Kamps. Web-centric language models. In Proc. [10] W. Kraaij and T. Westerveld. Tno/ut at trec-9: how [11] W. Kraaij, T. Westerveld, and D. Hiemstra. The [12] D. Losada and L. Azzopardi. An analysis on document [13] M. Porter. An algorithm for suffix stripping. Program , [14] S. Robertson and S. Walker. Some simple effective [15] S. Robertson, S. Walker, S. Jones, [16] A. Singhal, C. Buckley, and M. Mitra. Pivoted [17] K. Sparck Jones and C. J. van Rijsbergen. Report on [18] E. Voorhees. Variations in relevance judgments and [19] C. Zhai and J. Lafferty. A study of smoothing [20] J. Zobel. How reliable are the results of large-scale
