 classification (without labeled examples) which combines the strengths of both  X  X earn-based X  and  X  X exicon-based X  approaches as follows: we first use a lexicon-based technique to label a portion of informative examples from given task (or domain); then learn a new supervised classi fier based on these labeled ones; finally apply this classifier to the task. The experimental results indicate that proposed scheme could dramatically outperform  X  X earn-based X  and  X  X exic on-based X  techniques. I.2.7 [ Artificial Intelligence ]: Natural Language Processing Algorithms; Performance; Experimentation Sentiment Classification; Opini on Mining; Information Retrieval 
Up to now, many researches have been conducted on document sentiment classification. These researches have fallen into two categories [1]. The first ( X  X upervised techniques X  or  X  X achine learning techniques X  or  X  X earn-base d method X ) [1][2] attempts to train a sentiment classifier base d on occurrence frequencies of the various words in the documents. Pang X  X  researches [1] indicate that standard machine learning me thods perform very well, even definitively outperform human-produced baselines. However, supervised sentiment classifier typically requires a large amount of labeled training examples. As a result, when confronted with a task (or domain) without any labeled examples, supervised learning doesn X  X  work at all. 
The other approach ( X  X nsupervis ed techniques X  or  X  X emantic orientation X  or  X  X exicon-based met hod X ) [3-5] is to classify words overall positive/negative score for the text. If a document contains more positive than negative terms it is deemed as positive, and if the number of negative terms exceeds the number of positive unsupervised learning because it doesn X  X  require any labeled examples. Accordingly, it seems to be well suited for sentiment analysis task without providing an y labeled training data. In most cases, however, its pr ediction accuracy is definitively limited because it heavily depends on lingual experts. 
We see that both supervised a nd unsupervised techniques have their strength and weakness. In this work, we propose a novel scheme for sentiment classificati on which combines the strengths of both approaches as follows : we first use unsupervised technique to label a portion of informative examples from given task (or domain); then learn a ne w supervised classifier based on scheme is very flexible, whic h only needs one supervised and unsupervised technique, and ther e is no change to the two techniques in any way. As a result, any machine learning and semantic orientation techniques can work together effectively under this general framework. 
Detailed algorithm for proposed scheme is presented in Figure 1. The basic idea is to use one unsupervised technique to label some informative unlabelled exam ples in new domain and train a new supervised classifier over these selected examples. In this scheme, the parameter  X  Ratio  X  indicates what percentage of new-domain data shall be picked out as informative examples. 
It is believed to be possible th at proposed scheme could work effective and robust if the following conditions hold: 1. New-domain unlabeled data is abundant enough for selection of informative ones. 2. Unsupervised technique is e ffective to pick out informative ones from new-domain unlabeled data. 3. Supervised learning techniques can train a high-precision classifier over these selected examples, even when they consists of some outliers. 
In order to effectively detect informative examples using lexicon-based method, we make a simple assumption: for one example, the larger the negative term number T N , the more likely it is drawn from negative class; the larger the positive term number T P , the more likely it is taken from positive class. 
However, this presumption doesn X  X  hold when the length difference among different reviews is very large, because it is the T N or T P . To tackle this problem, we normalize (or divide) T and T P so that the adverse effect of length difference can be counteracted to a high degree. This is the basic idea of relative term number. Formally, we define Negative Relative Term Number ( T RN ) and Positive Relative Term Number ( T following, 
Up to this point, we can make a refined assumption that, for negative class; the larger the T RP , the more likely it is taken from positive class. According to this assumption, we propose Relative Term Number Ranking method ( RTNR ): we first rank T examples, and assign top n /2 largest examples as negative; then defined number indicating how ma ny examples in new domain shall be picked out as informative ones) 
A crucial problem underling this method is to acquire a semantic lexicon. The commonly us ed method is to make use of existent sentiment lexicon or dic tionary, such as General Inquirer (GI) [4] or Chinese Network Sentiment Dictionary (CNSD) NTU Sentiment Dictionary (N TUSD) [5]; another more complicated method is to use sema ntic orientation to construct a new semantic lexicon, such as Turney X  X  method [3]. In the following, we elaborate NTUSD. 
In our experiment, we onl y used NTUSD lexicon. 
After picking out informative example from new domain, we treat sentiment classification simply as a special case of topic-based categorization (with the two  X  X opics" being positive sentiment and negative sentiment ), and employ typical classifiers, such as Centroid Classifier [6], to learn a new classifier that is suitable for new domain. 
To implement Centroid Classifier , we use the standard bag-of-words framework. In this framework, each document d is considered to be a vector in the term-space. For term weight we employ normalized TFIDF . 
To validate the effectiveness and robustness of proposed scheme, we collected four domain-specific datasets: Movie Reviews (Mov), Computer Review s (Comp), Education Reviews (Edu) and House Reviews (Hou). The details are listed in Table 1. 
To conduct our experiments, we use 50% of  X  X ovie Reviews X  as old-domain labeled training set, and halve each of the other datasets into unlabeled set a nd testing set. For supervised techniques, we train Centroid Classifier using only old-domain labeled data, i.e.,  X  X ovie Reviews X  . With respect to unsupervised techniques, we only use Ku X  X  sen timent dictionary  X  X TUSD X  [5]. For the sake of simplicity, we call this method  X  X exicon based method X  in the rest of this pape r. Note that this method doesn X  X  require any labeled or unlabeled data. 
Table 2 shows the results of experiments comparing proposed scheme with supervised techni ques and unsupervised technique. For proposed scheme, we use Centro id Classifier as supervised technique and the Ratio is set to 0.4. As mentioned before, the parameter  X  Ratio  X  indicates what percentage of new-domain data shall be picked out as informative examples. 
Table 2 shows that Centroid Cla ssifier doesn X  X  work at all on domains without labeled traini ng data. This observation means that, in order to train a high-precision classifier, it is necessary to acquire a large amount of labeled data for one domain. 
Although having not trained over old-domain labeled data and new-domain unlabeled data, Le xicon based method performs much better than supervised techni ques. This result indicates that Lexicon based method doesn X  X  rely on training data and is able to provide robust performance for domains without any labeled training data. 
As expected, proposed scheme doe s indeed provide much better performance than supervised a nd unsupervised techniques. For example, the averaged accuracy of proposed scheme outperforms Centroid Classifier by 14 per cent, and beats Lexicon based method by 8 percent. The result is very encouraging and of enormous value in sentiment-anal ysis applications that require high-precision classification but ha rdly have any labeled training data. 
The results described in this pape r lead us to believe that the combination of supervised and uns upervised techniques is indeed useful for sentiment classificati on without labeled training data. We have shown that proposed scheme outperforms than supervised and unsupervised techniques. techniques indeed improves the classification accuracy using unlabeled data, there is a lot of room for improvement. For example, RTNR is not the best strategy for picking out informative examples; the size and quality of sentiment lexicon is severely dependent on some bias resources such as WordNet or Internet. This work was mainly supported by special fund of Chinese Academy of Sciences,  X  X esearch on Opinion Mining of Web Text X , under grant number 0704021000 and one another project, i.e., 2004CB318109. [1] B. Pang, L. Lee, et al. T humbs up? Sentiment classification using machine learning techniques. EMNLP, 2002. [2] A. Aue and M. Gamon. Custom izing Sentiment Classifiers to New Domains: a Case Study. RANLP. 2005. [3] P. D. Turney. Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervis ed Classification of Reviews. ACL, 2002. [4] P Philip J. Stone, Dexter C. Dunphy, et al. The General Inquirer: A Computer Approach to Content Analysis. The MIT Press, 1966. [5] L. Ku, Y. Liang, et al. Op inion Extraction, Summarization and Tracking in News and Blog Corpora. AAAI-2006 Spring Symposium on Computational Approaches to Analyzing Weblogs. [6] E. Han and G. Karypis. Centroid-Based Document Classification Analysis &amp; E xperimental Result. PKDD 2000. 
