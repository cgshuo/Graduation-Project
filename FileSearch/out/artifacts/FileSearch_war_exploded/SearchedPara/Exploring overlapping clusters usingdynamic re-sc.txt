 REGULAR PAPER Mei Kobayashi  X  Masaki Aono Abstract Until recently, the aim of most text-mining work has been to understand major topics and clusters. Minor topics and clusters have been relatively neglected even though they may represent important information on rare events. We present a novel method for exploring overlapping clusters of heterogeneous sizes, which is based on vector space modeling, covariance matrix analysis, random sampling, and dynamic re-weighting of document vectors in massive databases. Our system addresses a combination of difficult issues in database analysis, such as synonymy and polysemy, identification of minor clusters, accommodation of cluster overlap, automatic labeling of clusters based on their document contents, and the user-controlled trade-off between speed of computation and quality of results. We con-ducted implementation studies with new articles from the Reuters and LA Times TREC data sets and artificially generated data with a known cluster structure to demonstrate the effectiveness of our system.
 Keywords Clustering  X  Covariance matrix analysis  X  Text mining  X  Dimensional reduction  X  Latent semantic indexing (LSI)  X  Principal component analysis (PCA)  X  Random sampling  X  Vector space modeling 1 Introduction The proliferation of massive databases has created unforeseen challenges for many enterprises. Among these is the need to develop tools for analyzing massive repos-itories of heterogeneously formatted documents that have been generated by many different people and machines. Some successful methods for retrieving and ana-lyzing information have been developed by the text-mining community. Until re-cently, the primary aim has been to understand major topics and major trends. We present a novel method for mining overlapping clusters of heterogeneous sizes that overcomes some of the difficulties associated with exploration of massive databases, as described below.
 semy problems is expensive and time-consuming for large data sets. Synonymy object in most languages, and polysemy refers to the fact that some words have multiple, unrelated meanings. The failure to account for synonymy leads to many small, disjoint clusters, some of which should be clustered together, while a fail-ure to account for polysemy can lead to clustering of unrelated documents. Het-erogeneous sources of input add an additional layer of complexity to the problem since each source will introduce specialized jargon and slang that may not appear in pre-existing dictionaries. Our system addresses the synonymy and polysemy problems through a dimensional reduction technique based on document attribute covariance matrix analysis.
 ysis, even though major topics are often well known to practioners in the field 1 (e.g., through interactions with customers by sales persons). In contrast, the in-formation on minor clusters 2 is usually not known, and until recently, their dis-covery and analysis have been neglected. However, they may be just as valuable for business and government planning, as noted by Sakano and Yamada [ 23 ]. For example, minor clusters in a customer survey database may represent emerging trends or long-term, minor concerns that may contribute to customer dissatisfac-tion, or minor clusters may represent loyal, so-called gold customers or high-risk customers who are likely to default on a loan. In scientific databases, minor clus-ters may aid in the accurate diagnosis of diseases or prediction of natural disasters. Our system has been specifically designed to identify and label minor clusters. based. Our method does not partition document collections. In the late 1990s Zamir and Etzioni [ 27 ], Hearst [ 11 ], Kumar and Ghosh [ 18 ]  X  researchers from the Web document search, information visualization and text-mining communities X  noted that large data sets contain documents that naturally belong to a number of different clusters or classifications. Since overlaps correspond to natural relationships between different clusters, information about them is crucial for understanding the content of databases. Moreover, accommodation of cluster overlap may prevent incomplete (and possibly misleading) database content anal-ysis. Clusters that have substantial overlap with other clusters may be trimmed so much by partitioning algorithms that the fraction of the cluster that remains may be mistaken for noise or placed arbitrarily into any cluster. Our system finds and automatically labels both major and minor clusters that may or may not overlap. ory requirements but lead to a slight decrease in the number of clusters found. We leave it to users to select the best method for their application based on their own cost-benefit analysis.
 background material used in our work: vector space modeling, dimensional re-duction techniques, and clustering methods. The second section reviews work on minor cluster mining. The third section is a description of the main components of our system. Results from our implementation studies using artificial and real data are given in the final section along with a summary of our findings, and we discuss possible directions for future research. 1.1 Vector space modeling and dimensional reduction Vector space modeling has become a standard tool for information retrieval sys-tems since its introduction by Salton [ 24 ] over three decades ago. One of the ad-vantages of the method is that it enables the relevance ranking of documents of heterogeneous format (e.g., text, multilingual text, images, audio, video) with re-spect to user input queries as long as the attributes are well-defined characteristics of the documents. In our studies we use term frequency inverse document fre-quency (tf-idf) weighting , which takes into account the number of occurrences of each attribute term in each document as well in the collection of documents as a whole. We use the cosine of the angle defined by the document vectors to  X  X ea-sure X  the similarities between documents. For massive databases this similarity ranking method requires too many computations and comparisons for real-time output. One approach to solve this  X  curse of dimensionality  X  problem is to reduce the dimension of the mathematical model by projecting into a subspace of suf-ficiently small dimension to enable fast response times, but sufficiently large to retain characteristics for distinguishing the features of individual documents. We consider two dimensional reductional algorithms that are further enhanced to en-able minor cluster mining in our system. They are latent semantic indexing (LSI) and covariance matrix analysis (COV). 3 document-attribute matrix model for the database, with the i th, j th entry a ( i , j ) representing the importance of the i th term in the j th document. The fundamental idea of Deerwester et al. [ 5 ] in LSI is to reduce the dimension of the informa-tion retrieval or text-mining problem to k ,where k M , N , by projecting into the space spanned by the rows of the closest rank-k matrix to A in the Frobenius norm. The projection is carried out by constructing a modified matrix A k , from the k largest singular triplets of A , i.e., the singular values  X  i ; i = 1 , 2 ,..., k ,and their corresponding vectors. The matrix A k = U k k V T k ,where k is a diagonal matrix with monotonically decreasing diagonal elements  X  i . The columns of the matrices U k and V k are the left and right singular vectors of the k largest singular values of A . The similarities between documents in the subspace are computed using a similarity measure, such as the cosine of the angle of the corresponding vectors.
 accurate computation of the largest few hundred singular triplets of the document-attribute matrix. This scalability issue is resolved for many applications by COV, our covariance matrix-based dimensional reduction method. In COV, the docu-ment vectors are projected onto the subspace spanned by the k eigenvectors cor-responding to the largest k eigenvalues of the covariance matrix of the document vectors C , for details, see Jolliffe [ 15 ]. Since the covariance matrix is symmetric and positive semidefinite, it can be decomposed into the product C = V V T , where V is an orthogonal matrix that diagonalizes C so that the diagonal en-tries of are in monotonic decreasing order going from top to bottom, that is, diag( )=(  X  1 , X  2 ,..., X  N ), where  X  i  X   X  i + 1 for i = 1 , 2 ,..., N .Tore-duce the dimension of the problem to k M , N , we project all document vectors into the subspace spanned by the k eigenvectors { v 1 ,v 2 ,...,v k } cor-responding to the largest k eigenvalues {  X  1 , X  2 ,..., X  k } of the covariance ma-trix. 4 The eigenvectors can be computed using the standard algorithms for very large symmetric, positive semidefinite matrices given in Golub and Van Loan [ 8 ]. Interdocument similarities are computed in a manner analogous to the LSI algorithm.
 small subspace to reduce computational requirements but high enough dimension-ality to retain different characteristics of the documents to enable accurate search 5 and clustering results. Both use the dimensional reduction process to address syn-onymy and polysemy problems associated with text databases. Synonyms tend to appear in similar contexts so they are mapped into the same region of the sub-space during dimensional reduction, as shown in Figs. 1 and 2 . For example, LSI correctly processes the synonyms car and automobile that appear in the Reuters news article set by exploiting the concept of co-occurrence of terms . Synonyms tend to appear in documents with many of the same terms (i.e., text document at-tributes), because the documents will tend to discuss similar or related concepts. During the dimensional reduction process, co-occurring terms are projected onto the same dimension. which it appears. The word will be mapped to a different region for each separate meaning and context because the existence of co-occurring terms is unlikely for documents which cover different meanings of a word, as shown in Fig. 3 .An example of polysemy is the term book , which may be synonymous with novel, biography or text. An alternate meaning is synonymous with making a reservation, as in  X  X o book a flight X . Further details are discussed in Deerwester et al. [ 5 ]inthe context of LSI. The COV algorithm is designed to handle synonymy and polysemy in an analogous manner.
 lem associated with LSI. The primary bottleneck of COV is the computation of the largest few hundred eigenvalues and corresponding eigenvectors of a dense square, symmetric, positive semidefinite matrix, with height and width less than or equal to the dimension of the attribute space. Since the dimension of a covariance matrix is independent of the number of documents, COV can be used for real-time search and text-mining applications X  such as clustering X  X or databases with hun-dreds of thousands of documents when the attribute space is relatively small. For example, computations for 20,000 attribute dimensions can be performed on a PC. 1.2 Clustering methods Cluster analysis can also be used to understand topics addressed by documents in massive databases [ 10 ]. It is important to note that unlike an arithmetic com-putation, there is no  X  correct solution  X  to the clustering problem for very large databases. Depending on the perspective, two documents may or may not be sim-ilar. Studies have shown that manual (i.e., human) clustering, as well as human evaluation of clustering results, are riddled with inconsistencies (see, for example, Macskassy et al. [ 20 ]). The work provides insight into the difficulty of automat-ing this task. Nevertheless, studies have also shown that despite imperfections associated with automated and human clustering methods, they may still provide valuable insights into the contents of databases.
 overlaps, that is, whether or not documents are allowed to belong to more than one cluster. From this perspective, there are at least four categories of methods: 1. Hard clustering methods that require each document to belong to exactly one 2. Hard clustering methods that permit each document to belong to at most one 3. Soft clustering methods that accomodate overlapping clusters, although they 4. Soft clustering methods that actively support mining of overlapping clusters clustering methods. Examples include the k -means, k -medoid, k -modes, and k -prototypes algorithms, agglomerative and divisive algorithms, and gravitational methods. Variations of these hard clustering methods have been developed in which appropriate documents can be classified as noise rather than be forced to be associated with a cluster.
 equally important. Some classical methods that are not scalable to larger databases are reviewed in Everitt et al. [ 7 ]. More recent works on soft clustering are designed for processing document collections that are many orders larger. A survey paper by Hearst [ 11 ] states the need for overlapping cluster analysis and good visualization interfaces to help users understand situations in which many categories are associ-ated with a set of documents:  X  X  one-document/one-category assumption can be insufficient for guiding a user through hundreds or thousands of articles. X  Work from a computer science perspective by Kumar and Ghosh [ 18 ], from a machine-learning perspective by Bezdek [ 4 ], Pelleg and Moore [ 22 ] and from a Web page mining perspective by Zamir and Etzioni [ 27 ] and Lin and Kondadadi [ 19 ]also advocate soft clustering that allows membership in multiple clusters with differ-ent degrees of association. Methods that use fractional assignment of documents to more than one cluster tend to emphasize the computation of the assignments rather than understanding the contents of a database through understanding relationships between overlapping clusters. Houle [ 12 ] proposed a soft clustering method that recognizes the importance of mining clusters overlaps, and actively looks for and accommodates for their presence, however, his method requires massive compu-tational and hardware resources.
 aware, it is the only soft clustering algorithm for massive (non-Web) document databases that handles the synonymy and polysemy problems to some extent and reduces the dimensionality of the problem through projections, random sampling, and a user-specified trade-off between speed and the accuracy of the results. 1.3 Minor cluster mining X  X elated work Implementation studies [ 16 ] show that LSI and COV can successfully find major document clusters. However, they are not as successful at finding smaller, minor clusters, because the major clusters dominate the process. During the dimensional reduction step in LSI and COV, documents in minor clusters are often mistaken for noise or placed arbitrarily into any cluster.
 lections of documents, such as e-mail. Implementation studies were conducted on a small set of documents (683 articles from TREC). 6 The main idea in the algo-rithm is to prevent major themes from dominating the process of selecting the basis vectors for the reduced dimensional subspace. This is carried out during the basis vector selection process by introducing a fixed, unfavorable bias, i.e., a weight to documents that belong to clusters that are well-represented by basis vectors that have already been selected. The weight is imparted by computing the magnitude (i.e., the length in the Euclidean norm) of the residual of each document vector (i.e., the proportion of the document vector that cannot be represented by the basis vectors that have been selected thus far), then re-scaling the magnitude of each document vector by a power q of the magnitude of its residual.
 lowing problems can occur when the number of documents is large (i.e., greater than a few thousand): fixed weighting can obscure the associations of documents that belong to more than one cluster after one of the clusters to which it belongs is identified; all minor clusters may not be identified; the procedure for finding eigen-vectors may become unstable when the scaling factor q is large; the basis vectors b are not always orthogonal; and if the number of documents in the database is very large, the eigenvector cannot be computed on an ordinary PC because the residual matrix becomes dense after only a few iterations, leading to a memory overflow.
 bly overlapping) multiple major and minor document clusters that overcome some of the difficulties associated with the Iterative Re-Scaling algorithm. The primary new contribution is the introduction of dynamic control of the weighting to reduce loss of information about minor clusters. As a second modification, we replace the computation of eigenvectors in the Iterative Re-scaling algorithm with the com-putation of the SVD for robustness. Our third modification is the introduction of modified Gram X  X chmidt orthogonalization of the basis vectors (see, e.g., Golub and Van Loan [ 8 ]).
 RS , is a modification of COV, analogous to LSI-RS and LSI. In COV-RS, we compute the residual of the covariance matrix. Our implementation studies indi-cate that COV-RS is better than LSI, COV, and LSI-RS at identifying large and multiple minor clusters.
 quire re-scaling of all document vectors before computation of each additional basis vector for subspace projection. For even moderately large databases, after a few iterations the document attribute matrix becomes dense so that main mem-ory constraints become a bottleneck. Details on a comparison study of LSI-RS, COV-RS, and Iterative Re-Scaling algorithms are given in Kobayashi et al. [ 17 ]. In the next section we propose techniques for enhancing the LSI-RS algorithm to overcome the scalability issue for very large databases. 2 Selective scaling, sampling, and preservation of sparsity We recommend the basic COV algorithm to mine major clusters. In this section, we introduce COV with selective scaling (COV-SS) , an efficient variation for minor cluster mining algorithm that improves on our earlier algorithm, COV-RS. Like COV-RS, COV-SS allows users to skip over iterations that find major clusters and jump to iterations to find minor clusters. COV-SS reduces the computational costs by quickly testing whether re-scaling is necessary. Although the savings per iter-ation is modest, over many iterations, the total savings is large because sparsity is preserved for more iterations. To increase minor cluster mining capabilities to databases that are several orders of magnitude higher, we can integrate random sampling into the COV-SS algorithm. To even further increase the speed and scal-ability of our algorithm, we introduce a method for preserving, as much as pos-sible, the sparsity of the document vectors, at the expense of a slight degradation in the mining results. From empirical observations we conclude that there may be extremely rare cases in which introduction of the perturbations may lead to very poor results. However, we encountered no such cases and since the computational reduction is so significant, the sparsity preserving versions of our algorithm seem preferable (i.e., advantageous) for most commercial applications.
 as above,  X  is a threshold parameter, and  X  is the scaling off-set. Initially, the residual matrix R is set to be A . Matrix R does not need to be kept in the main memory; it suffices to keep just the N -dimensional residual vector r i during each of the M loops. The output is the set of basis vectors { b i : i = 1 , 2 ,..., k } for the k -dimensional subspace. M is either the total number of documents in the database or the number of randomly sampled documents from a very large database. Here P and Q are M -dimensional vectors; R is the residual matrix (which exists in theory, but is not allocated in practice); r i is the i th document vector of R (an N -dimensional vector);  X  r is the component-wise average of the set of all residual and t are double-precision floating point numbers; and first is a Boolean expres-sion initially set equivalent to true .
 measure P [i] of the dot product of the most recently computed basis vector and the document vector. The user-specified threshold  X  and offset parameter  X  control the number of minor clusters that will be associated with each basis vector. A small threshold and large offset value tend to lead to basis vectors associated with a large number of minor clusters. Conversely, a large threshold and a small offset value tend to lead to basis vectors with few minor clusters.
 (COV-RS and COV-SS) is significantly greater than the basic COV algorithm. Re-scaling is computationally expensive for large databases. COV has no re-scaling costs, but it uses a moderately expensive eigenvalue X  X igenvector solver for large, symmetric positive semidefinite matrices. COV-RS and COV-SS use the accurate and inexpensive power method (see, e.g., Golub and Van Loan [ 8 ]) to find the largest eigenvalue and its corresponding eigenvector after each round of (possible) re-scaling of residual vectors. A selective scaling algorithm for LSI, which we denote by LSI-SS, can be constructed in an analogous manner. It suffers from the same difficulties associated with sparsity preservation as COV-SS. prohibitively expensive if the database under consideration is large. One way to overcome the problem is to introduce random sampling of documents. We apply COV-SS to the covariance matrix constructed from a set of randomly selected document vectors in the database. Since different results can be expected for dif-ferent samples, this process of sampling followed by selective scaling should be repeated as many times as the stopping criterion permits. The stopping criterion may determined by a number of factors, such as computational, memory and stor-age resources or the number of new clusters found during each sampling. Note that in setting the sample sizes, we need to recognize the computational trade-off between sample sizes and the number of times sampling must be performed to find most minor clusters. Larger samples increase the cost of selective scaling. However, smaller samples are likely to lead to identification of fewer clusters. into dense vectors. However, we observe that many of the non-zero coordinates are very small relative to the original nonzero coordinates. To further increase the speed and scalability of our algorithm, we consider some methods for preserving, as much as possible, the sparsity of the document vectors. One idea is to select a threshold such that all non-zeros in the residual matrix smaller than will be reset to zero after rescaling. A larger threshold will lead to greater preservation of sparsity, hence more reduction in computation and data access times, but it will also introduce larger numerical error during computation. Another method is to consider the very small non-zero coordinates of a newly computed basis vector. Before rescaling the residual matrix, set the small values in the basis vector to zero. This will reduce the computational cost of rescaling and introduce fewer errors. We tested the first method in implementation studies that are described in the next section. 3 Implementation studies This section describes implementation studies of the algorithms we proposed in the previous section. We describe the data sets used; cluster identification and labeling algorithms developed; and some prototype GUIs for visualizing results. 3.1 Studies with TREC benchmark data sets We conducted numerical experiments with LSI, COV, COV-RS, and COV-SS with and without sampling using the Reuters and LA Times news databases from TREC (with 21,578 and 127,742 articles, respectively) and an artificially generated data set (with 10,000 documents consisting of three major clusters with 1,000 docu-ments each, five clusters with 500 documents each, 20 minor clusters with 50 doc-uments each, and 3,500 documents that do not belong to any cluster). Results from both the LSI-and COV-based search engines were good. However, the relevancy rankings and computed relevancies showed a slighter variation than in earlier ex-periments with a Reuters news database described in Kobayashi and Aono [ 16 ]. In both sets of our experiments (with the Reuters and LA Times articles) the LSI and COV algorithms could be used for finding major clusters, but they usually fail to find all minor clusters. The algorithms lost information about some minor clusters, because major clusters and their large subclusters dominate the subjects that will be preserved during dimensional reduction.
 tion algorithms based on theoretical considerations and our implementation stud-ies. For medium size databases, major clusters should be mined using basic COV and minor clusters using COV with selective scaling. Major cluster identification results from LSI and COV are usually identical. However, COV usually requires 20 X 30% fewer iterations to find major clusters because it can detect clusters in both the negative and positive directions along each basis vector, since the origin is shifted during the dimensional reduction phase in pre-computations. LSI can only detect clusters either in the positive direction or in the negative direction of each basis vector, but not both. For massive databases, major clusters should be mined using basic COV (since LSI is not scalable to massive databases) and COV with selective scaling and sampling should be used to find minor clusters. Selec-tive scaling is preferable to complete re-scaling since the results from both should be similar, but selective scaling is more computationally efficient.
 experiments with basic COV and its variations for the LA Times data set. We ex-amined the number of major and minor clusters retrieved using COV, COV with re-scaling and sampling, and COV with selective scaling and sampling with and without replacement of small non-zero entries by zero. We note several important conclusions:  X  COV with selective scaling and sampling finds the largest number of clusters.  X  Straightforward COV finds the largest number of major clusters, and its ratio  X  COV with re-scaling and sampling and COV with selective scaling and sam- X  Overlaps between document clusters appear to be more common than isolated 3.2 Cluster identification and labeling Our system uses information from basis vectors computations to find and label clusters using an embodiment of the Label Algorithm given below. The input con-sists of basis vectors b i output by COV or COV-SS; the number of cluster la-bels for extrinsic and intrinsic keywords (defined later); a threshold for separating clusters; and keyword data extracted from the original document data. We imple-mented  X  X abel X  as two separate subprograms: (1) cluster keyword generation and (2) cluster merging and labeling .
 tween each basis vector and all the document vectors in the original document X  keyword matrix. It produces extrinsic and intrinsic keywords if the similarity mea-sure between a basis vector and document vector is greater than a given threshold  X  . Extrinsic keywords are the top p ( e ) keywords that contribute towards making the similarity measure between a basis vector and the document vector greater than the threshold  X  . Intrinsic keywords are the top p ( i ) keywords that correspond to the largest p ( i ) tf-idf weights of the original document vector. Note that (unlike k -means and k -menoid clustering methods) our clustering method does not par-tition the data. In other words, we allow a document vector to be classified into more than one cluster as long as the extrinsic keywords of the document vector for two basis vectors have no keywords in common. Since database contents may be mined from different user perspectives allowing clusters to overlap is essential. And information on overlaps between clusters can yield valuable information on relationships between clusters and topics in the documents.
 3.3 Cluster visualization We propose two new cluster visualization interfaces: the first for a global, panoramic view of all mined clusters, and second targeted to a user X  X  specific interest based on keyword input.
 of all identified clusters in a massive database can be difficult since the number of clusters is large, and screen space is limited. For example, classical bar-like histogram displays with clusters in decreasing or increasing order cannot display all clusters at once since computer screens are not sufficiently wide. We propose a snail shell-like display that shows the clusters in order of largest to smallest as we move from the outside towards the inside of the spiral. Each colored area that represents a cluster is proportional to the size of the cluster, where cluster size is defined as the number of documents in a cluster. The keyword label of the cluster can be called up by a mouse click. An example of our display from a session with TREC data is shown in Fig. 7 .
 ters and documents about a specific topic, rather than the entire set of clusters. Users can input keywords to describe their interest(s), and a list of relevant docu-ments from the most relevant, down to the least relevant will appear. To view in-terrelations among documents, the user can select a particular document for view in a three-dimensional slice of attribute space. Our system recommends three axes that are likely to show an information rich view based on the input query. This re-commendation system is needed since there are many possible sets of basis vectors can serve as coordinate axes for a three-dimensional slice of a 200-dimensional space. For example, if the reduced dimensional space has 200 dimensions (hence 200 coordinate axes), there are (200!/197!3!) possible choices for selecting three axes. Our system determines potentially good axes for view by computing dis-tances between the query vector and the basis vectors for the 200-dimensional subspace. Closer basis vectors are given a higher ranking. The default setting dis-plays the highest ranked coordinate set. Users can ask our system for a list of recommended basis vectors from highest to lowest ranking before selecting them for display.
 200 basis vectors were computed using the straightforward COV algorithm for the LA Times data from TREC. Figure 8 shows a screeen image of the major clusters in the dataset. The three coordinate axes used for visualization are the first three basis vectors output by the COV algorithm. The three major clusters X  marked A, B, and C X  X re comprised of articles about { Bush, US, Soviet, Iraq } , { team, coach, league, inning } ,and { police, digest, county, officer } , respectively. A trace of a minor cluster X  X arked D X  X n { Zurich, Swiss, London, Switzerland } can be seen in the background. This minor cluster can be seen much more clearly when other coordinate axes are used for visualization and display. This example shows that careful analysis is needed before deciding whether or not a faint cloud of points is noise or part of a cluster.
 more iterations. Examples of 6 minor clusters appear in Fig. 9 which uses basis vectors 58, 63, and 104 for visualization. Note that two clusters may lie along a coordinate axis X  X ne each in the positive and negative directions. The clusters are comprised of articles on { abortion, anti-abortion, clinic, Roe } , { lottery, jackpot, California, ticket } , { AIDS, disease, virus, patient } , { gang, school, youth, murder } , {
Cypress, Santiago, team, tournament } ,and { jazz, pianist, festival, saxophonist } , respectively. A plus or minus sign is used to mark the direction in which a cluster lies. 4 Conclusions and future work We conclude by mentioning a number of interesting questions and open problems that have arisen in the course of developing our system. They include the following  X  How can users determine the optimal dimension k to which the attribute space  X  How can determine whether a massive database is suitable for random sam- X  How can one devise a reliable means for estimating the optimal sampling size  X  What is a good stopping criterion for sampling? That is, when is it appropriate  X  How can the GUI effectively map identified clusters and their inter-Although many open issues and possibilities for improvements and refinements remain, our prototype system demonstrates that the COV algorithms with selective scaling and random sampling can be effective for exploring the contents of very large databases.
 algorithm or cluster validation is becoming an important area of research. Some techniques and validity indices are surveyed and/or proposed in Halkidi et al. [ 9 ], Niu et al. [ 21 ], and Zaine et al. [ 26 ].
 References
