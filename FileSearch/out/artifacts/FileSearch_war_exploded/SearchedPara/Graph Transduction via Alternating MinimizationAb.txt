 Jun Wang jwang@ee.columbia.edu Department of Electrical Engineering, Columbia University Tony Jebara jebara@cs.columbia.edu Department of Computer Science, Columbia University Shih-Fu Chang sfchang@ee.columbia.edu Department of Electrical Engineering, Columbia University Graph transduction refers to a family of algorithms that achieve state of the art performance in semi-supervised learning and classification. These meth-ods incur a tradeoff between a classification func-tion X  X  accuracy on labeled examples and a regularizer term that encourages the function to remain smooth over a weighted graph connecting the data samples. The weighted graph and the minimized function ulti-mately propagate label information from labeled data to unlabeled data to provide the desired transductive predictions. Popular algorithms for graph transduc-tion include the Gaussian fields and harmonic func-tions based method ( GFHF ) (Zhu et al., 2003) as well as the local and global consistency method ( LGC ) (Zhou et al., 2004). Other closely related methods include the manifold regularization framework pro-posed in (Sindhwani et al., 2005; Belkin et al., 2006) where graph Laplacian regularization terms are com-bined with regularized least squares ( RLS ) or sup-port vector machine ( SVM ) function estimation cri-teria. These methods lead to graph-regularized vari-ants denoted as Laplacian RLS ( LapRLS ) and Lapla-cian SVM ( LapSVM ) respectively. For certain syn-thetic and real data problems, graph transduction ap-proaches do achieve promising performance. However, this article identifies several realistic settings and la-beling situations where this performance can be com-promised. An alternative algorithm which generalizes the previous techniques is proposed by defining a joint iterative optimization over the classification function and a balanced label matrix.
 Even if one assumes the graph structures used in the above methods faithfully describe the data manifold, graph transduction algorithms may still be misled by problems in the label information. Figure 1 depicts several cases where the label information leads to in-valid graph transduction solutions for all the aforemen-tioned algorithms. The top row of Figure 1 shows a separable pair of manifolds where unbalanced label in-formation affects the propagation results. Although a clear separation region is visible between the two man-ifolds, the imbalance in the labels misleads the previ-ous algorithms which prefer assigning points to the class with the majority of labels. In the bottom row of Figure 1, a non-separable problem is shown where two otherwise separable manifolds are peppered with noisy outlier samples. Here, the outliers do not be-long to either class but once again interfere with the propagation of label information. In both situations, conventional transductive learning approaches such as GFHF , LGC , LapRLS , and LapSVM fail to give ac-ceptable labeling results.
 In order to handle such situations, we extend the graph transduction optimization problem by casting it as a joint optimization over the classification function and the labels. The optimization is solved iteratively and remedies the instability previous methods seem to have vis-a-vis the initial labeling. In our novel framework, initial labels simply act as the starting value of the label matrix variable which is incrementally refined until convergence. The overall minimization over the continuous classification function and the binary label matrix proceeds by an alternating minimization over each term separately and converges to a local mini-mum. Moreover, to handle the imbalanced labels is-sue, a node regularizer term is introduced to balance the label matrix among different classes. These two fundamental changes to the graph transduction prob-lem produce significantly better performance on both artificial and real datasets.
 The remainder of this paper is organized as the follows. In Section 2, we revisit the graph regularization frame-work of (Zhou et al., 2004) and extend it into a bi-variate graph optimization problem. A corresponding algorithm is provided that solves the new optimization problem by iterative alternating minimization. Section 3 provides experimental validation for the algorithm on both toy and real classification datasets, including text classification and digital recognition. Compar-isons with leading semi-supervised methods are made. Concluding remarks and a discussion are then pro-vided in Section 4. Consider the dataset X = ( X l , X u ) of labeled in-puts X l = { x 1 , , x l } and unlabeled inputs X u = { x l +1 , , x n } along with a small portion of cor-responding labels { y 1 , , y l } , where y i  X  L = { 1 , , c } . For transductive learning, the objective is to infer the labels { y l +1 , , y n } of the unlabeled data { x l +1 , , x n } , where typically l &lt;&lt; n . The graph transduction methods define an undirected graph rep-resented by G = {X , E} , where the set of node or ver-tices is X = { x i } and the set of edges is E = { e ij } . Each sample x i is treated as the node on the graph and the weight of edge e ij is w ij . Typically, one uses a kernel function k ( ) over pairs of points to recover weights, in other words w ij = k ( x i , x j ) with the RBF kernel being a popular choice. The weights for edges are used to build a weight matrix which is denoted by W = { w ij } . Similarly, the node degree matrix D = diag ([ d 1 , , d n ]) is defined as d i = binary label matrix Y is described as Y  X  B n  X  c with Y ij = 1 if x i has label y i = j and Y ij = 0 oth-erwise. This article will often refer to row and col-umn vectors of such matrices, for instance, the i th row and j th column vectors of Y are denoted as Y i and Y j , respectively. The graph Laplacian is defined as  X  = D  X  W and the normalized graph Laplacian is 2.1. Consistent Label Propagation Graph based semi-supervised learning methods propa-gate label information from labeled nodes to unlabeled nodes by treating all samples as nodes in a graph and using edge-based affinity functions between all pairs of nodes to estimate the weight of each edge. Most methods then define a continuous classification func-tion F  X  X  n  X  c that is estimated on the graph to min-imize a cost function. The cost function typically en-forces a tradeoff between the smoothness of the func-tion on the graph of both labeled and unlabeled data and the accuracy of the function at fitting the label information for the labeled nodes. Such is the case for a large variety of graph based semi-supervised learn-ing techniques ranging from the the mincuts method (Blum &amp; Chawla, 2001), the Gaussian fields and har-monic functions ( GFHF ) method, and the local and global consistency ( LGC ) method. A detailed survey of these methods is available in (Zhu, 2005). In trading off smoothness for accuracy, both GFHF and LGC approaches attempt to preserve consistency on the data manifold during the optimization of the classification function. The loss function for both methods involves the additive contribution of two penalty terms the global smoothness Q smooth and local fitness Q fit as shown below: F  X  = arg min In particular, recall that LGC uses an elastic regular-izer framework with the following cost function (Zhou et al., 2004).
 where the coefficient balances global smoothness and local fitting penalty terms. If we set =  X  and use a standard graph Laplacian for the smoothness term, the above framework reduces to the harmonic function formulation as shown in (Zhu et al., 2003).
 While LGC and GFHF formulations remain popular and have been empirically validated in the past, it is possible to discern some key limitations. First, the optimization can be broken up into a separate paral-lel problems since the cost function decomposes into terms that only depend on individual columns of the matrix F . Because each column of F indexes the la-beling of a single class, such a decomposition reveals that biases may arise if the input labels are dispropor-tionately imbalanced. In practice, both propagation algorithms tend to prefer predicting the class with the majority of labels. Second, both learning algorithms are extremely dependent on the initial labels provided in Y . This is seen in practice but can also be explained mathematically by fact that Y is starts off extremely sparse and has many unknown terms. Third, when the graph contains background noise and makes class manifolds nonseparable, these graph transduction ap-proaches fail to output reasonable classification results. These difficulties were illustrated in Figure 1 and seem to plague many graph transduction approaches. How-ever, the proposed method, graph transduction via al-ternating minimization ( GTAM ) appears resilient. To address these problems, we will make modifications to the cost function in Eq. 1. The first one is to explic-itly show the optimization over both the classification function F and the binary label matrix Y : Where B n  X  c is the set of all binary matrices Y of size n  X  c that satisfy P j Y ij = 1 and, for the labeled data x i  X  X l , Y ij = 1 if y i = j . More specifically, our loss function is:
Q ( F , Y ) = where we have introduced the matrix V which is a node regularizer to balance the influence of labels from different classes. The matrix V = diag ( v ) is a function of the current label matrix Y : where the symbol  X  denotes the Hadamard product and column vector ~ 1 represents ~ 1 = [1 1] T . This node regularizer permits us to work with a normalized version of the label matrix Z defined as: Z = VY . By definition, we see that the normalized label matrix satisfies P i Z ij = 1. Using the normalized label ma-trix Z in a graph regularization allows labeled nodes with high degree to contribute more during the graph diffusion and label propagation process. However, the total diffusion of each class is kept equal and normal-ized to be one. Therefore, the influence of different classes is balanced even if the given class labels are imbalanced. If class proportion information is known a priori, it can be integrated by scaling the diffusion with the prior class proportion. However, because of the nature of graph transduction and unknown class prior knowledge, equal class balancing leads to gen-erally more reliable solutions than label proportional weighting. This intuition is in line with prior work that uses class proportion information in transductive inference such as (Chapelle et al., 2007) where class proportion is enforced as a hard constraint on the la-bels or in (Mann &amp; McCallum, 2007) where such infor-mation is used as a regularizer. We next discuss the alternating minimization procedure which is the key modification to the overall framework. 2.2. Alternating Minimization Procedure In our proposed graph regularization framework, the cost function involves two variables to be optimized. While simultaneously recovering both solutions is in-tractable due to the mixed integer programming prob-lem over binary Y and continuous F , we will pro-pose a greedy alternating minimization approach. The first update of the continuous classification function F is straightforward since the resulting cost function is convex and unconstrained allowing us to recover the optimal F by setting the partial derivative  X  Q  X  F to be zero. However, since Y  X  B is a binary matrix and subject to linear constraints of the form P j Y ij = 1, the other step in our alternating minimization requires solving a linearly constrained max cut problem which is NP (Karp, 1972). Due to the alternating minimiza-tion outer loop, investigating guaranteed approxima-tion schemes (Goemans &amp; Williamson, 1995) to solve a constrained max cut problem for Y is unjustified due to the solution X  X  dependence on the dynamically varying classification function F during the alternat-ing minimization procedure. Instead, we use a greedy gradient based approach to incrementally update Y , while keeping the classification function F at the corre-sponding optimal setting. Moreover, because the node regularizer term V normalizes the labeled data, we also interleave updates of V based on the revised Y . Minimization for F : The classification function F  X  X  n  X  c is continuous and its loss terms are convex allowing the minimum to be recovered by zeroing the partial derivative: where we denote P = ( L / + I )  X  1 as the propagation matrix and assume the graph is symmetrically built (i.e. L = L T ).
 Greedy minimization of Y: To update Y , first replace F in Eq. 4 by its optimal vlue F  X  from the solution of Eq. 6.
 Q ( Y )= The optimization still involves the node regularizer V in Eq. 5, which depends on Y and normalizes the la-bel matrix over columns. Due to the dependence on the current estimate of F and V , only an incremental step will be taken greedily to reduce Q ( Y ). In each iteration, we find position ( i  X  , j  X  ) in the matrix Y and change the binary value of Y i  X  j  X  from 0 to 1. The di-rection with largest negative gradient guides our choice of binary step on Y . Therefore, we need to evaluate k X  X  Y k and find the largest negative value to deter-mine ( i  X  , j  X  ).
 Note that setting Y i  X  ,j  X  = 1 is equivalent to a similar operation on the normalized label matrix Z by setting Z i  X  ,j  X  =  X , 0 &lt;  X  &lt; 1, and Y , Z have one to one corre-spondence. Thus, the greedy optimization of Q with respect to Y is equivalent to greedy minimization of Q with respect to Z . More formally:  X  Q  X  Y =  X  Q  X  Z  X  Z with straightforward algebra we see that: Then we can rewrite the loss function using the vari-able Z as:
Q ( Z ) = where A represents A = P T LP + ( P T  X  I )( P  X  I ). Notice that A is symmetric if the graph is symmetri-cally built. We derive the gradient of the above loss function and recover it with respect to Y as: As described earlier, we search the gradient matrix  X 
Z Q to find the minimal element for updating Then update the label matrix by setting Y i  X  j  X  = 1. Because of the binary nature of Y , we simply set Y i  X  j  X  = 1 instead of using a continuous gradient ap-proach. In the t + 1th iteration, the node regularizer v t +1 can be recalculated with the updated Y t +1 . The update Y is indeed greedy. Therefore, it could os-cillate and backtrack from predicted labelings in pre-vious iterations without convergence guarantees. We propose a straightforward way to guarantee conver-gence and avoid backtracking, inconsistency or un-stable oscillation in the greedy propagation of labels. Once an unlabeled point has been labeled, its labeling can no longer be changed. Thus, we remove the most recently labeled point ( i  X  , j  X  ) from future considera-tion and only permit the algorithm to search for the minimal gradient entries corresponding to the remain-ing unlabeled examples. Thus, to avoid changing the labeling of previous predictions, the new labeled node x i  X  will be removed from X u and added to X l . In the following, we summarize the updating rules from step t to t + 1 in the alternative minimization scheme. Although the optimal F  X  is being computed in each iteration as shown in Eq. 6, we do not explicitly need to update it. Instead, it is implicitly used in Eq. 8 to directly update Y . The procedure above repeats until all points have been labeled. 2.3. Algorithm Summary and Convergence From the above discussion, our method is unique in that it optimizes the loss function over both continuous-valued F space and binary-valued Y space. Starting from a few given labels, the method itera-tively and greedily updates the label matrix Y , node regularizer v , and gradient matrix  X  Z Q . In each indi-vidual iteration, new labeled samples are obtained to drive a better graph propagation in the next iteration. In our approach, we directly acquire new labels instead of calculating F  X  and then conducting a mapping to Y , which is the regular procedure in other graph transduc-tion methods like LGC and GFHF . This unique feature makes the proposed algorithm very efficient since we only update the gradient matrix  X  Z Q in each itera-tion. Furthermore, similar to the graph superposition approach introduced in (Wang et al., 2008), the cal-culation of the node regularizer v and gradient matrix  X 
Z Q can be more efficient by incremental updating as a result of the newly gained labels.
 Due to greedy assignment, the algorithm can only loop the alternative minimization (or the gradient compu-tation equivalently) at most n  X  l times. The update of the graph gradient, finding the largest element in the gradient and the matrix algebra involved can be done efficiently by modifying only a single entry in Y per loop. Each minimization step over F and Y thus re-quires O ( n 2 ) time and the total runtime of the greedy GTAM algorithm is O ( n 3 ). Empirically, the value of the loss function Q decreases rapidly in the the first dozen iterations and achieves steady convergence af-terward. This phenomenon indicates that the label propagation loop could be early stopped by solving for the labels from the optimized F  X  (Eq. 6) after only a few iterations. The above algorithm chart summarizes the proposed GTAM method. In this section, we demonstrate the superiority of the proposed GTAM method in comparison to state of the art semi-supervised learning methods over both syn-thetic and real data. For instance, on the WebKB data, previous work shows that LapSVM and LapRLS are better than other semi-supervised approaches, such as Transductive SVMs TSVM (Joachims, 1999) and  X  TSVM . Therefore, we only compare our method with LapRLS , LapSVM and two related methods, LapRLS joint and LapSVM joint (Sindhwani et al., 2005). In all experiments, we used the same param-eter settings reported in the literature. The GTAM approach only requires a single parameter which con-trols the tradeoff between the global smoothness and local fitting terms in the cost function. Although our experiments show that GTAM is fairly robust to the setting of , we set = 99 throughout all experiments. For all real implementations of graph-based methods, one needs a construction method that builds a graph from the training data X , which involves a proce-dure for computing the weight of links via a kernel or similarity function. Typically, practitioners use RBF kernels for image recognition and cosine distances for text classification (Zhou et al., 2004; Ng et al., 2001; Chapelle et al., 2003; Hein &amp; Maier, 2006). However, finding adequate parameters for the kernel or similar-ity function, such as the RBF kernel size  X  , is not always straightforward particularly if labeled data is scarce. Empirical evidence has shown that the prop-Algorithm 1 Graph Transduction via Alternating Minimization ( GTAM )
Input: data set X = { x 1 , , x l , x l +1 , , x n } , labeled subset X l = { x 1 , , x l } , unlabeled sub-set X u = { x l +1 , , x n } , labels { y 1 , , y j , , y where y j  X  L = { 1 , , l } . Affinity matrix W = { w ij } , node degree matrix D , initial label matrix
Initialization: iteration counter t = 0; normalized graph Laplacian L = D  X  1 / 2  X  X   X  1 / 2 ; propagation matrix P = ( L / + I )  X  1 ; matrix A = P T LP + ( P T  X  I )( P  X  I ); repeat until X t u =  X  Output:
The labels of unlabeled samples { y l +1 , , y n } . agation results highly depend on the kernel param-eter selection. Motivated by the approach reported in (Hein &amp; Maier, 2006), we use an adaptive kernel size based on the mean distance of k -nearest neigh-borhoods ( k = 6) for the experiments on real USPS handwritten digit data. On the WebKB data, we use the same graph construction suggested by (Sindhwani et al., 2005). For each dataset, the same graph is used for all the compared transductive learning approaches. 3.1. Two Moon Synthetic Data Figure 1 illustrated synthetic experiments on 2 D and 3D two-moon data. Despite the near-perfect classifica-tion results reported on such datasets in the literature (Zhou et al., 2004), we showed how small perturba-tions to the problem can have adverse effects on prior algorithms. The prior methods are overly sensitive to locations of the initial labels, ratios of the two-class labels, and the level of ambient noise or outliers. A more thorough experimental study is also possible for the two-moon data by exploring the effect of class imbalance. We start by fixing one class to have one observed label and select r labels from the other class. Here, r is also the imbalance ratio and the range we explore is 1  X  r  X  20. These experiments use the 3D noisy two-moon data which contain 300 positive and 300 negative sample points as well as 200 additional background noise samples. Multiple round tests (100 trails) are evaluated for each imbalance condition by calculating the average prediction accuracy on the rel-evant 600 samples. For a fair comparison, we use the same graph Laplacian, which is constructed using k -NN ( k = 6) neighbors with RBF weights. Moreover, the parameter for LGC is set as  X  = 0 . 99. The param-eters for LapRLS and LapSVM are  X  A = 1 ,  X  I = 1. Figure 2 demonstrates the performance advantage of the proposed GTAM approach versus the LGC , GFHF , LapRLS , and LapSVM methods. From the figure, we can conclude that all the four strawman approaches are extremely sensitive to the initial labels and label class imbalance since none of them can produce per-fect accuracy and the error rates of LGC and GFHF are dramatically increased when the label class be-comes more imbalanced even though more information is being provided to the algorithm . However, GTAM is clearly superior, achieving the best accuracy regardless of the imbalance ratio and despite contamination with noisy samples. In fact only 1 or 2 of the 100 trails for each individual setting of r were imperfect using the GTAM method. 3.2. WebKB Dataset For validation on real data, we first evaluated our method using the WebKB dataset, which has been widely used in semi-supervised learning experiments (Joachims, 2003; Sindhwani et al., 2005). The WebKB dataset contains two document categories, course and non-course . Each document has two types of informa-tion, the webpage text content called page representa-tion and link or pointer representation. For fair com-parison, we applied the same feature extraction proce-dure as discussed in (Sindhwani et al., 2005), obtained 1051 samples with 1840-dimensional page attributes and 3000 link attributes. The graph was built based on cosine-distance neighbors with Gaussian weights (number of nearest neighbors is 200 as in (Sindhwani et al., 2005)). We compared our method with four of the best known approaches, LapRLS , LapSVM , and the two problem specific methods, LapRLS joint , LapSVM joint reported in (Sindhwani et al., 2005). All the compared approaches used the same graph con-struction procedure and all parameter settings were set according to (Sindhwani et al., 2005), in other words  X 
A = 10  X  6 ,  X  I = 0 . 01. We varied the number of la-beled data to measure the performance gain with in-creasing supervision. For each fixed number of labeled samples, 100 random trails were tested. The means of the test errors are shown in Figure 3.
 As the Figure reveals, the proposed GTAM method achieved significantly better accuracy than all the other methods, except for the extreme case when only four labeled samples were available. The performance gain grows rapidly when the number of labeled sam-ples increases, although in some cases the error rate does not drop monotonically. 3.3. USPS digit data We also evaluated the proposed method in an im-age recognition task. Specifically, we used the data in (Zhou et al., 2004) for handwritten digit classifi-cation experiments. To evaluate the algorithms, we reveal a subset of the labels (randomly chosen and guaranteeing at least one labeled example is available for each digit). We compared our method with LGC and GFHF , LapRLS , and LapSVM . The error rates are calculated based on the average over 20 trials. From Figure 4, we can conclude that GTAM signifi-cantly improved the classification accuracy, compared to the other approaches, especially when very few la-beled samples are available. The mean accuracies of GTAM are consistently low for different numbers of labels and the standard deviation values are also very small (10  X  4 level). This demonstrates that the GTAM method is insensitive to the numbers and specified lo-cations of the initially given labels. Only 1% of the test digit images were mislabeled. These failure cases are presented in Figure 5 and are often ambiguous or extremely poorly drawn digits. Compared to the per-formance on WebKB dataset shown in Figure 3, the USPS digit database experiments exhibit even more promising results. One possible reason is that the USPS digit dataset has relatively more samples (3874) and a lower feature dimensionality (256), compared to the WebKB dataset (which has 1840 samples in 4800 dimensions). Therefore the graph construction pro-cedure is more reliable and the estimation of graph gradients in our algorithm is more robust.
 Existing graph-based transductive learning methods hinge on good labeling information and can easily be misled if the labels are not distributed evenly across classes, if the choice of initial label locations is varied or if excessive noise or outliers corrupt the underlying manifold structure. These degenerate settings seem to plague real world problems as well, compromising the performance of state-of-the-art graph transduction methods. Our experiments over synthetic data sets (two moon data sets) and real data sets (USPS dig-its and WebKB) confirm the shortcomings of existing tools.
 This article addresses these shortcomings and pro-poses a novel graph based semi-supervised learning method, graph transduction via alternating minimiza-tion ( GTAM ). Therein, both the classification function and the label matrix are treated as variables in a cost function that is iteratively minimized. While the op-timal classification function can be estimated exactly, greedy optimization is applied to update the label ma-trix. The algorithm iterates an alternating minimiza-tion between both variables and is guaranteed to con-verge via a greedy scheme. In each individual iteration, through the graph gradient, the unlabeled node with the largest cost reduction is labeled. We gradually up-date the label matrix by adding more labeled samples while keeping the classification function at its optimal setting. Furthermore, we enforce normalization of the label matrix to avoid degeneracies. This results in an algorithm that can cope with all the aforementioned degeneracies and in practice achieves significant gains in accuracy while remaining efficient and cubic in the number of samples. Future work will include out of sample extensions of this method such that new data points can be added to the training and test set with-out requiring a full retraining procedure. The authors would like to thank Mr. Yongtao Su and Shouzhen Liu for their valuable comments. We also thank Mr. We Liu and Deli Zhao for the collec-tion of the artificial data. TJ was supported by ONR Award N000140710507 (ModNo: 07PR04918-00) and NSF Award IIS-0347499.
 Belkin, M., Niyogi, P., &amp; Sindhwani, V. (2006). Man-ifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples. JMLR , 7 , 2399 X 2434.
 Blum, A., &amp; Chawla, S. (2001). Learning from labeled and unlabeled data using graph mincuts. Proc. 18th ICML (pp. 19 X 26).
 Chapelle, O., Sindhwani, V., &amp; Keerthi, S. (2007). Branch and Bound for Semi-Supervised Support Vector Machines. Proc. of NIPS .
 Chapelle, O., Weston, J., &amp; Scholkopf, B. (2003). Clus-ter kernels for semi-supervised learning. Proc. NIPS , 15 , 1.
 Goemans, M., &amp; Williamson, D. (1995). Improved ap-proximation algorithms for maximum cut and satis-fiability problems using semidefinite programming. Journal of the ACM (JACM) , 42 , 1115 X 1145.
 Hein, M., &amp; Maier, M. (2006). Manifold denoising. Proc. NIPS , 19 .
 Joachims, T. (1999). Transductive inference for text classification using support vector machines. Proc. of the ICML , 200 X 209.
 Joachims, T. (2003). Transductive learning via spec-tral graph partitioning. Proc. of ICML , 290 X 297. Karp, R. (1972). Reducibility among combinatorial problems. Complexity of Computer Computations , 43 , 85 X 103.
 Mann, G., &amp; McCallum, A. (2007). Simple, robust, scalable semi-supervised learning via expectation regularization. Proc. of the ICML , 593 X 600. Ng, A., Jordan, M., &amp; Weiss, Y. (2001). On spectral clustering: Analysis and an algorithm. Proc. NIPS , 14 , 849 X 856.
 Sindhwani, V., Niyogi, P., &amp; Belkin, M. (2005). Be-yond the point cloud: from transductive to semi-supervised learning. Proc. of ICML .
 Wang, J., Chang, S.-F., Zhou, X., &amp; Wong, T. C. S. (2008). Active microscopic cellular image annota-tion by superposable graph transduction with im-balanced labels. IEEE CVPR . Alaska, USA.
 Zhou, D., Bousquet, O., Lal, T., Weston, J., &amp;
Scholkopf, B. (2004). Learning with local and global consistency. Proc. NIPS (pp. 321 X 328).
 Zhu, X. (2005). Semi-supervised learning literature survey (Technical Report 1530). Computer Sciences, University of Wisconsin-Madison.
 Zhu, X., Ghahramani, Z., &amp; Lafferty, J. (2003). Semi-supervised learning using gaussian fields and har-
