 Cross-lingual sentiment classification aims to automatically predict sentiment polarity (e.g., positive or negative) of data in a label-scarce target language by exploiting labeled data from a label-rich language. The fundamental challenge of cross-lingual learning stems from a lack of overlap between the feature spaces of source language data and that of target language data. To address this challenge, previous studies have been performed to make use of the translated resources for sentiment classification in the target language, and the classification performance is far from satisfactory because of the language gap between the source language and the translated target language.

In this paper, to address the above challenge, we present a novel deep neural network structure, called Weakly Shared Deep Neural Networks (WSDNNs), to transfer the cross-lingual information from a source language to a target lan-guage. To share the sentiment labels between two languages, we build multiple weakly shared layers of features. It al-lows to represent both shared inter-language features and language-specific ones, making this structure more flexible and powerful in capturing the feature representations of bilin-gual languages jointly. We conduct a set of experiments with cross-lingual sentiment classification tasks on multilingual Amazon product reviews. The empirical results show that our proposed approach significantly outperforms the state-of-the-art methods for cross-lingual sentiment classification, especially when label data is scarce.
 Cross-lingual; sentiment classification; auto-encoders
With the development of Web 2.0, more and more user generated sentiment data have been shared on the Web. They exist in the form of user reviews on shopping or opin-ion sites, in posts of blogs or customer feedback in differ-ent languages. These labeled user generated sentiment data are considered as the most valuable resources for sentiment classification. However, such resources in different languages are very imbalanced. Manually labeling each individual lan-guage is a time-consuming and labor-intensive job, which makes cross-lingual sentiment classification essential for this application.

Cross-lingual sentiment classification aims to automati-cally predict sentiment polarity (e.g., positive or negative) of data in a label-scarce target language by exploiting labeled data from a label-rich language. The fundamental challenge of cross-lingual learning stems from a lack of overlap between the feature spaces of source language data and that of target language data. To address this challenge, previous work in the literature mainly relies on machine translation engines or bilingual resources to directly translate labeled data from source language to target language [19, 49, 46, 46, 34, 30, 50]. With the translated resources, one way is to jointly train the bi-view sentiment classifiers on labeled data from source language and their translations in target language via ensemble methods. Another way is to firstly select a subset of pivot features from source language and their transla-tions in target language, and then use these pivot pairs to induce the language-independent features by modeling the corrections between pivot features and non-pivot features in an unsupervised fashion. Though pivot studies have been performed to make use of the translated resources for senti-ment classification in target language, the methods are very straightforward by directly employing an inductive classifier, and the classification performance is far from satisfactory because of the language gap between source language and the translated target language.

Recently, many efforts have been initiated on learning fea-ture representations with deep neural networks (DNNs) in the context of cross-lingual sentiment analysis [21, 17, 7, 22, 53, 40, 56, 18], inspired by the success of work on monolin-gual feature representations. Usually, the paired sentences from parallel corpora are used to learn bilingual feature representations across languages, eliminating the need of machine translation systems. By transferring the feature representations and labeling information from the source language space, we can learn a semantic-intensive target language feature representation, which can greatly improve the performance of target language sentiment classification tasks. In other words, the rich information transferred from the source language can help train the target language DNNs even with little labeled target language data. This is con-trary to the popular strategy of trading the representation powerofDNNsforbettergeneralizationperformanceinthe literature. In this paper, we will present how to use trans-ferred cross-lingual information to train a powerful DNNs without sacrificing linguistic richness, which will yield much better performance for cross-lingual sentiment classification tasks.

To this end, we propose a novel DNNs that hierarchically learns to transfer the semantic knowledge from the source language to the target language, namely Weakly Shared Deep Neural Networks (WSDNNs). Our WSDNNs are trained in a novel way that minimizes the errors incurred by a cross-lingual information transfer process. As a hierarchically non-lineal model, WSDNNs differ from the existing shallow cross-lingual information transfer algorithms, such as cross-lingual mixture model [26], cross-lingual subspace learning [55], and cross-lingual correspondence learning [34], which learn the cross-lingual feature representations by linear mathe-matics models. In WSDNNs, we model two DNNs that take a pair of parallel bilingual sentences as input respec-tively, followed by multiple weakly-shared network layers at the top. The output of the shared layer in WSDNNs yields the translator function that can be used to transfer cross-lingual information. To the best of our knowledge, exist-ing DNNs in the context of cross-lingual sentiment classi-fication [7, 22, 53, 56, 18] are representation sharing. In  X  X epresentation sharing structure X , a set of common features are constructed, which usually overestimates the importance of language-specific features. While our weakly parameter-shared scheme makes our networks more robust of repre-senting shared inter-language features as well as language-specific ones. To evaluate the effectiveness of the proposed approach, we conduct extensive experiments on multilingual Amazon product reviews for cross-lingual sentiment classi-fication tasks. The empirical results show the proposed ap-proach is effective for cross-lingual sentiment classification, and outperforms the state-of-the-art methods.

Our contribution in this paper is three-fold: (1) we pro-pose a novel architecture of DNNs to transfer the cross-lingual information from a source language to a target lan-guage, which can adequately mitigate the problem of in-sufficient training data in the target language by bringing in rich sentiment labels from the source language; (2) the proposed WSDNNs are trained in a novel way that directly minimizes the loss incurred by a label transfer function via back-propagation, which can avoid unbalanced tuning of the source language and the target language DNNs; (3) we show superior results of the proposed WSDNNs on extensive ex-periments as compared with several strong baselines and the other state-of-the-art methods.

The remainder of this paper is organized as follows. Sec-tion 2 introduces the related work. Section 3 presents our proposed deep neural network transfer learning framework for cross-lingual sentiment classification. Section 4 presents the experimental results. Finally, we conclude the paper in Section 5.
Sentiment classification has gained widely interest in in-formation retrieval (IR) and natural language processing (NLP) communities, we point the readers to recent books [31, 24] for an in-depth survey of literature on sentiment classi-fication. Methods in the literature for sentiment classifica-tion heavily rely on quality and quantity of the label cor-pora, which are considered as the most valuable resources in sentiment classification task. However, such sentiment re-sources are imbalanced in different languages. To leverage resources in the source language to improve the sentiment classification in the target language, cross-lingual sentiment classification approaches have been investigated.
Cross-lingual sentiment classification aims to automati-cally predict sentiment polarity (e.g., positive or negative) of data in a label-scarce target language by exploiting la-beled data from a label-rich language. The fundamental challenge of cross-lingual learning stems from a lack of over-lap between the feature spaces of source language data and that of target language data.

To bridge the language gap, previous work in the litera-ture mainly relies on machine translation engines or bilin-gual lexicons to directly adapt labeled data from a source language to a target language. Banea et al. [3] employed the machine translation engines to bridge the language gap in different languages for multilingual subjectivity analysis. Wan [46] proposed to use ensemble methods to train Chi-nese sentiment classification model on English labeled data and their Chinese translations. English labeled data are first translated into Chinese, and then the bi-view sentiment classifiers are trained on English and Chinese labeled data respectively. Pan et al. [30] proposed a bi-view non-negative matrix tri-factorization (BNMTF) model for cross-lingual sentiment classification problem. They employed machine translation engines so that both training and test data are able to have two representations, one in a source language and the other in a target language. The proposed model is derived from the non-negative matrix factorization models in both languages in order to make more accurate predic-tion. Prettenhofer and Stein [34] proposed a cross-lingual structural correspondence learning (CL-SCL) method to in-duce language-independent features. Instead of using ma-chine translation engines to translate labeled texts, the au-thors first selected a subsect of pivot features in the source language to translate them into the target language, and then use these pivot pairs to induce cross-lingual represen-tations by modeling the correlations between pivot features and non-pivot features in an unsupervised fashion. Recently, Xiao and Guo [51] used the similar idea with [34] for cross-lingual sentiment classification. Instead of in a fully unsu-pervised fashion, Xiao and Guo [51] performed representa-tion learning in a semi-supervised manner by directly in-corporating discriminative information with respect to the target prediction task. Li et al. [37] selected the samples in the source language that were similar to those in the target language to reduce the gap between two languages. Other similar work includes [48, 25, 1, 52, 13, 6, 12, 41, 27, 8, 23, 2, 11, 36]. All these approaches rely on machine translation or bilingual dictionary to build language connection.
Another group of studies propose to use unlabeled paral-lel data to fill the gap between two languages. To solve the feature coverage problem, Meng et al. [26] leveraged the un-labeled parallel data to learn unseen sentiment words. Sim-ilarly, Popat et al. [33] used the unlabeled parallel data to cluster features in order to reduce the data sparsity problem. Miao and Guo [50] formulated the cross-lingual representa-tion as a matrix completion problem to infer unobserved feature values of the concatenated document-term matrix in the space of unified vocabulary set from the source language and target language by using unlabeled parallel bilingual data. Zhou et al. [55] proposed a subspace learning frame-work by leveraging the partial parallel data for cross-lingual sentiment classification. However, the parallel data is also a scarce resource. Besides, some other studies addressed the cross-lingual knowledge transferring using parallel or com-parable corpora [29, 15, 28, 47, 45, 43].

Over the past few years, DNNs have gained a lot of at-tention from the research community and industry for their ability to automatically learn feature representations for a give task. Most recently, a number of studies leverage DNNs to learn bilingual (or multilingual) feature representations with parallel corpora. Bi-view feature representation has been successfully applied to many IR and NLP tasks, such machine translation [57], cross-lingual sentiment analysis [21, 17, 7, 53, 10, 40, 16, 55, 18, 39], cross-lingual information retrieval [20, 44], question answering [54], etc. The gen-eral idea is that they learn bilingual feature representation with aligned sentences throughout two phases: the language-specific feature representation learning phase and the shared representation learning phase. The key difference is that these existing models are  X  X haring representations X , which usually overestimates the importance of language-specific features. While our proposed WSDNNs are  X  X haring param-eters X , which makes our networks are more robust of repre-senting shared inter-language features as well as language-specific ones.
Given a labeled data set D s = { ( x s i ,y s i ) } N s i guage, in which x s i  X  R m is a text data from the source language and y s i  X  X  1 , 0 } is the positive sentiment label or negative sentiment label. 1 Our goal is to transfer the labels from the source language to the target language for senti-ment classification task. To facilitate the label transfer pro-cess, we also have another co-occurrence set of parallel bilin-gual pairs C = { (  X  x s j ,  X  x t j ) } N c j =1 , where  X  x denote a data from the source language and target language respectively. By exploring this parallel bilingual set, we can reveal the alignment between the source language and target language, which will facilitate the sentiment label transfer between these two languages.

In this paper, we will jointly learn the bilingual feature representations to effectively transfer the discriminative in-formation from the source language to the target language. The key idea of the deep neural network transfer learning is an efficient cross-lingual translator which can transfer the sentiment labels from the source language to the target lan-guage, even with the challenge of scarcely labeled target language data. By leveraging the learned deep neural net-work translator, we can solve the target language sentiment classification with extremely insufficient training data.
An auto-encoder is an unsupervised neural network which is trained to reconstruct a given input vector from its dis-tributed representation [4]. It can be seen as a special neural
Besides positive and negative sentiments, there are also neural and mixed sentiments in practical applications. In this paper, we only consider binary (positive, negative) sen-timent, but it is not hard to extend the proposed framework to address multi-class sentiment classification problems. Figure 1: Weakly Shared Deep Neural Networks (WSDNNs). network with three layers: the input layer, the latent layer, and the reconstruction layer. An auto-encoder contains an encoder function h ( x )= s e ( Wx + b ) and a counterpart decoder function  X  h ( x )= s d (  X  Wx +  X  b ) to minimize the re-construction error of loss function E r ( x ,  X  h ( h ( x ))). s (  X  ) are the non-linear activation function and decoder X  X  activation function respectively. Several auto-encoders can be used as building blocks to form a Stacked Auto-Encoders (SAEs) [4, 42]. Once an auto-encoder has been trained, one can stack another auto-encoder on top of it, by training a second one which sees the latent representation of the first one as input.

Recently, many efforts have been initiated on learning feature representations with DNNs in the context of cross-lingual sentiment analysis [21, 17, 7, 22, 53, 40, 56, 18]. First, they pre-train two SAEs for the source language and the target language respectively, which output latent rep-resentations of these two languages via the multiple layers of nonlinear encoding. Assume that these two SAEs have L 1 layers, where they are structured and built separately. After the first L 1 layers, the two SAEs begin to share their structures where L 2 shared layers are constructed as shown in Figure 1. These shared layers provide a way to trans-fer the information across two languages represented by the two SAEs. Therefore, the input into the shared layers is the mixture of outputs of the bottom L 1 layers by the two SAEs.

However, we observe that this kind of representation shar-ing layers tend to over-mix the features learned from two lan-guages. In other words, although there exist many shared features across different languages, it is difficult to com-pletely ignore the language-specific features through the shared layers, since the source language and the target language often contain many elements that cannot be expressed by the same set of the neurons in the shared layers. In this paper, we relax such representation sharing structure, and propose a novel weakly shared deep neural networks (WS-DNNs), whose structure is shown in Figure 1. The advan-tage of our proposed WSDNNs is the flexibility of repre-senting shared inter-language features as well as language-specific ones, making it more powerful in modeling feature representations of bilingual languages than representation sharing layers in the literature.
 Formally, there are L + 1 layers in WSDNNs, where L = L 1 + L 2 . Given a pair of parallel bilingual texts x s j x j respectively, we use x note the latent representation of a hidden layer l for the two SAEs respectively. At the first layer, we set x (0) s j = x x following. For l =1 , 2 ,  X  X  X  ,L , the layer-wise processing of these two inputs through the whole networks are defined as follows resentations in source language and target language SAEs rameters in the source language and target language respec-tively.

The first L 1 layers are expected to learn the feature rep-resentations of a source language and a target language re-spectively, while the last L 2 layers provide shared feature representations. Under weakly-shared assumption, the pa-rameters of the last L 2 layers should be set to be close to each other in order to trade off the shared inter-language features as well as language-specific ones. Inspired by [38], we propose the following penalty term  X  to quantify such trading-off: Minimizing this term will minimize the difference of param-eters in the last L 2 layers of two SAEs networks. It will be added to the proposed objective function in the next Sub-section. A positive value of balancing coefficient will be multiplied with it in the objective function, which reflects the above trading-off for the weakly-shared layers.
The goal of the proposed cross-lingual transfer learning is to transfer the labels annotated on the source language data set D s to annotate an arbitrary test data x t in the target lan-guage. Following the literature [5, 38], for an arbitrary test data x t andalabeleddata x s i in D s , we define a translator shared layer outputs from the two SAEs, where denotes transpose operation in this paper, and M s,t is the trans-lator matrix. This translator function is used to transfer the sentiment labels from the source language to the target language as follows: which combines all the labeled data from the source language weighted by the corresponding translator functions. For the binary sentiment classification task, the distribution f ( x is the form of [1 , 0] for positive and [0 , 1] for negative, whose sign predicts the label of the target language data x t .
The parameters can be learned by minimizing the loss with the sentiment label transfer process, as well as mini-mizing the inconsistency between a set of parallel bilingual pairs to capture the cross-lingual alignment information. We model these two criteria as below:
Empirical loss on the labeled target language data set : Usually, we have a small size of labeled data A t = { ( x t i ,y t i ) } N t i =1 in the target language, where x target space and y t i  X  X  1 , 0 } is the sentiment label. We aim to minimize the training errors incurred by the label transfer function f on the labeled target language data set. The empirical loss on A t is as follows: where ( x )=log(1+exp(  X  x )) is a logistic loss function, which is used to measure the cross-lingual label transfer er-ror. The parameters are  X  = { W ( l ) s , b ( l ) s , W (
Empirical loss on the parallel bilingual pairs :Given a set of parallel bilingual pairs C = { (  X  x s j ,  X  x source language and target language, we wish to maximize the alignment between each pair of bilingual texts in this set, yielding a translator function that can well capture the alignment between these bilingual pairs. Specifically, we minimize the objective function as follows:
This loss function can be seen as a measurement of mis-alignment between parallel bilingual pairs caused by the translator function. Clearly, minimizing this loss function will ensure the consistency of translator function over the parallel bilingual pairs C .
After the above analysis, we propose the following ob-jective function to learn the parameters of deep semantic translator as input of the top-layers of WSDNNs:
M s,t 2 F is the regularization term. The parameters  X  ,  X  and  X  weigh the importance of different term. In particular,  X  is the importance weight on alignment between the parallel bilingual pairs,  X  is the importance weight of reconstruction error,  X  adjusts the weakly-shared structure, and  X  weights the regularization term.

To train the WSDNNs, we first pre-train each layer per time in a greedy fashion by using the unsupervised data as in conventional auto-encoder algorithms. The pre-train WSDNNs set up a good starting point that can fine tuned according to the objective function by employing the avail-able supervision information.

We implement a back propagation process starting from the top output layers down through the whole WSDNNs to adjust all parameters. Each parameter in  X  is updated by stochastic gradient descent in back-propagation algorithm below: where  X  is the learning rate. These parameters can be com-puted based on the stochastic gradient optimization. The convergence is guaranteed when the number of iterations reaches the max or the objective function value is smaller than a predefined threshold. The overall complexity of our WSDNNs is similar with an existing SAE [4, 42].

We exploit the supervision information on the labeled tar-get language data set A t to directly tune the target lan-guage SAE. Similar to [38], we add an additional softmax layer upon the target language SAE that outputs the senti-ment labels of the target language data. Then the labeled target language data in A t are used to compute the back-propagated errors to tune the parameters in the target lan-guage SAE. In this paper, the back-propagated errors only arise from the target language labels that are intended to enhance the tuning of target language SAE. This can avoid unbalanced tuning of source language and target language SAEs when much more labeled data are used in label trans-fer process.
We use the multilingual sentiment classification data sets provided by Prettenhofer and Stein [34], which contain Ama-zon product reviews in four languages (English (E), French (F), German (G) and Japanese (J)) of three categories (Books (B), DVD (D), Music (M)). The English product reviews are sampled from previous cross-domain sentiment classification data sets, while the other three language product reviews are crawled from Amazon by the authors in November. Each category of product reviews contains a balanced training set and test set, each of which consists of 1000 positive and 1000 negative reviews for each of the language. Besides, we also collect a set of parallel bilingual reviews between English and each of the other three languages. Following the literature [50], each review is represented as a unigram bag-of-words vector representation and each entry is com-puted with tf-idf. Given the four languages from the three categories, we construct 18 cross-lingual sentiment classi-ficationtasks(EFB,EFD,EFM,EGB,EGD,EGM,EJB, EJD, EJM, FEB, FED, FEM, GEB, GED, GEM, JEB, JED, JEM) between English and the other three languages. For example, the task EFB uses English Books reivews as the source language and uses French Books reviews as the target language.
In our experiments, we compare our proposed WSDNNs with the several strong baselines and state-of-the-art meth-ods in the literature for cross-lingual sentiment classification:
In all experiments, we train the sentiment classification model on the learned representations using the linear sup-port vector machine (SVM) [9]. For CL-LSA , CL-OPCA and CL-TS , we use the same parameter setting as suggested in the paper [50]: the latent dimension is set to 50. For CL-SCL , we use the same parameter setting as suggested in the paper [34]: the number of pivot features is set as 450, the threshold value for selecting pivot features is 30, and the re-duced dimensionality after singular value decomposition is 100. For CL-SP , we use the same parameter settings as suggested in the paper [55]: the shared latent dimension is set to 100 and the normalized parameter is set to 10  X  2 .We choose the above parameter values empirically because these
There is a slight exception: using the same data set and parameter setting, our re-implement systems have slight dif-ferences with the results reported in [50]. tasks. The bold formate indicates the best results,  X  indicates that the difference between our proposed WSDNNs and stateof-the-art CL-SP is significant with p&lt; 0 . 05 under a McNemar paired test for labeling disagreements, and  X  indicates the mildly significant with p&lt; 0 . 08 .
 parameter settings have shown superior performance on the same benchmark [50, 34, 55].
For each of the 18 cross-lingual sentiment classification tasks, we use all labeled data from the source language. For the target language, we use the test set as testing data while randomly choose 100 documents from the training set as la-beled data. Thus, for each task, we have 2000 labeled doc-uments from the source language, 100 labeled documents from the target language as auxiliary data ( A t = 100) and the rest of target language as test data, as well as a set of parallel bilingual pairs. We run each experiment 10 times with different random selections of 100 labeled train-ing documents from the target language. All parameters in our model are tuned based on a 5-fold cross-validation procedure on the training data, and the parameters are se-lected when the best performances are achieved. We train our model with 5-layers WSDNNs from bottom up. The last three ones are weakly shared layers. The average sen-timent classification accuracies and standard deviations are presented in Table 1.

From Table 1, we can see that our proposed WSDNNs clearly outperforms the several strong baselines on 16 out of the 18 tasks. The target baseline TB performs poorly on all the 18 tasks, which indicates that 100 labeled docu-ments from the target language is far from enough to obtain an accurate and robust sentiment classifier for sentiment classification. All the other seven cross-lingual sentiment classification methods, CL-LSA, CL-SCL, CL-MT, CL-MM, CL-OPCA, CL-TS and CL-SP, consistently outperform the baseline method TB across all the 18 tasks, which demon-strates that the labeled training data from source language is useful for classifying target language data. Among the 18 tasks, WSDNNs outperforms CL-LSA, CL-SCL, CL-MT, CL-MM, CL-OPCA and CL-TS across all the 18 tasks, out-performs CL-SP on 16 out of the 18 tasks and achieves the slight lower performance than CL-SP on the rest two tasks (FEB and GEM).

We also conduct significance tests for our proposed ap-proach and each of the other methods using a McNemar paired test for labeling disagreements [14]. The results in bold formate indicates the best results, the overall results indicate that they are significant with p&lt; 0 . 05. All these results demonstrate the efficacy and robustness of the pro-posed WSDNNs for cross-lingual sentiment classification.
Recently, many efforts have been initiated on learning feature representations with DNNs in the context of cross-lingual sentiment classification tasks [21, 17, 7, 53, 10, 40, 16, 18, 39]. However, these existing methods in the litera-ture like bilingual SAEs [53, 55, 18] are representation shar-ing. While our proposed WSDNNs are weakly parameter-sharing, which makes our networks more flexible, especially when cross-lingual sentiment classification requires more lan-guage specific features are learned. In other words, by shar-ing parameters, we allow deviation exists between the fea-ture representations of different languages. In contrast, in  X  X epresentation sharing X  structure, a set of common features are constructed, which usually overestimates the importance of language-specific features. Therefore, we also compare with the following representative representation sharing DNNs and state-of-the-art DNNs in the literature for cross-lingual sentiment classification: Chandar et al. [7] proposed a pred-icative auto-encoder for learning shared representation for cross-lingual tasks. Zhou et al. [53] proposed a two SAEs to learn feature representation for cross-lingual sentiment classification. Hermann and Blunsom [16] proposed to learn compositional distributed semantics for multilingual analy-sis. Zhou et al. [56] proposed to learn bilingual sentiment word embeddings for cross-lingual language analysis. Jain et al. [18] leveraged the sentence aligned corpora to develop a cross-lingual sentiment analysis tool. In this paper, we re-implement these representation sharing DNNs based on the original papers and train these models with a 5-fold cross-validation procedure on the training data. The average re-sults are presented in Table 2.

From Table 2, we can see that our proposed WSDNNs significantly outperforms the state-of-the-art representation sharing DNNs in general with p&lt; 0 . 05. Among the 18 tasks, WSDNNs achieves the best performance on 15 tasks. This tasks by using DNNs. The bold formate indicates the best results, our proposed WSDNNs and other DNNs are significant with p&lt; with p&lt; 0 . 08 .
 confirms that our proposed weakly shared layers are more suitable to model the DNNs than the existing representation sharing layers.
Next, we look into the performance of all eight approaches by varying the number of labeled training documents from the target language. We use the same experimental set-ting as before, but investigate a range of different num-bers, n t = { 100 , 200 , 300 , 400 , 500 } , as the number of labeled training data from the target language. Given a value n t randomly select n t documents from the training set of the target language as labeled data for each experiment. We also run the experiments on the same 2000 test data from the target language. Each experiment is repeated 10 times based on different random selection of the labeled training data from the target language. Figure 2 shows the average classification accuracies (%) vs. different numbers of labeled target language data on all the 18 tasks. 3
From Figure 2, we can see that when the number of labeled documents from the target language is small, TB performs poorly. By increasing the number of labeled data from the target language, TB can greatly increase the classification accuracy and even outperform the CL-MM method. The CL-MM has a stable performance across the range of differ-ent numbers. The CL-LSA method has inconsistent perfor-mance across the 18 tasks. Its performance is better than TB when the labeled training data in the target language is very limited and is poor than TB when the labeled tar-get data reaches 300 on most tasks. By using the machine translation resources, the CL-MT method outperforms TB, CL-LSA, CL-SCL and CL-MM. With a more sophisticated representation, the CL-TS, CL-SP and WSDNNs achieve the better performance than other five methods. Our pro-posed WSDNNs significantly outperforms all the other seven comparison methods across all experiments except on the
In Figure 2, we do not present the results of CL-OPCA because this method has used all data from both languages in the original paper. tasks of EJB and GEM. In addition, we also observe that WSDNNs achieves high accuracies even when the number of labeled target language data is small. This is important for transferring the knowledge from a source language to a target language in order to reduce the labeling effort.
In this subsection, we look into the parameters  X  and  X  of objective function in Eq. (6). We choose  X   X  X  0 , 0 . 5 , respectively. As usual, we set  X  =  X  = 1 to equally weigh the three types of loss functions. Here, we study their im-pacts on the performances in Figure 3. When  X  =0,the averageaccuracyisthelowest. Thereasonmaybethatin this case the source language and the target language SAEs are completely independent without any shared layers. This structure fails to jointly model the source language and the target language representations, and is unable of transfer-ring sentiment labels between two languages. The accuracy increases rapidly when  X  becomes large. On the other hand,  X  can also improve the accuracy when it is set to a proper value to regularize the model. The best accuracy on 16 cross-lingual tasks (except on the task of FEB and GEM) is achieved when  X  =1and  X  =0 . 5. The accuracies with dif-ferent values of parameters are not varied very much, which suggests that our proposed WSDNNs are not very sensitive to the parameters.
In this paper, we propose a novel weakly parameter-shared deep neural networks (WSDNNs) to transfer cross-lingual information from the source language to the target language. To share the sentiment labels between two languages, we build multiple weakly shared layers of features. It allows to represent both shared inter-language features and language-specific ones, making our networks more flexible and power-ful in capturing the feature representations of bilingual lan-guages than the existing representation sharing structure. The proposed WSDNNs are trained in a novel way that di-rectly minimizes the loss incurred by a label transfer func-tion. This yields a fine tuning strategy to train WSDNNs from top down with via back-propagation, which can avoid unbalanced tuning of the source language and the target language. We conduct extensive experiments on 18 cross-lingual sentiment classification tasks constructed from the Amazon product reviews in four languages. The empirical results show the superior performances as compared with several strong baselines and state-of-the-art methods.
There are three research directions that we are planning to investigate in future. First, a straightforward path of the future research is to use the proposed WSDNNs for other language pairs, and apply to other tasks such as cross-lingual information retrieval [44]. Second, another straightforward extension is to replace the simple feature representations of SAE with more systematic context-aware learning methods, such as convolutional neural networks (CNNs) or recursive neural networks (RNNs). Third, inspired by the successful application of attention mechanism in multilingual neural machine translation [35], we will try to model the cross-lingual sentiment information with a shared attention mech-anism.
 This work was supported by the National Natural Science Foundation of China (No. 61303180, No. 61300144 and No. 61573163), the Fundamental Research Funds for the Central Universities (No. CCNU15ZD003 and No. CCNU16A02024), and also supported by a Discovery grant from the Natu-ral Sciences and Engineering Research Council (NSERC) of Canada and an NSERC CREATE award. We thank the anonymous reviewers for their insightful comments. [1] B. A., A. Joshi, and P. Bhattacharyya. Cross-lingual [2] S. Artem, J. Laura, H. Felix, and R. Stefan. Boosting [3] C. Banea, R. Mihalcea, J. Wiebe, and S. Hassan. [4] Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle, [5] A. Bordes, J. Weston, and N. Usunier. Open question [6] G. Cao, J. Gao, J.-Y. Nie, and J. Bai. Extending query [7] S. Chandar A P, S. Lauly, H. Larochelle, M. Khapra, [8] A. Chen and F. C. Gey. Multilingual information [9] R.Fan,K.Chang,C.Hsieh,X.Wang,andC.Lin.
 [10] M. Faruqui and C. Dyer. Improving vector space word [11] T. Ferhan, L. Jimmy, and D. W. Oard. Combining [12] J. Gao, E. Xun, M. Zhou, C. Huang, J.-Y. Nie, and [13] W. Gao, C. Niu, J.-Y. Nie, M. Zhou, J. Hu, K.-F. [14] L. Gillick and S. Cox. Some statistical issues in the [15] A. Gliozzo and C. Strapparava. Exploiting comparable [16] K. M. Hermann and P. Blunsom. Multilingual models [17] J.-T.Huang,J.Li,D.Yu,L.Deng,andY.Gong.
 [18] S. Jain and S. Batra. Cross lingual sentiment analysis [19] S. James, G. Gregory, Q. Yan, and E. David. Mining [20] J. Kim, J. Nam, and I. Gurevych. Learning semantics [21] A. Klementiev, I. Titov, and B. Bhattarai. Inducing [22] T. Ko X  cisk  X  y, K. M. Hermann, and P. Blunsom. [23] W. Kraaij, J.-Y. Nie, and M. Simard. Embedding [24] B. Liu. Sentiment analysis and opinion mining. [25] B. Lu, C. Tan, C. Cardie, and B. K. Tsou. Joint [26] X. Meng, F. Wei, X. Liu, M. Zhou, G. Xu, and [27] S.-H. Na and H. T. Ng. Enriching document [28] X. Ni, J.-T. Sun, J. Hu, and Z. Chen. Cross lingual [29] J.-Y. Nie, M. Simard, P. Isabelle, and R. Durand. [30] J. Pan, G.-R. Xue, Y. Yu, and Y. Wang. Cross-lingual [31] B. Pang and L. Lee. Opinion mining and sentiment [32] J. Platt, K. Toutanova, and W. Yih. Translingual [33] K. Popat, B. A.R, P. Bhattacharyya, and G. Haffari. [34] P. Prettenhofer and B. Stein. Cross-language text [35] I. V. Serban, A. Sordoni, Y. Bengio, A. C. Courville, [36] S. Shigehiko, H. Felix, S. Artem, and R. Stefan. [37] L. Shouhan, W. Rong, L. Huanhuan, and H. Churen. [38] X. Shu, G. Qi, J. Tang, and J. Wang. Weakly-shared [39] G. Stephan, B. Yoshua, and C. Greg. Bilbowa: Fast [40] X. Tang and X. Wan. Learning bilingual embedding [41] D. Trieschnigg, D. Hiemstra, F. de Jong, and [42] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. [43] I. Vuli  X c and M.-F. Moens. A unified framework for [44] I. Vuli  X c and M.-F. Moens. Monolingual and [45] I. Vuli  X c, W. Smet, and M.-F. Moens. Cross-language [46] X. Wan. Co-training for cross-lingual sentiment [47] Z.Wang,Z.Li,J.Li,J.Tang,andJ.Z.Pan.Transfer [48] B. Wei and C. Pal. Cross lingual adaptation: An [49] K. Wu, X. Wang, and B. liang Lu. Cross language [50] M. Xiao and Y. Guo. A novel two-step method for [51] M. Xiao and Y. Guo. Semi-supervised representation [52] Z.Ye,X.Huang,B.He,andH.Lin.Mininga [53] G. Zhou, T. He, and J. Zhao. Bridging the language [54] G. Zhou, T. He, J. Zhao, and P. Hu. Learning [55] G. Zhou, T. He, J. Zhao, and W. Wu. A subspace [56] H. Zhou, L. Chen, F. Shi, and D. Huang. Learning [57] W. Y. Zou, R. Socher, D. M. Cer, and C. D. Manning.
