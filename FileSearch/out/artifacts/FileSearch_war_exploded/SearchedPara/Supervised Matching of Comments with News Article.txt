 Comments constitute an important part of Web 2.0. In this paper, we consider comments on news articles. To simplify the task of relating the comment content to the article con-tent the comments are about, we propose the idea of show-ing comments alongside article segments and explore auto-matic mapping of comments to article segments. This task is challenging because of the vocabulary mismatch between the articles and the comments. We present supervised and unsupervised techniques for aligning comments to seg-ments the of article the comments are about. More specif-ically, we provide a novel formulation of supervised align-ment problem using the framework of structured classifica-tion. Our experimental results show that structured classi-fication model performs better than unsupervised matching and binary classification model.
 H.3.1 [ Information Systems ]: Information Storage and Retrieval Algorithms Comments, Structured Classification, Enrichment, ESA User generated content is the central theme of Web 2.0. While there are several forms of user generated content like blogs, photo/video uploads, reviews, etc., comments are the primary form of user interaction in several sites. Comments form a light-weight mechanism for user participation as they are primarily reactive. Recently [3] proposed mapping com-ments to article segments to simplify readers X  task of relating comment content to article content.

In this paper we consider a variation of the problem pro-posed in [3]. More specifically, while [3] addressed the unsu-pervised matching problem, we consider supervised match-ing. We state the problem formally as follows.

Problem Statement: Let an article A be characterized by the set of segments S ( A )= { s 1 ,...,s n s } ,whereaseg-ment s k could be a paragraph, sentence, etc. and they form a partition of the article A . The comments associated with article A are denoted C ( A )= { c 1 ,  X  X  X  ,c n c } . Given a set of matching (comment, segment) pairs, the goal is to design a learning machine which, when presented with a article A and its associated comments, C ( A ), correctly identifies for each comment, c  X  C ( A ), the related segment or segments in S ( A ).

Our technical contributions directly address the above challenges and are two fold. In a machine learning scenario, the key elements are the features as well as the classification technique. We address and contribute on both fronts. Supervised Classification: We provide two different for-Enriched Topic Features: Extending the ideas of enrich-
The problem of retrieving article segments for comments has the distinct flavor of traditional information retrieval where segments can be considered documents and the com-ments can be considered queries. We show through our ex-periments that the proposed techniques outperform two tra-ditional IR representatives, viz., Lucene and Indri. We also show that the notion of enrichment complements traditional IR features.

The paper is organized as follows. Section 2 discusses the main contribution that of possible supervised approaches to match comments and segments. Section 3 proposes two novel feature representations: enriched representation using ESA [1] and coreference features. Section 4 contains the experimental results. The paper closes with conclusions and a discussion.
There is no readymade solution strategy for the problem statement given before. To this end we investigate both supervised and unsupervised strategies. To begin the dis-cussion, we assume that we are given a dataset D defined on a collection of articles { A 1 ,...,A n } where D = { ( c ij ,S ij ) | c ij  X  C ( A i ) , (1)
In this section we formulate the supervised learning prob-lem in two ways, namely the structured learning approach and the binary classification approach.
In the structured learning setup [4], one considers the problem of assigning labels y  X  X  , one of many labels, to an observation x  X  X  . Structured classification proceeds by assuming that, given an x and w , the following function is tractable where  X  ( x, y ): X X Y X  R d is a suitably defined feature function.

In this paper, we discuss relevant details of structured learning as applied to the problem at hand. For a detailed review of structured learning see [4].

In the structured learning setup we make an assumption that in (1), each c ij is mapped to only one segment, in other words | S ij | = 1, where the cardinality of S ij is denoted by |
S ij | . The resultant dataset is written as D = { ( c ij ,s where s ij denotes the segment correspond to c ij .Wewish to explore structured learning framework for this dataset.
We begin by assuming that  X  ( c, s ): C ( A )  X  S ( A )  X  R d is a given feature function. For more description of  X  ,see Section 3.3. Given a w  X  R d , we wish to infer the segment associated with a comment c in a particular article A which can be computed by straightforward enumeration as | S ( A ) | is low.

The learning of w on a training set depends on the specific loss function. For the problem at hand we use where 1 is the indicator function. The idea is to choose w such that for each comment c ij  X  C ( A i ), the score of the correct segment s ij will be highest among all the other seg-ments. This can be ensured by requiring that w  X  ( c ij ,s w  X  ( c, s ) ,  X  s  X  S ( A )  X  s ij . This immediately motivates the following optimization problem. subject to w T  X  ( c ij ,s ij )  X  w  X  ( c ij ,s )+1  X   X  ij ,
Lemma: At optimality, where  X  is defined in (3) and f is defined in (2) . Proof: Let s = f ( c ij ) = s ij be the segment identified by (2). Then see that By definition of s the difference of scores which implies that  X  ij  X  1. The claim holds whenever the assignment implemented by (2) disagrees with the correct segment s ij .Notingthat  X  ij is lower-bounded by 0, the claim is proved.

The optimization problem pertaining to the learning prob-lem is a Quadratic Program ( QP )involving d variables and are around 10 and that of | S ( A i ) | are often 10  X  15 depending on the segmentation. The number of constraints is linear in the number of articles. In general solving such problems is hard as there could be large number of constraints. See [4] for the general case. Fortunately, we can solve the original problem directly by using a Convex QP as the number of constraintsissmall.
In the previous section we discussed a strategy based on structured learning. But as noted the setting applies when dataset (1) is restricted to have only one segment assigned to each comment. We use the same  X  ( c, s ) and define a classification problem. We assign a label Y : C ( A )  X  S ( A ) { X  1 } to all (comment, segment) pairs as follows: Y ( c ij ,s )= 1 ,  X  s  X  S ij . Using this one could immediately rewrite (1) a binary classification problem. This immediately motivates a SVM-like approach subject to Again we use a generic SVM solver for this purpose.
One could choose to ignore the segments and instead choose to learn a similarity function by exploratory analysis. The key idea is to design a similarity function sim ( c, s ): C ( A ) S ( A )  X  R such that for a fixed c it will enable us to eval-uate how similar a comment c is to a particular segment s . To this end we consider that a comment or segment is rep-resented by a vector as represented by  X  (  X  ). Given such a representation, we define the the cosine similarity The vectors  X  ( c )and  X  ( s ) are obtained from various feature functions discussed in Section 3.
In our previous work [3] we have experimented with sev-eral feature representations for comment alignment namely bag of words (BOW), semi-supervised PLSA, and Latent Dirichlet Allocation (LDA). However because of extremely short length of comments and segments they are not very useful to capture the semantics . To address this issue we propose to leverage external corpus to have a more enriched representation. To this end we propose Explicit Semantic Analysis and coreference features. Unlike the above latent space approaches like LDA and SS-PLSA described above, explicit semantic analysis (ESA) uses a rich external corpus for representation [1]. Wikipedia is usually the corpus of choice. Let a be a Wikipedia article. For a term t ,let k t,a denote the strength of association of t to a . Since Wikipedia has pages for most of the important concepts (people, places, events, organizations, etc.), we can consider k t,a as the affinity of t to the concept a and the vector k t of all such weights as a representation of t in the concept space represented by Wikipedia.

Given a document d , the feature vector for d is given by In our case the document d could be a segment or a com-ment.
In addition to bag of words features, we also add a class of lexical and semantic features we call coreference features because of their widespread use in coreference resolution.
These features are listed below. 1. Number: Singular, Plural 2. Gender: Male, Female, Neuter, Unknown 3. Semantic class: Person, Location, Organization, Date, 4. Animacy: Human, Animal These features are calculated using dictionaries, Stanford NER and Wordnet. Using this, we associate a 16-dimensional binary feature with each token t .

As mentioned before, these features are very popular in coreference resolution literature [2]. We can think of com-ment mapping as more complex f orm of reference resolution where we want to identify whether a comment directly or indirectly refers to an article segment. While these features are defined at token level for references in the coreference lit-erature, we define them at segment or comment level. More formally, let f t be the binary feature vector associated with atoken t . The feature vector for a segment/comment s is given by
Given a comment c and a segment s , we can calculate fea-tures: BOW, LDA topics, SS-PLSA topics, Coreference fea-tures, and ESA for both c and s . For each of these features, we then calculate the cosine similarity. The final feature vector consists of BOW cosine similarity, Lucene score, In-dri Score, LDA topic similarity, SS-PLSA similarity, Coref-erence feature similarity, and ESA similarity.
In this section we investigate several key questions related to the models proposed before. Broadly there are three ques-tions one would like to explore. ( a ) The effectiveness of the feature representations. We in-vestigate empirically the effect of enrichment . ( b ) Does supervision increase performance? ( c ) Comparison between structured approach vis-a-vis bi-nary classification.
 In this section we explore these issues.
We created a dataset D by collecting 208 news articles along with  X  10 comments for each of the articles from http: //news.yahoo.com . There were a total of 1079 comments in the corpus. We also created another dataset D enriched with same articles and comments as of D but with each of the articles enriched with additional (4 X 8) related articles. The related articles for an original article have been found by Google news search ( http://news.google.com ) with title of the original article as the search query.

The articles have an average length of 383.9 words (after stemming and stop word removal), 19.3 segments and 5.3 comments. The average length of comments is 24.6 words. In D enriched , an average of 1316.2 words and 56.2 segments were added per article.

We created ground truth for for all the comments of all articles in the dataset. We have experimented with both the datasets D and D enriched with different methods discussed in Section 3.

Let S ij be the set of true related article-segments (found by human inspection) for comment c ij as in (1).

If | S ij | &gt; 1, then c ij has multiple related article-segments or if | S ij | =0, c ij has no related article-segment.
Let r ij be the retrieved result for comment c ij .Wecon-sider this to be correct if r ij  X  S ij .
 The Retrieval Index is defined as: i.e., RI is the ratio of number of correctly matched com-ments and total number of comments. Even if a comment is associated with multiple segments, we use only one segment from the classifier. It can be seen that most of the comments are short and are usually about one specific topic discussed in the article. Hence the above formulation of using only one retrieved segment is reasonable.

All the numbers reported here are the results of 10-fold cross validation. Table 1: RI for unsupervised and supevised match-ing techniques.

Features for supervised classification: To provide a rich feature space for the classification models, we use a binned representation of each feature. Let s be the value of afeature S (e.g., BOW similarity score). We choose a set T
S of thresholds for S . Then the binned representation of s is We thus convert each underlying feature to a T S -dimensional feature capturing T S + 1 bins. For example, if s =0 . 35 and T
S = { 0 . 1 , 0 . 5 , 0 . 9 } , then the binned feature representation of s is &lt; 100 &gt; .Inourexperiments, | T S | =10worked well.
In the first experiment, for each comments and the seg-ments of the article it corresponds to, we calculate the fol-lowing seven features: BOW, LDA (or enriched LDA, de-noted eLDA), SS-LDA, ESA, Coreference features, Lucene, and Indri. The features were binned as discussed above. The best results for SVMStruct were obtained with binned rep-resentation while that for binary SVM were obtained with non-binned representation. Since the number of matching segments for a comment is much smaller than the number of non-matching comments, the training data is unbalanced. We randomly sampled a subset of negative examples. The best C obtained using parameter sweeps for StructSVM is 0.5 while that for binary SVM is 1.1.

Table 1 shows RI for unsupervised matching as well as binary and structured SVM classifiers. 1. Supervised approaches have higher RI compared to the 2. Structured classification performs better than binary
The best performance using unsupervised matching is achieved by combining eLDA with BOW on enriched corpus  X  57.8% of the comments are matched correctly (Table 2(b)). eLDA+BOW improves LDA by 124% and BOW by 3.4%.

It should be noted that all feature bring down the effec-tiveness of BOW when combined with it except eLDA. This shows that enriched topics provide a representation that is complementary to BOW.

It can also be seen that while ESA outperforms eLDA (by 14%), eLDA+BOW is more effective compared to ESA+BOW (by 8%).
 Note: In Table 2, only eLDA features are computed on (b) Performance on the Enriched Dataset ( D enriched ). Table 2: Effectiveness of enrichment using unsuper-vised matching. The table show RI scores for differ-ent features on two datasets  X  D and D enriched . the enriched dataset as other features like BOW, PLSA, etc. cannot exploit the enriched dataset.
In this paper, we have proposed new approaches to the problem of aligning comments to relevant parts of the arti-cle to reduce the readers X  cognitive burden. We first pose this problem as a structured classification problem as well as a binary classification problem. We show that structured classification outperforms binary classification. We then ex-plore two additional features for this task: Explicit Semantic Analysis and coreference features. Explicit Semantic Rep-resentation using Wikipedia does not perform well. This implies corpus-specific enrichments are more effective com-pared to generic enrichment schemes. In future work, we plan to explore increasing the effectiveness of corpus-specific enrichment schemes. For enrichment, the source and selec-tion of documents, number of documents needed, etc. are very important and we plan to come up with systematic guidelines for these. [1] E. Gabrilovich and S. Markovitch. Computing semantic [2] A. Rahman and V. Ng. Narrowing the modeling gap: A [3] D. K. Sil, S. H. Sengamedu, and C. Bhattacharyya. [4] I. Tsochantaridis, T. Joachims, T. Hofmann, and
