 Stanford University Stanford University University of Leuven United States Naval Academy University of Arizona Stanford University builds on the previous model X  X  cluster output. The two stages of our sieve-based architecture, our approach makes use of global information through an entity-centric model that encourages its simplicity, our approach gives state-of-the-art performance on several corpora and genres, and has also been incorporated into hybrid state-of-the-art coreference systems for Chinese and
Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction
Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction.
 lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b;
Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference X  X erforming coreference resolution jointly for several or all mentions in a document X  X ather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and
Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011).
 entity-centric inference with machine learning approaches to coreference resolution.
But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune and difficult to apply to new problems and genres as well. Rule-based models like Lappin and Leass (1994) were a popular early solution to the subtask of pronominal anaphora resolution. Rules are easy to create and maintain and error analysis is more transparent. But early rule-based systems relied on hand-tuned weights and were not capable of global inference, two factors that led to poor performance and replacement by machine learning.
 supervised and unsupervised models with the advantages of deterministic, rule-based systems. We introduce a model that performs entity-centric coreference, where all men-tions that point to the same real-world entity are jointly modeled, in a rich feature space using solely simple, deterministic rules. Our work is inspired both by the seminal early work of Baldwin (1997), who first proposed that a series of high-precision rules could be used to build a high-precision, low-recall system for anaphora resolution, and by more recent work that has suggested that deterministic rules can outperform machine learning models for coreference (Zhou and Su 2004; Haghighi and Klein 2009) and for named entity recognition (Chiticariu et al. 2010).
 detection and coreference resolution, as well as a smaller post-processing step. In the mention detection stage, nominal and pronominal mentions are identified using a high-recall algorithm that selects all noun phrases (NPs), pronouns, and named entity mentions, and then filters out non-mentions (pleonastic it , i-within-i, numeric entities, partitives, etc.).
 erence models (or  X  X ieves X ), applied from highest to lowest precision. Precision can be informed by linguistic intuition, or empirically determined on a coreference corpus (see
Section 4.4.3). For example, the first (highest precision) sieve links first-person pronouns inside a quotation with the speaker of a quotation, and the tenth sieve (i.e., low precision but high recall) implements generic pronominal coreference resolution. 886 erence decision to be globally informed by the previously clustered mentions and their shared attributes. In particular, each deterministic rule is run on the entire discourse, using and extending clusters (i.e., groups of mentions pointing to the same real-world entity, built by models in previous tiers). Thus, for example, in deciding whether two mentions i and j should corefer, our system can consider not just the local features of i and j but also any information (head word, named entity type, gender, or number) about the other mentions already linked to i and j in previous steps.
 resolution models can be easily integrated.
 precision in the system and make use of entity-level information with rule-based architecture. The mention detection stage heavily favors recall, and the following sieves favor precision. Our results here and in our earlier papers (Raghunathan et al. 2010;
Lee et al. 2011) show that this design leads to state-of-the-art performance despite the simplicity of the individual components, and that the lack of language-specific lexical features makes the system easy to port to other languages. The intuition is not new; in addition to the prior coreference work mentioned earlier and discussed in Section 6, we draw on classic ideas that have proved to be important again and again in the history of natural language processing. The idea of beginning with the most accurate models or starting with smaller subproblems that allow for high-precision solutions combines the intuitions of  X  X haping X  or  X  X uccessive approximations X  first proposed for learning by
Skinner (1938), and widely used in NLP (e.g., the successively trained IBM MT models of Brown et al. [1993]) and the  X  X slands of reliability X  approaches to parsing and speech recognition [Borghesi and Favareto 1982; Corazza et al. 1991]). The idea of beginning with a high-recall list of candidates that are followed by a series of high-precision filters dates back to one of the earliest architectures in natural language processing, the part of speech tagging algorithm of the Computational Grammar Coder (Klein and Simmons 1963) and the TAGGIT tagger (Greene and Rubin 1971), which begin with a high-recall list of all possible tags for words, and then used high-precision rules to filter likely tags based on context.
 simple made-up text. We then describe our model in detail and test its performance on three different corpora widely used in previous work for the evaluation of coreference resolution. We show that our model outperforms the state-of-the-art on each corpus. Furthermore, in these sections we describe analytic and ablative experiments demonstrating that both aspects of our algorithm (the entity-centric aspect that allows the global sharing of features between mentions assigned to the same cluster and the precision-based ordering of sieves) independently offer significant improvements to coreference, perform an error analysis, and discuss the relationship of our work to previous models and to recent hybrid systems that have used our algorithm as a component to resolve coreference in English, Chinese, and Arabic. 2. Walking Through a Sample Coreference Resolution approach with the simple pedagogical example listed in Table 1.
 phrases (NP) and other modifier pronouns (PRP) (see Section 3.1 for details). In Table 1, this step identifies 11 different mentions and assigns them initially to distinct entities (Entity id and mention id in each step are marked by superscript and subscript).
This component also extracts mention attributes X  X or example, John :
Agirl : { gender:female, number:singular } . These mentions form the input for the following sequence of sieves.
 pronominal mentions that appear in a quotation block to the corresponding speaker.
In general, in all the coreference resolution sieves we traverse mentions left-to-right in a given document (see Section 3.2.1). The first match for this model is my merged with John 10 10 into the same entity (entity id: 9). This illustrates the advantages of our incremental approach: by assigning a higher priority to the quotation sieve, we avoid linking my 9 9 with Agirl 5 5 , a common mistake made by generic coreference models, cataphoric ones (Hobbs 1978).
 as the mention under consideration. This component resolves the tenth mention, John by linking it with John 1 1 . When searching for antecedents, we sort candidates in the same sentential clause from left to right, and we prefer sentences that are closer to the mention under consideration (see Section 3.2.2 for details). Thus, the sorted list of candidates for
John 9 10 is It 7 7 , My favorite 8 8 , My 9 9 , Agirl 5
The algorithm stops as soon as a matching antecedent is encountered. In this case, the algorithm finds John 1 1 and does not inspect a musician 2 string matching constraints than exact match (details in Section 3.3.3), but makes no change because there are no such mentions. The precise constructs sieve searches for several high-precision syntactic constructs, such as appositive relations and predicate nominatives. In this example, there are two predicate nominative relations in the first and fourth sentences, so this component clusters together John and my favorite 8 8 . 888 mentions that have the same head word with various other constraints. a new song and the song 6 6 are linked in this step.
 resolution. The three pronouns in this text, He 3 3 compatible antecedents based on their attributes, such as gender, number, and animacy.
It to entity 4, which represents an inanimate concept.
 corpus-specific rules. For example, to align our output with the OntoNotes annotation standard, we remove mentions assigned to singleton clusters (i.e., entities with a single mention in text) and links obtained through predicate nominative patterns. Note that even though we might remove some coreference links in this step, these links serve an important purpose in the algorithm flow, as they allow new features to be discovered for the corresponding entity and shared between its mentions. See Section 3.2.3 for details on feature extraction. 3. The Algorithm
We first describe our mention detection stage, then introduce the general architecture of the coreference stage, followed by a detailed examination of the coreference sieves. In describing the architecture, we will sometimes find it helpful to discuss the precision of individual components, drawn from our later experiments in Section 4. 3.1 Mention Detection
As we suggested earlier, the recall of our mention detection component is more impor-tant than its precision. This is because for the OntoNotes corpus and for many practical applications, any missed mentions are guaranteed to affect the final score by decreas-ing recall, whereas spurious mentions may not impact the overall score if they are assigned to singleton clusters, because singletons are deleted during post-processing.
Our mention detection algorithm implements this intuition via a series of simple yet broad-coverage heuristics that take advantage of syntax, named entity recognition and manually written patterns. Note that those patterns are built based on the OntoNotes annotation guideline because mention detection in general depends heavily on the annotation policy.
 entity tagset in Appendix A) that were not previously marked (i.e., they appear as modifiers in other NPs) as candidate mentions. From this set of candidates we remove the mentions that match any of the following exclusion rules: 1. We remove a mention if a larger mention with the same head word exists 2. We discard numeric entities such as percents, money, cardinals, and 3. We remove mentions with partitive or quantifier expressions (e.g., a total of 890 4. We remove pleonastic it pronouns, detected using a small set of patterns 5. We discard adjectival forms of nations or nationality acronyms (e.g., 6. We remove stop words from the following list determined by error
Note that some rules change depending on the corpus we use for evaluation. In particular, adjectival forms of nations are valid mentions in the Automated Content
Extraction (ACE) corpus (Doddington et al. 2004), thus they would not be removed when processing this corpus. 3.2 Resolution Architecture
Traditionally, coreference resolution is implemented as a quadratic problem, where potential coreference links between any two mentions in a document are consid-high-quality mention clusters, 3 and use an entity-centric model that allows the shar-ing of information across these incrementally constructed clusters. We achieve these goals by: (a) aggressively filtering the search space for which mention to consider for resolution (Section 3.2.1) and which antecedents to consider for a given men-tion (Section 3.2.2), and (b) constructing features from partially built mention clusters (Section 3.2.3). 3.2.1 Mention Selection in a Given Sieve. Recall that our model is a battery of resolution sieves applied sequentially. Thus, in each given sieve, we have partial mention clusters produced by the previous model. We exploit this information for mention selection, by considering only mentions that are currently first in textual order in their cluster. For example, given the following ordered list of mentions, { m the superscript indicates cluster id, our model will attempt to resolve only m is not resolved because it is the first mention in a text). These two are the only mentions that currently appear first in their respective clusters and have potential antecedents in the document. The motivation behind this heuristic is two-fold. First, early mentions are usually better defined than subsequent ones, which are likely to have fewer modifiers or be pronouns (Fox 1993). Because several of our models use features extracted from NP modifiers, it is important to prioritize mentions that include such information. Second, by definition, first mentions appear closer to the beginning of the document, hence there are fewer antecedent candidates to select from, and thus fewer opportunities to make a mistake.
 disable coreference for mentions appearing first in their corresponding clusters that: (a) are or start with indefinite pronouns (e.g., some , other ), (b) start with indefinite articles (e.g., a , an ), or (c) are bare plurals. One exception to (a) and (b) is the model deployed in the Exact String Match sieve, which only links mentions if their entire extents match exactly (see Section 3.3.2). This model is triggered for all nominal mentions regardless of discourse salience, because it is possible that indefinite mentions are repeated in a document when concepts are discussed but not instantiated, e.g., a sports bar in the following: 3.2.2 Antecedent Selection for a Given Mention. Given a mention m decline to propose a solution (in the hope that one of the subsequent models will solve it) or deterministically select a single best antecedent from a list of previous mentions m , ..., m i  X  1 . We sort candidate antecedents using syntactic information provided by the Stanford parser. Candidates are sorted using the following criteria: 892
The sorting of antecedent candidates is important because our algorithm stops at the first match. Thus, low-quality sorting negatively impacts the actual coreference links created.
 tion 3.3.1) and the sieve that applies appositive and predicate nominative patterns (Section 3.3.4). 3.2.3 Feature Sharing in the Entity-Centric Model. In a significant departure from previous work, each model in our framework gets (possibly incomplete) entity information for each mention from the clusters constructed by the earlier coreference models. In other words, each mention m i may already be assigned to an entity E their own cluster. We use this information to share information between same-entity mentions.
 in this section), which can be severely affected by missing attributes (which introduce precision errors because incorrect antecedents are selected due to missing information) generated due to attribute mismatch between mention and antecedent). To address this issue, we perform a union of all mention attributes (e.g., number, gender, animacy) for a given entity and share the result with all corresponding mentions. If attributes from different mentions contradict each other we maintain all variants. For example, our naive number detection assigns singular to the mention a group of students and plural to five students . When these mentions end up in the same cluster, the resulting number attributes becomes the set { singular , plural } . Thus this cluster can later be merged with both singular and plural pronouns. 3.3 Coreference Resolution Sieves
We describe next the sequence of coreference models proposed in this article. Table 2 lists all these models in the order in which they are applied. We discuss their individual contribution to the overall system later, in Section 4.4.3.
 3.3.1 Pass 1  X  Speaker Identification. This sieve matches speakers to compatible pronouns, using shallow discourse understanding to handle quotations and conversation transcripts, following the early work of Baldwin (1995, 1997). We begin by identifying speakers within text. In non-conversational text, we use a simple heuristic that searches sentences to a quotation. In conversational text, speaker information is provided in the data set.
 for [Nader] because [he] was most aligned with [my] values, X  [she] said.
 subsequent sieves: example (due to the third constraint). 3.3.2 Pass 2  X  Exact Match. This model links two mentions only if they contain exactly the same extent text, including modifiers and determiners (e.g., [the Shahab 3 ground-ground missile] and [the Shahab 3 ground-ground missile] ). As expected, this model is very precise, with a precision over 90% B 3 (see Table 8 in Section 4.4.3). 3.3.3 Pass 3  X  Relaxed String Match. This sieve considers two nominal mentions as coreferent if the strings obtained by dropping the text following their head words (such as relative clauses and PP and participial postmodifiers) are identical (e.g., [Clinton] and [Clinton, whose term ends in January] ). 3.3.4 Pass 4  X  Precise Constructs. This model links two mentions if any of the following conditions are satisfied: 894 of the overall model after adding this sieve is approximately 90%. In the OntoNotes corpus, this sieve does not enhance recall significantly, mainly because appositions and predicate nominatives are not annotated in this corpus (they are annotated in
ACE). Regardless of annotation standard, however, this sieve is important because it grows entities with high quality elements, which has a significant impact on the entity X  X  features (as discussed in Section 3.2.3). 3.3.5 Pass 5  X  Strict Head Match. Linking a mention to an antecedent based on the naive matching of their head words generates many spurious links because it completely ignores possibly incompatible modifiers (Elsner and Charniak 2010). For example, Ya l e
University and Harvard University have similar head words, but they are obviously different entities. To address this issue, this pass implements several constraints that must all be matched in order to yield a link: significantly (approximately 4.5 B 3 points). 3.3.6 Passes 6 and 7  X  Variants of Strict Head Match. Sieves 6 and 7 are different relaxations of the feature conjunction introduced in Pass 5, that is, Pass 6 removes the compatible modifiers only feature, and Pass 7 removes the word inclusion constraint. All in all, these two passes yield an improvement of 0.9 B 3 F1 points, due to recall improvements.
Table 8 in Section 4.4.3 shows that the word inclusion feature is more precise than compatible modifiers only , but the latter has better recall. 3.3.7 Pass 8  X  Proper Head Word Match. This sieve marks two mentions headed by proper nouns as coreferent if they have the same head word and satisfy the following constraints: 896 3.3.8 Pass 9  X  Relaxed Head Match. This pass relaxes the entity head match heuristic by allowing the mention head to match any word in the antecedent entity. For example, this heuristic matches the mention Sanders to an entity containing the mentions the judge , Circuit Judge N. Sanders Sauls } . To maintain high precision, this pass requires that both mention and antecedent be labeled as named entities and the types co-incide. Furthermore, this pass implements a conjunction of the given features with word inclusion and not i-within-i . This pass yields less than 0.4 point improvement in most metrics. previous coreference models focus on nominal coreference resolution. It would be incor-rect to say that our framework ignores pronominal coreference in the previous passes, however. In fact, the previous models prepare the stage for pronominal coreference by constructing precise entities with shared mention attributes. These are crucial factors for pronominal coreference.
 many decades: enforcing agreement constraints between the coreferent mentions. We use the following attributes for these constraints:
When we cannot extract an attribute, we set the corresponding value to unknown and treat it as a wildcard X  X hat is, it can match any other value. As expected, pronominal coreference resolution has a big impact on the overall score (e.g., 5 B development partition of OntoNotes). 3.4 Post Processing
This step implements several transformations required to guarantee that our out-put matches the annotation specification in the corresponding corpus. Currently this step is deployed only for the OntoNotes corpus and it contains the following two operations: 4. Experimental Results
We start this section with overall results on three corpora widely used for the evaluation of coreference resolution systems. We continue with a series of ablative experiments that analyze the contribution of each aspect of our approach and conclude with error analysis, which highlights cases currently not solved by our approach. 4.1 Corpora We used the following corpora for development and formal evaluation:
The corpora statistics are shown in Table 3. We used the first corpus (OntoNotes-Dev) for development and all others for the formal evaluation. We parsed all documents in the ACE and MUC corpora using the Stanford parser (Klein and Manning 2003) and the Stanford NER (Finkel, Grenager, and Manning 2005). We used the provided parse 898 trees and named entity labels (not gold) in the OntoNotes corpora to facilitate the com-parison with other systems. 4.2 Evaluation Metrics
We use five evaluation metrics widely used in the literature. B plementation variations in how to take system mentions into account. We followed the same implementation as used in CoNLL-2011 shared task.
 4.3 Experimental Results
Tables 4 and 5 compare the performance of our system with other state-of-the-art systems in the CoNLL-2011 shared task and the ACE and MUC corpora, respectively.
For the CoNLL-2011 shared task we report results in the closed track, which did not allow the use of external resources, and the open track, which allowed any other resources. For the closed track, the organizers provided dictionaries for gender and number information, in addition to parse trees and named entity labels (Pradhan et al. 2011). For the open track, we used the following additional resources: (a) a hand-built list of genders of first names that we created, incorporating frequent names from census lists and other sources (Vogel and Jurafsky 2012) (b) an animacy list (Ji and Lin 2009), (c) a country and state gazetteer, and (d) a demonym list. These resources were also used for the results reported in Table 5.
 its last block) we used predicted mentions (detected with the algorithm described in distinction is to facilitate comparison with previous work (all systems listed in Table 5 used gold mention boundaries).
 system generally outperforms the previous state of the art. In the CoNLL shared task, 900 our system scores 1.8 CoNLL F1 points higher than the next system in the closed track and 2.6 points higher than the second-ranked system in the open track. The Chang et al. (2011) system has marginally higher B 3 and BLANC F1 scores, but does not outperform our model on the other two metrics and the average F1 score. Table 5
ACE corpora. The model of Haghighi and Klein (2009) minimally outperforms ours by 0.6 B 3 F1 points in the MUC corpus. All in all, these results prove that our approach compares favorably with a wide range of models, which include most aspects deemed important for coreference resolution, among other things, supervised learning using using spectral clustering (Cai, Mujdricza-Maydt, and Strube 2011), and deterministic rule-based models (Haghighi and Klein 2009). We discuss in more detail the similarities and differences between our approach and previous work in Section 6.
 is a difference of only 0.5 CoNLL F1 points between our open-track and closed-track systems. We show in Section 5 that the explanation of this modest improvement is that most of the remaining errors require complex, context-sensitive semantics to be solved.
Such semantic models cannot be built with our shallow feature set that relies on simple semantic dictionaries (e.g., animacy or even hyponymy).
 affected by the performance of the coreference resolution model. For example, even if we start with a perfect set of gold mentions, if we miss all coreference relations in a text, every mention will remain as a singleton and will be removed by the OntoNotes post processing, resulting in zero mentions in the final output. Therefore, we included the score using gold mention boundaries in the last part of Table 4 ( X  X losed Track  X  gold boundaries X ) to isolate the performance of the coreference resolution component. This experiment shows that our system outperforms the others with a considerable margin, demonstrating that our coreference resolution model, rather than the mention detection component, is the one responsible for the overall performance. 4.4 Analysis
In this section, we present a series of analytic and ablative experiments that demonstrate that both aspects of our algorithm (the entity-centric approach and the multi-pass model with precision-ordered sieves) independently offer significant improvements to coreference. We also analyze the contribution of each proposed sieve and of the features deployed in our model. We conclude with an experiment that measures the performance drop as we move from an oracle system that uses gold information for mention boundaries, syntactic analysis, and named entity labels, to the actual system where all this information is predicted. For all the experiments reported here we used the OntoNotes-Dev corpus. centric approach, which enables the sharing of features between mentions assigned to pair model where this sharing is disabled. That is, when two mentions are com-pared, this model uses only the features that were extracted from the corresponding textual extents. The table shows that feature sharing has a considerable impact on all evaluation metrics, with an overall contribution of approximately 3.4 CoNLL F1 points. This is further proof that an entity-centric approach is beneficial for coreference resolution.
 is taken if feature sharing is disabled:
In the example text, the mention-pair model incorrectly links This and It , because all the features that can be extracted locally are compatible (e.g., number is singular for both pronouns). On the other hand, the entity-centric model avoids this decision because, in a previous sieve driven by predicate nominative relations, these pronouns are each 902 linked to incompatible noun phrases, i.e., the best result of a Chinese gymnast and the best result for Greek gymnasts . 4.4.2 Impact of the Multi-Pass Model. Table 7 shows the contribution of our multi-pass model. We compare this model with a single-pass baseline, which uses the same sieves mention under consideration, we select the first antecedent that matches any of the available sieves. This experiment shows that our multi-pass model, which sorts and deploys sieves using precision-based ordering, yields improvements across the board, with more than 6 CoNLL F1 points overall improvement.
 the higher the quality of mention clusters built in the previous sieves, the better the features extracted from these clusters will be in the current sieve X  X nd, of course, better features drive better clustering decisions in the next sieve, and so on. This incremental process is highlighted in the given example: Because the sieve based on predicate nomi-native patterns runs before pronominal coreference resolution, the two pronouns under consideration have additional, high-quality features that stops the incorrect clustering decision. 4.4.3 Contribution of Individual Sieves. Table 8 lists the performance of our system as ten sieves are incrementally added. This table illustrates our tuning process, which allowed us to deploy the sieves in descending order of their precision. With respect to individual contributions, this analysis highlights three significant performance increases. The first is caused by Sieve 2, exact string match. This sieve accounts for approximately 16
CoNLL F1 points improvement, which proves that a significant percentage of mentions in text are indeed repetitions of previously seen concepts. The second big jump in performance, almost 3 CoNLL F1 points, is caused by Sieve 5, strict head match, which is the first pass that compares individual headwords. These results are consistent with error analyses from earlier work which have shown the importance of string match in general (Zhou and Su 2004; Bengtson and Roth 2008; and Recasens, Can, and Jurafsky 2013) and the high precision of strict head match (Recasens and Hovy 2010).
 mately 9.5 CoNLL F1 points improvement. Thus it would be possible to build an even best model (based on the CoNLL score). This suggests that what is most important for coreference resolution, at least relative to today X  X  state of the art, is not necessarily the clustering decision mechanism, but rather the entire architecture behind it, and in particular the use of cautious decision-making based on high precision information, entity-centric modeling, and so forth. 4.4.4 Contribution of Feature Groups. Table 9 lists the results of an ablative experiment where each feature group was individually removed from the complete model. When a feature is eliminated, two mentions under consideration are always considered compat-ible with respect to that feature. For example, singular and plural mentions are number compatible when the number feature is removed.

This feature alone is responsible for 2.6 CoNLL F1 points. Removing this feature has a considerable negative impact on the pronoun resolution sieve, which makes a consid-erable number of errors without it (e.g., linking our and Jiaju Hou ). The second most relevant feature is animacy, with an overall contribution of 1 CoNLL F1 point. Animacy helps disambiguate clustering decisions where the two mentions under consideration are otherwise number and gender compatible. For example, animacy enables the linking of firms from Taiwan and they , and avoids the linking of 17 year and she . Lastly, the NE and gender features contribute 0.5 and 0.4 F1 points, respectively. This relatively minor contribution is caused by the overlap with the other features (e.g., many errors corrected by using NE information are corrected also by a combination of animacy and number).
Nevertheless, these features are still useful. For example, the NE feature covers many mentions that do not exist in our animacy dictionaries, which helps in several decisions, e.g., avoiding linking it and Saddam Hussein .
  X   X   X   X  904 the performance penalty suffered when using predicted information as input in our system (a realistic scenario) versus using gold information. We consider both linguistic boundaries. Table 10 shows the results when various inputs were replaced with gold information.

This is to be expected, because we use a constituent parser for mention identification, mention traversal, and for some of the sieves (e.g., the precise constructs model). All in all, if all linguistic information is replaced with gold annotations, the performance of the system increases by 1.6 CoNLL F1 points, or 2.7% relative improvement. We consider this relatively small difference a success story for the quality of natural language pro-cessors, especially considering our heavy reliance on such tools throughout the entire system. On the other hand, the difference between our actual system and the oracle system with gold mentions is 14.4 F1 points. This is because the gold mentions include the anaphoricity information, detection of which is already a hard task by itself. 4.4.6 Automatic Ordering. The ordering of our sieves was determined using linguistic intuition about how precise each sieve is (for example exact match is clearly more precise than partial match). We also supplemented this intuition, early on in our design process, by measuring the actual precision of some of the sieves on a development set from ACE.
 circumstances in the OntoNotes corpus, we performed a study to see if an automatically learned ordering for sieves could result in superior performance.
 pass. We tuned the ordering on OntoNotes-Train data, and evaluated this comparison on the OntoNotes-Dev set.
 ordering to our hand-built order: Hand Ordered: Learned Ordering: precise constructs sieves, which is easily explained by the fact that OntoNotes does not annotate appositive or predicate nominative relations.
 precise they are does remarkably well at choosing an ordering, despite the fact that the ordering was originally designed for ACE, a completely different corpus. 5. Error Analysis
To understand the errors in the system, we analyzed and categorized them into five distinct groups. The distribution of the errors is given in Table 11, with specific examples for each category given in Table 12. For this analysis, we inspected 115 precision and recall errors.

Semantics, discourse. Whereas simple examples can be solved by using shallow semantics such as knowledge about the semantic compatibility of headwords (e.g., McCain  X  course. For example, to know that the thrift and his property are coreferent, we need to understand the context and that both the thrift and his property are being seized, involving relations not only between the coreferent words, but also between other parts of the sentence as well.

Pronominal resolution errors. Our pronominal resolution algorithm includes several strong heuristics that model the matching of attributes (e.g., gender, number, animacy), the position of mentions in discourse (e.g., we model only the first mention in text for a given entity), or the distance between pronouns and antecedents. This is still far from language understanding, however. Table 12 shows that our approach often generates incorrect links when it finds other compatible antecedents that appear closer, according to our antecedent ordering, to the pronoun under consideration. In the example shown in the table, the land is selected as the antecedent for the pronoun its , because the land appears earlier than the correct antecedent, the ANC , in the sentence. Implementing a richer model of pronominal anaphora using syntactic and discourse information is an important next step.

Non-referential mentions. The third significant cause of errors is due to non-referential mentions such as pleonastic it or generic mentions. Our mention detection model generate precision errors. For example, in Table 12, the pronoun you is generic, but our system incorrectly links them. The large number of these errors suggests the need to 906 add more sophisticated anaphoricity detection to our system (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009).

Event mentions. Our system was tailored for the resolution of entity coreference and does not have any event-specific features, such as, for example, matching event partici-pants. Furthermore, our model considers only noun phrases as antecedent candidates, thus missing all mentions that are verbal phrases. Therefore, our system misses most coreference links between event mentions. For example, in Table 12 the pronoun That is coreferent with the event mention Support . Our system fails to detect the latter event mention and, as a consequence, incorrectly links That to the regime .

Miscellaneous. There are several other reasons for errors, including inconsistent annota-tions, parse or NER errors, and incorrect processing of enumerations. For example, the possessive (  X  X  ) is annotated inconsistently in several cases: sometimes it is included in the possessor mention in the gold mention annotation, but sometimes it is not. This will penalize the final score twice (once for recall due to the missed mention and once for precision due to the incorrectly detected mention).
 trees. NER errors can result in incorrect pronoun resolution due to incorrect attributes.
Parser errors are responsible for many additional coreference resolution errors. First, in-correct syntactic attachments lead to incorrect mention boundaries, which are penalized by our strict scorer. Second, parser errors often lead to the selection of an incorrect head word for a given constituent, which influences many of our sieves. Thirdly, because our parser does not always distinguish between coordinated nominal phrases and appo-sitions, our system sometimes takes an entire coordinated phrase as a single mention, leading to a series of mention errors. For example, the last example in the table shows ( Shenzhen, Zhuhai, Shantou, Xiamen, and Hainan ) as a single enumeration. Second, our system believed that Zhuhai, Shantou, Xiamen is an appositive phrase and kept it as a single mention, rather than separate it into three distinct mentions.
 assign content words as head words of syntactic constituents, we take the head word of the first noun phrase in the enumeration to be the head word of the coordinated nominal phrase (Kuebler, McDonald, and Nivre 2009; de Marneffe and Manning 2008). Because of this, the coordinated phrase is often linked to another mention of the first element in the enumeration. For example, our system marks Zhuhai, Shantou, Xiamen as a unique mention and incorrectly links it to Zhuhai , because they have the same headword. 6. Comparison with Previous Work
Algorithms for coreference (or just pronominal anaphora) include rule-based systems (Hobbs 1978; Brennan, Friedman, and Pollard 1987; Lappin and Leass 1994; Baldwin 1995; Zhou and Su 2004; Haghighi and Klein 2009, inter alia), supervised systems (Connolly, Burger, and Day 1994; McCarthy and Lehnert 1995; Kehler 1997; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Rahman and Ng 2009, inter alia), and unsupervised approaches (Cardie and Wagstaff 1999; Haghighi and Klein 2007; Ng 2008; Kobdani et al. 2011a). Our deterministic system draws from all of these, but specifically from three strands in the literature that cross-cut this classification.
 precision constraints was first proposed for pronominal anaphora in Baldwin X  X  (1995) important but undercited dissertation. Baldwin suggested using seven high-precision for example, resolved pronouns whose antecedents were unique in the discourse, and another resolved pronouns in quoted speech. Baldwin X  X  idea of starting with high-precision knowledge was adopted by later researchers, such as Ng and Cardie (2002b), who trained to the highest-confidence rather than nearest antecedent, or Haghighi and
Klein (2009), who began with syntactic constraints (which tend to be higher-precision) before applying semantic constraints. This general idea is known by different names in 908 many NLP applications: Brown et al. (1993) used simple models as  X  X tepping stones X  for more complex word alignment models; Collins and Singer (1999) used  X  X autious X  decision list learning for named entity classification; Borghesi and Favareto (1982) and recognition, and Spitkovsky et al. (2010) used  X  X aby steps X  for unsupervised depen-dency parsing, and so forth. Our work extends the intuition of Baldwin and others pronominal coreference) and shows that it can result in extremely high-performing resolution when combined with global inference.
 and Klein (2009), both of which extended Baldwin X  X  approach to generic nominal coref-erence. Zhou and Su proposed a multi-agent model that triggers a different agent with a specific set of deterministic constraints for each anaphor depending on its type and context (e.g., there are different constraints for noun phrases in appositive constructs, definite noun phrases, or bare noun phrases). Some of the constraints X  parameters (e.g., size of candidate search space for a given anaphor type) are learned from training data. The authors showed that this model outperforms the state of the art on the MUC-6 and
MUC-7 domains. To our knowledge, Zhou and Su X  X  approach is the first work to demon-strate that a deterministic approach obtains state-of-the-art results for both nominal and pronominal coreference resolution. Our approach extends Zhou and Su X  X  model in two significant ways. First, Zhou and Su solve the coreference task in a single pass over the text. We show that a multi-pass approach, which applies a series of sieves incrementally from highest to lowest precision, performs considerably better (see Table 7). Second,
Zhou and Su X  X  model follows a mention-pair approach, where coreference decisions are taken based only on information extracted from the two mentions under consideration.
We demonstrate that an entity-centric approach, which allows features to be shared between mentions of the same entity, outperforms the mention-pair model (see Table 6). proved that deterministic rules could achieve state-of-the-art performance. Haghighi assign possible coreference. The second, transductive pass identifies Wikipedia arti-cles relevant to the entity mentions in the test set, and then bootstraps a database of hyponyms and other semantically related head pairs from known syntactic patterns for apposition and predicate-nominatives. Haghighi and Klein found that this transductive learning was essential for semantic knowledge to be useful (Aria Haghighi, personal communication); other researchers have found that semantic knowledge derived from
Web resources can be quite noisy (Uryupina et al. 2011a). But although transductive learning (learning using test set mentions) thus offers advantages in precision, running a Web-based bootstrapping learner whenever a new data set is encountered is not practical and, ultimately, reduces the usability of this NLP component. Our system thus offers the deterministic simplicity and high performance of the Haghighi and Klein (2009) system without the need for gold mention labels or test-time learning.
Furthermore, our work extends the multi-pass model to ten passes and shows that this approach can be naturally combined with an entity-centric model for better results. Marcu 2005; Denis and Baldridge 2007; Haghighi and Klein 2007; Culotta et al. 2007;
Poon and Domingos 2008; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011) rather than the classic method of simply aggregrating local decisions about pairs of mentions. Like these systems, our model adopts the entity-mention model (Morton 2000; Luo et al. 2004; Yang et al. 2008; Ng 2010) over not just pairs of mentions but over entire clusters of mentions defining an entity.
Previous systems do this by encoding constraints using rich probabilistic models and complex global inference algorithms. By contrast, global reasoning is implemented in our system just by allowing the rules in each stage to reason about any features of a cluster from a previous stage, including attributes like gender and number as well as headword information derived from the first (most informative) mention. Because our system begins with high-precision clusters, accurate information naturally propagates to later stages. 7. Other Systems Incorporating this Algorithm
A number of recent systems have incorporated our algorithm as an important com-ponent in resolving coreference. For example, the CoNLL-2012 shared task focused on coreference resolution in a multi-lingual setting: English, Chinese, and Arabic (Pradhan et al. 2012). Forty percent of the systems in the shared task (6 of the 15 systems) made use of our sieve architecture (Chen and Ng 2012; Fernandes, dos Santos, and Milidiu 2012;
Shou and Zhao 2012; Xiong and Liu 2012; Yuan et al. 2012; Zhang, Wu, and Zhao 2012), (Fernandes, dos Santos, and Milidiu 2012; Chen and Ng 2012).
 score over all languages, and the best score for English and Arabic, by implement-ing a stacking of two models. Our sieve-based approach was first used to generate mention-link candidates, which are then reranked by a supervised model inspired from dependency parsing. This result demonstrates that our deterministic approach can be naturally combined with more-complex supervised models for further performance gains.
 observation that most sieves in our model are minimally lexicalized so they can be easily adapted to other languages. Their coreference model for Chinese incorporated our English sieves with only four modifications, only two of which were related to the differences between Chinese and English: The precise constructs sieve was extended to add patterns for Chinese name abbreviations, and the relaxed head-match sieve was removed, because Chinese tends not to have post-nominal modifiers.
Ng (2012) then added a second component which first linked mentions with high string-pair or head-pair probabilities before running the sieve architecture. The strong performance of our English sieve system on Chinese with only this small number of changes speaks to the multi-lingual strength of our approach.
 resolution. Our recent work (Lee et al. 2012) showed that an iterative method that cautiously constructs clusters of entity and event mentions, using linear regression to model cluster merge operations, allows information flow between entity and event coreference. 910
Stoyanov and Eisner (2012) also adopts this intuition. Their system greedily merges classifications ( X  X asier decisions X ) to guide harder decisions later.
 brid machine learning systems, either as a first pass in generating candidate links which are then incorporated in a probabilistic system, or as a second pass for generating links after high-probability mention-pairs have already been linked. These hybrid systems are the state-of-the-art in English, Chinese, and Arabic coreference resolution. Further, our algorithm can be extended to other tasks, for example, event coreference resolution. 8. Conclusion
We have presented a simple deterministic approach to coreference resolution that incorporates document-level information, which is typically exploited only by more complex, joint learning models. Our approach exploits document-level information through an entity-centric model, which allows features to be shared across mentions deterministic coreference models one at a time from highest to lowest precision, where approach outperforms or performs comparably to the state of the art on several corpora. models can be inserted in the system with limited understanding of the other features already deployed. Our code is publicly released 10 and can be used both as a stand-alone coreference system and as a platform for the development of future systems.
 a component in hybrid systems, and that of other recent rule-based systems in named entity recognition (Chiticariu et al. 2010) suggests that rule-based systems are still an important tool for modern natural language processing. Our results further suggest that precision-ordered sieves may be an important way to structure rule based systems, and suggests the use of sieves in other NLP tasks for which a variety of very high-precision features can be designed and non-local features can be shared. Likely candidates include relation and event extraction, template slot filling, and author name deduplication. proved, including better performance on pronouns. More sophisticated anaphoricity detection, drawing on the extensive literature in this area, could also help (Vieira and
Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). are due to shallow knowledge of semantics and discourse. This result points to the crucial need for more sophisticated methods of incorporating semantic and discourse knowledge. Unsupervised or semi-supervised approaches to semantics such as Yang and Su (2007), Kobdani et al. (2011b), Uryupina et al. (2011b), Bansal and Klein (2012), or Recasens, Can, and Jurafsky (2013) may point the way forward. Although sieve-based architectures are at the modern state of the art, it is only by incorporating these more powerful models of meaning that we can eventually deal with the full complexity and richness of coreference. Appendix A: The OntoNotes Named Entity Tag Set Appendix B: Set of Patterns for Detecting Pleonastic it Acknowledgments 912 914
