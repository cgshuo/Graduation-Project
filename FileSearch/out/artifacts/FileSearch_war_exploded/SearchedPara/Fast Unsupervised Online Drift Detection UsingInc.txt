 Data stream research has grown rapidly over the last decade. Two major features distinguish data stream from batch learn-ing: stream data are generated on the fly, possibly in a fast and variable rate; and the underlying data distribution can be non-stationary, leading to a phenomenon known as con-cept drift. Therefore, most of the research on data stream classification focuses on proposing efficient models that can adapt to concept drifts and maintain a stable performance over time. However, specifically for the classification task, the majority of such methods rely on the instantaneous avail-ability of true labels for all already classified instances. This is a strong assumption that is rarely fulfilled in practical applications. Hence there is a clear need for efficient meth-ods that can detect concept drifts in an unsupervised way. One possibility is the well-known Kolmogorov-Smirnov test, a statistical hypothesis test that checks whether two sam-ples differ. This work has two main contributions. The first one is the Incremental Kolmogorov-Smirnov algorithm that allows performing the Kolmogorov-Smirnov hypothesis test instantly using two samples that change over time, where the change is an insertion and/or removal of an observation. Our algorithm employs a randomized tree and is able to per-form the insertion and removal operations in O (log N ) with high probability and calculate the Kolmogorov-Smirnov test in O (1), where N is the number of sample observations. This is a significant speed-up compared to the O ( N log N ) cost of the non-incremental implementation. The second contribu-tion is the use of the Incremental Kolmogorov-Smirnov test to detect concept drifts without true labels. Classification algorithms adapted to use the test rely on a limited portion of those labels just to update the classification model after a concept drift is detected.
 Kolmogorov-Smirnov; Data Stream; Concept Drift; Carte-sian Tree; Treap; Lazy Propagation
In the last decade, there was a tremendous increase of in-terest in algorithms that can learn from data streams. Data streams are fast and potentially infinite sequences of data records in which the underlying distribution can change over time. Therefore, data stream mining requires efficient algo-rithms in terms of memory and processing time to detect and adapt to concept changes, also known as concept drifts.
Such an increase of interest has lead to the proposal of a large number of learning algorithms for diverse tasks such as classification, clustering and anomaly detection [14, 15, 18]. In particular, for classification, these proposals have been mostly evaluated and compared assuming that true labels are readily available as soon as predictions are issued [14, 7, 24, 5, 6]. The availability of all true labels seems unfeasible for most applications, since it may involve annotating data by expensive means in terms of costs and labor time, such as a hired domain expert. Therefore, minimizing the amount of necessary true labels and delaying their requisition are desirable perks for practical solutions.
 The first main contribution of this work is the Incremental Kolmogorov-Smirnov (IKS) algorithm. Kolmogorov-Smirnov (KS) is a non-parametric hypothesis test that is used to check whether two samples originate from the same distri-bution. Applying KS on a pair of samples takes O ( N log N ) time, where N is the total number of sample observations. This performance poses a limitation on data stream algo-rithms that require the test to be executed repeatedly with always-growing samples or with sliding windows. IKS, on the other hand, produces the exact same results as KS while enabling the samples to change over time at a cost of O (log N ) for insertion/removal of observations and O (1) for comput-ing the p -value for the test. This contribution may be use-ful across a wide variety of applications yet to come in data stream research and practice.
 The second main contribution is a direct application of IKS in data stream classification. We use IKS to identify detectable concept drifts online, using only data of the fea-ture space. Thus, the detection is affected neither by delayed true labeling nor by label scarcity. Additionally, it fits any base classifier. We propose actions that could be taken to update the classification model once a change is detected, using only a limited amount of data. We show that the overall performance, in terms of accuracy, is only slightly affected, while the number of true labels that are required to keep the model updated decreases considerably.

This paper is organized as follows. Section 2 presents an overview of the literature; Section 3 details the IKS algo-rithm; Section 4 presents our proposals regarding drift de-tection; Section 5 explains our experimental setup; Section 6 shows our experimental results; finally, Section 7 presents our conclusions and directions for future work.
Verification latency, or delay, is the period between the availability of an unlabeled (test) instance and the availabil-ity of its true label. Such period of time depends on the application domain. For instance, in applications for pre-dicting the tendency of a stock price or electrical demand, the verification latency is the forecasting window, i.e., the amount of time ahead of the prediction. In this case, the verification latency is fixed for all predictions. In other ap-plications, verification latency can be variable, as in the case of sensors that classify events originated by external condi-tions such as the environment [4].

The Null-latency scenarios are rare in practice, although they are common in research benchmark evaluation. They are rare because most of the real-world applications require a certain amount of time between prediction and actual event occurrence. Such time is used for taking the required actions for the prediction. For instance, in the electrical demand application, a reasonable period of time, say 30 minutes, is necessary for executing a plan to adapt the energy genera-tion to the predicted demand.

Different papers address the issue of availability of label information by considering different settings. One setting is known as extreme verification latency and considers that no true labels are available after a fixed time of the data stream, while all true labels previous to this point are known [26, 12]. Another setting assumes that only a portion of true labels becomes available along the time. In particular, for semi-supervised learning, it is common to assume that a small part of the labels becomes available with null verification latency, while the majority has infinite (extreme) verification latency [22].

Similarly to the semi-supervised setting, active learning also assumes that just part of the data is labeled. The crit-ical difference between these setting is that in the active learning setting, the algorithm can choose which examples will be labeled by an oracle. Our proposal is, in essence, an active learning algorithm. The labels are required to learn the classification model, as expected for a supervised task. The stream is monitored in an unsupervised way and no ad-ditional labels are required if no concept drift is detected. In the case of a concept drift, the algorithm asks for a set of labeled data so the model can be updated.

In the literature, the most related work to ours was re-cently proposed by Zliobaite [28]. She showed that some types of concept drifts become undetectable when no true labels are provided. She proposed a method to identify de-tectable drifts without label information. Two consecutive, non-intersecting, sliding windows of equal size slide through the stream. The windows can contain either information from the feature space, or information from the classifier X  X  output. At each time, i.e. one instance, a hypothesis test is performed to check whether the sliding windows have ob-servations generated by the same distribution. If this is not the case, a drift is detected.
 In the case of using information from the feature space, Zliobaite suggests using entire examples as multivariate ran-dom variables, or mapping them to univariate variables. However, no experiments were performed on either setting. In the case of use of information from the classifier X  X  output, Zliobaite suggests using the estimated probabilities of the labeled instances, if the classifier is capable of offering such information, or only the labels, otherwise.

Three hypothesis tests were applied in the experimental setup. Among them, the use of the standard Kolmogorov-Smirnov test [21] was advocated since it is non-parametric and computationally efficient  X  when compared to the other options. The non-parametric tests suit better the data stream mixture of distributions due to existence of different classes and also regions of concept transition. While [28] offers re-sults regarding drift detection, no results are reported re-garding the impact in classification in terms of accuracy or in number of required labels.

Haque et al. [17] proposed an algorithm that does not use labels to identify the optimal size of a dynamically sized sliding window that is supposed to contain instances belong-ing to the same concept. Once a drift is detected and the window is shrank in order to keep only the instances of the newest concept, the algorithm assumes the instantaneous availability of 99 . 7% of the true labels.

Masud et al. [22] presented an algorithm that is both capa-ble of dealing with delayed labeling and identifying novelty, i.e. , new classes during the stream. However, apart from the specific assumptions regarding the feature space, it also as-sumes that, once an instance is observed, the classifier puts it aside for a prolonged period of time before needing to pro-vide a classification, creating a slightly different scenario for the problem of delayed labeling.

Although Kuncheva et al. [20] and Amir and Toshniwal [2] introduced methods to directly tackle delayed labels, they target problems with stationary distributions, i.e. , problems without concept drift. Kuncheva proposes different varia-tions of the Nearest Neighbor Classifier (NNC) for online learning. In all of them, the reference set grows incremen-tally over time, with unlabeled instances being added when they are classified with low confidence. In this case, the predicted labels are employed as if they were correct, until they are revised later, when the true labels become avail-able. Amir X  X  proposal is a follow-up of Kuncheva X  X  work. The extension is the use of emerging patterns in order to se-lect which instances should be added to the NNC X  X  reference set.

Other papers have dealt with the problem of extreme ver-ification latency, i.e. , no labels are available during the clas-sification phase of the data stream. Extreme verification latency methods require only an initial amount of labeled data, that must be located at the beginning of the stream and, then, no labeled data is requested anymore.

Dyer et al. [12] and Souza et al. [26] aim at addressing exclusively the infinite delay problem. Both of them make assumptions regarding the shape of the data in the feature space and assume an incremental nature of the drifts. Dyer X  X  proposal applies semi-supervised classification on batches of unlabeled data, considering the predicted labels of the previ-ously classified instances as correct. Souza X  X  approach clus-ters the data from time to time and checks for spatial sim-ilarity between the clusters to assume a movement of the data.

The literature has also dealt with the problem of label scarcity and delayed labeling using semi-supervised learning.
Pozzolo [8] directly tackled verification delay in the credit card fraud detection task. The proposal assumes that, while a greater portion of the data is affected by delay, the true la-bels are instantly available for a much smaller portion. The results suggest that combining a model built on the instan-taneously labeled data and a different model built on labeled data that was affected by delay provides better results than using only one model induced upon all the data.

Masud et al. [23] addressed a problem where only a portion of the true labels is delivered. The labeled instances, among a supposed larger number of unlabeled instances, are used in a semi-supervised approach to classify future unlabeled data. The main difference of this approach is that, although only few true labels become available, they are provided without delay.

Wu et al. [27] introduced another semi-supervised approach to deal with scarce labeled data in streams. It is based on a decision tree that grows incrementally and holds clusters in its leaves to detect concept drifts.
In this section, we first briefly review the standard Kolmo-gorov-Smirnov test [21] and how to apply it. Later, we in-troduce the required operations for the incremental version of the test.
Suppose we have two samples A and B containing univari-ate observations. We would like to know, with a significance the level of  X  , whether we can reject the null hypothesis that the observations in A and B originate from the same probability distribution.

If no information is available regarding the data distribu-tion, but it is safe to assume that the drawn observations are i.i.d., we can use the rank-based Kolmogorov-Smirnov (KS) test to verify the proposed hypothesis. According to it, we can reject the null hypothesis at level  X  if the following inequality is satisfied: where the value of c (  X  ) can be retrieved from a known table, n is the number of observations in A and m is the number of observations in B . The right side of the inequality is the target p -value. D is the Kolmogorov-Smirnov statistic, i.e. , the obtained p -value, and is defined as follows: where We note that D can actually be computed as follows:
The incremental variant assumes that A and B can change over time. Precisely, we assume an abstract data type (ADT) with the following operations: a ) insert new observation into A ; b ) insert new observation into B ; c ) remove observation from A ; d ) remove observation from B ; e ) apply KS test.
In the next section we introduce an algorithm that allows us to perform the first four operations in logarithmic time with high probability according to the total number of ob-servations, and the last operation in constant time.
In this section, we introduce our proposal of an algorithm to fast compute the operations of the incremental version of the KS test. Before that, we need to clarify two points regarding our approach: 1. We created a distinction between | A | and n and, equiv-2. We will assume that we are interested in effectively With the constraint that | A | = r | B | , we can rewrite D as where F 0 A is defined as a sum of ones and F 0 B is now defined as a sum of r  X  X  have that
Let us assume the existence of an array in which we have all the observations o i  X  A  X  B sorted so that o i  X  o i +1 Table 1: G ( x ) for each observation x  X  A  X  B . The table represents a sorted array so that o i  X  o i +1 for each observation, we also have a corresponding value g = G ( o i ). Table 1 illustrates such a data structure.
Let us also assume that, when inserting a new observation o j into the structure, we obey the following order o o  X  o j +1 . In other words, all older observations with the same or higher value are kept on the right side of the new observation, and consequently have higher indexes. As a result, we note that g j = g j  X  1 + v , where v = 1 if o v =  X  r if o j  X  B . In addition, after the insertion, all g i &lt; j remain the same, while all g i with i &gt; j are increased by v . Similarly, the removal of such an observation decreases all g i with i &gt; j by v .

We can insert/remove a new observation o j into/from the structure, virtually add/subtract a constant value to/from all g i with i &gt; j in O (log | A | + | B | ) with high probability. We can also compute the maximum and the minimum val-ues of g i in O (1) using a randomized tree called Treap (or Cartesian Tree ) [3, 25] with bulk operation and lazy propa-gation . Therefore, it is possible to exactly compute D when-ever | A | = r | B | in O (1) and incrementally modify | A | or | B | with expected complexity of O (log | A | + | B | ).
Appendix A is dedicated to explain the Treap with lazy propagation . The next section explains how it is employed in the Incremental Kolmogorov-Smirnov algorithm. A full implementation of the Incremental Kolmogorov-Smirnov is freely available as supplementary material [11]. We implement the structure presented in Table 1 as a Treap with bulk operations (minimum, maximum and in-crease by) and lazy propagation. The values are the g i , the BST keys are the observations from a feature in the data stream and the priorities are random values. The final so-lution follows from Equation 1. The following algorithm describes the implementation of the five operations that we need in the Incremental Kolmogorov Smirnov: inserting ob-servation into A , inserting observation into B , removing ob-servation from A , removing observation from B and applying the KS test. The last operation requires | A | = r | B | . 16: 22: 29: 35:
The only constraint in the Incremental Kolmogorov-Smir-nov test over its original version is that | A | = r | B | , where r is a parameter that remains constant across the stream. Although it seems to be a restrictive limitation, in practice it is not.

If both samples have fixed size and the changes are only due to replacement of observations, i.e. , pairs of insertions and removals, the constraint is fulfilled regardless the size of the samples. In addition, if both samples always have the same size, r = 1 and the constraint is fulfilled no matter the size of the samples.

Otherwise, we suggest that r should be chosen as 1 by de-fault and, in case of disparity among | A | and | B | , resampling should be performed. In such case, the m and n should be used to compute the target p -value.

We note that, for instance, if m = 2 n and we insert all observations from A twice, so that | A | = | B | , both D and target p -value are exactly the same as the obtained by per-forming the standard version of Kolmogorov-Smirnov. In fact, if A has fixed size while B grows indefinitely, it is pos-sible to keep exact equivalence between the Incremental KS and the Standard KS every time that m = kn,k  X  N . Differ-ences in the Incremental KS and the Standard KS between consecutive values of k also become increasingly smaller.
This section explains how we applied the IKS algorithm in order to detect concept drift without the true label infor-mation and adapt the classification models.

For each feature of the feature space, we keep two samples of the same size W . The first of them, called reference set , stores the attribute values of the instances that were used to induce the classification model. Therefore, this sample is fixed. The second sample, called current set , is a sliding window that stores the attribute values of the last W in-stances of the stream. After each instance is classified, for each feature, the current set is updated and a KS test is performed between the current set and the reference set. If the KS rejects the null hypothesis from both samples coming from the same distribution, a drift is detected.

When a classifier outputs probability estimates, we can also perform the test as described with some minor changes. The current set contains probability estimates for the clas-sification labels that were output by the classifier (acting as confidence levels). The reference set contains probability es-timates for the labels obtained in a leave one out procedure with the training data.

Our proposal differs from [28] in two ways. First, we opted to keep one of the windows fixed, rather than maintaining two consecutive sliding windows. The rationale behind this decision is straightforward: the need of detecting drifts is ac-knowledging that the classification model is outdated. Thus, it is more practical to simply use the data that are in the training set of the classification model rather than assuming it is outdated indirectly.

The second difference is that, when we use the attribute space to detect drift, we apply the test for each attribute individually rather than using a multivariate test or a map function to transform the examples into a single univariate value. Both options would require larger samples and, as we note, a change in a single attribute already is a concept drift. The downside of our decision is that if a concept drift in a specific attribute is undetectable [28], it could possibly be detected when applying a multivariate test.

We propose three reactions for the system to perform once a drift is detected. They follow:
The next section explains how we evaluated our approaches.
Our experimental setup has two parts. In the first one, we evaluate the Incremental Kolmogorov-Smirnov X  X  perfor-mance in seconds against the successive application of the standard KS and a slightly optimized version for data streams. This particular version, called Optimized KS, computes the KS statistic in linear time for consecutive samples that dif-fer due the increment or replacement of one observation, by performing linear sorted insertion/remotion of observations and not re-sorting the sample later. For this purpose, we ran two experiments. First, we tested the performance for always growing samples. The second test evaluates the time necessary to perform the KS test using two fixed size slid-ing windows. All the experiments were run in an i7 4790k @3 . 6Ghz, 16GB DDR3 RAM @1600Mhz.

In the second part, we evaluate our proposals for drift detection and adaptation in a classification context. We measure the accuracy and number of required true labels on different datasets. Since the classes are balanced, the accuracy is a good indicator of prediction quality. We compare the results with two baselines and a topline. The first baseline (BL1) is a classifier that never adapts to drift. The second (BL2) is a classifier that randomly adapts to drift by retraining the classification model at random mo-ments. The number of adaptations of BL2 is the same as of MR. Due to its random nature, we averaged the results of BL2 in 100 runs. The topline is a classifier always re-trained upon the last W instances, where W is the size of the sliding window. The initial training size is also W , for all methods. Additionally, we compare our proposal with a classifier that detects drift using one of the methods pro-posed by Zliobaite [28]  X  specifically, two consecutive sliding windows with probability estimates. From now on, we will refer to Model Replacement with this method of detection as Consecutive Sliding Windows Detection (CD) .
 The classifiers in our experiments are Nearest Neighbor, Decision Tree and Naive Bayes. Baselines 1 and 2 (BL1 and BL2), Topline (TL) and Model Replacement (MR) re-sults are computed for all of them. Particularly, we apply MR with Na  X   X ve Bayes for probability estimates (MRP) and feature space (MR), since Na  X   X ve Bayes is the only accessed classifier that directly provides probability estimates. 1NN is also tested with  X / X  transformation, DT is also tested with Adaptree and CD is also tested with Na  X   X ve Bayes. For the methods that use decision trees, we only detect drifts of features that are decision nodes inside the tree.
 All pairwise comparisons of the second part are based on Wilcoxon Signed Rank Test for statistical validity with a significance level of 0 . 05. The following section lists the datasets that were used in our experiments.
We used 6 real datasets in our experiments. Three of them have artificially introduced drift. We explain each of the datasets below. (A) Arabic [16] contains audio features of 88 people pro-(B) Posture [19] contains data from a sensor that is car-(C) Bike [13] contains hourly count of rental bikes between (D) Keystroke [26] contains features from five people typ-(E) Insects [9] contains features from a laser sensor. The (F) Abrupt Insects is a modified version of the Insects
The exact datasets that were used in our experiments are freely available to the community [11].

All algorithms apply KS with a significance level of 0 . 001 to detect concept drift. The reasoning behind this decision is that, with a higher significance, we can avoid detecting drifts during periods when the sliding window is still transitioning between two concepts. The size of the sliding windows is 100 instances for all datasets but Arabic and Posture. As they are bigger and have a greater number of classes, we opted for sliding windows of 500 instances for them. Next section presents our results and analysis. We have split the experimental results into two parts. We first tackle only the time efficiency of the Incremental Kolmogorov-Smirnov. Later, we show our results for the data stream classification. In the first experiment, we evaluate the performance of IKS test for always growing samples. The two samples, ini-tially with 1 observation, increase their size by one at each time step and then the KS test is performed. We increased the size of the samples up to 5000 observations. The re-ported results are the average of 30 repetitions and the val-ues are generated at random.

Figure 1 shows the accumulated time by the samples X  size throughout the complete experiment. Although the confi-dence intervals were plotted, they are so tight that they have became indistinguishable from their corresponding curves. For instance, for a sample that grows up to 5000 observa-tions, Incremental KS took an average of 0 . 0491 seconds to complete the experiment. Optimized KS took 1 . 0297 sec-onds and Standard KS took 12 . 7645 seconds on average. In Accumulated time (seconds) Figure 1: Averaged accumulated time for always growing sample. summary, IKS was one order of magnitude faster than OKS and two orders of magnitude faster than standard KS.
The second test evaluates the time necessary to perform the KS test using two fixed size sliding windows through two parallel streams of observations: at each time step, one observation is removed from each sample and one is included. The stream has 10000 random observations and we vary the size of the window from 100 to 1000, increasing 100 at a time.

The stream experiment is more realistic and meaningful for the purpose of this work, since it meets our proposals for drift detection and classification. Figure 2 shows the time necessary to scan the whole streams by the size of the sliding windows that were in use. Again, confidence intervals were plotted, but they are too tight to be distinguishable from their correspondent curves. For instance, for a sliding window of size 1000 observations, Incremental KS took an average over 30 repetitions of 0 . 0410 seconds. Optimized KS took 0 . 9605 seconds and Standard KS took 5 . 326 seconds. Again, IKS was one order of magnitude faster than OKS and two orders of magnitude faster than standard KS. Time (seconds) Figure 2: Averaged time for scanning a stream with different sliding window sizes.
Tables 2 and 3 summarize our experimental results for ac-curacy and percentage of requested true labels, respectively. For all classifiers, Model Replacement statistically produced greater accuracy than the correspondent baselines and re-quested less true labels than the corresponding topline. Al-though the accuracy of the Model Replacement was sta-tistically smaller than the accuracy of the topline for all classifiers, Model Replacement lost by a very small margin. On average, the Model Replacement accuracy was 99.39%, 94.31% and 93.69% of the Topline accuracy, for the Nearest Neighbor, Na  X   X ve Bayes and Decision Tree classifiers, respec-tively. Conversely, Model Replacement requested, on aver-age, 47.59% for Nearest Neighbor and Na  X   X ve Bayes classifiers and 42.17% for Decision Tree classifier of the true labels.
All the remaining methods led to significantly smaller number of requested labels. However, the classification per-formance is usually reduced as well. A notable exception is Adaptree that obtained performance similar to Model Re-placement for decision trees, but using significantly fewer labels (42.17% for MR and 35.62% for Adaptree). Unfortu-nately, decision tree classifiers did not perform as well as the Nearest Neighbor classifiers.

The  X / X  Transformation statistically reduced the portion of true labels that were required if compared to Model Re-placement (16.89%, on average, for  X / X  ). However, this par-ticular transformation is sensitive to the nature of the drift since it assumes a monotonic drift [1]. If the assumption does not hold, even if the drift is detectable, it may cause lower accuracy. In our tests, the accuracy produced by this method was statistically smaller than Model Replacement alone. On average,  X / X  obtained 92.75% of the Topline ac-curacy.

Finally, using a fixed sample (reference set) was consis-tently more accurate than Consecutive Sliding Windows De-tection (82.15% of Topline accuracy for CD), while the lat-ter consistently consumed a smaller number of true labels (16.37% on average). We also note that detecting drifts based on the probability estimates led to more similar accu-racy rates than the ones obtained by detecting drifts based on the feature space, yet consistently requiring fewer labels. Thus, using probability estimates is a promising approach for high dimensional data, if the classifier at hand is com-patible. Additionally, mixing values from the feature space, probability estimates and outputted labels together is a pos-sible alternative approach.
This work presented the Incremental Kolmogorov-Smirnov algorithm, a much faster method of recomputing the KS statistic for samples that change gradually over time. We evaluated the proposed test in data stream classification sce-nario, supporting a fast mechanism of concept drift detec-tion. The classifiers were able to identify concept drifts with-out true label information and request only a portion of the true labels to adapt the models to new concepts. The use-fulness of the Incremental Kolmogorov-Smirnov algorithm is not limited to this case study. We hope it to be widely applied in the future to empower different approaches for streaming problems, where a fast non-parametric hypothe-sis test may be needed.

As future work, we intend to develop new and more robust approaches to deal with detected concept drifts, such as the use of ensembles of previous models for recurrent concepts. The first author X  X  research was supported by S  X ao Paulo Research Foundation (FAPESP) grants 2014/12333-7 and 2015/01701-8. The third author X  X  research was supported by the Natural Sciences and Engineering Research Council of Canada, by the CALDO Programme, and by the National Research Centre of Poland (NCN) grant DEC-2013/09/B/-ST6/01549. The fourth author X  X  research was supported by CNPq grants 03083/2013-1 and 446330/2014-0 and FAPESP grant 2013/50379-6. [1] R. Al-Otaibi, R. B. Prud X encio, M. Kull, and P. Flach. [2] M. Amir and D. Toshniwal. Instance-based [3] C. R. Aragon and R. G. Seidel. Randomized search [4] G. E. A. P. A. Batista, E. J. Keogh, A. M. Neto, and [5] A. Bifet and R. Gavalda. Learning from time-changing [6] A. Bifet and R. Gavald`a. Adaptive learning from [7] A. Bifet, G. Holmes, and B. Pfahringer. Leveraging [8] A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi, [9] V. M. De Souza, D. F. Silva, G. E. Batista, et al. [10] L. Devroye. A note on the height of binary search [11] D. dos Reis. Fast unsupervised online drift detection [12] K. B. Dyer, R. Capo, and R. Polikar. Compose: A [13] H. Fanaee-T and J. a. Gama. Event labeling [14] J. Gama, P. Medas, G. Castillo, and P. Rodrigues. [15] S. Guha, N. Mishra, R. Motwani, and L. O X  X allaghan. [16] N. Hammami and M. Bedda. Improved tree model for [17] A. Haque, L. Khan, and M. Baron. Semi supervised [18] D. J. Hill and B. S. Minsker. Anomaly detection in [19] B. Kalu X za, V. Mirchevska, E. Dovgan, M. Lu X strek, [20] L. I. Kuncheva et al. Nearest neighbour classifiers for [21] R. H. Lopes. Kolmogorov-smirnov test. In [22] M. M. Masud, J. Gao, L. Khan, J. Han, and [23] M. M. Masud, C. Woolam, J. Gao, L. Khan, J. Han, [24] N. C. Oza. Online bagging and boosting. In SMC , [25] R. Seidel and C. R. Aragon. Randomized search trees. [26] V. M. Souza, D. F. Silva, J. a. Gama, and G. E. [27] X. Wu, P. Li, and X. Hu. Learning from concept [28] I. Zliobaite. Change with delayed labeling: when is it
A Cartesian Tree is defined as binary tree which has the following properties: 1. It has the heap ordering property, i.e. a non-leaf node 2. It is built upon a sequence of numbers and the in-order
We can define a merge operation between two already ex-isting Cartesian Trees. One of them is called the left tree and the other the right tree. Such merge can be used to build a Cartesian Tree from scratch, simply considering that the right tree is a single node with the next observation from the sequence. After the merging, the following constraints must hold: the resultant tree still has the heap ordering property and an in-order traverse in it is equivalent to an in-order traverse in the left tree, followed by an in-order traverse in the right tree.

Such merge operation is achieved recursively. Let L be the left tree and R be the right tree. If L  X  X  root has a greater priority than R  X  X  root, then L  X  X  root is the resultant tree X  X  root Z . Z  X  X  left subtree is L  X  X  left subtree, preserving the in-order traverse to the left and the heap priority property. Z  X  X  right subtree is a recursive merge between L  X  X  right subtree  X  as left tree  X  and R  X  as right tree. If the priority of R  X  X  root is greater than the priority of L  X  X  root, the solution is analogous. The process is summarized by the following algorithm. 4: 8: 12:
A Treap is a special case of Cartesian Tree where each node has, apart from the heap priority value, an additional key. From the perspective of the priority, the tree is a heap, but from the perspective of the additional key (BST key), it is a binary search tree. A tree that meets both criteria is possible for any set of priority values and BST keys. The node with highest priority will be the root of the tree. The root of the left subtree will be composed by nodes with BST key smaller than the root X  X  BST key, and the right subtree with BST keys greater than or equal to the root X  X  BST key. The same idea recursively applies to the remainder of the tree.

The Treap is easily achieved by construction, using the merge algorithm defined for Cartesian Trees. In a merge operation, if the left and the right trees are both Treaps and all BST keys from the left tree are less or equal than the BST keys from the right tree, then the resultant tree is also a Treap. Figure 3 illustrates the merge operation of two Treaps. Figure 3: Merge operation between two treaps. The left tree is red and on the left side of the dashed ver-tical line. The right tree is blue and on the right side of the dashed vertical line. The arrows correspond to father X  X hildren relationships. The result tree X  X  new relationships are green and removed relation-ships are dashed.

Inserting a new pair (priority value, BST key) into an al-ready existing Treap may require a split operation. The fol-lowing algorithm describes how this operation can be achieved. The input is a Treap and a BST key k . Two Treaps are the output. The first (left tree) contains all the elements from the original Treap that have lower BST keys than k . The second (right tree) contains the remaining elements, includ-ing the ones which have the same BST keys as the one that was applied in the query. 4:
Finally, with both Merge and Split operations, we can insert new elements to an already existing Treap by splitting it and re-merging it adequately. The following algorithm describes this process.
The time complexity for both Merge and Split depends linearly on the depth of the Treap. For a list of pairs of unique keys and unique priorities, there is only one possible Treap, as the in-order traverse is locked by keys and the bal-ance  X  i.e. the father-children relationships  X  is locked by the ranking of the priorities. In other words, the resulting Treap after the insertion of all the elements of such list, in any or-der, is always the same. The probability distribution of the priorities is not a matter of concern, provided that they are unique, since the Treap only uses their ranks. Specifically, the ranking of the priorities is analogous to the order of the elements when constructing a basic binary search tree at once. Figure 3 illustrates this fact by plotting a Cartesian Tree on a Cartesian Plane, where the priority corresponds to the y -axis and the key corresponds to the x -axis. Each node is father of both the topmost node to its left and the topmost node to its right. If we insert all the keys from this figure using only basic binary search tree insertion, from top to bottom, we obtain exactly the same tree. Therefore, if the priorities are chosen at random, the Treap inherits a common property of randomized binary search trees that are constructed at once: the height of the tree grows loga-rithmic with high probability [10]. Consequently, Merging and Splitting a Treap and inserting and removing elements, logarithmic in time, according to the total number of ele-ments. Figure 4 empirically shows the logarithmic growth of the Treap. Figure 4: Height growth of a treap. The curve is an average of the heights of 30 cartesian trees and the shaded band indicates 95% confidence intervals.
As the priorities are fixed, if a node is predecessor of an-other one, it will never become the other way around. This property can be exploited so that we can easily put summary information about a subtree in its root and keep this infor-mation up to date, without interfering in the complexity of the operations that were presented so far. As an example, let us say that now each node of the tree has an additional and arbitrary value. The following algorithm makes changes in the previous operations in order to also efficiently com-pute the minimum and maximum values for each complete subtree. 7: 20:
Similarly, we can also efficiently perform bulk operations that change all values in a tree, if these operations are suit-able . An operation is suitable if its resulting summaries for the whole tree can be computed using only the already ex-isting summaries and without accessing the subtrees. The procedure of propagating a bulk operation from a root to its subtress only when it is strictly necessary is known as lazy propagation. It should be performed in the same time complexity as the complexity of applying the bulk operation in the root, usually O (1). The following algorithm presents the needed modifications to obtain a new operation to our tree, IncreaseBy , which adds a constant to all values of the tree. This new operation is O (1) and it does not change the complexity of the other operations. 8: 14: 22: 37:
A new version of the Split operation is needed in order to remove an element (given a key) from the Treap. We call it SplitFirst. After applying this operation, we obtain a left tree with one element that contains the smallest key in the original tree, and a right tree with the remaining elements. The following algorithm describes it. Additionally, it de-scribes the SplitLast method, which produces a right tree with the element that has the greatest key and a left tree with the remaining elements. 17:
