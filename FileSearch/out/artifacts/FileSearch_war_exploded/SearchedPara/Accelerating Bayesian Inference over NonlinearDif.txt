 Dept. of Computing Sci.
 Mechanistic system modeling employing nonlinear ordinary or delay differential equations 1 (ODEs or DDEs) is oftentimes hampered by incomplete knowledge of the system structure or the spe-cific parameter values defining the observed dynamics [16]. Bayesian, and indeed non-Bayesian, approaches for parameter estimation and model comparison [19] involve evaluating likelihood func-tions, which requires the explicit numerical solution of the differential equations describing the model. The computational cost of obtaining the required numerical solutions of the ODEs or DDEs can result in extremely slow running times. In this paper we present a method for performing Bayesian inference over mechanistic models by the novel use of Gaussian processes (GP) to predict the state variables of the model as well as their derivatives, thus avoiding the need to solve the sys-tem explicitly. This results in dramatically improved computational efficiency (up to four hundred times faster in the case of DDEs). We note that state space models offer an alternative approach for performing parameter inference over dynamical models particularly for on-line analysis of data, see [2]. Related to the work we present, we also note that in [6] the use of GPs has been proposed in obtaining the solution of fully parameterised linear operator equations such as ODEs. Likewise in [12] GPs are employed as emulators of the posterior response to parameter values as a means of improving the computational efficiency of a hybrid Monte Carlo sampler.
 Our approach is different and builds significantly upon previous work which has investigated the use of derivative estimates to directly approximate system parameters for models described by ODEs. A spline-based approach was first suggested in [18] for smoothing experimental data and obtaining derivative estimates, which could then be used to compute a measure of mismatch for derivative values obtained from the system of equations. More recent developments of this method are de-scribed in [11]. All of these approaches, however, are plagued by similar problems. The methods are all critically dependent on additional regularisation parameters to determine the level of data smoothing. They all exhibit the problem of providing sub-optimal point estimates; even [11] may not converge to a reasonable solution depending on the initial values selected, as we demonstrate in Section 5.1. Furthermore, it is not at all obvious how these methods can be extended for partially observed systems, which are typical in, e.g. systems biology [10, 1, 8, 19]. Finally, these methods only provide point estimates of the  X  X orrect X  parameters and are unable to cope with multiple so-lutions. (Although it should be noted that [11] does offer a local estimate of uncertainty based on second derivatives, at additional computational cost.) It is therefore unclear how objective model comparison could be implemented using these methods.
 In contrast we provide a Bayesian solution, which is capable of sampling from multimodal distribu-tions. We demonstrate its speed and statistical accuracy and provide comparisons with the current best methods. It should also be noted that the papers mentioned above have focussed only on param-eter estimation for fully observed systems of ODEs; we additionally show how parameter inference over both fully and partially observed ODE systems as well as DDEs may be performed efficiently using our state derivative approach. A dynamical system may be described by a collection of N ordinary differential equations and model parameters  X  which define a functional relationship between the process state, x ( t ) , and its time certain dynamic systems, where now an explicit time-delay  X  is employed. A sequence of process observations, y ( t ) , are usually contaminated with some measurement error which is modeled as y ( t ) = x ( t ) + ( t ) where ( t ) defines an appropriate multivariate noise process, e.g. a zero-mean Gaussian with variance  X  2 n for each of the N states. If observations are made at T distinct time points the N  X  T matrices summarise the overall observed system as Y = X + E . In order to obtain values for X the system of ODEs must be solved, so that in the case of an initial value problem X (  X  , x 0 ) denotes the solution of the system of equations at the specified time points for the parameters  X  and initial conditions x 0 . Figure 1(a) illustrates graphically the conditional dependencies of the overall statistical model and from this the posterior density follows by employing appropriate priors such can be obtained from this joint posterior 2 .
 Various sampling schemes can be devised to sample from the joint posterior. However, regardless of the sampling method, each proposal requires the specific solution of the system of differential equations which, as will be demonstrated in the experimental sections, is the main computational bottleneck in running an MCMC scheme for models based on differential equations. The computa-tional complexity of numerically solving such a system cannot be easily quantified since it depends on many factors such as the type of model and its stiffness, which in turn depends on the specific parameter values used. A method to alleviate this bottleneck is the main contribution of this paper. Let us assume independent 3 Gaussian process priors on the state variables such that p ( X n,  X  |  X  n ) = N ( 0 , C  X   X  where  X  n = C  X  GP model of the state-variables. Note that a non-Gaussian noise model may alternatively be implemented using warped GPs [14]. The conditional distribution for the state-derivatives is Figure 1: (a) Graphical model representing explicit solution of an ODE system, (b) Graphical model rep-p (  X 
X n,  X  | X n,  X  ,  X  n , X  n ) = N ( m n , K n ) , where the mean and covariance are given by the cross-covariances between the state and its derivative [13, 15]. The main advantage of using the Gaussian process model now becomes apparent. The GP specifies a jointly Gaussian distri-bution over the function and its derivatives ([13], pg.191). This allows us to evaluate a poste-rior over parameters  X  consistent with the differential equation based on the smoothed state and state derivative estimates, see Figure 1(b). Assuming Normal errors between the state-derivatives  X  X n,  X  and the functional, f n ( X ,  X  , t ) evaluated at the GP generated state-values, X corresponding gradients appear only linearly and their conditional distribution given X is Gaussian they can be marginalized exactly. In other words, given observations Y , we can sample from the conditional distribution for X and marginalize the augmented derivative space. The differential equation need now never be explicitly solved, its implicit solution is integrated into the sampling scheme. The introduction of the auxiliary model and its associated variables has enabled us to recast the differential equation as another component of the inference process. The relationship between the auxiliary model and the physical process that we are modeling is shown in Figure 1(b), where the dotted lines represent a transfer of information between the models. This information transfer takes place through sampling candidate solutions for the system in the GP model. Inference is performed by combining these approximate solutions with the system dynamics from the differential equations. It now remains to define an overall sampling scheme for the structural parameters. For brevity, we omit normalizing constants and assume that the system is defined in terms of ODEs. However, our scheme is easily extended for delay differential equations (DDEs) where now predictions at each time point t and the associated delay ( t  X   X  ) are required  X  we present results for a DDE system in Section 5.2. We can now consider the complete sampling scheme by also inferring the hyperparameters and corresponding predictions of the state variables and derivatives using the GP framework described in Section 3. We can obtain samples  X  from the desired marginal posterior p (  X  | Y ) 4 by sampling from the joint posterior p (  X  ,  X  , X ,  X  ,  X  | Y ) as follows where  X  n  X  f n  X  m n . This requires two Metropolis sampling schemes; one for inferring the param-eters of the GP,  X  and  X  , and another for the parameters of the structural system,  X  and  X  . However, as a consequence of the system induced dynamics the corresponding likelihood surface defined by p ( Y |  X  , x 0 ,  X  ) can present formidable challenges to standard sampling methods. As an example Figure 1(c) illustrates the induced likelihood surface of a simple dynamic oscillator similar to that presented in the experimental section. Recent advances in MCMC methodology suggest solutions to this problem in the form of population-based MCMC methods [8], which we therefore implement to sample the structural parameters of our model. Population MCMC enables samples to be drawn from a target density p (  X  ) by defining a product of annealed densities indexed by a temperature value of  X  i . It is convenient to fix a geometric path between the prior and posterior, which we do in our implementation, although other sequences are possible [3]. A time homogeneous Markov tran-sition kernel which has p (  X  ) as its stationary distribution can then be constructed from both local Metropolis proposal moves and global temperature switching moves between the tempered chains of the population [8], allowing freer movement within the parameter space.
 The computational scaling for each component of the sampler is now considered. Sampling of the GP covariance function parameters by a Metropolis step requires computation of a matrix deter-minant and its inverse, so for all N states in the system a dominant scaling of O ( NT 3 ) will be obtained. This poses little problem for many applications in systems biology since T is often fairly small ( T  X  10 to 100 ). For larger values of T , sparse approximations can offer much improved computational scaling of order O ( NM 2 T ) , where M is the number of time points selected [9]. Sampling from a multivariate Normal whose covariance matrix and corresponding decompositions have already been computed therefore incurs no dominating additional computational overhead. The final Metropolis step (Equation 3) requires each of the K n matrices to be constructed and the associated determinants and inverses computed thus incurring a total O ( NT 3 ) scaling per sample. An approximate scheme can be constructed by first obtaining the maximum a posteriori values for the GP hyperparameters and posterior mean state values,  X   X  ,  X   X  ,  X  X n , and then employing these in Equation 3. This will provide samples from p (  X  ,  X  |  X  X ,  X   X  ,  X   X , Y ) which may be a useful surrogate for the full joint posterior incurring lower computational cost as all matrix operations will have been pre-computed, as will be demonstrated later in the paper.
 We can also construct a sampling scheme for the important special case where some states are unobserved. We partition X into X o , and X u . Let o index the observed states, then we may infer all the unknown variables as follows where  X  o,u n  X  f n ( X o , X u ,  X  , t )  X  m n and  X  ( X u ) is an appropriately chosen prior. The values of unobserved species are obtained by propagating their sampled initial values using the corresponding discrete versions of the differential equations and the smoothed estimates of observed species. The p53 transcriptional network example we include requires inference over unobserved protein species, see Section 5.3. We now demonstrate our GP-based method using a standard squared exponential covariance func-tion on a variety of examples involving both ordinary and delay differential equations, and compare the accuracy and speed with other state-of-the-art methods. 5.1 Example 1 -Nonlinear Ordinary Differential Equations We first consider the FitzHugh-Nagumo model [11] which was originally developed to model the behaviour of spike potentials in the giant axon of squid neurons and is defined as  X  V = c V  X  V 3 / 3 + R ,  X  R =  X  ( V  X  a + bR ) /c . Although consisting of only 2 equations and 3 pa-rameters, this dynamical system exhibits a highly nonlinear likelihood surface [11], which is induced by the sharp changes in the properties of the limit cycle as the values of the parameters vary. Such a feature is common to many nonlinear systems and so this model provides an excellent test for our GP-based parameter inference method.
 Data is generated from the model, with parameters a = 0 . 2 , b = 0 . 2 , c = 3 , at { 40, 80, 120 } time points with additive Gaussian noise, N (0 ,v ) for v = 0 . 1  X   X  n , where  X  n is the standard deviation for the n th species. The parameters were then inferred from these data sets using the full Bayesian sampling scheme and the approximate sampling scheme (Section 4), both employing population MCMC. Additionally, we inferred the parameters using 2 alternative methods, the profiled estima-tion method of Ramsay et al. [11] and a Population MCMC based sampling scheme, in which the ODEs were solved explicitly (Section 2), to complete the comparative study. All the algorithms were coded in Matlab, and the population MCMC algorithms were run with 30 temperatures, and used a suitably diffuse  X (2 , 1) prior distribution for all parameters, forming the base distribution for the sampler. Two of these population MCMC samplers were run in parallel and the  X  R statistic [5] was used to monitor convergence of all chains at all temperatures. The required numerical approxi-mations to the ODE were calculated using the Sundials ODE solver, which has been demonstrated to be considerably (up to 100 times) faster than the standard ODE45/ODE15s solvers commonly used in Matlab. In our experiments the chains generally converged after around 5000 iterations, and 2000 samples were then drawn to form the posterior distributions. Ramsay X  X  method [11] was imple-mented using the Matlab code which accompanies their paper. The optimal algorithm settings were used, tuned for the FitzHugh-Nagumo model (see [11] for details) which they also investigated. Each experiment was repeated 100 times, and Table 1 shows summary statistics for each of the inferred parameters. All of the three sampling methods based on population MCMC produced low variance samples from posteriors positioned close to the true parameters values. Most noticeable from the results in Figure 2 is the dramatic speed advantage the GP based methods have over the more direct approach, whereby the differential equations are solved explicitly; the GP methods introduced in this paper offer up to a 10-fold increase in speed, even for this relatively simple system of ODEs. We found the performance of the profiled estimation method [11] to be very sensitive to the initial parameter values. In practice parameter values are unknown, indeed little may be known even about the range of possible values they may take. Thus it seems sensible to choose initial values from a wide prior distribution so as to explore as many regions of parameter space as possible. Employing Table 1: Summary statistics for each of the inferred parameters of the FitzHugh-Nagumo model. Each exper-profiled estimation using initial parameter values drawn from a wide gamma prior, however, yielded highly biased results, with the algorithm often converging to local maxima far from the true param-eter values. The parameter estimates become more biased as the variance of the prior is increased, i.e. as the starting points move further from the true parameter values. E.g. consider parameter a ; a range [Min, Median, Max] = [  X  0 . 329 , 0 . 205 , 9 . 3  X  10 9 ] and for a wider prior a,b,c  X   X (2 , 1) , then  X  a had range [Min, Median, Max] = [  X  1 . 4  X  10 10 , 0 . 195 , 2 . 2  X  10 9 ] . Lack of robustness therefore seems to be a significant problem with this profiled estimation method. The speed of the profiled estimation method was also extremely variable, and this was observed to be very depen-recorded were [Min, Mean, Max] = [193 , 308 , 475] . Using a different prior for initial values such that a,b,c  X   X (1 , 0 . 5) , the times were [Min, Mean, Max] = [200 , 913 , 3265] and similarly for a wider prior a,b,c  X   X (2 , 1) , [Min, Mean, Max] = [132 , 4171 , 37411] . Experiments performed with noise v = { 0 . 05 , 0 . 2 } X   X  n produced similar and consistent results, however they are omitted due to lack of space. 5.2 Example 2 -Nonlinear Delay Differential Equations This example model describes the oscillatory behaviour of the concentration of mRNA and its corre-sponding protein level in a genetic regulatory network, introduced by Monk [10]. The translocation of mRNA from the nucleus to the cytosol is explicitly described by a delay differential equation. where  X  m and  X  p are decay rates, p 0 is the repression threshold, n is a Hill coefficient and  X  is the time delay. The application of our method to DDEs is of particular interest since numerical solutions to DDEs are generally much more computationally expensive to obtain than ODEs. Thus inference of such models using MCMC methods and explicitly solving the system at each iteration becomes less feasible as the complexity of the system of DDEs increases.
 We consider data generated from the above model, with parameters  X  m = 0 . 03 ,  X  p = 0 . 03 , p 0 = 100 ,  X  = 25 , at { 40, 80, 120 } time points with added random noise drawn from a Gaus-sian distribution, N (0 ,v ) for v = 0 . 1  X   X  n , where  X  n is the standard deviation of the time-series data for the n th species. The parameters were then inferred from these data sets using our GP-based population MCMC methods. Figure 3 shows a time comparison for 10 iterations of the GP sampling algorithms and compares it to explicitly solving the DDEs using the Matlab solver DDE23 (which is generally faster than the Sundials solver for DDEs). The GP methods are around 400 times faster for 40 data points. Using the GP methods, samples from the full posterior can be obtained in less than an hour. Solving the DDEs explicitly, the population MCMC algorithm would take in excess of two weeks computation time, assuming the chains take a similar number of iterations to converge. Table 2: Summary statistics for each of the inferred parameters of the Monk model. Each experiment was Figure 3: Summary statistics of the time taken for the algorithms to complete 10 iterations using DDE model. 5.3 Example 3 -The p53 Gene Regulatory Network with Unobserved Species Our third example considers a linear and a nonlinear model describing the regulation of 5 target genes by the tumour repressor transcription factor protein p53. We consider the following differen-tial equations which relate the expression level x j ( t ) of the j th gene at time t to the concentration of the basal rate of gene j , S j is the sensitivity of gene j to the transcription factor and D j is the decay rate of the mRNA. Letting g ( f ( t )) = f ( t ) gives us the linear model originally investigated in [1], and letting g ( f ( t )) = exp( f ( t )) gives us the nonlinear model investigated in [4]. The transcription factor f ( t ) is unobserved and must be inferred along with the other structural parameters B j , S j and D j using the sampling scheme detailed in Section 4.1. In this experiment, priors on the unob-served species used were f ( t )  X   X (2 , 1) with a log-Normal proposal. We test our method using the Figure 4: The predicted output of the p53 gene using data from Barenco et al. [1] and the accelerated GP leukemia data set studied in [1], which comprises 3 measurements at each of 7 time points for each of the 5 genes. Figure 4 shows the inferred missing species and the results are in good accordance with recent biological studies. For this example, our GP sampling algorithms ran to completion in under an hour on a 2.2GHz Centrino laptop, with no difference in speed between using the linear and nonlinear models; indeed the equations describing this biological system could be made more complex with little additional computational cost. Explicit solution of differential equations is a major bottleneck for the application of inferential methodology in a number of application areas, e.g. systems biology, nonlinear dynamic systems. We have addressed this problem and placed it within a Bayesian framework which tackles the main shortcomings of previous solutions to the problem of system identification for nonlinear differential equations. Our methodology allows the possibility of model comparison via the use of Bayes factors, which may be straightforwardly calculated from the samples obtained from the population MCMC algorithm. Possible extensions to this method include more efficient sampling exploiting control variable methods [17], embedding characteristics of a dynamical system in the design of covariance functions and application of our method to models involving partial differential equations. Acknowledgments Ben Calderhead is supported by Microsoft Research through its European PhD Scholarship Pro-gramme. Mark Girolami is supported by an EPSRC Advanced Research Fellowship EP/EO52029 and BBSRC Research Grant BB/G006997/1.
 References
