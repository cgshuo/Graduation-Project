 Our goal is to develop an unsupervised method for acquiring inference rules that describe logical impli-cations between event occurrences. As clues to fi nd the rules, we chose Japanese coordinated sentences, which typically report two events that occur in a cer-tain temporal order. Of course, not every coordi-nated sentence necessarily expresses implications. We found, though, that reliable rules can be acquired by looking at co-occurrence frequencies between verbs in coordinated sentences and co-occurrences between verbs and nouns. For example, our method could ob-tain the rule  X If someone enforces a law, usually some-one enacts the law at the same time as or before the enforcing of the law X . In our experiments, when our method produced 400 rules for 1,000 given nouns, 70% of the rules were considered proper by at least three of four human judges.

Note that the acquired inference rules pose tempo-ral constraints on occurrences of the events described in the rules. In the  X enacting-and-enforcing-law X  ex-ample, the constraints were expressed by the phrase  X at the same time as or before the event of X . We think such temporally constrained rules should be bene fi cial in various types of NLP applications. The rules should allow Q&amp;A systems to guess or restrict the time at which a certain event occurs even if they cannot di-rectly fi nd the time in given documents. In addition, we found that a large part of the acquired rules can be regarded as paraphrases , and many possible applica-tions of paraphrases should also be target applications.
To acquire rules, our method uses a score, which is basically an approximation of the probability that par-ticular coordinated sentences will be observed. How-ever, it is weighted by a bias , which embodies our as-sumption that frequently observed verbs are likely to appear as the consequence of a proper inference rule. This is based on our intuition that frequently appear-ing verbs have a generic meaning and tend to describe a wide range of situations, and that natural language expressions referring to a wide range of situations are more likely to be a consequence of a proper rule than speci fi c expressions describing only a narrow range of events. A similar idea relying on word co-occurrence was proposed by Geffet and Dagan (Geffet and Da-gan, 2005) but our method is simpler and we expect it to be applicable to a wider range of vocabularies.
Research on the automatic acquisition of inference rules, paraphrases and entailments has received much attention. Previous attempts have used, for instance, the similarities between case frames (Lin and Pan-tel, 2001), anchor words (Barzilay and Lee, 2003; Shinyama et al., 2002; Szepektor et al., 2004), and a web-based method (Szepektor et al., 2004; Geffet and Dagan, 2005). There is also a workshop devoted to this task (Dagan et al., 2005). The obtained accuracies have still been low, however, and we think searching for other clues, such as coordinated sentences and the bias we have just mentioned, is necessary. In addition, research has also been done on the acquisition of the temporal relations (Fujiki et al., 2003; Chklovski and Pantel, 2004) by using coordinated sentences as we did, but these works did not consider the implications between events. In the following, we begin by providing an overview of our algorithm. We specify the basic steps in the al-gorithm and the form of the rules to be acquired. We also examine the direction of implications and tempo-ral ordering described by the rules. After that, we de-scribe a simpli fi ed version of the scoring function that our algorithm uses and then discuss a problem related to it. The bias mechanism, which we mentioned in the introduction, is described in the section after that. 2.1 Procedure and Generated Inference Rules Our algorithm is given a noun as its input and pro-duces a set of inference rules. A produced rule ex-presses an implication relation between two descrip-tions including the noun. Our basic assumptions for the acquisition can be stated as follows.  X  If verbs v 1 and v 2 frequently co-occur in coordi- X  The above tendency becomes stronger when the Our procedure consists of the following steps. Step 1 Select M verbs that take a given noun n as Step 2 For each possible pair of the selected verbs, Step 3 If the score value for a verb pair is higher than Note that we used 500 as the value of M . N was set to 4 and  X  was set to various values during our ex-periments. Another important point is that, in Step 3, the argument positions at which the given noun can appear is restricted to syntactic objects. This was be-cause we empirically found that the rules generated from such verb-noun pairs were relatively accurate.
Assume that a given noun is  X goods X  and the verb pair  X sell X  and  X manufacture X  is selected in Step 3. Then, the following rule is generated.  X  If someone sells goods , usually someone manu-Although the word  X someone X  occurs twice, we do not demand that it refers to the same person in both instances. It just works as a placeholder. Also note that the adverb  X usually X  1 was inserted to prevent the rule from being regarded as invalid by considering sit-uations that are logically possible but unlikely in prac-tice.

The above rule is produced when  X manufacture X  and  X sell X  frequently co-occur in coordinated sen-tences such as  X The company manufactured goods and it sold them X . One might be puzzled because the order of the occurrences of the verbs in the coordi-nated sentences is reversed in the rule. The verb  X sell X  in the second (embedded) sentence/clause in the coor-dinated sentence appears as a verb in the precondition of the rule, while  X manufacture X  in the fi rst (embed-ded) sentence/clause is the verb in the consequence.
A question then, is why we chose such an order, or such a direction of implication. There is another possibility, which might seem more straightforward. From the same coordinated sentences, we could pro-duce the rule where the direction is reversed; i.e,.,  X If someone manufactures goods , usually someone sells the goods at the same time as or after the manufactur-ing X . The difference is that the rules generated by our procedure basically infer a past event from another event, while the rules with the opposite direction have to predict a future event. In experiments using our de-velopment set, we observed that the rules predicting future events were often unacceptable because of the uncertainty that we usually encounter in predicting the future or achieving a future goal . For instance, peo-ple might do something (e.g., manufacturing) with an intention to achieve some other goal (e.g., selling) in the future. But they sometimes fail to achieve their fu-ture goal for some reason. Some manufactured goods are never sold because, for instance, they are not good enough. In our experiments, we found that the preci-sion rates of the rules with the direction we adopted were much higher than those of the rules with the op-posite direction. 2.2 Simpli fi ed Scoring Function To be precise, a rule generated by our method has the following form, where v pre and v con are verbs and n is a given noun.  X  If someone v pre n , usually someone v con the n at We assume that all three occurrences of noun n in the rule refer to the same entity .

Now, we de fi ne a simpli fi ed version of our scoring function as follows.

Here, P coord ( v con , v pre ) is the probability that v and v pre are observed in coordinated sentences in a way that the event described by v con temporally pre-cedes or occurs at the same time as the event de-scribed by v pre . (More precisely, v con and v pre must be the main verbs of two conjuncts S 1 and S 2 in a Japanese coordinated sentence that is literally trans-lated to the form  X  S 1 and S 2  X .) This means that in the coordinated sentences, v con appears fi rst and v pre second. P arg ( n | v pre ) and P arg ( n | v con ) are the condi-tional probabilities that n occupies the argument posi-tions arg of v pre and arg of v con , respectively. At the beginning, as possible argument positions, we speci-fi ed fi ve argument positions, including the syntactic object and the subject. Note that when v pre and v con frequently co-occur in coordinated sentences and n often becomes arguments of v pre and v con , the score has a large value. This means that the score embodies our assumptions for acquiring rules.
 BasicS is actually an approximation of the proba-bility P ( v pre , arg , n, v con , arg, n ) that we will ob-serve the coordinated sentences such that the two sen-tences/clauses in the coordinated sentence are headed by v pre and v con and n occupies the argument posi-tions arg of v pre and arg of v con . Another important point is that the score is divided by P ( n ) 2 . This is be-cause the probabilities such as P arg ( n | v con ) tend to be large for a frequently observed noun n . The division by P ( n ) 2 is done to cancel such a tendency. This di-vision does not affect the ranking for the same noun, but, since we give a uniform threshold for selecting the verb pairs for distinct nouns, such normalization is desirable, as we con fi rmed in experiments using our development set. 2.3 Paraphrases and Coordinated Sentences Thus, we have de fi ned our algorithm and a simpli fi ed scoring function. Now let us discuss a problem that is caused by the scoring function.

As mentioned in the introduction, a large por-tion of the acquired rules actually consists of para-phrases . Here, by a paraphrase, we mean a rule con-sisting of two descriptions referring to an identical event. The following example is an English transla-tion of such paraphrases obtained by our method. We think this rule is acceptable. Note that we invented a new English verb  X clearly-write X  as a translation of a Japanese verb meiki-suru while  X write X  is a trans-lation of another Japanese verb kaku .  X  If someone clearly-writes a phone number, usu-Note that  X clearly-write X  and  X write X  have almost the same meaning but the former is often used in texts related to legal matters. Evidently, in the above rule,  X clearly-write X  and  X write X  describe the same event, and it can be seen as a paraphrase . There are two types of coordinated sentence that our method can use as clues to generate the rule.  X  He clearly-wrote a phone number and wrote the  X  He clearly-wrote a phone number, and also wrote
The fi rst sentence is more similar to the inference rule than the second in the sense that the two verbs share the same object. However, it is ridiculous be-cause it describes the same event twice. Such a sen-tence is not observed frequently in corpora, and will not be used as clues to generate rules in practice.
On the other hand, we frequently observe sen-tences of the second type in corpora, and our method generates the paraphrases from the verb-verb co-occurrences taken from such sentences. However, there is a mismatch between the sentence and the ac-quired rule in the sense that the rule describes two events related to the same object (i.e., a phone num-ber), while the above sentence describes two events that are related to distinct objects (i.e., a phone num-ber and an address). Regarding this mismatch, two questions need to be addressed.

The fi rst question is why our method can acquire the rule despite the mismatch. The answer is that our method obtains the verb-verb co-occurrence prob-abilities ( P coord ( v con , v pre ) ) and the verb-noun co-occurrence probabilities (e.g., P arg ( n | v con ) ) indepen-dently, and that the method does not check whether the two verbs share an argument.

Then the next question is why our method can acquire accurate paraphrases from such coordinated sentences. Though we do not have a de fi nite answer now, our hypothesis is related to the strategy that peo-ple adopt in writing coordinated sentences. When two similar but distinct events, which can be described by the same verb, occur successively or at the same time, people avoid repeating the same verb to describe the two events in a single sentence. Instead they try to use distinct verbs that have similar meanings. Sup-pose that a person wrote his name and address. To report what she did, she may write  X I clearly-wrote my name and also wrote my address X  but will seldom write  X I clearly-wrote my name and also clearly-wrote my address X . Thus, we can expect to be able to fi nd in coordinated sentences a large number of verb pairs consisting of two verbs with similar meanings. Note that our method tends to produce two verbs that fre-quently co-occur with a given noun. This also helps to produce the inference rules consisting of two seman-tically similar verbs. We now describe a bias used in our full scoring func-tion, which signi fi cantly improves the precision. The full scoring function is de fi ned as The bias is denoted as P arg ( v con ) , which is the prob-ability that we can observe the verb v con , which is the verb in the consequence of the rule, and its argument position arg is occupied by a noun, no matter which noun actually occupies the position.

An intuitive explanation of the assumption behind this bias is that as the situation within which the de-scription of the consequence in a rule is valid becomes wider, the rule becomes more likely to be a proper one. Consider the following rules.  X  If someone demands a compensation payment,  X  If someone demands a compensation payment, We consider the fi rst rule to be unacceptable while the second expresses a proper implication. The difference is the situations in which the descriptions in the con-sequences hold. In our view, the situations described by  X  order  X  are more speci fi c than those referred to by  X  request  X . In other words,  X  order  X  holds in a smaller range of situations than  X  request  X . Requesting some-thing can happen in any situations where there exists someone who can demand something, but ordering can occur only in a situations where someone in a par-ticular social position can demand something. The ba-sic assumption behind our bias is that rules with con-sequences that can be valid in a wider range of situa-tions, such as  X requesting a compensation payment, X  are more likely to be proper ones than the rules with consequences that hold in a smaller range of situa-tions, such as  X ordering a compensation payment X .
The bias P arg ( v con ) was introduced to capture vari-ations of the situations in which event descriptions are valid. We assume that frequently observed verbs form generic descriptions that can be valid within a wide range of events, while less frequent verbs tend to de-scribe events that can occur in a narrower range of sit-uations and form more speci fi c descriptions than the frequently observed verbs. Regarding the  X request-order X  example, (a Japanese translation of)  X request X  is observed more frequently than (a Japanese transla-tion of)  X order X  in corpora and this observation is con-sistent with our assumption. A similar idea by Geffet and Dagan (Geffet and Dagan, 2005) was proposed for capturing lexical entailment. The difference is that they relied on word co-occurrences rather than the frequency of words to measure the speci fi city of the semantic contents of lexical descriptions, and needed Web search to avoid data sparseness in co-occurrence statistics. On the other hand, our method needs only simple occurrence probabilities of single verbs and we expect our method to be applicable to wider vocabu-lary than Geffet and Dagan X s method.

The following is a more mathematical justi fi cation for the bias. According to the following discussion, P arg ( v con ) can be seen as a metric indicating how easily we can establish an interpretation of the rule, which is formalized as a mapping between events. In our view, if we can establish the mapping easily , the rule tends to be acceptable. The discussion starts from a formalization of an interpretation of an inference rule. Consider the rule  X If exp 1 occurs, usually exp 2 occurs at the same time or before the occurrence of exp 1  X , where exp 1 and exp 2 are natural language ex-pressions referring to events. In the following, we call such expressions event descriptions and distinguish them from an actual event referred to by the expres-sions. An actual event is called an event instance .
A possible interpretation of the rule is that, for any event instance e 1 that can be described by the event description exp 1 in the precondition of the rule, there always exists an event instance e 2 that can be de-scribed by the event description exp 2 in the conse-quence and that occurs at the same time as or before e 1 occurs. Let us write e : exp if event instance e can be described by event description exp . The above interpretation can then be represented by the formula
Here, the mapping f represents a temporal relation between events, and the formula e 2 = f ( e 1 ) expresses that e 2 occurs at the same time as or before e 1 .
The bias P arg ( v con ) can be considered (an approx-imation of) a parameter required for computing the probability that a mapping f random satis fi es the re-quirements for f in  X  when we randomly construct f random . The probability is denoted as P { e 2 : exp 2  X  e the number of events describable by exp 1 . We as-sume that the larger this probability is, the more eas-ily we can establish f . We can approximate P { e 2 : exp 2  X  e 2 = f random ( e 1 ) | e 1 : exp 1 } as P ( exp observing that the probabilistic variables e 1 and e 2 are independent since f random associates them in a com-pletely random manner and by 2) assuming that the occurrence probability of the event instances describ-able by exp 2 can be approximated by the probability that exp 2 is observed in text corpora. This means that P ( exp 2 ) is one of the metrics indicating how easily we can establish the mapping f in  X  .

Then, the next question is what kind of expressions should be regarded as the event description exp 2 . A primary candidate will be the whole sentence appear-ing in the consequence part of the rule to be produced. Since we specify only a verb v con and its argument n in the consequence in a rule, P ( exp 2 ) can be denoted by P arg ( n, v con ) , which is the probability that we ob-serve the expression such that v con is a head verb and n occupies an argument position arg of v con . By mul-tiplying this probability to BasicS as a bias, we ob-tain the following scoring function.
 Score cooc ( n, v con , v pre , arg, arg ) = In our experiments, though, this score did not work well. Since P arg ( n, v con ) often has a small value, the problem of data sparseness seems to arise. Then, we used P arg ( v con ) , which denotes the probability of ob-serving sentences that contain v con and its argument position arg , no matter which noun occupies arg , in-stead of P arg ( n, v con ) . We multiplied the probability to BasicS as a bias and obtained the following score, which is actually the scoring function we propose. 4.1 Settings We parsed 35 years of newspaper articles (Yomiuri 87-01, Mainichi 91-99, Nikkei 90-00, 3.24GB in to-tal) and 92.6GB of HTML documents downloaded from the WWW using an existing parser (Kanayama et al., 2000) to obtain the word (co-occurrence) fre-quencies. All the probabilities used in our method were estimated by maximum likelihood estimation from these frequencies. We randomly picked 600 nouns as a development set. We prepared three test sets, namely test sets A, B, and C, which consisted of 100 nouns, 250 nouns and 1,000 nouns respectively. Note that all the nouns in the test sets were randomly picked and did not have any common items with the development set. In all the experiments, four human judges checked if each produced rule was a proper one without knowing how each rule was produced. 4.2 Effects of Using Coordinated Sentences In the fi rst series of experiments, we compared a simpli fi ed version of our scoring function BasicS with some alternative scores. This was mainly to check if coordinated sentences can improve accuracy. The alternative scores we considered Figure 1: Comparison with the alternatives (4 judges) Figure 2: Comparison with the alternatives (3 judges) are presented below. Note that we did not test our bias mechanism in this series of experiments. S -V V was obtained by approximating the proba-bilities of coordinated sentences, as in the case of BasicS . However, we assumed the occurrences of two verbs were independent. The difference between the performance of this score and that of BasicS will indicate the effectiveness of using verb-verb co-occurrences in coordinated sentences.

The second alternative, S -N V , simply ignores the noun-verb co-occurrences in BasicS . M I is a score based on mutual information and roughly corresponds to the score used in a previous attempt to acquire tem-poral relations between events (Chklovski and Pan-tel, 2004). Cond is an approximation of the proba-bility P ( n, v con | n, v pre ) ; i.e., the conditional proba-Figure 3: Comparison with the alternatives (3 judges) bility that the coordinated sentences consisting of n , v con and v pre are observed given the precondition part consisting of v pre and n . Rand is a random number and generates rules by combining verbs that co-occur with the given n randomly. This was used as a base-line method of our task
The resulting precisions are shown in Figures 1 and 2. The fi gure captions specify  X (4 judges) X , as in Fig-ure 1, when the acceptable rules included only those regarded as proper by all four judges; the captions specify  X (3 judges) X , as in Figure 2, when the ac-ceptable rules include those considered proper by at least three of the four judges. We used test set A (100 nouns) and produced the top four rule candidates for each noun according to each score. As the fi nal re-sults, all the produced rules for all the nouns were sorted according to each score, and a precision was obtained for top N rules in the sorted list. This was the same as the precision achieved by setting the score value of N -th rule in the sorted list as threshold  X  . No-tice that BasicS outperformed all the alternatives 2 , though the difference between S -V V and BasicS was rather small. Another important point is that the precisions obtained with the scores that ignored noun-verb co-occurrences were quite low. These fi ndings suggest that 1) coordinated sentences can be useful clues for obtaining temporally constrained rules and 2) noun-verb co-occurrences are also important clues.
In the above experiments, we actually allowed noun n to appear as argument types other than the syntac-tic objects of a verb. When we restricted the argu-
Figure 4: Two directions of implications (3 judges) ment types to syntactic objects, as described in Sec-tion 2, the precision shown in Figure 3 was obtained. In most cases, BasicS outperformed the alternatives. Although the number of produced rules was reduced because of this restriction, the precision of all pro-duced rules was improved. Because of this, we de-cided to restrict the argument type to objects.
The kappa statistic for assessing the inter-rater agreement was 0.53, which indicates moderate agree-ment according to Landis and Koch, 1977. The kappa value for only the judgments on rules produced by BasicS rose to 0.59. After we restricted the verb-noun co-occurrences to verb-object co-occurrences, the kappa became 0.49, while that for the rules pro-duced by BasicS was 0.54 3 . 4.3 Direction of Implications Next, we examined the directions of implications and the temporal order between events. We produced 1,000 rules for test set B (250 nouns) using the score BasicS , again without restricting the argument types of given nouns to syntactic objects. When we re-stricted the argument positions to objects, we obtained 347 rules. Then, from each generated rule, we created a new rule having an opposite direction of implica-tions. We swapped the precondition and the conse-quence of the rule and reversed its temporal order. For instance, we created  X If someone enacts a law, usually someone enforces the law at the same time as or after the enacting of the law X  from  X If someone enforces a law, usually someone enacts the law at the same time as or before the enforcing of the law X .

Figure 4 shows the results.  X Proposed direction X  refers to the precision of the rules generated by our method. The precision of the rules with the opposite direction is indicated by  X Reversed. X  The precision of  X Reversed X  was much lower than that of our method, and this justi fi es our choice of direction. The kappas values for  X BasicS X  and  X Reversed X  were 0.54 and 0.46 respectively. Both indicate moderate agreement. 4.4 Effects of the Bias Last, we compared Score and BasicS to see the ef-fect of our bias. This time, we used test set C (1,000 nouns). The rules were restricted to those in which the given nouns are syntactic objects of two verbs. The evaluation was done for only the top 400 rules for each score. The results are shown in Figures 5 and 6.  X Score X  refers to the precision obtained with Score , while  X BasicS X  indicates the precision with BasicS . For most data points in both graphs, the  X Score X  pre-cision was about 10% higher than the  X BasicS X  preci-sion. In Figure 6, the precision reached 70% when the 400 rules were produced. These results indicate the desirable effect of our bias for, at least, the top rules. The 400 rules generated by Score included 175 dis-tinct nouns and 272 distinct verb pairs. Examples of the inference rules acquired by Score are shown in Figure 7 along with the positions in the ranking and the numbers of judges who judged the rule as being proper. (We omitted the phrase  X the same time as or before X  in the examples.) The kappa was 0.57 (mod-erate agreement).

In addition, the graphs compare Score with some other alternatives. This comparison was made to check the effectiveness of our bias more carefully. The 400 rules generated by BasicS were re-ranked using Score and the alternative scores, and the pre-cision for each was computed using the human judg-ments for the rules generated by BasicS . (We did not evaluate the rules directly generated by the al-ternatives to reduce the workload of the judges.) The fi rst alternative was Score cooc , which was pre-sented in Section 3. Here,  X reranked by ScoreCooc X  refers to the precision obtained by re-ranking with of Score cooc . The precision was below that obtained by the re-ranking with Score , (referred to as  X reranked by Score) X . As discussed in Section 3, this indicates the bias P arg ( v con ) in Score works better than the bias P arg ( n, v con ) in Score cooc .

The second alternative was the scoring function ob-tained by replacing the bias P arg ( v con ) in Score with P arg ( v pre ) , which is roughly the probability that the verb in the precondition will be observed. The score is denoted as P reBias ( n, v con , v pre , arg, arg ) = P arg ( v pre ) BasicS ( n, v con , v pre , arg, arg ) . The precision of this score is indicated by  X reranked by PreBias X  and is much lower than that of  X reranked by Score X , indicating that only probability of the verbs in the consequences should be used as a bias. This is consistent with our assumption behind the bias. We have presented an unsupervised method for ac-quiring inference rules with temporal constraints, such as  X If someone enforces a law, someone enacts the law at the same time as or before the enforcing of the law X . We used the probabilities of verb-verb co-occurrences in coordinated sentences and verb-noun co-occurrences. We have also proposed a bias mecha-nism that can improve the precision of acquired rules.
