 A natural language discourse is usually composed of multiple subtopics, which in turn may convey only one main topic. In traditional text processing tasks such as subtopic structure of a text can be identified and consequently its semantic segments can be used in the basic processing unit, the performance of the system will be greatly improved [1][2]. In addition, the segment-based IR will provide users with answers of higher accuracy and less redundancy results. The core technology involved in the identification of subtopic structure and therefore semantic segments of a text is called text segmentation, which is the focus of this paper. 
In text segmentation, it becomes critical how to design a good criterion to evaluate the subtopic coherence of a document. According to the definition of text segmentation task, in an appropriate segmentation, sentences within a segment convey the same segment similarity and low between-segment similarity should be achieved. However, both within-segment and between-segment similarities. segmentation. Some algorithms use local optimization approaches, such as sliding use more global strategy by representing lexical distribution on a dotplot [4]. However this is still not a complete globalization strategy. A truly global optimization searching strategy is dynamic programming [5] [8] [10] This model adopts a segmentation criterion function attempting to optimize both within-maximum within-segment and minimum between-segment similarity is selected as the optimization, we implemented our MMS model using the dynamic programming automatically. Experimental results show that our MMS model outperforms other popular approaches in terms of P k [11]and WindowDiff [12] measure. segmentation algorithm are described in detail. In Section 4, experimental results are given to compare our approach with other popular systems. At last, we draw conclusion and address future work in Section 5. Existing text segmentation algorithms can be classified into two categories with respect to the segmentation criteria being employed. One is to make use of the segments are measured to find lexically homogeneous text fragments [6-10]. The the lowest lexical similarity [2-5]. two factors. Fundamental structural factors of written texts, such as segment length and sentence distance are also taken into account in the design of the segmentation used Multiple Discriminant Analysis (MDA) criterion function to find the best segmentation by means of the largest word similarity within a segment and the smallest word similarity between segments. However, the algorithm applied a full search to find complexity. In comparison, MMS model adopts dynamic programming strategy, which optimizing the segmentation cost function. But their method only considers within-segment similarity and needs prior information about segment length. 3.1 Problem Definition a point in a T -dimensional data space. Assume that the topic boundaries occur at the segmentation is to partition the sentences into N groups segmentation can be given as boundaries with segmentation G * among all possible segmentations. finding the segmentation with the highest evaluation score as follows: In the following section we will introduce our motivation in designing the criterion. 3.2 Motivation It will reasonably hold true that in an appropriately segmented text, sentences within a are topically unrelated conveying different subtopics. In much of previous work[4] [6-for a good segmentation strategy. 144 N. Ye et al. 3.3 Segmentation Criterion Function Following the lexical similarity property stated above, we propose our MMS model to comprise of the segmentation evaluating criterion function as follows: between-segment similarity. F is a function whose value increases as Sim Within increases, and decreases as Sim Between increases. The best segmentation can be achieved by maximizing the value of F , which is expressed as: between-segment lexical similarity, respectively. 
Within-segment lexical similarity is: where m and n are the m th and n th sentence in the text. between sentence m and sentence n . The value of represents the global density of word repetition within segments. 
Similarly, between-segment lexical similarity is defined as: Sim Between represents the global lexical similarity between adjacent segments. 
Combining Eq. 3 to Eq. 5, the segmentation evaluation function is computed: 3.4 Text Structure Weighting Factors In addition to segment lexical similarity, there are other text structure factors that are weighted into the proposed text segmentation algorithm. z Segment Length Factor independent subtopic. For example, if there is a sentence in a text, and is not closely between its preceding and its successive segments to enhance coherence. To address this phenomenon, we should avoid introducing too many segments. Restriction on segment number is added into the segmentation criterion function. It penalizes segmentation choices with too many segments by assigning a small evaluation function score to it. length L i , and the length of the whole text is L . Then the length factor can be defined  X  , where are produced. z Distance-based Similarity Weighting If we randomly select two sentences from a discourse, the probability of them sentences far apart are unlikely to belong to the same segment, whereas two adjacent sentences are much more likely. Therefore, we add a distance-based weighting factor to the density function, thus the lexical similarity of two sentences fluctuate with the distance between them. 
Having incorporated the above factors, the overall segmentation evaluating function for our proposed MMS model becomes: where W m,n is the weighting factor, and based on the distance between the sentence m and sentence n . The values of m and n represent the positions of each corresponding sentence. An exemplary definition of W m.n is as follows: We see that in our MMS model, rich information such as the within-segment similarity and between-segment similarity, segment length and the distance between sentences, are considered to discover topical coherence. which N stands for the desired number of segments. 3.5 Text Segmentation Algorithm To optimize the segmentation evaluating function (Eq. 7) globally, we provide an implementation using the dynamic programming searching strategy to find the best dimension dynamic programming is applied. The complete text segmentation algorithm is shown in Figure 1, followed by detailed explanation. 146 N. Ye et al. Input: The K  X  K sentence similarity matrix D ; the parameter  X  ,  X  Initialization For t = 1, 2, ..., K For s = 1, 2, ..., t For k = s , ..., t For w = s , ..., t End End End End Maximization For t = 1, 2, ..., K For s = 0, 1, ..., t -1 For w = 0, 1, ..., s -1 
Backtracking 
For t = 0, 1, ..., T -1 
N = N +1; 
For k =1, 2, ..., N
Output: The optimal segmentation vector maximization, we recursively compute C t,s , which is the optimal (maximum) value of the maximization part of the algorithm we have computed the maximum segmentation optimal segmentation number of segments N is computed automatically. The time complexity of the algorithm is O( K 3 ) ( K is the number of sentences in the text). The evaluation has been conducted systematically under a strict guideline in order to order to generate meaningful results; 2) The testing data should be publicly available; 3) In order to compare with other people X  X  work, we attempt to use their own maximum advantages of their merits. 148 N. Ye et al. 4.1 Experiment Settings one to evaluate the proposed model. The English testing corpus is the publicly available book Mars written by Percival Lowell in 1895. There are 11 chapters in all Exploring Nine Planets . There are 10 chapters in the corpus and each chapter consists of 2-6 sections. The boundaries of paragraphs in the sections are taken as the subtopic boundaries for reference. Sections with few paragraphs (less than 3) are excluded. 
In fact, there is another testing corpus developed by Choi 1 , which is widely used for each article is a concatenation of ten text segments. A segment is the first n sentences constructing corpus in this way is to avoid the difficulty of judging subtopic boundaries by human beings. However this strategy has introduced obvious limitations to the Therefore we used the real corpus instead of the synthetic one in our experiments. of their distance from the correct segment boundaries. In 1997, Beeferman[11] segmented 2 . P k metric is defined as: where (, ) to the same segment and zero otherwise. Similarly, (, ) sentences are hypothesized as belonging to the same segment and zero otherwise. The  X  operator is the XOR operator. The function D Low P k value indicates high segmentation accuracy. boundaries and a new metric called WindowDiff was proposed: www.lingware.co.uk/homepage/freddy.choi/index.htm homepage/freddy.choi/index.htm). where ref is the correct segmentation for reference, hyp is the segmentation produced Low WindowDiff value indicates high segmentation accuracy. We will make comparison under both metrics ( P k and WindowDiff ) on the testing corpus. 
In experiments, punctuation marks and stopwords are removed. The Porter[15] stemming algorithm is applied to the remaining words to obtain word stems for English experiments. 4.2 Experimental Results In MMS model, there are two parameters  X  and  X  that affect the quality of segmentation over certain ranges. To obtain the best parameter we randomly selected varies (the variation is 0.1 each time). Appropriate combination of  X  and  X  value is corpus is used as testing corpus. 
We evaluate MMS model in comparison to the C99 [6] method and Dotplotting [4] method including minimization algorithm (D_Min) and maximization algorithm (D_Max). The experimental results of the two methods come from Choi X  X  software and Chinese corpus, respectively. From experimental results we can see that our MMS model performs better with more Method 
Average homepage/freddy.choi/index.htm) 150 N. Ye et al. Method 
Average Chinese corpus. MMS model achieves more than 12% average error rate reduction from Dotplotting methods and up to 7.7% for C99. The tables also indicate that WindowDiff metric penalizes errors more heavily than P k metric. However the overall rank of the algorithms remains approximately the same on both metrics. On both corpora MMS achieves best performance on most chapters (5 out of 7 for English corpus and 8 out of 11 for Chinese corpus). This is because more weighting factors affecting the segmentation choices are considered in our model. As previously mentioned, either within-segment or between-segment lexical similarity is examined in Dotplotting and C99 while our MMS model examines both factors. In addition, using text structure factors such as segment lengths and sentence distances also leads to an improvement. 
The dynamic programming searching strategy adopted in our model is a global optimization algorithm. Compared to the divisive clustering algorithm of C99 and the global perspective of dynamic programming. With this strategy, the number of segments can be determined automatically when the best segmentation is achieved. In because it cannot decide when to stop inse rting boundaries. The same problem exists in C99 method. Although the author proposed an algorithm to determine the number performance of the method[6]. 
MMS model remedies two problems of Dotplotting. Ye [16] reported analysis of two problems in Dotplotting X  X  segmentation evaluating function: where K is the end of the whole text, and V x,y is a vector containing the word counts associated with word positions x through y in the article. start, a  X  X ackward X  function will be got in a form different from Eq. 11. This problem function (Eq. 7) is symmetrized. Dotplotting does not adequately take the pr eviously located boundaries into account. For each candidate boundary p i being examined, only the segment boundary before it ( p i-1 ) is taken into consideration, and may work less effectively because it ignores the restriction of the segment boundary after it ( p i+1 ). In our MMS model, the restrictions of adjacent segment boundaries on both si des are considered. From the within-strengthened. The optimization process of dynamic programming also helps to select boundaries globally. In this paper we presented a dynamic programming model for text segmentation. This model attempts to simultaneously maximizing within-segment and minimizing between-segment similarity. An analytical form of the segmentation evaluation function is given and a complete text segmentation algorithm using two-dimension dynamic programming searching scheme is described. In addition, other text structure model to capture subtopic changes. compared with popular systems. The MMS model is shown to be promising and effective in text segmentation that it outperforms all other systems in most testing data sets. In comparing with the best comparable system (C99), the MMS model has achieved a reduction of more than 6% in average error rate ( WindowDiff metric). 
In the future we plan to optimize our algorithm by incorporating more features of information trained from background corpus can help improve text segmentation performance. We will also consider introducing semantic knowledge in the model. Besides, the length factor in our model is in a simple form and more adequate segmentation tasks such as news stream and conversation segmentation is also an important research topic. Acknowledgments. This work was supported in part by the National Natural Science Foundation of China under Grant No.60473140, the National 863 High-tech Project No. 2006AA01Z154 ; the Program for New Century Excellent Talents in University No. NCET-05-0287; and the National 985 Project No.985-2-DB-C03 . 2. Hearst, M.A.: Multi-paragraph segmentation of expository text. In: 32nd Annual Meeting 152 N. Ye et al. 4. Reynar, J.C.: An automatic method of finding topic boundaries. In: 32nd Annual Meeting 5. Heinonen, O.: Optimal Multi-Paragraph Text Segmentation by Dynamic Programming. In: 9. Ji, X., Zha, H.: Domain-independent Text Segmentation Using Anisotropic Diffusion and 15. Porter, M.F.: An Algorithm for Suffix Stripping. Program, Vol. 14, pp. 130--137 (1980) 17. Bestgen, Y.: Improving Text Segmentation Using Latent Semantic Analysis: A Reanalysis 
