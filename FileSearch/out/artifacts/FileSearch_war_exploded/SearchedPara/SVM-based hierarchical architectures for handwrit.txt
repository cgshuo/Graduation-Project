 ORIGINAL PAPER Tapan Kumar Bhowmik  X  Pradip Ghanty  X  Anandarup Roy  X  Swapan Kumar Parui Abstract We propose support vector machine (SVM) based hierarchical classification schemes for recognition of hand-written Bangla characters. A comparative study is made among multilayer perceptron, radial basis function network and SVM classifier for this 45 class recognition problem. SVM classifier is found to outperform the other classifiers. A fusion scheme using the three classifiers is proposed which is marginally better than SVM classifier. It is observed that there are groups of characters having similar shapes. These groups are determined in two different ways on the basis of the confusion matrix obtained from SVM classifier. In the former, the groups are disjoint while they are overlapped in the latter. Another grouping scheme is proposed based on the confusionmatrixobtainedfromneural gas algorithm. Groups aredisjointhere.Threedifferenttwo-stagehierarchicallearn-ing architectures (HLAs) are proposed using the three group-ing schemes. An unknown character image is classified into a group in the first stage. The second stage recognizes the class within this group. Performances of the HLA schemes arefoundtobebetterthansinglestageclassificationschemes. The HLA scheme with overlapped groups outperforms the other two HLA schemes.
 Keywords SVM  X  RBF  X  MLP  X  Handwritten character recognition  X  Bangla  X  Fusion  X  Grouping of classes  X  Hierarchical learning architectures 1 Introduction Though optical character recognition (OCR) systems for some Indian scripts are available [ 1 ], there has not been much work on recognition of handwritten Indian scripts. The pres-ent paper deals with recognition of handwritten Bangla char-acters. Bangla is the fifth most popular language in the world and the second most popular language in the Indian subcon-tinent. Bangla script has 50 basic characters (39 consonants and 11 vowels). There are more than 300 Bangla compound characters along with vowel modifiers. The present study deals with Bangla basic characters only.

Most of the handwritten character recognition problems are complex and deal with a large number of classes. A lot of research effort has been made in this direction for several scripts [ 2 , 3 ] and has been applied successfully to various real life applications such as postal automation, bank check verification, etc. [ 4 , 5 ]. Multilayer perceptrons (MLP) and hidden Markov models (HMM) have been used for classi-fication purpose [ 6 , 7 ]. Support vector machine (SVM) has not yet been used much in handwritten character recogni-tion problems. Dong et al. [ 8 ] applied SVM classifier to im-prove the performance of a handwritten Chinese character recognition system. Camastra [ 9 ] applies SVM for English handwriting recognition. Liu et al. [ 10 ] have evaluated the performance of several classifiers for handwritten numeral recognition. In [ 11 ], G X nter and Bunke combine three HMM classifiers for English handwritten word recognition. Bellili et al. [ 12 ] combine MLP and SVM classifiers for handwrit-ten digit recognition. This hybrid architecture is based on the idea that the correct digit class mostly belongs to the two maximum MLP outputs. The classification algorithms can be separated into two main categories. Discriminative approaches try to find the better separation among all clas-ses. But, in general, they cannot deal with outliers. Besides, model-based approaches make the outlier detection possi-ble but are not sufficiently discriminative. Observing these characteristics, Milgram et al. [ 13 ] proposed a combination of model-based approach with support vector classifiers in a two-stage classification system. A number of other schemes with combination of different classifiers are available in the literature for this purpose [ 14 , 15 ].

There have been only a few studies in recognition of hand-written Bangla characters [ 16  X  18 ]. In [ 16 ], a multistage rec-ognition system is applied to a small database collected in laboratory environments. Bhowmik et al. [ 17 ] proposed an MLPbasedrecognitionschemeusingstrokefeaturesforBan-gla basic characters. Bhattacharya et al. [ 18 ] also designed a two-stage MLP based recognition system using shape based features. These two studies are based on large databases. In the present study, we apply MLP, radial basis function (RBF) and SVM classifiers to handwritten Bangla character recog-nition problem and compare their performances in terms of classification accuracy. The entire handwritten Bangla char-acter database is partitioned into training, validation and test sets. The validation set is used to determine several parame-ters of the classifiers. A majority voting fusion scheme is used for this problem on the basis of MLP, RBF and SVM classi-fiers. From the confusion matrices (on the basis of validation set) of different classifiers, it has been observed that there are groups of classes within each of which the misclassifi-cation rate is larger compared to the misclassification rate between such groups. Based on this observation, groups are identified using certain techniques. A hierarchical learning architecture (HLA) for handwritten Bangla character recog-nition is designed on the basis of these groups. In the first stage of HLA, the group for an unknown sample is identified and the following stage recognizes the sample into a class within this group. Three such different HLA schemes based on three different grouping algorithms are proposed.
In the present paper, we intend to explore how these clas-sifiers compare with respect to their classification accuracies in a large class problem like the present one. In order to deal with a large number of classes, a two-stage classifica-tion architecture is proposed so that each stage involves a smaller number of classes. An SVM classifier is basically meant for two-class problems. However, there exist methods to use SVM classifiers for larger number of classes. One-versus-all (OVA) and one-versus-one (OVO) are two such methods. OVO gives better classification accuracy than OVA [ 19 ]. But OVO is applicable when the number classes is smaller. Thus, a two-stage classification architecture is suit-able for OVO to achieve better accuracy.

The main purpose of the present paper is to design a hier-archical classification architecture with SVM classifiers in order to achieve better accuracy. For a hierarchical classifier, formation of groups of similar classes is necessary. Forma-tion of groups are normally done on an ad hoc manner [ 18 ]. To the best of the authors X  knowledge, no formal methodol-ogy for forming such groups is available in the literature. We develop here several formal grouping schemes on the basis of the confusion matrix produced by supervised or unsuper-vised classification. Such a hierarchical classifier with such grouping schemes can be useful for any large class recogni-tion problem.

The performance of a recognition system depends on the features being used for the classifiers. Different kinds of fea-tures have been proposed and their performances on standard databases have been reported [ 6 ]. The performance of recog-nition systems can still be improved by choosing better fea-tures. One of the popular and efficient features, namely, the wavelet features, has been used in many handwritten charac-ter recognition problems. Bhattacharya et al. [ 20 , 21 ] applied thesefeatures for handwrittenBanglaandDevnagari numeral recognition and reported satisfactory results. We explore how effective these features are for the present recognition prob-lem where the number of classes is large. 2 Feature extraction The wavelet transform [ 22 ] is a well-known tool that finds application in many areas including image processing. Due to the multi-resolution property, it decomposes the signal at different scales. For a given image, the wavelet transform produces one low frequency subband image reflecting an approximation of the original image and three high frequency components of the image reflecting the detail. The approxi-mation component is used here to generate the feature vector in the present recognition problem. In our experiment, we considerDaubechieswavelettransformwithfourcoefficients ( l , l another four coefficients ( h 0 , h 1 , h 2 , h 3 ) forming the high-pass filter where l and h 0 = l 3 , h 1 = X  l 2 , h 2 = l 1 , h 3 = X  l 0 . For an input image, we first calculate the bounding box of the image, then normalize it to a square image of size 64  X  64 with aninterpolationtechnique.Waveletdecompositionalgorithm with the above lowpass and highpass filters is applied to this normalized image twice (row-wise and column-wise) to get four 32  X  32 images that are shown in the third row of Fig. 1 . Its left-most image is obtained by using the lowpass filter twice and is considered for the recognition task. From this image, four 16  X  16 images are obtained in a similar fashion and are shown in the fourth row of Fig. 1 . Its left-most image is also considered for the recognition task. The binarized ver-sions of these two images are used as the input feature vector for the classifiers. In Fig. 1 , the image of Bangla character  X  X  X , its 64  X  64 normalized form and the components produced by Wavelet transform are shown. 3 Classifiers We will now briefly discuss here the classifiers that we use to discriminate between the classes on the basis of the feature vector described above. 3.1 Neural networks The neural networks (NNs) have been successfully used in various pattern classification applications [ 23 ]. Here, we use two different NN models, namely, multilayer perceptron (MLP) and radial basis function (RBF) networks. Use of MLP has become quite popular in handwriting recognition [ 20 ] because when a pattern has a lot of variations due to handwriting style, the MLP is quite effective for the recogni-tion task. The well-known back-propagation (BP) algorithm is used to train the MLP. For the present task, we use the modified BP algorithm [ 24 ] using self-adaptive learning rate values.

The RBF network is another model of neural network commonly used in pattern classification problems [ 23 ]. In the RBF network architecture, we use the Gaussian acti-vation function as the basis function [ 23 ]. Each output of the network is augmented by a sigmoid function. An unsu-pervised learning method is applied for the hidden units to estimate the initial basis function parameters. The Gradi-ent Descent learning method is applied to tune the network weights as well as thebasis functionparameters duringsuper-vised learning. 3.2 Support vector machine The SVM [ 25 , 26 ] is a machine learning method basically used for two-class pattern recognition problems. Suppose a training data set D consists of pairs { ( x i , y i ), 1 n } where input vectors are x y  X  X  X  1 , + 1 } corresponds to two classes. The SVM max-imizes the cost function 1 2 w T w subject to the constraints x  X  w + b  X + 1for y i =+ 1 and x i  X  w + b  X  X  X  1for y i = X  1, where w is the weight vector and b is the bias. When the train-ingpointsarenotlinearlyseparable,thecostfunctionisrefor-mulated by introducing slack variables  X  i  X  0; i = 1 ,..., The SVM now finds w that minimizes 1 2 w T w + C n i = 1  X  subject to x i  X  w + b  X + 1  X   X  i for y i =+ 1, x i . w +  X  1 +  X  i for y i = X  1 and  X  i  X  0 ,  X  i . The constant C is a reg-ularization parameter. When the decision function is non-lin-ear, the above scheme cannot be used directly. The SVM then evaluates a function  X  : R p  X  X  X  H to map the training data from R p to a higher dimensional feature space H . Since in H , the data may be linearly separable, the linear formulation can be applied to these data. A kernel K ( x , x i ) =  X ( is used to construct the optimal hyperplane in H without con-sidering it explicitly. The commonly used kernels are: Function (RBF): K ( x , x i ) = exp (  X   X  x  X  x i 2 ) for
We use both linear and nonlinear SVMs. The RBF kernel with different  X  values and polynomial kernel with different degrees ( d ) are used for nonlinear SVMs. We use SV M light [ 27 ] software for learning the SVM classifier. 3.3 Support vector machine for multiclass problems Multiclass SVMs are realized by combining several two-class SVMs. Two popular methods, namely, one-ver-sus-one (OVO) [ 28 ] and one-versus-all (OVA) [ 25 ] are used for this combination. In OVO method, c ( c  X  1 )/ 2 binary classifiers are constructed for a c -class problem. The binary classifier C ij , i &lt; j is trained using samples from class i and class j containing positive and negative samples respectively. The max-wins voting (MWV_SVM) [ 28 ]is used to classify an unknown sample. In MWV_SVM scheme if the decision function value for an unknown sample x is greater than or equal to 0 from classifier C ij then the vote for class i is increased by one. Otherwise, the vote for class j is increased by one. The sample x is assigned to class k where class k has the largest number of votes among all c ( c  X  classifiers. The OVA method on the other hand needs c bi-nary classifiers for a c -class problem. The i th binary classifier constructs a decision boundary between class i and the other c  X  1 classes. The winner-takes-all strategy (WTA_SVM) [ 25 ] is used to assign the class label to an unknown sample x . WTA_SVM strategy assigns x to the class having the larg-est value of the decision function from all c binary classifiers, even when all decision function values ( d i , i = 1 ,..., negative.

Both the methods OVO and OVA have some disadvan-tages. For a large class ( c -class, say) problem the OVO methodconstructs c ( c  X  1 )/ 2 binaryclassifiers,whereasOVA needs only c binary classifiers. So, OVA incurs less overhead. Yet, in general, the OVO method gives better classification accuracy than the OVA method for large class problems [ 19 ]. 4 Classification schemes The classifiers described in Sect. 3 maybeusedindifferent ways for a pattern classification problem. Classifiers can be used separately or may be combined to form a fused clas-sifier. A hierarchical architecture with the same or different classifiers may also be used. The following subsections detail the schemes adopted for the present problem. 4.1 Single stage classification scheme Three pattern recognition tools, namely, MLP, RBF and SVM are used for classification. In the single stage classification scheme, all the classifiers are employed separately. 4.2 Single stage classification with fusion scheme It is possible that different classifiers may have different sep-arating hyper-surface in the feature space. Thus, a natural option is to fuse the three classifiers, namely, MLP, RBF and SVM classifiers so that the resulting fusion classifier captures the discriminating characteristics of all these classifiers.
The fused classifier is implemented based on majority vot-ing. The principle of majority voting is that if at least two of the classifiers classify x , an unknown input vector, in class C then it is finally classified into C (note, we consider here three classifiers). However, a tie may occur where the three classifiers classify x into three different classes. For a large number of classes, the possibility of a tie increases which degrades the recognition accuracy. To remedy this situation, several methods have been suggested. One such method is to break the tie by choosing the decision of any arbitrary classifier. This method is abbreviated as the FMRS indicat-ing fusion of MLP, RBF and SVM classifiers. Another way is to rely on the decision of a particular classifier to break the tie. We term the schemes as FRMS_M, FRMS_R and FRMS_S when the ties are broken using the MLP, RBF and SVM classifiers respectively. 4.3 Hierarchical learning architectures (HLAs) For a large class problem, it is possible to find some groups of classes within each of which the misclassification rate is high compared to the misclassification rate between such groups. On the basis of this observation, several classes are merged to form groups. These groups however may or may not be dis-joint. A classification scheme based on the groups is termed as a hierarchical learning architecture (HLA). Here, we use a two stage classification scheme. The first stage identifies the correct group of an unknown sample. The second stage rec-ognizes the sample as a member of a particular class in that group. The most important aspect of HLA is to determine the groups and the classes they contain. We first propose two grouping schemes to build disjoint groups and overlapped groups, on the basis of the confusion matrix obtained from any supervised classification. Camastra [ 9 ] used Neural Gas method to identify the similar structures in the input patterns. We develop a third grouping scheme using the NG method. Finally the HLA is constructed using all the three grouping schemes. 5 Grouping schemes We describe here the grouping schemes used to identify the groups of similar characters. The proposed methods intend to merge the classes in such a way that minimizes the between group misclassification rate. The groups obtained from these schemes may be disjoint or overlapped. Sections 5.1 and 5.2 describe the first two grouping schemes. The grouping scheme based on NG method is described in Sect. 5.3 . 5.1 Disjoint grouping scheme Let the confusion matrix obtained from a supervised classi-fier with c classes be (( a ij )) where a ij ,( i , j = 1 the number of samples belonging to class i that are classified as class j . We define similarity between classes i and j as n groups of classes as follows. Suppose two groups G p and G q (having m and n classes, respectively) are represented by between G p and G q , ( p &lt; q ) is defined as: s Note that grouping done on the basis of this similarity mea-sure has resemblance to complete linkage clustering [ 29 ]. If operator  X  X in X  is replaced by operator  X  X ax X , the resultant grouping has resemblance to single linkage clustering [ 29 ]. In many cases, the latter operator leads to a situation where one group contains a large number of classes compared to other groups. This is the reason why the similarity measure defined in Eq. 1 is used here.

Initially,eachindividualclassisagroup(i.e., c classesrep-resenting c groups). First, two classes i and j are found such that their similarity value n ij is maximum among all pairs of classes. They are merged into one group X  X esulting in c  X  1 groups. Subsequently, similarity values between groups are computed and two most similar groups are merged into one. The process continues until we reach a stage when all the similarity values between groups are less than or equal to a certain non-negative threshold value thr .

Notethat usingtheabovealgorithm, it is not possibletoget less number of groups than what is achieved with thr = 0. However, it will be possible to get fewer groups if the opera-tor  X  X in X  in Eq. 1 is replaced by  X  j th min X , ( j = 2 , 3
As an illustration, consider the confusion matrix (( a ij in Table. 1 with 6 classes.

Let thr = 0. Note that n 34 = 35 is maximum among all n ij and the classes 3 , 4 are merged into one group in the first iteration resulting in 5 groups, namely, { ( 1 ), ( ( 5 ), ( 6 ) } . The resulting groups are renamed as { ( 1 ), ( ( 4 ), ( 5 ) } . The similarity matrix of these groups is shown in Table 2 a. Note, according to the definition of similarity (Eq. 1 ), the lower triangle and the main diagonal of the sim-ilarity matrix are not defined. After first merging we obtain s and 2 (Table 2 b). Accordingly, in the next two iterations we form groups with ( 3 , 4 ) and ( 1 , 2 ) respectively. The similar-ity matrices after these two merges are shown in Table 2 c,d. Note, now s 12 = 0 (  X  thr ) . Hence no further merging is per-formed with the groups. Thus in terms of the original classes we obtain the groups as { ( 1 , 2 , 3 , 4 ) and ( 5 , 6 ) } idea is that two groups are not merged if there is a pair of classes (across these groups) which are very dissimilar. The classes 4 and 6 form such a pair here. Note that if thr = the algorithm terminates earlier and outputs more number of groups, namely, { ( 1 , 2 ), ( 3 , 4 ), ( 5 , 6 ) } . 5.2 Overlapped grouping scheme In the grouping algorithm described above, we have used both the rows and columns of the confusion matrix to deter-mine a group. We now consider only a column for forming a group. For each column, we select the best (i.e., smallest) subset of classes (i.e., with size less than c ) that incurs an error rate or less. In this classification scheme, there are c classes or groups in the first stage. In the second stage, indi-vidual groups will have varying number of classes. Note that samples from one character class may belong to more than one group. This is why we call the groups overlapped groups. Note that in the earlier scheme, samples from one character class belong to exactly one group.
 Consider the confusion matrix (( a ij )) defined in Sect. 5.1 . Suppose the target error rate is .Let N j = c i = 1 a ij and thr j be an integer such that a ever, for the discrete nature of a ij , the equality may not hold for any thr j and, therefore, we take the largest value of thr j such that a G ple, the fourth column in the confusion matrix in Table. 1 is [ 51167630 ] where N 4 = 101. Suppose = 0 . 05. Note that a thr 4 = 4 and G 4 ={ 1 , 3 , 4 } .

For a c class problem, we construct c groups in the first stage and for each such group, a separate classifier is designed. Suppose a sample x is classified in class k in the first stage. Then the sample is fed to the classifier for G in the second stage. The final class is assigned to sample x by the second stage classifier. In this scheme, if a sample is misclassified in the first stage, it may sometimes be cor-rectly classified in the second stage. This is not possible in the disjoint grouping scheme. 5.3 Grouping scheme with neural gas Camastra [ 9 ] proposed a grouping scheme using the Neu-ral Gas (NG) method. Camastra used NG to verify whether the uppercase and the lowercase letters of English can be merged into a group or not [ 9 ]. We use NG here for auto-matic grouping of classes. The following subsections give a brief overview of the NG and the NG based grouping scheme. 5.3.1 The neural gas Vector quantization methods encode a set of data points in n -dimensional space with a smaller set of neurons described by their weight vectors W k , k = 1 ,..., N . Neural Gas (NG) [ 30 ] is a vector quantization technique with soft competi-tion between the neurons. In each training step, the squared Euclidean distances d 2 ik = x i  X  W k 2 between an input vector x i and all neurons W k are computed. The vector of these distances is d .The N distances are now given ranks r ( d ) = 0 ,..., N  X  1 according to the ascending order of distances. The learning rule is W The function h  X  ( r k ( d )) = e (  X  r / X ) is a monotonically decreasing function of the ranking. The width of h (  X  ) is deter-mined by the neighborhood range  X  . The learning rule is also affected by a global learning rate  X  . The values of and  X  decrease exponentially from an initial positive value ( X ( 0 ),  X ( 0 )) to a smaller final positive value ( X ( T ),  X ( according to  X ( t ) =  X ( 0 ) [  X ( T )/ X ( 0 ) ] ( t / T ) (3) and  X ( t ) =  X ( 0 ) [  X ( T )/ X ( 0 ) ] ( t / T ) (4) where t is the time step and T the total number of training steps. 5.3.2 Group identification from NG Suppose, after completion of the algorithm, the Voronoi region V k ( k = 1 ,..., N ) corresponding to the k th neu-ron, denotes the set of input vectors that are closest to the weight vector W k . Clearly, the union of all V k  X  X  constitutes the whole dataset. Now, if the underlaying classes in the data-set are quite apart from each other in the feature space, each V k is expected to contain samples from a single class. How-ever, in problems like the present one, this is not the case and there will be some V k  X  X  that will contain samples from more than one class. Now if two classes i and j have sim-ilarity, there will be neurons whose Voronoi region V k will contain samples from both the classes i and j .TheVoro-noi regions that have samples from only one class are called pure and others impure . Thus, a study of the composition of impure Voronoi regions can determine which classes are similar and hence should form a group. Based on this obser-vation, a grouping mechanism is formalized as follows. Sup-pose an impure Voronoi region contains m classes, namely, C , C Let q jk be min ( p j , p k ) where j &lt; k .Let (( n ij upper triangular similarity matrix where n ij is the sum of all q ij over all impure Voronoi regions. Here n ij represents sim-ilarity between classes i and j as in Sect. 5.1 . The similarity s pq between groups of classes is also defined in the same way. Finally, the formation of the groups of classes based on n ij and s pq is done using the algorithm in Sect. 5.1 .The difference between the grouping schemes in Sects. 5.1 and 5.3 is that the former is based on the results of a supervised classification while the latter makes use of the results of an unsupervised classification.

Note that the groups formed by the NG based method are always disjoint. The similarity matrix obtained by this method is upper triangular and hence formation of over-lapped groups on the basis of the columns cannot be con-sidered as in Sect. 5.2 . 6 Results and discussions In this section, we present the results obtained by different schemes described so far. In addition, we compare the per-formances of the single stage classification schemes with the proposed HLA schemes. In the single stage classifica-tion schemes, we have used binary representation of 16  X  16 decomposed image as the feature vector. The HLA schemes consist of two stages as discussed earlier in Sect. 4.3 .Inthe first stage, the binary representation of 16  X  16 decomposed image is used as the feature vector while the binary represen-tation of 32  X  32 decomposed image is used in the second stage. For all schemes, the parameters of the classifiers are determined on the basis of the validation sets. 6.1 Bangla handwritten database Experiments have been carried out on a moderately large database of basic handwritten Bangla isolated characters. The database has been collected from different sections (like different age groups, different educational levels, etc.) of the population in and around Kolkata, India, and is a representative of a wide spectrum of handwriting styles. Bangla character set consists of 11 vowels and 39 conso-nants. However, two consonant characters have nearly the same shape (Column 1 in Fig. 2 ) and each pair of characters in the other four columns have the same shape except the presence of a dot at the bottom of the character.
Hence, we consider the handwritten Bangla character rec-ognition problem as a 45 class problem here. The entire data-base has been divided into three sets, namely, training set, validation set and test set. The number of samples in each set is shown in Table 3 . The size of the whole database is 27,000.

Some handwritten samples from the database along with the class references are shown in Fig. 3 . 6.2 Performance of single stage classification scheme Three pattern recognition tools, namely, MLP, RBF and SVM are used for classification. The number of hidden nodes for MLP and RBF are chosen based on trial and error method on the validation set. The optimal number of hidden nodes for MLP is found to be 200 and for RBF to be 260. The OVA method is used to train 45 SVM classifiers. While using SVM, linear kernel, RBF kernel and polynomial kernel are used. The hyper parameter  X  = 0 . 10 for RBF kernel and degree d = 3 for polynomial kernel have resulted in the min-imum validation error in the present problem. The recogni-tion accuracies of SVM classifiers on the test set are 56 using linear kernel, 74 . 21% using RBF kernel (  X  = 0 . 10) and 79 . 47% using polynomial kernel with d = 3. The SVM classifier with polynomial kernel of degree 3 has the maxi-mum validation accuracy and hence is used for the rest of the experiments. Overall test accuracies of MLP, RBF and the SVM classifiers are 71.44%, 74.56% and 79.47%, respec-tively. 6.3 Performance of single stage classification with fusion In Sect. 4 , we have discussed the fusion of several classifiers. We here design a fusion classifier using the MLP, RBF and SVM classifiers. We first make a study on the distribution of correctly and incorrectly classified samples in the validation set, by MLP, RBF and SVM classifiers in the single stage classification scheme (Fig. 4 ).
 Let us now define the following sets: M: Set of character images that are classified correctly by MLP classifier.
 R: Set of character images that are classified correctly by RBF classifier.
 S: Set of character images that are classified correctly by SVM classifier.

D: Set of character images that are classified correctly by both MLP and RBF classifiers, but not by SVM classifier.
E: Set of character images that are classified correctly by both RBF and SVM classifiers, but not by MLP classifier.
F: Set of character images that are classified correctly by both SVM and MLP classifiers, but not by RBF classifier.
X: Set of character images that are classified correctly by only MLP classifier.

Y: Set of character images that are classified correctly by only RBF classifier.

Z: Set of character images that are classified correctly by only SVM classifier.
 U: Validation set.
 Onthebasisofourvalidationset,thesizesofthesetsshownin Fig. 4 are as follows: # ( M ) = 3210 , # ( R ) = 3465 , # 3586 , # ( D ) = 132 , # ( E ) = 418 , # ( F ) = 229 , # ( 124 , # ( Y ) = 190 , # ( Z ) = 214 , # ( M  X  R ) = 2857 , S ) = 3143 , # ( S  X  M ) = 2954 , # ( M  X  R  X  S ) = 2725 , # ( M  X  R  X  S ) = 4032 and # ( M  X  R  X  S ) = 468 . Mathematically, the accuracy of the majority voting scheme always lies within the range ( # [ ( M  X  R  X  S )  X  D  X  E  X  F ] / # ( U ))  X  100 to # (( M  X  R  X  S )/ # ( U ))  X  100 (which is 77.87 X 89.60% in our experiment). In fact, there does not exist any fusion scheme, which exceeds the accuracy of 89.60% on validation set. Earlier, we have pointed out the possibility of a tie and proposed certain methods to deal with it. The present problem has 45 classes, so the occurrence of a tie is not infrequent. We use the four schemes, namely, FMRS, FMRS_M, FMRS_R and FMRS_S to break the tie. The recognition accuracies of the fusion classifier on the val-idation and test sets are shown in Table 4 . From Table 4 it can be seen that FMRS_S gives the best result among all the four schemes. 6.4 Performances of hierarchical learning architectures From the results of the single stage classification schemes, it is clear that the SVM classifier individually gives the best result among the three classifiers under consideration. On the basis of this observation, we choose the SVM classi-fier from now on. So, the results of HLA schemes as well as the comparisons are made with respect to SVM only. Now, we construct a two stage HLA for the problem. The construction of HLA requires grouping of classes. We em-ploy the first two grouping schemes with the confusion ma-trix based on validation set. A third HLA is designed based on the groups obtained using the NG based approach. The following subsections describe the results obtained from the three HLA schemes. 6.4.1 HLA with disjoint groups On the basis of our validation set, the groups obtained by the algorithm described in Sect. 5.1 with thr = 0 are shown in Table 5 .
 With these groups, we construct a two-stage HLA using SVM. Since the performance of OVO method is better than OVA method and the number of classes is not too large in any single stage, we choose OVO method for both the stages of the two-stage HLA. In the first stage of this scheme, all the 13 groups are considered as 13 different classes. Thus, we train SVM classifiers for 13 classes. Here we construct 78 binary classifiers using the OVO method. To classify an unknown sample MWV_SVM scheme is used. In the sec-ond stage, 12 SVM classifiers for the 12 groups are designed (for single element group L, no second stage classifier is needed). The results of applying SVM on the test set in the first and second stages are shown in Table 6 . Note that in this scheme, if a sample is misclassified in the first stage as a character belonging to a group other than its own, then the character will never be classified correctly in the second stage.Theaveragerecognitionaccuracyobtainedontestsetis 85.22%. 6.4.2 HLA with overlapped groups We now identify the overlapped groups using the algorithm proposed in Sect. 5.2 and design the corresponding HLA. The algorithm of Sect. 5.2 is applied after setting the error rate = 0 . 05. We end up getting the groups given in Fig. 5 . It is clear that i th class is present in the i th group. The ticked entries in a row indicate the classes that form the group cor-responding to the row. We design the HLA using overlapped groups and SVM classifier. An average accuracy of 88.13% is obtained on the test set.
 6.4.3 HLA with groups using NG based approach After construction of the matrix (( n ij )) on the basis of the training set, we proceed with the grouping algorithm described in Sect. 5.3 with N = 1000. Using thr = 0, a total of 13 groups are obtained. The groups are different from the earlier groups (Table 5 ) though their number is the same by coincidence. In Table 7 , we list all the groups obtained from the grouping scheme using NG.

The classification scheme is similar to the HLA for dis-joint groups (see Sect. 5.1 ). Test set accuracies for individual groups are shown in Table 8 . The average recognition accu-racy on test set is obtained as 84.05%.
 6.5 Comparative study of different schemes We now compare the performance of the single stage classification scheme with the proposed HLA schemes. The test accuracies for each of 45 classes obtained by the five different classification schemes are given in Fig. 6 . The per-formance of SVM based single stage classification scheme is marked as SSC. The fusion scheme FMRS_S is taken for comparison as it outperforms the other three (Table 4 ). The proposed HLA schemes are marked as HLA_DG and HLA_OG for disjoint group and overlapped group HLA schemes respectively. The NG based HLA is termed as HLA_NG. It is clear from Fig. 6 that the performances of the proposed HLA schemes are better than the single stage classification schemes for most of the classes. For 12 clas-ses, the disjoint group HLA scheme performs better than the other schemes. The performances of the overlapped group HLA scheme are better than the other schemes for 26 clas-ses. The NG based HLA scheme is the best performer for the other 7 classes.

In order that the recognition results are not biased on a particular choice of the training-validation-test set combina-tion, we have randomly created six different combinations of training-validation-test sets and repeated each of the five classification schemes six times for these six combinations (the earlier results were based on one such combination). In Table 9 , the test accuracies (averaged over all six com-binations) for the five schemes are shown. The scheme is mentioned in the first column of the table. In the second column, the average test accuracies are shown. In the SVM based single stage classification scheme, test accuracy lies between 78.31 and 80.58%. Similarly for FMRS_S the accu-racy lies between 79.02 and 82.16%. Classification accuracy for disjoint group HLA lies between 83.80 and 86.80%. For overlapped group HLA, the accuracy is in between 86.47 and 88.38% and for HLA_NG accuracy is in between 82 . 69 and 85 . 56%. 6.6 Comparison of proposed HLA schemes As we have pointed out earlier Bhattacharya et al. [ 18 ] designed a two-stage MLP based recognition system for Bangla basic characters. The features are obtained by com-puting local chain code histograms of the contour of the character images. For comparison, Bhattacharya et al. also computedthesamefeaturesfromtheskeletonofthecharacter images. Different groups of the character classes are made by manual inspection with the confusion matrix obtained from 45 class MLP classifier. Results using the contour based fea-tures show better recognition accuracy. Hence for compara-tive study, we compute features only from the contour. We here take the following comparison strategy. We compute the contour based chain code features from our database and execute the MLP based recognition scheme of Bhattacharya et al. In this course, we adopt the groups as designed by Bhattacharya et al. When working with MLP one needs to specify the number of hidden layers and hidden nodes. Bhat-tacharya et al. defined only the first stage architecture of the MLP. We adopt the given architecture and use 40 nodes arranged in a single hidden layer. However, for the second stage MLP, we determine the number of hidden nodes based on trial and error method on the validation set used in the previous experiments. Here we have obtained 82.27% test accuracy using our database and the scheme of Bhattacharya et al. On the other hand, in a second experiment, we take the same features and execute the MLP based HLA schemes. Us-ing MLP based disjoint group HLA we get 83.42% and with overlapped group HLA we get 85.09% test accuracies. Since, we have the same features and classification tool (MLP) for all the experiments, evidently our automatic grouping schemes outperform the manual grouping of classes by Bhat-tacharya et al. However, with Wavelet features, we have earlier obtained the best results with the SVM based HLA schemes (Sect. 6.5 ). Hence, weperformour experiments with SVM based disjoint group HLA and overlapped group HLA using the contour based chain code features. The recognition accuracy obtained by HLA_DG is 86.07% and the same by HLA_OG is 89.22%. 7 Conclusions and future scope In this paper, SVM based hierarchical learning architecture (HLA) schemes for Bangla handwritten character recogni-tion have been proposed. A comparative study among MLP networks, RBF networks and SVMs has been made with respect to classification accuracy. Such a comparative study among different classifiers has not so far been done in the context of handwriting recognition of any Indian script.
Three new schemes for grouping similar classes have been developed on the basis of the confusion matrix using cer-tain objective criteria. However, to determine the number of groups, user intervention is necessary here. To automatically find the optimal number of groups in terms of classification accuracy, can be an issue of further study. Also, devising an overlapped grouping scheme on the basis of a ij + a ji Sect. 5.1 can be studied.

In the proposed HLA schemes, the features used in both the stages of classification, are essentially the same. Better classification may be achieved if different feature sets are used in different stages. For example, if features based on spectral characteristics are used in one stage, shape based or structural features can be used in the other. In fact, we have obtained better results by using the chain code based feature at both stages. Other features may produce still better results. References
