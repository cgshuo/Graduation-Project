 Frequent patterns provide solutions to datasets that do not have well-structured feature vectors. However, frequent pat-tern mining is non-trivial since the number of unique pat-terns is exponential but many are non-discriminative and correlated. Currently, frequen t pattern mining is performed in two sequential steps: enumerating a set of frequent pat-terns, followed by feature selection. Although many meth-ods have been proposed in the past few years on how to perform each separate step efficiently, there is still limited success in eventually finding highly compact and discrimi-native patterns. The culprit is due to the inherent nature of this widely adopted two-step approach. This paper discusses these problems and proposes a new and different method. It builds a decision tree that partitions the data onto different nodes. Then at each node, it directly discovers a discrimi-native pattern to further divide its examples into purer sub-sets. Since the number of examples towards leaf level is relatively small, the new approach is able to examine pat-terns with extremely low global support that could not be enumerated on the whole dataset by the two-step method. The discovered feature vectors are more accurate on some of the most difficult graph as well as frequent itemset problems than most recently proposed algorithms but the total size is typically 50% or more smaller. Importantly, the minimum support of some discriminative patterns can be extremely low (e.g. 0 . 03%). In order to enumerate these low support patterns, state-of-the-art frequent pattern algorithm either cannot finish due to huge memory consumption or have to enumerate 10 1 to 10 3 times more patterns before they can even be found. Software and datasets are available by con-tacting the author.
 H.2.8 [ Database Management ]: Database Applications-Data Mining Algorithms
Many real-world data mining problems have no pre-defined feature vectors that can be given to data mining algorithms to construct predictive models. Facing these challenges, frequent-patterns (e.g., frequent itemsets [3, 13], graph min-ing [17, 25] and sequential pattern mining [4, 21], etc) have been proposed and actively studied as candidate feature sets. These methods look for statistically significant struc-tures hidden in  X  X aw X  data. The main challenge and re-search interest for frequent pattern mining is how to dis-cover those discriminative and essential patterns efficiently. It has been proved that frequent-pattern enumeration is NP-complete [26]. For some graph mining problems, when the relative support is 5%, the number of mined closed sub-graphs (obvious redundancy excluded), can be up to  X  10 7 However, importantly, most discovered patterns either do not carry much information gain or are correlated in their predictability. On the other hand, if the support is set too low (such as &lt; 3%), the program simply may not finish since the virtual memory can be exhausted.

State-of-the-art frequent pattern mining algorithms [9, 5] employ a batch process, which first enumerates features above the given support and then performs feature selection on this initial pool, as shown in Figure 1 (a). Research inter-ests in this area focus on increasing the effectiveness of each of these steps, for example, on how to efficiently enumerate those unique and non-overlapping patterns, how to prune the search tree to avoid enumerating patterns that are un-likely to carry much information gain, how to ensure that each example is covered by sufficient number of patterns, etc. These methods are important improvements, however, there is still limited success in eventually finding those small set of discriminative features, and this is due to inherent problems of the batch process as discussed below.

First, the number of candidate patterns can still be too large (  X  X  10 6 ) for effective feature selection at the second
Figure 2: Information Gain in Different Subspace step. Second, if the frequency of discriminative features is below the support value chosen to enumerate the can-didates, those features won X  X  even be considered. Third, the discriminative power of each pattern is directly evalu-ated against the complete dataset, but not on some sub-set of examples that the other chosen patterns do not pre-dict well. As demonstrated by a synthetic example using Gaussian mixture model in Figure 2, a feature not quite predictive on the complete dataset, can actually be quite informative on a subspace. On the complete data space, the solid line has high information gain since it can almost clearly separate the two classes of data, while the dashed line is not quite useful. However, after the solid line sepa-rates the two classes, the dashed line can make the divided subspaces  X  X urer X , such that both region 1 and 3 contain only one class of examples. In the batch mode, patterns like the  X  X ashed line X  is unlikely to be chosen by feature selection since the criteria usually operates on the complete dataset. Fourth, the correlation among multiple features are not di-(a) Uncorrelated patterns and their decision boundaries Figure 3: Uncorrelated Patterns = higher accuracy rectly evaluated on their joint predictability. Assume that a feature which can correctly predict some examples is al-ready selected, to improve the overall accuracy, the features considered subsequently need to predict better on those ex-amples or subspaces of the dataset that the chosen feature cannot predict correctly. However, the batch approach does not address this directly, but prefers features that are un-correlated, either using covariance-based or coverage-based criteria. When two features are uncorrelated, they may not necessarily help each other to cover examples that each of them does not predict well by itself. Figure 3 (a) shows a dataset with two classes and two patterns  X  and  X  occurring in the data. Red circle represents the occurrence of  X  in cer-tain examples and blue triangle represents the occurrence of  X  in some others. The red dashed line is the decision boundary which separates the examples based on whether they contain pattern  X  or not, while the blue dotted line is the decision boundary separating the examples based on the occurrences of pattern  X  . The optimal boundary is repre-sented by the solid line which could not be derived solely based on  X  or  X  . Figure 3 (b) shows the decision boundary (in dotted dashed line) when both patterns are considered simultaneously. Although  X  and  X  are uncorrelated as they appear in disjoint examples, the selection of both at the same step will not necessarily help improve the accuracy. On the other hand, given that one feature or a number of features is already chosen, the next ideal feature should be the one that can cover the subset of examples where cur-rently mined patterns do not cover well. Importantly, this feature can be correlated with chosen features on all other examples. The bottom line is that correlation criteria may not be a direct heuristic to look for predictive features. One should purposefully look for features that can correctly cover subspaces where currently discovered features cannot.
To solve the inherent problems of the batch process and find small set of highly discriminative patterns, we propose a divide-and-conquer based approach to directly mine discrim-inative patterns as features v ectors. As shown in Figure 1 (b), the basic flow of the proposed algorithm proceeds by constructing a  X  X odel-based X  search tree. The concept of this search tree is quite different from traditional frequent pattern-based search tree where each node normally denotes a sub-item and a path in the tree is a frequent pattern. In the model-based search tree, each node maintains a discov-ered pattern as the discriminative feature. Examples in the original dataset are sorted and partitioned down the tree. As each node being expanded, a frequent-pattern algorithm is invoked only on the examples that the node is responsi-ble of. The frequent pattern with the highest information gain is chosen as the feature and maintained at the current node. Then based on the containment rule, the examples at the given node are partitioned into two disjoint subsets. The search and tree construction terminates when 1) either every example in the given node belongs to the same class or 2) the number of examples is less than a given threshold.
Algorithm 1 presents the recursive method that builds the model-based search tree. The basic idea is to partition the data in a top-down manner and construct the tree using the best feature at each step. It starts with the whole data set and mines a set of frequent patterns from the data. The best pattern is selected accord ing to some criterion and used to divide the data set into two subsets, one containing this pattern and the other not. The mining and pattern selection procedure is repeated on each of the subsets until the subset is small enough or the examples in the subset have the same class label. After the algorithm completes, a small set of informative features are uncovered and the corresponding model-based search t ree is constructed.
 Algorithm 1 Build Model-based Search Tree Input: 1: A set of examples D from which Output: 1: A selected set of features, F s 1: Call the frequent pattern algorithm, which returns 2: Evaluate the fitness of each pattern  X   X  X P ; 3: Choose the best pattern  X  m as the feature; 4:
F s = F s  X  X   X  m } ; 5: Maintain pattern  X  m as the testing feature 5:
D L = subset of examples in D containing  X  m ; 6:
D R = D X  X  L ; 7: for  X  X  L, R } 8: if |D | X  m or examples in D have the same class 9: label, make T a leaf node; 10: else 11: recursivel yconstruct T with D and p ; 12: return F s and T
The scale of patterns returned by frequent pattern algo-rithm can be formulated by O scaled size of the dataset ( &gt; 3) and p is the support in per-centage. The two constants c 1 and c 2 depend on both the dataset and the particular frequent pattern algorithm, and can be factored out. Thus, for simplicity, the scale of the problem is approximately O too low and the dataset is big, the number of patterns can be explosively large. Next, we look at how the proposed divide and conquer-based approach can significantly reduce the scale of the problem.

In the divide-conquer algorithm, without loss of general-ity, let us assume equal split. In fact, according to complex-ity analysis, unequal split will have the same big O result as that of the equal split and the difference is only in the constants. Then the number of patterns mined at each node of the tree (both leaf and non-leaf) can be expressed by the following recursive function: (1) T ( s )= s s (1  X  p ) +2 T ( s/ 2) The upper bound of the number of frequent patterns ever enumerated and considered by the recursive method is shown in the following theorem.

Theorem 1. For a problem of size s and support p ,the recursive algorithm enumerates O ( s s (1  X  p ) ) number of pat-terns in total during the tree construction process.
Proof. For a general recurrence problem: T ( n )= aT ( n/b f ( n ), where a  X  1and b&gt; 1 are constants and f ( n )isan asymptotically positive function, the Master Theorem pro-vides the solution to such problems. Specifically, in one of the three cases, if f ( n )= X ( n log b a + )forsomeconstant &gt; 0, and if af ( n/b )  X  cf ( n )forsomeconstant c&lt; all sufficiently large n ,then T ( n )= X ( f ( n )). For our problem, we show that it satisfies the conditions. First, s s (1  X  p ) is an asymptotically positive function and 2 are both positive integers. Second, the problem has limited size, so s is bounded by an integer M .Let = M (1  X  p )  X  which is greater than 0, then it is evident that s s (1  X  p )  X ( we need to show that which is equivalent to Let c =1 / 2, the above inequality is true since s&gt; 3and p is usually a small number so that s (1  X  p )  X  3 / 2, thus (2 s ) s (1  X  p )  X  8. Since this case applies to our problem, we could make the conclusion that the recursive algorithm con-siders O ( s s (1  X  p ) ) number of patterns.

It is worth noting that in the recursive algorithm, the support p is the support at each node, i.e., support is cal-culated among all records falling into the node. Actually, a pattern with support p at a node will have a global support p , which is much smaller than p . For example, assume that theleafnodesizeis10and p = 20%. For a problem size n = 10000, the normalized support in the complete dataset is 10  X  20% / 10000 = 0 . 02%.

To find such patterns, the traditional pattern mining al-gorithms will return an explosive number of patterns or fail due to resource constraints, since it will generate O ( s s (1  X  p ) patterns, which is a huge number. Suppose p is close to 0, then 1  X  p 1 and pattern mining algorithms could ob-tain up to s s patterns. However, the recursive algorithm could identify such patterns without considering every pat-tern, thus will not generate explosive number of patterns. According to Theorem 1, the upper bound of the number of patterns is s s (1  X  p ) . Comparing with traditional pattern mining approaches, the  X  X cale down X  ratio of the pattern numbers will be up to This demonstrates that the proposed recursive algorithm could get over the barrier of explosive growth of frequent pat-terns and successfully identify discriminative patterns with very small support.
Now we consider a different problem, the upper bound on the number of discriminative features returned by the recursive method. In the worst case, the tree is complete and every leaf node has exactly m examples, thus the upper bound is (3) O (2 log m ( s )  X  1  X  1) &lt;O ( s/ 2  X  1) = O ( s )= since m&gt; 2 and scaled problem size is exactly the number of examples n for the model-based search tree.
Assume we have a two-class problem with C = { 0 , 1 } and a pattern  X  . Examples with class label 1 are called positive examples and examples from class 0 are negatives. Let C 0 and C 1 be the number of negative and positive examples respectively; P 0 and P 1 be the number of occurrences of the pattern  X  among negative and positive class examples respectively. Let x denote an example from either negative or positive class, then by definition, the information gain of the pattern  X  is measured as
IG ( C| X )= H ( C )  X  H ( C| X ) =  X  where P ( c =1 | x =1)= P 1 P 0 + P 1 , P ( c =0 | x =1)= P ( inition are the following two proportion, P 0 C 0 and P 1 more difference between these two terms, the higher is the information gain.

Now consider only a subset of examples are selected from the original set. Further, assume C 0 and C 1 are the number of negative and positive examples respectively; P 0 and P be the number of occurrences of  X  in negative and positive class respectively, thus C i &lt;C i and P i &lt;P i , i  X  X 
In the original data set, if the relative frequency of  X  in the positive and negative classes is very close, i.e., P 0 then  X  is not discriminative and the information gain of  X  low, and should not be chosen. However, in the data subset with some examples eliminated, the relative frequency of  X  in the positive and negative class could be different, thus making P 0 C 0 P 1 C 1 or vice versa. In such cases, the infor-mation gain of  X  in the subset could increase substantially compared to the original dataset. Thus,  X  wouldhelpto distinguish the examples in the subset. Considering multi-ple patterns, unless the examples are being removed equally in the same portion for every pattern, the information gain for some patterns ought to increase. In this sense, we say the information of a pattern  X  is data context sensitive ,and it is incorrect to conclude that less frequent patterns have no information gain.

Another merit of our approach is that, as we select the feature and partition the dataset recursively, the number of features decreases, thus the conditional probability of select-ing a discriminative feature increases.
The proposed approach does not overfit and this can be shown in the following discussion as well as experiments. First, various generalization bounds on decision tree is only related to the number of training examples, the depth of the tree and the number of examples at the leaf node (the parameter m in our case), and, importantly, is unrelated to the feature vector. In particular, the bound on balanced trees is the smallest, and M b T is a balanced tree. So the key factor to avoid overfitting is the choice of m . It should not be set too small, such as 1 or 2, like traditional decision tree learning. Second, the support of feature is independent from overfitting. Frequent pattern is containment-based feature and its value is either 0 or 1. A high support pattern has most examples with value 1, while a low support pattern has most 0 values, and this is symmetrical for a classifier. On the other hand, low support features are important. Assuming that the probability of the positive class is 1%. In order to find just one single pattern to separate the two classes as much as possible, the support of this pattern ought to be either close to 99% or 1%. This is trivially true according to pigeon hole principle. In other words, the support of mined pattern is unrelated to generalization.
We study the optimality of M b T under exhaustive search conditions. Assuming that we were able to enumerate all features apriori and use M b T as a  X  X eature selection X  algo-rithm, we show below that the set of features chosen by M is still the best set of features.

Under the ideal situation of  X  X xhaustive enumeration X , one would consider M b T as a feature selection algorithm since all features are given apriori . Then, the benchmark for com-parison would be  X  X eature selection algorithm. X  Importantly, one would be interested in comparing the quality of  X  X elected features X  by M b T with those by bench mark feature selec-tion algorithms. Among the large family of feature selection algorithms, the one that is comparable, is a forward-based feature selection with decision tree or fDT. Assume that the large set of candidate features is { f 1 ,f 2 ,...,f N } ,boththe forward-based feature selection fDT and the proposed algo-rithm M b T selects some K features out of N candidates. To be comparable, the number K is determined by M b Twhen it reaches its stopping condition, i.e., (1) a node is pure or all the examples belong to the same class (2) the number of examples is  X  m .

For forward-based feature selection, assume that there are k features chosen thus far. Then, at the next iteration, it chooses one out of the remaining N  X  k features to include with the k features that gives the highest accuracy. For sim-plicity, we assume that the accu racy never decreases before it reaches the parameter K . On the other hand, considering M b T, at each iteration, M b T chooses one feature not tested along a decision path from the root to the current node that gives the maximal accuracy increase for that particular sub-space of examples within the  X  X urrent node. X  Assume that at the end, both M b T and fDT have each chosen K features independently, as follows, we prove that they choose exactly the same set of K features.

At the first step, obviously both algorithms will choose the same feature and build the same single node tree. Assume after some steps, fDT and M b T have constructed exactly the same partial tree. We prove that the next feature chosen by both algorithms will be the same and the resulting trees will be identical. First, neither M b TnorfDTwillreconstruct the current partial tree, but rather expand one leaf node. For M b T, this is true by definition. For fDT, if a new fea-ture would re-construct this partial tree, it would have been chosen previously and already been the testing feature of a non-terminal node of the partial tree. If one would impose that both fDT and M b T follow the same order on which leaf node to expand next, they would obviously choose exactly the same feature and construct the same tree every step along the way. Nonetheless, this order is not important if fDT satisfies the stopping conditions of M b T. In fact, when identical nodes from fDT and M b T get expanded, there is only one unique best feature to choose for both M b Tand fDT. Additionally, a node from both trees does not expand if it satisfies one of the two stopping conditions of M b
The performance of the proposed algorithm is evaluated on both frequent itemsets and graph datasets. We exper-imented on some of the most difficult benchmark datasets used in the community and specifically excluded those eas-ier cases. We compared the model-based search tree with closed pattern mining methods (FPClose for itemset [12] and CloseGraph for subgraph [25]) followed by feature selection, as well as a state-of-art integrated  X  X wo-step X  approach Pat-Class [5] for frequent itemsets mining. At the time of final copy preparation, a heuristic-based method to directly mine frequent itemsets DDPMine [6] is just published. A com-parison on performance is discussed at the end of Section 4. For each dataset, the results reported below are the average of 5-fold CV.
The main concerns on feature discovery algorithms are ef-ficiency and accuracy. For efficiency, it is necessary to find out if the model-based search tree is able to discover the use-ful needles in the haystack in reasonable amount of time and with reasonable amount of memory. Importantly, one ought to know if traditional methods may even be able to find these predictive patterns given reasonable amount of time and memory. Nonetheless, it is useful to find out the num-ber of additional features that traditional algorithms have to produce before even reaching these patterns. These numbers are important measures of  X  X earch quality, X  i.e., blind-search vs. targeted search. Moreover, the size of the returned fea-ture sets is a substantial quality measure. Obviously, in terms of model comprehensibility, a practitioner would pre-fer a small number features. Besides various scalability is-sues, one crucial measure for data mining is the  X  X redictive quality X  of the features returned by the model-based search tree. Ideally, one would like to expect an inductive model constructed by those features more accurate or at least as accurate as state-of-the art methods. To answer the above questions regarding the proposed method, we used some of (c) Minimal supports of itemsets Figure 4: Experiments on Adult; x-axis: Normalized Percentage Support the most difficult benchmark datasets, which are skewed in prior class distribution, and large in scaled problem size in terms of number of examples and feature space. To avoid duplicate and useless patterns, we have employed closed pat-tern algorithms for both frequent itemset [12] and frequent subgraph mining [25].

Scalability to Mine Frequent Itemsets In Table 1, we summarized the number of frequent itemsets, support employed to mine these itemsets, and most importantly, the minimal support among all those frequent itemsets selected by the proposed method. For model-based search tree, this number is not the same as, but significantly smaller than the minimal support used to invoke the algorithm. For no-tational convenience, the proposed algorithm is denoted by M b T, and #pat is the number of patterns or itemsets in this case. Each result column is numbered for convenience.
Among all results, the most interesting numbers to com-pare and observe are: (1) Column 4 vs. Column 5 -the number of patterns returned by calling frequent pattern al-gorithm, as compared to calling the proposed algorithm with the same support. (2) Column 3 vs. Column 6 -the minimal support used to generate M b T as compared to the minimal support of all patterns selected and returned by M b T.
For the first point, obviously the number of patterns se-lected by frequent pattern algorithm is much larger (up to  X  10 3 larger) than the proposed method. In the left bar chart of Table 1, their normalized  X  X og X  scale difference is plotted. We cannot plot it in the original scale since up to 10 3 dif-ference does not demonstrate the result of M b T. Practically, this difference ought to be inte rpreted as the evidence that the proposed method is much more selective in choosing the patterns to expand instead of  X  X lind X  enumeration. This is obviously due to step 5 of Algorithm 1, that employs a fit-ness function to maintain the best pattern  X  at the node, thus split the data into subspaces according to the testing result on  X  . This is theoretically analyzed in Section 2.1, and further emphasized by the  X  X redictive quality X  of these features discussed thereafter in Section 3.1.

For the second point above, evidently, with a much higher input support (such as 75% for Chess), M b T can find fea-tures whose support are up to  X  10 2 lower in value than this invocation parameter (that is 3.3% for Chess). Similarly, their normalized  X  X og X  scale difference is plotted in the right bar chart of Table 1. Practically, this means that M b T, in effect, can  X  X rune X  the search space significantly and avoid enumeration of less promising patterns.

Solving Combinatorial Explosion In Table 1, the number of patterns (column 8) returned by closed frequent pattern mining method using the minimum support among all features selected by M b T is summarized for each dataset. The ratio of this number as compared to the number of fea-tures selected by M b T (column 5) is calculated in column 9. Unequivocally, their difference is at least from  X  10  X  10 6 . Importantly, on Chess, it is impossible for the closed pattern mining algorithm to finish using the minimal sup-port returned by M b T. Due to combinatorial explosion, the program simply  X  X te X  all the memory that the Linux server could allocate, and was then killed by the operating system.
In order to clearly demonstrate the scalability of the pro-posed method in avoiding enumerating huge number of po-tentially useless features, but drilling down quickly to pat-terns with extremely low support, we ran several additional experiments on Adult with varying supports. In Figure 4(a), we compared the number of patterns mined by closed fre-quent itemset algorithm and M b T as the support goes down from 0.95 and 0.05. Clearly, the number of patterns returned by M b T exhibits linear-like growth as a function of 1  X  sup However, for traditional closed pattern mining algorithm, the plot appears to be exponential in shape.

As a different way to demonstrate scalability, Figure 4(b) plots the total number of patterns ever  X  X numerated X  by M b T (including all patterns enumerated at each non-leaf node), as well as the total patterns generated by state-of-the-art closed pattern algorithm using the minimal support of all features selected by M b T. The latter number could have been much larger if we had used the minimal support of all features  X  X numerated X  by M b T. It is evident that the growth on the total number of patterns ever enumerated Table 2: Accuracy on mined itemsets vs. bigger sets by M b T is much smaller than that of closed pattern mining method with the same support.

Yet, the third way to demonstrate the scalability is to examine the variation of the minimal support among all se-lected featured by M b T as the invocation support changes. As plotted in Figure 4(c), when the input support decreases, the minimal support returned by the proposed algorithm quickly goes down to nearly zero (after the invocation sup-port is less than 60%). This explains  X  X he quick convergence to high accuracy X  as discussed below.

Convergence Speed One practical concern is that one does not wish to experiment very low support before the ac-curacy converges, but rather prefer a method that converges fast and at high support. In Figure 4(d), we plotted the changes in accuracy as the input support goes down, com-paring both the proposed method M b T and a decision tree trained from the larger pattern sets returned by traditional closed pattern mining method. It is quite straightforward to see that at high support 70%, the proposed method already reaches promising accuracy, which can only be achieved by traditional approach at support &lt; 10%.
Accuracy of Mined Itemsets Predictive quality of the mined compact feature sets is measured against both (1) much larger feature sets, and (2) those mined feature sets by state-of-the-art  X  X wo-step X  approaches. For the first, the ac-curacy of M b T X  X  feature is compared against the much larger feature sets returned by closed pattern algorithm, both with the same invocation support. Clearly, as summarized in Ta-ble 2, except for hypo, the much smaller feature set (  X  10 to  X  10 3 smaller) can actually produce more accurate model than a much larger feature set and the accuracy increase is up to 8%. This clearly demonstrate both the predictive and comprehensible quality of mined features.

To further justify the predictive quality, we have also com-pared M b T with the most recently proposed closed pattern mining algorithm, PatClass [5]. One important understand-ing is that other feature discovery algorithm can be called as the baseline pattern searching algorithm by M b T. In a way, none of them are competing algorithm for the pro-posed approach, but they can be  X  X lugged X  in together to find even better patterns than the closed frequent itemset algorithm solely used in the experimental study. Nonethe-less, the motivation of this comparison with PatClass is to demonstrate that even M b T calls the  X  X on-feature selective X  (i.e, no feature selection and no pruning of search space) closed pattern mining algorithm, it can still reach or exceed the performance of the best  X  X elective X  two-step approach that we are aware of. The results using SVM as the in-ductive learner, with or without the original feature vector, are summarized in Table 3. As can be clearly shown, their accuracy are quite close. Both feature sets are small, and normally, a bigger feature set incurs higher accuracy due to more expressive power.
Frequent-subgraph based graph mining has been the re-cent active topic to use frequent pattern concept to mine predictive subgraphs as features, thus producing accurate inductive models that can be used in drug design, social network analysis, etc. In normal understanding, frequent-subgraph mining is more difficult than frequent itemsets since the scaled problem size is usually much larger and the graph isomorphism test itself is a non-trivial research prob-lem. Thus, frequent-subgraph mining provides an exciting test bed for the proposed method.

From PubChem project [2], we selected a series of graph datasets with rather skewed distributions. As commonly recognized by the graph mining community, these are some of the most challenging tasks. Each of the NCI anti-cancer screens forms a classification problem, where the class la-bels are either active or inactive. The active class is very rare compared with the inactive class. Another dataset is obtained from the AIDS anti-viral screen program [1]. The screening tests are evaluated in one of the following three categories: confirmed active (CA), confirmed moderately ac-tive (CM) and confirmed inactive (CI). Both CA and CM classes are extremely rare compared with CI. Two classifica-tion problems are formulated out of this dataset. The first problem is designed to classify between CM+CA and CI, denoted as H1; the second between CA and CI, denoted as H2. The characteristic of each graph dataset is described in columns 1-3 of Table 4.

Scalability to Mine Frequent Subgraphs Table 4 summarizes the number of subgraphs mined by traditional closed graph mining algorithm (column 6) and the proposed method (column 7). Over all graph sets, the number of sub-graphs selected by closed graph mining algorithm is at least two times and up to eleven times greater than that of M b Another most remarkable pair of columns are columns 5 and 9, which are respectively the input support and the minimal support among all subgraphs selected by M b T. Their mag-nitude of difference is from  X  10 2 to  X  10 3 .

Solving Combinatorial Explosion for Frequent Sub-graph Mining We also examined if the closed graph min-ing algorithm is able to generate any subgraphs if the input support is chosen to be the minimal support of all subgraphs selected by M b T.However,asindicatedincolumn10ofTa-ble 4, the program consumed all memory that could be al-located by Linux, and none of them could finish. This not only justifies the scalability of the proposed algorithm on frequent subgraph mining, but also provides a solution to combinatorial explosion for the same context.

Accuracy of Mined Frequent Subgraphs Similar to frequent itemsets, performances of M b Tarecomparedwith a classifier (E.g. DT) trained with much larger number of frequent subgraphs returned by the closed subgraph mining algorithm with the same invocation support. The results of the same classifier built on the mined subgraphs by M b T are also reported. Using the subgraphs selected by M b T (columns 7 in Table 4), and the much larger set of subgraphs mined by the closed graph mining algorithm (columns 6 in Table 4), Table 5 summarizes the AUC and accuracy results of M b T and a decision tree. Since these datasets are rather
Table 5: Performances of DT,M b TandDTM b T skewed, AUC or area under curve is a more appropriate measure than accuracy only. As highlighted in the table, the proposed method (M b T) and the decision tree built on the mined subgraphs by M b T(DTM b T) have achieved higher AUC and accuracy in almost all of the graph data than the decision tree constructed on the larger subgraph sets (DT). Across all datasets, the average improvement in AUC is 0.04 or 4%. Importantly, the most significant improvement, 21%, is achieved on the most skewed dataset NCI1 (1% positive).
We have also compared the AUC of the proposed method with two benchmark results where the graphs are gener-ated with the batch two step approach: first enumerating closed graphs with support 5% and then use feature selec-tion to choose the top 1000. The results are summarized in Table 6. There are two benchmark methods involved. Among them,  X  X rg X  is trained on the frequent subgraphs mined from the original skewed training set. On the other hand,  X  X Blcd X  use the subgraphs mined from a  X  X ebalanced X  sample where skewed positives are always kept, but nega-tives are down-sampled. Obviously, over all datasets, the AUC scores achieved by M b T and the decision tree built on the mined subgraphs by M b T(DTM b T) consistently dom-inate those of org and rBlcd via C4.5. For seven out of eleven graph sets, M b TorDTM b T performs significantly better than org and rBlcd based on SVM.
The usage of frequent pattern in classification has been explored by many recent studies. The association between frequent patterns and class labels is used for prediction. Ear-lier studies on associative classification [19, 18, 27] mainly fo-cus on mining high-support, high-confidence rules and build-ing a rule-based classifier. Prediction is made based on the top ranked rule or multiple rules. A recent work on top-rule mining [7] discovers top-k covering rule groups for high-dimensional gene expression profiles. A classifier RCBT is constructed from the top-k covering rule groups and achieves very high accuracy. Harmony [23] is another rule-based classifier which directly mines classification rules. It uses an instance-centric rule-generation approach and assures for each training instance, that one of the highest-confidence rules covering the instance is included in the rule set. In ad-dition, [5] is a newly proposed frequent pattern-based clas-sification method. Highly discriminative frequent itemsets are selected to represent the data in a feature space, based on which learning algorithm can be used for model learning. Table 6: AUC of M b T, DT M b T vs. Benchmarks With no initial feature vector representation, the primary problem in classification of complex data such as graphs is feature invention. In recent years, much work has been car-ried out to address the graph classification problem. Basi-cally these studies can be divided into three approaches: (1) structure or fragment-based approach [15, 9, 22], (2) kernel-based approach [20, 10], and (3) boosting method [16]. Typi-cally, the basic idea of structure or fragment-based approach is to extract frequent substructures [15, 9], local graph frag-ments [22], or cyclic patterns and trees [14] and use them as descriptors to represent the graph data. Studies with kernel-based approach aim at designing effective kernel functions to measure the similarity between graphs.

Several recent proposals have discussed how to make fre-quent pattern mining more co nscious of memory hierarchy and architecture, including [11, 8]. [11] proposed a cache-conscious prefix tree which improves spacial locality and en-hances the benefits from hardware cache line prefetching. [8] proposed a parallel mining algorithm of sequential pat-terns on a distributed memory system. These techniques are related to our mining task and can be applied to fur-ther improve the efficiency of the proposed method. Besides these efficient algorithms on the architecture level, there are some up-to-date methods DDPMine[6] and LEAP[24] on al-gorithm level which directly mines the most discriminative pattern via specially designed heuristics without mining the complete set of frequent patte rns. Since a lot of search space can be pruned, these methods can still find many of the most discriminative patterns, but are much more efficient than traditional frequent pattern mining methods. The dif-ference of this paper from DDPMine and LEAP is that the proposed techniques are applicable to frequent patterns in general, not limited to only either itemsets (DDPMine) or sub-graphs (LEAP) [24]. The accuracy of DDPMine mined itemsets as reported in Table IV of [6] are comparable and very similar to those numbers in Table 3 of this paper on M b T. Similar to the closed pattern mining algorithms called by M b Tinthispaper,M b T can also invoke the most recently proposed methods DDPMine (or LEAP) at its internal node to mine candidate features to split its data space.
To solve the scalability issue of mining frequent pattern as feature vectors from semi-structured and unstructured data, traditional methods employ a two-step batch process that first enumerates all candidate features, then performs feature selection. This process has limited success in identi-fying a small and compact set of f eatures. Furthermore, dif-ferent techniques are re-invented to reduce the search space for different types of patterns: frequent itemsets, frequent subgraphs, and sequential patterns. In other words, each technique is hard to generalize across different problems. To address these problems and others discussed in the pa-per, we propose a divide-and-conquer approach. The pro-posed method constructs a model-based search tree as it recursively invokes some frequent pattern enumeration al-gorithm. The main idea is to mine a discriminative feature that divides a subset of examples into purer subspaces that previously chosen patterns fail to distinguish. This process recursively runs on smaller and smaller data subspaces un-til either the subspace is too small or every example in the subspace belongs to the same class. At the end of feature discovery process, we have both a predictive decision tree and a set of discriminative features kept in non-leaf nodes of the tree. The proposed method can mine predictive pat-terns with extremely low global support, scales linearly to the scaled problem size and does not overfit.

Experimental studies have been conducted on both fre-quent itemset and graph mining problems. We have selected some of the most difficult datasets in each field. For exam-ple, some graph datasets have highly skewed distribution (1% positives) and the chemical compounds have hundreds of edges and vertices. The baseline algorithms invoked by model-based search tree are ba sic closed pattern algorithms. For scalability, the total number of patterns both enumer-ated during the pattern mining process and finally selected by the proposed algorithm is up to 10 3 smaller than those generated by the baseline algorithm using the same input support. The minimal support of the patterns discovered by the proposed algorithm can be so low (such as, 0.03%) that calling the closed pattern mining algorithms to enu-merate these features is impossible due to combinatorial ex-plosion and resource constraints. For predictive quality of those mined patterns, on the frequent itemset data, the ac-curacy is as good as most recently proposed methods, and significantly better than a model constructed from the large set of patterns discovered by traditional closed pattern min-ing. On the challenging skewed graph mining problem, the AUC using the mined subgraphs is up to 21% higher than comparable benchmark methods.

Future Work : (1) We generated biased dataset and found that M b T can still find good features even when the train-ing and testing data follow significantly different prior class distribution. More studies are being conducted. (2) M b Tis a scalable frequent pattern mining algorithm not limited to just itemsets and sub-graphs. It is interesting to systemati-cally compare M b T with heuristic-based scalable algorithm designed for different types of frequent patterns. To com-pare with DDPMine [6] on itemsets and with LEAP [24] on graphs, M b T can either invoke closed pattern methods [12, 25] or call DDPMine/LEAP instead at internal nodes. The research of Kun Zhang was partially funded by Louisiana Cancer Research Consortium.
