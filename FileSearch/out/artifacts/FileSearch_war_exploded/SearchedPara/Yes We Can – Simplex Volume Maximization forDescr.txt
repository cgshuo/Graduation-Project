 Matrix factorization methods are among the most common techniques for detecting latent components in data. Popular examples include the Singular Value Decomposition or Non-negative Matrix Factorization. Unfortunately, most meth-ods suffer from high computational complexity and therefore do not scale to massive data. In this paper, we present a lin-ear time algorithm for the factorization of gigantic matrices that iteratively yields latent components. We consider a constrained matrix factorization s.t. the latent components form a simplex that encloses most of the remaining data. The algorithm maximizes the volume of that simplex and thereby reduces the displacement of data from the space spanned by the latent components. Hence, it also lowers the Frobenius norm, a common criterion for matrix factorization quality. Our algorithm is efficient, well-grounded in distance geometry, and easily applicable to matrices with billions of entries. In addition, the resulting factors allow for an in-tuitive interpretation of data: every data point can now be expressed as a convex combination of the most extreme and thereby often most descriptive instances in a collection of data. Extensive experimental validations on web-scale data, including 80 million images and 1.5 million twitter tweets, demonstrate superior performance compared to related fac-torization or clustering techniques.
 I.5.3 [ Pattern Recognition ]: Clustering X  algorithms ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  clustering Algorithms Clustering, Matrix Factorization, Distance Geometry, End-member Determination
Understanding data by unmixing its latent components is an important task that received increasing attention through-out the last years. Common applications are found within various disciplines, including Geology, Economics, or As-tronomy [2, 5]. It also plays an important role for data analysis at web-scale, i.e. social networks or text mining us-ing Internet data. The main idea is to automatically acquire a descriptive representation by explaining data as a linear combination of certain important or dominant latent com-ponents. In this paper, we consider representations where data matrix V can be decomposed into a product of two lower rank matrices W H . The matrix W contains the la-tent components or basis vectors, H contains the mixing coefficients. In particular, we focus on a constraint factor-ization that restricts the representation to convex combi-nations of latent components and has already been consid-ered in Convex-NMF (C-NMF) [4], Convex-hull NMF (CH-NMF) [6], or Archetypal Analysis (AA) [3].

Convexity constraints yield latent components with inter-esting properties: First, the basis vectors are included in the data set and reside on actual data points; they are therefore readily interpretable even to non-experts. Second, convex-ity constrained basis vectors usually correspond to the most extreme data points and not to the most average ones which often further improves interpretability. Third, they span a simplex that encloses most of the remaining data.

In this paper, we present a novel algorithm to determine convexity constrained latent components. Simplex Volume Maximization runs in linear time and is, to the best of our knowledge, the fastest algorithm to date for solving the task at hand. Further, it is the first algorithm in this area that does not require subsampling for handling gigantic matri-ces. With respect to common error measures, such as the Frobenius norm, it shows a similar or even better perfor-mance than related methods. What might be even more important, is that it solely relies on iterative distance com-putations. As such, it inherently allows for parallelization and is well suited for web-scale problems.
Clustering is arguably one of the most common steps in data analysis. Dealing with n samples of d -dimensional vec-torial data gathered in a data matrix V d  X  n , the problem of determining useful clusters corresponds to finding a set of k n centroid vectors W d  X  k . If we express the mem-bership of data points in V to the centroids in W using a coefficient matrix H k  X  n , we note that clustering can be cast as a matrix factorization problem which aims at minimizing the expected Frobenius norm k V  X  W H k .

In this paper, we study a clustering approach where the data are expressed as convex combinations of certain points in V . The underlying problem can be formulated as where G  X  R n  X  k , H  X  R k  X  n are coefficient matrices such that H is restricted to convexity and G is restricted to unary column vectors In other words, the factorization in (1) approximates V us-ing convex combinations where the basis vectors W = V G are data points selected from V . The goal now is to deter-mine a basis that minimizes the Frobenius norm When minimizing (3), we have to simultaneously optimize W and H which is generally considered a difficult problem and known to suffer from many local minima. Archetypal Analysis (AA) as introduced in [3] applies an alternating least squares procedure where each iteration requires the solution of several constrained quadratic optimization prob-lems. It solves the case where G is restricted to convexity instead of to unarity. Convex-NMF (C-NMF) according to [4] iteratively updates intermediate matrices of size n  X  n . Neither approach scales well to gigantic data matrices.
Our contribution in this paper is a novel, highly efficient algorithm for estimating W = V G . It is based on the observation that, if v j is expressed as a convex combination v j = W h j , the coefficient vectors h j reside in a ( k  X  1)-simplex whose k vertices correspond to the basis vectors in W . Because of this duality, we may use the terms polytope and simplex interchangeably in the following.
If we assume that the basis vectors W d  X  k for a convex combination are selected from actual data samples v j  X  V , it is easy to prove that extending a given simplex W d  X  k adding a vertex w k +1 sampled from a data matrix V will not increase the Frobenius norm of the optimal convex approximation of the data. That is  X   X   X  if H k  X  n and H ( k +1)  X  n are convexity constrained coefficient matrices that result from solving constrained quadratic op-timization problems.

This hints at the idea of volume maximization for matrix factorization. Any increase of the volume of the k -simplex encoded in W will reduce the overall residual of the recon-struction. Next, we derive a highly efficient volume maxi-mization algorithm that determines a suitable basis W for convex reconstruction of a set of data. It is rooted in dis-tance geometry which studies sets of points based only on the distances between pairs of points.
 Distance geometry draws heavily on the notion of the Cayley-Menger determinant (CMD) [1] which indicates the volume of a polytope or simplex. Given the lengths d i,j the edges between the n + 1 vertices of an n -simplex S , its volume is given by where det( A ) = is the Cayley-Menger determinant.

With respect to data analysis, our goal is to select vertices { w 1 ,..., w k }  X  V such that they maximize the volume of the corresponding simplex. If a number of vertices has al-ready been acquired in a sequential manner, we can prove
Theorem 1. Let S be an ( n  X  1) -simplex. Suppose that the vertices w 1 ,..., w n are equidistant and that this distance is a . Also, suppose that the distances between vertex w n +1 and the other vertices are given by { d i,n +1 ,...,d n,n +1 the volume of S is determined by Theorem 1 indicates that instead of determining a suitable W from minimizing the Frobenius norm, we may equiva-lently determine a solution from fitting a simplex of maximal volume into the data. Such a simplex could be found by op-timizing the volume using the Cayley-Menger determinant. However, for large data sets this approach is ill-advised as it scales quadratically O ( n 2 ) with the number of samples. Fortunately, it is possible to iteratively determine a set of k basis vectors in O ( kn ) that maximize the volume of the simplex. Given a simplex S consisting of k  X  1 vertices, we simply seek to find a new vertex v  X   X  V such that
From Theorem 1 we can directly derive an iterative algo-rithm for finding the next best vertex (note that we omit constant values). After some tedious algebra, we arrive at v  X  = argmax This leads to the simple and efficient Simplex Volume Maximization (SiVM) Algorithm 1. We note that the pair-wise distances computed in earlier iterations can be reused in later steps. For retrieving k latent components, we need to compute the distance to all data samples exactly k + 1 times. The distances are computed with respect to the last selected basis vector. Informally, the algorithm can be for-mulated as finding vertex m + 1 that maximizes the simplex volume given the first m vertices. Figure 1 exemplifies how SiVM iteratively determines basis vectors.

Computing the coefficient matrix H is straight forward once a suitable set of basis vectors W has been determined. Algorithm 1 Simplex Volume Maximization (SiVM) 2: w 1 = argmax 3: for K = 2 . . . k do 7: w K = argmax 8: end for This process can be parallelized since the coefficients of data vectors v i and v j are independent and result from solving the following constrained quadratic optimization problem
Regarding computational complexity, SiVM basically con-sists of iterative distance computations. For each basis vec-tor w i we have to compute the distance d ( w i , v i ) , v only once. If we assume k basis vectors, this translates to O ( kn ) where k n . Note that we omit constants for the used distance metric. The 3 simple additive operations in steps 4, 5, and 6 of Algorithm 1 do not majorize compu-tation times for large n and conventional distance metrics (e.g. for a data set of 80 million images, they require less than 0.1% of the overall computation time).
In this section, we report on a series of experiments in-tended to evaluate the maximum volume algorithm in terms of computational efficiency and approximation quality.
Observation 1. If S is an n -simplex whose n +1 vertices were sampled from a large distribution s.t. they maximize the simplex volume and enclose most data, then the vectors rep-resenting the vertices are almost perpendicular on average.
Observation 1 does not allow a formal proof as it is essen-tially dependent on the data under consideration. Assume, Figure 3: Computational efficiency and approxima-tion quality of the Simplex Volume Maximization (SiVM) algorithm and AA. The x-axis shows the number of data samples, the y-axes show (averaged) Frobenius norm or computation time in seconds. for example, an iid data distribution in a 3D-cube. The vertices of a polytope that is supposed to enclose most of the data must reside on data points near the corners of that cube. Next, we provide empirical evidence that the proposed algorithm does indeed select vertices like this.

For example, we consider a set of 80 million tiny color im-ages [7]. We experimented with the publicly available 384-dimensional gist descriptors computed from these images. As this descriptor encodes an image using various Fourier bases, we expect SiVM to find images that resemble Fourier bases. Obviously, a data set consisting of random Inter-net images will hardly contain any real depiction of Fourier bases, but it may contain images that closely resemble them. Figure 2 shows the first 10 basis vectors determined by our algorithm. Apparently, they indeed resemble 2D Fourier ba-sis elements and are pairwise (almost) perpendicular.
To best of our knowledge, this is the first time that a data matrix of this proportion is reported to have been factorized using a non-negative factorization approach. It has a size of 79 , 302 , 017  X  384 = 30 , 451 , 974 , 528 elements. Using the algorithm proposed in this paper, it takes about 4 hours to compute a single basis vector on a single core machine.
In an ongoing project on social media analysis, we ex-amine the structure of data collected from micro blogging
Table 1: Basis tweets found for Barack Obama. services such as Twitter. We apply statistical methods to the problem of authorship analysis, in particular, we aim at determining whether or not there is a single person or a team of authors blogging under a single pseudonym.

In our experiments, we examine large collections of tweets associated to particular famous users. One of the benefits of SiVM for latent component analysis is, that the resulting basis vectors are readily interpretable even to non-experts. By extracting the most extreme instances of a data set, SiVM yields basis elements that are well distinguishable data points. If we capture the style of a tweet in appropriate fea-tures, it is likely that differences due to different authors are immediately visible in the resulting latent components.
Here, we consider a data set of 1.5 million twitter tweets of more than 300 popular twitter users (including tweets from Ashton Kutcher, Barack Obama, Britney Spears among oth-ers). From each tweet we compute a set of 30 stylistic fea-tures. These mainly indicate the ratios between adverbs, verbs, signs, own words, or punctuation signs, etc. and the length of a tweet. For each user as well as for the com-plete data we extract a number of basis vectors using the proposed algorithm. Computing all these basis vectors re-quires only a few seconds. An example of the resulting basis vectors, i.e. the corresponding most extreme tweets, for an exemplary user are listed in Table 1. Next to each tweet we show to which degree that basis vector contributes to the overall reconstruction (e.g. a value of 0.5 indicates that this tweet represents a style of writing that can be found among 50% of all tweets of this user).

With only 10 basis tweets per twitterer, we can already gain interesting insights. For example, for Barack Obama, we see noticeable differences. It is because of the spirit and resilience of Americans that I have never been more hope-ful about America X  X  future than I am tonight tells a different story than his rather short status messages. More or less sur-prisingly, the popular slogan Yes we can. was also detected among the first 10 basis vectors. Note again that at this point we do not apply any linguistic or semantic analysis. Rather, it is the extreme nature of the latent components found through SiVM that suggest different authors.
We presented a novel method for finding latent compo-nents in massive data-sets. Based on principles of distance geometry we have shown that for convexity constrained fac-torizations minimizing the Frobenius norm is equivalent to maximizing the volume of a simplex whose vertices cor-respond to basis vectors. The proposed approach allows for factorizing data matrices of several billion entries. To the best of our knowledge, the factorization of the matrix of 80,000,000 images presented this paper constitutes the first instance of a factorization of matrices of this size that, though carried out on a single computer, did not have to resort to sophisticated subsampling techniques. [1] L. M. Blumenthal. Theory and Applications of Distance [2] B. Chan, D. Mitchell, and L. Cram. Archetypal [3] A. Cutler and L. Breiman. Archetypal Analysis. [4] C. Ding, T. Li, and M. Jordan. Convex and [5] L. Miao and H. Qi. Endmember Extraction From [6] C. Thurau, K. Kersting, and C. Bauckhage. Convex [7] A. Torralba, R. Fergus, and W. T. Freeman. 80 Million
